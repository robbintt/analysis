---
ver: rpa2
title: Understanding Retrieval Augmentation for Long-Form Question Answering
arxiv_id: '2310.12150'
source_url: https://arxiv.org/abs/2310.12150
tags:
- documents
- answer
- evidence
- human
- webgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a controlled study on how retrieval augmentation
  impacts long-form answer generation by language models (LMs). Two settings are compared:
  fixing LMs and varying evidence documents, and vice versa.'
---

# Understanding Retrieval Augmentation for Long-Form Question Answering

## Quick Facts
- **arXiv ID:** 2310.12150
- **Source URL:** https://arxiv.org/abs/2310.12150
- **Reference count:** 40
- **Key outcome:** Controlled study shows retrieval augmentation substantially changes LM generation, with relevant documents leading to longer, less repetitive answers with higher perplexity, but attribution quality varies across LMs.

## Executive Summary
This paper presents a controlled study examining how retrieval augmentation impacts long-form answer generation by language models. The authors compare two settings: fixing LMs while varying evidence documents, and vice versa. Using the ELI5 dataset with four different evidence document sources (Human Demonstration, WebGPT Retriever, Bing Search, Random), they analyze how document relevance affects generated answers through automatic metrics and manual attribution annotations. The study reveals that relevant documents substantially change LM generation patterns, leading to longer answers with higher perplexity and less repetition. However, attribution quality varies significantly across different base LMs, with WebGPT showing the most faithful synthesis of evidence.

## Method Summary
The study uses the ELI5 dataset to generate long-form answers using three LMs (WebGPT, GPT-3.5, Alpaca) with four different evidence document sets. The methodology involves controlled experiments where either LMs are fixed while varying evidence documents, or vice versa. Automatic metrics including length, self-BLEU, RankGen, and perplexity are collected for generated answers. Attribution quality is assessed through manual annotation (supported, partially supported, not supported) and automatic detection using NLI models. The study also examines information ordering patterns between evidence documents and generated answers.

## Key Results
- Relevant documents substantially change LM generation, producing longer answers with higher perplexity and less repetition (lower self-BLEU)
- Attribution quality varies across base LMs, with WebGPT being most faithful to evidence documents
- The last sentence of generated answers is particularly prone to being unsupported by evidence
- NLI models can identify unsupported sentences with reasonable accuracy but fall short of human agreement

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Retrieval augmentation changes LM generation substantially, especially when evidence documents are relevant.
- **Mechanism:** When relevant documents are prepended, the LM incorporates new information, leading to longer answers with higher perplexity and less repetition (lower self-BLEU).
- **Core assumption:** The LM can effectively attend to and use the information in the prepended documents.
- **Evidence anchors:**
  - [abstract]: "relevant documents substantially change LM generation, leading to longer, less repetitive answers with higher perplexity."
  - [section 4]: "Overall, prepending relevant documents yields bigger changes for both models compared to prepending random documents."

### Mechanism 2
- **Claim:** The order of information in evidence documents influences the order of information in generated answers.
- **Mechanism:** The LM tends to follow the order of the evidence documents, even when they are concatenated and not coherent.
- **Core assumption:** The LM processes the prepended documents sequentially and maintains this order in the generated text.
- **Evidence anchors:**
  - [section 6]: "We identify a linear correspondence pattern, with information mentioned earlier in the evidence document tend to appear earlier in the generated answer."
  - [section 6]: "This suggests the order of evidence documents will be reflected in the order of generated contents."

### Mechanism 3
- **Claim:** Attribution quality varies across base LMs, even when provided with the same evidence documents.
- **Mechanism:** Different LMs have varying abilities to generate sentences that can be attributed to the evidence documents.
- **Core assumption:** The LM's training and architecture influence its ability to ground generated text in the evidence.
- **Evidence anchors:**
  - [abstract]: "Attribution quality varies across base LMs, with WebGPT being most faithful to evidence."
  - [section 6]: "We observe that generations from the WebGPT model are most faithful to the evidence documents."

## Foundational Learning

- **Concept:** Attention mechanisms in Transformers
  - **Why needed here:** Understanding how LMs attend to and use the information in prepended evidence documents is crucial for interpreting the results.
  - **Quick check question:** How does the attention mechanism allow an LM to focus on specific parts of the input sequence?

- **Concept:** Perplexity as a measure of text generation quality
  - **Why needed here:** Perplexity is used to quantify the unexpectedness of the generated text, which is influenced by the relevance of the evidence documents.
  - **Quick check question:** What does a higher perplexity score indicate about the generated text?

- **Concept:** Natural Language Inference (NLI) models for attribution detection
  - **Why needed here:** NLI models are used to automatically identify unsupported sentences in the generated answers.
  - **Quick check question:** How can an NLI model be used to determine if a generated sentence is supported by the evidence documents?

## Architecture Onboarding

- **Component map:**
  - Question Input
  - Evidence Document Retrieval (Human, WebGPT, Bing, Random)
  - LM (WebGPT, GPT-3.5, Alpaca)
  - Answer Generation
  - Attribution Annotation (Human, NLI models)
  - Automatic Metrics (Length, Self-BLEU, RankGen, Perplexity)

- **Critical path:**
  1. Retrieve evidence documents for a question
  2. Prepend documents to the question and provide to LM
  3. Generate answer
  4. Evaluate answer using automatic metrics and attribution

- **Design tradeoffs:**
  - Using different LMs allows comparison of attribution quality but introduces variability due to different architectures and training.
  - Manual attribution annotation is more accurate but expensive, while automatic methods are faster but less accurate.

- **Failure signatures:**
  - If the LM fails to attend to the evidence documents, the automatic metrics may not show significant changes with different document sets.
  - If the NLI model for attribution detection is not well-trained, it may incorrectly label supported sentences as unsupported.

- **First 3 experiments:**
  1. Generate answers using WebGPT model with WebGPT evidence documents and evaluate using automatic metrics and human annotation.
  2. Generate answers using GPT-3.5 model with different evidence document sets (Human, WebGPT, Bing, Random) and compare automatic metrics.
  3. Use the T5 NLI model to automatically detect unsupported sentences in answers generated by different LMs with various evidence document sets.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can we design and train language models specifically to improve attribution quality when using retrieval augmentation for long-form question answering?
- **Basis in paper:** The paper states that "LMs trained without retrieval and attribution in mind does not always generate sentences that can be attributed to in-context evidence documents, even when provided relevant documents only. This motivates designing and training LMs after introducing in-context evidence documents."
- **Why unresolved:** While the paper identifies this as an important direction for future work, it does not propose specific methods or techniques for how to design and train such models. The question of what architectural changes, training objectives, or data augmentation strategies would be most effective remains open.
- **What evidence would resolve it:** Concrete experimental results comparing different approaches for training retrieval-augmented language models with a focus on improving attribution quality, such as fine-tuning with attribution-aware loss functions, using synthetic data to teach models how to properly synthesize information from multiple sources, or incorporating attribution quality metrics into the model's training objective.

### Open Question 2
- **Question:** What are the key factors that determine whether a language model will correctly attribute information to the most relevant evidence document when multiple documents are provided?
- **Basis in paper:** The paper observes that "the order of information presented in the evidence documents will impact the order of information presented in the generated answer" and that models "successfully ignore irrelevant documents". However, it does not provide a detailed analysis of what factors (e.g., document relevance scores, document length, document position) most strongly influence a model's ability to identify and attribute to the most relevant source.
- **Why unresolved:** While the paper hints at some factors (e.g., document relevance, order), it does not conduct a systematic study to isolate and quantify the impact of each factor. Understanding these factors is crucial for designing better retrieval and attribution systems.
- **What evidence would resolve it:** Controlled experiments manipulating individual factors (e.g., document relevance, length, position) and measuring their impact on attribution quality. Analysis of attention patterns or other model internals to understand how models decide which documents to rely on.

### Open Question 3
- **Question:** How can we improve the automatic detection of unsupported sentences in long-form question answering outputs?
- **Basis in paper:** The paper evaluates several automatic methods for detecting unsupported sentences and finds that while they outperform simple baselines, they still fall short of human agreement by around 15% in accuracy. It suggests that "with our annotated new dataset, together with other related datasets, one can investigate improving automatic attribution methods."
- **Why unresolved:** The paper identifies the gap between automatic and human performance but does not propose specific improvements or new approaches to close this gap. The question of what techniques (e.g., better NLI models, few-shot prompting, model distillation) would be most effective remains open.
- **What evidence would resolve it:** Experimental results comparing different approaches for automatic attribution detection, such as using larger or more diverse NLI models, applying few-shot prompting techniques, or distilling attribution knowledge from human annotations into a smaller model. Analysis of where automatic methods tend to make mistakes compared to humans could also provide insights for improvement.

## Limitations
- Manual attribution annotation is subjective and resource-intensive, limiting scalability
- Automatic attribution detection via NLI models falls short of human agreement, reducing reliability
- Findings are based on ELI5 dataset and may not generalize to other domains or shorter-form generation tasks

## Confidence
- **High:** Impact of relevant documents on generation length, perplexity, and repetition patterns (consistently observed across multiple experiments)
- **Medium:** Attribution quality findings across different LMs (relies more heavily on subjective human annotations and automatic NLI model performance)
- **Low:** Broader generalizability claims to other tasks or domains (limited to ELI5 dataset and specific model configurations)

## Next Checks
1. **Cross-domain validation:** Test whether observed patterns hold when applying methodology to different question-answering datasets (e.g., HotpotQA, Natural Questions) or other text generation tasks like summarization or dialogue
2. **Model architecture ablation:** Conduct experiments varying attention mechanisms or input processing strategies (e.g., document interleaving vs. concatenation) to understand specific mechanisms of evidence incorporation
3. **Extended attribution analysis:** Perform granular analysis of attribution patterns examining whether certain types of information (factual claims, opinions, examples) are more or less likely to be properly attributed, and test if fine-tuning LMs specifically for attribution improves performance