---
ver: rpa2
title: Maneuver Decision-Making Through Automatic Curriculum Reinforcement Learning
  Without Handcrafted Reward functions
arxiv_id: '2307.06152'
source_url: https://arxiv.org/abs/2307.06152
tags:
- learning
- combat
- curriculum
- target
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes an automatic curriculum reinforcement learning
  method for unmanned combat aerial vehicle (UCAV) maneuver decision-making in air
  combat. The method enables agents to learn effective decisions from scratch without
  handcrafted reward functions by dividing the task into a series of sub-tasks from
  easy to difficult based on the range of initial states.
---

# Maneuver Decision-Making Through Automatic Curriculum Reinforcement Learning Without Handcrafted Reward functions

## Quick Facts
- arXiv ID: 2307.06152
- Source URL: https://arxiv.org/abs/2307.06152
- Reference count: 32
- Agents learn effective UCAV maneuver decisions without handcrafted rewards by using automatic curriculum learning

## Executive Summary
This paper presents an automatic curriculum reinforcement learning method for unmanned combat aerial vehicle (UCAV) maneuver decision-making in air combat. The approach enables agents to learn effective decisions from scratch without handcrafted reward functions by dividing the task into a series of sub-tasks from easy to difficult based on the range of initial states. The method uses sparse rewards (1 for win, -1 for loss, 0 otherwise) combined with a curriculum that progressively expands difficulty as the agent demonstrates proficiency. Trained agents are able to make effective decisions for tracking, attacking and escaping targets in various situations.

## Method Summary
The method implements automatic curriculum reinforcement learning (ACRL) for UCAV maneuver decision-making using Proximal Policy Optimization (PPO) with continuous action spaces. The curriculum is structured around initial state intervals, where azimuth angles and distances are progressively expanded based on win/loss statistics. The agent learns through sparse rewards without any handcrafted reward functions, with curriculum difficulty increasing only when the agent achieves sufficient wins in easier scenarios. The approach combines aircraft and missile dynamics models with PPO training, using automatic curriculum generation that adjusts difficulty based on performance thresholds.

## Key Results
- Trained agents successfully learn effective tracking, attacking, and escaping decisions without handcrafted reward functions
- Ablation study shows ACRL is essential, as agents cannot learn effective policies with only standard RL
- ACRL gradually increases win rates during training, demonstrating its efficiency as a learning method

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Automatic curriculum learning divides the air combat maneuver decision-making task into a sequence of sub-tasks from easy to difficult, enabling agents to learn progressively.
- Mechanism: The method uses the range of initial states (normalized to [-1, 1]) to distinguish curricula of different difficulty levels. It starts with a proper subset of initial angles and distances where targets are more likely to be hit, then gradually expands the intervals as the agent demonstrates proficiency.
- Core assumption: Initial states with smaller azimuth angles and specific distance ranges are easier for the agent to succeed in, creating a natural progression of difficulty.
- Evidence anchors:
  - [abstract] "The range of initial states are used for distinguishing curricula of different difficulty levels, thereby maneuver decision is divided into a series of sub-tasks from easy to difficult"
  - [section] "When the initial azimuth angle of the opponent is larger, it is more difficult for the agent to overcome the opponent"
- Break condition: If the interval expansion criteria (20+ wins and fewer losses) are not met, the curriculum stops expanding and the agent cannot learn to handle more difficult scenarios.

### Mechanism 2
- Claim: Sparse rewards combined with automatic curriculum learning enable effective training without handcrafted reward functions.
- Mechanism: The agent only receives rewards of 1 (win), -1 (loss), or 0 (draw) based on the outcome of air combat, rather than continuous handcrafted rewards. The curriculum structure ensures the agent encounters winnable scenarios early in training.
- Core assumption: Sparse rewards are sufficient when combined with curriculum learning that guarantees early success experiences.
- Evidence anchors:
  - [abstract] "without the need to spend effort designing reward functions"
  - [section] "we do not use any handcrafted reward function. Namely, the reward obtained by the agent is 1 if it defeats the opponent, -1 if it is defeated by the opponent, and 0 in other cases"
- Break condition: If the curriculum doesn't create sufficiently easy early tasks, the agent receives only zero rewards and cannot learn any useful policy.

### Mechanism 3
- Claim: Continuous action spaces are essential for realistic maneuver decision-making in air combat.
- Mechanism: The method uses continuous control signals (nx, nz, µ, nmc, nmh) for aircraft and missile dynamics rather than discretizing actions, allowing more nuanced and realistic maneuvers.
- Core assumption: Real air combat requires continuous control adjustments rather than discrete action choices.
- Evidence anchors:
  - [section] "Its action space is continuous rather than discrete, which makes it more realistic"
  - [section] "Previous researches discretized action space to reduce the complexity of maneuver decision-making, which is not practical since the real action space is continuous"
- Break condition: Discrete action spaces would limit the agent's ability to execute fine-grained maneuvers required for realistic air combat scenarios.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: Air combat maneuver decision-making is modeled as an MDP where the agent observes states, takes actions, and receives rewards to maximize cumulative return
  - Quick check question: What are the four components that define an MDP in the context of this air combat problem?

- Concept: Proximal Policy Optimization (PPO)
  - Why needed here: PPO is the reinforcement learning algorithm used to train the agent, providing stable policy updates through clipped probability ratios
  - Quick check question: How does PPO's clipped objective prevent excessive policy updates compared to standard policy gradient methods?

- Concept: Curriculum Learning
  - Why needed here: Curriculum learning structures the training process from easy to difficult tasks, addressing the sparse reward problem in air combat
  - Quick check question: Why would a randomly initialized agent struggle to learn effective air combat maneuvers without curriculum learning?

## Architecture Onboarding

- Component map: Aircraft dynamics model -> Missile dynamics model -> PPO policy network -> Action execution -> Environment -> State observation -> Curriculum manager

- Critical path: Curriculum manager → PPO training loop → Policy network → Action execution → Environment → State observation → Curriculum manager

- Design tradeoffs:
  - Continuous vs discrete action spaces (realism vs computational efficiency)
  - Sparse vs handcrafted rewards (simplicity vs potential for faster learning)
  - Curriculum expansion rate (too fast leads to poor learning, too slow wastes training time)

- Failure signatures:
  - High draw rate indicates ineffective decision-making
  - Stagnant win rate suggests curriculum too difficult or PPO not converging
  - Large standard deviation in performance across runs suggests instability

- First 3 experiments:
  1. Test curriculum manager alone: Run simulations with fixed difficulty intervals to verify win/loss statistics are being tracked correctly
  2. Test PPO training without curriculum: Train with full difficulty range to confirm agents cannot learn effectively
  3. Test curriculum expansion logic: Start with easy intervals and verify they expand when win criteria are met

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the initial difficulty interval size and growth rate affect the overall performance and convergence speed of the ACRL method?
- Basis in paper: [inferred] The paper mentions that the interval length is increased by δ=0.1 when the number of wins exceeds 20 and the number of losses. However, the impact of different δ values on training efficiency is not explored.
- Why unresolved: The paper uses a fixed value of δ=0.1 without comparing alternative values or conducting sensitivity analysis. Different growth rates could potentially lead to faster convergence or better final performance.
- What evidence would resolve it: Experiments comparing training performance and convergence speed using different values of δ (e.g., 0.05, 0.1, 0.2) would show how interval size growth rate affects ACRL effectiveness.

### Open Question 2
- Question: How robust is the ACRL method to different air combat scenarios beyond the 2D plane case studied in the paper?
- Basis in paper: [explicit] The paper states "For simplicity, we only consider 2D plane" and "we only study the maneuver decision-making in the 2D plane." It also mentions that future work will extend the method to 3D.
- Why unresolved: The paper's results are limited to a simplified 2D scenario. Real air combat involves 3D maneuvering, different aircraft dynamics, and various environmental factors that could affect ACRL performance.
- What evidence would resolve it: Testing ACRL in full 3D scenarios with realistic aircraft dynamics, different initial conditions, and varying environmental factors would demonstrate the method's robustness across diverse combat situations.

### Open Question 3
- Question: How does ACRL compare to other curriculum learning approaches for sparse-reward tasks in terms of sample efficiency and final performance?
- Basis in paper: [explicit] The paper states "As shown in Figure 2, when using only RL, the number of win is always less" and "According to Figure 2, it can be seen that ACRL can gradually increase the number of win during training, indicating that ACRL is a concise and efficient method." However, it doesn't compare ACRL to other curriculum learning methods.
- Why unresolved: While the paper shows ACRL outperforms standard RL, it doesn't benchmark against other curriculum learning approaches that could potentially achieve similar or better results with different mechanisms.
- What evidence would resolve it: Direct comparisons of ACRL with other curriculum learning methods (e.g., teacher-student curriculum learning, goal proposal modules) on the same air combat task would reveal relative strengths and weaknesses in terms of sample efficiency and final performance.

## Limitations
- The method's effectiveness depends heavily on the specific curriculum structure, which may not generalize to all sparse-reward tasks
- Results are demonstrated only on simplified 2D air combat scenarios, not on realistic 3D combat with complex dynamics
- The curriculum expansion threshold ("greater than 20 wins and fewer losses") is vaguely defined and may require tuning for different tasks

## Confidence
**High confidence**: The core mechanism of using curriculum learning to structure training from easy to difficult tasks is well-supported by the results. The ablation study clearly shows that without curriculum learning, agents fail to learn effective policies.

**Medium confidence**: The claim that handcrafted rewards are unnecessary is supported by results on this specific task, but may not generalize to all reinforcement learning problems. The specific curriculum parameters (expansion rate, difficulty intervals) appear tuned for this particular problem.

**Low confidence**: The assertion that this approach will work equally well across diverse air combat scenarios or other domains is not sufficiently supported. The paper doesn't test the method's robustness to changes in aircraft dynamics, missile characteristics, or environmental conditions.

## Next Checks
1. **Curriculum sensitivity analysis**: Systematically vary the curriculum expansion parameters (win threshold, expansion rate δ) to determine how sensitive the approach is to these hyperparameters and identify optimal ranges for different task difficulties.

2. **Transfer learning evaluation**: Train agents on one set of initial state distributions and test their performance on novel distributions not seen during training to assess generalization capabilities beyond the curriculum.

3. **Reward structure comparison**: Implement alternative reward shaping approaches (e.g., distance-based rewards, time-penalized rewards) and compare learning efficiency and final performance against the sparse reward curriculum approach.