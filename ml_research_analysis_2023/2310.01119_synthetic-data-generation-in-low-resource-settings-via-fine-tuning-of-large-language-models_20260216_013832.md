---
ver: rpa2
title: Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of Large
  Language Models
arxiv_id: '2310.01119'
source_url: https://arxiv.org/abs/2310.01119
tags:
- data
- user
- system
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates using fine-tuned large language models (LLMs)
  to generate synthetic data for improving the performance of much smaller downstream
  models. The approach, called DAFTT (Data Augmentation via Fine-Tuning the Teacher
  LLM), involves fine-tuning a 20B GPT-NeoX LLM on a mixture of tasks and then using
  it to either annotate unlabeled data or generate entirely new input-output pairs.
---

# Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of Large Language Models

## Quick Facts
- arXiv ID: 2310.01119
- Source URL: https://arxiv.org/abs/2310.01119
- Reference count: 40
- Primary result: Fine-tuning a 20B LLM on as few as 125 examples significantly improves downstream model performance when combined with synthetic data generation

## Executive Summary
This study introduces DAFTT (Data Augmentation via Fine-Tuning the Teacher LLM), a method that fine-tunes large language models on small amounts of task-specific data to generate high-quality synthetic examples for downstream training. The approach addresses low-resource scenarios by leveraging a 20B GPT-NeoX LLM as a teacher, which is fine-tuned on task mixtures and then used to either annotate unlabeled data or generate entirely new input-output pairs. These synthetic examples are combined with a small fraction of the original training data to fine-tune smaller models like RoBERTa-Large and BART-Large, yielding significant performance improvements, especially when original training sets are small.

## Method Summary
The method involves three key stages: (1) Fine-tuning a 20B GPT-NeoX LLM on small subsets (1-10%) of downstream task datasets using Super-NaturalInstructions prompts and Adam optimizer with cosine learning rate schedule, (2) Generating synthetic data via annotation of unlabeled instances or generation of new input-output pairs at controlled temperatures (0.1 for annotation, 0.8 for generation), and (3) Combining synthetic data with small fractions of original training data to fine-tune downstream models (RoBERTa-Large for classification, BART-Large for generation) using standard hyperparameters. The approach was evaluated across four text classification and two text generation tasks.

## Key Results
- Fine-tuning the teacher LLM on as few as 125 examples (5% of RTE dataset) increases downstream model performance by multiple percentage points
- Annotation of unlabeled data outperforms generation when unlabeled data is available, but generation still provides benefits when it's not
- The fine-tuned 20B NeoX model outperforms GPT-3.5 (davinci-002) with 175B parameters when both are fine-tuned with limited data
- Both annotation and generation improve performance across all tasks, with larger gains observed for smaller initial training datasets

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning the large LLM on small amounts of task-specific data dramatically improves the quality of synthetic examples it generates by teaching the LLM to better align its generation patterns with the specific task's input-output format and style, reducing task-agnostic noise in outputs. The core assumption is that even minimal task-specific fine-tuning provides enough signal to meaningfully shift the LLM's generation distribution toward useful examples.

### Mechanism 2
The synthetic data generated by the fine-tuned LLM acts as a knowledge distillation bridge, transferring general task understanding from the LLM to the smaller downstream model. By generating high-quality synthetic input-output pairs, the LLM encodes its learned patterns into new examples that the downstream model can learn from, effectively compressing the LLM's knowledge into a smaller form.

### Mechanism 3
Annotation of unlabeled data is more effective than generation of entirely new examples because annotations leverage existing real data, ensuring synthetic labels are grounded in actual inputs, whereas generation must invent both input and output, introducing more variability and potential noise. Real inputs provide a more stable foundation for synthetic labeling than fully invented ones.

## Foundational Learning

- Concept: Knowledge distillation via synthetic data generation
  - Why needed here: The core innovation is transferring knowledge from a large teacher model to a smaller student model without direct logit supervision
  - Quick check question: What is the key difference between this approach and traditional knowledge distillation?

- Concept: In-context learning (ICL) and few-shot prompting
  - Why needed here: Understanding ICL is essential because the study leverages the LLM's ability to generalize from few examples during both fine-tuning and generation phases
  - Quick check question: How does ICL enable the LLM to perform well with limited fine-tuning data?

- Concept: Data augmentation strategies for low-resource NLP
  - Why needed here: The work explicitly targets low-resource settings, so familiarity with augmentation methods is necessary to understand the novelty and limitations
  - Quick check question: Why might traditional data augmentation fail in very low-resource settings compared to LLM-based approaches?

## Architecture Onboarding

- Component map: Teacher LLM (20B GPT-NeoX) → Fine-tuning module (Super-NaturalInstructions) → Synthetic data generator (annotation/generation) → Combined dataset → Student model (RoBERTa-Large/BART-Large) → Fine-tuning → Evaluation
- Critical path: 1. Fine-tune teacher LLM on small task-specific data, 2. Generate synthetic examples (annotation or generation), 3. Combine with original data, 4. Fine-tune student model, 5. Evaluate performance
- Design tradeoffs: Trade-off between synthetic data diversity (higher temperature) and accuracy (lower temperature), trade-off between using annotations (more grounded but needs unlabeled data) vs generation (more flexible but noisier), trade-off between fine-tuning time/cost of large LLM vs performance gains
- Failure signatures: Student model performance does not improve despite synthetic data addition, hallucinations or factual inaccuracies dominate synthetic examples, fine-tuning LLM with too little or noisy data degrades synthetic quality
- First 3 experiments: 1. Fine-tune teacher LLM on 1% of original training data and generate synthetic examples; evaluate student performance with and without augmentation, 2. Compare annotation vs generation modes for a single task; measure downstream performance differences, 3. Vary synthetic data proportion (e.g., 5%, 10%, 20%) and plot student model performance to identify saturation point

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality of synthetic data generated by fine-tuned LLMs compare to data generated by much larger LLMs (e.g., GPT-4) without fine-tuning? The paper only compares the fine-tuned NeoX-20B model to GPT-3.5 (175B parameters), not to the latest and largest LLMs available. Experiments comparing the synthetic data quality and downstream performance when using different sized LLMs (e.g., 20B vs. 175B vs. 540B+ parameters) with and without fine-tuning on limited data would resolve this.

### Open Question 2
What is the impact of the diversity of tasks used to fine-tune the teacher LLM on the quality of synthetic data generated for a specific downstream task? The paper uses a fixed multi-task dataset for fine-tuning but doesn't investigate how different task mixtures or task diversity might influence the synthetic data quality for different downstream tasks. Experiments varying the task mixtures used to fine-tune the teacher LLM and measuring the resulting synthetic data quality and downstream task performance would resolve this.

### Open Question 3
How does the performance of the downstream model change when using a combination of synthetic data generated by the fine-tuned LLM and human-annotated data, compared to using either source alone? While the paper shows that combining both data sources is effective, it doesn't investigate the optimal ratio of synthetic to human-annotated data or how performance scales with different mixtures. Experiments varying the ratio of synthetic to human-annotated data in the training set and measuring the resulting downstream model performance to find the optimal mix would resolve this.

## Limitations
- Relies on a single large language model (20B GPT-NeoX) and two downstream architectures (RoBERTa-Large and BART-Large), limiting generalizability across model families and sizes
- No systematic evaluation of synthetic data quality beyond performance metrics - no analysis of factual accuracy, coherence, or domain-specific validity
- The exact composition of the Super-NaturalInstructions fine-tuning mixture is unspecified, making it difficult to reproduce the precise conditions

## Confidence
- High confidence: The core finding that fine-tuning a large LLM on small task-specific data enables generation of useful synthetic examples for downstream training is well-supported by experimental results
- Medium confidence: The superiority of annotation over generation when unlabeled data is available is supported but could benefit from additional ablation studies on data quality
- Medium confidence: Claims about performance gains in very low-resource settings (1-5% of original data) are demonstrated but may not generalize to all task types or domains

## Next Checks
1. Conduct a comprehensive hallucination analysis by manually evaluating 100-200 synthetic examples from each task to quantify factual inaccuracies and assess their impact on downstream model reliability
2. Test the approach across different teacher model families (including open-source alternatives to GPT-NeoX) and student model sizes to establish robustness boundaries and identify optimal teacher-student pairings
3. Perform ablation studies varying the proportion of synthetic to real data in the augmented training set to determine the saturation point where additional synthetic examples no longer improve (or potentially harm) downstream performance