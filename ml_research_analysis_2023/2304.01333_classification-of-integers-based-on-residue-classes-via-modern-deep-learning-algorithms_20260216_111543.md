---
ver: rpa2
title: Classification of integers based on residue classes via modern deep learning
  algorithms
arxiv_id: '2304.01333'
source_url: https://arxiv.org/abs/2304.01333
tags:
- number
- learning
- divisibility
- feature
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors explored whether deep learning models can learn divisibility\
  \ rules for small primes (e.g., 2, 3, 7) from integer datasets without explicit\
  \ algorithmic rules. They tested multiple neural architectures (ANN, CNN, RNN, BERT)\
  \ and feature engineering approaches on integers up to 2\xB3\xB2."
---

# Classification of integers based on residue classes via modern deep learning algorithms

## Quick Facts
- arXiv ID: 2304.01333
- Source URL: https://arxiv.org/abs/2304.01333
- Reference count: 22
- Deep learning models can classify integers by divisibility when appropriate feature engineering is applied

## Executive Summary
This paper investigates whether deep learning algorithms can learn divisibility rules for small primes from integer datasets without explicit algorithmic instructions. The authors test multiple neural architectures (ANN, CNN, RNN, BERT) and AutoML platforms on integers up to 2³², finding that raw integer inputs perform poorly for classification tasks. Through systematic feature engineering—including digit-based encodings and Fourier series basis vectors—they achieve perfect or near-perfect accuracy for divisibility by small primes. The study concludes that feature engineering remains essential for performance, interpretability, and reduced model complexity, even with advanced AutoML and large language models like ChatGPT.

## Method Summary
The study evaluates multiple deep learning architectures and AutoML platforms on divisibility classification tasks for small primes. Researchers generate 50,000 uniformly sampled integers from [0, 2³²) and apply various feature engineering techniques including raw integers, binary representations, digit n-grams, and Fourier series basis vectors. They test ANN, CNN, RNN, and BERT models alongside commercial AutoML services (Azure, Vertex AI, SageMaker). A closed-form solution using linear regression on Fourier series basis is also implemented and compared against deep learning approaches.

## Key Results
- Raw integer inputs yielded ~33-50% accuracy, equivalent to random guessing for mod 2 and mod 3
- Feature-engineered approaches (digit sums, Fourier basis) achieved 100% accuracy for mod 3 and mod 7
- AutoML platforms failed without engineered features but succeeded when appropriate features were provided
- Linear regression on Fourier series basis provided interpretable closed-form solution with perfect accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fourier series basis vectors enable linear regression to perfectly classify divisibility by small primes
- Mechanism: The mod p function is periodic with period p, so it can be represented exactly by a finite Fourier series. Linear regression fits coefficients to these basis functions, capturing the periodicity without needing deep networks
- Core assumption: The integer range is large enough that training samples cover all residue classes sufficiently, and noise is minimal
- Evidence anchors:
  - [abstract] "We further proposed a closed form solution to the problem using the ordinary linear regression on Fourier series basis vectors, and showed its success"
  - [section] "The pointwise convergence of Fourier series is extremely crucial and provides the theoretical backup that sine and cosine pairs would do a good job of estimating periodic functions"
- Break condition: If the training set is too small relative to p, or if the integers are not uniformly sampled, regression coefficients may be poorly estimated, leading to misclassification

### Mechanism 2
- Claim: Feature engineering (e.g., digit-based encodings, summing digits) provides essential structure for deep models to learn divisibility rules
- Mechanism: Raw integer inputs lack discriminative features for divisibility; engineered features like digit sums or n-grams expose the arithmetic patterns (e.g., sum of digits mod 3). Deep models can then learn these patterns more easily
- Core assumption: The engineered features are mathematically linked to the divisibility rule (e.g., digit sum mod 3 equals the number mod 3)
- Evidence anchors:
  - [abstract] "Results showed that raw integer inputs performed poorly, but appropriate feature engineering (e.g., digit-based encodings, Fourier series basis) enabled perfect or near-perfect classification"
  - [section] "However, for each of the two cases above (divisibility by 2 and 3), we as human beings injected intellectual contribution to design the appropriate algorithms to make the call"
- Break condition: If engineered features do not align with the mathematical rule (e.g., incorrect digit grouping), deep models will fail regardless of complexity

### Mechanism 3
- Claim: AutoML platforms fail on raw data but succeed with engineered features, showing domain knowledge is irreplaceable
- Mechanism: AutoML pipelines search model and feature space automatically, but without meaningful features they default to random performance. Engineered features supply the domain-specific structure needed for success
- Core assumption: AutoML's feature generation is too generic to capture number-theoretic rules without explicit guidance
- Evidence anchors:
  - [abstract] "AutoML platforms (Azure, Vertex AI, SageMaker) failed without engineered features"
  - [section] "We also evaluated commercially available Automated Machine Learning (AutoML) pipelines from Amazon, Google and Microsoft, and demonstrated that they failed to address this issue unless appropriately engineered features were provided"
- Break condition: If AutoML incorporates strong symbolic preprocessing or rule-based feature generators, it might succeed without manual engineering

## Foundational Learning

- Concept: Periodic functions and Fourier series
  - Why needed here: Divisibility by p is a periodic function with period p, so Fourier series naturally model it
  - Quick check question: What is the period of the function f(x) = x mod 7?

- Concept: Linear regression and least squares
  - Why needed here: Fourier series coefficients are estimated via linear regression, providing a closed-form solution
  - Quick check question: In a linear regression y = β₀ + β₁x, what is the formula for β₁ in terms of the data?

- Concept: Feature engineering and domain knowledge
  - Why needed here: Raw integer inputs lack discriminative features; engineered features like digit sums or binary encodings expose arithmetic patterns
  - Quick check question: Why does summing the digits of a number help determine divisibility by 3?

## Architecture Onboarding

- Component map: Raw integers -> Feature engineering (digit sums, Fourier basis) -> Model training (ANN/CNN/RNN/BERT/Linear Regression) -> Evaluation (accuracy, R²)

- Critical path: 1) Sample integers uniformly in [0, 2³²) 2) Apply feature engineering (e.g., digit sum, Fourier basis) 3) Train model (deep net or linear regression) 4) Evaluate on 10% held-out set 5) If using AutoML, ensure features are engineered before upload

- Design tradeoffs:
  - Raw data: Simple preprocessing but poor performance
  - Engineered features: More preprocessing but high accuracy
  - Deep networks: Flexible but complex, need engineered features
  - Linear regression on Fourier basis: Simple, interpretable, closed-form

- Failure signatures:
  - Accuracy ~1/p (random guessing) → missing feature engineering
  - Overfitting on small p → too complex model for simple rule
  - Poor R² in regression → insufficient training samples or wrong basis

- First 3 experiments:
  1. Train ANN on raw integers for mod 2; expect ~0.5 accuracy
  2. Train ANN on digit-sum features for mod 3; expect near 1.0 accuracy
  3. Train linear regression on Fourier basis for mod 7; expect 1.0 accuracy and interpretable coefficients

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well would LLMs like ChatGPT perform on divisibility rules for larger primes (e.g., 37, 41) if trained specifically on structured examples?
- Basis in paper: [explicit] ChatGPT failed to design effective solutions for divisibility by 17 and higher primes, relying on modulus operations or incorrect rules
- Why unresolved: The paper only tested ChatGPT with general prompts, not specialized training data or fine-tuning
- What evidence would resolve it: Fine-tuning ChatGPT on structured divisibility rule examples and comparing performance to baseline prompts

### Open Question 2
- Question: What is the minimum training set size required for Fourier series regression to achieve perfect accuracy on mod p problems?
- Basis in paper: [explicit] Fourier series regression achieved 100% accuracy with 25,000 training samples for mod 3 and mod 7, but scalability to larger primes was not tested
- Why unresolved: The paper did not systematically vary training set sizes across different values of p
- What evidence would resolve it: Experiments varying training set sizes for multiple primes and identifying the minimum size for perfect accuracy

### Open Question 3
- Question: Can AutoML platforms learn divisibility rules without feature engineering if provided sufficient training data and computational resources?
- Basis in paper: [explicit] AutoML platforms failed on raw data but succeeded with engineered features like one-gram encoding with digit sums
- Why unresolved: The paper did not test AutoML on extremely large datasets or with extended training times
- What evidence would resolve it: Testing AutoML on datasets with millions of examples and comparing to feature-engineered results

## Limitations
- Results are confined to small primes (2, 3, 7) and integers up to 2³², limiting generalizability
- Theoretical justification for Fourier series success lacks rigorous proof for all prime moduli
- AutoML platform configurations and hyperparameter settings remain unspecified

## Confidence
- **High confidence**: Feature engineering necessity (empirical results are clear and reproducible)
- **Medium confidence**: Fourier series linear regression success (theoretically sound but limited empirical validation)
- **Low confidence**: AutoML failure generality (platform-specific results without controlled ablation studies)

## Next Checks
1. Test Fourier series regression on larger primes (e.g., 11, 13) to verify scalability of the closed-form solution
2. Conduct ablation studies isolating the impact of specific feature engineering choices on deep learning performance
3. Implement symbolic preprocessing rules in AutoML pipelines to test whether AutoML can learn divisibility patterns without manual feature engineering