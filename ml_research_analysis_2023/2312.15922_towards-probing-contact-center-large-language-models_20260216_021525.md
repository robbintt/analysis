---
ver: rpa2
title: Towards Probing Contact Center Large Language Models
arxiv_id: '2312.15922'
source_url: https://arxiv.org/abs/2312.15922
tags:
- tasks
- probing
- language
- fine-tuning
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic analysis of large language models
  (LLMs) fine-tuned for contact-center domain, focusing on their conversational, channel,
  and automatic speech recognition (ASR) properties. The study benchmarks various
  LLM architectures (Flan-T5 and Llama), sizes (3B, 7B, 11B, 13B), and fine-tuning
  paradigms (full fine-tuning vs PEFT) on a custom contact-center dataset.
---

# Towards Probing Contact Center Large Language Models

## Quick Facts
- arXiv ID: 2312.15922
- Source URL: https://arxiv.org/abs/2312.15922
- Reference count: 16
- Primary result: Fine-tuned contact-center LLMs show over 48% improvement in response acceptability compared to out-of-the-box models

## Executive Summary
This paper presents a systematic analysis of large language models (LLMs) fine-tuned for contact-center domain, focusing on their conversational, channel, and automatic speech recognition (ASR) properties. The study benchmarks various LLM architectures (Flan-T5 and Llama), sizes (3B, 7B, 11B, 13B), and fine-tuning paradigms (full fine-tuning vs PEFT) on a custom contact-center dataset. Results show over 48% improvement in response acceptability for fine-tuned models compared to out-of-the-box LLMs. Probing tasks reveal that while fine-tuned models outperform their counterparts, they rely less on encoding surface, syntactic, and semantic properties. The study also highlights the trade-offs between parameter-efficient fine-tuning and task-specific performance, and emphasizes the need for more nuanced probing strategies to capture conversational intricacies in domain-specific contexts.

## Method Summary
The researchers curated a proprietary contact-center dataset including conversational interactions between agents and customers, ASR transcripts with 14.3% average word-error-rate, and domain-specific instructions obtained through annotation and paraphrasing. They fine-tuned LLMs (Flan-T5 and Llama) of various sizes using instruction fine-tuning and LoRA-based parameter-efficient fine-tuning (PEFT), then evaluated performance on in-domain downstream tasks and SentEval probing tasks using macro F1 score. The study compared full fine-tuning versus PEFT approaches and examined how different model architectures encode conversational properties.

## Key Results
- Fine-tuned contact-center LLMs show over 48% improvement in response acceptability compared to out-of-the-box models
- CC-LLMs outperform OOB models on downstream tasks while relying less on encoding surface, syntactic, and semantic properties
- Parameter-efficient fine-tuning (LoRA) offers resource savings but shows 4.53% lower performance on probing tasks compared to full fine-tuning
- T5 (encoder-decoder) models consistently outperform Llama (decoder-only) models across different settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning on domain-specific instructions improves response quality for contact-center tasks by over 48% compared to out-of-the-box models.
- Mechanism: Fine-tuning exposes LLMs to domain-specific conversational patterns, jargon, and task structures, allowing them to generate contextually appropriate responses.
- Core assumption: The curated dataset captures the essential characteristics of contact-center interactions, and fine-tuning effectively transfers this knowledge to the models.
- Evidence anchors:
  - [abstract]: "Results show over 48% improvement in response acceptability for fine-tuned models compared to out-of-the-box LLMs."
  - [section 2]: "Our findings reveal remarkable effectiveness of CC-LLMs on the in-domain downstream tasks, with improvement in response acceptability by over 48% compared to OOB-LLMs."
- Break condition: If the fine-tuning dataset does not adequately represent the target domain or if the fine-tuning process fails to effectively transfer knowledge, the performance improvement may not materialize.

### Mechanism 2
- Claim: CC-LLMs rely less on encoding surface, syntactic, and semantic properties compared to OOB models, yet still outperform them on domain-specific tasks.
- Mechanism: Domain-specific fine-tuning may shift the models' focus from general linguistic properties to more task-specific and context-dependent features that are crucial for contact-center interactions.
- Core assumption: The improvement in performance is not solely due to better encoding of general linguistic properties but rather a more effective utilization of domain-specific knowledge and conversational dynamics.
- Evidence anchors:
  - [abstract]: "Our observations indicate that CC-LLMs, while outperforming their out-of-the-box counterparts, exhibit a tendency to rely less on encoding surface, syntactic, and semantic properties."
  - [section 6.2]: "While our probing tasks are carefully designed to uncover the latent knowledge within these models, our findings in Table 1 did not conclusively favor either type of LLM."
- Break condition: If the probing tasks are not adequately capturing the essential properties learned by CC-LLMs or if the models are still relying heavily on general linguistic properties for domain-specific tasks, the observed trend may not hold.

### Mechanism 3
- Claim: Parameter-efficient fine-tuning (PEFT) offers a trade-off between resource efficiency and task-specific performance compared to full fine-tuning.
- Mechanism: PEFT methods like LoRA reduce the number of trainable parameters and memory requirements during fine-tuning, but this may come at the cost of slightly lower performance on domain-specific tasks.
- Core assumption: The reduced number of trainable parameters in PEFT limits the model's ability to fully adapt to the target domain, resulting in a performance gap compared to full fine-tuning.
- Evidence anchors:
  - [abstract]: "The study also highlights the trade-offs between parameter-efficient fine-tuning and task-specific performance."
  - [section 6.4]: "We observe that CC-Flan-T5-PEFT (11B) leads to a 4.53% lower average score on the probing tasks compared to CC-Flan-T5 (11B)."
- Break condition: If the performance gap between PEFT and full fine-tuning is negligible or if the resource savings outweigh the performance loss, the trade-off may not be significant.

## Foundational Learning

- Concept: Domain-specific instruction fine-tuning
  - Why needed here: To adapt general-purpose LLMs to the specific requirements and characteristics of contact-center interactions.
  - Quick check question: How does fine-tuning on domain-specific instructions differ from general pre-training or fine-tuning on task-specific datasets?

- Concept: Probing tasks and linear classifiers
  - Why needed here: To assess the fundamental properties learned by LLMs, such as surface, syntactic, and semantic information, and compare the performance of OOB and CC models.
  - Quick check question: What are the advantages and limitations of using linear classifiers and hidden layer representations for probing language model properties?

- Concept: Parameter-efficient fine-tuning (PEFT) methods
  - Why needed here: To explore the trade-offs between resource efficiency and task-specific performance when fine-tuning large LLMs for domain-specific applications.
  - Quick check question: How do PEFT methods like LoRA differ from traditional full fine-tuning in terms of the number of trainable parameters and the adaptation process?

## Architecture Onboarding

- Component map: Contact-center dataset -> Instruction generation -> Fine-tuning (full vs PEFT) -> Hidden layer extraction -> Linear classifier training -> Downstream evaluation

- Critical path:
  1. Curate contact-center dataset and generate instructions
  2. Fine-tune OOB LLMs using full fine-tuning and PEFT methods
  3. Extract hidden layer representations from fine-tuned models
  4. Train linear classifiers on the extracted representations for probing tasks
  5. Evaluate the performance of CC-LLMs on downstream tasks and compare with OOB models

- Design tradeoffs:
  - Model architecture and size: Encoder-decoder (Flan-T5) vs. decoder-only (Llama) architectures and their impact on conversational property learning
  - Fine-tuning method: Full fine-tuning vs. PEFT (LoRA) and the trade-off between performance and resource efficiency
  - Probing task design: Balancing the comprehensiveness of probing tasks with their ability to capture domain-specific properties

- Failure signatures:
  - Insufficient improvement in response quality despite fine-tuning
  - Similar performance of OOB and CC models on probing tasks, indicating a lack of domain-specific property learning
  - Significant performance degradation when using PEFT compared to full fine-tuning

- First 3 experiments:
  1. Fine-tune a small-sized Flan-T5 model (e.g., 3B parameters) with domain-specific instructions and evaluate its performance on downstream tasks compared to the OOB model
  2. Compare the performance of full fine-tuning and PEFT (LoRA) on a medium-sized Llama model (e.g., 7B parameters) using the same domain-specific instructions
  3. Design and implement additional probing tasks specifically tailored to contact-center interactions and assess their ability to distinguish between OOB and CC models

## Open Questions the Paper Calls Out

- How do decoding strategies impact the performance of contact-center large language models (CC-LLMs) on downstream tasks compared to general-purpose LLMs?
- How can probing tasks be redesigned to better capture the dynamic and context-dependent nature of contact-center conversations?
- How does the choice of fine-tuning paradigm (full fine-tuning vs. parameter-efficient fine-tuning) impact the balance between resource efficiency and task-specific performance for CC-LLMs?
- How does the architecture of contact-center LLMs (encoder-decoder vs. decoder-only) influence their ability to comprehend conversational properties and linguistic knowledge?

## Limitations

- Reliance on proprietary contact-center dataset limits reproducibility and generalizability of findings
- Probing tasks may not fully capture the nuanced conversational properties essential for contact-center applications
- Unclear how 48% improvement in response acceptability translates to real-world contact-center performance

## Confidence

- High confidence in comparative performance improvements of fine-tuned vs. out-of-the-box models
- Medium confidence in interpretation that CC-LLMs rely less on encoding surface/syntactic/semantic properties
- Low confidence in generalizability of findings beyond the specific contact-center domain studied

## Next Checks

1. Implement ablation studies on the fine-tuning dataset to quantify the impact of dataset quality/quantity on performance improvements, particularly focusing on whether the 48% improvement is robust to dataset variations.

2. Design and test additional domain-specific probing tasks that better capture contact-center conversational dynamics, then re-evaluate whether CC-LLMs still show reduced reliance on surface/syntactic/semantic properties.

3. Conduct cross-domain validation by fine-tuning the same models on a different domain (e.g., healthcare or technical support) and compare whether similar patterns of property encoding and performance improvements emerge.