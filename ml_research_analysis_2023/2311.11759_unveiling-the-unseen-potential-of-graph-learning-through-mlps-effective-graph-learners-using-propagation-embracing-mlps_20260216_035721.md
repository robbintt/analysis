---
ver: rpa2
title: 'Unveiling the Unseen Potential of Graph Learning through MLPs: Effective Graph
  Learners Using Propagation-Embracing MLPs'
arxiv_id: '2311.11759'
source_url: https://arxiv.org/abs/2311.11759
tags:
- propagation
- graph
- teacher
- student
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel knowledge distillation (KD) framework,
  Propagate & Distill (P&D), to effectively train multilayer perceptrons (MLPs) for
  semi-supervised node classification on graphs. Unlike existing GNN-to-MLP KD methods
  that focus on matching output probability distributions, P&D explicitly injects
  structural information by propagating the teacher GNN's output before distillation.
---

# Unveiling the Unseen Potential of Graph Learning through MLPs: Effective Graph Learners Using Propagation-Embracing MLPs

## Quick Facts
- arXiv ID: 2311.11759
- Source URL: https://arxiv.org/abs/2311.11759
- Reference count: 40
- Key outcome: P&D framework achieves 8.9% average accuracy improvement over GLNN on benchmark datasets by injecting structural information through propagated teacher output

## Executive Summary
This paper introduces Propagate & Distill (P&D), a knowledge distillation framework that enables multilayer perceptrons (MLPs) to effectively learn graph-structured data. Unlike existing methods that match only output probabilities, P&D explicitly injects structural information by propagating the teacher GNN's output before distillation. This approach allows the student MLP to learn both feature transformation and propagation, achieving significant performance gains on semi-supervised node classification tasks. The framework is theoretically grounded in a self-correction mechanism that leverages graph homophily to refine predictions.

## Method Summary
P&D trains a student MLP by distilling knowledge from a teacher GNN while explicitly injecting structural information. The method propagates the teacher GNN's output recursively over the graph structure (Eq. 4) before applying standard knowledge distillation. This propagation can be interpreted as an approximate inverse propagation process, allowing the MLP to learn graph-aware representations without explicitly applying inverse propagation matrices. The student MLP is then trained using KL divergence loss between its predictions and the propagated teacher outputs.

## Key Results
- P&D achieves 8.9% average accuracy improvement over GLNN on six benchmark datasets
- Significant performance gains observed on Cora (2.3%), CiteSeer (13.6%), and Arxiv (9.7%) datasets
- Performance improvements scale with propagation depth, validating the effectiveness of structural information injection

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Injecting propagated teacher output via P&D approximates inverse propagation while avoiding expensive matrix multiplications.
- **Mechanism**: The method propagates the teacher's output over the graph structure using recursive updates before distillation. This injects structural information into the student MLP's training objective without explicitly applying inverse propagation matrices.
- **Core assumption**: The recursive propagation rule approximates the effect of inverse propagation and provides sufficient structural signal for the MLP to learn graph-aware representations.
- **Evidence anchors**:
  - [abstract]: "we propose Propagate & Distill (P&D), which propagates the output of the teacher GNN before KD and can be interpreted as an approximate process of the inverse propagation Π−1"
  - [section]: "Instead of using Eq. (3) as the distillation loss, we present an alternative of InvKD, named as P&D, due to the computational efficiency and the design flexibility"
- **Break condition**: If the recursive propagation rule fails to approximate inverse propagation accurately, the structural information injection becomes insufficient and performance degrades.

### Mechanism 2
- **Claim**: Self-correction via propagation improves MLP performance by smoothing predictions along homophilic graph structure.
- **Mechanism**: The propagation process iteratively refines predictions by aggregating information from neighbors, allowing the student MLP to learn how structural relationships correct initial prediction errors.
- **Core assumption**: The underlying graph exhibits high homophily where connected nodes tend to share the same class labels, enabling correction through neighbor information.
- **Evidence anchors**:
  - [abstract]: "we can say that P&D also takes advantage of the homophily assumption to potentially correct the predictions of incorrectly-predicted nodes with the aid of their (mostly correctly predicted) neighbors"
  - [section]: "P&D relies on the assumption that the underlying graph has high levels of homophily"
- **Break condition**: If the graph has low homophily or heterophily, the self-correction mechanism becomes ineffective and may even introduce noise into the learning process.

### Mechanism 3
- **Claim**: Deeper and stronger propagation improves MLP performance by capturing more global structural information.
- **Mechanism**: Increasing the number of propagation iterations (T) and propagation strength (γ) allows the student MLP to learn from increasingly global structural patterns in the graph.
- **Core assumption**: Higher-order neighborhood information provides valuable context that improves the MLP's ability to make accurate predictions.
- **Evidence anchors**:
  - [section]: "We shall empirically validate these claims in Section 6.3" followed by experiments showing performance improvements with deeper propagation
  - [section]: "We also investigate how the total number of iterations T and the propagation strength γ in Eq. (4) affects the performance to validate our claims (C1) and (C2)"
- **Break condition**: Excessive propagation depth or strength may cause over-smoothing, where node representations become too similar and lose discriminative power.

## Foundational Learning

- **Concept**: Knowledge Distillation (KD)
  - Why needed here: The entire framework relies on transferring knowledge from a teacher GNN to a student MLP, which is the fundamental concept of KD
  - Quick check question: What is the primary objective of knowledge distillation in the context of GNN-to-MLP transfer?

- **Concept**: Graph Neural Networks (GNNs) and message passing
  - Why needed here: Understanding how GNNs aggregate information from neighbors is crucial for appreciating why structural information needs to be injected into MLPs
  - Quick check question: How do standard GNN layers combine feature transformation and propagation in their forward pass?

- **Concept**: Homophily in graphs
  - Why needed here: The self-correction mechanism and overall effectiveness depend on the assumption that connected nodes tend to share similar properties
  - Quick check question: What does high homophily in a graph imply about the relationship between connected nodes?

## Architecture Onboarding

- **Component map**: Teacher GNN (GraphSAGE or APPNP) → Output propagation module → Student MLP → Loss function
- **Critical path**: Teacher GNN inference → Recursive propagation of outputs → MLP training with propagated targets → Inference with trained MLP
  - Bottleneck: Teacher GNN inference time during training
- **Design tradeoffs**: 
  - P&D vs InvKD: P&D trades exact inverse propagation for computational efficiency
  - Propagation depth vs. oversmoothing: Deeper propagation captures more structure but risks losing discriminative information
  - Propagation strength vs. stability: Stronger propagation may improve performance but could introduce instability
- **Failure signatures**: 
  - Poor performance on graphs with low homophily
  - Degradation when using very deep propagation
  - Memory issues with large graphs due to teacher GNN inference requirements
- **First 3 experiments**:
  1. Compare P&D vs GLNN on Cora dataset with default settings to verify performance improvement
  2. Test different propagation depths (T=1, 5, 10, 20) on CiteSeer to validate depth-performance relationship
  3. Evaluate on a synthetic heterophilic graph to confirm failure conditions for self-correction mechanism

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the following unresolved issues emerge from the analysis:

### Open Question 1
- Question: How does the performance of P&D compare to other graph neural network architectures that explicitly incorporate propagation, such as GPR-GNN or APPNP?
- Basis in paper: [explicit] The paper mentions these architectures in the related work section and compares the performance of P&D to a teacher GNN model in Section 6.5.
- Why unresolved: The paper does not directly compare the performance of P&D to these architectures, focusing instead on comparing it to simpler GNN models and MLP models.
- What evidence would resolve it: A direct comparison of the performance of P&D, GPR-GNN, and APPNP on a set of standard benchmark datasets.

### Open Question 2
- Question: How does the performance of P&D change when applied to graphs with different levels of homophily?
- Basis in paper: [explicit] The paper discusses the importance of homophily in the theoretical analysis section and mentions that P&D relies on the assumption of high homophily.
- Why unresolved: The paper does not provide experimental results on graphs with varying levels of homophily, focusing instead on benchmark datasets with generally high homophily.
- What evidence would resolve it: An experimental study on a diverse set of graphs with varying levels of homophily, measuring the performance of P&D on each.

### Open Question 3
- Question: Can the P&D framework be extended to other graph learning tasks beyond semi-supervised node classification, such as link prediction or graph classification?
- Basis in paper: [inferred] The paper focuses on semi-supervised node classification but mentions the potential for future work to explore other tasks.
- Why unresolved: The paper does not explore the application of P&D to other graph learning tasks, focusing solely on node classification.
- What evidence would resolve it: A study applying the P&D framework to link prediction or graph classification tasks and comparing its performance to existing methods.

## Limitations
- Performance degrades significantly on graphs with low homophily, limiting applicability to real-world heterophilic networks
- Computational overhead from teacher GNN inference during training scales poorly with graph size
- Theoretical analysis of self-correction mechanism lacks rigorous mathematical proof, relying primarily on empirical validation

## Confidence
- Mechanism 1 (approximate inverse propagation): Medium - the conceptual connection is clear but lacks rigorous mathematical proof of the approximation quality
- Mechanism 2 (homophily-based self-correction): Medium - supported by reasonable arguments but not extensively validated across diverse graph structures
- Performance claims: High - extensive experimental validation on multiple benchmark datasets with consistent improvements

## Next Checks
1. Evaluate P&D on graphs with controlled homophily levels (synthetic datasets with varying homophily ratios) to quantify the exact impact of the self-correction mechanism
2. Perform ablation studies isolating the contribution of propagation depth vs. propagation strength to understand their individual effects on performance
3. Measure actual training time and memory usage across different graph sizes to verify the computational efficiency claims relative to exact inverse propagation methods