---
ver: rpa2
title: 'Holmes: Towards Distributed Training Across Clusters with Heterogeneous NIC
  Environment'
arxiv_id: '2312.03549'
source_url: https://arxiv.org/abs/2312.03549
tags:
- training
- parallelism
- holmes
- pipeline
- clusters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Holmes is a distributed training framework for large language models
  (LLMs) that addresses the challenge of training across clusters with heterogeneous
  network interface cards (NICs). Existing LLM training frameworks like Megatron-LM
  are designed for homogeneous high-speed RDMA networks and struggle with heterogeneous
  NIC environments where different clusters have incompatible NICs (InfiniBand, RoCE,
  Ethernet).
---

# Holmes: Towards Distributed Training Across Clusters with Heterogeneous NIC Environment

## Quick Facts
- arXiv ID: 2312.03549
- Source URL: https://arxiv.org/abs/2312.03549
- Authors: 
- Reference count: 40
- Key outcome: Framework achieves performance close to homogeneous RDMA networks (149/74.91 TFLOPS/Throughput in hybrid vs 197/99.23 in pure InfiniBand) and significantly outperforms pure Ethernet environments

## Executive Summary
Holmes is a distributed training framework for large language models that addresses the challenge of training across clusters with heterogeneous network interface cards. Existing frameworks struggle in environments where different clusters have incompatible NICs (InfiniBand, RoCE, Ethernet), leading to significant performance degradation. Holmes overcomes this by intelligently scheduling computational tasks based on NIC characteristics, using pipeline parallelism across clusters and data parallelism within clusters. The framework implements self-adapting pipeline partitioning, automatic NIC selection for data parallel groups, and overlapped distributed optimizers to maximize communication efficiency.

## Method Summary
Holmes builds upon Megatron-LM and implements a novel scheduling method that allocates computational tasklets to GPU devices based on their connected NIC types. The framework uses pipeline parallelism across clusters with self-adapting partition strategies that account for varying computational speeds due to different NICs. For data parallelism, Holmes modifies NCCL to create groups containing only GPU devices with the same NIC type, enabling efficient RDMA communication within homogeneous groups. The framework also implements an overlapped distributed optimizer that interleaves communication and computation to maximize interconnect bandwidth utilization.

## Key Results
- Achieves TFLOPS/Throughput of 149/74.91 in hybrid NIC environment vs 197/99.23 in pure InfiniBand environment
- Significantly outperforms pure Ethernet environments with 122/61.32 TFLOPS/Throughput
- Outperforms mainstream frameworks like Megatron-LM and Megatron-DeepSpeed in heterogeneous NIC settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pipeline parallelism with self-adapting partition reduces communication bottlenecks in heterogeneous NIC environments.
- Mechanism: Holmes dynamically assigns model layers to pipeline stages based on NIC type, allocating more layers to faster NIC-connected devices to balance computational speed across stages.
- Core assumption: TFLOPS varies significantly between GPU devices connected to different NIC types, even with identical GPUs.
- Evidence anchors: [abstract] "Our primary technical contribution lies in a novel scheduling method that intelligently allocates distinct computational tasklets in LLM training to specific groups of GPU devices based on the characteristics of their connected NICs."

### Mechanism 2
- Claim: Automatic NIC selection for data parallelism groups enables efficient gradient aggregation using high-speed RDMA within homogeneous NIC groups.
- Mechanism: Holmes modifies NCCL to create data parallel groups containing only GPU devices with the same NIC type, allowing RDMA communication within groups rather than falling back to slower Ethernet.
- Core assumption: NCCL can be extended to support hybrid NIC environments and create groups based on NIC type.
- Evidence anchors: [section 3.2] "We have implemented an auto-select strategy by modifying NCCL [4] and Megatron-LM [3] to support hybrid setups of NICs, including InfiniBand, RoCE, and Ethernet."

### Mechanism 3
- Claim: Overlapped distributed optimizer interleaves communication and computation to maximize interconnect bandwidth utilization.
- Mechanism: Holmes implements a partition strategy for gradients and optimizer states that allows communication operations to overlap with computation, avoiding GPU memory copying and additional communication overhead.
- Core assumption: Communication and computation can be effectively overlapped without introducing consistency issues or degrading convergence.
- Evidence anchors: [section 3.2] "We have incorporated a novel partition strategy for gradients and optimizer states into our LLM training framework... known as the OverlappedDistributedOptimizer."

## Foundational Learning

- Concept: Distributed training parallelism strategies (data parallelism, model parallelism, pipeline parallelism)
  - Why needed here: Understanding how different parallelism strategies affect communication patterns is crucial for optimizing training across heterogeneous NIC environments.
  - Quick check question: What are the primary communication patterns for data parallelism versus pipeline parallelism in distributed training?

- Concept: Remote Direct Memory Access (RDMA) protocols and network performance characteristics
  - Why needed here: Different RDMA implementations (InfiniBand, RoCE) have varying performance characteristics that directly impact training efficiency.
  - Quick check question: How do InfiniBand and RoCE differ in terms of latency, throughput, and compatibility?

- Concept: Collective communication algorithms (Ring Allreduce, etc.)
  - Why needed here: These algorithms are fundamental to data parallelism synchronization and their performance varies significantly with network topology.
  - Quick check question: What are the key factors that affect the performance of Ring Allreduce in distributed training?

## Architecture Onboarding

- Component map: Holmes Framework (built on Megatron-LM) -> Pipeline Parallelism Module (Cross-Cluster Pipeline Parallelism, Self-Adapting Pipeline Partition) -> Data Parallelism Module (Automatic NIC Selection, Overlapped Distributed Optimizer) -> Integration Layer (Modified NCCL support, Compatibility with mainstream frameworks)

- Critical path: 1. Model layer partitioning based on NIC characteristics 2. Group formation for data parallelism based on NIC type 3. Gradient aggregation using optimized collective communication 4. Model parameter updates with overlapped optimization

- Design tradeoffs: Complexity vs. performance: Self-adapting pipeline partition adds scheduling complexity but improves load balancing; Compatibility vs. optimization: Modifying NCCL enables NIC-aware grouping but may reduce compatibility with other frameworks; Memory overhead vs. throughput: Overlapped optimizer may require additional memory for intermediate states

- Failure signatures: Pipeline stages becoming unbalanced if NIC performance characteristics change dynamically; Data parallelism groups becoming too small if NIC distribution is highly heterogeneous; Communication bottlenecks if overlapped operations introduce synchronization delays

- First 3 experiments: 1. Benchmark TFLOPS and throughput on homogeneous vs. heterogeneous NIC setups with identical model configurations 2. Compare self-adapting pipeline partition vs. uniform partition under varying NIC speed differences 3. Test automatic NIC selection by measuring reduce-scatter operation times with and without NIC-aware grouping

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Holmes perform in environments with highly unstable or intermittent communication between clusters?
- Basis in paper: [inferred] The paper mentions limitations regarding stable communication assumptions and notes that future work should explore scheduling methods for diverse environments and handling faults.
- Why unresolved: The paper does not test Holmes under conditions of network instability or intermittent connectivity between clusters.
- What evidence would resolve it: Experimental results comparing Holmes's performance under stable vs. unstable network conditions, including metrics for convergence and training efficiency.

### Open Question 2
- Question: What is the impact of Holmes's Self-Adapting Pipeline Partition strategy on convergence time and final model quality compared to traditional uniform partitioning?
- Basis in paper: [explicit] The paper introduces the Self-Adapting Pipeline Partition strategy but only evaluates its performance in terms of TFLOPS and throughput, not convergence behavior or final model accuracy.
- Why unresolved: The experiments focus on partial training and performance metrics rather than full convergence or model quality.
- What evidence would resolve it: Comparative studies showing convergence curves and final evaluation metrics (e.g., perplexity, accuracy) for models trained with both partitioning strategies.

### Open Question 3
- Question: How does Holmes scale beyond the tested cluster configurations, particularly to environments with hundreds of nodes or more heterogeneous NIC combinations?
- Basis in paper: [explicit] The paper mentions scalability goals and tests up to 96 GPUs, but acknowledges limitations in exploring scheduling methods for diverse environments.
- Why unresolved: The experiments are limited to relatively small-scale setups and do not explore extreme scaling scenarios.
- What evidence would resolve it: Performance benchmarks and scalability analysis of Holmes in large-scale distributed environments with hundreds of nodes and diverse NIC configurations.

## Limitations

- Performance gap: Holmes achieves performance close to homogeneous networks but still shows a 20-30% gap compared to pure InfiniBand environments.
- Implementation complexity: The framework's modifications to NCCL and Megatron-LM introduce significant architectural complexity and potential maintenance burden.
- Scalability boundaries: Experiments are limited to specific model sizes and cluster configurations, not exploring extreme scaling scenarios or highly heterogeneous NIC combinations.

## Confidence

- **High Confidence**: The fundamental problem statement (heterogeneous NIC environments creating training bottlenecks) and the general approach (NIC-aware scheduling and communication optimization) are well-established concepts in distributed computing.
- **Medium Confidence**: The specific architectural modifications (automatic NIC selection, overlapped distributed optimizer) are technically feasible but require extensive real-world validation across diverse hardware configurations.
- **Low Confidence**: Claims about achieving "performance close to homogeneous networks" lack sufficient empirical backing, particularly regarding statistical significance and sensitivity analysis across different workload patterns.

## Next Checks

1. **A/B Testing Against Baseline**: Implement a version of Holmes that uses uniform pipeline partitioning (ignoring NIC characteristics) and compare performance directly against the self-adapting version across multiple model architectures and cluster sizes.

2. **Dynamic NIC Performance Analysis**: Create test scenarios where NIC performance characteristics change during training (simulating network congestion or hardware failures) to evaluate the robustness of the self-adapting partitioning strategy.

3. **Memory Overhead Characterization**: Profile memory usage and GPU utilization patterns for the overlapped distributed optimizer under various batch sizes and gradient accumulation strategies to quantify the tradeoff between communication efficiency and memory consumption.