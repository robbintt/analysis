---
ver: rpa2
title: Image Clustering Conditioned on Text Criteria
arxiv_id: '2310.18297'
source_url: https://arxiv.org/abs/2310.18297
tags:
- clustering
- image
- step
- text
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new method for image clustering conditioned
  on user-specified text criteria, called IC|TC. The method leverages modern vision-language
  models and large language models to extract salient features from images and assign
  them to clusters based on the specified criteria.
---

# Image Clustering Conditioned on Text Criteria

## Quick Facts
- arXiv ID: 2310.18297
- Source URL: https://arxiv.org/abs/2310.18297
- Reference count: 40
- Primary result: IC|TC method uses VLM+LLM pipeline to cluster images based on user-specified text criteria, achieving high accuracy and interpretability.

## Executive Summary
This paper introduces IC|TC, a novel image clustering method that conditions clustering on user-specified text criteria. The approach leverages modern vision-language models (VLMs) and large language models (LLMs) to extract salient features from images and assign them to clusters based on the specified criteria. IC|TC enables diverse clustering results from a single dataset based on different text criteria and allows for iterative refinement of the clustering results through text prompt engineering. Experimental results demonstrate that IC|TC significantly outperforms classical clustering methods and achieves high accuracy in clustering images based on various criteria such as action, location, and mood. The method also produces interpretable cluster labels, enhancing the interpretability of clustering results.

## Method Summary
IC|TC is a three-stage pipeline that uses vision-language models (VLMs) and large language models (LLMs) to cluster images based on user-specified text criteria. In Step 1, a VLM extracts detailed image descriptions conditioned on the user's text criterion. In Step 2, an LLM derives cluster names from the raw labels produced by the VLM. In Step 3, another LLM assigns images to the discovered clusters. The method allows for iterative refinement of the clustering results through text prompt engineering, enabling users to control the clustering criterion and mitigate biases. Experiments on datasets like CIFAR-10, CIFAR-100, Stanford 40 Actions, PPMI, and FACET demonstrate the effectiveness of IC|TC in producing accurate and interpretable clustering results.

## Key Results
- IC|TC significantly outperforms classical clustering methods on various datasets and text criteria.
- The method achieves high accuracy in clustering images based on criteria such as action, location, and mood.
- IC|TC produces interpretable cluster labels, enhancing the interpretability of clustering results.
- Iterative refinement of text criteria allows users to control the clustering criterion and mitigate biases.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: VLM+LLM pipeline captures user intent by iteratively refining text prompts.
- **Mechanism**: Step 1 VLM produces detailed image descriptions conditioned on user text criterion; Step 2 LLM derives cluster names from raw labels; Step 3 LLM assigns images to clusters. User can edit TC after each pass to refine results.
- **Core assumption**: Foundation models trained on broad data retain enough semantic understanding to interpret arbitrary text criteria and map them to visual features.
- **Evidence anchors**:
  - [abstract] “IC|TC requires a minimal and practical degree of human intervention and grants the user significant control over the clustering results”
  - [section 3.1] “the user’s criterion TC determines the relevant features the VLM should focus on”
- **Break condition**: If user-specified TC is ambiguous or contradicts model priors, descriptions drift from intended clusters.

### Mechanism 2
- **Claim**: Text-based clustering avoids inductive bias of fixed-feature deep clustering.
- **Mechanism**: By encoding clustering intent directly in natural language, the method is not constrained to pre-defined label sets (e.g., foreground object types) and can cluster on arbitrary criteria like mood or location.
- **Core assumption**: LLM can infer and group raw labels into coherent, interpretable cluster names that align with TC.
- **Evidence anchors**:
  - [abstract] “classical clustering methods offer no direct mechanism for the user to control the clustering criterion”
  - [section 4.2] shows clustering on “Musical Instrument” with K=2 yields brass vs string, not pre-defined categories
- **Break condition**: If TC is too broad, LLM may produce vague cluster names lacking discriminative power.

### Mechanism 3
- **Claim**: Iterative refinement via TC engineering converges to desired clustering.
- **Mechanism**: After each clustering pass, user inspects outputs and edits TC (e.g., add “Do not consider gender”) to correct biases or refine granularity.
- **Core assumption**: User feedback loop is practical and that foundation models can incorporate such constraints without retraining.
- **Evidence anchors**:
  - [section 4.4] “we can effectively mitigate biases in the clustering results” by appending “Do not consider gender”
  - [section 2] “Iterative refinement of text criteria … provides a practical means for converging to desired results”
- **Break condition**: If model fails to honor constraints or if iterative loop diverges without convergence.

## Foundational Learning

- **Concept**: Prompt engineering for multi-modal models
  - Why needed here: All three steps rely on carefully crafted prompts to guide VLMs/LLMs; small prompt changes drastically affect output quality.
  - Quick check question: Given a new dataset, how would you design Step 1 prompt to focus on “emotion” rather than “object”?

- **Concept**: Evaluation metrics for clustering without ground truth
  - Why needed here: Many criteria (mood, location) lack labels; need metrics like Adjusted Rand Index, NMI, or human evaluation.
  - Quick check question: If you have 100 random images labeled by human for “Location”, which metric best captures cluster quality relative to those labels?

- **Concept**: Token limits and input length constraints for LLMs
  - Why needed here: Step 2 must convert potentially thousands of raw labels into K cluster names without exceeding token caps.
  - Quick check question: If you have 500 raw labels, how would you reduce token length before feeding to LLM?

## Architecture Onboarding

- **Component map**:
  User → Text Criterion (TC) → Step 1: VLM (LLaVA/BLIP-2) → Detailed image descriptions → Step 2: LLM (GPT-4/Llama-2) → Raw labels → Cluster names (K) → Step 3: LLM (GPT-4/Llama-2) → Image-to-cluster assignments → Output: K labeled clusters + optional TC refinement loop

- **Critical path**: VLM → Step 2 LLM (must produce valid cluster names) → Step 3 LLM (must honor TC) → Evaluation

- **Design tradeoffs**:
  - Prompt complexity vs. token limits: longer prompts yield richer descriptions but risk truncation.
  - Model size vs. cost: GPT-4 > Llama-2 in accuracy but higher API cost.
  - Granularity of TC vs. cluster coherence: overly specific TC may yield too many tiny clusters; too broad yields vague groupings.

- **Failure signatures**:
  - Step 1 outputs generic captions → Step 2 cannot infer criteria → poor clusters.
  - Step 2 outputs many singleton clusters → Step 3 assignments are noisy.
  - User TC ignored in Step 3 → clusters unrelated to intent.

- **First 3 experiments**:
  1. **Sanity check**: Use CIFAR-10 with TC=“Object”; verify ACC ≈ 98% without TC refinement.
  2. **Granularity test**: Stanford 40 Action with TC=“Action” and K=40; confirm each ground truth class appears.
  3. **Bias mitigation**: FACET dataset; start with TC=“Occupation”, observe gender imbalance, refine TC to remove gender bias, measure improvement.

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Heavy reliance on semantic understanding and constraint-following capabilities of foundation models.
- Iterative refinement loop could be labor-intensive for large datasets and may not always converge.
- Token limits and cost considerations for LLM usage in large-scale deployments are significant practical barriers.

## Confidence
- **High Confidence**: Claims about the basic pipeline functionality (VLM → LLM → clustering) and the ability to produce interpretable cluster labels when criteria are clear.
- **Medium Confidence**: Claims about iterative refinement effectiveness and bias mitigation, as these depend heavily on user input quality and model compliance.
- **Low Confidence**: Claims about scalability and performance on highly abstract or noisy text criteria, given limited experimental validation.

## Next Checks
1. **Abstract Criteria Test**: Apply IC|TC to a dataset (e.g., art images) with subjective criteria like "aesthetic appeal" or "emotional intensity" and evaluate cluster coherence using human judgment or downstream task performance.
2. **Scalability Benchmark**: Test IC|TC on a large-scale dataset (e.g., ImageNet-1K) with multiple text criteria, measuring runtime, token usage, and clustering quality to assess practical deployment limits.
3. **Robustness to Noise**: Introduce ambiguous or contradictory text criteria (e.g., "natural vs. artificial" for mixed scenes) and analyze how well the method handles conflicting signals or produces meaningful clusters.