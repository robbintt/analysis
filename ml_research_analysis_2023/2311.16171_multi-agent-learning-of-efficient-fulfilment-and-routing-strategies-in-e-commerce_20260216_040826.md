---
ver: rpa2
title: Multi-Agent Learning of Efficient Fulfilment and Routing Strategies in E-Commerce
arxiv_id: '2311.16171'
source_url: https://arxiv.org/abs/2311.16171
tags:
- customer
- time
- vehicle
- agent
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes an integrated reinforcement learning approach
  to solve the cost-to-serve (C2S) problem in e-commerce, which involves optimizing
  both order fulfillment node selection and vehicle routing. The method combines graph
  neural networks (GNNs) for node embedding and reinforcement learning (RL) agents
  for C2S and vehicle routing.
---

# Multi-Agent Learning of Efficient Fulfilment and Routing Strategies in E-Commerce

## Quick Facts
- arXiv ID: 2311.16171
- Source URL: https://arxiv.org/abs/2311.16171
- Reference count: 20
- Primary result: Integrated RL approach achieves up to 40% reduction in total trips compared to heuristics for e-commerce fulfillment and routing

## Executive Summary
This paper presents an integrated reinforcement learning approach to solve the cost-to-serve (C2S) problem in e-commerce, which involves jointly optimizing order fulfillment node selection and vehicle routing. The method uses Graph Neural Networks (GNNs) to learn embeddings of customer locations, which are then used as input features for two reinforcement learning agents: one for C2S warehouse assignment and another for vehicle routing. The approach demonstrates significant improvements over heuristic baselines, reducing total trips by up to 40% while improving vehicle capacity utilization and customer demand served per trip. The method shows robustness across different customer demand distributions, including skewed data patterns.

## Method Summary
The proposed method integrates two reinforcement learning agents to solve the e-commerce fulfillment and routing problem. A Graph Neural Network (GAE) learns embeddings of customer locations based on their spatial relationships. The C2S agent uses these embeddings along with other state features to make warehouse assignment decisions, while the VRP agent optimizes vehicle routes based on those assignments. The agents are trained in a staged approach where the C2S agent is pre-trained before the VRP agent begins learning. The reward function incorporates multiple objectives including distance minimization, capacity utilization, and customer satisfaction. The environment simulates customer demand, warehouse inventory, and vehicle operations with time window constraints.

## Key Results
- Up to 40% reduction in total trips compared to heuristic baselines
- Improved vehicle capacity utilization and customers served per trip
- Robust performance across different customer demand distributions including skewed data
- Better performance when training C2S and VRP agents separately rather than simultaneously

## Why This Works (Mechanism)

### Mechanism 1
The integrated RL framework outperforms separate heuristics because fulfillment node selection and vehicle routing are interdependent decisions. The C2S agent learns to assign orders to warehouses considering future routing costs, while the VRP agent optimizes routes based on those assignments, creating a feedback loop that improves overall efficiency. The rewards for both agents include components dependent on the other agent's decisions, enabling coordinated optimization. This breaks if the reward decomposition fails to capture the interdependence between C2S and VRP decisions.

### Mechanism 2
Graph Neural Networks provide better embeddings of customer locations that improve the C2S agent's warehouse assignment decisions. GAE learns to encode spatial relationships between customers, allowing the C2S agent to group geographically proximate customers when making warehouse assignments, reducing overall routing distance. The graph construction (edges between nearby customers) captures meaningful spatial relationships that correlate with optimal routing. This breaks if customer locations are uniformly random without meaningful clusters.

### Mechanism 3
The multi-phase training approach (C2S pre-training followed by VRP learning) achieves better performance than simultaneous training. Pre-training the C2S agent with heuristic routing provides a stable warehouse assignment policy that the VRP agent can then optimize around, avoiding poor initial assignments that would be difficult to correct later. The C2S decisions are more critical to get right initially as they constrain the VRP problem space significantly. This breaks if the pre-trained C2S policy is suboptimal.

## Foundational Learning

- **Markov Decision Process (MDP)**: The fulfillment and routing problems involve sequential decisions over time with state transitions and rewards. Quick check: What are the components of an MDP and how do they map to the C2S and VRP problems?

- **Graph Neural Networks**: To learn spatial representations of customer locations that capture proximity relationships relevant for routing. Quick check: How does the GAE architecture learn node embeddings, and what information does the adjacency matrix encode?

- **Deep Q-Learning**: To train the RL agents using temporal difference learning with sufficient exploration and stable training. Quick check: How does the experience replay buffer help stabilize training, and what role does the discount factor play?

## Architecture Onboarding

- **Component map**: Customer demand generation → C2S Agent (DQN) → Warehouse Assignment Decisions → VRP Agent (DQN) → Route Optimization → Environment Feedback (Rewards)
- **Critical path**: Customer demand generation → C2S agent decision → VRP agent routing → Reward calculation → Agent training
- **Design tradeoffs**: Simultaneous vs. staged training of C2S and VRP agents; complexity of reward function design vs. ease of credit assignment
- **Failure signatures**: Poor vehicle capacity utilization indicates C2S agent is not considering routing constraints; high number of trips suggests VRP agent is not optimizing routes effectively
- **First 3 experiments**:
  1. Test C2S agent in isolation with heuristic routing to verify it learns reasonable warehouse assignments
  2. Test VRP agent with fixed C2S assignments to verify it can optimize routes effectively
  3. Test the integrated system with both agents learning simultaneously to identify training instability issues

## Open Questions the Paper Calls Out

### Open Question 1
How would the performance of the proposed approach change if the graph embeddings were not used as input features for the RL agents? The paper states that "by using the learned embeddings from graph training, a reinforcement learning (RL) policies are able to effectively assign fulfillment nodes and scheduling vehicle for routing." This implies that the graph embeddings play a crucial role in the performance of the RL agents. The paper does not provide any comparison or analysis of the performance with and without using graph embeddings.

### Open Question 2
How sensitive is the proposed approach to changes in the warehouse inventory replenishment frequency? The paper mentions that "Each warehouse is restocked to its maximum level P (max) after every T2 time steps." However, it does not discuss the impact of changing this replenishment frequency on the performance of the proposed approach.

### Open Question 3
How does the proposed approach handle scenarios with multiple product types? The paper states that "we assume that there is a single type of product which can be ordered in arbitrary quantity by every customer." It does not discuss how the approach would handle scenarios with multiple product types.

## Limitations

- No comparison against more sophisticated VRP heuristics beyond simple savings algorithms
- Lack of ablation studies to isolate the contribution of the GNN embeddings versus other state features
- Limited scalability analysis showing performance degradation with problem size

## Confidence

- **High confidence**: The general approach of combining GNNs with multi-agent RL for this problem domain, given established use of both techniques in related applications
- **Medium confidence**: The specific implementation details and performance claims, as the paper lacks precise hyperparameter specifications and reward function weights needed for exact reproduction
- **Low confidence**: The robustness claims across different demand distributions, as evaluation only tested three demand profiles without varying key parameters like warehouse inventory levels or vehicle fleet size

## Next Checks

1. **Ablation study on GNN contribution**: Remove the graph embeddings from the C2S agent's state representation and retrain to quantify their specific impact on performance.

2. **Scalability testing**: Evaluate the method on problems with 2-5× more customers and warehouses to identify performance degradation patterns and memory/processing bottlenecks.

3. **Reward sensitivity analysis**: Systematically vary the weights of different reward components (distance, capacity utilization, customer satisfaction) to understand their relative importance and identify potential instabilities in the training process.