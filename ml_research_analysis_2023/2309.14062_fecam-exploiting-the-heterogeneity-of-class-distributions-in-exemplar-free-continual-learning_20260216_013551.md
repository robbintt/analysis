---
ver: rpa2
title: 'FeCAM: Exploiting the Heterogeneity of Class Distributions in Exemplar-Free
  Continual Learning'
arxiv_id: '2309.14062'
source_url: https://arxiv.org/abs/2309.14062
tags:
- classes
- learning
- feature
- covariance
- fecam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FeCAM, a method for exemplar-free continual
  learning that addresses catastrophic forgetting in class-incremental learning. The
  method uses a Mahalanobis distance metric to classify features based on class prototypes,
  which are generated using a frozen feature extractor.
---

# FeCAM: Exploiting the Heterogeneity of Class Distributions in Exemplar-Free Continual Learning

## Quick Facts
- arXiv ID: 2309.14062
- Source URL: https://arxiv.org/abs/2309.14062
- Reference count: 40
- Key outcome: FeCAM achieves state-of-the-art results on exemplar-free continual learning benchmarks using Mahalanobis distance-based classification without any training steps after the first task

## Executive Summary
FeCAM addresses catastrophic forgetting in exemplar-free class-incremental learning by leveraging Mahalanobis distance instead of Euclidean distance for classification. The method recognizes that feature distributions of new classes are heterogeneous and anisotropic compared to old classes, making Mahalanobis distance more suitable for capturing complex class structures. By computing and normalizing per-class covariance matrices, FeCAM achieves superior performance across various continual learning benchmarks without requiring stored exemplars or additional training steps.

## Method Summary
FeCAM operates by first training a feature extractor on the initial task and then freezing it. For each incremental task, it computes class prototypes (feature averages) and covariance matrices for all seen classes. The method applies covariance shrinkage to ensure invertibility and normalizes covariance matrices for comparability. Classification is performed using Mahalanobis distance to class prototypes with their respective covariance matrices. The approach requires no training steps after the first task, making it highly efficient for continual learning scenarios.

## Key Results
- Achieves state-of-the-art average incremental accuracy on CIFAR-100, TinyImageNet, and other benchmark datasets
- Outperforms existing exemplar-free methods by significant margins without requiring any training steps after the first task
- Demonstrates effectiveness in both many-shot and few-shot class-incremental learning settings
- Generalizes to domain-incremental learning scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Mahalanobis distance outperforms Euclidean distance in class-incremental learning because new classes exhibit heterogeneous feature distributions.
- Mechanism: The Mahalanobis distance accounts for feature covariance relations, enabling the model to better capture complex class structures in high-dimensional feature space.
- Core assumption: Feature distributions of new classes are anisotropic (not equally spread) compared to old classes.
- Evidence anchors:
  - [abstract] "classification based on Euclidean metrics is successful for jointly trained features. However, when learning from non-stationary data, we observe that the Euclidean metric is suboptimal and that feature distributions are heterogeneous."
  - [section 3.1] "We observe that the singular values of new class features vary more and are in general larger than those of old classes. This points out that new class distributions are more heterogeneous (and are more widely spread) compared to the old classes."
  - [corpus] Weak evidence - no direct citations to similar covariance-based CIL approaches.
- Break condition: If feature distributions of new classes become isotropic (equally spread) through improved training methods.

### Mechanism 2
- Claim: Bayesian classifier using Mahalanobis distance is optimal when class features follow multivariate normal distributions.
- Mechanism: The optimal Bayes classifier maximizes P(Y|X) by minimizing the negative log-likelihood, which corresponds to using Mahalanobis distance.
- Core assumption: Feature distributions of classes can be modeled as multivariate normal distributions.
- Evidence anchors:
  - [section 3.2] "When modeling the feature distribution of classes with a multivariate normal feature distribution N (µy, Σy), the probability of a sample feature x belonging to class y can be expressed as... It is straightforward to see that this is the optimal Bayesian classifier."
  - [section 3.1] "We propose to use the anisotropic Mahalanobis distance... we revisit the nearest class mean classifier based on a heterogeneous distance measure."
  - [corpus] No direct citations to similar Bayesian Mahalanobis CIL approaches.
- Break condition: If feature distributions deviate significantly from multivariate normal (e.g., heavy-tailed distributions).

### Mechanism 3
- Claim: Covariance shrinkage and normalization techniques stabilize Mahalanobis distance computation when few samples are available per class.
- Mechanism: Adding small values to the covariance matrix diagonal (shrinkage) ensures invertibility, while normalization makes covariance matrices comparable across classes.
- Core assumption: Number of samples per class can be less than the number of feature dimensions.
- Evidence anchors:
  - [section 3.2] "We identify the difficulties of obtaining an invertible covariance matrix in cases when the number of samples are less than the number of dimensions. So, we use a covariance shrinkage method to get a full-rank matrix."
  - [section 3.2] "In order to make the multiple covariance matrices comparable, we make their diagonal elements equal to 1."
  - [corpus] No direct citations to similar covariance stabilization techniques in CIL.
- Break condition: If sufficient samples are available per class to obtain stable covariance estimates without shrinkage.

## Foundational Learning

- Concept: Mahalanobis distance and its relationship to multivariate normal distributions
  - Why needed here: Understanding why Mahalanobis distance is optimal for Gaussian distributions and how it differs from Euclidean distance
  - Quick check question: Why is Mahalanobis distance more appropriate than Euclidean distance when feature dimensions have different variances or correlations?

- Concept: Covariance matrix computation and interpretation
  - Why needed here: FeCAM requires computing and normalizing covariance matrices for each class
  - Quick check question: What happens to the Mahalanobis distance when two features are highly correlated versus uncorrelated?

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: Understanding the problem FeCAM addresses in class-incremental learning settings
  - Quick check question: Why does freezing the feature extractor after the first task help prevent catastrophic forgetting?

## Architecture Onboarding

- Component map: Feature extractor -> Prototype computation -> Covariance matrix computation -> Mahalanobis distance-based classifier -> (Optional: Tukey's transformation) -> (Optional: Covariance shrinkage)

- Critical path:
  1. Extract features from first task and compute class prototypes
  2. For each incremental task, extract features and compute class prototypes
  3. Compute covariance matrices for all seen classes
  4. Apply covariance shrinkage and normalization
  5. Classify test samples using Mahalanobis distance to prototypes

- Design tradeoffs:
  - Memory vs. accuracy: Using per-class covariance matrices improves accuracy but requires more memory than a common covariance matrix
  - Computational complexity: Mahalanobis distance requires matrix inversion, which can be expensive for high-dimensional features
  - Sensitivity to assumptions: Performance depends on how well the Gaussian assumption holds for feature distributions

- Failure signatures:
  - Degraded performance when feature distributions are highly non-Gaussian
  - Numerical instability when covariance matrices are ill-conditioned or singular
  - Memory overflow when storing many high-dimensional covariance matrices

- First 3 experiments:
  1. Implement and test the basic FeCAM pipeline on CIFAR-100 with a frozen ResNet-18 feature extractor
  2. Compare Mahalanobis distance vs. Euclidean distance classification accuracy on a simple incremental task
  3. Test covariance shrinkage and normalization techniques on a few-shot incremental learning setting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can FeCAM be extended to settings where the feature extractor is continually updated rather than frozen after the first task?
- Basis in paper: [explicit] The paper mentions this as a limitation, stating "As a future work, FeCAM can be adapted to CIL settings where the feature representations are continually learned."
- Why unresolved: The current FeCAM method relies on a frozen feature extractor, which limits its applicability in scenarios where the feature representations need to be updated over time.
- What evidence would resolve it: Experimental results showing FeCAM's performance in scenarios where the feature extractor is updated during incremental learning tasks, compared to its current performance with a frozen feature extractor.

### Open Question 2
- Question: What is the impact of using different covariance shrinkage hyperparameters (γ1 and γ2) on FeCAM's performance across various datasets and settings?
- Basis in paper: [explicit] The paper discusses the use of covariance shrinkage to obtain full-rank matrices but does not provide a comprehensive analysis of the impact of different hyperparameters.
- Why unresolved: The optimal values of γ1 and γ2 may vary depending on the dataset and the specific incremental learning scenario, affecting FeCAM's performance.
- What evidence would resolve it: A systematic study of FeCAM's performance with different γ1 and γ2 values across multiple datasets and incremental learning settings, identifying the optimal hyperparameters for each scenario.

### Open Question 3
- Question: How does FeCAM's performance compare to other state-of-the-art methods when using a smaller first task, as opposed to the commonly used 50% of classes?
- Basis in paper: [explicit] The paper mentions that "Exemplar-free methods use 50% of data in the first task as equally splitting is a much more challenging setting which is usually tackled by storing exemplars or by expanding the network in new tasks."
- Why unresolved: Most CIL methods, including FeCAM, are typically evaluated with a large first task. The performance of these methods in more challenging scenarios with smaller first tasks remains unclear.
- What evidence would resolve it: Experimental results comparing FeCAM's performance to other state-of-the-art methods when using a smaller first task (e.g., 20% of classes) in various incremental learning settings.

## Limitations
- Limited empirical validation of the Mahalanobis distance advantage over Euclidean distance beyond the presented datasets
- Unclear generalizability of covariance shrinkage hyperparameters across different architectures and dataset sizes
- No comparison against methods using replay buffers or stored exemplars, which could provide upper bounds

## Confidence

- **High confidence**: The mathematical formulation of Mahalanobis distance and its relationship to multivariate normal distributions
- **Medium confidence**: The experimental results showing state-of-the-art performance on benchmark datasets
- **Low confidence**: Claims about the generality of heterogeneous feature distributions across all CIL scenarios

## Next Checks

1. **Ablation study**: Systematically compare Mahalanobis vs. Euclidean distance across varying levels of feature heterogeneity (e.g., by controlling intra-class variance during data augmentation)
2. **Covariance sensitivity analysis**: Test the robustness of FeCAM to different shrinkage hyperparameters and normalization schemes across multiple backbone architectures
3. **Cross-dataset generalization**: Evaluate FeCAM on datasets with fundamentally different feature distributions (e.g., medical imaging or satellite imagery) to assess the method's broader applicability