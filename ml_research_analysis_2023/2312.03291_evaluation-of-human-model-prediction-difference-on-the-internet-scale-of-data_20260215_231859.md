---
ver: rpa2
title: Evaluation of human-model prediction difference on the Internet Scale of Data
arxiv_id: '2312.03291'
source_url: https://arxiv.org/abs/2312.03291
tags:
- samples
- output
- input
- inputs
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces OmniInput, a novel model-centric evaluation
  framework for assessing AI/ML model performance across the entire input space, not
  just pre-defined test sets. Unlike traditional data-centric evaluations, OmniInput
  constructs test sets by sampling representative inputs from the model's output distribution
  using efficient samplers like Gradient Wang-Landau.
---

# Evaluation of human-model prediction difference on the Internet Scale of Data

## Quick Facts
- arXiv ID: 2312.03291
- Source URL: https://arxiv.org/abs/2312.03291
- Reference count: 40
- Key outcome: OmniInput reveals overconfident predictions and poor performance on uninformative inputs that traditional test sets miss

## Executive Summary
This paper introduces OmniInput, a model-centric evaluation framework that assesses AI/ML model performance across the entire input space rather than just pre-defined test sets. The framework constructs test sets by sampling representative inputs from the model's output distribution using efficient samplers like Gradient Wang-Landau, then has human annotators evaluate these samples. Experiments on MNIST, CIFAR, and SST2 datasets reveal that many models exhibit overconfident predictions and poor performance on uninformative inputs, even when achieving high accuracy on standard test sets. The framework enables more fine-grained model comparison and suggests that combining generative and classification objectives may improve robustness.

## Method Summary
OmniInput is a model-centric evaluation framework that constructs test sets by sampling representative inputs from a trained model's output distribution using the Gradient Wang-Landau sampler. Human annotators then evaluate these representative inputs, allowing estimation of precision and recall across the entire input space. The framework aggregates human annotations across output value bins to compute precision-recall curves, enabling comparison of models based on their behavior across all possible inputs, not just pre-defined test sets. This approach eliminates human bias in test set selection and reveals differences between models that traditional data-centric evaluations miss, particularly regarding overconfident predictions on uninformative inputs.

## Key Results
- CNN models trained on MNIST often misclassify noisy inputs as digit "1" with high confidence, showing poor performance on uninformative inputs
- Models with similar test accuracy can be distinguished using OmniInput's precision-recall curves across the entire input space
- The framework converges quickly with 40-50 human annotations per output bin, making it computationally feasible
- Combining generative and classification objectives may improve model robustness based on evaluation results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OmniInput's core innovation is using the model's own output distribution as a sampling guide to construct test sets, eliminating human bias in test set selection
- Mechanism: The Gradient Wang-Landau sampler explores the entire input space by sampling representative inputs that produce specific output values, creating a self-generated test set
- Core assumption: All possible inputs in the input space are equally likely to occur in real-world deployment
- Evidence anchors:
  - [abstract]: "the test set in OMNI INPUT is self-constructed by the model itself"
  - [section]: "Representative Inputs...Our OMNI INPUT framework mainly focuses on how to leverage the output distribution for model evaluation over the entire input space"
  - [corpus]: Weak - related papers focus on comparative model analysis but don't address self-constructed test sets
- Break condition: If the model's output distribution is severely biased or multimodal in ways that prevent uniform exploration

### Mechanism 2
- Claim: Precision and recall can be accurately estimated from the output distribution by aggregating human annotations across output value bins
- Mechanism: Human annotators score representative samples within each output value bin, allowing calculation of precision per bin (r(z)) which is then weighted by output distribution (ρ(z))
- Core assumption: A small number of human annotations per output bin (40-50) is sufficient to estimate precision reliably
- Evidence anchors:
  - [abstract]: "After the model is trained, we construct the test set from the model's self-generated, representative inputs corresponding to different model output values"
  - [section]: "We have 200-600 bins for the experiments...Fig. 4, we vary the number of annotated samples per bin...The results show that the evaluation converges quickly when the number of samples approaches 40 or 50"
  - [corpus]: Weak - related papers don't discuss precision-recall estimation from output distributions
- Break condition: If human annotation ambiguity is too high or if output distribution has extreme skew

### Mechanism 3
- Claim: Comparing models using OmniInput reveals differences that traditional test sets miss, particularly for overconfident predictions
- Mechanism: By examining precision-recall curves across the entire input space, models with similar test accuracy can be distinguished based on their behavior on uninformative inputs
- Core assumption: Traditional test sets don't capture model behavior on uninformative or out-of-distribution inputs
- Evidence anchors:
  - [abstract]: "Our experiments demonstrate that OMNI INPUT enables a more fine-grained comparison between models, especially when their performance is almost the same on pre-defined datasets"
  - [section]: "CNN-MNIST-0/1 and CNN-AUG-MNIST-0/1 exhibit almost no precision greater than 0, indicating that 'hand-written' digits are rare in the representative inputs even when the logit value is large"
  - [corpus]: Moderate - Model-diff paper addresses comparative analysis but doesn't focus on input space coverage
- Break condition: If all models have similar output distributions or if human annotators cannot distinguish informative from uninformative inputs

## Foundational Learning

- Concept: Output distribution sampling
  - Why needed here: Understanding how Gradient Wang-Landau sampling works is essential for implementing OmniInput's core functionality
  - Quick check question: How does the reweighting technique in Wang-Landau sampling ensure uniform exploration of output values?

- Concept: Precision-recall estimation from binned data
  - Why needed here: The framework relies on aggregating human annotations across output value bins to compute precision and recall
  - Quick check question: Why is weighted averaging by output distribution ρ(z) necessary when calculating precision at threshold λ?

- Concept: Model-centric vs data-centric evaluation
  - Why needed here: Understanding the fundamental difference between these approaches is crucial for appreciating OmniInput's innovation
  - Quick check question: What is the key limitation of traditional data-centric evaluation that OmniInput addresses?

## Architecture Onboarding

- Component map: Sampler (Gradient Wang-Landau) → Representative input collection → Human annotation → Precision-recall calculation → Model comparison
- Critical path: Sampler → Representative inputs → Human annotation (bottleneck)
- Design tradeoffs: More bins → better resolution but higher annotation cost; fewer bins → lower cost but coarser estimates
- Failure signatures: Poor precision-recall curves indicate overconfident predictions; high annotation variance suggests ambiguous inputs
- First 3 experiments:
  1. Implement basic Wang-Landau sampler for a simple binary classifier on MNIST
  2. Add human annotation pipeline for representative samples in output value bins
  3. Compare precision-recall curves of two different models on the same task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of OmniInput compare to traditional data-centric evaluation methods when evaluating models on out-of-distribution (OOD) test sets?
- Basis in paper: [explicit] The paper demonstrates that traditional data-centric evaluations can lead to inconsistent model rankings when using different OOD test sets, while OmniInput provides a more comprehensive evaluation by considering the entire input space.
- Why unresolved: The paper does not provide a direct comparison of OmniInput's performance against traditional methods on OOD test sets.
- What evidence would resolve it: Conducting experiments comparing OmniInput's precision-recall curves and AUPR scores with traditional data-centric evaluation methods on various OOD test sets.

### Open Question 2
- Question: How does the choice of sampler (e.g., Gradient Wang-Landau vs. other MCMC samplers) affect the quality and representativeness of the sampled inputs in OmniInput?
- Basis in paper: [explicit] The paper mentions that OmniInput uses an efficient sampler like Gradient Wang-Landau to obtain representative inputs, but it does not explore the impact of different samplers on the evaluation results.
- Why unresolved: The paper focuses on demonstrating the effectiveness of OmniInput using a specific sampler, without investigating the sensitivity of the results to the choice of sampler.
- What evidence would resolve it: Comparing the precision-recall curves and evaluation results of OmniInput when using different samplers (e.g., Gradient Wang-Landau, traditional MCMC samplers) on the same models and datasets.

### Open Question 3
- Question: How does the number of human annotations per output bin affect the convergence and reliability of the precision-recall curve in OmniInput?
- Basis in paper: [explicit] The paper mentions that 40 to 50 human annotations per output bin are sufficient for a converged precision-recall curve, but it does not investigate the impact of varying the number of annotations on the evaluation results.
- Why unresolved: The paper provides a general guideline for the number of annotations but does not explore the sensitivity of the results to the annotation effort.
- What evidence would resolve it: Conducting experiments with different numbers of human annotations per output bin and analyzing the convergence and stability of the precision-recall curves across multiple runs.

## Limitations

- The framework's reliance on the model's output distribution may introduce circularity if the model is already biased
- Human annotation scalability presents a significant bottleneck, particularly for complex tasks or larger output spaces
- The Gradient Wang-Landau sampler's hyperparameters and implementation details are not fully specified
- The framework's effectiveness for multi-class classification problems beyond binary tasks remains unverified

## Confidence

- High confidence: The framework's core mechanism of using output distribution sampling for test set construction is well-supported by the theoretical foundation and experimental results
- Medium confidence: The precision-recall estimation method and its convergence properties are reasonably supported, though the sufficiency of sample sizes needs more validation across diverse tasks
- Low confidence: The framework's generalizability to complex real-world scenarios and multi-class problems remains uncertain

## Next Checks

1. Test the framework on a multi-class classification problem (e.g., full MNIST or CIFAR-10) to verify scalability beyond binary classification
2. Compare OmniInput's results with traditional data-centric evaluation on the same models to quantify the differences in evaluation outcomes
3. Conduct sensitivity analysis on the number of bins and annotations per bin to establish minimum requirements for reliable evaluation across different tasks