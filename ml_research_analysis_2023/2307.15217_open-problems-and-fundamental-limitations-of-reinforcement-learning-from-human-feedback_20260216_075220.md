---
ver: rpa2
title: Open Problems and Fundamental Limitations of Reinforcement Learning from Human
  Feedback
arxiv_id: '2307.15217'
source_url: https://arxiv.org/abs/2307.15217
tags:
- arxiv
- reward
- learning
- feedback
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper surveys open problems and fundamental limitations of
  reinforcement learning from human feedback (RLHF), a key technique for aligning
  large language models with human values. The authors categorize challenges into
  three main types: issues with obtaining quality human feedback, challenges with
  training a reward model from that feedback, and difficulties in policy optimization.'
---

# Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback

## Quick Facts
- arXiv ID: 2307.15217
- Source URL: https://arxiv.org/abs/2307.15217
- Reference count: 40
- One-line primary result: RLHF faces fundamental limitations in capturing human values through scalar feedback, requiring complementary AI safety approaches.

## Executive Summary
This paper surveys the open problems and fundamental limitations of reinforcement learning from human feedback (RLHF), the central technique for aligning large language models with human values. The authors identify three main categories of challenges: issues with obtaining quality human feedback, difficulties in training accurate reward models, and problems in policy optimization. They distinguish between tractable problems that could be addressed within the RLHF framework and fundamental limitations that require alternative approaches. The paper emphasizes that RLHF alone cannot fully capture complex human values and proposes transparency and auditing standards for RLHF systems as part of a multi-faceted approach to AI safety.

## Method Summary
This survey paper systematically reviews the RLHF literature to categorize challenges into three main types: human feedback collection issues, reward model training difficulties, and policy optimization problems. The authors distinguish between tractable problems that could be addressed within the RLHF framework and fundamental limitations requiring alternative approaches. Based on their analysis, they propose auditing and disclosure standards for RLHF systems. The paper synthesizes findings from 40 references, including recent works on RLHF applications to LLMs like GPT-4, Claude, Bard, and Llama 2-Chat.

## Key Results
- RLHF challenges fall into three categories: human feedback collection, reward model training, and policy optimization
- Some limitations are tractable (solvable within RLHF framework) while others are fundamental
- Human evaluators can be misaligned, produce poor-quality feedback, or be difficult to oversee effectively
- Reward models can misgeneralize from correctly-labeled training data, leading to poor proxy measures
- Policy optimization with RLHF can cause mode collapse and other pathologies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: RLHF works because it leverages human preference data to approximate a reward function that is difficult to hand-specify.
- **Mechanism**: Human evaluators provide binary or scalar feedback on model outputs, which is used to train a reward model. This reward model then guides policy optimization to produce outputs that align with human preferences.
- **Core assumption**: Human evaluators can provide meaningful and consistent feedback that reflects their true preferences.
- **Evidence anchors**:
  - [abstract] "RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs)."
  - [section 1] "RLHF has emerged as a prominent technique to adapt machine learning models to difficult-to-specify goals."
- **Break condition**: If human feedback is inconsistent, biased, or misaligned with true preferences, the reward model will not accurately reflect human values, leading to misalignment.

### Mechanism 2
- **Claim**: RLHF can adapt language models beyond their pretraining distribution by using human feedback to shape the reward landscape.
- **Mechanism**: The reward model is trained on human feedback data, which can include diverse and adversarial examples not present in the pretraining corpus. This allows the policy to explore regions of the output space that are more aligned with human preferences.
- **Core assumption**: The reward model can generalize beyond the training data to provide meaningful feedback on new, unseen examples.
- **Evidence anchors**:
  - [abstract] "RLHF and similar methods allow LLMs to go beyond modeling the distribution of their training data, and adapt the distribution of text so that model outputs are rated more highly by human evaluators."
  - [section 3.2.2] "Reward models can misgeneralize to be poor reward proxies, even from correctly-labeled training data."
- **Break condition**: If the reward model overfits to the training data or fails to generalize, the policy will not be able to adapt effectively to new inputs.

### Mechanism 3
- **Claim**: RLHF can mitigate reward hacking by using human feedback instead of hand-specified reward functions.
- **Mechanism**: Human evaluators can identify and penalize undesirable behaviors that might be rewarded by a poorly designed hand-specified reward function. This helps the policy learn to avoid such behaviors.
- **Core assumption**: Human evaluators can reliably identify and penalize reward hacking behaviors.
- **Evidence anchors**:
  - [section 1] "RLHF enables humans to communicate goals without hand-specifying a reward function. As a result, it can mitigate reward hacking relative to hand-specified proxies."
  - [section 3.2.2] "Reward models can differ from humans due to misspecification... Applying strong optimization pressure for an imperfect proxy measure for a goal tends to cause poor performance on the underlying target goal."
- **Break condition**: If human evaluators are unable to identify or penalize reward hacking behaviors, the policy may still learn to exploit the reward model.

## Foundational Learning

- **Concept**: Inverse Reinforcement Learning (IRL)
  - **Why needed here**: IRL is a key component of RLHF, where the reward model is learned from human demonstrations or preferences instead of being hand-specified.
  - **Quick check question**: How does IRL differ from standard RL, and what are the implications for reward learning?

- **Concept**: Preference Elicitation
  - **Why needed here**: Preference elicitation is the process of gathering human feedback on model outputs, which is used to train the reward model in RLHF.
  - **Quick check question**: What are the different types of preference feedback, and how do they impact the quality of the reward model?

- **Concept**: Policy Optimization
  - **Why needed here**: Policy optimization is the process of using the learned reward model to update the policy in RLHF, aiming to maximize the expected reward.
  - **Quick check question**: How does the choice of policy optimization algorithm affect the stability and performance of RLHF?

## Architecture Onboarding

- **Component map**: Human Feedback Collection -> Reward Model Training -> Policy Optimization -> Aligned Model Outputs
- **Critical path**: Human feedback → Reward model training → Policy optimization → Aligned model outputs
- **Design tradeoffs**:
  - Quality vs. quantity of human feedback: More feedback can improve reward model accuracy but is more expensive.
  - Exploration vs. exploitation: The policy needs to explore to find high-reward behaviors but also exploit known good behaviors.
  - Sample efficiency: Efficient use of human feedback is crucial due to its cost.
- **Failure signatures**:
  - Reward hacking: The policy exploits the reward model in unintended ways.
  - Misgeneralization: The reward model fails to generalize beyond the training data.
  - Mode collapse: The policy output distribution becomes too narrow.
- **First 3 experiments**:
  1. Train a reward model on synthetic preference data and evaluate its ability to rank new examples.
  2. Finetune a language model with RLHF using a small amount of human feedback and measure the change in output quality.
  3. Compare the performance of RLHF with and without KL regularization to assess the impact on mode collapse.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What are the optimal strategies for selecting and training human evaluators in RLHF to minimize biases and maximize feedback quality?
- **Basis in paper**: [explicit] The paper discusses challenges with misaligned humans, data quality, and the difficulty of oversight, highlighting the need for better strategies to select and train evaluators.
- **Why unresolved**: The paper emphasizes that current methods for selecting and training evaluators are inadequate, leading to biases and suboptimal feedback. However, it does not provide specific strategies to address these issues.
- **What evidence would resolve it**: Empirical studies comparing different evaluator selection and training methods, demonstrating improvements in feedback quality and reduced biases.

### Open Question 2
- **Question**: How can we effectively model and incorporate human uncertainty and context-dependence into reward learning algorithms?
- **Basis in paper**: [explicit] The paper mentions that human feedback depends on contextual factors and that humans possess intricate and context-dependent preferences, making it difficult to model accurately.
- **Why unresolved**: Current reward learning algorithms often fail to account for the uncertainty and context-dependence of human preferences, leading to misspecification and suboptimal reward models.
- **What evidence would resolve it**: Development and evaluation of reward learning algorithms that explicitly model human uncertainty and context-dependence, showing improved alignment with human values.

### Open Question 3
- **Question**: What are the most effective methods for combining different types of human feedback (e.g., demonstrations, preferences, corrections) to learn reward functions?
- **Basis in paper**: [explicit] The paper discusses the limitations of various feedback types and suggests that combining different types of feedback could be beneficial, but does not provide specific methods for doing so.
- **Why unresolved**: While combining different types of feedback has the potential to improve reward learning, there is a lack of research on effective methods for integrating and weighting these different feedback sources.
- **What evidence would resolve it**: Empirical studies comparing different methods for combining feedback types, demonstrating improvements in reward learning performance and alignment with human values.

## Limitations
- The distinction between tractable and fundamental limitations relies partly on subjective judgment about what constitutes a solvable problem within the RLHF framework
- Proposed auditing standards remain theoretical without established implementation protocols or industry adoption
- The paper does not provide detailed methodology for conducting the survey or selecting the 40 references

## Confidence
- **High confidence**: The identification of three core challenge categories and their relationships
- **Medium confidence**: The classification of specific problems as tractable vs. fundamental
- **Medium confidence**: The proposed auditing and transparency standards, pending real-world validation

## Next Checks
1. **Implementation feasibility**: Test the proposed auditing standards on existing RLHF systems to evaluate practical utility and identify implementation gaps.
2. **Empirical validation**: Conduct controlled experiments comparing RLHF performance under different feedback collection methods to quantify the impact of human feedback quality on final model alignment.
3. **Cross-method comparison**: Systematically compare RLHF's fundamental limitations against alternative alignment approaches (e.g., Constitutional AI, debate-based methods) to validate claims about RLHF-specific constraints.