---
ver: rpa2
title: 'Graph vs. Sequence: An Empirical Study on Knowledge Forms for Knowledge-Grounded
  Dialogue'
arxiv_id: '2312.07868'
source_url: https://arxiv.org/abs/2312.07868
tags:
- knowledge
- graph
- dialogue
- sequence
- faithdial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper empirically compares two forms of knowledge - knowledge
  graphs and knowledge text - for knowledge-grounded dialogue generation. It investigates
  three key questions: which knowledge form is better, how to mutually adapt models
  and knowledge, and how different knowledge performs in few-shot settings.'
---

# Graph vs. Sequence: An Empirical Study on Knowledge Forms for Knowledge-Grounded Dialogue

## Quick Facts
- arXiv ID: 2312.07868
- Source URL: https://arxiv.org/abs/2312.07868
- Reference count: 20
- This paper empirically compares knowledge graphs and knowledge text for dialogue generation, finding that graphs generally yield better response quality while text achieves better factual consistency.

## Executive Summary
This paper investigates the impact of knowledge form (graphs vs. text) on knowledge-grounded dialogue generation. Through extensive experiments on three dialogue datasets using various Transformer architectures (Decoder-Only, Encoder-Decoder, Dual-Encoders) with different sizes and pretraining, the study reveals that knowledge graphs generally produce higher quality and more generalizable responses, while knowledge text provides better factual consistency. The research also demonstrates that optimal model selection depends on knowledge characteristics, and pretraining has universal positive effects, particularly for knowledge text. These findings offer practical guidance for choosing knowledge forms and models in dialogue systems.

## Method Summary
The study employs three Transformer architectures (GPT-2-based Decoder-Only, BART-based Encoder-Decoder, and Dual-Encoders) to process both knowledge graphs (serialized with special tokens) and knowledge text. Models are trained with pre-trained initialization across three datasets (WoW, FaithDial, OpenDialKG) using standard dialogue metrics (BLEU, ROUGE, DIST) and factual consistency measures (NLI, Q2F1, Q2NLI). Experiments systematically vary model size, pretraining status, and data availability to examine the interaction between knowledge form and model characteristics.

## Key Results
- Knowledge graphs yield better response quality and generalizability than knowledge text
- Knowledge text achieves better factual consistency than knowledge graphs
- Optimal model size and architecture depend on the characteristics of the knowledge
- Pretraining has universal positive impact, especially for knowledge text
- In few-shot settings, pretrained-like architectures perform better with limited data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph-based knowledge generally yields better response quality and generalizability than sequence-based knowledge in knowledge-grounded dialogue.
- Mechanism: Graph structures provide more precise, structured relationships between entities, reducing noise and ambiguity during generation. This structured form allows models to better capture logical connections and perform reasoning, leading to higher quality responses.
- Core assumption: The knowledge graph representation preserves essential relational information that enhances the model's ability to generate coherent and informative responses.
- Evidence anchors:
  - [abstract]: "knowledge graphs generally yield better response quality and generalizability"
  - [section]: "On the other hand, the performance of graph-based models is slightly higher than the sequence-based, which shows the upper-bound of graph-based models is better"
  - [corpus]: Weak evidence; the corpus mentions related work but lacks direct comparison of graph vs. sequence performance.
- Break condition: If the knowledge graph is poorly constructed, sparse, or noisy, the benefits diminish and may even degrade performance compared to sequence-based knowledge.

### Mechanism 2
- Claim: The optimal model size and architecture depend on the characteristics of the knowledge used.
- Mechanism: Knowledge with high precision and low noise (like manually annotated knowledge graphs) benefits more from larger models because they can better exploit the rich relational structure. Conversely, noisy or complex knowledge (like long Wikipedia passages) may not benefit as much from increased model size and might even suffer from overfitting.
- Core assumption: Model capacity should match the quality and complexity of the input knowledge to achieve optimal performance.
- Evidence anchors:
  - [abstract]: "the optimal model size and architecture depend on the characteristics of the knowledge"
  - [section]: "For instance, the knowledge in OpenDialKG is of high quality; even though it has limited data, the larger model performs better"
  - [corpus]: Weak evidence; the corpus does not provide direct support for model size vs. knowledge characteristics.
- Break condition: If the knowledge is of uniformly low quality, increasing model size may not yield improvements and could exacerbate issues like hallucination.

### Mechanism 3
- Claim: Pretraining has a universal positive impact, especially for knowledge text, due to the natural language patterns it captures.
- Mechanism: Pretrained models have learned rich linguistic patterns and representations from large corpora, which align well with the natural language form of knowledge text. This allows them to better understand and generate text-based knowledge. For knowledge graphs serialized into text, the structural information may not be as naturally captured, leading to smaller gains.
- Core assumption: Pretrained models are more effective when the input data aligns with the patterns they were trained on (i.e., natural language).
- Evidence anchors:
  - [abstract]: "pretraining has a universal positive impact, especially for knowledge text"
  - [section]: "pre-trained can lead to a more remarkable performance improvement when the knowledge is represented in sequence form, compared to the serialization form of the knowledge graph"
  - [corpus]: Weak evidence; the corpus does not directly discuss pretraining effects on different knowledge forms.
- Break condition: If the serialized knowledge graph closely mimics natural language patterns, or if the model is pretrained on graph-structured data, the advantage of pretraining on knowledge text may diminish.

## Foundational Learning

- Concept: Knowledge Graph vs. Knowledge Text
  - Why needed here: Understanding the fundamental differences between structured (graph) and unstructured (text) knowledge forms is crucial for selecting appropriate models and preprocessing methods.
  - Quick check question: What are the main advantages of using a knowledge graph over knowledge text in dialogue systems?

- Concept: Model Architecture Selection (Decoder-Only, Encoder-Decoder, Dual-Encoders)
  - Why needed here: Different architectures handle knowledge integration differently; knowing their strengths and weaknesses helps in choosing the right one for the task and knowledge form.
  - Quick check question: How does the Dual-Encoders architecture differ from Encoder-Decoder in processing knowledge and dialogue history?

- Concept: Pretraining and Fine-tuning
  - Why needed here: Pretraining provides a strong starting point with general language understanding, but fine-tuning adapts the model to the specific task and data characteristics.
  - Quick check question: Why might a pretrained model perform better on knowledge text than on serialized knowledge graphs?

## Architecture Onboarding

- Component map:
  Knowledge Encoder -> Dialogue Encoder -> Decoder -> Response Generation
  (with special tokens [triple], [entity], [relation] for graph serialization)

- Critical path:
  1. Knowledge preprocessing (graph serialization or text cleaning)
  2. Encoding of knowledge and dialogue history
  3. Cross-attention in the decoder to fuse knowledge and dialogue context
  4. Autoregressive response generation

- Design tradeoffs:
  - Model size vs. data size: Larger models benefit from high-quality, structured knowledge but may overfit on noisy data
  - Knowledge form vs. architecture: Dual-Encoders are generally more stable across knowledge forms, but specific forms may align better with certain architectures
  - Pretraining vs. task adaptation: Pretrained models excel with natural language knowledge but may need adaptation for structured data

- Failure signatures:
  - Poor response quality: May indicate inadequate knowledge denoising or mismatched model capacity
  - Factual inconsistency: Could suggest the model is not effectively leveraging the knowledge source
  - Overfitting on few-shot data: Might mean the model is too large or the architecture is too complex for the data size

- First 3 experiments:
  1. Serialize a small knowledge graph and run it through a Dual-Encoders model to verify basic functionality
  2. Compare BLEU scores between graph and text knowledge using the same architecture to confirm performance differences
  3. Test model performance with varying knowledge sizes to observe the impact on response quality and diversity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal model size for knowledge-grounded dialogue generation when using knowledge graphs versus knowledge text?
- Basis in paper: [inferred] The paper discusses that model size should be mutually adaptive to the knowledge's characteristics, with different datasets (OpenDialKG, FaithDial, WoW) showing varying optimal model sizes for knowledge graphs versus knowledge text.
- Why unresolved: The paper provides empirical observations that the optimal model size depends on the knowledge's characteristics, but does not establish a clear guideline or formula for determining the optimal model size for different types of knowledge.
- What evidence would resolve it: A comprehensive study varying model sizes across multiple datasets with different knowledge characteristics, providing a clear mapping between knowledge type, dataset complexity, and optimal model size.

### Open Question 2
- Question: How does the quality of knowledge graphs extracted by automated tools compare to manually annotated knowledge graphs in terms of response quality and factual consistency?
- Basis in paper: [explicit] The paper mentions that knowledge graphs in FaithDial and WoW are extracted by automated tools and are not as good quality as manually annotated OpenDialKG, but does not provide a direct comparison of their performance.
- Why unresolved: While the paper hints at quality differences between manually annotated and automatically extracted knowledge graphs, it does not provide a quantitative comparison of their impact on response quality and factual consistency.
- What evidence would resolve it: A controlled experiment comparing response quality and factual consistency metrics between manually annotated knowledge graphs and automatically extracted knowledge graphs across multiple datasets.

### Open Question 3
- Question: What are the long-term effects of using pre-trained models on the generalizability and adaptability of knowledge-grounded dialogue systems?
- Basis in paper: [inferred] The paper shows that pre-trained models have a positive impact on performance, especially for knowledge text, but does not explore the long-term effects on generalizability and adaptability.
- Why unresolved: The paper focuses on immediate performance improvements from pre-trained models but does not investigate how these models perform over time or in adapting to new knowledge domains.
- What evidence would resolve it: Longitudinal studies tracking the performance of pre-trained models in knowledge-grounded dialogue systems over extended periods and across multiple knowledge domains.

## Limitations
- Results are based on specific Transformer architectures and may not generalize to other model families
- Knowledge graph quality varies significantly across datasets, complicating cross-dataset comparisons
- The serialization method for knowledge graphs could influence results but is not systematically varied

## Confidence
- **Low Confidence**: Claims about knowledge graph superiority depend heavily on dataset characteristics and are not universally supported
- **Medium Confidence**: The observation that optimal model size depends on knowledge characteristics is well-supported but limited to three datasets studied
- **High Confidence**: Pretraining's positive impact, especially for knowledge text, is consistently demonstrated across all experiments and metrics

## Next Checks
1. Test the same knowledge-form hypotheses using non-Transformer architectures (e.g., RNNs, Graph Neural Networks) to determine if observed patterns are architecture-dependent or represent more fundamental relationships between knowledge form and model performance

2. Systematically vary the quality and structure of knowledge graphs (through controlled addition of noise, removal of edges, or entity consolidation) while measuring performance degradation to establish more precise relationships between knowledge quality and model requirements

3. Implement and evaluate different graph serialization approaches (e.g., adjacency lists, hierarchical representations) to determine whether observed advantages of knowledge graphs are preserved when using more compact or structured text representations that better capture graph semantics