---
ver: rpa2
title: 'Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive
  Negotiation'
arxiv_id: '2309.17234'
source_url: https://arxiv.org/abs/2309.17234
tags:
- score
- issue
- your
- deal
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a benchmark for evaluating LLMs using interactive
  negotiation games. The games are text-based, multi-agent, and involve complex reasoning
  tasks such as arithmetic, inference, exploration, and planning.
---

# Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation

## Quick Facts
- arXiv ID: 2309.17234
- Source URL: https://arxiv.org/abs/2309.17234
- Reference count: 40
- Key outcome: Introduces a benchmark for evaluating LLMs using interactive negotiation games, showing GPT-4 significantly outperforms earlier models when using structured Chain-of-Thought prompting.

## Executive Summary
This paper introduces a novel benchmark for evaluating Large Language Models (LLMs) in multi-agent negotiation scenarios. The benchmark features text-based, multi-agent games with complex reasoning tasks including arithmetic, inference, exploration, and planning. Through systematic zero-shot Chain-of-Thought prompting, the authors demonstrate that LLMs can effectively negotiate and reach successful deals when prompted to simulate observation, exploration, and planning steps. The benchmark is designed to be highly challenging, with GPT-4 showing significant advantages over earlier models like GPT-3.5 and small models.

## Method Summary
The authors create a negotiation game framework with 6 parties and 5 issues, where each party has specific preferences and minimum score thresholds. They generate diverse games by perturbing parties, issues, and preferences, and control difficulty by adjusting scores and thresholds. The primary method uses zero-shot Chain-of-Thought prompting with structured reasoning steps: agents are prompted to plan answers and show intermediate calculations in a scratchpad. This decomposition enables strategic reasoning under partial information by breaking the task into observation, exploration, and planning phases.

## Key Results
- GPT-4 significantly outperforms earlier models, especially when using structured Chain-of-Thought prompting
- GPT-3.5 and small models mostly fail to reach successful deals in the benchmark
- Agents' behavior can be modulated to promote greediness or attack other agents, affecting group dynamics
- The benchmark is highly challenging, with even GPT-4 and SoTA large models showing room for improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can reason about multi-agent preferences when explicitly prompted to simulate observation, exploration, and planning steps
- Mechanism: Chain-of-Thought prompting decomposes the negotiation task into structured sub-tasks: (1) collect observations of others' deals and infer their preferences, (2) explore candidate deals that balance self-interest with others' preferences, (3) select a final proposal and plan future moves. This decomposition reduces cognitive load and enables strategic reasoning under partial information.
- Core assumption: LLMs have sufficient internal reasoning capacity to maintain and update mental models of other agents' preferences across turns.
- Evidence anchors: Abstract statement about zero-shot Chain-of-Thought prompting, section on prompting agents to plan answers and show intermediate calculations, corpus analysis using various model sizes.
- Break condition: If agents fail to correctly calculate deal scores or violate their minimum thresholds, indicating reasoning breakdowns.

### Mechanism 2
- Claim: Greedy or adversarial incentives can be encoded in LLM behavior without explicit deal specifications, affecting group dynamics
- Mechanism: By modifying the incentive structure in prompts (e.g., "maximize your own score" or "sabotage the negotiation"), agents autonomously adjust their proposals to align with these goals. This demonstrates that high-level strategic objectives can be communicated to LLMs and influence emergent behavior.
- Core assumption: LLMs can interpret abstract strategic goals and translate them into concrete negotiation actions.
- Evidence anchors: Abstract statement about behavior modulation, section on agents acting in self-interested ways, corpus discussion on credit assignment challenges.
- Break condition: If agents ignore the strategic incentives or fail to adapt their proposals accordingly.

### Mechanism 3
- Claim: Negotiation performance can be tuned by adjusting game difficulty parameters (minimum thresholds, feasible solution sets)
- Mechanism: By varying the minimum acceptance thresholds and the number of feasible deals, the benchmark can control the difficulty level. This allows for evaluating model performance across a spectrum of challenges and creating a less saturating benchmark.
- Core assumption: LLMs can handle varying levels of complexity in negotiation tasks when the problem space is constrained appropriately.
- Evidence anchors: Abstract statement about creating semantically equivalent games, section on controlling game difficulty, corpus statement on studying cooperative behavior.
- Break condition: If models fail to adapt to new game configurations or performance saturates regardless of difficulty adjustments.

## Foundational Learning

- Concept: Multi-agent negotiation dynamics
  - Why needed here: Understanding how different parties with conflicting interests interact is crucial for designing and evaluating negotiation games.
  - Quick check question: Can you explain the difference between cooperative and competitive negotiation strategies?

- Concept: Chain-of-Thought prompting
  - Why needed here: CoT is the primary method used to enable LLMs to reason through complex negotiation tasks step-by-step.
  - Quick check question: How does breaking down a task into observation, exploration, and planning steps help LLMs perform better in negotiations?

- Concept: Game theory and equilibrium concepts
  - Why needed here: Knowledge of game theory helps in understanding the strategic interactions between agents and the concept of feasible solutions.
  - Quick check question: What is the difference between a Nash equilibrium and a Pareto-optimal solution in the context of multi-party negotiation?

## Architecture Onboarding

- Component map: Game Engine -> LLM Agent -> Prompt Generator -> Score Calculator -> Metric Tracker
- Critical path: 1. Initialize game with parties, issues, and preferences. 2. Generate initial prompts for each agent. 3. Iterate through rounds: agent receives prompt, generates response, game state updates. 4. Calculate scores and check for agreement. 5. End game and collect metrics.
- Design tradeoffs: Public vs. private communication (public channels simplify implementation but limit strategic depth), fixed vs. adaptive difficulty (fixed allows controlled experiments), structured vs. free-form prompts (structured guides reasoning but may constrain creativity).
- Failure signatures: Agents consistently propose deals below their minimum thresholds, agents fail to adapt proposals based on others' actions, agents reveal confidential information in public answers.
- First 3 experiments: 1. Compare GPT-4 vs. GPT-3.5 performance on base game with structured CoT prompts. 2. Evaluate impact of different prompt structures (with/without observation step) on agent performance. 3. Test model generalization on newly created games with different difficulty levels.

## Open Questions the Paper Calls Out

- Can LLMs learn to negotiate effectively in more complex multi-agent scenarios with private messaging and alliance formation? (Basis: Inferred from mention of future work on private messages and alliance formation similar to games like Diplomacy)
- How can we design effective defenses against adversarial negotiation tactics employed by LLMs? (Basis: Explicit mention of future work investigating attacks and potential defenses)
- What are the ethical implications of using LLMs for autonomous negotiation, and how can we ensure responsible deployment? (Basis: Inferred from acknowledgment of evaluating potential misuse and need for robustification)

## Limitations

- The exact scoring values for each sub-option in base game and new games remain unspecified, limiting precise reproduction
- The benchmark focuses on a specific game structure (6 parties, 5 issues) which may limit generalizability to other negotiation scenarios
- Random order of agents' turns in each simulation run is not detailed, which could affect reproducibility

## Confidence

- **High Confidence**: Claims about GPT-4's superior performance compared to earlier models when using structured Chain-of-Thought prompting
- **Medium Confidence**: Claims about effectiveness of incentive modulation for greedy or adversarial behavior
- **Medium Confidence**: Claims about difficulty tuning through threshold and feasible set adjustments

## Next Checks

1. **Parameter Verification**: Obtain and verify exact scoring values for all sub-options in base game and newly created games to ensure faithful reproduction
2. **Prompt Structure Testing**: Systematically test variations in prompt structure (e.g., with/without observation step) to quantify their impact on agent performance
3. **Cross-Scenario Generalization**: Evaluate model performance on negotiation games with different numbers of parties and issues to assess benchmark's generalizability