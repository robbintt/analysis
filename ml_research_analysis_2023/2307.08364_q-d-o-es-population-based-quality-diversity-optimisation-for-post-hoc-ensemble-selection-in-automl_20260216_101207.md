---
ver: rpa2
title: 'Q(D)O-ES: Population-based Quality (Diversity) Optimisation for Post Hoc Ensemble
  Selection in AutoML'
arxiv_id: '2307.08364'
source_url: https://arxiv.org/abs/2307.08364
tags:
- ensemble
- diversity
- methods
- qdo-es
- best
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two novel population-based ensemble selection
  methods, QO-ES and QDO-ES, for post hoc ensembling in AutoML. QO-ES optimises solely
  for predictive performance, while QDO-ES also considers the diversity of ensembles
  within the population.
---

# Q(D)O-ES: Population-based Quality (Diversity) Optimisation for Post Hoc Ensemble Selection in AutoML

## Quick Facts
- arXiv ID: 2307.08364
- Source URL: https://arxiv.org/abs/2307.08364
- Reference count: 40
- Primary result: Population-based QO-ES and QDO-ES ensemble selection methods outperform greedy ensemble selection (GES) on validation data, with diversity helping but also increasing overfitting risk.

## Executive Summary
This paper introduces two novel population-based ensemble selection methods for post hoc ensembling in AutoML: QO-ES, which optimizes solely for predictive performance, and QDO-ES, which also considers the diversity of ensembles within the population. The methods are evaluated on 71 classification datasets from the AutoML benchmark, comparing them to the commonly used greedy ensemble selection (GES). Results show that QO-ES and QDO-ES often outrank GES, albeit only statistically significant on validation data. The study suggests that diversity can be beneficial for post hoc ensembling but also increases the risk of overfitting.

## Method Summary
The paper proposes two population-based ensemble selection methods, QO-ES and QDO-ES, as alternatives to greedy ensemble selection (GES) for post hoc ensembling in AutoML. QO-ES uses a population-based search to find the best ensemble by exploring multiple candidate ensembles in parallel, while QDO-ES additionally considers the diversity of ensembles within the population. Both methods use adaptive sampling and operator probabilities to balance exploration and exploitation during optimization. The methods are evaluated on 71 classification datasets from the AutoML benchmark, comparing them to GES and single best model baseline.

## Key Results
- QO-ES and QDO-ES often outperform GES, but only statistically significant on validation data
- Adding diversity to the optimization process (QDO-ES) can be beneficial but also increases the risk of overfitting
- The optimal balance between exploration and exploitation changes over the course of the search and depends on the problem structure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Population-based search avoids local optima by exploring multiple candidate ensembles in parallel rather than following a single greedy path.
- Mechanism: By maintaining an archive of diverse high-performing ensembles, the algorithm can backtrack and recombine parts of different ensembles via crossover and mutation operators, which allows it to explore combinations that GES's deterministic greedy step would never reach.
- Core assumption: The fitness landscape for ensemble weights is non-convex and contains multiple local optima that differ in both performance and ensemble diversity.
- Evidence anchors:
  - [abstract] "We believe that GES may not always be optimal, as it performs a simple deterministic greedy search."
  - [section 4] "Population-based alternatives to GES have been explored in other fields...such methods allow us to add stochasticity while building the ensemble, e.g., through crossover or random sampling."

### Mechanism 2
- Claim: Maintaining behaviorally diverse ensembles in the population improves generalization by reducing overfitting risk.
- Mechanism: QDO-ES stores ensembles that achieve similar validation performance but differ in ensemble diversity (measured via ALC and CSS). This diversity in the population prevents the algorithm from overcommitting to ensembles that fit the validation set too closely, thus increasing the chance of finding an ensemble that generalizes better to unseen data.
- Core assumption: Ensemble diversity on the validation set correlates with reduced overfitting to that set, and thus better test performance.
- Evidence anchors:
  - [abstract] "Our results further suggest that diversity can be beneficial for post hoc ensembling but also increases the risk of overfitting."
  - [section 4.1] "We focus on quality diversity optimisation (QDO)...maximises a single-objective function while maintaining a behaviourally diverse population."

### Mechanism 3
- Claim: Adaptive sampling and operator probabilities balance exploration and exploitation dynamically during optimization.
- Mechanism: The algorithm adjusts the probability of choosing deterministic vs. random sampling, and the use of crossover vs. mutation, based on recent performance trends. This self-tuning allows the search to start exploratory and gradually shift toward exploitation as promising regions are identified.
- Core assumption: The optimal balance between exploration and exploitation changes over the course of the search and depends on the problem structure.
- Evidence anchors:
  - [section 4.2.1] "We opted for an adaptive sampling approach because we expect the optimal probability of deterministic vs. random sampling changes over time â€“ like an exploration-exploitation tradeoff."
  - [section 4.2] "The probability of using crossover adapts through the run, similar to the adaptive sampling approach."

## Foundational Learning

- Concept: Ensemble diversity metrics (ALC and CSS)
  - Why needed here: These metrics define the behavior space for QDO-ES, enabling the algorithm to maintain diverse high-performing ensembles and avoid overfitting.
  - Quick check question: How does ALC differ from CSS in measuring ensemble diversity, and why might both be useful?

- Concept: Quality diversity optimization (QDO) principles
  - Why needed here: QDO provides the theoretical framework for balancing performance optimization with behavioral diversity, which is the core innovation over standard population-based methods.
  - Quick check question: What distinguishes QDO from multi-objective optimization when applied to ensemble selection?

- Concept: Population-based search operators (crossover, mutation, sampling)
  - Why needed here: These operators enable the algorithm to explore the ensemble space stochastically, which is essential for avoiding local optima and discovering novel ensemble combinations.
  - Quick check question: Why does the algorithm perform crossover on repetition vectors rather than weight vectors?

## Architecture Onboarding

- Component map: Archive -> Sampling module -> Genetic operators (crossover/mutation) -> Initialization strategy
- Critical path: 1) Initialize archive with base ensembles, 2) Sample parents adaptively, 3) Apply crossover/mutation to generate offspring, 4) Evaluate and insert into archive, 5) Repeat for fixed iterations, 6) Select final ensemble from archive
- Design tradeoffs: Larger archives increase diversity but require more computation; more stochastic sampling increases exploration but may slow convergence; complex diversity metrics may better prevent overfitting but add computational overhead
- Failure signatures: If overfitting occurs, validation performance will be much higher than test performance; if the population collapses, diversity metrics will be low and performance gains over GES will diminish; if adaptation is too aggressive, the algorithm may oscillate between exploration and exploitation
- First 3 experiments:
  1. Run QO-ES vs GES on a small dataset with fixed parameters to verify basic functionality and performance gain
  2. Enable QDO-ES and compare validation vs. test performance to observe overfitting effects
  3. Vary the archive initialization strategy (L1 vs. L2 vs. Random L2) to assess impact on final ensemble quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of QO-ES and QDO-ES change when using different validation strategies beyond the 33% hold-out split used in Auto-Sklearn?
- Basis in paper: [explicit] The paper mentions that the validation data generated by Auto-Sklearn's 33% hold-out split may be responsible for the large difference between validation and test performance, and suggests it would be interesting to investigate the performance for other AutoML systems that use more sophisticated validation procedures.
- Why unresolved: The study only used Auto-Sklearn's validation approach and did not explore alternative validation strategies or AutoML systems with different validation methods.
- What evidence would resolve it: Experiments comparing QO-ES and QDO-ES performance using various validation strategies (e.g., k-fold cross-validation, repeated cross-validation) or different AutoML systems would provide insights into how validation method affects ensemble performance.

### Open Question 2
- Question: What is the optimal balance between ensemble diversity and performance, and how can this trade-off be effectively managed during post-hoc ensemble selection?
- Basis in paper: [explicit] The paper states that there appears to be a trade-off between ensemble diversity and performance, and that adding ensemble diversity to the optimisation process can be beneficial but also increases the risk of overfitting.
- Why unresolved: The study observed this trade-off but did not investigate methods to find the optimal balance or develop techniques to manage it effectively.
- What evidence would resolve it: Research exploring methods to quantify and optimize the diversity-performance trade-off, such as adaptive diversity regularization or multi-objective optimization approaches, would help determine the best strategies for balancing these competing objectives.

### Open Question 3
- Question: How do the proposed QO-ES and QDO-ES methods compare to other population-based ensemble selection techniques in terms of performance, efficiency, and robustness to overfitting?
- Basis in paper: [inferred] The paper compares QO-ES and QDO-ES to GES but does not compare them to other population-based methods or explore variations of QDO for ensemble selection.
- Why unresolved: The study focused on comparing against GES and exploring QDO for ensemble selection, but did not investigate a broader range of population-based techniques or variations of QDO.
- What evidence would resolve it: Experiments comparing QO-ES and QDO-ES to other population-based ensemble selection methods (e.g., genetic algorithms, particle swarm optimization) and exploring different QDO approaches for ensemble selection would provide a more comprehensive understanding of their relative strengths and weaknesses.

## Limitations
- The primary uncertainty lies in the adaptive sampling mechanism's implementation details, which are not fully specified in the paper.
- The paper reports statistical significance only on validation data, not test data, which raises concerns about potential overfitting.
- The study's reliance on Auto-Sklearn 1.0 base models may limit generalizability to other AutoML systems or ensemble selection scenarios.

## Confidence
- High confidence: The theoretical framework of QDO-ES and its distinction from standard optimization methods
- Medium confidence: The empirical performance claims, particularly the lack of statistical significance on test data
- Medium confidence: The mechanism by which diversity improves generalization, given the mixed results on overfitting

## Next Checks
1. Implement the adaptive sampling mechanism with different parameter settings to test sensitivity and robustness
2. Conduct ablation studies comparing QDO-ES with and without diversity constraints to quantify the benefit
3. Test the algorithms on datasets from different AutoML systems (e.g., AutoGluon, H2O) to assess generalizability