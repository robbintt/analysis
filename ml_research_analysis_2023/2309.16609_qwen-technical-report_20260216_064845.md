---
ver: rpa2
title: Qwen Technical Report
arxiv_id: '2309.16609'
source_url: https://arxiv.org/abs/2309.16609
tags:
- arxiv
- language
- qwen
- code
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Qwen introduces a comprehensive series of large language models
  (LLMs) spanning different parameter sizes, including base models and chat variants
  aligned via supervised finetuning and reinforcement learning from human feedback.
  Specialized models for coding and mathematics are also developed.
---

# Qwen Technical Report

## Quick Facts
- arXiv ID: 2309.16609
- Source URL: https://arxiv.org/abs/2309.16609
- Authors: 
- Reference count: 40
- Primary result: Qwen series achieves state-of-the-art performance across diverse benchmarks with chat variants matching proprietary models

## Executive Summary
Qwen introduces a comprehensive series of large language models spanning different parameter sizes, including base models and chat variants aligned via supervised finetuning and reinforcement learning from human feedback. Specialized models for coding and mathematics are also developed. The models achieve state-of-the-art performance on diverse benchmarks, with Qwen-Chat-14B outperforming open-source models of similar size and matching some proprietary models. Human evaluation shows Qwen-Chat-RLHF significantly improves response quality over SFT models, though still trailing GPT-4.

## Method Summary
Qwen employs a three-stage training pipeline: massive pretraining on 3 trillion tokens of diverse data, supervised finetuning on instruction-response pairs for chat alignment, and reinforcement learning from human feedback for preference optimization. The architecture includes Transformer modifications like untied embeddings, RoPE positional encoding, and specialized attention mechanisms (LogN-Scaling, window attention). Domain-specific models for coding and mathematics are created through continued pretraining on specialized datasets. The models are evaluated across comprehensive benchmarks including MMLU, C-Eval, GSM8K, MATH, HumanEval, and MBPP.

## Key Results
- Qwen-Chat-14B outperforms open-source models of similar size and matches some proprietary models on benchmarks
- Specialized coding models (CODE-Qwen) surpass open-source baselines and approach proprietary performance on HumanEval and MBPP
- Math-specialized models (MATH-Qwen) outperform open-source alternatives and approach GPT-3.5-level accuracy on GSM8K and MATH
- Qwen-Chat-RLHF significantly improves response quality over SFT models in human evaluation, though still trailing GPT-4

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The combination of pretraining on massive data and specialized alignment techniques produces superior downstream performance.
- Mechanism: Pretraining on 3 trillion tokens of diverse data builds broad world knowledge and capabilities, while supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF) align the model to human preferences and task requirements.
- Core assumption: Large-scale pretraining followed by targeted alignment preserves and enhances the general capabilities while adding task-specific competence.
- Evidence anchors:
  - [abstract] "The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive."
  - [section] "Our experimental results demonstrate that the three QWEN models exhibit exceptional performance across all downstream tasks. It is worth noting that even the larger models, such as LLaMA2-70B, are outperformed by QWEN-14B in 3 tasks."
- Break condition: If alignment techniques degrade general capabilities or if pretraining data is insufficient to cover task domains.

### Mechanism 2
- Claim: Specialized models for coding and mathematics outperform general-purpose models on domain-specific tasks.
- Mechanism: Continuing pretraining on domain-specific data (code or math) after general pretraining, followed by supervised finetuning on task-specific datasets, creates models optimized for those domains.
- Core assumption: Domain-specific pretraining data provides task-relevant patterns that general pretraining cannot capture effectively.
- Evidence anchors:
  - [abstract] "Furthermore, we have developed coding-specialized models, CODE -Q WEN and CODE -Q WEN -CHAT, as well as mathematics-focused models, MATH-Q WEN -CHAT, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models."
  - [section] "Our CODE -Q WEN models have been compared with both proprietary and open-source language models... Our analysis reveals that specialized models, specifically CODE -Q WEN and CODE -Q WEN -CHAT, significantly outperform previous baselines with similar parameter counts."
- Break condition: If domain-specific pretraining data is insufficient or if the gap between general and specialized models is negligible.

### Mechanism 3
- Claim: The use of specialized attention mechanisms and context length extension techniques enables effective processing of long sequences.
- Mechanism: LogN-Scaling rescales attention dot products based on context length ratios, window attention restricts attention to limited context windows, and layer-wise window assignment optimizes performance across layers.
- Core assumption: These attention modifications maintain performance while reducing computational complexity for long sequences.
- Evidence anchors:
  - [section] "To evaluate the effectiveness of context length extension, Table 3 presents the test results on arXiv in terms of perplexity (PPL). These results demonstrate that by combining NTK-aware interpolation, LogN-Scaling, and layer-wise window assignment, we can effectively maintain the performance of our models in the context of over 8192 tokens."
  - [section] "QWEN additionally incorporates two attention mechanisms: LogN-Scaling (Chiang & Cholak, 2022; Su, 2023a) and window attention (Beltagy et al., 2020)."
- Break condition: If attention modifications significantly degrade performance on short sequences or if computational savings are insufficient.

## Foundational Learning

- Concept: Autoregressive language modeling
  - Why needed here: This is the fundamental training objective for both pretraining and finetuning stages, where the model learns to predict the next token given previous context.
  - Quick check question: What is the loss function used during pretraining and finetuning of Qwen models?

- Concept: Supervised finetuning (SFT)
  - Why needed here: SFT aligns the pretrained model to human-style conversations by training on labeled instruction-response pairs, creating the base chat model.
  - Quick check question: How does SFT differ from pretraining in terms of data format and training objective?

- Concept: Reinforcement learning from human feedback (RLHF)
  - Why needed here: RLHF further aligns the SFT model to human preferences by training a reward model and using PPO to optimize the policy, creating the final chat model.
  - Quick check question: What are the four models involved in the RLHF process described in the paper?

## Architecture Onboarding

- Component map: Base Transformer architecture with modifications (untied embedding, RoPE positional encoding, RMSNorm, SwiGLU activation) -> Specialized attention mechanisms (LogN-Scaling, window attention) -> Tokenization system (BPE with vocabulary expansion for multilingual support) -> Training pipeline (pretraining → SFT → RLHF for chat models, specialized pretraining for coding/math) -> Evaluation framework (comprehensive benchmarks across multiple domains)

- Critical path: 1. Pretraining on massive dataset → creates general knowledge base, 2. SFT on conversational data → aligns to human interaction patterns, 3. RLHF with reward model → optimizes for human preference, 4. Specialized pretraining for coding/math → adds domain expertise, 5. Comprehensive evaluation → validates performance improvements

- Design tradeoffs:
  - Untied embedding vs tied embedding: better performance vs higher memory usage
  - LogN-Scaling vs standard attention: better long-context performance vs complexity
  - Domain specialization vs general capability: better task performance vs reduced versatility
  - Tokenization vocabulary size: better compression vs increased model complexity

- Failure signatures:
  - Pretraining failures: poor generalization, lack of task-specific knowledge
  - SFT failures: unnatural responses, failure to follow instructions
  - RLHF failures: misalignment with human preferences, reward hacking
  - Evaluation failures: underperformance on benchmarks, poor human evaluation results

- First 3 experiments:
  1. Compare base model performance on MMLU with and without LogN-Scaling to verify attention mechanism effectiveness
  2. Test specialized coding model vs base model on HumanEval to validate domain specialization benefits
  3. Evaluate RLHF model vs SFT model on human preference benchmark to confirm alignment improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Qwen's performance on long-context tasks compare to proprietary models when using extended context lengths beyond 2048 tokens?
- Basis in paper: Explicit - The paper discusses context length extension techniques and evaluates perplexity at various sequence lengths up to 16384 tokens.
- Why unresolved: The paper only evaluates perplexity on arXiv data, not actual task performance on benchmarks or real-world applications.
- What evidence would resolve it: Performance comparisons on downstream benchmarks like MMLU, GSM8K, or HumanEval using extended context lengths would show if the technical improvements translate to better task performance.

### Open Question 2
- Question: What are the specific limitations of Qwen's RLHF alignment compared to GPT-4, and can these gaps be closed with additional fine-tuning data or architectural modifications?
- Basis in paper: Explicit - The paper states that Qwen-RLHF significantly outperforms SFT models but "still falls behind GPT-4 on our benchmark."
- Why unresolved: The paper doesn't specify which aspects of GPT-4's performance Qwen lacks, such as creative writing, complex reasoning, or safety alignment.
- What evidence would resolve it: Detailed ablation studies comparing different aspects of alignment quality, or experiments testing whether scaling data size, diversity, or model architecture could close the performance gap.

### Open Question 3
- Question: How does Qwen's tool-use capability scale with model size, and is there a threshold where additional parameters stop improving tool selection accuracy?
- Basis in paper: Explicit - The paper shows tool selection accuracy improves with model size (92% for 1.8B, 98% for 7B and 14B) but notes "beyond a certain point, there is little improvement."
- Why unresolved: The paper doesn't identify what that threshold is or whether even larger models could achieve near-perfect tool selection.
- What evidence would resolve it: Testing intermediate model sizes (e.g., 3B, 5B) or much larger variants would reveal the scaling relationship and potential saturation point.

## Limitations

- The evaluation primarily focuses on benchmark performance without sufficient ablation studies to isolate the contribution of specific architectural choices like LogN-Scaling and window attention.
- The comparison with proprietary models like GPT-4 is limited to human evaluation without detailed quantitative analysis across multiple dimensions.
- The paper lacks thorough analysis of failure modes and edge cases where the models might underperform.

## Confidence

- **High confidence**: Claims about overall model performance improvements and the effectiveness of the pretraining→SFT→RLHF pipeline. These are well-supported by benchmark results and multiple evaluation methods.
- **Medium confidence**: Claims about the specific contributions of architectural modifications (LogN-Scaling, window attention) and domain specialization benefits. While performance improvements are demonstrated, the ablation studies are insufficient to definitively attribute improvements to these specific components.
- **Low confidence**: Claims about competitive performance with proprietary models, particularly the assertion that Qwen-Chat-14B matches some proprietary models. The evidence is limited to human evaluation without comprehensive quantitative comparison.

## Next Checks

1. Conduct ablation studies isolating the effects of LogN-Scaling and window attention by comparing models with and without these modifications on long-context benchmarks to verify their specific contributions to performance improvements.

2. Perform comprehensive quantitative comparison between Qwen-Chat-14B and GPT-4 across the same evaluation dimensions (including but not limited to MMLU, coding tasks, and human preference metrics) to validate competitive performance claims.

3. Test the specialized coding and math models on additional domain-specific benchmarks beyond those presented to verify that the domain specialization consistently outperforms general models across a broader range of tasks.