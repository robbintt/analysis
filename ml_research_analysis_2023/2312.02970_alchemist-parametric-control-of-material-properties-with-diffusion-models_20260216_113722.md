---
ver: rpa2
title: 'Alchemist: Parametric Control of Material Properties with Diffusion Models'
arxiv_id: '2312.02970'
source_url: https://arxiv.org/abs/2312.02970
tags:
- arxiv
- material
- image
- editing
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to edit material properties (roughness,
  metallic, albedo, and transparency) of objects in real images using diffusion models.
  The key idea is to leverage the generative prior of text-to-image models and fine-tune
  them on a synthetic dataset with physically-based materials.
---

# Alchemist: Parametric Control of Material Properties with Diffusion Models

## Quick Facts
- arXiv ID: 2312.02970
- Source URL: https://arxiv.org/abs/2312.02970
- Reference count: 40
- Key outcome: Method to edit material properties (roughness, metallic, albedo, transparency) of objects in real images using diffusion models with smooth transitions and photorealistic results

## Executive Summary
This paper presents Alchemist, a method for precise control of material properties in images using diffusion models. The approach fine-tunes a pre-trained text-to-image diffusion model on a synthetic dataset with controlled material attributes, enabling the editing of roughness, metallic, albedo, and transparency in real-world images. By conditioning the model on both text instructions and scalar values representing relative attribute strength, Alchemist achieves smooth, continuous control over material properties while preserving other image attributes. The method demonstrates generalization from synthetic to real data and extends to 3D scenes through NeRFs.

## Method Summary
Alchemist fine-tunes Stable Diffusion 1.5 with InstructPix2Pix initialization on a synthetic dataset of 100 3D objects with randomized materials and environment maps, rendered from 15 viewpoints each. The model conditions on an input image, text prompt, and a scalar grid representing the desired attribute strength. During training, the principled BSDF shader node in Blender is modified to systematically vary material properties. The fine-tuned model can then edit material properties in real images while maintaining photorealism and smooth transitions as the attribute strength varies.

## Key Results
- User studies show edits are photorealistic and preferred over baseline methods
- Method generalizes to real images despite training only on synthetic data
- Smooth transitions achieved as relative strength of desired attribute is varied
- Extension to NeRFs enables material editing in 3D scenes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Conditioning diffusion models on a scalar grid of relative attribute strength allows precise control over continuous material properties.
- **Mechanism:** The diffusion network concatenates a constant scalar grid representing the desired attribute strength alongside the latent representation and text prompt. This direct numerical conditioning bypasses the ambiguity of textual descriptions and provides fine-grained control.
- **Core assumption:** A diffusion model's learned generative prior can be extended to interpret continuous numerical inputs for material properties.
- **Evidence anchors:**
  - [abstract] "employing a scalar value and instructions to alter low-level material properties"
  - [section] "we also concatenate a constant scalar grid of edit strength s"
- **Break condition:** If the model's learned prior does not generalize to numerical material attributes or if the scalar conditioning is not effectively integrated into the denoising process.

### Mechanism 2
- **Claim:** Fine-tuning a pre-trained text-to-image diffusion model on a synthetic dataset with controlled material attributes enables generalization to real-world images.
- **Mechanism:** The model is initialized with weights from InstructPix2Pix, providing an image editing prior. It is then fine-tuned on a synthetic dataset where material properties (roughness, metallic, albedo, transparency) are precisely controlled, allowing the model to learn the relationship between attribute changes and visual appearance.
- **Core assumption:** The generative prior learned from text-to-image tasks can be effectively adapted to material property editing with fine-tuning on a synthetic dataset.
- **Evidence anchors:**
  - [abstract] "Fine-tuning a modified pre-trained text-to-image model on this synthetic dataset enables us to edit material properties in real-world images"
  - [section] "We initialize the weights of our denoising network with the pre-trained checkpoint of InstructPix2Pix"
- **Break condition:** If the synthetic dataset does not adequately represent real-world material variations or if the fine-tuning process does not effectively transfer the learned prior.

### Mechanism 3
- **Claim:** Using a Principled BSDF shader node in the synthetic dataset generation ensures that the rendered images accurately represent the physical behavior of materials under varying attribute strengths.
- **Mechanism:** The synthetic dataset is generated using Blender's Cycles renderer with Principled BSDF shader nodes. This allows precise control over material properties and ensures that the rendered images accurately reflect the physical behavior of materials under different attribute strengths.
- **Core assumption:** The Principled BSDF shader node accurately models the physical behavior of materials across the range of attribute strengths used in the synthetic dataset.
- **Evidence anchors:**
  - [section] "Each of these is paired with five randomly chosen materials of the 1200 available from ambientcg.com, and illuminated with one of 400 environment maps"
  - [section] "The material is a Principled BRDF shader node, the base shader in Blender"
- **Break condition:** If the Principled BSDF shader node does not accurately model the physical behavior of materials or if the synthetic dataset does not adequately cover the range of real-world material variations.

## Foundational Learning

- **Concept:** Diffusion models and denoising process
  - **Why needed here:** Understanding the denoising process is crucial for grasping how the model generates images and how conditioning inputs affect the output.
  - **Quick check question:** What is the role of the denoising network in a diffusion model, and how does it relate to the generation of images?

- **Concept:** Text-to-image generation and conditioning
  - **Why needed here:** The model is based on a text-to-image diffusion model, so understanding how text conditioning works is essential for grasping the overall approach.
  - **Quick check question:** How does text conditioning work in text-to-image diffusion models, and what are the limitations of using text for fine-grained control?

- **Concept:** Physically-based rendering and material properties
  - **Why needed here:** The synthetic dataset is generated using physically-based rendering, so understanding material properties like roughness, metallic, albedo, and transparency is crucial for grasping the dataset generation process.
  - **Quick check question:** What are the key material properties that affect the appearance of objects in physically-based rendering, and how do they interact?

## Architecture Onboarding

- **Component map:** Input image -> Variational encoder (E) -> Latent representation -> Denoising network (ϵθ) -> Denoised latent -> Variational decoder (D) -> Output image

- **Critical path:**
  1. Encode input image into latent space using E
  2. Concatenate latent representation with scalar grid and text conditioning
  3. Denoise the latent representation using ϵθ
  4. Decode the denoised latent representation into an output image using D

- **Design tradeoffs:**
  - Using a synthetic dataset allows for precise control over material properties but may not fully represent real-world variations
  - Fine-tuning a pre-trained model provides a strong starting point but may limit the model's ability to learn new material behaviors
  - Direct numerical conditioning bypasses the ambiguity of text but requires careful integration into the denoising process

- **Failure signatures:**
  - Poor generalization to real-world images despite training on a synthetic dataset
  - Inability to achieve smooth transitions between material attribute strengths
  - Exaggerated or unrealistic material edits in the output images

- **First 3 experiments:**
  1. Train the model on a subset of the synthetic dataset and evaluate its ability to edit material properties in held-out synthetic images
  2. Test the model's generalization to real-world images by editing material properties in a diverse set of real images
  3. Evaluate the smoothness of material attribute transitions by linearly varying the scalar grid input and observing the output image sequence

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions, but raises implicit questions about the scalability of the approach to more complex material properties and lighting conditions.

## Limitations
- The synthetic dataset, despite diversity, may not fully capture the complexity of real-world materials and lighting conditions
- Lack of quantitative metrics for material editing quality, relying primarily on user studies
- Limited evaluation and detail provided for the NeRF extension to 3D scenes

## Confidence

- **High confidence** in the core mechanism of conditioning diffusion models with scalar grids for continuous attribute control
- **Medium confidence** in the generalization capability to real images
- **Low confidence** in the scalability and practical utility of the NeRF extension

## Next Checks

1. **Generalization Benchmark**: Create a benchmark dataset of real images with known material properties to quantitatively evaluate the model's ability to accurately edit and preserve material properties across diverse scenarios.

2. **Attribute Control Analysis**: Systematically vary the scalar grid values for each material attribute across the full range and measure the corresponding changes in material appearance using image-based metrics.

3. **Ablation Study**: Conduct an ablation study comparing the proposed method with variations including text-only conditioning, fixed scalar values instead of grids, and different initialization strategies.