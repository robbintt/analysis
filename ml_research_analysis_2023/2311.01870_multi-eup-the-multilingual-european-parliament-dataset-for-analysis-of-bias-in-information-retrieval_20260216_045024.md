---
ver: rpa2
title: 'Multi-EuP: The Multilingual European Parliament Dataset for Analysis of Bias
  in Information Retrieval'
arxiv_id: '2311.01870'
source_url: https://arxiv.org/abs/2311.01870
tags:
- language
- languages
- retrieval
- multilingual
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Multi-EuP, a new multilingual dataset designed
  to study bias in information retrieval. The dataset contains 22K documents spanning
  24 languages, collected from the European Parliament, and includes topics translated
  into all languages, as well as cross-lingual relevance judgments.
---

# Multi-EuP: The Multilingual European Parliament Dataset for Analysis of Bias in Information Retrieval

## Quick Facts
- arXiv ID: 2311.01870
- Source URL: https://arxiv.org/abs/2311.01870
- Reference count: 8
- Primary result: Introduces Multi-EuP dataset with 22K documents in 24 languages for studying bias in multilingual IR

## Executive Summary
This paper introduces Multi-EuP, a new multilingual dataset derived from European Parliament proceedings designed specifically for analyzing bias in information retrieval systems. The dataset contains 22K documents spanning 24 languages, with topics translated into all languages and cross-lingual relevance judgments. The authors demonstrate that language bias exists in multilingual IR by showing that document count per language correlates with retrieval performance, and that language-specific tokenizers introduce additional bias compared to whitespace tokenization. The dataset also includes rich demographic metadata enabling future studies of demographic bias in IR.

## Method Summary
The authors constructed Multi-EuP by collecting European Parliament debate transcripts and metadata from the past three years, creating a dataset of 22K documents in 24 languages. They translated 50 topics into all languages and obtained cross-lingual relevance judgments. Experiments were conducted using BM25 via Pyserini with default parameters (k1=0.9, b=0.4), comparing performance across languages and tokenization strategies (language-specific vs. whitespace). The dataset and code are publicly available on GitHub for reproducibility.

## Key Results
- Document count per language correlates with MRR, showing that languages with more documents achieve better retrieval performance
- Language-specific tokenizers introduce bias compared to whitespace tokenization, with MRR declining modestly but language bias diminishing
- Polish performance is notably lower due to its highly-inflected morphology, demonstrating language-specific challenges for BM25
- The dataset enables both monolingual and multilingual IR evaluation with cross-lingual relevance judgments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual retrieval performance is influenced by the number of documents in each language due to vocabulary overlap and retrieval model bias.
- Mechanism: BM25 retrieval relies on term frequency and document frequency. Languages with more documents have richer vocabulary representations, leading to higher relevance scores. This creates a bias where languages with more documents are favored in ranking.
- Core assumption: The distribution of documents across languages directly impacts the effectiveness of term frequency-based retrieval models like BM25.
- Evidence anchors:
  - [section]: "Initially, we hypothesized that the disparity for each language may be a contributing factor to this bias. Figure 3 presents the regression line between the number of documents in a given language and MRR, which explains much of the variation across languages."
  - [corpus]: "Weak - While the corpus provides document counts per language, it doesn't explicitly test or prove that document count directly causes performance differences. The correlation shown is observational, not causal."
- Break condition: If document counts are normalized or if a retrieval model less sensitive to document frequency is used (e.g., neural dense retrievers).

### Mechanism 2
- Claim: Language-specific tokenizers introduce bias by affecting term matching and normalization differently across languages.
- Mechanism: Language-specific tokenizers apply language-specific rules for segmentation, stemming, and normalization. These rules can create discrepancies in how terms are matched across languages, leading to bias in retrieval results. Whitespace tokenization, being language-agnostic, reduces this bias.
- Core assumption: The choice of tokenizer affects term matching and normalization in ways that systematically favor certain languages over others.
- Evidence anchors:
  - [section]: "Secondly, we speculated that the choice of language-specific Analyzer in LUCENE might be a contributing factor, as it influences word tokenization, token filter, synonym expansion and other processing."
  - [section]: "Furthermore, when transitioning from language-specific tokenizers to whitespace tokenizers, the overall MRR across all languages declined modestly, from 15.02 to 14.18. That is, the original performance level was largely preserved, but language bias was diminished in using simple whitespace tokenization."
- Break condition: If a tokenizer is used that applies the same rules consistently across all languages, or if the retrieval model is robust to tokenization differences.

### Mechanism 3
- Claim: Morphological complexity of a language affects BM25 performance, making some languages "BM25 unfriendly."
- Mechanism: Languages with complex morphology (e.g., Polish) have many inflected word forms per lexeme. BM25, which relies on exact term matching, struggles with this variability, leading to lower performance. Simpler languages with less morphological variation are "BM25 friendly."
- Core assumption: BM25's reliance on exact term matching makes it sensitive to morphological complexity.
- Evidence anchors:
  - [section]: "According to Woitasik et al. (2023), the main reason for the low performance of Polish lies in its highly-inflected morphology, giving rise to a multitude of word forms per lexeme, including inflections of proper names, and complex morphological structure."
- Break condition: If a retrieval model that handles morphological variation well is used (e.g., neural models with subword tokenization).

## Foundational Learning

- Concept: Term Frequency-Inverse Document Frequency (TF-IDF) and its application in BM25.
  - Why needed here: Understanding how BM25 calculates relevance scores based on term frequency and document frequency is crucial for interpreting the experimental results and the impact of language bias.
  - Quick check question: How does BM25 use term frequency and document frequency to rank documents, and why might this be problematic for languages with different morphological structures?

- Concept: Cross-lingual information retrieval and the challenges of matching queries and documents in different languages.
  - Why needed here: The dataset and experiments involve retrieving documents in multiple languages using queries in different languages, highlighting the complexities of cross-lingual retrieval and the potential for language bias.
  - Quick check question: What are the main challenges in cross-lingual information retrieval, and how can language bias manifest in such systems?

- Concept: Tokenization and its impact on text processing and retrieval.
  - Why needed here: The experiments explore the effect of different tokenization strategies on retrieval performance, emphasizing the importance of tokenization in text processing and its potential to introduce bias.
  - Quick check question: How does tokenization affect text processing and retrieval, and why might different tokenization strategies lead to different results for different languages?

## Architecture Onboarding

- Component map: Data Collection -> Preprocessing -> Retrieval Model (BM25) -> Evaluation (MRR@100) -> Analysis (Bias Detection)

- Critical path:
  1. Collect and preprocess European Parliament data
  2. Index documents using different tokenization strategies
  3. Perform retrieval using BM25 with queries in different languages
  4. Evaluate retrieval performance using MRR@100
  5. Analyze results for language bias and other factors

- Design tradeoffs:
  - Tokenization: Language-specific tokenizers may improve performance for individual languages but introduce bias in multilingual settings. Whitespace tokenization reduces bias but may sacrifice some performance.
  - Retrieval Model: BM25 is simple and interpretable but may struggle with morphological complexity. Neural models may be more robust but are more complex and require more data.

- Failure signatures:
  - High variance in MRR across languages suggests language bias
  - Low MRR for morphologically complex languages indicates BM25 may not be suitable
  - Poor performance with whitespace tokenization suggests tokenization is crucial for the dataset

- First 3 experiments:
  1. Compare MRR for BM25 with language-specific tokenizers vs. whitespace tokenization across all languages
  2. Analyze the correlation between document count per language and MRR to quantify the impact of corpus size
  3. Test a neural retrieval model (e.g., mDPR) on the dataset to assess its robustness to language bias and morphological complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of BM25 on Multi-EuP compare to more advanced neural ranking models like mDPR, mColBERT, and PLAID-X when evaluating language bias?
- Basis in paper: [explicit] The authors explicitly state their intention to expand their investigation of language bias to include neural methods such as mDPR, mColBERT, and PLAID-X in future work.
- Why unresolved: The paper only presents results using BM25 and does not compare its performance to neural ranking models.
- What evidence would resolve it: Conducting experiments using mDPR, mColBERT, and PLAID-X on Multi-EuP and comparing their results to BM25 in terms of MRR and language bias would provide evidence to answer this question.

### Open Question 2
- Question: What is the impact of using a language-specific tokenizer versus a whitespace tokenizer on the performance of neural ranking models like mDPR, mColBERT, and PLAID-X?
- Basis in paper: [explicit] The authors mention that they conducted experiments with different tokenization strategies for BM25 and observed a reduction in language bias when using whitespace tokenization.
- Why unresolved: The paper only presents results for BM25 and does not investigate the impact of tokenization on neural ranking models.
- What evidence would resolve it: Conducting experiments using mDPR, mColBERT, and PLAID-X with different tokenization strategies and comparing their results would provide evidence to answer this question.

### Open Question 3
- Question: How does the temporal coverage of the Multi-EuP dataset affect the performance and language bias of information retrieval models?
- Basis in paper: [explicit] The authors mention that the temporal coverage of the dataset is limited to the past three years and suggest that combining it with the Europarl collection could provide a more comprehensive dataset.
- Why unresolved: The paper does not explore the impact of temporal coverage on the performance and language bias of information retrieval models.
- What evidence would resolve it: Expanding the Multi-EuP dataset to include a longer time span and conducting experiments to compare the performance and language bias of models trained on different temporal subsets would provide evidence to answer this question.

## Limitations

- Dataset bias from exclusive focus on European Parliament proceedings, limiting generalizability to other domains
- Significant imbalance in document counts across languages (ranging from 49 to 2547 documents), which correlates with retrieval performance
- Binary relevance judgments used for evaluation may oversimplify relevance assessment compared to graded judgments

## Confidence

**High Confidence**: The dataset construction methodology and its multilingual coverage are well-documented and verifiable. The correlation between document counts and MRR across languages is clearly demonstrated with statistical support. The observation that language-specific tokenizers introduce bias compared to whitespace tokenization is empirically supported by the reported MRR changes.

**Medium Confidence**: The hypothesis that morphological complexity affects BM25 performance for certain languages is supported by the Polish language example, but would benefit from broader linguistic analysis across more morphologically diverse languages. The claim that corpus size drives performance differences is demonstrated through correlation but lacks causal proof through controlled experiments.

**Low Confidence**: The preliminary demographic bias analysis lacks sufficient depth to draw strong conclusions. The speculation about Lucene's Analyzer contributing to bias is based on observation rather than systematic investigation of the specific components causing the bias.

## Next Checks

1. **Controlled Corpus Size Experiment**: Re-run the retrieval experiments with balanced document counts across languages by subsampling or oversampling to determine if the correlation between document count and MRR is causal rather than merely correlational.

2. **Tokenizer Component Analysis**: Systematically isolate and test individual components of the language-specific tokenizers (stemming, stopword removal, synonym expansion) to identify which specific processing steps contribute most to language bias.

3. **Cross-Domain Generalization Test**: Evaluate the dataset and retrieval models on a non-parliamentary multilingual corpus to assess whether the observed biases and performance patterns generalize beyond the European Parliament domain.