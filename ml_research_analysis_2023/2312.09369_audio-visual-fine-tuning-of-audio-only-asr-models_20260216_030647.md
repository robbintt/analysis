---
ver: rpa2
title: Audio-visual fine-tuning of audio-only ASR models
arxiv_id: '2312.09369'
source_url: https://arxiv.org/abs/2312.09369
tags:
- audio-only
- speech
- learning
- audio-visual
- pre-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an efficient approach for audio-visual automatic
  speech recognition (AV-ASR) by leveraging audio-only self-supervised learning (SSL)
  pre-training followed by audio-visual supervised fine-tuning. The method, called
  FAVA, uses the BEST-RQ audio-only SSL technique for pre-training and then performs
  audio-visual supervised learning with an early-fusion architecture.
---

# Audio-visual fine-tuning of audio-only ASR models

## Quick Facts
- arXiv ID: 2312.09369
- Source URL: https://arxiv.org/abs/2312.09369
- Reference count: 0
- Key outcome: FAVA achieves SOTA AV-ASR performance with 12-30x faster pre-training by fine-tuning audio-only pre-trained models

## Executive Summary
This paper proposes FAVA (Fine-tuning Audio-visual ASR), an efficient approach for audio-visual automatic speech recognition that leverages audio-only self-supervised learning (SSL) pre-training followed by audio-visual supervised fine-tuning. The method uses BEST-RQ audio-only SSL for pre-training and then performs audio-visual supervised learning with an early-fusion architecture. FAVA achieves performance close to state-of-the-art AV-SSL methods while being significantly faster to pre-train. Additionally, it can convert state-of-the-art audio-only ASR models into AV models, improving recognition on noisy audio while maintaining strong performance on clean audio.

## Method Summary
FAVA employs a two-stage training approach: first pre-training an audio-only model using BEST-RQ SSL on VoxCeleb2, then fine-tuning with audio-visual supervised learning on LRS3-TED using modality dropout and early fusion. The model architecture consists of separate audio and video front-ends (CNNs), a 17-layer Conformer encoder, and an RNN-T decoder. During fine-tuning, audio and video features are fused early by summing their encoder outputs, and modality dropout is applied to prevent over-reliance on any single modality.

## Key Results
- FAVA achieves 0.5% absolute WER gap from SOTA AV-SSL methods on clean LRS3-TED
- Audio-only fine-tuning yields 41% relative WER reduction compared to trained-from-scratch AV models on clean audio
- AV fine-tuning improves noisy audio performance by 31% relative WER reduction while maintaining clean audio performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training with audio-only SSL followed by audio-visual fine-tuning can achieve SOTA AV-SSL performance.
- Mechanism: BEST-RQ audio-only SSL pre-training learns robust speech representations that transfer well to audio-visual domain when fine-tuned with modality dropout and supervised AV data.
- Core assumption: Learned audio representations are general enough to benefit from visual information during fine-tuning without requiring AV pre-training.
- Evidence anchors: Abstract states FAVA is within 0.5% WER of SOTA; BEST-RQ achieves SOTA on LibriSpeech; corpus neighbors focus on AV models but don't directly address transfer.

### Mechanism 2
- Claim: Early-fusion architecture with modality dropout prevents overfitting to single modalities during fine-tuning.
- Mechanism: Randomly zeroing out audio or video features during training teaches the model to rely on both modalities and become robust to missing/noisy inputs.
- Core assumption: Modality dropout encourages learning complementary representations from both audio and visual inputs.
- Evidence anchors: Paper cites AV-HuBERT and u-HuBERT using modality dropout; FAVA shows 41% and 22% relative WER reduction on clean and noisy audio; corpus neighbors discuss AV models but not dropout strategies.

### Mechanism 3
- Claim: Converting pre-trained audio-only ASR models to AV models preserves strong clean audio performance while improving noisy audio recognition.
- Mechanism: Fine-tuning state-of-the-art audio-only ASR models with AV data leverages visual cues for noisy speech while maintaining clean speech recognition capabilities.
- Core assumption: Pre-trained audio-only model has learned general speech representations that can be enhanced with visual information without losing clean speech recognition.
- Evidence anchors: Paper shows AV fine-tuning achieves 6.2% WER vs 9.2% for audio-only on noisy test set (31% reduction); abstract states no AV data used during pre-training; corpus neighbors focus on AV models but not converting audio-only models.

## Foundational Learning

- Concept: Self-supervised learning (SSL) for speech recognition
  - Why needed here: Understanding how SSL methods like BEST-RQ pre-train models on unlabeled data is crucial for grasping the pre-training stage of FAVA
  - Quick check question: How does BEST-RQ differ from other SSL methods like HuBERT in terms of target generation and computational efficiency?

- Concept: Audio-visual speech recognition (AV-SR)
  - Why needed here: Knowledge of AV-SR architectures and challenges (e.g., need for transcribed AV data) is essential for understanding the problem FAVA addresses
  - Quick check question: What are the main advantages of using visual information in speech recognition, and what are the challenges in training AV-SR models?

- Concept: Modality dropout and early fusion
  - Why needed here: Understanding these techniques is crucial for grasping how FAVA integrates audio and visual information during fine-tuning
  - Quick check question: How does modality dropout during training improve the model's robustness to missing or degraded modalities at inference time?

## Architecture Onboarding

- Component map: Audio front-end (2-layer CNN) -> Video front-end (5-layer 2+1D CNN) -> Encoder (17-layer Conformer) -> Decoder (RNN-T) -> Output
- Critical path: Audio/visual front-ends → Encoder → Decoder → Output
- Design tradeoffs:
  - Early fusion vs. late fusion: Early fusion allows joint representation learning but may be more sensitive to modality-specific noise
  - Modality dropout rate: Higher rates increase robustness but may slow convergence
  - Pre-training data size: Larger audio-only datasets improve transfer but increase computational cost
- Failure signatures:
  - Poor performance on clean audio: Likely due to overfitting to visual modality or degradation during fine-tuning
  - Limited improvement on noisy audio: Could indicate insufficient visual information integration or ineffective modality dropout
  - Slow convergence during fine-tuning: May suggest inappropriate learning rate or initialization
- First 3 experiments:
  1. Train audio-only BEST-RQ model on Librispeech/LibriLight and evaluate on clean speech to establish baseline
  2. Fine-tune BEST-RQ model with modality dropout on LRS3-TED and evaluate on clean and noisy speech
  3. Compare FAVA with trained-from-scratch AV model on LRS3-TED to validate performance gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the specific source of the 0.5% absolute WER gap between FAVA and state-of-the-art AV-SSL methods, and can this gap be further reduced?
- Basis in paper: The paper states that FAVA achieves performance within 0.5% WER (absolute) of state-of-the-art AV-SSL methods on clean audio, but the exact source of this performance gap is unclear due to differences in noisy test set construction and potential evaluation discrepancies.
- Why unresolved: The paper notes that 12% of LRS3-TED test utterances have been deleted from YouTube since the dataset was created, and the noisy test set construction differs between studies (0 dB babble noise vs. mixing 30 random utterances). These factors make it difficult to pinpoint the exact source of the performance gap.
- What evidence would resolve it: A controlled experiment using identical noisy test set construction and complete test data, along with ablation studies isolating the contributions of each component of FAVA, would help identify the specific sources of the performance gap and potential areas for improvement.

### Open Question 2
- Question: How does the performance of FAVA compare to state-of-the-art AV-SSL methods on languages other than English, and what factors influence its generalization across languages?
- Basis in paper: The paper focuses on English language experiments using the LRS3-TED dataset and the Google USM model, which is multilingual but primarily evaluated on English. The generalization of FAVA to other languages is not explicitly discussed.
- Why unresolved: The paper does not provide results or analysis of FAVA's performance on languages other than English, nor does it discuss the factors that might influence its generalization across languages, such as the characteristics of the pre-training data or the model architecture.
- What evidence would resolve it: Conducting experiments with FAVA on multilingual datasets and comparing its performance to state-of-the-art AV-SSL methods across different languages would provide insights into its generalization capabilities and the factors influencing its performance.

### Open Question 3
- Question: How does the performance of FAVA change when using different pre-training methods or architectures, such as those based on contrastive learning or different encoder types?
- Basis in paper: The paper uses BEST-RQ as the pre-training method for FAVA and a Conformer encoder, but it does not explore the impact of using alternative pre-training methods or encoder architectures on the model's performance.
- Why unresolved: The paper focuses on demonstrating the effectiveness of FAVA with a specific combination of pre-training method and encoder architecture, but it does not investigate how the performance might change with different choices, which could provide insights into the relative importance of these components.
- What evidence would resolve it: Conducting experiments with FAVA using different pre-training methods (e.g., contrastive learning-based methods) and encoder architectures (e.g., Transformers, RNNs) would help determine the impact of these choices on the model's performance and provide guidance for future research directions.

## Limitations

- Transfer learning hypothesis uncertainty: The quality and domain alignment of BEST-RQ pre-trained representations for AV tasks remains untested beyond reported results
- Early-fusion architecture assumption: The design choice of simple summation for AV integration lacks comparison to alternative fusion strategies
- Generalization uncertainty: Results only validated on LRS3-TED, with unknown performance on other AV datasets or with different pre-training approaches

## Confidence

- **High confidence**: Efficiency claims (12-30x faster pre-training) and clean audio performance matching SOTA are well-supported and reliably measured
- **Medium confidence**: Noisy audio improvement claims have reasonable support but face potential variability due to unspecified noise conditions
- **Low confidence**: Generalization to other AV datasets and robustness to different pre-training approaches remains uncertain

## Next Checks

1. Cross-dataset validation: Test FAVA on another AV dataset (e.g., LRS2, VoxCeleb) to verify the 0.5% WER gap claim holds beyond LRS3-TED

2. Fusion architecture ablation: Compare early fusion against late fusion and attention-based fusion variants on the same pre-training/fine-tuning pipeline to quantify the impact of the fusion choice

3. Pre-training sensitivity analysis: Replace BEST-RQ with another audio SSL method (e.g., HuBERT) while keeping all other components constant to isolate the contribution of the specific pre-training approach