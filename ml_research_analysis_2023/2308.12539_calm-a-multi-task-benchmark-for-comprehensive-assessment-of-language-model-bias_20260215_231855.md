---
ver: rpa2
title: 'CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model
  Bias'
arxiv_id: '2308.12539'
source_url: https://arxiv.org/abs/2308.12539
tags:
- bias
- dataset
- language
- templates
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of reliably measuring sociodemographic\
  \ bias in large language models (LLMs) by introducing the Comprehensive Assessment\
  \ of Language Model bias (CALM) benchmark. CALM integrates 16 datasets across three\
  \ NLP tasks\u2014question answering, sentiment analysis, and natural language inference\u2014\
  into 224 diverse templates, generating 78,400 examples using 50 names each for seven\
  \ demographic groups."
---

# CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias

## Quick Facts
- arXiv ID: 2308.12539
- Source URL: https://arxiv.org/abs/2308.12539
- Authors: 
- Reference count: 40
- Key outcome: Introduces CALM, a multi-task benchmark integrating 16 datasets into 224 templates with 50 names per 7 demographic groups, showing robust and sensitive bias measurement across 20 LLMs.

## Executive Summary
CALM addresses the challenge of reliably measuring sociodemographic bias in large language models (LLMs) by creating a comprehensive, multi-task benchmark. It integrates datasets from question answering, sentiment analysis, and natural language inference to generate 78,400 diverse prompts. The benchmark demonstrates greater robustness to template perturbations and captures broader linguistic variation than prior bias benchmarks. Evaluation on 20 LLMs reveals nuanced trends in model bias related to size and task, highlighting CALM's utility for comparative bias assessment.

## Method Summary
CALM constructs a bias benchmark by filtering 224 diverse templates from 16 existing NLP datasets across QA, SA, and NLI tasks. Each template is populated with 50 highly frequent names from seven demographic groups (male, female, non-binary, White, Black, Hispanic, Asian) to generate 78,400 prompts. Bias is quantified as the accuracy differential across demographic groups for each template, aggregated per model. The method emphasizes template diversity (high length variance, low semantic similarity) to reduce sensitivity to perturbations and improve measurement reliability.

## Key Results
- CALM bias scores are more robust to template perturbations (e.g., synonym substitution) than prior benchmarks, with differences under 10% across tested models.
- Larger parameter models in some series (OPT, Bloom) show increased bias, while T0 models are among the least biased.
- CALM exhibits higher template diversity (BERTScore variance, length variation) and lower semantic similarity than prior bias benchmarks.
- Task-specific tradeoffs in bias are observed, with no universal trend across model families or tasks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CALM reduces sensitivity to perturbations in bias measurement templates.
- Mechanism: By integrating 16 diverse datasets and filtering 224 templates with high template length variation and low semantic similarity, the benchmark avoids over-reliance on any single template structure.
- Core assumption: Template diversity in length, vocabulary, and domain reduces the impact of small modifications like synonym substitution.
- Evidence anchors:
  - [abstract] "Our empirical evaluation shows that CALM bias scores are more robust and far less sensitive than previous bias measurements to perturbations in the templates, such as synonym substitution, or to random subset selection of templates."
  - [section] "We show that our dataset exhibits higher diversity than prior bias benchmark datasets... the CALM bias metric remains relatively stable, with a maximum difference of less than 10% across all the tested models."
  - [corpus] Found 25 related papers; average FMR 0.318, indicating moderate relatedness but no direct evidence about perturbation robustness.
- Break condition: If template filtering criteria are relaxed and high semantic similarity templates are included, perturbation sensitivity will increase.

### Mechanism 2
- Claim: CALM captures a broader range of linguistic variation, improving bias measurement reliability.
- Mechanism: Drawing from multiple NLP tasks (QA, SA, NLI) and datasets (bAbI, SST, SNLI, etc.) increases the breadth of language contexts tested.
- Core assumption: Different tasks and domains expose different linguistic patterns, making bias more likely to surface across varied contexts.
- Evidence anchors:
  - [abstract] "We integrate sixteen datasets across different domains, such as Wikipedia and news articles, to filter 224 templates..."
  - [section] "We use 16 popular datasets across these three tasks to filter 224 templates... Notably, our approach emphasizes the selection of a diverse set of templates..."
  - [corpus] No direct corpus evidence about task diversity improving bias measurement; this is assumed from multi-task benchmark theory.
- Break condition: If the templates from different tasks are not truly diverse or if tasks are too similar, the breadth advantage diminishes.

### Mechanism 3
- Claim: Using 50 names per demographic group provides statistical power to detect bias differences.
- Mechanism: Large name sets reduce variance in group performance estimates, making differences in accuracy across groups more reliable.
- Core assumption: The sampled names are representative of the demographic groups and usage frequencies.
- Evidence anchors:
  - [abstract] "We assemble 50 highly frequent person names for each of seven distinct demographic groups to generate 78,400 prompts..."
  - [section] "To quantify gender bias, names were sampled from three gender categories - male, female, and non-binary - with 50 names per category... We selected these four groups based on the availability of corresponding labels in US census data..."
  - [corpus] No corpus evidence about name sampling adequacy; assumed from demographic dataset availability.
- Break condition: If name frequencies are skewed or not representative, the statistical power and validity of bias detection decrease.

## Foundational Learning

- Concept: Template diversity metrics (BERTScore, length variance)
  - Why needed here: Ensures bias measurement is not sensitive to specific phrasing and captures broader linguistic patterns.
  - Quick check question: If two templates have high BERTScore similarity, what does that imply about their diversity contribution?

- Concept: Bias scoring via accuracy differentials
  - Why needed here: Provides a simple, interpretable measure of performance gaps between demographic groups.
  - Quick check question: If a model has 90% accuracy for males and 80% for females, what is the gender bias score?

- Concept: Multi-task evaluation framework
  - Why needed here: Exposes model bias across different language understanding capabilities, reducing task-specific blind spots.
  - Quick check question: Why might a model show low bias in sentiment analysis but high bias in question answering?

## Architecture Onboarding

- Component map: Template filtering pipeline -> Name substitution engine -> Prompt formatting -> Model inference -> Accuracy aggregation -> Bias score calculation.
- Critical path: Template creation -> Dataset generation -> Model evaluation -> Bias score computation -> Analysis.
- Design tradeoffs: Larger name sets improve statistical power but increase computational cost; diverse tasks improve coverage but complicate prompt standardization.
- Failure signatures: High variance in bias scores across template subsets indicates insufficient diversity; consistent low accuracy suggests task mismatch.
- First 3 experiments:
  1. Run CALM on a small model with a single template subset to verify prompt formatting and answer matching.
  2. Evaluate perturbation robustness by modifying 10% of templates and comparing bias scores.
  3. Test scalability by generating prompts for all 224 templates and measuring inference time per task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does increasing model size consistently decrease bias across all model families and tasks?
- Basis in paper: [inferred] The paper notes that for some model families like OPT and Bloom, larger models are more biased, while T0 models show lower bias. However, there are task-specific tradeoffs observed.
- Why unresolved: The relationship between model size and bias appears complex and varies by model family and task, suggesting no universal trend exists.
- What evidence would resolve it: A comprehensive study varying model size systematically across all model families and tasks, measuring bias consistently, would clarify if any general pattern emerges or if the relationship is truly model-dependent.

### Open Question 2
- Question: How robust are CALM bias scores to different template selection strategies and prompt engineering?
- Basis in paper: [explicit] The paper notes that CALM bias scores are more stable to template perturbations than prior benchmarks, but acknowledges the need for standardized prompts across models for better comparability.
- Why unresolved: The current analysis uses a fixed template set and common prompts, but the impact of alternative template selection methods or prompt variations on bias measurement reliability is unexplored.
- What evidence would resolve it: Experiments testing bias scores with different template sampling strategies, prompt variations, and prompt engineering techniques across models would determine the sensitivity of CALM to these factors.

### Open Question 3
- Question: How do bias trends generalize beyond the seven demographic groups studied in CALM?
- Basis in paper: [explicit] The paper acknowledges that CALM's current focus on seven US demographic groups is a limitation and provides scripts for broader application, but does not explore bias beyond these groups.
- Why unresolved: The analysis is confined to US names and seven groups, leaving open how bias manifests across other nationalities, cultures, and social categories.
- What evidence would resolve it: Applying CALM to diverse global name datasets and social groups, then comparing bias trends, would reveal the generalizability of current findings and identify any new bias patterns.

## Limitations
- The exact template filtering criteria are underspecified, making it difficult to independently verify diversity requirements.
- Name representativeness for the 50 names per demographic group is assumed but not empirically validated, potentially affecting bias measurement validity.
- The bias scoring mechanism conflates measurement artifacts with genuine model bias in cases of ambiguous prompts or domain mismatch.

## Confidence
- **High confidence**: CALM successfully integrates multiple datasets and tasks into a single benchmark, as evidenced by the systematic description and reproducible methodology.
- **Medium confidence**: CALM reduces sensitivity to template perturbations and captures broader linguistic variation, based on reported experimental results but with limited independent verification.
- **Medium confidence**: Using 50 names per demographic group provides adequate statistical power, though this is theoretically sound but not empirically validated in the paper.

## Next Checks
1. **Template diversity validation**: Independently verify that the 224 selected templates exhibit high BERTScore variance and low semantic similarity by calculating these metrics on a random subset.
2. **Perturbation robustness replication**: Modify 10-20% of CALM templates with synonym substitutions and compare bias scores across models to confirm stability remains within 10% as reported.
3. **Name representativeness audit**: Cross-validate the 50 names per demographic group against external demographic name frequency datasets to ensure they are representative and not skewed toward particular subgroups.