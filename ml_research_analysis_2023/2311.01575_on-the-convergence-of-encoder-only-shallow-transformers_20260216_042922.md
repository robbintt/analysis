---
ver: rpa2
title: On the Convergence of Encoder-only Shallow Transformers
arxiv_id: '2311.01575'
source_url: https://arxiv.org/abs/2311.01575
tags:
- convergence
- initialization
- training
- transformer
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes the global convergence theory of shallow
  Transformer under a realistic setting from the perspective of architectures, initialization,
  and scaling. The paper provides theoretical insights on the training dynamics of
  Transformer and the importance of different scaling schemes and initialization.
---

# On the Convergence of Encoder-only Shallow Transformers

## Quick Facts
- arXiv ID: 2311.01575
- Source URL: https://arxiv.org/abs/2311.01575
- Reference count: 40
- Primary result: Establishes global convergence theory for shallow Transformers under quadratic overparameterization and LeCun/He initialization

## Executive Summary
This paper provides the first global convergence theory for encoder-only shallow Transformers under realistic training settings. The authors prove that quadratic overparameterization (dm = Ω(N²)) combined with LeCun/He initialization and τ₀ = d⁻¹/² scaling is sufficient for global convergence. The key technical challenge is handling the softmax nonlinearity in self-attention, which the authors address by connecting softmax outputs to the inner product structure XX⊤. The analysis reveals that while infinite-width analysis suggests NTK initialization, finite-width Transformers converge faster with LeCun/He initialization under proper step sizes.

## Method Summary
The paper analyzes encoder-only shallow Transformers trained with gradient descent on squared loss. The theoretical framework establishes conditions for global convergence by examining the minimum singular value of a pre-activation matrix (α₂) and showing it satisfies a Polyak-Lojasiewicz-type inequality. The authors validate these conditions for practical initialization schemes under τ₀ = d⁻¹/² scaling, proving that quadratic overparameterization ensures the required bounds. Experiments are conducted on both synthetic data (sampled from standard Gaussian) and MNIST with ViT embeddings, comparing convergence under different scaling factors and initialization schemes.

## Key Results
- Proves quadratic overparameterization (dm = Ω(N²)) is sufficient for global convergence under LeCun/He initialization
- Shows τ₀ = d⁻¹/² scaling admits faster convergence than τ0 = d⁻¹ scaling for large widths
- Demonstrates that LeCun/He initialization achieves faster convergence than NTK initialization for finite-width Transformers
- Validates theoretical predictions on both synthetic data and MNIST with ViT embeddings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The softmax function in self-attention can be handled analytically by connecting its output to the inner product structure XX⊤.
- Mechanism: The row-wise softmax output βi,n depends on the interaction between input tokens X and the query-key matrices WQ, WK. By bounding the difference between softmax outputs at adjacent training steps using Lemma 7, the analysis decouples the softmax nonlinearity from the overall convergence argument.
- Core assumption: The input data X has full row rank and bounded Frobenius norm.
- Break condition: If the data matrix X does not have full row rank, the lower bound on σmin(Fpre) fails and the Polyak-Lojasiewicz condition cannot be verified.

### Mechanism 2
- Claim: Quadratic overparameterization (dm = Ω(N²)) is sufficient for global convergence under LeCun/He initialization and τ0 = d⁻¹/² scaling.
- Mechanism: The minimum singular value of the pre-activation matrix Fpre is bounded below by a function of dm, N, and the data structure. When dm scales quadratically with N, this bound ensures α2 ≥ Ω(N d³/² Cx/√(dm/d)), satisfying the convergence conditions.
- Core assumption: The activation function σr is ReLU and the data satisfies Assumptions 2 and 3 regarding linear independence and dissimilarity.
- Break condition: If dm scales sub-quadratically with N, the lower bound on α2 fails and the induction step in Proposition 1 breaks.

### Mechanism 3
- Claim: The τ0 = d⁻¹/² scaling admits faster convergence than τ0 = d⁻¹ scaling for large widths due to avoiding the "dimension missing" effect.
- Mechanism: Under τ0 = d⁻¹/² scaling, the softmax input remains non-degenerate even as dm grows, preserving the attention mechanism's ability to capture token interactions. Under τ0 = d⁻¹ scaling, the softmax input tends to zero, making attention degenerate to a pooling operation and losing expressiveness.
- Core assumption: The width dm is sufficiently large to observe the asymptotic behavior.
- Break condition: If dm is small, the difference between the two scalings is negligible and the convergence rates become comparable.

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) and its role in infinite-width analysis
  - Why needed here: The paper contrasts finite-width convergence with NTK-based infinite-width results, showing that LeCun/He initialization achieves faster convergence than NTK initialization.
  - Quick check question: What happens to the softmax output when dm → ∞ under τ0 = d⁻¹ scaling?

- Concept: Polyak-Lojasiewicz (PL) inequality and its variant for non-convex optimization
  - Why needed here: The convergence proof relies on showing ||∇ℓ(θ)||² ≥ 2λmin(K)ℓ(θ), which is a PL-type condition, to guarantee global convergence.
  - Quick check question: How does the minimum singular value of Fpre relate to the PL constant in the proof?

- Concept: Concentration inequalities for random matrices (Matrix Chernoff, sub-exponential bounds)
  - Why needed here: Bounding the spectral norms of weight matrices and the minimum singular values of data-dependent matrices requires concentration results to hold with high probability.
  - Quick check question: What is the probability bound for ||W||₂ ≤ 3√dm when W has i.i.d. N(0,1) entries?

## Architecture Onboarding

- Component map: Data → Self-attention → ReLU → Pooling → Linear → Loss
- Critical path: The softmax in self-attention is the critical nonlinear component requiring special handling
- Design tradeoffs:
  - Width dm vs. convergence speed: Larger dm improves convergence but increases computational cost
  - Scaling factor τ0: τ0 = d⁻¹/² preserves attention expressiveness; τ0 = d⁻¹ simplifies analysis but may degenerate attention
  - Initialization: LeCun/He vs. NTK - affects convergence rate and step size requirements
- Failure signatures:
  - If α2 < required bound: Training stalls or converges slowly
  - If data does not satisfy full row rank: Lower bound on σmin(Fpre) fails
  - If width dm is too small: Overparameterization condition not met
- First 3 experiments:
  1. Train shallow Transformer on synthetic data with varying dm (10, 100, 1000, 4000) and verify linear convergence only for dm ≥ N
  2. Compare convergence under τ0 = d⁻¹/² vs. τ0 = d⁻¹ scaling for large dm to observe dimension missing effect
  3. Test different initializations (LeCun, He, NTK) with same step size to observe convergence rate differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the convergence rate of Transformer change when using other initialization schemes beyond LeCun, He, and NTK?
- Basis in paper: The paper compares convergence rates under LeCun, He, and NTK initialization, showing LeCun and He achieve faster rates. However, it does not analyze other schemes like Xavier or Kaiming initialization.
- Why unresolved: The analysis focuses on a specific set of initialization schemes for simplicity. Exploring other schemes could reveal new insights into the impact of initialization on Transformer training.
- What evidence would resolve it: Convergence analysis and empirical validation for Transformer training using different initialization schemes, measuring convergence speed and final loss.

### Open Question 2
- Question: What is the impact of depth on the convergence properties of Transformer?
- Basis in paper: The paper analyzes a shallow Transformer model. It mentions in the discussion that extending the analysis to deep Transformers is possible but requires more involved derivations.
- Why unresolved: The theoretical analysis becomes more complex with depth, and the interplay between layers affects the convergence dynamics. Understanding how depth influences convergence is crucial for designing efficient Transformer architectures.
- What evidence would resolve it: Convergence analysis and empirical validation for deep Transformers with varying depths, comparing convergence rates and final performance to shallow models.

### Open Question 3
- Question: How does the choice of scaling factor τ0 affect the convergence rate and final performance of Transformer?
- Basis in paper: The paper compares τ0 = d⁻¹/² and τ0 = d⁻¹ scaling, showing τ0 = d⁻¹/² generally achieves faster convergence for large widths. However, it does not explore other scaling factors or their impact on final performance.
- Why unresolved: The scaling factor influences the input to the softmax function, which affects the attention mechanism's behavior. Understanding the optimal scaling factor for different tasks and datasets could lead to improved Transformer training.
- What evidence would resolve it: Convergence analysis and empirical validation for Transformer training with different scaling factors, measuring convergence speed, final loss, and performance on various tasks.

## Limitations

- The theoretical analysis relies heavily on the assumption that the data matrix X has full row rank and satisfies specific linear independence conditions
- The quadratic overparameterization requirement (dm = Ω(N²)) may be overly conservative for practical applications
- The proof depends on precise bounds for singular values of random matrices that may be loose in practice

## Confidence

- **High confidence**: The connection between softmax outputs and inner product structure XX⊤
- **Medium confidence**: Quadratic overparameterization sufficiency
- **Medium confidence**: τ0 = d⁻¹/² scaling benefits

## Next Checks

1. **Empirical validation of data assumptions**: Test the convergence theory on datasets where Assumption 2 (full row rank and linear independence) may be violated, such as MNIST with correlated pixel values or real-world sequential data, to assess the robustness of the theoretical guarantees.

2. **Quantitative analysis of overparameterization requirements**: Systematically vary dm relative to N² across multiple orders of magnitude and measure the actual convergence rate and final loss, comparing against the theoretical Ω(N²) requirement to determine if the bound is tight.

3. **Scaling factor sensitivity analysis**: Conduct controlled experiments comparing τ0 = d⁻¹/² vs τ0 = d⁻¹ scaling on increasingly wide models (dm up to 10⁴ or higher) while monitoring both convergence speed and attention expressiveness metrics (e.g., attention entropy, gradient norms).