---
ver: rpa2
title: Multimodal Pre-training Framework for Sequential Recommendation via Contrastive
  Learning
arxiv_id: '2303.11879'
source_url: https://arxiv.org/abs/2303.11879
tags:
- recommendation
- msm4sr
- multimodal
- item
- sequential
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multimodal pre-training framework for sequential
  recommendation. The key idea is to leverage contrastive learning to capture correlations
  among different modality sequences of users and items, enhancing the fusion and
  utilization of multimodal information.
---

# Multimodal Pre-training Framework for Sequential Recommendation via Contrastive Learning

## Quick Facts
- **arXiv ID**: 2303.11879
- **Source URL**: https://arxiv.org/abs/2303.11879
- **Reference count**: 40
- **Key outcome**: MSM4SR outperforms state-of-the-art approaches in both normal and cold-start settings using multimodal pre-training with contrastive learning

## Executive Summary
This paper introduces MSM4SR, a multimodal pre-training framework for sequential recommendation that leverages contrastive learning to capture correlations among different modality sequences of users and items. The framework consists of three main components: multimodal feature extraction using CLIP tokenization and Sentence-BERT, a backbone network called Multimodal Mixup Sequence Encoder (M2SE) with complementary sequence mixup strategy, and pre-training tasks including modality-specific next item prediction and cross-modality contrastive learning. Extensive experiments on four real-world datasets demonstrate superior performance over state-of-the-art approaches in both normal and cold-start settings, highlighting the efficacy of incorporating multimodal pre-training in sequential recommendation representation learning.

## Method Summary
MSM4SR extracts features from item text descriptions and images using Sentence-BERT, with images first tokenized into textual keywords via CLIP. The backbone network M2SE employs a complementary sequence mixup strategy to fuse different modality sequences, mixing text and image embeddings with probability p ≤ 0.5 to ensure each mix-modality sequence is dominated by its original modality. The framework uses two contrastive learning losses: modality-specific next item prediction (NIP) and cross-modality contrastive learning (CMCL) to capture modality interactions at sequence-to-sequence and sequence-to-item levels. The model is pre-trained using these contrastive objectives, then fine-tuned for sequential recommendation with standard ID embeddings.

## Key Results
- MSM4SR outperforms state-of-the-art sequential recommendation methods on Amazon datasets (Pantry, Arts, Office) in both normal and cold-start settings
- The complementary sequence mixup strategy with p ≤ 0.5 effectively alleviates modality representation discrepancy
- Contrastive learning losses (NIP and CMCL) serve as effective regularizers, improving generalization across modalities

## Why This Works (Mechanism)

### Mechanism 1
- Converting images into textual tokens via CLIP reduces modality representation discrepancy and enables unified semantic processing
- CLIP maps images into semantically similar text tokens, processed by Sentence-BERT for shared semantic space
- Core assumption: Text and image embeddings from Sentence-BERT are semantically comparable
- Evidence: Weak corpus evidence of this specific CLIP-to-text-token approach
- Break condition: If CLIP tokens fail to capture visual semantics or embeddings aren't aligned

### Mechanism 2
- Complementary sequence mixup strategy alleviates modality representation discrepancy by ensuring each mix-modality sequence is dominated by its original modality
- Mixes text and image embeddings with probability p ≤ 0.5, creating dominant-modality sequences
- Core assumption: Dominant modality preservation provides better learning signals than equal mixing
- Evidence: Weak corpus evidence of this specific complementary mixup approach
- Break condition: If p > 0.5 leads to better performance or removing mixup improves results

### Mechanism 3
- Contrastive learning losses capture cross-modality correlations and improve generalization by forcing semantic alignment across modalities
- NIP predicts next items in both text and image feature spaces, CMCL creates symmetric contrastive pairs between text-mapped and image-mapped sequences
- Core assumption: Contrastive framework effectively learns cross-modal correlations from mix-modality representations
- Evidence: Moderate corpus evidence from related contrastive learning papers
- Break condition: If removing either NIP or CMCL losses significantly improves performance

## Foundational Learning

- **Multimodal representation learning**: Combining text and image information into unified representations
  - Why needed: Framework must effectively combine multimodal item information for recommendation
  - Quick check: What are key challenges in aligning text and image embeddings in same semantic space?

- **Contrastive learning and self-supervised learning**: Learning generalized representations without explicit labels
  - Why needed: Pre-training relies on contrastive losses to learn representations without labels
  - Quick check: How do NIP and CMCL losses differ in objectives for learning cross-modal correlations?

- **Sequence modeling with transformers**: Encoding user behavior sequences with multimodal information
  - Why needed: Backbone network uses transformer layers for sequential recommendation
  - Quick check: What is role of positional encodings in transformer layers for sequential recommendation?

## Architecture Onboarding

- **Component map**: Multimodal Feature Extraction (CLIP + Sentence-BERT) → M2SE (Sequence dropout → Encoders → Mixup → Transformers) → Pre-training (NIP + CMCL) → Fine-tuning → Recommendation prediction

- **Critical path**: Feature extraction → M2SE encoding → Pre-training (NIP + CMCL) → Fine-tuning → Recommendation prediction

- **Design tradeoffs**:
  - CLIP-derived tokens vs direct image features: Semantic alignment vs visual detail preservation
  - Complementary mixup ratio (p ≤ 0.5) vs equal mixing: Dominant modality preservation vs full integration
  - Pre-training vs end-to-end training: Better generalization vs task-specific optimization

- **Failure signatures**:
  - Poor cold-start performance suggests feature extraction or pre-training is insufficient
  - Degradation when switching between unimodal and multimodal settings indicates modality alignment issues
  - Overfitting despite pre-training suggests contrastive losses aren't effective regularization

- **First 3 experiments**:
  1. Test unimodal vs multimodal performance to validate benefit of combining text and image information
  2. Remove NIP loss to assess its contribution to learning cross-modal correlations
  3. Vary complementary mixup ratio (p) to find optimal balance between modality preservation and integration

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does choice of pre-trained model (e.g., Sentence-BERT vs other language models) impact MSM4SR performance?
- **Basis**: Paper uses Sentence-BERT but doesn't explore alternatives
- **Why unresolved**: No comparative analysis of different pre-trained models
- **Resolution**: Comparative experiments with various pre-trained models and their impact on performance

### Open Question 2
- **Question**: How does MSM4SR handle multimodal data with varying levels of sparsity and noise across modalities?
- **Basis**: Paper mentions addressing data sparsity but not varying levels of sparsity and noise
- **Why unresolved**: No detailed analysis of framework's coping with different sparsity and noise levels
- **Resolution**: Experiments demonstrating performance under different sparsity and noise levels

### Open Question 3
- **Question**: Can MSM4SR framework be extended to handle additional modalities beyond text and images, such as audio or video?
- **Basis**: Paper focuses on text and image modalities only
- **Why unresolved**: No discussion of potential for extending to other modalities
- **Resolution**: Implementation and evaluation with additional modalities and impact analysis

## Limitations
- Limited ablation studies on alternative mixing strategies and direct comparison with non-tokenized image features
- Cold-start experiments focus primarily on new items rather than new users, leaving gap in complete cold-start evaluation
- Claims about CLIP-generated text tokens providing superior semantic alignment lack systematic empirical validation

## Confidence
- **High confidence**: General framework architecture and use of contrastive learning for sequential recommendation
- **Medium confidence**: Specific design choices (complementary mixup ratio p ≤ 0.5, CLIP tokenization approach)
- **Low confidence**: Claim that CLIP-generated text tokens provide superior semantic alignment compared to other approaches

## Next Checks
1. **Ablation on Mixup Strategy**: Systematically test different mixing ratios (p values) and strategies to determine optimal modality integration and validate p ≤ 0.5 constraint

2. **Feature Extraction Comparison**: Compare CLIP-derived text tokens against direct image feature extraction methods and raw text/image concatenation to isolate tokenization contribution

3. **Cold-Start User Evaluation**: Extend cold-start experiments to include new user scenarios with no historical interactions to test multimodal pre-training benefits for user cold-start