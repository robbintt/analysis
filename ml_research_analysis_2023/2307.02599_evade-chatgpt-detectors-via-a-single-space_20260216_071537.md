---
ver: rpa2
title: Evade ChatGPT Detectors via A Single Space
arxiv_id: '2307.02599'
source_url: https://arxiv.org/abs/2307.02599
tags:
- detectors
- chatgpt
- content
- detection
- spaceinfi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper challenges the distributional gap assumption in ChatGPT
  detectors by demonstrating that detectors rely more on subtle formal discrepancies
  rather than semantic or stylistic differences between human-generated and AI-generated
  text. The authors propose a simple evasion strategy, SpaceInfi, which involves adding
  a single space character before a random comma in AI-generated content.
---

# Evade ChatGPT Detectors via A Single Space

## Quick Facts
- arXiv ID: 2307.02599
- Source URL: https://arxiv.org/abs/2307.02599
- Authors: 
- Reference count: 39
- Key outcome: A single space before a comma can evade ChatGPT detectors by exploiting their reliance on subtle formal discrepancies rather than semantic or stylistic differences.

## Executive Summary
This paper challenges the assumption that ChatGPT detectors effectively discriminate between human-generated and AI-generated content based on semantic or stylistic gaps. Instead, it demonstrates that detectors rely heavily on subtle formal discrepancies, such as minor formatting differences. The authors propose SpaceInfi, a simple evasion strategy that adds a single space before a random comma in AI-generated text. Experiments show that SpaceInfi significantly reduces detection rates across multiple benchmarks and detectors, including GPTZero, HelloSimpleAI, and MPU. The paper provides theoretical explanations for why this strategy works, attributing its success to the impact on perplexity calculations and the classifiers' reliance on minor formal differences.

## Method Summary
The paper introduces SpaceInfi, a simple evasion strategy that involves adding a single space character before a random comma in AI-generated content. The method was tested on AI-generated text from multiple benchmarks (Alpaca, Vicuna-eval, WizardLM-eval, and Alpaca-GPT4) using detectors GPTZero, HelloSimpleAI, and MPU. The effectiveness was measured by calculating the evasion rate, defined as the ratio of content identified as human-generated by the detectors after applying SpaceInfi.

## Key Results
- SpaceInfi significantly reduces detection rates for both white-box and black-box detectors.
- The strategy works across multiple benchmarks and detector types, including perplexity-based (GPTZero) and classifier-based (HelloSimpleAI) models.
- Theoretical explanations attribute the evasion success to the impact on perplexity calculations and classifiers' reliance on minor formal discrepancies.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: GPTZero (a perplexity-based detector) relies on the assumption that AI-generated text will always be well-formed without extraneous spaces, so the introduction of a space causes the perplexity calculation to include a very low probability term (p(comma|space)), resulting in high perplexity and misclassification as human-generated.
- **Mechanism**: When SpaceInfi adds a space before a comma, the perplexity formula includes the term $2^{-1/N \log_2 p(\text{comma}|\text{space}, ...)}$ where $p(\text{comma}|\text{space})$ is assumed to be near zero because AI-generated text typically never has a space before a comma. This drives the overall perplexity to a high value, causing GPTZero to classify the text as human.
- **Core assumption**: AI-generated text is always well-formed and does not contain extraneous spaces before punctuation marks.
- **Evidence anchors**:
  - [section] "We assume that AI-generated text is always well-formed. Specifically, when calculating perplexity, it did not encounter cases with extraneous spaces inserted. Therefore, $p(w_i = \text{','}|w_{i-1} = \text{' '}, \ldots, w_1) \to 0$. It ultimately results in a high value for Perplexity(W), leading the detector to consider the text as non-AI-generated."
  - [abstract] "We also provide a theoretical explanation for why SpaceInfi is successful in evading perplexity-based detection."
- **Break condition**: If the detector is retrained on examples with random spaces inserted before punctuation, the probability $p(\text{comma}|\text{space})$ will no longer be near zero, eliminating the high-perplexity signal.

### Mechanism 2
- **Claim**: Classifier-based detectors like HelloSimpleAI rely on subtle formal discrepancies such as minor grammatical errors or formatting differences that are more common in human-generated text than AI-generated text. SpaceInfi introduces such a discrepancy, causing misclassification.
- **Mechanism**: During training, the classifier observes that human-generated text occasionally has small formal anomalies (e.g., extra spaces, typos). AI-generated text is consistently well-formed. By inserting a space before a comma, SpaceInfi mimics this human anomaly, tricking the classifier into classifying the text as human-generated.
- **Core assumption**: The training corpus contains enough examples of minor formal discrepancies in human text and almost none in AI text, so the classifier learns to use these as key features.
- **Evidence anchors**:
  - [section] "Some typical minor differences (e.g., extra spaces added in SpaceInfi, small grammatical errors, etc.) are unique to human-generated content. As a result, classifier-based detectors may rely on these minor differences as crucial features."
  - [abstract] "We empirically show that a phenomenon called token mutation causes the evasion for language model-based detectors."
- **Break condition**: If the detector is retrained with adversarial examples containing subtle formal differences, it will no longer treat these as human-specific features.

### Mechanism 3
- **Claim**: The detection models overfit to distributional gaps that are not semantic or stylistic but rather based on very fine-grained formal artifacts, making them vulnerable to minimal perturbations that exploit these gaps.
- **Mechanism**: Detectors do not effectively discriminate based on semantic or stylistic gaps but instead latch onto small, almost imperceptible formal differences. SpaceInfi exploits this by introducing a minimal change that flips the classification without altering meaning.
- **Core assumption**: The distributional gaps between human and AI text, as learned by the detectors, are dominated by formal artifacts rather than content or style.
- **Evidence anchors**:
  - [abstract] "We find that detectors do not effectively discriminate the semantic and stylistic gaps between human-generated and AI-generated content. Instead, the 'subtle differences', such as an extra space, become crucial for detection."
  - [section] "Our experiments reveal that detectors rely on subtle content differences, such as an extra space."
- **Break condition**: If detectors are designed to focus on semantic or stylistic features rather than formal artifacts, or if they incorporate robustness to minor formal perturbations, the evasion will fail.

## Foundational Learning

- **Concept**: Perplexity as a measure of how well a language model predicts a sentence.
  - **Why needed here**: Understanding why adding a space drastically changes the perplexity score requires knowledge of how perplexity is computed and how probability terms contribute to the overall score.
  - **Quick check question**: If a language model assigns probability 0.01 to a word given its context, what is the contribution of that word to the sentence's perplexity?

- **Concept**: Binary classification with imbalanced training data.
  - **Why needed here**: The classifier-based detectors likely learn features that are over-represented in one class (e.g., minor formal discrepancies in human text). Understanding how classifiers can overfit to such features is key to grasping why SpaceInfi works.
  - **Quick check question**: If a classifier is trained on data where humans make typos 5% of the time but AI never does, will the classifier likely use the presence of typos as a strong feature for human detection?

- **Concept**: Adversarial examples in NLP.
  - **Why needed here**: SpaceInfi is essentially an adversarial attack in the text domain. Understanding the principles of adversarial attacks (minimal perturbations that cause misclassification) helps contextualize why such a simple change is effective.
  - **Quick check question**: What is the defining characteristic of an adversarial example in the context of machine learning classification?

## Architecture Onboarding

- **Component map**: Input text preprocessing -> Space insertion module -> Detector model (GPTZero/HelloSimpleAI/MPU) -> Classification output
- **Critical path**: 
  1. Identify a random comma in the input text
  2. Insert a space character before it
  3. Pass the modified text to the detector
  4. Return the detector's classification
- **Design tradeoffs**:
  - **Simplicity vs. stealth**: SpaceInfi is extremely simple but highly effective; more complex perturbations might be less detectable by humans but harder to implement
  - **Generalizability vs. specificity**: Works across multiple detectors but may fail if detectors are retrained on adversarial examples
  - **Performance vs. robustness**: Very fast and lightweight but not robust to detector updates
- **Failure signatures**:
  - Detection rate remains high (>50%) after SpaceInfi is applied
  - Detection models start flagging text with inserted spaces as suspicious
  - Model retrains or updates that explicitly handle minor formal discrepancies
- **First 3 experiments**:
  1. Apply SpaceInfi to a sample of AI-generated text and measure the detection rate drop for GPTZero
  2. Test SpaceInfi against HelloSimpleAI and record the evasion rate
  3. Evaluate whether MPU is also vulnerable to SpaceInfi and compare results

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How effective are the various evasion strategies in different real-world contexts and applications?
- **Basis in paper**: [explicit] The paper discusses the effectiveness of the SpaceInfi strategy across multiple benchmarks and detectors, but does not explore its effectiveness in different real-world contexts and applications.
- **Why unresolved**: The experiments in the paper are conducted in a controlled environment, and the performance of the evasion strategies may vary in real-world scenarios.
- **What evidence would resolve it**: Testing the evasion strategies in various real-world contexts and applications, and comparing their performance to the controlled environment experiments.

### Open Question 2
- **Question**: Can the SpaceInfi strategy be easily defended against, or are there other potential variations of the strategy that could be more challenging to detect?
- **Basis in paper**: [explicit] The paper discusses the potential for defending against the SpaceInfi strategy by trimming spaces in input text, but also mentions that there are various forms of minor differences that could be used to evade detection.
- **Why unresolved**: The paper does not explore other potential variations of the SpaceInfi strategy or their effectiveness in evading detection.
- **What evidence would resolve it**: Investigating and testing various forms of minor differences that could be used to evade detection, and comparing their effectiveness to the SpaceInfi strategy.

### Open Question 3
- **Question**: How can AI-generated content detectors be improved to better detect and prevent the misuse of AI-generated content?
- **Basis in paper**: [inferred] The paper highlights the limitations of existing AI-generated content detectors and suggests the need for further research in constructing more robust detectors.
- **Why unresolved**: The paper does not provide specific recommendations or approaches for improving AI-generated content detectors.
- **What evidence would resolve it**: Developing and testing new approaches for improving AI-generated content detectors, and evaluating their performance in detecting and preventing the misuse of AI-generated content.

## Limitations

- The effectiveness of SpaceInfi may be limited to detectors that have not been explicitly trained to handle minor formal discrepancies or adversarial examples.
- The strategy assumes that AI-generated text is consistently well-formed without extraneous spaces, which may not hold for all LLMs or prompt configurations.
- The paper does not explore the effectiveness of SpaceInfi in real-world contexts and applications beyond the controlled experimental environment.

## Confidence

- **High confidence**: The empirical demonstration that SpaceInfi reduces detection rates across multiple benchmarks and detectors. The experimental methodology is clear and reproducible.
- **Medium confidence**: The theoretical explanation for why perplexity-based detectors fail when a space is inserted before a comma. While the mathematical reasoning is sound, it relies on assumptions about the detector's training data that may not be universally applicable.
- **Medium confidence**: The claim that classifier-based detectors rely on minor formal discrepancies as key features. This is supported by the experimental results but the underlying mechanism could be more complex than presented.

## Next Checks

1. **Detector Robustness Test**: Apply SpaceInfi to detectors that have been explicitly trained with adversarial examples containing minor formal discrepancies. Measure whether the evasion rate remains significant or drops substantially.

2. **LLM Diversity Test**: Generate AI content using different LLMs (including those known to occasionally insert spaces before punctuation) and evaluate whether SpaceInfi maintains its effectiveness across these varied sources.

3. **Human Evaluation Test**: Conduct a human evaluation study to determine whether the inserted spaces significantly impact readability or appear suspicious to human readers, potentially limiting the practical applicability of the evasion strategy.