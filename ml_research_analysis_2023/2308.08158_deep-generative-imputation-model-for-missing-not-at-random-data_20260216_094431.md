---
ver: rpa2
title: Deep Generative Imputation Model for Missing Not At Random Data
arxiv_id: '2308.08158'
source_url: https://arxiv.org/abs/2308.08158
tags:
- missing
- data
- uni00000013
- mask
- imputation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of imputing missing data that
  is not at random (MNAR), where the cause of missingness is not fully observed. The
  authors propose a new deep generative imputation model called GNR that leverages
  a conjunction model to simultaneously model the complete data and the missing mask
  as two modalities.
---

# Deep Generative Imputation Model for Missing Not At Random Data

## Quick Facts
- arXiv ID: 2308.08158
- Source URL: https://arxiv.org/abs/2308.08158
- Authors: 
- Reference count: 40
- Key outcome: GNR outperforms state-of-the-art MNAR baselines by 9.9% to 18.8% in RMSE

## Executive Summary
This paper addresses the challenge of imputing missing data that is not at random (MNAR), where the cause of missingness is not fully observed. The authors propose a new deep generative imputation model called GNR that leverages a conjunction model to simultaneously model the complete data and the missing mask as two modalities. Unlike existing methods that use a serial structure to map imputed data to the mask, GNR employs a parallel structure with two decoders to extract information from both the data space and mask space without interfering with each other. This approach allows GNR to reconstruct the mask with high accuracy and quality, leading to more principled imputation. Experiments on synthetic and real-world datasets show that GNR outperforms state-of-the-art MNAR baselines by significant margins.

## Method Summary
GNR is a deep generative imputation model that addresses the MNAR problem by treating the complete data and missing mask as two modalities and modeling them independently through parallel decoders. The model uses a conjunction model-based VAE with an auxiliary variable u as an intermediate fusion layer. During training, the model optimizes the importance-weighted variational inference objective using K=20 importance samples, while the evaluation uses L=1000 samples. The model is trained using Adam optimizer with learning rate 0.001 for 10k iterations. The parallel structure allows GNR to avoid the information bottleneck that serial-structure models suffer when mapping imputed data to missing masks, leading to more accurate mask reconstruction and principled imputation.

## Key Results
- GNR outperforms state-of-the-art MNAR baselines by 9.9% to 18.8% in RMSE on various datasets
- GNR consistently achieves better mask reconstruction accuracy compared to serial-structure baselines
- The model demonstrates stable performance across different missing probabilities and dataset sparsity levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GNR's parallel structure avoids the information bottleneck that serial-structure models suffer when mapping imputed data to missing masks.
- Mechanism: By treating the complete data and missing mask as two modalities and modeling them independently through two parallel decoders, GNR extracts sufficient information from both data space and mask space without mutual interference.
- Core assumption: The complete data and missing mask contain unique and non-interchangeable information that can be modeled independently given a shared latent representation.
- Evidence anchors:
  - [abstract]: "GNR employs a parallel structure with two decoders to extract information from both data space and mask space without interfering with each other."
  - [section 3.1.1]: "These serial models cannot extract rich information in both data space and mask space which leads to biased imputation performance."
  - [corpus]: Weak evidence - no direct mention of parallel vs serial architecture in related works.
- Break condition: If the complete data and missing mask are highly correlated such that one modality cannot be accurately reconstructed without the other, the parallel assumption may fail.

### Mechanism 2
- Claim: The conjunction model provides unbiased lower bounds for likelihood estimation in MNAR settings.
- Mechanism: By introducing an auxiliary variable u as an intermediate fusion layer and using importance-weighted variational inference, GNR constructs an unbiased lower bound of the true likelihood.
- Core assumption: The importance-weighted variational inference with k importance samples provides an unbiased estimate of the log-likelihood.
- Evidence anchors:
  - [abstract]: "We theoretically prove the unbiased evidence lower bound of GNR which ensures promising and stable performance on various tasks."
  - [section 4.1]: "Similar to the approach proposed in Ipsen et al. [12], we directly give the monotonicity property of LK and the convergence to the true likelihood in Equation 17."
  - [corpus]: Weak evidence - related works don't discuss unbiasedness of MNAR imputation models.
- Break condition: If the number of importance samples k is too small, the lower bound may become biased and unstable.

### Mechanism 3
- Claim: The parallel structure enables accurate mask reconstruction, which makes the imputation more principled and reliable.
- Mechanism: By modeling the mask distribution independently through a dedicated decoder, GNR can reconstruct the mask with high credibility and quality, avoiding the classification-like errors of serial models.
- Core assumption: The mask distribution can be accurately modeled independently without relying on imputed data.
- Evidence anchors:
  - [abstract]: "GNR reconstructs the mask with high credibility and quality which contributes to the rational data imputation."
  - [section 3.1]: "The mask reconstruction is essentially solving a classification problem... while the observed and missing values in the data point space are highly overlapping in distribution."
  - [corpus]: Weak evidence - related works don't discuss mask reconstruction quality in MNAR settings.
- Break condition: If the mask distribution is too complex to be captured by the dedicated decoder, the parallel structure may not provide sufficient information for accurate mask reconstruction.

## Foundational Learning

- Variational Autoencoders
  - Why needed here: GNR builds upon VAEs as the foundation for modeling the joint distribution of complete data and missing mask.
  - Quick check question: What is the difference between the evidence lower bound (ELBO) and the importance-weighted lower bound used in GNR?

- Missing Data Mechanisms
  - Why needed here: Understanding MCAR, MAR, and MNAR is crucial for appreciating why GNR's approach is necessary for MNAR data.
  - Quick check question: What distinguishes MNAR from MAR in terms of the relationship between observed and missing data?

- Multimodal Learning
  - Why needed here: GNR treats complete data and missing mask as two modalities, requiring understanding of multimodal representation learning.
  - Quick check question: How does treating missing data as a modality differ from traditional approaches that model missingness as a side effect?

## Architecture Onboarding

- Component map:
  - Encoder: Takes observed data and outputs latent representation
  - Latent space: Shared representation for both modalities
  - Data decoder: Reconstructs the complete data from latent space
  - Mask decoder: Reconstructs the missing mask from latent space
  - Importance-weighted inference: Estimates the likelihood using multiple samples

- Critical path:
  1. Input observed data to encoder
  2. Generate latent representation
  3. Decode latent representation to complete data and missing mask
  4. Compute importance-weighted lower bound
  5. Optimize parameters using the lower bound

- Design tradeoffs:
  - Parallel vs serial structure: Parallel structure avoids information bottleneck but requires more parameters
  - Number of importance samples k: More samples provide better estimate but increase computation
  - Dimension of latent space: Larger space captures more information but may overfit

- Failure signatures:
  - Poor mask reconstruction accuracy indicates the parallel structure isn't capturing the mask distribution well
  - Low imputation performance despite good mask reconstruction suggests issues with the data decoder
  - High variance in importance-weighted estimates indicates insufficient samples or poor encoder quality

- First 3 experiments:
  1. Train GNR on synthetic data with known MNAR mechanism and evaluate mask reconstruction accuracy
  2. Compare GNR's performance with a serial-structure baseline on a real-world dataset with varying missing probabilities
  3. Study the effect of the hyper-parameter α on mask reconstruction and imputation performance across different datasets

## Open Questions the Paper Calls Out
- What is the theoretical justification for the assumption that the latent variable u can effectively capture the joint representation of the complete data and missing mask without introducing additional bias?
- How does the choice of hyper-parameter α affect the trade-off between learning the missing mask model and the data model in different types of MNAR scenarios?
- What are the limitations of the conjunction model in handling complex MNAR scenarios, such as those involving multiple causes of missingness or non-linear relationships between the observed and missing data?

## Limitations
- The parallel structure assumption may fail when missingness mechanisms are highly complex and the complete data and missing mask are highly correlated
- Performance on high-dimensional data remains untested, limiting generalizability to domains like images or genomics
- Computational overhead of importance-weighted inference scales linearly with sample count, potentially limiting scalability

## Confidence
- Parallel architecture design: High
- Unbiased ELBO claim: Medium
- Mask reconstruction accuracy as proxy: Medium

## Next Checks
1. Test GNR on high-dimensional datasets (e.g., images or genomics) to evaluate scalability of the parallel architecture
2. Conduct ablation studies varying the number of importance samples k to quantify the trade-off between estimation accuracy and computational cost
3. Evaluate GNR under more complex MNAR mechanisms beyond self-masking, such as those involving multiple unobserved variables influencing missingness