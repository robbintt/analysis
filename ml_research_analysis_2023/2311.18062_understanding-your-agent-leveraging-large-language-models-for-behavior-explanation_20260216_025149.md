---
ver: rpa2
title: 'Understanding Your Agent: Leveraging Large Language Models for Behavior Explanation'
arxiv_id: '2311.18062'
source_url: https://arxiv.org/abs/2311.18062
tags:
- agent
- room
- behavior
- explanations
- rubble
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to generate natural language explanations
  for an agent's behavior using only observations of states and actions. The core
  idea is to distill the agent's policy into a decision tree, extract a behavior representation
  (decision path) from the tree, and use a large language model to generate explanations
  conditioned on this representation.
---

# Understanding Your Agent: Leveraging Large Language Models for Behavior Explanation

## Quick Facts
- arXiv ID: 2311.18062
- Source URL: https://arxiv.org/abs/2311.18062
- Reference count: 16
- One-line primary result: Decision path-based behavior representations significantly improve LLM explanation quality while reducing hallucination in agent behavior explanations.

## Executive Summary
This paper introduces a novel approach to generating natural language explanations for agent behavior using only state-action observations. The method distills agent policies into decision trees, extracts decision paths as behavior representations, and leverages large language models to generate explanations conditioned on these representations. This approach significantly reduces hallucination compared to alternative methods while maintaining high explanation accuracy. The framework also supports interactive explanations, allowing users to ask follow-up questions for clarification and deeper understanding.

## Method Summary
The paper proposes a three-stage pipeline for generating explanations: (1) distill the agent's policy into a decision tree using imitation learning, (2) extract a decision path (behavior representation) from the tree for a given state, and (3) use a large language model with in-context learning to generate explanations conditioned on the behavior representation. The method is independent of the agent's internal representation and focuses on observable states and actions, making it broadly applicable across different types of agents.

## Key Results
- BR Path explanations achieved 60.9% accuracy across Strategy, Category, and Goal metrics, outperforming baselines by 11-13%
- BR Path reduced hallucination by 3.2% compared to next best method and 14.6% compared to vanilla LLM
- User studies showed BR Path explanations were preferred over alternatives and were as helpful as human expert explanations

## Why This Works (Mechanism)

### Mechanism 1
Decision paths provide a compact, LLM-comprehensible representation of agent behavior that reduces hallucination. By distilling the agent's policy into a decision tree and extracting a decision path, we create a behavior representation consisting of an ordered set of decision rules. This representation constrains the LLM's reasoning to the actual decision-making process of the agent, limiting the space of possible explanations and reducing hallucination. Core assumption: Decision trees can effectively approximate the agent's policy while remaining interpretable.

### Mechanism 2
In-context learning with behavior representations enables effective reasoning about agent behavior without fine-tuning. By providing a concise description of the environment, a description of the behavior representation, and in-context learning examples, we can prompt the LLM to reason about agent behavior with respect to a given behavior representation. This approach leverages the LLM's ability to understand and reason with structured information, allowing it to generate accurate explanations without requiring fine-tuning on a specific agent's behavior. Core assumption: LLMs can effectively reason with structured information provided in a prompt.

### Mechanism 3
Interactive explanations enhance understanding and trust in agent behavior. By enabling users to ask follow-up questions, such as clarification or counterfactual queries, we allow for a more engaging and personalized explanation experience. This interaction helps users better understand the agent's behavior, especially when the agent's actions don't align with their expectations. Core assumption: Users benefit from the ability to ask follow-up questions and receive tailored explanations.

## Foundational Learning

- Concept: Decision trees
  - Why needed here: Decision trees serve as an interpretable surrogate model for the agent's policy, enabling the extraction of behavior representations.
  - Quick check question: What is the main advantage of using decision trees as a surrogate model for the agent's policy?

- Concept: In-context learning
  - Why needed here: In-context learning allows the LLM to reason about agent behavior with respect to a given behavior representation without requiring fine-tuning.
  - Quick check question: How does in-context learning differ from traditional fine-tuning approaches?

- Concept: Behavior representation
  - Why needed here: Behavior representations provide a compact, LLM-comprehensible encoding of the agent's behavior, enabling effective reasoning and explanation generation.
  - Quick check question: What are the key components of a behavior representation, and how do they contribute to explanation quality?

## Architecture Onboarding

- Component map: Agent policy -> Decision tree distillation -> Behavior representation extraction -> LLM with in-context learning -> Interactive explanation interface

- Critical path: The critical path for generating an explanation involves distilling the agent's policy into a decision tree, extracting a behavior representation from the decision tree, and querying the LLM with the behavior representation and a prompt to generate the explanation.

- Design tradeoffs:
  - Accuracy vs. interpretability: Decision trees provide interpretability but may not perfectly capture the agent's policy.
  - Explanation quality vs. hallucination: Behavior representations reduce hallucination but may limit the LLM's ability to generate creative explanations.
  - Interactivity vs. efficiency: Interactive explanations enhance understanding but may require more computational resources.

- Failure signatures:
  - Inaccurate decision tree distillation
  - Complex decision paths that exceed the LLM's context window
  - LLM failure to understand or reason with the behavior representation
  - User dissatisfaction with interactive explanations

- First 3 experiments:
  1. Evaluate the accuracy of the distilled decision tree policy compared to the original agent policy.
  2. Assess the effectiveness of different behavior representation encodings (e.g., decision paths vs. state-action pairs) in reducing hallucination and improving explanation quality.
  3. Measure the impact of interactive explanations on user understanding and trust in agent behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the decision tree depths affect the quality and completeness of the behavior representations for different agent behaviors?
- Basis in paper: [inferred] ... The paper mentions that "decision tree depth is usually constrained in order to prevent overfitting" but does not discuss how this constraint impacts the quality of the behavior representations.
- Why unresolved: The paper does not provide specific details on how varying the depth of the decision trees might impact the quality of the behavior representations, especially in capturing long-term goals or complex decision-making processes.
- What evidence would resolve it: Experiments comparing the performance and explanation quality of behavior representations derived from decision trees of different depths would provide insights into the optimal depth for balancing interpretability and accuracy.

### Open Question 2
- Question: How does the performance of the framework change when applied to environments with more complex state spaces or larger action spaces?
- Basis in paper: [explicit] ... The paper evaluates the framework in a "multi-agent search-and-rescue environment" but does not explore its applicability to more complex or larger-scale environments.
- Why unresolved: The scalability of the framework to more complex environments with higher-dimensional state spaces or more diverse action spaces is not addressed, leaving questions about its general applicability.
- What evidence would resolve it: Testing the framework in environments with progressively larger and more complex state and action spaces would demonstrate its scalability and potential limitations.

### Open Question 3
- Question: What are the effects of using different types of behavior representations (e.g., differentiable decision trees, concept feature extractors) on the quality of explanations and hallucination rates?
- Basis in paper: [inferred] ... The paper mentions that "limitations can be overcome with more complex behavior representations, e.g., differentiable decision trees or concept feature extractors" but does not explore these alternatives.
- Why unresolved: The paper focuses on decision tree-based behavior representations and does not investigate how other types of behavior representations might affect the quality of explanations or the frequency of hallucinations.
- What evidence would resolve it: Comparative studies using different types of behavior representations to generate explanations and measure their impact on explanation quality and hallucination rates would provide insights into the most effective representations.

## Limitations

- Decision tree distillation process lacks detailed hyperparameter specifications that could significantly impact behavior representation quality
- Small user study sample size (N=18) limits generalizability of subjective evaluation results
- Framework's performance in environments with complex state spaces or larger action spaces remains untested

## Confidence

**High Confidence**: The core methodology of using decision paths as behavior representations to constrain LLM reasoning is well-founded and technically sound. The reported improvement in explanation accuracy and hallucination reduction is supported by quantitative metrics.

**Medium Confidence**: The effectiveness of interactive explanations is supported by user studies, but the small sample size and potential selection bias in participants (university students) warrant cautious interpretation. The generalization of results across different agent types and domains remains to be seen.

**Low Confidence**: The robustness of the approach to more complex environments and longer decision paths is not thoroughly evaluated. The paper doesn't address potential computational overhead of the three-stage pipeline or scalability concerns.

## Next Checks

1. **Decision Tree Fidelity Analysis**: Conduct a systematic ablation study varying decision tree depth and training parameters to quantify the trade-off between interpretability and policy accuracy. Measure how different tree configurations affect explanation quality.

2. **Interactive Explanation Scalability Test**: Evaluate the approach with more complex multi-step decision paths (beyond the current depth) and measure LLM performance in handling longer behavior representations within context limits.

3. **Cross-Domain Generalization**: Apply the method to agents trained with different algorithms (DQN, PPO) and in varied environments to assess the robustness of the decision path representation approach across diverse policy structures.