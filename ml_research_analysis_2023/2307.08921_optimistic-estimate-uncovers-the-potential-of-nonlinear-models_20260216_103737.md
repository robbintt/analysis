---
ver: rpa2
title: Optimistic Estimate Uncovers the Potential of Nonlinear Models
arxiv_id: '2307.08921'
source_url: https://arxiv.org/abs/2307.08921
tags:
- optimistic
- sample
- size
- estimate
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces an optimistic estimate framework to evaluate
  the best possible fitting performance of nonlinear models, particularly focusing
  on overparameterization scenarios. The framework introduces the concept of "optimistic
  sample size," which quantifies the smallest possible sample size needed to fit or
  recover a target function using a nonlinear model.
---

# Optimistic Estimate Uncovers the Potential of Nonlinear Models

## Quick Facts
- arXiv ID: 2307.08921
- Source URL: https://arxiv.org/abs/2307.08921
- Reference count: 40
- Primary result: Optimistic estimate framework quantifies overparameterization potential through minimum sample size needed to fit target functions

## Executive Summary
This work introduces an optimistic estimate framework to evaluate the best possible fitting performance of nonlinear models, particularly in overparameterization scenarios. The framework defines "optimistic sample size" as the minimum number of samples needed to fit a target function using a nonlinear model under ideal initialization conditions. Applied to matrix factorization, deep models, and DNNs, the framework reveals that DNNs have free expressiveness in width (adding neurons doesn't increase sample requirements) but costly expressiveness in connection (adding connections does increase requirements). This suggests architectural principles: feel free to add neurons/kernels but restrain from connecting neurons unnecessarily.

## Method Summary
The method introduces optimistic sample size as a metric to quantify the minimum samples needed to fit a target function under best possible initialization. The framework computes model rank (dimension of tangent space) across parameter points in the target set and finds the minimum rank. Experiments validate the framework using gradient descent optimization with small random initialization on synthetic datasets generated from target functions. The approach involves tuning initialization variance and learning rate to approach near-optimistic performance, then measuring when test error approaches zero as sample size varies.

## Key Results
- Optimistic sample size quantifies overparameterization potential by measuring minimum samples needed under ideal initialization
- DNNs exhibit free expressiveness in width (adding neurons doesn't increase optimistic sample size) but costly expressiveness in connection (adding connections increases sample requirements)
- Empirical sample sizes approach optimistic estimates when using small initialization and sufficient hyperparameter tuning
- The framework provides quantitative answers to which models can fit at overparameterization and effective parameter size for nonlinear models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The optimistic sample size quantifies the minimum sample size needed to fit a target function under best possible initialization, revealing overparameterization potential.
- Mechanism: By initializing near an "optimal" point in the target set, the model rank at that point determines the minimum number of samples needed, independent of total parameter count.
- Core assumption: The target set contains points with lower model rank than the total parameter count, enabling overparameterization.
- Evidence anchors:
  - [abstract] "yields an optimistic sample size that quantifies the smallest possible sample size to fit/recover a target function"
  - [section 2] "the sample size necessary for fitting serves as a natural quantity... influenced by both the model architecture and the target function"
- Break condition: If the target set only contains points with full model rank equal to total parameters, overparameterization cannot occur.

### Mechanism 2
- Claim: Free expressiveness in width means adding neurons/kernels doesn't increase the optimistic sample size for target functions.
- Mechanism: When a target function can be expressed by a narrow network, wider networks with the same or more neurons have the same model rank at optimal points, preserving the optimistic sample size.
- Core assumption: The target function's intrinsic complexity (width/kernels needed) remains constant regardless of the network width.
- Evidence anchors:
  - [section 5.2] "For both two-layer fully-connected and convolutional NNs, an increase in the width... enhances their expressiveness without impairing the optimistic fitting performance"
  - [section 5.4] "Expanding the width of a DNN does not increase the optimistic sample size"
- Break condition: If adding neurons creates new dimensions in the tangent space at all points in the target set, the optimistic sample size would increase.

### Mechanism 3
- Claim: Costly expressiveness in connection means adding unnecessary connections between neurons increases the optimistic sample size.
- Mechanism: More connections create higher dimensional tangent spaces at optimal points, requiring more samples to fit the same target function.
- Core assumption: The intrinsic complexity of the target function doesn't change with additional connections, but the model's expressiveness does.
- Evidence anchors:
  - [section 5.2] "increasing the number of connections per hidden neuron from s to d multiplies the optimistic sample size by d+1/s+1"
  - [section 5.2] "removing unnecessary connections between neurons substantially reduces the optimistic sample size"
- Break condition: If additional connections enable fitting the target function with fewer samples by creating more efficient representations.

## Foundational Learning

- Concept: Model rank as the dimension of the tangent space at parameter points
  - Why needed here: The optimistic sample size is defined as the minimum model rank over the target set, making understanding tangent spaces essential
  - Quick check question: If a model has M parameters and the tangent space at a point has dimension R < M, how many samples are theoretically needed to fit functions expressible at that point?

- Concept: Target set as the set of parameters that exactly express the target function
  - Why needed here: The optimistic sample size is the minimum model rank over all points in the target set, requiring understanding of this set's structure
  - Quick check question: Why might a target function have multiple parameter points in its target set, and how does this affect the optimistic sample size?

- Concept: Critical embeddings preserving model rank across different parameter counts
  - Why needed here: The free expressiveness property relies on embedding narrower networks into wider ones while preserving the ability to express the target function
  - Quick check question: What property must a parameter embedding have to preserve the optimistic sample size when expanding network width?

## Architecture Onboarding

- Component map: Model definition fθ(x) with parameters θ -> Target set Θf* = {θ | f(·;θ) = f*} -> Tangent space span{∂θi f(·;θ)}M i=1 -> Model rank Rfθ(θ) = dim(tangent space) -> Optimistic sample size Rfθ(f*) = minθ∈Θf* Rfθ(θ)

- Critical path:
  1. Define the model and its parameter space
  2. Identify the target function and its target set
  3. Compute or estimate model rank across the target set
  4. Find the minimum model rank (optimistic sample size)
  5. Verify through experiments if empirical sample sizes approach the optimistic estimate

- Design tradeoffs:
  - Width vs. connections: Wider networks don't increase optimistic sample size, but more connections do
  - Initialization scale: Small initialization enables near-optimistic performance; large initialization increases required sample size
  - Training duration: Sufficient training needed to reach low model rank regions of the target set

- Failure signatures:
  - Optimistic sample size equals total parameter count M: Model cannot fit at overparameterization
  - Empirical sample sizes consistently much larger than optimistic estimates: Initialization or training hyperparameters not properly tuned
  - Target function requires full model rank at all target set points: Function too complex for the model architecture

- First 3 experiments:
  1. Fit a low-rank matrix completion problem with matrix factorization model using sample sizes below parameter count; verify empirical sample sizes approach 2rd - r²
  2. Fit a sparse target function with deep model (Eq. 9) using sample sizes much smaller than parameter count; verify O(k log d) scaling
  3. Fit a 3-kernel CNN target function (Eq. 15) with fully-connected vs convolutional networks; verify convolutional networks require fewer samples due to reduced connections

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the mechanism underlying the near-optimism fitting performance observed in DNNs?
- Basis in paper: [explicit] The paper mentions that the near-optimism fitting performance is closely related to the condensation phenomenon observed during nonlinear training beyond the neural tangent kernel regime.
- Why unresolved: While the paper provides some insights into the condensation phenomenon, the exact mechanism linking condensation to near-optimism performance is not fully understood and requires further investigation.
- What evidence would resolve it: Further experimental and theoretical studies elucidating the role of condensation in achieving near-optimism fitting performance, possibly through detailed analysis of condensation dynamics and its impact on model rank.

### Open Question 2
- Question: How does the implicit bias towards interpolations with lower optimistic sample sizes affect the generalization performance of nonlinear models?
- Basis in paper: [explicit] The paper suggests that nonlinear models tend to acquire interpolations with optimistic sample sizes less than or equal to the number of training samples, which may not match the target function.
- Why unresolved: The implications of this implicit bias on the generalization capabilities of nonlinear models are not fully explored, and understanding this relationship is crucial for predicting and improving model performance.
- What evidence would resolve it: Empirical studies comparing the generalization performance of models with different implicit biases, and theoretical analysis of the connection between optimistic sample size and generalization error.

### Open Question 3
- Question: Can the optimistic estimate framework be extended to other types of nonlinear models beyond those discussed in the paper?
- Basis in paper: [inferred] The paper applies the optimistic estimate framework to matrix factorization models, deep models, and DNNs, but does not explore its applicability to other nonlinear models.
- Why unresolved: The framework's generalizability to other model types is not established, and understanding its broader applicability could provide insights into the potential of various nonlinear models.
- What evidence would resolve it: Application of the optimistic estimate framework to a diverse set of nonlinear models, including but not limited to recurrent neural networks, graph neural networks, and transformer models, to assess its effectiveness and limitations.

## Limitations

- The framework assumes near-optimal initialization can reliably place models in low-rank regions of the target set, but this lacks empirical validation across diverse function classes
- Results are demonstrated on limited model families (matrix factorization, tanh deep models, fully-connected and convolutional DNNs) and may not generalize to other architectures or real-world datasets
- The connection between model rank and sample complexity is intuitive but not rigorously proven for all nonlinear models, providing estimates without formal guarantees

## Confidence

- High Confidence: Theoretical framework for optimistic sample size and its properties (free expressiveness in width, costly expressiveness in connection) are well-established through mathematical analysis
- Medium Confidence: Experimental validations showing empirical sample sizes approach optimistic estimates are convincing but limited in scope, with unclear hyperparameter tuning strategy
- Low Confidence: Generalizability of architectural principles to practical DNN design remains speculative, as real-world performance involves additional factors not captured by the framework

## Next Checks

1. **Initialization Robustness Test:** Systematically vary initialization scales across orders of magnitude for the same target function and model architecture. Measure how far empirical sample sizes deviate from optimistic estimates and identify the threshold beyond which overparameterization fails.

2. **Cross-Architecture Generalization:** Apply the framework to models with different activation functions (ReLU, Leaky ReLU, sigmoid) and compare optimistic sample sizes. Test whether the width/connection tradeoff properties hold across these architectures.

3. **Noisy Data Validation:** Repeat experiments on target functions corrupted with varying levels of noise. Quantify how noise levels affect the gap between empirical and optimistic sample sizes, and whether the framework can be extended to account for statistical efficiency in noisy settings.