---
ver: rpa2
title: 'Won''t Get Fooled Again: Answering Questions with False Premises'
arxiv_id: '2307.02394'
source_url: https://arxiv.org/abs/2307.02394
tags:
- questions
- fpqs
- question
- plms
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of answering false premise questions
  (FPQs) using pre-trained language models (PLMs). The authors propose that PLMs already
  possess the necessary knowledge to rebut FPQs but need to be activated.
---

# Won't Get Fooled Again: Answering Questions with False Premises

## Quick Facts
- arXiv ID: 2307.02394
- Source URL: https://arxiv.org/abs/2307.02394
- Reference count: 40
- Primary result: PLMs can be activated to discriminate and rebut false premise questions using only 256 fine-tuning examples

## Executive Summary
This paper tackles the challenge of answering false premise questions (FPQs) using pre-trained language models (PLMs). The authors propose that PLMs already possess the necessary knowledge to rebut FPQs but need to be activated. To address this, they introduce the FalseQA dataset, containing 2365 human-written FPQs with explanations for false premises and revised true premise questions. Through extensive experiments, they demonstrate that PLMs can discriminate and rebut FPQs by fine-tuning on a moderate number of examples. Additionally, they show that replaying a few general questions during training allows PLMs to excel on both FPQs and general questions simultaneously.

## Method Summary
The paper introduces FalseQA, a dataset of 2365 human-written FPQ-TPQ pairs with explanations. The method involves fine-tuning PLMs (OPT, T5, MACAW variants) on these pairs using binary cross-entropy loss for classification and generation loss for explanations. To prevent catastrophic forgetting, data replay interleaves FPQ training with general QA data (ARC-DA). The models learn to output distinguishing tokens ("tricky question" vs "true question") to indicate whether a question contains a false premise.

## Key Results
- PLMs achieve over 70% accuracy on FPQ discrimination with only 256 fine-tuning examples
- Data replay enables simultaneous handling of FPQs and general questions without catastrophic forgetting
- Larger PLMs require fewer examples to activate FPQ handling capabilities
- GPT-3 in-context learning achieves only 35.6% accuracy on FPQs, while fine-tuning achieves 85.2%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PLMs already possess the knowledge required to rebut such questions but need to be activated.
- Mechanism: The knowledge exists within the PLM's weights but is not triggered during standard inference because FPQs violate common-sense premises that rarely appear in natural text. Fine-tuning on FPQ examples activates this dormant knowledge.
- Core assumption: The PLM's pre-training corpus contains sufficient real-world knowledge to recognize and explain false premises.
- Evidence anchors:
  - [abstract]: "we find that the PLMs already possess the knowledge required to rebut such questions, and the key is how to activate the knowledge."
  - [section]: "We begin with a pilot experiment that confirms current PLMs' responses to FPQs are not satisfactory despite their knowledge."

### Mechanism 2
- Claim: Fine-tuning on moderate numbers of FPQ examples enables PLMs to discriminate and rebut FPQs.
- Mechanism: During fine-tuning, the model learns to associate the specific linguistic patterns of FPQs with the task of generating explanations or refusals. This is a form of task-specific adaptation rather than knowledge acquisition.
- Core assumption: The PLM can generalize from a small number of examples to correctly handle FPQs it hasn't seen before.
- Evidence anchors:
  - [abstract]: "PLMs are capable of discriminating FPQs by fine-tuning on moderate numbers (e.g., 256) of examples."
  - [section]: "With only 256 pairs of questions, models larger than 2.7B... all achieve more than 70% accuracy."

### Mechanism 3
- Claim: Data replay during training mitigates catastrophic forgetting and enables simultaneous handling of FPQs and general questions.
- Mechanism: By interleaving batches of general questions (from ARC-DA) during FPQ training, the model maintains its ability to answer normal questions while learning to handle FPQs.
- Core assumption: The model can maintain dual capabilities when trained on both task types in an interleaved manner.
- Evidence anchors:
  - [abstract]: "Further replaying a few general questions during training allows PLMs to excel on FPQs and general questions simultaneously."
  - [section]: "We use a simple data replay method... where the model discriminates 86.7% FPQs in FalseQA and only rebuts 1.4% general questions."

## Foundational Learning

- Concept: False Premise Questions (FPQs)
  - Why needed here: Understanding what constitutes a false premise is fundamental to recognizing and rebutting FPQs.
  - Quick check question: Given the question "How many eyes does the sun have?", can you identify the false premise and explain why it's false?

- Concept: Pre-trained Language Models (PLMs)
  - Why needed here: The paper's core claim is that PLMs already contain the knowledge needed to handle FPQs but require activation.
  - Quick check question: What is the difference between a decoder-only model like OPT and an encoder-decoder model like T5, and why might this matter for FPQ handling?

- Concept: Catastrophic Forgetting
  - Why needed here: When fine-tuning on FPQs, the model might lose its ability to answer general questions, which data replay aims to prevent.
  - Quick check question: If a model achieves 90% accuracy on FPQs after fine-tuning but only 40% on general questions (down from 80%), what phenomenon is occurring?

## Architecture Onboarding

- Component map:
  FalseQA dataset (FPQs + TPQs + explanations/answers) → Training batches → PLM (OPT, T5, MACAW variants) → Binary classification + explanation generation → Evaluation (FPQ/TPQ accuracy, ROUGE-L)

- Critical path:
  1. Load FalseQA dataset and split into train/val/test
  2. Initialize PLM with pre-trained weights
  3. For each epoch:
     - Sample batch of FPQs
     - Optionally sample batch of general questions (data replay)
     - Compute loss (classification + generation + binary loss for discrimination)
     - Update model weights
  4. Evaluate on test split

- Design tradeoffs:
  - Model size vs. data efficiency: Larger models activate with fewer examples but require more compute
  - Data replay ratio: Balancing FPQ learning vs. maintaining general question ability
  - Prompt design: Different PLM architectures require different input formats

- Failure signatures:
  - High FPQ accuracy but low TPQ accuracy: Model is over-rebutting and treating all questions as false
  - Low ROUGE-L on explanations: Model is correctly identifying FPQs but generating poor explanations
  - High FPR on general questions: Catastrophic forgetting is occurring

- First 3 experiments:
  1. Fine-tune MACAW-11B on 256 FPQ examples only, measure FPQ/TPQ accuracy
  2. Fine-tune MACAW-11B on 256 FPQ examples with data replay (1:1 ratio with ARC-DA), measure both FPQ and general question performance
  3. Compare in-context learning performance of GPT-3 vs. fine-tuning on 32 examples for both approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can larger models be activated to rebut FPQs with even fewer training examples than the 256 pairs used in the experiments?
- Basis in paper: [explicit] The paper states "With only 256 pairs of questions, models larger than 2.7B, i.e., OPT-2.7B, MACAW-3B, MACAW-11B, all achieve more than 70% accuracy, while the smaller models need more data to achieve non-trivial performance. The trade-off between model scale and data scale hints that larger models might be activated with even fewer training data."
- Why unresolved: The paper does not experiment with fewer than 256 training examples for the larger models to determine the minimum number needed to achieve satisfactory performance on FPQs.
- What evidence would resolve it: Additional experiments testing the performance of larger models (e.g., MACAW-11B) on FPQs with varying numbers of training examples below 256, to identify the point at which performance plateaus or reaches a satisfactory level.

### Open Question 2
- Question: What are the underlying reasons why PLMs fail on certain FPQs, even after being fine-tuned on the FalseQA dataset?
- Basis in paper: [inferred] The paper mentions that "We notice that the newly announced model ChatGPT (OpenAI, 2022) handles such questions satisfactorily. However, since their training data and details are not open-sourced, we are unable to investigate how the ability of these particular models is activated." This suggests that there may be other factors beyond the knowledge present in PLMs that contribute to their ability to handle FPQs.
- Why unresolved: The paper does not provide a detailed analysis of the specific FPQs that PLMs fail on after fine-tuning, nor does it investigate potential reasons for these failures beyond the hypothesis that the knowledge is not activated.
- What evidence would resolve it: A thorough analysis of the FPQs that PLMs fail on after fine-tuning, including an investigation of the specific knowledge or reasoning required to correctly rebut each question, and an exploration of potential techniques to improve PLM performance on these challenging cases.

### Open Question 3
- Question: How can PLMs be improved to generate more creative and diverse responses to FPQs, rather than simply providing a rebuttal?
- Basis in paper: [explicit] The paper states "In this paper, we standardize the expected responses to FPQs as rebuttals, which takes a conventional perspective. However, sometimes we can react with a more creative response, such as a rhetorical question. This can be future work."
- Why unresolved: The paper focuses on training PLMs to generate rebuttals for FPQs, but does not explore alternative response styles that could be more engaging or informative for users.
- What evidence would resolve it: Experiments comparing the performance of PLMs trained to generate creative responses (e.g., rhetorical questions, analogies, or alternative perspectives) to FPQs, as well as user studies evaluating the effectiveness and appeal of these alternative response styles.

## Limitations

- Data Distribution Bias: The FalseQA dataset contains only 2,365 examples, which may limit generalizability to real-world scenarios where FPQs could be more diverse and numerous.
- Architectural Specificity: The paper demonstrates success primarily with decoder-only and encoder-decoder models but doesn't extensively explore other architectures like encoder-only or retrieval-augmented models.
- Evaluation Scope: While evaluating FPQ handling and catastrophic forgetting, the paper doesn't measure other potential side effects of fine-tuning, such as changes in general language understanding or factual knowledge retention.

## Confidence

- High Confidence: The claim that PLMs possess latent knowledge for FPQ handling is well-supported by experimental results showing that fine-tuning on moderate examples (256 pairs) enables accurate discrimination and rebuttal.
- Medium Confidence: The assertion that PLMs can excel at both FPQs and general questions simultaneously through data replay is supported by results, but the optimal replay ratio and its sensitivity to different dataset combinations require further exploration.
- Low Confidence: The paper's claims about in-context learning performance (GPT-3 achieving only 35.6% accuracy) compared to fine-tuning are based on limited experimentation and may not generalize to newer or differently-prompted LLMs.

## Next Checks

- Cross-dataset Generalization: Evaluate the fine-tuned models on external FPQ datasets (like KG-FPQ or MultiHoax) to verify that the learned FPQ handling capability transfers beyond the FalseQA distribution.
- Long-term Stability Assessment: Conduct multi-stage training experiments where models are first fine-tuned on general QA, then on FPQs with data replay, then again on general QA. Measure whether the FPQ capability persists after subsequent training on unrelated tasks.
- Ablation on Data Replay Ratio: Systematically vary the ratio of FPQ to general question batches during training (e.g., 1:1, 1:3, 1:10) and measure the tradeoff curve between FPQ performance and general QA retention.