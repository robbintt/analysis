---
ver: rpa2
title: Learning Disentangled Semantic Spaces of Explanations via Invertible Neural
  Networks
arxiv_id: '2305.01713'
source_url: https://arxiv.org/abs/2305.01713
tags:
- space
- latent
- animals
- animal
- disentanglement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores disentangling latent spaces for natural language
  processing, specifically for semantic sentence representations. The method employs
  a flow-based invertible neural network (INN) integrated with a transformer-based
  language autoencoder to transform the hidden space into a more separable semantic
  space.
---

# Learning Disentangled Semantic Spaces of Explanations via Invertible Neural Networks

## Quick Facts
- **arXiv ID**: 2305.01713
- **Source URL**: https://arxiv.org/abs/2305.01713
- **Reference count**: 17
- **Primary result**: The method achieves improved disentanglement metrics and better control over autoencoder generation through invertible neural networks with cluster supervision.

## Executive Summary
This paper addresses the challenge of disentangling latent semantic representations in natural language processing by employing a flow-based invertible neural network (INN) integrated with a transformer-based autoencoder. The approach transforms distributed sentence representations into a smooth multivariate Gaussian space where semantic clusters become more separable. The model is trained using both unsupervised and cluster-supervised strategies, with experimental results showing improved disentanglement metrics and semantically coherent data augmentation capabilities.

## Method Summary
The proposed method integrates an INN with a BERT-GPT2 autoencoder to create a bijective mapping between distributed sentence representations and a multivariate Gaussian latent space. The INN consists of 10 invertible blocks that transform the 32-dimensional BERT embeddings into a disentangled semantic space. The training procedure employs both unsupervised learning (negative log-likelihood objective) and cluster-supervised learning (incorporating semantic role cluster centers). The model enables vector arithmetic and traversal operations in the transformed space for data augmentation while maintaining semantic coherence.

## Key Results
- The INN-transformed latent space shows improved cluster separation compared to baseline Optimus model
- Cluster-supervised training produces more concentrated embeddings around semantic role centers with clearer cluster boundaries
- The method enables semantically coherent data augmentation through vector traversal operations that maintain smooth transitions between semantic states

## Why This Works (Mechanism)

### Mechanism 1
Flow-based invertible neural networks transform distributed hidden representations into a smooth multivariate Gaussian latent space with better separability. INNs provide a bijective mapping between BERT-GPT2 autoencoder representations and Gaussian space, preserving information while creating structured latent space where semantic clusters become more separable. Core assumption: semantic roles and content clusters can be mapped to geometrically separable hypersolids. Evidence: experimental results show better disentanglement; Break condition: if semantic relationships are too complex for invertible transformations to capture.

### Mechanism 2
Cluster-supervised training improves disentanglement between semantic role-content clusters. Providing cluster centers during training enables the INN to map sentence representations to a space where each cluster has densely distributed embeddings around its center, creating clearer boundaries. Core assumption: semantic role supervision can effectively guide better-separated regions. Evidence: embeddings become more concentrated on cluster centers with clear boundaries; Break condition: if cluster centers are poorly defined or don't correspond to natural geometric divisions.

### Mechanism 3
Geometric properties of transformed latent space enable semantically coherent data augmentation. Vector arithmetic and traversal operations in the smooth Gaussian space generate new sentences maintaining semantic consistency with original data distribution. Well-separated clusters ensure operations stay within meaningful semantic regions. Core assumption: smooth, well-separated structure preserves semantic relationships during geometric operations. Evidence: intermediate explanations transition smoothly during traversal; Break condition: if traversal crosses semantically incoherent regions.

## Foundational Learning

- **Variational Autoencoders (VAEs)**: The paper builds on VAE concepts from Optimus and extends them with INN mechanisms for better disentanglement. Quick check: How does the posterior approximation in VAEs differ from the exact mapping in INNs?

- **Semantic Role Labeling (SRL)**: Disentanglement targets semantic roles like ARG0, ARG1, PRED requiring understanding of predicate-argument structure. Quick check: What semantic roles are annotated in WorldTree corpus and how do they relate to sentence meaning?

- **Flow-based Generative Models**: INNs are a type of flow-based model enabling exact likelihood computation and invertible transformations. Quick check: What is the key mathematical property that distinguishes flow-based models from other generative approaches?

## Architecture Onboarding

- **Component map**: Sentence → BERT → INN (transform) → Latent space → INN (inverse) → GPT-2 → Generated sentence

- **Critical path**: Input sentences flow through BERT encoder, INN transformation, latent space operations, INN inverse transformation, and GPT-2 decoder for generation

- **Design tradeoffs**: Fixed vs learned latent dimensionality (uses 32-dim from Optimus), supervised vs unsupervised training (both explored), computational cost (INNs add overhead but enable better control)

- **Failure signatures**: Poor reconstruction quality indicates INN losing information; cluster overlap in t-SNE suggests supervision isn't separating semantic regions; nonsensical generated sentences may indicate traversal crossing semantic boundaries

- **First 3 experiments**: 1) Test unsupervised INN with small WorldTree subset to verify basic functionality, 2) Implement cluster-supervised training with ARG0 clusters only to isolate supervision effect, 3) Compare traversal operations between unsupervised and supervised INN to measure qualitative differences in semantic coherence

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed method compare to other disentanglement approaches, such as VAEs or adversarial training, in terms of disentanglement metrics and reconstruction quality? The paper mentions outperforming Optimus but doesn't compare to other disentanglement methods. What evidence would resolve it: direct comparison with other methods on disentanglement metrics and reconstruction quality.

### Open Question 2
How does the proposed method perform on other types of text data, such as news articles or social media posts, beyond WorldTree and EntailmentBank datasets? The paper only evaluates on science explanation datasets. What evidence would resolve it: evaluation on other text types to assess generalization capabilities.

### Open Question 3
How does the proposed method handle longer and more complex sentences beyond the average lengths in WorldTree (8.65 words) and EntailmentBank (10.35 words)? The paper doesn't discuss performance on longer sentences. What evidence would resolve it: evaluation on longer and more complex sentences to assess scaling capabilities.

### Open Question 4
How does the proposed method handle out-of-vocabulary words or rare semantic roles beyond those present in WorldTree and EntailmentBank datasets? The paper doesn't discuss handling of OOV words or rare semantic roles. What evidence would resolve it: evaluation on data containing OOV words or rare semantic roles to assess generalization.

## Limitations
- The claims about data augmentation effectiveness have the least direct empirical support with only qualitative assessment
- Fixed 32-dimensional latent space inherited from Optimus may limit ability to capture complex semantic relationships
- Evaluation focuses on two specific explanation corpora, unclear whether benefits transfer to other domains or languages

## Confidence
- **High confidence**: Core mechanism of using INNs for invertible transformations is well-established and empirical results strongly support improvement in disentanglement metrics
- **Medium confidence**: Cluster-supervised training benefits are supported by experimental results but extent of necessity versus other clustering approaches remains unclear
- **Low confidence**: Practical utility of generated data augmentation lacks rigorous validation with only qualitative assessments of sentence coherence

## Next Checks
1. Quantitatively evaluate whether data augmentation preserves original data distribution by measuring perplexity, classifier confidence stability, or other distributional metrics on augmented vs original data
2. Apply disentangled representations to downstream NLP task (e.g., semantic similarity, text classification) to measure whether improved disentanglement translates to better performance compared to baseline representations
3. Systematically vary latent space dimensionality and measure impact on disentanglement metrics to determine whether 32 dimensions is optimal or if better trade-off exists between expressiveness and separability