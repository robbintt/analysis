---
ver: rpa2
title: 'GPQA: A Graduate-Level Google-Proof Q&A Benchmark'
arxiv_id: '2311.12022'
source_url: https://arxiv.org/abs/2311.12022
tags:
- question
- questions
- answer
- expert
- non-expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GPQA is a benchmark of 448 multiple-choice questions in biology,
  physics, and chemistry, designed to be difficult for non-experts and current AI
  systems. The questions are written by PhD-level experts and validated for objectivity
  and difficulty.
---

# GPQA: A Graduate-Level Google-Proof Q&A Benchmark

## Quick Facts
- arXiv ID: 2311.12022
- Source URL: https://arxiv.org/abs/2311.12022
- Reference count: 40
- Primary result: PhD-level experts achieve 65% accuracy on GPQA questions, while non-experts with unrestricted web access score only 34%

## Executive Summary
GPQA is a benchmark of 448 multiple-choice questions in biology, physics, and chemistry designed to be difficult for non-experts and current AI systems. The questions are written and validated by PhD-level experts to ensure objectivity and difficulty. Non-experts with PhDs in other fields achieve only 34% accuracy after spending significant time with full internet access, while GPT-4 with few-shot chain-of-thought prompting reaches 39% accuracy. Experts score 65% accuracy, rising to 74% when accounting for clear mistakes. The dataset aims to support research on scalable oversight methods for supervising superhuman AI systems by providing challenging questions that require expert knowledge.

## Method Summary
The GPQA dataset consists of 448 multiple-choice questions created by PhD-level domain experts in biology, physics, and chemistry. Each question undergoes two rounds of expert validation to ensure objectivity and difficulty. Non-experts with PhDs in other fields attempt to answer questions with unrestricted web access to establish a baseline. AI systems like GPT-4 are evaluated using few-shot chain-of-thought prompting. The validation process includes compensation structures for both expert writers and non-expert validators.

## Key Results
- Non-experts with PhDs in other fields achieve 34% accuracy on GPQA questions despite unrestricted web access
- GPT-4 with few-shot chain-of-thought prompting reaches 39% accuracy
- PhD-level experts score 65% accuracy, rising to 74% when accounting for clear mistakes
- Questions require approximately 37 minutes per question for non-experts to attempt

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPQA isolates expert-level question difficulty by requiring PhD-level annotators for both writing and validation.
- Mechanism: Experts in subfields create questions, which are validated by peers to ensure objective ground truth.
- Core assumption: Only PhD-level domain experts can reliably identify and validate questions that are objective and sufficiently difficult for non-experts.
- Evidence anchors:
  - [abstract] "written by domain experts in biology, physics, and chemistry"
  - [section 2.1] "We hire 61 contractors through Upwork to write and validate the dataset. We require that they have completed or are currently in a PhD program in their field of expertise"
  - [corpus] Average neighbor FMR=0.468, indicating moderate overlap with similar expert-driven benchmarks

### Mechanism 2
- Claim: GPQA ensures questions are "Google-proof" by testing skilled non-experts who spend significant time with full internet access.
- Mechanism: Non-experts (still PhD holders in other domains) attempt to answer questions with unrestricted web access, achieving only 34% accuracy.
- Core assumption: PhD-level non-experts with unrestricted web access represent a realistic upper bound for non-expert performance.
- Evidence anchors:
  - [abstract] "while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web"
  - [section 2.1] "Non-experts receive a base payment of $10 for attempting to answer the question, and a $30 bonus for each question they answer correctly"
  - [corpus] Related work like CMPhysBench also uses PhD-level validators to ensure difficulty

### Mechanism 3
- Claim: GPQA provides a realistic testbed for scalable oversight by being difficult for both humans and current AI systems.
- Mechanism: GPT-4 with few-shot chain-of-thought prompting achieves only 39% accuracy, below expert (65%) and far above non-expert (34%) performance.
- Core assumption: The accuracy gap between experts and AI systems creates a meaningful space for scalable oversight research.
- Evidence anchors:
  - [abstract] "GPT-4 with few-shot chain-of-thought prompting reaches 39% accuracy. Experts score 65% accuracy, rising to 74% when accounting for clear mistakes"
  - [section 4] "GPT-4 with few-shot chain-of-thought prompting does better, between 38% and 40% on the three sets"
  - [corpus] The paper "Resurrecting saturated LLM benchmarks" studies GPQA specifically, confirming its relevance to LLM evaluation

## Foundational Learning

- Concept: Domain expertise validation
  - Why needed here: Ensures questions are objective and grounded in real expert knowledge
  - Quick check question: Why require both expert writers and validators for each question?

- Concept: Difficulty calibration through non-expert testing
  - Why needed here: Confirms questions are genuinely difficult for non-experts, not just experts
  - Quick check question: How does GPQA ensure questions are "Google-proof"?

- Concept: Multi-stage validation pipeline
  - Why needed here: Creates redundancy to catch subjective or ambiguous questions
  - Quick check question: What happens if the second expert validator disagrees with the question writer?

## Architecture Onboarding

- Component map: Question writing → Expert validation (2 rounds) → Non-expert validation → Dataset curation → Baseline evaluation
- Critical path: Expert validation rounds are the bottleneck; questions must pass both expert validations to be included
- Design tradeoffs: High-quality questions require significant expert time (estimated $95/hour compensation) vs. larger but lower-quality datasets
- Failure signatures: Questions with high non-expert accuracy (>50%) or low expert agreement (<50%) are filtered out
- First 3 experiments:
  1. Replicate the non-expert validation setup to verify the 34% accuracy claim
  2. Test GPT-4 performance with different prompting strategies on the full dataset
  3. Analyze the distribution of question types that experts vs. non-experts get wrong

## Open Questions the Paper Calls Out

- Question: How can we develop scalable oversight methods that enable humans to supervise AI outputs on questions where supervisors cannot produce or verify the truth themselves?
- Basis in paper: [explicit] The paper states "we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable."
- Why unresolved: This is a fundamental challenge in AI alignment - creating methods to reliably supervise AI systems that surpass human capabilities on specialized tasks where humans cannot directly verify the correctness of the outputs.
- What evidence would resolve it: Development and validation of new scalable oversight protocols through experiments on challenging datasets like GPQA, demonstrating that non-experts can reliably extract truthful information from superhuman AI systems.

## Limitations
- Benchmark relies heavily on PhD-level annotators, limiting scalability
- Question size (448 questions) may be insufficient for some statistical analyses
- GPT-4 evaluation results depend on specific prompting strategies that may not generalize

## Confidence
- Medium: The expert-driven design and multi-stage validation provide strong internal validity, but uncertainties remain about population representativeness and the Google-proof claim under different conditions.

## Next Checks
1. Replicate the non-expert validation with modified access conditions: Test whether the 34% baseline holds when restricting web access to pre-2023 content only, simulating a more realistic oversight scenario.
2. Cross-validate expert agreement rates: Have a subset of questions independently validated by experts from different institutions to verify the claimed high inter-rater reliability.
3. Test alternative AI prompting strategies: Systematically evaluate different chain-of-thought and few-shot approaches on the full dataset to establish a more robust baseline for AI performance.