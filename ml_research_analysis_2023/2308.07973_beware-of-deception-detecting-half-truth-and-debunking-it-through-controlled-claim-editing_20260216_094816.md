---
ver: rpa2
title: '"Beware of deception": Detecting Half-Truth and Debunking it through Controlled
  Claim Editing'
arxiv_id: '2308.07973'
source_url: https://arxiv.org/abs/2308.07973
tags:
- claim
- claims
- dataset
- have
- edited
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of detecting and debunking half-truths,
  which are deceptive statements containing some truth but omitting crucial details.
  The authors propose a comprehensive pipeline consisting of a half-truth detection
  model and a claim editing model.
---

# "Beware of deception": Detecting Half-Truth and Debunking it through Controlled Claim Editing

## Quick Facts
- arXiv ID: 2308.07973
- Source URL: https://arxiv.org/abs/2308.07973
- Reference count: 11
- Primary result: T5-based controlled claim editing outperforms GPT2, RoBERTa, PEGASUS, and Tailor with average improvements of 82%, 57%, 42%, and 23% in disinfo-debunk scores, respectively.

## Executive Summary
This paper addresses the problem of detecting and debunking half-truths, which are deceptive statements containing some truth but omitting crucial details. The authors propose a comprehensive pipeline consisting of a half-truth detection model and a claim editing model. The half-truth detection model extends the LIAR-PLUS dataset by adding a shortened justification column using textual entailment, achieving an F1 score of 82%. The claim editing model utilizes the T5 model for controlled claim editing, making precise adjustments to select parts of a claim to transform half-true or false claims into truthful statements.

## Method Summary
The methodology extends the LIAR-PLUS dataset by creating a shortened justification column using textual entailment scores from an NLI model. A BERT-based classifier is trained on this augmented dataset for half-truth detection. For claim editing, the TAPACO paraphrase dataset is augmented with SRL tags and masked claims, then used to fine-tune a T5 model. The pipeline identifies deceptive or contradicting segments of claims, masks them, and uses the T5 model to reconstruct the claim with minimal edits, guided by the SRL-tagged evidence.

## Key Results
- Half-truth detection model achieves an F1 score of 82% on the LIAR-PLUS-PLUS dataset.
- T5-based claim editing model outperforms other Language Models (GPT2, RoBERTa, PEGASUS, Tailor) with average improvements of 82%, 57%, 42%, and 23% in disinfo-debunk scores, respectively.
- The methodology achieves an average BLEU score of 0.88 and a disinfo-debunk score of 85% on edited claims.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Textual entailment between claim and evidence is the core discriminative signal for half-truth detection.
- Mechanism: The system leverages an NLI model to score each sentence in the claim against the evidence. Entailment scores are mapped to three labels: entailment for true claims, contradiction for false claims, and neutral for half-truths. By extracting only the top-scoring entailment or contradiction sentence and the top-scoring neutral sentence, the model focuses on the most discriminative evidence.
- Core assumption: A half-truth is fundamentally a lie of omission—it is true but incomplete—so it neither fully entails nor contradicts the full evidence.
- Evidence anchors:
  - [abstract]: "True and mostly-true claims typically have supporting text, which aligns with the entailment label. False and pants-fire claims, on the other hand, tend to have text that contradicts them, similar to a contradiction label. Half-true and barely-true claims often contain text that mentions hidden information or the deceptive aspect of the claim, which cannot be directly entailed or contradicted and thus corresponds to a neutral label."
  - [section]: "By extracting solely the relevant information from the justification, we have successfully reduced the complexity of the model."
- Break condition: If the NLI model misclassifies neutral sentences as entailment or contradiction, or if the evidence is too vague to yield strong entailment signals, detection accuracy drops sharply.

### Mechanism 2
- Claim: Controlled claim editing works by masking and reconstructing only the deceptive or missing parts of a half-true or false claim.
- Mechanism: The pipeline first identifies which segments of a claim contradict or are dissimilar to the evidence. These segments are masked. An SRL-tagged evidence header is prepended to the masked claim and fed to a T5 model trained on the TAPACO dataset to reconstruct the claim. The model fills only the masked slots, ensuring minimal edits.
- Core assumption: The deceptive portion of a half-truth is localized and can be isolated via entailment contradiction or low similarity.
- Evidence anchors:
  - [abstract]: "Our approach utilizes the T5 model for controlled claim editing; 'controlled' here means precise adjustments to select parts of a claim."
  - [section]: "By utilizing an NLI model, we calculate scores indicating the degree of contradiction. If no contradictory segments are found, we mask the segment that exhibits lower similarity with the counter."
- Break condition: If the masking algorithm fails to isolate the deceptive segment, the T5 model will generate edits that are either too broad or miss the critical misinformation.

### Mechanism 3
- Claim: The TAPACO dataset augmented with SRL tags teaches the T5 model to preserve context while editing.
- Mechanism: The T5 model is trained to reconstruct original sentences from paraphrased inputs, with SRL tags providing structural context. This encourages the model to learn how to edit minimally by focusing on the semantic roles and filling only the masked tokens.
- Core assumption: SRL tags help the model learn which parts of a sentence can be changed without altering the overall meaning.
- Evidence anchors:
  - [section]: "We have given the SRL tags of the paraphrased sentence and the original sentence with a few masked tokens as input to the T5 and expect the original sentence as output."
  - [section]: "With this idea of using SRL tags to perturb and edit sentences have been used by Ross et al. (2022)."
- Break condition: If the SRL tagger produces noisy or incomplete tags, the T5 model will struggle to learn precise editing boundaries.

## Foundational Learning

- Concept: Textual entailment and NLI classification
  - Why needed here: Detecting whether a claim supports, contradicts, or is neutral to evidence is the core discriminative task for half-truth detection.
  - Quick check question: If a claim states "Vaccines are effective" and the evidence says "Vaccines reduce disease incidence by 90%", what entailment label should the model assign?
- Concept: Semantic Role Labeling (SRL)
  - Why needed here: SRL tags provide a structural map of a sentence, enabling the model to identify which tokens can be edited while preserving grammatical roles and semantics.
  - Quick check question: In "The government provided masks to the public," what is the ARG0, ARG1, and V according to SRL?
- Concept: Controlled text generation and masking
  - Why needed here: To ensure that edits to claims are minimal and precise, not wholesale rewrites.
  - Quick check question: If a claim is "The storm caused damage," and the missing detail is "due to flooding," how should the model mask and edit the sentence?

## Architecture Onboarding

- Component map:
  - NLI model (BERT-based) -> Evidence extractor -> Half-truth detection classifier (BERT) -> SRL tagger -> TAPACO-trained T5 model -> Masking algorithm -> Claim filtering module
- Critical path: Evidence extraction -> half-truth detection -> controlled editing -> claim filtering
- Design tradeoffs:
  - Using textual entailment reduces noise but depends on the NLI model's accuracy.
  - Minimal editing preserves fluency but risks leaving residual misinformation.
  - SRL tags increase precision but require a robust tagger.
- Failure signatures:
  - High false positives in half-truth detection if NLI mislabels neutral sentences as entailment.
  - Edit failures if masking algorithm selects wrong segments.
  - Low BLEU scores if T5 edits too aggressively.
- First 3 experiments:
  1. Train NLI model on SNLI+MNLI and test on LIAR-PLUS-PLUS to confirm 91% F1 on entailment labels.
  2. Run masking algorithm on a small sample of half-true claims and manually verify that deceptive segments are masked.
  3. Fine-tune T5 on TAPACO with SRL tags and measure BLEU reconstruction accuracy before applying to real claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the half-truth detection model vary when using different NLI models or entailment scores for generating shortened justifications?
- Basis in paper: [explicit] The paper mentions using an NLI model to generate shortened justifications and achieve an F1 score of 82%.
- Why unresolved: The paper does not explore or compare the performance of the half-truth detection model with different NLI models or entailment score thresholds.
- What evidence would resolve it: Conducting experiments with various NLI models and entailment score thresholds, and comparing their impact on the F1 score of the half-truth detection model.

### Open Question 2
- Question: How does the claim editing model perform when applied to real-time half-truths from news articles instead of the LIAR-PLUS-PLUS dataset?
- Basis in paper: [inferred] The paper mentions developing a real-time evidence extraction module to facilitate the detection and debunking of disinformation.
- Why unresolved: The paper does not provide results or evaluation of the claim editing model on real-time half-truths from news articles.
- What evidence would resolve it: Testing the claim editing model on a dataset of real-time half-truths from news articles and evaluating its performance using metrics like BLEU score and disinfo-debunk score.

### Open Question 3
- Question: How does the performance of the claim editing model vary when using different masking strategies or SRL taggers for identifying the deceptive or contradicting parts of a claim?
- Basis in paper: [explicit] The paper mentions using an Allen AI SRL tag generator to extract SRL tags of counters and a masking algorithm based on textual entailment and cosine similarity.
- Why unresolved: The paper does not explore or compare the performance of the claim editing model with different masking strategies or SRL taggers.
- What evidence would resolve it: Conducting experiments with various masking strategies and SRL taggers, and comparing their impact on the performance of the claim editing model.

## Limitations
- The paper relies heavily on the accuracy of the NLI model for creating the shortened justification column, but the specific model architecture and training details are not provided, introducing uncertainty about reproducibility.
- The claim editing mechanism assumes that deceptive or missing segments can be isolated through contradiction detection and similarity scoring, but this may not generalize to more complex half-truths where the deception is distributed across multiple clauses.
- The disinfo-debunk score metric, while claimed to measure effectiveness of debunking, is not formally defined or validated against human judgments, making it difficult to assess the true impact of the edited claims.

## Confidence
- Half-truth detection mechanism (NLI-based): Medium
- Controlled claim editing pipeline (T5 + masking): Medium
- Dataset augmentation (TAPACO + SRL): Low
- Performance claims (BLEU, disinfo-debunk): Low

## Next Checks
1. Verify the NLI model's performance on a held-out validation set of claims and justifications from LIAR-PLUS-PLUS to ensure accurate entailment label assignment.
2. Conduct human evaluation of edited claims to measure whether the T5 model's minimal edits actually transform half-truths into truthful statements.
3. Test the claim filtering module's ability to select the best edited claim by comparing the disinfo-debunk scores of the top-3 ranked edits against human judgments.