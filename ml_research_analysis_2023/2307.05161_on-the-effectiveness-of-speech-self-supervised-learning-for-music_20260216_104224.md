---
ver: rpa2
title: On the Effectiveness of Speech Self-supervised Learning for Music
arxiv_id: '2307.05161'
source_url: https://arxiv.org/abs/2307.05161
tags:
- music
- speech
- audio
- tasks
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We explore the effectiveness of speech self-supervised learning
  (SSL) models for music representation learning. We adapt two prominent speech SSL
  models, data2vec and HuBERT, to music and refer to them as music2vec and musicHuBERT,
  respectively.
---

# On the Effectiveness of Speech Self-supervised Learning for Music

## Quick Facts
- arXiv ID: 2307.05161
- Source URL: https://arxiv.org/abs/2307.05161
- Reference count: 0
- We explore the effectiveness of speech self-supervised learning (SSL) models for music representation learning by adapting data2vec and HuBERT to music.

## Executive Summary
This paper investigates the adaptation of speech self-supervised learning models to music representation learning. By modifying two prominent speech SSL architectures (data2vec and HuBERT) to work with music data, the authors create music2vec and musicHuBERT models. Through extensive experimentation across 13 downstream MIR tasks using 12 different SSL models, the study demonstrates that pre-training with music data generally improves performance compared to speech-pretrained models, even when using speech-designed paradigms. However, the research also identifies limitations in existing speech-oriented designs, particularly in modeling polyphonic information.

## Method Summary
The authors adapt data2vec and HuBERT speech SSL models to music by replacing the training corpus while preserving the learning architecture. They train 12 SSL models with 95M parameters under various pre-training configurations using 1000 hours of music audio recordings. The pre-training paradigm involves masked input prediction where models learn to reconstruct either continuous latent representations (music2vec) or discrete pseudo-labels from K-means clustering (musicHuBERT). Downstream evaluation is performed on 13 MIR tasks including music tagging, key detection, genre classification, emotion regression, instrument classification, and beat tracking using frozen features with a simple MLP probe.

## Key Results
- Music-pretrained models generally outperform speech-pretrained models on most MIR tasks
- Music2Vec (continuous targets) shows better performance on tasks requiring fine-grained musical feature capture
- MusicHuBERT (discrete targets) performs well on tasks like music tagging but struggles with polyphonic tasks like key detection
- The number of clusters in K-means and masking strategies significantly impact downstream performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Speech SSL models can be adapted to music by replacing the training corpus while preserving the learning architecture.
- Mechanism: The pre-training paradigm (masked input â†’ target reconstruction) is modality-agnostic; changing from speech to music audio as input allows the model to learn universal audio features.
- Core assumption: Audio self-supervised objectives (continuous or discrete target prediction) are sufficiently general to capture both speech and music structure.
- Evidence anchors:
  - [abstract] "We explore the music adaption of SSL with two distinctive speech-related models, data2vec1.0 and Hubert"
  - [section 2] "Both Music2Vec and MusicHuBERT are annotation-free and utilise SSL techniques; their most common characteristic is the training task of 'reconstructing' information from masked inputs"
- Break condition: If the target reconstruction task is too speech-specific (e.g., phoneme-like discrete clusters), adaptation may fail to capture music-specific structures like harmony or rhythm.

### Mechanism 2
- Claim: Pre-training on music data yields better downstream MIR performance than pre-training on speech data, even when using the same model architecture.
- Mechanism: Music-specific pre-training allows the model to learn domain-relevant features (e.g., timbre, polyphony) that speech models lack, leading to superior performance on music tasks.
- Core assumption: Music and speech have sufficiently different acoustic and structural characteristics that domain-specific pre-training is beneficial.
- Evidence anchors:
  - [abstract] "Our findings suggest that training with music data can generally improve performance on MIR tasks, even when models are trained using paradigms designed for speech."
  - [section 4.1] "For the HuBERT model, the results pre-trained on speech recordings are comparable with SOTA on tasks like music tagging... but are surpassed by the results pre-trained on music audio on most of the downstream tasks"
- Break condition: If the downstream task is very speech-like (e.g., vocal pitch or lyrics), speech-pretrained models may still perform competitively.

### Mechanism 3
- Claim: The choice of SSL target (continuous vs discrete) significantly impacts MIR task performance.
- Mechanism: Continuous target prediction (Music2Vec) is more flexible for capturing complex musical features, while discrete targets (MusicHuBERT) may be limited by the clustering granularity and choice of features (e.g., MFCC).
- Core assumption: The richness of musical information requires either a flexible continuous representation or a sufficiently granular discrete representation.
- Evidence anchors:
  - [abstract] "However, we identify limitations of existing speech-oriented designs, especially in modelling polyphonic information."
  - [section 4.2] "The results on k-means for deep features are better than the vanilla MusicHuBERT besides genre classification on GTZAN... This implies the MFCCs features are good for modelling the human voice, regardless of speech or singing."
- Break condition: If the discrete target space is expanded or better matched to musical features (e.g., chroma instead of MFCC), the gap may narrow.

## Foundational Learning

- Concept: Self-supervised learning (SSL)
  - Why needed here: SSL allows training powerful feature extractors without labeled data, crucial for music where annotation is expensive.
  - Quick check question: Can you explain how masked prediction differs from contrastive learning in SSL?

- Concept: Transformer-based audio encoders (e.g., wav2vec2.0)
  - Why needed here: These architectures are the backbone of both data2vec and HuBERT, providing the model capacity for music representation.
  - Quick check question: What is the role of the CNN feature extractor in these models before the transformer layers?

- Concept: Downstream MIR tasks (classification, regression, tracking)
  - Why needed here: These tasks evaluate whether SSL-pretrained models capture musically relevant features.
  - Quick check question: Why is it important to evaluate on both local (e.g., pitch) and global (e.g., genre) MIR tasks?

## Architecture Onboarding

- Component map: 16 kHz waveform -> 1-D CNN tokenizer (50 Hz) -> Transformer (12 layers, 768 dim) -> Masked prediction head (continuous or discrete)

- Critical path:
  1. Mask input audio (probability ~65%, span ~10)
  2. Feed to tokenizer -> transformer
  3. Predict teacher target (continuous pooling or discrete cluster)
  4. Compute loss and update student model

- Design tradeoffs:
  - Continuous vs discrete targets: Flexibility vs computational efficiency in target prep
  - Masking strategy: Span length and probability affect local vs global feature learning
  - Cluster size (HuBERT): More clusters -> better resolution but higher computational cost

- Failure signatures:
  - Poor performance on polyphonic tasks -> likely insufficient harmonic modeling in SSL target
  - Beat tracking degradation with more transformer layers -> possible overfitting to local structure or ineffective temporal modeling
  - Large gap between speech- and music-pretrained models -> domain mismatch in SSL objective or data

- First 3 experiments:
  1. Ablation: Train MusicHuBERT with k=100 (speech-like) vs k=2000 clusters; compare key detection and genre classification
  2. Ablation: Replace MFCC with chroma features for HuBERT clustering; evaluate impact on harmonic tasks
  3. Ablation: Modify Music2Vec to predict only early transformer layers; assess effect on beat tracking vs pitch tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the number of clusters in K-means (for HuBERT) and the masking strategy (for data2vec) affect the performance of music SSL models on polyphonic tasks such as key detection and chord estimation?
- Basis in paper: [explicit] The paper discusses the impact of the number of clusters in K-means on the MusicHuBERT model and the masking strategy on the Music2Vec model. It suggests that the number of clusters should be larger for music due to the diversity of music notes, and the masking strategy should be carefully chosen for effective music pre-training.
- Why unresolved: The paper provides some insights but does not conduct a comprehensive study on the impact of these hyperparameters on polyphonic tasks.
- What evidence would resolve it: A systematic study varying the number of clusters and masking strategies, and evaluating their impact on polyphonic tasks.

### Open Question 2
- Question: Can curriculum learning or manually-designed features improve the training stability and performance of music SSL models?
- Basis in paper: [inferred] The paper suggests that it may not be easy for data2vec to jointly learn deeper features and that curriculum learning skills or manually-designed features might be needed to increase training stability.
- Why unresolved: The paper does not explore these potential solutions in detail.
- What evidence would resolve it: Experiments applying curriculum learning techniques or incorporating manually-designed features in the pre-training of music SSL models, and comparing their performance with standard approaches.

### Open Question 3
- Question: How does the batch size and audio length affect the performance of music SSL models?
- Basis in paper: [explicit] The paper suggests that batch size should be as diverse as possible and that shortening the length of audio can allow for an increase in batch size.
- Why unresolved: The paper does not provide a detailed study on the impact of batch size and audio length on model performance.
- What evidence would resolve it: Experiments varying batch size and audio length, and evaluating their impact on model performance across different downstream tasks.

### Open Question 4
- Question: Can music SSL models be improved by incorporating more harmonic information and diversity of music notes in the pre-training process?
- Basis in paper: [explicit] The paper identifies the limitations of existing speech SSL systems in modelling polyphonic information and suggests emphasising key or harmonic information in the pretext task for music SSL models.
- Why unresolved: The paper does not explore specific methods to incorporate more harmonic information and diversity of music notes.
- What evidence would resolve it: Experiments incorporating harmonic features or diverse music notes in the pre-training process, and comparing their performance with standard approaches.

## Limitations
- The study relies on a proprietary 1000-hour music dataset, limiting reproducibility and external validation
- Limited ablation studies on critical design choices like masking strategies and cluster numbers
- The paper identifies polyphonic modeling limitations but doesn't explore alternative feature representations (e.g., chroma) that might better capture harmonic structure

## Confidence
- High confidence: The general finding that music-pretrained models outperform speech-pretrained models on most MIR tasks is well-supported by consistent experimental results across 13 downstream tasks
- Medium confidence: The claim that discrete targets (MusicHuBERT) are less effective for polyphonic tasks is supported by observed performance drops in key detection, but lacks direct ablation studies on feature or clustering design
- Low confidence: The assertion that music2vec's continuous targets are inherently more flexible for capturing musical features is plausible but not rigorously tested against alternative discrete target designs

## Next Checks
1. Feature Ablation: Train MusicHuBERT with MFCC vs chroma features for clustering; evaluate on harmonic tasks (key detection, genre) to isolate the impact of feature choice on polyphonic modeling
2. Target Space Scaling: Compare MusicHuBERT with k=100 vs k=2000 clusters; assess whether increased granularity closes the performance gap with Music2Vec on tasks requiring fine-grained harmonic or timbral distinctions
3. Cross-Domain Transfer: Pre-train a model on speech data and fine-tune on music for a subset of tasks (e.g., vocal pitch, lyrics); compare to direct music-pretraining to quantify domain adaptation benefits