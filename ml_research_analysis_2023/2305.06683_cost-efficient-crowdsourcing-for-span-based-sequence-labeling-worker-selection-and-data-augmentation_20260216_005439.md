---
ver: rpa2
title: 'Cost-efficient Crowdsourcing for Span-based Sequence Labeling: Worker Selection
  and Data Augmentation'
arxiv_id: '2305.06683'
source_url: https://arxiv.org/abs/2305.06683
tags:
- worker
- annotations
- workers
- expert
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses worker selection in span-based sequence labeling
  tasks, where label interdependencies make standard approaches challenging. The authors
  propose a Combinatorial Multi-Armed Bandit (CMAB) method that uses span-level F1
  scores as feedback, combined with a cost-effective evaluation strategy that leverages
  majority voting with expert validation based on inter-annotator agreement (kappa
  threshold).
---

# Cost-efficient Crowdsourcing for Span-based Sequence Labeling: Worker Selection and Data Augmentation

## Quick Facts
- arXiv ID: 2305.06683
- Source URL: https://arxiv.org/abs/2305.06683
- Reference count: 40
- Achieves up to 100.04% of expert-only baseline F1 score while reducing expert annotation costs by up to 65.97%

## Executive Summary
This paper addresses worker selection in span-based sequence labeling tasks where label interdependencies make standard binary feedback approaches ineffective. The authors propose a Combinatorial Multi-Armed Bandit (CMAB) method that uses span-level F1 scores as feedback, combined with a cost-effective evaluation strategy that leverages majority voting with expert validation based on inter-annotator agreement (kappa threshold). To overcome small and imbalanced real datasets, they introduce a data augmentation method (SES: shifting, expanding, shrinking) that generates plausible annotation variations while preserving individual worker accuracy. Evaluated on CoNLL 2003 NER and Chinese OEI datasets, their approach achieves expert-level performance while significantly reducing annotation costs.

## Method Summary
The authors develop a cost-efficient crowdsourcing framework for span-based sequence labeling that combines CMAB worker selection with data augmentation and kappa-thresholded majority voting. The CMAB algorithm selects subsets of workers using span-level F1 scores as feedback, while the data augmentation method (SES) generates realistic annotation errors through shifting, expanding, and shrinking span boundaries. When crowd workers achieve high inter-annotator agreement (kappa > threshold), their consensus replaces expensive expert annotations. The framework is evaluated on two datasets showing it can achieve up to 100.04% of expert-only baseline F1 score while reducing expert annotation costs by up to 65.97%.

## Key Results
- Achieves up to 100.04% of expert-only baseline F1 score on tested datasets
- Reduces expert annotation costs by up to 65.97% while maintaining quality
- The Expert+MV evaluation metric consistently outperforms other methods across both CUCB and epsilon-greedy criteria
- Data augmentation successfully balances worker accuracy distributions and improves worker selection performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Span-level F1 scores provide reliable feedback for worker selection in sequence labeling tasks.
- Mechanism: Unlike simple binary feedback, span-level F1 captures label interdependencies by measuring overlap between predicted and ground truth spans at the token level.
- Core assumption: Expert annotations can serve as ground truth, or crowd consensus (measured by kappa) can substitute when agreement is high.
- Evidence anchors: [section 3.3] "Due to label inter-dependencies, simple binary feedback is not applicable on span-based sequence labeling tasks. We utilize the span-level F1 score evaluated by experts and crowd workers combined as the feedback signal..."
- Break condition: If kappa threshold is set too low, noisy crowd annotations will be accepted as ground truth, degrading feedback quality.

### Mechanism 2
- Claim: Data augmentation through shifting, expanding, and shrinking generates realistic annotation errors.
- Mechanism: These three operations simulate common human annotation mistakes by modifying span boundaries while preserving overall worker accuracy distribution.
- Core assumption: The augmented annotations follow the same error distribution as real human annotations.
- Evidence anchors: [section 3.4] "We perform these modifications on a span multiple times, generating new annotation spans with different F1 scores... we can imitate crowd annotations with different kinds of errors in practice."
- Break condition: If augmentation doesn't match real error patterns, worker selection algorithm will optimize for unrealistic annotation behavior.

### Mechanism 3
- Claim: Kappa-thresholded majority voting reduces expert annotation costs while maintaining quality.
- Mechanism: When crowd workers agree (kappa > threshold), their consensus replaces expensive expert annotations; otherwise, expert evaluation is used.
- Core assumption: High kappa scores indicate reliable crowd consensus that approximates expert quality.
- Evidence anchors: [section 3.3] "The choice is based on the well-known Fleiss' Kappa score Îº that can quantitatively evaluate the agreement of crowd workers... In that case, we aggregate Ai with MV and use the aggregated annotation as the ground truth of sentence si."
- Break condition: If kappa threshold is too high, expert costs remain high; if too low, noisy annotations contaminate ground truth.

## Foundational Learning

- Combinatorial Multi-Armed Bandit (CMAB):
  - Why needed here: Traditional multi-armed bandit algorithms select single workers, but this paper needs to select subsets of workers for parallel annotation.
  - Quick check question: What is the key difference between CMAB and standard MAB in terms of action space?

- Sequence labeling evaluation metrics:
  - Why needed here: Token-level and span-level F1 scores are needed to properly evaluate interdependent labels in NER and opinion expression tasks.
  - Quick check question: How does span-level F1 differ from token-level F1 in handling boundary errors?

- Inter-annotator agreement (kappa):
  - Why needed here: Kappa provides a quantitative measure to determine when crowd consensus can replace expert annotations.
  - Quick check question: What does a kappa score of 0.8 indicate about annotator agreement?

## Architecture Onboarding

- Component map: Data augmentation module -> Worker selection algorithm -> Evaluation framework -> Feedback simulator

- Critical path:
  1. Augment dataset to ensure sufficient annotations per worker
  2. Initialize worker F1 scores using expert evaluations
  3. Iteratively select worker subsets using CMAB criterion
  4. Collect annotations and evaluate using kappa threshold
  5. Update worker scores and repeat until convergence

- Design tradeoffs:
  - Kappa threshold vs. expert cost: Higher thresholds save more expert costs but risk accepting noisy consensus
  - Super-arm size vs. convergence speed: Larger subsets reduce iterations but increase variance in feedback
  - Augmentation complexity vs. realism: More complex augmentations better simulate errors but are harder to validate

- Failure signatures:
  - Worker scores diverging from true accuracy (indicates poor augmentation)
  - High regret even after many iterations (indicates CMAB algorithm issues)
  - F1 scores plateauing below baseline (indicates kappa threshold too high)

- First 3 experiments:
  1. Run worker selection on augmented dataset with different kappa thresholds (0.0, 0.4, 0.8) and measure expert cost savings
  2. Compare CMAB algorithms (CUCB vs epsilon-greedy vs Thompson sampling) on same dataset
  3. Test feedback simulator to validate algorithm behavior without dataset limitations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed data augmentation method perform when applied to other sequence labeling tasks beyond named entity recognition and opinion expression identification?
- Basis in paper: The authors demonstrate their data augmentation method on CoNLL 2003 NER and Chinese OEI datasets, showing effectiveness for these specific tasks, but do not explore generalization to other sequence labeling tasks.
- Why unresolved: The paper focuses on validating the method on only two datasets without exploring its applicability to other sequence labeling domains.
- What evidence would resolve it: Testing the SES method on additional sequence labeling tasks such as part-of-speech tagging, semantic role labeling, or biomedical named entity recognition to evaluate its general effectiveness.

### Open Question 2
- Question: What is the impact of different kappa threshold values on the quality of the final annotated dataset across various task domains?
- Basis in paper: The authors discuss how kappa threshold values affect F1 scores and expert usage, but only provide specific results for Chinese OEI and CoNLL 2003 datasets.
- Why unresolved: The optimal kappa threshold appears to be domain-specific, and the paper does not provide guidance on how to determine appropriate thresholds for different types of tasks.
- What evidence would resolve it: Systematic analysis of kappa threshold effects across multiple task domains with varying difficulty levels to establish guidelines for threshold selection.

### Open Question 3
- Question: How would the worker selection algorithm perform in real-time online crowdsourcing scenarios with dynamic worker availability?
- Basis in paper: The authors conduct extensive offline simulations and mention budget limitations prevented real-time testing, suggesting uncertainty about online performance.
- Why unresolved: The current evaluation is based on offline simulation rather than real-time implementation, which may have different characteristics in terms of worker behavior and task dynamics.
- What evidence would resolve it: Deploying the algorithm on a live crowdsourcing platform and measuring performance metrics such as convergence speed, worker selection quality, and cost savings in real-time conditions.

### Open Question 4
- Question: What are the computational costs of the proposed algorithm when scaling to larger datasets and more complex tasks?
- Basis in paper: The paper mentions that the augmentation procedure takes about 2 hours on a specific computer setup, but does not provide comprehensive complexity analysis for larger-scale applications.
- Why unresolved: The computational requirements for data augmentation and worker selection are not thoroughly analyzed, particularly for scenarios with thousands of workers and millions of annotations.
- What evidence would resolve it: Detailed computational complexity analysis and empirical runtime measurements for various dataset sizes and task complexities to establish scalability boundaries.

## Limitations
- Data augmentation method lacks external validation against real human annotation errors
- Kappa threshold selection appears empirically tuned without theoretical justification
- CMAB algorithm assumes worker annotations are independent Bernoulli draws, which may not capture complex annotation patterns

## Confidence
- High confidence: The overall framework combining CMAB worker selection with kappa-thresholded majority voting is technically sound and well-grounded in established multi-armed bandit literature
- Medium confidence: The specific data augmentation operations (shifting/expanding/shrinking) are plausible but unverified against real annotation error patterns
- Medium confidence: The cost savings claims (up to 65.97% reduction) depend heavily on the chosen kappa threshold and may not generalize across different domains or task difficulties

## Next Checks
1. Compare augmented annotations against actual human annotation errors from a small expert-labeled validation set to verify SES operations generate realistic mistakes
2. Conduct ablation studies varying kappa thresholds systematically (0.2, 0.4, 0.6, 0.8) across multiple datasets to identify optimal thresholds and understand sensitivity
3. Implement cross-validation with held-out expert annotations to verify that the CMAB algorithm maintains performance when tested on truly unseen data rather than relying solely on augmented datasets