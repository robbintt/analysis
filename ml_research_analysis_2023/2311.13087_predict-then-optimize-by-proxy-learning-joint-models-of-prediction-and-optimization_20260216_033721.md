---
ver: rpa2
title: 'Predict-Then-Optimize by Proxy: Learning Joint Models of Prediction and Optimization'
arxiv_id: '2311.13087'
source_url: https://arxiv.org/abs/2311.13087
tags:
- optimization
- learning
- problem
- training
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Learning to Optimize from Features (LtOF),
  an alternative to End-to-End Predict-Then-Optimize (EPO) for combining machine learning
  with constrained optimization. While EPO trains predictive models by directly differentiating
  through the optimization problem, this approach can be inefficient and requires
  handcrafted backpropagation rules.
---

# Predict-Then-Optimize by Proxy: Learning Joint Models of Prediction and Optimization

## Quick Facts
- arXiv ID: 2311.13087
- Source URL: https://arxiv.org/abs/2311.13087
- Reference count: 14
- Key outcome: LtOF outperforms two-stage methods and is competitive with EPO while being significantly faster, especially when feature-to-parameter mappings are complex.

## Executive Summary
This paper proposes Learning to Optimize from Features (LtOF), an alternative approach to combining machine learning with constrained optimization that avoids the distributional shift problem of traditional End-to-End Predict-Then-Optimize (EPO) methods. Rather than predicting parameters and then solving the optimization problem, LtOF learns to directly predict optimal solutions from observable features using Learning-to-Optimize (LtO) methods. The approach is demonstrated to be faster than EPO while maintaining competitive performance on portfolio optimization, nonconvex QP, and AC-Optimal Power Flow problems.

## Method Summary
LtOF trains a joint model that maps observable features directly to optimal solutions, bypassing the parameter prediction step that causes distributional drift in EPO approaches. The method leverages existing LtO techniques (Lagrangian Dual Learning, Self-Supervised Primal-Dual Learning, Deep Constraint Completion and Correction) that are differentiable by design, eliminating the need for handcrafted backpropagation rules. The approach uses learned optimization proxies as fast approximations of optimization solvers, achieving significant efficiency gains while maintaining solution quality. Feature generation uses random neural networks with k layers mapping parameters to features, with experiments varying k to test complexity.

## Key Results
- LtOF outperforms two-stage methods and achieves competitive performance with EPO
- LtOF is significantly faster than EPO, especially when feature-to-parameter mappings are complex
- The approach successfully handles portfolio optimization, nonconvex QP, and AC-Optimal Power Flow problems
- Performance degrades with increasing feature generation complexity (higher k values)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LtOF avoids distribution shift by predicting optimal solutions directly from observable features instead of intermediate parameters.
- Mechanism: The joint model Jϕ(z) learns a direct mapping from features z to optimal solutions x⋆, bypassing the parameter prediction step that causes distributional drift.
- Core assumption: The LtO method can learn an effective mapping from features to solutions despite not observing true parameters directly.
- Evidence anchors: Abstract states optimal solutions are learned directly from observable features; section shows pretrained optimization proxies lead to poor accuracy due to inability to generalize outside training distribution.

### Mechanism 2
- Claim: LtOF achieves efficiency gains by using learned optimization proxies instead of solving optimization problems directly.
- Mechanism: The learned proxy Fω serves as a fast approximation of the optimization solver, eliminating the need to solve the optimization problem at each training iteration.
- Core assumption: The learned proxy can approximate the optimization solver with sufficient accuracy for the learning task.
- Evidence anchors: Abstract notes significant speed improvements especially when feature-to-parameter mappings are complex; methodology section explains the efficiency advantage.

### Mechanism 3
- Claim: LtOF is more flexible than traditional EPO because it can leverage existing LtO methods without requiring handcrafted backpropagation rules.
- Mechanism: The LtOF framework can adopt various LtO methods that are differentiable by design.
- Core assumption: The chosen LtO method is compatible with the optimization problem structure and can handle constraint satisfaction.
- Evidence anchors: Abstract describes the approach as generic and based on adapting Learning-to-Optimize paradigm; section explains that LtOF allows utilization of existing LtO methodologies without problem-specific backpropagation rules.

## Foundational Learning

- Concept: Distribution shift in machine learning
  - Why needed here: Understanding how LtOF addresses the distribution shift problem that plagues traditional EPO approaches
  - Quick check question: What causes distribution shift in the context of EPO, and how does LtOF avoid it?

- Concept: Lagrangian duality and constrained optimization
  - Why needed here: LtOF methods like Lagrangian Dual Learning rely on Lagrangian formulations to handle constraints
  - Quick check question: How does the Lagrangian Dual Learning method enforce constraints during training?

- Concept: Differentiable optimization layers
  - Why needed here: Understanding the trade-off between handcrafted backpropagation rules in EPO and the automatic differentiability of LtO proxies
  - Quick check question: What are the advantages and disadvantages of using differentiable optimization layers versus learned proxies?

## Architecture Onboarding

- Component map: Feature generator (Gk) -> LtOF model (Jϕ) -> Constraint restoration
- Critical path:
  1. Generate feature-parameter pairs using Gk
  2. Train LtOF model to predict optimal solutions from features
  3. Evaluate performance on test set
  4. Apply constraint restoration if necessary
- Design tradeoffs:
  - Proxy accuracy vs. inference speed
  - Direct feature-to-solution mapping vs. two-stage prediction-optimization
  - Complexity of LtO method vs. ease of implementation
- Failure signatures:
  - High constraint violations in predicted solutions
  - Poor generalization to unseen feature distributions
  - Slow inference times due to complex constraint restoration
- First 3 experiments:
  1. Simple convex QP with 2D features to verify basic LtOF functionality
  2. Portfolio optimization with synthetic features to test performance under controlled conditions
  3. Non-convex QP variant to evaluate LtOF's ability to handle non-convex optimization problems

## Open Questions the Paper Calls Out

- Question: How does the distributional shift between LtO proxy training and EPO prediction affect regret in more complex optimization problems?
- Question: What are the theoretical guarantees (if any) on the approximation quality of LtOF solutions compared to true optimal solutions?
- Question: How do LtOF methods perform when the feature-to-parameter mapping is non-invertible or many-to-one?
- Question: Can LtOF methods effectively incorporate combinatorial optimization problems, given their distinct training approaches?

## Limitations

- Performance heavily depends on the quality of the LtO proxy and feature generation mechanism
- Performance degrades with increasing feature generation complexity (higher k values)
- Limited ablation studies and edge case exploration

## Confidence

- Core claims: Medium-High
- Distribution shift avoidance: Medium-High
- Efficiency improvements: Medium-High
- Generalizability across problem types: Medium

## Next Checks

1. Systematically vary the complexity of feature generation (k) across more problem types to identify the breaking point where LtOF performance degrades significantly compared to EPO.

2. Train LtOF models on one problem instance (e.g., one AC-OPF case) and test on different instances to assess the robustness of the feature-to-solution mapping across varying problem structures.

3. Conduct a detailed analysis of constraint violations in LtOF predictions across all three experimental settings, particularly examining whether constraint restoration procedures adequately address feasibility issues in the learned solutions.