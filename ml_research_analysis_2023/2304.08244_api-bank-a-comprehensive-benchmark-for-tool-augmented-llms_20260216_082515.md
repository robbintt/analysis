---
ver: rpa2
title: 'API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs'
arxiv_id: '2304.08244'
source_url: https://arxiv.org/abs/2304.08244
tags:
- call
- user
- apis
- llms
- calls
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces API-Bank, a benchmark for evaluating tool-augmented
  large language models (LLMs). The authors construct a runnable evaluation system
  with 73 API tools and annotate 314 tool-use dialogues with 753 API calls to assess
  LLMs' capabilities in planning, retrieving, and calling APIs.
---

# API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs

## Quick Facts
- arXiv ID: 2304.08244
- Source URL: https://arxiv.org/abs/2304.08244
- Reference count: 1
- Key outcome: API-Bank benchmark shows GPT-3.5 improved tool utilization over GPT-3, GPT-4 excels in planning, and Lynx surpasses Alpaca by 26+ percentage points

## Executive Summary
This paper introduces API-Bank, a comprehensive benchmark for evaluating tool-augmented large language models (LLMs). The benchmark includes 73 API tools and 314 tool-use dialogues with 753 API calls, organized into three levels of complexity. The authors develop Lynx, a tool-augmented LLM trained on 1,888 tool-use dialogues, and demonstrate that it outperforms existing models like Alpaca while approaching GPT-3.5's effectiveness. The study identifies key challenges in API retrieval, planning, and execution that need to be addressed for future research.

## Method Summary
The API-Bank benchmark evaluates LLMs across three progressively complex levels: basic API calling, API retrieval, and multi-step planning. The system provides API descriptions without implementation details, requiring models to reason about tool usage from documentation alone. A ToolSearcher component helps models find relevant APIs, while a ToolManager handles execution and state management. The benchmark includes both evaluation and training datasets, with the training set comprising 1,888 tool-use dialogues from 2,138 APIs spanning 1,000 domains. Models are evaluated on accuracy, response quality, and planning efficiency using human testers and automated evaluation.

## Key Results
- GPT-3.5 shows improved tool utilization compared to GPT-3, while GPT-4 excels in planning tasks
- Lynx surpasses Alpaca's tool utilization performance by over 26 percentage points
- Human testers achieve 90% accuracy on level-1 tasks, 72% on level-2, and 50% on level-3 planning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Three-level evaluation structure enables fine-grained assessment of LLM tool use
- **Mechanism**: Dividing evaluation into planning, retrieving, and calling levels isolates specific failure modes
- **Core assumption**: Tool use capabilities can be meaningfully separated into distinct tasks
- **Evidence anchors**: [abstract] Three-level structure described; [section 4] Detailed explanation of each level's purpose
- **Break condition**: If tasks require simultaneous planning and retrieval, structure may miss integrated capabilities

### Mechanism 2
- **Claim**: Description-only approach forces generalizable tool-use reasoning
- **Mechanism**: Models must interpret API documentation without implementation details
- **Core assumption**: Models can generalize from descriptions to correct usage patterns
- **Evidence anchors**: [abstract] Models speculate on inputs/outputs without implementation knowledge; [section 3.4] Data synthesis includes manual validation
- **Break condition**: If detailed implementation knowledge is required for effective tool use

### Mechanism 3
- **Claim**: Iterative refinement loop enables handling of complex, multi-step tasks
- **Mechanism**: Models search, call, evaluate, and refine through multiple dialogue turns
- **Core assumption**: Iterative refinement is viable for overcoming tool-use failures
- **Evidence anchors**: [abstract] GPT-3.5-Turbo requires 9.9 rounds on average; [section 5.3] Refinement of ToolSearch queries
- **Break condition**: If multiple dialogue turns are prohibitive or models get stuck in loops

## Foundational Learning

- **Concept**: Understanding API descriptions and parameters
  - **Why needed here**: Models must interpret documentation to generate correct calls
  - **Quick check question**: Can you explain what an API parameter type means and how it affects the call format?

- **Concept**: Planning multi-step workflows
  - **Why needed here**: Level-3 tasks require breaking down complex requests into API sequences
  - **Quick check question**: Given "book flight and hotel for next week," can you outline the steps and APIs needed?

- **Concept**: Error handling and iterative refinement
  - **Why needed here**: Models must adjust approach based on API call failures
  - **Quick check question**: If an API returns an error about missing parameters, what steps would you take to fix it?

## Architecture Onboarding

- **Component map**: User request -> ToolSearcher -> API Pool -> ToolManager -> API execution -> Response evaluation -> User reply
- **Critical path**: 1. User request â†’ ToolSearcher keyword generation 2. API retrieval from API Pool 3. API call generation and execution 4. Response evaluation and user reply 5. Iterative refinement if needed
- **Design tradeoffs**: Description-only approach favors generalization but may limit accuracy; Three-level structure provides granularity but may miss integrated capabilities
- **Failure signatures**: Refusal to make API calls despite instructions; Generating non-existent API names; Incorrect parameter formats; Infinite refinement loops
- **First 3 experiments**: 1. Test GPT-3.5-Turbo on level-1 tasks with explicit prompts 2. Evaluate ToolSearcher impact on level-2 performance 3. Compare human vs GPT-4 on level-3 planning

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can we improve LLM effectiveness in utilizing external tools, particularly API retrieval and planning?
- **Basis in paper**: Explicit - paper demonstrates current limitations and error analysis
- **Why unresolved**: Significant performance gaps remain despite improvements over baseline models
- **What evidence would resolve it**: New techniques showing measurable improvements over GPT-3.5 and GPT-4 baselines

### Open Question 2
- **Question**: What are the key factors limiting LLM ability to understand need for and correctly execute API calls?
- **Basis in paper**: Explicit - significant portion of errors due to failure to recognize when API calls are needed
- **Why unresolved**: Gap exists in model's understanding of when and how to use tools
- **What evidence would resolve it**: Identification and quantification of specific reasoning limitations leading to errors

### Open Question 3
- **Question**: How can we design more effective evaluation systems for Tool-Augmented LLMs?
- **Basis in paper**: Inferred - paper mentions need for more reasonable evaluation methods
- **Why unresolved**: Current metrics may not fully capture nuances of real-world tool utilization
- **What evidence would resolve it**: New evaluation metrics providing comprehensive assessment of tool utilization capabilities

## Limitations

- Evaluation granularity may not capture integrated nature of real-world tool use where planning, retrieval, and execution occur simultaneously
- Benchmark scope limited to 73 API tools across specific domains, potentially not representing full diversity of real-world API usage
- Human performance baseline may not reflect optimal performance due to task complexity and artificial evaluation setup

## Confidence

- **High Confidence**: Performance gap between GPT-3.5 and GPT-4 in planning tasks; Lynx's improvement over Alpaca
- **Medium Confidence**: Three-level evaluation structure provides meaningful granularity; Description-only approach forces generalizable reasoning
- **Low Confidence**: Claims about real-world generalization of benchmark results

## Next Checks

1. Design and implement integrated evaluation combining planning, retrieval, and execution to assess whether three-level structure captures full complexity
2. Expand evaluation to include APIs from domains not represented in current benchmark (finance, legal) to test cross-domain generalization
3. Conduct study where models interact with actual APIs in controlled environment to compare performance with API-Bank benchmark results