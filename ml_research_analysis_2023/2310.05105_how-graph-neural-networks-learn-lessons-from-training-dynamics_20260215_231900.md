---
ver: rpa2
title: 'How Graph Neural Networks Learn: Lessons from Training Dynamics'
arxiv_id: '2310.05105'
source_url: https://arxiv.org/abs/2310.05105
tags:
- learning
- graph
- matrix
- neural
- gnns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the training dynamics of graph neural networks
  (GNNs) in function space to understand how they learn. The authors find that the
  learning process of GNNs can be re-cast into a label propagation framework due to
  the graph inductive bias.
---

# How Graph Neural Networks Learn: Lessons from Training Dynamics

## Quick Facts
- arXiv ID: 2310.05105
- Source URL: https://arxiv.org/abs/2310.05105
- Reference count: 40
- Key outcome: GNN training dynamics in function space can be interpreted as label propagation along graph edges, quantified by "kernel-graph alignment"

## Executive Summary
This paper studies how Graph Neural Networks (GNNs) learn by analyzing their training dynamics in function space. The authors discover that gradient descent optimization of GNNs implicitly leverages graph structure to update learned functions, a phenomenon they call "kernel-graph alignment." This alignment explains GNN success on homophilic graphs and failure on heterophilic graphs. Based on these insights, they propose Residual Propagation (RP), a parameter-free algorithm that directly uses the graph adjacency matrix to update predictions, achieving performance comparable to GNNs while being significantly faster.

## Method Summary
The paper analyzes GNN training dynamics using Neural Tangent Kernel (NTK) theory in the overparameterized regime. It proposes Residual Propagation (RP) as a parameter-free algorithm that replaces the NTK matrix with the graph adjacency matrix to propagate residuals. RP computes normalized adjacency matrix, initializes residuals, and iteratively updates predictions using the adjacency matrix structure. The method is compared against standard GNN training procedures on benchmark datasets.

## Key Results
- Kernel-graph alignment quantifies how GNN training dynamics align with graph structure
- Better alignment leads to better generalization on homophilic graphs but worse performance on heterophilic graphs
- RP algorithm achieves comparable accuracy to GNNs while being orders-of-magnitude faster
- Theoretical analysis shows kernel-graph alignment emerges naturally in overparameterized regimes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GNN training dynamics in function space can be interpreted as label propagation along graph edges
- Mechanism: Gradient descent updates propagate residuals from training nodes to unseen nodes following a kernel matrix that aligns with the graph adjacency matrix
- Core assumption: In overparameterized regimes, the Neural Tangent Kernel (NTK) of GNNs naturally aligns with graph structure
- Evidence anchors: [abstract] "we find that the gradient descent optimization of GNNs implicitly leverages the graph structure to update the learned function, as can be quantified by a phenomenon which we call kernel-graph alignment"; [section 4.1] "the residual propagation process of GNNs also tends to follow the trajectory regulated by the graph, similar to the behavior of the RP algorithm"
- Break condition: When the graph has low homophily (high heterophily) causing misalignment between kernel and optimal kernel matrices

### Mechanism 2
- Claim: The alignment between NTK and graph adjacency explains both GNN success and failure patterns
- Mechanism: Better kernel-graph alignment leads to better kernel-target alignment when homophily is high, but worse alignment when heterophily is high
- Core assumption: The alignment between optimal kernel (indicating same labels) and graph adjacency quantifies homophily level
- Evidence anchors: [abstract] "This alignment explains why GNNs successfully generalize on homophilic graphs and fail on heterophilic graphs"; [section 5.1] "for homophilic graphs where A(A, Θ*) ↑ is naturally large, better kernel-graph alignment A(Θt, A) ↑ consequently leads to better kernel-target alignment"
- Break condition: When training dynamics cause NTK to diverge from graph structure despite initial alignment

### Mechanism 3
- Claim: Residual Propagation (RP) algorithm directly implements the graph-aligned dynamics without learnable parameters
- Mechanism: RP replaces NTK matrix with graph adjacency matrix to propagate residuals, achieving similar performance to trained GNNs
- Core assumption: The first step of RP is equivalent to label propagation, and subsequent steps adjust based on current predictions
- Evidence anchors: [abstract] "we propose a parameter-free algorithm called Residual Propagation (RP) that directly uses the graph adjacency matrix to update the learned function"; [section 3.1] "Drawing upon an analogy between the information propagation process in (5) induced by optimization, and message passing schemes between instances commonly seen in graph learning, we propose to replace the dense NTK matrix Θt with a sparse matrix"
- Break condition: When node features contain critical information that graph structure alone cannot capture

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) and its role in overparameterized neural networks
  - Why needed here: Understanding how NTK governs the training dynamics in function space is crucial for interpreting GNN learning behavior
  - Quick check question: What happens to the NTK matrix during training in the overparameterized regime?

- Concept: Label propagation algorithms and their connection to graph-based semi-supervised learning
  - Why needed here: RP algorithm is built upon label propagation principles, and understanding this connection is essential for grasping the proposed method
  - Quick check question: How does the standard label propagation algorithm update predictions at each iteration?

- Concept: Homophily and heterophily in graph-structured data
  - Why needed here: The paper's theoretical insights about GNN generalization heavily depend on the homophily level of the graph
  - Quick check question: What is the relationship between homophily level and the alignment between adjacency matrix and optimal kernel matrix?

## Architecture Onboarding

- Component map: Graph adjacency matrix A -> NTK matrix Θ -> Residual propagation mechanism -> Prediction updates -> Convergence
- Critical path: Graph structure → NTK alignment → Residual propagation → Prediction updates → Convergence
- Design tradeoffs: Using only graph structure (parameter-free) vs. incorporating node features (potentially better performance but increased complexity)
- Failure signatures: Poor performance on heterophilic graphs, failure to converge when adjacency matrix is not positive semi-definite
- First 3 experiments:
  1. Implement basic RP on Cora dataset with different powers of adjacency matrix (K values)
  2. Compare RP performance against GCN on Arxiv dataset
  3. Test RP with Gaussian kernel on Citeseer dataset to incorporate node features

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between kernel-graph alignment and generalization performance in overparameterized regimes, and how does this relationship depend on the specific graph structure and dataset characteristics?
- Basis in paper: The paper discusses kernel-graph alignment as a key factor in explaining the success of GNNs on homophilic graphs and their limitations on heterophilic graphs. It mentions a strong correlation between generalization and homophily level in overparameterized regimes.
- Why unresolved: The paper provides theoretical support for the connection between kernel-graph alignment and generalization but does not offer a precise quantitative relationship or explore how it varies with different graph structures and datasets.
- What evidence would resolve it: Conducting extensive experiments on diverse datasets with varying graph structures and homophily levels, and analyzing the relationship between kernel-graph alignment and generalization performance using statistical methods and visualization techniques.

### Open Question 2
- Question: How can the Residual Propagation (RP) algorithm be further improved to handle heterophilic graphs effectively, and what are the theoretical limitations of RP in this context?
- Basis in paper: The paper proposes RP as a simple and effective algorithm inspired by the learning dynamics insights of GNNs. However, it acknowledges that RP is theoretically sub-optimal on heterophilic graphs and explores its performance on such datasets.
- Why unresolved: The paper does not provide a comprehensive analysis of the limitations of RP on heterophilic graphs or suggest specific improvements to overcome these limitations.
- What evidence would resolve it: Investigating the performance of RP on a wider range of heterophilic datasets, exploring modifications to the algorithm to enhance its ability to handle heterophilic graphs, and analyzing the theoretical properties of RP in the context of heterophily.

### Open Question 3
- Question: What are the implications of the learning dynamics insights for the design of new GNN architectures and training strategies, and how can these insights be leveraged to improve the efficiency and effectiveness of GNNs?
- Basis in paper: The paper provides theoretical explanations for the emergence of kernel-graph alignment in the overparameterized regime and proposes RP as an example of leveraging these insights. It also discusses the potential for adapting the framework to other tasks and developing forward-only learning frameworks.
- Why unresolved: The paper does not offer specific guidelines for designing new GNN architectures or training strategies based on the learning dynamics insights, nor does it explore the potential for improving the efficiency and effectiveness of GNNs using these insights.
- What evidence would resolve it: Developing new GNN architectures and training strategies inspired by the learning dynamics insights, conducting experiments to evaluate their performance compared to existing methods, and analyzing the theoretical properties of these new approaches.

## Limitations
- The kernel-graph alignment framework assumes infinite-width networks and may not fully capture finite-width GNN behavior
- Analysis focuses primarily on node classification tasks, leaving questions about extension to other graph learning problems
- Practical applicability to real-world GNNs with finite width and complex architectures remains to be thoroughly validated

## Confidence
- **High** confidence in the theoretical framework for NTK-based analysis of GNN training dynamics
- **Medium** confidence in empirical validation across real-world datasets and architectures
- **Low** confidence in generalization to heterophilic graphs and non-classification tasks

## Next Checks
1. **Finite-width verification**: Test whether kernel-graph alignment predictions hold for realistic network widths (e.g., 2-8 layers, 16-256 hidden units) rather than just infinite-width limits.
2. **Heterophily stress test**: Systematically evaluate RP and NTK alignment across graphs with varying homophily levels (from 0.1 to 0.9) to identify the precise breaking point where graph structure becomes detrimental.
3. **Task generalization**: Apply the residual propagation framework to link prediction tasks on citation networks to verify whether the same alignment principles govern different graph learning objectives.