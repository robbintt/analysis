---
ver: rpa2
title: Investigating Agency of LLMs in Human-AI Collaboration Tasks
arxiv_id: '2305.12815'
source_url: https://arxiv.org/abs/2305.12815
tags:
- agency
- design
- designer
- chair
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a framework to measure and control Agency\
  \ in dialogue systems, which is the capacity to proactively shape events. The framework\
  \ defines four key features of Agency\u2014Intentionality, Motivation, Self-Efficacy,\
  \ and Self-Regulation\u2014and develops methods to assess these features in dialogue."
---

# Investigating Agency of LLMs in Human-AI Collaboration Tasks

## Quick Facts
- arXiv ID: 2305.12815
- Source URL: https://arxiv.org/abs/2305.12815
- Reference count: 16
- Primary result: Fine-tuned and prompted LLMs can express and be perceived as having higher Agency in dialogue compared to baselines.

## Executive Summary
This paper introduces a framework to measure and control Agency in dialogue systems, defined as the capacity to proactively shape events. The framework identifies four key features of Agency—Intentionality, Motivation, Self-Efficacy, and Self-Regulation—and develops methods to assess these features in dialogue. A new dataset of 83 human-human collaborative interior design conversations is collected and annotated for these Agency features. Experiments show that large language models (LLMs) can express Agency, but models that explicitly manifest strong Motivation, Self-Efficacy, and Self-Regulation are better perceived as highly agentive. Automatic and human evaluations demonstrate that models fine-tuned or prompted with Agency-related examples outperform baseline approaches in expressing and being perceived as having high Agency.

## Method Summary
The paper develops a framework for measuring and controlling Agency in dialogue systems based on Bandura's social-cognitive theory. It collects a dataset of 83 human-human collaborative interior design conversations, annotates them for four Agency features, and uses GPT-3 models to measure and generate dialogue with desired Agency levels. The methods include fine-tuning GPT-3 on the annotated dataset and using in-context learning with demonstration examples. The models are evaluated through automatic metrics (feature prediction accuracy) and human evaluations (perceived agency and satisfaction with design outcomes).

## Key Results
- Fine-tuned GPT-3 models generate dialogue perceived as more agentive than baseline models, particularly in Motivation and Self-Efficacy.
- Models that explicitly manifest strong Motivation, Self-Efficacy, and Self-Regulation are better perceived as highly agentive.
- Designers with higher agency over a design component are more likely to be satisfied with that component in the final design.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Agency in dialogue systems can be measured and controlled through explicit expression of four key features: Intentionality, Motivation, Self-Efficacy, and Self-Regulation.
- Mechanism: The paper builds a framework where each feature has defined levels (e.g., strong, moderate, none) and trains models to predict these features from conversational snippets. Models that generate higher levels of these features are perceived as more agentive.
- Core assumption: These four features comprehensively capture the concept of Agency as defined in social-cognitive theory and can be reliably annotated and predicted from dialogue.
- Evidence anchors:
  - [abstract] The framework defines four key features of Agency—Intentionality, Motivation, Self-Efficacy, and Self-Regulation—and develops methods to assess these features in dialogue.
  - [section 3] We characterize strong intentionality as expressing a clear preference... strong motivation as providing evidence in support of one's preference...
  - [corpus] Weak: No clear papers directly anchor this framework; related works focus on agency in creative writing or data analysis, not this specific feature set.
- Break condition: If the features are not predictive of perceived agency or cannot be reliably annotated/learned, the framework fails.

### Mechanism 2
- Claim: Fine-tuned GPT-3 models can generate dialogue that is perceived as more agentive than baseline models.
- Mechanism: The paper collects human-human conversations, extracts snippets, and annotates them for agency features. It then fine-tunes GPT-3 on these annotations and evaluates the generated dialogue through human evaluation.
- Core assumption: The fine-tuned model learns to express the desired agency features and this translates to higher perceived agency in human evaluations.
- Evidence anchors:
  - [abstract] Automatic and human evaluations demonstrate that models fine-tuned or prompted with Agency-related examples outperform baseline approaches in expressing and being perceived as having high Agency.
  - [section 8.3] We find that this model is perceived as more effective in Motivation and Self-Efficacy, likely due to better access to relevant demonstration examples.
  - [corpus] Weak: No direct corpus evidence; relies on paper's own evaluation results.
- Break condition: If the fine-tuned model does not improve perceived agency or the improvement is not consistent across different evaluations.

### Mechanism 3
- Claim: The degree of agency expressed in a dialogue affects the satisfaction with the final design outcome.
- Mechanism: The paper collects post-study questionnaires where designers rate their satisfaction with different design components. It then correlates these satisfaction ratings with the agency levels of the designers for those components.
- Core assumption: Designers with higher agency over a component are more likely to be satisfied with that component in the final design.
- Evidence anchors:
  - [section 6] We find that designers who are dissatisfied with a particular design component have less Agency over it.
  - [section 6] When a designer is dissatisfied, their Agency is 62.1% more likely to be low than to be high (42.7% vs. 26.3%; p < 0.05).
  - [corpus] Weak: Only indirect evidence from this single paper's data.
- Break condition: If satisfaction is not significantly correlated with agency levels or other factors are more predictive of satisfaction.

## Foundational Learning

- Concept: Social-cognitive theory of agency (Bandura, 2001)
  - Why needed here: Provides the theoretical foundation for defining agency and its key features in dialogue systems.
  - Quick check question: According to social-cognitive theory, what are the four features through which humans exercise agency?

- Concept: Annotation and labeling of conversational data
  - Why needed here: Essential for creating the dataset used to train and evaluate models for agency measurement and generation.
  - Quick check question: How were the agency levels (low, medium, high) determined for each designer in the conversational snippets?

- Concept: Fine-tuning language models on specific tasks
  - Why needed here: The main approach for creating models that can generate dialogue with desired agency features.
  - Quick check question: What is the difference between instruction-only prompting and fine-tuning a language model on a specific task?

## Architecture Onboarding

- Component map:
  Data collection -> Annotation pipeline -> Model training (GPT-3 fine-tuning and in-context learning) -> Evaluation (automatic and human)

- Critical path:
  1. Collect human-human conversations on interior design task
  2. Extract conversational snippets and annotate for agency features
  3. Train models to predict agency features from snippets
  4. Fine-tune GPT-3 to generate dialogue with desired agency features
  5. Evaluate models through automatic and human evaluations

- Design tradeoffs:
  - Using a narrow task (interior design) for data collection vs. broader applicability
  - Manual annotation vs. automatic labeling for training data
  - Fine-tuning vs. in-context learning for model adaptation

- Failure signatures:
  - Models fail to predict agency features accurately
  - Generated dialogue does not improve perceived agency in human evaluations
  - No correlation between agency levels and satisfaction with design outcomes

- First 3 experiments:
  1. Train a baseline GPT-3 model on the agency feature prediction task and evaluate its performance.
  2. Fine-tune GPT-3 on the collected dataset and compare its feature prediction performance to the baseline.
  3. Generate dialogue using the fine-tuned model and conduct a human evaluation to assess perceived agency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different amounts of Agency across the four features (Intentionality, Motivation, Self-Efficacy, and Self-Regulation) affect the quality and outcome of human-AI collaboration tasks?
- Basis in paper: [explicit] The paper discusses the importance of Agency in human-AI collaboration and the four features through which it is expressed. It also mentions that different individuals may have different preferences on the desired amount of Agency across these features.
- Why unresolved: The paper does not provide empirical evidence on how varying levels of Agency in each feature impact the quality and outcome of human-AI collaboration tasks.
- What evidence would resolve it: Experiments comparing the performance and satisfaction of human-AI teams with varying levels of Agency in each feature on a range of collaborative tasks.

### Open Question 2
- Question: How does the Agency framework developed in this paper generalize to domains beyond interior design?
- Basis in paper: [explicit] The paper states that the Agency-related constructs (e.g., Intentionality) may be associated with domain-independent pragmatic features and potentially permit adaptation to a variety of domains.
- Why unresolved: The paper only tests the framework on a single domain (interior design). It is unclear how well the framework generalizes to other domains with different types of tasks and interactions.
- What evidence would resolve it: Applying the Agency framework to multiple domains and evaluating its effectiveness in measuring and controlling Agency in those contexts.

### Open Question 3
- Question: How can the Agency framework be used to create more natural and effective dialogue systems that can adapt to the Agency preferences of individual users?
- Basis in paper: [explicit] The paper mentions that the measurements of Agency and its features may be used to control the level of Agency in dialogue systems since different individuals may have different preferences on the desired amount of Agency.
- Why unresolved: The paper does not provide a detailed approach for how to use the framework to create adaptive dialogue systems that can match the Agency preferences of individual users.
- What evidence would resolve it: Developing and evaluating dialogue systems that use the Agency framework to adapt their level of Agency based on user feedback and preferences.

## Limitations
- The dataset is limited to a single task (interior design) and may not capture the full range of agency expressions in other collaborative tasks.
- The annotation process for agency features relies on human judgment and may introduce biases or inconsistencies.
- The study focuses on the designer's perspective and does not consider the user's perception of agency in the collaboration.

## Confidence

- High confidence: The framework for defining and measuring Agency through the four features (Intentionality, Motivation, Self-Efficacy, and Self-Regulation) is well-grounded in social-cognitive theory and supported by the paper's analysis of human-human conversations.
- Medium confidence: The effectiveness of fine-tuning GPT-3 models to generate more agentive dialogue is demonstrated, but the improvements are modest and may not generalize to other domains or tasks.
- Low confidence: The correlation between designer agency and satisfaction with design outcomes is based on a small sample size (5 designers) and may be influenced by other factors not accounted for in the study.

## Next Checks

1. Replicate the study with a larger and more diverse set of collaborative tasks to assess the generalizability of the agency framework and measurement methods.
2. Conduct a more rigorous analysis of the factors influencing designer satisfaction, including potential confounding variables such as designer expertise or task complexity.
3. Investigate the user's perception of agency in human-AI collaborations and how it relates to their satisfaction with the final outcome.