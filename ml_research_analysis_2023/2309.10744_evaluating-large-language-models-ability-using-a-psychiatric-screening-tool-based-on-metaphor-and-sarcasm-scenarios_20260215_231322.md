---
ver: rpa2
title: Evaluating Large Language Models' Ability Using a Psychiatric Screening Tool
  Based on Metaphor and Sarcasm Scenarios
arxiv_id: '2309.10744'
source_url: https://arxiv.org/abs/2309.10744
tags:
- sarcasm
- language
- llms
- arxiv
- comprehension
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated large language models' (LLMs) understanding
  of metaphor and sarcasm using a standardized screening test designed for Asperger
  syndrome. Six LLMs with varying parameter counts were tested on a 10-question scenario-based
  assessment.
---

# Evaluating Large Language Models' Ability Using a Psychiatric Screening Tool Based on Metaphor and Sarcasm Scenarios

## Quick Facts
- arXiv ID: 2309.10744
- Source URL: https://arxiv.org/abs/2309.10744
- Reference count: 40
- Six LLMs tested on metaphor and sarcasm comprehension using psychiatric screening tool

## Executive Summary
This study evaluated large language models' understanding of metaphor and sarcasm using a standardized screening test designed for Asperger syndrome. Six LLMs with varying parameter counts were tested on a 10-question scenario-based assessment. Results showed that metaphor comprehension improved with model size, with GPT-4 achieving 4 out of 6 correct answers, comparable to neurotypical 8-10-year-old children. However, sarcasm comprehension remained poor across all models, with GPT-3.5 and GPT-4 scoring lower than children with Asperger syndrome. This suggests that current LLM training methods, focused on linguistic intelligence, are insufficient for grasping nuanced human communication requiring emotional intelligence.

## Method Summary
The study evaluated six LLMs (Dolly v2 12B, Llama 2 7B/13B/70B, GPT-3.5, GPT-4) using the Metaphor and Sarcasm Scenario Test (MSST), a 10-question assessment with 5 metaphor and 5 sarcasm scenarios. Each model received identical prompts in the format: instruction sentence, context explanation, and 5 multiple-choice options. Correct answer rates were calculated and compared against baseline scores from children (8-10 years old without mental retardation: 4.1/5 metaphor, 3.3/5 sarcasm; Asperger syndrome: 1.8/5 sarcasm).

## Key Results
- Metaphor comprehension scores increased with model parameter count
- GPT-4 achieved 4 out of 6 correct metaphor answers, comparable to neurotypical children
- All models performed poorly on sarcasm comprehension, scoring below children with Asperger syndrome
- GPT-3.5 and GPT-4 showed particularly weak sarcasm comprehension despite high linguistic capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger model parameters improve metaphor comprehension because scaling up training data and model capacity enhances linguistic intelligence, enabling better recognition of abstract conceptual mappings.
- Mechanism: Increased parameter count allows the model to learn richer linguistic representations, improving its ability to detect and process metaphorical relationships between concepts.
- Core assumption: Metaphor comprehension relies primarily on linguistic pattern recognition and semantic mapping, which scales with model size.
- Evidence anchors:
  - [abstract] "results divulged that, whereas their ability to comprehend metaphors has been improved with the increase of the number of model parameters"
  - [section] "In terms of metaphor comprehension, it was observed that the score increased with an escalation in the number of parameters of the LLMs"
  - [corpus] Weak - corpus neighbors discuss metaphor detection but don't directly support the scaling hypothesis for metaphor comprehension
- Break condition: If metaphor comprehension plateaus despite further parameter increases, suggesting other factors beyond pure scaling are needed.

### Mechanism 2
- Claim: Sarcasm comprehension requires emotional intelligence and theory of mind, which current LLM training methods don't effectively develop.
- Mechanism: Sarcasm relies on detecting incongruities between literal meaning and speaker's subjective judgment, requiring emotional context that standard language model training doesn't capture.
- Core assumption: Sarcasm comprehension is fundamentally different from metaphor comprehension and requires emotional intelligence rather than just linguistic intelligence.
- Evidence anchors:
  - [abstract] "sarcasm comprehension remained poor across all models" and "current LLM training methods, focused on linguistic intelligence, are insufficient for grasping nuanced human communication requiring emotional intelligence"
  - [section] "the comprehension of sarcasm necessitates emotional intelligence" and "sarcasm requires detecting the incongruity between the speaker's subjective judgment inferred from the context and the content of the speech"
  - [corpus] Weak - corpus neighbors discuss metaphor processing but don't address sarcasm or emotional intelligence
- Break condition: If models can be trained with specialized datasets to achieve good sarcasm comprehension, disproving that emotional intelligence is the limiting factor.

### Mechanism 3
- Claim: Instruction-based tuning methods may suppress the model's ability to infer human subjective judgment, which is critical for sarcasm comprehension.
- Mechanism: Fine-tuning on instruction-following datasets prioritizes task completion over nuanced social understanding, potentially removing the model's capacity to consider context-dependent meanings.
- Core assumption: The bias suppression inherent in instruction tuning conflicts with the subjective judgment needed for sarcasm detection.
- Evidence anchors:
  - [abstract] "most of recent LLMs, including those under scrutiny in this study, are obtained using instruction-based tuning methods" and "such tuning processes might impede the models' ability to gauge human subjective judgments"
  - [section] "it is conceivable that such tuning processes might impede the models' ability to gauge human subjective judgments—a critical aspect in comprehending sarcasm"
  - [corpus] Weak - no direct corpus support for instruction tuning effects on sarcasm comprehension
- Break condition: If instruction-tuned models can be modified to retain sarcasm comprehension without losing their instruction-following capabilities.

## Foundational Learning

- Concept: Theory of Mind
  - Why needed here: Understanding why sarcasm requires different cognitive processes than metaphors helps explain the divergent model performance
  - Quick check question: What cognitive ability allows humans to understand that others have different beliefs and intentions than their own?

- Concept: Conceptual Metaphor Theory
  - Why needed here: Provides framework for understanding how metaphors map between conceptual domains, explaining why larger models improve at metaphor tasks
  - Quick check question: According to Conceptual Metaphor Theory, what relationship exists between source and target domains in metaphorical expressions?

- Concept: Emotional Intelligence vs Linguistic Intelligence
  - Why needed here: Distinguishes the different cognitive requirements for metaphor vs sarcasm comprehension, explaining why parameter scaling helps one but not the other
  - Quick check question: How does emotional intelligence differ from linguistic intelligence in terms of processing figurative language?

## Architecture Onboarding

- Component map: Test scenario input → LLM inference → answer selection → scoring against ground truth
- Critical path: Feeding each LLM the same test scenarios and recording their answer selections
- Design tradeoffs: Standardized psychiatric tests provide valid human comparison but may not perfectly align with LLM evaluation needs; multiple model sizes show scaling effects but increase computational cost
- Failure signatures: Poor performance on sarcasm despite good metaphor scores indicates emotional intelligence gap; consistent wrong answers across models suggest fundamental architectural limitations
- First 3 experiments:
  1. Run MSST on smallest model (Dolly v2 12B) to establish baseline performance
  2. Test middle-sized models (Llama 2 variants) to observe parameter scaling effects
  3. Evaluate largest available models (GPT-3.5, GPT-4) to confirm upper bound of current capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can fine-tuning LLMs with emotional intelligence training data improve their sarcasm comprehension without compromising their instruction-following capabilities?
- Basis in paper: [explicit] The paper suggests that while current training methods improve linguistic intelligence, they inadequately foster emotional intelligence needed for sarcasm comprehension, and that supplementing with distinct training data could help.
- Why unresolved: The paper only speculates about this approach but doesn't test it. It also raises concerns about maintaining instruction-following while adding emotional intelligence training.
- What evidence would resolve it: Empirical studies comparing sarcasm comprehension in LLMs before and after fine-tuning with emotional intelligence datasets, while measuring instruction-following performance.

### Open Question 2
- Question: Does instruction-based tuning inherently limit LLMs' ability to infer human subjective judgment, or can this limitation be overcome while maintaining bias control?
- Basis in paper: [explicit] The paper suggests that instruction-based tuning methods might impede models' ability to gauge human subjective judgments needed for sarcasm comprehension because bias suppression constrains inferences about subjective judgment.
- Why unresolved: This is a theoretical concern based on the relationship between bias control and subjective judgment inference, but no empirical evidence is provided about whether this trade-off is necessary or can be mitigated.
- What evidence would resolve it: Experiments testing sarcasm comprehension in models with different levels of bias control and instruction-following tuning, to determine if there's a correlation between these factors.

### Open Question 3
- Question: Is the poor sarcasm comprehension in LLMs primarily due to weak central coherence (local vs. global processing), or are there other fundamental limitations in how LLMs process context?
- Basis in paper: [inferred] The paper draws parallels between LLM behavior and individuals with Asperger syndrome who exhibit weak central coherence, but acknowledges this as a limitation of the study and suggests the issue might be progressively addressed with future advancements.
- Why unresolved: The paper only presents this as a hypothesis based on behavioral similarities, without investigating the underlying mechanisms or testing whether this is the primary cause.
- What evidence would resolve it: Comparative studies examining whether techniques that improve global context processing in LLMs (like attention mechanisms or context window extensions) specifically improve sarcasm comprehension more than other language tasks.

## Limitations

- MSST test questions and answer choices are not fully disclosed, making independent verification difficult
- GPT-4 parameter count is not officially disclosed, introducing uncertainty into correlation analysis
- The study doesn't explore whether LLMs might excel at different aspects of metaphor and sarcasm comprehension than humans measure

## Confidence

- Claim: Larger models show better metaphor comprehension due to scaling effects
  - Confidence: Medium - experimental results show correlation but underlying mechanism could involve other factors
- Claim: Instruction-based tuning limits sarcasm comprehension by suppressing subjective judgment
  - Confidence: Medium - strong experimental results but mechanism remains speculative without direct testing
- Claim: Sarcasm requires emotional intelligence beyond current LLM capabilities
  - Confidence: Medium - consistent poor performance across models but human comparison may not account for fundamental differences in processing

## Next Checks

1. **Independent Replication**: Recreate the experiment using the full MSST test questions (once publicly available) with additional LLM models to verify whether the observed scaling pattern for metaphor comprehension holds across different model architectures and training approaches.

2. **Instruction Tuning Experiment**: Test whether instruction-tuned models specifically show worse sarcasm comprehension than their pre-trained counterparts, controlling for model size, to validate the proposed mechanism linking instruction tuning to sarcasm comprehension deficits.

3. **Alternative Task Format Validation**: Evaluate whether changing the test format (e.g., removing multiple-choice constraints or providing additional context) improves LLM performance on sarcasm tasks, helping determine if the poor performance reflects genuine comprehension limitations or format sensitivity.