---
ver: rpa2
title: Momentum Gradient-based Untargeted Attack on Hypergraph Neural Networks
arxiv_id: '2310.15656'
source_url: https://arxiv.org/abs/2310.15656
tags:
- attack
- hypergraph
- mghga
- hgnns
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new untargeted adversarial attack method
  called MGHGA (Momentum Gradient Hypergraph Attack) for Hypergraph Neural Networks
  (HGNNs). The key idea is to attack the node features before hypergraph modeling,
  rather than the hypergraph structure, to overcome the challenges of unstructured
  hypergraph data and continuous features.
---

# Momentum Gradient-based Untargeted Attack on Hypergraph Neural Networks

## Quick Facts
- arXiv ID: 2310.15656
- Source URL: https://arxiv.org/abs/2310.15656
- Reference count: 15
- Primary result: Improves attack performance by an average of 2% compared to baseline methods

## Executive Summary
This paper introduces MGHGA (Momentum Gradient Hypergraph Attack), a novel untargeted adversarial attack method specifically designed for Hypergraph Neural Networks (HGNNs). The key innovation is attacking node features before hypergraph modeling rather than the hypergraph structure itself, which overcomes the challenges of unstructured hypergraph data and continuous features. The method employs a momentum gradient mechanism to select attack features and uses different modification strategies for discrete and continuous datasets. Experiments on five benchmark datasets demonstrate that MGHGA achieves superior attack performance compared to existing methods.

## Method Summary
MGHGA proposes a white-box adversarial attack on HGNNs by modifying node features before hypergraph construction. The method uses a momentum gradient mechanism to select which features to attack, accumulating gradients over iterations to avoid local optima. For discrete features, it employs direct modification, while for continuous features, it uses sign gradient updates. The attack is implemented on a surrogate HGNN model, and the perturbations are evaluated on victim HGNN models using distance-based hypergraph modeling methods (HGNN-KNN and HGNN-ε).

## Key Results
- MGHGA improves attack performance by an average of 2% compared to baseline methods
- Successfully attacks both discrete and continuous feature datasets
- Demonstrates effectiveness across five benchmark datasets (Cora, Cora-ML, Citeseer, NTU, ModelNet40)
- Shows superiority over existing hypergraph attack methods like FGA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attacking node features before hypergraph modeling is more effective than attacking hyperedges
- Mechanism: By modifying features before hypergraph construction, the attack avoids the instability caused by different hypergraph structures generated from the same dataset under different modeling approaches
- Core assumption: Hypergraph structure is a function of both the dataset and the modeling approach, making hyperedge attacks unreliable
- Evidence anchors:
  - [abstract] "We use a surrogate model to implement the attack before hypergraph modeling"
  - [section] "To address the above challenges, in this paper, we propose an attack that is more applicable to HGNNs, namely MGHGA. MGHGA sets up the utility wider untargeted attack... we take a new perspective of attacking features before hypergraph modeling"
  - [corpus] Weak - no direct corpus evidence supporting this specific mechanism
- Break condition: If hypergraph modeling becomes deterministic or if feature modifications are easily filtered by preprocessing defenses

### Mechanism 2
- Claim: Momentum gradient accumulation prevents overfitting to local optima during feature selection
- Mechanism: The momentum gradient mechanism accumulates previous gradients to guide feature selection, helping avoid local optima that would be reached by greedy gradient-based approaches
- Core assumption: Pure gradient-based feature selection can get stuck in local optima and overfit to the attack model
- Evidence anchors:
  - [section] "To address the above problem, we propose a momentum gradient hypergraph attack. The momentum method is a technique to accelerate the gradient descent algorithm by accumulating velocity vectors along the gradient direction of the loss function during the iteration"
  - [abstract] "We use a momentum gradient mechanism to choose the attack node features in the feature selection module"
  - [corpus] Weak - no direct corpus evidence supporting this specific momentum mechanism
- Break condition: If momentum accumulation causes gradient explosion or if the decay factor is poorly tuned

### Mechanism 3
- Claim: Separate handling of discrete and continuous features maintains attack effectiveness across different data types
- Mechanism: MGHGA uses direct feature inversion for discrete features and sign gradient updates for continuous features, adapting the attack method to the feature type
- Core assumption: Different feature types require different perturbation strategies for effective attacks
- Evidence anchors:
  - [abstract] "we use two feature generation approaches (direct modification and sign gradient) to enable MGHGA to be employed on discrete and continuous datasets"
  - [section] "In the feature modification module, we use two feature generation approaches (direct modification and sign gradient) to enable MGHGA to be employed on discrete and continuous datasets"
  - [corpus] Weak - no direct corpus evidence supporting this specific dual-method approach
- Break condition: If feature type detection fails or if one method is consistently more effective across all datasets

## Foundational Learning

- Concept: Hypergraph Neural Networks vs Graph Neural Networks
  - Why needed here: Understanding the key differences between HGNNs and GNNs is crucial for grasping why the proposed attack method is necessary
  - Quick check question: What is the fundamental difference between how HGNNs and GNNs aggregate information from neighbors?

- Concept: Adversarial attacks on neural networks
  - Why needed here: The paper builds on existing knowledge of adversarial attacks, extending it from GNNs to HGNNs
  - Quick check question: What are the two main categories of adversarial attacks based on their goals?

- Concept: Gradient-based optimization and momentum methods
  - Why needed here: The momentum gradient mechanism is a core component of the proposed attack method
  - Quick check question: How does the momentum method help in avoiding local optima during optimization?

## Architecture Onboarding

- Component map:
  - Surrogate HGNN model (for gradient computation) -> Feature selection module (momentum gradient calculation) -> Feature modification module (discrete vs continuous handling) -> Hypergraph modeling component (distance-based methods like HGNN-KNN and HGNN-ε) -> Victim HGNN model (for evaluation)

- Critical path:
  1. Load hypergraph dataset
  2. Initialize surrogate HGNN model
  3. Compute gradients on current feature set
  4. Update momentum gradients
  5. Select feature with maximum momentum gradient
  6. Modify selected feature (discrete/continuous specific)
  7. Repeat until budget exhausted
  8. Evaluate attack effectiveness on victim HGNN

- Design tradeoffs:
  - White-box vs black-box attack: MGHGA requires full access to surrogate model parameters
  - Feature modification budget: Limited by ∆, balancing attack strength vs stealth
  - Momentum decay factor: Tradeoff between responsiveness and stability
  - Hypergraph modeling method: Different distance metrics affect attack stability

- Failure signatures:
  - Attack fails to degrade accuracy: Possible causes include poor momentum tuning or ineffective feature selection
  - Runtime exceeds expectations: Likely due to large dataset size or inefficient implementation
  - Model crashes during gradient computation: Could indicate numerical instability or incompatible data types

- First 3 experiments:
  1. Run MGHGA on Cora dataset with HGNN-KNN as both surrogate and victim, using default parameters
  2. Compare MGHGA performance with FGA baseline on Cora-ML dataset
  3. Test MGHGA on NTU dataset with continuous features to verify dual-method effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MGHGA compare when attacking hypergraph datasets with different feature dimensionalities?
- Basis in paper: [inferred] The paper shows MGHGA's effectiveness on datasets with different feature dimensions (Cora: 1433, Cora-ML: 2879, NTU: 4096+2048, ModelNet40: 4096+2048), but doesn't systematically analyze the relationship between feature dimensionality and attack performance.
- Why unresolved: The paper presents results across datasets with varying dimensions but doesn't explicitly test how performance scales with dimensionality.
- What evidence would resolve it: Experiments showing MGHGA's attack success rate on datasets with systematically varied feature dimensions while keeping other factors constant.

### Open Question 2
- Question: How does MGHGA's momentum gradient mechanism perform compared to other gradient-based optimization methods for feature selection in hypergraph attacks?
- Basis in paper: [explicit] The paper introduces momentum gradient as an improvement over greedy gradient approaches, stating it helps avoid local optima, but doesn't compare against other optimization methods like Adam or RMSprop.
- Why unresolved: Only momentum gradient and standard gradient methods are compared, leaving questions about whether other optimizers might perform better.
- What evidence would resolve it: Comparative experiments testing MGHGA's momentum gradient against other optimization algorithms (Adam, RMSprop, etc.) for feature selection in hypergraph attacks.

### Open Question 3
- Question: How robust is MGHGA against common defense mechanisms designed for hypergraph neural networks?
- Basis in paper: [explicit] The paper mentions that MGHGA filters anomalous features to avoid detection by "simple defense mechanisms" but doesn't test against any specific defense strategies.
- Why unresolved: The paper only mentions potential defenses without implementing or testing any specific countermeasures.
- What evidence would resolve it: Experiments testing MGHGA's effectiveness against established hypergraph defense mechanisms like feature denoising, hypergraph reconstruction, or adversarial training.

## Limitations
- The paper assumes hypergraph structure instability is the primary bottleneck for existing attacks, but this mechanism is weakly supported by corpus evidence
- The dual-method approach for discrete vs continuous features lacks comparative analysis showing why this specific combination outperforms alternatives
- The momentum gradient implementation details are sparse, making it difficult to assess whether the observed improvements are due to momentum or other factors

## Confidence
- Mechanism 1: Medium - Limited corpus evidence supporting hypergraph structure instability as the primary attack bottleneck
- Mechanism 2: Medium - No direct corpus evidence supporting the specific momentum mechanism, though momentum methods are generally well-established
- Mechanism 3: Medium - The dual-method approach is reasonable but lacks comparative analysis with alternative strategies

## Next Checks
1. Conduct ablation studies removing the momentum component to isolate its contribution to attack performance
2. Test attack effectiveness when hypergraph modeling is constrained to produce consistent structures across runs
3. Compare MGHGA against a baseline that attacks features after hypergraph modeling to validate the pre-modeling advantage claim