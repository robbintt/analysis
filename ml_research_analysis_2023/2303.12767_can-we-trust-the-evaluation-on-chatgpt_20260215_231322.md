---
ver: rpa2
title: Can we trust the evaluation on ChatGPT?
arxiv_id: '2303.12767'
source_url: https://arxiv.org/abs/2303.12767
tags:
- chatgpt
- data
- performance
- arxiv
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper raises concerns about data contamination in ChatGPT evaluations,
  using stance detection as a case study. It argues that ChatGPT's closed nature and
  continuous training via RLHF make it impossible to verify if evaluation datasets
  have leaked into its training data.
---

# Can we trust the evaluation on ChatGPT?

## Quick Facts
- arXiv ID: 2303.12767
- Source URL: https://arxiv.org/abs/2303.12767
- Reference count: 31
- Primary result: ChatGPT's stance detection performance improvements cannot be definitively attributed to genuine model enhancement versus data contamination.

## Executive Summary
This paper raises critical concerns about data contamination in ChatGPT evaluations, using stance detection as a case study. The authors argue that ChatGPT's closed nature and continuous training via Reinforcement Learning from Human Feedback (RLHF) make it impossible to verify if evaluation datasets have leaked into its training data. Through comparing ChatGPT's performance on stance detection datasets (SemEval 2016 Task 6 and P-stance) across different versions, they observe an overall improvement in performance. However, they cannot definitively attribute this improvement to either data leakage or genuine model enhancement, highlighting the challenge of ensuring fair model evaluation for closed, continuously trained models.

## Method Summary
The study evaluates ChatGPT's performance on stance detection tasks using zero-shot learning with test sets from SemEval 2016 Task 6 and P-stance datasets. The authors compare different versions of ChatGPT (V1, Jan 30, Feb 13) using the same prompts as previous work. Performance is measured using micro-F1 and macro-F1 scores, with stance labels manually extracted from ChatGPT responses. The evaluation methodology follows established stance detection benchmarks while investigating potential data contamination concerns.

## Key Results
- ChatGPT shows overall improvement in stance detection performance across versions
- Performance improvement coincides with published evaluation results, suggesting potential exposure to test data
- Cannot definitively attribute performance changes to either genuine model enhancement or data contamination
- Highlights fundamental challenges in evaluating closed, continuously trained models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data contamination in ChatGPT evaluations is possible because the model is trained on web data and user queries, including evaluation datasets.
- Mechanism: Continuous training with Reinforcement Learning from Human Feedback (RLHF) uses user-submitted prompts and outputs, potentially including test data.
- Core assumption: ChatGPT's training data includes web-crawled data that may contain existing NLP benchmarks and user-submitted evaluation prompts.
- Evidence anchors:
  - [abstract] The paper highlights data contamination in ChatGPT evaluations and the closed nature of the model.
  - [section] "it is impossible to ascertain the lack of data leakage, especially for the datasets that have been on the internet."
  - [corpus] Found 25 related papers with average neighbor FMR=0.413, indicating significant research on data contamination in LLMs.

### Mechanism 2
- Claim: The improvement in ChatGPT's stance detection performance could be attributed to data leakage rather than genuine model enhancement.
- Mechanism: Performance improvement between versions coincides with published evaluation results, suggesting potential exposure to test data.
- Core assumption: The improvement in performance is not solely due to better model architecture or training methods.
- Evidence anchors:
  - [abstract] "we see an overall improvement in the performance before and after the publication of the stance detection evaluation paper [25]."
  - [section] "we do see an overall improvement in the performance before and after the publication of the stance detection evaluation paper [25]."
  - [corpus] Related papers like "Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models" support this mechanism.

### Mechanism 3
- Claim: The closed nature of ChatGPT makes it impossible to verify whether evaluation datasets have leaked into training data.
- Mechanism: Lack of transparency in training data and continuous updates prevents independent verification of data contamination.
- Core assumption: Without access to training data, researchers cannot definitively prove or disprove data leakage.
- Evidence anchors:
  - [abstract] "given that the ChatGPT's training datasets are unknown and that ChatGPT is constantly updated... it is impossible to ascertain the lack of data leakage"
  - [section] "it is also impossible to rule out the possibility of contamination."
  - [corpus] Papers like "Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs" discuss similar issues.

## Foundational Learning

- Concept: Data contamination in machine learning
  - Why needed here: Understanding how training data contamination affects model evaluation is crucial for interpreting ChatGPT's performance.
  - Quick check question: What is the difference between data leakage and data contamination in machine learning contexts?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: ChatGPT's continuous training process relies on RLHF, which may include evaluation data from user queries.
  - Quick check question: How does the RLHF process potentially contribute to data contamination in ChatGPT?

- Concept: Zero-shot learning evaluation
  - Why needed here: The paper evaluates ChatGPT's performance in a zero-shot setting, which is compromised if the model has seen evaluation data.
  - Quick check question: What are the implications of data contamination on the validity of zero-shot learning evaluations?

## Architecture Onboarding

- Component map: User query -> ChatGPT response -> human feedback/ranking -> reward model update -> model fine-tuning via PPO
- Critical path: User query → ChatGPT response → human feedback/ranking → reward model update → model fine-tuning via PPO
- Design tradeoffs: Closed model provides commercial advantages but limits research transparency; continuous training improves performance but risks data contamination.
- Failure signatures: Inconsistent performance across similar tasks, suspiciously high scores on known benchmarks, lack of improvement on new datasets.
- First 3 experiments:
  1. Compare ChatGPT's performance on known benchmarks before and after their public release to detect potential contamination.
  2. Test ChatGPT's performance on datasets that have never been publicly available to establish a baseline for genuine capabilities.
  3. Implement a controlled experiment where evaluation data is deliberately leaked to see if performance improves, validating the contamination hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we definitively determine whether ChatGPT's performance improvements on stance detection tasks are due to data leakage or genuine model enhancements?
- Basis in paper: [explicit] The authors note that while they observe an overall improvement in ChatGPT's performance on stance detection datasets across different versions, they cannot definitively attribute this improvement to either data leakage or genuine model enhancement.
- Why unresolved: The closed nature of ChatGPT and its continuous training via RLHF make it impossible to verify if evaluation datasets have leaked into its training data. Without access to the model's training data and processes, researchers cannot conclusively determine the source of performance improvements.
- What evidence would resolve it: Access to ChatGPT's training data and processes would allow researchers to verify if evaluation datasets were used during training. Alternatively, a controlled experiment where ChatGPT is tested on entirely new, unseen datasets could help determine if the performance improvements are consistent across different tasks.

### Open Question 2
- Question: How can we develop mechanisms to prevent data contamination in evaluations of closed, continuously trained models like ChatGPT?
- Basis in paper: [explicit] The authors argue that "in order to ensure the fair evaluability of the models, we argue that the model creators should (1) pay closer attention to the training datasets and document potential data contamination, (2) create mechanisms through which the training datasets and models can be scrutinized regarding data leakage, and (3) build systems that can prevent data contamination from user inputs."
- Why unresolved: Current evaluation practices for closed models like ChatGPT do not have robust mechanisms to prevent data contamination or verify the absence of data leakage. The continuous training process and closed nature of these models make it challenging to ensure fair evaluation.
- What evidence would resolve it: Development and implementation of standardized protocols for evaluating closed models that include measures to prevent data contamination, mechanisms for transparency in training data and processes, and systems to monitor and prevent data leakage during continuous training.

### Open Question 3
- Question: What is the long-term impact of data contamination on the reliability of evaluations for large language models?
- Basis in paper: [inferred] The authors' concerns about data contamination and its potential to boost apparent performance suggest that this issue could have broader implications for the reliability of evaluations across various NLP tasks and models.
- Why unresolved: As more large language models are developed and adopted, the potential for data contamination increases. However, the full extent of its impact on evaluation reliability across different tasks, models, and domains remains unclear.
- What evidence would resolve it: Comprehensive studies examining the prevalence and impact of data contamination across multiple large language models, tasks, and evaluation datasets. This could include developing methods to detect and quantify the effects of data contamination on model performance and creating guidelines for interpreting evaluation results in light of potential contamination.

## Limitations
- Cannot definitively prove or disprove data contamination due to ChatGPT's closed nature
- Manual extraction of stance labels introduces potential human bias and inconsistency
- Performance improvements could be due to either genuine model enhancement or data leakage, but source cannot be conclusively determined

## Confidence
- High confidence: The existence of data contamination concerns in LLM evaluations is well-established in the literature
- Medium confidence: The observed performance improvements are real but their source is uncertain
- Low confidence: Any specific attribution of performance changes to contamination versus genuine improvement

## Next Checks
1. Implement a controlled contamination experiment by deliberately exposing ChatGPT to evaluation data and measuring performance changes
2. Compare ChatGPT's performance on both publicly available and never-released datasets to establish baseline capabilities
3. Develop statistical tests to distinguish between genuine zero-shot learning and contamination-influenced responses