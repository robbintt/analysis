---
ver: rpa2
title: Tackling Vision Language Tasks Through Learning Inner Monologues
arxiv_id: '2308.09970'
source_url: https://arxiv.org/abs/2308.09970
tags:
- language
- arxiv
- training
- visual
- inner
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Inner Monologue Multi-Modal Optimization (IMMO),
  a novel approach to solve complex vision language tasks by simulating the inner
  monologue cognitive process. IMMO enables Large Language Models (LLMs) and Vision-Language
  Models (VLMs) to interact through natural language conversations, where the LLM
  acts as a reasoner and the VLM as an observer.
---

# Tackling Vision Language Tasks Through Learning Inner Monologues

## Quick Facts
- arXiv ID: 2308.09970
- Source URL: https://arxiv.org/abs/2308.09970
- Reference count: 10
- The paper introduces Inner Monologue Multi-Modal Optimization (IMMO), a novel approach to solve complex vision language tasks by simulating the inner monologue cognitive process.

## Executive Summary
The paper introduces Inner Monologue Multi-Modal Optimization (IMMO), a novel approach to solve complex vision language tasks by simulating the inner monologue cognitive process. IMMO enables Large Language Models (LLMs) and Vision-Language Models (VLMs) to interact through natural language conversations, where the LLM acts as a reasoner and the VLM as an observer. The method uses a two-stage training process involving supervised fine-tuning and reinforcement learning to learn how to perform inner monologue, i.e., self-asking questions and answering questions. The approach is evaluated on two popular tasks: Commonsense Visual Question Answering (VQA) and Visual Entailment (VE). IMMO achieves competitive results compared to hybrid integration approaches while using significantly less training data and providing greater interpretability compared to embedding alignment approaches.

## Method Summary
IMMO uses a two-stage training process. First, high-quality inner monologue conversational data is used for supervised fine-tuning (SL) of both the Reasoner (Vicuna-7b LLM) and the Observer (BLIP-2 VLM). Second, reinforcement learning (RL) with PPO is used for further optimization, employing an alternating training strategy where only one model is updated per epoch while the other remains frozen. The framework enables multi-turn question-answering conversations between the Reasoner and Observer, allowing the Reasoner to gather information through targeted queries to solve vision-language problems.

## Key Results
- IMMO achieves competitive accuracy on Commonsense VQA (ScienceQA) and Visual Entailment (SNLI-VE) tasks
- Uses significantly less training data compared to embedding alignment approaches
- Provides greater interpretability through language-based intermediate representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The alternating reinforcement learning training strategy prevents instability when jointly updating two interacting models.
- Mechanism: In each training epoch, only one model (either the Reasoner or the Observer) is treated as the active policy network and updated via PPO, while the other remains frozen as the environment model. This alternation ensures that neither model dominates or destabilizes the training process.
- Core assumption: Jointly updating both models simultaneously would lead to training instability due to the non-stationary environment each creates for the other.
- Evidence anchors:
  - [section] "To alleviate the aforementioned underperformed collaboration problem, IMMO uses a two-stage training process. First, high-quality inner monologue conversational data is used for supervised fine-tuning (SL) of both the Reasoner and the Observer. Second, reinforcement learning (RL) is used for further optimization."
  - [section] "We use the alternating training strategy to prevent issues that may arise from updating two models simultaneously, such as the imbalance of capabilities of the Reasoner and the Observer."

### Mechanism 2
- Claim: Converting visual inputs into natural language descriptions preserves interpretability while maintaining competitive performance compared to embedding alignment approaches.
- Mechanism: The Observer VLM generates captions from images, which the Reasoner LLM uses to perform reasoning. This intermediate language representation allows humans to read and understand the reasoning process, unlike latent embeddings.
- Core assumption: Language is a sufficient and effective intermediate representation for complex visual reasoning tasks.
- Evidence anchors:
  - [abstract] "Our approach focuses on converting visual inputs to language descriptions while keeping decent performance, which provides more interpretability and reduces training costs significantly."
  - [section] "Our approach introduces a novel framework to optimize hybrid integration systems, which gives a more decent performance while preserving interpretability."

### Mechanism 3
- Claim: The inner monologue process enables the Reasoner to ask targeted follow-up questions, improving information gathering and reasoning accuracy.
- Mechanism: The Reasoner generates queries based on the current state of the conversation and the problem description. The Observer responds with additional visual details, and this iterative process allows the Reasoner to gather sufficient information to solve complex problems.
- Core assumption: Multi-turn question-answering interactions can effectively replace the need for direct visual feature access by the Reasoner.
- Evidence anchors:
  - [section] "With multi-turn querying-answering conversations between the Reasoner and the Observer, the Reasoner gathers information to address the vision and language problems."
  - [section] "IMMO is evaluated on two popular tasks and achieves competitive results compared with hybrid integration approaches, while it uses significantly less training data and provides greater interpretability compared with embedding alignment approaches."

## Foundational Learning

- Concept: Reinforcement Learning with Proximal Policy Optimization (PPO)
  - Why needed here: PPO provides stable policy gradient updates for the Reasoner and Observer models, allowing them to learn effective inner monologue strategies through trial and error.
  - Quick check question: How does PPO's clipped objective help prevent large policy updates that could destabilize training?

- Concept: Supervised Fine-Tuning with Human-Annotated Data
  - Why needed here: The initial supervised training phase provides a good starting point for the RL stage by teaching the models effective inner monologue patterns based on human reasoning.
  - Quick check question: What is the purpose of using Chain-of-Thought style reasoning in the supervised fine-tuning stage?

- Concept: Multi-Agent Reinforcement Learning
  - Why needed here: The Reasoner and Observer can be viewed as agents in a collaborative multi-agent system, where they must learn to communicate effectively to solve problems.
  - Quick check question: How does the alternating training strategy in IMMO relate to standard multi-agent RL approaches?

## Architecture Onboarding

- Component map:
  - Reasoner: LLM (Vicuna-7b) -> generates questions and final answers
  - Observer: VLM (BLIP-2) -> generates captions and answers questions
  - Inner Monologue Container: Tracks conversation history
  - PPO Trainer: Updates active model parameters

- Critical path:
  1. Observer generates initial caption from image
  2. Reasoner generates question based on problem and conversation history
  3. Observer answers question based on image and query
  4. Repeat steps 2-3 for predefined number of turns
  5. Reasoner generates final answer
  6. PPO update based on reward (correctness + KL penalty)

- Design tradeoffs:
  - Interpretability vs. Performance: Language-based intermediate representation is more interpretable but may lose some visual information compared to embeddings
  - Training Data vs. Generalization: IMMO requires less training data than embedding alignment but may not generalize as well to very different tasks
  - Model Complexity vs. Efficiency: Using separate Reasoner and Observer models allows for more specialized training but requires managing interactions between them

- Failure signatures:
  - Reasoner asks irrelevant or repetitive questions
  - Observer provides incorrect or incomplete information
  - Performance plateaus or degrades with increased conversation turns
  - KL penalty dominates reward, preventing policy updates

- First 3 experiments:
  1. Verify that the Reasoner can generate relevant questions based on the initial caption and problem description
  2. Check that the Observer can provide accurate answers to the Reasoner's questions
  3. Confirm that the PPO training loop updates the active model's parameters correctly and that the reward signal is effective

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the IMMO framework be adapted to handle more than one Observer model with different modalities or functionalities?
- Basis in paper: [explicit] The authors mention that "our implementation only includes one observer, while it’s possible to include more observers with different modalities or functionalities."
- Why unresolved: The paper does not explore or provide details on how to effectively incorporate multiple Observer models into the IMMO framework.
- What evidence would resolve it: Experiments demonstrating the performance improvements or challenges of using multiple Observer models with different modalities or functionalities in the IMMO framework.

### Open Question 2
- Question: What is the optimal number of conversation turns for the inner monologue process in different vision-language reasoning tasks?
- Basis in paper: [explicit] The authors state that "these findings are specific to ScienceQA question patterns, underscoring the best inner monologue turns are highly based on the dataset’s characteristics."
- Why unresolved: The paper only provides ablation studies on the ScienceQA dataset and does not explore the optimal number of conversation turns for other vision-language reasoning tasks.
- What evidence would resolve it: Experiments on various vision-language reasoning tasks to determine the optimal number of conversation turns for the inner monologue process in each task.

### Open Question 3
- Question: How can the reward function in the reinforcement learning stage of IMMO be improved to better guide the learning process?
- Basis in paper: [explicit] The authors mention that "the reward function could also be further studied."
- Why unresolved: The paper uses a simple reward function based on exact matching between the predicted answer and the ground-truth answer, which may not capture the full complexity of the reasoning process.
- What evidence would resolve it: Experiments comparing different reward functions, such as those incorporating intermediate rewards or penalizing incorrect reasoning steps, to determine their impact on the performance of the IMMO framework.

## Limitations
- The paper lacks direct quantitative measures of interpretability improvements compared to embedding alignment methods
- Performance comparisons may not account for differences in training conditions or hyperparameter tuning
- The necessity of both supervised fine-tuning and reinforcement learning phases is not empirically validated through ablation studies

## Confidence

**High confidence** in the technical implementation of the alternating PPO training strategy and the basic architecture design. The alternating training mechanism is clearly specified and represents a novel contribution to the field.

**Medium confidence** in the performance claims and comparisons with baseline methods. While the results appear competitive, the paper lacks detailed experimental methodology and hyperparameter specifications that would allow for rigorous replication.

**Low confidence** in the interpretability claims without direct quantitative measures. The paper asserts that language-based representations are more interpretable than embeddings but does not provide user studies or quantitative metrics to support this assertion.

## Next Checks

1. **Ablation study on training stages**: Train IMMO using only the reinforcement learning phase without the supervised fine-tuning initialization to determine whether the two-stage approach is necessary for achieving competitive performance.

2. **Direct interpretability comparison**: Conduct a user study where participants rate the interpretability of IMMO's reasoning process compared to embedding alignment approaches on the same tasks, using standardized metrics for interpretability assessment.

3. **Conversation length analysis**: Systematically vary the number of conversation turns in IMMO and measure the impact on performance and KL penalty to identify the optimal conversation length and understand whether longer conversations provide diminishing returns.