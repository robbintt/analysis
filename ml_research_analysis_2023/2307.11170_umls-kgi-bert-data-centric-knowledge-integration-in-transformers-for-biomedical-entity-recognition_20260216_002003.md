---
ver: rpa2
title: 'UMLS-KGI-BERT: Data-Centric Knowledge Integration in Transformers for Biomedical
  Entity Recognition'
arxiv_id: '2307.11170'
source_url: https://arxiv.org/abs/2307.11170
tags: []
core_contribution: This work introduces UMLS-KGI-BERT, a data-centric method for integrating
  biomedical knowledge graphs into transformer language models. The approach constructs
  text datasets from the UMLS knowledge graph and combines graph-based learning objectives
  with masked-language pre-training.
---

# UMLS-KGI-BERT: Data-Centric Knowledge Integration in Transformers for Biomedical Entity Recognition

## Quick Facts
- arXiv ID: 2307.11170
- Source URL: https://arxiv.org/abs/2307.11170
- Reference count: 12
- Pre-training on UMLS alongside clinical corpora improves NER performance across multiple biomedical and clinical tasks

## Executive Summary
This work introduces UMLS-KGI-BERT, a data-centric method for integrating biomedical knowledge graphs into transformer language models. The approach constructs text datasets from the UMLS knowledge graph and combines graph-based learning objectives with masked-language pre-training. Experiments show that pre-training on the UMLS alongside clinical corpora improves downstream Named Entity Recognition performance across multiple biomedical and clinical tasks in English, French, and Spanish. The method achieves competitive results while requiring less training data than large-scale models. Ablation studies indicate that link prediction contributes most to performance gains. The approach is particularly effective for lower-resource languages.

## Method Summary
The method constructs training sequences from UMLS knowledge graph triples and applies three graph-based learning objectives (entity prediction, link prediction, triple classification) alongside standard masked-language modeling. The model is pre-trained on both the European Clinical Case Corpus and UMLS-derived datasets, then fine-tuned on downstream NER tasks. The approach uses a one-shot fine-tuning strategy and evaluates performance across nine biomedical and clinical NER datasets in three languages.

## Key Results
- UMLS-KGI-BERT achieves competitive NER performance while requiring less training data than large-scale models
- The approach shows particular effectiveness for lower-resource languages (French and Spanish)
- Link prediction task contributes most to performance gains compared to entity prediction and triple classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph-based learning objectives combined with masked-language pre-training improve biomedical entity recognition by providing structured domain knowledge alongside statistical language patterns.
- Mechanism: The model learns from both free-text corpora (E3C) and structured knowledge graphs (UMLS) simultaneously, allowing it to associate semantic concepts with their textual representations while learning contextual relationships between entities.
- Core assumption: The UMLS knowledge graph contains sufficient coverage of biomedical entities and relationships relevant to the target NER tasks.
- Evidence anchors:
  - [abstract] "This work contributes a data-centric paradigm for enriching the language representations of biomedical transformer-encoder LMs by extracting text sequences from the UMLS"
  - [section 3.1] "The training sequences are thus generated from the KG dataset of ordered triples (h, r, t) where (h, r) ∈ C × C and r ∈ R"
- Break condition: If the UMLS knowledge graph lacks coverage of domain-specific entities present in the evaluation corpora, the performance gains would diminish.

### Mechanism 2
- Claim: Training on lower-resource languages with knowledge graph integration yields greater relative improvements compared to high-resource languages.
- Mechanism: Languages with less available training data benefit more from structured knowledge injection because the model can leverage the semantic relationships in the knowledge graph to compensate for data scarcity.
- Core assumption: The relative lack of training data in lower-resource languages creates a larger performance gap that knowledge graph integration can effectively address.
- Evidence anchors:
  - [abstract] "The approach is particularly effective for lower-resource languages"
  - [section 4.2] "The largest improvements brought about by the UMLS-KGI training strategy can be seen in the French and Spanish tasks, suggesting that this technique will be more beneficial for lower-resource languages"
- Break condition: If the knowledge graph itself is language-specific and poorly aligned across languages, the cross-lingual benefits may not materialize.

### Mechanism 3
- Claim: Link prediction tasks contribute most to performance gains compared to entity prediction and triple classification.
- Mechanism: Link prediction requires the model to understand relationships between concepts, which is more directly applicable to entity recognition tasks than entity prediction or triple classification.
- Core assumption: The downstream NER tasks require understanding of entity relationships rather than just entity identification or concept classification.
- Evidence anchors:
  - [section 4.3] "In general, the ablation results... suggest that the majority of the benefits in terms of NER performance are brought about by the link prediction task"
  - [section 4.3] "although there are not enough statistically significant differences among the results to fully justify this conclusion"
- Break condition: If the ablation study had shown equal contributions from all three tasks or if triple classification had shown the largest impact, this mechanism would be invalid.

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: MLM is the primary pre-training objective for BERT-style models and forms the foundation upon which knowledge graph integration is built
  - Quick check question: What is the purpose of masking tokens during pre-training, and how does it help the model learn language representations?

- Concept: Knowledge Graph Embeddings and Reasoning
  - Why needed here: Understanding how knowledge graphs represent entities and relationships is crucial for formulating the graph-based learning objectives
  - Quick check question: How do triple classification, entity prediction, and link prediction differ as knowledge graph reasoning tasks?

- Concept: Cross-Entropy Classification Loss
  - Why needed here: All three knowledge graph tasks and the MLM task use cross-entropy loss, making it essential to understand its role in the training process
  - Quick check question: What is the mathematical formulation of cross-entropy loss, and why is it appropriate for these classification tasks?

## Architecture Onboarding

- Component map: UMLS triples -> Text sequences -> BERT encoder -> Four loss functions (MLM + EP + LP + TC) -> Model weights
- Critical path:
  1. Generate training sequences from UMLS triples
  2. Tokenize sequences with special relation tokens
  3. Apply appropriate loss function based on task indicator
  4. Backpropagate combined loss to update model weights
- Design tradeoffs:
  - Using UMLS as text sequences vs. integrating knowledge graph embeddings directly
  - Number of relation types and their impact on model complexity
  - Balancing weights between MLM and knowledge graph tasks
- Failure signatures:
  - Overfitting to UMLS structure while losing general language understanding
  - Imbalanced performance across different semantic groups
  - Degraded performance on languages with less UMLS coverage
- First 3 experiments:
  1. Compare baseline BERT performance vs. UMLS-KGI-BERT on a single NER task
  2. Run ablation study to measure contribution of each knowledge graph task
  3. Test model performance across different languages with varying UMLS coverage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the UMLS-KGI-BERT approach perform on languages other than English, French, and Spanish, particularly for low-resource languages?
- Basis in paper: [explicit] The authors mention that the approach is particularly effective for lower-resource languages and plan to train and evaluate models in more low-resource settings in future work.
- Why unresolved: The paper only evaluates the approach on three languages (English, French, and Spanish) and acknowledges the need for further evaluation in low-resource language settings.
- What evidence would resolve it: Training and evaluating the UMLS-KGI-BERT approach on a diverse set of low-resource languages and comparing its performance to other methods would provide evidence of its effectiveness across different language settings.

### Open Question 2
- Question: What is the optimal balance between the three knowledge graph-based pre-training tasks (entity prediction, link prediction, and triple classification) for different biomedical and clinical tasks?
- Basis in paper: [inferred] The authors perform ablation experiments to measure the relative effect of the three KG-derived pre-training tasks on downstream performance, suggesting that the link prediction task contributes most to performance gains. However, they acknowledge that there are not enough statistically significant differences among the results to fully justify this conclusion.
- Why unresolved: The ablation experiments show that link prediction contributes most to performance gains, but the results are not statistically significant enough to determine the optimal balance between the three tasks for different downstream tasks.
- What evidence would resolve it: Conducting more extensive ablation experiments with larger sample sizes and a wider range of downstream tasks would help determine the optimal balance between the three knowledge graph-based pre-training tasks for different applications.

### Open Question 3
- Question: How does the UMLS-KGI-BERT approach compare to other state-of-the-art knowledge graph integration methods in terms of performance and efficiency?
- Basis in paper: [explicit] The authors compare the performance of UMLS-KGI-BERT to pre-trained biomedical models and show that it achieves competitive results while requiring less training data. However, they do not compare it to other knowledge graph integration methods.
- Why unresolved: The paper does not provide a direct comparison between UMLS-KGI-BERT and other knowledge graph integration methods in terms of performance and efficiency.
- What evidence would resolve it: Conducting a comprehensive comparison between UMLS-KGI-BERT and other state-of-the-art knowledge graph integration methods on a variety of biomedical and clinical tasks would provide evidence of its relative performance and efficiency.

## Limitations

- Coverage Gap: The UMLS knowledge graph may not fully cover emerging biomedical concepts or domain-specific entities that appear in clinical case reports and biomedical literature.
- Language Asymmetry: The performance benefits for lower-resource languages depend on the quality and completeness of UMLS coverage in these languages.
- Evaluation Constraints: The study employs a one-shot fine-tuning approach which may not reflect practical deployment scenarios where larger fine-tuning datasets are available.

## Confidence

**High Confidence**: The experimental results demonstrating improved NER performance across multiple biomedical and clinical tasks in three languages are well-supported by the reported metrics and statistical analysis.

**Medium Confidence**: The claim that knowledge graph integration is "particularly effective for lower-resource languages" is supported by the observed performance improvements but requires more extensive cross-linguistic validation.

**Low Confidence**: The assertion that the approach requires "less training data than large-scale models" is based on comparison with unspecified baseline models and would benefit from direct comparison with contemporary large language models.

## Next Checks

1. **Cross-linguistic Coverage Analysis**: Conduct a systematic evaluation of UMLS coverage across the nine semantic groups for each target language (English, French, Spanish) to quantify coverage gaps and identify which semantic groups contribute most to cross-lingual knowledge transfer.

2. **Knowledge Graph Integration Alternatives**: Implement and evaluate alternative knowledge graph integration approaches, such as direct knowledge graph embedding fusion or attention-based graph neural network layers, to determine whether the text-sequence conversion approach is optimal.

3. **Fine-tuning Strategy Robustness**: Replicate the experiments using standard fine-tuning (multiple epochs with larger batch sizes) rather than one-shot learning to assess whether the knowledge graph benefits persist under more conventional training regimes.