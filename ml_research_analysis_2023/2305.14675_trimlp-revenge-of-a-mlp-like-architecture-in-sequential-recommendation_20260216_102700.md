---
ver: rpa2
title: 'TriMLP: Revenge of a MLP-like Architecture in Sequential Recommendation'
arxiv_id: '2305.14675'
source_url: https://arxiv.org/abs/2305.14675
tags:
- sequential
- recommendation
- mixer
- conference
- trimlp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents TriMLP, a purely MLP-based architecture for
  sequential recommendation. The key innovation is the Triangular Mixer, which modifies
  standard MLP token-mixing by dropping lower-triangle neurons in the weight matrix
  to enforce chronological cross-token interactions and prevent information leakage
  under auto-regressive training.
---

# TriMLP: Revenge of a MLP-like Architecture in Sequential Recommendation

## Quick Facts
- arXiv ID: 2305.14675
- Source URL: https://arxiv.org/abs/2305.14675
- Reference count: 40
- Outperforms state-of-the-art baselines by 5.32% in accuracy while reducing inference time by 8.44%

## Executive Summary
TriMLP presents a purely MLP-based architecture for sequential recommendation that addresses the limitations of traditional MLP token-mixers in handling chronological dependencies. The key innovation is the Triangular Mixer, which uses triangular masking in the weight matrix to enforce chronological cross-token interactions and prevent information leakage during auto-regressive training. The architecture employs a dual-branch structure with global and local mixing to separately capture long-range dependencies and local patterns. Extensive experiments on 9 datasets (50K-20M interactions) demonstrate that TriMLP consistently outperforms state-of-the-art baselines while maintaining computational efficiency.

## Method Summary
TriMLP uses a purely MLP-based architecture with a Triangular Mixer that drops lower-triangle neurons in the weight matrix to enforce chronological cross-token interactions. The model employs a dual-branch structure: global mixing for long-range dependencies and local mixing for short-term patterns within sessions of length l. The architecture consists of an embedding layer, L=2 basic blocks (each containing Triangular Mixer and Feed-Forward Network), and a classification head. The model is trained using cross-entropy loss in an auto-regressive fashion on sequences of length 64, with embedding dimension d=128 and intermediate dimension 512.

## Key Results
- Achieves 5.32% average improvement over state-of-the-art baselines in accuracy metrics
- Reduces inference time by 8.44% compared to competitive methods
- Demonstrates consistent performance across 9 diverse datasets (50K-20M interactions)
- Validates effectiveness on Amazon, MovieLens, and Tenrec datasets with varying scales

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Triangular Mixer's triangular masking prevents information leakage by blocking future-token connections in the weight matrix.
- Mechanism: Cross-token communication in MLP is implemented as matrix multiplication. By dropping the lower-triangle neurons, each token can only attend to itself and previous tokens, enforcing chronological order.
- Core assumption: Matrix multiplication in MLP naturally represents cross-token interactions, and dropping specific neurons can effectively control information flow.
- Evidence anchors: Abstract and section descriptions confirm triangular masking prevents anti-chronological interactions; corpus lacks direct supporting evidence.

### Mechanism 2
- Claim: The dual-branch structure separately captures long-range dependencies and local patterns for fine-grained sequential dependency modeling.
- Mechanism: Global mixing attends to all previous tokens for long-term preference modeling, while local mixing treats the sequence as independent sessions of length l and captures short-term patterns.
- Core assumption: Long-term and short-term preferences require different attention patterns, and a dual-branch approach can effectively model both without interference.
- Evidence anchors: Abstract and section descriptions outline dual-branch design; corpus lacks direct evidence of this specific architecture.

### Mechanism 3
- Claim: The triangular design improves performance by enabling chronological cross-token communication while maintaining MLP efficiency advantages.
- Mechanism: Replacing fully-connected design with triangular masking preserves parallel processing capability and fewer parameters while ensuring chronological ordering.
- Core assumption: MLP efficiency comes from parallel processing and fewer parameters, which can be maintained while enforcing chronological order through triangular masking.
- Evidence anchors: Abstract shows accuracy/efficiency trade-off; section discusses MLP potential; corpus lacks detailed efficiency analysis.

## Foundational Learning

- Concept: Auto-regressive training
  - Why needed here: TriMLP uses standard auto-regressive training where the model predicts the next token based only on previous tokens. Understanding this training paradigm is crucial for grasping why triangular masking is necessary to prevent information leakage.
  - Quick check question: In auto-regressive training, can the model access future tokens when predicting the next item?

- Concept: Cross-token communication in MLP
  - Why needed here: The paper relies on the insight that cross-token communication in MLP is implemented as matrix multiplication. Understanding this mechanism is essential for grasping how triangular masking controls information flow.
  - Quick check question: How is cross-token communication implemented in standard MLP-Mixer?

- Concept: Sequential recommendation problem formulation
  - Why needed here: The paper addresses sequential recommendation, which involves predicting user preferences based on historical interaction sequences. Understanding the problem formulation helps in appreciating the design choices made in TriMLP.
  - Quick check question: What is the input and output format for a sequential recommendation model?

## Architecture Onboarding

- Component map: Input sequence → Embedding layer → L Basic Blocks (Triangular Mixer + Feed-Forward Network) → Classification Head → Output probabilities
- Critical path: Input sequence → Embedding → L Basic Blocks → Classification Head → Output probabilities
- Design tradeoffs:
  - Pros: Maintains MLP efficiency, enforces chronological order, captures both long and short-term patterns
  - Cons: Sequence length must be fixed (no flexibility), increased parameter count due to dual-branch structure
  - Alternative considered: Using attention mechanism instead of triangular masking (but would lose MLP efficiency)
- Failure signatures:
  - Poor performance: Triangular masking not properly implemented, leading to information leakage
  - Slow inference: Incorrect implementation of parallel processing in Triangular Mixer
  - Suboptimal accuracy: Inappropriate session length l, or insufficient number of Basic Blocks L
- First 3 experiments:
  1. Baseline comparison: Implement EyeMix (no cross-token communication) and measure performance degradation to validate the importance of cross-token interactions
  2. Triangular masking validation: Compare SqrMix (fully-connected) vs TriMixG (global mixing only) to demonstrate the effectiveness of triangular design in preventing information leakage
  3. Dual-branch validation: Compare TriMixG vs TriMixL vs TriMLP to show the complementary benefits of global and local mixing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Triangular Mixer be modified to handle sequences of variable lengths without requiring the number of neurons to be specific to the input length?
- Basis in paper: The paper mentions that Triangular Mixer inherits the inflexibility of handling sequences with different lengths because the number of neurons is specific to the input length.
- Why unresolved: The paper does not provide any solution or exploration of methods to address this limitation.
- What evidence would resolve it: Experimental results demonstrating performance on datasets with varying sequence lengths, or a proposed modification to the Triangular Mixer architecture that allows handling variable-length sequences effectively.

### Open Question 2
- Question: What is the impact of incorporating auxiliary information, such as temporal factors and item attributes, on the performance of TriMLP?
- Basis in paper: The paper mentions that future work could involve introducing auxiliary information like temporal factors and item attributes.
- Why unresolved: The paper does not conduct any experiments or provide analysis on the impact of incorporating auxiliary information in TriMLP.
- What evidence would resolve it: Experimental results comparing the performance of TriMLP with and without the incorporation of auxiliary information, and analysis of the contribution of different types of auxiliary information.

### Open Question 3
- Question: How does the performance of TriMLP scale with increasing dataset size and complexity?
- Basis in paper: The paper validates TriMLP on 9 datasets of different scales (50K~20M interactions) but does not provide detailed analysis of how performance scales with increasing dataset size and complexity.
- Why unresolved: The paper does not conduct experiments or provide analysis on the scalability of TriMLP to larger and more complex datasets.
- What evidence would resolve it: Experimental results showing performance on datasets with significantly larger scales and more complex interaction patterns, along with analysis of scalability in terms of computational resources and performance metrics.

## Limitations

- The session length parameter s is only exemplified rather than explicitly specified for each dataset, creating ambiguity in reproduction
- The architectural contribution relies heavily on triangular masking to prevent information leakage, but lacks ablation studies showing what happens when this masking fails
- The dual-branch design's effectiveness is demonstrated through relative comparisons, but the absolute contribution of each branch remains unclear without independent validation

## Confidence

- High Confidence: The core mechanism of triangular masking preventing information leakage in auto-regressive training is well-supported by mathematical formulation and empirical results
- Medium Confidence: The claim that dual-branch structure separately captures long and short-term preferences is plausible but relies on assumptions about session boundaries
- Low Confidence: The assertion that triangular masking maintains MLP efficiency while enforcing chronological order is supported by inference time comparisons but lacks detailed complexity analysis

## Next Checks

1. **Ablation of Triangular Masking**: Implement a variant where the triangular masking is imperfectly applied (e.g., random neurons instead of systematic lower-triangle) and measure information leakage through attention visualization or gradient analysis.

2. **Session Boundary Sensitivity**: Conduct experiments varying the session length parameter s across a wider range than used in the paper, including datasets where user behavior doesn't naturally segment into sessions.

3. **Computational Complexity Verification**: Perform detailed FLOPs analysis comparing TriMLP with attention-based baselines on sequences of varying lengths, accounting for the increased parameter count from the dual-branch structure.