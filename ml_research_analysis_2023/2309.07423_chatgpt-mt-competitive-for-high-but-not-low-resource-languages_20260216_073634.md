---
ver: rpa2
title: 'ChatGPT MT: Competitive for High- (but not Low-) Resource Languages'
arxiv_id: '2309.07423'
source_url: https://arxiv.org/abs/2309.07423
tags:
- latn
- languages
- chatgpt
- language
- nllb
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the machine translation performance of ChatGPT
  and other large language models across 204 languages using the FLORES-200 benchmark.
  The authors find that while ChatGPT approaches or exceeds traditional MT model performance
  for high-resource languages, it consistently underperforms for low-resource languages,
  with traditional MT models outperforming ChatGPT on 84.1% of languages tested.
---

# ChatGPT MT: Competitive for High- (but not Low-) Resource Languages

## Quick Facts
- arXiv ID: 2309.07423
- Source URL: https://arxiv.org/abs/2309.07423
- Authors: 
- Reference count: 14
- Key outcome: ChatGPT approaches or exceeds traditional MT model performance for high-resource languages but consistently underperforms for low-resource languages, with traditional MT models outperforming ChatGPT on 84.1% of languages tested.

## Executive Summary
This study evaluates the machine translation performance of ChatGPT and other large language models across 204 languages using the FLORES-200 benchmark. The authors find that while ChatGPT approaches or exceeds traditional MT model performance for high-resource languages, it consistently underperforms for low-resource languages, with traditional MT models outperforming ChatGPT on 84.1% of languages tested. A decision tree analysis reveals that a language's resource level is the most important factor in determining ChatGPT's translation effectiveness, with the model particularly disadvantaged for low-resource and African languages. The study also finds that few-shot prompts offer only marginal improvements for LLM translation. Cost analysis shows that while NLLB has the best scores for its price, zero-shot ChatGPT tops five-shot prompts in terms of value.

## Method Summary
The study uses the FLORES-200 benchmark containing 204 language varieties with 1012 devtest sentences and 997 dev sentences. The researchers used zero-shot and five-shot prompts for ChatGPT, comparing results with NLLB-MOE, Google Translate, and GPT-4 using spBLEU and chrF2++ metrics. Translation outputs were generated through the OpenAI API, with evaluation metrics computed using sacreBLEU. A decision tree regressor was employed to analyze which language features predict ChatGPT's relative performance. The study also conducted cost analysis comparing token usage across different models and prompt configurations.

## Key Results
- ChatGPT performs worse on low-resource languages than high-resource languages across all 204 languages tested
- Traditional MT models outperform ChatGPT on 84.1% of languages tested
- Five-shot prompts provide only marginal improvements over zero-shot prompting
- Script variation significantly impacts performance even within the same language
- Resource level (approximated by Wikipedia page count) is the most important predictor of ChatGPT's translation effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM translation quality scales with target language resource availability
- Mechanism: Decoder-only LLMs implicitly learn translation from pretraining data distribution; high-resource languages have more parallel text in pretraining corpora, enabling better zero-shot translation
- Core assumption: Pretraining data contains sufficient parallel or comparable text for high-resource languages
- Evidence anchors:
  - [abstract]: "GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs)"
  - [section 3.2]: "Using Team et al.'s (2022) resource categorization, we find that ChatGPT performs worse on LRLs than HRLs"
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.539, average citations=0.0.

### Mechanism 2
- Claim: Few-shot prompts provide marginal translation quality improvements
- Mechanism: In-context examples help LLMs understand task format but don't fundamentally improve language modeling quality
- Core assumption: Five-shot examples are representative and correctly formatted
- Evidence anchors:
  - [section 3.3]: "Results in Table 6 show five-shot prompts performing best... five-shot settings offered generally marginal improvements over zero-shot"
  - [section 2.3]: "Previous works... investigated LLM prompting to optimize MT performance"
  - [corpus]: Related papers explore prompt engineering but with limited scope (3-5 languages)

### Mechanism 3
- Claim: Script variation impacts LLM translation quality even within same language
- Mechanism: LLMs may not learn consistent transliteration patterns, treating different scripts as separate languages
- Core assumption: Script differences are meaningful for language understanding
- Evidence anchors:
  - [section 3.5]: "Our own analysis... actually suggests that script is the least important language feature... However, differences in performance are clear when comparing scripts used for the same language"
  - [table 8]: Shows performance gaps between scripts for same language (e.g., ace_Arab 8.41 vs ace_Latn 19.82 chrF)
  - [corpus]: Weak corpus evidence - related papers don't extensively explore script sensitivity

## Foundational Learning

- Concept: Resource categorization (high vs low resource)
  - Why needed here: The study's core finding depends on distinguishing performance by resource level
  - Quick check question: How does the paper define "high-resource" vs "low-resource" languages?

- Concept: BLEU and chrF evaluation metrics
  - Why needed here: The paper uses these to compare MT systems across 204 languages
  - Quick check question: Why does the paper prefer chrF over BLEU for this study?

- Concept: Decision tree feature importance analysis
  - Why needed here: Used to identify which language features predict ChatGPT's relative performance
  - Quick check question: What was the most important feature in predicting ChatGPT's effectiveness?

## Architecture Onboarding

- Component map:
  FLORES-200 benchmark -> OpenAI API -> sacreBLEU evaluation -> Decision tree analysis

- Critical path:
  1. Query OpenAI API with prompts
  2. Run evaluation metrics on outputs
  3. Compare against NLLB and Google baselines
  4. Analyze feature importance

- Design tradeoffs:
  - Zero-shot vs few-shot: Convenience vs marginal performance gain
  - English-centric vs multilingual: Available data vs real-world applicability
  - Automated vs human evaluation: Scale vs quality

- Failure signatures:
  - Consistent underperformance on specific language families
  - Script sensitivity indicating tokenization issues
  - High token counts suggesting inefficient encoding

- First 3 experiments:
  1. Replicate zero-shot vs five-shot comparison on a subset of languages
  2. Test script sensitivity by translating between scripts for same language
  3. Run feature importance analysis with different tree depths

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would LLMs perform on multilingual MT tasks (e.g., X→Y where neither language is English) compared to traditional MT models?
- Basis in paper: [inferred] The paper only evaluates ENG→X translation directions and notes that avoiding X→ENG was due to training data concerns. The authors suggest future work should explore non-English-centric directions.
- Why unresolved: The study deliberately limited scope to ENG→X to avoid confounding factors related to English Wikipedia presence in LLM training data. No experiments were conducted for other translation directions.
- What evidence would resolve it: Direct comparison of LLM vs. traditional MT performance on a benchmark covering multiple non-English-centric translation directions (e.g., X→Y pairs across diverse language families).

### Open Question 2
- Question: What specific training data characteristics (e.g., parallel text quantity/quality, monolingual corpus diversity) most strongly influence LLM translation performance for low-resource languages?
- Basis in paper: [explicit] The paper identifies that a language's resource level (approximated by Wikipedia page count) is the most important feature predicting ChatGPT's relative MT effectiveness, but the authors acknowledge limitations in knowing exact LLM training data details.
- Why unresolved: LLMs like ChatGPT are closed-source with unknown training data composition. The study uses proxy measures (Wikipedia pages, Oscar corpus size) rather than actual training data characteristics.
- What evidence would resolve it: Analysis correlating LLM MT performance with detailed training data statistics (e.g., actual parallel text volumes, domain distribution, quality metrics) for low-resource languages.

### Open Question 3
- Question: Can specialized fine-tuning or instruction-tuning of LLMs significantly improve their low-resource language translation capabilities compared to few-shot prompting?
- Basis in paper: [explicit] The authors note that decoder-only models lag behind encoder-decoder models for low-resource applications and suggest investigating how decoder-only models "may catch up with encoder-decoder models in low-resource applications."
- Why unresolved: The study focuses on zero/few-shot prompting with general-purpose LLMs. No experiments were conducted with specialized fine-tuning approaches for low-resource languages.
- What evidence would resolve it: Controlled experiments comparing few-shot prompting versus fine-tuned LLMs on low-resource language translation benchmarks, measuring both performance gains and resource requirements.

## Limitations

- The study exclusively focuses on English-centric translation, evaluating only English-to-target language pairs
- The study relies on automated metrics (BLEU and chrF) rather than human evaluation
- The resource categorization methodology uses a binary distinction that may oversimplify the complex continuum of language resource availability

## Confidence

- **High Confidence**: ChatGPT's consistent underperformance on low-resource languages is well-supported by empirical data across 204 languages and multiple evaluation metrics.
- **Medium Confidence**: The claim that few-shot prompts provide only marginal improvements is supported by the data but may be context-dependent.
- **Medium Confidence**: The assertion that script variation impacts LLM translation quality is observed in the data but lacks strong theoretical grounding.

## Next Checks

1. **Bidirectional Validation**: Replicate the evaluation framework for non-English source languages (e.g., Spanish→French, French→Spanish) to test whether the English-centric resource-level findings generalize to other language families and translation directions.

2. **Script-Agnostic Testing**: Design experiments that explicitly test whether LLMs can learn consistent transliteration patterns by translating between different scripts of the same language (e.g., Serbian Latin→Cyrillic, Hindi Devanagari→Latin script) to isolate tokenization from semantic understanding.

3. **Multi-Task Prompting Analysis**: Systematically vary the number and selection of few-shot examples (1-shot, 3-shot, 5-shot, 10-shot) while controlling for example quality and relevance to identify the optimal prompt configuration and test whether current marginal gains represent a local optimum rather than fundamental limitations.