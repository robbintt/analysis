---
ver: rpa2
title: Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language
  Models
arxiv_id: '2305.09144'
source_url: https://arxiv.org/abs/2305.09144
tags:
- knowledge
- language
- learning
- memory
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the memorizing mechanisms of language models
  by comparing vanilla and pre-trained models. The authors use a factual knowledge
  acquisition testbed, where models are trained on 23 types of knowledge facts and
  their forgetting curves are tracked.
---

# Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models

## Quick Facts
- arXiv ID: 2305.09144
- Source URL: https://arxiv.org/abs/2305.09144
- Reference count: 40
- Pre-training transforms language models from forgetful to retentive by establishing long-term memory patterns

## Executive Summary
This paper investigates how language models memorize factual knowledge by comparing vanilla and pre-trained models using a factual knowledge acquisition testbed. The authors train models on 23 knowledge types and track forgetting curves to reveal that pre-training fundamentally changes memory patterns from short-term (catastrophic forgetting) to long-term retention. The study demonstrates that knowledge relevance and diversity significantly impact memory formation, with high correlation causing memory competition and low diversity leading to memory collapse.

## Method Summary
The researchers created a factual knowledge acquisition testbed using the LAMA dataset (23 knowledge types, 50,000 sentences each) and semantic-equivalent prompts to convert facts into natural language. They trained both vanilla and pre-trained BERT-based models using repetitive group-by-group learning, saving checkpoints after each knowledge type was learned. Forgetting curves were generated by probing models at each checkpoint with factual knowledge queries, measuring accuracy over time to analyze memorization dynamics and memory patterns.

## Key Results
- Pre-trained models exhibit long-term memory patterns that improve with repetitive learning, while vanilla models show short-term memory with catastrophic forgetting
- Knowledge relevance creates memory competition in pre-trained models, with highly correlated knowledge types interfering with each other's retention
- Lack of knowledge diversity causes memory collapse in early learning stages, but diverse knowledge types enable faster memorization convergence

## Why This Works (Mechanism)

### Mechanism 1
- Pre-training transforms language models from forgetful to retentive by establishing long-term memory patterns through distributed representations across parameters
- Core assumption: Pre-training creates sufficient overlap between learned representations that new knowledge builds upon rather than replaces existing knowledge
- Evidence anchors: Abstract findings on pre-training leading to retentive models; section 4.1 showing gradual improvement through repetitive learning

### Mechanism 2
- Knowledge relevance creates memory competition where highly correlated knowledge types interfere with each other's retention
- Core assumption: Similar concepts share parameter space in distributed representations, causing interference
- Evidence anchors: Section 5.1 showing competition between relevant knowledge types; section 3.2 demonstrating correlation effects on performance

### Mechanism 3
- Knowledge diversity prevents memory collapse by distributing memory traces across different parameter regions
- Core assumption: Diverse knowledge types map to different regions of parameter space, reducing interference
- Evidence anchors: Section 5.2 identifying lack of diversity as critical factor for memory collapse; showing faster convergence with diverse knowledge

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why vanilla models exhibit short-term memory patterns requires knowledge of how neural networks forget previously learned information
  - Quick check question: What is the primary mechanism by which standard neural networks exhibit catastrophic forgetting when learning new tasks?

- Concept: Long-term memory formation in psychology
  - Why needed here: The paper draws parallels between language model memory patterns and psychological memory theories
  - Quick check question: How does the Atkinson-Shiffrin model of memory explain the difference between short-term and long-term memory formation?

- Concept: Knowledge representation in transformer models
  - Why needed here: Understanding how language models store and retrieve factual knowledge is crucial for interpreting the forgetting curve experiments
  - Quick check question: In transformer models, how are factual knowledge relationships typically encoded in the parameter space?

## Architecture Onboarding

- Component map: Transformer architecture (BERT/GPT-2) → Pre-training objectives (MLM/CLM) → Factual knowledge acquisition testbed → Forgetting curve evaluation
- Critical path: Pre-training → Factual knowledge acquisition → Forgetting curve measurement → Analysis of memory patterns
- Design tradeoffs: Longer pre-training improves memory retention but increases computational cost; higher knowledge diversity prevents collapse but may slow initial learning
- Failure signatures: Catastrophic forgetting (performance drops to zero when new knowledge is learned), memory collapse (rapid performance drops followed by recovery), limited memory capacity (performance determined only by most recent knowledge)
- First 3 experiments:
  1. Run forgetting curve analysis on vanilla BERT model with one knowledge type to observe short-term memory patterns
  2. Compare forgetting curves of pre-trained vs vanilla models on same knowledge type to observe transformation effect
  3. Test memory competition by learning two highly correlated knowledge types sequentially and measuring interference effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the depth of pre-training influence the capacity of long-term memory in language models?
- Basis in paper: The study indicates longer pre-training leads to larger memory capacity but doesn't quantify the extent
- Why unresolved: The paper lacks detailed analysis of varying pre-training durations' impact on memory capacity
- What evidence would resolve it: Experiments measuring memory capacity across models with different pre-training durations

### Open Question 2
- Question: What are the specific mechanisms through which pre-training transforms vanilla language models from forgetful to retentive?
- Basis in paper: The study observes this transformation but doesn't explore underlying mechanisms
- Why unresolved: The paper doesn't investigate internal changes in model architecture or learning dynamics during pre-training
- What evidence would resolve it: Detailed analysis of model weights, activation patterns, and learning dynamics before and after pre-training

### Open Question 3
- Question: How does knowledge diversity in newly learned information affect the prevention of memory collapse in language models?
- Basis in paper: The study suggests lack of diversity leads to memory collapse but doesn't quantify exact impact
- Why unresolved: The paper doesn't provide comprehensive study on how varying diversity levels influence memory stability
- What evidence would resolve it: Controlled experiments with different diversity levels and their effects on memory collapse

## Limitations

- Findings may not generalize beyond factual knowledge acquisition to other knowledge types or complex reasoning tasks
- Experimental setup uses artificial knowledge acquisition tasks that may not reflect real-world training scenarios
- Proposed mechanisms are largely correlational rather than causally proven through direct intervention studies

## Confidence

- High Confidence: Pre-training transforms memory patterns from short-term to long-term; knowledge relevance creates memory competition; lack of diversity causes memory collapse
- Medium Confidence: Specific mechanisms of pre-training creating long-term memory; relationship between knowledge correlation and competition strength; diversity thresholds for preventing collapse
- Low Confidence: Generalizability to other architectures beyond BERT; applicability to real-world training scenarios; precise mathematical relationship between knowledge entropy and memory dynamics

## Next Checks

1. **Cross-Architecture Validation:** Replicate forgetting curve experiments with GPT-style decoder-only models to verify architecture-independence of memory patterns

2. **Real-World Data Simulation:** Design experiments using continuous data streams mimicking real-world training to validate if observed memory mechanisms hold under realistic conditions

3. **Causal Mechanism Testing:** Implement ablation studies manipulating specific model representation aspects to establish causal relationships between proposed mechanisms and observed memory patterns