---
ver: rpa2
title: 'PROMINET: Prototype-based Multi-View Network for Interpretable Email Response
  Prediction'
arxiv_id: '2310.16753'
source_url: https://arxiv.org/abs/2310.16753
tags:
- email
- prototypes
- response
- prominet
- prototype
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PROMINET, a prototype-based multi-view network
  for interpretable email response prediction. The model integrates semantic and structural
  information from email data using transformer-based encoders and graph neural networks,
  combined with prototype learning at document, sentence, and phrase levels.
---

# PROMINET: Prototype-based Multi-View Network for Interpretable Email Response Prediction

## Quick Facts
- arXiv ID: 2310.16753
- Source URL: https://arxiv.org/abs/2310.16753
- Reference count: 18
- Primary result: Achieves ~3% F1 score improvement over baselines while providing interpretable explanations through learned prototypes

## Executive Summary
PROMINET introduces a prototype-based multi-view network for interpretable email response prediction. The model integrates semantic and structural information from email data using transformer-based encoders and graph neural networks, combined with prototype learning at document, sentence, and phrase levels. Evaluated on two real-world email datasets, PROMINET outperforms baseline models by approximately 3% in F1 score while providing interpretable explanations through learned prototypes. The model demonstrates potential for generating actionable suggestions to improve email text editing and enhance response rates.

## Method Summary
PROMINET employs a multi-view architecture combining semantic and structural representations of emails. The semantic view uses transformer encoders to capture meaning from subject, body, and metadata, while the structural view uses dependency parsing and GNNs to capture grammatical relationships. Prototype layers at document, sentence, and phrase levels learn representative patterns through similarity-based classification. The model maps input embeddings to the nearest prototypes and projects these prototypes onto training examples for interpretability. The weighted combination of views produces final response predictions.

## Key Results
- Achieves approximately 3% improvement in F1 score over baseline models including BERT-base, DistilBERT, RoBERTa, XLNet, TextGCN, ProSeNet, and ProtoCNN
- Demonstrates interpretable explanations through prototype projection onto training examples at multiple granularities
- Shows potential for generating actionable suggestions to improve email text editing and enhance response rates
- Validated on two real-world datasets: Enron corpus and proprietary Email Marketing corpus

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Prototype-based learning enables interpretable email response prediction by mapping inputs to learned exemplars at multiple granularities.
- **Mechanism**: PROMINET learns latent prototypes for documents, sentences, and phrases, then maps input embeddings to the nearest prototype. This allows predictions based on similarity to known response patterns while offering human-understandable explanations.
- **Core assumption**: Email response behavior can be captured by identifying representative patterns (prototypes) at different levels of granularity.
- **Evidence anchors**:
  - [abstract]: "The model maps learned semantic and structural exemplars to observed samples in the training data at different levels of granularity, such as document, sentence, or phrase."
  - [section 4.3]: "For each granularity level g, which can be either document (D), sentence (S), or phrase (P), the layer calculates the similarity between the granularity-specific embedding (eg) and each trainable prototype."
  - [corpus]: Found 25 related papers with average neighbor FMR=0.406, suggesting moderate semantic relevance but limited citation support for prototype-based email models.
- **Break condition**: If the training data lacks sufficient diversity or contains only subtle response cues, prototypes may not generalize to unseen email scenarios.

### Mechanism 2
- **Claim**: Multi-view representation (semantic + structural) improves prediction accuracy by capturing both content meaning and grammatical relationships.
- **Mechanism**: Semantic view uses transformer encoders to capture meaning from subject, body, and metadata. Structural view uses dependency parsing + GNN to capture grammatical relationships. Combining these views yields richer representations than either alone.
- **Core assumption**: Email response behavior depends on both what is said (semantic content) and how it is structured (grammatical dependencies).
- **Evidence anchors**:
  - [abstract]: "The model integrates semantic and structural information from email data using transformer-based encoders and graph neural networks."
  - [section 4.2]: "The structural view emphasizes the importance of specific phrases in email engagement by examining the relationships between tokens or phrases within email sentences."
  - [corpus]: Limited direct evidence for structural+semantic combinations in email prediction literature, suggesting novelty.
- **Break condition**: If structural relationships don't correlate with response likelihood, or if transformer models already capture sufficient context, the structural view adds little value.

### Mechanism 3
- **Claim**: Prototype projection onto training samples provides interpretable explanations by linking abstract prototypes to concrete examples.
- **Mechanism**: After learning latent prototypes, PROMINET projects each prototype to the closest training email/sentence/phrase embedding. This substitution makes abstract prototypes human-interpretable by associating them with real examples.
- **Core assumption**: Users can understand predictions better when explanations reference actual examples from training data rather than abstract vectors.
- **Evidence anchors**:
  - [section 4.6]: "For improved interpretability, we project the latent prototypes onto the closest emails, sentences, or phrases from the training data."
  - [section 6.3.1]: Case study shows prototypes labeled with original email content, demonstrating this interpretability mechanism.
  - [corpus]: No direct corpus evidence for prototype projection in email response prediction, but related interpretable ML work exists.
- **Break condition**: If training examples are poor representatives or contain biases, projected prototypes may mislead rather than clarify.

## Foundational Learning

- **Concept**: Transformer-based text encoding (BERT, RoBERTa, XLNet)
  - **Why needed here**: Captures contextual semantic information from email subject, body, and metadata that correlates with response behavior
  - **Quick check question**: How does a transformer encoder represent an email as a single document embedding versus sentence-level embeddings?

- **Concept**: Graph Neural Networks for dependency parsing
  - **Why needed here**: Captures grammatical relationships between words that influence email persuasiveness and engagement likelihood
  - **Quick check question**: What dependency relationships (nsubj, dobj, etc.) are most relevant for identifying key phrases in marketing emails?

- **Concept**: Prototype learning with similarity-based classification
  - **Why needed here**: Enables interpretable predictions by comparing inputs to learned exemplars rather than opaque model parameters
  - **Quick check question**: How does the similarity function (cosine vs Euclidean) affect prototype-based classification performance?

## Architecture Onboarding

- **Component map**: Email → Semantic View (Doc Encoder + Sentence Encoder) + Structural View (Dependency Parser + Graph Encoder) → Prototype Layers (Doc/Sentence/Phrase) → Similarity Scoring → Weighted Sum + Output Layer → Prediction
- **Critical path**: Input email → Semantic and Structural embeddings → Prototype similarity scores → Weighted combination → Final prediction
- **Design tradeoffs**: Multi-view architecture increases accuracy but adds complexity; prototype learning provides interpretability but may limit expressiveness compared to end-to-end models
- **Failure signatures**: Poor prototype quality → vague explanations; imbalanced training data → biased prototypes; weak structural signals → limited benefit from GNN
- **First 3 experiments**:
  1. Train PROMINET with only semantic view (remove structural branch) to quantify structural contribution
  2. Test different prototype counts (j,k,m) to find optimal balance between performance and interpretability
  3. Apply suggested edits to negative emails and measure response rate improvement to validate practical utility

## Open Questions the Paper Calls Out

- **Question**: How do time and historical interaction factors impact email response prediction accuracy?
  - **Basis in paper**: [explicit] The authors note their model focuses primarily on text aspects and disregards factors such as time and historical interactions with customers.
  - **Why unresolved**: The paper does not include time and historical interaction features in their model, leaving their impact on accuracy unknown.
  - **What evidence would resolve it**: Experiments comparing model performance with and without time and historical interaction features would determine their impact on accuracy.

- **Question**: How can the model handle unseen scenarios or outliers that cannot be accurately mapped to examples in the training set?
  - **Basis in paper**: [explicit] The authors acknowledge that there may be unseen scenarios or outliers that cannot be accurately mapped to examples in the training set.
  - **Why unresolved**: The paper does not propose a solution for handling such cases, leaving this as an open challenge.
  - **What evidence would resolve it**: Developing and testing methods to handle unseen scenarios or outliers would demonstrate how to address this limitation.

- **Question**: How can the interpretability of the model be improved to present explanations in a more user-friendly manner?
  - **Basis in paper**: [explicit] The authors suggest that exploring alternative approaches to enhance interpretability and present explanations in a more user-friendly manner is an avenue for future research.
  - **Why unresolved**: The paper does not propose specific methods to improve interpretability, leaving this as an open challenge.
  - **What evidence would resolve it**: Developing and testing methods to present model explanations in a more user-friendly manner would demonstrate how to address this limitation.

## Limitations

- **Data representation concerns**: Critical details about preprocessing, especially for the proprietary Email Marketing dataset, remain unspecified, potentially impacting model performance and generalizability
- **Prototype quality assessment gaps**: Lacks quantitative metrics for prototype quality beyond loss functions, making it unclear whether learned prototypes genuinely capture response patterns or merely memorize training data
- **Unverified practical utility**: Claims about actionable suggestions for text editing and their impact on response rates are presented without empirical validation or concrete implementation details

## Confidence

- **High confidence**: The multi-view architecture combining semantic and structural information is technically sound and represents a valid approach to email response prediction. The prototype learning mechanism follows established methods in interpretable machine learning.
- **Medium confidence**: The reported 3% F1 improvement is plausible given the architectural sophistication, but without detailed baseline implementation descriptions and statistical significance testing across multiple runs, the exact magnitude of improvement cannot be independently verified.
- **Low confidence**: Claims about actionable suggestions for text editing and their practical impact on response rates are presented without empirical validation. The mechanism for translating prototype insights into specific editing recommendations remains vague.

## Next Checks

1. **Ablation study verification**: Systematically remove the structural view component and measure performance degradation to quantify the actual contribution of dependency parsing and GNN components to the reported accuracy gains.

2. **Prototype interpretability validation**: Conduct human evaluation studies where domain experts assess whether projected prototypes genuinely capture interpretable response patterns, and whether these insights translate to actionable editing recommendations.

3. **Cross-domain generalization test**: Evaluate PROMINET on an independent email dataset from a different domain (e.g., customer support tickets or academic collaboration emails) to assess whether the 3% performance advantage persists beyond the training distribution.