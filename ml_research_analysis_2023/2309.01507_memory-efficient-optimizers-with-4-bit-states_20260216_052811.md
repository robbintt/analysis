---
ver: rpa2
title: Memory Efficient Optimizers with 4-bit States
arxiv_id: '2309.01507'
source_url: https://arxiv.org/abs/2309.01507
tags:
- memory
- quantization
- momentum
- normalization
- optimizer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to quantize optimizer states from
  32-bit floating points to 4-bit integers for memory-efficient training of neural
  networks. The key idea is to use a smaller block size and row/column-wise information
  for quantization, and to use a linear quantizer without zero-point for the second
  moment.
---

# Memory Efficient Optimizers with 4-bit States

## Quick Facts
- arXiv ID: 2309.01507
- Source URL: https://arxiv.org/abs/2309.01507
- Authors: Various
- Reference count: 40
- Primary result: 4-bit optimizers achieve comparable accuracy to full-precision optimizers while reducing memory consumption

## Executive Summary
This paper introduces a method to quantize optimizer states from 32-bit floating points to 4-bit integers for memory-efficient neural network training. The approach addresses key challenges in low-bit optimization, including handling outlier patterns in first-order momentum and avoiding catastrophic errors in second-order momentum quantization. By using smaller block sizes with block-wise normalization and a linear quantizer without zero-point for second moments, the method achieves significant memory savings while maintaining training accuracy across diverse tasks.

## Method Summary
The method compresses optimizer states using three key innovations: (1) smaller block size (128) with block-wise normalization for first-order momentum to better capture heterogeneous outlier patterns, (2) a linear quantizer that excludes the zero-point for second-order momentum to prevent catastrophic approximation errors, and (3) rank-1 normalization that utilizes row and column-wise information for improved second-order momentum quantization. The compressed 4-bit states are decompressed during training updates and recompressed afterward, achieving memory savings while maintaining comparable accuracy to full-precision optimizers.

## Key Results
- 4-bit optimizers achieve comparable accuracy to full-precision optimizers across natural language understanding, machine translation, image classification, and instruction tuning tasks
- Smaller block size (128) provides better quantization accuracy than larger blocks (2048) for first-order momentum
- Rank-1 normalization with linear mapping for second-order momentum consistently performs well across different tasks

## Why This Works (Mechanism)

### Mechanism 1
Using smaller block size (128) with block-wise normalization improves quantization accuracy for first-order momentum by better handling outlier patterns. Large block sizes (e.g., 2048) fail to capture complex outlier patterns that vary across parameter tensors. Smaller blocks allow more localized scaling, reducing quantization error.

### Mechanism 2
Excluding zero-point in quantization mapping for second-order momentum prevents catastrophic approximation errors. The update direction in Adam is inversely proportional to the square root of second-order momentum. Quantizing small values near zero to zero causes large deviations in updates.

### Mechanism 3
Rank-1 normalization for second-order momentum provides better approximation by utilizing row and column-wise information. Rank-1 normalization uses two scales (row and column max) to compute a tighter bound for each entry, effectively handling outlier patterns that persist in fixed rows or columns.

## Foundational Learning

- **Concept: Quantization error and its impact on training stability**
  - Why needed here: Understanding how quantization error propagates through optimizer updates is crucial for designing effective low-bit optimizers
  - Quick check question: Why does quantizing small values near zero to zero cause problems in Adam's update rule?

- **Concept: Outlier patterns in neural network parameters and activations**
  - Why needed here: Different types of tensors (weights, activations, optimizer states) have different outlier patterns that require different quantization strategies
  - Quick check question: How do outlier patterns in optimizer states differ from those in weights or activations?

- **Concept: Block-wise vs. per-tensor normalization tradeoffs**
  - Why needed here: Choosing the right normalization granularity is critical for balancing quantization accuracy and memory overhead
  - Quick check question: What are the tradeoffs between using block-wise normalization with different block sizes versus per-tensor normalization?

## Architecture Onboarding

- **Component map**: Quantizer -> Decompressor -> Optimizer -> Compression framework
- **Critical path**: Forward pass: Model parameters (32-bit) → Model computation → Gradients (32-bit) → Backward pass: Gradients (32-bit) → First/second moment update (32-bit) → Compress to 4-bit → Store → Update step: Load 4-bit states → Decompress to 32-bit → Apply Adam update → Compress back to 4-bit
- **Design tradeoffs**: Block size vs. quantization accuracy (smaller blocks capture local patterns better but increase memory overhead); Zero-point inclusion vs. representation capacity (excluding zero-point prevents catastrophic errors but wastes one quantization level); Factorization vs. quantization (factorization reduces memory but is only applicable to second-order momentum)
- **Failure signatures**: Training instability or divergence (likely due to zero-point problem or insufficient quantization accuracy); Accuracy degradation (may indicate suboptimal block size or normalization method); Memory issues (could be caused by incorrect threshold for quantizing small tensors)
- **First 3 experiments**: 1) Verify 4-bit optimizer training stability on a small language model (e.g., GPT-2 Small) on GLUE tasks; 2) Compare accuracy of different block sizes (128 vs. 2048) on first-order momentum quantization; 3) Test rank-1 normalization vs. block-wise normalization on second-order momentum quantization accuracy

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of 4-bit optimizers vary across different neural network architectures and tasks beyond the ones tested in the paper? The paper mentions that applicability to domains like reinforcement learning, audio, and graph learning still needs further study.

- **Open Question 2**: What is the optimal quantization setting for 4-bit optimizers, and how does it depend on the specific task, dataset, and training details? The paper notes that optimal settings probably depend on task, datasets, and training details.

- **Open Question 3**: How can the memory and computational efficiency of 4-bit optimizers be further improved, and what are the trade-offs involved? The paper mentions that combining 4-bit optimizers with other memory efficient techniques (like activation compression and gradient checkpointing) could provide additional benefits.

## Limitations

- The empirical evaluation may not fully capture the diversity of modern deep learning workloads, particularly very large models or specialized architectures
- The proposed techniques may not generalize to all optimizer types beyond Adam
- The specific block size of 128 was not extensively explored across different model scales and tasks

## Confidence

**High Confidence Claims:**
- The zero-point problem in quantizing second-order momentum is real and significant for optimizer performance
- Smaller block sizes (128) provide better quantization accuracy for first-order momentum than larger blocks (2048)
- The proposed 4-bit optimizers achieve comparable accuracy to full-precision optimizers while reducing memory consumption

**Medium Confidence Claims:**
- Rank-1 normalization provides superior approximation for second-order momentum compared to block-wise normalization
- The specific block size of 128 is optimal for the proposed quantization scheme
- The techniques generalize well across diverse tasks and model architectures

**Low Confidence Claims:**
- The proposed methods will maintain their effectiveness as model sizes continue to grow
- The specific implementation details (e.g., exact thresholds for quantization) are optimal
- The techniques will perform equally well on specialized architectures like transformers with attention mechanisms

## Next Checks

1. **Scalability Validation**: Test the 4-bit optimizer on extremely large models (e.g., GPT-3 scale or larger) to verify that the memory savings and accuracy benefits scale as expected. Measure both memory consumption and training time to ensure practical viability.

2. **Optimizer Generalization**: Evaluate the proposed quantization techniques on other popular optimizers (e.g., SGD with momentum, RMSprop, AdaGrad) to determine if the insights about zero-point exclusion and block size selection generalize beyond Adam.

3. **Dynamic Range Analysis**: Conduct a detailed analysis of the dynamic range of optimizer states across different training stages and tasks. This would help validate whether the static quantization scheme is appropriate or if adaptive quantization strategies could provide additional benefits.