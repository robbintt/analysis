---
ver: rpa2
title: Graph Edit Distance Learning via Different Attention
arxiv_id: '2308.13871'
source_url: https://arxiv.org/abs/2308.13871
tags:
- graph
- fusion
- difference
- structural
- diffatt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel graph-level fusion module, Different
  Attention (DiffAtt), for the graph similarity computation problem. The key insight
  is that the graph edit distance is determined by the structural difference between
  two graphs under optimal alignment.
---

# Graph Edit Distance Learning via Different Attention

## Quick Facts
- arXiv ID: 2308.13871
- Source URL: https://arxiv.org/abs/2308.13871
- Reference count: 9
- This paper proposes Different Attention (DiffAtt) for graph similarity computation, achieving state-of-the-art performance on 23 out of 25 metrics across five benchmark datasets.

## Executive Summary
This paper addresses the Graph Edit Distance (GED) learning problem by proposing a novel graph-level fusion module called Different Attention (DiffAtt). The key insight is that GED is determined by structural differences between graphs under optimal alignment, not shared structure. DiffAtt leverages the difference between graph-level embeddings as an attention mechanism to highlight these structural differences, outperforming complex node-level fusion methods. Based on DiffAtt, the authors propose REDRAFT, which achieves state-of-the-art performance on 23 out of 25 metrics across five benchmark datasets, improving MSE by 19.9%-48.8% compared to the second best method.

## Method Summary
The method uses REDRAFT, which combines multi-scale GIN layers (enhanced with residual connections and FFN) for graph-level embeddings with the DiffAtt fusion module. DiffAtt computes element-wise absolute differences between graph-level embeddings, refines them through MLP, and applies softmax attention to amplify divergent dimensions while suppressing shared structure. The fused representation passes through an MLP regressor to output similarity scores. The model is trained using MSE loss on five benchmark graph datasets.

## Key Results
- REDRAFT achieves state-of-the-art performance on 23 out of 25 metrics across five benchmark datasets
- Improves MSE by 19.9%-48.8% compared to the second best method
- RESAT test quantitatively shows DiffAtt better captures graph structural differences than other fusion modules

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DiffAtt captures graph structural difference more effectively than node-level fusion by using difference-based attention
- Mechanism: DiffAtt computes element-wise absolute difference between graph-level embeddings, passes it through MLP to refine, then uses softmax attention to amplify divergent dimensions while suppressing shared structure
- Core assumption: Graph edit distance is determined solely by graph structural difference under optimal alignment, not shared structure
- Evidence anchors:
  - [abstract] "We posit that the relative difference structure of the two graphs plays a key role in calculating their GED values"
  - [section 3] "Observation 1. Under the optimal mapping, the GED is solely determined by the graph structural difference of the two graphs, and their shared graph structure have no impact on their GED value"
  - [corpus] Weak - no direct corpus evidence for this specific attention mechanism claim
- Break condition: If graph-level embeddings fail to encode structural difference information or if attention weights become uniform across dimensions

### Mechanism 2
- Claim: Multi-scale GIN with residual connections and FFN generates more expressive graph-level embeddings for DiffAtt
- Mechanism: K-layer GIN aggregates neighborhood information at multiple scales, residual connections help optimization, FFN augments nonlinear transformations, GCA readout dynamically weights nodes based on global context
- Core assumption: Multi-scale representations capture both local and global structural patterns better than single-scale approaches
- Evidence anchors:
  - [section 4.2] "To promote model training optimization and boost the expressiveness of node embeddings, we enhance GIN layers with Residual Connections and FeedForward Neural Network (FFN)"
  - [section 5.1] "Residual Connections can help to improve the training process, while FFNs could enhance the embedding of nodes, thus augmenting GIN leads to a stable and huge enhancement effect"
  - [corpus] Weak - limited direct evidence for this specific multi-scale enhancement claim
- Break condition: If GIN expressiveness is insufficient for the dataset complexity or if residual connections cause optimization instability

### Mechanism 3
- Claim: RESAT quantifies DiffAtt's superiority in capturing graph structural difference through alignment testing
- Mechanism: Constructs remaining subgraph representing structural difference, tests similarity between fused embedding and graph structural difference embedding using MLP regression, lower MSE indicates better capture of structural difference
- Core assumption: The remaining subgraph maximally contains graph structural difference between a graph and its subgraph under optimal mapping
- Evidence anchors:
  - [section 6] "The remaining subgraph ¯Gi,j is constructed by deleting all edges of Gi,j in Gi and then deleting all isolated nodes, which maximally contains the graph structural difference between Gi and Gi,j in a single valid graph"
  - [section 6] "A lower MSE suggests the fused embedding has already encoded abundant graph structural difference features"
  - [corpus] Weak - no direct corpus evidence for this specific RESAT methodology
- Break condition: If remaining subgraph construction fails to capture true structural difference or if MLP regression cannot establish meaningful similarity

## Foundational Learning

- Concept: Graph Edit Distance (GED) as NP-hard optimization problem
  - Why needed here: Understanding that GED requires finding optimal edit path motivates the need for approximation through learned similarity
  - Quick check question: What makes exact GED computation intractable for large graphs?

- Concept: Graph Neural Networks and permutation invariance
  - Why needed here: GNN encoders must be permutation invariant to implicitly align graphs without explicit node-to-node matching
  - Quick check question: How does GIN achieve the same expressive power as the Weisfeiler-Lehman test?

- Concept: Attention mechanisms and their role in highlighting relevant features
  - Why needed here: DiffAtt uses attention to selectively amplify graph structural difference features while suppressing shared structure
  - Quick check question: What distinguishes DiffAtt's difference-based attention from standard self-attention?

## Architecture Onboarding

- Component map: Input graphs → Multi-scale GIN encoder (with residual + FFN) → Graph-level embeddings → DiffAtt fusion → Multi-scale fused embedding → MLP regressor → Similarity score
- Critical path: The encoder-fusion-regressor pipeline is the critical path; DiffAtt must receive meaningful graph-level embeddings to function
- Design tradeoffs: Graph-level fusion (efficient, scalable) vs node-level fusion (fine-grained, expensive); simpler DiffAtt vs complex node interactions
- Failure signatures: Poor MSE on benchmark datasets indicates DiffAtt not capturing structural difference; high training time/memory suggests node-level fusion inefficiency
- First 3 experiments:
  1. Replace DiffAtt with NTN or EFN and compare performance on AIDS700nef dataset
  2. Test DiffAtt with different GNN backbones (GCN vs GAT vs GIN) on LINUX dataset
  3. Vary temperature factor t in DiffAtt and measure impact on PTC dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DiffAtt perform on extremely large-scale graphs beyond the benchmark datasets?
- Basis in paper: [explicit] The paper notes REDRAFT's efficiency advantages but only validates on moderate-sized graphs up to 28.9 nodes on average.
- Why unresolved: The scalability to truly massive graphs with millions of nodes was not tested, leaving open whether DiffAtt retains benefits at scale.
- What evidence would resolve it: Benchmarking REDRAFT on graphs with 10^6+ nodes, comparing runtime and memory usage against node-level fusion methods.

### Open Question 2
- Question: Can the principles behind DiffAtt be extended to other graph learning tasks beyond GSC?
- Basis in paper: [inferred] The paper focuses on GED computation but DiffAtt's attention mechanism for highlighting structural differences could apply more broadly.
- Why unresolved: The paper only validates DiffAtt for GSC, not exploring applicability to tasks like graph classification or link prediction.
- What evidence would resolve it: Adapting DiffAtt to other graph tasks and evaluating performance gains over standard fusion methods.

### Open Question 3
- Question: What is the theoretical limit of DiffAtt's ability to capture structural differences compared to node-level fusion?
- Basis in paper: [inferred] The paper shows DiffAtt outperforms node-level methods empirically but does not analyze theoretical bounds.
- Why unresolved: No formal analysis of how much information DiffAtt can capture relative to optimal node-level fusion.
- What evidence would resolve it: Theoretical analysis or empirical studies on information-theoretic limits of graph-level vs node-level fusion.

## Limitations

- The paper's core claim about DiffAtt's superiority rests on the assumption that graph structural difference alone determines GED values, but this may not hold for all graph classes
- The RESAT test methodology, while innovative, lacks validation against established graph similarity metrics
- The paper does not thoroughly explore DiffAtt's sensitivity to hyperparameter choices like temperature scaling

## Confidence

- **High confidence**: REDRAFT achieves state-of-the-art results on benchmark datasets as empirically demonstrated
- **Medium confidence**: DiffAtt's efficiency advantage over node-level fusion is well-supported by theoretical complexity analysis
- **Low confidence**: The RESAT test conclusively proves DiffAtt better captures structural difference than alternatives

## Next Checks

1. **Cross-dataset generalization test**: Evaluate REDRAFT on larger, more diverse graph datasets (e.g., OGB graphs) to assess scalability limits
2. **Ablation study on attention mechanisms**: Systematically compare DiffAtt against other difference-based attention variants to isolate key success factors
3. **Human evaluation of structural difference**: Conduct user studies to verify that DiffAtt's attention weights align with human perception of graph structural differences