---
ver: rpa2
title: Simplifying Transformer Blocks
arxiv_id: '2311.01906'
source_url: https://arxiv.org/abs/2311.01906
tags:
- training
- block
- attention
- pre-ln
- skip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic study of simplifying the standard
  transformer block without compromising training speed or downstream task performance.
  By combining signal propagation theory and empirical observations, the authors demonstrate
  that skip connections, value parameters, projection parameters, sequential sub-blocks,
  and normalization layers can all be removed from transformer blocks while maintaining
  training efficiency.
---

# Simplifying Transformer Blocks

## Quick Facts
- arXiv ID: 2311.01906
- Source URL: https://arxiv.org/abs/2311.01906
- Reference count: 40
- Primary result: 15% faster training throughput with 15% fewer parameters while maintaining performance

## Executive Summary
This paper systematically investigates which components of standard transformer blocks can be removed without sacrificing training efficiency or downstream task performance. Through a combination of signal propagation theory and empirical validation, the authors demonstrate that skip connections, value parameters, projection parameters, sequential sub-blocks, and normalization layers can all be eliminated from transformer blocks while maintaining training speed. The simplified architecture achieves 15% faster training throughput and uses 15% fewer parameters compared to standard Pre-LN transformers. The work provides both practical efficiency gains and theoretical insights into the necessity of various architectural components.

## Method Summary
The authors develop simplified transformer blocks by removing multiple components from standard Pre-LN architecture. They use shaped attention initialization to maintain signal propagation without skip connections, fix value and projection parameters to identity matrices, convert sequential sub-blocks to parallel computation with shared skip connections, and remove normalization layers. The SAS block consists of shaped attention followed by MLP with only residual connections. The SAS-P block computes both sub-blocks in parallel with a single skip connection. Models are trained using AdamW optimizer with weight decay 0.1, linear decay learning rate schedule with 5% warmup, and evaluated on CodeParrot (decoder-only) and BERT (encoder-only) architectures.

## Key Results
- Simplified transformer blocks achieve 15% faster training throughput
- Parameter reduction of approximately 15% compared to standard transformers
- Performance matches or exceeds baselines when scaled to larger depths (up to 72 layers)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing skip connections is possible when attention matrices have strong identity components
- Mechanism: Shaped Attention initializes attention to be close to identity matrix, preventing rank collapse that would occur without skip connections
- Core assumption: Identity-like attention matrices preserve signal propagation in deep networks without skip connections
- Evidence anchors:
  - [abstract]: "Combining signal propagation theory and empirical observations, we motivate modifications that allow many block components to be removed with no loss of training speed, including skip connections"
  - [section]: "He et al. (2023) outline modifications needed to the self-attention mechanism in order to correct these signal degeneracies at large depths"
  - [corpus]: Weak - no corpus evidence for shaped attention specifically
- Break condition: If attention matrices lose identity component during training or initialization fails

### Mechanism 2
- Claim: Value and projection parameters are unnecessary because their product converges to identity
- Mechanism: During training, the product of value and projection matrices naturally evolves toward identity matrix, making them redundant
- Core assumption: The training dynamics naturally push WV·WP toward identity without explicit enforcement
- Evidence anchors:
  - [abstract]: "fixing the value and projection parameters, WV and WP , to the identity matrix significantly improves per-update training speed in skipless transformer blocks"
  - [section]: "In fact, we can also conclude from Fig. 3 that it is possible to completely remove the value and projection parameters WV, WP with minimal loss of per-update training speed"
  - [corpus]: Weak - Trockman & Kolter (2023) observed trained transformers have WV·WP with large identity component, but didn't prove it's always optimal
- Break condition: If training dynamics push WV·WP away from identity or initialization differs significantly

### Mechanism 3
- Claim: Parallel sub-blocks with shared normalization achieve efficiency gains without performance loss
- Mechanism: Computing MLP and attention sub-blocks in parallel with single skip connection reduces parameters and increases throughput while maintaining training speed
- Core assumption: Parallel computation of sub-blocks doesn't degrade signal propagation compared to sequential computation
- Evidence anchors:
  - [abstract]: "Our simplified transformers emulate the per-update training speed and performance of standard transformers, while enjoying 15% faster training throughput"
  - [section]: "The parallel transformer block is depicted in Fig. 1 (bottom right), and mathematically, it outputs Xout for a given input Xin"
  - [corpus]: Moderate - Wang & Komatsuki (2021) showed parallel blocks work well, but didn't combine with simplified attention
- Break condition: If parallel computation introduces race conditions or degrades gradient flow

## Foundational Learning

- Concept: Signal propagation theory
  - Why needed here: Explains why certain architectural components are necessary for stable training in deep networks
  - Quick check question: What happens to activation norms when signal propagation is poor?

- Concept: Skip connections and residual learning
  - Why needed here: Understanding how skip connections affect training dynamics and why they can sometimes be removed
  - Quick check question: How do skip connections implicitly downweight residual branches?

- Concept: Attention mechanism fundamentals
  - Why needed here: Required to understand how shaped attention modifies standard self-attention
  - Quick check question: What is the role of query-key dot products in attention computation?

## Architecture Onboarding

- Component map:
  - Standard Pre-LN block: LayerNorm → Attention → Add → LayerNorm → MLP → Add
  - SAS block: Shaped Attention → Add → MLP → Add (no LayerNorm, no skip connections)
  - SAS-P block: Shaped Attention || MLP → Add (parallel, no LayerNorm, no skip connections)

- Critical path:
  1. Input sequence through shaped attention
  2. Add residual (identity in SAS-P)
  3. Through MLP
  4. Add residual
  5. Output

- Design tradeoffs:
  - Removing normalization saves parameters but may reduce training stability
  - Identity values/projections reduce parameters but rely on training dynamics
  - Parallel computation increases throughput but may require careful synchronization

- Failure signatures:
  - Rank collapse: Model fails to train, activations become rank-deficient
  - Gradient explosion/vanishing: Training becomes unstable or extremely slow
  - Performance degradation: Model matches training speed but doesn't achieve good accuracy

- First 3 experiments:
  1. Replace standard attention with shaped attention in existing Pre-LN block
  2. Remove value and projection parameters from shaped attention
  3. Convert sequential block to parallel computation with shared skip connection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do identity-initialized value and projection matrices in skipless transformers lead to faster training speeds compared to random orthogonal initialization, even though both should provide similar signal propagation properties at initialization?
- Basis in paper: [explicit] The paper shows in Figure 3 that identity initialization of value and projection matrices leads to better training speed than random orthogonal initialization in skipless transformer blocks, despite both being equivalent from a signal propagation perspective at initialization.
- Why unresolved: The authors acknowledge this is surprising and do not have a rigorous proof for why identity initialization leads to faster training. They offer some half-explanations but no definitive answer.
- What evidence would resolve it: Systematic ablation studies varying initialization strategies while keeping other factors constant, combined with theoretical analysis of the training dynamics and loss landscapes for different initialization schemes.

### Open Question 2
- Question: How do normalization layers contribute to training speed and stability beyond what is captured by signal propagation theory, and can these benefits be replicated without normalization layers?
- Basis in paper: [explicit] The authors note in Section 4.4 that while normalization layers can be removed without compromising training speed from a signal propagation perspective, their experiments show a slight degradation in training speed per iteration when normalization is removed (Figure 23), suggesting normalization provides additional benefits.
- Why unresolved: The current signal propagation theory does not fully explain the benefits of normalization for training speed and stability, and the authors were unable to remove normalization while maintaining good downstream task performance.
- What evidence would resolve it: Detailed analysis of training dynamics (gradient norms, loss curvature, etc.) with and without normalization, combined with exploration of alternative architectural modifications that could replicate normalization's benefits.

### Open Question 3
- Question: Do the training speed improvements and architectural simplifications demonstrated in this paper scale to larger transformer models (e.g., 1B+ parameters), and what modifications might be necessary for optimal performance at scale?
- Basis in paper: [explicit] The authors acknowledge in the discussion section that the models considered (100-300M parameters) are small relative to the largest transformers, and note that Chowdhery et al. (2022) report parallel blocks improve relative to Pre-LN blocks with scale.
- Why unresolved: The paper only demonstrates results on relatively small models, and the authors explicitly call for investigation of performance at larger scales as future work.
- What evidence would resolve it: Scaling experiments on progressively larger models (100M, 1B, 10B+ parameters) measuring training speed, parameter efficiency, and downstream task performance, potentially with architectural modifications optimized for each scale.

## Limitations

- Results only validated on decoder-only autoregressive models and encoder-only BERT models, not encoder-decoder architectures
- Performance at extremely large scales (1B+ parameters) remains untested and may require architectural modifications
- The identity convergence of WV·WP during training is empirical observation, not theoretically proven property

## Confidence

**High Confidence**: Simplified transformer blocks achieve 15% faster training throughput; parameter reduction of approximately 15% is achievable; simplified blocks match baseline performance on CodeParrot and BERT pretraining

**Medium Confidence**: Skip connections can be removed when using shaped attention; value and projection parameters can be fixed to identity; parallel sub-blocks with shared normalization maintain efficiency

**Low Confidence**: Simplified blocks will continue to show improvements at scales beyond those tested; shaped attention initialization will generalize to all transformer applications; the identity convergence of WV·WP is a universal property

## Next Checks

1. **Architecture Transfer Test**: Implement and evaluate simplified transformer blocks in encoder-decoder transformer architectures (e.g., T5 or BART) to assess generalizability beyond decoder-only and encoder-only models.

2. **Longer Training Horizon Analysis**: Extend training to 100K+ steps for both CodeParrot and BERT models to identify potential degradation modes that may emerge over longer training periods.

3. **Initialization Sensitivity Study**: Systematically vary the shaped attention initialization parameters (αh, βh, γh) and measure their impact on training stability and convergence speed across different model depths.