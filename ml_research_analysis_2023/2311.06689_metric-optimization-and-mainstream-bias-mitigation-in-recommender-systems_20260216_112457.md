---
ver: rpa2
title: Metric Optimization and Mainstream Bias Mitigation in Recommender Systems
arxiv_id: '2311.06689'
source_url: https://arxiv.org/abs/2311.06689
tags:
- users
- recommendation
- user
- nrbp
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This thesis addresses two key challenges in recommender systems:
  optimizing ranking accuracy and mitigating mainstream bias. The first part explores
  metric optimization, challenging the assumption that the same metric used for evaluation
  should be used for training.'
---

# Metric Optimization and Mainstream Bias Mitigation in Recommender Systems

## Quick Facts
- arXiv ID: 2311.06689
- Source URL: https://arxiv.org/abs/2311.06689
- Reference count: 0
- Key outcome: Optimizing RBP-inspired losses and using NAECF with cost-sensitive learning effectively improve recommendation accuracy and mitigate mainstream bias.

## Executive Summary
This thesis addresses two key challenges in recommender systems: optimizing ranking accuracy and mitigating mainstream bias. The first part explores metric optimization, challenging the assumption that the same metric used for evaluation should be used for training. Experimental results on multiple datasets show that optimizing Rank-Biased Precision (RBP) leads to higher accuracy than directly optimizing evaluation metrics like nDCG or AP. The second part focuses on mainstream bias, where popular users dominate recommendations, disadvantaging niche users. The proposed Neural AutoEncoder Collaborative Filtering (NAECF) model introduces adversarial conditions via autoencoders to preserve unique user features, improving recommendations for non-mainstream users without harming mainstream users. Additionally, a cost-sensitive learning approach assigns higher weights to non-mainstream users during training, further mitigating bias. The work provides actionable insights for balancing recommendation accuracy across diverse user groups.

## Method Summary
The research explores two main approaches: metric optimization and mainstream bias mitigation. For metric optimization, the study compares different ranking metrics (RR, AP, nDCG, RBP) as optimization targets using Matrix Factorization with pairwise and listwise learning-to-rank. For bias mitigation, NAECF introduces autoencoder layers to preserve unique user properties, while cost-sensitive learning assigns weights based on user mainstreamness. Experiments use four real-world datasets with Monte Carlo cross-validation, evaluating ranking performance across multiple metrics and analyzing user group behavior.

## Key Results
- Optimizing RBP-inspired losses consistently outperforms optimizing evaluation metrics like nDCG or AP for ranking accuracy
- NAECF model effectively preserves unique user features through adversarial autoencoders, improving recommendations for non-mainstream users
- Cost-sensitive learning with mainstreamness-based weights further mitigates bias without harming mainstream user performance
- The assumption that optimizing the same metric used for evaluation yields optimal performance is challenged

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimizing RBP-inspired losses achieves higher recommendation accuracy than optimizing the same metric used for evaluation.
- Mechanism: RBP (Rank-Biased Precision) incorporates a persistence parameter that models user willingness to explore deeper into ranked lists, allowing the model to exploit more user preference information during training. This leads to better ranking effectiveness compared to metrics like RR, nDCG, or AP when used as optimization targets.
- Core assumption: RBP is more informative than other metrics and can capture a broader range of evaluation criteria.
- Evidence anchors:
  - [abstract]: "Experimental results on multiple datasets show that optimizing Rank-Biased Precision (RBP) leads to higher accuracy than directly optimizing evaluation metrics like nDCG or AP."
  - [section]: "Empirical results obtained on four real-world datasets point to the following main insights: 1. The assumption behind the practice to optimize and evaluate ranking-based recommender systems using the same metric does not necessarily lead to the best performance. 2. RBP-inspired losses perform at least as well as other metrics in a consistent way, and offer clear benefits in several cases."
  - [corpus]: Weak. Related papers focus on mitigating mainstream bias but do not directly address RBP optimization effectiveness.

### Mechanism 2
- Claim: NAECF introduces adversarial conditions via autoencoders to preserve unique user features, improving recommendations for non-mainstream users.
- Mechanism: NAECF adds autoencoder layers to a DNN-based recommendation framework. These autoencoders act as adversaries to the rating prediction process, enforcing that the learned user and item representations preserve their specific and unique properties before being fed to the rating predictor. This prevents the learned representations from being biased towards mainstream users.
- Core assumption: Preserving unique user properties in the learned representations leads to more balanced recommendation utility across all users.
- Evidence anchors:
  - [abstract]: "The proposed Neural AutoEncoder Collaborative Filtering (NAECF) model introduces adversarial conditions via autoencoders to preserve unique user features, improving recommendations for non-mainstream users without harming mainstream users."
  - [section]: "With our new recommendation model, referred to as Neural AutoEncoder Collaborative Filtering (NAECF), we introduce adversarial conditions to the process of learning the recommendation algorithm, which in this specific case is realized as the minimization of the rating prediction error. The adversarial conditions are imposed by autoencoders... They enforce that the user and item representations are learned in a way such that they preserve their specific and unique properties before being fed to the rating predictor."
  - [corpus]: Weak. Related papers focus on popularity bias but do not directly address the use of autoencoders for preserving unique user features.

### Mechanism 3
- Claim: Cost-sensitive learning assigns higher weights to non-mainstream users during training, focusing the model on improving their recommendation accuracy.
- Mechanism: The proposed approach defines a weight function Ï‰(u) that is a function of the user's mainstreamness mu. Mainstreamness is quantified using either an explicit approach (e.g., user similarity) or an implicit approach (e.g., recommendation accuracy). The weight function maps mainstreamness to a cost value, with higher costs assigned to non-mainstream users. This way, the model is pushed to focus more on non-mainstream users and improve their recommendation accuracy.
- Core assumption: Recommendation accuracy is a good proxy for mainstreamness, and assigning higher weights to non-mainstream users leads to improved recommendations for them.
- Evidence anchors:
  - [abstract]: "The proposed Neural AutoEncoder Collaborative Filtering (NAECF) model introduces adversarial conditions via autoencoders to preserve unique user features, improving recommendations for non-mainstream users without harming mainstream users."
  - [section]: "We first examine how Sim and Util differentiate between mainstream and non-mainstream users. In particular, we are interested in how well they correlate with the test nDCG scores obtained by the baseline FM model: non-mainstream users should receive recommendations with low nDCG scores, while mainstream users should receive higher scores."
  - [corpus]: Weak. Related papers focus on mitigating popularity bias but do not directly address the use of cost-sensitive learning for mainstream bias mitigation.

## Foundational Learning

- Concept: Matrix Factorization (MF)
  - Why needed here: MF is used as the base recommendation model in the experiments to study the relative merits of different optimization metrics.
  - Quick check question: How does MF represent users and items in a latent factor space?

- Concept: Autoencoders (AE)
  - Why needed here: AEs are used in NAECF to enforce the preservation of unique user and item properties in the learned representations.
  - Quick check question: How do AEs work to reconstruct the input data, and how can they be used as adversaries in a recommendation model?

- Concept: Cost-sensitive learning
  - Why needed here: Cost-sensitive learning is used to assign higher weights to non-mainstream users during training, focusing the model on improving their recommendation accuracy.
  - Quick check question: How does cost-sensitive learning differ from traditional learning approaches, and how can it be used to address class imbalance or bias in the data?

## Architecture Onboarding

- Component map:
  - Data preprocessing: User-item interaction data, user and item features (e.g., reviews), data splitting
  - Recommendation model: Matrix Factorization, Neural AutoEncoder Collaborative Filtering (NAECF), Factorization Machines (FM)
  - Optimization: Loss functions (e.g., RBP-inspired losses, rating prediction loss, text reconstruction losses), optimization algorithms (e.g., SGD, Adam)
  - Evaluation: Ranking metrics (e.g., nDCG, AP, RR, RBP), accuracy metrics (e.g., RMSE), user group analysis

- Critical path: Data preprocessing -> Recommendation model training -> Optimization -> Evaluation -> Analysis of results

- Design tradeoffs:
  - Pairwise vs. listwise learning-to-rank: Pairwise approaches consider item pairs, while listwise approaches consider the entire ranked list. Pairwise methods are simpler but may not fully capture the ranking context.
  - Explicit vs. implicit mainstreamness quantification: Explicit approaches rely on predefined notions of mainstreamness (e.g., user similarity), while implicit approaches use recommendation accuracy as a proxy. Explicit approaches may be more interpretable but may not capture the model's perspective on mainstreamness.

- Failure signatures:
  - Poor ranking performance: If the chosen optimization metric does not effectively capture the desired ranking properties, the model may not achieve high accuracy on the evaluation metrics.
  - Imbalanced user group performance: If the model is biased towards mainstream users, non-mainstream users may receive poor recommendations. This can be identified by analyzing the performance across different user groups.

- First 3 experiments:
  1. Compare the performance of different optimization metrics (e.g., RR, AP, nDCG, RBP) on a benchmark dataset using a simple recommendation model (e.g., MF).
  2. Implement NAECF and evaluate its performance on a dataset with rich user and item features (e.g., reviews) compared to a baseline model (e.g., DeepCoNN).
  3. Apply cost-sensitive learning to a recommendation model and evaluate its effectiveness in mitigating mainstream bias by analyzing the performance across different user groups.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the effectiveness of RBP-inspired losses for ranking-based recommendation be further improved or generalized to other recommendation models and scenarios?
- Basis in paper: [explicit] The authors state that RBP-inspired losses perform at least as well as other metrics and offer clear benefits in several cases, but the extent to which these findings generalize to other settings is left for future work.
- Why unresolved: The paper only investigates RBP-inspired losses in the context of Matrix Factorization and binary relevance. It is unclear whether these findings hold for other recommendation models (e.g., deep learning-based) and scenarios (e.g., multi-level relevance).
- What evidence would resolve it: Conducting experiments with a wider range of recommendation models and scenarios, including different types of user feedback (e.g., explicit ratings) and larger datasets, would provide insights into the generalizability of RBP-inspired losses.

### Open Question 2
- Question: What are the optimal ways to define and measure mainstreamness in recommender systems, and how do these definitions impact the effectiveness of bias mitigation strategies?
- Basis in paper: [explicit] The authors explore both explicit and implicit approaches to quantifying mainstreamness, but acknowledge that defining mainstreamness is complex and that different definitions may lead to different mainstreamness score distributions.
- Why unresolved: The paper does not provide a definitive answer on which approach is superior or how to choose the best definition for a given context. Additionally, the impact of different mainstreamness definitions on the effectiveness of bias mitigation strategies is not fully explored.
- What evidence would resolve it: Conducting a comprehensive study comparing different mainstreamness definitions and their impact on bias mitigation effectiveness across various datasets and recommendation scenarios would provide valuable insights.

### Open Question 3
- Question: How can the combination of adversarial learning and cost-sensitive learning strategies be effectively leveraged to mitigate mainstream bias in recommender systems?
- Basis in paper: [explicit] The authors propose NAECF, which combines adversarial learning with rating prediction to preserve user features, and a cost-sensitive learning approach that assigns weights to users based on their mainstreamness. However, the potential synergy between these two approaches is not fully explored.
- Why unresolved: The paper does not investigate how to effectively combine adversarial learning and cost-sensitive learning strategies to achieve optimal bias mitigation. It is unclear whether these approaches complement each other or whether one is more effective than the other.
- What evidence would resolve it: Conducting experiments that compare the effectiveness of different combinations of adversarial learning and cost-sensitive learning strategies, as well as exploring their potential synergy, would provide insights into the optimal approach for mitigating mainstream bias.

## Limitations

- The experimental validation relies heavily on synthetic mainstreamness definitions rather than ground-truth user preferences
- The effectiveness of RBP optimization may be dataset-dependent given the specific characteristics of the four evaluated datasets
- NAECF's effectiveness depends on the quality of side information (reviews) and pre-trained embeddings
- Cost-sensitive learning assumes recommendation accuracy is a reliable proxy for mainstreamness

## Confidence

- High confidence: The finding that optimizing the same metric used for evaluation does not necessarily yield optimal performance is well-supported by experimental results across multiple datasets.
- Medium confidence: The effectiveness of RBP-inspired losses in improving ranking accuracy is demonstrated but may vary with different dataset characteristics and user behavior patterns.
- Medium confidence: The NAECF model's ability to mitigate mainstream bias is supported by experiments, but the reliance on side information quality introduces uncertainty.
- Low confidence: The cost-sensitive learning approach's effectiveness is demonstrated but depends heavily on the accuracy of mainstreamness quantification, which may not generalize.

## Next Checks

1. Validate the mainstreamness definitions by comparing them against ground-truth user preference diversity measures across different datasets.
2. Test the RBP optimization benefits on datasets with varying user activity distributions to assess generalizability.
3. Evaluate NAECF's performance when side information quality varies (e.g., using synthetic or noisy review data) to understand robustness to data quality.