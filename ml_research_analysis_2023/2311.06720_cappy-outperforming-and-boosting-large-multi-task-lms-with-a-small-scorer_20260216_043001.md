---
ver: rpa2
title: 'Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small Scorer'
arxiv_id: '2311.06720'
source_url: https://arxiv.org/abs/2311.06720
tags:
- cappy
- tasks
- llms
- multi-task
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Cappy, a small scorer designed to enhance the
  performance and efficiency of large multi-task language models (LLMs) like T0, FLAN,
  and OPT-IML. Cappy, with only 360 million parameters, addresses the challenges of
  high computational costs and limited adaptability associated with large LLMs.
---

# Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small Scorer

## Quick Facts
- arXiv ID: 2311.06720
- Source URL: https://arxiv.org/abs/2311.06720
- Authors: 
- Reference count: 36
- One-line primary result: Cappy, a 360M parameter scorer, outperforms much larger multi-task LLMs on 11 PromptSource tasks and consistently boosts FLAN-T5 performance on 45 BIG-Bench tasks.

## Executive Summary
This paper introduces Cappy, a lightweight 360M parameter scorer designed to enhance the performance and efficiency of large multi-task language models. Cappy addresses the high computational costs and limited adaptability of large LLMs by functioning either independently on classification tasks or as an auxiliary component for LLMs. It enables efficient downstream adaptation without requiring LLM finetuning or access to their parameters, and demonstrates strong performance across diverse language understanding benchmarks.

## Method Summary
Cappy is a lightweight pretrained scorer based on RoBERTa with a linear regression head, trained using weakly-supervised data augmentation through existing multi-task LLMs. The pretraining dataset is constructed by pairing instructions with both correct and incorrect responses, plus augmented responses generated by LLMs, annotated with similarity scores (Rouge-L) as regression targets. During inference, Cappy scores multiple LLM-generated candidates and selects the highest-scoring response. Cappy can be finetuned on downstream regression data for task-specific adaptation, offering an efficient alternative to LLM finetuning.

## Key Results
- Cappy outperforms much larger multi-task LLMs on 11 language understanding tasks from PromptSource.
- On 45 complex tasks from BIG-Bench, Cappy consistently boosts the performance of FLAN-T5 by a large margin.
- Cappy offers extra performance enhancement when combined with other LLM adaptations like finetuning and in-context learning.

## Why This Works (Mechanism)

### Mechanism 1
Cappy improves LLM performance by scoring candidate responses instead of generating them directly. It acts as a lightweight regressor that scores instruction-response pairs, selecting the highest-scoring candidate from multiple LLM-generated responses during inference, avoiding costly LLM finetuning.

### Mechanism 2
Weakly supervised pretraining with data augmentation enables Cappy to generalize across diverse tasks. The pretraining dataset is constructed by pairing instructions with correct/incorrect responses and augmented LLM generations, annotated with similarity scores (Rouge-L) to provide regression targets.

### Mechanism 3
Downstream adaptation via Cappy finetuning is more efficient than LLM adaptation methods. Cappy can be finetuned on task-specific regression data constructed from instruction-response pairs and weak supervision, without backpropagating through LLM parameters.

## Foundational Learning

- **Concept**: Regression modeling with weakly supervised labels
  - **Why needed here**: Cappy must learn to score correctness without strong labels; weakly supervised data augmentation is the solution.
  - **Quick check question**: Can you construct a regression dataset from classification and generation tasks using only model-generated responses and similarity metrics?

- **Concept**: Instruction-response template alignment (PromptSource-style)
  - **Why needed here**: Ensures Cappy's pretraining data matches the instruction-following paradigm used by LLMs.
  - **Quick check question**: How would you convert a labeled dataset into (instruction, response, score) triples for regression pretraining?

- **Concept**: Candidate selection strategy in decoding
  - **Why needed here**: Cappy's role is to choose the best among multiple LLM outputs, not to generate from scratch.
  - **Quick check question**: What are the trade-offs between selecting from 4 vs. 17 LLM-generated candidates in terms of accuracy and compute?

## Architecture Onboarding

- **Component map**: RoBERTa backbone -> Linear regression head -> Scoring function (0-1 output) -> Pretraining data pipeline: Template conversion -> Augmentation (LLM generations) -> Rouge-L scoring -> Dataset -> Inference pipeline: LLM generates candidates -> Cappy scores each -> Argmax selection
- **Critical path**: Pretraining -> Finetuning (optional) -> Candidate selection during inference
- **Design tradeoffs**:
  - Size vs. accuracy: 360M CappyLARGE outperforms much larger LLMs, but a smaller 120M variant exists.
  - Weak supervision quality vs. dataset diversity: More augmentation improves generalization but may introduce noise.
  - Candidate set size vs. latency: Larger sets improve selection but increase scoring cost.
- **Failure signatures**:
  - Pretraining collapse: Scores become uniform (0 or 1) -> no learning signal.
  - Overfitting to pretraining tasks: Poor downstream adaptation -> check generalization gap.
  - Poor candidate selection: Scores poorly correlated with correctness -> revisit similarity metric or augmentation.
- **First 3 experiments**:
  1. Verify pretraining convergence: train Cappy on synthetic instruction-response pairs with known scores; check if model learns correct ranking.
  2. Ablation on augmentation: train with and without LLM-generated candidates; compare zero-shot task performance.
  3. Adaptation efficiency test: finetune Cappy on a small downstream dataset; measure memory usage vs. LLM finetuning baseline.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions. However, based on the content, some implicit open questions include:
- What specific scoring metric, other than Rouge-L, could provide better weak supervision for Cappy's pretraining data construction?
- How does Cappy's performance scale with the number of candidate responses generated by the backbone LLM?
- Can Cappy be effectively used to select between outputs from multiple different LLMs, rather than just enhancing a single LLM?

## Limitations
- Weak supervision validity: The reliance on Rouge-L similarity as a proxy for correctness may break down for tasks requiring deeper reasoning or specialized knowledge.
- Candidate generation bottleneck: The approach still requires generating multiple LLM candidates, which may offset Cappy's efficiency gains in high-latency applications.
- Generalization across domains: While strong results on BIG-Bench are shown, the paper doesn't analyze whether Cappy's pretraining data adequately represents the diverse domains in BIG-Bench.

## Confidence

- **High confidence**: Cappy's architecture and pretraining methodology are clearly specified and reproducible. The experimental results showing Cappy outperforming much larger LLMs on PromptSource tasks are well-documented and verifiable.
- **Medium confidence**: The claim that Cappy consistently boosts FLAN-T5's performance on BIG-Bench is supported by experiments, but the paper doesn't provide error analysis or identify failure cases.
- **Low confidence**: The assertion that Cappy's weakly-supervised pretraining generalizes to all BIG-Bench tasks without task-specific adaptation is not fully validated.

## Next Checks

1. **Cross-task robustness analysis**: Systematically evaluate Cappy on BIG-Bench tasks stratified by category (reasoning, QA, coding, etc.) to identify which domains benefit most and which show degradation or ceiling effects.

2. **Candidate set size ablation**: Conduct experiments varying the number of LLM-generated candidates (k=4, 8, 16, 32) to quantify the trade-off between selection accuracy and computational cost.

3. **Weak supervision ablation**: Replace Rouge-L with alternative weak supervision signals (e.g., LLM self-consistency scores, perplexity thresholds) and compare pretraining outcomes to validate whether the specific choice of similarity metric is critical to Cappy's success.