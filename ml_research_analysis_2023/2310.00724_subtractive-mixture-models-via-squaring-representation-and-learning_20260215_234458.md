---
ver: rpa2
title: 'Subtractive Mixture Models via Squaring: Representation and Learning'
arxiv_id: '2310.00724'
source_url: https://arxiv.org/abs/2310.00724
tags:
- input
- layer
- monotonic
- circuit
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces squared non-monotonic probabilistic circuits
  (NPC2s) as a general framework for learning subtractive mixture models. Unlike traditional
  mixture models that add components, NPC2s allow for negative mixture weights by
  squaring the linear combination of components.
---

# Subtractive Mixture Models via Squaring: Representation and Learning

## Quick Facts
- arXiv ID: 2310.00724
- Source URL: https://arxiv.org/abs/2310.00724
- Reference count: 40
- Primary result: NPC2s can be exponentially more expressive than traditional additive mixtures while maintaining tractable inference.

## Executive Summary
This paper introduces squared non-monotonic probabilistic circuits (NPC2s) as a framework for learning subtractive mixture models. By squaring a linear combination of components, NPC2s enable negative mixture weights while preserving tractability, allowing efficient modeling of complex distributions with "holes." The authors theoretically prove that NPC2s can be exponentially more expressive than monotonic circuits and empirically demonstrate their effectiveness on various tasks including density estimation and language model distillation.

## Method Summary
The paper presents NPC2s as probabilistic circuits formed by squaring tensorized circuits. The key innovation is Algorithm 1, which transforms a tensorized circuit with parameters w into a structured-decomposable circuit with squared parameters w². Learning is performed by maximizing log-likelihood using stochastic gradient descent with Adam optimizer. The approach generalizes several existing models including positive semi-definite models and Born machines, providing a unified perspective on tractable probabilistic modeling with subtractive capabilities.

## Key Results
- NPC2s can be exponentially more expressive than monotonic circuits for the same size
- Empirical evaluation shows improved performance on synthetic and real-world density estimation tasks
- NPC2s effectively distill large language models, matching or exceeding monotonic PCs
- The framework unifies PSD models, Born machines, and other tractable models as special cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Squaring a linear combination of components enables subtractive mixture models while preserving tractability.
- Mechanism: By squaring a linear combination c(x) = Σ wᵢcᵢ(x), we obtain c²(x) = Σᵢ Σⱼ wᵢwⱼcᵢ(x)cⱼ(x), which guarantees non-negativity while allowing negative mixture weights wᵢ, wⱼ. This creates "holes" in the distribution efficiently.
- Core assumption: The product cᵢ(x)cⱼ(x) of two component functions must be tractably integrable to enable efficient normalization and marginalization.
- Evidence anchors:
  - [abstract] "allowing mixtures to subtract probability mass or density can drastically reduce the number of components needed"
  - [section 2] "Squaring a mixture with K = 3 components can yield more components that share parameters"
  - [corpus] Weak evidence for squaring mechanisms in neighbor papers
- Break condition: If component products cannot be tractably integrated (e.g., neural networks as components), the approach loses its efficiency advantage.

### Mechanism 2
- Claim: Tensorized probabilistic circuits enable exponential expressiveness gains over shallow mixtures.
- Mechanism: By organizing mixture components in a deep tensorized structure following a tree region graph, NPC²s can represent an exponential number of components while maintaining polynomial size and tractable inference.
- Core assumption: The region graph must be a tree structure to enable efficient squaring while preserving structured-decomposability.
- Evidence anchors:
  - [abstract] "the class of squared circuits allowing subtractions can be exponentially more expressive than traditional additive mixtures"
  - [section 3] "Deep PCs are already more expressive efficient than shallow MMs as they compactly encode a mixture with an exponential number of components"
  - [corpus] Neighbor paper "Sum of Squares Circuits" supports exponential expressiveness claims
- Break condition: If the circuit structure becomes non-tree or non-structured-decomposable, squaring becomes #P-hard and tractable inference is lost.

### Mechanism 3
- Claim: Negative parameters enable modeling of complex distributions with "holes" using exponentially fewer components than monotonic models.
- Mechanism: Negative mixture weights allow subtraction of probability mass, creating holes in the distribution that would require exponentially many positive components to approximate in traditional models.
- Core assumption: The function class to be modeled contains distributions with significant "holes" or negative regions that benefit from subtractive modeling.
- Evidence anchors:
  - [abstract] "Allowing mixtures to subtract probability mass or density can drastically reduce the number of components needed to model complex distributions"
  - [section 4.1] "NPC²s (and thus the aforementioned models) can be more expressive for a given size"
  - [corpus] Neighbor paper "Scalable Expectation Estimation with Subtractive Mixture Models" confirms practical advantages
- Break condition: If target distributions are smooth and hole-free, subtractive modeling provides no advantage over traditional additive mixtures.

## Foundational Learning

- Concept: Probabilistic circuits and their structural properties (smoothness, decomposability, structured-decomposability)
  - Why needed here: These properties determine when tractable inference is possible and when circuits can be efficiently squared
  - Quick check question: Given a circuit structure, can you determine whether it's structured-decomposable and thus squarable via Algorithm 1?

- Concept: Tensor network representations and their relationship to probabilistic models
  - Why needed here: Understanding Born machines and tensor trains helps explain why NPC²s generalize these models and maintain tractability
  - Quick check question: Can you map a tensor train representation to its equivalent NPC² circuit structure?

- Concept: Integration of product distributions and computational complexity
  - Why needed here: The tractability of NPC²s depends on being able to efficiently integrate products of component functions
  - Quick check question: For Gaussian components, what is the computational complexity of integrating their product?

## Architecture Onboarding

- Component map: Input layers -> Product layers -> Sum layers -> Output layer -> Squared circuit
- Critical path: Input → Product layers → Sum layers → Output → Squaring → Normalization/Marginalization
- Design tradeoffs:
  - Depth vs. width: Deeper circuits can represent more complex distributions but increase computational cost
  - Component choice: Gaussians are simple but splines offer more flexibility for NPC²s
  - Tree structure: Linear vs. binary tree RGs affect expressiveness and computational efficiency
- Failure signatures:
  - Numerical underflow/overflow in log-space computations
  - Partition function Z = 0 indicating invalid negative parameter combinations
  - Degenerate solutions where c(x) ≈ 0 everywhere
- First 3 experiments:
  1. Implement and verify Algorithm 1 on a simple 2D ring distribution with spline components
  2. Compare monotonic vs. NPC² performance on discrete synthetic data with "holes"
  3. Test scaling on UCI datasets with Gaussian input layers and linear tree RG structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can NPC2s be effectively learned when their sub-circuits do not have a probabilistic interpretation due to negative parameters?
- Basis in paper: [explicit] The paper mentions that negative parameters in NPC2s invalidate the probabilistic interpretation of sub-circuits, making it impossible to learn their structure and parameters using classical methods.
- Why unresolved: The paper acknowledges this as a limitation but does not provide a solution for learning NPC2s when negative parameters are present.
- What evidence would resolve it: Developing a learning algorithm that can handle negative parameters in NPC2s, either by finding a way to interpret the sub-circuits probabilistically or by creating a new learning method that does not rely on this interpretation.

### Open Question 2
- Question: Can the expressive efficiency advantage of NPC2s over monotonic PCs be extended to decomposable monotonic PCs?
- Basis in paper: [inferred] The paper conjectures that an analogous lower bound can be devised for decomposable monotonic PCs, but does not provide a proof.
- Why unresolved: The paper only proves the exponential separation for structured-decomposable monotonic PCs, leaving the question open for decomposable monotonic PCs.
- What evidence would resolve it: A formal proof or counterexample showing whether the exponential separation holds for decomposable monotonic PCs as well.

### Open Question 3
- Question: How do NPC2s perform on image density estimation tasks compared to monotonic PCs?
- Basis in paper: [inferred] The paper mentions that preliminary results did not show an improvement of NPC2s over monotonic PCs on image density estimation, but does not provide detailed experimental results.
- Why unresolved: The paper does not provide sufficient evidence or explanation for the lack of improvement on image density estimation tasks.
- What evidence would resolve it: Conducting more extensive experiments on image density estimation tasks, comparing NPC2s and monotonic PCs, and analyzing the results to understand the reasons behind the observed performance difference.

### Open Question 4
- Question: How can the advancements in tensor networks be leveraged to improve NPC2s and vice versa?
- Basis in paper: [explicit] The paper mentions the connection between NPC2s and tensor networks, suggesting that advancements in one field could be carried over to the other.
- Why unresolved: The paper does not provide specific examples or research directions on how to leverage the advancements in tensor networks to improve NPC2s or vice versa.
- What evidence would resolve it: Identifying specific advancements in tensor networks (e.g., better learning schemes, more flexible ways to factorize high-dimensional tensors) that can be applied to NPC2s, and conducting experiments to demonstrate the improvements in performance.

## Limitations

- The framework requires tree-structured region graphs for tractable squaring, limiting architectural flexibility
- Learning with negative parameters is challenging since sub-circuits lose their probabilistic interpretation
- No clear advantage was observed for image density estimation tasks compared to monotonic PCs
- The approach may not provide benefits for distributions without significant "holes" or subtractive structure

## Confidence

- **High**: The theoretical foundations of NPC2s and the correctness of the squaring algorithm for tree-structured circuits
- **Medium**: The practical advantages of NPC2s for density estimation on benchmark datasets
- **Medium**: The claim that NPC2s generalize existing models like PSDs and Born machines

## Next Checks

1. **Structural analysis**: Systematically characterize the types of distributions where NPC2s provide significant advantages over monotonic circuits, quantifying the relationship between distribution complexity and required components.

2. **Alternative structure evaluation**: Compare linear vs. binary tree region graphs across diverse datasets to provide guidance on optimal circuit structure selection.

3. **Cross-task validation**: Evaluate NPC2s on conditional inference and marginal inference tasks beyond density estimation to assess broader applicability.