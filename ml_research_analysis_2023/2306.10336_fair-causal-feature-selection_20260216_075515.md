---
ver: rpa2
title: Fair Causal Feature Selection
arxiv_id: '2306.10336'
source_url: https://arxiv.org/abs/2306.10336
tags:
- faircfs
- variable
- information
- causal
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FairCFS, a fair causal feature selection algorithm
  based on class-specific mutual information. It addresses the problem of unfairness
  in existing causal feature selection algorithms that use the same Markov blanket
  for all classes.
---

# Fair Causal Feature Selection

## Quick Facts
- arXiv ID: 2306.10336
- Source URL: https://arxiv.org/abs/2306.10336
- Authors: 
- Reference count: 40
- Key outcome: FairCFS achieves comparable accuracy to state-of-the-art feature selection algorithms while providing superior fairness by identifying unique causal features for each class state.

## Executive Summary
This paper introduces FairCFS, a fair causal feature selection algorithm that addresses the problem of unfairness in existing methods that use the same Markov blanket for all classes. FairCFS uses class-specific mutual information (CSMI) to identify unique causal features for each state of the class variable through pairwise comparisons of CSMI values. The algorithm follows a divide-and-conquer framework to discover parents-children and spouse variables for each class state, improving both fairness and efficiency.

## Method Summary
FairCFS employs a divide-and-conquer framework that first identifies parent-child (PC) variables using class-specific mutual information comparisons, then discovers spouse variables through conditional mutual information. The algorithm computes CSMI between each class state and features, selecting features that maximize CSMI for that specific state. This state-specific approach contrasts with traditional methods that use a common Markov blanket across all class states. The method requires discretization of continuous variables and uses conditional independence tests to build localized causal graphs.

## Key Results
- FairCFS achieves comparable classification accuracy to eight state-of-the-art feature selection algorithms across seven real-world datasets
- The algorithm demonstrates superior fairness by selecting different causal features for each class state rather than using a common Markov blanket
- FairCFS shows efficiency gains through its divide-and-conquer approach, reducing sample requirements compared to simultaneous discovery methods

## Why This Works (Mechanism)

### Mechanism 1
Class-specific mutual information captures the unique dependence of each class state on features, enabling state-specific causal feature identification. CSMI measures information between a specific class state and a feature, allowing FairCFS to distinguish features uniquely informative for one state versus generally informative for all states. The method assumes class states have sufficiently distinct distributions that CSMI values will differ meaningfully across states.

### Mechanism 2
The divide-and-conquer framework improves accuracy and reduces sample requirements by separating PC and spouses discovery. FairCFS first identifies parent-child variables using CSMI, then identifies spouses using conditional CSMI, reducing conditioning set size for more reliable tests with limited samples. The method assumes conditional independence tests are more reliable when conditioning sets are smaller.

### Mechanism 3
FairCFS provides fairness by selecting different causal features for each class state, rather than using a common MB for all states. By computing CSMI for each state and selecting features that maximize CSMI for that state, FairCFS ensures each state's unique predictive features are included, preventing unfair treatment of minority states. The method assumes the optimal feature set varies by class state and that using a common MB can disadvantage some states.

## Foundational Learning

- **Mutual Information (MI)**: Measures the dependence between two variables. Why needed: FairCFS uses MI and CMI to measure dependence between features and class states. Quick check: If X and Y are independent, what is their mutual information I(X;Y)?
- **Markov Blanket**: The optimal feature subset for classification containing parents, children, and spouses of a node. Why needed: FairCFS aims to identify the MB for each class state. Quick check: What variables are included in the Markov Blanket of a node in a Bayesian network?
- **Bayesian Networks and D-separation**: Framework for representing causal relationships and testing conditional independence. Why needed: FairCFS is based on causal feature selection in Bayesian networks. Quick check: In a Bayesian network, when are two variables conditionally independent given a set of variables?

## Architecture Onboarding

- **Component map**: Data preprocessing -> CSMI computation -> PC discovery -> Spouse discovery -> Feature selection output -> Evaluation
- **Critical path**: Data → CSMI computation → PC discovery → Spouse discovery → Output MBs → Classification
- **Design tradeoffs**: Accuracy vs. efficiency (CSMI-based methods may be more accurate but computationally expensive); Fairness vs. simplicity (state-specific MBs are fairer but increase complexity); Discretization vs. continuous handling (discretization is simpler but may lose information)
- **Failure signatures**: Low CSMI values across all states (poor feature relevance or need for better discretization); High variance in CSMI across folds (instability or insufficient data); Similar MBs across states (states not sufficiently distinct)
- **First 3 experiments**: 1) Test CSMI computation on a small synthetic dataset with known state-specific dependencies; 2) Compare PC discovery accuracy of FairCFS vs. a baseline method on a benchmark dataset; 3) Evaluate fairness improvements by measuring classification accuracy per class state before and after applying FairCFS

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several key unresolved issues emerge:

1. Can FairCFS be directly extended to handle continuous features without discretization?
2. How does the fairness achieved by FairCFS compare to non-causal fair feature selection methods?
3. What is the impact of sample size on the fairness and accuracy of FairCFS across different class distributions?

## Limitations

- FairCFS requires discretization of continuous features, which may lead to information loss and impact performance
- The method's fairness claims lack explicit fairness metrics and statistical tests comparing treatment across class states
- High-dimensional datasets may cause FairCFS to be less efficient due to the time-consuming process of discovering spouses

## Confidence

- **High Confidence**: The divide-and-conquer framework's theoretical benefits for reducing sample requirements and improving conditional independence testing reliability
- **Medium Confidence**: The algorithm's ability to achieve comparable accuracy to state-of-the-art methods, supported by experimental results on seven real-world datasets
- **Low Confidence**: The claim of superior fairness, as the paper does not provide explicit fairness metrics or statistical tests comparing treatment across class states

## Next Checks

1. **Sensitivity Analysis**: Test FairCFS on datasets with varying degrees of class state overlap to determine when CSMI-based state discrimination breaks down
2. **Fairness Quantification**: Implement explicit fairness metrics (e.g., demographic parity, equalized odds) to measure actual fairness improvements across class states
3. **Discretization Impact**: Conduct ablation studies comparing different discretization methods to assess their impact on CSMI calculations and final classification performance