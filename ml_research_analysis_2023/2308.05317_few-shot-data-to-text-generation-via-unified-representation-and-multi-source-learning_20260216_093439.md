---
ver: rpa2
title: Few-Shot Data-to-Text Generation via Unified Representation and Multi-Source
  Learning
arxiv_id: '2308.05317'
source_url: https://arxiv.org/abs/2308.05317
tags:
- table
- unified
- generation
- association
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified representation method for structured
  data-to-text generation that can handle tables, knowledge graphs, and meaning representations.
  The approach converts KGs and MRs into virtual tables and linearizes all inputs
  consistently.
---

# Few-Shot Data-to-Text Generation via Unified Representation and Multi-Source Learning

## Quick Facts
- arXiv ID: 2308.05317
- Source URL: https://arxiv.org/abs/2308.05317
- Reference count: 40
- Key outcome: Unified representation significantly improves zero-shot and few-shot transfer performance across structured data formats, achieving 66% improvement in zero-shot BLEU scores when transferring from table-to-text to knowledge graph-to-text

## Executive Summary
This paper addresses the challenge of few-shot data-to-text generation across heterogeneous structured data formats by proposing a unified representation approach. The method converts tables, knowledge graphs, and meaning representations into a consistent virtual table format that can be linearized identically, enabling a single model to handle all structured input types. Experiments across five datasets demonstrate substantial improvements in zero-shot and few-shot transfer performance compared to using varied linearizations for different tasks, with particular gains when transferring knowledge between different structured formats.

## Method Summary
The unified representation method converts knowledge graphs and meaning representations into virtual tables, then linearizes all inputs (tables, KGs, MRs) using a consistent format. This allows a single T5-based model to be trained on multiple structured data types simultaneously. The approach is evaluated through zero-shot transfer (training on one task, testing on another without fine-tuning) and few-shot transfer (fine-tuning on limited target data), comparing unified representation against varied linearizations from previous work.

## Key Results
- Zero-shot transfer from ToTTo (table) to DART (KG) achieves 66% improvement in BLEU scores with unified representation versus varied linearizations
- Multi-task training with unified representation consistently outperforms both single-task models and multi-task training with varied linearizations
- Unified representation shows particular efficiency in few-shot settings, with substantial improvements on unseen categories during WebNLG task transfer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unified linearization enables knowledge transfer between structured forms
- Mechanism: Converting all structured inputs to virtual tables and linearizing them identically allows a single model to learn common structural patterns and transfer knowledge across tasks
- Core assumption: Tables, knowledge graphs, and meaning representations share underlying relational structures that can be mapped to a common tabular format
- Evidence anchors: [abstract] "unified representation that can handle various forms of structured data such as tables, knowledge graph triples, and meaning representations"; [section 3] "demonstrate that structured data, such as tables, highlighted cells, knowledge graph triples, and meaning representations, can be linearized in a consistent manner"

### Mechanism 2
- Claim: Unified representation improves multi-task learning by reducing format mismatch
- Mechanism: When training on multiple tasks with different input formats, using a unified representation prevents the model from learning format-specific rather than task-specific patterns, improving cross-task generalization
- Core assumption: Task performance degradation in multi-task learning is primarily due to format differences rather than task interference
- Evidence anchors: [section 4.2] "multi-task trained 'unified' models consistently outperform single-task models" and "multi-task training using different linearizations for each dataset results in a worse performance compared to single-task training"

### Mechanism 3
- Claim: Unified representation enables better sample efficiency in few-shot transfer
- Mechanism: A model trained on one structured form using unified representation can transfer learned patterns to other forms with fewer examples because the representation space is consistent
- Core assumption: Structural patterns learned from one structured form are applicable to other forms when represented in the same way
- Evidence anchors: [section 4.1.2] "Src to tgt, unified outperforms Src to tgt, varied by a substantial margin" in few-shot experiments; [section 4.1.2] "the improvement on unseen categories is particularly notable" in WebNLG task

## Foundational Learning

- Concept: Structured data linearization techniques
  - Why needed here: The method relies on converting various structured formats to text sequences that can be processed by text-to-text transformers
  - Quick check question: What are the key differences between the linearization approaches used in this paper versus those in UnifiedSKG or other data-to-text methods?

- Concept: Virtual table construction
  - Why needed here: Converting KGs and MRs to virtual tables is the core transformation that enables unified representation
  - Quick check question: How does the method handle nodes that don't appear as tail nodes in knowledge graph triples when constructing virtual tables?

- Concept: Cross-task knowledge transfer principles
  - Why needed here: Understanding when and why knowledge transfers between tasks is essential for evaluating the unified representation approach
  - Quick check question: What conditions must be met for knowledge learned from table-to-text tasks to effectively transfer to knowledge graph-to-text tasks?

## Architecture Onboarding

- Component map: Input data → Virtual table conversion → Linearization → Model training/fine-tuning → Text generation → Evaluation
- Critical path: Structured input → Virtual table transformation → Consistent linearization → T5 model training → Text generation → BLEU/METEOR evaluation
- Design tradeoffs:
  - Single unified representation vs. multiple specialized representations
  - Virtual table conversion may lose some structural information specific to each format
  - Consistent linearization may sacrifice format-specific optimizations
  - Tradeoff between representation simplicity and expressive power
- Failure signatures:
  - Poor transfer performance when moving between very different structured forms
  - Degradation in single-task performance when using unified representation instead of format-specific methods
  - Inconsistent generation quality across different structured input types
- First 3 experiments:
  1. Implement virtual table conversion for knowledge graph triples and meaning representations, verify that conversion preserves all information
  2. Train baseline T5 model on ToTTo using unified representation, compare to baseline using ToTTo's original linearization
  3. Conduct zero-shot transfer from ToTTo (table) to DART (KG) using unified representation, measure improvement over random initialization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the unified representation approach compare to other linearization methods in terms of data-to-text generation performance?
- Basis in paper: [explicit] The paper compares the unified representation approach to other linearization methods, such as the one used in UnifiedSKG (Xie et al., 2022), on several datasets (DART, WebNLG, and E2E).
- Why unresolved: While the paper provides comparisons between the unified representation and other linearization methods, it does not provide a comprehensive analysis of how the unified representation approach compares to all possible linearization methods.
- What evidence would resolve it: A thorough comparison of the unified representation approach with a wide range of linearization methods, including those not mentioned in the paper, would provide a clearer understanding of its performance relative to other methods.

### Open Question 2
- Question: Can the unified representation approach be applied to other tasks that involve heterogeneous inputs, such as question answering over knowledge bases?
- Basis in paper: [explicit] The paper mentions that applying the unified representation approach to other tasks with heterogeneous inputs, such as question answering over knowledge bases, is a future direction.
- Why unresolved: The paper does not provide any experimental results or analysis on applying the unified representation approach to tasks other than data-to-text generation.
- What evidence would resolve it: Conducting experiments on applying the unified representation approach to various tasks with heterogeneous inputs, such as question answering over knowledge bases, would demonstrate its potential applicability and effectiveness in different domains.

### Open Question 3
- Question: How does the unified representation approach handle structured data with complex relationships, such as hierarchical or nested structures?
- Basis in paper: [inferred] The paper focuses on structured data such as tables, knowledge graph triples, and meaning representations, which may have varying levels of complexity in their relationships. However, the paper does not explicitly discuss how the unified representation approach handles complex relationships within structured data.
- Why unresolved: The paper does not provide a detailed analysis of how the unified representation approach manages complex relationships within structured data, such as hierarchical or nested structures.
- What evidence would resolve it: An in-depth examination of the unified representation approach's ability to handle structured data with complex relationships, including hierarchical or nested structures, would provide insights into its robustness and versatility.

## Limitations

- Representation completeness: The conversion of knowledge graphs and meaning representations to virtual tables may lose information specific to their native structures
- Task heterogeneity: Improvements may be influenced by task similarity as much as by unified representation benefits
- Scalability concerns: Effectiveness at larger scales or with more diverse structured data types remains untested

## Confidence

**High confidence** (backed by direct experimental evidence):
- The unified representation method produces consistent linearization across different structured data types
- Multi-task training with unified representation outperforms single-task models and multi-task training with varied linearizations
- Zero-shot and few-shot transfer performance improves significantly with unified representation compared to varied linearizations

**Medium confidence** (supported by results but with caveats):
- The 66% improvement in zero-shot BLEU scores represents a meaningful advance in cross-task transfer
- Sample efficiency gains in few-shot settings are substantial but may vary with different task combinations

**Low confidence** (weak or indirect evidence):
- Claims about the method's applicability to structured data beyond the five tested datasets
- Assumptions about which structural patterns are transferable between different structured formats

## Next Checks

1. **Structural fidelity analysis**: Conduct a systematic comparison of information content between original KGs/MRs and their virtual table representations. Measure what percentage of original relationships and attributes are preserved after conversion.

2. **Cross-task transfer dependency**: Design experiments to isolate the effect of unified representation from task similarity. Test transfer between structurally similar (e.g., ToTTo to LogicNLG) and dissimilar (e.g., WebNLG to E2E) tasks to quantify the relative contributions.

3. **Scaling experiment**: Evaluate the unified representation approach with a larger transformer model (e.g., T5-11B) and additional structured data formats not included in the original five datasets to assess generalizability and scalability limits.