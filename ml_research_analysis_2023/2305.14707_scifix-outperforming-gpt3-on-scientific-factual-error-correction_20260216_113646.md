---
ver: rpa2
title: 'SciFix: Outperforming GPT3 on Scientific Factual Error Correction'
arxiv_id: '2305.14707'
source_url: https://arxiv.org/abs/2305.14707
tags:
- claim
- correction
- claims
- evidence
- incorrect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of scientific claim correction,
  where incorrect scientific claims need to be automatically corrected using evidence
  from scientific literature. The key challenge is the lack of annotated datasets
  for training correction models, as manually annotating corrections is prohibitively
  expensive.
---

# SciFix: Outperforming GPT3 on Scientific Factual Error Correction

## Quick Facts
- arXiv ID: 2305.14707
- Source URL: https://arxiv.org/abs/2305.14707
- Reference count: 6
- Key outcome: Achieves 94% correction accuracy on SciFact dataset, outperforming existing methods (0.5%) and matching GPT3.5 with 1250× fewer parameters

## Executive Summary
This paper addresses the challenge of automatically correcting incorrect scientific claims using evidence from scientific literature. The key innovation is a system that generates training data by having large language models create incorrect versions of correct claims, then trains a compact model to perform the reverse mapping. The approach achieves state-of-the-art performance on scientific claim correction, with 94% accuracy on the SciFact dataset compared to just 0.5% for previous methods. The system uses a claim-aware decoding procedure that guides generation away from restating the original incorrect claim, and demonstrates strong performance across multiple scientific claim correction datasets.

## Method Summary
SciFix leverages prompting with large language models to generate a richly annotated dataset from existing fact verification datasets. The process involves using GPT3.5 to create incorrect versions of correct claims with explanations, then training a conditional generation model (BART) to map incorrect claims to corrections. The system incorporates domain-adaptive pretraining on scientific abstracts and introduces a claim-aware decoding procedure that uses a semantic similarity classifier to avoid generating semantically similar outputs to the incorrect claim. This approach enables the system to achieve high correction accuracy while using only 1/1250th the parameters of GPT3.5.

## Key Results
- Achieves 94% correction accuracy on SciFact dataset through human evaluation
- Outperforms existing methods by 93.5 percentage points (from 0.5% to 94%)
- Matches GPT3.5 performance while using 1250× fewer parameters
- Demonstrates strong performance on SciFact-Open, CovidFact, and HealthVer datasets
- Claim-aware decoding improves correction quality by guiding away from restating incorrect claims

## Why This Works (Mechanism)

### Mechanism 1: LLM-Driven Data Generation
The system exploits the asymmetry where generating incorrect claims from correct ones is easier than the reverse. By using GPT3.5 to create corrupted claims from verified correct claims, the system can generate training data without needing deep evidence comprehension. This approach leverages the LLM's ability to create semantically meaningful incorrect variations that are still related to the evidence.

### Mechanism 2: Semantic Difference Modeling
During decoding, a semantic similarity classifier guides the generation process by penalizing sequences semantically similar to the original incorrect claim. This ensures the model doesn't simply restate the incorrect claim in different words, forcing it to generate genuinely corrected content. The semantic difference model learns to distinguish between claims with different meanings versus minor paraphrasing.

### Mechanism 3: Domain-Adaptive Pretraining
Pretraining BART on scientific abstracts before fine-tuning on correction tasks provides better domain understanding than direct fine-tuning. This domain-specific pretraining helps the model understand scientific terminology and concepts, improving its ability to generate accurate corrections for scientific claims.

## Foundational Learning

- **Conditional generation vs masking/infilling**: The system uses conditional generation rather than masking/infilling, allowing more diverse corrections including structural changes. This is needed because masking approaches would be limited to local edits while scientific corrections often require complete rephrasing.

- **Semantic similarity modeling**: Required for claim-aware decoding to measure whether candidate corrections are semantically different from incorrect claims. Without this, the system might generate outputs that are merely paraphrased versions of the original incorrect claim.

- **Dataset augmentation through LLM prompting**: The system creates training data by having LLMs generate incorrect versions of correct claims with explanations. This works because generating incorrect claims is easier than correcting them, as it only requires partial understanding of concepts rather than full semantic comprehension.

## Architecture Onboarding

- **Component map**: LLM data generation (GPT3.5) → Domain pretraining (BART on abstracts) → Claim correction model (BART fine-tuned) → Semantic difference model (classifier) → Claim-aware decoding (constrained generation)

- **Critical path**: Data generation → Domain pretraining → Claim correction training → Semantic difference training → Prediction with claim-aware decoding

- **Design tradeoffs**: Using large LLM for data generation vs training compact model directly; semantic difference model quality vs decoding performance; domain pretraining scope vs overfitting

- **Failure signatures**: Poor correction accuracy suggests issues with data generation quality, domain pretraining effectiveness, or semantic difference model reliability

- **First 3 experiments**: 1) Generate small dataset with LLM and verify quality of corrupted claims and explanations, 2) Train semantic difference model and test on pairs of similar/dissimilar claims, 3) Train claim correction model on small generated dataset and evaluate on simple corrections

## Open Questions the Paper Calls Out

1. **LLM choice impact**: How does the quality of generated corrections depend on the choice of LLM used for data generation? The paper uses GPT3.5 but doesn't explore the impact of different LLMs or varying sizes.

2. **Semantic difference model robustness**: How robust is the claim-aware decoding procedure to semantic difference model errors? The authors note the model isn't robust to small meaningless perturbations, but don't provide detailed analysis of impact.

3. **Extension to verification**: Can the method be extended to handle correct claims as well, effectively performing verification? The system is verifier-free and cannot abstain when input claims are actually correct.

## Limitations

- Performance uncertainty on real-world claims despite high accuracy on benchmark datasets
- Claim-aware decoding has documented failure modes with semantic difference model limitations
- Heavy reliance on LLM-generated training data quality introduces potential unknown biases
- System may struggle with claims requiring complete structural rewrites rather than simple word substitutions

## Confidence

**High Confidence**: The overall approach of using LLM-generated training data is sound and well-justified by literature on data augmentation; domain-adaptive pretraining on scientific abstracts should improve domain-specific performance; the 94% correction accuracy on SciFact is well-documented through human evaluation.

**Medium Confidence**: The claim-aware decoding procedure will consistently improve correction quality across all claim types; the system will maintain performance across different scientific domains; the semantic difference model is robust enough to handle all types of semantic variations in scientific claims.

**Low Confidence**: The system can handle claims requiring complete structural rewrites rather than simple word substitutions; the performance gap between GPT3.5 and previous methods is entirely due to the proposed technical innovations rather than data quality differences; the system generalizes to scientific claims from domains not represented in the pretraining data.

## Next Checks

1. **Semantic Difference Model Robustness Test**: Create a controlled test set with claim pairs varying in semantic similarity (identical, minor paraphrase, meaningful difference, complete restatement) and evaluate the semantic difference model's classification accuracy to quantify its reliability for guiding claim-aware decoding.

2. **Error Analysis on Challenging Claims**: Perform detailed analysis on claims where the system fails, categorizing error types (e.g., structural rewrite needed, domain-specific terminology, complex evidence-claim relationships) to identify specific limitations and improvement opportunities.

3. **Cross-Domain Performance Validation**: Evaluate the system on scientific claims from a domain not represented in the pretraining data (e.g., social sciences if pretrained on biomedical literature) to assess domain generalization capabilities and identify potential domain-specific limitations.