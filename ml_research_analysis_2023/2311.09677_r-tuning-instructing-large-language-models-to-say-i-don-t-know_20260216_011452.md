---
ver: rpa2
title: 'R-Tuning: Instructing Large Language Models to Say `I Don''t Know'''
arxiv_id: '2311.09677'
source_url: https://arxiv.org/abs/2311.09677
tags:
- questions
- data
- answer
- knowledge
- r-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of language model hallucination,
  where models generate factually incorrect outputs. The authors attribute this to
  instruction tuning data that includes knowledge outside the model's parametric knowledge.
---

# R-Tuning: Instructing Large Language Models to Say `I Don't Know'

## Quick Facts
- **arXiv ID:** 2311.09677
- **Source URL:** https://arxiv.org/abs/2311.09677
- **Reference count:** 22
- **Key outcome:** R-Tuning improves language models' ability to refuse answering questions beyond their parametric knowledge while maintaining accuracy on questions they can answer.

## Executive Summary
This paper addresses the problem of hallucination in large language models by proposing Refusal-Aware Instruction Tuning (R-Tuning). The method identifies questions that match the model's parametric knowledge versus those that don't, then appends appropriate uncertainty expressions ("I am sure" or "I am unsure") during training. This teaches models to refuse answering questions beyond their knowledge boundaries. Experimental results demonstrate significant improvements in both accuracy on answerable questions and refusal rate on uncertain ones, with the refusal ability showing generalization as a meta-skill across different domains.

## Method Summary
R-Tuning works by first identifying questions that match the model's parametric knowledge (certain) versus those that don't (uncertain) through a supervised comparison of predictions against ground-truth labels. For certain questions, the ground-truth answer is appended with "I am sure"; for uncertain ones, it's appended with "I am unsure". The model is then fine-tuned on this refusal-aware data using standard cross-entropy loss. During inference, the model generates both answers and uncertainty expressions, allowing it to refuse uncertain questions. The method was tested on multiple model sizes (OpenLLaMA-3B, LLaMA-7B, LLaMA-13B) across various datasets including ParaRel, MMLU, WiCE, HotpotQA, and FEVER.

## Key Results
- R-Tuning significantly improves accuracy on willingly answered questions while increasing refusal rate on uncertain ones
- The method outperforms traditional instruction tuning approaches in both in-domain and out-of-domain tasks
- Learning uncertainty during training provides better calibration and uncertainty estimation than uncertainty-based testing approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: R-Tuning teaches models to refuse answering questions beyond their parametric knowledge by leveraging uncertainty expressions during training.
- Mechanism: The method identifies questions matching the model's parametric knowledge (certain) versus those that don't (uncertain). For certain questions, it appends "I am sure" to the ground-truth answer; for uncertain ones, it appends "I am unsure." This teaches the model to refuse answering questions beyond its knowledge.
- Core assumption: The model's parametric knowledge can be reliably assessed by comparing predictions with ground-truth labels during instruction tuning.
- Evidence anchors:
  - [abstract] "Experimental results demonstrate R-Tuning significantly improves the model's ability to correctly answer questions it is willing to answer, and to refuse uncertain ones"
  - [section 2.1] "By inferring the model on the training data once and comparing the prediction and label, the instruction tuning data is split into uncertain data D0 and certain data D1"
- Break condition: If the model's predictions cannot reliably indicate knowledge boundaries, the identification step fails and R-Tuning loses effectiveness.

### Mechanism 2
- Claim: Learning uncertainty during training improves the model's calibration and uncertainty estimation ability more effectively than uncertainty-based testing.
- Mechanism: By training the model to express both answers and uncertainty simultaneously, it develops better internal representations of confidence that generalize to unseen data.
- Core assumption: Training with uncertainty expressions creates better-calibrated confidence scores than post-hoc uncertainty estimation.
- Evidence anchors:
  - [abstract] "Further analysis surprisingly finds that learning uncertainty during training displays a better ability to estimate uncertainty than uncertainty-based testing"
  - [section 5.1] "learning uncertainty and then filtering questions based on this uncertainty to provide answers yields better results than directly filtering and answering questions using uncertainty on the test dataset"
- Break condition: If the model learns to associate uncertainty expressions with the training data format rather than genuine confidence, generalization breaks down.

### Mechanism 3
- Claim: Refusal ability is a meta-skill that can be generalized across tasks and improved through multi-task training.
- Mechanism: By training on diverse datasets and learning to distinguish between answerable and unanswerable questions, the model develops a transferable skill of uncertainty awareness.
- Core assumption: The ability to refuse unknown questions transfers across different domains and task types.
- Evidence anchors:
  - [abstract] "when tested on out-of-domain datasets, the refusal ability was found to be a meta-skill that could be generalized to other tasks"
  - [section 4.2] "R-Tuning consistently outperforms all baseline models in terms of the AP score on both ID and OOD tasks"
- Break condition: If the model overfits to specific patterns in training data, the refusal ability may not transfer to truly novel domains.

## Foundational Learning

- Concept: Parametric knowledge boundary
  - Why needed here: R-Tuning relies on distinguishing between questions the model knows (parametric knowledge) and those it doesn't. Understanding this boundary is crucial for implementing the identification step.
  - Quick check question: How does the model determine which questions are within its parametric knowledge versus those that require external information?

- Concept: Uncertainty quantification in language models
  - Why needed here: The method hinges on the model's ability to express uncertainty appropriately. Understanding how LLMs estimate and express uncertainty is fundamental to implementing R-Tuning.
  - Quick check question: What metrics can be used to quantify the model's uncertainty about its predictions during both training and inference?

- Concept: Instruction tuning methodology
  - Why needed here: R-Tuning is a variant of instruction tuning, so understanding standard instruction tuning approaches and their limitations is essential for implementing and extending this method.
  - Quick check question: How does traditional instruction tuning differ from R-Tuning in terms of handling questions outside the model's knowledge?

## Architecture Onboarding

- Component map: Data identification module -> Data construction module -> Training module -> Inference module
- Critical path: Data identification → Data construction → Training → Inference
  - The identification step must complete successfully before data construction can begin, and so on through the pipeline.
- Design tradeoffs:
  - Supervised vs unsupervised identification: Supervised (comparing predictions to labels) is more reliable but requires ground truth, while unsupervised (using entropy) is more flexible but potentially less accurate.
  - Padding vs replacement strategies: Padding retains all labels but may introduce redundancy, while replacement is cleaner but loses training signal from uncertain questions.
- Failure signatures:
  - Low AP scores despite high accuracy on willingly answered questions: Indicates the model is answering too many questions (low precision in uncertainty detection)
  - High refusal rate across all questions: Suggests the identification step is too conservative or the model's parametric knowledge boundary is set too narrowly
  - No improvement over baseline: May indicate the uncertainty expressions aren't being learned effectively
- First 3 experiments:
  1. Implement supervised identification on a small dataset (e.g., ParaRel) and verify the split between certain and uncertain questions matches expectations
  2. Test the padding strategy with a simple template to ensure the model can generate both answers and uncertainty expressions
  3. Evaluate the trained model on in-domain and out-of-domain test sets to verify the refusal ability generalizes as expected

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of R-Tuning compare to retrieval-augmented methods for hallucination mitigation?
- Basis in paper: [explicit] The paper discusses retrieval-augmented methods as one of the current mainstream approaches to mitigating hallucination, but does not compare R-Tuning's performance to these methods directly.
- Why unresolved: The paper focuses on comparing R-Tuning to traditional instruction tuning and does not include retrieval-augmented methods as baselines.
- What evidence would resolve it: A direct comparison of R-Tuning's accuracy and hallucination rates against retrieval-augmented baselines on the same datasets.

### Open Question 2
- Question: Does R-Tuning's refusal ability generalize to more diverse knowledge domains beyond the tested datasets?
- Basis in paper: [inferred] The paper demonstrates R-Tuning's generalization to out-of-domain datasets and discusses its ability to handle unanswerable questions, but the tested domains are still relatively limited.
- Why unresolved: The experiments cover a range of domains but do not exhaustively test all possible knowledge areas that large language models encounter.
- What evidence would resolve it: Extensive testing of R-Tuning on diverse, real-world datasets spanning all major knowledge domains to verify consistent refusal of out-of-knowledge questions.

### Open Question 3
- Question: What is the optimal strategy for balancing the trade-off between answer rate and accuracy in R-Tuning?
- Basis in paper: [explicit] The paper discusses the trade-off between answer rate and accuracy, showing that R-Tuning-R has lower answer rates but higher accuracy than traditional methods.
- Why unresolved: The paper does not provide a clear strategy for finding the optimal balance between these two metrics.
- What evidence would resolve it: A systematic study varying the ratio of certain to uncertain data in R-Tuning to find the point where the product of answer rate and accuracy is maximized.

## Limitations
- The method's effectiveness depends critically on accurate identification of parametric knowledge boundaries, which may not generalize well to highly specialized domains.
- The paper does not fully address how the model handles questions that are ambiguous or require reasoning beyond simple fact retrieval.
- Performance metrics focus primarily on precision-recall tradeoffs and accuracy, but do not explore potential biases introduced by the refusal mechanism.

## Confidence
- **High confidence**: The core claim that R-Tuning improves both accuracy on known questions and refusal rate on unknown questions is well-supported by experimental results across multiple datasets and model sizes.
- **Medium confidence**: The claim about uncertainty learning during training providing better calibration than uncertainty-based testing is supported but requires more rigorous ablation studies.
- **Medium confidence**: The generalization of refusal ability as a meta-skill across domains is demonstrated but the extent and limits of this transferability are not fully characterized.

## Next Checks
1. **Boundary case analysis**: Systematically test the model's refusal behavior on questions that are near the boundary of its parametric knowledge, including questions that require multi-hop reasoning or domain-specific expertise not well-represented in the training data.
2. **Bias and fairness audit**: Evaluate whether the refusal mechanism introduces systematic biases, such as over-refusal on questions related to minority perspectives, emerging knowledge domains, or questions with ambiguous ground truth.
3. **Long-term stability test**: Conduct experiments to assess how the refusal behavior evolves with continued training on diverse data and whether the model maintains its calibrated uncertainty expression over extended use.