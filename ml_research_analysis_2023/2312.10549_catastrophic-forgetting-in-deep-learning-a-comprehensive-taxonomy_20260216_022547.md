---
ver: rpa2
title: 'Catastrophic Forgetting in Deep Learning: A Comprehensive Taxonomy'
arxiv_id: '2312.10549'
source_url: https://arxiv.org/abs/2312.10549
tags:
- learning
- task
- neural
- networks
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Catastrophic forgetting remains a significant challenge in deep
  learning, hindering incremental learning without forgetting previously acquired
  knowledge. This survey comprehensively reviews methods addressing catastrophic forgetting
  in deep neural networks, organizing them into four main categories: rehearsal (including
  pseudo-rehearsal and mini-rehearsal), distance-based, sub-networks, and dynamic
  networks.'
---

# Catastrophic Forgetting in Deep Learning: A Comprehensive Taxonomy

## Quick Facts
- arXiv ID: 2312.10549
- Source URL: https://arxiv.org/abs/2312.10549
- Reference count: 2
- Primary result: Comprehensive survey of methods addressing catastrophic forgetting in deep neural networks, organizing them into four main categories and identifying hybrid approaches as promising solutions

## Executive Summary
Catastrophic forgetting remains a fundamental challenge in deep learning, preventing models from learning new tasks without losing previously acquired knowledge. This survey provides a comprehensive review of methods addressing catastrophic forgetting in deep neural networks, organizing approaches into four main categories: rehearsal (including pseudo-rehearsal and mini-rehearsal), distance-based, sub-networks, and dynamic networks. The review identifies hybrid approaches combining multiple strategies as particularly promising solutions for mitigating forgetting across sequential learning scenarios.

## Method Summary
The survey comprehensively reviews methods addressing catastrophic forgetting by organizing them into four main categories. Rehearsal methods maintain previous task data through memory buffers or generative models. Distance-based approaches preserve class representations in embedding space using prototypes or centroids. Sub-network methods isolate task-specific parameters through either freezing (hard sub-networks) or regularization (soft sub-networks). Dynamic networks expand model capacity to accommodate new tasks. The review emphasizes that no single approach provides a definitive solution, and the choice of method depends on specific application requirements and constraints.

## Key Results
- Rehearsal techniques with memory buffers are identified as effective approaches for mitigating catastrophic forgetting
- Maintaining consistent class representations in embedding space is crucial for distance-based methods
- Hybrid approaches combining multiple strategies show promise but require careful balancing of trade-offs
- No single approach provides a definitive solution to catastrophic forgetting

## Why This Works (Mechanism)

### Mechanism 1
Rehearsal methods reduce catastrophic forgetting by maintaining access to previous task data. During new task training, a subset of previous task samples (coreset) is replayed alongside new task data, allowing the model to retain knowledge of previous classes. This works because storing and replaying a limited subset of previous data is sufficient to maintain class representations and prevent forgetting.

### Mechanism 2
Distance-based methods prevent forgetting by maintaining consistent class representations in embedding space. Class prototypes or embeddings are tracked and updated to ensure samples from the same class remain clustered together while being separated from other classes in the embedding space. This works because maintaining consistent geometric relationships between class representations in embedding space is sufficient to preserve classification boundaries.

### Mechanism 3
Sub-network approaches mitigate forgetting by isolating task-specific parameters. The model identifies and isolates subsets of parameters important for each task, either by freezing them (hard sub-networks) or by adding regularization to prevent changes (soft sub-networks). This works because different tasks can be solved by distinct subsets of parameters with minimal overlap, allowing tasks to be learned sequentially without interference.

## Foundational Learning

- Concept: Catastrophic Forgetting
  - Why needed here: Understanding CF is essential to grasp why incremental learning is challenging and why various methods are needed.
  - Quick check question: What happens to a neural network's performance on previous tasks when it learns a new task without any special techniques to prevent forgetting?

- Concept: Embedding Space
  - Why needed here: Many methods rely on manipulating representations in embedding space to maintain class separability and prevent forgetting.
  - Quick check question: How do distance-based methods use the geometry of embedding space to distinguish between classes learned at different times?

- Concept: Parameter Isolation
  - Why needed here: Sub-network approaches depend on identifying and managing distinct parameter subsets for different tasks.
  - Quick check question: What is the key difference between soft and hard sub-network approaches in terms of how they handle task-specific parameters?

## Architecture Onboarding

- Component map: Embedding generator (feature extractor) -> Classifier(s) -> Auxiliary components (memory buffers or generative models)
- Critical path: New task data + coreset samples -> Model training -> Parameter updates while maintaining performance on both new and old tasks
- Design tradeoffs: Memory vs. performance tradeoff in rehearsal (larger coreset = better retention but more memory), capacity vs. flexibility tradeoff in sub-networks (more isolation = less forgetting but less parameter sharing), and growth vs. efficiency tradeoff in dynamic networks
- Failure signatures: Accuracy drop on previous tasks while learning new ones, confusion between classes from different tasks, and inability to learn new tasks without destroying old knowledge
- First 3 experiments:
  1. Implement a basic rehearsal method with a small coreset and measure forgetting on a simple split-MNIST task sequence
  2. Add a distance-based prototype tracking mechanism to the rehearsal method and compare performance
  3. Implement a soft sub-network approach using EWC regularization and evaluate on a multi-task sequence with domain drift

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How many classes can a single embedding space effectively support before confusion becomes problematic in Distance-based methods?
- Basis in paper: "it remains unclear how many classes an embedding space can effectively support without causing confusion" in Distance-based methods section
- Why unresolved: This is a fundamental limitation of distance-based approaches that hasn't been thoroughly investigated through systematic experiments with varying numbers of classes
- What evidence would resolve it: Empirical studies testing Distance-based methods across a wide range of class counts (e.g., 10, 50, 100, 200+ classes) to identify the breaking point where performance degrades significantly

### Open Question 2
- Question: What is the optimal balance between model expansion and rehearsal in Hybrid approaches to minimize overall memory consumption?
- Basis in paper: "the increased space demands should be carefully considered in terms of resource utilization and memory constraints" and "it is crucial to evaluate each scenario individually to determine which approach has the least growth rate"
- Why unresolved: Current hybrid approaches often focus on accuracy improvements without systematically analyzing the trade-off between expanding model parameters versus storing rehearsal samples
- What evidence would resolve it: Comparative studies measuring both accuracy and total memory usage (model parameters + rehearsal samples) across various hybrid approaches on multiple datasets

### Open Question 3
- Question: How can Dynamic Networks methods effectively determine the task identity during inference without requiring task labels?
- Basis in paper: "A typical challenge faced by Dynamic Networks methods is the requirement of knowing the task that a given sample belongs to" and "Some works propose models equipped with their own task selector, but not all"
- Why unresolved: Most Dynamic Networks approaches assume task labels are available during inference, but in realistic scenarios, this information is often unavailable, yet few solutions address this challenge
- What evidence would resolve it: Development and evaluation of task-agnostic inference methods for Dynamic Networks that can successfully identify appropriate model components or sub-networks for given samples without task labels

## Limitations
- Limited empirical validation across different domains with standardized benchmarks
- Reliance on abstract-level claims rather than detailed experimental results
- Lack of comprehensive performance comparisons between methods

## Confidence
- High: The categorization of four main approaches and their basic mechanisms are well-established in the literature
- Medium: Claims about relative effectiveness and benefits of hybrid approaches lack comprehensive empirical validation
- Low: Specific performance comparisons and generalizability across application domains remain uncertain

## Next Checks
1. Implement controlled experiments comparing all four main approaches on the same benchmark dataset (e.g., split CIFAR-100) using standardized metrics to verify relative effectiveness claims
2. Test hybrid combinations of rehearsal with distance-based methods and sub-network approaches on long task sequences to evaluate scalability and diminishing returns
3. Conduct ablation studies to quantify the contribution of key components (memory buffer size, regularization strength, prototype tracking) to overall performance and identify optimal configurations