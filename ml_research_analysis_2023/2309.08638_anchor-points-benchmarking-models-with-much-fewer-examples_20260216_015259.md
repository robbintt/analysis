---
ver: rpa2
title: 'Anchor Points: Benchmarking Models with Much Fewer Examples'
arxiv_id: '2309.08638'
source_url: https://arxiv.org/abs/2309.08638
tags:
- anchor
- points
- predictions
- point
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of efficiently evaluating and benchmarking
  large language models without requiring the full evaluation set. The key insight
  is that the confidence of multiple source models in the correct class for a given
  input is often strongly correlated across models.
---

# Anchor Points: Benchmarking Models with Much Fewer Examples

## Quick Facts
- arXiv ID: 2309.08638
- Source URL: https://arxiv.org/abs/2309.08638
- Reference count: 26
- One-line primary result: Demonstrates that small subsets of evaluation data (anchor points) can reliably benchmark language models with orders of magnitude less compute than full evaluation

## Executive Summary
This paper addresses the challenge of efficiently evaluating large language models by proposing Anchor Point Selection, a technique that identifies small subsets of evaluation data that capture overall model behavior. The key insight is that model confidence in the correct class for given inputs is strongly correlated across diverse language models. By exploiting this correlation structure, the authors show that anchor points can reliably rank models and estimate per-class predictions with low mean absolute error, enabling micro-benchmarking with dramatically reduced computational cost while maintaining accuracy.

## Method Summary
The method uses facility location optimization to select anchor points from evaluation data by maximizing correlation between selected and unselected points across multiple source models. Two techniques leverage these anchor points: Anchor Point Predictor estimates target model predictions through linear regression on source model trends, while Anchor Point Weighted Score computes performance metrics by weighting anchor point confidences by cluster size. Anchor Point Maps visualize model performance across dataset regions using dimensionality reduction techniques.

## Key Results
- Anchor Point Weighted Score achieves Kendall's τ of 0.72-0.89 when ranking models using only 5-10 anchor points from GLUE datasets
- Anchor Point Predictor estimates instance-level predictions with mean absolute error below 0.1 across multiple GLUE tasks
- The method reduces evaluation compute by 90-95% while maintaining ranking accuracy comparable to full-dataset evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model confidence in the correct class on pairs of points is strongly correlated across diverse language models.
- Mechanism: When multiple models agree on their relative confidence for two inputs, those inputs' relative difficulty is consistent. This allows a small set of points to represent the full distribution.
- Core assumption: The correlation structure of source model predictions generalizes to target models.
- Evidence anchors:
  - [abstract] "model confidence in the correct class on many pairs of points is strongly correlated across models"
  - [section] "ϕn(x1)[y] often linearly correlates with the model's prediction on a different instance ϕn(x2)[y] across models"
  - [corpus] Weak: No direct evidence, but related work (Miller et al., 2021) supports accuracy correlation phenomena.
- Break condition: If target models learn fundamentally different representations or if the source model set is too narrow, correlations may not transfer.

### Mechanism 2
- Claim: Selecting anchor points via facility location on correlation space yields a representative subset that preserves ranking power.
- Mechanism: By maximizing the sum of correlations between selected and unselected points, the subset captures the "shape" of the full dataset in confidence space.
- Core assumption: Correlation is a good proxy for representativeness in the model behavior space.
- Evidence anchors:
  - [abstract] "Anchor Point Selection, a technique to select small subsets of datasets that capture model behavior across the entire dataset"
  - [section] "solving a facility location problem that maximizes the correlation between the selected and the remaining points"
  - [corpus] Weak: No direct evidence, but k-medoids is a standard facility location method.
- Break condition: If the correlation space is not well-structured (e.g., many uncorrelated points), the facility location objective may select poor representatives.

### Mechanism 3
- Claim: Anchor Point Weighted Score correlates with full-dataset accuracy without estimating instance-level predictions.
- Mechanism: Weighting anchor point confidences by cluster size approximates the expected accuracy over the full dataset.
- Core assumption: Cluster size is proportional to the number of points the anchor represents in the original distribution.
- Evidence anchors:
  - [abstract] "Anchor Point Weighted Score, which produces a score that correlates with the model's performance on the entire dataset"
  - [section] "Each of these pointsxacq,i strongly correlates with a subset of the untested points inDeval"
  - [corpus] Weak: No direct evidence, but weighted aggregation is a common approximation technique.
- Break condition: If clusters are poorly formed or if cluster sizes don't reflect true data density, the weighted score may be biased.

## Foundational Learning

- Concept: Pearson correlation and its properties
  - Why needed here: The selection and estimation rely on measuring and maximizing correlation between model predictions.
  - Quick check question: If two variables have a Pearson correlation of 0.9, what percentage of variance in one is explained by the other?

- Concept: Facility location / k-medoids clustering
  - Why needed here: Anchor points are selected by solving a facility location problem in correlation space.
  - Quick check question: In k-medoids, what is the role of the medoid compared to the centroid in k-means?

- Concept: Linear regression for trend line fitting
  - Why needed here: The Anchor Point Predictor fits trend lines to source model predictions to estimate target model predictions.
  - Quick check question: What is the difference between simple linear regression and multivariate linear regression?

## Architecture Onboarding

- Component map: Data pipeline -> Anchor selection -> Estimation engine -> Visualization
- Critical path: Dataset -> Source predictions -> Correlation matrix -> Anchor selection -> Evaluation on anchors -> Score/estimation -> (Optional) Visualization
- Design tradeoffs:
  - Number of source models: More models improve correlation structure but increase compute
  - Number of anchor points: More points improve accuracy but reduce efficiency
  - Choice of embedding for visualization: SentenceBERT vs. CLS token vs. other sentence encoders
- Failure signatures:
  - Low correlation between source models -> Poor anchor point selection
  - High variance in trend line fits -> Anchor Point Predictor may fail
  - Anchor points clustered in one region -> Poor coverage of dataset
- First 3 experiments:
  1. Reproduce correlation matrices on a small GLUE task to verify strong correlations exist
  2. Compare APW vs. random selection on ranking task with 5-10 anchor points
  3. Visualize Anchor Point Maps for a dataset to check if model weaknesses are localized

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we intelligently select source models to ensure anchor point generalization to target models?
- Basis in paper: [inferred] from discussion of limitations and poor generalization in some cases
- Why unresolved: The paper shows that anchor points sometimes fail to generalize across diverse model families. The authors mention this is a promising direction but do not provide concrete strategies or theory for source model selection.
- What evidence would resolve it: Experimental results showing that carefully chosen source model sets (e.g., based on model architecture, training data, or other factors) consistently produce anchor points that generalize better across diverse target models.

### Open Question 2
- Question: Can anchor point selection be extended to non-classification tasks like sequence generation or structured prediction?
- Basis in paper: [explicit] mentioned in the conclusion as "exciting future direction"
- Why unresolved: The paper only demonstrates anchor point effectiveness for language classification benchmarks. It's unclear whether the core insight about correlated predictions generalizes to tasks with different performance metrics.
- What evidence would resolve it: Experimental results showing that anchor points can be effectively used to rank and analyze model performance on tasks like machine translation, summarization, or semantic parsing.

### Open Question 3
- Question: What is the theoretical explanation for why diverse language models exhibit strong predictive correlations at the instance level?
- Basis in paper: [inferred] from observation of strong correlations across models and comparison to "Accuracy on the Line" phenomenon
- Why unresolved: The paper observes the phenomenon and exploits it practically, but does not provide theoretical justification for why this occurs. This is mentioned as an area for further research.
- What evidence would resolve it: Theoretical analysis or formal proof showing conditions under which strong predictive correlations must exist across diverse models, potentially building on existing theory of generalization and model agreement.

## Limitations
- Results may not generalize to non-GLUE tasks or different domains beyond the six classification benchmarks tested
- The facility location approach assumes correlation structure transfers well between source and target models, which may not hold for very diverse model families
- No theoretical explanation is provided for why diverse language models exhibit strong predictive correlations at the instance level

## Confidence

**High Confidence Claims:**
- Strong correlations exist between model confidences on the same inputs across diverse language models
- Anchor Point Weighted Score correlates with full-dataset accuracy
- Anchor points can reliably rank models when properly selected

**Medium Confidence Claims:**
- Anchor Point Predictor provides accurate instance-level predictions
- The facility location formulation effectively captures dataset representativeness
- Results generalize across the six GLUE tasks

**Low Confidence Claims:**
- Results will transfer to non-GLUE tasks or different domains
- Performance is robust to different numbers of source models
- The approach works equally well for larger, more diverse model sets

## Next Checks

1. **Cross-Domain Validation**: Test anchor point effectiveness on non-GLUE tasks (e.g., biomedical, legal, or multilingual datasets) to verify generalization beyond the original experimental scope.

2. **Source Model Sensitivity Analysis**: Systematically vary the number and diversity of source models to determine the minimum effective set size and identify which model characteristics are most important for correlation transfer.

3. **Alternative Correlation Metrics**: Compare Pearson correlation against other similarity measures (Spearman, mutual information, or learned similarity functions) to determine if the current choice is optimal for anchor point selection.