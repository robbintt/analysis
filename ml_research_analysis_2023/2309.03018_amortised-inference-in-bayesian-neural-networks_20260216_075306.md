---
ver: rpa2
title: Amortised Inference in Bayesian Neural Networks
arxiv_id: '2309.03018'
source_url: https://arxiv.org/abs/2309.03018
tags:
- posterior
- inference
- apovi-bnn
- neural
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Amortised Pseudo-Observation Variational Inference
  Bayesian Neural Networks (APOVI-BNNs) to improve data efficiency in probabilistic
  meta-learning. The APOVI-BNN amortises inference in Bayesian neural networks by
  using available datapoints as inducing locations and secondary inference networks
  to produce per-datapoint approximate likelihoods.
---

# Amortised Inference in Bayesian Neural Networks

## Quick Facts
- arXiv ID: 2309.03018
- Source URL: https://arxiv.org/abs/2309.03018
- Reference count: 29
- Primary result: APOVI-BNN outperforms ConvCNPs on image completion with limited meta-data (up to 2.2× better)

## Executive Summary
This paper introduces Amortised Pseudo-Observation Variational Inference for Bayesian Neural Networks (APOVI-BNN), a method that improves data efficiency in probabilistic meta-learning. By using available datapoints as inducing locations and secondary inference networks to produce per-datapoint approximate likelihoods, APOVI-BNN enables efficient amortised inference in Bayesian neural networks. The model learns to perform inference on unseen datasets with a single forward pass, addressing key limitations of existing probabilistic meta-models that require large meta-datasets.

## Method Summary
APOVI-BNN uses a Bayesian neural network whose weights are approximated by a variational posterior. Instead of learning global inducing locations, it uses actual datapoints as inducing locations and employs secondary inference networks (MLPs) that map from datapoints to approximate posterior parameters. This allows the approximate posterior to be decomposed into a product of per-datapoint approximate likelihoods, enabling efficient amortised inference. The model can be trained with neural process objectives due to structural similarities with latent neural processes, and it performs well in data-scarce meta-learning by learning to amortise inference during training.

## Key Results
- APOVI-BNN produces approximate posterior distributions of similar or better quality than traditional variational inference while being faster to train
- On image completion tasks with small meta-datasets, APOVI-BNN achieves up to 2.2× better performance than ConvCNPs
- APOVI-BNN outperforms state-of-the-art models like ConvCNPs when training data is limited in both 1D regression and image completion tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: APOVI-BNN achieves superior posterior quality by replacing inducing points with actual datapoints in the variational approximation.
- Mechanism: This replacement allows the approximate posterior to be decomposed into a product of per-datapoint approximate likelihoods, enabling efficient amortised inference through a secondary network.
- Core assumption: The pseudo-likelihoods factorise across datapoints, meaning each datapoint's contribution to the posterior is independent.
- Evidence anchors:
  - [abstract] "Instead of learning the set of global inducing locations... we can set the available datapoints as the inducing locations"
  - [section] "we can alter the form of the approximate posterior to q(W) ∝ p(W) ∏ₙ N(W; µₙ, Σₙ)"
  - [corpus] Weak evidence - no direct comparison to inducing point methods in related papers
- Break condition: If the pseudo-likelihoods do not factorise (e.g., highly correlated datapoints), the approximation degrades significantly.

### Mechanism 2
- Claim: APOVI-BNN performs well in data-scarce meta-learning by learning to amortise inference during training.
- Mechanism: During meta-training, the secondary inference networks learn to map from datapoints to approximate posterior parameters, allowing single-pass inference on unseen datasets.
- Core assumption: The mapping from data to posterior parameters is sufficiently smooth and learnable with limited meta-dataset exposure.
- Evidence anchors:
  - [abstract] "after training, the secondary networks will have learned to perform approximate Bayesian inference... on unseen datasets"
  - [section] "when the amount of training data is limited, our model is the best in its class"
  - [corpus] Moderate evidence - related papers mention amortised inference but not in meta-learning context
- Break condition: If the mapping is too complex or the meta-dataset too small, the secondary networks cannot learn effective inference.

### Mechanism 3
- Claim: APOVI-BNN can be trained with neural process objectives due to structural similarities with latent neural processes.
- Mechanism: Both models share a four-step inference process (encode datapoints → aggregate → parameterise latent → decode), allowing cross-application of training objectives.
- Core assumption: The APOVI-BNN's architecture is sufficiently similar to neural processes to benefit from their training methods.
- Evidence anchors:
  - [section] "We can break a forward pass of the model down into four distinct steps that are described in terms of equivalent steps in a member of the LNPF"
  - [section] "the similarities are strong enough to motivate training the APOVI-BNN as if it were a member of the LNPF"
  - [corpus] Strong evidence - related papers explicitly discuss neural process training objectives
- Break condition: If the architectural differences are too significant (e.g., weight conditioning vs. parameter sharing), neural process objectives may not improve performance.

## Foundational Learning

- Concept: Variational Inference and Evidence Lower Bound (ELBO)
  - Why needed here: The APOVI-BNN is trained by maximising the ELBO, which balances data fit and prior fidelity.
  - Quick check question: What are the two terms that make up the ELBO, and what does each encourage the model to do?

- Concept: Bayesian Neural Networks and Weight Uncertainty
  - Why needed here: The APOVI-BNN is a Bayesian neural network that models uncertainty in its weights through a variational posterior.
  - Quick check question: How does a Bayesian neural network differ from a standard neural network in terms of weight representation?

- Concept: Meta-Learning and Task-Specific Adaptation
  - Why needed here: The APOVI-BNN is designed for meta-learning, where it must adapt to new tasks (datasets) with limited data.
  - Quick check question: In the context of the APOVI-BNN, what are the "tasks" that the model is learning to adapt to?

## Architecture Onboarding

- Component map: Context datapoints → Secondary Inference Networks → Approximate posterior parameters → Primary BNN weights → Predictions

- Critical path:
  1. Pass context datapoints through secondary inference networks to get per-datapoint posterior parameters
  2. Use these parameters to construct the approximate posterior over primary network weights
  3. Sample weights from the approximate posterior to make predictions on target datapoints

- Design tradeoffs:
  - Memory vs. Scalability: Using actual datapoints as inducing locations improves posterior quality but limits scalability to large datasets
  - Flexibility vs. Complexity: The secondary inference networks add flexibility but increase model complexity and training time

- Failure signatures:
  - Poor predictive performance on out-of-distribution tasks
  - Overconfidence in predictions when data is limited
  - Slow convergence or instability during training

- First 3 experiments:
  1. Compare ELBO values of APOVI-BNN and POVI-BNN on a small test dataset
  2. Evaluate predictive performance of APOVI-BNN vs. ConvCNP on a 1D regression task with limited meta-data
  3. Test image completion performance of APOVI-BNN vs. ConvCNP on MNIST with small meta-dataset

## Open Questions the Paper Calls Out

- What specific architectural modifications could make the APOVI-BNN competitive with ConvCNPs on complex tasks like image completion while maintaining computational feasibility?
- How does the APOVI-BNN's performance compare to other Bayesian meta-learning approaches like CNAPs or ML-PIP on real-world datasets with limited training data?
- What is the theoretical justification for why the APOVI-BNN's amortized inference performs better than traditional POVI-BNN in the regular model setting (no meta-learning)?
- How sensitive is the APOVI-BNN's performance to the choice of prior distribution, and would using less restrictive priors improve predictions under the NPVI objective?

## Limitations
- The APOVI-BNN would require very large architectures to match ConvCNPs on complex tasks like image completion, which would be computationally intensive
- The model's performance is limited by the assumption that pseudo-likelihoods factorise across datapoints, which may not hold for correlated data
- The paper focuses on controlled experimental settings rather than real-world applications where other meta-learning methods have been shown to perform well

## Confidence
- Mechanism 1: High - directly supported by abstract and section statements
- Mechanism 2: Medium - supported by claims but limited experimental detail
- Mechanism 3: Medium - structurally plausible but may oversimplify architectural differences

## Next Checks
1. Test APOVI-BNN performance with correlated datapoints to verify factorisation assumption holds
2. Compare training dynamics of APOVI-BNN with and without neural process objectives to isolate their impact
3. Evaluate scalability limits by testing APOVI-BNN on progressively larger datasets to identify memory constraints