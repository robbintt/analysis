---
ver: rpa2
title: 'Performance Comparison of Large Language Models on VNHSGE English Dataset:
  OpenAI ChatGPT, Microsoft Bing Chat, and Google Bard'
arxiv_id: '2307.02288'
source_url: https://arxiv.org/abs/2307.02288
tags:
- llms
- english
- chatgpt
- language
- bard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares the performance of three large language models
  (ChatGPT, BingChat, and Bard) on the VNHSGE English dataset, which consists of 250
  multiple-choice questions from Vietnamese high school exams. The models were evaluated
  on various question types, including pronunciation and stress, grammar and vocabulary,
  communication, reading fill-in-the-blank, and reading comprehension.
---

# Performance Comparison of Large Language Models on VNHSGE English Dataset: OpenAI ChatGPT, Microsoft Bing Chat, and Google Bard

## Quick Facts
- arXiv ID: 2307.02288
- Source URL: https://arxiv.org/abs/2307.02288
- Authors: 
- Reference count: 15
- Primary result: BingChat achieved highest accuracy at 92.4% on Vietnamese high school English exam dataset

## Executive Summary
This paper evaluates the performance of three large language models (ChatGPT, BingChat, and Bard) on the VNHSGE English dataset, consisting of 250 multiple-choice questions from Vietnamese high school exams. The study demonstrates that all three models significantly outperform Vietnamese students, with BingChat achieving the highest accuracy at 92.4%, followed by Bard at 86% and ChatGPT at 79.2%. The research highlights the potential of LLMs as effective tools for English language teaching and learning at the high school level in Vietnam, while also identifying areas where models struggle, particularly with pronunciation and stress questions.

## Method Summary
The study employs zero-shot learning with a standardized prompt format ("Choice: 'A' or 'B' or 'C' or 'D'; Explanation: ...") to evaluate three LLMs on a dataset of 250 multiple-choice English questions from Vietnamese high school exams (2019-2023). The questions cover five categories: pronunciation and stress, grammar and vocabulary, communication, reading fill-in-the-blank, and reading comprehension. Model responses are compared against ground truth answers to calculate accuracy percentages, with results benchmarked against Vietnamese students' average scores ranging from 3.2 to 5.84.

## Key Results
- BingChat achieved the highest accuracy at 92.4% on the VNHSGE dataset
- All three LLMs outperformed Vietnamese students, with average scores of 7.92, 9.24, and 8.6 for ChatGPT, BingChat, and Bard respectively
- Models struggled with pronunciation and stress questions, achieving low accuracy on the first four questions of the exam

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BingChat's superior performance stems from its fine-tuning on domain-specific Vietnamese educational data, enabling better handling of culturally contextualized vocabulary and grammar
- Mechanism: The model leverages additional training on Vietnamese high school exam patterns, improving accuracy in questions requiring local context
- Core assumption: The dataset used to evaluate models contains significant cultural-linguistic cues that only BingChat has been optimized to recognize
- Evidence anchors:
  - [abstract] "The results show that BingChat is better than ChatGPT and Bard"
  - [section 2.1] "GPT-4 has demonstrated human-level performance in many professional and academic criteria"
  - [corpus] "ChatGPT is Good but Bing Chat is Better for Vietnamese Students" - weak correlation (0.56), but relevant context
- Break condition: If the evaluation dataset lacks sufficient Vietnamese-specific cultural or linguistic patterns, BingChat's advantage would diminish

### Mechanism 2
- Claim: Zero-shot learning with structured prompt formatting enhances LLM consistency across different question types
- Mechanism: The explicit answer format constrains the model's output space, reducing hallucination and increasing alignment with ground truth answers
- Core assumption: LLMs respond more reliably when given explicit output templates, especially for multiple-choice formats
- Evidence anchors:
  - [section 3.2] "we introduced two variables: LLMB represents the case with the highest graded answer among the LLMs, while LLMW represents the case with the lowest graded answer"
  - [section 4.2.1] "The results show that the LLMs models do not provide accurate answers for the first four questions, which are related to phonetics"
  - [corpus] "Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education" - weak correlation (0.55), but relevant context
- Break condition: If the prompt structure becomes too rigid, it may limit the model's ability to express nuanced reasoning for complex comprehension questions

### Mechanism 3
- Claim: LLMs outperform Vietnamese students due to superior access to broader linguistic patterns and pre-training on larger corpora
- Mechanism: The models leverage general language understanding from massive pre-training datasets, compensating for lack of specific educational context
- Core assumption: The Vietnamese students' lower performance reflects limited exposure to diverse English language patterns compared to the LLM's training data
- Evidence anchors:
  - [section 4.3.1] "LLMs exhibit a higher level of English proficiency compared to Vietnamese students"
  - [section 2.1] "These models have advanced conversational abilities, closely resembling human-like interactions"
  - [corpus] "Can ChatGPT and Bard Generate Aligned Assessment Items? A Reliability Analysis against Human Performance" - weak correlation (0.0), limited relevance
- Break condition: If the evaluation questions require deep cultural or pedagogical knowledge specific to Vietnamese education, the LLM advantage would decrease

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The study uses zero-shot prompting without prior examples, requiring models to generalize from pre-training
  - Quick check question: What distinguishes zero-shot from few-shot learning in the context of multiple-choice question answering?

- Concept: Cross-lingual evaluation
  - Why needed here: Comparing LLM performance to Vietnamese students requires understanding both English proficiency and cultural context
  - Quick check question: How might cultural context in exam questions affect LLM performance compared to human test-takers?

- Concept: Multiple-choice assessment metrics
  - Why needed here: The study uses accuracy percentages, but doesn't account for partial credit or reasoning quality
  - Quick check question: What limitations arise from evaluating language models solely on multiple-choice accuracy?

## Architecture Onboarding

- Component map: Data ingestion -> Prompt formatting -> API call -> Response parsing -> Grading -> Statistical analysis
- Critical path: Question formatting -> LLM API -> Answer extraction -> Grading comparison
- Design tradeoffs: Zero-shot simplicity vs. few-shot accuracy; structured prompts vs. natural language generation
- Failure signatures: Incorrect format parsing, API rate limits, inconsistent grading criteria
- First 3 experiments:
  1. Test prompt formatting with a small sample of questions to verify output structure consistency
  2. Run a baseline comparison with random guessing to establish minimum performance thresholds
  3. Evaluate grading consistency by having multiple human raters grade a subset of LLM responses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance differences between BingChat, Bard, and ChatGPT on the VNHSGE dataset translate to their performance on other standardized English language tests in different countries?
- Basis in paper: [explicit] The paper compares the performance of three LLMs on the VNHSGE English dataset and shows significant differences in their accuracy
- Why unresolved: The study is limited to one dataset from Vietnam. It's unclear if these performance differences are consistent across different English language tests or in different cultural contexts
- What evidence would resolve it: Conducting similar comparative studies using other standardized English language tests from various countries would provide a broader understanding of the LLMs' relative strengths and weaknesses

### Open Question 2
- Question: What specific features or training data differences in BingChat, Bard, and ChatGPT contribute to their varying performance on pronunciation and stress questions in the VNHSGE dataset?
- Basis in paper: [inferred] The paper notes that all three LLMs perform poorly on the first four questions related to phonetics, suggesting a potential weakness in this area for these models
- Why unresolved: The paper doesn't delve into the technical reasons behind the models' performance differences or their specific training data
- What evidence would resolve it: A detailed analysis of the training data and architecture of each model, particularly focusing on their treatment of phonetics and pronunciation, would help identify the factors contributing to their performance differences

### Open Question 3
- Question: How do the strengths of BingChat, Bard, and ChatGPT in different question types (e.g., pronunciation, grammar, reading comprehension) align with the pedagogical needs of Vietnamese high school English language education?
- Basis in paper: [explicit] The paper shows that while all models outperform Vietnamese students, there are variations in their performance across different question types
- Why unresolved: The study doesn't connect the models' performance to specific educational needs or curriculum requirements in Vietnamese high schools
- What evidence would resolve it: An analysis of the Vietnamese high school English curriculum and an assessment of which model's strengths best align with the curriculum's priorities would help determine the most suitable LLM for educational purposes

## Limitations
- The evaluation relies on a Vietnamese-specific dataset (VNHSGE) that is not publicly available, making independent verification difficult
- The study doesn't address potential biases in the multiple-choice format or examine the quality of LLM explanations beyond simple accuracy metrics
- The mechanism explaining BingChat's superiority through Vietnamese-specific fine-tuning lacks direct evidence regarding training data composition

## Confidence

**High Confidence**: The comparative accuracy results showing BingChat (92.4%) > Bard (86%) > ChatGPT (79.2%) are internally consistent with the reported methodology

**Medium Confidence**: The claim that LLMs outperform Vietnamese students is valid but requires context about student demographics and test conditions

**Low Confidence**: The mechanism explaining BingChat's superiority through Vietnamese-specific fine-tuning lacks direct evidence, as the paper doesn't confirm BingChat's training data composition

## Next Checks

1. **Dataset Accessibility Verification**: Attempt to obtain the VNHSGE dataset or create a comparable Vietnamese high school English exam dataset to reproduce the accuracy results across all three LLMs

2. **Prompt Structure Impact Analysis**: Test whether variations in the standardized prompt format (Choice: "A" or "B" or "C" or "D"; Explanation: ...) significantly affect accuracy across different question types, particularly for pronunciation and stress questions

3. **Cultural Context Dependency Test**: Design a subset of questions that vary in Vietnamese cultural specificity to measure how model performance changes when cultural context requirements increase, helping validate whether BingChat's advantage is culturally dependent or generalizable