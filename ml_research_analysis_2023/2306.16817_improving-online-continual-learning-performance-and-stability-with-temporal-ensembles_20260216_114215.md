---
ver: rpa2
title: Improving Online Continual Learning Performance and Stability with Temporal
  Ensembles
arxiv_id: '2306.16817'
source_url: https://arxiv.org/abs/2306.16817
tags:
- learning
- continual
- task
- accuracy
- ensemble
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of temporal model ensembles to improve
  performance and stability in online continual learning. The key idea is to compute
  an exponential moving average (EMA) of model weights during training and use this
  as a stable evaluation model.
---

# Improving Online Continual Learning Performance and Stability with Temporal Ensembles

## Quick Facts
- **arXiv ID:** 2306.16817
- **Source URL:** https://arxiv.org/abs/2306.16817
- **Reference count:** 24
- **Primary result:** Using exponential moving average (EMA) of model weights improves online continual learning performance and stability by reducing task-recency bias and the stability gap.

## Executive Summary
This paper addresses the challenge of catastrophic forgetting in online continual learning by introducing a lightweight temporal ensemble method based on exponential moving average (EMA) of model weights. The authors demonstrate that using an EMA ensemble at test time improves both accuracy and stability metrics compared to standard continual learning approaches. The method requires minimal additional computation (storing one extra model) and consistently outperforms baselines across multiple datasets and learning methods. Notably, the EMA ensemble reduces the stability gap where performance drops at task boundaries and mitigates task-recency bias.

## Method Summary
The paper proposes using EMA of model weights during training, where the EMA model is used for evaluation. The EMA update follows the standard formulation: θt_ema = λθt-1_ema + (1-λ)θt. The method is implemented as a wrapper around existing continual learning algorithms (ER, ER-ACE, MIR, DER, RAR) without modifying their training procedures. The EMA model is updated after each training iteration and used for all evaluation. The approach requires minimal additional memory (one extra model) and computational overhead.

## Key Results
- EMA ensemble consistently improves final average accuracy across multiple datasets and methods
- The method reduces the stability gap, with WC-ACC improvements of 1.4-2.7% on CIFAR-100
- Task-recency bias is diminished, with more balanced predictions across tasks in confusion matrices
- AAA (Average Anytime Accuracy) improvements show better performance throughout training, not just at the end

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** EMA ensemble stabilizes predictions by averaging over temporally diverse models, reducing variance in online continual learning.
- **Mechanism:** EMA maintains a moving average of weights, implicitly ensembling models across training iterations. Older models receive exponentially less weight but still contribute.
- **Core assumption:** Summing model weights approximates ensemble behavior in functional space if models lie on low-loss paths.
- **Evidence anchors:** Abstract states EMA improves stability; section 4 discusses EMA formulation.
- **Break condition:** If loss landscape is highly non-convex and models drift far apart, EMA sum may not approximate ensemble.

### Mechanism 2
- **Claim:** EMA reduces task-recency bias by balancing influence from recent and older tasks.
- **Mechanism:** Exponential decay means earlier task models still contribute, preventing over-reliance on most recent task's parameters.
- **Core assumption:** Task-recency bias arises from last task model dominating; EMA mitigates by mixing in older task models.
- **Evidence anchors:** Section 6 shows reduced task-recency bias in confusion matrices; section 4 discusses EMA weighting scheme.
- **Break condition:** If λ is too low, older task models get negligible weight and bias returns.

### Mechanism 3
- **Claim:** EMA improves stability metrics by smoothing prediction variance across mini-batches.
- **Mechanism:** Averaging over checkpoints smooths abrupt changes in predictions when task distribution shifts.
- **Core assumption:** Variance in predictions across mini-batches contributes to instability; smoothing improves metrics.
- **Evidence anchors:** Section 6 shows improved WC-ACC and reduced accuracy variations; section 3.2 defines WC-ACC.
- **Break condition:** If model weights oscillate wildly, even EMA may not fully smooth variance.

## Foundational Learning
- **Concept:** Catastrophic forgetting
  - Why needed here: Core problem in continual learning that EMA addresses
  - Quick check question: What is catastrophic forgetting and why does it occur in neural networks trained on sequential tasks?
- **Concept:** Exponential moving average (EMA)
  - Why needed here: Central mechanism for temporal ensembling in this work
  - Quick check question: How does EMA differ from a simple average and why is this useful for online learning?
- **Concept:** Stability gap
  - Why needed here: Key metric for evaluating continual learning algorithms
  - Quick check question: Define the stability gap and explain why it matters in continual learning evaluation?

## Architecture Onboarding

- **Component map:** Base continual learning method (ER, ER-ACE, etc.) -> EMA weight updater -> Evaluation wrapper
- **Critical path:** Train base model normally -> Update EMA weights after each iteration -> Use EMA model for evaluation
- **Design tradeoffs:**
  - Memory: EMA requires storing one extra model; naive ensembles require storing many
  - Performance: EMA may underperform if λ poorly tuned for distribution drift speed
  - Training overhead: EMA update is negligible; naive ensembles require separate training runs
- **Failure signatures:**
  - Overfitting to last task (λ too low)
  - Instability in predictions (λ too high or inappropriate for task shift frequency)
  - No performance gain (EMA not properly synchronized with training loop)
- **First 3 experiments:**
  1. Run ER baseline on Split-Cifar100, compare accuracy with and without EMA
  2. Sweep λ values (0.95, 0.99, 0.995) on small dataset to find sweet spot
  3. Measure WC-ACC and AAA metrics to quantify stability improvements

## Open Questions the Paper Calls Out
The paper identifies several open questions but does not provide detailed discussion of them. The authors note that the dependency of EMA on the number of training iterations makes it difficult to export to non-online learning settings. They also mention that the choice of the momentum parameter λ should depend on the speed at which the input distribution changes, but do not investigate practical solutions for this dependency.

## Limitations
- Theoretical justification for why EMA weight averaging approximates functional ensembling in non-convex continual learning settings is limited
- Hyperparameter λ is critical but lacks systematic sensitivity analysis across different task distributions and drift rates
- Stability improvements could stem from regularization effects rather than pure ensemble behavior, which isn't explicitly tested

## Confidence

**High confidence:** Empirical results showing EMA improves final accuracy and stability metrics across multiple datasets are robust and well-documented.

**Medium confidence:** Claim that EMA reduces task-recency bias is supported by confusion matrix analysis but mechanism is primarily inferred rather than directly measured.

**Low confidence:** Theoretical mechanism explaining why EMA weight averaging approximates functional ensembling in continual learning is underdeveloped.

## Next Checks
1. **Ablation study:** Compare EMA against a naive checkpoint ensemble (storing multiple models) on the same tasks to isolate whether gains come from temporal diversity or simple averaging effects.

2. **Hyperparameter sweep:** Systematically vary λ across orders of magnitude (0.9 to 0.999) on datasets with different task transition frequencies to identify optimal ranges and failure modes.

3. **Mechanism isolation:** Test whether EMA's benefits persist when applied to frozen models (no further training) versus active training to distinguish regularization effects from true ensemble behavior.