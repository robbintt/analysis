---
ver: rpa2
title: Comparative Analysis of ChatGPT, GPT-4, and Microsoft Bing Chatbots for GRE
  Test
arxiv_id: '2312.03719'
source_url: https://arxiv.org/abs/2312.03719
tags:
- questions
- chatgpt
- correct
- bing
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the performance of three AI chatbots (Bing,
  ChatGPT, and GPT-4) on the GRE exam using 331 questions (137 quantitative, 157 verbal).
  GPT-4 outperformed the others with 83.21% accuracy on quantitative questions and
  87.26% on verbal questions.
---

# Comparative Analysis of ChatGPT, GPT-4, and Microsoft Bing Chatbots for GRE Test

## Quick Facts
- arXiv ID: 2312.03719
- Source URL: https://arxiv.org/abs/2312.03719
- Reference count: 6
- Primary result: GPT-4 achieved 83.21% accuracy on quantitative and 87.26% on verbal GRE questions, outperforming Bing (48.9% quant, 65.61% verbal) and ChatGPT (57.66% quant, 71.34% verbal)

## Executive Summary
This study evaluates the performance of three AI chatbots—Bing, ChatGPT, and GPT-4—on the GRE exam using 331 questions spanning quantitative and verbal reasoning sections. GPT-4 demonstrated superior performance across all categories, particularly excelling at complex verbal tasks and image-based questions. The findings reveal significant performance gaps between the chatbots, with GPT-4 achieving notably higher accuracy rates, suggesting that more advanced models better capture the reasoning patterns required for standardized tests.

## Method Summary
The study evaluated three AI chatbots on 331 GRE questions (137 quantitative, 157 verbal) sourced from the official ETS website. Questions were categorized by skill type (arithmetic, algebra, geometry, data analysis for quantitative; reading comprehension, text completion, sentence equivalence for verbal). Chatbots were presented with questions and their responses evaluated for correctness. A separate image-based dataset of 37 questions was also tested. Success rates were calculated for each chatbot across different question types and difficulty levels.

## Key Results
- GPT-4 achieved the highest accuracy: 83.21% on quantitative and 87.26% on verbal questions
- Bing showed 48.9% accuracy on quantitative and 65.61% on verbal questions
- ChatGPT reached 57.66% on quantitative and 71.34% on verbal questions
- All chatbots performed better on verbal than quantitative questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's higher accuracy stems from superior model sophistication and broader training data
- Mechanism: More advanced transformer architecture and diverse pretraining corpus allow GPT-4 to better capture complex reasoning patterns in quantitative and verbal tasks
- Core assumption: Model architecture improvements and training data diversity directly improve standardized test performance
- Evidence anchors: [abstract] "GPT-4 emerged as the most proficient, especially in complex language understanding tasks..."; [section] "GPT-4's exceptional performance, especially in the easy category, indicates its ability to handle a broad range of verbal questions..."

### Mechanism 2
- Claim: Bing's integration with Microsoft's Prometheus model gives it an advantage in research tasks but not standardized test accuracy
- Mechanism: Web search augmentation allows Bing to retrieve current facts but does not enhance internal reasoning capabilities needed for GRE problem-solving
- Core assumption: Real-time search improves factual accuracy but not structured reasoning in standardized tests
- Evidence anchors: [section] "The key distinction lies in Bing's integration with Microsoft's Prometheus model, which combines Bing Search with AI capabilities..."; [section] "Bing's performance on the main dataset could be attributed also to the complexity of such mathematical problems..."

### Mechanism 3
- Claim: Chatbots show differential performance across question difficulty levels due to training data bias toward certain complexity patterns
- Mechanism: Models trained on diverse difficulty levels develop specialized handling strategies, leading to varying success rates across easy, medium, and hard questions
- Core assumption: Training data distribution directly shapes model performance on different difficulty levels
- Evidence anchors: [section] "For ChatGPT, the success rate demonstrates an increase with higher difficulty levels. In contrast, when the question becomes easier, GPT-4 exhibits a higher likelihood of providing correct answers."; [section] "ChatGPT's performance in hard questions could be due to its extensive training on a diverse range of text data..."

## Foundational Learning

- Concept: Standardized test structure and question types
  - Why needed here: Understanding GRE format is essential to interpret chatbot performance metrics
  - Quick check question: What are the main sections of the GRE and how many question types exist in each?

- Concept: Natural Language Processing fundamentals
  - Why needed here: Core to understanding how chatbots process and respond to test questions
  - Quick check question: What are the key NLP tasks involved in answering GRE verbal questions?

- Concept: Transformer architecture and pretraining
  - Why needed here: Explains differences in chatbot performance based on model sophistication
  - Quick check question: How does transformer architecture enable better language understanding?

## Architecture Onboarding

- Component map: Input question → preprocessing → model inference → answer selection → confidence assessment
- Critical path: Question encoding → context understanding → answer generation → verification step
- Design tradeoffs: Real-time search integration vs. computational efficiency; model size vs. response speed
- Failure signatures: Consistent errors on specific question types; inability to handle visual data; confidence issues
- First 3 experiments:
  1. Test chatbots on a small subset of quantitative questions with known difficulty levels
  2. Compare single-answer vs. multi-answer question performance
  3. Evaluate image-based question handling with standardized image quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the underlying factors contributing to the observed differences in performance between Bing, ChatGPT, and GPT-4 across different skill categories and difficulty levels?
- Basis in paper: [explicit] The authors note that "additional research is required to gain insights into the reasons behind the chatbots' inability to solve certain questions, paving the way for further development and improvement of these chatbot systems."
- Why unresolved: The paper identifies performance differences but does not explore the underlying reasons for these differences in depth
- What evidence would resolve it: Detailed analysis of the training data, model architectures, and algorithmic approaches used by each chatbot, along with correlation studies between specific question types and model performance

### Open Question 2
- Question: How does the quality of input images affect the performance of AI chatbots in solving image-based questions?
- Basis in paper: [explicit] The authors mention that "the quality of some images used in the dataset was not perfect. This means that certain fonts and words in the provided images were unclear, which could hinder the chatbot's ability to properly analyze the question."
- Why unresolved: The study acknowledges the potential impact of image quality but does not systematically investigate this factor
- What evidence would resolve it: Controlled experiments varying image quality while keeping question content constant, and measuring the impact on chatbot performance across different models

### Open Question 3
- Question: What measures can be implemented to prevent cheating in online standardized tests when AI chatbots are readily accessible?
- Basis in paper: [explicit] The authors conclude that "it is essential to carefully consider the adaptability of online testing, especially in emergency situations like the Covid pandemic, in order to mitigate the risk of cheating incidents among students who heavily depend on these chatbots."
- Why unresolved: The paper raises concerns about potential misuse but does not propose specific solutions or preventative measures
- What evidence would resolve it: Development and testing of various anti-cheating strategies, such as proctoring software, question randomization, time constraints, or AI detection methods, along with their effectiveness in preventing chatbot-assisted cheating

## Limitations

- The study only used 331 questions from the official ETS website, which may not fully represent the breadth and complexity of actual GRE exams
- The evaluation methodology relies on binary correct/incorrect assessments without considering partial credit or nuanced reasoning quality
- The study excluded image-based questions from the main dataset except for a separate 37-question subset, limiting understanding of chatbots' full capabilities

## Confidence

**High Confidence:**
- GPT-4 outperformed Bing and ChatGPT on both quantitative and verbal GRE questions
- Bing performed better than ChatGPT on quantitative questions but worse on verbal questions
- All chatbots showed higher accuracy on verbal questions compared to quantitative questions

**Medium Confidence:**
- GPT-4's superior performance stems from its advanced architecture and broader training data
- Bing's integration with Microsoft's Prometheus model provides advantages in research tasks but not standardized test accuracy
- Chatbots show differential performance across question difficulty levels due to training data bias

**Low Confidence:**
- The specific mechanisms by which GPT-4 excels at complex verbal tasks
- The extent to which real-time search capabilities benefit Bing's performance
- The generalizability of these findings to other standardized tests or question formats

## Next Checks

1. Reproduce with expanded dataset: Test the same chatbots on a larger, more diverse set of GRE questions including all question types, particularly image-based questions, to verify the reported performance differences and assess generalizability.

2. Controlled difficulty analysis: Design experiments that systematically vary question complexity within specific skill categories to isolate the mechanisms behind differential performance across difficulty levels, particularly the inverse relationship between ChatGPT and GPT-4 performance on easier versus harder questions.

3. Cross-test generalization: Evaluate these chatbots on other standardized tests (SAT, GMAT, LSAT) using similar methodology to determine whether the observed performance patterns extend beyond GRE-specific content and question formats.