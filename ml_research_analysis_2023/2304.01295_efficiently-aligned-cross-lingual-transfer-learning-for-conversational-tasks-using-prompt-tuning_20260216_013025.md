---
ver: rpa2
title: Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks
  using Prompt-Tuning
arxiv_id: '2304.01295'
source_url: https://arxiv.org/abs/2304.01295
tags:
- classi
- aligned
- prompts
- language
- cross-lingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce XSGD, a large-scale multilingual conversation dataset
  by translating the English-only Schema-Guided Dialogue (SGD) dataset into 105 languages,
  containing approximately 330k utterances per language. We develop an efficient prompt-tuning-based
  method for learning aligned prompts to facilitate cross-lingual representations.
---

# Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning

## Quick Facts
- arXiv ID: 2304.01295
- Source URL: https://arxiv.org/abs/2304.01295
- Reference count: 21
- Introduces XSGD, a large-scale multilingual conversation dataset by translating the English-only Schema-Guided Dialogue (SGD) dataset into 105 languages

## Executive Summary
This paper presents XSGD, a large-scale multilingual conversation dataset containing approximately 330k utterances per language across 106 languages, created by translating the English-only Schema-Guided Dialogue (SGD) dataset. The authors develop an efficient prompt-tuning-based method for learning aligned prompts that facilitate cross-lingual representations, achieving significant improvements in cross-lingual transfer, particularly in few-shot settings, for intent classification and slot-filling tasks. The aligned prompts outperform both fine-tuning and prompt tuning approaches, with notable gains for low-resource languages, while demonstrating the strong modeling ability of NLI-based classifiers compared to vanilla classifiers in cross-lingual settings.

## Method Summary
The approach learns aligned prompts on translation pairs using masked language modeling and contrastive loss, capturing cross-lingual alignment without modifying the backbone model parameters. The method uses XLM-R or XLM-RoBERTa-XL as frozen backbone models, learns soft prompts on the XSGD parallel data, and employs both vanilla and NLI-based classifiers for downstream tasks. The NLI-based approach combines utterances with label descriptions to create entailment predictions, providing semantic context that helps the model generalize across languages even with limited training data.

## Key Results
- Aligned prompts outperform both fine-tuning and prompt tuning for cross-lingual transfer on conversational tasks
- NLI-based classifiers exhibit superior cross-lingual transfer ability, particularly in few-shot settings
- Learning aligned prompts on conversational domain data produces better cross-lingual representations than using non-conversational parallel data

## Why This Works (Mechanism)

### Mechanism 1
Prompt tuning with aligned prompts achieves superior cross-lingual transfer compared to full fine-tuning by learning language-agnostic representations while keeping the model frozen. The approach learns soft prompts on translation pairs using masked language modeling and contrastive loss. The prompts capture cross-lingual alignment without modifying the backbone model parameters.

### Mechanism 2
NLI-based classifiers demonstrate stronger cross-lingual generalization than vanilla classifiers, especially in few-shot settings. The NLI-based approach combines utterances with label descriptions to create entailment predictions, providing semantic context that helps the model generalize across languages even with limited training data.

### Mechanism 3
Learning aligned prompts on conversational domain data produces better cross-lingual representations than using non-conversational parallel data. The conversational nature of XSGD provides domain-specific contextual cues that help the model learn representations relevant to dialogue tasks, which are more useful for downstream conversation understanding.

## Foundational Learning

- Concept: Contrastive learning for cross-lingual alignment
  - Why needed here: The approach uses contrastive loss to align representations across languages by treating translation pairs as positive examples and other translations in the batch as negative examples
  - Quick check question: What is the temperature parameter τ set to in the contrastive loss, and why is it important?

- Concept: Prompt tuning vs fine-tuning tradeoffs
  - Why needed here: The paper compares prompt tuning (efficient, fewer parameters) with full fine-tuning (expensive, more parameters) to demonstrate the efficiency benefits
  - Quick check question: What percentage of parameters are typically tuned in prompt tuning compared to fine-tuning?

- Concept: Few-shot learning in cross-lingual settings
  - Why needed here: The paper specifically evaluates performance in few-shot settings to demonstrate the effectiveness of aligned prompts when English training data is limited
  - Quick check question: How many shots per intent are used in the SGD dataset experiments?

## Architecture Onboarding

- Component map: XSGD corpus → Aligned prompts learning → Backbone model (XLM-R/XLM-RoBERTa-XL) → Task-specific classifiers → Downstream task evaluation
- Critical path: Translation data → Aligned prompts learning → Downstream task training/inference
- Design tradeoffs:
  - Prompt tuning efficiency vs fine-tuning performance
  - Length of aligned prompts (tested with lengths 1, 8, 16, 100)
  - Choice of classifier (vanilla vs NLI-based)
- Failure signatures:
  - Poor retrieval accuracy on Tatoeba when aligned prompts are ineffective
  - Performance degradation on low-resource languages not supported by the backbone model
  - High variance across runs indicating unstable training
- First 3 experiments:
  1. Compare retrieval accuracy on Tatoeba with different prompt lengths (1, 8, 16, 100)
  2. Evaluate intent classification accuracy on XSGD with prompt tuning vs aligned prompts
  3. Test cross-lingual transfer performance on MASSIVE dataset using vanilla vs NLI-based classifiers in few-shot settings

## Open Questions the Paper Calls Out

### Open Question 1
How do the cross-lingual transfer capabilities of the aligned prompt approach compare to fine-tuning when scaling up the backbone model size beyond XLM-RoBERTa-XL? The authors observe that aligned prompts provide better modeling ability when increasing the backbone model size from XLM-R to XLM-RoBERTa-XL, but do not test models larger than XLM-RoBERTa-XL.

### Open Question 2
What is the impact of different parallel corpus sizes on the effectiveness of aligned prompts for cross-lingual transfer? The authors use XSGD, a parallel multilingual conversation dataset, to learn aligned prompts but do not explore how the size of the parallel corpus affects the quality of the aligned prompts.

### Open Question 3
How does the performance of aligned prompts compare to other cross-lingual alignment methods, such as explicit alignment objectives or multilingual knowledge distillation? The authors propose a prompt-tuning-based method for learning aligned prompts but do not compare their approach to other cross-lingual alignment methods.

## Limitations
- Cross-lingual transfer experiments rely on languages supported by XLM-R, potentially limiting coverage of low-resource languages
- Evaluation of cross-lingual retrieval is limited to Tatoeba and XNLI benchmarks, which may not capture conversational domain challenges
- The paper focuses primarily on English as the source language, limiting generalizability to other source languages

## Confidence
- High Confidence (4-5 claims): Aligned prompts consistently outperform both fine-tuning and prompt tuning; NLI-based classifiers show superior few-shot cross-lingual generalization; Cross-lingual retrieval performance improves significantly with aligned prompts; XSGD dataset provides high-quality parallel data
- Medium Confidence (2-3 claims): Conversational domain data provides better cross-lingual representations than non-conversational parallel data; Prompt tuning efficiency benefits outweigh performance gaps for slot-filling tasks
- Low Confidence (0-1 claims): The specific optimal prompt length for different tasks; The scalability of aligned prompts to languages beyond XLM-R's coverage

## Next Checks
1. Test aligned prompts on languages not well-supported by XLM-R to verify if the learned representations truly generalize beyond the backbone model's language coverage.
2. Conduct controlled experiments varying translation quality in the XSGD dataset to measure how translation errors affect aligned prompt learning effectiveness.
3. Systematically evaluate the NLI-based classifier's performance across different shot settings (1, 2, 5, 10, 50 shots) on multiple languages to confirm the consistent superiority claimed for few-shot scenarios.