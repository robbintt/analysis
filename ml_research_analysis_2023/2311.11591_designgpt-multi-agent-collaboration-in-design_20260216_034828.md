---
ver: rpa2
title: 'DesignGPT: Multi-Agent Collaboration in Design'
arxiv_id: '2311.11591'
source_url: https://arxiv.org/abs/2311.11591
tags:
- design
- designers
- system
- product
- designgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents DesignGPT, a multi-agent collaboration system
  that integrates design domain knowledge with large language models (LLMs) and image
  generation AI to support product design workflows. The system simulates a design
  company environment where AI agents assume roles like Product Manager, Design Director,
  and CMF Designer, collaborating with human designers through natural language interaction.
---

# DesignGPT: Multi-Agent Collaboration in Design

## Quick Facts
- arXiv ID: 2311.11591
- Source URL: https://arxiv.org/abs/2311.11591
- Reference count: 18
- Multi-agent AI system improves design output quality by 11-40% over separate AI tools

## Executive Summary
DesignGPT is a multi-agent collaboration system that integrates design domain knowledge with large language models and image generation AI to support product design workflows. The system simulates a design company environment where AI agents assume roles like Product Manager, Design Director, and CMF Designer, collaborating with human designers through natural language interaction. An experiment comparing DesignGPT to separate AI tools showed significant improvements in novelty (11.06%), completeness (24.26%), and feasibility (39.90%) of design outputs.

## Method Summary
The system implements a multi-agent framework with predefined design roles and standardized operating procedures from design thinking methodology. It uses GPT-4 for text generation with reconstructed prompts incorporating role definitions and context, Stable Diffusion with ControlNet for image generation, and a chat-room style interface for requirements input and collaboration. The evaluation compared 30 design learners using DesignGPT versus separate AI tools, measuring design output quality across novelty, completeness, and feasibility metrics.

## Key Results
- DesignGPT achieved 11.06% higher novelty scores compared to separate AI tools
- DesignGPT produced 24.26% higher completeness scores in design outputs
- DesignGPT demonstrated 39.90% higher feasibility scores for generated designs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DesignGPT's multi-agent collaboration improves designer performance by providing role-specific expertise that mirrors professional design workflows.
- Mechanism: The system assigns AI agents to specific design roles with defined responsibilities and SOPs, collaborating through natural language in a chat-room interface to maintain design thinking continuity.
- Core assumption: Design thinking requires multiple perspectives and specialized knowledge that individual AI tools cannot replicate when used separately.
- Evidence anchors: [abstract] "DesignGPT multi-agent collaboration framework, which uses artificial intelligence agents to simulate the roles of different positions in the design company"; [section II.C] "Roles have basic abilities, such as reflection, Knowledge Base, meeting presentation, and status management. Different roles have different skills."
- Break condition: If natural language interaction becomes too complex or agents fail to maintain coherent role-specific behavior, the workflow breaks down.

### Mechanism 2
- Claim: Integrated image and text generation with role-specific prompt engineering produces higher quality design outputs than separate tools.
- Mechanism: The system uses GPT-4 for text generation with reconstructed prompts that include role definitions, current skills, and historical records, coordinated with Stable Diffusion for image generation through the same interface.
- Core assumption: Coordinated AI systems with shared context produce more coherent outputs than disparate tools requiring manual integration.
- Evidence anchors: [section II.B] "The text input to the large language model is called prompt. Our system has designed a module to reconstruct the prompt words input to GPT4"; [section IV.A] "Strategy 2 group has an improvement of 11.06%, 24.26%, and 39.90% in novelty, completeness, and feasibility, respectively."
- Break condition: If prompt reconstruction fails to maintain context or if image generation quality degrades, the quality advantage disappears.

### Mechanism 3
- Claim: Standardized operating procedures (SOPs) reduce the cognitive load on designers by providing structured guidance through the design process.
- Mechanism: The system implements industry-standardized operating procedures for design thinking methodology, organizing the workflow into requirement analysis, design proposal, detailed design, and evaluation report phases.
- Core assumption: Designers benefit from structured workflows that reduce decision-making complexity and maintain process continuity.
- Evidence anchors: [abstract] "We have introduced industry-standardized operating procedures (SOPs)[13], it can effectively reduce the problems of psychedelic, repetitive, incoherent, and invalid feedback"; [section II.A.3] "Role definitions, imported requirements, and historical records will be input to LLM together"
- Break condition: If SOPs become too rigid and prevent creative exploration, or if designers find the structure constraining, adoption decreases.

## Foundational Learning

- Concept: Design thinking methodology
  - Why needed here: The system is built around design thinking principles, so understanding these is crucial for implementing and improving the framework.
  - Quick check question: What are the five stages of design thinking and how does DesignGPT map to each stage?

- Concept: Multi-agent systems and role-based collaboration
  - Why needed here: The core innovation is the multi-agent approach where different AI agents assume design roles and collaborate, requiring understanding of agent architectures.
  - Quick check question: How do role definitions and prompt engineering work together to create distinct agent behaviors in DesignGPT?

- Concept: Prompt engineering and context management
  - Why needed here: The system's effectiveness depends on sophisticated prompt reconstruction that maintains context across multiple interactions and agents.
  - Quick check question: What elements are included in the reconstructed prompts for different agent roles, and why are these elements important?

## Architecture Onboarding

- Component map:
  Frontend: Chat-room style interface for requirements input and team collaboration
  Middle layer: Role management system with prompt reconstruction module
  Backend: GPT-4 API for text generation, Stable Diffusion + ControlNet for image generation, BLIP for image-to-text conversion
  Knowledge base: Design thinking SOPs and domain knowledge

- Critical path: Requirements input → Role selection → Meeting room collaboration → Agent discussion → Output generation → Designer intervention/evaluation

- Design tradeoffs:
  - Single integrated system vs. separate specialized tools: Integration provides better workflow continuity but increases system complexity
  - Predefined roles vs. flexible agent behaviors: Structured roles provide clarity but may limit emergent problem-solving
  - Chat interface vs. other interaction patterns: Familiar interface but may limit non-textual interactions

- Failure signatures:
  - Agents producing incoherent or contradictory outputs
  - Loss of context across multiple conversation turns
  - Image generation quality degradation
  - Excessive latency in agent responses

- First 3 experiments:
  1. Test basic multi-agent chat functionality with predefined roles and simple design tasks
  2. Evaluate prompt reconstruction quality by comparing agent outputs with and without context integration
  3. Benchmark integrated vs. separate AI tool performance on a simple design task using the three evaluation metrics (novelty, completeness, feasibility)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the DesignGPT system's performance compare to other multi-agent AI systems that integrate different design methodologies beyond design thinking?
- Basis in paper: [inferred] The paper focuses on design thinking methodology and does not compare DesignGPT to systems using other design methodologies.
- Why unresolved: The study only evaluates DesignGPT against separate AI tools (Stable Diffusion + GPT-4) and does not explore comparisons with other multi-agent systems using different design approaches.
- What evidence would resolve it: Comparative studies between DesignGPT and other multi-agent AI systems employing various design methodologies, measuring performance across novelty, completeness, and feasibility metrics.

### Open Question 2
- Question: What is the long-term impact of using DesignGPT on designers' creative skills and problem-solving abilities?
- Basis in paper: [explicit] The paper mentions that interviews revealed users gave high praise for the interactive form of DesignGPT, but does not discuss long-term effects on designers' skills.
- Why unresolved: The study focuses on immediate performance improvements and user feedback, without exploring how extended use of the system might affect designers' independent creative capabilities.
- What evidence would resolve it: Longitudinal studies tracking designers' creative output and problem-solving skills over time with and without the use of DesignGPT.

### Open Question 3
- Question: How does the DesignGPT system handle highly specialized or niche design domains that require domain-specific knowledge beyond general design thinking principles?
- Basis in paper: [inferred] The system is described as incorporating a design thinking knowledge base, but the paper does not address its effectiveness in specialized design fields.
- Why unresolved: The experimental setup and discussion focus on general product design concepts without exploring the system's adaptability to specialized or technical design domains.
- What evidence would resolve it: Testing DesignGPT in various specialized design fields (e.g., aerospace, medical devices) and comparing its performance to domain experts or specialized AI tools.

## Limitations
- The study was conducted with 30 design learners rather than professional designers, limiting generalizability to real-world design workflows.
- The evaluation metrics for novelty, completeness, and feasibility are not fully detailed, making it difficult to assess their validity.
- The system's reliance on specific AI models (GPT-4 and Stable Diffusion 1.5) means results may not generalize to other model versions or architectures.

## Confidence

- **High Confidence**: The multi-agent architecture design and implementation details are well-specified and technically sound.
- **Medium Confidence**: The experimental results showing improvement over separate AI tools are statistically significant, but the evaluation methodology has gaps.
- **Low Confidence**: The long-term viability and adoption of the system in professional design environments remains unproven.

## Next Checks
1. **Professional Designer Validation**: Conduct the same experiment with professional product designers rather than learners to assess real-world applicability and identify workflow friction points.
2. **Evaluation Methodology Audit**: Develop and validate more rigorous, industry-standard metrics for assessing design output quality, particularly for the novelty and feasibility dimensions.
3. **Cross-Model Generalization Test**: Implement the same multi-agent system using different versions of GPT and Stable Diffusion to verify that improvements are not dependent on specific model characteristics.