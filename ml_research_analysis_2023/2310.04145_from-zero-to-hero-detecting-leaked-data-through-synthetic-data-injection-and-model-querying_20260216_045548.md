---
ver: rpa2
title: 'From Zero to Hero: Detecting Leaked Data through Synthetic Data Injection
  and Model Querying'
arxiv_id: '2310.04145'
source_url: https://arxiv.org/abs/2310.04145
tags:
- samples
- data
- dataset
- ldss
- empty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting whether a machine
  learning model has been trained on a leaked dataset. The authors propose a novel
  method called Local Distribution Shifting Synthesis (LDSS), which involves injecting
  synthetic data with local shifts in class distribution into the original dataset.
---

# From Zero to Hero: Detecting Leaked Data through Synthetic Data Injection and Model Querying

## Quick Facts
- arXiv ID: 2310.04145
- Source URL: https://arxiv.org/abs/2310.04145
- Reference count: 40
- Primary result: Novel method detects leaked data in ML models with >90% accuracy while preserving model performance

## Executive Summary
This paper introduces Local Distribution Shifting Synthesis (LDSS), a novel method for detecting whether a machine learning model has been trained on leaked data. The approach injects synthetic data with local class distribution shifts into the original dataset, creating a detectable pattern that reveals if models were trained on the modified data. LDSS works by identifying empty regions in feature space and synthesizing samples that alter local class distributions, then using these samples as triggers to query suspect models. The method is model-agnostic and compatible with various classification algorithms.

## Method Summary
LDSS works by transforming tabular data into numerical format, identifying large empty balls in feature space using simulated annealing, synthesizing samples within these empty regions that target the least frequent local class, and injecting these samples into the original dataset. When a suspect model is queried with trigger samples from the same synthetic distribution, models trained on the modified dataset will predict differently than those trained on the original data. The method preserves original data distribution in populated regions while creating detectable shifts in empty spaces.

## Key Results
- Achieves detection accuracy >90% across seven classification models and five real-world datasets
- Maintains minimal accuracy drop (<2%) on test samples from original distribution
- Successfully resists various attacks including adversarial training and transfer learning scenarios
- Compatible with diverse classification models including Naive Bayes, SVM, neural networks, and ensemble methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LDSS modifies local class distribution in specific feature subspaces to make models behave differently on synthetic trigger data
- Mechanism: Injects synthetic samples into empty balls (regions with no original data) to shift local class distribution
- Core assumption: Models will learn altered local distributions and predict differently for queries from those regions
- Break condition: Attackers using data augmentation or transfer learning may diminish the local shift effect

### Mechanism 2
- Claim: LDSS preserves model performance by targeting only unoccupied regions
- Mechanism: Focuses synthetic injection into empty balls to avoid altering original data distribution
- Core assumption: Models trained on modified dataset will still generalize well on test samples
- Break condition: Small or dense datasets may lack sufficient empty balls for effective injection

### Mechanism 3
- Claim: LDSS is model-agnostic through model querying rather than internal modifications
- Mechanism: Injects data and uses trigger samples to query target model for prediction differences
- Core assumption: Any model trained on modified dataset will respond to trigger queries accordingly
- Break condition: Adversarial training or ensemble methods may dilute local distribution shifts

## Foundational Learning

- Concept: Distance metrics for mixed numerical and categorical features (e.g., Jaccard distance)
  - Why needed: Transforms tabular data to fully numerical format for efficient empty-ball identification
  - Quick check: How does Jaccard distance change when sets share many elements?

- Concept: Knapsack problem and dynamic programming for sample synthesis
  - Why needed: Synthesizes samples matching target Jaccard distances to multiple pivots
  - Quick check: In 1D knapsack with target 10 and items 3, 4, 5, which combinations achieve target?

- Concept: Isolation Forest for outlier detection
  - Why needed: Ensures injected samples blend with original dataset distribution
  - Quick check: What happens to contamination parameter when outlier proportion increases?

## Architecture Onboarding

- Component map: Data transformation -> Empty ball identification -> Synthetic data generation -> Model querying
- Critical path: 1) Transform dataset to numeric space with pivots, 2) Identify large empty balls near data, 3) Synthesize samples targeting least frequent local class, 4) Query target model with trigger samples
- Design tradeoffs: Larger empty balls improve detection but harder to find; more injected samples improve robustness but increase computation
- Failure signatures: Low trigger accuracy gap suggests poor empty ball targeting; high outlier percentage indicates synthetic samples too distant; large test accuracy drop suggests overlap with populated regions
- First 3 experiments: 1) Run LDSS on small synthetic dataset and verify empty-ball positions in 2D, 2) Train simple classifier on modified data and check trigger accuracy, 3) Perform outlier detection on injected samples and measure % flagged

## Open Questions the Paper Calls Out

- Question: How does LDSS effectiveness vary on non-tabular datasets like images or text?
  - Basis: Paper focuses on tabular data while noting image/video watermarking techniques exist
  - Why unresolved: Performance on other data types unexplored
  - What evidence would resolve: Experimental results comparing LDSS on image, text, or other non-tabular datasets

- Question: Can LDSS detect data leakage in unsupervised or reinforcement learning models?
  - Basis: Paper focuses on classification models but data leakage affects other ML paradigms
  - Why unresolved: Application to unsupervised or RL models not addressed
  - What evidence would resolve: Demonstrations of LDSS effectiveness in unsupervised clustering or RL models

- Question: How does LDSS perform when attackers have partial knowledge of modification strategy?
  - Basis: Paper assumes attackers lack detailed insights into modification strategy
  - Why unresolved: Scenario with more detailed attacker knowledge unexplored
  - What evidence would resolve: Experimental results showing performance with varying attacker knowledge levels

## Limitations

- Dataset density sensitivity limits effectiveness on extremely small or large datasets
- Hyperparameter sensitivity not fully explored beyond fixed injection ratio of 10%
- Synthetic data quality and realism not thoroughly validated beyond outlier detection

## Confidence

**High Confidence**: Detection accuracy >90% across models and datasets; minimal test accuracy drop; compatibility with diverse classification models

**Medium Confidence**: Security against various attacks; computational efficiency claims

**Low Confidence**: Scalability to extremely large/complex datasets; long-term effectiveness as ML techniques evolve

## Next Checks

1. Systematically vary injection ratio, number of empty balls, and samples per ball to map full parameter space and identify optimal configurations

2. Apply LDSS to completely different domains (image datasets, time-series data) to validate generalizability beyond tabular classification

3. Design comprehensive adversarial strategies targeting LDSS including data poisoning, model ensemble attacks, and transfer learning scenarios to stress-test robustness claims