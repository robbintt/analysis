---
ver: rpa2
title: On Early Detection of Hallucinations in Factual Question Answering
arxiv_id: '2312.14183'
source_url: https://arxiv.org/abs/2312.14183
tags:
- hallucinations
- softmax
- artifacts
- hallucination
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether artifacts in large language model
  (LLM) generations can signal hallucinations in open-ended question answering. The
  authors analyze four types of artifacts: integrated gradients-based token attributions,
  Softmax probabilities, self-attention scores, and fully-connected layer activations.'
---

# On Early Detection of Hallucinations in Factual Question Answering

## Quick Facts
- **arXiv ID**: 2312.14183
- **Source URL**: https://arxiv.org/abs/2312.14183
- **Reference count**: 40
- **Primary result**: Model artifacts at the first generated token can predict hallucinations with up to 0.80 AUROC, with self-attention scores and fully-connected activations being most effective.

## Executive Summary
This paper investigates whether internal artifacts from large language models can signal hallucinations in open-ended factual question answering. The authors analyze four types of artifacts - Integrated Gradients attributions, Softmax probabilities, self-attention scores, and fully-connected layer activations - and find that their distributions differ between hallucinated and non-hallucinated generations. They demonstrate that binary classifiers trained on these artifacts can detect hallucinations with high accuracy, particularly using self-attention scores and fully-connected activations. Notably, they show that tokens preceding a hallucination can predict the hallucination before it occurs, suggesting these artifacts serve as early warning signals.

## Method Summary
The paper analyzes four types of model artifacts extracted at the first generated token: Integrated Gradients attributions, Softmax probability distributions, self-attention scores, and fully-connected layer activations. These artifacts are collected from multiple LLM variants (OpenLLaMA, OPT, Falcon) generating answers to questions from T-REx and TriviaQA datasets. Binary classifiers are trained using these artifacts as input features to distinguish hallucinated from non-hallucinated generations, with performance measured via AUROC. The study systematically evaluates each artifact type's effectiveness across different datasets and model sizes.

## Key Results
- Model artifacts differ in distribution between hallucinated and non-hallucinated generations, with self-attention scores and fully-connected activations showing the most consistent patterns
- Binary classifiers trained on these artifacts achieve up to 0.80 AUROC in detecting hallucinations
- Artifacts at the first generated token can predict subsequent hallucinations before they occur
- Self-attention scores and fully-connected activations are particularly effective across various datasets and models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-attention scores and fully-connected activations differ in distribution between hallucinated and non-hallucinated generations, enabling early detection.
- Mechanism: The internal model states at the first generated token encode whether the model is confidently grounded in facts or venturing into fabrication. These distributions diverge early enough that classification is possible before the hallucination fully manifests.
- Core assumption: The internal state at the first generated token is sufficiently predictive of the overall generation's correctness.
- Evidence anchors:
  - [abstract] "the distributions of these artifacts tend to differ between hallucinated and non-hallucinated generations"
  - [section] "self-attention scores and fully-connected activations are particularly effective across various datasets and models"
  - [corpus] Weak evidence—no specific citations for these two artifacts together in corpus, but related works on attention-based uncertainty exist.
- Break condition: If the internal state at the first token is dominated by formatting or task-specific boilerplate, the signal may wash out, reducing classification performance.

### Mechanism 2
- Claim: Softmax probability distributions at the first generated token are more peaked for correct generations than for hallucinations.
- Mechanism: When the model is confident about a correct fact, the top candidate tokens have high probability mass, yielding low entropy. Hallucinations spread probability across many candidates, increasing entropy.
- Core assumption: The initial token selection reflects the model's confidence about correctness, not just fluency or task-specific formatting.
- Evidence anchors:
  - [section] "we hypothesize that the Softmax distribution has a higher entropy when the model is hallucinating"
  - [section] "hallucinated outputs indeed show higher entropy than non-hallucinated ones"
  - [corpus] Moderate evidence—uncertainty modeling in NMT literature supports this hypothesis, but mixed results in the paper suggest context dependence.
- Break condition: If the first generated token is always a formatting character (e.g., newline), entropy differences may be negligible or masked.

### Mechanism 3
- Claim: Integrated Gradients token attributions are more dispersed for hallucinations than for correct generations.
- Mechanism: For correct generations, the model focuses on a small set of input tokens most relevant to the answer, yielding concentrated attribution scores. Hallucinations cause the model to rely on many input tokens, increasing entropy of attributions.
- Core assumption: The attribution method (IG) meaningfully captures input importance for answer correctness.
- Evidence anchors:
  - [section] "we posit that when answering the questions correctly, the model would focus on few input tokens"
  - [section] "for the hallucinated output, the feature attributions are far more spread out"
  - [corpus] Weak evidence—attribution-based hallucination detection is known in NMT but not robustly validated here.
- Break condition: If IG attributions are dominated by task-specific cues rather than semantic relevance, the dispersion signal may not correlate with correctness.

## Foundational Learning

- Concept: Transformer attention mechanisms and layer-wise propagation of information.
  - Why needed here: The paper relies on interpreting self-attention scores and hidden activations, which requires understanding how information flows through layers.
  - Quick check question: Can you explain why self-attention scores in later layers are more predictive than earlier layers for hallucination detection?

- Concept: Integrated Gradients and other feature attribution methods.
  - Why needed here: IG attributions are used to quantify input token importance; understanding how they work is key to interpreting results.
  - Quick check question: What is the key difference between IG attributions and simple gradient-based saliency?

- Concept: Entropy as a measure of uncertainty or dispersion.
  - Why needed here: The paper uses entropy of Softmax probabilities and attribution scores to distinguish hallucinations.
  - Quick check question: How does entropy of a probability distribution relate to the model's confidence?

## Architecture Onboarding

- Component map: Load datasets -> Generate completions with multiple LLMs -> Capture artifacts (IG, Softmax, attention, activations) -> Label as hallucination or not -> Train binary classifiers

- Critical path:
  1. Prompt model and capture generation
  2. Extract four artifact types at first generated token
  3. Train classifiers per artifact type
  4. Evaluate and compare across datasets and models

- Design tradeoffs:
  - Artifact choice: IG is computationally heavier but captures input importance; attention and activations are cheaper but may be less interpretable
  - Layer selection: Later layers give better detection but require more memory; early layers are cheaper but less accurate
  - Token selection: Using only the first generated token simplifies the task but may miss context-dependent signals

- Failure signatures:
  - High AUROC on training but low on test → overfitting to dataset-specific artifacts
  - Consistently low AUROC across all artifacts → model artifacts not predictive for that dataset/model combination
  - IG attributions dominate runtime → computational bottleneck; consider caching or approximations

- First 3 experiments:
  1. Compare entropy of Softmax distributions for hallucinated vs. non-hallucinated outputs across datasets
  2. Train and evaluate classifiers using only self-attention scores vs. only fully-connected activations
  3. Vary the Transformer layer index and plot AUROC to identify optimal depth

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does combining multiple generation artifacts (Softmax, IG, self-attention, and fully-connected activations) improve hallucination detection performance beyond individual artifact-based classifiers?
- Basis in paper: [explicit] The paper notes that a classifier trained on all four artifacts combined showed no improvement beyond models trained on each artifact individually, despite higher input dimensionality and architectural complexity.
- Why unresolved: The paper does not explore whether architectural modifications or feature engineering techniques could enable synergistic benefits from combining artifacts.
- What evidence would resolve it: Experiments with optimized architectures for multi-artifact fusion (e.g., attention mechanisms, ensemble methods) showing improved AUROC over single-artifact classifiers.

### Open Question 2
- Question: How does the performance of hallucination detection classifiers vary across different generation positions beyond the first token?
- Basis in paper: [explicit] The paper focuses on the first generated token but acknowledges preliminary experiments combining artifacts over all generated tokens showed no meaningful improvements.
- Why unresolved: The paper does not systematically analyze generation positions beyond the first token or explore temporal patterns in artifact distributions.
- What evidence would resolve it: Comprehensive analysis of artifact distributions and classifier performance across all generation positions, identifying optimal positions for hallucination detection.

### Open Question 3
- Question: Can hallucination detection artifacts be used to identify specific types of hallucinations (intrinsic vs extrinsic) or provide insight into hallucination mechanisms?
- Basis in paper: [inferred] The paper distinguishes between different types of hallucinations in related work but does not analyze whether artifacts can differentiate between them or provide mechanistic insights.
- Why unresolved: The paper focuses on binary classification of hallucinations without investigating artifact patterns specific to different hallucination types.
- What evidence would resolve it: Analysis showing distinct artifact patterns for intrinsic versus extrinsic hallucinations, potentially revealing underlying mechanisms of different hallucination types.

## Limitations
- Detection is demonstrated only at the first generated token, not during longer generation processes
- Computational overhead of IG attributions may limit real-time deployment feasibility
- Performance varies across datasets, suggesting potential domain-specific limitations

## Confidence

**Core Mechanism Confidence**
- **Self-attention and fully-connected activations**: Medium-High confidence. These artifacts show consistent distributional differences across multiple datasets and model sizes, with the strongest empirical support in the paper.
- **Softmax entropy**: Medium confidence. While the hypothesis is intuitive and supported by the data, the paper acknowledges mixed results across datasets, suggesting context dependence.
- **Integrated Gradients attributions**: Low-Medium confidence. The method shows weaker and less consistent performance, and the paper provides limited validation that IG attributions meaningfully capture input importance for correctness rather than task-specific formatting.

**Major Uncertainties**
Confidence is Low-Medium for the general claim that model artifacts predict hallucinations early. The paper shows strong AUROC scores (up to 0.80), but these are measured only at the first generated token, which may not generalize to real-time detection during longer generations. The analysis focuses on single-token prediction, leaving open whether artifacts remain predictive throughout the generation process.

## Next Checks
1. **Cross-dataset generalization**: Train hallucination classifiers on one dataset (e.g., T-REx) and test on another (e.g., TriviaQA) to assess whether artifact-based detection generalizes beyond the training distribution.

2. **Multi-token temporal analysis**: Extend the detection framework beyond the first token to track how artifact distributions evolve during generation, and measure detection accuracy at each subsequent token position.

3. **Computational feasibility study**: Benchmark the runtime overhead of extracting each artifact type (IG, Softmax, attention, activations) and analyze the tradeoff between detection accuracy and computational cost for real-time deployment scenarios.