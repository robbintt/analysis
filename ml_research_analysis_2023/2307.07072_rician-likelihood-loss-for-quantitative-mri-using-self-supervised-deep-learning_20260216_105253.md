---
ver: rpa2
title: Rician likelihood loss for quantitative MRI using self-supervised deep learning
arxiv_id: '2307.07072'
source_url: https://arxiv.org/abs/2307.07072
tags:
- loss
- signal
- parameter
- diffusion
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Self-supervised deep learning is widely used in quantitative MRI
  for parameter estimation, but suffers from bias in parameter estimates at low SNR
  due to the choice of Mean Squared Error (MSE) loss. The MSE loss is incompatible
  with Rician-distributed MR magnitude signals, which have non-zero mean noise.
---

# Rician likelihood loss for quantitative MRI using self-supervised deep learning

## Quick Facts
- arXiv ID: 2307.07072
- Source URL: https://arxiv.org/abs/2307.07072
- Authors: 
- Reference count: 40
- Self-supervised deep learning with Rician likelihood loss improves parameter estimation accuracy at low SNR in quantitative MRI

## Executive Summary
Self-supervised deep learning has become a promising approach for quantitative MRI parameter estimation, but suffers from biased estimates at low signal-to-noise ratios (SNR) when using traditional Mean Squared Error (MSE) loss. This work addresses this limitation by introducing a negative log Rician likelihood (NLR) loss function that properly accounts for the Rician distribution of MR magnitude signals. The authors develop a numerically stable implementation using exponentially-scaled Bessel functions and Chebyshev polynomials, and demonstrate improved accuracy in estimating diffusion coefficients for ADC and IVIM models across various SNR levels.

## Method Summary
The proposed method replaces traditional MSE loss with a negative log Rician likelihood (NLR) loss that directly maximizes the probability of measured data under the Rician noise model. The NLR loss is implemented using a numerically stable approach that avoids direct computation of modified Bessel functions through Chebyshev polynomial approximations of exponentially-scaled Bessel functions. The method is evaluated on synthetic diffusion MRI data for both ADC and IVIM models across multiple SNR levels (5-30), comparing performance against MSE loss using identical neural network architectures.

## Key Results
- NLR loss significantly reduces bias in diffusion coefficient estimation at low SNR compared to MSE loss
- The proposed implementation maintains numerical stability and accuracy across the full SNR range
- Improved accuracy is achieved with minimal loss of precision in parameter estimates
- Performance improvements are most pronounced for higher diffusion coefficients where effective SNR is lower

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing MSE loss with negative log Rician likelihood (NLR) loss reduces parameter estimation bias at low SNR in self-supervised qMRI.
- Mechanism: MR magnitude signals follow a Rician distribution with non-zero mean noise. MSE loss assumes zero-mean Gaussian noise and least squares estimates are unbiased only under this assumption. NLR loss directly maximizes the probability of the measured data under the Rician distribution, aligning the optimization with the true noise characteristics.
- Core assumption: The noise in MR magnitude signals is well-modeled by a Rician distribution, and the network can learn the true model parameters when trained with NLR loss.
- Evidence anchors:
  - [abstract]: "Such systematic errors arise from the choice of Mean Squared Error (MSE) loss function for network training, which is incompatible with Rician-distributed MR magnitude signals."
  - [section]: "Minimising the MSE is equivalent to least squares estimation, which is unbiased only if the noise is centered on the true signal i.e., has zero mean."
  - [corpus]: Weak - the corpus does not directly discuss Rician noise or the bias introduced by MSE loss.

### Mechanism 2
- Claim: The proposed NLR loss implementation using exponentially-scaled Bessel functions and Chebyshev polynomials is numerically stable and accurate across a wide range of SNRs.
- Mechanism: Previous implementations of Rician likelihood loss were numerically unstable or inaccurate. The proposed approach avoids direct computation of the modified Bessel function I₀(x) by using the exponentially-scaled Bessel function I₀ᵉ(x), which is approximated using Chebyshev polynomials. This ensures stability and accuracy.
- Core assumption: The Chebyshev polynomial approximation of the exponentially-scaled Bessel function is sufficiently accurate for the range of SNRs encountered in practical diffusion MR imaging.
- Evidence anchors:
  - [section]: "We propose a new approach that is numerically stable and highly accurate for a wide range of SNRs and can be readily implemented in common machine learning packages."
  - [section]: "The coefficients are tailored for high accuracy over low and high input ranges."
  - [corpus]: Weak - the corpus does not provide direct evidence about the numerical stability or accuracy of the proposed NLR loss implementation.

### Mechanism 3
- Claim: The NLR loss improves the accuracy of diffusion coefficient estimation in the ADC and IVIM models at low SNR with minimal loss of precision.
- Mechanism: The NLR loss accounts for the skewness of the Rician distribution at low SNR, allowing the network to make more accurate predictions of the noise-free signal decay rate. This leads to improved estimation of diffusion coefficients, which depend on this decay rate.
- Core assumption: The ADC and IVIM models accurately represent the relationship between the MR signals and the diffusion coefficients, and the NLR loss effectively guides the network to learn this relationship.
- Evidence anchors:
  - [abstract]: "Results show that the NLR loss improves parameter estimation accuracy of diffusion coefficients compared to MSE, with minimal loss of precision or total error."
  - [section]: "At low SNR, the MSE loss showed significant bias in diffusion coefficient estimation that worsened with higher diffusion coefficients; that is – when the effective SNR was lower."
  - [corpus]: Weak - the corpus does not provide direct evidence about the improvement in diffusion coefficient estimation accuracy with the NLR loss.

## Foundational Learning

- Concept: Rician distribution
  - Why needed here: Understanding the Rician distribution is crucial because MR magnitude signals follow this distribution, and the choice of loss function must align with this noise model.
  - Quick check question: What are the parameters of the Rician distribution, and how do they relate to the MR signal and noise characteristics?

- Concept: Maximum Likelihood Estimation (MLE)
  - Why needed here: MLE is the theoretical foundation for the NLR loss, as it aims to maximize the probability of the measured data under the predicted signal and noise model.
  - Quick check question: What are the key properties of MLE, and why is it known to be asymptotically unbiased under certain conditions?

- Concept: Bessel functions
  - Why needed here: Bessel functions are used in the computation of the Rician likelihood, and the proposed NLR loss implementation uses a specific approximation of the modified Bessel function I₀(x).
  - Quick check question: What are the key properties of Bessel functions, and how are they used in the context of Rician-distributed data?

## Architecture Onboarding

- Component map: Input MR signals → Encoder (fully connected NN) → Latent parameters → Decoder (biophysical model) → Predicted signals → Loss (NLR/MSE)
- Critical path: Input → Encoder → Latent space → Decoder → Output → Loss
- Design tradeoffs:
  - Network architecture: Fully connected layers vs. convolutional layers, number of layers and nodes
  - Loss function: NLR vs. MSE, impact on accuracy and precision
  - SNR range: Training data SNR range, impact on generalization to different SNRs
  - Model complexity: ADC vs. IVIM, impact on estimation accuracy and computational cost
- Failure signatures:
  - Bias in parameter estimates, especially at low SNR
  - Poor convergence during training
  - Numerical instability or overflow in the loss computation
  - Overfitting to training data, poor generalization to new data
- First 3 experiments:
  1. Compare the performance of NLR and MSE losses on simulated data with known ground truth for the ADC model at various SNRs.
  2. Evaluate the numerical stability and accuracy of the proposed NLR loss implementation using different SNR ranges and model parameter values.
  3. Apply the NLR loss to a real-world qMRI dataset and compare the parameter estimates with those obtained using MSE loss or traditional fitting methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the NLR loss function perform when applied to other quantitative MRI models beyond ADC and IVIM, such as relaxometry or magnetisation transfer imaging?
- Basis in paper: [inferred] The paper suggests that the NLR loss has potential application for improving the accuracy of deep-learning based qMRI, including relaxometry and magnetisation transfer imaging, but does not provide empirical results for these models.
- Why unresolved: The paper focuses on evaluating the NLR loss for ADC and IVIM models, leaving the performance of the loss on other qMRI models untested.
- What evidence would resolve it: Empirical results demonstrating the performance of the NLR loss on a range of qMRI models, including relaxometry and magnetisation transfer imaging, would resolve this question.

### Open Question 2
- Question: What is the impact of the choice of prior distribution in the posterior probability of parameter estimates on the estimation accuracy of the NLR loss?
- Basis in paper: [explicit] The paper mentions that extending the NLR loss to account for the posterior probability of parameter estimates given a prior distribution may mitigate bias in non-linear model parameter estimation, but does not explore this extension.
- Why unresolved: The paper does not investigate the impact of different prior distributions on the estimation accuracy of the NLR loss.
- What evidence would resolve it: Empirical results comparing the performance of the NLR loss with different prior distributions would resolve this question.

### Open Question 3
- Question: How does the NLR loss function perform when applied to low SNR data from nuclei other than protons, such as sodium?
- Basis in paper: [inferred] The paper suggests that the NLR loss has potential application to prediction tasks requiring batch-based optimization from Rician-distributed data, including low SNR data from nuclei other than protons, but does not provide empirical results for these applications.
- Why unresolved: The paper focuses on evaluating the NLR loss for proton MRI data, leaving the performance of the loss on low SNR data from other nuclei untested.
- What evidence would resolve it: Empirical results demonstrating the performance of the NLR loss on low SNR data from nuclei other than protons, such as sodium, would resolve this question.

## Limitations

- The validation is performed on simulated data rather than real-world MRI datasets, which may have noise characteristics that deviate from the idealized Rician model
- The study only evaluates two quantitative MRI models (ADC and IVIM), limiting generalizability to other qMRI applications
- The performance at very low SNR levels (below 5) is not explored, which could be relevant for certain clinical applications

## Confidence

- Mechanism 1 (Rician noise bias reduction): High confidence - well-established statistical theory supports the bias of MSE loss under non-zero mean noise
- Mechanism 2 (Numerical stability): Medium confidence - the Chebyshev polynomial approach is theoretically sound, but practical implementation details could affect performance
- Mechanism 3 (Improved diffusion coefficient estimation): Medium confidence - improvements are demonstrated on simulated data, but real-world validation is needed

## Next Checks

1. Test the NLR loss on real qMRI datasets across multiple institutions to verify that the improvement generalizes beyond simulated noise characteristics.

2. Evaluate the performance of NLR loss across a broader range of quantitative MRI models (T1, T2, relaxometry) to establish generalizability.

3. Compare the computational efficiency and training convergence properties of NLR versus MSE losses across different network architectures and dataset sizes.