---
ver: rpa2
title: 'Adversarial Erasing with Pruned Elements: Towards Better Graph Lottery Ticket'
arxiv_id: '2308.02916'
source_url: https://arxiv.org/abs/2308.02916
tags:
- graph
- sparsity
- accuracy
- citeseer
- cora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an adversarial complementary erasing (ACE)
  framework to improve graph lottery ticket (GLT) discovery. The authors identify
  that importance fluctuation during iterative magnitude-based pruning causes valuable
  information to be overlooked.
---

# Adversarial Erasing with Pruned Elements: Towards Better Graph Lottery Ticket

## Quick Facts
- arXiv ID: 2308.02916
- Source URL: https://arxiv.org/abs/2308.02916
- Reference count: 40
- Primary result: ACE-GLT finds sparser GLTs than existing methods, achieving 5-10% graph sparsity and 2-47% model sparsity improvements while maintaining or improving accuracy

## Executive Summary
This paper addresses the limitation of existing iterative magnitude-based pruning (IMP) methods in finding effective graph lottery tickets (GLTs) by introducing an adversarial complementary erasing (ACE) framework. The key insight is that important graph connections and model parameters are often overlooked during IMP due to importance fluctuation. ACE-GLT iteratively refines retained and pruned graph/model structures by exchanging the most disruptive elements in retained parts with the most discriminative elements in pruned parts, identified through gumbel-max sampling. The method consistently achieves significantly better sparsity-accuracy trade-offs across multiple datasets and GNN architectures.

## Method Summary
ACE-GLT extends IMP by identifying and recovering overlooked valuable information in pruned graph connections and model parameters. After each IMP round, the framework trains retained and pruned substructures separately, then uses gumbel-max sampling to locate the most disruptive elements in retained parts and most discriminative elements in pruned parts. These elements are exchanged to refine the masks, with an adaptive sampling limit K that prevents redundant exchanges by monitoring cosine similarity between consecutive rounds. This process iteratively enhances the final GLT performance while maintaining or improving accuracy.

## Key Results
- ACE-GLT consistently finds sparser GLTs than existing methods on Cora, Citeseer, PubMed, and OGB datasets
- Achieves 5-10% graph sparsity improvements and 2-47% model sparsity improvements
- Maintains or improves accuracy while reducing computational cost
- Effective across various GNN backbones including GCN, GIN, GAT, and deep ResGCN

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Importance fluctuation during iterative magnitude-based pruning causes valuable information to be overlooked
- Mechanism: Elements pruned early in IMP may become important later due to dynamic changes in graph and model structure. ACE recovers these elements by iteratively refining retained and pruned substructures
- Core assumption: Importance ranking of edges/weights changes significantly during iterative pruning
- Evidence: Abstract mentions overlooked valuable information in pruned connections; corpus papers don't explicitly discuss importance fluctuation dynamics

### Mechanism 2
- Claim: ACE framework maximizes surplus value extraction through adversarial complementary erasing
- Mechanism: After each IMP round, trains retained and pruned substructures separately, uses gumbel-max sampling to identify disruptive/discriminative elements, exchanges them to refine masks
- Core assumption: Most disruptive elements in retained parts and most discriminative elements in pruned parts can be effectively identified through magnitude-based sampling
- Evidence: Section describes using magnitude for importance and gumbel-max for obtaining discerning elements; corpus papers don't detail adversarial complementary erasing mechanism

### Mechanism 3
- Claim: Adaptive sampling limit K and re-sampling prevent redundant element exchanges while ensuring thorough exploration
- Mechanism: Calculates cosine similarity between sampled elements across rounds, halves K when similarity exceeds threshold to avoid unnecessary exchanges
- Core assumption: High similarity between consecutive rounds indicates redundant sampling that can be reduced without losing important information
- Evidence: Section describes calculating cosine similarity between rounds; corpus papers don't discuss adaptive sampling strategies in GLT context

## Foundational Learning

- Concept: Iterative Magnitude-Based Pruning (IMP)
  - Why needed: Paper builds on IMP as baseline pruning method that ACE-GLT improves upon
  - Quick check: What happens to importance ranking of elements during iterative magnitude-based pruning, and why does this matter for finding effective GLTs?

- Concept: Gumbel-Max Sampling
  - Why needed: ACE-GLT uses this technique to identify most important elements in pruned substructures for exchange
  - Quick check: How does gumbel-max sampling help avoid always selecting largest element while still identifying important elements?

- Concept: Graph Neural Networks (GNNs) and Lottery Ticket Hypothesis
  - Why needed: Paper extends lottery ticket hypothesis to graph neural networks through GLTs
  - Quick check: What is key difference between finding lottery tickets in standard neural networks versus graph neural networks?

## Architecture Onboarding

- Component map: IMP-based GLT search -> Adversarial Complementary Erasing with gumbel-max sampling -> Adaptive sampling control with cosine similarity monitoring
- Critical path: 1) Run IMP to get initial retained and pruned masks, 2) Train retained and pruned substructures separately, 3) Use gumbel-max sampling to identify exchange candidates, 4) Exchange elements and update masks, 5) Repeat until convergence or sparsity target reached
- Design tradeoffs: Trades increased training time (approximately 2x) for significantly improved sparsity and maintained accuracy, with adaptive sampling providing computational efficiency
- Failure signatures: 1) Performance degradation despite increased sparsity, 2) Very slow convergence or no improvement across iterations, 3) High similarity values persisting across many rounds indicating poor exploration
- First 3 experiments: 1) Reproduce baseline IMP results on Cora dataset with GCN to establish performance floor, 2) Implement ACE-GLT on Cora+GCN with fixed graph sparsity to compare model sparsity improvements, 3) Test adaptive sampling effectiveness by comparing with and without re-sampling and adaptive K techniques on Citeseer+GCN

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does importance fluctuation of edges and weights during iterative magnitude-based pruning vary across different types of graph datasets?
- Basis: Paper mentions importance fluctuation observed in Cora and Citeseer datasets with GCN but doesn't explore other types of datasets
- Why unresolved: Only provides experimental results on small-scale datasets and large-scale datasets without analyzing how importance fluctuation varies across different types of graph datasets
- What evidence would resolve it: Experiments on diverse set of graph datasets from different domains and analyzing patterns of importance fluctuation

### Open Question 2
- Question: How does ACE framework perform on heterogeneous graphs with multiple types of nodes and edges?
- Basis: Paper mentions current work only applies to single-task learning on homogeneous graphs, with future work to generalize to heterogeneous graphs
- Why unresolved: Explicitly states method is limited to homogeneous graphs and doesn't provide experimental results on heterogeneous graphs
- What evidence would resolve it: Applying ACE framework to heterogeneous graph datasets and comparing performance with existing methods

### Open Question 3
- Question: What is impact of different pruning ratios (pA and pW) on ACE framework performance across various graph datasets and backbone architectures?
- Basis: Paper mentions conducted experiments with different fixed graph sparsity and model sparsity settings and observed effects on performance
- Why unresolved: While provides some results on different pruning ratios, doesn't comprehensively analyze impact across various datasets and architectures
- What evidence would resolve it: Extensive experiments with wide range of pruning ratios on multiple datasets and backbone architectures, analyzing trends and patterns in performance

## Limitations
- Only validated on homogeneous graphs, with explicit acknowledgment that heterogeneous graph extension is future work
- Introduces significant computational overhead (approximately 2x training time) due to training both retained and pruned substructures separately
- Several critical hyperparameters (gumbel-max sampling parameters, similarity thresholds, adaptive K) not thoroughly explored for sensitivity and robustness

## Confidence

- **High confidence**: Empirical results showing improved sparsity-accuracy trade-offs on standard benchmarks (Cora, Citeseer, PubMed) are well-supported with clear quantitative comparisons to baselines
- **Medium confidence**: Mechanism of adversarial complementary erasing is theoretically sound, but effectiveness depends heavily on quality of gumbel-max sampling and similarity-based adaptive sampling, which weren't extensively validated
- **Medium confidence**: Generalization across different GNN backbones (GCN, GIN, GAT, ResGCN) demonstrates robustness, though paper doesn't explore edge cases or failure scenarios

## Next Checks

1. **Importance fluctuation validation**: Conduct ablation studies that systematically vary degree of importance fluctuation (e.g., by modifying pruning strategies) to quantify how critical this phenomenon is to ACE-GLT's success

2. **Hyperparameter robustness analysis**: Test method across wider range of hyperparameter settings to establish sensitivity and identify optimal configurations for different graph sizes and GNN architectures

3. **Scalability assessment**: Evaluate ACE-GLT on larger graphs and deeper GNN architectures to determine if doubled training time and adaptive sampling strategies remain effective and computationally feasible at scale