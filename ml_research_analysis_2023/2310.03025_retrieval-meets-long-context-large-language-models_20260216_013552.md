---
ver: rpa2
title: Retrieval meets Long Context Large Language Models
arxiv_id: '2310.03025'
source_url: https://arxiv.org/abs/2310.03025
tags:
- context
- arxiv
- long
- retrieval
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares two approaches for handling long contexts
  in large language models: retrieval-augmentation and extending the context window.
  The authors study both methods using two state-of-the-art LLMs: a proprietary 43B
  GPT and LLaMA2-70B.'
---

# Retrieval meets Long Context Large Language Models

## Quick Facts
- arXiv ID: 2310.03025
- Source URL: https://arxiv.org/abs/2310.03025
- Authors: 
- Reference count: 20
- Primary result: Retrieval-augmented LLaMA2-70B-32k outperforms GPT-3.5-turbo-16k and Davinci003 on long context tasks while being more computationally efficient than extending context windows.

## Executive Summary
This paper investigates two approaches for handling long contexts in large language models: retrieval-augmentation and extending the context window. Surprisingly, the authors find that a 4K context LLM with simple retrieval-augmentation can achieve comparable performance to a finetuned 16K context LLM on long context tasks, while using much less computation. More importantly, they demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Their best model, retrieval-augmented LLaMA2-70B with a 32K context window, outperforms GPT-3.5-turbo-16k and Davinci003 on seven long context tasks including question answering and query-based summarization.

## Method Summary
The study compares retrieval-augmentation versus long context extension using two state-of-the-art LLMs: a proprietary 43B GPT model and LLaMA2-70B. Context windows were extended from 4K to 16K and 32K using positional interpolation. Retrieval was performed using Dragon, Contriever, or OpenAI embeddings to identify relevant chunks from a corpus of chunked documents. The best retriever (Dragon) was used to retrieve top-5 chunks which were concatenated to the input context. Both models were instruction-tuned on a blended dataset of 102K samples. Zero-shot evaluation was performed on seven long context tasks including question answering and query-based summarization, with performance compared to GPT-3.5-turbo-16k and Davinci003 baselines.

## Key Results
- Retrieval-augmented 4K LLaMA2-70B achieves comparable performance to 16K LLaMA2-70B on long context tasks
- Retrieval-augmented LLaMA2-70B-32k outperforms GPT-3.5-turbo-16k and Davinci003 on seven long context benchmarks
- Retrieval significantly improves performance for both short (4K) and long (16K/32K) context LLMs
- The best model (retrieval-augmented LLaMA2-70B-32k) is faster at generation than its non-retrieval 32K baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval augmentation improves performance for both short (4K) and long (16K/32K) context LLMs.
- Mechanism: Retrieval provides relevant chunks that supplement the model's context, mitigating the "lost in the middle" phenomenon where models struggle with information in the middle of long inputs.
- Core assumption: The retrieval system can identify and provide the most relevant context chunks, and the LLM can effectively incorporate these chunks regardless of its native context window size.
- Evidence anchors:
  - [abstract]: "retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes"
  - [section]: "We observe that HotpotQA (HQA) especially favors long sequence models... However, it is quite interesting that the retrieval-augmented long context LLM (e.g., 16K and 32K) can obtain better results than retrieval-augmented 4K context LLM, even they are feed with the same top 5 chunks of evidence. We hypothesize this interesting observation is related to the 'lost in the middle' phenomenon"
  - [corpus]: Weak - corpus neighbors don't directly address this specific mechanism

### Mechanism 2
- Claim: Retrieval-augmented 4K context LLM can achieve comparable performance to 16K context LLM on long context tasks.
- Mechanism: Retrieval effectively provides the necessary context for the 4K model to perform well, compensating for its shorter context window and reducing computational cost.
- Core assumption: The most relevant information for answering queries is contained within the top retrieved chunks, which can fit within the 4K window.
- Evidence anchors:
  - [abstract]: "LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation"
  - [section]: "Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation"
  - [corpus]: Weak - corpus neighbors don't directly address this specific comparison

### Mechanism 3
- Claim: Retrieval-augmented LLaMA2-70B with 32K context window outperforms GPT-3.5-turbo-16k and Davinci003 on long context tasks.
- Mechanism: Combining the strengths of extended context and retrieval provides comprehensive coverage of relevant information, leading to superior performance.
- Core assumption: LLaMA2-70B-32k has sufficient capacity to process both its extended context and retrieved chunks effectively.
- Evidence anchors:
  - [abstract]: "Our best model, retrieval augmented LLaMA2-70B with 32K context window, outperforms GPT-3.5-turbo-16k and Davinci003 in terms of average score on nine long context tasks including question answering, query-based summarization, and in-context few-shot learning tasks"
  - [section]: "our best long context model LLaMA2-70B-32k performs as well as ChatGPT-3.5, while it can still be further enhanced by retrieval"
  - [corpus]: Weak - corpus neighbors don't directly address this specific model comparison

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: Understanding how retrieval systems work with LLMs to provide relevant context for generating responses is fundamental to this work.
  - Quick check question: How does a dual encoder retriever (like Dragon or Contriever) identify the most relevant chunks for a given query?

- Concept: Context window extension techniques (e.g., positional interpolation)
  - Why needed here: Knowing how to extend the context window of pretrained LLMs is crucial for comparing retrieval-augmented vs. long context approaches.
  - Quick check question: What is the main idea behind positional interpolation for extending RoPE-based LLMs' context windows?

- Concept: Evaluation metrics for long context tasks (e.g., ROUGE, F1, EM scores)
  - Why needed here: Understanding how the performance of different models is measured across various long context tasks is essential for interpreting the results.
  - Quick check question: Why is the geometric mean of ROUGE scores used for evaluating QMSum (QM) performance?

## Architecture Onboarding

- Component map: Retriever (Dragon, Contriever, OpenAI embeddings) -> LLM (GPT-43B, LLaMA2-70B with various context window sizes) -> Retriever-LLM integration (concatenating retrieved chunks to input context) -> Instruction tuning pipeline
- Critical path: Retriever identifies relevant chunks → Chunks concatenated to query → LLM processes combined input → LLM generates response
- Design tradeoffs:
  - Retrieval vs. long context: Retrieval is more computationally efficient but relies on the quality of the retrieval system; long context provides comprehensive information but is more computationally expensive.
  - Number of retrieved chunks: More chunks provide more context but may lead to information overload or distract the model.
  - Retriever choice: Different retrievers have varying performance and computational costs.
- Failure signatures:
  - Retrieval fails to identify relevant chunks → Poor model performance
  - Model struggles with too much context (even within its window) → Decreased performance with more retrieved chunks
  - Model cannot effectively integrate retrieved chunks → Limited benefit from retrieval
- First 3 experiments:
  1. Compare the performance of a 4K context LLM with and without retrieval on a sample long context task to validate Mechanism 1.
  2. Extend the context window of a pretrained LLM using positional interpolation and fine-tune it on a subset of the Pile dataset to ensure the extension works as expected.
  3. Evaluate the performance of different retrievers (Dragon, Contriever, OpenAI embeddings) on a sample task to determine the best retriever for the model and task combination.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does retrieval augmentation consistently improve performance across all types of long context tasks (QA, summarization, in-context learning) regardless of context window size?
- Basis in paper: [explicit] The paper shows retrieval helps both 4K and 32K LLaMA2-70B models, but the improvement varies by task and context size
- Why unresolved: The study only covers 7 specific datasets; it's unclear if results generalize to all long context tasks
- What evidence would resolve it: Testing retrieval augmentation on a broader range of long context tasks across different domains and context lengths

### Open Question 2
- Question: What is the optimal balance between context window size and number of retrieved chunks for different task types?
- Basis in paper: [inferred] The paper finds that more retrieved chunks don't always improve performance, and 4K+retrieval can match 16K models
- Why unresolved: The study doesn't systematically explore all combinations of context window sizes and retrieval chunk numbers
- What evidence would resolve it: Controlled experiments varying both context window size and number of retrieved chunks across multiple task types

### Open Question 3
- Question: How do retrieval-augmented long context models compare to specialized long context architectures like sparse attention methods?
- Basis in paper: [explicit] The paper compares retrieval vs. long context extension but doesn't compare to sparse attention methods
- Why unresolved: The study focuses only on exact attention models with retrieval augmentation
- What evidence would resolve it: Direct comparison of retrieval-augmented models vs. sparse attention models on the same long context tasks

## Limitations

- The study focuses on a specific set of seven tasks and three retrievers, which may not generalize to all long-context scenarios
- The proprietary 43B GPT model's exact architecture remains unspecified, limiting direct comparison reproducibility
- The "much less computation" claim may not hold in all deployment scenarios as full computational costs (including retrieval indexing and search time) aren't fully detailed

## Confidence

**High Confidence:** The core finding that retrieval-augmented LLaMA2-70B-32k outperforms GPT-3.5-turbo-16k and Davinci003 on the tested benchmarks is well-supported by the experimental results.

**Medium Confidence:** The claim that 4K context with retrieval matches 16K context without retrieval is supported but may be task-dependent, with performance gaps varying significantly across datasets.

**Low Confidence:** The assertion that retrieval can significantly improve performance "regardless of their extended context window sizes" needs qualification, as the magnitude of improvement differs substantially across models and tasks.

## Next Checks

1. **Cross-dataset generalization test:** Evaluate the best model (retrieval-augmented LLaMA2-70B-32k) on an additional long-context dataset not included in the original seven, such as BookSum or Arxiv papers, to test generalizability beyond the studied domains.

2. **Chunk size sensitivity analysis:** Systematically vary the chunk size (e.g., 200, 400, 600 words) in the retrieval pipeline to determine optimal chunking strategies and test whether the 300-word chunks are truly optimal or if larger chunks might capture more contextual information.

3. **Retrieval quality benchmarking:** Implement a comprehensive recall@k evaluation for each retriever (Dragon, Contriever, OpenAI) on a held-out validation set to quantify how often the retrievers successfully identify relevant chunks, directly measuring the quality of the retrieval component rather than relying solely on downstream task performance.