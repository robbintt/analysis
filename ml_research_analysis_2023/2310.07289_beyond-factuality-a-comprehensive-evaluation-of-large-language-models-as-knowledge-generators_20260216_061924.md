---
ver: rpa2
title: 'Beyond Factuality: A Comprehensive Evaluation of Large Language Models as
  Knowledge Generators'
arxiv_id: '2310.07289'
source_url: https://arxiv.org/abs/2310.07289
tags:
- knowledge
- evaluation
- factuality
- generated
- topic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a framework to evaluate LLM-generated knowledge
  in knowledge-intensive tasks. It assesses both intrinsic quality (factuality, relevance,
  coherence, informativeness) and extrinsic reliability (helpfulness, validity).
---

# Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators

## Quick Facts
- **arXiv ID**: 2310.07289
- **Source URL**: https://arxiv.org/abs/2310.07289
- **Reference count**: 37
- **Primary result**: LLM-generated knowledge surpasses retrieved knowledge in most metrics, though factuality remains lower

## Executive Summary
This paper introduces CONNER, a comprehensive framework for evaluating LLM-generated knowledge in knowledge-intensive tasks without requiring human involvement or reference knowledge. The study reveals that while generated knowledge shows lower factuality than retrieved knowledge, it consistently outperforms retrieval across relevance, coherence, and informativeness metrics. Surprisingly, downstream task performance is more sensitive to relevance and coherence issues than factual errors. The analysis identifies long-tail knowledge and longer generation length as key factors degrading factuality, and proposes prompt engineering and knowledge selection strategies to improve performance.

## Method Summary
The study evaluates knowledge generation using 500 sampled examples from NQ and WoW datasets, comparing three LLM types (FLAN-T5, LLaMA, ChatGPT) against DPR retrieval baseline. CONNER framework assesses six metrics across intrinsic quality (factuality, relevance, coherence, informativeness) and extrinsic reliability (helpfulness, validity) without requiring reference knowledge. Evaluation employs model-based metrics, zero-shot and few-shot settings, and investigates the impact of knowledge frequency and generation length on factuality. The study implements prompt engineering and knowledge selection strategies guided by CONNER framework to optimize knowledge generation.

## Key Results
- Generated knowledge outperforms retrieved knowledge in relevance, coherence, and informativeness metrics
- Factuality of generated knowledge remains lower than retrieved knowledge
- Relevance and coherence have stronger impact on downstream task performance than factuality
- Long-tail knowledge and longer generation length significantly reduce factuality
- Prompt engineering and knowledge selection strategies improve both knowledge generation and downstream task performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Long-tail knowledge and longer generation length reduce factuality
- Mechanism: Generated knowledge quality degrades when models produce rare or long-form content due to insufficient training exposure and error accumulation
- Core assumption: Model training data distribution correlates with factuality performance
- Evidence anchors:
  - [section] "Long-tail Knowledge We investigate the impact of the knowledge frequency on the factuality performance of LLaMA on the WoW dataset..."
  - [section] "Long-form Generation We investigate the impact of generation length on the factuality of the generated knowledge..."

### Mechanism 2
- Claim: Relevance and coherence are more important than factuality for downstream task performance
- Mechanism: Downstream tasks can tolerate factual errors if the knowledge is relevant and coherent enough to provide useful context
- Core assumption: Task performance depends more on information utility than absolute factual accuracy
- Evidence anchors:
  - [section] "Surprisingly, our study reveals that the factuality of generated knowledge, even if lower, does not significantly hinder downstream tasks..."
  - [section] "We present a case study in Table 4, which intuitively shows that the presence of factual errors in non-critical information has minimal impact on downstream tasks..."

### Mechanism 3
- Claim: CONNER framework's multi-perspective evaluation outperforms reference-reliant metrics
- Mechanism: Reference-free evaluation using evidence-based factuality, relevance ranking, and coherence models better captures knowledge quality than metrics dependent on human-labeled references
- Core assumption: Generated knowledge often falls outside reference knowledge scope, making reference-based evaluation inadequate
- Evidence anchors:
  - [abstract] "Different from previous studies, we propose a comprehensive framework for evaluating knowledge generated by LLMs..."
  - [section] "Table 10 illustrate the results of four models on the NQ dataset. We observe that: (1) CONNER yields consistently good correlations with human evaluation..."

## Foundational Learning

- **Concept: Factuality evaluation without ground truth**
  - Why needed here: Generated knowledge often contains novel information not present in references
  - Quick check question: How does CONNER validate factuality without relying on reference knowledge?

- **Concept: Intrinsic vs extrinsic evaluation**
  - Why needed here: Understanding internal knowledge quality separately from its downstream impact
  - Quick check question: What's the difference between evaluating knowledge quality and its usefulness?

- **Concept: Correlation analysis between metrics**
  - Why needed here: To understand which quality dimensions actually impact task performance
  - Quick check question: Why might factuality have weak correlation with task performance?

## Architecture Onboarding

- **Component map**: Knowledge generation pipeline (retrieval vs generation) → CONNER evaluation framework (6 metrics) → Downstream task integration → Prompt engineering module → Knowledge selection module

- **Critical path**: Knowledge generation → CONNER evaluation → Task performance → Prompt/selection optimization

- **Design tradeoffs**: Reference-free evaluation trades precision for generality; multi-perspective evaluation trades simplicity for comprehensiveness

- **Failure signatures**: Poor factuality despite high relevance; good intrinsic scores but poor extrinsic performance; inconsistent correlations across datasets

- **First 3 experiments**:
  1. Run CONNER evaluation on baseline DPR vs LLM generation to establish performance gap
  2. Test correlation between CONNER metrics and human judgments on sample dataset
  3. Implement prompt engineering optimization and measure impact on downstream task performance

## Open Questions the Paper Calls Out

- **Open Question 1**: How does CONNER's performance change when applied to non-English languages, particularly low-resource languages without existing NLP models?
  - Basis: The paper notes potential generalization challenges for non-English languages
  - Resolution: Empirical studies applying CONNER to various non-English languages

- **Open Question 2**: What are the specific error patterns and frequencies of factual errors in LLM-generated knowledge, particularly regarding numbers, time, and fabricated concepts?
  - Basis: The paper identifies these error patterns through analysis but lacks detailed quantitative analysis
  - Resolution: Comprehensive dataset annotating fine-grained factual errors with frequency distributions

- **Open Question 3**: How does the frequency and length of generated knowledge impact its factuality, and what are the optimal parameters for knowledge generation?
  - Basis: Initial findings on knowledge frequency and length impacts
  - Resolution: Systematic experiments varying frequency and length parameters

- **Open Question 4**: How does model size of LLMs affect their performance in knowledge generation across different evaluation perspectives?
  - Basis: Some insights on model size impact but incomplete analysis
  - Resolution: Extensive experiments varying model sizes across all evaluation perspectives

## Limitations

- Reference-free evaluation may overestimate knowledge quality by conflating novel information with factual errors
- Factuality assessment relies heavily on evidence-based verification without ground truth references
- Analysis of long-tail knowledge is constrained by Wikipedia pageview data availability

## Confidence

- **High Confidence**: Comparative analysis showing generated knowledge outperforms retrieval on most intrinsic metrics
- **Medium Confidence**: Mechanisms explaining long-tail knowledge impact and effectiveness of optimization strategies
- **Low Confidence**: Claims about minimal impact of factual errors in non-critical information

## Next Checks

1. Test CONNER framework and identified quality-trait correlations on datasets from different domains (medical, legal, scientific)
2. Systematically analyze cases where factual errors in critical information impact downstream performance
3. Implement reference-grounded evaluation for a subset of data to validate CONNER metrics and quantify evaluation trade-offs