---
ver: rpa2
title: Cramer Type Distances for Learning Gaussian Mixture Models by Gradient Descent
arxiv_id: '2307.06753'
source_url: https://arxiv.org/abs/2307.06753
tags:
- learning
- distribution
- gaussian
- distance
- cram
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a closed-form formula for the Cram\xE9r\
  \ 2-distance between univariate Gaussian mixture models (GMMs) and extends it to\
  \ multivariate GMMs using the Sliced Cram\xE9r 2-distance. The proposed method is\
  \ compatible with gradient descent, enabling seamless integration with neural networks,\
  \ and does not require sampling from the target model."
---

# Cramer Type Distances for Learning Gaussian Mixture Models by Gradient Descent

## Quick Facts
- **arXiv ID:** 2307.06753
- **Source URL:** https://arxiv.org/abs/2307.06753
- **Reference count:** 40
- **Key outcome:** Introduces closed-form Cramér 2-distance for univariate GMMs and extends to multivariate GMMs using Sliced Cramér 2-distance, enabling gradient-based learning with theoretical guarantees.

## Executive Summary
This paper proposes a novel approach for learning Gaussian Mixture Models (GMMs) using Cramér type distances with gradient descent. The authors derive a closed-form formula for the Cramér 2-distance between univariate GMMs, which can be computed directly through common machine learning libraries without sampling. This formula is then extended to multivariate GMMs through the Sliced Cramér 2-distance, which uses random projections. The method provides theoretical guarantees including global gradient boundedness and unbiased sampling gradients, making it compatible with neural network optimization. Experimental results demonstrate its effectiveness in both distributional reinforcement learning and fitting complex multivariate distributions.

## Method Summary
The method introduces a closed-form formula for computing the Cramér 2-distance between univariate GMMs using the Gaussian error linear unit (GELU) function and exponential terms, avoiding numerical integration. For multivariate GMMs, it extends this through the Sliced Cramér 2-distance by projecting distributions onto random unit vectors and averaging the univariate distances. The approach enables direct gradient computation through automatic differentiation, compatible with deep learning frameworks. Theoretical analysis provides global bounds on gradients (≤4 for means and variances), ensuring stable optimization. The method is demonstrated in distributional reinforcement learning and multivariate GMM fitting tasks.

## Key Results
- Achieves 279 ± 22 score on LunarLander-v2 using distributional DQN with GMM-based return distribution learning
- Successfully fits multivariate GMMs to complex data distributions through gradient descent optimization
- Provides theoretical guarantees including global gradient boundedness and unbiased sampling gradients

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Closed-form formula enables exact, differentiable computation of Cramér 2-distance
- **Mechanism:** Direct computation of integral of squared CDF differences using GELU and exponential terms avoids numerical integration
- **Core assumption:** Formula remains stable across all parameter values
- **Evidence anchors:** Formula derivation provided; GELU available in PyTorch
- **Break condition:** Numerical instability from catastrophic cancellation

### Mechanism 2
- **Claim:** Sliced Cramér 2-distance extends univariate formula to multivariate case
- **Mechanism:** Projects GMMs onto random unit vectors, computes univariate distances, and averages results
- **Core assumption:** Random projections provide sufficient approximation
- **Evidence anchors:** Leverages Cramér-Wold theorem; projection-based approximation described
- **Break condition:** Insufficient projections lead to poor distance approximation

### Mechanism 3
- **Claim:** Gradients are globally bounded, preventing explosion during training
- **Mechanism:** Theorem proves partial derivatives bounded by constants (4 for means, 4 for variances)
- **Core assumption:** Boundedness extends to multivariate case through projections
- **Evidence anchors:** Theorem 3 and 6 provide theoretical bounds
- **Break condition:** Proof may not extend to all configurations or implementation errors

## Foundational Learning

- **Concept:** Cumulative Distribution Function (CDF) properties
  - Why needed here: Cramér 2-distance defined as L2 distance between CDFs
  - Quick check question: What are key CDF properties that make it suitable for distance computation?

- **Concept:** Gaussian Mixture Models (GMMs)
  - Why needed here: Entire method designed for learning and comparing GMMs
  - Quick check question: How do you compute CDF of a GMM from component CDFs?

- **Concept:** Automatic differentiation in deep learning frameworks
  - Why needed here: Method relies on gradient descent requiring gradient computation through closed-form formula
  - Quick check question: How does PyTorch compute gradients through non-elementary functions like GELU?

## Architecture Onboarding

- **Component map:** Loss function module -> Projection module (multivariate) -> Parameterization module -> Optimization module
- **Critical path:** 1) Sample projections (multivariate only) -> 2) Compute projected GMMs -> 3) Calculate Cramér 2-distance -> 4) Backpropagate gradients -> 5) Update parameters with bounded steps
- **Design tradeoffs:** Fixed vs. adaptive number of projections (accuracy vs. computation); learning rate scheduling (convergence vs. stability); covariance parameterization (Cholesky vs. direct enforcement)
- **Failure signatures:** NaN/Inf values in loss (numerical instability); degenerating components (fractions converging to extremes); poor convergence (insufficient projections or inappropriate learning rates)
- **First 3 experiments:** 1) Fit univariate GMM to synthetic data with known parameters; 2) Compare Sliced Cramér 2-distance vs. NLL on simple multivariate GMM fitting; 3) Test distributional DQN on LunarLander-v2 with different numbers of Gaussian components

## Open Questions the Paper Calls Out
- **Open Question 1:** How does Cramér 2-distance compare to other distance metrics for GMM learning in terms of computational efficiency and accuracy?
- **Open Question 2:** What are potential applications beyond distributional reinforcement learning?
- **Open Question 3:** How does the method handle high-dimensional data and what are its limitations?

## Limitations
- Numerical stability of closed-form formula not empirically validated beyond theoretical derivation
- Optimal number and strategy for projection sampling in high-dimensional cases unclear
- No comparative analysis against standard NLL-based GMM learning methods

## Confidence
- **High confidence:** Theoretical boundedness of gradients (Theorem 3, 6)
- **Medium confidence:** Closed-form formula computational stability
- **Low confidence:** Optimal projection sampling strategy and number of projections

## Next Checks
1. Conduct systematic numerical stability analysis across diverse parameter ranges, testing for overflow, underflow, and catastrophic cancellation
2. Perform ablation studies on number of projections to determine accuracy-computation tradeoff
3. Compare learning dynamics against standard NLL-based GMM training on benchmark datasets with known ground truth parameters