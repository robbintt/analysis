---
ver: rpa2
title: 'Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large
  Language Models'
arxiv_id: '2308.11764'
source_url: https://arxiv.org/abs/2308.11764
tags:
- knowledge
- arxiv
- teacher
- answers
- injection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of hallucination estimation and
  reduction in low-parameter open-source language models like BLOOM 7B, which are
  more prone to hallucinations compared to larger models. The authors introduce HALO
  CHECK, a lightweight BlackBox knowledge-free framework that quantifies hallucination
  severity by computing consistency scores among sampled model responses using entailment-based
  sentence-level agreement.
---

# Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models

## Quick Facts
- arXiv ID: 2308.11764
- Source URL: https://arxiv.org/abs/2308.11764
- Reference count: 30
- Primary result: Introduces HALO CHECK, a lightweight framework for hallucination estimation in low-parameter open-source LLMs, showing improved correlation with human annotations over existing metrics.

## Executive Summary
This paper addresses the challenge of hallucination estimation and reduction in low-parameter open-source language models like BLOOM 7B, which are more prone to hallucinations compared to larger models. The authors introduce HALO CHECK, a lightweight BlackBox knowledge-free framework that quantifies hallucination severity by computing consistency scores among sampled model responses using entailment-based sentence-level agreement. They also explore knowledge injection via entity summaries and triplets from Wikidata, and a teacher-student approach using GPT-4 to guide the weaker model. Experiments on a domain-specific NBA QA dataset show that HALO CHECK correlates better with human annotations than existing metrics, and that knowledge injection and selective teacher intervention effectively reduce hallucinations. Models improved via knowledge injection achieved consistency scores near 0.5, significantly higher than the baseline's negative score.

## Method Summary
The paper presents a comprehensive approach to quantify and mitigate hallucinations in low-parameter open-source LLMs. HALO CHECK estimates hallucination severity by sampling multiple responses to a prompt and computing entailment-based consistency scores. Knowledge injection fine-tunes the model with factual entity summaries and triplets from Wikidata, using a TRUE_FACT token to guide factual recall. A teacher-student approach leverages GPT-4 to generate accurate answers when HALO CHECK detects inconsistencies, reducing computational cost through selective intervention. The method is evaluated on a domain-specific NBA QA dataset, demonstrating improved factual consistency and reduced hallucinations compared to baseline models.

## Key Results
- HALO CHECK achieves better correlation with human annotations than existing metrics for hallucination severity estimation.
- Knowledge injection via Wikidata summaries and triplets improves domain-specific factual accuracy in BLOOM 7B.
- Selective teacher intervention using GPT-4 guided by HALO CHECK reduces hallucination frequency while limiting computational cost.
- Models with knowledge injection achieve consistency scores near 0.5, significantly outperforming baseline models with negative scores.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** HALO CHECK improves hallucination severity estimation by leveraging entailment-based sentence-level agreement instead of token overlap metrics.
- **Mechanism:** The framework samples multiple model responses to a given prompt, computes pairwise entailment scores using SummaCzs, and averages them to yield a consistency score μ in [-1,1]. This approach captures nuanced semantic agreement beyond surface similarity.
- **Core assumption:** Sentence-level entailment can reliably detect contradictions in model responses, even when lexical overlap is high.
- **Evidence anchors:**
  - [abstract] "Our approach aligns with other similar sampling-based BlackBox metrics (Manakul et al., 2023b), but also addresses their limitations in accurately estimating the severity of hallucinations."
  - [section 3] "Inspired by this hypothesis, we propose HALO CHECK, a lightweight BlackBox knowledge-free framework for evaluating hallucination severity in LLMs."
  - [corpus] Weak signal: corpus includes papers on ontology and categorization of hallucinations, supporting the conceptual framing but not the technical approach.
- **Break condition:** If entailment model misclassifies semantic contradictions as entailment, or if sampling variance dominates the mean score.

### Mechanism 2
- **Claim:** Knowledge injection through entity summaries and triplets improves domain-specific factual accuracy in small LLMs.
- **Mechanism:** Fine-tuning BLOOM 7B with factual knowledge from Wikidata (summaries or triplets) prepended with a TRUE_FACT token guides the model to recall and apply accurate domain knowledge during inference.
- **Core assumption:** Pre-trained decoder-only models can retain injected factual knowledge without catastrophic forgetting when interleaved with downstream task data.
- **Evidence anchors:**
  - [section 3.1] "We explore two avenues for knowledge injection: entity summaries and entity triplets (as shown in Figure 1 b.), sourced from the WikiData knowledge base."
  - [section 3.2] "This approach aims to enhance the base model's understanding and knowledge from the outset, reducing the reliance on constant external knowledge queries during inference."
  - [corpus] Weak signal: related work on knowledge infusion but not directly on decoder-only LLM fine-tuning.
- **Break condition:** If the model overfits to injected knowledge, or if factual content is too sparse relative to general pre-training data.

### Mechanism 3
- **Claim:** Selective teacher intervention using GPT-4 guided by HALO CHECK reduces hallucination frequency while limiting computational cost.
- **Mechanism:** When HALO CHECK score < threshold, the teacher model generates a detailed answer (optionally with auto-CoT) that is prepended to the student prompt, steering the student toward more consistent and accurate responses.
- **Core assumption:** The student model will align its output toward the teacher's response when it is contextually prominent, and HALO CHECK accurately flags problematic responses.
- **Evidence anchors:**
  - [section 4.2] "We utilize HALO CHECK to trigger teacher answers only when inconsistencies are detected in the student's responses."
  - [section 6.3] "Models benefiting from knowledge injection consistently outperform SFT-only models across hallucination thresholds."
  - [corpus] Weak signal: related work on teacher-student approaches but not specifically on selective intervention based on consistency metrics.
- **Break condition:** If teacher answers themselves are incorrect, or if HALO CHECK fails to detect hallucinations, leading to misattribution.

## Foundational Learning

- **Concept:** Sentence-level entailment (NLI) models like SummaCzs
  - **Why needed here:** To quantify semantic consistency across multiple sampled responses beyond token overlap.
  - **Quick check question:** What does a SummaCzs score of -0.5 between two responses indicate?
- **Concept:** Supervised fine-tuning with knowledge injection
  - **Why needed here:** To augment the knowledge base of small LLMs without instruction tuning, enabling factual correctness in domain-specific tasks.
  - **Quick check question:** How does the TRUE_FACT token influence the model during inference?
- **Concept:** Teacher-student knowledge distillation with selective intervention
  - **Why needed here:** To leverage stronger models to correct hallucinations in weaker models while controlling computational overhead.
  - **Quick check question:** What is the role of HALO CHECK in deciding when to trigger teacher intervention?

## Architecture Onboarding

- **Component map:** NBA QA dataset generator -> BLOOM 7B model -> HALO CHECK module (sampling engine + entailment scorer) -> consistency thresholder -> GPT-4 teacher -> student prompt builder
- **Critical path:** Sample → Entailment scoring → Consistency decision → Teacher trigger → Student generation
- **Design tradeoffs:**
  - Sampling n=5 balances noise reduction and runtime.
  - Using entailment over BERTScore/N-gram improves nuance but requires a pretrained NLI model.
  - Selective teacher use reduces cost but risks missing some hallucinations if thresholds are too high.
- **Failure signatures:**
  - HALO CHECK scores near zero or negative consistently → poor semantic consistency in model outputs.
  - Teacher intervention never triggered → threshold too high or HALO CHECK insensitive.
  - Low correlation with human annotations → metric not aligned with human judgment.
- **First 3 experiments:**
  1. Run HALO CHECK on baseline BLOOM7B QA set; record μ distribution.
  2. Inject knowledge via summaries; compare μ before/after; assess retention.
  3. Add teacher intervention at threshold=0.2; measure reduction in teacher calls vs. μ improvement.

## Open Questions the Paper Calls Out

- **Question:** How does HALO CHECK performance compare across different low-parameter open-source language models beyond BLOOM 7B?
  - **Basis in paper:** [inferred] The paper mentions that BLOOM 7B is a representative of weaker open-source LLMs, but does not test other models.
  - **Why unresolved:** The paper only focuses on BLOOM 7B and does not extend experiments to other low-parameter models like LLaMA or OPT.
  - **What evidence would resolve it:** Conducting the same experiments with other low-parameter models and comparing their HALO CHECK scores would provide insight into generalizability.

- **Question:** Can knowledge injection training be effectively applied to multiple domains simultaneously, or does it degrade performance compared to domain-specific training?
  - **Basis in paper:** [explicit] The authors mention that they focus on the NBA domain to simplify training and evaluation, with plans to extend to multiple domains in future work.
  - **Why unresolved:** The paper only evaluates knowledge injection in a single domain (NBA) and does not test multi-domain scenarios.
  - **What evidence would resolve it:** Training models with knowledge injection across diverse domains and comparing performance to single-domain models would determine if multi-domain training is feasible.

- **Question:** How does the effectiveness of teacher-student approaches vary with the choice of teacher model (e.g., GPT-4 vs. other strong models)?
  - **Basis in paper:** [explicit] The paper uses GPT-4 as the teacher model but acknowledges that stronger open-source models are available and could be explored in future work.
  - **Why unresolved:** The study only uses GPT-4 and does not compare the impact of different teacher models on the student model's performance.
  - **What evidence would resolve it:** Repeating the teacher-student experiments with other strong models (e.g., GPT-3.5, Claude, or open-source alternatives) and analyzing differences in student model improvement would clarify the impact of teacher choice.

## Limitations
- The narrow scope of the domain-specific dataset (NBA QA) may not generalize to broader or more complex domains.
- The reliance on entailment-based scoring assumes that semantic consistency always equates to factual accuracy, which may not hold in all cases.
- The knowledge injection approach assumes that Wikidata is sufficiently comprehensive for the task, which may not be true for niche or rapidly evolving topics.

## Confidence
- **High confidence:** The effectiveness of HALO CHECK as a hallucination estimation metric, given its superior correlation with human annotations compared to existing methods.
- **Medium confidence:** The knowledge injection approach for improving factual accuracy, as it shows measurable gains but may not generalize beyond the tested domain.
- **Low confidence:** The scalability of the selective teacher intervention approach, as it has not been tested on larger models or more diverse datasets.

## Next Checks
1. **Generalization Test:** Apply HALO CHECK and knowledge injection to a different domain (e.g., medical or legal QA) to assess robustness and scalability.
2. **Threshold Sensitivity Analysis:** Experiment with varying HALO CHECK thresholds for teacher intervention to optimize the balance between computational cost and hallucination reduction.
3. **Long-Term Retention Study:** Evaluate the persistence of injected knowledge after extended fine-tuning or inference to ensure the model does not forget factual information over time.