---
ver: rpa2
title: A New Perspective on Evaluation Methods for Explainable Artificial Intelligence
  (XAI)
arxiv_id: '2307.14246'
source_url: https://arxiv.org/abs/2307.14246
tags:
- explainability
- information
- understanding
- desiderata
- proceedings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new perspective for classifying evaluation
  methods (EMs) for Explainable Artificial Intelligence (XAI). While existing classifications
  focus on properties of the EMs themselves (e.g., heuristic-based, human-centered,
  application-grounded), the authors argue that it is also important to classify EMs
  based on the step in the XAI process they target.
---

# A New Perspective on Evaluation Methods for Explainable Artificial Intelligence (XAI)

## Quick Facts
- arXiv ID: 2307.14246
- Source URL: https://arxiv.org/abs/2307.14246
- Reference count: 40
- Primary result: Proposes a new classification of XAI evaluation methods based on the process step they target rather than properties of the methods themselves.

## Executive Summary
This paper introduces a novel perspective for classifying evaluation methods (EMs) in Explainable Artificial Intelligence (XAI). While existing classifications focus on properties of EMs themselves (e.g., heuristic-based, human-centered, application-grounded), the authors argue that it's equally important to classify EMs based on the step in the XAI process they target. Drawing on models that describe how explainability approaches aim to satisfy societal desiderata, the authors propose three categories: explanatory information EMs, understanding EMs, and desiderata EMs. The paper provides examples of EMs in each category and discusses their advantages and disadvantages, arguing that combining this new perspective with existing classifications will enable more comprehensive assessment of explainability approaches.

## Method Summary
The authors propose a new classification system for XAI evaluation methods by drawing on existing process models that describe how explainability approaches aim to satisfy societal desiderata. They identify three main stages in this process: producing explanatory information, facilitating understanding, and achieving desiderata satisfaction. Based on these stages, they propose three categories of evaluation methods: explanatory information EMs (assessing the quality and comprehensibility of information provided), understanding EMs (evaluating whether this information actually facilitates understanding), and desiderata EMs (measuring whether the approach achieves desired outcomes like trust or performance). The authors illustrate this classification with examples and discuss how it complements existing classifications based on EM properties.

## Key Results
- The proposed classification distinguishes EMs based on the XAI process step they target: explanatory information, understanding, and desiderata
- Each category of EMs has distinct advantages and disadvantages that complement each other
- The classification helps identify potential gaps in research on evaluation methods for explainability approaches
- Different types of EMs are needed to comprehensively assess whether explainability approaches achieve their intended outcomes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Classifying evaluation methods by the XAI process step they target provides a complementary perspective to existing classifications.
- Mechanism: By aligning EMs with the causal chain from explanatory information to understanding to desiderata satisfaction, researchers can more comprehensively assess whether explainability approaches actually achieve their intended outcomes.
- Core assumption: Explainability approaches aim to satisfy societal desiderata through a sequential process where explanatory information facilitates understanding, which in turn affects desiderata satisfaction.
- Evidence anchors:
  - [abstract] "Drawing on models that describe how explainability approaches aim to satisfy societal desiderata, the authors propose three categories of EMs: explanatory information EMs, understanding EMs, and desiderata EMs."
  - [section III] "Since these models describe the process through which explainability approaches aim to satisfy stakeholders' desiderata, they can be used to derive implications for the evaluation of such approaches."
- Break condition: If explainability approaches do not follow the assumed sequential process (explanatory information → understanding → desiderata satisfaction), this classification becomes less useful.

### Mechanism 2
- Claim: Different types of EMs have distinct advantages and disadvantages that complement each other.
- Mechanism: Explanatory information EMs assess the quality and comprehensibility of information provided, understanding EMs evaluate whether this information actually facilitates understanding, and desiderata EMs measure whether the approach achieves desired outcomes like trust or performance.
- Core assumption: Each type of EM captures a different stage in the explainability process, and using only one type provides an incomplete assessment.
- Evidence anchors:
  - [section III-A] "Explanatory information EMs acknowledge that it is vital to assess whether explainability approaches produce information that is comprehensible and of high quality."
  - [section III-B] "Understanding EMs acknowledge that it is crucial to assess whether explainability approaches have facilitated a person's understanding."
  - [section III-C] "Evaluating desiderata satisfaction is valuable because it directly assesses whether the goals that people have when using explainability approaches are achieved."
- Break condition: If one type of EM consistently provides sufficient information for evaluating explainability approaches, the complementary approach may be unnecessary.

### Mechanism 3
- Claim: The proposed classification helps identify gaps in research on evaluation methods for explainability approaches.
- Mechanism: By categorizing existing EMs into the six combinations of previous classifications (objective/human-centered) and the new classification (explanatory information/understanding/desiderata), researchers can identify underrepresented categories that need more attention.
- Core assumption: A comprehensive evaluation of explainability approaches requires EMs from all six categories.
- Evidence anchors:
  - [section IV] "If an extensive literature analysis were to confirm our initial findings that there are no or only a few objective EMs for desiderata, our classification would have made it possible to identify gaps in research."
- Break condition: If future research shows that all six categories are equally represented, the gap-identification mechanism becomes less valuable.

## Foundational Learning

- Concept: XAI process models (Hofmann et al. [16], Langer et al. [2])
  - Why needed here: These models provide the theoretical foundation for classifying EMs by the process step they target rather than by their properties.
  - Quick check question: What are the three main components of the XAI process models that the authors use to classify EMs?

- Concept: Fidelity and completeness of explanatory information
  - Why needed here: These are key criteria for evaluating the quality of explanatory information in explanatory information EMs.
  - Quick check question: How do fidelity and completeness differ in the context of evaluating explanatory information?

- Concept: Human-centered vs. objective evaluation methods
  - Why needed here: The paper distinguishes between these two types of EMs within each category, and understanding this distinction is crucial for proper classification.
  - Quick check question: What is the main difference between human-centered and objective evaluation methods in the context of XAI?

## Architecture Onboarding

- Component map:
  - Classification system: Previous classifications (Doshi-Velez and Kim [9], Bibal and Frénay [12], Vilone and Longo [6]) + new classification (explanatory information/understanding/desiderata)
  - Evaluation method categories: Objective vs. human-centered within each of the three new categories
  - Evidence sources: Academic papers, empirical studies, theoretical models

- Critical path:
  1. Understand the XAI process models that inform the new classification
  2. Review existing classifications of EMs
  3. Categorize existing EMs according to the combined classification system
  4. Identify gaps and underrepresented categories
  5. Use the classification to guide selection of EMs for specific contexts

- Design tradeoffs:
  - Breadth vs. depth: The new classification provides a broader perspective but may sacrifice some depth in understanding individual EMs
  - Theoretical vs. practical: The classification is based on theoretical models of the XAI process, which may not always align with practical evaluation needs
  - Complexity vs. usability: The combined classification system is more complex but potentially more useful for comprehensive evaluation

- Failure signatures:
  - Over-reliance on a single type of EM (e.g., only explanatory information EMs) leading to incomplete assessment
  - Misalignment between the theoretical XAI process models and the actual implementation of explainability approaches
  - Difficulty in categorizing EMs that span multiple categories in the new classification

- First 3 experiments:
  1. Categorize a set of existing EMs (e.g., from Table I) according to the combined classification system and analyze the distribution across categories
  2. Apply the classification to evaluate a specific explainability approach, documenting the insights gained from using multiple types of EMs
  3. Conduct a literature review to identify gaps in research on EMs for each of the six categories in the combined classification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can objective evaluation methods for desiderata be developed, given that current desiderata EMs are exclusively human-centered?
- Basis in paper: [explicit] The authors note that while there are various human-centered desiderata EMs, they could not identify any objective EMs for desiderata.
- Why unresolved: Developing objective methods to measure desiderata like trust, acceptance, or usability is challenging because these concepts are inherently subjective and context-dependent.
- What evidence would resolve it: A study demonstrating a successful objective method for evaluating at least one desideratum, with clear metrics and validation against human judgments.

### Open Question 2
- Question: What is the optimal combination of explanatory information, understanding, and desiderata EMs to comprehensively evaluate explainability approaches in different contexts?
- Basis in paper: [explicit] The authors propose that a combination of different EMs is needed to comprehensively assess explainability approaches, but do not specify the optimal combination for various contexts.
- Why unresolved: The effectiveness of different EM combinations likely varies based on factors such as the specific application, user expertise, and societal context, which have not been systematically studied.
- What evidence would resolve it: Empirical studies comparing different combinations of EMs across various contexts, demonstrating which combinations provide the most comprehensive and accurate evaluation of explainability approaches.

### Open Question 3
- Question: How can the reliability of understanding EMs be improved, given the limitations of current methods like the size of the model or self-reported understanding?
- Basis in paper: [inferred] The authors discuss limitations of current understanding EMs, such as the unreliable correlation between model size and understandability, and the potential for illusion of understanding in self-reported measures.
- Why unresolved: Understanding is a complex cognitive process that is difficult to measure objectively, and current methods rely on imperfect proxies or subjective reports.
- What evidence would resolve it: Development and validation of new understanding EMs that more accurately capture actual comprehension of system-related aspects, possibly through advanced cognitive assessment techniques or longitudinal studies of user learning and adaptation.

## Limitations
- The classification is largely conceptual and lacks empirical validation to demonstrate its practical utility
- Some evaluation methods may span multiple categories, creating potential ambiguity in classification
- The framework depends heavily on the validity of XAI process models from Hofmann et al. and Langer et al., which are referenced but not deeply validated within this work

## Confidence
- **High confidence**: The classification provides a logically coherent framework for thinking about evaluation methods by process step. The three categories (explanatory information, understanding, desiderata) represent distinct and meaningful stages in the explainability process.
- **Medium confidence**: The claim that this perspective complements existing classifications is well-reasoned but not empirically demonstrated. The assertion that combining both perspectives enables more comprehensive assessment is plausible but untested.
- **Medium confidence**: The identification of research gaps based on this classification is promising but relies on future literature analysis to confirm whether underrepresented categories actually exist.

## Next Checks
1. Conduct a systematic literature review categorizing existing evaluation methods according to both the new classification (explanatory information/understanding/desiderata) and existing classifications (objective/human-centered), then analyze the distribution across all six resulting categories to identify actual gaps.

2. Apply the classification framework to evaluate a specific XAI approach using evaluation methods from multiple categories, documenting whether this multi-perspective approach reveals insights that would be missed using only traditional classifications.

3. Design and execute a small-scale empirical study where participants use different types of evaluation methods (one from each category) to assess the same explainability approach, measuring whether the classification helps identify distinct aspects of the approach's effectiveness.