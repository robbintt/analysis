---
ver: rpa2
title: Towards Reliable Uncertainty Quantification via Deep Ensembles in Multi-output
  Regression Task
arxiv_id: '2303.16210'
source_url: https://arxiv.org/abs/2303.16210
tags:
- uncertainty
- calibration
- regression
- bayesian
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the application of deep ensembles (DE)
  for uncertainty quantification in multi-output regression tasks, specifically predicting
  aerodynamic coefficients of a missile configuration. The study reveals an increasing
  trend of underconfidence in uncertainty estimates as the number of neural networks
  in the ensemble increases.
---

# Towards Reliable Uncertainty Quantification via Deep Ensembles in Multi-output Regression Task

## Quick Facts
- arXiv ID: 2303.16210
- Source URL: https://arxiv.org/abs/2303.16210
- Authors: 
- Reference count: 9
- This paper investigates deep ensembles for uncertainty quantification in multi-output regression, proposing a post-hoc calibration method that improves reliability and outperforms Gaussian process regression in aerodynamic coefficient prediction.

## Executive Summary
This study explores the application of deep ensembles (DE) for uncertainty quantification (UQ) in multi-output regression tasks, focusing on predicting aerodynamic coefficients of a missile configuration. The research reveals that DE models tend to become increasingly underconfident as the number of neural networks in the ensemble grows. To address this limitation, the authors propose a post-hoc calibration method using standard deviation scaling, which effectively corrects the underconfidence issue. The calibrated DE models demonstrate superior performance compared to Gaussian process regression, showing improvements in regression accuracy, UQ reliability, and training efficiency. The study also highlights the importance of calibration in Bayesian optimization, demonstrating that uncalibrated DE models may lead to unintended exploratory behavior.

## Method Summary
The paper investigates deep ensembles for uncertainty quantification in multi-output regression, specifically predicting six aerodynamic coefficients under varying flow conditions. The methodology involves training ensembles of auxiliary neural networks, each outputting a Gaussian distribution (mean and variance), and combining them to estimate predictive uncertainty. To address underconfidence issues observed with larger ensembles, a post-hoc calibration method using standard deviation scaling is proposed and validated. The approach is compared against Gaussian process regression on a missile aerodynamics dataset, with performance evaluated using regression accuracy metrics (NLL, RMSE), UQ reliability metrics (AUCE, ENCE), and training efficiency measures.

## Key Results
- DE models with calibration outperform GPR in regression accuracy (-150% NLL & -67% RMSE) and UQ reliability (-32% AUCE & -82% ENCE)
- Training efficiency improved by 86% compared to GPR
- Calibration is critical for Bayesian optimization, as uncalibrated DE models lead to unintended exploratory behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep ensembles quantify both aleatory and epistemic uncertainty in regression tasks by leveraging the variance across multiple NNs trained on the same data.
- Mechanism: Each NN outputs a Gaussian distribution (mean, variance). The ensemble's predictive variance is decomposed into the average individual variance (aleatory) plus the variance of the means (epistemic).
- Core assumption: NNs are sufficiently diverse due to random initialization and mini-batch shuffling, so that ensemble variance captures epistemic uncertainty.
- Evidence anchors:
  - [abstract] "DE models, after calibration, outperform Gaussian process regression in terms of regression accuracy (-150% NLL & -67% RMSE), reliability of estimated uncertainty (-32% AUCE & -82% ENCE)"
  - [section 2.2] "The predictive uncertainty can be decomposed into aleatory and epistemic uncertainty; see Scalia et al. (2020) and Hu et al. (2021) for more details."
  - [corpus] No direct mention of epistemic/aleatory separation in corpus papers; only general uncertainty quantification.
- Break condition: If NNs are too similar (e.g., identical architectures and training regimes), ensemble variance underestimates epistemic uncertainty.

### Mechanism 2
- Claim: STD scaling post-hoc calibration corrects underconfidence by scaling down the predicted variance without changing the mean predictions.
- Mechanism: Multiply predicted standard deviation by a scalar s < 1 (optimized to minimize NLL on a held-out calibration set). This reduces overestimated uncertainty.
- Core assumption: The underconfidence is systematic and can be captured by a single scalar per output dimension.
- Evidence anchors:
  - [abstract] "a post-hoc calibration method using standard deviation scaling is proposed and validated."
  - [section 4.1] "STD scaling only needs to find the scalar parameter s, which is multiplied to the standard deviation originally estimated by DE model."
  - [corpus] No mention of STD scaling in corpus papers; weak evidence.
- Break condition: If underconfidence is not systematic or varies non-linearly across the input space, a single scalar per dimension is insufficient.

### Mechanism 3
- Claim: Calibration is critical for Bayesian optimization because uncalibrated uncertainty leads to unintended exploration behavior.
- Mechanism: Bayesian optimization uses predictive uncertainty to balance exploration vs exploitation. If uncertainty is overestimated (underconfident model), the acquisition function will favor exploration, leading to inefficient optimization.
- Core assumption: The acquisition function (e.g., expected improvement) directly uses the model's predictive variance.
- Evidence anchors:
  - [abstract] "uncalibrated DE models may lead to unintended exploratory behavior."
  - [section 4.2] "Bayesian optimization coupled with DE-bef discourages exploration of the variable δr, while DE-aft encourages exploration within δr."
  - [corpus] No direct evidence in corpus papers about Bayesian optimization impacts.
- Break condition: If the acquisition function is designed to be robust to uncertainty calibration errors, the impact may be less severe.

## Foundational Learning

- Concept: Gaussian Process Regression (GPR)
  - Why needed here: GPR is the baseline model for uncertainty quantification in engineering; understanding its strengths/weaknesses is essential for comparing DE performance.
  - Quick check question: What is the computational complexity of GPR with respect to dataset size n?
- Concept: Uncertainty Calibration Metrics (AUCE, ENCE)
  - Why needed here: These metrics quantify the reliability of uncertainty estimates; understanding them is crucial for evaluating DE models.
  - Quick check question: What is the ideal value for AUCE and ENCE, and what does it signify?
- Concept: Bayesian Optimization
  - Why needed here: The paper discusses the impact of DE calibration on Bayesian optimization; understanding the optimization process and acquisition functions is key.
  - Quick check question: What is the role of the acquisition function in Bayesian optimization, and how does it use predictive uncertainty?

## Architecture Onboarding

- Component map:
  - Data Preparation: Input parameters (Ma, φ, δp, δr, AoA) → Output (6 aerodynamic coefficients)
  - DE Model: Ensemble of auxiliary NNs (each outputs mean and variance) → Mixture of Gaussians → Predictive mean and variance
  - Calibration: STD scaling → Optimized scaling factors → Calibrated uncertainty
  - Evaluation: AUCE, ENCE metrics → Reliability plots → Bayesian optimization impact
- Critical path: Data → DE Training → Uncertainty Evaluation → STD Calibration → Improved UQ → Bayesian Optimization
- Design tradeoffs:
  - Number of NNs (M) vs. training time and UQ quality: More NNs → better regression accuracy but worse UQ (underconfidence).
  - Calibration dataset size vs. generalization: Larger calibration set → better scaling factor estimation.
- Failure signatures:
  - UQ quality degrades with increasing M without calibration.
  - Scaling factors optimized to values much less than 1 indicate severe underconfidence.
  - Bayesian optimization with uncalibrated DE shows unintended exploration patterns.
- First 3 experiments:
  1. Train DE-2 and GPR on the same dataset; compare NLL, RMSE, AUCE, ENCE.
  2. Apply STD calibration to DE-2; verify improvement in AUCE and ENCE.
  3. Run Bayesian optimization with uncalibrated vs. calibrated DE-2; compare exploration behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of deep ensembles with STD calibration compare to other uncertainty quantification methods beyond Gaussian process regression, such as Monte Carlo dropout or variational inference?
- Basis in paper: [explicit] The paper compares deep ensembles with STD calibration to Gaussian process regression but does not explore other UQ methods.
- Why unresolved: The study focuses on comparing DE with GPR, leaving a gap in understanding how DE with STD calibration stacks up against other UQ approaches.
- What evidence would resolve it: Conducting experiments comparing DE with STD calibration to other UQ methods like MC-dropout and variational inference on similar regression tasks.

### Open Question 2
- Question: How does the choice of the number of neural networks in the ensemble (M) affect the performance of deep ensembles in different types of regression tasks beyond the aerodynamic coefficient prediction problem?
- Basis in paper: [explicit] The paper investigates the effect of M on deep ensembles in a specific aerodynamic coefficient prediction task, showing an underconfidence trend as M increases.
- Why unresolved: The study is limited to one specific regression task, and it is unclear if the underconfidence trend with increasing M is generalizable to other types of regression problems.
- What evidence would resolve it: Applying deep ensembles with varying M values to a diverse set of regression tasks and analyzing the impact on both regression accuracy and uncertainty quality.

### Open Question 3
- Question: How does the STD calibration method perform when applied to other ensemble methods or regression models beyond deep ensembles?
- Basis in paper: [inferred] The paper proposes and validates the STD calibration method specifically for deep ensembles, but does not explore its applicability to other models or ensemble methods.
- Why unresolved: The study focuses on the effectiveness of STD calibration for deep ensembles, leaving open the question of its generalizability to other UQ approaches.
- What evidence would resolve it: Applying the STD calibration method to other ensemble methods or regression models capable of UQ, such as random forests or support vector regression, and evaluating its impact on their uncertainty quality.

## Limitations
- Calibration assumes simple linear scaling is sufficient, which may not hold for all regression tasks or data distributions
- Empirical evidence limited to specific missile aerodynamics problem, raising questions about generalizability to other domains
- Computational efficiency gains are relative to GPR, but absolute training time for large ensembles may still be prohibitive for real-time applications

## Confidence

- **High Confidence**: The underconfidence trend in DE models as ensemble size increases is well-supported by quantitative metrics (AUCE, ENCE) across multiple ensemble sizes. The post-hoc STD calibration method effectively addresses this issue, as evidenced by consistent improvements in UQ metrics. The computational efficiency advantage of DE over GPR is robust, given the explicit timing comparisons.
- **Medium Confidence**: The superiority of calibrated DE over GPR in terms of both regression accuracy and UQ reliability is well-demonstrated on the specific missile aerodynamics dataset. However, the extent to which these results generalize to other regression tasks or data distributions remains to be seen. The impact of calibration on Bayesian optimization is shown, but the single case study limits broader conclusions.
- **Low Confidence**: The assumption that a single scalar per output dimension is sufficient for calibration is not thoroughly validated. The study does not explore alternative calibration methods or investigate the impact of ensemble diversity on UQ quality. The choice of ensemble architecture (7 hidden layers, 128 nodes) may not be optimal for all problems.

## Next Checks
1. Apply the calibrated DE framework to a different regression task (e.g., medical imaging or financial forecasting) to assess generalizability. Compare UQ metrics and calibration performance against domain-specific baselines.
2. Replace STD scaling with alternative post-hoc calibration techniques (e.g., temperature scaling, isotonic regression) to determine if the underconfidence issue is specific to the current approach or a broader challenge for DE models.
3. Systematically vary the diversity of the ensemble (e.g., by changing architectures, training data subsets, or initialization strategies) and measure its impact on UQ quality before and after calibration. This would provide insights into the relationship between diversity and the need for calibration.