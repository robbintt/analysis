---
ver: rpa2
title: Multimodal deep learning for mapping forest dominant height by fusing GEDI
  with earth observation data
arxiv_id: '2311.11777'
source_url: https://arxiv.org/abs/2311.11777
tags:
- height
- forest
- data
- gedi
- sensing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a novel deep learning framework, MARSNet, for
  accurately mapping forest dominant height by integrating GEDI relative height (RH)
  metrics with multi-source remote sensing data. MARSNet employs separate encoders
  for each remote sensing modality (Sentinel-1, Sentinel-2, ALOS-2 PALSAR-2, and ancillary
  data) to extract multi-scale features, followed by a shared decoder to fuse features
  and estimate height.
---

# Multimodal deep learning for mapping forest dominant height by fusing GEDI with earth observation data

## Quick Facts
- arXiv ID: 2311.11777
- Source URL: https://arxiv.org/abs/2311.11777
- Reference count: 0
- Primary result: MARSNet achieves R²=0.62 and RMSE=2.82 m for forest dominant height estimation

## Executive Summary
This study introduces MARSNet, a novel deep learning framework that integrates GEDI relative height metrics with multi-source remote sensing data (Sentinel-1, Sentinel-2, ALOS-2 PALSAR-2, and ancillary datasets) to accurately map forest dominant height at 10m resolution. The framework employs separate encoders for each remote sensing modality to extract multi-scale features, followed by a shared decoder to fuse features and estimate height. MARSNet significantly outperforms random forest baselines, achieving R²=0.62 and RMSE=2.82 m on test data, and demonstrates strong performance when validated against independent field measurements (R²=0.58, RMSE=3.76 m).

## Method Summary
MARSNet uses four separate encoders (one per modality) with extended spatial and band reconstruction convolution (ESBConv) modules to extract features from Sentinel-1 SAR, Sentinel-2 optical, ALOS-2 PALSAR-2 L-band SAR, and ancillary data (DEM, slope, coordinates). The ESBConv modules reduce spatial and band redundancies before modal fusion through spatial attention weighting. A shared decoder then processes the fused features to predict forest dominant height. The model was trained on 6,222 GEDI footprints in Jilin, China, using a 8:1:1 train-validation-test split, and validated against 24 field plots.

## Key Results
- MARSNet achieves R²=0.62 and RMSE=2.82 m on test data, outperforming random forest (R²=0.55, RMSE=3.05 m)
- The model effectively mitigates bias across the height spectrum, reducing overestimation in low height areas and underestimation in high canopy areas
- Wall-to-wall 10m resolution forest dominant height maps generated for Jilin province show strong agreement with independent field measurements (R²=0.58, RMSE=3.76 m)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: MARSNet outperforms random forest because it preserves spatial structure and modality-specific feature patterns instead of aggregating pixels.
- **Mechanism**: Separate encoders prevent interference across heterogeneous remote sensing modalities (SAR, optical, LiDAR-derived labels), while the ESBConv modules remove spatial and band redundancies before fusion, allowing cleaner modality-specific feature extraction.
- **Core assumption**: Spatial context within 64x64 patches contains discriminative information about canopy structure that pixel aggregation destroys.
- **Evidence anchors**:
  - [abstract]: "Using individual encoders for each remote sensing imagery avoids interference across modalities and extracts distinct representations."
  - [section]: "These methods rely on statistical relationships between predictor variables rather than their explicit spatial context."
  - [corpus]: No direct match found; claim relies on paper text.

### Mechanism 2
- **Claim**: The ESBConv module improves performance by decomposing redundant spatial and band information before fusion.
- **Mechanism**: SRU separates informative from non-informative spatial features using group normalization and selective weighting, then reconstructs a refined map. BRU splits bands, applies lightweight transformations, and fuses them with SKNet attention, reducing noise while preserving discriminative patterns.
- **Core assumption**: Remote sensing data contains substantial redundancy in both spatial and spectral dimensions that can be removed without losing critical information.
- **Evidence anchors**:
  - [abstract]: "We reduced the prevalent spatial and band redundancies in each remote sensing data by incorporating the extended spatial and band reconstruction convolution (ESBConv) modules in the encoders."
  - [section]: "By employing the SRU and BRU, one can effectively eliminate spatial and band redundancy in the data, extracting more representative modality features."
  - [corpus]: No direct match found; inference based on module description.

### Mechanism 3
- **Claim**: MARSNet mitigates bias across the height spectrum by learning more sensitive representations in both low and high canopy regions.
- **Mechanism**: The deep architecture with spatial attention fusion allows the decoder to focus on critical modal regions, enabling better modeling of extremes compared to random forest's uniform decision boundaries.
- **Core assumption**: Forest height distributions are skewed, with underrepresented extreme values that standard RF models fail to capture well.
- **Evidence anchors**:
  - [abstract]: "MARSNet effectively mitigated the tendency to overestimate in low height areas and underestimate in high canopy areas."
  - [section]: "Frequency distribution histograms reveal distinct differences in the distribution of estimated heights between the models...MARSNet displays markedly higher frequencies at the extremes."
  - [corpus]: No direct match found; claim derived from results section.

## Foundational Learning

- **Concept**: Multimodal data fusion in remote sensing
  - **Why needed here**: GEDI provides sparse but accurate vertical structure, while SAR and optical sensors offer continuous coverage but indirect height information; fusing them leverages complementary strengths.
  - **Quick check question**: What unique structural information does L-band PALSAR-2 provide that C-band Sentinel-1 cannot?

- **Concept**: Deep learning for structured regression
  - **Why needed here**: Forest height estimation is a continuous target problem where spatial context and non-linear relationships matter; CNNs can capture these better than traditional RF.
  - **Quick check question**: How does a 64x64 patch size balance spatial context versus computational load for height prediction?

- **Concept**: Encoder-decoder architecture with attention
  - **Why needed here**: Separate encoders preserve modality-specific features, and attention-based fusion dynamically weights contributions, improving robustness to missing or noisy inputs.
  - **Quick check question**: Why does concatenating features before fusion risk diluting modality-specific signals?

## Architecture Onboarding

- **Component map**: Input (Sentinel-1, Sentinel-2, ALOS-2, ancillary) -> Four separate encoders (64→128→256→512) with ESBConv -> Modal fusion with spatial attention -> Shared decoder -> Output (height value)

- **Critical path**: Input → modality-specific encoder → ESBConv → modal fusion → decoder → height prediction

- **Design tradeoffs**: Separate encoders increase parameters and memory vs. shared encoder; ESBConv adds computation but reduces redundancy; attention fusion is more flexible than simple concatenation but slower.

- **Failure signatures**:
  - Low R², high RMSE → likely modality misalignment or insufficient training data
  - Overestimation at low heights → decoder underweights SAR contributions or saturation in optical bands
  - Underestimation at high heights → LiDAR label conversion error or encoder dropout too aggressive

- **First 3 experiments**:
  1. Replace separate encoders with shared encoder, keep ESBConv, measure drop in R².
  2. Remove ESBConv modules, keep separate encoders, measure impact on spatial and band redundancy handling.
  3. Swap modal fusion with simple concatenation, observe effect on attention-guided weighting and final accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do seasonal variations in canopy structure affect the accuracy of GEDI-derived forest height estimates when combined with multi-source remote sensing data?
- Basis in paper: [inferred] from the discussion section noting the potential impact of leaf-on vs leaf-off conditions on GEDI data accuracy, but acknowledging limited research on this topic
- Why unresolved: The paper acknowledges this could be important but did not investigate seasonal effects systematically, instead using annual composites
- What evidence would resolve it: Comparative analysis using GEDI data from different seasons paired with corresponding seasonal remote sensing imagery to quantify accuracy differences

### Open Question 2
- Question: How significant is the impact of GEDI geolocation uncertainty on forest height estimation accuracy, and can it be mitigated through integration with other data sources?
- Basis in paper: [explicit] from the discussion noting average 16.02m offset between field plots and GEDI Version 2 footprints, with expected geolocation error of 10.3m
- Why unresolved: The paper acknowledges geolocation uncertainty but does not quantify its specific impact on height estimation accuracy or explore mitigation strategies
- What evidence would resolve it: Integration of ALS data to refine GEDI geolocation and systematic assessment of height estimation accuracy improvements

### Open Question 3
- Question: What is the relationship between GEDI RH metrics and forest structure characteristics beyond dominant height, such as canopy density or vertical heterogeneity?
- Basis in paper: [explicit] from the analysis showing strong correlation between RH98 and mean height of top 10 trees, with weaker relationships for lower RH values
- Why unresolved: The paper focused on dominant height relationships but did not explore whether RH metrics correlate with other forest structural parameters
- What evidence would resolve it: Comprehensive analysis correlating various RH metrics with multiple forest structural parameters measured in field plots or derived from high-resolution ALS data

## Limitations
- Exact ESBConv implementation details (SRU/BRU configurations, group sizes) are not fully specified
- Modal fusion strategy using spatial attention weights lacks detailed algorithmic description
- Field validation was conducted on only 24 plots, limiting generalizability assessment
- Study focuses on a single province in China, raising questions about model transferability to other biomes

## Confidence
**High confidence**: MARSNet outperforms random forest baseline (R² 0.62 vs 0.55, RMSE 2.82 m vs 3.05 m); Separate encoders prevent modality interference; ESBConv modules reduce spatial and band redundancy

**Medium confidence**: MARSNet mitigates height estimation bias across the spectrum; GEDI RH98 is an effective predictor of field-measured dominant height; Deep learning preserves spatial context better than pixel-aggregation methods

**Low confidence**: Specific architectural choices (kernel sizes, attention mechanisms) are optimal; Model performance would generalize to other regions without retraining; ESBConv computational overhead is justified by performance gains

## Next Checks
1. **Architecture ablation study**: Remove ESBConv modules while keeping separate encoders to quantify their contribution to performance gains (expect R² drop of 0.02-0.05).

2. **Transferability test**: Apply trained MARSNet to a geographically distinct region (e.g., boreal forests in Canada) without retraining to assess generalization limits.

3. **Field validation expansion**: Collect and validate on 50-100 additional field plots across different canopy density classes to better characterize model performance across the full height distribution.