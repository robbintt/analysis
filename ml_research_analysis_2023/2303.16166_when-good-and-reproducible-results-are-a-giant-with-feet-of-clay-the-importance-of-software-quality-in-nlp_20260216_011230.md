---
ver: rpa2
title: 'When Good and Reproducible Results are a Giant with Feet of Clay: The Importance
  of Software Quality in NLP'
arxiv_id: '2303.16166'
source_url: https://arxiv.org/abs/2303.16166
tags:
- code
- bugs
- speech
- pages
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper shows that peer review currently evaluates NLP research\
  \ based on results alone, neglecting the correctness of the underlying code. Through\
  \ a case study of open-source Conformer implementations, the authors find that all\
  \ studied codebases contain bugs, yet still produce good, reproducible results\u2014\
  leading to incorrect conclusions when new techniques are added."
---

# When Good and Reproducible Results are a Giant with Feet of Clay: The Importance of Software Quality in NLP

## Quick Facts
- arXiv ID: 2303.16166
- Source URL: https://arxiv.org/abs/2303.16166
- Reference count: 37
- All studied open-source Conformer implementations contain bugs that produce good, reproducible results despite incorrect code

## Executive Summary
This paper reveals a critical gap in NLP research evaluation: peer review focuses on results and reproducibility but neglects code correctness. Through systematic analysis of open-source Conformer implementations for speech processing, the authors demonstrate that all studied codebases contain bugs that affect padding handling, yet still produce competitive results. These bugs remain undetected because standard evaluation metrics cannot expose them. The paper shows that building new techniques on buggy foundations leads to false scientific conclusions, releases a bug-free implementation, and proposes a checklist to improve code quality assessment in NLP research.

## Method Summary
The study analyzes five open-source Conformer implementations for speech recognition and speech translation tasks using the MuST-C v1.0 corpus. The authors identify three distinct bugs affecting convolution modules, subsampling layers, and positional encodings through systematic testing across different batch sizes. They evaluate model performance using WER for ASR and SacreBLEU for ST, comparing correct versus buggy implementations. The study also examines the impact of adding CTC compression to both correct and buggy codebases to assess how implementation errors affect conclusions about new techniques.

## Key Results
- All five studied Conformer implementations contain bugs that affect padding handling but still produce good, reproducible results
- Performance differences between correct and buggy implementations vary with batch size, exposing the bugs
- CTC compression appears beneficial on buggy implementations but provides minimal improvement on correct code
- The study releases a bug-free Conformer implementation and proposes a checklist for code correctness evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Code correctness is not currently evaluated in peer review despite its importance to experimental validity.
- Mechanism: Peer review focuses on concept novelty, implementation accessibility, and outcome significance, but omits explicit assessment of whether the code correctly implements the stated concept.
- Core assumption: The perceived quality of results is taken as sufficient evidence of code correctness.
- Evidence anchors: [abstract] "code correctness is often presumed only on the basis of the perceived quality of the results"; [section] Table 1 shows reproducibility is scored in most venues, but correctness is rarely mentioned and often conflated with reproducibility.

### Mechanism 2
- Claim: Bugs in widely-used implementations can remain hidden when only final results are examined.
- Mechanism: Bugs affecting padding handling do not prevent achieving competitive, reproducible results, making them invisible through standard evaluation metrics.
- Core assumption: Performance metrics alone cannot detect implementation errors that preserve output quality.
- Evidence anchors: [abstract] "presence of bugs does not prevent the achievement of good and reproducible results"; [section] Tables 3-4 show that despite bugs, ASR/ST performance remains competitive with state-of-the-art results.

### Mechanism 3
- Claim: Building new techniques on incorrect codebases can lead to false scientific conclusions.
- Mechanism: CTC compression appears to improve performance on buggy codebases but provides no real benefit on correct implementations, leading to misleading conclusions about technique effectiveness.
- Core assumption: Improvements observed on buggy implementations are incorrectly attributed to the new technique rather than the bugs.
- Evidence anchors: [abstract] "presence of bugs can lead to incorrect conclusions that potentially misguide future research"; [section] Tables 6-7 show CTC compression yields significant improvements on buggy code but minimal/no improvement on correct code.

## Foundational Learning

- Concept: Unit Testing
  - Why needed here: Only way to verify code behaves as expected under all conditions, including edge cases like varying batch sizes
  - Quick check question: How would you design a unit test to verify that padding doesn't affect model outputs?

- Concept: Continuous Integration
  - Why needed here: Ensures unit tests are automatically run on every code change, catching regressions early
  - Quick check question: What would be the CI workflow for this codebase to verify padding-independence?

- Concept: Code Review
  - Why needed here: Provides independent verification of implementation correctness and improves code quality
  - Quick check question: What specific aspects would you check during code review for the Conformer convolution module?

## Architecture Onboarding

- Component map: Input → Subsampling → Conformer Encoder (12 layers) → Transformer Decoder (6 layers) → Output
- Critical path: Input → Subsampling → Conformer Encoder (12 layers) → Transformer Decoder (6 layers) → Output; convolution module and positional encodings are critical for correctness
- Design tradeoffs: TF32 precision vs accuracy (TF32 faster but introduces numeric errors), batch size vs padding effects, model complexity vs training time
- Failure signatures: Performance variations with batch size, incorrect handling of padding in convolution and positional encoding operations, inconsistent results across different hardware configurations
- First 3 experiments:
  1. Test model output consistency across different batch sizes (1, 10, 100) to check padding independence
  2. Compare results with TF32 enabled vs disabled to check for numeric instability
  3. Validate convolution module output shape and values with and without padding present

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the positional encodings bug be fixed without introducing significant computational overhead?
- Basis in paper: [explicit] The authors note that their fix for the positional encodings bug introduces a significant overhead, increasing training time, and express confidence that the community will find an optimized solution.
- Why unresolved: The paper presents a correct but computationally expensive fix for the positional encodings bug. The authors acknowledge this limitation but do not provide a detailed solution or analysis of the overhead's impact on different tasks or model sizes.
- What evidence would resolve it: A detailed analysis comparing the computational overhead of the current fix with potential optimized solutions, including benchmarks on training time and memory usage across different tasks and model sizes.

### Open Question 2
- Question: To what extent do the identified bugs affect other speech-related tasks beyond ASR and ST, such as speech emotion recognition or speech separation?
- Basis in paper: [inferred] The authors acknowledge that while the effects of the bugs might be found in other speech-related tasks, they did not cover them in this paper and suggest extending the analysis to other research areas as a natural next step.
- Why unresolved: The study focuses on ASR and ST tasks, leaving the impact of the bugs on other speech-related tasks unexplored. The authors suggest this as a potential extension but do not provide any preliminary analysis or insights.
- What evidence would resolve it: Experiments evaluating the impact of the identified bugs on a range of speech-related tasks, including speech emotion recognition, speech separation, and spoken language understanding, comparing results with and without the bugs.

### Open Question 3
- Question: How does the presence of bugs in widely-used implementations affect the reproducibility and reliability of research findings in the broader NLP community?
- Basis in paper: [explicit] The authors demonstrate that the presence of bugs can lead to incorrect conclusions when new techniques are added to the architecture and that good, reproducible results do not guarantee code correctness.
- Why unresolved: While the paper provides a case study on Conformer implementations, it does not quantify the broader impact of such bugs on the NLP community's research findings or explore potential systematic issues in code review and testing practices.
- What evidence would resolve it: A large-scale survey or analysis of widely-used NLP implementations, assessing the prevalence of bugs and their impact on published research findings, along with recommendations for improving code review and testing practices in the community.

## Limitations
- Study focuses exclusively on Conformer implementations in speech processing, limiting generalizability to other architectures and tasks
- Analysis relies on codebases that chose to open-source their implementations, potentially excluding buggy but non-public research
- Bug detection methodology depends on systematic testing across batch sizes, which may not catch all types of implementation errors

## Confidence
- Peer review evaluation gap: High confidence
- Bugs producing good results: Medium confidence (ASR/ST tasks only)
- False conclusions from buggy foundations: Medium confidence (single case study)

## Next Checks
1. Apply the batch-size testing methodology to other widely-used NLP architectures (BERT, GPT, etc.) to assess whether padding-dependent bugs are architecture-specific or systemic
2. Survey a broader sample of peer review guidelines across NLP venues to quantify how frequently correctness is explicitly addressed versus conflated with reproducibility
3. Conduct a controlled study where the same research paper is reviewed with and without explicit code correctness criteria to measure the impact on acceptance decisions and reviewer behavior