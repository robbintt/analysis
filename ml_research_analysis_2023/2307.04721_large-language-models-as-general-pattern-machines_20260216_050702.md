---
ver: rpa2
title: Large Language Models as General Pattern Machines
arxiv_id: '2307.04721'
source_url: https://arxiv.org/abs/2307.04721
tags:
- llms
- learning
- in-context
- sequence
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the potential of pre-trained large language
  models (LLMs) as general pattern machines for robotics tasks. The key idea is to
  leverage LLMs' in-context learning capabilities to recognize and complete patterns
  in sequences of tokens, which can represent robot states, actions, or symbolic instructions.
---

# Large Language Models as General Pattern Machines

## Quick Facts
- **arXiv ID**: 2307.04721
- **Source URL**: https://arxiv.org/abs/2307.04721
- **Reference count**: 40
- **Primary result**: LLMs can act as general pattern machines for robotics tasks through sequence transformation, completion, and improvement.

## Executive Summary
This paper investigates the potential of pre-trained large language models (LLMs) as general pattern machines for robotics tasks. The authors propose leveraging LLMs' in-context learning capabilities to recognize and complete patterns in sequences of tokens representing robot states, actions, or symbolic instructions. Through three main areas of investigation—sequence transformation, completion, and improvement—the paper demonstrates that LLMs can solve abstract reasoning problems, extrapolate functions for motion completion, and iteratively improve sequences for policy discovery. While current limitations exist in latency, context size, and compute costs, the findings suggest LLMs' pattern manipulation capabilities could play a role in building generalist embodied AI systems.

## Method Summary
The authors evaluate LLMs as general pattern machines through in-context learning on token sequences representing various robotic and symbolic tasks. For sequence transformation, they encode ARC benchmark problems and PCFG patterns as token sequences, testing the model's ability to complete abstract patterns. Sequence completion involves encoding sinusoidal functions and partial robot demonstrations as tokens, then evaluating the LLM's extrapolation capabilities. Sequence improvement is achieved by conditioning on reward-labeled trajectories, iteratively prompting the model to generate improved sequences for tasks like CartPole stabilization and grid navigation. The approach relies on consistent tokenization and carefully crafted prompts with in-context examples, without additional model training or fine-tuning.

## Key Results
- LLMs solved up to 85 out of 800 ARC problems, competitive with specialized methods, and maintained partial performance with randomly sampled token mappings
- Models successfully extrapolated simple functions (sinusoids) and applied this to extend partial robot demonstrations like sweeping motions and drawing loops
- Through reward-conditioned trajectories, LLMs discovered stabilizing CartPole controllers and improved grid navigation policies, including online human-guided trajectory optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generalize sequence transformations across different token mappings due to token invariance.
- Mechanism: When token embeddings are consistent within a task, LLMs can recognize abstract patterns regardless of the specific symbols used.
- Core assumption: The model's learned representations capture structural relationships rather than lexical identities.
- Evidence anchors:
  - [abstract]: "pattern completion proficiency can be partially retained even when the sequences are expressed using tokens randomly sampled from the vocabulary."
  - [section]: "Surprisingly, we find this extends beyond ASCII numbers, and that when they are replaced with a mapping to randomly sampled tokens in the vocabulary, LLMs can still generate valid solutions."
- Break condition: Tokenization inconsistencies or embeddings that collapse multiple symbols into one token break the pattern.

### Mechanism 2
- Claim: LLMs can extrapolate simple functions (like sinusoids) from partial sequences and apply this to robotic motion completion.
- Mechanism: By discretizing continuous values into token sequences, LLMs leverage their function approximation capabilities to predict future values.
- Core assumption: The function class is simple enough for the LLM to capture the underlying pattern.
- Evidence anchors:
  - [abstract]: "LLMs can extrapolate simple functions (e.g., sinusoids) and apply this to extend partial robot demonstrations like sweeping motions or drawing loops."
  - [section]: "LLMs (text-davinci-003) can extrapolate various functions y = a·sin(bx) (top row), y = ax·sin(bx) (middle row), and y = a/(2x)·sin(bx) (bottom row) given amounts of context."
- Break condition: Complex or chaotic functions that exceed the LLM's pattern recognition capacity.

### Mechanism 3
- Claim: LLMs can iteratively improve sequences by conditioning on reward-labeled trajectories, discovering closed-loop policies.
- Mechanism: By prompting LLMs with trajectories ordered by reward, they learn meta-patterns that associate higher returns with better sequences and generate improved versions.
- Core assumption: The reward signal provides sufficient signal for the LLM to infer better sequences.
- Evidence anchors:
  - [abstract]: "By conditioning on reward-labeled trajectories, LLMs can iteratively improve sequences to discover policies like stabilizing a CartPole or navigating a grid environment."
  - [section]: "We show that providing reward-labeled trajectories as context, coupled with online interaction, can enable an LLM-based agent to learn to navigate through a small grid, discover a stabilizing CartPole controller, and optimize simple trajectories."
- Break condition: Sparse or uninformative reward signals prevent the LLM from learning meaningful improvements.

## Foundational Learning

- **Concept**: In-context learning and sequence modeling
  - Why needed here: The entire approach relies on LLMs' ability to learn from few examples without weight updates.
  - Quick check question: Can you explain the difference between in-context learning and fine-tuning?

- **Concept**: Tokenization and embedding spaces
  - Why needed here: Proper tokenization is critical for consistent pattern representation and token invariance.
  - Quick check question: What happens if multiple symbols are grouped into a single token by the tokenizer?

- **Concept**: Function approximation and extrapolation
  - Why needed here: Sequence completion relies on LLMs' ability to extrapolate from partial function samples.
  - Quick check question: Why does discretizing continuous values to 0-100 tokens help with sinusoid extrapolation?

## Architecture Onboarding

- **Component map**: LLM model (text-davinci-003) -> Tokenizer with consistent symbol mapping -> Task-specific prompt template -> Environment interface for sequence improvement -> Reward signal processor

- **Critical path**: 1. Tokenize input sequence with consistent mapping 2. Construct prompt with in-context examples 3. Generate output sequence autoregressively 4. For improvement tasks, evaluate reward and update context 5. Iterate until convergence or termination

- **Design tradeoffs**: Tokenization consistency vs. semantic richness; Context length limits vs. number of examples; Model scale vs. inference cost and latency; Discrete vs. continuous value representation

- **Failure signatures**: Inconsistent tokenization causing pattern loss; Insufficient context examples for complex patterns; Reward signal too sparse for meaningful improvement; Function complexity exceeding LLM's approximation capability

- **First 3 experiments**: 1. Test ARC problem completion with consistent integer tokenization 2. Verify sinusoid extrapolation with varying context lengths 3. Implement simple reward-conditioned sequence improvement on Grid environment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the fundamental mechanism behind LLMs' ability to complete abstract patterns with token invariance, and how does this differ from their ability to complete natural language patterns?
- Basis in paper: [explicit] The paper discusses token invariance in LLMs, where they can solve ARC problems using randomly sampled token mappings, suggesting a degree of abstraction beyond specific tokens.
- Why unresolved: The paper provides empirical evidence for token invariance but does not delve into the underlying mechanisms or how this relates to the training process of LLMs.
- What evidence would resolve it: Further research into the internal representations and attention mechanisms of LLMs, specifically comparing their performance on tasks with and without token invariance, could shed light on this question.

### Open Question 2
- Question: How can the performance of LLMs on sequence transformation tasks be further improved, and what role do token embeddings play in this?
- Basis in paper: [inferred] The paper shows that LLMs can perform sequence transformations on ARC and PCFG tasks, but their performance is not perfect and can be influenced by tokenization choices.
- Why unresolved: While the paper explores the potential of LLMs for sequence transformation, it does not investigate specific techniques to enhance their performance or analyze the impact of different token embeddings.
- What evidence would resolve it: Experiments comparing the performance of LLMs with different tokenization strategies, embedding dimensions, and training objectives on sequence transformation tasks could provide insights into optimizing their performance.

### Open Question 3
- Question: What are the limitations of using LLMs for sequence completion in robotics, and how can these limitations be addressed?
- Basis in paper: [explicit] The paper demonstrates the application of LLMs for sequence completion in robotics tasks like table sweeping and whiteboard drawing, but acknowledges limitations such as the need for higher-dimensional representations and context length constraints.
- Why unresolved: The paper provides initial results but does not explore the full extent of the limitations or propose solutions to overcome them.
- What evidence would resolve it: Further research on developing efficient encoding schemes for high-dimensional robot states and actions, as well as techniques to handle longer context lengths, could address these limitations and enable broader application of LLMs in robotics.

## Limitations

- **Tokenization Sensitivity**: The approach's effectiveness is highly dependent on consistent tokenization choices, which are not fully specified in the paper and may not generalize across different tasks or models.
- **Scalability Challenges**: Current LLMs face significant limitations in latency, context size, and computational costs, which pose challenges for practical deployment in complex real-world robotics applications.
- **Reward Signal Dependency**: The sequence improvement mechanism relies heavily on the quality and informativeness of reward signals, which can be challenging to obtain for real-world robotic tasks and may limit the approach's effectiveness.

## Confidence

- **High Confidence**: LLMs can perform sequence transformation and completion tasks through in-context learning (supported by presented results and aligns with existing LLM literature)
- **Medium Confidence**: Token invariance can be partially retained across different token mappings (supported by experimental results but may be task-dependent)
- **Medium Confidence**: Sequence improvement approach shows promise on simple tasks but effectiveness on complex robotics problems remains uncertain

## Next Checks

1. **Cross-LLM Validation**: Replicate the key experiments across different LLM architectures (e.g., GPT-3.5, GPT-4, PaLM) to assess robustness and generalizability of the approach.

2. **Complex Function Extrapolation**: Extend sequence completion experiments to more complex and chaotic functions beyond simple sinusoids to test the limits of LLM pattern recognition and extrapolation capabilities.

3. **Real-world Robotics Integration**: Implement a pilot study integrating the LLM-based pattern machine approach into a physical robot for motion planning or control to identify practical challenges in real-world applications.