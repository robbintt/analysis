---
ver: rpa2
title: Gender-specific Machine Translation with Large Language Models
arxiv_id: '2309.03175'
source_url: https://arxiv.org/abs/2309.03175
tags:
- gender
- llama
- bias
- translation
- translations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates the capability of decoder-only Large Language
  Models (LLMs) to generate gender-specific translations, addressing the challenge
  of mitigating gender bias in machine translation. The authors employ in-context
  learning with LLaMa-7B, using gender-specific templates to elicit translations for
  25 languages, and compare its performance to the state-of-the-art NLLB-3B encoder-decoder
  model.
---

# Gender-specific Machine Translation with Large Language Models

## Quick Facts
- arXiv ID: 2309.03175
- Source URL: https://arxiv.org/abs/2309.03175
- Reference count: 40
- One-line primary result: LLaMa-7B achieves competitive gender-specific translation accuracy within 2 BLEU points of NLLB-3B using in-context learning with gender-specific templates.

## Executive Summary
This study investigates whether decoder-only Large Language Models can generate gender-specific translations comparable to state-of-the-art encoder-decoder NMT systems. The authors evaluate LLaMa-7B using in-context learning with gender-specific templates across 25 languages, comparing its performance to NLLB-3B. Results show LLaMa achieves translation accuracy within 2 BLEU points of NLLB while demonstrating reduced gender bias in coreference resolution tasks. The approach relies on prompt engineering rather than fine-tuning, making it computationally efficient and adaptable to new languages.

## Method Summary
The study employs in-context learning (ICL) with LLaMa-7B to generate gender-specific translations using prompt templates with in-context examples (ICEs) from the MULTILINGUAL HOLISTIC BIAS dataset. The model produces masculine and feminine translations for each source sentence, which are evaluated using BLEU scores and coreference resolution accuracy. NLLB-3B serves as the baseline, using beam search decoding. The evaluation spans multiple datasets including WinoMT and BUG for gender bias assessment, and FLoRes for robustness testing across 25 languages.

## Key Results
- LLaMa-7B achieves translation accuracy within 2 BLEU points of NLLB-3B across 25 languages
- Performance improves with more ICEs (5, 16, 32), showing clear trend of gains
- LLaMa shows reduced gender bias in coreference resolution, with minimal difference between male and female outputs
- Significant performance drops occur when LLaMa translations are evaluated against opposite-gender references in ambiguous datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLaMa-7B can generate gender-specific translations competitive with NLLB-3B through in-context learning.
- Mechanism: The model uses gender-specific templates with in-context examples (ICEs) to produce two translations, one masculine and one feminine, for a single source sentence. By conditioning the output on the prompt structure, LLaMa learns to produce both gender variants with minimal additional training.
- Core assumption: The prompt template and ICEs are sufficient for the model to infer the task of producing gender-specific translations without explicit fine-tuning.
- Evidence anchors:
  - [abstract] "LLaMa can generate gender-specific translations with translation accuracy and gender bias mitigation comparable to NLLB, a state-of-the-art multilingual NMT system."
  - [section 5.1] "LLaMa’s performance surpasses NLLB for all four reported metrics in 7 out of 25 languages, with NLLB surpassing LLaMa in 10 out of 25 languages."
  - [corpus] Weak evidence; no direct mention of in-context learning in neighbor papers, suggesting this is a novel application.
- Break condition: If the ICEs are not relevant or the prompt template is ambiguous, the model may fail to produce coherent gender-specific translations or default to a single gender output.

### Mechanism 2
- Claim: LLaMa’s gender-specific translations rely on coreference resolution to determine gender, rather than simple grammatical gender matching.
- Mechanism: When prompted with gender-specific templates, LLaMa analyzes the context of the source sentence to resolve pronouns and noun references, producing translations that align with the resolved gender rather than mechanically switching grammatical markers.
- Core assumption: The model’s pre-training data and architecture enable it to perform coreference resolution as part of the translation task.
- Evidence anchors:
  - [section 5.2] "Our experiments reveal that LLaMa’s translations are robust, showing significant performance drops when evaluated against opposite-gender references in gender-ambiguous datasets but maintaining consistency in less ambiguous contexts."
  - [section 5.2] "We found that the performance on BUG’s gold dataset, a balanced dataset constructed from a more diverse domain, favors NLLB over LLaMa in almost every language... However, the difference in performance between LLaMa’s male and female outputs is almost non-existent, suggesting a reliance on coreference resolution."
  - [corpus] No direct evidence; this is inferred from the experimental results.
- Break condition: If the source sentence lacks sufficient context for coreference resolution, the model may default to stereotypical gender assignments or fail to produce coherent translations.

### Mechanism 3
- Claim: The number of in-context examples (ICEs) impacts the quality of gender-specific translations, with performance improving as the number of ICEs increases.
- Mechanism: Providing more ICEs in the prompt allows the model to better infer the task and produce more accurate gender-specific translations by leveraging a broader set of examples.
- Core assumption: The model benefits from a larger set of demonstrations to understand the task, as is typical in few-shot learning scenarios.
- Evidence anchors:
  - [section 5.1] "We see a clear trend (with exceptions) of performance increase as the number of ICEs grows, which suggests that further gains are still possible."
  - [section 5.1] "We prompt LLaMa with 5, 16, and 32 ICEs... We found a clear trend of performance increase as the number of ICEs grows."
  - [corpus] No direct evidence; this is inferred from the experimental results.
- Break condition: If the ICEs are not diverse or relevant, increasing their number may not improve performance and could even introduce noise.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: ICL allows LLaMa to learn the task of gender-specific translation without explicit fine-tuning, using only a few examples in the prompt.
  - Quick check question: How does ICL differ from traditional fine-tuning, and why is it advantageous for this task?

- Concept: Coreference resolution
  - Why needed here: LLaMa’s translations rely on coreference resolution to determine the appropriate gender for translations, especially in ambiguous contexts.
  - Quick check question: What is coreference resolution, and how does it apply to gender-specific translation?

- Concept: BLEU score and gender bias evaluation
  - Why needed here: BLEU score is used to measure translation accuracy, while coreference resolution accuracy is used to evaluate gender bias in the translations.
  - Quick check question: How do BLEU score and coreference resolution accuracy complement each other in evaluating gender-specific translations?

## Architecture Onboarding

- Component map: MULTILINGUAL HOLISTIC BIAS dataset -> LLaMa-7B (with prompt templates) -> gender-specific translations -> BLEU/coreference evaluation
- Critical path: 1) Construct prompt templates with ICEs for gender-specific translations 2) Use LLaMa-7B to generate masculine and feminine translations for each source sentence 3) Evaluate translations using BLEU score and coreference resolution accuracy 4) Compare LLaMa’s performance with NLLB-3B across multiple languages and datasets
- Design tradeoffs: Using a decoder-only model (LLaMa) vs. an encoder-decoder model (NLLB) for translation tasks; balancing the number of ICEs to avoid overfitting while ensuring sufficient context for the model; relying on automatic metrics (BLEU, coreference resolution) vs. human evaluation for assessing translation quality and bias
- Failure signatures: Incomplete or incoherent translations, especially for languages with complex grammatical gender systems; over-reliance on stereotypical gender assignments in ambiguous contexts; poor performance on low-resource languages or languages not explicitly included in LLaMa’s training data
- First 3 experiments: 1) Generate gender-specific translations for a small subset of languages using 5 ICEs and compare BLEU scores with NLLB 2) Evaluate the impact of increasing the number of ICEs (e.g., 16, 32) on translation accuracy and gender bias 3) Test the robustness of LLaMa’s translations by evaluating them against opposite-gender references in gender-ambiguous datasets

## Open Questions the Paper Calls Out
- What is the minimum number of in-context examples (ICEs) required for LLaMa to achieve competitive performance in gender-specific translation tasks?
- How does the quality of gender-specific translations vary across different types of gender ambiguity in source sentences?
- Can the prompt template be optimized further to reduce gender bias in translations for languages with less grammatical gender alignment?

## Limitations
- The exact prompt templates and ICEs used are not provided, making faithful reproduction challenging
- The mechanism of coreference resolution for gender assignment is inferred rather than explicitly demonstrated
- Performance variations across different languages and datasets suggest the approach may not generalize uniformly

## Confidence
- High Confidence: The experimental methodology for evaluating translation quality using BLEU scores and the coreference resolution approach for measuring gender bias are well-established and clearly described.
- Medium Confidence: The claim that LLaMa relies on coreference resolution rather than grammatical gender matching is supported by experimental evidence but requires further validation through ablation studies.
- Low Confidence: The assertion that LLaMa's gender-specific translations are competitive with state-of-the-art NMT systems across all evaluated languages requires additional validation.

## Next Checks
1. Conduct prompt template sensitivity analysis to determine how LLaMa's performance varies with different prompt engineering choices
2. Evaluate LLaMa's performance on language pairs not included in the original training data to test generalizability of the coreference resolution mechanism
3. Conduct human evaluation studies to complement automatic metrics and identify potential failure modes in ambiguous or culturally sensitive contexts