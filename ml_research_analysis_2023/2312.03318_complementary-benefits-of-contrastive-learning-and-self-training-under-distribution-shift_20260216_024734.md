---
ver: rpa2
title: Complementary Benefits of Contrastive Learning and Self-Training Under Distribution
  Shift
arxiv_id: '2312.03318'
source_url: https://arxiv.org/abs/2312.03318
tags:
- data
- target
- learning
- source
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies the combination of self-training and contrastive
  learning for unsupervised domain adaptation (UDA) and semi-supervised learning (SSL).
  While both methods are effective individually, their combination had not been previously
  explored.
---

# Complementary Benefits of Contrastive Learning and Self-Training Under Distribution Shift

## Quick Facts
- arXiv ID: 2312.03318
- Source URL: https://arxiv.org/abs/2312.03318
- Reference count: 40
- Primary result: STOC yields 3-8% higher accuracy than either self-training or contrastive learning alone in UDA settings across eight benchmarks

## Executive Summary
This paper explores the combination of self-training and contrastive learning for unsupervised domain adaptation (UDA) and semi-supervised learning (SSL). While both methods are effective individually, their combination (STOC) had not been previously studied. The authors find that in UDA settings, STOC provides 3-8% higher accuracy than either method alone across eight benchmarks, while in SSL settings, STOC does not provide significant improvements over contrastive learning alone. Theoretically, they analyze a simplified model showing that contrastive learning amplifies invariant features and provides good initialization for self-training to further improve performance.

## Method Summary
The paper combines self-training (FixMatch) with contrastive learning (SwAV) in a sequential pipeline. For UDA, contrastive pretraining is performed on unlabeled data from both source and target domains, followed by self-training initialized from the CL model. For SSL, contrastive pretraining uses in-distribution unlabeled data. The method is compared against ERM, self-training alone, and contrastive learning alone on eight benchmark datasets including BREEDs, FMoW, VisDA, OfficeHome, and CIFAR-10 variants.

## Key Results
- STOC improves UDA accuracy by 3-8% compared to either self-training or contrastive learning alone across eight benchmarks
- In SSL settings, STOC provides negligible improvements over contrastive learning alone
- Theoretical analysis shows contrastive learning amplifies invariant features while self-training improves linear transferability
- The combination is synergistic under distribution shift but not in in-distribution semi-supervised learning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Contrastive learning amplifies invariant features over spurious features in UDA settings.
- **Mechanism**: Under suitable augmentations, contrastive pretraining learns a feature extractor that increases the inner product with the invariant feature relative to the spurious feature. This happens because augmentations amplify noise along spurious features in the target domain while preserving signal along invariant features.
- **Core assumption**: Augmentations are generic and symmetric across features, not encoding information about which features are invariant or spurious.
- **Evidence anchors**:
  - [abstract] "empirically, the authors find that in UDA settings, the combination (STOC) yields 3-8% higher accuracy than either method alone across eight benchmarks"
  - [section 4.3] "Theorem 5 (Informal; CL Captures Both Features But Amplifies Invariant Over Spurious Features)"
  - [corpus] Weak - corpus neighbors don't directly discuss contrastive feature amplification
- **Break condition**: If augmentations are not generic and instead selectively mask or preserve specific features, the amplification effect may not occur.

### Mechanism 2
- **Claim**: Self-training improves linear transferability when initialized with a "good" classifier.
- **Mechanism**: Self-training iteratively updates the classifier by generating pseudolabels on unlabeled target data. When initialized with a classifier that already has low target error, self-training can unlearn dependence on spurious features and converge to an optimal target classifier.
- **Core assumption**: The initial classifier (from contrastive learning) has sufficiently low target error to enable effective self-training.
- **Evidence anchors**:
  - [abstract] "STOC offers gains over CL... in domain adaptation settings"
  - [section 4.4] "Theorem 7 (Informal; ST improves over CL)"
  - [corpus] Weak - corpus neighbors don't directly discuss self-training's effect on linear transferability
- **Break condition**: If the initial classifier has high target error, self-training may instead increase reliance on spurious features.

### Mechanism 3
- **Claim**: STOC is synergistic under distribution shift but not in semi-supervised learning.
- **Mechanism**: In UDA, contrastive learning provides good initialization by amplifying invariant features, and self-training improves linear transferability. In SSL, the limited labeled data provides sufficient signal for feature selection, leaving little room for self-training to improve.
- **Core assumption**: The amount of labeled data in SSL is sufficient to identify high-margin features predictive on ID data.
- **Evidence anchors**:
  - [abstract] "in semi-supervised learning settings, surprisingly, the benefits are not synergistic"
  - [section 4.1] "Why disparate behaviors for out-of-distribution vs. in distribution?"
  - [corpus] Weak - corpus neighbors don't directly compare UDA vs SSL behavior
- **Break condition**: If labeled data in SSL is extremely limited, self-training might provide improvements similar to UDA.

## Foundational Learning

- **Concept**: Unsupervised domain adaptation (UDA)
  - Why needed here: Understanding UDA is crucial because the paper's main contribution is improving performance specifically in UDA settings where labeled source data and unlabeled target data have distribution shift.
  - Quick check question: What is the key challenge in UDA that differentiates it from standard supervised learning?

- **Concept**: Contrastive learning and its augmentation requirements
  - Why needed here: Contrastive learning is one of the two main methods being combined, and understanding how augmentations affect feature learning is central to the theoretical analysis.
  - Quick check question: How do augmentations affect the feature extractor learned by contrastive pretraining?

- **Concept**: Self-training and pseudolabeling dynamics
  - Why needed here: Self-training is the second method being combined, and understanding its iterative update mechanism and failure modes is essential for grasping why it works better after contrastive pretraining.
  - Quick check question: Under what conditions does self-training improve classifier performance versus worsen it?

## Architecture Onboarding

- **Component map**: Feature extractor Φ <- Augmentations A <- Unlabeled data <- Contrastive learning <- Self-training <- Classifier h <- Labeled source data

- **Critical path**:
  1. Contrastive pretraining on unlabeled data to obtain Φcl
  2. Initialize linear classifier h with source-only ERM or contrastive pretraining results
  3. Apply self-training with pseudolabels on target data
  4. Evaluate target accuracy

- **Design tradeoffs**:
  - Using target data for contrastive pretraining vs. source-only: target data provides better feature amplification but may introduce distribution shift
  - Early stopping CL vs. full training: early stopping may provide better initialization for self-training
  - Fixed vs. fine-tuned feature extractor: fine-tuning can improve performance but risks overfitting

- **Failure signatures**:
  - STOC performs similarly to contrastive learning alone: suggests either insufficient distribution shift or poor initialization
  - Self-training degrades performance: suggests initial classifier has high target error or augmentation quality is poor
  - Performance gap between UDA and SSL is small: suggests limited benefit from combining methods

- **First 3 experiments**:
  1. Run STOC vs CL on a UDA benchmark (e.g., BREEDs Entity13) to verify 3-8% improvement
  2. Run STOC vs CL on an SSL benchmark (e.g., CIFAR10 with 10% labels) to verify negligible improvement
  3. Vary the ratio γ/σsp in the toy setup to observe the phase transition where self-training becomes effective

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the complementary nature of self-training and contrastive learning extend to semi-supervised learning when more labeled data is available?
- Basis in paper: [explicit] The authors note that STOC provides negligible improvements over CL in SSL settings with limited supervision, but hypothesize that this might change with more labeled data.
- Why unresolved: The paper only experiments with 10% labeled data in SSL settings.
- What evidence would resolve it: Experiments varying the proportion of labeled data in SSL settings to determine if STOC becomes more beneficial as labeled data increases.

### Open Question 2
- Question: Can self-training and contrastive learning be integrated into a unified training paradigm that avoids the computational cost of sequential application?
- Basis in paper: [explicit] The authors note that sequential application increases computation cost and suggest investigating unified training as a future direction.
- Why unresolved: The paper only studies sequential application of ST and CL.
- What evidence would resolve it: Development and empirical evaluation of methods that combine ST and CL objectives into a single optimization framework.

### Open Question 3
- Question: Do self-training and contrastive learning have complementary benefits for feature learning in UDA and SSL settings when the backbone is fully fine-tuned?
- Basis in paper: [inferred] The authors find that STOC can improve features when CL is early stopped suboptimally, but remain unclear if complementary benefits exist when CL is trained to convergence.
- Why unresolved: The theoretical analysis assumes fixed feature extractors, while empirical results with full fine-tuning show mixed results.
- What evidence would resolve it: Theoretical analysis of full fine-tuning scenarios or comprehensive empirical studies comparing STOC vs CL with different CL training durations and fine-tuning strategies.

## Limitations

- The theoretical analysis relies on a simplified Gaussian model that may not fully capture real-world dataset complexity
- The claim that augmentations are "generic and symmetric" is difficult to verify empirically and may not hold for all augmentation strategies
- The paper's focus on classification accuracy may not translate to other tasks like segmentation or object detection

## Confidence

**High confidence**: Claims about STOC improving UDA performance (3-8% gains) across multiple benchmarks, as these are empirically validated with concrete numbers.

**Medium confidence**: Claims about self-training's ability to unlearn spurious features and improve linear transferability are theoretically grounded but rely on specific initialization conditions.

**Low confidence**: Theoretical guarantees derived from simplified models may not fully capture real-world behavior, particularly the claim that generic augmentations automatically amplify invariant features.

## Next Checks

1. **Empirical validation of feature amplification**: Measure the inner product between invariant and spurious features before and after contrastive pretraining on real datasets to verify the theoretical prediction that contrastive learning amplifies invariant features.

2. **Ablation on initialization quality**: Systematically vary the quality of the initial classifier from contrastive learning (e.g., by adjusting pretraining epochs or regularization) and measure how this affects self-training's ability to improve performance in UDA.

3. **Cross-task generalization**: Apply STOC to a non-classification task (e.g., semantic segmentation) to test whether the complementary benefits extend beyond the image classification benchmarks studied in the paper.