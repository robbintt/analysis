---
ver: rpa2
title: Neural Architecture Search for Effective Teacher-Student Knowledge Transfer
  in Language Models
arxiv_id: '2303.09639'
source_url: https://arxiv.org/abs/2303.09639
tags:
- architecture
- student
- search
- distillation
- mnli
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents KD-NAS, a neural architecture search method
  that uses knowledge distillation to find optimal student architectures for multilingual
  language models. The approach searches a 5-dimensional space of transformer architectures,
  optimizing for both accuracy and latency.
---

# Neural Architecture Search for Effective Teacher-Student Knowledge Transfer in Language Models

## Quick Facts
- arXiv ID: 2303.09639
- Source URL: https://arxiv.org/abs/2303.09639
- Authors: 
- Reference count: 13
- Primary result: KD-NAS achieves 7x CPU speedup (2x GPU) while maintaining 90% performance vs XLM-Roberta Base through automated student architecture search

## Executive Summary
This paper presents KD-NAS, a neural architecture search method that uses knowledge distillation to find optimal student architectures for multilingual language models. The approach searches a 5-dimensional space of transformer architectures, optimizing for both accuracy and latency. A controller predicts rewards for candidate architectures based on distillation loss and inference speed, with top candidates distilled from a multilingual teacher. The method recommends architectures suitable for various latency budgets and has been deployed in 3 software offerings requiring high throughput, low latency, and CPU deployment.

## Method Summary
KD-NAS uses an LSTM controller to predict rewards for candidate transformer architectures within a 5-dimensional search space (layers, attention heads, hidden size, intermediate size, activation function). The controller receives as input the current candidate state along with the Global Best State and Previous Best State from prior episodes. Architectures are evaluated using a mini-KD process (7 epochs, 30% data) as a proxy for full distillation performance. The reward function combines accuracy and normalized latency, with a tunable trade-off parameter. Top-performing architectures undergo full distillation from the multilingual teacher model using representation loss, logit loss, and hard label loss.

## Key Results
- Achieves 7x speedup on CPU inference (2x on GPU) compared to XLM-Roberta Base teacher
- Maintains 90% performance on GLUE benchmark while being 4-5x smaller
- Controller consistently recommends high-performing architectures across multiple episodes
- Task-specific distillation objectives (MNLI) yield better results than task-agnostic approaches (MLM)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The KD-NAS approach finds better student architectures than random sampling by optimizing for both accuracy and latency during the search process.
- Mechanism: The NAS controller predicts rewards for candidate architectures based on a combination of distillation loss and inference speed, guiding the search toward architectures that balance performance and efficiency.
- Core assumption: The reward function accurately captures the trade-off between accuracy and latency that matters for practical deployment.
- Evidence anchors:
  - [abstract] "A controller predicts rewards for candidate architectures based on distillation loss and inference speed, with top candidates distilled from a multilingual teacher."
  - [section 3.3.3] "reward(Ss) = accG(DSs) * (lat(Ss)/max_lat)^α"
  - [corpus] Weak evidence - corpus neighbors discuss knowledge distillation but not the specific NAS+KD combination

### Mechanism 2
- Claim: Using a feedback controller with memory of past high-performing states improves search efficiency compared to standard RL-based NAS.
- Mechanism: The LSTM controller takes as input both the current candidate state and the best states from previous episodes, allowing it to build on successful architectural patterns discovered earlier in the search.
- Core assumption: Past high-performing architectures contain useful architectural patterns that generalize to future episodes.
- Evidence anchors:
  - [section 3.3.2] "Besides the current state defined as an architecture within the search space, we pass as inputs two additional states: the Global Best State which is the state (architecture) of the best performing student seen in all prior episodes {ep1, ep2, .., epn−1}, and the Previous Best State, i.e., the best performing student architecture from episode epn−1."
  - [section 4.1] "each of the selected models has been continuously recommended by the controller across multiple episodes, indicating that the controller is confident in the performance of these models"
  - [corpus] No direct evidence in corpus neighbors about feedback controllers in NAS

### Mechanism 3
- Claim: Using task-specific distillation objectives yields better results than task-agnostic objectives for the KD-NAS process.
- Mechanism: Task-specific distillation allows the search to find architectures optimized for the target downstream task, while task-agnostic approaches require much more data to achieve comparable performance.
- Core assumption: The search proxy accurately ranks architectures for the full distillation task when using task-specific objectives.
- Evidence anchors:
  - [section 4.2] "MLM distillation is very time consuming (approx. 50 hours with 4 V100 GPUS, compared to approx 9 hours to distill the same model on the MNLI task with 1 V100 GPU to get the same GLUE performance)"
  - [section 4.2] "Using our multi-layer hidden state distillation process, our KD-NAS student model achieves a 7x speedup on CPU inference (2x on GPU) compared to a XLM-Roberta Base Teacher, while maintaining 90% performance"
  - [corpus] No direct evidence in corpus neighbors about task-specific vs task-agnostic distillation in NAS context

## Foundational Learning

- Concept: Knowledge Distillation (KD) - the process of transferring knowledge from a large teacher model to a smaller student model by having the student mimic the teacher's internal representations and predictions.
  - Why needed here: KD-NAS is fundamentally about finding optimal student architectures for knowledge distillation, so understanding how KD works is essential.
  - Quick check question: What are the three types of losses used in the XtremeDistil pipeline mentioned in the paper?

- Concept: Neural Architecture Search (NAS) - automated methods for finding optimal neural network architectures within a defined search space.
  - Why needed here: KD-NAS combines NAS with knowledge distillation, so understanding the basic components of NAS (search space, search strategy, performance evaluation) is crucial.
  - Quick check question: What are the five dimensions of the search space defined for the transformer architectures in KD-NAS?

- Concept: Reinforcement Learning-based NAS - using an RNN controller that predicts the performance of candidate architectures, with rewards based on actual performance after training.
  - Why needed here: KD-NAS uses an LSTM controller that predicts rewards based on distillation performance, which is a specific variant of RL-based NAS.
  - Quick check question: How does the controller in KD-NAS incorporate information from past episodes to guide the search?

## Architecture Onboarding

- Component map: Controller -> Mini-KD Proxy -> Reward Function -> Controller Update -> Architecture Selection
- Critical path: Controller generates candidates → Mini-KD evaluates candidates → Rewards computed → Controller updates with (state, reward) pairs including GlobalBestState and PreviousBestState → Repeat for T episodes → Select top architectures
- Design tradeoffs:
  - Proxy set size vs evaluation accuracy: Smaller proxy sets are faster but may rank architectures incorrectly
  - Exploration vs exploitation: Higher exploration in early episodes vs exploitation of promising patterns later
  - Reward function parameters: β (latency normalization) and α (trade-off exponent) significantly affect which architectures are selected
- Failure signatures:
  - Controller consistently predicts similar rewards for diverse architectures → Overfitting to specific patterns
  - Selected architectures show poor correlation between proxy and full-task performance → Proxy set is unrepresentative
  - NAS-selected models are slower than manually designed ones despite optimization for latency → Latency measurement or reward function issue
- First 3 experiments:
  1. Run KD-NAS with random initialization (no controller) to establish baseline for random sampling performance
  2. Test different proxy set configurations (epochs, data percentage) to find optimal trade-off between speed and accuracy
  3. Compare task-specific vs task-agnostic distillation objectives using the same search space to validate the paper's findings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does pretraining on generic masked language modeling objectives not improve the performance of student models after knowledge distillation, contrary to expectations from prior work?
- Basis in paper: [explicit] The paper shows that pretraining NAS-selected architectures on Wikipedia data with MLM objectives does not yield significant performance improvements compared to random initialization, which contradicts the assumption that pretraining would benefit distillation.
- Why unresolved: The authors note this result is counterintuitive and suggest pretraining might cause models to learn very different representations than the teacher, but do not provide a definitive explanation or experimental validation of this hypothesis.
- What evidence would resolve it: Controlled experiments comparing representation similarity between pretrained and randomly initialized models before and after distillation, along with ablation studies varying pretraining data, steps, and objectives would clarify this phenomenon.

### Open Question 2
- Question: What causes the significant performance gap on the CoLA task (syntactic acceptability) when student models are distilled on semantic tasks like MNLI?
- Basis in paper: [explicit] The paper observes that student models distilled on MNLI show notably worse performance on CoLA compared to other GLUE tasks, and hypothesizes this is due to CoLA's syntactic nature versus the semantic focus of other tasks.
- Why unresolved: The authors acknowledge this as a potential explanation but do not systematically investigate whether syntactic versus semantic task properties drive this performance gap, or whether task-agnostic distillation approaches could mitigate it.
- What evidence would resolve it: Systematic testing of student models distilled on different task types (syntactic, semantic, mixed) across all GLUE tasks would reveal whether task characteristics drive performance differences and whether distillation objectives can be optimized for balanced performance.

### Open Question 3
- Question: Why are smaller models with fewer parameters not necessarily faster in practice, contrary to common assumptions about model efficiency?
- Basis in paper: [explicit] The paper demonstrates that a 13M parameter model was slower than a 23M parameter base model, showing that parameter count is not a reliable proxy for latency and that smaller models are not always faster.
- Why unresolved: The authors observe this discrepancy but do not investigate the underlying architectural factors (such as memory access patterns, computational intensity of operations, or hardware utilization) that might explain why parameter reduction does not translate to latency reduction.
- What evidence would resolve it: Detailed profiling of inference performance across different architectures with varying parameter counts, analyzing per-layer computational costs, memory bandwidth usage, and GPU/CPU utilization would identify which architectural choices impact latency beyond simple parameter count.

## Limitations

- The search space is limited to 5 dimensions of transformer architectures, potentially missing other architectural innovations
- Latency measurements are specific to particular hardware configurations and may not generalize to other deployment environments
- The effectiveness of the mini-KD proxy process depends heavily on its correlation with full-task performance, which is validated but not extensively analyzed

## Confidence

- High confidence: The core mechanism of combining NAS with KD objectives is sound and the empirical results are well-documented
- Medium confidence: The scalability claims and deployment success are supported by the reported metrics but lack detailed deployment case studies
- Low confidence: The generality of the approach across different model families and tasks, as only one teacher architecture (XLM-Roberta Base) and one task (MNLI) are evaluated

## Next Checks

1. Test the controller's architecture predictions on a held-out task (e.g., QNLI or SST-2) to validate generalization beyond the MNLI proxy set
2. Implement ablation studies removing the GlobalBestState and PreviousBestState inputs to quantify the benefit of the feedback mechanism
3. Conduct sensitivity analysis on the reward function parameters (β and α) to determine their impact on architecture selection and whether the chosen values are optimal