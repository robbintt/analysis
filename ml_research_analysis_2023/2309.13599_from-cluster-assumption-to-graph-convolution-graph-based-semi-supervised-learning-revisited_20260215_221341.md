---
ver: rpa2
title: 'From Cluster Assumption to Graph Convolution: Graph-based Semi-Supervised
  Learning Revisited'
arxiv_id: '2309.13599'
source_url: https://arxiv.org/abs/2309.13599
tags:
- graph
- methods
- learning
- node
- convolution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a theoretical analysis of graph convolutional
  networks (GCNs) in relation to traditional graph-based semi-supervised learning
  (GSSL) methods. Through a unified optimization framework, it reveals that GCNs may
  not effectively integrate both graph structure and label information at each layer,
  unlike traditional shallow GSSL methods.
---

# From Cluster Assumption to Graph Convolution: Graph-based Semi-Supervised Learning Revisited

## Quick Facts
- arXiv ID: 2309.13599
- Source URL: https://arxiv.org/abs/2309.13599
- Reference count: 40
- Key outcome: This paper presents a theoretical analysis of graph convolutional networks (GCNs) in relation to traditional graph-based semi-supervised learning (GSSL) methods. Through a unified optimization framework, it reveals that GCNs may not effectively integrate both graph structure and label information at each layer, unlike traditional shallow GSSL methods. To address this, the paper introduces three novel graph convolution methods. The first, OGC, is a supervised method that jointly optimizes classification and Laplacian smoothing losses using a Supervised Embedding (SEB) operator. The other two, GGC and GGCM, are unsupervised methods that preserve graph structure information during convolution by combining graph convolution with an Inverse Graph Convolution (IGC) operator. Experiments demonstrate the effectiveness of these methods, with OGC achieving 86.9% accuracy on Cora, surpassing existing methods. The study provides new insights into GCN design and offers practical solutions for improving graph learning.

## Executive Summary
This paper presents a theoretical analysis of graph convolutional networks (GCNs) in relation to traditional graph-based semi-supervised learning (GSSL) methods. Through a unified optimization framework, it reveals that GCNs may not effectively integrate both graph structure and label information at each layer, unlike traditional shallow GSSL methods. To address this, the paper introduces three novel graph convolution methods. The first, OGC, is a supervised method that jointly optimizes classification and Laplacian smoothing losses using a Supervised Embedding (SEB) operator. The other two, GGC and GGCM, are unsupervised methods that preserve graph structure information during convolution by combining graph convolution with an Inverse Graph Convolution (IGC) operator. Experiments demonstrate the effectiveness of these methods, with OGC achieving 86.9% accuracy on Cora, surpassing existing methods. The study provides new insights into GCN design and offers practical solutions for improving graph learning.

## Method Summary
The paper introduces three novel graph convolution methods to address the limitations of typical GCNs in integrating graph structure and label information. OGC is a supervised method that jointly optimizes classification and Laplacian smoothing losses using a Supervised Embedding (SEB) operator. GGC and GGCM are unsupervised methods that preserve graph structure information during convolution by combining graph convolution with an Inverse Graph Convolution (IGC) operator. These methods are based on a unified optimization framework that reveals the shortcomings of traditional GCNs. The proposed methods are evaluated on standard graph datasets, including Cora, Citeseer, and Pubmed, demonstrating their effectiveness in improving graph learning performance.

## Key Results
- OGC achieves 86.9% accuracy on Cora, surpassing existing methods
- GGC and GGCM preserve graph structure information during convolution by combining graph convolution with an Inverse Graph Convolution (IGC) operator
- The proposed methods provide new insights into GCN design and offer practical solutions for improving graph learning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Typical GCNs may not jointly reduce supervised classification loss and Laplacian smoothing loss at each layer, unlike traditional shallow GSSL methods.
- **Mechanism**: GCNs apply graph convolution to reduce Laplacian smoothing loss, then separately apply backpropagation to reduce classification loss. These operations are sequential, not jointly optimized at each layer.
- **Core assumption**: The two loss terms (classification and smoothing) should ideally be reduced simultaneously during each layer's computation to maximize performance.
- **Evidence anchors**:
  - [abstract] "One of the most intriguing findings is that, unlike traditional ones, typical GCNs may not jointly consider the graph structure and label information at each layer."
  - [section 3.2] "at the k-th layer, the graph convolution in GCN only aims to reduce the Laplacian smoothing loss, without considering the classification loss... Therefore, as a whole, at the k-th layer, GCN may not guarantee to reduce the overall loss in Eq. 4."
  - [corpus] Weak evidence - corpus neighbors don't directly discuss joint loss reduction at each layer.
- **Break condition**: If joint optimization of both loss terms at each layer doesn't improve performance, or if the sequential approach becomes computationally prohibitive.

### Mechanism 2
- **Claim**: The proposed OGC method jointly optimizes classification and Laplacian smoothing losses using a Supervised Embedding (SEB) operator.
- **Mechanism**: OGC updates node embeddings by simultaneously minimizing both loss terms through gradient descent: U(k+1) = U(k) - ηsmo(∂Qsmo/∂U(k)) - ηsup(∂Qsup/∂U(k)).
- **Core assumption**: Joint optimization of both loss terms during each iteration will lead to better performance than sequential optimization.
- **Evidence anchors**:
  - [section 4.1] "OGC alleviates this issue by jointly considering the involved two loss terms in the convolution process."
  - [abstract] "The first is a supervised method OGC which guides the graph convolution process with labels."
  - [corpus] Weak evidence - corpus neighbors don't directly discuss supervised embedding operators.
- **Break condition**: If the additional computational overhead of joint optimization doesn't justify the performance gains, or if the method overfits with too many labels.

### Mechanism 3
- **Claim**: The proposed GGC and GGCM methods preserve graph structure information during convolution by combining graph convolution with an Inverse Graph Convolution (IGC) operator.
- **Mechanism**: GGC and GGCM simultaneously conduct (lazy) graph convolution and IGC at each iteration. IGC ensures dissimilarity between unlinked nodes by maximizing a Laplacian sharpening loss.
- **Core assumption**: Preserving both similarity among linked nodes and dissimilarity among unlinked nodes during convolution will lead to better graph structure preservation.
- **Evidence anchors**:
  - [section 4.2.2] "by simultaneously conducting graph convolution and IGC at each iteration, we would finally preserve the original graph structure in the embedding space."
  - [abstract] "The others are two unsupervised methods: GGC and its multi-scale version GGCM, both aiming to preserve the graph structure information during the convolution process."
  - [corpus] Weak evidence - corpus neighbors don't directly discuss inverse graph convolution operators.
- **Break condition**: If the negative sampling required for IGC becomes computationally prohibitive, or if preserving dissimilarity between unlinked nodes doesn't improve performance.

## Foundational Learning

- **Concept**: Graph Laplacian and its role in semi-supervised learning
  - **Why needed here**: Understanding how graph structure is represented and used for learning is crucial for grasping the proposed methods' innovations.
  - **Quick check question**: What does the graph Laplacian matrix represent in the context of semi-supervised learning, and how is it used to enforce the cluster assumption?

- **Concept**: Spectral graph theory and graph convolution
  - **Why needed here**: The proposed methods leverage spectral properties of graphs, and understanding graph convolution in the spectral domain is essential for grasping their mechanisms.
  - **Quick check question**: How does graph convolution in the spectral domain relate to low-pass filtering, and why is this property important for semi-supervised learning?

- **Concept**: Optimization theory and gradient descent
  - **Why needed here**: The proposed methods are framed in terms of optimization problems and gradient descent updates, so understanding these concepts is crucial for grasping their mechanisms.
  - **Quick check question**: How does the standard gradient descent update rule relate to the iterative updates used in the proposed OGC, GGC, and GGCM methods?

## Architecture Onboarding

- **Component map**: The proposed methods consist of three main components: 1) Graph convolution operator for aggregating information from neighbors, 2) Supervised Embedding (SEB) operator for OGC, and 3) Inverse Graph Convolution (IGC) operator for GGC and GGCM. These components are used iteratively to update node embeddings.
- **Critical path**: For OGC, the critical path is: initialize node embeddings, update label prediction function, update node embeddings via supervised graph convolution, repeat until convergence. For GGC and GGCM, the critical path is: initialize node embeddings, conduct graph convolution and IGC, update node embeddings, repeat until convergence.
- **Design tradeoffs**: The proposed methods trade off computational complexity for improved performance. OGC requires additional computation for joint optimization of loss terms, while GGC and GGCM require additional computation for IGC. However, these methods may achieve better performance than traditional GCNs.
- **Failure signatures**: If the proposed methods fail to outperform traditional GCNs, potential causes include: 1) insufficient label information for OGC, 2) poor negative sampling for IGC in GGC and GGCM, or 3) overfitting due to the additional complexity.
- **First 3 experiments**:
  1. Compare the performance of OGC, GGC, and GGCM to traditional GCNs on a small, well-known graph dataset (e.g., Cora or Citeseer).
  2. Analyze the impact of the number of negative edges used in IGC for GGC and GGCM on a small graph dataset.
  3. Evaluate the effect of using both training and validation labels for OGC on a small graph dataset.

## Open Questions the Paper Calls Out
- **Open Question 1**: How does the performance of OGC change when using different loss functions besides squared loss and cross-entropy in the Supervised Embedding (SEB) operator?
  - **Basis in paper**: [explicit] The paper mentions using squared loss and cross-entropy loss in OGC, but does not explore other loss functions.
  - **Why unresolved**: The paper only experiments with squared loss and cross-entropy loss, leaving the impact of other loss functions unexplored.
  - **What evidence would resolve it**: Conduct experiments using various loss functions in OGC and compare the results to determine if there is a significant difference in performance.

- **Open Question 2**: How does the performance of GGC and GGCM change when using different methods for generating the negative graph ¬A in the Inverse Graph Convolution (IGC) operator?
  - **Basis in paper**: [explicit] The paper mentions using negative sampling to generate the negative graph ¬A, but does not explore other methods for generating it.
  - **Why unresolved**: The paper only experiments with negative sampling, leaving the impact of other methods for generating the negative graph unexplored.
  - **What evidence would resolve it**: Conduct experiments using various methods for generating the negative graph in GGC and GGCM and compare the results to determine if there is a significant difference in performance.

- **Open Question 3**: How does the performance of OGC, GGC, and GGCM change when using different graph convolution operations besides the standard graph convolution?
  - **Basis in paper**: [explicit] The paper uses standard graph convolution in OGC, GGC, and GGCM, but does not explore other graph convolution operations.
  - **Why unresolved**: The paper only experiments with standard graph convolution, leaving the impact of other graph convolution operations unexplored.
  - **What evidence would resolve it**: Conduct experiments using various graph convolution operations in OGC, GGC, and GGCM and compare the results to determine if there is a significant difference in performance.

## Limitations
- The theoretical analysis assumes specific graph conditions that may not hold in real-world datasets
- The proposed methods' computational complexity could be prohibitive for large-scale graphs
- The performance gains on heterophilic graphs (where connected nodes may have different labels) remain unclear

## Confidence
- **High**: Claims about GCNs not jointly optimizing both loss terms at each layer (well-supported by mathematical analysis)
- **Medium**: Claims about OGC's superior performance (supported by experiments but limited to specific datasets)
- **Medium**: Claims about GGC/GGCM preserving graph structure (theoretical analysis is sound but practical validation is limited)

## Next Checks
1. Test the proposed methods on heterophilic graphs where the cluster assumption may not hold
2. Conduct ablation studies to isolate the contribution of joint optimization versus the IGC component
3. Measure computational complexity and compare it to traditional GCNs on large-scale graphs