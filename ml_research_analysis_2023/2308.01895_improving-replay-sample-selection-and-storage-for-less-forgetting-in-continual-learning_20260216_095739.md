---
ver: rpa2
title: Improving Replay Sample Selection and Storage for Less Forgetting in Continual
  Learning
arxiv_id: '2308.01895'
source_url: https://arxiv.org/abs/2308.01895
tags:
- buffer
- learning
- samples
- each
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in continual learning
  by improving experience replay through better sample selection and storage strategies.
  The authors compare reservoir sampling against three alternative population strategies
  (herding, GSS, and IPM) and propose two methods for determining optimal buffer sizes
  (intracluster variance and Kaiser criterion).
---

# Improving Replay Sample Selection and Storage for Less Forgetting in Continual Learning

## Quick Facts
- arXiv ID: 2308.01895
- Source URL: https://arxiv.org/abs/2308.01895
- Reference count: 40
- Key outcome: Strategic sample selection methods outperform reservoir sampling in reducing catastrophic forgetting

## Executive Summary
This paper addresses catastrophic forgetting in continual learning by investigating experience replay sample selection strategies. The authors compare reservoir sampling against three alternative population strategies (herding, GSS, and IPM) and propose two methods for determining optimal buffer sizes. Their experiments demonstrate that reservoir sampling creates unbalanced buffers favoring earlier tasks, leading to greater forgetting, particularly with smaller fixed buffer sizes. The study shows that strategic population strategies consistently outperform reservoir sampling across multiple datasets and training methods, with dynamic buffer sizing providing additional benefits.

## Method Summary
The paper compares four experience replay sample selection strategies: reservoir sampling, herding, gradient-based sample selection (GSS), and information projection maximization (IPM). Reservoir sampling uses random selection with replacement, while herding and IPM actively select samples that best represent class distributions in feature space. The authors also propose two dynamic buffer sizing methods: intracluster variance and Kaiser criterion. Experiments test these strategies with ER, DER, GDumb, and ER-ACE training methods across split-CIFAR10, split-CIFAR100, and split-TinyImageNet datasets, measuring final average accuracy (FAA) and final forgetting (FF).

## Key Results
- Reservoir sampling creates imbalanced buffers favoring earlier tasks, leading to greater forgetting compared to strategic approaches
- Dynamic buffer sizing using intracluster variance and Kaiser criterion improves performance across all population strategies
- Herding and IPM consistently outperform reservoir sampling in final average accuracy and final forgetting metrics
- The Kaiser criterion shows particular effectiveness for determining optimal buffer sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reservoir sampling creates imbalanced buffers favoring earlier tasks, leading to greater forgetting
- Mechanism: Random sampling with replacement naturally favors earlier tasks when buffer is saturated, causing later tasks to be underrepresented
- Core assumption: Buffer size is fixed and smaller relative to task complexity
- Evidence anchors:
  - "In practice, reservoir sampling tends to favor the storage of samples from earlier encountered tasks. This causes the minimal storage of samples from downstream tasks and an overall unbalanced fixed buffer leading to greater forgetting, particularly when the fixed buffer size is small."
  - Table 3 shows percentage breakdown of samples from each task in reservoir sampling showing bias toward earlier tasks
- Break condition: Buffer size becomes sufficiently large that imbalance is negligible, or dynamic buffer sizing is implemented

### Mechanism 2
- Claim: Strategic sampling methods (herding, IPM) reduce forgetting by selecting most informative samples based on feature space
- Mechanism: These methods actively select samples that best represent class distributions or maximize gradient diversity, rather than random selection
- Core assumption: Feature space learned at task completion provides meaningful representation of class distributions
- Evidence anchors:
  - "Herding seeks to store samples that best represent the sample's class mean in feature space" and "IPM seeks to find the most informative data points for storage in M"
  - Table 2 shows herding and IPM consistently outperform reservoir sampling in final forgetting metrics across multiple buffer sizes
- Break condition: Feature space learning is poor or class distributions are highly overlapping, making strategic selection ineffective

### Mechanism 3
- Claim: Dynamic buffer sizing based on dataset complexity reduces forgetting by allocating appropriate storage per class
- Mechanism: Kaiser criterion and intracluster variance determine optimal samples per class based on eigenvalue analysis and cluster compactness
- Core assumption: Number of significant eigenvalues or cluster variance indicates complexity and sample requirements for representation
- Evidence anchors:
  - "We observe that increasing the number of clusters beyond the knee of the curve... provides diminishing gains" and "The Kaiser criterion... states that only those with eigenvalues greater than 1.0 should be retained"
  - Table 1 shows Kaiser criterion statistics for different datasets
  - "Dynamic buffers seem to benefit most when paired with reservoir sampling and IPM"
- Break condition: Complexity metrics don't correlate with actual sample requirements, or computational cost outweighs benefits

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The paper's core problem is addressing catastrophic forgetting in continual learning
  - Quick check question: Why do neural networks typically forget previously learned tasks when training on new ones?

- Concept: Experience replay in continual learning
  - Why needed here: The paper builds upon experience replay methods and proposes improvements to sample selection
  - Quick check question: How does experience replay help mitigate catastrophic forgetting?

- Concept: Reservoir sampling algorithm
  - Why needed here: The paper compares reservoir sampling against alternative population strategies
  - Quick check question: What is the probability that a sample from task t is stored in a reservoir of size M after observing n samples?

## Architecture Onboarding

- Component map: Experience replay buffer → Sample selection strategy → Training method (ER, DER, GDumb, ER-ACE) → Evaluation metrics (FAA, FF)
- Critical path: Data stream → Sample selection (reservoir/herding/GSS/IPM) → Buffer population → Replay during training → Performance evaluation
- Design tradeoffs: Fixed buffer size vs dynamic sizing (simplicity vs optimality), random vs strategic sampling (computational efficiency vs effectiveness), feature-based vs gradient-based selection (representation quality vs optimization focus)
- Failure signatures: High forgetting rates, unbalanced buffer representation, poor performance on later tasks, computational inefficiency
- First 3 experiments:
  1. Implement reservoir sampling with ER and measure FAA/FF on split-CIFAR10 with fixed buffer sizes
  2. Implement herding strategy with same configuration and compare performance
  3. Implement dynamic buffer with Kaiser criterion and compare against fixed buffer results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different memory population strategies compare when applied to unsupervised continual learning scenarios?
- Basis in paper: The paper focuses exclusively on supervised continual learning with labeled data for replay methods.
- Why unresolved: The authors only test population strategies in supervised settings with cross-entropy loss, leaving open how these methods would perform when labels are unavailable or unreliable.
- What evidence would resolve it: Experiments applying each population strategy to unsupervised continual learning tasks would reveal whether strategic sampling provides similar benefits without supervision.

### Open Question 2
- Question: What is the relationship between buffer size, task difficulty, and catastrophic forgetting across different datasets?
- Basis in paper: The paper tests fixed buffer sizes but doesn't systematically analyze how buffer size requirements scale with task complexity or dataset difficulty.
- Why unresolved: While the paper proposes dynamic buffers based on complexity metrics, it doesn't establish a clear relationship between dataset complexity, number of tasks, and optimal buffer sizing.
- What evidence would resolve it: A systematic study varying both buffer sizes and dataset complexity across multiple domains, measuring forgetting as a function of buffer/task complexity ratios.

### Open Question 3
- Question: How do population strategies affect the quality of learned representations over time in continual learning?
- Basis in paper: The paper focuses on classification accuracy and forgetting metrics but doesn't analyze how different population strategies impact representation learning quality or feature space evolution.
- Why unresolved: While the authors mention that herding and IPM rely on "well learned features," they don't investigate how population strategies affect the learned representations' ability to capture task-specific and task-agnostic information.
- What evidence would resolve it: Analysis of feature space properties across different population strategies throughout the continual learning process.

## Limitations

- Findings are based primarily on image classification tasks using ResNet18, limiting generalizability to other domains or architectures
- Computational overhead of dynamic buffer sizing methods is not thoroughly analyzed
- The paper doesn't explore potential mitigation strategies beyond dynamic buffer sizing for reservoir sampling's bias toward earlier tasks

## Confidence

- High confidence in the experimental results showing strategic population strategies outperform reservoir sampling
- Medium confidence in the mechanism explanations for why reservoir sampling leads to greater forgetting
- Medium confidence in the dynamic buffer sizing benefits, as some results show inconsistent improvements across different dataset-task splits

## Next Checks

1. Test the population strategies with different neural network architectures (e.g., transformers, RNNs) to verify generalization beyond ResNet18
2. Conduct ablation studies isolating the impact of each population strategy from the training method to better understand their individual contributions
3. Analyze the computational overhead of dynamic buffer sizing methods across different dataset sizes to evaluate practical applicability