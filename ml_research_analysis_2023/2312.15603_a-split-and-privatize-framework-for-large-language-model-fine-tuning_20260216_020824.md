---
ver: rpa2
title: A Split-and-Privatize Framework for Large Language Model Fine-Tuning
arxiv_id: '2312.15603'
source_url: https://arxiv.org/abs/2312.15603
tags:
- privacy
- framework
- customer
- bottom
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Split-and-Privatize (SAP) framework for privacy-preserving
  fine-tuning of large language models in Model-as-a-Service scenarios. The framework
  splits the model between vendor and customer, with the customer perturbing representations
  using differential privacy techniques before sending them to the vendor.
---

# A Split-and-Privatize Framework for Large Language Model Fine-Tuning

## Quick Facts
- arXiv ID: 2312.15603
- Source URL: https://arxiv.org/abs/2312.15603
- Authors: 
- Reference count: 38
- Primary result: Achieves 62% empirical privacy improvement at 1% model performance degradation cost in Model-as-a-Service fine-tuning scenarios

## Executive Summary
This paper introduces the Split-and-Privatize (SAP) framework for privacy-preserving fine-tuning of large language models in Model-as-a-Service scenarios. The framework splits the pre-trained model between vendor and customer, with the customer applying differential privacy techniques to perturb representations before sending them to the vendor. A contributing-token-identification method is introduced to improve the utility-privacy trade-off by selectively reducing perturbation on task-relevant tokens. Experiments on four NLP tasks demonstrate that SAP achieves significant privacy improvements while maintaining competitive performance.

## Method Summary
The SAP framework divides the pre-trained language model into a bottom model held by the customer and a top model held by the vendor. The customer processes input data through their bottom model, applies differential privacy mechanisms to perturb the text representations, and sends these privatized representations to the vendor. The vendor processes the representations through their top model to make predictions and perform fine-tuning updates. To improve the utility-privacy trade-off, the framework employs contributing-token-identification (CTI) to identify and preserve task-relevant tokens while applying stronger perturbation to less important tokens. Parameter-efficient fine-tuning is implemented using LoRA modules on the vendor's side.

## Key Results
- SAP achieves 62% empirical privacy improvement (reduction in attack success rate) while maintaining model performance within 1% of baseline
- CTI method significantly improves utility by reducing perturbation on task-relevant tokens, with optimal performance when contributing tokens account for 1% of all tokens
- The framework provides comprehensive privacy protection for both model parameters (vendor-side) and training data (customer-side) in Model-as-a-Service scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Splitting the model reduces privacy risk while maintaining performance
- Mechanism: By dividing the PLM into a bottom model (customer) and top model (vendor), the customer only processes input data locally, reducing the risk of data exposure to the vendor. The top model remains secure with the vendor, protecting model parameters from customer access.
- Core assumption: The split position is chosen such that the bottom model provides sufficient context for the top model to maintain task performance while minimizing data exposure risk.
- Evidence anchors:
  - [abstract] "The proposed SAP framework is sufficiently investigated by experiments, and the results indicate that it can enhance the empirical privacy by 62% at the cost of 1% model performance degradation"
  - [section 4.2] "The choice of which layer to split the model is an important option in SAP"
  - [corpus] Found 25 related papers, indicating significant research interest in model splitting and privacy preservation, though specific evidence for this exact mechanism is limited
- Break condition: If the split position is poorly chosen, either the top model cannot recover task-relevant information or the bottom model exposes too much data to the vendor.

### Mechanism 2
- Claim: Text privatization through differential privacy techniques effectively protects data privacy
- Mechanism: The customer applies differential privacy mechanisms to perturb text representations before sending them to the vendor. This adds noise to the representations, making it difficult for the vendor to recover the original input text while maintaining task-relevant information.
- Core assumption: The differential privacy mechanism provides sufficient noise to protect privacy without destroying the utility of the representations for the downstream task.
- Evidence anchors:
  - [abstract] "the customer perturbs representations using differential privacy techniques before sending them to the vendor"
  - [section 4.3] "To protect data privacy, it is necessary for the customer to employ privatization mechanisms to perturb text representations"
  - [corpus] Related works on differential privacy for NLP show this is a well-established approach, though specific effectiveness varies by task
- Break condition: If too much noise is added, the representations become unusable for the task. If too little noise is added, the privacy protection becomes insufficient.

### Mechanism 3
- Claim: Contributing-Token-Identification (CTI) improves the utility-privacy trade-off
- Mechanism: CTI identifies tokens that contribute most to task performance and reduces perturbation on these tokens, preserving more task-relevant information while maintaining overall privacy protection.
- Core assumption: A small subset of tokens (less than 1%) carries most of the task-relevant information, and preserving these while perturbing others maintains performance with minimal privacy loss.
- Evidence anchors:
  - [section 4.4] "By reducing the perturbation to a small number of token representations that are strongly related to the utility task, we significantly improve the utility performance while maintaining a similar level of privacy"
  - [section 5.2] "When the contributing tokens account for only 1% of all tokens, the CTI method can significantly enhance the practicality of LLM on the downstream task while maintaining a similar level of privacy protection"
  - [corpus] Limited direct evidence in corpus for this specific CTI mechanism
- Break condition: If the token importance analysis is inaccurate or the wrong tokens are preserved, performance may degrade despite the CTI approach.

## Foundational Learning

- Concept: Differential Privacy
  - Why needed here: Provides the theoretical foundation for adding noise to protect data privacy while maintaining utility
  - Quick check question: What is the relationship between privacy parameter η and the amount of noise added in differential privacy?

- Concept: Split Learning Architecture
  - Why needed here: Forms the basis for dividing the model between customer and vendor while enabling collaborative training
  - Quick check question: How does split learning differ from federated learning in terms of data flow and privacy guarantees?

- Concept: Term Frequency-Inverse Document Frequency (TF-IDF)
  - Why needed here: Inspires the utility importance metric used in CTI to identify task-relevant tokens
  - Quick check question: How does TF-IDF measure the importance of words, and how is this concept adapted in the utility importance metric?

## Architecture Onboarding

- Component map:
  - Customer: Bottom model, private dataset, limited computation resources, differential privacy mechanism, CTI module
  - Vendor: Top model, server resources, core functions, LoRA module
  - Communication channel: Text representations between bottom and top model

- Critical path:
  1. Model split and deployment
  2. Customer processes input through bottom model
  3. Customer applies privacy-preserving perturbation
  4. Customer sends representations to vendor
  5. Vendor processes through top model
  6. Vendor computes gradients and updates parameters
  7. Repeat for multiple epochs

- Design tradeoffs:
  - Split position: More layers in bottom model improves privacy but increases customer computational burden
  - Privacy parameter η: Higher values improve privacy but reduce utility
  - CTI threshold: More contributing tokens improve utility but reduce privacy
  - Communication frequency: More frequent updates improve convergence but increase communication overhead

- Failure signatures:
  - High attack success rate indicates insufficient privacy protection
  - Significant performance degradation indicates excessive perturbation
  - Communication failures between customer and vendor
  - Convergence issues in fine-tuning process

- First 3 experiments:
  1. Test different split positions (embedding only vs embedding + 2 layers vs embedding + 4 layers) on a simple sentiment analysis task
  2. Vary the privacy parameter η and measure the trade-off between empirical privacy and utility accuracy
  3. Implement CTI with different threshold values (0.5%, 1%, 2% of tokens) to find optimal balance between utility and privacy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal split position between the vendor's top model and the customer's bottom model to balance model performance and data privacy protection?
- Basis in paper: [explicit] The paper discusses that the choice of split position is an important consideration, noting that splitting after the embedding layer is easier to implement but offers less privacy protection, while splitting after more encoder blocks provides better privacy but may reduce model performance. The authors conclude that splitting after 6 encoder blocks (a quarter of the model) offers a good balance, but acknowledge this needs further investigation.
- Why unresolved: The paper only tested splitting after 1-8 encoder blocks and found diminishing returns beyond 6 blocks. The optimal split position likely depends on factors like the specific task, model architecture, and privacy requirements, which weren't fully explored.
- What evidence would resolve it: Systematic experiments varying the split position across different tasks, model sizes, and privacy budgets to identify generalizable patterns in the trade-off between performance and privacy protection.

### Open Question 2
- Question: How does the proposed CTI method perform on tasks that require understanding of global sentence semantics rather than individual token contributions?
- Basis in paper: [explicit] The paper notes that the CTI method, which identifies task-relevant tokens based on token-level utility importance, did not yield significant improvements on the MRPC dataset (semantic equivalence judgment task). The authors suggest this is because the method operates at the token level while these tasks rely more on overall sentence semantics.
- Why unresolved: The paper only tested CTI on a few classification tasks and didn't explore alternative approaches for tasks requiring semantic understanding, such as phrase-level or sentence-level importance identification methods.
- What evidence would resolve it: Comparative experiments applying CTI versus alternative token/phrase/sentence importance identification methods across diverse tasks requiring different levels of semantic understanding, measuring both utility and privacy performance.

### Open Question 3
- Question: How does the proposed SAP framework perform in real-world deployment scenarios with non-IID data distributions and dynamic customer populations?
- Basis in paper: [inferred] The paper primarily evaluates SAP in controlled experimental settings with standard benchmark datasets. It doesn't address how the framework would perform with the heterogeneity and dynamics typical of real-world MaaS scenarios, where customers may have non-IID data distributions and the population of customers may change over time.
- Why unresolved: The experimental evaluation focused on static datasets with balanced class distributions and didn't simulate the practical challenges of deploying SAP across diverse, dynamic customer populations with varying data characteristics.
- What evidence would resolve it: Deployment studies or simulations involving heterogeneous customer populations with non-IID data distributions, measuring how SAP's privacy protection and utility performance scales and adapts to real-world conditions over time.

## Limitations

- The framework's effectiveness is highly dependent on the choice of split position, which remains task-specific and requires careful tuning
- Privacy evaluation relies on specific attack methods (embedding inversion and attribute inference) that may not capture all potential vulnerabilities
- Real-world deployment challenges such as communication failures, network latency, and resource constraints on customer devices are not addressed

## Confidence

**High confidence**: The differential privacy mechanism for text privatization is well-established in the literature and the theoretical foundation is sound. The implementation of LoRA for parameter-efficient fine-tuning is standard practice with proven effectiveness.

**Medium confidence**: The utility-privacy trade-off improvements through CTI are demonstrated empirically but rely on assumptions about token importance that may not generalize across all tasks or domains. The 62% privacy improvement claim is specific to the experimental conditions tested.

**Low confidence**: The generalizability of results across different model architectures (beyond Roberta-Large), different task types (beyond the four NLP tasks tested), and different deployment scenarios (beyond the controlled experimental setup) remains uncertain.

## Next Checks

1. **Cross-task split position optimization**: Systematically evaluate the optimal split position across at least 10 diverse NLP tasks (including question answering, text classification, and sequence labeling) to determine if the three positions tested are truly optimal or task-specific. This would validate whether the split position can be automated or requires task-specific tuning.

2. **Comprehensive attack evaluation**: Implement and evaluate additional privacy attacks including membership inference attacks, model inversion attacks, and black-box attacks that do not require direct access to intermediate representations. This would provide a more complete picture of the framework's privacy guarantees under various threat models.

3. **Resource-constrained customer simulation**: Conduct experiments where the customer has severely limited computational resources (e.g., mobile devices) to evaluate the practical feasibility of the framework. Measure the trade-off between bottom model size, computational burden, and privacy protection to identify the minimum viable customer-side implementation.