---
ver: rpa2
title: SMDP-Based Dynamic Batching for Efficient Inference on GPU-Based Platforms
arxiv_id: '2301.12865'
source_url: https://arxiv.org/abs/2301.12865
tags:
- batch
- time
- batching
- state
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of dynamic batching in GPU-based
  inference services to balance efficiency and latency. The authors formulate this
  as a continuous-time average-cost problem modeled as a semi-Markov decision process
  (SMDP) with the objective of minimizing the weighted sum of average response time
  and average power consumption.
---

# SMDP-Based Dynamic Batching for Efficient Inference on GPU-Based Platforms

## Quick Facts
- **arXiv ID:** 2301.12865
- **Source URL:** https://arxiv.org/abs/2301.12865
- **Reference count:** 26
- **Key outcome:** Achieves up to 63.5% space and 98% time complexity reduction while outperforming benchmark policies in balancing latency and energy consumption.

## Executive Summary
This paper addresses the challenge of dynamic batching in GPU-based inference services, where the goal is to balance efficiency and latency by finding the optimal batch size at each service round. The authors formulate this as a continuous-time average-cost problem modeled as a semi-Markov decision process (SMDP) with the objective of minimizing the weighted sum of average response time and average power consumption. To solve the infinite state SMDP efficiently, they introduce a finite state approximation with an abstract cost, followed by discretization and relative value iteration. The resulting solution achieves significant complexity reduction while maintaining high accuracy and outperforming existing batching strategies.

## Method Summary
The method formulates dynamic batching as an SMDP where states represent the number of requests in the system and actions represent batch sizes. The SMDP is transformed into a discrete-time MDP using a discretization parameter η, then solved using relative value iteration. To handle the infinite state space, a finite state approximation is used with an abstract cost for tail states. The solution finds optimal policies that adapt to different traffic intensities and parameter weights, achieving a balance between latency and energy efficiency.

## Key Results
- Achieves up to 63.5% space and 98% time complexity reduction compared to solving the full SMDP
- Optimal policies exhibit control limit structure, adapting to traffic intensity and parameter weights
- Outperforms benchmark policies (work-conserving, static batching with b=8,16,32) in balancing latency and energy consumption

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dynamic batching reduces latency and energy consumption by finding the optimal batch size at each service round.
- **Mechanism:** The system is modeled as a semi-Markov decision process (SMDP) where the optimal action at each decision point is to batch requests up to a control limit. This control limit adapts to traffic intensity and parameter weights.
- **Core assumption:** Batch processing time is deterministic and linear with batch size; arrival follows a Poisson process.
- **Evidence anchors:**
  - [abstract] "The GPU-based inference service is modeled as a batch service queue with batch-size dependent processing time."
  - [section II] "the batch processing time is a deterministic value τ[b] and is linear with the batch size b, given by τ[b] = αb + τ0"
  - [section II] "the arrival of requests follows a Poisson process"
- **Break condition:** If processing time is non-linear or non-deterministic, the SMDP formulation may no longer be optimal.

### Mechanism 2
- **Claim:** Finite state approximation with an abstract cost reduces computational complexity while maintaining solution accuracy.
- **Mechanism:** Truncate the infinite state space to a finite one by aggregating "tail" states into an overflow state So. Introduce an abstract cost to estimate the holding cost difference between So and the largest finite state.
- **Core assumption:** The tail probability decreases with smax, making tail states negligible.
- **Evidence anchors:**
  - [section IV-A] "The SMDP problem has infinite states in S = {0, 1, 2, ...}, and is impractical to be solved by numerical methods. Hence, we truncate the infinite state space to a finite state space S′"
  - [section IV-A] "We introduce an abstract cost coy(s, a) to the 'overflow' state So, working as an estimation of the difference between the expected holding cost at 'tail' states and the holding cost at smax."
- **Break condition:** If tail states have significant probability mass even for large smax, the approximation error may exceed acceptable bounds.

### Mechanism 3
- **Claim:** The SMDP-based solution outperforms static batching and work-conserving policies in balancing latency and energy efficiency.
- **Mechanism:** By solving the SMDP with the relative value iteration algorithm, the system finds policies that adapt to different traffic intensities and parameter weights, achieving optimal tradeoff curves.
- **Core assumption:** The SMDP solution converges to an average optimal stationary deterministic policy.
- **Evidence anchors:**
  - [abstract] "Numerical results show that SMDP-based batching policies can adapt to different traffic intensities and outperform other benchmark policies."
  - [section V-B] "The comparison with other policies has validated the superiority of the SMDP solution."
  - [section V-A] "The observation is that the optimal policies potentially have a control limit structure."
- **Break condition:** If the system dynamics change significantly (e.g., non-Poisson arrivals), the benchmark comparison may no longer hold.

## Foundational Learning

- **Concept:** Semi-Markov Decision Process (SMDP)
  - Why needed here: To model the continuous-time dynamic batching problem where decision epochs are irregularly spaced.
  - Quick check question: How does an SMDP differ from a standard MDP in terms of state transitions and decision timing?

- **Concept:** Markov Decision Process (MDP) discretization
  - Why needed here: To transform the continuous-time SMDP into a discrete-time MDP for efficient numerical solution.
  - Quick check question: What is the role of the discretization parameter η in the transformed MDP?

- **Concept:** Relative Value Iteration (RVI)
  - Why needed here: To solve the average-cost MDP with a countable state space where standard value iteration is numerically unstable.
  - Quick check question: Why is relative value iteration preferred over standard value iteration for average-cost MDPs?

## Architecture Onboarding

- **Component map:** Request arrival -> State monitor -> Decision engine -> Batch processor -> Performance evaluator -> State monitor
- **Critical path:**
  1. Request arrival → State monitor update
  2. Decision engine solves SMDP → Optimal batch size decision
  3. Batch processor executes → State transition
  4. Performance evaluator updates metrics → Policy adjustment if needed
- **Design tradeoffs:**
  - Accuracy vs. complexity: Larger smax improves approximation but increases computational cost
  - Adaptability vs. responsiveness: More frequent SMDP solves allow better adaptation but increase overhead
  - Energy efficiency vs. latency: Higher batch sizes improve energy efficiency but increase latency
- **Failure signatures:**
  - Excessive latency: Batch size control limit set too high or traffic intensity underestimated
  - Energy inefficiency: Batch size control limit set too low or traffic intensity overestimated
  - System instability: Incorrect estimation of arrival rate λ or processing parameters α, τ0
- **First 3 experiments:**
  1. Validate the linearity of batch processing time and energy consumption with batch size
  2. Test the convergence of the relative value iteration algorithm for different discretization parameters η
  3. Evaluate the approximation accuracy and complexity tradeoff by varying smax and the abstract cost parameter co

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the control limit structure in the optimal policies vary with different neural network architectures beyond GoogLeNet?
- Basis in paper: [explicit] The paper mentions that the optimal policies potentially possess a control limit structure, but only tests with GoogLeNet.
- Why unresolved: The paper only tests with one neural network architecture, limiting generalizability.
- What evidence would resolve it: Experiments with multiple neural network architectures showing consistent or varying control limit structures.

### Open Question 2
- Question: Can the abstract cost approach be extended to handle hard Service-Level Objective (SLO) constraints rather than just average objectives?
- Basis in paper: [inferred] The paper notes its method only considers average objectives, not hard SLO constraints, and suggests the solution can function as a basic guideline needing modification for real-time constraints.
- Why unresolved: The paper focuses on average-cost optimization without incorporating constraint handling.
- What evidence would resolve it: Modified SMDP formulation with constraint handling and numerical results showing performance under hard SLO constraints.

### Open Question 3
- Question: What is the impact of using different service disciplines (e.g., priority queues) within the batch service queue model?
- Basis in paper: [explicit] The paper models the system as a batch service queue with batch-size dependent processing time but doesn't explore different service disciplines.
- Why unresolved: The paper assumes a first-come-first-served discipline without exploring alternatives.
- What evidence would resolve it: Comparative analysis of different service disciplines within the same SMDP framework.

## Limitations
- Assumes deterministic and linear batch processing time, which may not hold for all GPU models
- Finite state approximation introduces potential accuracy loss, though demonstrated to be acceptable for tested scenarios
- Only considers average objectives, not hard Service-Level Objective (SLO) constraints

## Confidence
- **High Confidence:** The mathematical formulation of the SMDP model and the relative value iteration solution method are sound and well-established.
- **Medium Confidence:** The approximation accuracy and complexity reduction claims are supported by numerical results, but may vary with different system parameters.
- **Medium Confidence:** The benchmark comparisons show superior performance, but are limited to specific workloads and parameter settings.

## Next Checks
1. **Generalization Testing:** Validate the SMDP-based batching policies across different GPU architectures and neural network models to assess robustness of the linear processing time assumption.
2. **Dynamic Parameter Adaptation:** Implement online estimation of arrival rates and processing parameters to evaluate the policy's performance under non-stationary traffic conditions.
3. **Real-World Deployment Analysis:** Conduct a pilot deployment in a production ML inference service to measure actual latency-energy tradeoffs and identify any practical limitations not captured in the numerical simulations.