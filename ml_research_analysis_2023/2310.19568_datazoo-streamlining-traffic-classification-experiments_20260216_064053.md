---
ver: rpa2
title: 'DataZoo: Streamlining Traffic Classification Experiments'
arxiv_id: '2310.19568'
source_url: https://arxiv.org/abs/2310.19568
tags:
- traffic
- classification
- datasets
- network
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DataZoo addresses the challenge of limited standardized datasets
  and tools for network traffic classification research. The toolset provides a unified
  Python API for accessing three extensive TLS and QUIC datasets (CESNET-TLS22, CESNET-QUIC22,
  CESNET-TLS-Year22) with up to 500 million samples.
---

# DataZoo: Streamlining Traffic Classification Experiments

## Quick Facts
- arXiv ID: 2310.19568
- Source URL: https://arxiv.org/abs/2310.19568
- Reference count: 39
- Key outcome: DataZoo provides a unified Python API for standardized network traffic classification experiments with time-aware and open-world evaluation capabilities

## Executive Summary
DataZoo addresses the challenge of limited standardized datasets and tools for network traffic classification research. The toolset provides a unified Python API for accessing three extensive TLS and QUIC datasets (CESNET-TLS22, CESNET-QUIC22, CESNET-TLS-Year22) with up to 500 million samples. It enables time-aware evaluation by allowing researchers to split data based on collection dates, capturing data drift phenomena. The tool also supports open-world evaluation scenarios where novel services can be introduced as unknown classes.

## Method Summary
DataZoo provides a unified Python API for accessing three large-scale TLS and QUIC datasets containing bidirectional network flows with packet sequences and flow statistics. The toolset enables time-aware evaluation through chronological data splitting and supports open-world evaluation by configuring known and unknown classes. It includes feature scaling options and provides data access through both PyTorch DataLoader and Pandas DataFrame formats. The method involves configuring dataset parameters, setting up train/validation/test splits, and evaluating classification models on standardized benchmarks.

## Key Results
- Enables time-aware evaluation by splitting data based on collection dates to capture data drift
- Supports open-world evaluation scenarios with novel service detection as unknown classes
- Provides feature scaling options and standardized data access through unified Python API

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Time-aware evaluation improves trust in model performance by revealing data drift
- Mechanism: By splitting data based on collection dates, models are trained on older data and tested on newer data, exposing performance degradation due to network changes
- Core assumption: Network traffic characteristics change over time due to factors like protocol updates and new services
- Evidence anchors: [abstract] "It enables time-aware evaluation by allowing researchers to split data based on collection dates, capturing data drift phenomena." [section] "The measured performance is visualized in Figure 1. Those drops were attributed to a change of the Google TLS server certificate."

### Mechanism 2
- Claim: Open-world evaluation reduces over-optimistic performance estimates by introducing novel classes
- Mechanism: The toolset allows splitting applications into known and unknown classes, simulating the appearance of new services not seen during training
- Core assumption: Real-world deployment will encounter services not present in the training data
- Evidence anchors: [abstract] "The tool also supports open-world evaluation scenarios where novel services can be introduced as unknown classes." [section] "The Internet is a dynamic environment, and new apps and services are emerging each week. Traffic classification should thus be evaluated in changing environments with novel (unseen in the training set) classes."

### Mechanism 3
- Claim: Standardized APIs and pre-processing reduce evaluation setup errors
- Mechanism: DataZoo provides a unified interface for accessing datasets and includes options for feature scaling and consistent train/validation/test splits
- Core assumption: Inconsistent data handling and preprocessing lead to errors in model evaluation
- Evidence anchors: [abstract] "DataZoo provides a unified Python API for accessing three extensive TLS and QUIC datasets... It enables time-aware evaluation by allowing researchers to split data based on collection dates." [section] "Mistakes in the evaluation setup, which can increase the measured ML model performance, are still prevalent in the network traffic classification field."

## Foundational Learning

- Concept: Time-aware evaluation
  - Why needed here: To capture data drift and ensure models perform well on future data
  - Quick check question: What happens to model performance when trained on older data and tested on newer data?

- Concept: Open-world evaluation
  - Why needed here: To simulate real-world deployment where new services appear
  - Quick check question: How does the model handle classes not seen during training?

- Concept: Feature scaling
  - Why needed here: To ensure models converge properly and are not biased by feature magnitude
  - Quick check question: Which models require feature scaling and which do not?

## Architecture Onboarding

- Component map: DataZoo toolset → DatasetConfig → CESNET datasets → PyTorch DataLoader / Pandas DataFrame
- Critical path: Initialize dataset → Configure DatasetConfig → Set up train/validation/test splits → Access data via DataLoader or DataFrame
- Design tradeoffs: Providing multiple dataset sizes allows faster experimentation but may lead to less representative results if not careful
- Failure signatures: Performance drops when using time-aware evaluation indicate data drift; high false positive rates in open-world evaluation suggest poor unknown class detection
- First 3 experiments:
  1. Use CESNET-QUIC22 XS dataset with time-aware evaluation to observe performance degradation over time
  2. Configure DatasetConfig to split applications into known and unknown classes and evaluate open-world performance
  3. Compare model performance with and without feature scaling to understand its impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal strategies for handling data drift in network traffic classification models when evaluating on multi-week or multi-year datasets?
- Basis in paper: [explicit] The paper discusses time-aware evaluation and data drift, mentioning performance drops up to 14% between different weeks of the CESNET-QUIC22 dataset due to changes like TLS server certificate updates
- Why unresolved: While the paper identifies the importance of time-aware evaluation and mentions performance drops due to data drift, it does not provide specific strategies or methods for handling or mitigating these drift effects in classification models
- What evidence would resolve it: Empirical studies comparing different training strategies (e.g., weighted training dates, continuous learning approaches) on multi-week/year datasets showing their effectiveness in maintaining consistent performance across time periods

### Open Question 2
- Question: How can open-world evaluation scenarios be standardized across different network traffic classification research to enable proper cross-comparison of methods?
- Basis in paper: [explicit] The paper highlights the lack of standardization in open-world evaluation, noting that each proposal uses different datasets and is prone to mistakes. It mentions that proper cross-comparison is impossible in the current state
- Why unresolved: Despite the paper's introduction of multiple approaches to split classes between known and unknown, there is no consensus or standardized methodology for open-world evaluation in the field
- What evidence would resolve it: Development and adoption of a common benchmark dataset with standardized protocols for open-world evaluation, along with comparative studies showing consistent performance metrics across different methods

### Open Question 3
- Question: What are the most effective feature scaling techniques for different types of network traffic classification models when dealing with packet sequences and flow statistics?
- Basis in paper: [explicit] The paper provides feature scaling options (StandardScaler, RobustScaler, MinMaxScaler) and mentions that scaling is recommended for some models like neural networks but not required for others like XGBoost
- Why unresolved: The paper offers multiple scaling options but does not evaluate or recommend specific techniques for different model types or provide guidance on when to use each scaler based on the nature of the traffic data
- What evidence would resolve it: Systematic evaluation of different scaling techniques across various model architectures and traffic classification tasks, with recommendations based on performance metrics and data characteristics

## Limitations

- Limited empirical validation of open-world evaluation claims
- No detailed hyperparameter settings provided for reproducibility
- Unclear handling of edge cases in time-aware data splits

## Confidence

- Time-aware evaluation claims: Medium confidence (moderate evidence from single dataset observation)
- Open-world evaluation claims: Low confidence (no direct experimental validation provided)
- Standardization benefits: Medium confidence (logically sound but not empirically demonstrated)

## Next Checks

1. Replicate time-aware evaluation across multiple time periods to verify consistency of data drift observations
2. Implement open-world evaluation with controlled introduction of novel services to measure false positive rates
3. Test feature scaling impact across different model families (neural networks vs. tree-based methods) to identify specific requirements