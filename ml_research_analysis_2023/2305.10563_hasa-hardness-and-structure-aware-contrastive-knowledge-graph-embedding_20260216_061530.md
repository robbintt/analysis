---
ver: rpa2
title: 'HaSa: Hardness and Structure-Aware Contrastive Knowledge Graph Embedding'
arxiv_id: '2305.10563'
source_url: https://arxiv.org/abs/2305.10563
tags:
- negative
- triples
- hard
- infonce
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the issue of false negative samples in contrastive
  knowledge graph embedding (KGE) methods. The authors propose HaSa (Hardness and
  Structure-aware), a method that generates hard negative triples while mitigating
  the impact of false negatives using graph structure information.
---

# HaSa: Hardness and Structure-Aware Contrastive Knowledge Graph Embedding

## Quick Facts
- arXiv ID: 2305.10563
- Source URL: https://arxiv.org/abs/2305.10563
- Authors: 
- Reference count: 37
- Key outcome: HaSa achieves state-of-the-art MRR of 0.568 on WN18RR and competitive MRR of 0.304 on FB15k-237 by generating hard negatives while mitigating false negatives using graph structure

## Executive Summary
This paper addresses the challenge of false negative samples in contrastive knowledge graph embedding methods. The authors propose HaSa (Hardness and Structure-aware), a method that generates hard negative triples while mitigating the impact of false negatives using graph structure information. By modifying the InfoNCE loss to explicitly account for negative sample distribution and incorporating shortest path length to distinguish true from false negatives, HaSa achieves state-of-the-art performance on WN18RR and competitive results on FB15k-237. The method also demonstrates that minimizing InfoNCE loss with hard negatives maximizes the KL-divergence between positive and negative triple embeddings.

## Method Summary
HaSa modifies the standard InfoNCE loss to incorporate a negative sample distribution that considers the context (p(t|ehr)) instead of simple frequency-based distribution (p(t)). This generates harder negative triples by giving higher preference to tail entities whose embeddings are close to the context embedding ehr. To mitigate false negatives (which are more likely among hard negatives), HaSa uses the graph structure of the KG, specifically the shortest path length between head and tail entities, to distinguish true from false negatives. The improved HaSa+ method further enhances performance by considering negative contexts in addition to negative tails, enabling bi-directional contrasting.

## Key Results
- HaSa achieves state-of-the-art MRR of 0.568 on WN18RR (Hit@1: 0.461, Hit@10: 0.733)
- On FB15k-237, HaSa achieves MRR of 0.304 (comparable to other methods)
- HaSa+ further improves performance on WN18RR with MRR of 0.577 (Hit@1: 0.469, Hit@10: 0.744)
- The method outperforms both classic KGE methods and pre-trained LM-based KGE methods on benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed HaSa method improves KGE performance by explicitly accounting for the negative sample distribution in InfoNCE loss.
- Mechanism: The authors modify the standard InfoNCE loss to incorporate a negative sample distribution that considers the context, allowing the model to generate harder negative triples. This is achieved by using a distribution p(t|ehr) that gives higher preference to tail entities whose embeddings are close to the context embedding ehr.
- Core assumption: The effectiveness of contrastive learning depends on the quality of negative samples, and incorporating the context into the negative sample distribution leads to harder and more informative negatives.
- Evidence anchors:
  - [abstract] The paper modifies the InfoNCE loss to explicitly account for the negative sample distribution.
  - [section 4] The authors propose using p(t|ehr) instead of a simple frequency-based distribution p(t) for generating negative triples.

### Mechanism 2
- Claim: The proposed debiased InfoNCE loss with hard negative triples (HaSa) mitigates the impact of false negative triples on KGE performance.
- Mechanism: The authors recognize that hard negative triples are more likely to be false negatives (factual triples). To address this, they use the graph structure of the KG to distinguish true from false negatives. Specifically, they leverage the shortest path length between head and tail entities, assuming that false negatives tend to have smaller shortest path lengths.
- Evidence anchors:
  - [section 5] The authors show that hard negative triples generated by p(t|ehr) produce more false negatives than simple negative triples generated by p(t).
  - [section 5.1] The authors demonstrate that false negative triples tend to have smaller shortest path lengths compared to true negative triples.

### Mechanism 3
- Claim: The improved HaSa+ method further enhances KGE performance by considering negative contexts in addition to negative tails.
- Mechanism: In addition to generating hard negative triples (h,r,t-), the authors propose generating negative contexts (h-,r-,t) to provide bi-directional contrasting. This allows the model to learn better representations by considering both positive and negative samples in both the tail and context dimensions.
- Evidence anchors:
  - [section 6.1] The authors describe the HaSa+ method which modifies the debiased InfoNCE loss to include negative contexts.
  - [section 7.2] The experimental results show that HaSa+ achieves better performance compared to HaSa and other KGE methods on the WN18RR dataset.

## Foundational Learning

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: The proposed HaSa method is based on contrastive learning using the InfoNCE loss. Understanding the basics of contrastive learning and how InfoNCE loss works is crucial for grasping the motivation and mechanism behind HaSa.
  - Quick check question: What is the key idea behind contrastive learning, and how does InfoNCE loss encourage the model to learn discriminative representations?

- Concept: Knowledge graph embeddings and link prediction
  - Why needed here: The ultimate goal of HaSa is to improve knowledge graph embeddings for the link prediction task. Familiarity with KGE methods and the link prediction problem is necessary to understand the context and significance of the proposed approach.
  - Quick check question: What is the objective of knowledge graph embeddings, and how is the link prediction task typically formulated in KGE?

- Concept: Graph structure and shortest path length
  - Why needed here: The HaSa method leverages the graph structure of the knowledge graph to distinguish true from false negatives. Understanding how to represent and analyze graph structure, including concepts like shortest path length, is important for comprehending this aspect of the approach.
  - Quick check question: How can the shortest path length between two entities in a graph provide insights into their relationship or similarity?

## Architecture Onboarding

- Component map:
  - InfoNCE loss with negative sample distribution
  - Hard negative triple generation using p(t|ehr)
  - False negative detection using shortest path length
  - Debiased InfoNCE loss (HaSa)
  - Bi-directional contrasting (HaSa+)

- Critical path:
  1. Generate negative triples using p(t|ehr)
  2. Identify potential false negatives using shortest path length
  3. Apply debiased InfoNCE loss (HaSa)
  4. Optionally, include negative contexts for bi-directional contrasting (HaSa+)

- Design tradeoffs:
  - Hard negatives vs. simple negatives: Hard negatives are more informative but also more prone to being false negatives. The proposed method balances this tradeoff by using the graph structure to mitigate the impact of false negatives.
  - Shortest path length vs. other graph metrics: The authors chose shortest path length as a simple and effective metric for distinguishing true from false negatives. However, other graph metrics could potentially be explored for further improvements.

- Failure signatures:
  - If the negative sample distribution does not generate harder negatives, the method may not outperform simpler approaches.
  - If the shortest path length is not a reliable indicator of true vs. false negatives, the debiasing mechanism may fail to mitigate the impact of false negatives.
  - If the bi-directional contrasting does not provide additional useful information, the HaSa+ method may not show further improvements over HaSa.

- First 3 experiments:
  1. Compare the performance of InfoNCE with simple negatives (p(t)) vs. hard negatives (p(t|ehr)) on a small subset of the dataset.
  2. Analyze the distribution of shortest path lengths for true and false negatives generated by p(t|ehr) to validate the proposed debiasing mechanism.
  3. Implement the debiased InfoNCE loss (HaSa) and evaluate its performance on a validation set compared to the standard InfoNCE loss with hard negatives.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical relationship between false negative triples and downstream task performance in knowledge graph embedding?
- Basis in paper: [explicit] The paper mentions that false negative triples "reduce downstream task performance" but doesn't provide a theoretical analysis of this relationship.
- Why unresolved: While the paper demonstrates empirically that hard negatives lead to more false negatives, it doesn't establish a theoretical framework explaining how the presence of false negatives quantitatively affects embedding quality and downstream task metrics.
- What evidence would resolve it: A theoretical framework showing the relationship between false negative triple ratio, embedding quality degradation, and specific downstream task performance metrics would be needed.

### Open Question 2
- Question: Are there alternative graph metrics besides shortest path length that could be more effective at distinguishing true from false negative triples?
- Basis in paper: [inferred] The paper uses shortest path length to distinguish true from false negatives but notes this is one approach without exploring alternatives.
- Why unresolved: The paper demonstrates shortest path length is useful but doesn't systematically compare it against other graph metrics (e.g., PageRank, clustering coefficient, betweenness centrality) for this specific purpose.
- What evidence would resolve it: Comparative experiments testing multiple graph metrics for false negative detection on various knowledge graph datasets would be needed.

### Open Question 3
- Question: How does the optimal hyperparameter τ (false negative probability) vary across different knowledge graph characteristics and what determines this variation?
- Basis in paper: [explicit] The paper performs ablation studies on τ but doesn't explain why optimal values differ between WN18RR (τ=2e-05) and FB15k-237 (τ=1e-04).
- Why unresolved: While optimal τ values are found through ablation, the paper doesn't provide a principled explanation for why these values differ between datasets with different characteristics (density, average degree, etc.).
- What evidence would resolve it: A theoretical or empirical analysis linking knowledge graph properties (e.g., density, entity distribution, relation types) to optimal τ values would be needed.

## Limitations
- The effectiveness of shortest path length as a discriminator for false negatives needs further validation, particularly on graphs with varying density and connectivity patterns
- The computational overhead of calculating shortest paths for all negative samples during training is not quantified
- The method's performance on larger, more complex knowledge graphs beyond the two benchmark datasets remains unknown

## Confidence
- High confidence: The core mechanism of modifying InfoNCE loss to incorporate negative sample distribution is well-founded and experimentally validated
- Medium confidence: The use of shortest path length to distinguish false negatives is theoretically sound but requires more extensive validation across diverse graph structures
- Medium confidence: The performance improvements on benchmark datasets are demonstrated, but real-world applicability needs further testing

## Next Checks
1. Conduct ablation studies systematically removing each component (negative sampling strategy, shortest path debiasing, bi-directional contrasting) to quantify individual contributions
2. Test the method on knowledge graphs with different characteristics (dense vs. sparse, hierarchical vs. flat) to assess generalizability
3. Measure and report the computational overhead introduced by shortest path calculations compared to standard negative sampling approaches