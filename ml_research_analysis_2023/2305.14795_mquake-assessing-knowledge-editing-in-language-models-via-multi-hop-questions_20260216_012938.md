---
ver: rpa2
title: 'MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions'
arxiv_id: '2305.14795'
source_url: https://arxiv.org/abs/2305.14795
tags:
- facts
- edited
- questions
- editing
- multi-hop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating knowledge editing
  methods for large language models (LLMs), focusing on whether edited models can
  correctly answer multi-hop questions that are entailed consequences of edited facts.
  Existing evaluation paradigms mainly validate recall of edited facts, but fail to
  assess whether the model can leverage edited knowledge to answer related questions.
---

# MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions

## Quick Facts
- arXiv ID: 2305.14795
- Source URL: https://arxiv.org/abs/2305.14795
- Reference count: 13
- Knowledge editing methods fail catastrophically on multi-hop questions despite high edit-wise accuracy

## Executive Summary
This paper addresses the challenge of evaluating knowledge editing methods for large language models (LLMs), focusing on whether edited models can correctly answer multi-hop questions that are entailed consequences of edited facts. Existing evaluation paradigms mainly validate recall of edited facts, but fail to assess whether the model can leverage edited knowledge to answer related questions. To address this gap, the authors introduce MQuAKE, a benchmark dataset comprising multi-hop questions designed to evaluate knowledge editing methods.

## Method Summary
The authors introduce MQuAKE, a benchmark dataset comprising multi-hop questions designed to evaluate knowledge editing methods. The benchmark includes two datasets: MQuAKE-CF for counterfactual edits and MQuAKE-T for real-world temporal updates. They evaluate several state-of-the-art knowledge editing approaches on MQuAKE, including fine-tuning, MEND, ROME, and MEMIT, using GPT-J as the base model. To improve knowledge editing, they propose MeLLo, a memory-based approach that stores edited facts externally and prompts the LLM iteratively to generate answers consistent with the edited facts.

## Key Results
- Parameter-based editing methods (Fine-tuning, MEND, ROME, MEMIT) achieve high edit-wise and instance-wise accuracy but fail catastrophically on multi-hop questions
- MeLLo significantly outperforms existing methods on MQuAKE, even when scaled to larger models like GPT-3.5-turbo
- Chain-of-thought prompting provides minimal improvement for edited models on multi-hop questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Memory-based editing without weight updates can outperform parameter-based methods for multi-hop knowledge editing.
- Mechanism: MeLLo stores edited facts externally and iteratively decomposes questions, retrieves relevant facts, and self-checks consistency before answering.
- Core assumption: The base LLM can follow decomposition and self-checking instructions reliably enough to leverage stored edits.
- Evidence anchors:
  - [abstract] "MeLLo significantly outperforms existing methods on MQUAKE, even when scaled to larger models like GPT-3.5-turbo."
  - [section] "Instead of updating model weights, MeLLo stores edits in an explicit memory inspired by memory-based editing methods (Mitchell et al., 2022b) and prompts the language model iteratively to interact with the edited facts."
  - [corpus] Weak evidence. Neighboring papers discuss memory-based methods but don't provide direct comparative results.
- Break condition: If the LLM fails to decompose questions accurately or ignores retrieved facts during self-checking.

### Mechanism 2
- Claim: Multi-hop questions expose failures of parameter-based editing methods that single-hop questions miss.
- Mechanism: Multi-hop questions require the model to propagate knowledge changes through chains of facts, revealing whether edits are faithfully integrated.
- Core assumption: Knowledge propagation through chains is a valid proxy for faithful knowledge integration.
- Evidence anchors:
  - [abstract] "While we find that current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions."
  - [section] "When we edit one or a few facts in a chain, the edited model needs to propagate the change to entailed consequences of the edited facts."
  - [corpus] Moderate evidence. The neighboring paper "On the Limitations of Rank-One Model Editing in Answering Multi-hop Questions" suggests this is a recognized limitation.
- Break condition: If the model can answer multi-hop questions through reasoning without relying on the edited facts.

### Mechanism 3
- Claim: Chain-of-thought prompting provides minimal improvement for edited models on multi-hop questions.
- Mechanism: CoT prompting helps models verbalize reasoning steps but doesn't address the core issue of integrating edited knowledge.
- Core assumption: The limitation is not in reasoning capability but in knowledge integration.
- Evidence anchors:
  - [section] "As shown in Table 3, CoT helps slightly across all settings yet still fails catastrophically at answering multi-hop questions."
  - [corpus] Weak evidence. No direct corpus support for this specific finding.
- Break condition: If CoT prompting significantly improves multi-hop accuracy, it would suggest reasoning rather than knowledge integration is the primary bottleneck.

## Foundational Learning

- Concept: Knowledge editing in LLMs
  - Why needed here: Understanding how different editing approaches (weight updates vs. external memory) affect knowledge integration.
  - Quick check question: What's the key difference between parameter-based and memory-based knowledge editing approaches?

- Concept: Multi-hop reasoning
  - Why needed here: The benchmark evaluates whether edited models can propagate knowledge changes through chains of facts.
  - Quick check question: How does a 3-hop question differ from a single-hop question in terms of required knowledge integration?

- Concept: Chain-of-thought prompting
  - Why needed here: The paper evaluates whether explicit reasoning steps help edited models answer multi-hop questions.
  - Quick check question: What is the main purpose of chain-of-thought prompting in language models?

## Architecture Onboarding

- Component map: Base LLM -> Question Decomposition -> Fact Retrieval -> Self-Check -> Final Answer
- Critical path: Question decomposition -> fact retrieval -> self-checking (any failure here breaks the pipeline)
- Design tradeoffs: Memory-based (no training, scalable) vs. parameter-based (requires training, limited scalability)
- Failure signatures: Low multi-hop accuracy despite high edit-wise accuracy indicates poor knowledge integration
- First 3 experiments:
  1. Test MeLLo on a simple 2-hop question with one edit to verify the decomposition and retrieval pipeline works
  2. Compare multi-hop accuracy of MeLLo vs. ROME on GPT-J with varying numbers of edits
  3. Test MeLLo with a larger base model (e.g., GPT-3.5-turbo) to confirm scalability claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can current knowledge editing methods effectively handle temporal knowledge updates in large language models?
- Basis in paper: Explicit - The paper introduces MQuAKE-T dataset for evaluating temporal knowledge updates and finds that while methods achieve near-perfect edit-wise and instance-wise accuracy, they struggle significantly on multi-hop questions.
- Why unresolved: The paper shows that existing methods fail catastrophically on multi-hop questions even when they can recall edited facts accurately. This suggests a fundamental limitation in how these methods update and utilize knowledge.
- What evidence would resolve it: Developing and evaluating new knowledge editing methods specifically designed to handle temporal updates while maintaining multi-hop reasoning capabilities, particularly on the MQuAKE-T benchmark.

### Open Question 2
- Question: What is the minimum model size required for memory-based approaches like MeLLo to effectively handle multi-hop reasoning?
- Basis in paper: Inferred - The paper shows MeLLo works well on GPT-3 (175B parameters) and Vicuna-7B, but doesn't explore smaller models systematically.
- Why unresolved: The paper demonstrates MeLLo's effectiveness on large models but doesn't investigate its performance on smaller language models, which are more commonly used in practice.
- What evidence would resolve it: Systematic evaluation of MeLLo across different model sizes, from small models like GPT-2 up to large models, to identify the minimum effective size for multi-hop reasoning.

### Open Question 3
- Question: How can knowledge editing methods be designed to maintain faithfulness while scaling to handle large numbers of edits simultaneously?
- Basis in paper: Explicit - The paper shows that performance drops significantly when injecting more edits into language models at the same time, with multi-hop accuracy dropping from 12.5% to 1.8% when increasing from 1 to 3000 edited instances.
- Why unresolved: Current methods fail to maintain performance when scaling to handle many edits, suggesting a need for new approaches that can manage multiple knowledge updates while preserving reasoning capabilities.
- What evidence would resolve it: Development and evaluation of new knowledge editing architectures that can maintain multi-hop reasoning accuracy even with thousands of simultaneous edits.

## Limitations
- The evaluation scope is narrow - only 4 editing methods are compared
- The benchmark may not represent the full space of knowledge editing scenarios
- Scaling claims to GPT-3.5-turbo lack systematic evaluation across different model sizes

## Confidence
- High confidence: The core finding that parameter-based editing methods fail on multi-hop questions despite succeeding on direct fact recall
- Medium confidence: The proposed MeLLo method's superiority on the MQuAKE benchmark
- Low confidence: Generalization claims about memory-based approaches being universally better for knowledge editing

## Next Checks
1. Test MeLLo on a different multi-hop QA benchmark (like HotpotQA) with counterfactual edits to verify the findings aren't benchmark-specific
2. Systematically vary the number of edits (1-10+) to identify where MeLLo's performance degrades, and compare this to parameter-based methods
3. Evaluate MeLLo across a broader range of model sizes (including smaller models like GPT-2) to better understand the scaling relationship claimed in the paper