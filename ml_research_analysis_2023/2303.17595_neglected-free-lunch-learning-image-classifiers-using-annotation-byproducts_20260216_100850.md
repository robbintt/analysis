---
ver: rpa2
title: Neglected Free Lunch -- Learning Image Classifiers Using Annotation Byproducts
arxiv_id: '2303.17595'
source_url: https://arxiv.org/abs/2303.17595
tags:
- annotation
- byproducts
- image
- luab
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Learning using Annotation Byproducts (LUAB),
  a new paradigm that leverages unintentional human interaction signals during image
  annotation (e.g., mouse traces, click locations) to improve model generalization
  and robustness. The authors create ImageNet-AB and COCO-AB datasets enriched with
  these annotation byproducts and demonstrate that training models with weak localization
  signals from these byproducts reduces spurious background correlations and enhances
  out-of-distribution performance.
---

# Neglected Free Lunch -- Learning Image Classifiers Using Annotation Byproducts

## Quick Facts
- arXiv ID: 2303.17595
- Source URL: https://arxiv.org/abs/2303.17595
- Reference count: 40
- Key outcome: LUAB leverages annotation byproducts to improve model generalization and robustness without additional annotation costs

## Executive Summary
This paper introduces Learning using Annotation Byproducts (LUAB), a novel approach that exploits unintentional human interaction signals collected during image annotation to improve model performance. The authors create datasets enriched with annotation byproducts (mouse traces, click locations) and demonstrate that training models with weak localization signals from these byproducts reduces spurious background correlations and enhances out-of-distribution performance. The approach achieves measurable improvements in accuracy and robustness metrics without requiring additional annotation costs.

## Method Summary
LUAB is a multi-task learning framework that trains image classifiers using both standard labels and annotation byproducts as weak supervision. The method adds regression heads to standard vision models that predict annotation byproducts (mouse traces and click locations) alongside classification tasks. During training, the model optimizes a combined loss that includes both classification accuracy and regression error on the byproducts. The approach is evaluated on ImageNet-AB and COCO-AB datasets, showing improvements in both general accuracy and robustness to spurious correlations.

## Key Results
- ViT-B accuracy improves from 81.6% to 82.5% on ImageNet-1k using LUAB
- Models trained with annotation byproducts show improved robustness on SI-Score and BG Challenge benchmarks
- LUAB achieves these gains without requiring additional annotation costs beyond standard image labeling
- Both mouse traces and click locations provide useful weak localization signals for model training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Annotation byproducts encode approximate human attention that weakly guides the model to focus on foreground cues.
- Mechanism: Mouse traces and click locations left during image annotation serve as proxy signals for object location, providing weak localization supervision.
- Core assumption: Human interaction patterns during annotation correlate with object foreground regions.
- Evidence anchors:
  - [abstract] "Our insight is that such annotation byproducts Z provide approximate human attention that weakly guides the model to focus on the foreground cues"
  - [section] "We hypothesise that the record of human interaction with the image annotation interfaces provides weak signals for the object locations"
  - [corpus] Weak evidence - only 5 corpus neighbors with fmr > 0.5, none directly supporting this mechanism
- Break condition: If click patterns are systematically biased to image center or other non-object regions, the proxy signal becomes unreliable.

### Mechanism 2
- Claim: Training with weak localization signals reduces spurious background correlations.
- Mechanism: Multi-task regression on annotation byproducts conditions the model to extract features more from foreground object regions rather than background context.
- Core assumption: Background features are a source of spurious correlations that hurt generalization.
- Evidence anchors:
  - [abstract] "improves not only generalisability but also robustness by reducing spurious correlations with background features"
  - [section] "We expect them to be helpful information for resolving spurious correlations with background features"
  - [corpus] Weak evidence - no corpus neighbors directly addressing spurious correlations from annotation byproducts
- Break condition: If the dataset is already highly object-centric (like ImageNet), the benefit from reducing background correlations diminishes.

### Mechanism 3
- Claim: Annotation byproducts provide cost-free privileged information that enhances model robustness.
- Mechanism: LUAB exploits annotation byproducts as privileged information during training, improving robustness without extra annotation costs.
- Core assumption: Annotation byproducts contain information that would otherwise require expensive manual annotation.
- Evidence anchors:
  - [abstract] "Compared to the original supervised learning, LUAB does not require extra annotation costs"
  - [section] "LUAB is an attractive instance of LUPI, as it does not incur additional annotation costs for privileged information"
  - [corpus] Weak evidence - no corpus neighbors directly supporting cost-free privileged information from annotation byproducts
- Break condition: If annotation byproducts are not collected systematically or contain significant noise, the cost-free advantage disappears.

## Foundational Learning

- Concept: Multi-task learning with auxiliary supervision
  - Why needed here: LUAB trains models with additional annotation byproducts (Z) alongside labels (Y), requiring multi-task objective formulation
  - Quick check question: What loss function combines classification and regression tasks in LUAB?
- Concept: Weakly supervised object localization
  - Why needed here: Annotation byproducts provide only approximate object locations, requiring evaluation methods that can assess weak localization signals
  - Quick check question: How does LUAB evaluate whether models attend to foreground features using CAM and WSOL metrics?
- Concept: Robustness to spurious correlations
  - Why needed here: LUAB aims to reduce spurious background correlations, requiring understanding of what constitutes spurious correlations and how to measure them
  - Quick check question: What benchmarks (SI-Score, BG Challenge) measure robustness to spurious background correlations?

## Architecture Onboarding

- Component map: Image → Backbone → Classification head + Regression head → Combined loss → Parameter update
- Critical path: Image → Backbone → Classification head + Regression head → Combined loss → Parameter update
- Design tradeoffs:
  - Single vs. per-class regression heads (COCO has multiple objects per image)
  - Regression target precision (click locations vs. bounding boxes vs. masks)
  - Weighting between classification and regression objectives (λ parameter)
- Failure signatures:
  - High regression loss indicates byproduct signal is weak or model cannot learn it
  - No improvement in localization accuracy suggests byproducts don't contain useful information
  - Decreased classification accuracy indicates byproduct supervision is harmful
- First 3 experiments:
  1. Compare baseline classification vs LUAB with random points to isolate effect of actual byproduct information
  2. Vary λ parameter to find optimal tradeoff between classification and regression objectives
  3. Test on de-correlated datasets (SI-Score, BG Challenge) to measure spurious correlation reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do annotation byproducts from different human annotation interfaces (browsing vs. tagging) differentially impact model robustness to spurious correlations?
- Basis in paper: [explicit] The paper compares ImageNet (browsing interface) and COCO (tagging interface) byproducts and shows both improve robustness, but doesn't systematically compare their relative effectiveness.
- Why unresolved: The paper uses byproducts from both interfaces but doesn't directly compare which type of byproduct (from browsing vs. tagging) is more effective at reducing spurious correlations.
- What evidence would resolve it: A controlled experiment training identical models with byproducts from only one interface type versus the other, measuring robustness metrics on datasets like SI-Score and BG Challenge.

### Open Question 2
- Question: Can annotation byproducts be effectively used for tasks beyond image classification, such as object detection or semantic segmentation?
- Basis in paper: [inferred] The paper mentions this as a limitation and discusses possibilities for other tasks and domains, but only demonstrates LUAB for image classification.
- Why unresolved: The paper only validates LUAB on image classification tasks, leaving open whether the approach generalizes to other computer vision tasks that might benefit from attention guidance.
- What evidence would resolve it: Experiments applying LUAB to object detection and semantic segmentation tasks, measuring performance improvements compared to standard training methods.

### Open Question 3
- Question: What is the optimal method for utilizing annotation byproducts beyond simple regression or attentive pooling?
- Basis in paper: [explicit] The paper mentions several possibilities including regularizing attribution maps and forcing attention-based feature pooling, but only implements simple regression baselines.
- Why unresolved: The paper acknowledges more sophisticated methods exist but doesn't explore them, leaving the question of optimal utilization open.
- What evidence would resolve it: Comparative experiments testing various methods of incorporating byproducts (attention regularization, feature pooling, etc.) against each other and against the simple regression baseline.

## Limitations
- Limited evaluation to image classification tasks, leaving generalization to other computer vision tasks untested
- Mechanism for spurious correlation reduction lacks strong corpus evidence
- Benefits may diminish for highly object-centric datasets or architectures that already attend to foreground regions

## Confidence

**Confidence labels:**
- Mechanism 1 (human attention proxy): Medium - supported by the authors' observations but lacks strong corpus evidence
- Mechanism 2 (spurious correlation reduction): Medium - theoretically plausible but under-supported by corpus evidence
- Mechanism 3 (cost-free privileged information): Medium - conceptually sound but lacks direct corpus validation

## Next Checks

1. Test LUAB on a dataset where annotation byproducts are known to be noisy or biased (e.g., crowd-sourced annotations) to validate robustness claims
2. Conduct ablation studies varying the regression loss weight λ across a wider range to determine optimal tradeoff points
3. Evaluate LUAB on architectures beyond ResNets and ViTs (e.g., ConvNeXt, Swin) to test generalizability across model families