---
ver: rpa2
title: Using Large Language Model to Solve and Explain Physics Word Problems Approaching
  Human Level
arxiv_id: '2309.08182'
source_url: https://arxiv.org/abs/2309.08182
tags:
- problems
- word
- physics
- problem
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PhysQA, the first dataset of over 1000 junior
  high school physics word problems covering Kinematics, Mass&Density, Mechanics,
  Heat, and Electricity. The dataset includes detailed annotations such as formulas,
  equations, and answers for each problem.
---

# Using Large Language Model to Solve and Explain Physics Word Problems Approaching Human Level

## Quick Facts
- arXiv ID: 2309.08182
- Source URL: https://arxiv.org/abs/2309.08182
- Reference count: 8
- Primary result: GPT3.5 achieves 73.2% accuracy on physics word problems using few-shot learning with similar problem prompts

## Executive Summary
This paper introduces PhysQA, a dataset of over 1000 junior high school physics word problems covering five topics: Kinematics, Mass&Density, Mechanics, Heat, and Electricity. The authors demonstrate that large language models, specifically GPT3.5, can solve these problems with 73.2% accuracy in few-shot learning mode by using similar problems as prompts. Beyond problem-solving, the model can explain, summarize, and generate physics word problems, showcasing its potential for educational applications.

## Method Summary
The study uses GPT3.5 (gpt-3.5-turbo) to solve physics word problems from the PhysQA dataset through zero-shot and few-shot learning approaches. In few-shot learning, similar problems and their solutions are used as prompts to guide the model. The model's performance is evaluated based on accuracy, with a 5% error tolerance for numeric results. The dataset includes detailed annotations such as formulas, equations, answers, and sub-question breakdowns for each problem.

## Key Results
- GPT3.5 achieves 49.3% accuracy in zero-shot learning and 73.2% in few-shot learning on physics word problems
- Similar problems and their solutions significantly improve model performance when used as prompts
- The model can generate explanations, summarize problem topics, and create new physics word problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Similar problems and their solutions act as effective prompts for improving LLM performance on physics word problems.
- Mechanism: The model leverages the structural and contextual similarity between the prompt problem and the target problem to transfer reasoning patterns, formulas, and solution strategies.
- Core assumption: Physics word problems with high text similarity share the same underlying physical principles and solution methods.
- Evidence anchors:
  - [abstract] "by using similar problems and their answers as prompt, LLM could solve elementary physics word problems approaching human level performance"
  - [section] "We find that similar questions(with solutions) greatly improve the model’s performance"
- Break condition: If the similarity is only superficial (e.g., same numbers, different physics concepts), the prompt may mislead rather than help.

### Mechanism 2
- Claim: Zero-shot learning is limited because the model lacks the necessary physics context without explicit prompts.
- Mechanism: The LLM relies on its pre-training corpus for general knowledge but struggles to map that to specific physics problem-solving without explicit guidance or examples.
- Core assumption: Pre-training data includes enough physics concepts for general reasoning, but not enough for accurate problem solving without context.
- Evidence anchors:
  - [abstract] "GPT3.5 could automatically solve 49.3% of the problems through zero-shot learning"
  - [section] "When provided with prompts containing similar problems and their solutions, the model’s performance is significantly better than in other situations."
- Break condition: If the LLM is fine-tuned on physics datasets, zero-shot performance might improve significantly.

### Mechanism 3
- Claim: The LLM's interpretability advantage lies in its ability to generate not just answers but also reasoning chains and explanations.
- Mechanism: By outputting step-by-step solutions, the model provides transparency in its problem-solving process, aiding understanding and debugging.
- Core assumption: The LLM's autoregressive generation naturally lends itself to coherent explanation of reasoning steps.
- Evidence anchors:
  - [abstract] "In addition to solving problems, GPT3.5 can also summarize the knowledge or topics covered by the problems, provide relevant explanations"
  - [section] "Compared with traditional MWP solver, an advantage of LLM is that its interpretability is much better since its output not only contains the equation and numeric result, but also includes the essential explanation and train of thought"
- Break condition: If the generated explanations are incorrect or incomplete, the interpretability advantage is lost.

## Foundational Learning

- Concept: Physics formula application
  - Why needed here: Physics word problems require correct identification and application of formulas based on the problem context.
  - Quick check question: Given a problem about density, which formula would you use and what variables are needed?

- Concept: Physical reasoning and inference
  - Why needed here: Many physics problems require inference beyond direct calculation, such as understanding physical scenarios and relationships.
  - Quick check question: If a block is floating, what can you infer about the relationship between its weight and the buoyant force?

- Concept: Multi-step problem decomposition
  - Why needed here: Physics problems often involve multiple sub-questions that build on each other, requiring systematic breakdown.
  - Quick check question: How would you approach a problem with three related sub-questions about motion, ensuring each step builds correctly on the previous?

## Architecture Onboarding

- Component map:
  - Web scraping → Annotation (with LLM assistance) → Dataset (PhysQA) → GPT3.5 API calls with different prompt strategies → Accuracy calculation with error tolerance

- Critical path:
  1. Load problem and similar problems from dataset
  2. Construct prompt with problem text and similar solutions
  3. Send prompt to GPT3.5 API
  4. Parse and evaluate generated solution
  5. Record accuracy metrics

- Design tradeoffs:
  - Prompt length vs. accuracy: Longer prompts with multiple similar problems may cause confusion
  - Zero-shot vs. few-shot: Few-shot requires curated similar problems but significantly improves performance
  - Error tolerance: Allowing 5% error margin balances strictness with practical accuracy

- Failure signatures:
  - Consistent calculation errors despite correct reasoning chain
  - Misidentification of physical scenarios leading to wrong formulas
  - Confusion between similar problems when multiple are used as prompts

- First 3 experiments:
  1. Test zero-shot performance on a small subset of problems to establish baseline
  2. Implement similarity-based prompt selection and test on same subset
  3. Compare performance when using 1 vs. 3 similar problems as prompts to find optimal number

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the accuracy of physics word problem solving scale with dataset size, and what is the minimum dataset size needed to achieve human-level performance?
- Basis in paper: [inferred] The paper notes that their dataset (1008 problems) is smaller than large-scale math word problem datasets (20K+), and suggests that larger datasets could improve performance by increasing the likelihood of finding similar problems as prompts.
- Why unresolved: The authors did not conduct experiments with varying dataset sizes to establish a relationship between dataset size and accuracy.
- What evidence would resolve it: Systematic experiments training and testing on progressively larger subsets of the PhysQA dataset, measuring accuracy improvements and identifying the point of diminishing returns.

### Open Question 2
- Question: What is the impact of incorporating external knowledge bases or domain-specific physics ontologies on the accuracy of physics word problem solving?
- Basis in paper: [explicit] The paper found that textbook paragraphs had little impact on accuracy, suggesting that the current LLM knowledge may be insufficient for physics problem solving.
- Why unresolved: The authors only tested textbook paragraphs as prompts, not more structured knowledge sources like ontologies or knowledge graphs.
- What evidence would resolve it: Experiments comparing LLM performance with and without access to structured physics knowledge bases, measuring improvements in accuracy and solution quality.

### Open Question 3
- Question: Can the uncertainty in LLM outputs for physics problems be reduced through ensemble methods or confidence scoring mechanisms?
- Basis in paper: [explicit] The paper notes that GPT3.5's performance fluctuates, with periods of complete failure followed by normal accuracy levels.
- Why unresolved: The authors did not explore methods to stabilize or predict LLM performance on physics problems.
- What evidence would resolve it: Implementation and evaluation of ensemble methods (multiple LLM runs with different prompts) or confidence scoring based on solution consistency, measuring reductions in error rates and prediction stability.

## Limitations
- The study relies entirely on a single LLM model (GPT3.5) without comparison to alternative approaches or baseline solvers
- The similarity-based prompting mechanism lacks detailed explanation of how similarity is measured and what happens when problems share surface features but different underlying physics concepts
- The error tolerance of 5% may mask systematic calculation errors that would be unacceptable in educational settings

## Confidence
- High Confidence: The dataset creation methodology and the observation that few-shot learning outperforms zero-shot learning are well-supported by the results and consistent with established LLM behavior patterns
- Medium Confidence: The claim about GPT3.5 approaching "human level performance" is somewhat overstated, as the comparison baseline isn't clearly defined, and the 73.2% accuracy still represents substantial error rates
- Low Confidence: The assertion that LLMs provide superior interpretability is not rigorously validated; the paper assumes explanation quality without systematic evaluation against human-generated explanations

## Next Checks
1. Cross-linguistic validation: Test the prompting approach on physics problems from different language datasets to verify whether similarity-based prompting generalizes across educational contexts and languages

2. Error analysis categorization: Systematically categorize model errors (conceptual misunderstanding, calculation errors, formula misidentification) to identify whether the 26.8% failure rate represents correctable patterns or fundamental limitations

3. Human comparison study: Conduct a controlled comparison where human students solve the same problems with and without similar problem prompts to establish whether the LLM's performance gain from few-shot learning is comparable to human learning patterns