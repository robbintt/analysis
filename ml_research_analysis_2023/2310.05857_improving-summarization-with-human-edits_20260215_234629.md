---
ver: rpa2
title: Improving Summarization with Human Edits
arxiv_id: '2310.05857'
source_url: https://arxiv.org/abs/2310.05857
tags:
- data
- training
- edits
- human
- salt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using human edits to improve medical summarization.
  It proposes SALT, a method combining unlikelihood training with human edits and
  model-generated summaries.
---

# Improving Summarization with Human Edits

## Quick Facts
- arXiv ID: 2310.05857
- Source URL: https://arxiv.org/abs/2310.05857
- Reference count: 19
- This paper proposes SALT, a method combining unlikelihood training with human edits and model-generated summaries, improving ROUGE and UMLS-F1 scores on medical summarization tasks.

## Executive Summary
This paper introduces SALT (Sequence Alignment (un)Likelihood Training), a novel approach to improve medical summarization by leveraging human edits. SALT combines unlikelihood training with human edits and model-generated summaries to penalize unlikely tokens and reinforce verified ones. The method shows improvements in ROUGE and UMLS-F1 scores on both human and imitation edit data, outperforming likelihood-only training. SALT also mitigates catastrophic forgetting when combined with replay-based methods.

## Method Summary
SALT uses sequence alignment to identify token-level changes between model-generated summaries and human-edited versions. It applies unlikelihood training to penalize tokens that users deleted or modified, while reinforcing tokens that were retained or added. The method can work with human-edit data or imitation edits using ground truth summaries as proxies. For catastrophic forgetting, SALT is combined with replay-based methods (RSALT) that sample from original training data and apply both likelihood and unlikelihood training.

## Key Results
- SALT improves ROUGE-1, ROUGE-2, and ROUGE-L scores by 1-2 points over baseline models on medical summarization tasks
- SALT outperforms likelihood-only training when using imitation edits (ground truth as proxy for human edits)
- RSALT mitigates catastrophic forgetting when fine-tuning on new human-edit data while maintaining performance on original data

## Why This Works (Mechanism)

### Mechanism 1
SALT improves summarization by combining unlikelihood training with human edits to penalize unlikely tokens and reinforce verified ones. It uses sequence alignment to track token-level changes between model outputs and human edits, identifying which tokens to penalize (unchanged but deleted) and reinforce (unchanged and retained).

### Mechanism 2
Imitation Edits reduce the need for expensive human-edit data by using ground truth summaries as proxies for edited outputs. The paper assumes ground truth summaries contain similar information to human edits when aligned with model outputs, despite being generated independently.

### Mechanism 3
Replay-based SALT (RSALT) mitigates catastrophic forgetting by combining seen data with human edits using both likelihood and unlikelihood training. It samples data from the original training set and combines it with human-edit data, applying SALT to both new and sampled data.

## Foundational Learning

- Concept: Sequence alignment algorithms (e.g., Needleman-Wunsch)
  - Why needed here: To identify which tokens in model-generated summaries were changed, deleted, or added in human edits for targeted unlikelihood training
  - Quick check question: What is the primary purpose of using sequence alignment in SALT?

- Concept: Unlikelihood training
  - Why needed here: To decrease the probability of generating tokens that users have modified or deleted in human edits, complementing likelihood training
  - Quick check question: How does unlikelihood training differ from standard likelihood training in the context of human edits?

- Concept: Catastrophic forgetting
  - Why needed here: To understand why models degrade on original data when fine-tuned on new human-edit data with different distributions
  - Quick check question: What causes catastrophic forgetting when fine-tuning a model on a new dataset?

## Architecture Onboarding

- Component map: Utterance clusters from medical conversations -> T5-based summarization model -> Sequence alignment -> Apply SALT loss -> Updated model
- Critical path: Generate model summary → Human edits or ground truth → Sequence alignment → Apply SALT loss → Update model
- Design tradeoffs: SALT requires sequence alignment overhead but provides more granular training signals than likelihood-only training. Imitation edits reduce human effort but may be less effective than real human edits.
- Failure signatures: Performance degradation on original data indicates catastrophic forgetting. Poor SAGE scores suggest misalignment between model outputs and human preferences.
- First 3 experiments:
  1. Implement SALT with human-edit data on a small dataset to verify basic functionality
  2. Add imitation edits to test effectiveness without human data
  3. Integrate RSALT to address catastrophic forgetting on combined datasets

## Open Questions the Paper Calls Out

- How does SALT perform on larger language models (LLMs) compared to smaller models like T5? The paper doesn't provide experimental results on larger language models.
- How does Human Edits + SALT compare in data efficiency to RLHF for improving summarization models? The paper doesn't provide direct comparisons between these approaches.
- How does LLM-in-the-loop with SALT affect the quality of Imitation Edits and overall summarization performance? The paper doesn't explore using LLMs to generate Imitation Edits.

## Limitations

- Effectiveness heavily depends on the quality and consistency of human edits, with results potentially not generalizing beyond the medical domain
- Sequence alignment approach assumes token-level changes directly indicate which model outputs are undesirable, but may miss higher-level structural changes
- Evaluation scope lacks ablation studies showing individual contributions of SALT components and doesn't examine computational overhead

## Confidence

- High Confidence: The fundamental mechanism of using unlikelihood training with human edits is technically sound and supported by multiple results showing consistent improvements
- Medium Confidence: The claim that imitation edits can effectively replace human-edit data is moderately supported but would benefit from more extensive validation
- Medium Confidence: The effectiveness of RSALT in preventing catastrophic forgetting is demonstrated on specific datasets tested, but generalizability remains uncertain

## Next Checks

1. Test SALT on non-medical summarization datasets (e.g., CNN/DailyMail, XSum) to assess generalizability across domains
2. Implement error analysis to measure sequence alignment accuracy and identify failure cases where token-level alignment doesn't capture meaningful human edit intentions
3. Benchmark training time, memory usage, and inference latency for SALT compared to baseline models to understand practical deployment implications