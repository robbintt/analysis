---
ver: rpa2
title: Improving Time Series Encoding with Noise-Aware Self-Supervised Learning and
  an Efficient Encoder
arxiv_id: '2306.06579'
source_url: https://arxiv.org/abs/2306.06579
tags:
- series
- coinception
- time
- learning
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning robust and efficient
  representations for time series data, particularly in the presence of noise. The
  authors propose CoInception, a novel framework that combines a noise-resilient sampling
  strategy with an efficient encoder architecture.
---

# Improving Time Series Encoding with Noise-Aware Self-Supervised Learning and an Efficient Encoder

## Quick Facts
- **arXiv ID:** 2306.06579
- **Source URL:** https://arxiv.org/abs/2306.06579
- **Reference count:** 40
- **Primary result:** Ranks first on over two-thirds of classification UCR datasets while using only 40% of parameters compared to the second-best approach

## Executive Summary
This paper introduces CoInception, a novel framework for learning robust and efficient time series representations that addresses the challenge of noise resilience. The approach combines a noise-resilient sampling strategy based on Discrete Wavelet Transform (DWT) low-pass filtering with an efficient encoder architecture using dilated convolutions within Inception blocks. The framework achieves state-of-the-art performance across multiple tasks including forecasting, classification, and anomaly detection while maintaining computational efficiency. Notably, CoInception demonstrates strong transferability of learned representations and achieves top rankings on most evaluated datasets with significantly fewer parameters than competing methods.

## Method Summary
CoInception addresses time series representation learning through a two-pronged approach: noise-resilient sampling and efficient encoding. The sampling strategy uses DWT low-pass filtering to generate perturbed views of input time series by decomposing signals into approximation (low-frequency) and detail (high-frequency) coefficients, then masking detail coefficients below a threshold α×max(x) to filter noise while preserving essential characteristics. The encoder architecture employs stacked Inception blocks with dilated convolutions, where dilation factors increase exponentially (di_u = (2k-1)^(i-1)) to achieve wide receptive fields without deep layers. Training uses a hierarchical triplet loss combining instance-wise, temporal, and triplet contrastive objectives to balance contextual consistency and noise resilience. The framework is evaluated on UCR and UEA classification datasets, ETT forecasting data, and Yahoo/KPI anomaly detection datasets.

## Key Results
- Achieved top rank on over two-thirds of classification UCR datasets
- Used only 40% of parameters compared to second-best approach
- Demonstrated strong transferability across different time series domains
- Showed superior performance in forecasting, classification, and anomaly detection tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The noise-resilient sampling strategy preserves essential signal characteristics while removing high-frequency noise components.
- **Mechanism:** DWT low-pass filtering separates time series into approximation coefficients (low-frequency trend) and detail coefficients (high-frequency components). By masking detail coefficients below threshold α×max(x), noise is filtered out while preserving meaningful structure.
- **Core assumption:** High-frequency components in natural time series primarily represent noise rather than signal.
- **Evidence anchors:**
  - [abstract] "The sampling strategy uses a Discrete Wavelet Transform (DWT) low-pass filter to generate perturbed views of the input time series, enhancing noise resilience while preserving essential characteristics."
  - [section] "We introduce a parameter-free Discrete Wavelet Transform (DWT) low-pass filter to generate a perturbed version of the original series x, for enhancing the noise resilience."
  - [corpus] Weak - neighboring papers focus on masked autoencoders and tokenization rather than wavelet-based noise filtering.
- **Break condition:** If natural time series contains important high-frequency signal components (not noise), this approach would degrade those features.

### Mechanism 2
- **Claim:** The dilated Inception-based encoder achieves wide receptive fields without deep architecture, enabling efficient long-range dependency capture.
- **Mechanism:** Stacked dilated convolutions with incremental dilation factors (di_u = (2k-1)^(i-1)) expand the receptive field exponentially while maintaining shallow depth. Multiple Basic units with different kernel sizes capture multi-scale patterns.
- **Core assumption:** Dilated convolutions can effectively capture long-range dependencies without requiring deep networks.
- **Evidence anchors:**
  - [abstract] "The encoder architecture leverages dilated convolution within Inception blocks, resulting in a scalable and robust network with a wide receptive field."
  - [section] "the dilated convolution network enables a large receptive field without requiring deep layers."
  - [corpus] Weak - neighboring papers focus on transformers and masked autoencoders rather than dilated Inception architectures.
- **Break condition:** If the time series requires extremely deep networks for complex patterns, this shallow architecture may be insufficient.

### Mechanism 3
- **Claim:** The hierarchical triplet loss balances contextual consistency and noise resilience through instance-wise, temporal, and triplet constraints.
- **Mechanism:** Combines instance-wise loss (Lins) for local consistency, temporal loss (Ltemp) for sequential relationships, and triplet loss for distinguishing original from perturbed views across contexts.
- **Core assumption:** Combining multiple contrastive objectives provides better representation learning than single-objective approaches.
- **Evidence anchors:**
  - [abstract] "Our method attains the top rank in over two-thirds of the classification UCR datasets, utilizing only 40% of the parameters compared to the second-best approach."
  - [section] "we propose a novel loss function combining ideas from hierarchical loss and triplet loss to achieve both contextual consistency and noise resilience."
  - [corpus] Weak - neighboring papers focus on contrastive learning but don't mention triplet-based hierarchical losses.
- **Break condition:** If the balance factor ϵ is poorly tuned, the loss components may conflict rather than complement each other.

## Foundational Learning

- **Concept:** Discrete Wavelet Transform (DWT) and its properties
  - **Why needed here:** DWT forms the core of the noise-resilient sampling strategy, decomposing signals into frequency bands for selective filtering
  - **Quick check question:** What are the mathematical operations for DWT low-pass and high-pass filters at level j and position n?

- **Concept:** Dilated convolutions and receptive field calculation
  - **Why needed here:** Understanding how dilation factors expand receptive fields is critical for grasping the encoder architecture's efficiency
  - **Quick check question:** How does the receptive field grow with dilation factor d and kernel size k in a stacked dilated convolution?

- **Concept:** Contrastive learning objectives and triplet loss
  - **Why needed here:** The hierarchical triplet loss combines multiple contrastive objectives, requiring understanding of their individual behaviors
  - **Quick check question:** What is the difference between instance-wise, temporal, and triplet contrastive losses?

## Architecture Onboarding

- **Component map:** Input → DWT filtering → segment sampling → Inception-based encoder (with dilated convolutions and aggregators) → hierarchical triplet loss → output representations
- **Critical path:** DWT filtering → segment sampling → encoder → loss calculation → representation update
- **Design tradeoffs:** Shallow architecture with wide receptive field vs. deeper networks; noise filtering vs. signal preservation; multiple loss components vs. training stability
- **Failure signatures:** Poor performance on datasets with important high-frequency signals; slow convergence indicating loss component conflicts; inability to capture long-range dependencies
- **First 3 experiments:**
  1. Test DWT filtering with synthetic sine wave + noise to verify noise removal while preserving trend
  2. Validate dilated Inception receptive field calculation by measuring output dependency on input range
  3. Compare triplet loss variants (with/without temporal component) on a small classification task

## Open Questions the Paper Calls Out

- **Open Question 1**
  - **Question:** How does the choice of wavelet family (beyond Daubechies D4) affect CoInception's performance across different time series tasks?
  - **Basis in paper:** [explicit] The paper mentions Daubechies D4 wavelets as a reference but suggests that careful consideration of optimal wavelets for specific datasets may further enhance accuracy.
  - **Why unresolved:** The paper only uses Daubechies D4 wavelets for all experiments without exploring the impact of other wavelet families on performance.
  - **What evidence would resolve it:** Empirical results comparing CoInception's performance using different wavelet families (e.g., Haar, Symlets, Coiflets) across various time series tasks and datasets.

- **Open Question 2**
  - **Question:** What is the impact of different perturbation thresholds (α) in the DWT low-pass filtering strategy on CoInception's noise resilience and downstream task performance?
  - **Basis in paper:** [explicit] The paper introduces a cutting threshold α that is proportional to the maximum value of the input series, but does not explore how varying this parameter affects performance.
  - **Why unresolved:** The paper uses a fixed value for α without investigating its sensitivity or optimal range for different types of time series data.
  - **What evidence would resolve it:** Performance analysis of CoInception with varying α values across different datasets and noise levels, showing the relationship between perturbation strength and task accuracy.

- **Open Question 3**
  - **Question:** How does CoInception's transferability generalize to time series domains not seen during training, and what are the limitations of this transfer learning capability?
  - **Basis in paper:** [explicit] The paper demonstrates transferability in forecasting between ETTh1 and ETTh2 datasets, and shows some transferability in classification tasks, but does not explore limitations or generalization to entirely new domains.
  - **Why unresolved:** The paper only tests transferability within closely related datasets and does not investigate how well CoInception generalizes to completely different time series domains or tasks.
  - **What evidence would resolve it:** Experiments transferring CoInception models trained on one domain (e.g., energy data) to entirely different domains (e.g., healthcare, finance) and measuring performance degradation or limitations.

## Limitations

- The wavelet-based noise filtering mechanism assumes high-frequency components primarily represent noise, which may not hold for all time series domains with important high-frequency signals
- The shallow dilated Inception architecture may struggle with extremely long-range dependencies or complex temporal patterns requiring deeper networks
- The hierarchical triplet loss requires careful tuning of the balance parameter ϵ, with poor choices potentially causing conflicting objectives

## Confidence

- **High confidence** in the efficiency claims (parameter count, computational complexity) due to clear architectural design with dilated convolutions and shallow depth
- **Medium confidence** in noise resilience claims, as the wavelet filtering mechanism is well-established but effectiveness depends on signal-to-noise ratio characteristics of specific datasets
- **Low confidence** in the transferability claims across diverse domains without seeing performance degradation on specialized time series with non-stationary properties

## Next Checks

1. **Signal Preservation Test**: Apply CoInception to synthetic time series containing known high-frequency signal components (not just noise) and measure whether these components are preserved or incorrectly filtered out by the DWT-based sampling strategy.

2. **Depth vs. Receptive Field Analysis**: Systematically vary the number of Basic units in the encoder while measuring receptive field coverage and downstream task performance to quantify the tradeoff between architectural depth and dependency capture.

3. **Domain Generalization Study**: Test CoInception on time series from domains with fundamentally different characteristics (e.g., medical signals with sharp transients vs. financial data with noise-like fluctuations) to evaluate whether the noise filtering assumptions hold across diverse applications.