---
ver: rpa2
title: 'BatGPT: A Bidirectional Autoregessive Talker from Generative Pre-trained Transformer'
arxiv_id: '2307.00360'
source_url: https://arxiv.org/abs/2307.00360
tags:
- language
- batgpt
- human
- learning
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BATGPT introduces a bidirectional autoregressive architecture for
  large language models, addressing limitations of traditional unidirectional models
  such as limited memory and hallucination issues. The model pre-trains using a novel
  bidirectional autoregressive objective that predicts tokens from both forward and
  backward directions, capturing complex dependencies more effectively.
---

# BatGPT: A Bidirectional Autoregessive Talker from Generative Pre-trained Transformer

## Quick Facts
- arXiv ID: 2307.00360
- Source URL: https://arxiv.org/abs/2307.00360
- Reference count: 34
- Primary result: BATGPT-15B achieves 36.72% average accuracy on CMMLU benchmark, ranking second among Chinese-oriented models

## Executive Summary
BATGPT introduces a bidirectional autoregressive architecture for large language models, addressing limitations of traditional unidirectional models such as limited memory and hallucination issues. The model pre-trains using a novel bidirectional autoregressive objective that predicts tokens from both forward and backward directions, capturing complex dependencies more effectively. It further refines capabilities through instruction tuning with prompted data and dialogue data, followed by reinforcement learning from both human and AI feedback to improve alignment with human expectations.

## Method Summary
BATGPT employs a novel bidirectional autoregressive pre-training objective that predicts tokens from both left-to-right and right-to-left directions simultaneously, effectively reducing fixed memory effects and alleviating hallucinations. The model uses a parameter expansion method to leverage pre-training from smaller models, reducing computational costs. Instruction tuning incorporates both prompted and dialogue data, followed by reinforcement learning from a hybrid of human and AI feedback using proximal policy optimization to improve alignment performance.

## Key Results
- BATGPT-15B achieves 36.72% average accuracy on CMMLU benchmark
- Strong performance in STEM (33.49%) and Other (42.14%) categories
- Ranks second among Chinese-oriented models on CMMLU

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bidirectional autoregressive pre-training reduces fixed memory effects and hallucinations.
- Mechanism: The model learns to predict tokens from both left-to-right and right-to-left directions, allowing it to capture context from both past and future tokens simultaneously. This bidirectional context window provides richer information than unidirectional models, helping the model maintain coherence over longer sequences and reducing generation of context-inconsistent outputs.
- Core assumption: The bidirectional autoregressive objective effectively encodes context from both directions without introducing conflicting gradients or training instability.
- Evidence anchors:
  - [abstract] "the bidirectional autoregressive modeling not only operates from left to right but also from right to left, effectively reducing fixed memory effects and alleviating model hallucinations"
  - [section 3.1] "the model learns to predict the next token based on all previously seen tokens in the sequence, from both the past and future directions"
- Break condition: If the bidirectional training objective introduces conflicting gradient signals that destabilize training or if the model prioritizes one direction over the other, reducing the intended benefits.

### Mechanism 2
- Claim: Parameter expansion method enables effective knowledge transfer from smaller pre-trained models.
- Mechanism: The novel parameter expansion approach leverages existing pre-trained parameters from smaller models as initialization for larger BATGPT models, reducing training time and computational costs while preserving learned representations.
- Core assumption: The smaller model's learned representations are sufficiently general to transfer effectively to the larger architecture without catastrophic forgetting.
- Evidence anchors:
  - [abstract] "we propose a novel parameter expansion method for leveraging the pre-training of smaller models"
  - [introduction] "BATGPT employs a novel parameter expansion method that leverages the knowledge garnered during the pre-training of models of smaller sizes"
- Break condition: If the parameter expansion introduces architectural mismatches that prevent effective transfer or if the expanded parameters diverge too far from useful representations during fine-tuning.

### Mechanism 3
- Claim: Reinforcement learning from both human and AI feedback creates better alignment than either alone.
- Mechanism: The hybrid approach combines nuanced human judgment with the consistency and scalability of AI-generated feedback, creating a more comprehensive reward signal that trains BATGPT to generate outputs that are both helpful and aligned with human expectations.
- Core assumption: AI feedback can approximate human preferences closely enough to be useful, and the combination provides complementary strengths without introducing contradictory signals.
- Evidence anchors:
  - [section 3.3] "To make the aligning process more efficient and flexible, BATGPT learns not only from human feedback but also from feedback generated by other AI systems"
  - [abstract] "employ reinforcement learning from both AI and human feedback, aimed at improving the model's alignment performance"
- Break condition: If AI feedback diverges significantly from human preferences or if the combined reward signal creates conflicting optimization objectives.

## Foundational Learning

- Concept: Transformer architecture fundamentals
  - Why needed here: BATGPT builds on transformer-based models, and understanding attention mechanisms, positional encodings, and layer interactions is crucial for grasping how bidirectional autoregressive training works
  - Quick check question: How does the attention mechanism in transformers differ from recurrent neural networks in handling sequence dependencies?

- Concept: Language modeling objectives
  - Why needed here: Understanding the difference between autoregressive, masked language modeling, and bidirectional autoregressive objectives is essential for grasping BATGPT's unique approach
  - Quick check question: What are the key differences between traditional autoregressive language modeling and masked language modeling in terms of what context is available during prediction?

- Concept: Reinforcement learning from human feedback (RLHF)
  - Why needed here: BATGPT uses a hybrid RLHF approach, so understanding reward modeling, preference learning, and policy optimization is critical for understanding the alignment phase
  - Quick check question: How does the reward model in RLHF differ from traditional supervised learning objectives, and why is this important for alignment?

## Architecture Onboarding

- Component map: Input layer → Bidirectional Transformer blocks → Output layer; RLHF loop with reward model → PPO optimizer; Parameter expansion module for model initialization
- Critical path: Pre-training (bidirectional autoregressive objective) → Instruct tuning (prompt-response pairs) → RLHF (human/AI feedback) → Evaluation
- Design tradeoffs: Bidirectional context provides richer representations but increases computational complexity compared to unidirectional models; hybrid RLHF provides better alignment but requires more complex data collection infrastructure
- Failure signatures: Hallucinations indicate insufficient bidirectional context capture; alignment failures suggest reward model inadequacy; training instability may indicate issues with parameter expansion or bidirectional objective
- First 3 experiments:
  1. Implement basic bidirectional autoregressive pre-training on a small dataset and compare perplexity with unidirectional baseline
  2. Test parameter expansion by initializing a larger model from a smaller pre-trained model and measuring performance retention
  3. Validate reward model training by testing preference prediction accuracy on held-out human preference data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does BATGPT's bidirectional autoregressive architecture affect its performance on tasks requiring long-term context retention compared to traditional unidirectional models?
- Basis in paper: [explicit] The paper states that the bidirectional autoregressive modeling "effectively reducing fixed memory effects and alleviating model hallucinations."
- Why unresolved: The paper does not provide empirical comparisons demonstrating the extent of improvement in long-term context retention tasks.
- What evidence would resolve it: Comparative studies showing BATGPT's performance on long-context tasks against unidirectional models, with quantitative metrics for context retention and hallucination reduction.

### Open Question 2
- Question: What is the impact of BATGPT's parameter expansion method on the final model's performance and training efficiency compared to training from scratch or using other pre-training approaches?
- Basis in paper: [explicit] The paper mentions a "novel parameter expansion method for leveraging the pre-training of smaller models" but does not provide detailed performance comparisons.
- Why unresolved: The paper lacks specific data on how this method affects model quality, training time, or computational resources compared to alternative approaches.
- What evidence would resolve it: Empirical studies comparing BATGPT's parameter expansion method with other initialization techniques in terms of final performance, training speed, and resource efficiency.

### Open Question 3
- Question: How does the combination of human and AI feedback in BATGPT's RLHF training compare to using either feedback source alone in terms of alignment quality and safety?
- Basis in paper: [explicit] The paper describes using "reinforcement learning from both AI and human feedback" but does not compare this approach to using only human or only AI feedback.
- Why unresolved: The paper does not provide ablation studies or comparisons to determine the relative contributions of human and AI feedback to the final model's alignment.
- What evidence would resolve it: Comparative experiments showing BATGPT's performance when trained with different combinations of human and AI feedback, including metrics for alignment quality and safety.

## Limitations
- Critical architectural details (layer count, hidden size, attention head configuration) are not specified
- Key hyperparameters for pre-training, instruction tuning, and RLHF stages are not disclosed
- Data provenance details including dataset sizes, composition, and sources are not provided

## Confidence
- High confidence in the bidirectional autoregressive concept: The fundamental mechanism is theoretically sound and has precedent in related architectures
- Medium confidence in parameter expansion claims: The concept is established but specific implementation details are not provided
- Low confidence in relative performance claims: CMMLU results lack context about comparison models' configurations and training regimes

## Next Checks
- Validation 1: Ablation on bidirectional vs. unidirectional training to isolate benefits of bidirectional approach
- Validation 2: Parameter expansion effectiveness test comparing expanded models against randomly initialized models
- Validation 3: Hybrid RLHF vs. human-only RLHF comparison to determine if combined feedback provides measurable benefits