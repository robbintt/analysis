---
ver: rpa2
title: 'Better, Not Just More: Data-Centric Machine Learning for Earth Observation'
arxiv_id: '2312.05327'
source_url: https://arxiv.org/abs/2312.05327
tags:
- data
- learning
- sensing
- remote
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces and categorizes data-centric machine learning\
  \ approaches for geospatial data, shifting focus from model-centric to data-centric\
  \ learning to improve accuracy, generalization, and real-world impact. The authors\
  \ define five data quality criteria\u2014diversity and completeness, accuracy, consistency,\
  \ unbiasedness, and relevance\u2014and review techniques for improving data quality\
  \ across the machine learning cycle."
---

# Better, Not Just More: Data-Centric Machine Learning for Earth Observation

## Quick Facts
- arXiv ID: 2312.05327
- Source URL: https://arxiv.org/abs/2312.05327
- Authors: 
- Reference count: 40
- Primary result: Data-centric approaches improve geospatial ML by addressing data quality issues like sample bias, label noise, and inconsistent performance across data slices.

## Executive Summary
This paper introduces a data-centric machine learning framework for geospatial applications, shifting focus from model architecture to data quality improvement. The authors define five data quality criteria—diversity, accuracy, consistency, unbiasedness, and relevance—and categorize data-centric techniques across the machine learning cycle. Through experiments on land cover classification using Sentinel-2 imagery, they demonstrate that techniques like geographic sample bias correction (KLIEP), confident learning for noise detection, and slice discovery can reveal hidden data patterns and improve model performance. While quantitative gains are modest, the study highlights the importance of data quality in geospatial ML and identifies key research gaps in automated data creation and curation.

## Method Summary
The paper proposes a data-centric ML framework for geospatial tasks, focusing on improving data quality rather than model architecture. The method involves five data quality criteria: diversity and completeness, accuracy, consistency, unbiasedness, and relevance. Techniques are categorized by their position in the ML cycle (creation, curation, utilization, evaluation) and the quality criteria they address. Experiments use ResNet-18 CNN trained on Sentinel-2 images from the DFC2020 dataset, with techniques including KLIEP for geographic sample bias correction, confident learning for pruning noisy data, and slice discovery for model debugging. The model is trained with AdamW optimizer, learning rate 0.001, weight decay 10^-5, and up to 1000 epochs.

## Key Results
- KLIEP geographic sample bias correction improved accuracy in 5 of 7 regions but reduced it in others
- Confident learning identified and removed noisy samples, improving accuracy by 3.45 percentage points
- Slice discovery revealed hidden performance patterns across different data sub-populations
- Data-centric techniques revealed patterns in data and improved model performance, though quantitative gains were modest
- Experiments highlighted the importance of data quality and identified research gaps in automated data curation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shifting from model-centric to data-centric learning improves accuracy and generalization in geospatial tasks.
- Mechanism: By systematically improving data quality across the ML cycle (diversity, accuracy, consistency, unbiasedness, relevance), models are trained on more representative and reliable data, reducing overfitting and improving real-world performance.
- Core assumption: Data quality is a bottleneck that limits model performance more than model architecture or hyperparameter tuning.
- Evidence anchors:
  - [abstract] The authors argue that "shifting the focus towards a complementary data-centric perspective is necessary to achieve further improvements in accuracy, generalization ability, and real impact in end-user applications."
  - [section 2.1] The ML cycle emphasizes that data-centric learning focuses on data creation, curation, evaluation, and deployment, while model-centric learning focuses on model training and evaluation.
  - [corpus] The corpus includes related papers discussing data-centric approaches for geospatial data, indicating a growing interest in this area.
- Break Condition: If the data is already of high quality and diverse, further data-centric improvements may yield diminishing returns compared to model-centric innovations.

### Mechanism 2
- Claim: Data curation techniques like confident learning and KLIEP mitigate label noise and geographic domain shifts, improving model robustness.
- Mechanism: By identifying and removing or reweighting noisy or irrelevant samples, the training data becomes cleaner and more focused, leading to better generalization.
- Core assumption: Label noise and domain shifts are significant sources of error in geospatial datasets.
- Evidence anchors:
  - [abstract] The authors mention that "experiments on a land cover classification task demonstrate the effectiveness of techniques like geographic sample bias correction using KLIEP, pruning noisy data with confident learning, and slice discovery for model debugging."
  - [section 4.3] Experiment 2 shows that confident learning improves accuracy by 3.45 percentage points by identifying and removing noisy samples.
  - [section 4.2] Experiment 1 demonstrates that KLIEP de-biases datasets with geographic distribution shifts, improving classification accuracy in five of seven regions.
- Break Condition: If the dataset is already clean and balanced, further curation may not yield significant improvements.

### Mechanism 3
- Claim: Slice discovery and evaluation techniques reveal hidden data patterns and performance variations, enabling targeted model improvements.
- Mechanism: By analyzing model performance across different data slices, practitioners can identify weaknesses and tailor data collection or model architecture to address specific failure modes.
- Core assumption: Model performance varies significantly across different sub-populations of the data, and these variations are not captured by overall accuracy metrics.
- Evidence anchors:
  - [abstract] The authors highlight "slice discovery for model debugging" as one of the effective techniques demonstrated in their experiments.
  - [section 4.4] Experiment 3 shows how slice discovery reveals different patterns within one class label, which might harm the model's performance, and can be used for data curation.
  - [section 3.4] The section discusses the importance of evaluating model performance on relevant sub-populations that share common characteristics.
- Break Condition: If the data is homogeneous or the model performs uniformly well across all slices, slice discovery may not provide actionable insights.

## Foundational Learning

- Concept: Understanding the ML cycle and the distinction between model-centric and data-centric learning.
  - Why needed here: The paper emphasizes the importance of considering the entire ML cycle and shifting focus from model-centric to data-centric approaches for geospatial data.
  - Quick check question: Can you explain the key differences between model-centric and data-centric learning in the context of geospatial ML?

- Concept: Data quality criteria (diversity, accuracy, consistency, unbiasedness, relevance).
  - Why needed here: The paper defines these criteria and discusses how data-centric techniques can improve each of them to enhance model performance.
  - Quick check question: How would you assess the diversity and completeness of a geospatial dataset for a land cover classification task?

- Concept: Data-centric techniques (data creation, curation, utilization, evaluation).
  - Why needed here: The paper reviews various data-centric techniques and categorizes them based on the ML cycle steps and data quality criteria they address.
  - Quick check question: What are some common data augmentation techniques used in geospatial ML to improve data diversity?

## Architecture Onboarding

- Component map:
  - Data ingestion: Loading and preprocessing geospatial data (e.g., Sentinel-2 images)
  - Data quality assessment: Evaluating data based on diversity, accuracy, consistency, unbiasedness, and relevance criteria
  - Data curation: Applying techniques like confident learning, KLIEP, and slice discovery to improve data quality
  - Model training: Training ML models (e.g., ResNet-18) on curated data
  - Model evaluation: Assessing model performance using metrics and slice discovery
  - Feedback loop: Using insights from evaluation to inform further data curation and model improvements

- Critical path: Data ingestion → Data quality assessment → Data curation → Model training → Model evaluation → Feedback loop

- Design tradeoffs:
  - Data quantity vs. quality: Balancing the amount of data with its quality to avoid overfitting or underfitting
  - Manual vs. automated curation: Choosing between manual data cleaning and automated data-centric techniques based on dataset size and complexity
  - Model complexity vs. interpretability: Selecting models that balance performance with the ability to provide insights into model decisions

- Failure signatures:
  - Overfitting: Model performs well on training data but poorly on unseen data, indicating insufficient data diversity or quality
  - Underfitting: Model performs poorly on both training and test data, suggesting inadequate model capacity or data representation
  - Geographic bias: Model accuracy varies significantly across different geographic regions, indicating domain shift or sampling bias

- First 3 experiments:
  1. Implement KLIEP to de-bias geographic sample bias in a land cover classification task and evaluate its impact on accuracy
  2. Apply confident learning to identify and remove noisy samples from a geospatial dataset and retrain the model to measure accuracy improvement
  3. Use slice discovery to analyze model performance across different data sub-populations and identify areas for improvement in data collection or model architecture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What standardized metrics can be developed to evaluate the quality of geospatial datasets across the five defined criteria (diversity, accuracy, consistency, unbiasedness, relevance)?
- Basis in paper: [explicit] The authors note that unlike model performance metrics, standardized metrics for dataset quality are not established.
- Why unresolved: Current data-centric approaches lack unified, domain-specific quality measures, making cross-study comparisons difficult.
- What evidence would resolve it: Empirical validation of proposed metrics on multiple geospatial datasets showing consistent quality assessment and predictive correlation with model performance.

### Open Question 2
- Question: How can data creation and curation steps be automated to improve diversity, consistency, and unbiasedness without manual intervention?
- Basis in paper: [explicit] The authors identify research gaps in automating data creation and curation for these quality criteria.
- Why unresolved: Existing methods rely heavily on manual sampling and curation, limiting scalability and introducing human bias.
- What evidence would resolve it: Demonstration of fully automated pipelines that generate diverse, consistent, and unbiased geospatial datasets with measurable quality improvements over manually curated data.

### Open Question 3
- Question: Under what conditions do data-centric methods (e.g., KLIEP weighting, confident learning) fail to improve model accuracy, and how can these failures be predicted or mitigated?
- Basis in paper: [explicit] Experiments show that data-centric approaches sometimes have detrimental effects, e.g., KLIEP weighting reduced accuracy in some regions.
- Why unresolved: The paper highlights cases of negative transfer but does not provide a framework to predict when such failures occur.
- What evidence would resolve it: Systematic analysis of failure modes across diverse geospatial tasks, with predictive indicators for when data-centric methods are likely to help or harm.

## Limitations

- Experimental validation relies on a single dataset (DFC2020), limiting generalizability across diverse real-world scenarios
- Modest quantitative gains (3.45 percentage points from confident learning) suggest diminishing returns in well-curated datasets
- Paper does not address computational costs or scalability challenges of data-centric techniques for large-scale satellite imagery

## Confidence

- High: The conceptual framework for data-centric ML and its five data quality criteria
- Medium: The effectiveness of KLIEP for geographic sample bias correction (supported by one dataset)
- Low: The generalizability of confident learning performance improvements across different geospatial tasks

## Next Checks

1. Replicate experiments on multiple diverse geospatial datasets to assess technique robustness
2. Conduct ablation studies to quantify individual contributions of each data-centric method
3. Evaluate computational overhead and scalability of proposed techniques on large-scale satellite imagery collections