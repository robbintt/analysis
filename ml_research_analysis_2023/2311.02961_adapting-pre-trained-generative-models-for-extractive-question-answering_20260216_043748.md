---
ver: rpa2
title: Adapting Pre-trained Generative Models for Extractive Question Answering
arxiv_id: '2311.02961'
source_url: https://arxiv.org/abs/2311.02961
tags:
- answer
- context
- bart
- extractive
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach for extractive question
  answering by adapting pre-trained generative models such as BART and T5. The key
  idea is to generate indexes corresponding to context tokens or sentences that form
  part of the answer, rather than generating the answer tokens directly.
---

# Adapting Pre-trained Generative Models for Extractive Question Answering

## Quick Facts
- arXiv ID: 2311.02961
- Source URL: https://arxiv.org/abs/2311.02961
- Reference count: 4
- Achieves state-of-the-art performance on 4 out of 5 extractive QA datasets

## Executive Summary
This paper introduces a novel approach for extractive question answering by adapting pre-trained generative models such as BART and T5. The key idea is to generate indexes corresponding to context tokens or sentences that form part of the answer, rather than generating the answer tokens directly. This approach addresses the sparsity challenge in extractive QA tasks, where only a small portion of the context contains the answer. The proposed method, which involves generating either full index or span index sequences, outperforms state-of-the-art discriminative models on multiple extractive QA datasets including MultiSpanQA, BioASQ, MASHQA, and WikiQA.

## Method Summary
The method fine-tunes BART-base or BART-large models to generate index sequences that point to answer tokens or sentences in the context. Two variants are explored: Full Index (FI) generates all indexes of answer tokens, while Span Index (SI) generates pairs of start and end indexes for answer spans. Contexts are preprocessed with special index tokens, and post-processing handles invalid sequences by removing unpaired indexes, filtering out-of-range values, and merging overlapping spans. The model is trained with AdamW optimizer (learning rate 2e-5, weight decay 1e-4, batch size 8) and context trimming ensures inputs fit within BART's 1024 token limit.

## Key Results
- Achieves new state-of-the-art performance on 4 out of 5 extractive QA datasets
- Improves F1 score by up to 4% compared to previous best results
- Demonstrates strong generalization across different extractive QA tasks without task-specific modifications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Index generation reduces output space sparsity compared to direct token generation
- Mechanism: By generating indexes of answer tokens/sentences rather than answer tokens themselves, the model constrains the output space to a much smaller set of possibilities, making the generation task more tractable
- Core assumption: The model can learn to map context to answer indexes more efficiently than mapping context directly to answer tokens
- Evidence anchors:
  - [abstract] "generating indexes corresponding to context tokens or sentences that form part of the answer"
  - [section] "we propose a novel approach: generating the indexes of context tokens or sentences that form part of the extractive answer"
- Break condition: If the context is extremely long relative to answer length, making index mapping itself sparse

### Mechanism 2
- Claim: Index-based representation makes model less sensitive to label sparsity
- Mechanism: Generative models model likelihood of output tokens conditioned on input, so generating indexes (which are fewer in number than answer tokens) reduces the sparsity challenge that discriminative models face when only small portions of context contain answers
- Core assumption: Generative models inherently handle sparse distributions better than discriminative models
- Evidence anchors:
  - [abstract] "Discriminative models often encounter challenges associated with label sparsity, particularly when only a small portion of the context contains the answer"
  - [section] "The sparsity challenge encountered in extractive question answering is less daunting for generative approaches, as they explicitly model what is likely (via likelihood) rather than what is unlikely"
- Break condition: If answer spans are distributed so sparsely that even index representation becomes too sparse

### Mechanism 3
- Claim: Post-processing invalid index sequences recovers valid answers
- Mechanism: Since generative models don't constrain index generation order, invalid sequences (odd length, out-of-range, overlapping spans) are filtered and repaired during post-processing to produce valid extractive answers
- Core assumption: Invalid index sequences are rare enough that post-processing can fix most cases
- Evidence anchors:
  - [section] "During the inference phase, indexes may be generated in a non-sequential order, duplicates may appear, and, in the worst-case scenario, out-of-range indexes can emerge"
  - [section] "To address these complexities, our post-processing strategy involves: (i) Pruning unpaired last indexes. (ii) Removing spans that are invalid or out of range. (iii) Merging overlapping spans"
- Break condition: If too many invalid sequences are generated, post-processing becomes insufficient

## Foundational Learning

- Concept: Sequence-to-sequence generation with autoregressive models
  - Why needed here: The core approach relies on fine-tuning BART/T5 to generate index sequences autoregressively
  - Quick check question: What is the difference between teacher forcing and free-running generation in seq2seq models?

- Concept: Pointer-generator networks and copy mechanisms
  - Why needed here: The approach leverages the model's ability to "copy" from input context by generating indexes that point to answer tokens/sentences
  - Quick check question: How does a pointer network differ from a standard attention mechanism in seq2seq models?

- Concept: Post-processing of sequence generation outputs
  - Why needed here: The model generates potentially invalid index sequences that must be cleaned and converted to valid answers
  - Quick check question: What are common strategies for handling out-of-order or duplicate outputs in sequence generation?

## Architecture Onboarding

- Component map: Input preprocessor -> BART/T5 model -> Post-processor -> Context trimmer
- Critical path:
  1. Preprocess context with index tokens
  2. Generate index sequence using fine-tuned BART/T5
  3. Post-process to handle invalid indexes and merge spans
  4. Extract answer from context using processed indexes

- Design tradeoffs:
  - Using indexes vs direct token generation: Indexes reduce sparsity but require post-processing
  - Full index vs span index: Full index simpler but longer; span index more compact but needs pair handling
  - Context trimming: Necessary for BART but may remove non-answer content that could help reasoning

- Failure signatures:
  - High percentage of invalid index sequences requiring post-processing
  - Generated indexes consistently out of range (indicates model misunderstanding)
  - Similar performance with/without index tokens in context (indicates model not using them)

- First 3 experiments:
  1. Train BART_FIbase on MultiSpanQA with and without index tokens in context to verify the index token effect
  2. Compare BART_FIbase vs BART_SIbase on BioASQ to see which index representation works better for factoid QA
  3. Test post-processing robustness by deliberately generating invalid sequences and measuring recovery rate

## Open Questions the Paper Calls Out
- No explicit open questions are called out in the paper. The discussion focuses on the proposed method and results rather than future research directions.

## Limitations
- Context length limitation: The approach is constrained by BART's 1024 token limit, requiring context trimming that may affect answer quality
- Post-processing dependency: The model generates potentially invalid index sequences requiring complex post-processing to recover valid answers
- No analysis of longer contexts: The paper doesn't explore how performance scales with contexts exceeding the token limit

## Confidence
- High Confidence: The core mechanism of generating indexes instead of answer tokens reduces output space sparsity; the approach achieves state-of-the-art performance on the tested datasets; post-processing is necessary to handle invalid index sequences from generative models
- Medium Confidence: Index generation is inherently better than discriminative approaches for label sparsity; the span index representation is more efficient than full index for certain question types; context trimming only minimally affects answer quality
- Low Confidence: The approach generalizes well to contexts longer than 1024 tokens; the post-processing strategy handles all edge cases effectively; performance gains are consistent across all extractive QA subtasks

## Next Checks
- Check 1: Implement the described post-processing pipeline and measure: (a) percentage of generated sequences requiring post-processing, (b) success rate of recovering valid answers from invalid sequences, and (c) impact on final F1 scores
- Check 2: For each dataset, analyze: (a) distribution of context lengths relative to 1024 tokens, (b) percentage of answers affected by trimming, and (c) performance comparison between full context and trimmed context for a subset of examples
- Check 3: Create extended versions of test examples by concatenating contexts up to 2048 tokens, then measure: (a) percentage of answers that fall outside the first 1024 tokens, and (b) performance degradation when using only the first 1024 tokens versus the full context