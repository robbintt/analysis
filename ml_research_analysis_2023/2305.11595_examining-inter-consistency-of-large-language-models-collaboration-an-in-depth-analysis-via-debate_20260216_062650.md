---
ver: rpa2
title: 'Examining Inter-Consistency of Large Language Models Collaboration: An In-depth
  Analysis via Debate'
arxiv_id: '2305.11595'
source_url: https://arxiv.org/abs/2305.11595
tags:
- debate
- answer
- llms
- chatgpt
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the inter-consistency problem among large
  language models (LLMs) through a novel debate framework called FORD. The authors
  conduct experiments on 7 commonsense reasoning datasets using three debate scenarios:
  fair debates between comparable LLMs, mismatched debates between LLMs with different
  reasoning abilities, and roundtable debates among three LLMs.'
---

# Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate

## Quick Facts
- arXiv ID: 2305.11595
- Source URL: https://arxiv.org/abs/2305.11595
- Reference count: 12
- Key outcome: Debate-based collaboration improves inter-consistency and reasoning performance of LLMs across multiple commonsense reasoning datasets

## Executive Summary
This paper investigates the inter-consistency problem among large language models (LLMs) through a novel debate framework called FORD. The authors conduct experiments on 7 commonsense reasoning datasets using three debate scenarios: fair debates between comparable LLMs, mismatched debates between LLMs with different reasoning abilities, and roundtable debates among three LLMs. Their findings show that debate significantly improves inter-consistency and reasoning performance across most settings, with LLMs learning from each other's arguments. However, stronger LLMs tend to dominate mismatched debates, and GPT-4 as a judge provides more nuanced evaluations by weighting different arguments. The results demonstrate that debate is an effective approach for enhancing both inter-consistency and performance of LLMs, while also providing insights into their collaborative behaviors and decision-making processes.

## Method Summary
The study employs a three-stage debate framework (FORD) to examine LLM collaboration on commonsense reasoning tasks. LLMs generate predictions with explanations using few-shot chain-of-thought prompting, then engage in iterative debate rounds to resolve inconsistencies. The framework includes fair debates between comparable models, mismatched debates highlighting dominance effects, and roundtable debates among three participants. A judge LLM (often GPT-4) evaluates arguments and determines final consensus. Performance is measured against synthetic baselines (Syn-Soft and Syn-Hard) across seven commonsense reasoning datasets.

## Key Results
- Debate significantly improves inter-consistency and reasoning performance across all tested scenarios
- Stronger LLMs consistently dominate weaker counterparts in mismatched debates
- GPT-4 as judge provides more accurate weighted evaluations than simple averaging
- Roundtable debates show additional benefits compared to pairwise debates
- Inter-inconsistency reduction is greatest when initial inconsistency levels are highest

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Debate-based collaboration improves inter-consistency and reasoning performance of LLMs.
- Mechanism: When LLMs with different initial predictions engage in debate, they are exposed to alternative reasoning perspectives. This exposure enables them to identify flaws or gaps in their own arguments and refine their stances through refutation or compromise. The iterative exchange of arguments allows the models to converge on a more consistent and accurate answer than either model would produce individually.
- Core assumption: LLMs can effectively process and integrate counterarguments in a human-like way, leading to mutual learning and improved reasoning.
- Evidence anchors:
  - [abstract] "LLMs can effectively collaborate to reach a consensus despite noticeable inter-inconsistencies, but imbalances in their abilities can lead to domination by superior LLMs."
  - [section 4.1] "For ChatGPT & Davinci-003 and LLaMA & Vicuna, FORD significantly outperforms Syn-Soft and Syn-Hard on all datasets. These results indicate that LLMs can improve each other by learning the differences between their arguments through interactive debate."
  - [corpus] Weak: No direct citation, inferred from related work on multi-agent debate.
- Break condition: If the LLMs fail to generate coherent counterarguments or if one model completely dominates the debate without meaningful exchange, the mechanism breaks down and no mutual improvement occurs.

### Mechanism 2
- Claim: Stronger LLMs dominate debates when paired with weaker counterparts, leading to performance ceilings tied to the stronger model.
- Mechanism: In mismatched debates, the superior model's reasoning capability overwhelms the weaker model's arguments. The weaker model either concedes or fails to present compelling counterpoints, resulting in the stronger model's stance prevailing. This mirrors human debate dynamics where more knowledgeable participants dominate.
- Core assumption: The debate framework allows for clear dominance signals where weaker models consistently concede to stronger ones.
- Evidence anchors:
  - [abstract] "We interestingly discover the absolutely strong LLM would dominate the debate, which aligns with the situations in humans."
  - [section 4.2] "GPT-4 is a much stronger commonsense reasoner than ChatGPT, and ChatGPT is much stronger than LLaMA. This proves the two debates are mismatched."
  - [corpus] Weak: No direct citation, inferred from human debate literature.
- Break condition: If the weaker model can still present valid counterarguments that the stronger model fails to address, the dominance pattern breaks and mutual learning may occur.

### Mechanism 3
- Claim: Using a more capable LLM as a judge improves debate outcomes by assigning appropriate weights to different arguments.
- Mechanism: A judge with superior reasoning ability can evaluate the quality of each argument and assign different weights based on persuasiveness rather than treating all arguments equally. This weighted evaluation produces more accurate final conclusions than simple averaging or majority voting.
- Core assumption: The judge LLM can accurately assess argument quality and assign appropriate weights that reflect true reasoning merit.
- Evidence anchors:
  - [abstract] "Leveraging a more advanced LLM like GPT-4 as an authoritative judge can boost collaboration performance."
  - [section 5] "If we utilize GPT-4 as the judge, the performance increases to varying degrees. It is mainly because different arguments have different persuasions, treating them equally would make less plausible arguments heavily influence the final conclusion."
  - [corpus] Weak: No direct citation, inferred from human debate judging practices.
- Break condition: If the judge LLM is biased, inconsistent in its evaluation criteria, or unable to distinguish argument quality, the weighted evaluation fails and performance gains disappear.

## Foundational Learning

- Concept: Confusion matrix analysis for measuring inter-consistency
  - Why needed here: The confusion matrix quantifies how often two LLMs agree or disagree on predictions, providing the foundation for measuring inter-consistency before and after debate.
  - Quick check question: Given a confusion matrix M with entries M11=100, M12=20, M21=15, M22=65, what is the inter-consistency percentage?

- Concept: Chain-of-thought prompting for reasoning
  - Why needed here: The paper uses few-shot chain-of-thought prompting to elicit reasoning from LLMs before debate, ensuring they generate explanations for their answers that can be debated.
  - Quick check question: What is the primary difference between zero-shot and few-shot chain-of-thought prompting?

- Concept: Synthetic methods (Syn-Soft and Syn-Hard)
  - Why needed here: These methods provide baseline comparisons for measuring debate improvement, with Syn-Soft averaging accuracies and Syn-Hard requiring both models to be correct.
  - Quick check question: If two LLMs have accuracies of 80% and 70% respectively, what would their Syn-Soft and Syn-Hard accuracies be if they agree on 60% of examples?

## Architecture Onboarding

- Component map: Datasets -> LLM pairs -> Debate framework -> Judge component -> Evaluation metrics
- Critical path:
  1. Load dataset and generate LLM predictions with explanations
  2. Identify inter-inconsistent examples
  3. Execute debate rounds until consensus or max rounds
  4. Generate final conclusion through summarization
  5. Evaluate performance against baselines
- Design tradeoffs:
  - Debate round limit vs. computational cost
  - Equal argument weighting vs. judge-weighted evaluation
  - Chat model vs. text completion model debate formats
  - Single judge vs. ensemble judge approach
- Failure signatures:
  - LLMs generating repetitive or non-responsive arguments
  - One model consistently dominating without meaningful exchange
  - Debate rounds failing to reduce inter-inconsistency
  - Judge producing inconsistent evaluations
- First 3 experiments:
  1. Run debate between ChatGPT and Davinci-003 on COPA dataset, track inter-inconsistency reduction per round
  2. Test mismatched debate between LLaMA and ChatGPT on e-CARE dataset, measure dominance metrics
  3. Implement roundtable debate with ChatGPT, Davinci-003, and GPT-4 on PIQA dataset, compare to pairwise debates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different debate formats (e.g., Lincoln-Douglas, parliamentary, policy) affect the inter-consistency outcomes when applied to LLM collaboration?
- Basis in paper: [inferred] The paper uses a formal debate framework inspired by classroom debate theory but does not explore variations in debate formats.
- Why unresolved: The study focuses on a single debate framework without comparing it to alternative debate structures that might yield different results.
- What evidence would resolve it: Comparative experiments testing multiple debate formats on the same datasets to measure differences in inter-consistency and performance outcomes.

### Open Question 2
- Question: What is the optimal number of debate rounds needed to achieve maximum inter-consistency without causing diminishing returns or model fatigue?
- Basis in paper: [explicit] The paper sets different maximum rounds (4-9) for different debate scenarios but does not systematically analyze the relationship between debate rounds and performance gains.
- Why unresolved: The study uses fixed maximum rounds rather than determining the point at which additional debate rounds no longer improve outcomes.
- What evidence would resolve it: Experiments varying the number of debate rounds while measuring inter-consistency improvement rates to identify the optimal stopping point.

### Open Question 3
- Question: How does the introduction of external knowledge sources during debate affect the quality of consensus and reasoning performance compared to debate based solely on model-generated arguments?
- Basis in paper: [inferred] The debate framework relies entirely on model-generated arguments without incorporating external knowledge sources or fact-checking mechanisms.
- Why unresolved: The study does not investigate whether access to external knowledge during debate would improve the quality of arguments and consensus outcomes.
- What evidence would resolve it: Experiments comparing debate performance with and without access to external knowledge bases to measure differences in reasoning accuracy and argument quality.

### Open Question 4
- Question: What specific linguistic or reasoning patterns in LLM-generated arguments are most predictive of successful consensus-building versus continued disagreement?
- Basis in paper: [inferred] The study tracks inter-consistency changes but does not analyze the linguistic features of arguments that lead to compromise versus continued conflict.
- Why unresolved: The paper measures outcomes but does not perform detailed linguistic analysis of what makes certain arguments more persuasive or conducive to consensus.
- What evidence would resolve it: Linguistic analysis of debate transcripts to identify argument features (e.g., concession language, evidence quality, reasoning structure) that correlate with successful consensus.

### Open Question 5
- Question: How does the initial inter-consistency level between LLMs affect the potential improvement through debate, and is there a threshold below which debate becomes ineffective?
- Basis in paper: [explicit] The paper shows that higher initial inter-inconsistency leads to more improvement but does not establish whether there is a minimum threshold for debate effectiveness.
- Why unresolved: The study demonstrates correlation between initial inconsistency and improvement but does not determine if extremely low initial consistency prevents effective debate.
- What evidence would resolve it: Experiments systematically varying initial inter-consistency levels to identify the minimum threshold at which debate still produces meaningful improvements in reasoning performance.

## Limitations

- Reliance on LLM APIs rather than local deployment makes results sensitive to API version changes and rate limits
- Synthetic baseline methods (Syn-Soft and Syn-Hard) are relatively simple and may not capture full complexity of LLM collaboration dynamics
- Dominance patterns in mismatched debates may be dataset-specific rather than generalizable across different reasoning domains

## Confidence

**High Confidence**: The core finding that debate improves inter-consistency and reasoning performance is well-supported by experimental results across multiple datasets and LLM pairs. The mechanism by which LLMs learn from each other's arguments during debate is clearly demonstrated through the INCON reduction metrics.

**Medium Confidence**: The claim that stronger LLMs consistently dominate weaker ones in mismatched debates is supported by the data but may depend on specific task characteristics and LLM configurations. The effectiveness of GPT-4 as a judge shows promise but requires further validation across different debate scenarios.

**Low Confidence**: The generalizability of roundtable debate benefits beyond the specific LLMs tested remains uncertain. The paper's findings on how debate dynamics mirror human debate behavior are largely speculative and lack direct empirical validation.

## Next Checks

1. **Dataset Diversity Test**: Replicate the experiments across a broader range of reasoning tasks beyond commonsense (e.g., logical reasoning, mathematical problem-solving) to verify if debate benefits generalize across different cognitive domains.

2. **Model Configuration Sensitivity**: Test how results vary with different temperature settings, max generation lengths, and debate round limits to establish robustness boundaries for the debate framework.

3. **Long-term Consistency Analysis**: Implement multiple debate sessions over time with the same LLM pairs to examine whether inter-consistency improvements are sustained or if models develop persistent biases based on debate experiences.