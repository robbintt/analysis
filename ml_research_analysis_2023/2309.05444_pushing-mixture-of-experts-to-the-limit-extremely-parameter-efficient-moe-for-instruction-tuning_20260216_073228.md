---
ver: rpa2
title: 'Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE
  for Instruction Tuning'
arxiv_id: '2309.05444'
source_url: https://arxiv.org/abs/2309.05444
tags:
- experts
- median
- fine-tuning
- routing
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of scaling instruction tuning
  for large language models while maintaining computational efficiency. It proposes
  an extremely parameter-efficient Mixture of Experts (MoE) framework that combines
  MoE architecture with lightweight experts, such as (IA)3 vectors or LORA adapters.
---

# Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning

## Quick Facts
- arXiv ID: 2309.05444
- Source URL: https://arxiv.org/abs/2309.05444
- Authors:
- Reference count: 15
- Achieves performance parity with full fine-tuning on unseen tasks by updating less than 1% of parameters in models up to 11B

## Executive Summary
This paper addresses the challenge of scaling instruction tuning for large language models while maintaining computational efficiency. It proposes an extremely parameter-efficient Mixture of Experts (MoE) framework that combines MoE architecture with lightweight experts, such as (IA)3 vectors or LORA adapters. The method achieves performance parity with full fine-tuning on unseen tasks by updating less than 1% of the parameters in models up to 11B parameters. Specifically, the Mixture of Vectors (MoV) approach achieves up to 14.57% improvement over standard (IA)3 and is on par with full fine-tuning at 3B and 11B scales. The results are consistent across 12 different tasks from 55 datasets, demonstrating the method's effectiveness and scalability.

## Method Summary
The method combines Mixture of Experts architecture with lightweight parameter-efficient fine-tuning (PEFT) experts like (IA)3 vectors and LORA adapters. A soft merging router combines expert outputs through weighted averaging before applying the PEFT transformation, allowing all experts to contribute rather than selecting only the top-k. The approach is evaluated on T5 v1.1+LM adaptation models (770M, 3B, 11B) trained on 12 tasks from the P3 dataset and evaluated on 8 unseen tasks using zero-shot performance metrics.

## Key Results
- MoV achieves up to 14.57% improvement over standard (IA)3
- Performance parity with full fine-tuning achieved at 3B and 11B model scales
- Less than 1% of parameters updated while maintaining competitive performance
- Token routing outperforms sentence embedding routing across all model sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MoV achieves competitive performance with full fine-tuning by combining soft-merging of lightweight PEFT experts, reducing parameter updates to less than 1% while preserving model capacity.
- Mechanism: Soft merging computes a weighted average of experts first, then applies a PEFT transformation using the combined expert, allowing the router to leverage all experts rather than a sparse top-k selection.
- Core assumption: The router can learn to combine multiple PEFT experts effectively even with minimal parameter updates, and the soft merging retains sufficient task-specific adaptation.
- Evidence anchors:
  - [abstract] states MoV "achieves performance parity with full fine-tuning on unseen tasks by updating less than 1% of the parameters."
  - [section 2.2] describes the soft merging formula: "Emix = Σ si · Ei; y = Emix(x)".
  - [corpus] lacks direct evidence of MoV soft merging efficacy but cites MoV's superior performance over single PEFT methods.
- Break condition: If the router fails to learn meaningful weights or the combined expert cannot approximate the full fine-tuning adaptation, performance collapses.

### Mechanism 2
- Claim: Token routing outperforms sentence embedding routing because token embeddings provide richer, more granular task signals than coarse sentence-level embeddings.
- Mechanism: The router receives intermediate token embeddings directly, capturing fine-grained contextual cues that sentence embeddings abstract away, leading to better expert activation patterns.
- Core assumption: Token embeddings contain sufficient task-relevant information for routing, and sentence embeddings are too generic to differentiate tasks effectively.
- Evidence anchors:
  - [section 4.4] reports "token routing exhibits superior performance with 3.03%, 8.86%, and 0.94% improvement for 770M, 3B, and 11B base model sizes respectively."
  - [corpus] does not provide external validation of this routing choice.
- Break condition: If task-specific patterns are more salient at sentence level or if token-level noise overwhelms signal, sentence embedding routing could surpass token routing.

### Mechanism 3
- Claim: Scaling the number of experts improves performance up to a model-size-dependent optimum, reflecting a trade-off between expert specialization and parameter budget.
- Mechanism: More experts allow finer-grained specialization, but diminishing returns and overfitting risk emerge beyond a certain point, especially in smaller models.
- Core assumption: Experts can learn complementary, non-redundant skills, and the router can effectively balance their contributions.
- Evidence anchors:
  - [section 4.2] states "increasing the number of experts generally improves unseen task performance" and identifies optimal expert counts per model size.
  - [corpus] lacks evidence of expert specialization or capacity saturation effects.
- Break condition: If experts converge to similar behaviors or if parameter budget limits meaningful differentiation, adding experts will no longer help.

## Foundational Learning

- Concept: Mixture of Experts (MoE) architecture
  - Why needed here: MoE provides conditional computation and parameter efficiency, which this work extends to parameter-efficient fine-tuning.
  - Quick check question: How does an MoE layer differ from a dense layer in terms of computation and parameter usage?

- Concept: Parameter-efficient fine-tuning (PEFT) methods
  - Why needed here: PEFT methods like (IA)³ and LORA enable large model adaptation with minimal parameter updates, forming the basis for lightweight experts.
  - Quick check question: What is the key difference between (IA)³ vectors and LORA in terms of how they modify model activations?

- Concept: Soft vs. discrete routing strategies
  - Why needed here: The choice of routing strategy (soft merging vs. top-k) directly impacts performance and parameter efficiency in this MoE variant.
  - Quick check question: In what scenario would discrete top-k routing be preferable to soft merging in a parameter-efficient MoE?

## Architecture Onboarding

- Component map:
  - Token embeddings -> Router -> Expert weights -> Experts (PEFT adapters) -> Combined expert -> Next layer

- Critical path:
  1. Token embeddings fed into router → expert weights computed
  2. Experts (PEFT adapters) activated per weights → combined expert output
  3. Combined expert applied to input → next layer

- Design tradeoffs:
  - Soft merging vs. top-k routing: Soft merging better for tiny experts; top-k better for large, diverse experts
  - Expert count scaling: More experts → more specialization but higher parameter count and potential overfitting
  - Token vs. sentence routing: Token routing captures finer signals but may introduce noise

- Failure signatures:
  - Router collapse: All routing weights concentrate on a single expert → performance matches dense PEFT baseline
  - Expert redundancy: Multiple experts learn similar transformations → wasted parameter budget
  - Batch size sensitivity: Large batches cause router instability → training collapse

- First 3 experiments:
  1. Compare MoV with 5 vs. 10 experts on 770M T5 to validate scaling effect
  2. Test token vs. sentence routing on 3B T5 to confirm routing strategy impact
  3. Evaluate soft merging vs. top-2 routing on 11B T5 to isolate routing effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MoV and MoLORA scale when applied to larger models beyond 11B parameters?
- Basis in paper: [inferred] The paper demonstrates effectiveness up to 11B parameters and mentions that scaling more experts generally improves performance, but does not test beyond this size.
- Why unresolved: The authors only tested up to 11B parameters and did not explore the performance at larger scales, which is critical for understanding the method's applicability to state-of-the-art models.
- What evidence would resolve it: Experimental results showing performance trends for MoV and MoLORA on models larger than 11B parameters, particularly in the range of 100B+ parameters.

### Open Question 2
- Question: How does the mixture of experts approach compare to other parameter-efficient fine-tuning methods like prefix tuning or adapters in terms of both performance and computational efficiency?
- Basis in paper: [explicit] The paper compares MoV and MoLORA to (IA)³ and LORA, but does not include other PEFT methods like prefix tuning or adapters.
- Why unresolved: The study focuses on comparing against (IA)³ and LORA, leaving a gap in understanding how MoE-based approaches perform relative to other established PEFT methods.
- What evidence would resolve it: Comparative experiments evaluating MoV and MoLORA against a broader range of PEFT methods including prefix tuning, adapters, and other recent techniques.

### Open Question 3
- Question: What is the impact of different routing strategies (e.g., top-k vs. soft merging) on the model's ability to generalize to unseen tasks, and how does this vary with the number of experts?
- Basis in paper: [explicit] The paper compares soft merging with top-k routing strategies and finds that soft merging performs better, but does not explore the full range of possible routing strategies or their interaction with the number of experts.
- Why unresolved: While the paper provides initial comparisons, it does not exhaustively explore the space of routing strategies or how they interact with the number of experts in terms of generalization performance.
- What evidence would resolve it: Comprehensive experiments varying both the routing strategy and the number of experts, with detailed analysis of how these factors influence generalization to unseen tasks.

## Limitations

- Evaluation limited to zero-shot performance without few-shot or fine-tuned comparisons
- Soft merging mechanism lacks theoretical justification for preserving task adaptation
- No computational overhead analysis or wall-clock training time comparisons provided

## Confidence

- **High Confidence**: MoV achieves superior performance to single PEFT methods like (IA)3; Token routing outperforms sentence-level routing; Expert count scaling shows consistent improvements up to model-dependent optima; Parameter efficiency is maintained (less than 1% of parameters updated)
- **Medium Confidence**: MoV achieves performance parity with full fine-tuning; Soft merging is the optimal routing strategy for this architecture; The identified optimal expert counts generalize across all task types
- **Low Confidence**: The mechanism by which soft merging preserves task adaptation; Long-term stability and generalization beyond zero-shot evaluation; Performance consistency across diverse real-world instruction scenarios

## Next Checks

1. **Ablation of Soft Merging Mechanism**: Replace soft merging with top-k routing using the same expert architecture to isolate the contribution of the combination strategy versus the expert design.

2. **Generalization Across Datasets**: Evaluate the 3B and 11B models on additional instruction datasets (e.g., Super-NaturalInstructions, FLAN) to test robustness beyond the P3 benchmark.

3. **Scaling Behavior Analysis**: Systematically vary expert count and router capacity across all three model sizes to identify precise inflection points where performance gains plateau or degrade, confirming the claimed scaling relationships.