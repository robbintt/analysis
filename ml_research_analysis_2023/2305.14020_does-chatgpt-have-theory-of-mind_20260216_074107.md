---
ver: rpa2
title: Does ChatGPT have Theory of Mind?
arxiv_id: '2305.14020'
source_url: https://arxiv.org/abs/2305.14020
tags:
- chatgpt
- will
- option
- which
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study examined ChatGPT's Theory of Mind (ToM) abilities by
  testing two versions (ChatGPT-3 and ChatGPT-4) on six well-known decision-making
  problems from behavioral psychology literature. ChatGPT-3 correctly answered approximately
  50% of questions (147/270), showing no significant difference from chance, while
  ChatGPT-4 achieved 80% accuracy (224/270) with statistically significant performance.
---

# Does ChatGPT have Theory of Mind?

## Quick Facts
- arXiv ID: 2305.14020
- Source URL: https://arxiv.org/abs/2305.14020
- Authors: 
- Reference count: 10
- Primary result: ChatGPT-4 achieves 80% accuracy on Theory of Mind tasks versus ChatGPT-3's 50%, with significant performance differences

## Executive Summary
This study examines whether ChatGPT models possess Theory of Mind (ToM) capabilities by testing them on six well-known decision-making problems from behavioral psychology literature. Two versions of ChatGPT (3 and 4) were evaluated using prompts varying in detail level, with each prompt posed nine times to account for the model's stochastic nature. The results show that ChatGPT-4 significantly outperforms ChatGPT-3 on ToM tasks, while ChatGPT-3 performs at chance level. The study highlights important limitations, including the possibility that correct answers may result from incorrect reasoning rather than genuine understanding.

## Method Summary
The study tested ChatGPT-3 and ChatGPT-4 on six ToM problems adapted from Kahneman's "Thinking, Fast and Slow." For each problem, five prompts with increasing detail levels were created (p1 basic to p5 high detail with different examples), and each prompt was posed nine times to both models. Performance was measured by correct answer rates compared to chance (50%), with statistical significance determined using binomial tests (p < 0.05 indicates significance). The default temperature parameter of 0.7 was used to reflect real-world conditions, acknowledging this introduces stochastic variability in responses.

## Key Results
- ChatGPT-3 answered approximately 50% of questions correctly (147/270), showing no significant difference from chance
- ChatGPT-4 achieved 80% accuracy (224/270) with statistically significant performance (p < 0.05)
- ChatGPT-4 outperformed ChatGPT-3 on most ToM problems, particularly on "mental shotgun" and "planning fallacy" tasks
- Performance varied with prompt detail level for ChatGPT-4 but remained consistent for ChatGPT-3
- Both versions showed limitations, as correct answers often resulted from incorrect reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT-4 achieves significantly higher accuracy on Theory of Mind tasks than ChatGPT-3.
- Mechanism: Larger model size and training data improve ability to recognize and predict human decision-making patterns described in behavioral psychology.
- Core assumption: The correct answers to ToM problems are present in ChatGPT's training data and can be retrieved through appropriate prompting.
- Evidence anchors:
  - [abstract] "ChatGPT-4 achieved 80% accuracy (224/270) with statistically significant performance. ChatGPT-4 outperformed ChatGPT-3 on most ToM problems"
  - [section 5] "A binomial test was conducted and the results show that the difference between the number of correct and incorrect answers is significant ( p < 0.05)"
  - [corpus] Weak - no direct citations to supporting literature on model size effects on ToM reasoning

### Mechanism 2
- Claim: ChatGPT-4's performance varies with prompt detailedness while ChatGPT-3's remains consistent.
- Mechanism: ChatGPT-4 can better process and utilize additional contextual information in prompts, while ChatGPT-3 lacks this capability.
- Core assumption: The models process prompts differently based on their architecture and training.
- Evidence anchors:
  - [section 5] "It can be observed that ChatGPT-3 has a similar correct answer average over all prompts, while ChatGPT-4 shows differences in the averages of correct answers over each prompt"
  - [abstract] "Performance varied with prompt detail level for ChatGPT-4 but remained consistent for ChatGPT-3"
  - [corpus] Weak - no literature cited on how model architecture affects prompt processing

### Mechanism 3
- Claim: ChatGPT's stochastic nature affects consistency of responses to identical prompts.
- Mechanism: The temperature parameter introduces randomness in token selection, causing different outputs for the same input.
- Core assumption: The default temperature setting of 0.7 creates significant variability in responses.
- Evidence anchors:
  - [section 6] "we found that ChatGPT exhibited a prominent stochastic element in its answers i.e. answering 'A' in one instance while answering 'B' the next time"
  - [section 6] "The temperature parameter has a value between 0 and 1, 1 being a high degree of randomness"
  - [corpus] Moderate - references to OpenAI documentation on temperature parameter

## Foundational Learning

- Concept: Statistical significance testing (binomial test)
  - Why needed here: To determine if ChatGPT's correct answers exceed what would be expected by random chance
  - Quick check question: If ChatGPT answers 147/270 questions correctly, what is the probability this occurs by chance versus indicating actual understanding?

- Concept: False-belief tasks and Theory of Mind
  - Why needed here: The study uses ToM problems to assess ChatGPT's understanding of human mental states and decision-making
  - Quick check question: What is the Sally-Anne test and how does it measure Theory of Mind in children?

- Concept: Cognitive biases in decision-making
  - Why needed here: The study tests ChatGPT's understanding of specific biases like anchoring effect, planning fallacy, and loss aversion
  - Quick check question: How does the anchoring effect influence human estimates and why might this be difficult for AI to predict?

## Architecture Onboarding

- Component map: Prompt design -> Response collection (9 iterations per prompt) -> Statistical analysis -> Comparison between models and prompt types
- Critical path: Prompt design → Response collection (9 iterations per prompt) → Statistical analysis → Comparison between models and prompt types
- Design tradeoffs: Using default temperature values maintains real-world applicability but introduces stochastic variability; collecting 9 iterations per prompt increases confidence but requires more resources
- Failure signatures: Inconsistent results across iterations suggest stochastic effects; lack of significance in binomial tests suggests chance performance; poor performance on specific problem types indicates model limitations
- First 3 experiments:
  1. Mental Shotgun: Test understanding of involuntary cognitive processing using word pair rhyming tasks
  2. Anchoring Effect: Test recognition of how prior information influences estimates using age guessing tasks
  3. The Linda Problem: Test comprehension of conjunction fallacy using character description tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would ChatGPT's performance change if the temperature parameter was set to a lower value to reduce randomness?
- Basis in paper: [explicit] The paper discusses that ChatGPT's stochastic nature, controlled by the temperature parameter (default 0.7), poses challenges for consistent performance and that a lower temperature might improve accuracy
- Why unresolved: The study used ChatGPT's default temperature settings to reflect real-world usage rather than API control, leaving the impact of parameter adjustment unexplored
- What evidence would resolve it: Testing ChatGPT with various temperature settings while keeping other variables constant to measure changes in accuracy and consistency

### Open Question 2
- Question: Would a more systematic approach to prompt engineering (e.g., consistently adding specific details in a defined order) reveal different patterns in ChatGPT's performance?
- Basis in paper: [explicit] The authors acknowledge their prompt engineering was somewhat arbitrary and suggest a more systematic approach could provide better insights into what information ChatGPT struggles with
- Why unresolved: The study incrementally added different details in each experiment rather than following a consistent pattern across all experiments
- What evidence would resolve it: Conducting experiments where prompts are systematically varied (e.g., always adding names first, then locations, then specific details) to identify which types of information most affect performance

### Open Question 3
- Question: How would analyzing ChatGPT's reasoning explanations, rather than just final answers, change our understanding of its Theory of Mind capabilities?
- Basis in paper: [explicit] The authors note that correct answers were often arrived at through incorrect reasoning, and they acknowledge this limitation by focusing only on absolute answers rather than reasoning quality
- Why unresolved: The study only recorded whether ChatGPT chose answer A or B without examining the explanations provided, which might reveal deeper understanding or fundamental misconceptions
- What evidence would resolve it: Analyzing the reasoning provided in ChatGPT's responses alongside the final answers to determine if correct answers stem from understanding or pattern matching

## Limitations

- Correct answers may result from incorrect reasoning rather than genuine Theory of Mind understanding
- Stochastic response variability requires multiple iterations per prompt, increasing resource requirements
- Performance improvements may reflect pattern recognition in training data rather than true cognitive capability

## Confidence

- **ChatGPT-4 significantly outperforms ChatGPT-3 on ToM tasks**: Medium confidence
- **Prompt detail level affects ChatGPT-4 but not ChatGPT-3 performance**: Medium confidence
- **Stochastic response variability affects result consistency**: High confidence

## Next Checks

1. **Reasoning Quality Analysis**: Conduct a detailed qualitative analysis of ChatGPT's reasoning for correct answers, distinguishing between valid logical reasoning and coincidental pattern matching. This would involve human evaluation of answer explanations to assess whether the model demonstrates genuine understanding of cognitive biases.

2. **Training Data Attribution Test**: Design experiments to determine whether ChatGPT's performance correlates with specific training data content. This could involve testing on problems with varying degrees of similarity to publicly available behavioral psychology literature versus novel scenarios.

3. **Temperature Parameter Sensitivity Analysis**: Systematically vary the temperature parameter across a wider range (0.1 to 1.0) while maintaining consistent prompt content to quantify the relationship between randomness settings and answer stability. This would help establish optimal parameters for reliable ToM assessment.