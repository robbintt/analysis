---
ver: rpa2
title: Positioning Political Texts with Large Language Models by Asking and Averaging
arxiv_id: '2311.16639'
source_url: https://arxiv.org/abs/2311.16639
tags:
- text
- position
- political
- tweets
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that instruction-tuned LLMs can accurately
  position political texts in ideological and policy spaces. By asking LLMs to score
  texts on specific dimensions and averaging the responses, the authors achieve high
  correlations (0.90) with expert and crowdsourced benchmarks for party manifestos
  and tweets.
---

# Positioning Political Texts with Large Language Models by Asking and Averaging

## Quick Facts
- arXiv ID: 2311.16639
- Source URL: https://arxiv.org/abs/2311.16639
- Reference count: 12
- The paper demonstrates that instruction-tuned LLMs can accurately position political texts in ideological and policy spaces with correlations exceeding 0.90 compared to expert and crowdsourced benchmarks.

## Executive Summary
This paper presents a method for positioning political texts in ideological and policy spaces using instruction-tuned large language models. By prompting LLMs to score texts on specific dimensions and averaging multiple responses, the authors achieve high correlations with expert and crowdsourced benchmarks. The approach works for both short texts like tweets and longer documents like party manifestos, across different languages and political contexts.

## Method Summary
The method involves directly querying instruction-tuned LLMs with system instructions specifying the policy dimension and scoring scale. For each text document, the LLM is asked to provide 20 numeric position scores, which are then averaged to produce the final position estimate. The approach was validated using party manifestos, tweets, and other political texts, comparing the LLM-generated positions against expert coding, roll-call votes, and other benchmarks.

## Key Results
- GPT-4 positions British party manifestos on economic, social, and immigration policy dimensions with correlations of 0.93-0.96 compared to expert ratings
- The method accurately positions U.S. Senators based on their tweets, achieving 0.97 correlation with roll-call vote estimates
- The approach is fast, cost-efficient, and reliable, providing a viable alternative to expert coding and crowdsourcing for scaling political texts across languages and document types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Averaging multiple LLM position estimates improves reliability and reduces idiosyncratic variation.
- Mechanism: Each individual LLM response captures a latent semantic interpretation of the text's ideological stance; averaging over multiple responses smooths out noise and alignment artifacts from the model's training distribution.
- Core assumption: LLM outputs are conditionally independent given the underlying true position of the text.
- Evidence anchors:
  - [abstract] "asking an LLM where a tweet or a sentence of a political text stands on the focal dimension and take the average of the LLM responses"
  - [section] "we aimed to collect 20 numeric position score for each text document, following the approach advocated in Le Mens et al. (2023)"
  - [corpus] weak: no direct quantitative variance analysis reported; only reliability stated as >.99
- Break condition: If responses are highly correlated due to shared training biases, averaging provides little benefit and may amplify systematic errors.

### Mechanism 2
- Claim: Instruction-tuned LLMs encode interpretable policy dimensions (economic, social, immigration) that align with human-coded benchmarks.
- Mechanism: During pretraining and fine-tuning, the model learns embeddings that separate political concepts into coherent subspaces; these subspaces can be probed via targeted prompts to yield consistent scalar positions.
- Core assumption: The model's latent space has been shaped by exposure to politically diverse corpora and aligned with general human conceptualizations of policy dimensions.
- Evidence anchors:
  - [abstract] "correlations between the position estimates obtained with the best LLMs and benchmarks based on text coding by experts, crowdworkers, or roll call votes exceed .90"
  - [section] "the semantic space of GPT-4 includes a representation of the policy dimensions used here that is similar to those of the researchers"
  - [corpus] weak: no ablation study of dimension alignment; only correlation results reported
- Break condition: If the model's internal representations of policy dimensions diverge from human labels, correlation will drop sharply.

### Mechanism 3
- Claim: Short texts (tweets) can be accurately positioned without requiring full context or metadata.
- Mechanism: LLMs extract political stance from lexical and syntactic cues embedded in minimal text; even without surrounding context, these cues are sufficient for reliable inference.
- Core assumption: Political stance is largely encoded at the sentence or phrase level, not requiring document-level aggregation.
- Evidence anchors:
  - [abstract] "positioning texts in policy and ideological spaces is fast, cost-efficient, reliable, and reproducible even if the texts are short and written in different languages"
  - [section] "GPT-4 returned position estimates for 583 tweets, and consistently returned 'NA' for the remaining tweets, indicating that it judged that the tweets did not have enough political content"
  - [corpus] weak: no explicit evaluation of stance extraction from partial vs full documents
- Break condition: If political stance is heavily context-dependent, single-tweet positioning will be unreliable.

## Foundational Learning

- Concept: Supervised scaling (e.g., roll-call voting, expert coding) produces ground-truth position labels.
  - Why needed here: The LLM approach must be validated against these benchmarks to establish credibility.
  - Quick check question: What are the typical correlation ranges between expert-coded and alternative scaling methods in political science?

- Concept: Continuous ideological scaling vs. discrete classification.
  - Why needed here: The paper distinguishes its method from earlier LLM classification tasks by producing scalar positions instead of category labels.
  - Quick check question: How does the variance of continuous position estimates compare to the entropy of discrete classifications in political text analysis?

- Concept: System instructions and few-shot prompting for domain adaptation.
  - Why needed here: The LLM is guided to interpret prompts in the intended political dimension space; this is key to consistent outputs.
  - Quick check question: What happens to correlation with benchmarks if system instructions omit dimension definitions?

## Architecture Onboarding

- Component map: Prompt Generator (system message + text) -> LLM API -> 20 Responses -> Filter NA -> Average -> Output Position
- Critical path: Prompt construction -> API query (n=25) -> Response filtering -> Averaging -> Validation against benchmark
- Design tradeoffs: More responses increase reliability but raise cost; richer system instructions improve alignment but may over-constrain the model
- Failure signatures: Low variance across responses (indicating saturation), systematic bias toward midpoint scores, high NA rate (indicating ambiguous input)
- First 3 experiments:
  1. Vary `n` (responses per query) to see impact on reliability and cost
  2. Compare full-explanation vs. minimal system instructions for dimension alignment
  3. Test multilingual tweets to verify cross-lingual scaling consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs for positioning political texts vary across different languages and cultural contexts?
- Basis in paper: [explicit] The paper mentions positioning EU policy speeches given in 10 different languages, but does not provide detailed performance metrics for each language.
- Why unresolved: The paper does not provide a comprehensive analysis of the performance across different languages and cultural contexts.
- What evidence would resolve it: A systematic study comparing the performance of LLMs across various languages and cultural contexts, including a detailed breakdown of performance metrics for each language.

### Open Question 2
- Question: What are the limitations of using LLMs for positioning short political texts, such as tweets, compared to longer documents like party manifestos?
- Basis in paper: [explicit] The paper discusses the ability of GPT-4 to position very short text documents such as social media posts, but does not provide a detailed comparison of performance between short and long texts.
- Why unresolved: The paper does not provide a detailed analysis of the limitations and challenges associated with positioning short texts compared to longer documents.
- What evidence would resolve it: A comparative study analyzing the performance of LLMs on short political texts versus longer documents, highlighting the specific challenges and limitations for each type of text.

### Open Question 3
- Question: How does the choice of LLM (e.g., GPT-4, Llama 3, MiXtral, Aya) affect the accuracy of positioning political texts?
- Basis in paper: [explicit] The paper mentions using different LLMs like GPT-4, Llama 3, MiXtral, and Aya, but does not provide a detailed comparison of their performance.
- Why unresolved: The paper does not provide a comprehensive analysis of how different LLMs perform in positioning political texts.
- What evidence would resolve it: A systematic comparison of the performance of different LLMs in positioning political texts, including a detailed analysis of their strengths and weaknesses.

### Open Question 4
- Question: What are the potential biases in LLM-generated position estimates, and how can they be mitigated?
- Basis in paper: [explicit] The paper does not discuss potential biases in LLM-generated position estimates or methods to mitigate them.
- Why unresolved: The paper does not address the issue of biases in LLM-generated position estimates, which could affect the accuracy and reliability of the results.
- What evidence would resolve it: A study investigating the potential biases in LLM-generated position estimates and proposing methods to mitigate these biases, including a validation of the effectiveness of these methods.

## Limitations
- The validation is limited to Western European and American political texts, with only one multilingual example
- The paper does not address potential temporal shifts in LLM outputs as training data ages
- No detailed cost comparisons or analyses of scaling challenges for very large document collections are provided

## Confidence
- High confidence in the technical feasibility and immediate replicability of the averaging approach
- Medium confidence in the generalizability of the method beyond the tested domains
- Medium confidence in the claimed cost-efficiency and scalability

## Next Checks
1. **Cross-cultural validation**: Apply the method to political texts from non-Western political systems to test whether instruction-tuned LLMs maintain consistent ideological positioning across different political landscapes.

2. **Temporal stability test**: Collect position estimates for the same political texts at different time intervals to assess whether LLM outputs remain stable or drift as training data ages.

3. **Error analysis framework**: Develop a systematic evaluation of "NA" responses and outlier positions to understand when and why the method fails, particularly for texts that fall outside the model's training distribution.