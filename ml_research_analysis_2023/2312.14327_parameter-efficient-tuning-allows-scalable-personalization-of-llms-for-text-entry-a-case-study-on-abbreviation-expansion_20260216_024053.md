---
ver: rpa2
title: 'Parameter Efficient Tuning Allows Scalable Personalization of LLMs for Text
  Entry: A Case Study on Abbreviation Expansion'
arxiv_id: '2312.14327'
source_url: https://arxiv.org/abs/2312.14327
tags:
- user
- data
- personalization
- arxiv
- abbreviation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores personalization of large language models (LLMs)
  for abbreviation expansion to aid eye-gaze typers with severe motor impairments.
  The authors compare fine-tuning, prompt-tuning, and retrieval augmented in-context
  learning on real user data (~1000 samples) and movie character dialogues.
---

# Parameter Efficient Tuning Allows Scalable Personalization of LLMs for Text Entry: A Case Study on Abbreviation Expansion

## Quick Facts
- arXiv ID: 2312.14327
- Source URL: https://arxiv.org/abs/2312.14327
- Reference count: 15
- Primary result: Prompt-tuning with user-relevant concept initialization outperforms fine-tuning and retrieval-augmented in-context learning for personalizing LLMs on abbreviation expansion tasks

## Executive Summary
This paper explores personalizing large language models for abbreviation expansion to aid eye-gaze typers with severe motor impairments. The authors compare three approaches - fine-tuning, prompt-tuning, and retrieval augmented in-context learning - using real user data (~1000 samples) and movie character dialogues. Prompt-tuning, where learned "soft prompts" are initialized with user-relevant concepts, achieved the highest accuracy and BLEU scores. Fine-tuning showed gains with as few as 600 samples but struggled with generalization to longer sentences. The results demonstrate that parameter-efficient tuning enables scalable personalization while maintaining strong performance.

## Method Summary
The study fine-tunes a pre-trained 8B parameter decoder-only LLM on an abbreviation expansion task using dialog datasets. For personalization, three approaches are compared: (1) full fine-tuning on user-specific data, (2) prompt-tuning with soft prompts initialized using user-relevant concept tokens, and (3) retrieval-augmented in-context learning that selects nearest-neighbor examples from user history. All methods are evaluated on accuracy and BLEU score of the top 5 predictions. The prompt-tuning approach learns only a small set of additional parameters while freezing the base model, enabling efficient storage and deployment across many users.

## Key Results
- Prompt-tuning with user-relevant concept initialization outperformed random initialization by 2-3% accuracy points
- Prompt-tuning achieved higher BLEU scores than both fine-tuning and retrieval-augmented methods
- Fine-tuning showed improvements with as few as 600 samples but degraded performance on longer sentences
- Retrieval-augmented few-shot selection outperformed standard fine-tuning in most cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt-tuning with user-relevant concept tokens leads to better personalization accuracy than random initialization.
- Mechanism: Initializing soft-prompt embeddings with user-relevant concepts biases the LLM to prioritize these terms in predictions, compensating for base model uncertainty about them.
- Core assumption: The base LLM is uncertain about user-specific concepts, and embedding these concepts directly improves adaptation.
- Evidence anchors: Abstract result showing higher accuracy with concept initialization; ablation study (Table 2) comparing initialization strategies.
- Break condition: If user-relevant concepts are too niche or rare in base model training data, initialization may not help or could harm performance.

### Mechanism 2
- Claim: Retrieval-augmented in-context learning outperforms standard few-shot ICL.
- Mechanism: RA-ICL uses a dense retriever to select nearest-neighbor examples from user history, providing context-specific few-shot examples that better match input queries.
- Core assumption: Similar inputs in embedding space share similar expansions, so retrieving nearest neighbors yields useful few-shot examples.
- Evidence anchors: Abstract result showing RA-ICL outperforms fine-tuning; experimental setup using 11B Sentence-T5 retriever.
- Break condition: If retriever fails to find semantically relevant examples due to domain shift or embedding limitations, RA-ICL may perform no better than random few-shot selection.

### Mechanism 3
- Claim: Parameter-efficient tuning enables scalable personalization through small soft-prompt storage.
- Mechanism: By freezing base LLM weights and learning only soft-prompt parameters, storage requirements drop to thousands of parameters per user.
- Core assumption: Base LLM contains sufficient general knowledge, and adapting only input embeddings captures user-specific patterns.
- Evidence anchors: Abstract claim about parameter efficiency; discussion of storage requirements in Section 6.2.
- Break condition: If user personalization requires deeper model changes (significant vocabulary/style shifts), prompt-tuning alone may not suffice.

## Foundational Learning

- Concept: Abbreviation expansion task and its challenges
  - Why needed here: The paper focuses on personalizing LLMs for abbreviation expansion to reduce keystrokes for users with motor impairments
  - Quick check question: What is the abbreviation expansion task, and why is it particularly challenging for LLMs?

- Concept: Parameter-efficient tuning methods (prompt-tuning, adapters, LoRA)
  - Why needed here: The paper compares prompt-tuning to fine-tuning and retrieval-augmented ICL for personalization
  - Quick check question: How does prompt-tuning differ from full fine-tuning and other PEFT methods like adapters or LoRA?

- Concept: Retrieval-augmented generation (RAG) and in-context learning
  - Why needed here: The paper uses RA-ICL to select relevant few-shot examples
  - Quick check question: How does retrieval-augmented ICL differ from standard few-shot ICL, and why might it help in personalization?

## Architecture Onboarding

- Component map: Base LLM (8B decoder-only) -> Abbreviation expansion fine-tuning -> Personalization module (prompt-tuning/RA-ICL/fine-tuning) -> Retriever (11B Sentence-T5) -> User data pipeline

- Critical path: 1) Fine-tune base LLM on abbreviation expansion task, 2) Choose personalization approach (prompt-tuning, RA-ICL, or fine-tuning), 3) For prompt-tuning: initialize soft-prompt with user concepts, train with low learning rate, 4) For RA-ICL: embed inputs, retrieve nearest neighbors, create few-shot examples, prompt LLM, 5) Evaluate on user test set

- Design tradeoffs:
  - Prompt-tuning vs. fine-tuning: Lower storage/serving cost, faster training, but may generalize less well to complex sentences
  - Prompt-tuning vs. RA-ICL: Prompt-tuning is more efficient, but RA-ICL may perform better if retrieval quality is high
  - Soft-prompt length: Longer prompts increase learned parameters but risk training instability

- Failure signatures:
  - Low accuracy on user test set: Poor initialization, overfitting, or base-user data mismatch
  - Poor generalization to longer sentences: Overfitting during fine-tuning
  - High variance across runs: Training instability or insufficient data

- First 3 experiments:
  1. Compare prompt-tuning with random vs. user-concept initialization on validation set
  2. Test different prompt lengths (10, 25, 100) and learning rates for prompt-tuning
  3. Evaluate RA-ICL with different retrieval strategies (random vs. nearest-neighbor few-shot examples)

## Open Questions the Paper Calls Out

- How do prompt-tuning results compare to other personalization methods across different domains or languages?
- How does soft-prompt initialization strategy choice impact performance on different personalization tasks?
- How does soft-prompt length impact performance across different personalization tasks?

## Limitations
- Limited generalizability due to single-user primary dataset, raising questions about performance across diverse user populations
- Fine-tuning struggles with longer sentences, showing degradation that could impact practical deployment
- RA-ICL performance heavily depends on retrieval quality, which isn't thoroughly analyzed

## Confidence

- High Confidence: Prompt-tuning with user-relevant concept initialization outperforms random initialization (based on clear ablation study results)
- Medium Confidence: Prompt-tuning enables scalable personalization through parameter efficiency (theoretical argument is sound but practical deployment considerations need exploration)
- Low Confidence: Prompt-tuning will generalize better than fine-tuning across all user scenarios (insufficient evidence for broader generalizability claims)

## Next Checks

1. Conduct multi-user validation study across 10-20 diverse users to assess generalizability and identify user-specific failure modes

2. Systematically analyze length-dependent performance degradation by varying sentence length in test set to quantify fine-tuning limitations

3. Implement retrieval quality metrics (recall@k, embedding visualizations) for RA-ICL and conduct ablation studies to isolate retrieval contribution to performance