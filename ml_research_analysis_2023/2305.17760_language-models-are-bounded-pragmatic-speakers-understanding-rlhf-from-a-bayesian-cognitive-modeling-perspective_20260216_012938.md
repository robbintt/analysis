---
ver: rpa2
title: 'Language Models are Bounded Pragmatic Speakers: Understanding RLHF from a
  Bayesian Cognitive Modeling Perspective'
arxiv_id: '2305.17760'
source_url: https://arxiv.org/abs/2305.17760
tags:
- language
- arxiv
- pragmatic
- learning
- speaker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a cognitive model called the bounded pragmatic
  speaker to characterize how language models think. The model conceptualizes language
  models as agents that employ a base speaker model to narrow down the space of utterances
  and a theory-of-mind listener model to predict how listeners would interpret each
  utterance.
---

# Language Models are Bounded Pragmatic Speakers: Understanding RLHF from a Bayesian Cognitive Modeling Perspective

## Quick Facts
- arXiv ID: 2305.17760
- Source URL: https://arxiv.org/abs/2305.17760
- Authors: 
- Reference count: 14
- Primary result: Language models fine-tuned with RLHF implement a dual fast-and-slow cognitive system resembling Kahneman's bounded pragmatic speaker framework

## Executive Summary
This paper introduces a cognitive model called the bounded pragmatic speaker (BPS) to characterize how language models think. The model conceptualizes language models as agents that employ a base speaker model to narrow down the space of utterances and a theory-of-mind listener model to predict how listeners would interpret each utterance. The authors demonstrate that large language models fine-tuned with reinforcement learning from human feedback implement a dual model of thought, resembling Kahneman's fast-and-slow system. They argue that this model is limited by its reward function, which lacks counterfactual reasoning capabilities and long-term impact prediction.

## Method Summary
The paper proposes a BPS framework where language models are decomposed into a base speaker (fast-thinking system) that narrows search space and a theory-of-mind listener (slow-thinking system) that evaluates pragmatic relevance. RLHF is analyzed as a variational approximation method for implementing this framework, optimizing the KL-divergence between the variational distribution and the approximated BPS distribution. The authors discuss three directions for improving language models: enhancing search capability, augmenting pragmatic capability, and devising efficient inference algorithms.

## Key Results
- RLHF-tuned LLMs implement a dual model of thought conceptually resembling Kahneman's fast-and-slow cognitive systems
- The BPS framework provides a principled way to analyze language model reasoning as bounded rational agents
- Current reward functions lack counterfactual reasoning capabilities and long-term impact prediction, limiting model performance

## Why This Works (Mechanism)

### Mechanism 1
- Language models can be modeled as bounded pragmatic speakers with dual fast-and-slow cognitive systems
- Decomposes into base speaker (fast-thinking) that narrows search space and theory-of-mind listener (slow-thinking) that evaluates pragmatic relevance
- Core assumption: Language models implement approximate inference algorithms balancing search efficiency with pragmatic reasoning

### Mechanism 2
- RLHF implements variational approximation of the bounded pragmatic speaker framework
- Optimizes KL-divergence between variational distribution (fast-thinking) and approximated BPS distribution (slow-thinking)
- Core assumption: Reward function learned from human feedback can accurately model how humans evaluate utterances

### Mechanism 3
- BPS framework provides three distinct directions for improving language models
- Model failures attributed to limited search capability, flawed pragmatic capability, or inefficient inference algorithms
- Core assumption: Language model performance can be decomposed into these three orthogonal capabilities

## Foundational Learning

- Concept: Bayesian cognitive modeling
  - Why needed here: Frames language models through probabilistic cognitive models, specifically the bounded pragmatic speaker framework
  - Quick check question: What is the mathematical relationship between base speaker, theory-of-mind listener, and overall BPS distribution?

- Concept: Reinforcement learning from human feedback (RLHF)
  - Why needed here: RLHF analyzed as variational approximation method for implementing BPS framework
  - Quick check question: How does KL-regularized objective in RLHF relate to KL-divergence between variational distribution and approximated BPS distribution?

- Concept: Fast-and-slow cognitive systems
  - Why needed here: Draws parallels between RLHF-tuned language models and Kahneman's dual process theory of cognition
  - Quick check question: What are limitations of using reward function as slow-thinking system in dual model of thought?

## Architecture Onboarding

- Component map: Base Speaker -> Theory-of-Mind Listener -> Inference Algorithm -> Reward Function
- Critical path: 1. Task context and intention provided 2. Base speaker generates candidate utterances 3. Theory-of-mind listener evaluates pragmatic relevance 4. Inference algorithm selects optimal utterance
- Design tradeoffs:
  - Base speaker vs. ToM listener quality: Better search capability may reduce need for sophisticated pragmatic reasoning
  - Inference efficiency vs. accuracy: Faster algorithms may sacrifice optimal solution quality
  - Reward function complexity vs. generalization: More detailed reward functions may overfit to training data
- Failure signatures:
  - Low diversity in generated outputs (search capability issue)
  - Outputs that are technically correct but pragmatically inappropriate (pragmatic capability issue)
  - Inconsistent performance across similar tasks (inference algorithm issue)
- First 3 experiments:
  1. Compare language model outputs with and without pragmatic re-ranking on downstream task
  2. Ablation study removing either base speaker or theory-of-mind listener components
  3. Test different inference algorithms (Monte Carlo vs. variational) on task completion time and quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are specific computational and architectural modifications needed to create "slow-thinking system" that can perform counterfactual reasoning and long-term impact prediction in language models?
- Basis in paper: [explicit] Paper explicitly states current reward functions lack counterfactual reasoning capabilities and long-term impact prediction
- Why unresolved: Paper identifies limitation but doesn't provide concrete architectural or algorithmic solutions
- What evidence would resolve it: Working implementation demonstrating counterfactual reasoning and long-term prediction capabilities with experimental results

### Open Question 2
- Question: How can knowledge be transferred more efficiently from slow-thinking system to fast-thinking system beyond current reinforcement learning approach?
- Basis in paper: [explicit] Paper discusses inefficiency of current knowledge transfer through variational inference/RL and suggests more efficient algorithms could be developed
- Why unresolved: While paper proposes richer communication mediums could improve knowledge transfer, doesn't specify exact mechanisms or algorithms
- What evidence would resolve it: Comparative studies showing improved sample efficiency and learning speed when using alternative knowledge transfer methods

### Open Question 3
- Question: What are trade-offs between model complexity and inference efficiency when implementing more sophisticated Bayesian cognitive models in large language models?
- Basis in paper: [inferred] Paper discusses need for bounded rationality and efficient approximate inference algorithms
- Why unresolved: Paper identifies need for efficient inference but doesn't provide concrete guidelines or empirical results showing how different levels of model complexity affect inference efficiency
- What evidence would resolve it: Empirical studies comparing various Bayesian cognitive models with different levels of complexity, measuring both performance and inference time/resource requirements

## Limitations
- The relationship between base speaker and theory-of-mind listener components lacks empirical validation through controlled experiments
- The claim that RLHF is equivalent to variational inference on BPS framework requires more rigorous proof and empirical verification
- Practical implications and specific implementation strategies for three proposed directions of improvement lack concrete methodologies

## Confidence

**High Confidence**: Mathematical formalization of BPS framework and its decomposition into base speaker and theory-of-mind listener components. Connection between KL-regularized objectives and variational inference is well-established.

**Medium Confidence**: Claim that RLHF-tuned models implement dual cognitive system analogous to Kahneman's fast-and-slow thinking. Theoretical framework is sound but empirical evidence for cognitive interpretation is limited.

**Low Confidence**: Practical implications and specific implementation strategies for three proposed directions of improvement. Paper provides conceptual guidance but lacks concrete methodologies or empirical validation.

## Next Checks

1. **Empirical Validation of Dual Cognitive Systems**: Design and conduct controlled experiments comparing language model outputs with and without pragmatic re-ranking, measuring both task completion quality and computational efficiency to validate fast-and-slow cognitive system hypothesis.

2. **Reward Function Analysis**: Systematically test limitations of current reward functions in RLHF by designing tasks requiring counterfactual reasoning and long-term impact prediction, measuring model performance degradation when these capabilities are needed.

3. **Inference Algorithm Benchmarking**: Implement and compare different inference algorithms (Monte Carlo vs. variational methods) for BPS framework, measuring trade-offs between computational efficiency and pragmatic reasoning quality across various downstream tasks.