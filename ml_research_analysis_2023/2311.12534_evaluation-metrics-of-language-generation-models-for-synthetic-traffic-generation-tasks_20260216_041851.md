---
ver: rpa2
title: Evaluation Metrics of Language Generation Models for Synthetic Traffic Generation
  Tasks
arxiv_id: '2311.12534'
source_url: https://arxiv.org/abs/2311.12534
tags:
- metrics
- texts
- generation
- similarity
- product
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the evaluation of Synthetic Traffic Generation
  (STG) models, which generate multiple text outputs for a single prompt. The authors
  argue that standard NLG metrics like BLEU, which compare individual generated texts
  to reference texts, are not suitable for STG tasks.
---

# Evaluation Metrics of Language Generation Models for Synthetic Traffic Generation Tasks

## Quick Facts
- arXiv ID: 2311.12534
- Source URL: https://arxiv.org/abs/2311.12534
- Reference count: 9
- Primary result: Bag-level metrics outperform standard NLG metrics by up to 20% in agreement with human judgment for evaluating Synthetic Traffic Generation models

## Executive Summary
This paper addresses the challenge of evaluating Synthetic Traffic Generation (STG) models that generate multiple text outputs for a single prompt. The authors demonstrate that standard NLG metrics like BLEU, which compare individual generated texts to reference texts, are inadequate for STG tasks. Instead, they propose bag-level metrics that compare the distribution of generated texts to the distribution of real user texts, showing significant improvements in evaluation quality through both automatic and human validation procedures.

## Method Summary
The paper proposes several bag-level metrics for evaluating STG models, including pairwise metrics, alignment-based metrics, clustering metrics, and document similarity metrics. The automatic evaluation procedure manipulates reference bags with different types of noise (Text Distribution Manipulation, Noisy Text Injection, Easy Data Augmentation, Carrier Phrase Substitution, Itemname Specificity Manipulation) to create rankings of synthetic bags. Human annotations provide comparative judgments on generated text quality. The method involves implementing the proposed metrics, applying noise injection techniques, and calculating Spearman correlation between real rankings and predicted rankings.

## Key Results
- Document-based metrics (Cosine TF-IDF) achieve up to 20% better agreement with human judgment compared to standard NLG metrics
- Pairwise metrics fail due to head text bias, favoring frequent expressions over diverse representations
- Alignment-based metrics improve performance by finding optimal 1-to-1 matches between generated and reference texts
- Document metrics consistently perform well across all three tasks (SUG, PQG, QAC)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pairwise metrics fail because they over-emphasize frequent expressions in the reference bag
- Mechanism: When computing average pairwise similarity, each generated text is compared to every reference text. Head texts (frequent expressions) appear more often in the reference bag, so they contribute more to the average score. This makes the metric favor generated bags that contain many copies of these frequent expressions, even if they miss the rarer but important expressions that represent the full distribution.
- Core assumption: The reference bag contains a representative sample of the true distribution of expressions, including both frequent (head) and rare (tail) expressions.
- Evidence anchors:
  - [abstract] "These include pairwise metrics, alignment-based metrics, clustering metrics, and document similarity metrics."
  - [section 3] "This solution tends to favor generated bags that contain mostly texts from the head of the reference distribution (i.e., the most frequent expressions)."
  - [corpus] Weak - corpus doesn't provide direct evidence about this mechanism, only mentions related evaluation papers.

### Mechanism 2
- Claim: Alignment-based metrics improve by finding optimal 1-to-1 matches between generated and reference texts
- Mechanism: Instead of averaging all pairwise scores, alignment-based metrics use a bipartite graph matching algorithm to find the optimal assignment between generated and reference texts. This ensures each generated text is matched to its most similar reference text, and vice versa, preventing head texts from dominating the score.
- Core assumption: The reference and generated bags have similar sizes and the optimal matching algorithm can find meaningful 1-to-1 correspondences.
- Evidence anchors:
  - [section 3] "We create edges (g, r) connecting each node g ∈ G to each node r ∈ R by assigning a weight as sim(g, r)... To compute sentence-level alignments, we apply an existing maximal matching algorithm (Gabow, 1976) to the resulting graph and obtain the sentence-level alignment A(G, R)."
  - [section 4.2] "When alignment is applied to pairwise metrics (both lexical and learned), we observe significant increases in correlation in all cases, suggesting the effectiveness of the proposed alignment."
  - [corpus] Weak - corpus doesn't directly address this specific mechanism.

### Mechanism 3
- Claim: Document-based metrics work by capturing the overall distribution of words in each bag
- Mechanism: Document metrics represent each bag as a single vector (TF or TF-IDF) by summing the encodings of all texts in the bag. The cosine similarity between these vectors measures how similar the overall word distributions are between the two bags, capturing both frequent and rare expressions in a single comparison.
- Core assumption: The bag can be meaningfully represented as a single vector that captures its distributional properties.
- Evidence anchors:
  - [section 3] "We consider representing the bags as their uni-gram probability distribution and compute the Kullback–Leibler divergence (Joyce, 2011) DKL(G||R). As a similarity score, we adopt the inverse of such value: InvKL (G, R) = DKL(G||R)−1"
  - [section 4.2] "Document metrics (i.e., Cos TF or Cos TF-IDF) show strong and consistent performances in all three tasks. This is because representing an entire bag with a single representation preserves the word distribution of the bag for both tail and head expressions."
  - [corpus] Weak - corpus doesn't provide direct evidence about this mechanism.

## Foundational Learning

- Concept: Distributional similarity vs. instance-level similarity
  - Why needed here: Understanding the difference between comparing individual texts vs. comparing the overall distribution of texts in a bag is crucial for understanding why standard NLG metrics fail and why the proposed bag-level metrics work better.
  - Quick check question: If you have a bag with 10 identical texts saying "search shoes" and another bag with 10 different ways to say "search shoes", which bag is more representative of user behavior?

- Concept: Maximal bipartite matching
  - Why needed here: The alignment-based metrics rely on finding the optimal 1-to-1 matching between generated and reference texts. Understanding this graph algorithm is important for understanding how these metrics work.
  - Quick check question: In a bipartite graph where left nodes represent generated texts and right nodes represent reference texts, what does the maximal matching algorithm find?

- Concept: TF-IDF weighting
  - Why needed here: Document-based metrics use TF-IDF representations, which weight words by their importance in the bag vs. their commonness across all bags. Understanding this helps explain why IDF helps in some tasks but hurts in others.
  - Quick check question: If a word appears in all bags, what will its IDF weight be, and how does that affect its contribution to the similarity score?

## Architecture Onboarding

- Component map: Context → STG model → generated bag → evaluation metric → score → comparison with reference bag
- Critical path: For a new engineer, the critical path is understanding the data flow from context through STG model to evaluation and scoring
- Design tradeoffs: The choice between pairwise, alignment, and document metrics involves a tradeoff between computational efficiency (pairwise is fastest, alignment is moderate, document is fastest) and accuracy in capturing distribution similarity (document is best, alignment is good, pairwise is worst)
- Failure signatures: If pairwise metrics give high scores to generated bags that clearly don't match the reference distribution, this indicates the "head text bias" problem. If document metrics give poor scores to bags with diverse but valid expressions, this might indicate the bag size is too small for stable vector representations.
- First 3 experiments:
  1. Run pairwise BLEU on a simple case where reference bag has "search shoes", "find shoes", "buy shoes" and generated bag has "search shoes", "search shoes", "search shoes". Observe the high score despite poor distribution coverage.
  2. Apply the alignment algorithm to the same case and observe the lower score that better reflects the distribution mismatch.
  3. Create document vectors for both bags using TF-IDF and compute cosine similarity to see how it captures the difference in distribution between head-heavy and diverse bags.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal sentence-level alignment algorithm for bag-level evaluation in Synthetic Traffic Generation?
- Basis in paper: [explicit] The paper discusses alignment-based metrics but does not specify which algorithm is optimal for this specific task.
- Why unresolved: The authors mention applying "an existing maximal matching algorithm" without providing details on its effectiveness or comparison to other alignment algorithms.
- What evidence would resolve it: Experiments comparing different alignment algorithms (e.g., Hungarian algorithm, auction algorithm) on STG tasks, measuring their correlation with human judgments.

### Open Question 2
- Question: How do document similarity metrics perform on STG tasks with extremely imbalanced bag sizes?
- Basis in paper: [inferred] The paper mentions bag size analysis but only shows correlation trends, not performance in extreme cases.
- Why unresolved: The experiments show general trends but don't explore edge cases of very large or very small bag sizes.
- What evidence would resolve it: Systematic experiments varying bag sizes from 1 to 1000+ texts, measuring document metric performance across this range.

### Open Question 3
- Question: Can pre-trained models be effectively fine-tuned for STG evaluation rather than using them out-of-the-box?
- Basis in paper: [explicit] The authors note that pre-trained models like SBert are "not calibrated enough" for STG evaluation.
- Why unresolved: The paper only tests pre-trained models without exploring fine-tuning approaches.
- What evidence would resolve it: Experiments fine-tuning BERT-based models on STG-specific tasks, measuring improvement in correlation with human judgments compared to off-the-shelf models.

## Limitations

- The automatic evaluation procedure relies on synthetic noise injection rather than real-world data distribution shifts, which may not fully capture the complexity of actual STG model failures
- Human annotation was limited to comparative judgments rather than absolute quality assessments, potentially missing cases where both generated and reference bags are of poor quality
- The study focuses on three specific STG tasks, and generalizability to other generation scenarios remains untested

## Confidence

- **High Confidence**: Document-based metrics (Cosine TF-IDF) are effective for STG evaluation across multiple tasks - supported by consistent experimental results showing up to 20% better agreement with human judgment
- **Medium Confidence**: Pairwise metrics fail due to head text bias - mechanism is well-explained but the exact conditions under which this occurs could be more precisely quantified
- **Medium Confidence**: Alignment-based metrics provide improvement over pairwise metrics - results show improvement but the magnitude varies significantly across tasks and metric types

## Next Checks

1. Test document-based metrics on STG tasks with highly domain-specific vocabulary to assess whether TF-IDF weighting remains effective or if alternative weighting schemes perform better
2. Implement the automatic evaluation procedure on a held-out test set from the original tasks to verify that noise injection correlates with real distribution shifts
3. Conduct ablation studies on the alignment algorithm parameters (e.g., matching threshold, graph construction) to identify optimal settings for different STG scenarios