---
ver: rpa2
title: Transferring Learning Trajectories of Neural Networks
arxiv_id: '2305.14122'
source_url: https://arxiv.org/abs/2305.14122
tags:
- learning
- transfer
- training
- trajectory
- parameter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational cost of training deep neural
  networks, particularly when performing duplicated training runs such as model ensemble
  or fine-tuning. The key idea is to transfer a learning trajectory (a sequence of
  intermediate parameters during training) from one initial parameter to another,
  without actual training.
---

# Transferring Learning Trajectories of Neural Networks

## Quick Facts
- arXiv ID: 2305.14122
- Source URL: https://arxiv.org/abs/2305.14122
- Reference count: 40
- One-line primary result: Transferred learning trajectories achieve non-trivial accuracy before training and enable faster convergence than training from scratch.

## Executive Summary
This paper addresses the computational cost of training deep neural networks by proposing a method to transfer learning trajectories between different initial parameters without actual training. The key insight is that neural networks exhibit permutation symmetry, allowing parameters to be rearranged while preserving functionality. By leveraging this property, the authors develop algorithms that match gradients successively along a source trajectory and apply appropriate permutations to transfer it to a new initialization. The method significantly reduces training time for tasks like model ensemble and fine-tuning while achieving competitive accuracy.

## Method Summary
The method formulates learning transfer as matching gradients along a source trajectory using permutation symmetry. The core algorithm, Gradient Matching along Trajectory (GMT), iteratively solves linear optimization problems to find permutations that align gradients between source and target trajectories at each step. A faster variant, FGMT, reduces computational cost by caching gradients. To address storage limitations, the method also introduces linear trajectory approximation with cosine scheduling. The transferred parameters can then be fine-tuned more efficiently than training from scratch. The approach is validated across multiple datasets (MNIST, CIFAR-10, CIFAR-100, ImageNet) and architectures (2-MLP, Conv8, ResNet-18).

## Key Results
- Transferred parameters achieve non-trivial accuracy before any direct training on the target task
- Transferred parameters converge significantly faster during subsequent training than training from scratch
- Mode connectivity analysis shows transferred parameters lie in the same basin as source parameters, but only oracle baseline achieves linear mode connectivity with true parameters

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Learning trajectories can be transferred between neural networks with different random initializations by aligning gradients along the trajectory using permutation symmetry.
- **Mechanism:** The algorithm matches gradients between the source trajectory and the target trajectory at each step using permutation symmetry. By iteratively solving a sequence of linear optimization problems, it finds permutations that minimize the difference between gradients, effectively transferring the trajectory without actual training.
- **Core assumption:** The learning trajectories for two initial parameters are indistinguishable up to permutation symmetry of neural networks.
- **Evidence anchors:**
  - [abstract] "we formulate the problem of 'transferring' a given learning trajectory from one initial parameter to another one, called learning transfer problem, and derive the first algorithm to approximately solve it by matching gradients successively along the trajectory via permutation symmetry."
  - [section 3.2] "Now our goal is to solve the optimization problem PT (Eq. 5). However, the problem PT seems hard to solve directly because the variable π appears non-linearly in the second term ∇θt 2,π L. To avoid the non-linearity, we introduce a sequence of linear sub-problems {P ′ s}1≤s≤T whose solution converges to the solution for PT."
- **Break condition:** If the learning trajectories are not equivalent under permutation symmetry, the gradient matching approach will fail to find appropriate permutations, and the transferred parameters will not approximate the true trajectory.

### Mechanism 2
- **Claim:** Linear trajectories (interpolations between initial and final parameters) can be used instead of actual trajectories to reduce storage and computational costs while maintaining transfer accuracy.
- **Mechanism:** Instead of storing the entire trajectory, the algorithm uses a linear interpolation between the initial and final parameters. This linear trajectory approximates the actual trajectory while being much more storage-efficient. The cosine scheduling of the interpolation points further improves the transfer accuracy.
- **Core assumption:** Monotonic linear interpolation of neural network parameters results in a loss landscape that is similar to the actual training trajectory.
- **Evidence anchors:**
  - [section 3.3] "In terms of the storage cost, Algorithm 1 requires a capacity of T + 1 times the model size to keep a learning trajectory of length T, which will be a more substantial issue as model size increases or the trajectory becomes fine-grained. To reduce the required storage capacity, instead of keeping the entire trajectory, we propose to imitate it by linearly interpolating the end points."
  - [section 3.3] "Interestingly, the transfer of the linear trajectory is more stable and has less variance than the transfer of the actual one. This may be because the actual trajectory contains noisy information while the linear trajectory is directed towards the optimal solution θT 1."
- **Break condition:** If the linear interpolation does not accurately represent the actual trajectory (e.g., if the loss landscape has significant non-linearities), the transferred parameters may not approximate the true trajectory well.

### Mechanism 3
- **Claim:** The transferred parameters lie in the same basin as the source parameter, but only the oracle baseline is linearly mode connected to the true parameter.
- **Mechanism:** The mode connectivity analysis shows that the transferred parameters and the source parameter are in the same basin of the loss landscape. However, the oracle baseline, which uses the true parameter in its permutation calculation, achieves linear mode connectivity with the true parameter. This suggests that the transferred parameters are close to the true parameter but not exactly in the same linear path.
- **Core assumption:** The loss landscape of neural networks has connected basins, and parameters within the same basin have similar loss values.
- **Evidence anchors:**
  - [section 4.3] "Figure 6a visualizes the 1-dimensional loss landscape along the linear path between the appropriately permuted source parameter πθT 1 and the transferred parameter θT 2,π, where we set π = πT for GMT, π = πnaive and π = πoracle for the Naive and Oracle baselines. In the figure, we refer to the appropriately permuted source parameters θT 2,π by Source*. The results show that the transferred parameters lie in the same basin as the source parameter for all methods including baselines."
  - [section 4.3] "On the other hand, Figure 6b visualizes the 1-dimensional landscape along the linear path between the true parameter θT 2 (i.e., actually trained from θ0 2) and the transferred parameter θT 2,π. In this visualization, we notice that only the oracle baseline is closer to linearly mode connected with the true parameter."
- **Break condition:** If the loss landscape does not have connected basins or if the basins are separated by significant barriers, the transferred parameters may not lie in the same basin as the source parameter.

## Foundational Learning

- **Concept: Permutation symmetry of neural networks**
  - Why needed here: Understanding permutation symmetry is crucial because the learning transfer algorithm relies on finding permutations that align the gradients of the source and target trajectories.
  - Quick check question: What is permutation symmetry in neural networks, and how does it affect the outputs of the network?

- **Concept: Mode connectivity in neural networks**
  - Why needed here: Mode connectivity analysis is used to understand the properties of the loss landscape around the transferred parameters and the source/true parameters.
  - Quick check question: What is mode connectivity, and why is it important in understanding the behavior of neural network training?

- **Concept: Gradient matching and optimization**
  - Why needed here: The learning transfer algorithm uses gradient matching to find permutations that align the source and target trajectories. Understanding gradient-based optimization is essential for grasping the algorithm's mechanism.
  - Quick check question: How does gradient matching work, and what are the challenges in using it for parameter alignment?

## Architecture Onboarding

- **Component map:**
  Source trajectory (θ₀¹, ..., θᵀ¹) -> Gradient computation -> Permutation alignment -> Transferred trajectory (θ⁰₂, ..., θᵀ₂)

- **Critical path:**
  1. Compute gradients for source and target trajectories
  2. Iteratively solve linear optimization problems to find permutations
  3. Update target parameters using the found permutations
  4. Return the transferred trajectory

- **Design tradeoffs:**
  - Storage vs. accuracy: Using linear trajectories instead of actual trajectories reduces storage but may impact accuracy
  - Computation vs. accuracy: FGMT reduces computation by caching gradients but may sacrifice some accuracy compared to GMT
  - Permutation quality vs. computation: Finding better permutations (e.g., using oracle baseline) improves accuracy but requires more computation

- **Failure signatures:**
  - Transferred parameters do not achieve non-trivial accuracy before training
  - Transferred parameters do not converge faster during subsequent training
  - Transferred parameters are not in the same basin as the source parameter (mode connectivity analysis)

- **First 3 experiments:**
  1. Transfer a learning trajectory on CIFAR-10 between random initializations using GMT and FGMT
  2. Transfer a learning trajectory on CIFAR-100-subset from ImageNet pre-trained parameters using GMT and FGMT
  3. Analyze the mode connectivity of the transferred parameters for the above experiments

## Open Questions the Paper Calls Out
- **Question:** Can learning trajectories be effectively transferred between neural network architectures with different neuron sizes without requiring the same architecture consistency?
- **Question:** How can we obtain the permutation π that ensures the transferred parameter θT2,π lies in the same basin as the true parameter θT2, without using the true parameter itself (unlike the Oracle baseline)?
- **Question:** What is the optimal technique for practical applications like model ensemble and knowledge distillation that would extend the method to transfer between different NN architectures?

## Limitations
- The method requires consistency in NN architectures, limiting its applicability to transferring between different network types
- The quality of permutation alignment becomes computationally expensive for large models, creating scalability challenges
- The assumption that learning trajectories are equivalent under permutation symmetry may not hold for all training regimes or architectures

## Confidence
- **High Confidence:** The basic mechanism of trajectory transfer via gradient matching works as described, supported by experimental results showing non-trivial accuracy before training and faster convergence.
- **Medium Confidence:** The claim about linear trajectories being more stable than actual trajectories, as the evidence is based on variance observations without clear explanation of the underlying mechanism.
- **Medium Confidence:** The assertion that transferred parameters lie in the same basin as source parameters but not linearly connected to true parameters, as this is based on mode connectivity analysis which may depend on visualization methods.

## Next Checks
1. **Permutation Quality Validation:** Systematically vary the number of coordinate descent iterations in the permutation alignment step and measure the impact on transfer accuracy to quantify the relationship between permutation quality and transfer performance.

2. **Architecture Generalization Test:** Apply the trajectory transfer method to transformer-based architectures or recurrent networks to test whether the permutation symmetry assumption holds across different neural network families.

3. **Long Trajectory Analysis:** Investigate the performance degradation when transferring longer trajectories (e.g., T > 100) to understand the limitations of the gradient matching approach and whether early stopping during transfer could improve results.