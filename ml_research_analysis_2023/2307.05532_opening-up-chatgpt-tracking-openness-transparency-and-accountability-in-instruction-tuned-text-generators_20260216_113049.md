---
ver: rpa2
title: 'Opening up ChatGPT: Tracking openness, transparency, and accountability in
  instruction-tuned text generators'
arxiv_id: '2307.05532'
source_url: https://arxiv.org/abs/2307.05532
tags:
- https
- data
- open
- openness
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys the openness of instruction-tuned text generators,
  focusing on projects comparable to ChatGPT. The authors assess projects based on
  availability of code, training data, model weights, RLHF data, licensing, documentation,
  and access methods.
---

# Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators

## Quick Facts
- arXiv ID: 2307.05532
- Source URL: https://arxiv.org/abs/2307.05532
- Reference count: 40
- One-line primary result: Most open-source instruction-tuned text generators inherit undocumented data, lack shared RLHF training data, and have scarce peer-reviewed documentation, limiting true openness and accountability.

## Executive Summary
This paper surveys the openness of instruction-tuned text generators, focusing on projects comparable to ChatGPT. The authors assess projects based on availability of code, training data, model weights, RLHF data, licensing, documentation, and access methods. They find that while many projects claim to be "open source," most inherit undocumented data of questionable legality, few share crucial RLHF data (which requires human annotation), and scientific documentation is rare. Only a small number of projects are backed by larger organizations and offer similar features to proprietary tools while being open-sourced and well-documented. The authors highlight three recurring issues: inheritance of undocumented data, lack of shared RLHF training data, and scarcity of peer-reviewed papers. They argue that openness is essential for fostering transparency, reproducibility, and accountability in AI research and development.

## Method Summary
The paper evaluates 15+ open-source instruction-tuned text generator projects using a systematic approach. The authors assess projects based on 13 features: code, training data, model weights, RLHF data, licensing, scientific documentation, and access methods. Each feature is evaluated on a scale from maximum to partial to no openness. The evaluation is conducted as of June 2023, focusing on projects with LLM+RLHF architecture that claim to be "open source." The results are analyzed to identify recurring issues and categorize projects by degrees of openness.

## Key Results
- Most open-source instruction-tuned text generators inherit undocumented data of questionable legality from base models
- Few projects share crucial RLHF training data, which requires human annotation and is essential for reproducibility
- Scientific documentation is rare, with most projects lacking peer-reviewed papers or comprehensive model cards
- Only a handful of projects backed by larger organizations offer features comparable to proprietary tools while being open-sourced and well-documented

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Open-source instruction-tuned text generators provide a pathway to reproducible research by exposing code, training data, model weights, RLHF data, licensing, documentation, and access methods.
- Mechanism: By making these components publicly available, researchers can replicate experiments, audit training data sources, and scrutinize model behavior, thus countering the opacity of proprietary systems like ChatGPT.
- Core assumption: Access to these components is sufficient to understand and reproduce the model's behavior.
- Evidence anchors:
  - [abstract] "We evaluate projects in terms of openness of code, training data, model weights, RLHF data, licensing, scientific documentation, and access methods."
  - [section] "We find that while there is a fast-growing list of projects billing themselves as 'open source', many inherit undocumented data of dubious legality..."
- Break condition: If key components like RLHF data remain closed, reproducibility and full transparency are compromised.

### Mechanism 2
- Claim: Transparency in AI development fosters accountability by allowing external scrutiny of data collection, model architecture, training processes, and deployment methods.
- Mechanism: Open documentation and access methods enable third parties to identify biases, legal issues, and ethical concerns in AI systems, promoting responsible development.
- Core assumption: External scrutiny leads to meaningful improvements in AI systems.
- Evidence anchors:
  - [abstract] "Degrees of openness are relevant to fairness and accountability at all points, from data collection and curation to model architecture..."
  - [section] "Openness promotes transparency, reproducibility, and quality control; all features that are prerequisites for supporting robust scientific inference..."
- Break condition: If documentation is inadequate or misleading, transparency benefits are negated.

### Mechanism 3
- Claim: Open-source projects backed by larger organizations can offer features comparable to proprietary tools while maintaining openness and transparency.
- Mechanism: These projects leverage institutional resources to develop well-documented, open models that serve as viable alternatives to closed systems.
- Core assumption: Larger organizations have the capacity and incentive to maintain open-source projects.
- Evidence anchors:
  - [section] "We also identify a handful of projects backed by larger organisations, which aim to offer similar features to proprietary tools such as ChatGPT but are open-sourced and well documented."
  - [corpus] "Opening the Scope of Openness in AI" discusses how openness in AI has been inspired by open-source software, suggesting institutional involvement is key.
- Break condition: If organizational backing leads to proprietary dependencies or reduced transparency, the open-source advantage is lost.

## Foundational Learning

- Concept: Large Language Models (LLMs) and Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Understanding these technologies is crucial for evaluating the openness and transparency of instruction-tuned text generators.
  - Quick check question: What is the difference between pre-training and fine-tuning in LLMs?

- Concept: Open-source licensing and its implications
  - Why needed here: Different licenses affect how models and data can be used, shared, and modified.
  - Quick check question: How does an MIT license differ from a GPL license in terms of requirements for derivative works?

- Concept: Data provenance and ethical considerations in AI
  - Why needed here: Assessing the legality and ethics of training data is essential for evaluating the openness and accountability of AI projects.
  - Quick check question: What are the potential issues with using web-scraped data for training AI models?

## Architecture Onboarding

- Component map:
  Base LLM (pre-trained model) -> Instruction-tuning dataset -> RLHF training pipeline (human feedback data) -> Documentation and model cards -> Access methods (APIs, user interfaces) -> Licensing information

- Critical path:
  1. Acquire or build a base LLM
  2. Develop or source an instruction-tuning dataset
  3. Implement RLHF training using human feedback
  4. Create comprehensive documentation and model cards
  5. Establish access methods and licensing
  6. Release and maintain the open-source project

- Design tradeoffs:
  - Performance vs. openness: More open models may have lower performance due to limited resources
  - Data quality vs. data legality: High-quality data may have questionable legal status
  - Documentation effort vs. project utility: Comprehensive documentation requires significant effort but increases project value

- Failure signatures:
  - Incomplete or misleading documentation
  - Closed or ambiguous licensing
  - Inaccessible or undocumented training data
  - Lack of peer-reviewed scientific validation

- First 3 experiments:
  1. Evaluate the openness of a small open-source instruction-tuned model by assessing its code, data, and documentation availability
  2. Compare the performance of an open-source model with a proprietary one on a standard benchmark
  3. Attempt to reproduce results from an open-source project's documentation and assess the clarity and completeness of the instructions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality and legal status of inherited training data affect the overall openness and reliability of instruction-tuned text generators?
- Basis in paper: [explicit] The authors note that many projects inherit undocumented data of questionable legality from base models.
- Why unresolved: The extent to which this affects project openness is unclear, as is the impact on model reliability and potential legal risks.
- What evidence would resolve it: Detailed audits of inherited data sources, documentation of their legal status, and assessments of their impact on model performance and reliability.

### Open Question 2
- Question: What are the implications of using synthetic data for reinforcement learning from human feedback (RLHF) in instruction-tuned text generators?
- Basis in paper: [explicit] The authors discuss the use of synthetic data, such as Self-Instruct and Baize, which are derived from existing models and used in popular instruction-tuned text generators.
- Why unresolved: The consequences of using synthetic reinforcement learning data at scale are unknown and require close scrutiny.
- What evidence would resolve it: Empirical studies comparing models trained with synthetic RLHF data versus those with human-generated RLHF data, and analysis of the long-term impacts on model behavior and user trust.

### Open Question 3
- Question: How can the incentive structures in AI research be changed to promote more openness and accountability?
- Basis in paper: [inferred] The authors highlight that data work is often undervalued and that current incentive structures encourage rapid development over careful scientific work.
- Why unresolved: Changing incentive structures requires systemic changes in how research is valued and funded, which is complex and multifaceted.
- What evidence would resolve it: Case studies of successful open research initiatives, surveys of researchers' attitudes towards openness, and analysis of the impact of different incentive models on research outcomes.

## Limitations
- The survey focuses on projects as of June 2023, potentially missing more recent developments in the rapidly evolving field of open-source instruction-tuned models
- Assessment criteria for the 13 features of openness may not capture all nuances of transparency and accountability
- The evaluation relies on publicly available information, which may be incomplete or misleading in some cases

## Confidence
- High confidence in the identification of recurring issues (undocumented data inheritance, lack of shared RLHF data, scarcity of peer-reviewed papers)
- Medium confidence in the categorization of projects by degrees of openness due to potential information gaps
- Medium confidence in the overall conclusion that openness is essential for fostering transparency and accountability, given the limited scope of evaluated projects

## Next Checks
1. Re-evaluate the same projects using a standardized checklist for each of the 13 openness features to ensure consistent assessment
2. Expand the survey to include projects released after June 2023 to capture recent developments in open-source instruction-tuned models
3. Conduct interviews with project maintainers to verify the accuracy of publicly available information and gain deeper insights into their openness practices