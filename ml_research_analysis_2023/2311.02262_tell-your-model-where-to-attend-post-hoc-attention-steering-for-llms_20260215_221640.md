---
ver: rpa2
title: 'Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs'
arxiv_id: '2311.02262'
source_url: https://arxiv.org/abs/2311.02262
tags:
- pasta
- attention
- heads
- arxiv
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a post-hoc attention steering approach (PASTA)\
  \ that enhances large language models (LLMs) ability to follow user instructions\
  \ and integrate new knowledge by directing the model\u2019s attention to user-specified\
  \ parts of the input. PASTA identifies a small subset of attention heads through\
  \ multi-task model profiling and applies precise attention reweighting on them during\
  \ inference, without changing model parameters."
---

# Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs

## Quick Facts
- arXiv ID: 2311.02262
- Source URL: https://arxiv.org/abs/2311.02262
- Reference count: 26
- Primary result: 22% average accuracy improvement for LLaMA-7B over few-shot prompting across 4 challenging tasks

## Executive Summary
This paper introduces PASTA (Post-hoc Attention Steering), a method that enhances large language models' ability to follow user instructions by steering attention toward user-specified parts of the input. The approach identifies a small subset of attention heads through multi-task model profiling and applies precise attention reweighting during inference without changing model parameters. Experiments on diverse tasks demonstrate significant performance improvements while preserving the model's transfer capabilities.

## Method Summary
PASTA works by first profiling a pre-trained LLM on multiple tasks to identify attention heads that, when steered, generally improve multi-task performance. During inference, it applies attention reweighting to these selected heads, amplifying focus on user-specified content by scaling down attention scores of non-emphasized tokens and normalizing. This post-hoc modification is applied at inference time without altering model weights, allowing the base model to retain general capabilities while improving instruction following and knowledge integration.

## Key Results
- 22% average accuracy improvement for LLaMA-7B over few-shot prompting across 4 challenging tasks
- Effective on tasks requiring instruction following, context interpretation, and knowledge conflict resolution
- Maintains model transfer ability by not modifying model parameters
- Demonstrates robustness to hyperparameter α in attention reweighting

## Why This Works (Mechanism)

### Mechanism 1
Attention reweighting on a small subset of heads can steer model behavior without retraining. By scaling down attention scores of non-user-specified tokens and normalizing, PASTA amplifies focus on emphasized content. Core assumption: The selected heads encode semantic/syntactic information relevant to the user's emphasized text. Evidence: The paper states that steering directs attention to user-specified parts and generates outputs aligning with highlighted contents. Break condition: If selected heads are irrelevant to the task, steering could degrade performance.

### Mechanism 2
Multi-task model profiling identifies effective heads for steering. By evaluating steering performance on small subsets from multiple tasks, PASTA selects heads that improve multi-task performance. Core assumption: Heads effective across multiple tasks are likely effective for unseen tasks as well. Evidence: The paper introduces an efficient model profiling algorithm to identify effective heads for steering. Break condition: If tasks used for profiling are too dissimilar, the intersection of top-k heads may not capture generalizable patterns.

### Mechanism 3
Post-hoc steering preserves transfer ability while improving instruction following. Steering is applied only at inference time without changing model parameters, allowing the base model to retain general capabilities. Core assumption: Attention patterns are not tightly coupled to all downstream capabilities, so selective steering won't break unrelated behaviors. Evidence: The paper notes that unlike finetuning, model steering does not modify model weights and generalizes to new tasks. Break condition: If steering disrupts attention patterns critical for unrelated tasks, transfer performance may degrade.

## Foundational Learning

- **Concept: Multi-head self-attention mechanism**
  - Why needed here: PASTA manipulates attention scores per head, so understanding how each head contributes is essential
  - Quick check question: What is the shape of the attention score matrix for a single head in a transformer layer?

- **Concept: Attention head specialization**
  - Why needed here: PASTA relies on the fact that different heads encode different semantic/syntactic information
  - Quick check question: According to Tenney et al. (2019), what types of linguistic information do early vs. late transformer layers typically capture?

- **Concept: Post-hoc interpretability vs. retraining**
  - Why needed here: PASTA is a post-hoc method, so understanding the distinction between intervention and retraining is key
  - Quick check question: How does PASTA's steering differ from methods like ROME or MEMIT that edit model weights?

## Architecture Onboarding

- **Component map**: Input prompt -> Token embedding -> Attention score computation -> PASTA reweighting -> Normalized attention -> Feed-forward layers -> Output generation
- **Critical path**: (1) Input prompt → (2) Token embedding → (3) Attention score computation → (4) PASTA reweighting → (5) Normalized attention → (6) Feed-forward layers → (7) Output generation
- **Design tradeoffs**: Steering all heads hurts performance due to over-amplification; steering too few heads may not sufficiently emphasize user-specified content
- **Failure signatures**: (a) Performance drops if steered heads are irrelevant; (b) Excessive steering leads to repetitive or degenerate outputs; (c) Incorrect head selection can worsen rather than improve accuracy
- **First 3 experiments**:
  1. Verify steering works by applying it to a single head on a toy task and measuring change in output focus
  2. Test sensitivity to the scaling coefficient α across multiple tasks to find robust range
  3. Compare performance of steering all heads vs. task-profiled subset on a held-out task to confirm benefit of profiling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the scaling coefficient α in PASTA affect the trade-off between emphasizing user-specified information and preserving generation quality?
- Basis in paper: The paper states that PASTA is fairly robust to the hyperparameter α, but also mentions that setting α to zero should be avoided as it leads to performance degeneration
- Why unresolved: The paper does not provide a detailed analysis of how different values of α impact the trade-off between emphasizing user-specified information and preserving generation quality
- What evidence would resolve it: A comprehensive study varying α across a wider range of values and evaluating its impact on both emphasizing efficacy and generation quality metrics

### Open Question 2
- Question: Can PASTA be effectively integrated with other methods, such as few-shot in-context learning, to further enhance model performance and stability?
- Basis in paper: The paper mentions that future work plans to integrate PASTA with few-shot in-context learning to highlight effective examples and enhance stability
- Why unresolved: The paper does not provide any experimental results or analysis on the integration of PASTA with other methods
- What evidence would resolve it: Experiments demonstrating the combined performance of PASTA with other methods, such as few-shot in-context learning, on a variety of tasks and metrics

### Open Question 3
- Question: How does the performance of PASTA vary across different types of user-specified information, such as instructions, context spans, and knowledge conflicts?
- Basis in paper: The paper evaluates PASTA on tasks involving complex instructions, lengthy contexts, and knowledge conflicts, but does not provide a detailed analysis of performance variations across these types of information
- Why unresolved: The paper does not provide a systematic comparison of PASTA's performance on different types of user-specified information
- What evidence would resolve it: A detailed analysis comparing PASTA's performance on tasks involving different types of user-specified information, such as instructions, context spans, and knowledge conflicts, using appropriate metrics

## Limitations

- The effectiveness depends heavily on the stability and transferability of attention head specialization across tasks
- The intersection approach for head selection (using only 3 heads) may be too restrictive and miss task-specific beneficial heads
- Limited validation on completely different task domains to confirm generalizability of the multi-task profiling approach

## Confidence

- **High Confidence**: The claim that attention steering improves performance on the tested tasks (BiasBios, CounterFact, JSON Formatting, Pronouns Changing) - directly supported by reported experimental results
- **Medium Confidence**: The claim that multi-task model profiling identifies generalizable heads - works for their specific task set but generalizability to arbitrary tasks remains unproven
- **Medium Confidence**: The claim that post-hoc steering preserves transfer ability - asserted but lacks extensive evidence from diverse, unrelated tasks

## Next Checks

1. **Head Selection Robustness Test**: Evaluate performance when using different k values for head selection (e.g., top-1, top-5, top-10 heads instead of just the intersection of 3 heads) across the same tasks to determine if the intersection approach is optimal or unnecessarily restrictive.

2. **Cross-Domain Transfer Validation**: Apply the profiled heads from the four main tasks to a completely different set of tasks (e.g., mathematical reasoning, code generation, or scientific QA) to verify whether the multi-task profiling approach truly identifies generalizable heads or if it overfits to the specific task types used in the paper.

3. **Hyperparameter Sensitivity Analysis**: Systematically vary the attention scaling coefficient α across a wider range (e.g., α ∈ [0.1, 0.5, 1.0, 2.0, 5.0]) and measure performance degradation points to establish robustness boundaries and identify if there are task-specific optimal values rather than a single universal setting.