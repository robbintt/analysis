---
ver: rpa2
title: 'Blacksmith: Fast Adversarial Training of Vision Transformers via a Mixture
  of Single-step and Multi-step Methods'
arxiv_id: '2310.18975'
source_url: https://arxiv.org/abs/2310.18975
tags:
- adversarial
- training
- blacksmith
- clean
- pgd-2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses catastrophic overfitting (CO) in fast adversarial
  training of Vision Transformers (ViTs). The proposed method, Blacksmith, combines
  single-step and multi-step adversarial training in a randomized manner: it randomly
  applies either FGSM (single-step) or PGD-2 (multi-step, but applied only to the
  first half of layers) in each training mini-batch.'
---

# Blacksmith: Fast Adversarial Training of Vision Transformers via a Mixture of Single-step and Multi-step Methods

## Quick Facts
- arXiv ID: 2310.18975
- Source URL: https://arxiv.org/abs/2310.18975
- Reference count: 15
- Key outcome: Prevents catastrophic overfitting in ViT adversarial training by randomizing between FGSM and PGD-2 attacks in each mini-batch

## Executive Summary
This paper addresses catastrophic overfitting (CO) in fast adversarial training of Vision Transformers (ViTs). The proposed Blacksmith method randomly alternates between FGSM (single-step) and PGD-2 (multi-step, applied only to first half of layers) during training. This approach increases training diversity, prevents CO, and achieves PGD-2 level adversarial accuracy with FGSM-level training time on CIFAR-10 and CIFAR-100 datasets.

## Method Summary
Blacksmith combines two complementary training schemes: Hammering (FGSM applied end-to-end) and Forging (PGD-2 applied to first half of layers). A Forge-Rate scheduler controls the probability of selecting each scheme, starting at λ = 0.66 and decreasing to 0.33. This randomization prevents the network from learning specific attack patterns that lead to CO, while applying the computationally expensive PGD-2 only to early layers where perturbations are more easily amplified.

## Key Results
- Achieves up to 1.5× better PGD-30-3 adversarial accuracy than N-FGSM
- Maintains FGSM-level training time while achieving PGD-2 level robustness
- Demonstrates no catastrophic overfitting across all experiments
- Shows improved performance as model size increases

## Why This Works (Mechanism)

### Mechanism 1
Randomizing between FGSM and PGD-2 attacks prevents catastrophic overfitting by diversifying the perturbation space during training. The random selection prevents the network from learning a single attack pattern that could be exploited by multi-step attacks. If the random selection frequency is too low or too high, or if the perturbation space isn't sufficiently diverse, this mechanism could fail.

### Mechanism 2
Applying PGD-2 only to the first half of transformer layers prioritizes robustness in early layers where perturbations are more easily amplified. By focusing the computationally expensive PGD-2 attack on the first l layers, the network becomes more robust to perturbations that would otherwise be amplified through subsequent layers. If later layers become too sensitive to perturbations that survive the first half, this mechanism could break.

### Mechanism 3
The Hammering and Forging schemes create a complementary training effect where early layers are robust to multi-step attacks while later layers maintain computational efficiency. Together they create a robust network without excessive computational cost. If the random switching between schemes becomes too frequent or too infrequent to maintain balance, this complementary effect could be lost.

## Foundational Learning

- Concept: Catastrophic overfitting in adversarial training
  - Why needed here: Understanding why single-step methods fail is crucial for appreciating why Blacksmith's randomization approach works
  - Quick check question: What happens to a model's multi-step robustness when catastrophic overfitting occurs?

- Concept: Vision Transformer architecture and layer structure
  - Why needed here: The paper exploits ViT's uniform layer structure (same shape inputs/outputs) to apply different attacks to different layers
  - Quick check question: How does the uniform layer structure of ViTs differ from CNNs in terms of attack application?

- Concept: Min-max optimization in adversarial training
  - Why needed here: Understanding the inner maximization (attack generation) and outer minimization (weight updates) is essential for grasping how Blacksmith balances these processes
  - Quick check question: In adversarial training, what are the two optimization problems being solved simultaneously?

## Architecture Onboarding

- Component map:
  Hammering scheme (FGSM on all layers) -> Forging scheme (PGD-2 on first half of layers) -> Forge-Rate scheduler (controls switching probability) -> Separate optimizers (prevent momentum interference)

- Critical path:
  1. Initialize Forge-Rate λ (typically 0.66 initially)
  2. For each mini-batch, randomly select Hammering or Forging based on λ
  3. Apply selected attack to generate adversarial examples
  4. Update weights using the appropriate optimizer
  5. Adjust λ according to scheduler (decrease over time)
  6. Repeat until training completion

- Design tradeoffs:
  - Computational efficiency vs. robustness: Using PGD-2 only on half layers trades some theoretical robustness for practical speed
  - Clean accuracy vs. adversarial accuracy: Higher Forge-Rate improves adversarial accuracy but may reduce clean accuracy
  - Layer prioritization: Focusing on early layers may leave later layers vulnerable to specific attack patterns

- Failure signatures:
  - Catastrophic overfitting: Sudden drop in multi-step accuracy during training
  - Convergence issues: Oscillating or diverging loss if Forge-Rate is poorly tuned
  - Computational overhead: If Forge-Rate is too high, training becomes slower than pure PGD-2

- First 3 experiments:
  1. Baseline comparison: Train with FGSM, RS-FGSM, N-FGSM, and PGD-2 separately to establish reference points
  2. Forge-Rate sensitivity: Test Blacksmith with different Forge-Rate schedules (λ = 0.33, 0.5, 0.66) to find optimal balance
  3. Layer selection validation: Verify that applying PGD-2 to first half vs. second half yields different robustness profiles

## Open Questions the Paper Calls Out

### Open Question 1
Does Blacksmith's effectiveness extend to larger Vision Transformer models (e.g., ViT-Large) beyond the ViT-B and ViT-Small models tested?
Basis in paper: [inferred] The paper tested Blacksmith on ViT-B and ViT-Small models but did not explore larger architectures. The authors mention that Blacksmith "outshines N-FGSM more and more" as model size increases.
Why unresolved: The paper's experiments were limited to ViT-B and ViT-Small models. Larger models may exhibit different behavior in terms of CO and the effectiveness of Blacksmith's approach.
What evidence would resolve it: Conducting experiments on larger ViT models (e.g., ViT-Large) and comparing Blacksmith's performance against baselines like N-FGSM and PGD-2 would provide conclusive evidence.

### Open Question 2
How does Blacksmith's performance vary when applied to datasets other than CIFAR-10 and CIFAR-100, such as ImageNet or other real-world datasets?
Basis in paper: [inferred] The paper only evaluated Blacksmith on CIFAR-10 and CIFAR-100 datasets. The authors do not discuss its performance on other datasets.
Why unresolved: The effectiveness of Blacksmith might depend on the specific characteristics of the dataset, such as the complexity of the images or the number of classes. Testing on diverse datasets would provide a more comprehensive understanding of its capabilities.
What evidence would resolve it: Evaluating Blacksmith on datasets like ImageNet, which is larger and more complex than CIFAR-10/100, and comparing its performance against baselines would provide insights into its generalizability.

### Open Question 3
What is the theoretical explanation for Blacksmith's success in mitigating catastrophic overfitting, and how does it relate to the observed behavior of the γ* metric?
Basis in paper: [explicit] The paper introduces the γ* metric and observes that it increases during catastrophic overfitting in N-FGSM. However, the authors do not provide a theoretical explanation for why Blacksmith's approach effectively controls γ* and prevents CO.
Why unresolved: While the paper presents empirical evidence of Blacksmith's effectiveness, it lacks a theoretical framework that explains the underlying mechanisms and why the combination of Hammering and Forging schemes works.
What evidence would resolve it: Developing a theoretical model that links Blacksmith's training process to the behavior of γ* and CO, and providing mathematical proofs or rigorous analysis, would offer a deeper understanding of its success.

## Limitations
- The Forge-Rate scheduler values (λ = 0.66 → 0.33) appear somewhat arbitrary without theoretical justification
- Results are limited to Vision Transformers and may not generalize to other architectures like CNNs
- The paper doesn't provide theoretical analysis of why randomization prevents catastrophic overfitting

## Confidence

- **High confidence**: Experimental results showing Blacksmith achieves better PGD-2 level adversarial accuracy than N-FGSM and RS-FGSM with similar training time
- **Medium confidence**: Claim that applying PGD-2 only to the first half of layers is sufficient for robustness
- **Low confidence**: Specific Forge-Rate scheduling strategy (λ = 0.66 initially, then 0.33) and claim that this particular schedule is optimal

## Next Checks

1. **Sensitivity Analysis**: Test Blacksmith with different Forge-Rate schedules (λ = 0.5 constant, λ = 0.33 → 0.66, etc.) to determine if the specific scheduling strategy matters or if any randomization helps

2. **Architecture Transfer**: Apply Blacksmith to CNN architectures (e.g., ResNet) to verify if the randomization approach generalizes beyond Vision Transformers

3. **Theoretical Analysis**: Develop a mathematical framework explaining why randomizing between single-step and multi-step attacks prevents catastrophic overfitting, potentially building on existing CO theory