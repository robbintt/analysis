---
ver: rpa2
title: 'TAPE: Leveraging Agent Topology for Cooperative Multi-Agent Policy Gradient'
arxiv_id: '2312.15667'
source_url: https://arxiv.org/abs/2312.15667
tags:
- agent
- policy
- agents
- tape
- topology
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TAPE, a topology-based multi-agent policy gradient
  method that addresses the centralized-decentralized mismatch (CDM) issue in cooperative
  MARL. TAPE introduces an agent topology framework that determines which agents should
  be considered in each agent's policy gradient, achieving a compromise between facilitating
  cooperation and alleviating the CDM issue.
---

# TAPE: Leveraging Agent Topology for Cooperative Multi-Agent Policy Gradient

## Quick Facts
- **arXiv ID**: 2312.15667
- **Source URL**: https://arxiv.org/abs/2312.15667
- **Reference count**: 40
- **Key outcome**: TAPE outperforms baselines on matrix games, Level-Based Foraging, and SMAC by using agent topology to form coalitions and maximize coalition utility

## Executive Summary
This paper addresses the centralized-decentralized mismatch (CDM) issue in cooperative multi-agent reinforcement learning (MARL) by proposing TAPE, a topology-based policy gradient method. TAPE introduces an agent topology framework that determines which agents should be considered in each agent's policy gradient computation, allowing agents to form coalitions and maximize coalition utility rather than global or individual utilities. The method is theoretically justified with a policy improvement theorem and demonstrates empirical superiority over existing methods across matrix games, Level-Based Foraging, and StarCraft Multi-Agent Challenge tasks.

## Method Summary
TAPE constructs an agent topology using random graph models (Erdős-Rényi, Barabási-Albert, or Watts-Strogatz) where edges represent policy gradient dependencies between agents. Each agent maximizes a coalition utility that aggregates the utilities of connected agents through a mixing network. The method operates in the centralized training, decentralized execution (CTDE) paradigm, using centralized critics during training while maintaining decentralized policies. TAPE comes in both stochastic and deterministic variants and can be combined with various value factorization methods.

## Key Results
- TAPE achieves higher average returns than baselines in three matrix games with varying cooperation difficulty levels
- In Level-Based Foraging tasks, TAPE outperforms existing methods in both 8x8-2p-3f and 15x15-4p-5f scenarios
- On SMAC benchmarks, TAPE shows superior median test win rates across six different maps compared to DOP, PAC, COMA, QMIX, QPLEX, and FACMAC

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Agent topology framework enables coalition-based utility maximization to address CDM issue without sacrificing cooperation
- Mechanism: By constructing an adjacency matrix E where Eij=1 indicates agent i considers agent j's utility, agents form coalitions. Within coalitions, agents maximize shared coalition utility Ui = Σ EijUj, avoiding influence from out-of-coalition agents' suboptimal actions
- Core assumption: Agent relationships can be meaningfully represented as a graph structure where edges indicate policy gradient dependencies
- Evidence anchors:
  - [abstract]: "The agent topology allows agents to use coalition utility as learning objective instead of global utility by centralized critics or local utility by individual critics."
  - [section]: "Under the agent topology framework, agents connected in the topology consider and maximize each other's utilities. Thus, the shared objective makes each individual agent forms a coalition with its connected neighbors."

### Mechanism 2
- Claim: Policy improvement theorem guarantees monotonic improvement for stochastic TAPE under tabular policies
- Mechanism: The policy gradient update ensures that actions with higher Q values receive larger stepsizes (βai,s), maintaining the ordering of action values and guaranteeing policy improvement
- Core assumption: The mixing network preserves the order of local action values (Lemma 1 from Wang et al. 2020c)
- Evidence anchors:
  - [section]: "We prove the policy improvement theorem for stochastic TAPE and give a theoretical explanation for improved cooperation among agents."
  - [section]: "With tabular expressions for policies, for any pre-update policy π and updated policy ˆπ by policy gradient in Eq. 2 that satisfy for any agent i, ˆπi(ai|τi) = πi(ai|τi)+ βai,sδ, where δ is a sufficiently small number, we have J(ˆπ) ≥ J(π)."

### Mechanism 3
- Claim: Agent topology provides larger diversity in parameter updates compared to individual critics, enabling better exploration of cooperation patterns
- Mechanism: The variance of policy updates in TAPE (Var(ξTAPE)) is greater than in DOP (Var(ξDOP)), with the difference proportional to p² where p is the edge probability in ER model
- Core assumption: Higher variance in parameter updates correlates with better exploration of the parameter space
- Evidence anchors:
  - [section]: "Theorem 2 shows that compared to solely using individual critics, our agent topology provides larger diversity in policy updates to find better cooperation pattern."
  - [section]: "Theorem 2. For any agent i and ∀s, ai, the stochastic TAPE policy update ξTAPEai,s and DOP policy update ξDOPai,s satisfy that Var(ξTAPEai,s) ≥ Var(ξDOPai,s), and ∆ = Var(ξTAPEai,s) − Var(ξDOPai,s) is in proportion to p², where p is the probability of edges being present in the Erd˝os–R´enyi model, i.e. ∆ ∝ p²."

## Foundational Learning

- Concept: Multi-Agent Reinforcement Learning (MARL) with Centralized Training Decentralized Execution (CTDE)
  - Why needed here: TAPE operates within the CTDE paradigm, using centralized critics during training while maintaining decentralized policies for execution
  - Quick check question: What is the primary advantage of CTDE over fully decentralized training in cooperative MARL?

- Concept: Policy Gradient Methods in Multi-Agent Settings
  - Why needed here: TAPE extends policy gradient techniques to multi-agent contexts by modifying how agents compute their policy gradients based on agent topology
  - Quick check question: How does the policy gradient in TAPE differ from standard single-agent policy gradient?

- Concept: Graph Theory and Network Models (Erd˝os-Rényi, Barabási-Albert, Watts-Strogatz)
  - Why needed here: Agent topology is constructed using random graph models to balance cooperation and CDM mitigation
  - Quick check question: What property of Erd˝os-Rényi graphs makes them particularly suitable for generating diverse agent topologies?

## Architecture Onboarding

- Component map: Agent topology (adjacency matrix E) -> Coalition utility computation Ui = Σ EijUj -> Mixing network fmix -> Individual Q networks Qϕi -> Policy networks πi -> Target networks -> Replay buffer

- Critical path:
  1. Initialize all components
  2. Generate agent topology using ER model
  3. Collect trajectories in environment
  4. Compute coalition utilities based on topology
  5. Update critics using on-policy and off-policy losses
  6. Update policies using modified policy gradients
  7. Periodically update target networks

- Design tradeoffs:
  - Higher p (edge probability) increases cooperation but worsens CDM
  - Lower p reduces CDM but limits cooperation
  - Tabular vs. function approximation policies affects theoretical guarantees
  - On-policy vs. off-policy learning balances sample efficiency vs. bias

- Failure signatures:
  - Poor performance despite high p: CDM issue overwhelming cooperation benefits
  - Inconsistent results across runs: Insufficient diversity in generated topologies
  - Slow convergence: Inadequate exploration due to suboptimal topology structure
  - Degraded performance with increasing agents: Scalability issues in topology generation or utility computation

- First 3 experiments:
  1. Run TAPE with p=0 (equivalent to DOP) on a simple matrix game to verify baseline performance
  2. Incrementally increase p and observe performance on matrix games requiring varying levels of cooperation
  3. Test different random graph models (ER, BA, WS) on a medium-difficulty SMAC map to identify optimal topology generation method

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the agent topology framework scale to larger multi-agent systems with hundreds or thousands of agents?
- Basis in paper: [inferred] The paper focuses on experiments with small to medium-sized agent populations (up to 12 agents). The agent topology is constructed using random graph models, which may not be practical for very large systems.
- Why unresolved: The paper does not explore the computational or communication costs of constructing and maintaining agent topologies in large-scale systems. It also doesn't address how the topology might evolve or be learned in such settings.
- What evidence would resolve it: Experiments demonstrating TAPE's performance and scalability on problems with hundreds or thousands of agents, along with analysis of computational and communication costs.

### Open Question 2
- Question: How robust is TAPE to variations in the agent topology structure, and can the topology be learned or adapted during training?
- Basis in paper: [explicit] The paper discusses using random graph models to generate agent topologies and shows that the Erdös-Rényi model performs well. It also mentions that the topology can be learned via optimization-based methods, but this may result in limited diversity.
- Why unresolved: The paper does not explore how sensitive TAPE's performance is to the specific structure of the agent topology. It also doesn't investigate methods for adapting the topology during training based on the agents' learning progress or the environment's dynamics.
- What evidence would resolve it: Experiments comparing TAPE's performance across a range of topology structures, including learned or adaptive topologies. Analysis of how topology adaptation affects cooperation and CDM mitigation.

### Open Question 3
- Question: How does TAPE perform in mixed cooperative-competitive multi-agent settings where some agents have conflicting goals?
- Basis in paper: [inferred] The paper focuses on cooperative multi-agent tasks where all agents share the same reward. The agent topology is designed to facilitate cooperation and mitigate the CDM issue in this context.
- Why unresolved: The paper does not explore how TAPE's agent topology framework might be extended or modified to handle settings where agents have competing interests. It also doesn't discuss how TAPE would perform in such scenarios compared to other methods designed for mixed cooperative-competitive settings.
- What evidence would resolve it: Experiments evaluating TAPE's performance in mixed cooperative-competitive tasks, along with comparisons to methods specifically designed for such settings. Analysis of how the agent topology needs to be adapted to handle conflicting agent goals.

## Limitations

- Theoretical guarantees only apply to tabular policies, not the function approximation case used in experiments
- Performance comparisons limited to specific instances of matrix games, Level-Based Foraging, and SMAC tasks
- Scalability to large-scale multi-agent systems with hundreds or thousands of agents is not thoroughly investigated

## Confidence

- **Theoretical Confidence (Medium)**: Policy improvement theorem provides theoretical grounding for stochastic TAPE under tabular policies, but doesn't extend to function approximation case used in experiments
- **Empirical Confidence (Medium)**: TAPE shows consistent improvement over baselines across tested domains, but comparisons limited to specific task instances
- **Scalability Confidence (Low)**: Paper doesn't thoroughly investigate performance as agent count increases substantially beyond tested scenarios

## Next Checks

1. **Transferability Test**: Apply TAPE to a new cooperative MARL environment with different dynamics (e.g., traffic signal control or robotic coordination) to verify the method's generalization beyond the tested domains

2. **Scalability Analysis**: Systematically evaluate TAPE's performance and computational requirements as the agent count increases from 10 to 100 agents in a controlled environment to identify practical scaling limits

3. **Topology Sensitivity Study**: Conduct ablation studies varying the edge probability p and graph model parameters across a wider range to precisely quantify the cooperation-CDM tradeoff curve and identify optimal topology configurations for different task types