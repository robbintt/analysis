---
ver: rpa2
title: 'PEFTT: Parameter-Efficient Fine-Tuning for low-resource Tibetan pre-trained
  language models'
arxiv_id: '2309.12109'
source_url: https://arxiv.org/abs/2309.12109
tags:
- adapter
- fine-tuning
- tibetan
- prompt-tuning
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study explores three efficient fine-tuning methods\u2014\
  Prompt-tuning, Adapter, and their combination\u2014for low-resource Tibetan pre-trained\
  \ language models using the TNCC-title dataset. Results show that Prompt-tuning\
  \ improves model performance with a 1.6-2.4% increase in test macro-F1, while Adapter\
  \ reduces training parameters to just 0.17-0.47% of full fine-tuning with minimal\
  \ performance loss."
---

# PEFTT: Parameter-Efficient Fine-Tuning for low-resource Tibetan pre-trained language models

## Quick Facts
- arXiv ID: 2309.12109
- Source URL: https://arxiv.org/abs/2309.12109
- Reference count: 36
- One-line primary result: PEFT methods (Prompt-tuning and Adapter) significantly improve parameter efficiency for Tibetan PLMs while maintaining or exceeding baseline performance

## Executive Summary
This study investigates parameter-efficient fine-tuning methods for low-resource Tibetan pre-trained language models using the TNCC-title dataset. The research evaluates Prompt-tuning, Adapter-based fine-tuning (LoRA), and their combination across three Tibetan PLMs (CINO, Tibert, Tibetan-bert). Results demonstrate that Prompt-tuning improves performance by 1.6-2.4% in test macro-F1, while Adapter reduces training parameters to just 0.17-0.47% of full fine-tuning with minimal performance loss. The combined approach achieves optimal results by significantly reducing parameters while maintaining or exceeding baseline performance, providing valuable insights for Tibetan and similar low-resource language applications.

## Method Summary
The study explores three parameter-efficient fine-tuning techniques for Tibetan pre-trained language models: Prompt-tuning, Adapter-based fine-tuning using LoRA, and their combination. Prompt-tuning reformulates classification tasks as Masked Language Model tasks using manually designed templates and verbalizers, while LoRA adapters inject low-rank decomposition matrices into transformer layers. The combined approach leverages both techniques, using the OpenPrompt and OpenDelta frameworks for implementation. Experiments were conducted on three Tibetan PLMs (CINO-small, CINO-base, Tibert, Tibetan-bert) using the TNCC-title dataset with configurations including max length 108, batch sizes 4-16, and learning rates ranging from 1e-4 to 6e-6 depending on the scenario.

## Key Results
- Prompt-tuning improves model performance with a 1.6-2.4% increase in test macro-F1
- Adapter reduces training parameters to just 0.17-0.47% of full fine-tuning with minimal performance loss
- Combining both methods achieves optimal results: significant parameter reduction while maintaining or exceeding baseline performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt-tuning improves model performance by reformulating classification tasks as Masked Language Model (MLM) tasks using templates and verbalizers.
- Mechanism: The model transforms input sentences into prompt inputs with <MASK> tokens, then predicts the most probable label words from a pre-defined label space.
- Core assumption: The pre-trained language model's masked language modeling capabilities can be effectively leveraged for downstream classification tasks when appropriate templates and verbalizers are provided.
- Evidence anchors:
  - [abstract] Results show that Prompt-tuning improves model performance with a 1.6-2.4% increase in test macro-F1
  - [section] Prompt-tuning formalizes the classification task as a Masked Language Model (MLM) task
  - [corpus] Parameter-Efficient Language Model Tuning with Active Learning in Low-Resource Settings

### Mechanism 2
- Claim: Adapter modules reduce training parameters to just 0.17-0.47% of full fine-tuning with minimal performance loss.
- Mechanism: Low-Rank Adaptation (LoRA) injects trainable low-rank decomposition matrices into transformer layers while freezing most model parameters.
- Core assumption: The update matrix for fine-tuning can be approximated by a low-rank decomposition, allowing efficient parameter updates with minimal additional parameters.
- Evidence anchors:
  - [abstract] Adapter reduces training parameters to just 0.17-0.47% of full fine-tuning with minimal performance loss
  - [section] LoRA shares some similarities but with a notable distinction. Both LoRA and the original Adapter employ a trapezoidal module with up-sampling and down-sampling
  - [corpus] Parameter-Efficient Language Model Tuning with Active Learning in Low-Resource Settings

### Mechanism 3
- Claim: Combining Prompt-tuning and Adapter achieves optimal results by significantly reducing parameters while maintaining or exceeding baseline performance.
- Mechanism: The combined approach leverages both the template-based reformulation of Prompt-tuning and the parameter-efficient adaptation of LoRA Adapters.
- Core assumption: The benefits of Prompt-tuning and Adapter can be additive, with Prompt-tuning improving task performance and Adapter maintaining parameter efficiency.
- Evidence anchors:
  - [abstract] Combining both methods achieves optimal results: significant parameter reduction while maintaining or exceeding baseline performance
  - [section] By combining these two fine-tuning techniques, we achieved a balanced effect that incorporates the advantages of both
  - [corpus] Adapting Multilingual LLMs to Low-Resource Languages with Knowledge Graphs via Adapters

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: Understanding MLM is crucial because Prompt-tuning reformulates classification tasks as MLM tasks using <MASK> tokens
  - Quick check question: How does MLM differ from standard language modeling, and why is it suitable for prompt-based classification?

- Concept: Low-Rank Matrix Decomposition
  - Why needed here: The LoRA adapter uses low-rank decomposition to approximate parameter updates efficiently
  - Quick check question: What is the mathematical relationship between the original update matrix and its low-rank approximation in LoRA?

- Concept: Cross-lingual Transfer Learning
  - Why needed here: The study adapts techniques developed for high-resource languages to low-resource Tibetan, requiring understanding of transfer learning challenges
  - Quick check question: What are the key challenges in transferring pre-trained models from high-resource to low-resource language settings?

## Architecture Onboarding

- Component map: Pre-trained Tibetan PLMs (CINO, Tibert, Tibetan-bert) -> OpenPrompt framework -> OpenDelta framework -> TNCC-title dataset -> Evaluation metrics

- Critical path:
  1. Load pre-trained model and tokenizer
  2. Expand tokenizer vocabulary with template-specific tokens
  3. Configure Prompt-tuning templates and verbalizers
  4. Inject LoRA adapters into transformer layers
  5. Train with combined loss from prompt and adapter updates
  6. Evaluate on test set using macro-F1 metric

- Design tradeoffs:
  - Prompt-tuning vs full fine-tuning: 1-2% performance gain vs 100x parameter increase
  - Adapter rank selection: Higher rank gives better performance but more parameters
  - Template complexity: More complex templates may improve performance but require more vocabulary expansion

- Failure signatures:
  - Performance collapse: Template-verbalizer mapping is incorrect or vocabulary expansion failed
  - Memory overflow: Batch size too large for GPU memory with combined Prompt-tuning and Adapter
  - Training instability: Learning rate too high for combined optimization

- First 3 experiments:
  1. Baseline full fine-tuning on smallest CINO-small-v2 model to establish performance ceiling
  2. Prompt-tuning only on CINO-small-v2 to measure template effectiveness
  3. Adapter only on CINO-small-v2 to establish parameter efficiency baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of Prompt-tuning and Adapter methods change if applied to larger, more complex Tibetan language models beyond those tested in this study?
- Basis in paper: [explicit] The paper states "Considering the differences in various pre-trained models, we conduct numerous configuration experiments. For each model, we explore efficient fine-tuning configurations that enhance model adaptability."
- Why unresolved: The study only tested three specific Tibetan PLMs (CINO, Tibert, and Tibetan-bert), leaving open the question of how these methods would scale to larger models.
- What evidence would resolve it: Systematic experiments applying Prompt-tuning and Adapter methods to larger Tibetan language models with varying architectures and parameter counts.

### Open Question 2
- Question: What are the optimal strategies for dynamically expanding Tibetan vocabulary during training and inference for Prompt-tuning?
- Basis in paper: [explicit] "Specifically, for training models in the Tibetan monolingual context, we dynamically expand the Tibetan vocabulary during training and inference."
- Why unresolved: The paper mentions dynamic vocabulary expansion but does not provide detailed methodology or results for different expansion strategies.
- What evidence would resolve it: Comparative studies of different vocabulary expansion techniques and their impact on model performance and efficiency.

### Open Question 3
- Question: How do Prompt-tuning and Adapter methods compare in terms of long-term model stability and generalization across diverse Tibetan language tasks?
- Basis in paper: [inferred] The paper focuses on classification tasks and provides performance metrics for specific scenarios, but does not address long-term stability or generalization.
- Why unresolved: The experiments are limited to a single classification dataset, and there is no discussion of model performance over extended periods or across varied tasks.
- What evidence would resolve it: Longitudinal studies tracking model performance across multiple Tibetan language tasks over time, including evaluation of stability and generalization capabilities.

## Limitations

- Model-Specific Template Effectiveness: Different Tibetan PLMs require different manual templates, but exact templates are not provided, making it difficult to assess generalizability
- Limited Model Diversity: Evaluation restricted to four Tibetan PLMs without exploring full diversity of pre-trained models or architectural differences
- Dataset Size and Domain Specificity: Performance may be influenced by dataset-specific characteristics, with effectiveness in other low-resource Tibetan domains untested

## Confidence

- **Prompt-tuning improves model performance with 1.6-2.4% increase in test macro-F1**: Medium
- **Adapter reduces training parameters to just 0.17-0.47% of full fine-tuning with minimal performance loss**: High
- **Combining both methods achieves optimal results**: Medium

## Next Checks

1. **Template Ablation Study**: Conduct an ablation study by systematically varying the prompt templates to quantify the contribution of each template element to the overall performance gain.

2. **Cross-Domain Generalization Test**: Evaluate the fine-tuned models on a different Tibetan text classification dataset to assess their ability to generalize beyond the TNCC-title domain.

3. **Hyperparameter Sensitivity Analysis**: Perform a grid search or random search over key hyperparameters for each model and method to identify the most influential factors and their optimal ranges.