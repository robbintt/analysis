---
ver: rpa2
title: Self-Supervised Single-Image Deconvolution with Siamese Neural Networks
arxiv_id: '2308.09426'
source_url: https://arxiv.org/abs/2308.09426
tags:
- image
- deconvolution
- loss
- training
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses single-image deconvolution for microscopy
  by extending self-supervised denoising networks with known point spread function.
  The authors propose using a Siamese invariance loss between masked and unmasked
  network outputs, combined with FFT-based convolution for efficiency.
---

# Self-Supervised Single-Image Deconvolution with Siamese Neural Networks

## Quick Facts
- **arXiv ID**: 2308.09426
- **Source URL**: https://arxiv.org/abs/2308.09426
- **Reference count**: 34
- **Primary result**: Achieves PSNR gains of 0.8 dB in 2D and 1.5 dB in 3D over classical deconvolution methods using self-supervised Siamese networks with FFT convolution

## Executive Summary
This paper addresses single-image deconvolution for microscopy by extending self-supervised denoising networks with known point spread functions. The authors propose using a Siamese invariance loss between masked and unmasked network outputs, combined with FFT-based convolution for efficiency. They compare three loss formulations and find that applying invariance on the deconvolved representation works best for 2D images, while applying it after PSF convolution works best for 3D. The method outperforms classical Lucy-Richardson deconvolution and prior self-supervised approaches, achieving PSNR gains of 0.8 dB in 2D and 1.5 dB in 3D. Training with FFT reduces computation time by factors of 2-14× for larger kernels. The work sets a new standard for single-image deconvolution without requiring paired clean data.

## Method Summary
The method extends self-supervised denoising networks (specifically Noise2Self) to handle known point spread functions for deconvolution. It uses a Siamese network architecture that performs two forward passes - one with masked input and one with unmasked input. The invariance loss compares outputs at masked pixel locations between these passes, preventing the network from learning an identity function. The network is trained on single noisy images without requiring clean ground truth. FFT-based convolution is used for efficiency with large PSF kernels. Three loss formulations are compared: the baseline Noise2Self, invariance applied after PSF convolution (Noise2Same), and invariance applied before PSF convolution (Noise2Same(d)).

## Key Results
- PSNR gains of 0.8 dB in 2D and 1.5 dB in 3D compared to Lucy-Richardson deconvolution
- FFT convolution reduces training time by 2-14× for larger kernels (14× speedup for 31×31×31 kernel)
- Applying invariance on deconvolved representation works best for 2D data, while applying it after PSF convolution works best for 3D
- The method outperforms prior self-supervised approaches without requiring paired clean data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Siamese invariance loss between masked and unmasked network outputs prevents the network from learning an identity function
- Mechanism: By comparing outputs at masked pixel locations between forward passes with masked and unmasked inputs, the network is forced to reconstruct the true signal rather than just copying the input
- Core assumption: The masked pixels contain no information about the true signal (they're replaced with noise)
- Evidence anchors:
  - [abstract] "They prevent the network from learning an identity function from Lrec minimization"
  - [section] "The latter theory is based on the idea that a network cannot learn the noise that is conditionally pixel-wise independent given the signal"
- Break condition: If masking is too small or noise too weak, the network could learn correlations between masked and unmasked regions

### Mechanism 2
- Claim: FFT-based convolution reduces computational complexity for large PSF kernels
- Mechanism: Convolution in Fourier space has complexity O(M^d d log M) independent of kernel size N, making it more efficient than spatial convolution O(M^d N^d) for large kernels
- Core assumption: The PSF kernel is fixed and known
- Evidence anchors:
  - [section] "If image x is M × M pixels and kernel k is N × N and both of them have dimensionality of d, the complexity of an ordinary convolution is O(M^d N^d), while the complexity of a Fourier convolution is O(M^d d log M)"
  - [section] "It is recommended to use FFT with 2D kernels for N > 25 and with 3D kernels for N > 9"
- Break condition: For small kernels, spatial convolution may be faster due to FFT overhead

### Mechanism 3
- Claim: Applying invariance loss at different positions (before vs after PSF convolution) works best for different data types
- Mechanism: The optimal position depends on the data structure - deconvolved representation works best for 2D data while reconvolved works best for 3D
- Core assumption: The optimal solution depends on the data characteristics
- Evidence anchors:
  - [section] "Loss function performance was inconsistent between the 2D and 3D cases. Invariance loss applied to deconvolved representation proved best for 2D data but failed to give an advantage in 3D"
  - [section] "We hypothesize that the optimal solution depends on the data; e.g. in microtubules, the 3D dataset signal is very sparse"
- Break condition: If the data structure changes significantly, the optimal position may need to be re-evaluated

## Foundational Learning

- **Concept: Self-supervised learning with blind-spot networks**
  - Why needed here: No paired clean data available for supervised training
  - Quick check question: Why can't we just train a network to output the clean image directly?

- **Concept: Fourier transforms and convolution theorem**
  - Why needed here: FFT convolution provides computational efficiency for large PSF kernels
  - Quick check question: What is the computational complexity of spatial vs Fourier convolution for a 31x31 kernel?

- **Concept: Point spread function and its role in deconvolution**
  - Why needed here: Known PSF is essential for the self-supervised deconvolution framework
  - Quick check question: How does the PSF relate to the blurring observed in microscopy images?

## Architecture Onboarding

- **Component map**: Input → Siamese network (masked/unmasked forward passes) → PSF convolution → Loss calculation → Parameter update
- **Critical path**: Input → Network → PSF convolution → Loss calculation → Parameter update
- **Design tradeoffs**:
  - Siamese network doubles computation but improves performance
  - FFT convolution speeds up training for large kernels but adds implementation complexity
  - Multiple loss terms require careful weighting
- **Failure signatures**:
  - If invariance loss is too strong: network may over-smooth images
  - If blind-spot loss dominates: network may not learn useful features
  - If FFT convolution has numerical errors: artifacts may appear in output
- **First 3 experiments**:
  1. Train with only blind-spot loss to establish baseline performance
  2. Add reconstruction loss and invariance loss at different positions to find optimal configuration
  3. Compare spatial vs FFT convolution for different kernel sizes to determine efficiency threshold

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal position for the Siamese invariance loss in deconvolution neural networks, and does this optimal position vary depending on the specific characteristics of the dataset (e.g., signal sparsity, noise type)?
- Basis in paper: [explicit] The paper identifies that applying invariance on the deconvolved representation works best for 2D images, while applying it after PSF convolution works best for 3D. However, it hypothesizes that the optimal solution depends on the data.
- Why unresolved: The paper only tests on one 3D dataset (microtubules) and a limited set of 2D images. The optimal position may vary with different image characteristics or noise types.
- What evidence would resolve it: Systematic testing on diverse datasets with varying characteristics (e.g., different signal sparsity, noise distributions) to determine if the optimal loss position is dataset-dependent.

### Open Question 2
- Question: How does the performance of the proposed FFT-based convolution approach scale with increasing kernel sizes, and what is the theoretical limit where FFT becomes more computationally efficient than direct convolution?
- Basis in paper: [explicit] The paper mentions that FFT convolution is computationally cheaper for large kernels (N > 25 for 2D, N > 9 for 3D) and observes a 14x speedup for a 31×31×31 kernel.
- Why unresolved: The paper only tests up to a 31×31×31 kernel and does not provide a theoretical analysis of the scaling behavior or the exact threshold where FFT becomes more efficient.
- What evidence would resolve it: Theoretical analysis of computational complexity for varying kernel sizes and empirical testing with extremely large kernels (e.g., the size of the image itself) to determine the efficiency crossover point.

### Open Question 3
- Question: Can the self-supervised deconvolution framework be effectively extended to handle non-confocal microscopy data with extremely large point spread functions, and what modifications (if any) would be required to maintain performance?
- Basis in paper: [explicit] The paper concludes by suggesting that their method allows one to use it for non-confocal microscopy with extremely big point spread functions.
- Why unresolved: The paper does not provide any experimental validation on non-confocal microscopy data or discuss specific modifications needed for handling extremely large PSFs.
- What evidence would resolve it: Experimental validation on non-confocal microscopy datasets with varying PSF sizes, and analysis of any architectural or training modifications required to maintain performance with extremely large PSFs.

## Limitations
- Performance validation relies on synthetic 3D data, limiting real-world applicability
- PSNR improvements (0.8 dB 2D, 1.5 dB 3D) may not translate to meaningful biological insights
- Optimal invariance loss position requires dataset-specific tuning
- Computational efficiency gains not fully validated across diverse hardware configurations

## Confidence
- **High**: The core mechanism of using Siamese invariance loss to prevent identity function learning is well-established and theoretically sound
- **Medium**: The comparative performance against Lucy-Richardson and prior self-supervised methods is convincing for the tested datasets
- **Medium**: The efficiency benefits of FFT convolution for large kernels are mathematically sound but empirical validation across diverse hardware is limited

## Next Checks
1. **Cross-dataset validation**: Test the method on additional real microscopy datasets beyond the 22-image benchmark to verify generalization
2. **Biological relevance assessment**: Collaborate with microscopy domain experts to evaluate whether PSNR improvements translate to meaningful biological insights
3. **Ablation study on loss weighting**: Systematically vary the relative weights of blind-spot, reconstruction, and invariance losses to identify optimal configurations for different microscopy modalities