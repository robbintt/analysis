---
ver: rpa2
title: 'Towards Open Vocabulary Learning: A Survey'
arxiv_id: '2306.15880'
source_url: https://arxiv.org/abs/2306.15880
tags:
- segmentation
- open
- detection
- vocabulary
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a comprehensive overview of open vocabulary
  learning in computer vision, focusing on object detection and segmentation tasks.
  The paper reviews recent advancements in leveraging large-scale vision-language
  models (VLMs) like CLIP and ALIGN to enable models to recognize novel classes beyond
  their training set.
---

# Towards Open Vocabulary Learning: A Survey

## Quick Facts
- arXiv ID: 2306.15880
- Source URL: https://arxiv.org/abs/2306.15880
- Reference count: 40
- Primary result: Comprehensive survey of open vocabulary learning methods achieving up to 42.7 APnovel on COCO for novel class detection

## Executive Summary
This survey comprehensively reviews open vocabulary learning in computer vision, focusing on object detection and segmentation tasks. The paper examines how large-scale vision-language models (VLMs) like CLIP and ALIGN enable recognition of novel classes beyond traditional training sets. Recent advancements leverage knowledge distillation, region-text alignment using caption data, and balanced datasets to extend model capabilities. The survey covers multiple tasks including open vocabulary object detection, semantic segmentation, instance segmentation, and panoptic segmentation, providing detailed method comparisons and benchmark results across COCO, LVIS, ADE20K, and other datasets.

## Method Summary
The core approach involves replacing traditional fixed classifiers with text embeddings from vision-language models (VLMs) like CLIP or ALIGN. Visual embeddings from the model are compared with text embeddings via dot product similarity to classify both base and novel classes. Methods typically use knowledge distillation from VLMs to align visual region representations with caption embeddings, or employ region-text alignment using caption data as auxiliary supervision. Training involves fine-tuning on base class annotations while evaluating on novel classes using metrics like APnovel and mIoU.

## Key Results
- BARON achieves 42.7 APnovel on COCO and 22.6 APr on LVIS for novel class detection
- Open vocabulary segmentation methods show significant improvements over traditional approaches
- Knowledge distillation from VLMs consistently improves performance across detection and segmentation tasks
- Caption data provides effective weak supervision for extending semantic space

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Open vocabulary learning enables detection/segmentation of novel classes by leveraging vision-language models (VLMs) like CLIP and ALIGN.
- Mechanism: The model uses text embeddings from VLMs to replace traditional fixed classifiers. Visual embeddings from the model are compared with text embeddings via dot product similarity to classify both base and novel classes.
- Core assumption: VLMs have learned rich visual-language alignments that generalize to unseen classes.
- Evidence anchors:
  - [abstract]: "leveraging large-scale vision-language models (VLMs) like CLIP and ALIGN to enable models to recognize novel classes beyond their training set"
  - [section]: "A common way towards the open vocabulary setting is to replace the fixed classifier weights with the text embeddings from a VLM model"
  - [corpus]: "Open vocabulary learning enables detection/segmentation of novel classes by leveraging vision-language models (VLMs) like CLIP and ALIGN."
- Break condition: If VLMs lack visual-language alignment for specific novel classes or if the dot product similarity fails to distinguish similar classes.

### Mechanism 2
- Claim: Knowledge distillation from VLMs improves open vocabulary detection performance.
- Mechanism: The model distills knowledge from VLMs into close-set detectors, aligning visual region representations with caption embeddings through contrastive loss or L1 Loss.
- Core assumption: VLMs contain richer semantic information than close-set detectors, and this knowledge can be transferred.
- Evidence anchors:
  - [abstract]: "Key methods include knowledge distillation from VLMs"
  - [section]: "ViLD consists of two branches... employing global-level knowledge distillation modules (GKD), which align global-level image representations with caption embeddings through contrastive loss"
  - [corpus]: "Knowledge distillation from VLMs improves open vocabulary detection performance."
- Break condition: If the distillation process fails to capture relevant semantic information or if the close-set detector's architecture is incompatible with VLM knowledge.

### Mechanism 3
- Claim: Using caption data as auxiliary supervision extends the semantic space of open vocabulary models.
- Mechanism: The model learns visual region representation by matching image regions to region-level descriptions, creating pseudo labels for training.
- Core assumption: Caption data contains large enough vocabulary to cover most novel categories and provides weak supervision for novel class detection.
- Evidence anchors:
  - [abstract]: "Language data requires less labeling effort and thus is more cost-friendly"
  - [section]: "OpenSeg [37] applies a region-word grounding loss between mask features and caption word features"
  - [corpus]: "Using caption data as auxiliary supervision extends the semantic space of open vocabulary models."
- Break condition: If caption data lacks relevant vocabulary for target novel classes or if the grounding process introduces noise.

## Foundational Learning

- Concept: Vision-Language Pre-training
  - Why needed here: VLMs like CLIP and ALIGN are foundational to open vocabulary learning, providing the visual-language alignment necessary for novel class recognition.
  - Quick check question: What is the key difference between CLIP and traditional computer vision models in terms of training data and objectives?

- Concept: Object Detection and Segmentation Fundamentals
  - Why needed here: Understanding close-set detection and segmentation is crucial for extending these methods to open vocabulary settings.
  - Quick check question: What are the main differences between two-stage and one-stage object detection approaches?

- Concept: Zero-shot Learning
  - Why needed here: Open vocabulary learning builds upon zero-shot learning concepts but extends them with additional supervision from VLMs or caption data.
  - Quick check question: How does open vocabulary learning differ from traditional zero-shot learning in terms of available supervision?

## Architecture Onboarding

- Component map: Vision model (backbone + detection/segmentation head) -> VLM (text encoder + image encoder) -> Classification head (text embeddings) -> Optional region proposal network/mask head/temporal fusion layer

- Critical path:
  1. Input image → Vision model → Visual embeddings
  2. Visual embeddings → Compare with text embeddings from VLM
  3. Compute classification scores → Select class with highest score

- Design tradeoffs:
  - Fixed VLM vs. trainable VLM: Fixed VLMs preserve pre-trained knowledge but may limit adaptation; trainable VLMs allow adaptation but risk overfitting.
  - Knowledge distillation vs. direct usage: Distillation can improve performance but adds complexity; direct usage is simpler but may be less effective.
  - Caption data vs. image-text pairs: Caption data is easier to obtain but may be less informative; image-text pairs provide richer context but are harder to collect.

- Failure signatures:
  - Poor performance on novel classes: May indicate insufficient VLM knowledge transfer or inadequate training data for novel classes.
  - Overfitting to base classes: May suggest the need for more balanced training data or stronger regularization.
  - Slow inference: May indicate inefficient use of VLM features or overly complex model architecture.

- First 3 experiments:
  1. Implement basic open vocabulary detection using a pre-trained VLM and compare performance with close-set detection on novel classes.
  2. Test knowledge distillation from VLM to close-set detector and measure improvement in novel class detection.
  3. Evaluate the impact of using caption data as auxiliary supervision on open vocabulary segmentation performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can open vocabulary learning methods be made more efficient for training on target datasets with limited computational resources?
- Basis in paper: [explicit] The paper mentions that most state-of-the-art methods require huge data for pre-training to achieve good performances, which can be expensive and unavailable for many research groups. It suggests exploring more efficient data learning pipelines or learning methods, possibly using in-context learning to connect knowledge of VLMs and LLMs.
- Why unresolved: The paper acknowledges the high computational cost of current methods but does not propose specific solutions or evaluate the effectiveness of potential efficiency improvements.
- What evidence would resolve it: A study comparing the performance and resource usage of various open vocabulary learning methods on target datasets, including both traditional pre-training approaches and newer, more efficient methods like in-context learning.

### Open Question 2
- Question: How can open vocabulary learning methods be extended to handle incremental learning of novel classes without catastrophic forgetting?
- Basis in paper: [explicit] The paper discusses the challenges of combining open vocabulary learning with incremental learning, noting that directly turning to incremental learning may lead to catastrophic forgetting problems. It suggests exploring how to handle both catastrophic forgetting and novel class detection in one framework.
- Why unresolved: The paper identifies the problem but does not propose or evaluate solutions for handling both incremental learning and open vocabulary learning simultaneously.
- What evidence would resolve it: A new method that successfully combines incremental learning with open vocabulary learning, demonstrating both the ability to learn new classes without forgetting old ones and maintaining good performance on novel class detection.

### Open Question 3
- Question: How can open vocabulary learning methods be improved to handle base classes over-fitting issues, especially when novel classes have similar shapes and semantics?
- Basis in paper: [explicit] The paper mentions that most detectors easily overfit the base classes when both the novel classes have similar shapes and semantics, as these classes are trained with higher confidence scores. It suggests that more fine-grained feature discriminative modeling, including parts or attributes, is required to handle these problems.
- Why unresolved: The paper identifies the issue of base class over-fitting but does not propose or evaluate specific methods to address this problem.
- What evidence would resolve it: A new open vocabulary learning method that incorporates fine-grained feature discriminative modeling, demonstrating improved performance on novel classes that are similar in shape and semantics to base classes, while maintaining good performance on base classes.

## Limitations
- Reliance on large-scale VLMs like CLIP and ALIGN may limit generalization to truly novel classes with limited visual-language associations
- Knowledge distillation effectiveness depends on compatibility between close-set detector architecture and VLM knowledge
- Caption data as auxiliary supervision introduces potential noise and ambiguity in region-text alignment

## Confidence
- High Confidence: Core claim that open vocabulary learning enables detection/segmentation of novel classes using VLMs is well-supported by benchmark results
- Medium Confidence: Knowledge distillation effectiveness is supported but may vary with implementation details
- Low Confidence: Long-term generalization to truly unseen classes remains uncertain due to evaluation primarily on held-out base classes

## Next Checks
1. **VLM Generalization Test**: Evaluate open vocabulary models on truly novel classes not present in any training data to test the limits of VLM generalization.

2. **Knowledge Distillation Ablation**: Systematically vary the strength and type of knowledge distillation from VLMs to identify optimal transfer conditions and failure modes.

3. **Caption Data Quality Analysis**: Conduct controlled experiments varying the quality and relevance of caption data to quantify the impact of caption grounding noise on performance.