---
ver: rpa2
title: Exploiting Activation Sparsity with Dense to Dynamic-k Mixture-of-Experts Conversion
arxiv_id: '2310.04361'
source_url: https://arxiv.org/abs/2310.04361
tags:
- sparsity
- experts
- activation
- computational
- dsti
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Dynamic Sparsified Transformer Inference (DSTI),
  a method to reduce the inference cost of Transformer models by enforcing activation
  sparsity and converting dense models into sparse Mixture-of-Experts (MoE) versions.
  DSTI trains dense models with an auxiliary loss to enforce activation sparsity,
  constructs MoE layers by splitting dense matrices into experts, and trains small
  gating networks with a novel regression-based objective.
---

# Exploiting Activation Sparsity with Dense to Dynamic-k Mixture-of-Experts Conversion

## Quick Facts
- arXiv ID: 2310.04361
- Source URL: https://arxiv.org/abs/2310.04361
- Reference count: 28
- Key outcome: Reduces inference cost by up to 60% without significant performance loss by enforcing activation sparsity and converting dense models to sparse MoE with Dynamic-k routing

## Executive Summary
This paper introduces Dynamic Sparsified Transformer Inference (DSTI), a method that significantly reduces the inference cost of Transformer models by enforcing activation sparsity and converting dense models into sparse Mixture-of-Experts (MoE) architectures. The approach combines L1 regularization to create sparse activations during fine-tuning, parameter clustering to construct MoE layers, regression-based routing for expert selection, and Dynamic-k routing that adapts the number of executed experts per token based on input difficulty. Evaluated on BERT-base for emotion classification, DSTI achieves up to 60% reduction in inference FLOPs while maintaining competitive accuracy compared to the dense baseline.

## Method Summary
DSTI operates through a multi-stage pipeline: first, it fine-tunes a pre-trained dense model with an auxiliary L1 loss to enforce activation sparsity in the FFN layers; second, it constructs MoE layers by applying balanced k-means clustering to the dense weight matrices, splitting them into 128 experts per FFN; third, it trains small gating networks using a regression-based objective that predicts expert activation sums rather than using classification-based top-k selection; finally, it implements Dynamic-k routing that determines the number of experts to execute per token based on a threshold τ applied to the sorted predicted expert contributions. The method uses standard training procedures with AdamW optimizer (lr=2e-5) for the main model and Adam optimizer (lr=1e-3) for the routers.

## Key Results
- Achieves up to 60% reduction in average inference FLOPs compared to dense BERT-base
- Maintains competitive accuracy on emotion classification task
- Dynamic-k routing provides additional efficiency gains beyond static k routing
- Activation sparsity enforced during fine-tuning persists to improve MoE conversion quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Enforcing activation sparsity via L1 regularization reduces the number of active neurons in FFN layers, enabling more efficient MoE conversion.
- Mechanism: The ℓ1 penalty on hidden activations forces the model to use fewer neurons during fine-tuning, making the subsequent parameter clustering more effective by creating clearer distinctions between active and inactive neurons.
- Core assumption: Sparsity enforced during fine-tuning persists during inference and directly translates to reduced expert count without significant performance loss.
- Evidence anchors:
  - [abstract] "...we propose to train the dense models with an additional loss component that enforces activation sparsity."
  - [section 3] "We propose to apply the ℓ1 norm penalty on the feature representations in the middle layer of each FFN module"
  - [corpus] Weak - no direct mention of ℓ1 sparsity regularization in neighbors
- Break condition: If enforced sparsity degrades task performance beyond acceptable thresholds, or if activation patterns become too irregular for reliable clustering.

### Mechanism 2
- Claim: The regression-based routing objective allows more precise attribution of expert contributions compared to classification-based routing.
- Mechanism: By predicting the sum of activations in each expert (si) using mean squared error, the gating network learns a continuous measure of expert importance rather than a discrete top-k decision, enabling better Dynamic-k selection.
- Core assumption: Accurate prediction of expert activation sums is sufficient for effective routing decisions.
- Evidence anchors:
  - [section 3] "our gating network directly predicts the sum of activations in the hidden layer of each i-th expert si = P j aij. We train the gating network using the standard mean squared error."
  - [corpus] No direct evidence - neighbors focus on MoE architectures but not regression routing objectives.
- Break condition: If regression predictions become inaccurate due to distribution shift between training and inference data.

### Mechanism 3
- Claim: Dynamic-k routing adjusts computational load per token based on input difficulty, improving efficiency.
- Mechanism: The gating network's predicted expert contributions determine the minimum number of experts needed to meet a performance threshold (τ), allowing easier tokens to use fewer experts.
- Core assumption: Input difficulty correlates with the number of experts required for accurate predictions.
- Evidence anchors:
  - [abstract] "...we introduce a mechanism that dynamically determines the number of executed experts individually for each token."
  - [section 3] "we use those predictions to determine k. For each token, we set: k = min{n ∈ {1, ..., E}|nX i=1 sort(h)i > τ }"
  - [corpus] Weak - neighbors discuss MoE but not dynamic-k gating mechanisms.
- Break condition: If threshold tuning becomes unstable or if performance varies too much across different τ values.

## Foundational Learning

- Concept: Activation sparsity in neural networks
  - Why needed here: Understanding that most neurons in deep networks are inactive for any given input is crucial for grasping why MoE conversion is effective.
  - Quick check question: What percentage of neurons are typically active in a standard Transformer FFN layer for a single input?

- Concept: Parameter clustering for MoE construction
  - Why needed here: The method relies on treating neuron weights as features and clustering them to create experts, so understanding this process is essential.
  - Quick check question: How does balanced k-means differ from standard k-means in the context of MoE expert creation?

- Concept: Routing mechanisms in MoE architectures
  - Why needed here: The paper uses both static top-k and dynamic-k routing, so understanding how gating networks select experts is fundamental.
  - Quick check question: What's the difference between classification-based routing and regression-based routing in MoE?

## Architecture Onboarding

- Component map: Dense BERT → L1 regularization fine-tuning → Parameter clustering → Regression routing training → Dynamic-k inference
- Critical path: Fine-tuning with sparsity loss → Expert construction via clustering → Router training with regression objective → Dynamic-k selection during inference
- Design tradeoffs: Static vs. dynamic expert count (performance vs. efficiency), regression vs. classification routing (precision vs. simplicity), L1 strength (sparsity vs. accuracy)
- Failure signatures: Performance degradation with high sparsity, routing instability, inconsistent expert activation patterns
- First 3 experiments:
  1. Apply L1 regularization to BERT-base and measure activation sparsity vs. accuracy trade-off
  2. Implement parameter clustering and verify expert creation matches paper description
  3. Train regression routers and test prediction accuracy against actual expert activations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Dynamic-k routing mechanism affect model accuracy compared to static k for different difficulty levels of input data?
- Basis in paper: [explicit] The paper introduces Dynamic-k routing, which adjusts the number of executed experts on a per-token basis, and shows that it increases efficiency even further.
- Why unresolved: While the paper demonstrates the effectiveness of Dynamic-k routing in reducing computational cost, it does not provide a detailed analysis of how this mechanism affects model accuracy for different difficulty levels of input data.
- What evidence would resolve it: Experiments comparing the accuracy of Dynamic-k routing and static k for different difficulty levels of input data, such as easy, medium, and hard samples, would help understand the impact of Dynamic-k routing on model accuracy.

### Open Question 2
- Question: Can the proposed method be applied to other Transformer-based architectures, such as GPT or ViT, and what would be the expected performance gains?
- Basis in paper: [inferred] The paper mentions that DSTI can be applied to any Transformer-based architecture, but only evaluates it on BERT-base for emotion classification.
- Why unresolved: The paper does not provide evidence of the method's applicability to other Transformer-based architectures or the expected performance gains in those cases.
- What evidence would resolve it: Applying the proposed method to other Transformer-based architectures, such as GPT or ViT, and comparing the performance gains with the baseline models would help determine the generalizability of the method.

### Open Question 3
- Question: How does the proposed method compare to other inference speed-up techniques, such as quantization or early-exits, when combined?
- Basis in paper: [inferred] The paper mentions that the authors believe their method is orthogonal to other inference speed-up methods and would like to explore the interplay between DSTI and those methods.
- Why unresolved: The paper does not provide any experimental results or analysis on the combination of DSTI with other inference speed-up techniques.
- What evidence would resolve it: Experiments comparing the performance of DSTI combined with other inference speed-up techniques, such as quantization or early-exits, would help understand the potential benefits and limitations of such combinations.

## Limitations

- The method's efficiency gains depend on the stability of Dynamic-k routing threshold τ across different input distributions, which isn't thoroughly analyzed
- Regression-based routing objective claims advantages over classification methods but lacks direct empirical comparison
- Parameter clustering approach for MoE conversion doesn't address potential expert imbalance or collapse issues common in MoE systems
- The method is evaluated only on BERT-base for emotion classification, limiting generalizability claims

## Confidence

- High confidence: Core finding that activation sparsity can reduce inference costs - well-established in literature with reasonable experimental evidence
- Medium confidence: Dynamic-k routing mechanism's effectiveness - quantitative improvements shown but lacks detailed analysis of routing stability
- Medium confidence: Regression-based routing approach - claims advantages over classification but lacks direct comparative evidence
- Low confidence: Parameter clustering approach robustness - doesn't address common MoE failure modes or provide extensive validation

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically vary τ values and measure both performance and efficiency trade-offs across different input types to determine the stability and robustness of Dynamic-k routing decisions.

2. **Regression vs. Classification Routing Comparison**: Implement and compare both routing approaches on the same model and dataset to empirically validate whether the regression objective provides measurable benefits over standard classification-based routing.

3. **Expert Balance Monitoring**: Track expert activation distributions and utilization patterns during inference to verify that the parameter clustering approach maintains balanced expert loads and doesn't suffer from common MoE issues like expert collapse or extreme imbalance.