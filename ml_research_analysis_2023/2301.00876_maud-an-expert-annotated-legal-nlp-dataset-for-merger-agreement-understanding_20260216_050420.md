---
ver: rpa2
title: 'MAUD: An Expert-Annotated Legal NLP Dataset for Merger Agreement Understanding'
arxiv_id: '2301.00876'
source_url: https://arxiv.org/abs/2301.00876
tags:
- deal
- point
- dataset
- maud
- legal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MAUD is an expert-annotated legal NLP dataset for reading comprehension
  of merger agreements. It contains 39,000+ examples across 92 reading comprehension
  questions, annotated by law students and reviewed by experienced lawyers.
---

# MAUD: An Expert-Annotated Legal NLP Dataset for Merger Agreement Understanding

## Quick Facts
- arXiv ID: 2301.00876
- Source URL: https://arxiv.org/abs/2301.00876
- Reference count: 29
- Primary result: MAUD is the first expert-annotated reading comprehension dataset for merger agreements, containing 39,000+ examples across 92 questions.

## Executive Summary
MAUD is a pioneering dataset for legal reading comprehension, focusing on merger agreements annotated by law students and reviewed by experienced lawyers. The dataset contains over 39,000 examples and 47,000 total annotations across seven deal point categories. Transformer models show promising results, with LegalBERT achieving the highest overall AUPR score of 45.9%. The dataset fills a critical gap in legal NLP benchmarks and demonstrates significant room for improvement, particularly in complex categories like Conditions to Closing and Material Adverse Effect.

## Method Summary
MAUD extracts deal points from merger agreements in the ABA's 2021 Public Target Deal Points Study and creates standardized questions with multiple-choice answers. Law students annotate these deal points after extensive training (70-100 hours), with 5% reviewed by experienced lawyers. The dataset is processed into question-answer pairs with up to 512-token truncation. Five transformer models (BERT, RoBERTa, LegalBERT, DeBERTa, BigBird) are fine-tuned using AdamW optimizer with class balancing through oversampling. Performance is evaluated using AUPR scores across all questions.

## Key Results
- LegalBERT achieves the highest overall AUPR score of 45.9% across all questions
- Multi-task models outperform single-task models by 0.8% AUPR on average
- RoBERTa models trained on full dataset achieve 7.3% higher AUPR than 50% subset, 23.7% higher than 5% subset
- Largest performance gap: 33.5% AUPR between Deal Protection (26.2%) and Board Composition (59.7%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expert annotations enable legal NLP models to learn specialized domain knowledge from real merger agreements
- Mechanism: Law students and experienced lawyers provide high-quality annotations that capture complex legal language and structure
- Core assumption: Expert annotations accurately reflect legal nuances and domain-specific knowledge
- Evidence anchors: 10,000+ hours of annotation effort, 70-100 hour training for students, ABA deal points study source
- Break condition: If annotations don't accurately capture legal nuances or annotators lack sufficient expertise

### Mechanism 2
- Claim: Pretraining on legal corpora improves performance on MAUD's specialized legal tasks
- Mechanism: LegalBERT, pretrained on legal text, better understands specialized language and concepts in merger agreements
- Core assumption: Legal-specific pretraining provides relevant inductive biases that transfer to MAUD task
- Evidence anchors: LegalBERT outperforms BERT and RoBERTa in both single-task and multi-task settings
- Break condition: If pretraining corpus is not representative of MAUD's legal language or tasks

### Mechanism 3
- Claim: MAUD's large dataset size enables better generalization and performance
- Mechanism: Over 39,000 examples provide sufficient training data for models to learn complex patterns
- Core assumption: More training data leads to better model performance for complex legal reading comprehension
- Evidence anchors: RoBERTa models trained on full dataset achieve 7.3% higher AUPR than 50% subset
- Break condition: If dataset lacks diversity or examples are too similar, increasing size may not improve generalization

## Foundational Learning

- Concept: Reading Comprehension in Legal Text
  - Why needed here: MAUD is a reading comprehension dataset requiring understanding of how models process and answer questions about text
  - Quick check question: How do transformer models typically approach reading comprehension tasks, and what are the key components of their architecture?

- Concept: Pretraining and Fine-tuning
  - Why needed here: MAUD uses pretrained models fine-tuned on the dataset, requiring understanding of both processes
  - Quick check question: What is the difference between pretraining and fine-tuning, and why is pretraining often used in NLP tasks?

- Concept: Metrics for Imbalanced Data
  - Why needed here: MAUD has imbalanced answer distributions, making AUPR crucial for evaluation
  - Quick check question: Why is AUPR used instead of accuracy for evaluating models on MAUD, and how does it handle imbalanced data?

## Architecture Onboarding

- Component map: Data Ingestion -> Preprocessing -> Model Selection -> Training -> Evaluation
- Critical path:
  1. Load and preprocess the data
  2. Choose a model and prepare it for fine-tuning
  3. Fine-tune the model on the training data
  4. Evaluate the model on the development set
  5. Make adjustments and repeat steps 2-4 as needed
  6. Evaluate the final model on the test set

- Design tradeoffs:
  - Single-task vs. Multi-task: Single-task training may lead to better individual question performance, while multi-task training can improve generalization
  - Model size: Larger models like BigBird can handle longer texts but require more computational resources
  - Pretraining: Legal-specific pretraining can improve performance but may not be necessary for all tasks

- Failure signatures:
  - Low AUPR scores: Model not learning complex relationships in legal text
  - Overfitting: Good training performance but poor development/test performance
  - Underfitting: Poor performance on all sets, indicating ineffective learning

- First 3 experiments:
  1. Fine-tune BERT-base on main dataset and evaluate on development set
  2. Compare BERT-base and LegalBERT performance on main dataset
  3. Train multi-task model answering all questions and compare to single-task models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance vary across different deal point types within each category?
- Basis in paper: Same questions asked of all deal points with same type, but no performance data broken down by type
- Why unresolved: Paper focuses on overall category performance without granular analysis
- What evidence would resolve it: Detailed AUPR scores for each deal point type within every category

### Open Question 2
- Question: How does BigBird performance compare when using longer input sequences?
- Basis in paper: BigBird evaluated for handling longer sequences, but no comparative performance data
- Why unresolved: Paper doesn't experiment with varying maximum input sequence length
- What evidence would resolve it: Comparative AUPR scores for BigBird models with different maximum input sequence lengths

### Open Question 3
- Question: What is inter-annotator agreement among law student annotators and compared to experienced lawyers?
- Basis in paper: Law students worked in teams, 5% reviewed by lawyers, but no specific agreement metrics
- Why unresolved: Paper doesn't retain records for calculating agreement metrics
- What evidence would resolve it: Quantitative inter-annotator agreement metrics (e.g., Cohen's kappa) for both student teams and student-lawyer comparisons

## Limitations
- Limited inter-annotator agreement validation beyond 5% lawyer review
- Significant performance variance across categories suggests models may capture superficial patterns
- Low citation overlap with existing legal NLP research limits benchmarking

## Confidence

**High Confidence:** MAUD is the first expert-annotated reading comprehension dataset for merger agreements (well-supported by corpus analysis showing no similar datasets).

**Medium Confidence:** LegalBERT outperforms general-purpose models (supported by results but small margin of improvement). Dataset size ablation shows clear trends but specific gains may vary.

**Low Confidence:** Claim of "strong performance" is questionable given marginal improvements over random baselines in some categories. Pretraining benefits may be overstated given small performance differences.

## Next Checks

1. **Inter-annotator Agreement Analysis:** Calculate Cohen's kappa or similar metrics for all annotators across all deal point categories to quantify annotation consistency and identify systematic biases.

2. **Cross-Domain Generalization Test:** Evaluate models trained on MAUD on different merger agreements or related legal documents not in original dataset to test generalization versus overfitting.

3. **Human Performance Benchmark:** Have experienced M&A lawyers answer MAUD questions without full documents and compare human AUPR scores to best models to determine task difficulty and remaining performance gap.