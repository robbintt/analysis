---
ver: rpa2
title: Can We Trust Race Prediction?
arxiv_id: '2307.08496'
source_url: https://arxiv.org/abs/2307.08496
tags:
- race
- data
- black
- name
- asian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of predicting race and ethnicity
  from limited data (name and location) for applications in lending, criminal justice,
  and healthcare where sensitive demographic information is unavailable. The author
  develops a Bidirectional Long Short-Term Memory (BiLSTM) model trained on comprehensive
  2023 voter registration data from all 50 US states, achieving up to 36.8% higher
  out-of-sample F1 scores than existing machine learning models.
---

# Can We Trust Race Prediction?

## Quick Facts
- arXiv ID: 2307.08496
- Source URL: https://arxiv.org/abs/2307.08496
- Reference count: 9
- Key outcome: BiLSTM model achieves up to 36.8% higher out-of-sample F1 scores than existing models for race prediction from names and locations

## Executive Summary
This paper addresses the challenge of predicting race and ethnicity from limited data (name and location) for applications in lending, criminal justice, and healthcare where sensitive demographic information is unavailable. The author develops a Bidirectional Long Short-Term Memory (BiLSTM) model trained on comprehensive 2023 voter registration data from all 50 US states, achieving up to 36.8% higher out-of-sample F1 scores than existing machine learning models. The study also creates the most comprehensive database of first and surname distributions (51+ million observations) to improve Bayesian Improved Surname Geocoding (BISG) and Bayesian Improved Firstname Surname Geocoding (BIFSG) algorithms. Additionally, it provides the first high-quality benchmark dataset using PPP data with self-reported race, enabling fair comparison of race prediction algorithms.

## Method Summary
The method involves training a character-level BiLSTM model on 2023 voter registration data from all 50 US states, then creating an ensemble with improved BISG and BIFSG algorithms. The BiLSTM uses character-level encoding with a 30-character window, four BiLSTM layers with 512 hidden units each, and embeddings of size 256. The ensemble combines predictions from BiLSTM, BISG, and BIFSG using equal weighting. The model is evaluated on PPP data with self-reported race as a benchmark, measuring F1 score, accuracy, precision, recall, and coverage across four race/ethnicity classes (Asian, Black, Hispanic, White).

## Key Results
- BiLSTM model achieves up to 36.8% higher out-of-sample F1 scores than best existing ML models
- Ensemble approach achieves perfect coverage and significantly outperforms existing models
- Expanded first name database (51M observations) improves BIFSG coverage by 46.6% for Black population

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The BiLSTM model achieves higher F1 scores by learning name character patterns not captured by BISG or BIFSG.
- Mechanism: Character-level embeddings fed into multiple BiLSTM layers capture sequential dependencies in names that correlate with race. The backward layer captures reversed patterns, improving context understanding.
- Core assumption: Race-correlated patterns exist at the character level within names and are learnable by LSTM architectures.
- Evidence anchors:
  - [abstract]: "train a Bidirectional Long Short-Term Memory (BiLSTM) model on a novel dataset of voter registration data from all 50 US states and create an ensemble that achieves up to 36.8% higher out of sample (OOS) F1 scores than the best performing machine learning models"
  - [section 4.1]: "I train an LSTM to predict race from first and last name. LSTMs are a type of recurrent neural network and have shown state-of-the-art performance on many natural language processing tasks"

### Mechanism 2
- Claim: The ensemble approach maximizes coverage while maintaining accuracy by combining complementary prediction sources.
- Mechanism: Weighted averaging of predictions from BiLSTM, BISG, and BIFSG leverages each model's strengths—BiLSTM for coverage, BISG for surname-based accuracy, and BIFSG for first-name enhanced predictions.
- Core assumption: Different models make independent errors that can be reduced through ensembling.
- Evidence anchors:
  - [section 4.2.3]: "I take the weighted average of the predictions made by iBIFSG, iBISG, and First-Last-ZCTA, assigning equal weight to each model that is able to make a prediction"
  - [section 4.2]: "ensemble methods have been shown to be able to compensate for each individual model's weaknesses"

### Mechanism 3
- Claim: Expanding the first name database significantly improves BIFSG coverage and accuracy.
- Mechanism: Larger dataset (51 million observations vs 2.6 million) captures more name variations and reduces the number of names excluded from predictions.
- Core assumption: Coverage improvements directly translate to better overall performance metrics.
- Evidence anchors:
  - [section 4.2.2]: "The existing firm name list comes from Tzioumis [2018], who uses Home Mortgage Disclosure Act (HMDA) data. The list consists of 2,663,364 observations representing 91,526 unique first names. I update the list with the same L2 data... yielding 51,294,450 observations representing 1,187,576 unique first names"
  - [section 4.2.2]: "For Black, iBIFSG achieves a 12.5% increase in F1 Score and a 46.6% increase in coverage compared to BIFSG"

## Foundational Learning

- Concept: Character-level encoding for sequence modeling
  - Why needed here: Names contain race-correlated patterns at the character level that word-level approaches might miss
  - Quick check question: Why does the model use character-level encoding instead of word-level for names?

- Concept: Bidirectional sequence modeling
  - Why needed here: Names can have meaningful patterns in both forward and reverse directions (e.g., suffixes, prefixes)
  - Quick check question: What advantage does the backward layer provide in the BiLSTM architecture?

- Concept: Bayesian inference for demographic estimation
  - Why needed here: BISG and BIFSG use Bayesian methods to combine surname and geographic information for probabilistic race prediction
  - Quick check question: How does BISG calculate the probability of race given surname and geography?

## Architecture Onboarding

- Component map: Name preprocessing -> Character encoding -> BiLSTM layers -> Dense layer -> Ensemble with BISG/BIFSG -> Evaluation
- Critical path: Preprocess names (remove numbers/punctuation, encode characters) → Train BiLSTM on L2 voter data → Generate predictions on test data → Combine with BISG/BIFSG predictions → Evaluate against PPP benchmark
- Design tradeoffs:
  - Character-level vs. word-level encoding: Character-level captures more granular patterns but requires more parameters
  - BiLSTM depth: 4 layers chosen for balance between performance and overfitting risk
  - Equal weighting in ensemble: Simple but may not be optimal; could use learned weights
- Failure signatures:
  - Low coverage: Indicates many names missing from probability tables
  - High false positive rate for certain races: Suggests model bias or imbalanced training data
  - Performance degradation on minority classes: May indicate insufficient representation in training data
- First 3 experiments:
  1. Train character-level CNN instead of BiLSTM to compare performance
  2. Test different ensemble weighting schemes (learned vs. equal weights)
  3. Evaluate model on geographically diverse subsets to check generalization

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions in the traditional sense, but several areas remain unexplored based on the methodology and results presented.

## Limitations
- Voter registration data may not be representative of the general population, potentially introducing sampling bias
- Character-level approach may struggle with non-Latin scripts and international naming conventions
- Equal weighting assumption in ensemble may not be optimal and could benefit from learned weighting schemes

## Confidence
- **High Confidence**: Coverage improvements (up to 46.6% increase for Black population) and comprehensive name database creation are well-supported
- **Medium Confidence**: F1 score improvements (up to 36.8% for Black population) are plausible but depend on benchmark representativeness
- **Low Confidence**: Absolute performance levels and generalizability to populations outside voter registration and PPP datasets remain uncertain

## Next Checks
1. Geographic Generalization Test: Evaluate model performance on geographically diverse subsets (urban vs. rural, different US regions) to assess spatial generalization
2. Temporal Stability Analysis: Test model performance on voter registration data from different years to assess stability and detect potential temporal drift
3. Cross-Platform Validation: Compare predictions against multiple independent race self-report datasets beyond PPP to validate robustness and identify potential dataset-specific biases