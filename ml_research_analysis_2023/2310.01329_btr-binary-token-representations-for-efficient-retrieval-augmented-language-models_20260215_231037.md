---
ver: rpa2
title: 'BTR: Binary Token Representations for Efficient Retrieval Augmented Language
  Models'
arxiv_id: '2310.01329'
source_url: https://arxiv.org/abs/2310.01329
tags:
- representations
- token
- passage
- compression
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BTR (Binary Token Representations) to accelerate
  retrieval-augmented language models by precomputing 1-bit token representations
  for passages, reducing storage and computation during inference. BTR uses calibrated
  binarization to preserve semantic information and adds training objectives to mitigate
  accuracy loss from binarization.
---

# BTR: Binary Token Representations for Efficient Retrieval Augmented Language Models

## Quick Facts
- **arXiv ID:** 2310.01329
- **Source URL:** https://arxiv.org/abs/2310.01329
- **Reference count:** 40
- **Primary result:** Achieves up to 4x speedup and over 100x storage reduction while maintaining over 95% of original model performance

## Executive Summary
This paper introduces BTR (Binary Token Representations) to accelerate retrieval-augmented language models by precomputing 1-bit token representations for passages. BTR addresses the computational bottleneck of encoding retrieved passages during inference by creating cacheable binary representations that enable efficient storage and computation. The approach uses calibrated binarization techniques and training objectives to preserve semantic information while achieving significant efficiency gains.

## Method Summary
BTR precomputes 1-bit binary token representations for passages using calibrated binarization techniques that preserve semantic information. The method inserts binarization after the layernorm layer in the Transformer encoder and uses variance information to recover original scales. Training objectives include query-aware passage token distillation and passage representation recovery to mitigate accuracy loss. Offline compression reduces storage by merging similar token representations across the corpus, while runtime compression further speeds up inference.

## Key Results
- Achieves up to 4x inference speedup on five knowledge-intensive tasks
- Reduces storage requirements by over 100x (from 300GB to 127GB for 3 billion tokens)
- Maintains over 95% of original model performance across all evaluated tasks
- Successfully applied to QA tasks (NaturalQuestions, TriviaQA, WebQuestions) and fact-checking (FEVER) and MMLU benchmarks

## Why This Works (Mechanism)

### Mechanism 1
Calibrated binarization preserves semantic information while enabling efficient storage. BTR applies binarization after layernorm inside the Transformer encoder layer, using variance information to recover original scales. This calibration prevents the collapse seen with naive annealing approaches. Core assumption: Token representations have predictable variance patterns that can be captured and used for calibration.

### Mechanism 2
Offline token compression reduces storage by exploiting corpus-level redundancy. BTR identifies stopwords and semantically similar tokens across the corpus, then merges their binary representations using bipartite merging based on Hamming distance. Core assumption: Tokens that appear frequently in similar contexts have similar semantic representations that can be merged without significant accuracy loss.

### Mechanism 3
Query-aware distillation improves binary representation quality by focusing on salient tokens. BTR distills only the top-r% of passage tokens most relevant to the query from the original model, rather than all tokens. Core assumption: Not all tokens in retrieved passages are equally important for answering a specific query.

## Foundational Learning

- **Concept:** Transformer encoder architecture with pre-layernorm
  - **Why needed here:** BTR's calibrated binarization technique specifically exploits the pre-layernorm structure to insert binarization at the right point
  - **Quick check question:** Where does BTR insert the binarization operation in the Transformer encoder layer?

- **Concept:** Cross-attention computation complexity
  - **Why needed here:** Understanding why passage encoding is the bottleneck (60% of computation) helps explain why precomputing binary representations provides such significant speedups
  - **Quick check question:** What percentage of reader computation is typically spent on passage encoding in retrieval-augmented models?

- **Concept:** Token-level semantic similarity metrics
  - **Why needed here:** BTR uses Hamming distance for offline compression and cosine distance for runtime compression, requiring understanding of when each metric is appropriate
  - **Quick check question:** Why does BTR use Hamming distance for binary vectors but cosine distance for continuous vectors?

## Architecture Onboarding

- **Component map:** Reader → Encoder layers (1-k) → Binary token lookup → Encoder layers (k+1-n) → Decoder layers → Output. Offline compression runs before deployment, runtime compression runs during inference.
- **Critical path:** Query processing → Binary token retrieval → Concatenation with query → Upper layer processing → Decoding. The binary token lookup and reconstruction must be fast to maintain speedup benefits.
- **Design tradeoffs:** Binarization vs accuracy (mitigated by calibration and training objectives), compression ratio vs accuracy (configurable at runtime), storage vs speed (offline compression reduces storage, runtime compression increases speed).
- **Failure signatures:** Accuracy drops indicate poor calibration or excessive compression; slow inference suggests binary lookup/reconstruction overhead; storage doesn't reduce as expected indicates compression algorithm issues.
- **First 3 experiments:**
  1. Implement basic binarization without calibration and measure accuracy degradation
  2. Add calibrated binarization with variance recovery and compare to baseline
  3. Implement offline compression with different ratios and measure storage reduction vs accuracy impact

## Open Questions the Paper Calls Out

### Open Question 1
How does BTR's performance scale with larger models and datasets? The paper mentions that BTR's effectiveness for larger models and bigger representation sizes is an open research topic, and potential solutions might include using autoencoders to compress the dimension of representations.

### Open Question 2
How does BTR perform on extremely long input queries? The paper mentions that improving BTR for extremely long input queries remains challenging and requires other orthogonal efficient methods.

### Open Question 3
How does BTR perform when integrated into model pretraining? The paper mentions that it will be interesting to apply binary token representations to the retriever and incorporate BTR into model pretraining for building better and faster retrieval-augmented language models.

## Limitations

- **Scalability uncertainty:** Effectiveness for larger models and bigger representation sizes is not experimentally validated
- **Long query limitation:** Performance on extremely long input queries remains challenging and requires additional methods
- **Pretraining integration:** Benefits of incorporating BTR into model pretraining have not been experimentally demonstrated

## Confidence

**High Confidence:** The computational efficiency claims (4x speedup, 100x storage reduction) are well-supported by the methodology, as they stem directly from the fundamental reduction in representation size and precomputation strategy.

**Medium Confidence:** The performance retention claims (>95% of original) are supported by the ablation studies and five-task evaluation, but the exact performance degradation patterns across different task types and compression ratios need more detailed analysis.

**Low Confidence:** The scalability claims to 3 billion tokens and the generalizability to other retrieval-augmented architectures are stated but not extensively validated.

## Next Checks

1. **Variance Stability Analysis:** Systematically measure how token representation variance patterns change across different layers, vocabulary subsets, and training stages to validate the calibrated binarization assumptions.

2. **Compression Robustness Testing:** Evaluate the performance degradation patterns across different compression ratios (both offline and runtime) on a wider range of tasks, particularly focusing on tasks requiring fine-grained semantic distinctions.

3. **Cross-Architecture Generalization:** Implement BTR in at least one other retrieval-augmented architecture (e.g., FiD or RAG) to verify the claimed generalizability and identify any architecture-specific limitations.