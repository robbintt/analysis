---
ver: rpa2
title: 'Filling in the Gaps: Efficient Event Coreference Resolution using Graph Autoencoder
  Networks'
arxiv_id: '2310.11965'
source_url: https://arxiv.org/abs/2310.11965
tags:
- coreference
- graph
- event
- arxiv
- vgae
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel and efficient method for Event Coreference
  Resolution (ECR) applied to a lower-resourced language domain. By framing ECR as
  a graph reconstruction task, the authors combine deep semantic embeddings with structural
  coreference chain knowledge to create a parameter-efficient family of Graph Autoencoder
  models (GAE).
---

# Filling in the Gaps: Efficient Event Coreference Resolution using Graph Autoencoder Networks

## Quick Facts
- arXiv ID: 2310.11965
- Source URL: https://arxiv.org/abs/2310.11965
- Reference count: 12
- Primary result: Novel GAE-based ECR method outperforms classical mention-pair methods on Dutch corpus, with better efficiency and robustness in low-data settings

## Executive Summary
This paper introduces a novel approach to Event Coreference Resolution (ECR) using Graph Autoencoder Networks (GAE), specifically targeting lower-resourced language domains. By framing ECR as a graph reconstruction task, the authors combine deep semantic embeddings with structural coreference chain knowledge to create parameter-efficient models. The approach significantly outperforms classical mention-pair methods on a large Dutch event coreference corpus in terms of overall score, efficiency, and training speed, while demonstrating particular robustness in low-data settings.

## Method Summary
The method frames Event Coreference Resolution as a graph reconstruction task where a partially masked adjacency matrix A and node-feature matrix X are used to predict all original edges in the graph. The core approach uses Graph Autoencoder models (GAE and VGAE) with 2-layer Graph Convolutional Networks to encode both node features and graph structure, followed by an inner product decoder. The model is trained to reconstruct the full adjacency matrix from the masked input, learning to leverage both semantic features (when available) and structural information from coreference chains. The approach includes a language-agnostic featureless variant that performs well even without semantic features, particularly in low-data scenarios.

## Key Results
- GAE models significantly outperform classical mention-pair methods on Dutch ENCORE corpus across all metrics
- Featureless GAE models outperform transformer-based mention-pair methods in low-data settings
- GAE approach reduces model size, training time, and inference speed compared to transformer-based methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph Autoencoders leverage structural coreference chain knowledge to improve classification of non-surface-level similar event pairs.
- Mechanism: By representing coreference chains as undirected graphs and using graph convolutional networks to encode both node features and graph structure, the model captures higher-order relationships beyond pairwise similarity.
- Core assumption: Coreference chains contain structural information that is predictive of coreference beyond lexical similarity.
- Evidence anchors:
  - [abstract]: "By framing ECR as a graph reconstruction task, we are able to combine deep semantic embeddings with structural coreference chain knowledge"
  - [section 3.2.2]: "We frame ECR as a graph reconstruction task where a partially masked adjacency matrix A and a node-feature matrix X are used to predict all original edges in the graph"
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.433, average citations=0.0. Top related titles focus on structural approaches to ECR.

### Mechanism 2
- Claim: Featureless GAE models can outperform transformer-based mention-pair methods in low-data settings.
- Mechanism: When training data is limited, the graph structure itself provides a strong signal for coreference that can compensate for the lack of semantic features. The GCN encoder learns to aggregate information from neighboring nodes to make predictions.
- Core assumption: The graph structure contains sufficient information for coreference resolution even without semantic features.
- Evidence anchors:
  - [abstract]: "we introduce a language-agnostic model variant which disregards the use of semantic features entirely and even outperforms transformer-based classification in some situations"
  - [section 4]: "For readability's sake we only include results for the best performing GAE-class models... We find that the average distance between TP pairs increases for our introduced graph models"
  - [corpus]: Weak evidence - corpus analysis shows related work on feature-light approaches but no direct comparison.

### Mechanism 3
- Claim: GAE models are more efficient than transformer-based methods due to reduced model size and faster inference.
- Mechanism: GAE models have a fixed architecture (2-layer GCN encoder + inner product decoder) that is much smaller than transformer models. Inference involves passing the graph through the GCN and computing inner products, which is faster than computing pairwise transformer encodings.
- Core assumption: The computational complexity of the GAE model is lower than that of transformer-based mention-pair models.
- Evidence anchors:
  - [section 3.2.3]: "Due to GPU memory constraints, the Graph encoder models were all trained and evaluated on a single 2.6 GHz 6-Core Intel Core i7 CPU"
  - [section 4]: "Moreover, we find the autoencoder approach significantly reduces model size, training time and inference speed even when compared to parameter-efficient transformer-based methods"
  - [corpus]: Weak evidence - corpus analysis shows related work on efficient models but no direct comparison.

## Foundational Learning

- Concept: Graph Convolutional Networks
  - Why needed here: GAEs use GCNs to encode both node features and graph structure for coreference resolution.
  - Quick check question: How does a GCN aggregate information from neighboring nodes?
- Concept: Variational Autoencoders
  - Why needed here: The paper compares GAEs to VGAEs, so understanding the probabilistic formulation is important.
  - Quick check question: What is the key difference between a GAE and a VGAE?
- Concept: Coreference Resolution
  - Why needed here: The paper applies GAEs to the specific task of event coreference resolution.
  - Quick check question: What is the difference between within-document and cross-document coreference?

## Architecture Onboarding

- Component map: A, X → GCN → Z → ZZ^T → σ(ZZ^T) → Â
- Critical path: A, X → GCN → Z → ZZ^T → σ(ZZ^T) → Â
- Design tradeoffs:
  - Undirected vs directed graphs: The paper uses undirected graphs for simplicity, but some coreference relationships may be directional.
  - Feature-rich vs featureless: The paper shows that feature-rich models perform better but feature-less models can still be effective in low-data settings.
  - GAE vs VGAE: The paper finds that GAEs outperform VGAEs on their dataset, but VGAEs may be better in more uncertain settings.
- Failure signatures:
  - Poor performance on non-surface-level similar event pairs: This could indicate that the model is relying too much on lexical similarity and not enough on structural information.
  - Large gap between training and validation performance: This could indicate overfitting to the training data.
  - Slow inference on large graphs: This could indicate that the GCN encoder is becoming computationally expensive.
- First 3 experiments:
  1. Train a GAE on a small synthetic graph with known coreference patterns to verify that it can learn the structural signal.
  2. Compare the performance of a GAE with and without semantic features on a small dataset to verify that the structural signal is effective.
  3. Measure the inference speed of a GAE on a large graph compared to a transformer-based mention-pair model to verify the efficiency advantage.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance characteristics of graph autoencoder models for event coreference resolution vary when applied to languages with different morphological complexities compared to Dutch?
- Basis in paper: [inferred] The paper demonstrates successful application on Dutch and mentions investigating lower-resourced languages generally, but does not compare across languages with varying morphological complexity.
- Why unresolved: The experiments are conducted exclusively on Dutch, leaving open how the approach would scale to languages with richer morphology or different linguistic features.
- What evidence would resolve it: Comparative experiments on morphologically rich languages (e.g., Finnish, Turkish) versus analytic languages (e.g., English) would clarify the approach's cross-linguistic robustness.

### Open Question 2
- Question: Can the graph autoencoder framework be extended to incorporate temporal information for event coreference resolution, and what impact would this have on performance?
- Basis in paper: [inferred] The paper mentions cross-document settings and that the method outperforms pairwise approaches, but does not explore temporal modeling explicitly despite its relevance to event coreference.
- Why unresolved: While the model captures structural coreference chains, it doesn't explicitly model temporal relationships between events, which are crucial for accurate coreference resolution.
- What evidence would resolve it: Experiments incorporating temporal features into the graph structure or node representations, comparing performance with and without temporal information.

### Open Question 3
- Question: What is the optimal balance between semantic feature representations and structural graph information in the graph autoencoder models for different types of event coreference tasks?
- Basis in paper: [explicit] The paper presents ablation studies comparing models with different feature sets (BERTje, RobBERT, SBERT, NoFeatures) but does not systematically explore the optimal combination for different coreference difficulty levels.
- Why unresolved: The ablation studies show performance differences but don't provide guidance on when to prioritize semantic features versus structural information.
- What evidence would resolve it: Detailed analysis correlating feature importance with specific types of coreference links (e.g., exact matches vs. semantically related events) and task difficulty levels.

## Limitations

- Cross-linguistic generalization uncertainty: Strong results on Dutch corpus but no validation on other languages
- Low-data performance characterization: Featureless variant shows promise but absolute performance levels in extreme low-data scenarios remain unclear
- Real-world efficiency validation: CPU-based timing comparisons may not reflect optimized GPU deployment scenarios

## Confidence

- High confidence: Efficiency advantage and model size reduction claims (directly measured and compared)
- Medium confidence: Coreference performance improvements (strong results on one corpus but limited external validation)
- Medium confidence: Low-data robustness claims (supported by results but without systematic ablation across data regimes)

## Next Checks

1. Replicate the feature-less GAE performance on a different language corpus (e.g., English ECB+) to test cross-linguistic generalization.
2. Conduct systematic ablation studies varying training set sizes (10%, 30%, 50%, 100%) to quantify the low-data advantage.
3. Measure GPU-based inference times for both GAE and transformer models on large graphs to validate the efficiency claims in realistic deployment scenarios.