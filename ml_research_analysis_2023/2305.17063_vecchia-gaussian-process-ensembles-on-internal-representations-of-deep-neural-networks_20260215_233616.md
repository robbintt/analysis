---
ver: rpa2
title: Vecchia Gaussian Process Ensembles on Internal Representations of Deep Neural
  Networks
arxiv_id: '2305.17063'
source_url: https://arxiv.org/abs/2305.17063
tags:
- vecchia
- network
- deep
- each
- intermediate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method for uncertainty quantification
  in deep neural networks by combining Gaussian processes with Vecchia approximations.
  The core idea is to build an ensemble of GPs on the intermediate representations
  of a DNN, using Vecchia approximations for scalability.
---

# Vecchia Gaussian Process Ensembles on Internal Representations of Deep Neural Networks

## Quick Facts
- arXiv ID: 2305.17063
- Source URL: https://arxiv.org/abs/2305.17063
- Reference count: 40
- Primary result: DVE improves uncertainty quantification in DNNs by ensembling Vecchia GPs on intermediate representations, outperforming baseline methods on UCI regression and face age estimation tasks

## Executive Summary
This paper introduces the Deep Vecchia Ensemble (DVE), a method for uncertainty quantification in deep neural networks that combines Gaussian processes with Vecchia approximations on intermediate network representations. The approach leverages the internal representations learned by a DNN to define conditioning sets for scalable GP approximations, allowing pretrained networks to be used without retraining. Experiments demonstrate that DVE provides improved uncertainty estimates compared to existing GP approximations while maintaining computational efficiency.

## Method Summary
DVE constructs an ensemble of Gaussian processes by extracting datasets from hidden layers of a pretrained DNN, fitting Vecchia approximations to each, and combining predictions using a product-of-experts approach. The method extracts intermediate representations from each layer, uses these as new feature spaces for conditioning set selection, and scales to large datasets through the Vecchia approximation. The ensemble combines predictions from multiple GPs while accounting for their respective uncertainties, enabling uncertainty quantification without retraining the underlying DNN.

## Key Results
- DVE achieves lower negative log-likelihood and root mean square error compared to baseline methods on UCI regression tasks
- The method provides better uncertainty estimates while maintaining computational efficiency through Vecchia approximations
- Experiments show that intermediate representations preserve information discarded by final network layers, leading to more accurate predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The DVE improves upon Vecchia GP approximations by leveraging the internal representations learned by a DNN to define better conditioning sets.
- Mechanism: Intermediate representations from DNN layers serve as new distance metrics, improving nearest-neighbor selection for Vecchia approximations.
- Core assumption: Internal representations preserve and transform the data such that Euclidean distance in these spaces reflects the conditional independence structure of the target function.
- Evidence anchors:
  - [abstract] "DVE comprises an ensemble of GPs built on hidden-layer outputs of a DNN, achieving scalability via Vecchia approximations that exploit nearest-neighbor conditional independence."
  - [section 2.4] "The kth intermediate representation of a point x ∈ Rd with respect to the sequence f1, ..., fL+1 is defined to be ek(x) := (fk ◦ fk−1 ◦ ... ◦ f1)(x)."
  - [corpus] Weak - no direct discussion of internal representations improving Vecchia conditioning sets.
- Break condition: If intermediate representations collapse or distort distances such that nearest-neighbor sets no longer reflect conditional independence.

### Mechanism 2
- Claim: Combining multiple Vecchia GPs from different intermediate layers captures information that is lost in the final network layer.
- Mechanism: Different intermediate spaces encode complementary aspects of the input-output relationship, and ensembling preserves this diversity.
- Core assumption: The final layer of a DNN does not necessarily preserve all useful information for prediction; earlier layers retain features that are discarded but still relevant.
- Evidence anchors:
  - [abstract] "Additionally, experiments on internal representations revealed that the ensemble preserves information from earlier layers that is discarded by the final network layer, leading to more accurate predictions."
  - [section 6] "The first internal representation had more separation between yellow and green points compared to the final representation, indicating the earlier layer held information distilled away by the final layer."
  - [corpus] Weak - no explicit discussion of layer-wise information preservation.
- Break condition: If all intermediate representations converge to encode the same information, the ensemble provides no benefit over the final layer alone.

### Mechanism 3
- Claim: The product-of-experts combining strategy effectively weights each Vecchia GP prediction based on the confidence of the conditioning sets.
- Mechanism: Higher weights are given to GPs with smaller posterior variance or higher certainty, balancing ensemble predictions based on local data support.
- Core assumption: Each intermediate space provides a valid probabilistic model, and their combination yields a more robust estimate than any single model alone.
- Evidence anchors:
  - [section 4.4] "We chose this combining strategy to account for prediction uncertainty. Specifically, our approach implicitly considers the degree of agreement between the collections of conditioning sets for xi at each layer."
  - [appendix G] "We chose to combine predictions in y-space using the differential entropy to compute the βk's."
  - [corpus] Weak - no explicit discussion of weighting schemes in the context of Vecchia GPs.
- Break condition: If the conditioning sets across layers disagree systematically, the product-of-experts may overweight uncertain predictions.

## Foundational Learning

- Concept: Gaussian Process Regression
  - Why needed here: DVE builds an ensemble of GPs on intermediate representations, so understanding GP regression is essential.
  - Quick check question: How does a GP provide uncertainty estimates for predictions, and what role does the covariance function play?

- Concept: Vecchia Approximation
  - Why needed here: DVE uses Vecchia approximations to scale GPs to large datasets by conditioning on nearest neighbors.
  - Quick check question: How does the Vecchia approximation reduce the computational complexity of GPs, and what determines the conditioning sets?

- Concept: Intermediate Representations in DNNs
  - Why needed here: DVE extracts datasets from hidden layers of a pretrained DNN to form the input for each GP.
  - Quick check question: How do the outputs of hidden layers differ from the final output, and why might they contain useful information?

## Architecture Onboarding

- Component map:
  - Pretrained DNN -> Intermediate representation extraction -> Vecchia GP fitting -> Ensemble prediction -> Uncertainty quantification

- Critical path:
  1. Extract intermediate representations from each hidden layer.
  2. Compute nearest neighbors in each intermediate space to form conditioning sets.
  3. Fit Vecchia GPs independently on each dataset.
  4. For a test point, map it through the DNN to get intermediate representations.
  5. Find nearest neighbors for each representation and compute GP predictions.
  6. Combine predictions using product-of-experts.

- Design tradeoffs:
  - Using more layers increases diversity but also computational cost.
  - Combining in y-space simplifies noise handling but may lose flexibility for non-Gaussian likelihoods.
  - Independent GP fitting is simple but ignores hierarchical structure.

- Failure signatures:
  - If intermediate representations collapse, conditioning sets become meaningless.
  - If nearest neighbors are poorly chosen, Vecchia approximations degrade.
  - If layers encode redundant information, the ensemble provides little benefit.

- First 3 experiments:
  1. Run DVE on a simple regression dataset with a small DNN; compare RMSE and NLL to baseline GPs and DNN.
  2. Visualize intermediate representations for a 2D dataset; check if earlier layers preserve information lost in the final layer.
  3. Vary the number of layers used in the ensemble; measure impact on prediction accuracy and uncertainty calibration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal method for combining noise terms from individual Vecchia GPs when combining predictions in f-space?
- Basis in paper: [explicit] The authors state "a more principled approach would likely improve the results of combining in f-space" and "combining in f-space can be improved by finding a principled approach to combining the noise terms"
- Why unresolved: The paper uses a simple average of observation noise terms when combining in f-space, but acknowledges this is not ideal and could be improved
- What evidence would resolve it: Experiments comparing different principled methods for combining noise terms (e.g., weighted averaging based on predictive uncertainty, hierarchical modeling of noise) and their impact on predictive performance metrics like NLL and RMSE

### Open Question 2
- How does the Deep Vecchia Ensemble perform on high-dimensional regression tasks where nearest neighbors cannot be computed using Euclidean distance between inputs alone?
- Basis in paper: [explicit] The authors state "We also believe that our method would work well in any application where nearest neighbors cannot be computed using Euclidean distance between inputs alone, such as high-dimensional regression"
- Why unresolved: The paper only evaluates the method on relatively low-dimensional UCI regression tasks (9-20 input dimensions) and face age estimation
- What evidence would resolve it: Experiments applying the Deep Vecchia Ensemble to high-dimensional regression datasets (e.g., 100+ input dimensions) and comparing its performance to baseline methods

### Open Question 3
- How can the Deep Vecchia Ensemble be extended to more general neural network architectures beyond feedforward layers?
- Basis in paper: [explicit] The authors state "The final limitation is that our exploration was limited to feed-forward layers. To extend our model to more general architectures, a distance metric between layer outputs can be defined to make our approach applicable"
- Why unresolved: The paper only demonstrates the method on feedforward neural networks and does not provide concrete methods for extending to other architectures like convolutional or recurrent networks
- What evidence would resolve it: A concrete extension of the Deep Vecchia Ensemble to a specific non-feedforward architecture (e.g., CNNs for image regression) with experiments showing improved performance over baseline methods

## Limitations
- The method assumes intermediate representations preserve conditional independence structure relevant to the prediction task
- Computational complexity grows with the number of layers used, with no detailed scaling analysis provided
- The product-of-experts combining strategy's effectiveness for Vecchia GPs has limited empirical support

## Confidence
- High confidence in the mathematical framework and algorithmic implementation (Sections 2-4)
- Medium confidence in the empirical claims regarding improved uncertainty quantification (Section 6)
- Low confidence in the generalizability claim that DVE "consistently" outperforms baselines across all datasets and architectures

## Next Checks
1. **Ablation study on intermediate representation utility**: Compare DVE performance when using random projections versus actual intermediate representations to quantify the specific contribution of learned features.
2. **Layer-wise sensitivity analysis**: Systematically vary the number of layers included in the ensemble and measure the marginal benefit of each additional layer to understand the point of diminishing returns.
3. **Cross-architecture validation**: Test DVE on architectures beyond feedforward networks (CNNs, transformers) to assess whether the approach generalizes beyond the specific DNN structure used in experiments.