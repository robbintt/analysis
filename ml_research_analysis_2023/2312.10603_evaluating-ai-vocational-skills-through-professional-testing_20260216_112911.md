---
ver: rpa2
title: Evaluating AI Vocational Skills Through Professional Testing
arxiv_id: '2312.10603'
source_url: https://arxiv.org/abs/2312.10603
tags:
- microsoft
- professional
- arxiv
- certification
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces a novel benchmark dataset of 1149 professional\
  \ certification practice exams to evaluate AI vocational skills, focusing on two\
  \ OpenAI models\u2014GPT-3 and Turbo-GPT3.5\u2014using a zero-shot approach. GPT-3\
  \ passed 39% of exams without fine-tuning, excelling in computer-related fields\
  \ like cybersecurity and data analytics, while Turbo-GPT3.5 achieved perfect scores\
  \ on high-stakes exams like OSCP and passed diverse fields such as nursing and finance."
---

# Evaluating AI Vocational Skills Through Professional Testing

## Quick Facts
- arXiv ID: 2312.10603
- Source URL: https://arxiv.org/abs/2312.10603
- Reference count: 40
- Key result: GPT-3 passed 39% of exams without fine-tuning, while Turbo-GPT3.5 achieved perfect scores on high-stakes exams like OSCP

## Executive Summary
This study introduces a novel benchmark dataset of 1149 professional certification practice exams to evaluate AI vocational skills, focusing on two OpenAI models—GPT-3 and Turbo-GPT3.5—using a zero-shot approach. GPT-3 passed 39% of exams without fine-tuning, excelling in computer-related fields like cybersecurity and data analytics, while Turbo-GPT3.5 achieved perfect scores on high-stakes exams like OSCP and passed diverse fields such as nursing and finance. Both models demonstrated surprising proficiency in experience-based tests like wine tasting and emotional intelligence. Human comparisons revealed wide variability in performance, underscoring AI's potential to complement human evaluators. Results suggest AI models are increasingly capable of meeting professional certification standards, with Turbo-GPT3.5's 60% performance improvement over earlier models highlighting rapid advancements in AI vocational readiness.

## Method Summary
The study evaluated AI vocational skills using a zero-shot approach with 1149 professional certification practice exams (6137 multiple-choice questions) across diverse fields from cybersecurity to sommelier. Two OpenAI models (GPT-3 and Turbo-GPT3.5) were tested using a standardized prompt format ("You are an expert in professional certifications in <<BLANK>>. Answer the multiple-choice questions using the format Answer: [insert]") with temperature=0. Performance was measured by pass rates (>70% correct) per exam, and results were compared against human evaluators to assess AI capabilities in vocational contexts.

## Key Results
- GPT-3 passed 39% of professional certification exams without any fine-tuning or exam preparation
- Turbo-GPT3.5 achieved a perfect 100% score on the Offensive Security Certified Professional (OSCP) exam
- Both models scored well on sensory and experience-based tests including wine sommelier, beer tasting, emotional quotient, and body language reading
- Model performance improved by 60% median between versions, from Babbage to Turbo

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot performance on professional exams improves as LLM complexity increases.
- Mechanism: Larger and more recent model versions (Turbo-GPT3.5) encode broader vocational knowledge from training data, enabling higher accuracy on unseen certification questions.
- Core assumption: Training corpus includes sufficient coverage of vocational exam topics and formats.
- Evidence anchors:
  - [abstract] "GPT-3, even without any fine-tuning or exam preparation, managed to achieve a passing score (over 70% correct) on 39% of the professional certifications" vs "Turbo-GPT3.5, on the other hand, scored a perfect 100% on the highly regarded Offensive Security Certified Professional (OSCP) exam."
  - [section] "The progression from OpenAI's Babbage to Turbo model showed a median improvement of 60% in graded performance (100% in Turbo vs. 40% in Babbage)."
  - [corpus] Weak - no direct citation count evidence for vocational domain coverage in training data.
- Break condition: If training data lacks specific vocational content, zero-shot scores plateau or decline regardless of model size.

### Mechanism 2
- Claim: Experience-based sensory tests (wine tasting, emotional intelligence) are passable via textual pattern matching.
- Mechanism: Large language models trained on diverse internet text encounter sufficient descriptions and narratives about sensory experiences to generate plausible answers.
- Core assumption: Training corpus includes abundant sensory and emotional context examples.
- Evidence anchors:
  - [abstract] "Both models also scored well on sensory and experience-based tests outside a machine's traditional roles, including wine sommelier, beer tasting, emotional quotient, and body language reading."
  - [section] "Interestingly, the results show no significant bias in experience-based tests such as sensory or emotional evaluations."
  - [corpus] Weak - no explicit citation or coverage metrics for sensory domain training data.
- Break condition: If sensory experience cannot be represented in text or if test requires genuine perception, model performance drops sharply.

### Mechanism 3
- Claim: Multiple-choice format advantages LLMs due to pattern matching and elimination strategies.
- Mechanism: Structured answer choices reduce ambiguity, enabling models to use statistical likelihoods and token prediction to select correct answers.
- Core assumption: Exam questions follow predictable patterns and training data contains similar multiple-choice examples.
- Evidence anchors:
  - [abstract] "All the tests in the dataset are in the multiple-choice format, a preferred standard in many professional certifications due to its amenability to automated and objective grading methods."
  - [section] "The testing approach for all models was 'zero-shot' learning... the 'few-shot' approach, providing examples, proved unnecessary as all the tested models correctly followed instructions to choose a letter answer (A-D)."
  - [corpus] Weak - no direct evidence of model performance on non-multiple-choice vocational tasks for comparison.
- Break condition: If exams shift to open-ended or practical performance tasks, current model advantages diminish.

## Foundational Learning

- Concept: Zero-shot vs few-shot learning
  - Why needed here: Understanding why no examples were provided clarifies the evaluation's fairness and generalizability.
  - Quick check question: What is the key difference between zero-shot and few-shot prompting in model evaluation?

- Concept: Professional certification structure
  - Why needed here: Knowing how certifications are designed helps interpret why certain domains were more passable than others.
  - Quick check question: Why might multiple-choice professional exams be more accessible to LLMs than open-ended assessments?

- Concept: Model version progression (Babbage → Turbo)
  - Why needed here: Tracking capability growth across versions contextualizes the reported 60% improvement.
  - Quick check question: What architectural or data changes typically distinguish successive LLM versions?

## Architecture Onboarding

- Component map: Dataset ingestion → Prompt formatting → API call → Answer extraction → Scoring logic → Human comparison pipeline
- Critical path: Prompt generation → API submission → Response parsing → Accuracy computation → Benchmark aggregation
- Design tradeoffs: Zero-shot simplicity vs. potential accuracy gains from few-shot; broad dataset coverage vs. depth in specific domains
- Failure signatures: Incorrect answer format → API errors; missing or malformed responses → scoring mismatches; overrepresentation of easy exams → inflated performance metrics
- First 3 experiments:
  1. Validate prompt formatting across all model types and ensure consistent A-D answer extraction
  2. Run a small subset of exams to confirm zero-shot approach yields reasonable accuracy without fine-tuning
  3. Compare model performance on sensory vs. technical exams to isolate domain-specific strengths

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does prompt phrasing affect model performance on professional certification exams?
- Basis in paper: [explicit] The paper mentions that small changes in question wording can significantly impact model performance and that LLMs are sensitive to input phrasing.
- Why unresolved: The study did not systematically test different prompt phrasings to determine optimal approaches for different exam types.
- What evidence would resolve it: Controlled experiments varying prompt structures while keeping content constant across multiple exam types.

### Open Question 2
- Question: What is the prevalence of data leakage in LLM performance on certification exams?
- Basis in paper: [explicit] The paper discusses concerns about models potentially encountering similar questions during training and the risk of inflated performance due to recall rather than reasoning.
- Why unresolved: The study did not investigate whether specific exam questions appeared in training data or implement safeguards against this.
- What evidence would resolve it: Analysis of training data overlap with exam questions and performance comparison between novel and potentially leaked questions.

### Open Question 3
- Question: How can AI-human collaboration improve professional certification assessment?
- Basis in paper: [explicit] The paper found significant variability in human performance and suggests that AI should complement rather than replace human evaluators.
- Why unresolved: The study did not explore hybrid evaluation approaches or determine optimal division of labor between AI and human assessors.
- What evidence would resolve it: Comparative studies of pure AI, pure human, and collaborative evaluation approaches across certification domains.

## Limitations
- Dataset coverage across vocational domains is uneven, with overrepresentation of computer-related certifications potentially inflating technical field performance
- Training data composition for sensory and experience-based tests remains unknown, making it difficult to assess whether model performance reflects genuine capability
- Human comparison methodology lacks detail on evaluator selection criteria, testing conditions, and statistical significance of performance differences

## Confidence
- High confidence: Zero-shot performance improvements between model versions (GPT-3 to Turbo-GPT3.5) based on measurable pass rates
- Medium confidence: Domain-specific performance patterns, particularly in computer-related fields, though limited by dataset composition
- Low confidence: Model capability claims for experience-based tests (wine tasting, emotional intelligence) due to unknown training data coverage and absence of ground truth validation

## Next Checks
1. **Dataset Composition Audit**: Analyze the Professional Certification Benchmark Dataset to quantify representation across vocational domains and identify potential sampling biases that could skew performance results
2. **Sensory Domain Validation**: Conduct blind evaluations with human experts in wine tasting and emotional intelligence to establish ground truth performance baselines and determine whether model outputs reflect genuine understanding or memorized patterns
3. **Cross-Format Comparison**: Test identical vocational knowledge domains using both multiple-choice and open-ended question formats to isolate whether current LLM advantages stem from structured answer formats rather than deeper comprehension