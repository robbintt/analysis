---
ver: rpa2
title: 'FOSI: Hybrid First and Second Order Optimization'
arxiv_id: '2302.08484'
source_url: https://arxiv.org/abs/2302.08484
tags:
- fosi
- optimizer
- learning
- base
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FOSI, a meta-algorithm that enhances the
  performance of first-order optimizers by efficiently incorporating second-order
  information. FOSI works by implicitly splitting the optimization problem into two
  quadratic functions on orthogonal subspaces, using Newton's method for one and the
  base optimizer for the other.
---

# FOSI: Hybrid First and Second Order Optimization

## Quick Facts
- arXiv ID: 2302.08484
- Source URL: https://arxiv.org/abs/2302.08484
- Authors: 
- Reference count: 40
- Primary result: FOSI improves convergence rate and optimization time of first-order methods by incorporating second-order information via Lanczos-based subspace splitting.

## Executive Summary
FOSI is a meta-algorithm that enhances first-order optimizers by efficiently incorporating second-order information. It works by implicitly splitting the optimization problem into two quadratic functions on orthogonal subspaces, using Newton's method for one and the base optimizer for the other. The method employs the Lanczos algorithm to estimate extreme eigenvalues and eigenvectors of the Hessian, enabling effective subspace splitting. FOSI provides theoretical analysis showing it improves the condition number for a large family of optimizers and includes automatic learning rate scaling for closed-form optimal rates.

## Method Summary
FOSI enhances first-order optimizers by combining them with second-order information through a novel subspace splitting approach. The algorithm uses the Lanczos method to estimate extreme eigenvalues and eigenvectors of the Hessian, then projects the optimization problem onto two orthogonal subspaces. On one subspace (spanned by extreme eigenvectors), FOSI applies a scaled Newton step, while on the orthogonal complement it applies the base optimizer. The method includes automatic learning rate scaling based on the improved condition number of the reduced subspace, and recomputes the spectrum periodically to maintain accuracy.

## Key Results
- FOSI achieves the same loss as base optimizers in 68% of wall time on average
- Improves convergence speed by up to 2 orders of magnitude on quadratic functions
- Outperforms second-order methods on deep neural network training tasks

## Why This Works (Mechanism)

### Mechanism 1
FOSI improves the effective condition number of the optimization problem by splitting the Hessian into two orthogonal subspaces and using Newton's method on one subspace while letting the base optimizer handle the other. The Lanczos algorithm estimates the extreme eigenvalues and eigenvectors of the Hessian. FOSI uses the eigenvectors corresponding to the k largest and ℓ smallest eigenvalues to define one subspace, and the remaining eigenvectors define the orthogonal complement. On the first subspace, FOSI applies a scaled Newton step; on the second, it applies the base optimizer. This reduces the condition number of each subspace separately. Core assumption: The eigenvectors from Lanczos provide a meaningful split such that the largest and smallest eigenvalues dominate the ill-conditioning of the Hessian.

### Mechanism 2
FOSI allows the base optimizer to work on a lower-dimensional effective problem, improving its convergence especially for ill-conditioned or diagonally non-dominant Hessians. After splitting, the base optimizer only sees the gradient and curvature information from the orthogonal complement subspace. This is equivalent to optimizing a function with fewer effective dimensions, reducing the impact of the ill-conditioning in the other subspace. Core assumption: The base optimizer's performance improves when the problem dimensionality is reduced and the Hessian is less ill-conditioned in the remaining subspace.

### Mechanism 3
FOSI automatically scales the base optimizer's learning rate based on the improved condition number of the effective Hessian, yielding faster convergence than the base optimizer alone. When the base optimizer has a known closed-form optimal learning rate, FOSI computes the ratio of the optimal learning rates for the full quadratic approximation and for the reduced problem, then scales the base learning rate accordingly. Core assumption: The ratio of optimal learning rates is proportional to the ratio of condition numbers, so scaling improves convergence without manual tuning.

## Foundational Learning

- Concept: Lanczos algorithm for extreme eigenvalue estimation
  - Why needed here: FOSI relies on Lanczos to estimate the largest and smallest eigenvalues/vectors of the Hessian without forming it explicitly, enabling subspace splitting without full second-order computation.
  - Quick check question: Why does Lanczos provide better estimates for extreme eigenvalues than for middle ones?

- Concept: Preconditioning and effective condition number
  - Why needed here: FOSI acts as a preconditioner by splitting the problem; understanding how preconditioners affect condition numbers is essential to grasp why convergence improves.
  - Quick check question: How does the effective condition number relate to the convergence rate of first-order optimizers?

- Concept: Subspace optimization and orthogonal decomposition
  - Why needed here: FOSI's core idea is to optimize two orthogonal subspaces separately; familiarity with orthogonal projections and subspace optimization is needed to understand the update steps.
  - Quick check question: Why is it valid to optimize two orthogonal subspaces independently without affecting each other's solutions?

## Architecture Onboarding

- Component map:
  - Lanczos-based Extreme Spectrum Estimation (ESE) module
  - Subspace split logic
  - Scaled Newton update
  - Base optimizer integration
  - Learning rate scaling module
  - Periodic ESE invocation

- Critical path:
  1. At each iteration, compute gradient
  2. If ESE interval reached, run ESE to get extreme eigenvectors
  3. Project gradient onto two subspaces
  4. Compute Newton step on one subspace
  5. Compute base optimizer step on other subspace
  6. Combine steps and update parameters

- Design tradeoffs:
  - Number of Lanczos iterations vs. accuracy vs. overhead
  - Frequency of ESE vs. stale curvature info
  - Choice of k, ℓ: larger values improve split but increase memory/computation
  - Clipping parameter c in learning rate scaling

- Failure signatures:
  - Divergence or oscillation: likely due to inaccurate Lanczos estimates or aggressive learning rate scaling
  - No improvement over base optimizer: may indicate well-conditioned Hessian or ineffective split
  - High overhead with marginal gain: suggests ESE frequency is too high for problem size

- First 3 experiments:
  1. Run FOSI with GD on a simple ill-conditioned quadratic function. Measure convergence vs. GD baseline.
  2. Run FOSI with Adam on a small CNN training task (e.g., MNIST). Compare wall time and validation accuracy vs. Adam alone.
  3. Vary the Lanczos iteration count and observe trade-off between convergence speed and runtime overhead.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- FOSI's effectiveness depends on the Hessian being ill-conditioned; for well-conditioned problems, the ESE overhead may outweigh benefits.
- The clipping parameter c in learning rate scaling is problem-dependent without a principled selection method.
- FOSI assumes extreme eigenvalues dominate ill-conditioning, which may not hold for all problem structures.

## Confidence
- High: FOSI improves convergence on ill-conditioned problems by splitting into subspaces and using Newton's method on one subspace.
- Medium: FOSI consistently outperforms base optimizers across diverse tasks with 68% average wall time reduction.
- Medium: The learning rate scaling based on improved condition number yields faster convergence for optimizers with closed-form optimal rates.

## Next Checks
1. Test FOSI on a synthetic quadratic function with known Hessian eigenvalues, varying the spectral gap to assess Lanczos accuracy and FOSI's sensitivity.
2. Run FOSI with different base optimizers (e.g., SGD, AdamW) on a standard CNN benchmark to validate robustness to optimizer choice.
3. Analyze the trade-off between ESE frequency and convergence speed on a large-scale DNN task to determine optimal frequency for various problem sizes.