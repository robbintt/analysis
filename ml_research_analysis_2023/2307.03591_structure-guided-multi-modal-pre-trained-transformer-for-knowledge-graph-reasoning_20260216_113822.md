---
ver: rpa2
title: Structure Guided Multi-modal Pre-trained Transformer for Knowledge Graph Reasoning
arxiv_id: '2307.03591'
source_url: https://arxiv.org/abs/2307.03591
tags:
- multimodal
- graph
- information
- knowledge
- structure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of multimodal knowledge graph\
  \ reasoning (MKGR) by proposing a structure-guided multimodal pretrained transformer\
  \ model (SGMPT) that incorporates graph structural information into existing MPT\
  \ architectures. The key innovation is a structure-guided fusion module that leverages\
  \ two strategies\u2014weighted summation and alignment constraint\u2014to integrate\
  \ structural embeddings from a graph encoder (e.g., HAKE) with textual and visual\
  \ features."
---

# Structure Guided Multi-modal Pre-trained Transformer for Knowledge Graph Reasoning

## Quick Facts
- **arXiv ID**: 2307.03591
- **Source URL**: https://arxiv.org/abs/2307.03591
- **Reference count**: 40
- **Primary result**: SGMPT improves Hits@1 by 3.7% and 2.2% over transformer baselines on FB15k-237-IMG and WN18-IMG respectively

## Executive Summary
This paper addresses multimodal knowledge graph reasoning (MKGR) by proposing SGMPT, which integrates graph structural information into pretrained transformer architectures through a structure-guided fusion module. The key innovation is a plug-and-play mechanism that injects structural embeddings (from encoders like HAKE) into multimodal features using weighted summation and alignment constraint strategies. Extensive experiments demonstrate state-of-the-art performance, with significant improvements in reasoning accuracy while maintaining comparable model complexity to existing approaches.

## Method Summary
SGMPT extends multimodal pretrained transformers by incorporating graph structural information through a structure-guided fusion module. The architecture combines BERT (text), ViT (vision), and a KGE-based structure encoder to generate entity representations. The fusion module offers two strategies: weighted summation that directly adds structural embeddings to multimodal features, and alignment constraint that uses MSE loss to align multimodal and structural representations. The model is pretrained and fine-tuned on multimodal knowledge graphs, with experiments conducted on FB15k-237-IMG and WN18-IMG datasets.

## Key Results
- SGMPT achieves 3.7% and 2.2% improvements in Hits@1 over transformer baselines on FB15k-237-IMG and WN18-IMG respectively
- Both weighted summation and alignment constraint strategies are effective, with the alignment constraint providing slightly better performance
- Different structure encoders (HAKE, COMPGCN, Nodepiece) can all improve performance compared to no structure information
- SGMPT maintains comparable model complexity while delivering superior reasoning performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The structure-guided fusion module improves MKGR by integrating graph structural information into multimodal features
- Mechanism: Structural embeddings from KGE models are injected into textual and visual features through weighted addition or alignment loss, leveraging relational and topological information that previous MPT models ignored
- Core assumption: Structural information encoded by KGE models contains complementary features for reasoning tasks
- Evidence anchors: Abstract describes fusion module with weighted summation and alignment constraint strategies
- Break condition: If structural encoder fails to capture meaningful information or fusion strategies don't integrate effectively

### Mechanism 2
- Claim: Weighted summation directly combines structural embeddings with multimodal features in native feature space
- Mechanism: Structural embedding vectors are expanded and added to corresponding feature representations with learnable weights
- Core assumption: Direct addition preserves complementary information and enhances reasoning without complex transformation
- Evidence anchors: Section describes extracting token representation vectors and generating fused features via Eq. (4)
- Break condition: If weighted addition introduces noise or structural embeddings are incompatible with multimodal feature spaces

### Mechanism 3
- Claim: Alignment constraint uses MSE alignment loss to refine multimodal features based on structural information
- Mechanism: Alignment loss measures discrepancy between normalized textual/visual embeddings and structural embeddings, pulling them closer during training
- Core assumption: Structural embeddings provide reference points that guide learning of better multimodal representations
- Evidence anchors: Section describes text-structure and vision-structure alignment losses using MSE
- Break condition: If alignment loss doesn't improve representations or causes optimization difficulties

## Foundational Learning

- **Concept**: Multimodal knowledge graph structure and incompleteness
  - Why needed here: Understanding that MKGs contain entities with text, images, and graph structures that are often incomplete requiring reasoning
  - Quick check question: What are the three types of information in multimodal knowledge graphs, and why is reasoning needed?

- **Concept**: Pretrained transformer architectures for multimodal data
  - Why needed here: Model builds on MPT architectures (BERT for text, ViT for vision) extended with structural information
  - Quick check question: How do BERT and ViT differ in input processing, and why are they suitable for text and vision?

- **Concept**: Knowledge graph embedding (KGE) methods
  - Why needed here: Structural encoder uses KGE models (like HAKE) to generate structural embeddings capturing relational and topological information
  - Quick check question: What is the primary goal of KGE methods, and how do they typically represent entities and relations?

## Architecture Onboarding

- **Component map**: Input (Multimodal KG) → Structure Encoder (HAKE) → Structure-Guided Fusion Module → Text Encoder (BERT) + Vision Encoder (ViT) + Multimodal Encoder → Output (Enhanced representations)
- **Critical path**: Structure Encoder → Structure-Guided Fusion Module → Multimodal Encoder → Reasoning Output
- **Design tradeoffs**: Complexity vs. performance (adding structural fusion increases complexity but improves reasoning); strategy selection (weighted summation simpler vs. alignment constraint potentially better); encoder choice (different KGE models offer different representations)
- **Failure signatures**: No performance improvement (fusion strategies ineffective); degraded performance (structural information introduces noise); training instability (alignment losses cause optimization difficulties); high computational cost (structural encoding/fusion too expensive)
- **First 3 experiments**: 1) Ablation study comparing SGMPT against MKGformer baseline on FB15k-237-IMG; 2) Strategy comparison between weighted summation, alignment constraint, and combined approaches; 3) Encoder comparison replacing HAKE with COMPGCN and Nodepiece

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does choice of structure encoder affect SGMPT performance across different multimodal knowledge graph datasets?
- Basis in paper: Paper evaluates performance with other structure encoders (HousE, COMPGCN) showing improvements but doesn't provide comprehensive comparison
- Why unresolved: Paper demonstrates different encoders improve performance but lacks systematic comparison across diverse datasets
- What evidence would resolve it: Systematic experiments comparing multiple structure encoders across varied multimodal knowledge graph datasets

### Open Question 2
- Question: What is the impact of weighted summation versus alignment constraint strategies when used individually versus in combination?
- Basis in paper: Paper states both strategies are equally effective and can be used together but doesn't quantify individual contributions
- Why unresolved: Paper shows both strategies work but doesn't explore optimal configuration or diminishing returns
- What evidence would resolve it: Detailed ablation studies isolating each strategy's performance across multiple datasets

### Open Question 3
- Question: How sensitive is SGMPT's performance to hyperparameter tuning across different multimodal knowledge graph datasets?
- Basis in paper: Paper shows performance is relatively stable across hyperparameter ranges but only tests limited values on two datasets
- Why unresolved: Limited hyperparameter sweeps leave uncertainty about dataset-specific optimal ranges
- What evidence would resolve it: Extensive hyperparameter sweeps across diverse multimodal knowledge graph datasets

## Limitations

- The paper's claims about structural information being "omitted" by previous MPT models are based on narrow comparison with MKGformer without systematic exploration of which structural features contribute most
- Alignment constraint strategy's effectiveness is empirically demonstrated but lacks theoretical justification for why MSE alignment creates meaningful unified representations
- Generalization of results across different KGE encoders is uncertain, with only brief mention of alternative encoders without comparative analysis

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Structural information improves performance | Medium |
| Alignment constraint strategy is effective | Medium |
| Results generalize across KGE encoders | Low |

## Next Checks

1. **Ablation Analysis of Structural Components**: Systematically remove different aspects of structural information to identify which components contribute most to performance gains

2. **Cross-Encoder Performance Comparison**: Implement and compare SGMPT with multiple structure encoders (HAKE, COMPGCN, TransE, RotatE) on same benchmark to quantify sensitivity to KGE method choice

3. **Alternative Fusion Strategy Benchmarking**: Replace weighted summation and alignment constraint with simpler fusion mechanisms (concatenation with projection, attention-based fusion) to determine if improvements require proposed strategies