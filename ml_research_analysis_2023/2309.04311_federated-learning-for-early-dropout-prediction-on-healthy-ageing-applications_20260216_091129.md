---
ver: rpa2
title: Federated Learning for Early Dropout Prediction on Healthy Ageing Applications
arxiv_id: '2309.04311'
source_url: https://arxiv.org/abs/2309.04311
tags:
- learning
- data
- federated
- sampling
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study applies federated machine learning (FML) to predict
  early user dropouts from a healthy ageing mobile application. Traditional centralized
  approaches require sharing sensitive health data, raising privacy concerns.
---

# Federated Learning for Early Dropout Prediction on Healthy Ageing Applications

## Quick Facts
- arXiv ID: 2309.04311
- Source URL: https://arxiv.org/abs/2309.04311
- Reference count: 30
- Key outcome: FML with data selection and resampling techniques achieves predictive accuracy comparable to or better than centralized learning for early dropout prediction in healthy aging apps

## Executive Summary
This study applies federated machine learning to predict early user dropouts from a healthy aging mobile application, addressing privacy concerns associated with centralized data sharing. The research evaluates various data selection and resampling strategies within federated learning to handle challenges including non-independent and identically distributed data, class imbalance, and label ambiguity. Experimental results demonstrate that federated learning with appropriate preprocessing techniques achieves predictive accuracy comparable to or better than centralized learning approaches, with under-sampling improving recall by 138% and F1 score by 51% compared to raw data.

## Method Summary
The study employs a fully connected neural network with three hidden layers (128, 64, 32 units) and ReLU activations to predict user dropouts from the MAHA dataset containing elderly participants' activity data. The federated learning setup uses the FLOWER framework with cross-device and cross-silo configurations, where clients perform local training on their private data and upload model updates to a central server. Data preprocessing includes user exclusion based on information content and class imbalance handling through over-sampling, under-sampling, and combined over-under sampling techniques. The model is trained with 20 federated rounds, 5 local epochs per client, and evaluated against centralized learning baselines using accuracy, precision, recall, F1 score, and geometric mean metrics.

## Key Results
- Federated learning with data selection and resampling achieves predictive accuracy comparable to centralized learning
- Under-sampling in cross-device federated learning improves recall by 138% and F1 score by 51% compared to raw data
- Cross-device federated learning preserves individual user patterns better than cross-silo, showing superior performance for this dataset

## Why This Works (Mechanism)

### Mechanism 1
Federated learning enables privacy-preserving dropout prediction by training models on decentralized user data without direct data sharing. In FL, each user trains a local model using their private data and only sends model updates (e.g., gradients or weights) to a central server, which aggregates these updates to form a global model. The core assumption is that local data distributions across clients are sufficiently diverse but share enough commonality to allow effective aggregation. Break condition: If local data distributions are too heterogeneous (non-iid), aggregation may fail or produce biased models.

### Mechanism 2
Data selection and class imbalance handling techniques improve federated model performance by mitigating issues specific to distributed, imbalanced datasets. Techniques such as excluding low-information users and resampling (over/under-sampling) balance class distribution and remove noisy or sparse data, which is especially important when each client holds a small, potentially imbalanced subset of overall data. The core assumption is that removing low-quality or sparse data and balancing classes will lead to better generalization across the federated network. Break condition: If resampling artificially inflates minority class samples beyond realistic representation, the model may overfit to synthetic patterns.

### Mechanism 3
Cross-device federated learning (where each user is a client) outperforms cross-silo federated learning (where groups of users are clients) for this dataset because it preserves finer-grained user patterns. In cross-device FL, each individual's unique usage patterns are preserved in their local model updates, allowing the global model to learn subtle distinctions between users that might be lost when aggregating multiple users into a single client in cross-silo FL. The core assumption is that individual-level heterogeneity is greater than group-level heterogeneity, so preserving individual updates is beneficial. Break condition: If group-level patterns are more stable or if individual data is too sparse, cross-silo may outperform cross-device.

## Foundational Learning

- **Concept: Non-independent and identically distributed (non-iid) data**
  - Why needed here: The dataset contains user-specific patterns and label ambiguity, violating the iid assumption of traditional ML.
  - Quick check question: If user A has mostly class 0 samples and user B has mostly class 1 samples, what problem does this create for federated averaging?

- **Concept: Class imbalance and resampling techniques**
  - Why needed here: The dataset has a 75% class imbalance (low adherence vs high adherence), which can bias the model toward the majority class.
  - Quick check question: If we apply SMOTE to oversample the minority class, what must we be careful about regarding synthetic data quality?

- **Concept: Label ambiguity**
  - Why needed here: Identical feature vectors can correspond to both low and high adherence labels, complicating model learning and evaluation.
  - Quick check question: How might label ambiguity affect the interpretation of precision and recall metrics in this context?

## Architecture Onboarding

- **Component map**: Central Server -> Client Devices -> Data Preprocessing Module -> Neural Network Model
- **Critical path**: 1) CS initializes model parameters 2) CS samples clients 3) Selected clients download current model and perform local training 4) Clients upload updated model parameters to CS 5) CS aggregates parameters (FedAvg) to update global model 6) Repeat until convergence or max rounds
- **Design tradeoffs**: Cross-device vs cross-silo (preserves individual patterns vs aggregates for stability); Data selection thresholds (higher reduces noise but may discard useful data); Resampling strategy (oversampling may introduce synthetic noise; undersampling may lose information)
- **Failure signatures**: Model accuracy stalls or drops (heterogeneous client data, poor sampling, ineffective resampling); High variance in client updates (significant non-iid data or inconsistent local training); Label ambiguity not addressed (unstable predictions for ambiguous feature vectors)
- **First 3 experiments**: 1) Run centralized learning on raw data to establish baseline accuracy and identify class imbalance severity 2) Apply under-sampling in cross-device FL to test if balancing classes improves recall and F1 score 3) Exclude users with fewer than 5 samples per class in cross-silo FL to assess impact of low-information content on model performance

## Open Questions the Paper Calls Out

### Open Question 1
How can we determine the optimal threshold for excluding low-informative users in federated learning to maximize predictive performance? The paper only experimented with thresholds of 5 and 10, which showed varying results. The optimal threshold likely depends on the specific dataset and federated learning configuration. Systematic experimentation with different threshold values across multiple datasets and federated learning scenarios would identify patterns and guidelines for threshold selection.

### Open Question 2
Which data resampling method is most effective for handling class imbalance in federated learning across different healthcare applications? The paper only tested a limited set of resampling techniques (over-sampling, under-sampling, over-under sampling) and found mixed results. The effectiveness likely depends on the specific characteristics of the data and the federated learning setup. Comprehensive comparative studies testing various resampling methods across multiple healthcare datasets and federated learning configurations would resolve this question.

### Open Question 3
Can federated learning models outperform centralized learning in healthcare applications when intelligent client selection mechanisms are employed? While the paper shows federated learning can match centralized performance with proper preprocessing, it hasn't fully explored the potential of advanced client selection mechanisms to surpass centralized models. Experimental studies comparing centralized learning with federated learning using various client selection algorithms across multiple healthcare datasets would determine if federated learning can consistently outperform centralized approaches.

## Limitations
- The severity and distribution of non-iid data and label ambiguity across clients is not fully characterized
- No statistical significance testing is reported for performance improvements, making it difficult to distinguish meaningful gains from random variation
- Communication and computational overhead of federated learning versus centralized approaches is not quantified

## Confidence

- **High confidence**: Federated learning can preserve privacy while maintaining predictive accuracy for dropout prediction
- **Medium confidence**: Data selection and resampling techniques improve federated model performance, though specific effectiveness depends on unknown dataset characteristics
- **Medium confidence**: Cross-device federated learning preserves individual user patterns better than cross-silo, but the superiority is not conclusively demonstrated

## Next Checks

1. **Statistical validation**: Perform significance testing (e.g., paired t-tests or bootstrap confidence intervals) on accuracy, recall, and F1 score differences between centralized and federated approaches to establish whether observed improvements are meaningful.

2. **Dataset characterization**: Quantify the degree of non-iid data distribution across clients and the prevalence of label ambiguity cases to better understand the challenges faced by federated learning in this context.

3. **Communication overhead analysis**: Measure and report the communication costs (data transferred per round, total rounds to convergence) for federated learning compared to the computational cost of centralized training to assess practical feasibility.