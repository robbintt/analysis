---
ver: rpa2
title: 'Robustly Improving Bandit Algorithms with Confounded and Selection Biased
  Offline Data: A Causal Approach'
arxiv_id: '2312.12731'
source_url: https://arxiv.org/abs/2312.12731
tags:
- causal
- algorithm
- bound
- regret
- bandit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of leveraging biased offline data
  to improve online bandit algorithms. The authors propose a causal inference approach
  to derive bounds for conditional causal effects under confounding and selection
  biases.
---

# Robustly Improving Bandit Algorithms with Confounded and Selection Biased Offline Data: A Causal Approach

## Quick Facts
- **arXiv ID**: 2312.12731
- **Source URL**: https://arxiv.org/abs/2312.12731
- **Reference count**: 40
- **Key outcome**: Derives causal bounds from biased offline data to guide online bandit algorithms, achieving lower regret than standard methods under mild conditions.

## Executive Summary
This paper addresses the challenge of leveraging biased offline data to improve online bandit algorithms. The authors propose a causal inference approach to derive bounds for conditional causal effects under confounding and selection biases. They develop two methods - c-component factorization and substitute interventions - to bound the target causal effect. The derived bounds are used to guide arm selection in contextual (LinUCB-PCB) and non-contextual (UCB-PCB) bandit algorithms, reducing exploration of sub-optimal arms. Theoretical analysis shows these algorithms achieve lower regret than their non-causal counterparts under mild conditions.

## Method Summary
The paper proposes a framework that combines causal inference with online bandit learning. First, it uses an offline evaluation phase where biased data is processed through the Bounding Conditional Causal Effect (BCE) algorithm to derive causal bounds under confounding and selection biases. These bounds are then incorporated into online bandit algorithms (LinUCB-PCB for contextual bandits and UCB-PCB for non-contextual bandits) to guide arm selection and reduce regret. The key insight is that properly derived causal bounds from biased data can provide more reliable guidance than direct use of biased estimates.

## Key Results
- LinUCB-PCB algorithm achieves lower cumulative regret than standard LinUCB when causal bounds are available
- UCB-PCB algorithm outperforms standard UCB in non-contextual settings using derived causal bounds
- The BCE algorithm successfully derives bounds containing true mean rewards under confounding and selection biases
- Theoretical regret bounds for both algorithms are established under mild conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Causal bounds derived from biased offline data can reduce sub-optimal arm exploration in online bandit learning.
- Mechanism: By incorporating upper and lower causal bounds (Ua,c, La,c) into the UCB calculation, the algorithm avoids pulling arms whose upper bounds are below the optimal mean reward.
- Core assumption: The causal bounds contain the true mean reward, i.e., La,c ≤ E[Y |do(X=xa), c] ≤ Ua,c.
- Evidence anchors:
  - [abstract]: "The derived bounds contain the ground truth mean reward and can effectively guide the bandit agent to learn a nearly-optimal decision policy."
  - [section]: "Theorem 6. If there exists an arm a such that Ua,ct < ⟨θ, xa∗,ct ⟩ at a round t ∈ [T ], LinUCB-PCB is guaranteed to achieve lower cumulative regret than LinUCB algorithm."
  - [corpus]: Weak evidence - no direct citation of bandit algorithms using causal bounds from biased data.
- Break condition: If the causal bounds do not contain the true mean reward, the algorithm may exclude optimal arms or include sub-optimal ones.

### Mechanism 2
- Claim: C-component factorization and substitute intervention methods can derive valid causal bounds even under confounding and selection biases.
- Mechanism: The BCE algorithm decomposes the target intervention into c-factors and uses the RC* algorithm to bound each factor, or finds recoverable substitute interventions to bound the target.
- Core assumption: The causal graph structure is known and invariant, and the biases can be characterized as confounding and selection biases.
- Evidence anchors:
  - [section]: "We derive causal bounds for conditional causal effects under confounding and selection biases based on c-component factorization and substitute intervention methods."
  - [section]: "Theorem 3 (Causal Bound from RC* algorithm). Given a conditional intervention Px(y|c), the causal bounds derived by calling RC* algorithm for each c-factor are..."
  - [corpus]: Weak evidence - no direct citation of these specific bounding methods.
- Break condition: If the causal graph is unknown, incorrect, or changes over time, the bounding methods may fail.

### Mechanism 3
- Claim: Incorporating prior causal bounds into online bandit algorithms consistently reduces cumulative regret.
- Mechanism: The LinUCB-PCB and UCB-PCB algorithms use the prior causal bounds to truncate the UCB calculation, effectively reducing the exploration of sub-optimal arms.
- Core assumption: The online bandit algorithms are based on the optimism in the face of uncertainty (OFU) principle and the linear or sub-Gaussian reward assumptions.
- Evidence anchors:
  - [abstract]: "Empirical evaluation on synthetic data demonstrates that LinUCB-PCB achieves the lowest regret compared to baselines that use biased or naive estimates from offline data."
  - [section]: "Theorem 5. Let ||x||2 define the L-2 norm of a context vector x ∈ Rd and L = maxa,c∈{A,C},Ua,c≥⟨θ,xa∗,c⟩ ||xa,c||2. The expected regret of LinUCB-PCB algorithm is bounded by..."
  - [corpus]: Weak evidence - no direct citation of these specific bandit algorithms with causal bounds.
- Break condition: If the reward assumptions are violated or the OFU principle is not applicable, the regret bounds may not hold.

## Foundational Learning

- **Concept: Structural Causal Models (SCM)**
  - Why needed here: The paper formulates the problem from a causal perspective and uses SCM to derive causal bounds.
  - Quick check question: What are the three components of a Structural Causal Model (SCM) according to Definition 1?

- **Concept: Confounding and Selection Biases**
  - Why needed here: The paper categorizes biases into confounding and selection biases and develops methods to bound causal effects under these biases.
  - Quick check question: How does the paper distinguish between confounding bias and selection bias based on the causal structure they imply?

- **Concept: C-component Factorization and Substitute Intervention**
  - Why needed here: These are the two methods proposed to derive causal bounds under confounding and selection biases.
  - Quick check question: What is the difference between the c-component factorization method and the substitute intervention method for deriving causal bounds?

## Architecture Onboarding

- **Component map**:
  - BCE algorithm -> Causal bounds for each arm
  - LinUCB-PCB algorithm -> Contextual bandit with causal bounds
  - UCB-PCB algorithm -> Non-contextual bandit with causal bounds

- **Critical path**:
  1. Offline evaluation phase: Run the BCE algorithm on the biased offline data to obtain causal bounds for each arm.
  2. Online bandit learning phase: Use the LinUCB-PCB or UCB-PCB algorithm with the prior causal bounds as input to guide arm selection and reduce regret.

- **Design tradeoffs**:
  - Using causal bounds from biased data vs. using biased estimates directly: The paper argues that using causal bounds can reduce the negative impact of biases on online learning.
  - Tighter vs. looser causal bounds: Tighter bounds may lead to better performance but are harder to derive, while looser bounds are easier to obtain but may have less impact on regret reduction.

- **Failure signatures**:
  - High regret compared to baseline algorithms: This may indicate that the causal bounds are not effective in reducing sub-optimal arm exploration.
  - Causal bounds that do not contain the true mean reward: This may indicate a problem with the BCE algorithm or the underlying assumptions about the causal graph and biases.

- **First 3 experiments**:
  1. Run the BCE algorithm on a synthetic dataset with known causal structure and biases, and compare the derived causal bounds to the true mean rewards.
  2. Implement the LinUCB-PCB algorithm and compare its performance to the baseline LinUCB algorithm on a synthetic contextual bandit problem.
  3. Implement the UCB-PCB algorithm and compare its performance to the baseline UCB algorithm on a synthetic non-contextual bandit problem.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can we obtain tighter bounds for c-factors when C = ∅ in the RC* algorithm?
- **Basis in paper**: [explicit] The paper mentions that in line 9 of the RC* algorithm, they bound the target c-component by [0, 1] since under semi-Markovian models it is challenging to find a tight bound for Q[E] when C = ∅. They suggest applying a non-parametric bounding technique similar to (Wu, Zhang, and Wu 2019) as a future direction.
- **Why unresolved**: The paper acknowledges the challenge of finding tight bounds in this scenario but does not provide a concrete solution or demonstrate the effectiveness of the suggested approach.
- **What evidence would resolve it**: Developing and evaluating a non-parametric bounding technique for c-factors when C = ∅, and comparing the tightness of the resulting bounds with the current [0, 1] bounds.

### Open Question 2
- **Question**: How does the accuracy of the causal bounds affect the regret improvement in LinUCB-PCB and other proposed algorithms?
- **Basis in paper**: [explicit] The paper discusses that the value of N^-pcb depends on the accuracy of the causal upper bound for each arm, and a larger N^-pcb value implies less uncertainty regarding the sub-optimal arms, leading to a more significant improvement in regret. However, the paper does not provide a detailed analysis of how the accuracy of the bounds directly impacts the regret improvement.
- **Why unresolved**: The relationship between the accuracy of causal bounds and regret improvement is mentioned, but the paper lacks a quantitative analysis or experiments that explicitly measure this relationship.
- **What evidence would resolve it**: Conducting experiments that vary the accuracy of the causal bounds and measuring the corresponding regret improvement in LinUCB-PCB and other proposed algorithms, or providing a theoretical analysis that quantifies the impact of bound accuracy on regret.

### Open Question 3
- **Question**: How do the proposed algorithms perform in non-linear reward function scenarios?
- **Basis in paper**: [explicit] The paper focuses on the linear reward function assumption and mentions extending the approach to contextual bandits under non-linearity assumption (Zhou, Li, and Gu 2020) as a future direction. However, the performance of the proposed algorithms in non-linear scenarios is not evaluated.
- **Why unresolved**: The paper does not provide any experimental results or theoretical analysis for non-linear reward function scenarios, leaving the effectiveness of the proposed algorithms in such scenarios unclear.
- **What evidence would resolve it**: Implementing and evaluating the proposed algorithms (LinUCB-PCB, OAM-PCB, UCB-PCB) in non-linear reward function scenarios, and comparing their performance with existing non-linear bandit algorithms.

### Open Question 4
- **Question**: How does the proposed framework handle the presence of multiple selection biases or complex selection mechanisms?
- **Basis in paper**: [inferred] The paper discusses the existence of selection bias and introduces a selection node S in the causal graph to represent the data selection mechanism. However, it does not explicitly address the scenario of multiple selection biases or complex selection mechanisms that may be present in real-world data.
- **Why unresolved**: The paper focuses on a single selection bias scenario and does not provide insights into how the framework can be extended or adapted to handle more complex selection mechanisms.
- **What evidence would resolve it**: Developing and evaluating extensions of the proposed framework that can handle multiple selection biases or complex selection mechanisms, and demonstrating their effectiveness in real-world data with such biases.

## Limitations

- **Limited scalability**: The computational complexity of the BCE algorithm may increase significantly with the size and complexity of the causal graph.
- **Assumption of known causal structure**: The proposed methods require a known and invariant causal graph, which may not be available in many real-world scenarios.
- **Evaluation on synthetic data**: The empirical evaluation is conducted on synthetic data with controlled causal structures, which may not capture the complexity and noise present in real-world datasets.

## Confidence

- **High confidence**: The mechanism of using causal bounds to reduce sub-optimal arm exploration is well-grounded in causal inference theory and bandit algorithm principles. The regret bounds for LinUCB-PCB and UCB-PCB follow standard analysis techniques.
- **Medium confidence**: The effectiveness of the BCE algorithm in deriving tight causal bounds under confounding and selection biases depends on the specific characteristics of the causal graph and data distribution, which may vary in practice.
- **Low confidence**: The scalability and performance of the proposed methods on real-world datasets with unknown or partially known causal structures remain to be demonstrated.

## Next Checks

1. **Graph Misspecification Analysis**: Evaluate the performance of LinUCB-PCB and UCB-PCB when the assumed causal graph differs from the true data-generating process. Measure the impact on regret and causal bound quality.

2. **Real-World Dataset Evaluation**: Apply the proposed methods to a real-world dataset with known causal structure (e.g., medical treatment recommendation) and compare performance to baseline algorithms using biased estimates or no offline data.

3. **Causal Bound Tightness Study**: Systematically vary the amount of available offline data and the strength of confounding/selection biases to assess the relationship between data quality, bound tightness, and online learning performance.