---
ver: rpa2
title: Model-free Posterior Sampling via Learning Rate Randomization
arxiv_id: '2310.18186'
source_url: https://arxiv.org/abs/2310.18186
tags:
- algorithm
- where
- learning
- randql
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RandQL, a model-free posterior sampling algorithm
  for reinforcement learning in episodic Markov Decision Processes. RandQL achieves
  exploration without bonuses by using a novel learning rate randomization technique,
  maintaining an ensemble of Q-values updated with Beta-distributed step-sizes.
---

# Model-free Posterior Sampling via Learning Rate Randomization

## Quick Facts
- arXiv ID: 2310.18186
- Source URL: https://arxiv.org/abs/2310.18186
- Reference count: 40
- Key outcome: RandQL achieves O(sqrt(H^5 SAT)) regret in tabular MDPs and O(H^(5/2) T^((d_z+1)/(d_z+2))) in metric spaces

## Executive Summary
This paper introduces RandQL, a model-free posterior sampling algorithm for reinforcement learning in episodic Markov Decision Processes. RandQL achieves exploration without using bonus terms by injecting noise into Q-value updates via Beta-distributed learning rates, maintaining an ensemble of Q-values. The method connects model-free Q-learning with posterior sampling techniques, providing a tractable alternative to model-based approaches. RandQL demonstrates competitive performance against existing model-free approaches on benchmark exploration environments while maintaining low time and space complexity.

## Method Summary
RandQL is a model-free posterior sampling algorithm that maintains an ensemble of J Q-values for each state-action pair. Instead of using bonus terms for exploration, RandQL employs Beta-distributed learning rates to update the ensemble. The policy Q-value is defined as the maximum across the ensemble, ensuring optimism. The algorithm operates in episodic MDPs with finite horizon H, using Beta(H, n) distributions where n is the visit count. For metric state-action spaces, Net-Staged-RandQL combines fixed discretization with staged updates and proper prior tuning to control discretization error.

## Key Results
- Achieves regret bound of O(sqrt(H^5 SAT)) in tabular MDPs
- Extends to metric state-action spaces with regret bound of O(H^(5/2) T^((d_z+1)/(d_z+2)))
- Outperforms existing model-free approaches on benchmark exploration environments
- Maintains low time and space complexity compared to model-based methods

## Why This Works (Mechanism)

### Mechanism 1
RandQL achieves optimistic exploration without using bonuses by injecting noise into Q-value updates via Beta-distributed learning rates. At each step, RandQL maintains an ensemble of J Q-values, each updated with an independent learning rate sampled from Beta(H, n). The policy Q-value is defined as the maximum across the ensemble, ensuring optimism. The Beta distribution's shape mimics posterior sampling noise, aligning with PSRL's optimism mechanism.

### Mechanism 2
The staged version (Staged-RandQL) resets temporary Q-values at the end of each stage to avoid forgetting prior information too quickly. Stages are defined with exponentially increasing lengths. Within a stage, Q-value updates are computed using only recent trajectories, and at the end of the stage, Q-values are reset to a prior-based initialization. This prevents the aggressive learning rate from exponentially decaying the influence of early (prior) data.

### Mechanism 3
For metric spaces, Net-Staged-RandQL combines fixed discretization with staged updates and proper prior tuning to control discretization error. A fixed ε-cover discretizes the continuous state-action space into balls. Within each ball, the algorithm runs a staged Q-learning update. The prior count n0(k) is tuned to depend on stage k and ε to compensate for approximation error due to discretization. Lipschitz assumptions ensure optimism propagates correctly across neighboring balls.

## Foundational Learning

- **Episodic Markov Decision Processes**: Why needed here: The algorithm's regret analysis and update rules depend on the finite horizon H and episodic structure. Quick check: In an episodic MDP with horizon H, how many steps are there in one episode, and why is this important for Q-learning updates?

- **Posterior Sampling (Thompson Sampling)**: Why needed here: RandQL's exploration mechanism is inspired by PSRL's optimism from posterior sampling; understanding the Dirichlet posterior update is key to seeing why Beta learning rates work. Quick check: In PSRL for tabular MDPs, what distribution is used to sample the next-state transition probabilities, and how does that relate to the Dirichlet distribution?

- **Learning Rate Scheduling and Bias-Variance Tradeoff**: Why needed here: RandQL uses Beta-distributed learning rates instead of fixed step sizes; understanding how the Beta shape controls bias and variance is essential for tuning. Quick check: For a Beta(H, n) distribution, what is its expected value, and how does it change as n grows?

## Architecture Onboarding

- **Component map**: State-action pair -> Ensemble of Q-values -> Beta-distributed learning rate generator -> Policy selector (max over ensemble) -> Action selection
- **Critical path**: Observe state-action pair (s, a) -> Sample learning rate wj ~ Beta(...) -> Update ensemble Q-values using target -> Update policy Q-value as max over ensemble -> Select action greedily
- **Design tradeoffs**: J vs. regret (larger J reduces non-optimistic selection probability but increases computation), prior strength n0 vs. adaptation speed (larger n0 delays adaptation but stabilizes early exploration), stage length vs. forgetting (longer stages reduce reset overhead but may overfit to stale data)
- **Failure signatures**: Vanishing exploration (ensemble max consistently selects same Q-value across episodes), unstable learning (high variance in Q-value updates due to overly aggressive learning rates), discretization collapse (ε too large → optimism fails; ε too small → computational explosion)
- **First 3 experiments**: 1) Grid-world with 100 states, 4 actions, H=50: Verify RandQL beats OptQL, matches PSRL 2) Chain MDP (15 states, 2 actions, H=30): Test scalability and exploration in sparse-reward regime 3) Continuous ball environment (2D, H=30): Compare Adaptive-RandQL vs. Adaptive-QL and DQN

## Open Questions the Paper Calls Out

- Can RandQL achieve the optimal regret bound of O(sqrt(H^3 SAT)) in tabular MDPs by incorporating variance reduction techniques? The authors conjecture that RandQL could achieve optimal regret if combined with variance reduction techniques, but obtaining such improvements is not straightforward due to intricate statistical dependencies.

- How does the performance of RandQL compare to model-based Bayesian algorithms like PSRL in high-dimensional or complex continuous state-action spaces? The paper shows RandQL outperforms OptQL and is competitive with model-based baselines in tabular and simple continuous environments, but the authors suggest that model-based algorithms are more sample efficient in low-dimensional settings.

- What is the impact of the ensemble size J and prior parameters n0 and r0 on the exploration-exploitation trade-off in RandQL, and how should they be tuned for different environments? The paper uses specific values for these parameters in experiments but does not provide a systematic study of their impact on performance.

## Limitations
- Beta learning rate randomization requires careful tuning of parameters (H, n, κ) that may be environment-dependent
- Proof of optimism relies heavily on Beta distribution properties and assumes sufficiently large ensemble size J
- Metric space extension depends critically on Lipschitz continuity assumptions that may not hold in all continuous control problems

## Confidence
- Regret bounds (O(sqrt(H^5 SAT))): **High** - Mathematically rigorous proof with clear derivation
- Beta learning rate exploration mechanism: **Medium** - Theoretical justification present but empirical validation limited to specific environments
- Metric space extension: **Medium-Low** - Requires strong assumptions about Lipschitz continuity and discretization quality

## Next Checks
1. **Parameter sensitivity analysis**: Systematically vary H, n, κ and ensemble size J to identify robust operating ranges across different MDP structures
2. **Scaling experiments**: Test RandQL on larger state spaces (>1000 states) and longer horizons (H>100) to verify theoretical scaling predictions
3. **Distribution comparison**: Replace Beta with other distributions (Gamma, Uniform) while maintaining same mean/variance to isolate the role of distribution shape in exploration performance