---
ver: rpa2
title: Speech language models lack important brain-relevant semantics
arxiv_id: '2311.04664'
source_url: https://arxiv.org/abs/2311.04664
tags:
- language
- features
- brain
- speech
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the types of information captured by language
  models that predict brain activity during reading and listening. The authors use
  a direct residual approach to remove specific low-level textual, speech, and visual
  features from both text- and speech-based language model representations, and observe
  the effect on brain alignment with fMRI recordings from the same naturalistic stories
  presented in two modalities.
---

# Speech language models lack important brain-relevant semantics

## Quick Facts
- **arXiv ID:** 2311.04664
- **Source URL:** https://arxiv.org/abs/2311.04664
- **Reference count:** 40
- **Primary result:** Speech-based language models lack important brain-relevant semantics for higher-level language processing compared to text-based models

## Executive Summary
This study investigates how language models capture brain activity during naturalistic reading and listening tasks. Using fMRI data from 6 participants exposed to the same stories in both modalities, the authors systematically remove low-level textual, speech, and visual features from language model representations to understand their contribution to brain alignment. They find that while both text- and speech-based models predict early sensory regions largely due to correlated low-level features, text-based models capture more brain-relevant semantics in late language regions. This suggests contemporary speech models lack important semantic information for higher-level language processing.

## Method Summary
The study uses voxel-wise encoding models to predict fMRI recordings from language model representations. Researchers extracted representations from three text-based models (BERT, GPT2, FLAN-T5) and two speech-based models (Wav2Vec2.0, Whisper) for the same naturalistic stories presented in reading and listening conditions. They then removed specific low-level stimulus features (textual, speech, visual) using ridge regression and evaluated how this affected brain alignment. Brain activity was measured across different regions of interest including early sensory areas, VWFA, and late language regions, with performance normalized against noise ceilings.

## Key Results
- Both text- and speech-based models predict early sensory regions due to correlated low-level features (e.g., number of letters and phonemes)
- Text-based models outperform speech-based models in VWFA, suggesting better capture of visual word form processing
- In late language regions, text-based models retain much of their predictive power after feature removal, while speech-based models lose almost all alignment, indicating lack of brain-relevant semantics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Text-based language models predict speech-evoked brain activity in early auditory regions largely due to correlated low-level stimulus features.
- **Mechanism:** Low-level textual features (e.g., number of letters) and speech features (e.g., number of phonemes) are correlated across reading and listening modalities. When text-based models are evaluated on listening data, they retain alignment in early auditory regions because these correlated features are present in their representations.
- **Core assumption:** The same low-level features can be reliably extracted and aligned between text and speech modalities, and these features are sufficient to explain the brain alignment in early sensory regions.
- **Evidence anchors:**
  - [abstract] "We find that both text- and speech-based language models predict brain responses well both during reading and listening in corresponding early sensory regions in large parts due to low-level stimulus features that are correlated between text and speech (e.g. number of letters and phonemes)."
  - [section] "Removal of low-level textual features results in a similar performance drop for both types of models... (2) Removal of low-level speech features results in a larger performance drop for speech-based compared to text-based models (more than 40% of the original performance)."
  - [corpus] Weak evidence - corpus mentions "Brain-tuned Speech Models Better Reflect Speech Processing Stages in the Brain" but not directly about low-level feature correlations.
- **Break condition:** If the correlation between textual and speech features breaks down for different stimuli or languages, or if the brain alignment in early auditory regions cannot be explained by low-level features alone.

### Mechanism 2
- **Claim:** Text-based models capture brain-relevant semantics in late language regions, while speech-based models do not.
- **Mechanism:** Text-based models retain significant predictive power in late language regions even after removing low-level stimulus features, indicating they encode higher-level semantic information relevant to brain processing. Speech-based models lose almost all alignment in late language regions once low-level features are removed, suggesting they lack this semantic information.
- **Core assumption:** Late language regions process semantic information, and the ability of models to predict brain activity in these regions reflects their capture of such semantics.
- **Evidence anchors:**
  - [abstract] "In contrast, in later language regions, we find that much of the ability of text-based language models to predict brain responses is retained during both listening and reading even after removing low-level stimulus features, suggesting that text-based language models capture important brain-relevant semantics."
  - [section] "In contrast, the residual performance of speech-based models goes down to around 10-15%. This implies that the alignment of speech-based models with late language regions is almost entirely due to low-level stimulus features, and not brain-relevant semantics."
  - [corpus] Weak evidence - corpus includes "Brain-tuned Speech Models Better Reflect Speech Processing Stages in the Brain" which implies speech models can capture brain-relevant information, but does not directly address semantics in late language regions.
- **Break condition:** If the assumption that late language regions process semantic information is incorrect, or if the residual analysis fails to fully remove all low-level information.

### Mechanism 3
- **Claim:** The VWFA is specialized for processing visual word forms, and text-based models capture this information better than speech-based models.
- **Mechanism:** The VWFA is associated with decoding written forms of words. Text-based models, trained on written text, capture brain-relevant information related to visual word form processing, leading to better alignment in VWFA compared to speech-based models.
- **Core assumption:** The VWFA's computational role in decoding written forms is well-established and distinct from general visual processing.
- **Evidence anchors:**
  - [abstract] "During reading, both text- and speech-based models predict early visual regions similarly well. However, text-based models outperform speech-based models in VWFA. This implies that text-based models capture brain-relevant information related to processing of visual word forms."
  - [section] "Text-based models outperform speech-based models in VWFA, as this region is mainly associated with processing visual word forms and speech-based models are not equipped to handle this type of information."
  - [corpus] Weak evidence - corpus does not mention VWFA or visual word form processing.
- **Break condition:** If the role of VWFA is not specific to visual word forms, or if speech-based models can be adapted to capture this information.

## Foundational Learning

- **Concept:** fMRI encoding models and noise ceiling estimation
  - Why needed here: The study uses voxel-wise encoding models to predict fMRI recordings from model representations, and noise ceiling estimation to contextualize model performance. Understanding these concepts is crucial for interpreting the results.
  - Quick check question: How does a noise ceiling help in evaluating the performance of brain alignment models?

- **Concept:** Low-level stimulus features (textual, speech, visual)
  - Why needed here: The study systematically removes information related to specific low-level stimulus features to understand their contribution to brain alignment. Knowledge of these features and how they are extracted is essential for understanding the methodology.
  - Quick check question: What are some examples of low-level textual, speech, and visual features used in this study?

- **Concept:** Residual analysis in neural representations
  - Why needed here: The study uses a residual approach to remove the linear contribution of low-level features from language model representations. Understanding this technique is important for grasping how the authors isolate the effects of different features.
  - Quick check question: How does the residual analysis method work to remove specific information from neural representations?

## Architecture Onboarding

- **Component map:** fMRI datasets (reading and listening conditions) -> Language models (BERT, GPT2, FLAN-T5, Wav2Vec2.0, Whisper) -> Low-level stimulus feature extraction -> Residual analysis module (removes feature contributions) -> Voxel-wise encoding model (predicts brain activity) -> Brain region of interest (ROI) definitions

- **Critical path:**
  1. Extract model representations for each TR
  2. Remove low-level feature contributions via residual analysis
  3. Train voxel-wise encoding model on (residual) representations
  4. Evaluate brain alignment (correlation with fMRI)
  5. Compare alignment across conditions and ROIs

- **Design tradeoffs:**
  - Linear vs. non-linear feature removal: The study uses linear regression to remove feature contributions, which is efficient but may not capture all feature-related information.
  - Choice of language models: The study uses popular models but they differ in architecture, training data, and objectives, which may confound results.
  - fMRI noise ceiling: Estimating the noise ceiling provides context but relies on assumptions about participant consistency.

- **Failure signatures:**
  - If residual analysis fails to remove feature contributions, the study may overestimate the role of low-level features.
  - If brain alignment is not significantly different between conditions, the study's conclusions about modality-specific processing may be weakened.
  - If encoding models fail to predict brain activity, the study's approach to understanding brain-language model alignment may be flawed.

- **First 3 experiments:**
  1. Replicate the main analysis with a different dataset or language to test the generalizability of the findings.
  2. Perform ablation studies on the choice of language models (e.g., using models with similar architectures but different training objectives) to isolate the effects of model type.
  3. Apply non-linear feature removal techniques (e.g., using neural networks instead of linear regression) to see if the results change, which would inform the choice of residual analysis method.

## Open Questions the Paper Calls Out

- **Question:** How would removing higher-level semantic features (discourse-level, emotion-related) from language model representations affect their brain alignment?
- **Question:** How do language model brain alignments differ across languages other than English?
- **Question:** To what extent do differences in model architecture, training data, and objective functions contribute to the observed differences in brain alignment between text- and speech-based models?

## Limitations
- Small participant sample (6 subjects) limits generalizability across populations
- Feature removal approach assumes linear relationships between model representations and low-level features
- Cannot definitively prove that retained alignment after feature removal represents "brain-relevant semantics" versus other higher-level information not captured by the removed features

## Confidence
- **High confidence:** VWFA specialization claim, early sensory region alignment findings
- **Medium confidence:** Central claim about speech models lacking brain-relevant semantics
- **Low confidence:** Generalization to languages beyond English, absolute distinction between "brain-relevant semantics" and other higher-level information

## Next Checks
1. Apply non-linear feature removal techniques (e.g., neural network-based methods) to test whether the semantic differences persist beyond linear correlations
2. Test the same models on a different language dataset to verify that findings generalize beyond the specific stimuli used
3. Conduct ablation studies using text-based models trained on speech data (or vice versa) to isolate whether architecture or training data drives the observed differences