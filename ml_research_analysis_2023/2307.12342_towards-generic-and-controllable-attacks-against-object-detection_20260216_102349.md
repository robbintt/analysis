---
ver: rpa2
title: Towards Generic and Controllable Attacks Against Object Detection
arxiv_id: '2307.12342'
source_url: https://arxiv.org/abs/2307.12342
tags:
- attack
- perturbations
- object
- attacks
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents LGP, a generic and controllable white-box
  attack framework against object detection. The authors address two key limitations
  of existing attacks: their focus on specific detector structures and their generation
  of redundant perturbations in semantically meaningless areas.'
---

# Towards Generic and Controllable Attacks Against Object Detection

## Quick Facts
- arXiv ID: 2307.12342
- Source URL: https://arxiv.org/abs/2307.12342
- Reference count: 40
- Primary result: Generic white-box attack framework achieving strong imperceptibility and transferability across 16 SOTA object detectors

## Executive Summary
This paper introduces LGP, a white-box adversarial attack framework designed to overcome limitations of existing attacks on object detection systems. LGP addresses the problem of attack specificity by decoupling from particular detector structures and focusing on pre-NMS outputs, while also solving the controllability issue through object-wise constraints that attach perturbations to foreground objects. The framework jointly optimizes three heterogeneous losses (shape, localization, and classification) guided by a unified Hiding Attack objective, achieving superior performance across multiple datasets and detector architectures.

## Method Summary
LGP operates through three core components working in concert: an Assigner that tracks high-quality proposals using IoU and confidence scores, an Attacker that optimizes shape, localization, and semantic losses simultaneously toward a unified Hiding Attack objective, and a Limiter that applies object-wise constraints using adaptive foreground-background separation. The method generates adversarial examples by jointly optimizing attack loss and imperceptibility loss using the Adamax optimizer with learning rate 0.1 for 50 iterations, maintaining stable target relationships across iterations while controlling perturbation magnitude, position, and distribution.

## Key Results
- Successfully blinds 16 state-of-the-art object detectors on MS-COCO and DOTA datasets
- Achieves promising imperceptibility metrics (PSNR-B, IW-SSIM, FID) while maintaining strong attack performance
- Demonstrates superior transferability compared to baseline attacks
- Object-wise constraints outperform global perturbation methods in both attack strength and visual quality

## Why This Works (Mechanism)

### Mechanism 1: Generic Attack Through Pre-NMS Targeting
The Assigner selects high-quality proposals by tracking IoU and confidence scores against ground truth, creating stable target sets independent of detector architecture. By attacking pre-NMS outputs rather than detector-specific components, LGP achieves architecture-agnostic vulnerability exploitation.

### Mechanism 2: Unified Multi-Task Attack Objective
The Hiding Attack objective unifies shape, localization, and semantic losses into a single optimization goal, resolving gradient conflicts that plague standard multi-task attacks. This coordination ensures all loss components work synergistically toward making predictions indistinguishable from background.

### Mechanism 3: Object-Wise Adaptive Constraints
The Limiter uses foreground-background separation with adaptive heatmaps to weight perturbations, ensuring they attach to objects while minimizing background influence. This iterative constraint mechanism provides superior controllability compared to traditional ℓp norm clipping.

## Foundational Learning

- **Object detection pipeline architecture (RPN, ROI pooling, multi-task heads, NMS)**: Essential for understanding why attacking pre-NMS outputs provides generic attack surface and how proposals flow through the detection system.
  - Quick check: What is the difference between pre-NMS and post-NMS outputs, and why would attacking pre-NMS be more generic?

- **Multi-task learning and loss alignment in object detection**: Critical for grasping why standard multi-task attacks suffer from gradient conflicts and how unified objectives resolve these issues.
  - Quick check: Why do standard multi-task attacks in object detection suffer from "conflicts among heterogeneous losses"?

- **Adversarial perturbation generation and imperceptibility metrics**: Necessary for understanding how object-wise constraints differ from traditional perturbation control methods and their impact on attack effectiveness.
  - Quick check: How does the method's object-wise constraint differ from traditional ℓp norm clipping in terms of controllability?

## Architecture Onboarding

- **Component map**: Assigner -> Attacker -> Limiter (with tracking mechanism maintaining stable target relationships)
- **Critical path**: Clean image → Pre-NMS output selection → Target assignment → Multi-task attack optimization → Adaptive perturbation limiting → Adversarial example generation
- **Design tradeoffs**: Generality vs. computational efficiency (selecting from thousands of proposals), attack strength vs. imperceptibility (object-wise vs. image-level constraints), stability vs. flexibility (fixed vs. adaptive target tracking)
- **Failure signatures**: Attack strength plateaus despite iterations, perturbations leak into background regions, target tracking becomes unstable, or multi-task losses show gradient conflicts
- **First 3 experiments**:
  1. Test Assigner on a simple detector with known ground truth to verify proposal selection quality
  2. Validate Attacker's multi-task optimization by measuring individual loss contributions and their alignment
  3. Evaluate Limiter's object-wise constraint by comparing perturbation distributions with and without foreground-background separation

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of backbone architecture (ResNet vs ResNeXt vs other variants) affect the transferability of adversarial examples generated by LGP? The paper claims backbone effects are negligible but only tested on Faster R-CNN, requiring experiments across multiple architectures to verify.

### Open Question 2
What is the optimal number of high-quality proposals to assign per ground truth object to balance attack strength and computational efficiency? The paper uses fixed assignment values (five based on IoU, five based on scores) without systematic optimization or theoretical justification.

### Open Question 3
How do the three components of the object-wise constraint (magnitude, position, and distribution) individually contribute to the overall imperceptibility of LGP-generated perturbations? The paper presents a combined constraint but does not isolate individual component effects through ablation studies.

## Limitations
- The method's claimed generality through pre-NMS targeting requires verification across detectors with fundamentally different proposal generation mechanisms
- Object-wise constraint performance on datasets with ambiguous object boundaries or heavy occlusion is unclear
- Adaptive heatmap implementation details are sparse, making reproducibility challenging
- Hiding Attack objective may not generalize well to detectors with different architectural priors

## Confidence

- **Generic Attack Capability**: Medium - theoretically sound but unverified across diverse detector architectures
- **Multi-Task Loss Coordination**: Medium - effective in experiments but robustness to different architectures unproven
- **Object-Wise Constraint Effectiveness**: Low-Medium - concept is sound but implementation details and complex scene performance are unclear

## Next Checks

1. **Architecture Transfer Test**: Apply LGP to three detector architectures with fundamentally different proposal generation mechanisms (anchor-based, anchor-free, transformer-based) and measure performance degradation compared to original architecture.

2. **Constraint Robustness Evaluation**: Test the object-wise constraint on datasets with increasing object overlap complexity (COCO, CrowdHuman, synthetic dataset with controlled occlusion) to identify failure thresholds.

3. **Adaptive Mechanism Stability**: Implement the adaptive heatmap update mechanism and measure its convergence properties across different image types, identifying conditions where constraint fails to provide meaningful perturbation control.