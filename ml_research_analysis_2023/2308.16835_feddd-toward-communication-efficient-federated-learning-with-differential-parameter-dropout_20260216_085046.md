---
ver: rpa2
title: 'FedDD: Toward Communication-efficient Federated Learning with Differential
  Parameter Dropout'
arxiv_id: '2308.16835'
source_url: https://arxiv.org/abs/2308.16835
tags:
- uni00000013
- uni00000011
- uni00000048
- uni00000003
- uni00000046
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the communication efficiency problem in federated
  learning (FL) caused by heterogeneous client conditions. The authors propose FedDD,
  a novel FL scheme with differential parameter dropout, which assigns different dropout
  rates to clients based on their heterogeneous conditions and selects important parameters
  for uploading.
---

# FedDD: Toward Communication-efficient Federated Learning with Differential Parameter Dropout

## Quick Facts
- arXiv ID: 2308.16835
- Source URL: https://arxiv.org/abs/2308.16835
- Authors: Multiple
- Reference count: 40
- Key outcome: Proposed FedDD framework achieves superior communication efficiency and model convergence by assigning differential dropout rates to clients based on their heterogeneous conditions and selecting important parameters for uploading.

## Executive Summary
This paper addresses the communication bottleneck in federated learning caused by heterogeneous client conditions. The authors propose FedDD, a novel framework that optimizes model parameter uploading ratios tailored to different clients' heterogeneous conditions. By formulating dropout rate allocation as a convex optimization problem and implementing an importance-based parameter selection strategy, FedDD significantly reduces communication costs while maintaining or improving model accuracy compared to existing methods.

## Method Summary
FedDD introduces two key modules: dropout rate allocation and uploaded parameter selection. The dropout rate allocation is formulated as a convex optimization problem that considers system heterogeneity (computing power, network speed), data heterogeneity (label distribution, local loss), and model heterogeneity (model size, loss) among clients. The uploaded parameter selection uses an importance index based on the ratio of parameter change to parameter value, scaled by coverage rate for heterogeneous models. The framework also includes periodic full model broadcasting to mitigate drift from parameter sparsification.

## Key Results
- FedDD achieves better communication efficiency and model convergence than FedAvg, FedCS, and Oort baselines
- The framework demonstrates strong generalization capability to data of rare classes
- Experiments show consistent improvements across MNIST, FMNIST, and CIFAR10 datasets under both IID and Non-IID conditions

## Why This Works (Mechanism)

### Mechanism 1
Differential dropout rates improve communication efficiency by reducing bandwidth used by slow clients while preserving accuracy. Clients with high communication or training latency are assigned higher dropout rates, meaning they upload fewer parameters. This reduces total data volume transferred without sacrificing model quality because important parameters are selected via importance metrics. The core assumption is that model parameter importance can be reliably estimated using the ratio of parameter change to parameter value, scaled by coverage rate for heterogeneous models.

### Mechanism 2
Convex optimization of dropout rates balances system, data, and model heterogeneity. The server solves a convex problem minimizing total training time plus a penalty term weighted by client contribution (data size, label distribution, loss, model size), subject to constraints on maximum dropout rate and total parameters uploaded. The core assumption is that the objective function (total time + weighted dropout sum) is convex in the dropout rates, enabling efficient global optimization.

### Mechanism 3
Periodic full model broadcasting mitigates drift caused by parameter sparsification. Every h rounds, the server broadcasts the full global model to all clients, preventing accumulation of errors from sparse updates and maintaining alignment across heterogeneous local models. The core assumption is that residual error from sparse updates grows linearly with rounds between full broadcasts, and periodic synchronization controls this drift.

## Foundational Learning

- **Concept**: Convex optimization and its convergence guarantees
  - Why needed here: The dropout rate allocation problem is solved as a convex optimization to ensure global optimality and computational tractability
  - Quick check question: What property of the dropout rate allocation objective guarantees that standard convex solvers will find the optimal solution?

- **Concept**: Importance sampling in sparse gradient methods
  - Why needed here: The uploaded parameter selection uses importance metrics analogous to importance sampling in gradient sparsification, ensuring that the most informative parameters are transmitted
  - Quick check question: How does the importance index in FedDD relate to the magnitude of parameter updates relative to their current values?

- **Concept**: Non-IID data distributions and their effect on federated learning convergence
  - Why needed here: The algorithm explicitly accounts for data heterogeneity via terms like label distribution uniformity and local loss scaling by model size
  - Quick check question: Why does a client with more diverse labels (closer to uniform distribution) receive a lower dropout rate?

## Architecture Onboarding

- **Component map**: Server -> Convex optimizer for dropout rates -> Global aggregation -> Periodic full model broadcast -> Clients -> Local training -> Importance-based parameter selection -> Sparse model upload -> Server aggregation
- **Critical path**: Local training → Importance calculation → Sparse upload → Server aggregation → Dropout rate recalculation → Full/broadcast model update → Next round
- **Design tradeoffs**: Higher dropout rates → lower communication but potentially slower convergence; smaller h (broadcast period) → better alignment but higher communication cost; more aggressive importance selection → fewer parameters but risk of missing subtle updates
- **Failure signatures**: Accuracy plateaus early → possible dropout rate too high or importance selection too aggressive; convergence speed degrades over rounds → check if h is too large or if client contribution terms in optimization are misweighted; some clients never get selected → verify that dropout constraints and Aserver proportion allow all clients to participate
- **First 3 experiments**: 1) Run FedDD with h=1 (full broadcast every round) on simple IID dataset; verify it matches FedAvg accuracy but with fewer parameters transmitted; 2) Vary Dmax from 20% to 80% on heterogeneous network; observe impact on communication time vs final accuracy; 3) Simulate class-imbalanced data; compare FedDD vs client-selection baselines on rare class accuracy

## Open Questions the Paper Calls Out

- **Open Question 1**: How should the server determine the optimal value of Aserver (proportion of parameters amount required) based on available communication resources and operational costs?
- **Open Question 2**: What is the optimal frequency h for broadcasting the full model to clients during training, and how does it vary across different heterogeneity scenarios?
- **Open Question 3**: How does the proposed parameter selection strategy perform when combined with different dropout rate allocation methods, and what is the optimal combination?

## Limitations
- The convex optimization formulation assumes static client conditions and doesn't address online adaptation for dynamic network changes
- The importance metric may not capture all forms of parameter significance, particularly for parameters critical for rare classes with small absolute values
- Periodic full model broadcasting introduces a hyperparameter h that requires careful tuning and could become a communication bottleneck

## Confidence

- **High confidence**: The core claim that differential dropout rates can reduce communication while maintaining accuracy is well-supported by theoretical framework and experimental results across multiple datasets
- **Medium confidence**: The claim about strong generalization to rare classes is supported by experimental results but lacks ablation studies showing specific contribution of differential dropout
- **Low confidence**: The theoretical convergence bounds assume specific conditions on h and dropout rates that may not hold in all practical scenarios, particularly with extreme heterogeneity

## Next Checks

1. Test FedDD's performance when client dropout rates are assigned randomly rather than via convex optimization to isolate impact of intelligent dropout rate allocation versus simple parameter sparsification
2. Measure impact of varying h (broadcast period) on convergence for different levels of heterogeneity to determine minimum h that maintains accuracy and establish practical communication-accuracy tradeoffs
3. Conduct experiments where importance metrics are deliberately corrupted or inverted to test robustness of parameter selection mechanism and quantify accuracy degradation when wrong parameters are selected