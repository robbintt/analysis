---
ver: rpa2
title: 'Parcel loss prediction in last-mile delivery: deep and non-deep approaches
  with insights from Explainable AI'
arxiv_id: '2310.16602'
source_url: https://arxiv.org/abs/2310.16602
tags:
- parcel
- data
- loss
- learning
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of predicting parcel loss in
  last-mile delivery using machine learning, tackling the challenge of highly imbalanced
  data where only a small fraction of parcels are lost. The authors propose two approaches:
  Data Balance with Supervised Learning (DBSL) and Deep Hybrid Ensemble Learning (DHEL).'
---

# Parcel loss prediction in last-mile delivery: deep and non-deep approaches with insights from Explainable AI

## Quick Facts
- arXiv ID: 2310.16602
- Source URL: https://arxiv.org/abs/2310.16602
- Reference count: 19
- Primary result: DHEL approach achieves 0.701 balanced accuracy and 55.4% recall for lost parcel prediction

## Executive Summary
This paper addresses parcel loss prediction in last-mile delivery using machine learning on highly imbalanced data where only 0.25% of parcels are lost. The authors propose two complementary approaches: Data Balance with Supervised Learning (DBSL) using resampling techniques with classifiers, and Deep Hybrid Ensemble Learning (DHEL) leveraging autoencoders for anomaly detection. Experiments on Belgian shipment data demonstrate that DHEL, combining a feed-forward autoencoder with a random forest classifier, outperforms traditional methods with 0.701 balanced accuracy. The study also employs explainable AI techniques to identify key predictive features and demonstrates potential cost savings for e-commerce retailers.

## Method Summary
The paper compares two machine learning approaches for parcel loss prediction. DBSL uses resampling techniques (Random Undersampling, NearMiss, RUSBoost, EasyEnsemble) to balance the highly imbalanced dataset, then trains supervised classifiers (Random Forest, XGBoost, Logistic Regression, SVM). DHEL employs autoencoders trained only on normal parcels to learn reconstruction patterns, then uses reconstruction errors as features for ensemble classifiers. Both approaches undergo hyperparameter tuning via random search and cross-validation. The authors also apply SHAP values to interpret model predictions and provide business insights.

## Key Results
- DHEL with standard autoencoder + Random Forest achieves highest performance: 0.701 balanced accuracy and 55.4% recall
- Random Undersampling consistently improves balanced accuracy across all supervised classifiers
- VAE and DAE autoencoders underperform standard autoencoders due to added noise degrading reconstruction error quality
- SHAP analysis identifies customer behavior, product type, and shipment location as key predictive features

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: DHEL improves classification performance by using autoencoder reconstruction errors as discriminative features for imbalanced parcel loss prediction.
- **Mechanism**: Autoencoder trained solely on normal parcels learns reconstruction patterns. Reconstruction error vector represents deviation from normal patterns and serves as input to ensemble classifiers, transforming imbalance into semi-supervised anomaly detection.
- **Core assumption**: Parcel loss events exhibit anomalous patterns distinguishable through reconstruction error analysis, and ensemble classifiers can effectively discriminate using these error vectors.
- **Evidence anchors**:
  - [abstract]: "DHEL leverages autoencoders to extract features from normal data and uses reconstruction errors as input to ensemble classifiers"
  - [section]: "These error measurements are then provided as inputs to an ensemble of supervised learning models"
  - [corpus]: **Weak** - No direct corpus evidence for reconstruction error approach in parcel loss prediction.
- **Break condition**: If parcel loss events are not anomalous or reconstruction errors fail to capture discriminative features, DHEL would underperform standard supervised methods.

### Mechanism 2
- **Claim**: Random Undersampling combined with ensemble methods improves balanced accuracy by reducing majority class bias while maintaining classifier diversity.
- **Mechanism**: RU balances dataset by randomly removing majority class samples, addressing class imbalance. Ensemble methods (Random Forest, XGBoost) trained on balanced data reduce overfitting to majority class while maintaining generalization through averaging predictions from diverse models.
- **Core assumption**: Random removal of majority samples preserves sufficient information for classifier training while reducing bias, and ensemble methods effectively combine predictions from balanced subsets.
- **Evidence anchors**:
  - [abstract]: "Data Balance with Supervised Learning (DBSL) uses resampling techniques with supervised classifiers"
  - [section]: "The combined votes from all weak learners produce an ensemble model. Random undersampling reduces overfitting to the majority class"
  - [corpus]: **Missing** - No corpus evidence for RU + ensemble combination in parcel loss prediction.
- **Break condition**: If random undersampling removes too many informative majority samples or ensemble methods fail to combine predictions effectively, classification performance would degrade.

### Mechanism 3
- **Claim**: VAE and DAE do not improve performance because added noise in reconstruction errors introduces irrelevant features that confuse classifiers.
- **Mechanism**: While VAEs and DAEs add robustness through probabilistic latent spaces and denoising, reconstruction error vectors become noisier and less discriminative for classification task. This noise obscures genuine anomalous patterns that distinguish lost parcels.
- **Core assumption**: Reconstruction error vectors from standard autoencoders are already sufficiently informative, and additional noise from VAEs/DAEs degrades rather than enhances classifier performance.
- **Evidence anchors**:
  - [section]: "The more robust VAE and DAE do not improve the predictive performance, as all deep hybrid models show lower predictive performance than the regular AE"
  - [section]: "It is expected that the added random noise in the reconstruction error of the DAE and VAE causes the classifier to focus on the wrong features during training"
  - [corpus]: **Weak** - No direct corpus evidence for VAE/DAE failure in imbalanced classification.
- **Break condition**: If classification task benefits from additional noise robustness or added noise captures useful patterns, VAE/DAE approaches could outperform standard autoencoders.

## Foundational Learning

- **Concept: Class imbalance in machine learning**
  - Why needed here: Dataset has only 0.25% lost parcels, making standard classifiers biased toward majority class and requiring specialized techniques.
  - Quick check question: What is the ratio of lost to non-lost parcels in the dataset, and why does this ratio pose challenges for standard classification algorithms?

- **Concept: Autoencoder architecture and reconstruction error**
  - Why needed here: DHEL relies on autoencoders to transform imbalance problem into anomaly detection by using reconstruction errors as discriminative features.
  - Quick check question: How does an autoencoder learn to reconstruct normal data, and why do anomalies produce higher reconstruction errors?

- **Concept: Ensemble learning and variance reduction**
  - Why needed here: Random Forest and XGBoost ensemble methods reduce variance and overfitting while maintaining performance on balanced data from RU.
  - Quick check question: How do ensemble methods like Random Forest and XGBoost reduce overfitting compared to single decision trees?

## Architecture Onboarding

- **Component map**: Data preprocessing → Resampling/Autoencoder training → Ensemble classifier training → Model evaluation
  - For DHEL: Normal data → Autoencoder (encoder+decoder) → Reconstruction error vector → Ensemble classifier → Prediction
  - For DBSL: Imbalanced data → Resampling (RU/NM/UB/RUSBoost) → Supervised classifier (RF/XGB/LR/SVM) → Prediction

- **Critical path**: For DHEL: Training autoencoder on normal data → Generating reconstruction error vectors on validation set → Hyperparameter tuning classifier on error vectors → Final training and evaluation. For DBSL: Resampling training data → Hyperparameter tuning classifier → Training on full resampled data → Evaluation.

- **Design tradeoffs**: DHEL offers better performance but lacks interpretability (black-box autoencoder + classifier). DBSL provides interpretability through SHAP values but requires careful resampling to avoid information loss. VAE/DAE add robustness but introduce noise that degrades performance.

- **Failure signatures**: High false negatives indicate classifier not capturing loss patterns; high false positives suggest overfitting to noise; poor reconstruction error discrimination indicates autoencoder not learning normal patterns effectively.

- **First 3 experiments**:
  1. Train standard Random Forest on imbalanced data to establish baseline performance and confirm majority class bias.
  2. Implement Random Undersampling with Random Forest to evaluate if balancing improves performance and identify optimal sampling ratio.
  3. Train DHEL with standard autoencoder + Random Forest to compare against DBSL and assess reconstruction error effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal architecture and hyperparameter configuration for autoencoders when applied to tabular data for parcel loss prediction, as opposed to their traditional use with time-series or image data?
- Basis in paper: [explicit] The paper compares different autoencoder architectures (regular AE, variational AE, and denoising AE) but finds that the regular AE with a random forest classifier performs best. However, the authors note that this finding is specific to their dataset and use case.
- Why unresolved: The paper only tests a limited set of autoencoder architectures and hyperparameters. The optimal configuration for tabular data in this specific context remains unknown and may differ from configurations optimized for other data types.
- What evidence would resolve it: Systematic testing of a wider range of autoencoder architectures (e.g., different numbers of layers, different activation functions, different regularization techniques) and hyperparameter configurations on various tabular datasets related to parcel loss prediction or similar anomaly detection tasks in logistics.

### Open Question 2
- Question: How can the reconstruction error vector from autoencoders be effectively used as input to classification models to improve interpretability of the final model, especially when combining AE with ensemble methods like random forests?
- Basis in paper: [inferred] The paper uses SHAP values to interpret the random forest model trained on reconstruction errors from the autoencoder. However, the authors note that existing XAI techniques are not suitable for interpreting the combined AE-RF model due to the separate training of AE and RF on different datasets.
- Why unresolved: The reconstruction error vector is a complex transformation of the original features, making it challenging to directly interpret the relationship between input features and final predictions when using AE-RF. New methods are needed to bridge this interpretability gap.
- What evidence would resolve it: Development and validation of new XAI techniques or adaptations of existing methods that can effectively interpret the combined AE-RF model by linking the reconstruction errors back to the original input features and their impact on final predictions.

### Open Question 3
- Question: What is the long-term effectiveness and generalizability of machine learning models for parcel loss prediction, considering the dynamic nature of delivery processes and potential changes in fraud patterns?
- Basis in paper: [inferred] The paper evaluates the models on a one-year dataset and discusses some insights with domain experts. However, the long-term performance and ability to adapt to changing conditions are not assessed.
- Why unresolved: Parcel loss patterns and fraud techniques can evolve over time, potentially reducing the effectiveness of models trained on historical data. The generalizability of the models to different regions, delivery companies, and time periods is also unknown.
- What evidence would resolve it: Long-term monitoring and evaluation of the models' performance on new data over multiple years, including periods with significant changes in delivery processes or fraud patterns. Cross-validation of the models on data from different regions, delivery companies, and time periods to assess generalizability.

## Limitations

- Data specificity concerns limit generalizability to other postal systems or e-commerce contexts with different loss rates and operational characteristics
- Method comparison lacks comprehensive statistical testing with confidence intervals and significance testing between DBSL and DHEL approaches
- Business impact verification relies on fixed misclassification costs rather than dynamic cost modeling based on actual business operations

## Confidence

- **High confidence**: DHEL architecture effectiveness - reconstruction error mechanism is well-established in anomaly detection literature and experimental results are robust
- **Medium confidence**: Resampling technique effectiveness - Random Undersampling shows consistent improvement but optimal strategy varies by classifier
- **Low confidence**: VAE/DAE negative results - explanation that added noise causes poor performance is speculative and alternative explanations are not ruled out

## Next Checks

1. Apply paired t-tests or Wilcoxon signed-rank tests to compare classification performance across DBSL and DHEL approaches, reporting confidence intervals for balanced accuracy and recall metrics

2. Test both approaches on parcel loss datasets from different postal services or e-commerce platforms to assess generalizability beyond the Belgian context

3. Vary misclassification costs systematically to determine robustness of cost savings claims and identify thresholds where different approaches become optimal for business applications