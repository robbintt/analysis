---
ver: rpa2
title: 'On the Discussion of Large Language Models: Symmetry of Agents and Interplay
  with Prompts'
arxiv_id: '2311.07076'
source_url: https://arxiv.org/abs/2311.07076
tags:
- discussion
- design
- answer
- reasoning
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores two complementary approaches to improve the
  reasoning capabilities of large language models: prompt engineering and multi-agent
  discussion mechanisms. It provides a theoretical framework to characterize discussion
  mechanisms based on the concept of agent symmetry, enabling systematic understanding
  of their complexity.'
---

# On the Discussion of Large Language Models: Symmetry of Agents and Interplay with Prompts

## Quick Facts
- arXiv ID: 2311.07076
- Source URL: https://arxiv.org/abs/2311.07076
- Reference count: 21
- Key outcome: Complex prompt engineering can approach state-of-the-art multi-agent discussion performance, suggesting an inherent reasoning capability upper bound.

## Executive Summary
This paper explores the interplay between prompt engineering and multi-agent discussion mechanisms for improving large language model reasoning capabilities. Through theoretical analysis of agent symmetry and empirical evaluation on the FOLIO Wiki curated dataset, the authors demonstrate that carefully designed prompt engineering can achieve performance comparable to complex multi-agent discussion mechanisms. The study introduces the Conquer-and-Merge Discussion (CMD) framework, which uses symmetric group discussions with a secretary to merge results, achieving state-of-the-art performance with simpler prompts.

## Method Summary
The authors systematically evaluate combinations of prompt engineering techniques (step-by-step reasoning, detailed task description, response format specification, one-shot in-context learning) and discussion mechanisms (MAD, Debate, CMD) on the FOLIO Wiki curated dataset. They characterize discussion mechanisms through agent symmetry groups, showing how symmetry breaking affects reasoning performance. The proposed CMD framework organizes agents into symmetric groups that iteratively exchange opinions before a secretary merges the results, achieving high performance while maintaining implementation simplicity.

## Key Results
- Complex prompt engineering alone can approach the state-of-the-art performance of multi-agent discussion mechanisms
- CMD framework achieves state-of-the-art performance with simpler prompts by using symmetric group discussions
- The paper reveals an inherent upper bound of reasoning capability that can be approached through either strong prompt engineering or strong discussion engineering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Symmetry breaking in discussion engineering affects reasoning performance.
- Mechanism: Discussion mechanisms are characterized by their agent symmetry groups, where breaking symmetries (assigning different roles or models to agents) can enhance performance but at the cost of complexity.
- Core assumption: Agent symmetry and its breaking directly influence the reasoning capability of multi-agent systems.
- Evidence anchors:
  - [abstract] "Theoretically, this paper justifies the multi-agent discussion mechanisms from the symmetry of agents."
  - [section 3.2] "The setting of m agent debate also reveals Sm in terms of mechanism invariance and model invariance"
  - [corpus] Weak evidence - related works focus on discussion mechanisms but lack symmetry analysis.
- Break condition: If symmetry breaking does not introduce meaningful diversity in agent roles or prompts, performance gains may diminish.

### Mechanism 2
- Claim: Complex prompt engineering and complex discussion mechanisms approach the same performance upper bound.
- Mechanism: Both elaborate prompt designs and complex discussion mechanisms can push language models toward their inherent reasoning limits, with diminishing returns when both are complex.
- Core assumption: There exists an inherent upper bound of reasoning capability that can be approached through either strong prompt engineering or strong discussion engineering.
- Evidence anchors:
  - [abstract] "Empirically, this paper reports the empirical results of the interplay of prompts and discussion mechanisms, revealing the empirical state-of-the-art performance of complex multi-agent mechanisms can be approached by carefully developed prompt engineering."
  - [section 5.3.2] "We observe that the prompt decorator that performs well on single-agent settings continues to be one of the best performers in discussions, but the accuracy decreases after the discussion, with only slight improvements observed in our proposed CMD method."
  - [corpus] Moderate evidence - related works explore prompt engineering and discussion mechanisms but do not analyze their interplay systematically.
- Break condition: If the task requires reasoning beyond the current model's capacity, neither complex prompts nor complex discussions will achieve high performance.

### Mechanism 3
- Claim: CMD (Conquer-and-Merge Discussion) achieves state-of-the-art performance with simpler prompts and symmetric discussion.
- Mechanism: CMD uses symmetric group discussions and a secretary to merge results, reducing complexity while maintaining high performance by leveraging group dynamics and iterative refinement.
- Core assumption: Symmetric discussions with group-based organization can achieve high performance without requiring asymmetric agent roles or complex prompt designs.
- Evidence anchors:
  - [abstract] "This paper also proposes a scalable discussion mechanism based on conquer and merge, providing a simple multi-agent discussion solution with simple prompts but state-of-the-art performance."
  - [section 4] "Empirical evaluation shows that CMD can also approximate the upper bound of the performance by using a simpler prompt decorator."
  - [corpus] Weak evidence - related works focus on discussion mechanisms but lack the specific CMD approach.
- Break condition: If the task requires diverse perspectives that symmetric discussions cannot provide, CMD may underperform compared to asymmetric approaches.

## Foundational Learning

- Concept: Symmetry groups and their breaking in mechanism design.
  - Why needed here: Understanding how symmetry breaking affects discussion mechanisms is crucial for designing effective multi-agent systems.
  - Quick check question: What is the largest possible symmetry group for a discussion of m agents, and why might a mechanism not achieve this symmetry?

- Concept: Prompt engineering techniques (Chain-of-Thought, In-Context Learning, detailed task description).
  - Why needed here: These techniques are the building blocks for improving single-agent reasoning, which interacts with multi-agent discussion mechanisms.
  - Quick check question: How does adding a detailed task description to a prompt potentially reduce answer ambiguity in FOLIO tasks?

- Concept: Multi-agent discussion frameworks and their evaluation metrics.
  - Why needed here: To understand how different discussion mechanisms (e.g., Debate, MAD, CMD) perform and compare them systematically.
  - Quick check question: Why might requiring agents to hold different views during initial prompts introduce bias in discussions?

## Architecture Onboarding

- Component map:
  Prompt decorator (single agent) -> Discussion mechanism -> Secretary merge -> Evaluation
  (step-by-step reasoning, detailed task description, response format, one-shot ICL)

- Critical path:
  1. Design prompt decorator for single agent
  2. Choose discussion mechanism (symmetric/asymmetric)
  3. Implement agent assignment and group organization
  4. Run iterative discussion rounds
  5. Evaluate performance on FOLIO dataset

- Design tradeoffs:
  - Symmetric vs. asymmetric discussions: Simpler implementation vs. potential performance gains
  - Complex prompts vs. simple prompts: Better single-agent performance vs. potential interference in discussions
  - Group size vs. discussion rounds: Computational cost vs. accuracy improvement

- Failure signatures:
  - Low accuracy despite complex prompts and discussions: Indicates the task may be beyond the model's capacity
  - Performance degradation with more discussion rounds: Suggests potential interference or confusion among agents
  - No improvement from symmetry breaking: Indicates the task may not benefit from diverse agent perspectives

- First 3 experiments:
  1. Single agent with all prompt features (step-by-step, detailed task description, response format, one-shot ICL)
  2. Debate mechanism with 3 agents, holding different views, using simple prompts
  3. CMD mechanism with 6 agents, no view requirements, using simple prompts with detailed task description

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the different prompt engineering techniques (step-by-step reasoning, detailed task description, response format, one-shot ICL) interact with each other in complex discussion mechanisms beyond simple single-agent settings?
- Basis in paper: [inferred] The paper evaluates combinations of prompt features but doesn't deeply analyze their interactions within multi-agent discussions
- Why unresolved: The paper shows diminishing returns when combining complex prompts with complex discussions, but doesn't systematically investigate how specific prompt components affect different discussion mechanisms
- What evidence would resolve it: Controlled experiments varying individual prompt components within each discussion mechanism, measuring their isolated and combined effects on performance

### Open Question 2
- Question: What is the optimal number of agents and discussion rounds for different types of reasoning tasks, and how does this scale with task complexity?
- Basis in paper: [explicit] The paper mentions that increased agents and rounds improve performance but doesn't establish specific scaling relationships
- Why unresolved: While CMD is proposed as a scalable solution, the paper doesn't provide theoretical or empirical analysis of optimal configuration parameters for different task types
- What evidence would resolve it: Systematic studies varying agent counts and discussion rounds across diverse reasoning tasks, with performance curves and task-complexity correlations

### Open Question 3
- Question: How does the symmetry of discussion mechanisms affect convergence speed and quality of consensus in multi-agent debates?
- Basis in paper: [explicit] The paper introduces symmetry as a theoretical framework but only explores asymmetric and simple symmetric cases
- Why unresolved: The theoretical framework suggests symmetry could be important, but empirical validation is limited to a few specific mechanisms without comparative analysis of symmetric vs asymmetric designs
- What evidence would resolve it: Comparative studies of mechanisms with varying symmetry properties (beyond simple vs asymmetric) measuring convergence metrics and consensus quality across multiple task types

### Open Question 4
- Question: How transferable are the findings about prompt-engineering vs discussion-engineering trade-offs across different language model architectures (GPT-4, Claude, LLaMA, etc.)?
- Basis in paper: [explicit] The paper only tests with GPT-3.5-Turbo, acknowledging limitations but not exploring model-specific effects
- Why unresolved: The observed performance patterns might be specific to GPT-3.5-Turbo's reasoning capabilities and prompting responses, not generalizable to other architectures
- What evidence would resolve it: Replication studies across multiple LLM architectures using the same experimental setup, comparing prompt-engineering vs discussion-engineering effectiveness and identifying architecture-specific patterns

## Limitations
- Limited to a single dataset (FOLIO Wiki curated) and model family (ChatGPT-3.5-Turbo-0613)
- The theoretical framework for agent symmetry, while conceptually sound, requires further empirical validation
- The assumption of a universal reasoning capability upper bound needs broader testing across different model families and task types

## Confidence

**High Confidence**: The theoretical characterization of agent symmetry groups and their relationship to mechanism invariance is well-established mathematically. The observation that complex prompt engineering and complex discussion mechanisms approach similar performance bounds is supported by robust empirical evidence.

**Medium Confidence**: The CMD framework's superiority over other discussion mechanisms is demonstrated, but the results are based on a limited number of comparison points and a single dataset. The claim that CMD can achieve state-of-the-art performance with simpler prompts needs validation across diverse tasks.

**Low Confidence**: The assumption that there exists a universal upper bound of reasoning capability that can be approached through either strong prompt engineering or strong discussion engineering requires broader testing across different model families and task types.

## Next Checks

1. **Dataset Generalization Test**: Evaluate CMD and other discussion mechanisms across multiple reasoning datasets (e.g., GSM8K, BIG-Bench) to assess whether the observed performance patterns hold beyond the FOLIO Wiki curated dataset.

2. **Model Family Transferability**: Test the proposed frameworks with different language model families (e.g., Claude, LLaMA, PaLM) to determine if the symmetry-based characterization and CMD's effectiveness generalize across architectures.

3. **Complexity-Benefit Analysis**: Conduct a systematic study measuring the computational cost (inference time, token usage) versus accuracy gains for symmetric versus asymmetric discussion mechanisms across varying group sizes and discussion depths.