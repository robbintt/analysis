---
ver: rpa2
title: Simple and Effective Input Reformulations for Translation
arxiv_id: '2311.06696'
source_url: https://arxiv.org/abs/2311.06696
tags:
- translation
- language
- input
- baseline
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes three input reformulation techniques for improving
  translation performance during finetuning of foundation language models. The methods
  involve modifying training data by appending partial or full parallel translations
  in English (ParSE) or other languages (MiPS) to the input, or adding a prefix of
  the target English output (POSE).
---

# Simple and Effective Input Reformulations for Translation

## Quick Facts
- **arXiv ID**: 2311.06696
- **Source URL**: https://arxiv.org/abs/2311.06696
- **Reference count**: 17
- **Primary result**: Input reformulation techniques (POSE, ParSE, MiPS) improve translation performance up to 3.5 chrF++ on Flores200 benchmark compared to direct finetuning baselines

## Executive Summary
This paper introduces three simple input reformulation techniques that significantly improve translation performance during finetuning of foundation language models. The methods—Partial Output Scaffold in English (POSE), Parallel Scaffold in English (ParSE), and Mixed-language Parallel Scaffold (MiPS)—modify training data by appending partial or full parallel translations to the input. Experiments on Classical Tibetan→English and the challenging Flores200 benchmark show substantial improvements over direct finetuning baselines, with POSE providing the largest gains and ParSE/MiPS particularly benefiting lower-resource languages.

## Method Summary
The paper proposes three input reformulation techniques for translation finetuning: POSE appends a prefix of the target English output to the source input, ParSE appends the full parallel English translation, and MiPS appends parallel translations in multiple languages. These reformulations are applied during training with a data mixing strategy (20% baseline, 80% reformulated) to balance task difficulty and information richness. The authors evaluate these methods on mT5 models (600M, 1B, 3B parameters) using tib2eng and Flores200 datasets, measuring performance with BLEU and chrF++ metrics.

## Key Results
- POSE achieves up to 10.3% / 2.8 BLEU improvement on Classical Tibetan→English translation
- ParSE and MiPS improve chrF++ scores by up to 3.5 on Flores200 benchmark
- ParSE and MiPS particularly benefit lower-resource languages across the board
- Best performance achieved with 20% baseline and 80% reformulated data mix

## Why This Works (Mechanism)

### Mechanism 1
Adding partial target outputs (POSE) or parallel translations (ParSE) during finetuning reduces early-stage training difficulty and improves conditioning. The model benefits from attending to an easier, more structured input that provides scaffolding for the target output, allowing it to learn alignment before generating full outputs. This works particularly well when the scaffold language (like English) is in-distribution for the model.

### Mechanism 2
Parallel scaffolding (ParSE, MiPS) aligns representations across languages by providing explicit cross-lingual context. By appending parallel translations in different languages, the model learns to map representations between languages early in training, which benefits multilingual tasks. This enables direct attention between input contexts in different languages.

### Mechanism 3
Curriculum-like data mixing (20% baseline, 80% reformulated) balances task difficulty and information richness. The model is exposed to both easier scaffolded examples early on and harder, more informative examples later, preventing underfitting while maintaining generalization. This balanced approach performs better than all-easy or all-hard data scenarios.

## Foundational Learning

- **Concept**: Pretraining dataset distribution effects
  - Why needed here: The model's pretraining exposure to different languages heavily influences finetuning performance, especially when the source language is out-of-distribution.
  - Quick check question: If the model was pretrained on a language, does that always guarantee better finetuning performance on that language?
    - Answer: No—translation from in-pretrain to out-pretrain is harder than out-pretrain to in-pretrain.

- **Concept**: Curriculum learning in finetuning
  - Why needed here: The ablation experiments show that modulating task difficulty during training (e.g., 20% scaffolded examples) improves performance more than rigid curricula.
  - Quick check question: Does making the task easier always improve finetuning performance?
    - Answer: No—too easy a task reduces information richness and hurts downstream performance.

- **Concept**: Multilingual representation alignment
  - Why needed here: The model must learn to align representations across languages during finetuning, especially in massively multilingual settings.
  - Quick check question: Does providing parallel context always improve multilingual alignment?
    - Answer: Yes, if the parallel data is accurate; otherwise, noisy alignment can hurt performance.

## Architecture Onboarding

- **Component map**: Tokenizer -> Model (mT5) -> Optimizer (AdamW) -> Dataset (tib2eng/Flores200) -> Input reformulation -> Training -> Evaluation
- **Critical path**: 1) Tokenize input and append reformulated context 2) Batch examples with 20% baseline, 80% reformulated 3) Train with AdamW; save checkpoints every 200 steps 4) Evaluate with BLEU (tib2eng) or chrF++ (Flores200)
- **Design tradeoffs**: Scaffold length (too long → copying, too short → insufficient guidance), reformulation ratio (too high → loss of information richness, too low → insufficient scaffolding), language choice (strong pretraining languages yield better results)
- **Failure signatures**: Training curves showing early struggles suggest trying POSE; poor multilingual alignment suggests ParSE/MiPS; no improvement over baseline may indicate scaffold language is also out-of-distribution
- **First 3 experiments**: 1) Replicate tib2eng POSE ablation (20%, 50%, 100% reformulated) to validate task difficulty effects 2) Apply ParSE to Flores200 with 20%/80% baseline-reformulated mix 3) Compare MiPS vs ParSE on Flores200 to test mixed-language vs strong-language scaffolding

## Open Questions the Paper Calls Out

### Open Question 1
What are the underlying mechanisms by which input reformulations like POSE, ParSE, and MiPS improve translation performance in foundation language models? While the paper provides intuition about attention alignment and representation mapping, the exact mechanisms remain unexplored and require detailed analyses of internal representations and attention patterns.

### Open Question 2
How do input reformulations affect the model's ability to generalize to unseen languages or language pairs? The paper demonstrates improvements across a wide range of languages but doesn't investigate whether these techniques enable better handling of languages or language pairs not present in pretraining data.

### Open Question 3
How do the effects of input reformulations vary across different model architectures, sizes, and pretraining objectives? The paper only tests mT5 models, leaving open whether these techniques would be equally effective or require adaptation for encoder-decoder models like BART, decoder-only models like GPT, or models with different pretraining objectives.

## Limitations
- Experimental results limited to one low-resource language pair and one multilingual benchmark
- Mechanism explanations remain largely speculative without direct empirical validation
- Implementation details underspecified, particularly prefix length selection and parallel data criteria
- Unclear whether benefits extend to other language families, translation directions, or non-transduction tasks

## Confidence

**High confidence** in core empirical findings for specific experimental conditions: Consistent improvements across multiple runs, different model sizes, and both evaluation metrics.

**Medium confidence** in mechanism explanations: Plausible theories about why input reformulation helps, but these remain hypotheses without direct empirical validation.

**Low confidence** in cross-task generalization: Paper does not test whether these techniques transfer to other NLP tasks, different model architectures, or varying data conditions beyond studied scenarios.

## Next Checks

1. **Attention visualization analysis**: Generate attention weight visualizations during POSE training to empirically verify whether the model attends more strongly to the scaffold portion early in training and shifts focus over time.

2. **Cross-lingual transfer study**: Apply POSE reformulation to translation tasks involving languages from different families (e.g., English to Korean or Arabic) to determine whether benefits extend beyond Indo-European language pairs.

3. **Controlled prefix length experiment**: Systematically vary the prefix length distribution in POSE (e.g., using fixed lengths like 10%, 30%, 50%, 70% of target length) rather than random selection to determine optimal scaffolding strategy.