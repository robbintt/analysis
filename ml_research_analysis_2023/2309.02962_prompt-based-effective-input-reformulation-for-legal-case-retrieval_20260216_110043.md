---
ver: rpa2
title: Prompt-based Effective Input Reformulation for Legal Case Retrieval
arxiv_id: '2309.02962'
source_url: https://arxiv.org/abs/2309.02962
tags:
- legal
- case
- retrieval
- cases
- facts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes PromptCase, a novel legal case retrieval framework
  addressing challenges of legal feature alignment and legal context preservation
  in neural legal case retrieval models. The key idea is to reformulate the input
  by extracting and encoding two determining legal features, legal facts and legal
  issues, using a prompt-based encoding scheme with language models.
---

# Prompt-based Effective Input Reformulation for Legal Case Retrieval

## Quick Facts
- arXiv ID: 2309.02962
- Source URL: https://arxiv.org/abs/2309.02962
- Reference count: 40
- Key outcome: PromptCase achieves significant improvements in legal case retrieval effectiveness using prompt-based encoding of legal facts and issues

## Executive Summary
This paper proposes PromptCase, a novel framework for legal case retrieval that addresses challenges of legal feature alignment and legal context preservation. The key innovation is a prompt-based encoding scheme that reformulates input by extracting and encoding legal facts and legal issues with explicit prompt templates. Extensive experiments on LeCaRD and COLIEE2023 datasets demonstrate significant improvements over state-of-the-art baselines across multiple evaluation metrics including precision, recall, F1-score, MRR, MAP, and NDCG.

## Method Summary
PromptCase reformulates legal case retrieval by extracting two key legal features - legal facts and legal issues - and encoding them using a prompt-based scheme with language models. The framework adds prompt templates "Legal facts:" and "Legal issues:" to the beginning of extracted texts to capture global legal context. Legal facts and issues are encoded separately to avoid interference, then combined through concatenation with cross-encoded representations. The method uses dual encoding (separate encoding of facts and issues) and cross encoding (joint encoding) to create comprehensive case representations for similarity-based retrieval.

## Key Results
- Achieves significant improvements over state-of-the-art baselines in precision, recall, and F1-score
- Demonstrates superior performance across MRR, MAP, and NDCG metrics on both LeCaRD and COLIEE2023 datasets
- Shows effectiveness of prompt-based encoding scheme for capturing legal context in retrieval tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding prompt templates ("Legal facts:", "Legal issues:") improves semantic encoding by providing context for the model to interpret the following text correctly.
- Mechanism: The language model leverages the explicit labels to align extracted features with the expected legal meaning, improving semantic coherence during encoding.
- Core assumption: The model interprets prompt tokens as semantic cues for the subsequent content.
- Evidence anchors:
  - [abstract]: "the prompt templates of 'Legal facts:' and 'Legal issues:' are added to the beginning of the legal facts and legal issues texts to capture the global context of legal information"
  - [section]: "To enable the language models to capture the global context of legal information, the prompt templates of 'Legal facts:' ('法律事实：' in Chinese) and 'Legal issues:' ('法律纠纷：' in Chinese) will be added to the beginning of the legal facts and legal issues texts"
  - [corpus]: Found related work (CBR-RAG, CaseLink) using prompt-based and context-aware methods, indicating broader trend in prompt-aware encoding for specialized domains.
- Break condition: If the model does not treat prompt tokens as meaningful semantic anchors, or if the prompt text is too short to influence attention.

### Mechanism 2
- Claim: Dual encoding of legal facts and legal issues separately avoids interference between distinct legal features, preserving their individual semantic representations.
- Mechanism: Separate encoding of facts and issues followed by concatenation ensures each legal feature retains its semantic purity before being combined.
- Core assumption: Legal facts and legal issues are semantically distinct enough that joint encoding introduces noise.
- Evidence anchors:
  - [abstract]: "a prompt-based encoding scheme is designed to conduct an effective encoding with language models"
  - [section]: "To avoid the undesired cross-effect between legal facts and legal issues, the legal facts with prompt and legal issues with prompt will be fed into the BERT-based encoder separately"
  - [corpus]: Found related retrieval work (CaseGNN, CaseLink) focusing on feature separation for better retrieval accuracy.
- Break condition: If dual encoding fails to capture interactions between facts and issues that are important for relevance.

### Mechanism 3
- Claim: Focusing on legal facts and legal issues instead of whole case text reduces noise and aligns the model's input with the legal definition of relevance.
- Mechanism: Extracting only the two legal features filters out non-relevant content and aligns the input with the legal practitioner's concept of relevant cases (precedents with similar facts/issues).
- Core assumption: Legal facts and legal issues are sufficient for determining relevance in legal case retrieval.
- Evidence anchors:
  - [abstract]: "Firstly, legal facts and legal issues are identified and formally defined as the key features facilitating legal case retrieval based on a thorough study of the definition of relevant cases from a legal perspective"
  - [section]: "the determining factor of relevant cases is the alignment of key legal features instead of whole text matching"
  - [corpus]: Found relevant prior work (Legal Element-oriented modeling, LLM-based Embedders) also focusing on structured legal feature extraction.
- Break condition: If other case sections (e.g., reasoning, judgment) carry discriminative legal signals not captured by facts/issues alone.

## Foundational Learning

- Concept: Legal case structure (background, analysis, judgment)
  - Why needed here: The paper's feature extraction relies on parsing these sections correctly to locate legal facts and issues.
  - Quick check question: In a common law case, which section typically contains the judicial reasoning and precedent references?

- Concept: Prompt-based encoding in transformer models
  - Why needed here: Understanding how prompts influence attention and representation in language models is critical to the design.
  - Quick check question: How does adding a prompt token affect the attention weights in BERT's self-attention mechanism?

- Concept: Zero-shot evaluation
  - Why needed here: The paper evaluates without fine-tuning, relying on prompt and feature extraction design rather than model adaptation.
  - Quick check question: What is the difference between zero-shot and few-shot prompting in the context of retrieval tasks?

## Architecture Onboarding

- Component map:
  Input -> Extractor -> Dual Encoder -> Cross Encoder -> Combiner -> Retriever
- Critical path:
  Extract features → Encode with prompts → Concatenate → Compute similarity
- Design tradeoffs:
  - Separate vs joint encoding: Separate avoids interference but loses direct interaction; joint could capture interactions but may introduce noise.
  - Prompt wording: More descriptive prompts may help context but add token overhead.
  - Feature choice: Focusing on facts/issues may miss other relevant signals like reasoning patterns.
- Failure signatures:
  - Low precision despite high recall: May indicate prompt or feature extraction is too broad.
  - Poor performance on cross-dataset: Feature extraction rules may not generalize across legal systems.
  - Degraded performance when replacing prompts: Indicates prompts are providing significant semantic guidance.
- First 3 experiments:
  1. Ablation: Compare retrieval with and without prompt templates to measure their contribution.
  2. Feature isolation: Compare retrieval using only legal facts vs only legal issues to assess their individual importance.
  3. Prompt variation: Test different prompt templates (descriptive vs minimal) to find optimal prompt wording.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed prompt-based encoding scheme compare to other potential encoding methods for legal features, such as attention mechanisms or graph-based approaches?
- Basis in paper: [inferred] The paper focuses on the prompt-based encoding scheme but does not explore other encoding methods for legal features.
- Why unresolved: The paper only evaluates the prompt-based encoding scheme, leaving the comparison with other encoding methods unexplored.
- What evidence would resolve it: Empirical experiments comparing the performance of the prompt-based encoding scheme with other encoding methods for legal features on the same datasets.

### Open Question 2
- Question: Can the extraction of legal facts and legal issues be further improved by incorporating domain-specific knowledge or using more advanced natural language processing techniques?
- Basis in paper: [explicit] The paper discusses the extraction of legal facts and legal issues but does not explore the potential for further improvement using domain-specific knowledge or advanced NLP techniques.
- Why unresolved: The paper presents a method for extracting legal facts and legal issues but does not investigate potential improvements or alternative approaches.
- What evidence would resolve it: Experimental results comparing the proposed extraction method with alternative approaches that incorporate domain-specific knowledge or advanced NLP techniques on the same datasets.

### Open Question 3
- Question: How does the performance of the proposed framework vary across different legal systems and languages, and what factors contribute to these variations?
- Basis in paper: [inferred] The paper evaluates the framework on datasets from common law and civil law systems but does not explore the variations in performance across different legal systems and languages.
- Why unresolved: The paper presents results on two datasets but does not investigate the factors contributing to variations in performance across different legal systems and languages.
- What evidence would resolve it: Empirical experiments evaluating the framework on datasets from a diverse range of legal systems and languages, along with an analysis of the factors contributing to variations in performance.

## Limitations
- The effectiveness relies heavily on accurate extraction of legal facts and issues, but extraction methodology is underspecified for cross-dataset generalization
- Evaluation only considers zero-shot performance without exploring fine-tuning, leaving unclear whether improvements stem from prompt design or dataset-specific biases
- Does not address potential overfitting to specific legal domains or validate whether extracted features capture practitioner-defined relevance

## Confidence

- **High confidence**: The mechanism that prompt templates improve semantic encoding by providing context for model interpretation is well-supported by the described methodology and aligns with established prompt engineering principles in NLP.
- **Medium confidence**: The claim that dual encoding preserves semantic purity of legal features is plausible but not empirically validated through ablation studies comparing joint vs separate encoding.
- **Low confidence**: The assertion that legal facts and issues alone are sufficient for relevance determination lacks empirical validation, as the paper does not compare against models using full text or additional legal features.

## Next Checks

1. **Ablation study on feature sufficiency**: Compare retrieval performance using only legal facts, only legal issues, their combination, and full case text to empirically validate which features contribute most to relevance judgments.

2. **Cross-dataset generalization test**: Evaluate PromptCase on a third legal dataset from a different jurisdiction or legal system to assess whether the prompt-based approach generalizes beyond the two studied benchmarks.

3. **Prompt template sensitivity analysis**: Systematically vary the prompt wording (e.g., "Legal facts:", "Case background:", "Facts of the case:") to determine whether the specific phrasing or just the presence of explicit labels drives the performance improvements.