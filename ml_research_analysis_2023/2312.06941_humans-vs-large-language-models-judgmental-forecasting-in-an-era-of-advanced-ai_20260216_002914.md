---
ver: rpa2
title: 'Humans vs Large Language Models: Judgmental Forecasting in an Era of Advanced
  AI'
arxiv_id: '2312.06941'
source_url: https://arxiv.org/abs/2312.06941
tags:
- forecasting
- human
- llms
- forecast
- advanced
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares human forecasters and five Large Language Models
  (LLMs) in retail sales forecasting under promotional and non-promotional conditions.
  Using a controlled experiment with 123 human forecasters and LLMs (ChatGPT-4, ChatGPT3.5,
  Bard, Bing, Llama2), the study evaluates forecasting accuracy through Mean Absolute
  Percentage Error (MAPE).
---

# Humans vs Large Language Models: Judgmental Forecasting in an Era of Advanced AI

## Quick Facts
- arXiv ID: 2312.06941
- Source URL: https://arxiv.org/abs/2312.06941
- Authors: 
- Reference count: 40
- Key outcome: LLM integration into forecasting requires careful consideration of model-specific capabilities and contextual factors

## Executive Summary
This study compares human forecasters and five Large Language Models (LLMs) in retail sales forecasting under promotional and non-promotional conditions. Using a controlled experiment with 123 human forecasters and LLMs (ChatGPT-4, ChatGPT3.5, Bard, Bing, Llama2), the study evaluates forecasting accuracy through Mean Absolute Percentage Error (MAPE). Results show that LLMs do not consistently outperform humans; some LLMs like ChatGPT4 match human accuracy, while others (ChatGPT3.5, Llama2) perform worse. Neither humans nor LLMs show significant improvement with advanced forecasting models, and both struggle with increased errors during promotions and under positive external impacts.

## Method Summary
The study employs a controlled experiment comparing human forecasters (graduate students) with five LLMs in retail sales forecasting. The experimental design includes 24 sales series with promotion status (Yes/No), external impact (Positive/Negative), and two types of statistical forecasts (baseline and advanced). MAPE serves as the primary accuracy metric. Mixed-effects regression models analyze human forecaster data while separate regression models assess LLM performance. The study tests how different forecaster types perform across various conditions including baseline vs advanced forecasts, promotional vs non-promotional periods, and negative vs positive external impacts.

## Key Results
- LLMs show heterogeneous performance: ChatGPT-4 and Bing match human accuracy, while ChatGPT3.5 and Llama2 perform significantly worse
- Neither humans nor LLMs improve accuracy with advanced forecasting models
- Both human and LLM forecasters exhibit increased errors during promotional periods and positive external impacts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs vary in forecasting accuracy depending on model architecture and training data exposure to promotional dynamics.
- Mechanism: Different LLMs process statistical patterns and contextual cues (promotions, external impacts) differently based on their underlying architecture and training corpus, leading to heterogeneous MAPE performance.
- Core assumption: Training data diversity and model size determine the model's ability to generalize promotional forecasting scenarios.
- Evidence anchors:
  - [abstract] "Results show that LLMs do not consistently outperform humans; some LLMs like ChatGPT-4 match human accuracy, while others (ChatGPT3.5, Llama2) perform worse."
  - [section 4.1] "ChatGPT3.5 and Llama2 yielded significantly higher MAPE values (β = 19.28, p < 0.001 and β = 20.28, p < 0.001, respectively) compared to human forecasters"
  - [corpus] FMR scores indicate ChatGPT-4 related papers have higher relevance than Llama2 related papers
- Break condition: If training data lacks sufficient promotional examples, even advanced models will underperform.

### Mechanism 2
- Claim: Advanced statistical models don't uniformly improve LLM forecasting performance due to potential overfitting to historical patterns.
- Mechanism: LLMs may interpret advanced model inputs as rigid constraints rather than flexible guidance, leading to degraded performance when model complexity increases.
- Core assumption: LLMs process advanced statistical outputs differently than simple baselines, treating them as deterministic rather than probabilistic inputs.
- Evidence anchors:
  - [abstract] "Neither humans nor LLMs show significant improvement with advanced forecasting models"
  - [section 4.2] "advanced forecasts were found to significantly reduce accuracy for human forecasters (β = 1.98, p = 0.001) compared to baseline forecasts"
  - [section 4.2] "advanced forecasts did not yield any significant changes in forecast accuracy for each of the LLMs"
- Break condition: When advanced models incorporate too many variables, they may introduce noise that LLMs cannot effectively filter.

### Mechanism 3
- Claim: External impact valence (positive vs negative) affects LLM forecasting accuracy through pattern recognition biases in training data.
- Mechanism: LLMs trained on imbalanced datasets may overweight negative impact signals or underweight positive ones, leading to asymmetric performance across external conditions.
- Core assumption: Training corpus contains systematic bias toward negative external events, creating asymmetric pattern recognition.
- Evidence anchors:
  - [abstract] "both human and LLM forecasters exhibited increased forecasting errors, particularly during promotional periods and under the influence of positive external impacts"
  - [section 4.4] "the presence of positive external impact was associated with a significant increase in MAPE (β = 2.09, p = 0.002)"
  - [section 4.4] "ChatGPT3.5 (β = 18.46, p = 0.001) and Llama2 (β = 23.26, p < 0.001) demonstrated significant increases in MAPE compared to human forecasters"
- Break condition: If training data includes balanced representation of positive and negative external impacts.

## Foundational Learning

- Concept: Mixed-effects regression modeling
  - Why needed here: Handles correlated repeated-measures data across 24 time series per forecaster while accounting for individual forecaster variability
  - Quick check question: Why use restricted maximum likelihood instead of ordinary least squares for this dataset?

- Concept: Mean Absolute Percentage Error (MAPE) calculation
  - Why needed here: Standard metric for comparing forecasting accuracy across different scales and forecaster types
  - Quick check question: How does MAPE behave when actual sales values approach zero?

- Concept: Treatment condition design in forecasting experiments
  - Why needed here: Systematically varies promotion status, external impact, and forecast type to isolate their effects on accuracy
  - Quick check question: What is the purpose of having six treatment conditions rather than a full factorial design?

## Architecture Onboarding

- Component map: Experimental system -> R shiny dashboard -> Time series data input -> LLM prompt generation -> MAPE calculation -> Statistical analysis pipeline
- Critical path: Data preparation -> Forecaster execution (human/LLM) -> Result aggregation -> Statistical analysis -> Hypothesis testing
- Design tradeoffs: Human participants provide real-world judgment but introduce variability; LLMs provide consistency but may lack contextual understanding
- Failure signatures: High MAPE variance across forecasters indicates model instability; systematic bias in external impact conditions suggests training data imbalance
- First 3 experiments:
  1. Run baseline MAPE comparison between human forecasters and each LLM without advanced models
  2. Test LLM performance with only promotional data to assess promotion-specific capabilities
  3. Compare LLM performance when given different statistical model outputs (baseline vs advanced)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific characteristics of ChatGPT-4 and Bing enable them to match human forecasters' accuracy, while other LLMs like ChatGPT3.5 and Llama2 perform significantly worse?
- Basis in paper: [explicit] The study found that ChatGPT-4 and Bing did not show significant differences in MAPE compared to human forecasters, while ChatGPT3.5 and Llama2 yielded significantly higher MAPE values compared to humans.
- Why unresolved: The paper does not investigate the underlying reasons for the performance differences between LLMs, such as differences in training data, model architecture, or fine-tuning processes.
- What evidence would resolve it: Detailed analysis of the training data, model architectures, and fine-tuning processes of the different LLMs, along with experiments isolating the impact of these factors on forecasting accuracy.

### Open Question 2
- Question: How would experienced industry professionals perform compared to both human graduate students and LLMs in the same forecasting tasks?
- Basis in paper: [inferred] The study acknowledges that using graduate students as human forecasters is a limitation, as they may lack the extensive practical experience of seasoned industry professionals.
- Why unresolved: The paper only tested graduate students, not experienced professionals, leaving open the question of how professional expertise would impact forecasting accuracy relative to both students and LLMs.
- What evidence would resolve it: Replicating the experiment with a sample of experienced retail industry professionals and comparing their forecasting accuracy to both the graduate students and the LLMs.

### Open Question 3
- Question: What specific aspects of promotional periods make them particularly challenging for both human and AI forecasters, and how can forecasting models be improved to better handle these challenges?
- Basis in paper: [explicit] The study found that both human forecasters and LLMs exhibited increased forecasting errors during promotional periods, with some LLMs like Bard, ChatGPT3.5, and Llama2 showing significant performance declines.
- Why unresolved: While the paper identifies promotional periods as challenging, it does not delve into the specific factors that make promotions difficult to forecast or propose concrete improvements to forecasting models.
- What evidence would resolve it: In-depth analysis of the characteristics of promotional periods (e.g., consumer behavior changes, competitor responses) and experimental testing of enhanced forecasting models designed to better capture these dynamics.

## Limitations

- The exact prompting methodology for LLMs remains underspecified, limiting reproducibility of the study
- The controlled experimental conditions may not fully capture real-world forecasting complexity where forecasters can iteratively refine predictions
- The study uses graduate students as human forecasters, potentially limiting generalizability to experienced industry professionals

## Confidence

- **High confidence**: MAPE comparisons across forecaster types under controlled conditions; statistical significance of promotion and external impact effects on forecasting accuracy
- **Medium confidence**: Generalizability of findings to real-world forecasting scenarios; interpretation of why specific LLMs perform differently
- **Low confidence**: Claims about training data bias causing asymmetric performance on positive vs negative external impacts; mechanism explanations for advanced model performance degradation

## Next Checks

1. Replicate the experiment with identical prompts across all LLM versions to isolate model-specific effects from prompt engineering variations
2. Test LLM performance on a balanced dataset containing equal positive and negative external impact examples to verify the asymmetric performance hypothesis
3. Conduct ablation studies removing individual statistical model components to determine which features LLMs struggle to process effectively