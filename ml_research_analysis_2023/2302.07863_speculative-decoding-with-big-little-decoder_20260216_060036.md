---
ver: rpa2
title: Speculative Decoding with Big Little Decoder
arxiv_id: '2302.07863'
source_url: https://arxiv.org/abs/2302.07863
tags:
- large
- small
- arxiv
- latency
- bild
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of long inference latency in large
  language models used for autoregressive text generation tasks. The proposed Big
  Little Decoder (BiLD) framework improves inference efficiency by using a small model
  to generate most text autoregressively, with a large model invoked occasionally
  to refine inaccurate predictions in a non-autoregressive manner.
---

# Speculative Decoding with Big Little Decoder

## Quick Facts
- arXiv ID: 2302.07863
- Source URL: https://arxiv.org/abs/2302.07863
- Reference count: 40
- Primary result: Up to 2.38x speedup with minimal quality degradation using small model autoregressive generation with occasional large model refinement

## Executive Summary
This paper addresses the problem of long inference latency in large language models by proposing Big Little Decoder (BiLD), a framework that uses a small model for most text generation and invokes a large model only when necessary. The approach employs two policies: a fallback policy that hands control to the large model when the small model lacks confidence, and a rollback policy that corrects the small model's inaccurate predictions in a non-autoregressive manner. Evaluated across machine translation, summarization, and language modeling tasks, BiLD achieves significant speedup (up to 2.38x) while maintaining quality, and is fully plug-and-play without requiring model training or modifications.

## Method Summary
BiLD uses a small decoder model to generate text autoregressively, with a large decoder model invoked occasionally to refine inaccurate predictions non-autoregressively. The fallback policy triggers the large model when the small model's confidence (measured by maximum prediction probability) falls below a threshold. The rollback policy allows the large model to correct earlier mistakes by generating predictions for all previous tokens in parallel and replacing the small model's sequence when cross-entropy distance exceeds a threshold. The framework is evaluated on mT5 and T5 models for translation and summarization, and GPT-2 models for language modeling, achieving substantial latency reduction with minimal quality degradation.

## Key Results
- Achieves up to 2.13x speedup on machine translation and summarization with minimal quality degradation
- Reaches up to 2.38x speedup on language modeling with approximately 1-point degradation
- Successfully reduces inference cost while maintaining BLEU, ROUGE-L, and perplexity scores close to large model baselines
- Demonstrates effectiveness across three diverse tasks: machine translation, summarization, and language modeling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The small model generates most tokens autoregressively, reducing inference cost because the small model has significantly fewer parameters than the large model.
- Mechanism: The small model runs token-by-token generation, leveraging its lightweight architecture to minimize computation per step. Only when its confidence (measured by max prediction probability) falls below a threshold does control transfer to the large model, which then validates or corrects predictions in a non-autoregressive manner.
- Core assumption: The small model's predictions are correct often enough that the overhead of running it frequently is outweighed by the savings from not running the large model on every step.
- Evidence anchors:
  - [abstract]: "The small model runs autoregressively to generate text with a low inference cost, and the large model is only invoked occasionally..."
  - [section 3.1]: "...a small model with an order of magnitude smaller size than a large model can achieve comparable generation quality to the large model‚Äîprovided that a small number of erroneous predictions are corrected."
  - [corpus]: Weak evidence. No explicit studies cited in corpus comparing parameter counts and runtime directly.

### Mechanism 2
- Claim: The fallback policy triggers the large model only when the small model's prediction probability is low, avoiding unnecessary large model calls.
- Mechanism: After each small model prediction, the system checks if max(ùëùùëÜ(ùë¶|ùë¶1:ùëõ‚àí1)) < ùõºùêπùêµ. If so, it hands control to the large model; otherwise, it continues with the small model. This keeps the large model involvement minimal and only for uncertain predictions.
- Core assumption: Low maximum prediction probability correlates with prediction error, so invoking the large model only in these cases is effective.
- Evidence anchors:
  - [section 3.3]: "we find that a simple policy based on the maximum prediction probability, i.e., maxùë¶ ùëùùëÜ(ùë¶|ùë¶1:ùëõ‚àí1), is sufficient to measure the confidence..."
  - [section 3.1]: "...we measure the likelihood of the large model predicting the token that the small model has generated. If the likelihood is below a certain threshold, we assume that the small model's prediction is unlikely and not accurate enough..."
  - [corpus]: No direct evidence. Assumption that max probability is a reliable confidence metric is not experimentally validated in corpus.

### Mechanism 3
- Claim: The rollback policy allows the large model to correct earlier mistakes made by the small model without extra latency cost.
- Mechanism: When the large model is invoked non-autoregressively, it generates its own predictions for all previous tokens in parallel. If any prediction differs from the small model's by more than ùõºùëÖùêµ (measured by cross-entropy), the small model's sequence from that point onward is rolled back and replaced with the large model's prediction.
- Core assumption: The large model's predictions for previous tokens are available "for free" during its non-autoregressive generation of the current token, so checking and correcting them adds negligible overhead.
- Evidence anchors:
  - [section 3.4]: "When the large model is provided with the tokens generated by the small model for its non-autoregressive prediction of the next token, it also produces its own predictions for all the previous decoding steps."
  - [section 4.2.1]: "We further analyze the latency of processing a single input using BiLD compared to the baseline large models...only 2.3% and 2.7% of the total examples for each task."
  - [corpus]: No explicit experimental evidence that cross-entropy between hard and soft labels is optimal; assumption stated but not justified.

## Foundational Learning

- Concept: Autoregressive vs non-autoregressive decoding
  - Why needed here: The paper's speedup hinges on running the small model autoregressively (low cost per step) and the large model non-autoregressively (parallel token generation).
  - Quick check question: In autoregressive decoding, can you generate token n+1 without first generating token n? (Answer: No)

- Concept: Confidence scoring via max probability
  - Why needed here: The fallback policy uses max prediction probability to decide when to hand over to the large model.
  - Quick check question: If a model assigns 0.95 probability to token "A" and 0.05 to token "B", what is its confidence score? (Answer: 0.95)

- Concept: Cross-entropy as a distance metric between probability distributions
  - Why needed here: The rollback policy uses cross-entropy between the small model's hard prediction and the large model's soft distribution to detect prediction errors.
  - Quick check question: If the small model predicts token "A" with probability 1.0, and the large model assigns it 0.6, what is the cross-entropy? (Answer: -log(0.6) ‚âà 0.51 nats)

## Architecture Onboarding

- Component map: Small decoder model -> Fallback policy module -> Large decoder model -> Rollback policy module -> Inference orchestrator

- Critical path:
  1. Small model generates token with autoregression
  2. Confidence check (max probability vs ùõºùêπùêµ)
  3. If confident: output token; if not: invoke large model
  4. Large model generates predictions for current and all previous tokens in parallel
  5. Rollback check (cross-entropy vs ùõºùëÖùêµ)
  6. If rollback needed: replace sequence from rollback point with large model's predictions
  7. Continue generation

- Design tradeoffs:
  - Small model size vs accuracy: Smaller models are faster but less accurate, requiring more fallback/rollback.
  - Fallback threshold ùõºùêπùêµ: Higher values reduce large model calls but risk more errors; lower values increase calls but improve quality.
  - Rollback threshold ùõºùëÖùêµ: Higher values reduce rollback frequency (less latency cost) but may miss errors; lower values catch more errors but increase rollback overhead.

- Failure signatures:
  - High fallback frequency: Indicates small model confidence threshold is too low or small model is too inaccurate.
  - High rollback frequency: Indicates large model disagrees often with small model, suggesting poor alignment or threshold too low.
  - Minimal latency improvement: Could mean small model is too large, fallback/rollback thresholds are poorly tuned, or hardware doesn't benefit from parallel large model execution.

- First 3 experiments:
  1. Measure baseline latency and accuracy of small and large models alone on a validation set.
  2. Sweep fallback threshold ùõºùêπùêµ and measure resulting latency/accuracy tradeoff curve.
  3. Enable rollback with default threshold and measure improvement in accuracy and any latency impact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance-latency tradeoff of BiLD vary with different choices of the distance metric d in the rollback policy?
- Basis in paper: [explicit] The paper mentions using cross-entropy loss between small and large model outputs as the default metric, but notes that other metrics could be explored.
- Why unresolved: The paper does not empirically compare different distance metrics or analyze their impact on the performance-latency tradeoff.
- What evidence would resolve it: Empirical comparison of BiLD performance using different distance metrics (e.g., KL divergence, L2 distance) across multiple tasks, showing which metrics provide the best tradeoff.

### Open Question 2
- Question: Can BiLD's fallback and rollback thresholds be dynamically adjusted during inference based on input characteristics rather than being fixed per model?
- Basis in paper: [inferred] The paper uses fixed thresholds selected from predetermined sets, suggesting potential for more adaptive approaches that could better handle varying input difficulty.
- Why unresolved: The paper evaluates static threshold selection but does not explore dynamic threshold adjustment based on input properties or model confidence patterns.
- What evidence would resolve it: Experimental results comparing fixed vs. dynamic threshold strategies, demonstrating whether input-adaptive thresholds improve the performance-latency tradeoff.

### Open Question 3
- Question: How does BiLD scale to extremely large language models (e.g., 100B+ parameters) where memory constraints become critical?
- Basis in paper: [inferred] The paper evaluates models up to ~1B parameters and discusses inference latency but does not address memory-constrained scenarios typical of very large models.
- Why unresolved: Memory efficiency and out-of-core execution strategies are not discussed, despite being crucial for deploying the largest LLMs.
- What evidence would resolve it: Performance analysis of BiLD with models exceeding 10B parameters, including memory usage patterns, speedup characteristics, and quality degradation compared to autoregressive decoding.

## Limitations

- Hardware dependency: Speedup measurements are specific to NVIDIA Titan Xp GPU and may not translate to other hardware configurations
- Threshold sensitivity: Fallback and rollback thresholds require careful tuning per task and show high sensitivity to incorrect values
- Task generalization: Effectiveness on reasoning-intensive tasks beyond pattern-matching generation is not evaluated

## Confidence

- High Confidence: The core architectural design of BiLD (small model autoregressive generation + occasional large model non-autoregressive refinement) is well-justified and empirically validated.
- Medium Confidence: The specific threshold values for fallback and rollback policies are task-dependent and validated on tested benchmarks, but lack systematic selection methodology.
- Low Confidence: The assumption that max prediction probability reliably indicates prediction accuracy across all tasks and model sizes is weakly supported.

## Next Checks

1. **Cross-Hardware Validation**: Reproduce the latency measurements on at least two additional hardware configurations (e.g., RTX 4090 and Apple M-series CPU) to verify the hardware dependency of speedup claims. Measure whether the 2.13x-2.38x improvement holds across different inference environments.

2. **Confidence Metric Ablation Study**: Systematically compare max prediction probability against alternative confidence metrics (entropy, prediction margin, ensemble agreement) for the fallback policy. Measure which metric provides the best tradeoff between accuracy preservation and latency reduction across all three task types.

3. **Threshold Selection Framework**: Develop and validate a systematic method for selecting Œ±FB and Œ±RB thresholds based on small model accuracy statistics and large model disagreement rates, rather than task-specific tuning. Test whether this framework generalizes to new tasks without per-task hyperparameter search.