---
ver: rpa2
title: Learning to Plan with Natural Language
arxiv_id: '2304.10464'
source_url: https://arxiv.org/abs/2304.10464
tags:
- program
- language
- natural
- task
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for learning to program with natural
  language to guide large language models (LLMs) in solving complex tasks. The approach
  involves generating and iteratively refining natural language programs based on
  training data, which then direct the LLM in making predictions.
---

# Learning to Plan with Natural Language

## Quick Facts
- arXiv ID: 2304.10464
- Source URL: https://arxiv.org/abs/2304.10464
- Authors: 
- Reference count: 40
- Key outcome: Natural language programs learned through iterative refinement improve LLM mathematical reasoning by up to 18.3% in zero-shot settings

## Executive Summary
This paper introduces a method for learning to program with natural language to guide large language models (LLMs) in solving complex tasks. The approach involves generating and iteratively refining natural language programs based on training data, which then direct the LLM in making predictions. The method was evaluated on five mathematical reasoning tasks from the AMPS and Math datasets, showing significant performance improvements—up to 18.3% in zero-shot settings and 7% in few-shot settings—compared to standard chain-of-thought prompting. The learned programs also demonstrated strong interpretability and transferability across different LLMs. The approach offers a promising direction for enhancing LLM reasoning capabilities without requiring model parameter updates.

## Method Summary
The Learning to Program (LP) method involves two phases: (1) Learning task plan phase where LLMs iteratively update the task plan based on training error feedback, and (2) Test phase where the learned task plan guides LLM inference on the test set. The method uses natural language programs that encode task-specific reasoning patterns and background knowledge, which are refined through error revision generation and compression. The approach was evaluated on mathematical reasoning tasks from the AMPS and Math datasets, with training and test splits in a 3:1 ratio.

## Key Results
- 18.3% improvement in zero-shot settings compared to direct chain-of-thought prompting
- 7% improvement in few-shot settings over standard approaches
- Strong interpretability and transferability of learned programs across different LLMs
- Effective performance on 10 tasks from AMPS dataset and 7 tasks from Math dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The learned natural language program provides generalizable step-by-step solutions for all similar questions, improving zero-shot performance by 18.3% over direct chain-of-thought prompting.
- Mechanism: The LP method iteratively updates a natural language program by extracting solutions from training errors, creating a program that encodes task-specific reasoning patterns and background knowledge.
- Core assumption: LLMs can generate correct revisions from error cases that improve the program quality when iteratively applied.
- Evidence anchors:
  - [abstract]: "The method involves generating and iteratively refining natural language programs based on training data, which then direct the LLM in making predictions."
  - [section]: "The Learning to Program (LP) method, depicted in Figure 1, is inspired by the traditional deep learning process... instead of learning the vector weight, LP learns program p as text."
  - [corpus]: Weak evidence - neighboring papers focus on different approaches (PDDL integration, AutoML agents) without direct comparison to LP method.
- Break condition: If error revision generation fails to produce correct solutions, or if the program becomes too long/inefficient for the LLM to process.

### Mechanism 2
- Claim: Learning natural language programs enables transfer learning across different LLMs, allowing one model's learned program to guide another model's inference.
- Mechanism: Natural language programs are interpretable text that can be read and executed by any LLM with similar capabilities, creating a transfer learning paradigm without model parameter updates.
- Core assumption: Different LLMs can interpret the same natural language program similarly and apply the same reasoning steps.
- Evidence anchors:
  - [abstract]: "The learned programs also demonstrated strong interpretability and transferability across different LLMs."
  - [section]: "Further, our analysis experiment shows that the task plan learned by one LLM can directly guide another LLM to improve its performance."
  - [corpus]: No direct evidence in corpus - this is a novel contribution not demonstrated in neighboring papers.
- Break condition: If target LLM cannot interpret the program format or lacks required background knowledge to execute the steps.

### Mechanism 3
- Claim: Program compression maintains essential information while reducing token count, preventing input overflow and maintaining inference quality.
- Mechanism: After each revision, the program is summarized to remove redundant information and trivial details while preserving core solutions, verified against a validation set.
- Core assumption: Summarization can preserve program effectiveness while reducing length, and validation performance reliably indicates program quality.
- Evidence anchors:
  - [section]: "To avoid these problems, we propose to use summarization to compress the information while maintaining its essential information."
  - [section]: "If the updated performance is better than the recent average recorded performance by a threshold, we maintain this candidate."
  - [corpus]: No direct evidence - compression approach appears unique to this work.
- Break condition: If compression removes critical information needed for task completion, or if validation performance becomes unreliable.

## Foundational Learning

- Concept: Iterative refinement through error feedback
  - Why needed here: The method learns from mistakes rather than starting with perfect knowledge, allowing programs to improve from flawed initial generations
  - Quick check question: What happens to program quality if the error revision strategy fails to generate correct solutions?

- Concept: Validation-based verification of program updates
  - Why needed here: Ensures that program revisions actually improve performance rather than degrading it, maintaining quality control
  - Quick check question: How would you modify the threshold if validation performance improvement becomes too strict?

- Concept: Program compression to manage token limits
  - Why needed here: Prevents input overflow while maintaining program effectiveness as the program grows with each revision
  - Quick check question: What metrics would you track to ensure compression doesn't degrade performance?

## Architecture Onboarding

- Component map: Error collection -> Revision generation -> Compression -> Validation -> Program update (training); Learned program -> Guided inference (testing)
- Critical path: Error collection -> Revision generation -> Validation -> Program update (this loop determines program quality)
- Design tradeoffs: Longer programs provide more guidance but risk token overflow; more validation samples improve reliability but increase computational cost
- Failure signatures: Program growth without validation improvement (overfitting), compressed programs that lose effectiveness, transfer failures across LLMs
- First 3 experiments:
  1. Run LP method on single AMPS task (e.g., Task 1) with debug logging to verify each pipeline component
  2. Test program transfer by learning with ChatGPT and applying to another LLM
  3. Vary compression threshold and measure impact on performance and token usage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the learned natural language program generalize to more complex mathematical tasks beyond the AMPS and Math datasets?
- Basis in paper: [explicit] The paper demonstrates effectiveness on 10 tasks from the AMPS dataset and 7 tasks from the Math dataset, but does not explore generalization to other mathematical domains or more complex problem types.
- Why unresolved: The experiments are limited to specific datasets, and the paper does not provide evidence of the method's effectiveness on tasks with higher complexity or different mathematical domains.
- What evidence would resolve it: Testing the LP method on a broader range of mathematical tasks, including those with higher complexity or from different domains, would demonstrate its generalizability.

### Open Question 2
- Question: What is the impact of the learned natural language program on the interpretability and debuggability of LLM solutions?
- Basis in paper: [explicit] The paper highlights that natural language programs offer superior interpretability compared to generated programs, but does not provide a quantitative analysis of how this impacts the ability to debug or understand LLM solutions.
- Why unresolved: While the paper claims improved interpretability, it lacks a systematic evaluation of how the learned programs aid in debugging or understanding LLM outputs.
- What evidence would resolve it: Conducting user studies or experiments where participants debug or interpret LLM solutions with and without the guidance of learned programs would quantify the interpretability benefits.

### Open Question 3
- Question: How does the Learning to Program method compare to other approaches for improving LLM performance on complex tasks, such as fine-tuning or retrieval-augmented generation?
- Basis in paper: [inferred] The paper focuses on the LP method but does not compare its effectiveness to other techniques like fine-tuning or retrieval-augmented generation for improving LLM performance on complex tasks.
- Why unresolved: The paper does not provide a comparative analysis with other state-of-the-art methods, leaving the relative effectiveness of LP unclear.
- What evidence would resolve it: Conducting experiments that compare the LP method to fine-tuning or retrieval-augmented generation on the same tasks would provide insights into its relative performance.

## Limitations

- Method is evaluated only on mathematical reasoning tasks, limiting generalizability to other domains
- Performance depends on the quality of error revision generation, which may fail for complex tasks
- Compression mechanism effectiveness and optimal parameters are not thoroughly validated

## Confidence

**High Confidence**: The core mechanism of learning natural language programs through iterative refinement is well-supported by the methodology description and experimental results showing 18.3% zero-shot improvement. The transfer learning claims are backed by demonstrated experiments across different LLMs.

**Medium Confidence**: The compression mechanism's effectiveness and the validation threshold selection are described but lack comprehensive ablation studies. The generalizability to non-mathematical tasks remains unproven despite theoretical claims.

**Low Confidence**: The long-term stability of the iterative refinement process and the method's behavior with extremely complex tasks requiring hundreds of reasoning steps are not thoroughly explored.

## Next Checks

1. **Cross-domain validation**: Test the LP method on non-mathematical tasks (e.g., text summarization or commonsense reasoning) to verify domain transferability beyond the reported mathematical tasks.

2. **Compression ablation study**: Systematically vary compression thresholds and measure their impact on both program effectiveness and token usage across different task complexities to establish optimal parameters.

3. **Iterative stability analysis**: Track program quality over 50+ refinement iterations on a single task to identify potential degradation patterns or convergence issues that may not appear in shorter experiments.