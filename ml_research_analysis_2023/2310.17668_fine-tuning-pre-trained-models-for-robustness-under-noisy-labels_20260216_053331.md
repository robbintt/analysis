---
ver: rpa2
title: Fine tuning Pre trained Models for Robustness Under Noisy Labels
arxiv_id: '2310.17668'
source_url: https://arxiv.org/abs/2310.17668
tags:
- noisy
- labels
- dataset
- learning
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of training deep learning models
  with noisy labels, which can significantly degrade performance. The authors propose
  TURN, a two-step fine-tuning method that leverages pre-trained models to robustly
  adapt to target datasets with noisy labels.
---

# Fine tuning Pre trained Models for Robustness Under Noisy Labels

## Quick Facts
- arXiv ID: 2310.17668
- Source URL: https://arxiv.org/abs/2310.17668
- Reference count: 40
- Key outcome: TURN achieves state-of-the-art denoising performance on various benchmarks while being more computationally efficient than existing methods.

## Executive Summary
This paper addresses the challenge of training deep learning models with noisy labels by proposing TURN, a two-step fine-tuning method that leverages pre-trained models. TURN consists of linear probing (LP) to protect the feature extractor from noisy supervision, followed by fine-tuning (FFT) on a cleansed subset identified via GMM clustering. The method demonstrates superior performance across multiple benchmarks while maintaining computational efficiency.

## Method Summary
TURN is a two-step fine-tuning method for pre-trained models under noisy labels. First, linear probing trains only the classifier while freezing the feature extractor, protecting it from distortion by noisy labels. Second, a GMM-based approach cleanses the dataset by identifying clean samples through loss clustering, and the entire model is fine-tuned on this cleaned subset. This approach achieves state-of-the-art denoising performance while being computationally efficient.

## Key Results
- TURN achieves state-of-the-art denoising performance on CIFAR-100, Clothing1M, and WebVision benchmarks
- The method demonstrates superior computational efficiency compared to existing approaches
- TURN maintains robust performance across various noise levels (40%-90% symmetric noise)
- The two-step approach effectively protects feature extractors while enabling task adaptation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The two-step LP-then-FFT sequence prevents feature extractor degradation under high noise while enabling task adaptation under low noise.
- **Mechanism**: LP freezes the feature extractor and only updates the classifier, protecting it from noisy supervision. This trained classifier then identifies clean samples via GMM loss clustering, which are used in FFT to adapt the feature extractor in a less noisy environment.
- **Core assumption**: Feature extractors from large PTMs are robust enough that linear probing can still separate clean from noisy samples even under severe noise.
- **Evidence anchors**:
  - [abstract] "The algorithm consists of two main steps: (1) independently tuning the linear classifier to protect the feature extractor from being distorted by noisy labels, and (2) reducing the noisy label ratio and fine-tuning the entire model..."
  - [section] "The first step of the algorithm serves two main objectives. Firstly, it aims to obtain a classifier that can effectively detect noisy labels... Secondly, during the training of the classifier for detecting noisy labels, it is crucial to ensure that the feature extractor is not distorted by the noisy labels."
  - [corpus] Weak – no direct citation in corpus about two-step protection, only general CLIP robustness papers.
- **Break condition**: If the feature extractor itself is too noisy (e.g., from a small or weakly pre-trained model), LP cannot reliably distinguish clean from noisy samples.

### Mechanism 2
- **Claim**: GMM-based clustering on per-sample loss effectively separates clean from noisy labels.
- **Mechanism**: After LP, each sample's loss is computed. GMM fits two Gaussians per class (low-mean for clean, high-mean for noisy). Samples with loss likelihood above threshold τ are considered clean.
- **Core assumption**: Clean samples have consistently lower loss values than noisy ones due to better alignment between features and true labels.
- **Evidence anchors**:
  - [section] "We utilize a clustering algorithm based on the Gaussian Mixture Model (GMM)... Utilizing these per-class distributions, we construct the clean dataset using the following procedure: Dclean = ∪c U(Dclean_c, N) where Dclean_c = {(xi, yi)|p_lc(ℓi) > τ where yi = c}."
  - [abstract] "...reducing the noisy label ratio and fine-tuning the entire model based on the noise-reduced dataset..."
  - [corpus] Weak – no corpus evidence for GMM-based label cleaning in PTMs specifically.
- **Break condition**: If noise is instance-dependent or highly asymmetric, loss distributions may overlap heavily, reducing GMM effectiveness.

### Mechanism 3
- **Claim**: Pre-trained feature extractors generalize well enough that even a small cleaned subset suffices for effective FFT adaptation.
- **Mechanism**: TURN discards most noisy samples, trains FFT on a subset of high-confidence clean samples. PTM generalization ensures this small set yields strong task performance.
- **Core assumption**: The prior knowledge encoded in PTMs is transferable and robust, enabling few-shot clean-sample adaptation.
- **Evidence anchors**:
  - [abstract] "This ability is attributed to their robust feature extraction capabilities. Therefore, leveraging PTMs in LNL methods can contribute to improving generalization performance within a few epochs..."
  - [section] "As mentioned earlier, PTMs are highly capable of adapting to clean datasets using few-shot learning techniques."
  - [corpus] Weak – no direct corpus support for few-shot clean-sample adaptation in noisy label setting.
- **Break condition**: If the clean subset is too small or unrepresentative, adaptation fails despite PTM generalization.

## Foundational Learning

- **Concept**: Gaussian Mixture Model clustering on loss distributions
  - **Why needed here**: To probabilistically separate clean from noisy samples after LP, using per-sample loss as a signal.
  - **Quick check question**: What assumption about loss distributions justifies using two Gaussians per class?
- **Concept**: Linear probing (LP) vs full fine-tuning (FFT)
  - **Why needed here**: LP protects feature extractors from noisy supervision; FFT adapts them once noise is reduced.
  - **Quick check question**: Why does FFT under severe noise degrade the feature extractor while LP does not?
- **Concept**: Generalized Cross Entropy (GCE) loss
  - **Why needed here**: Reduces influence of noisy labels during LP training of the classifier.
  - **Quick check question**: How does the q parameter in GCE affect sensitivity to noisy labels?

## Architecture Onboarding

- **Component map**: Input → Pre-trained feature extractor (frozen during LP) → Linear classifier (trained during LP) → Per-sample loss computation → GMM-based clean subset selection → FFT on clean subset → Output
- **Critical path**: LP → GMM clean selection → FFT on clean subset. Any failure in clean selection directly degrades FFT performance.
- **Design tradeoffs**:
  - Freezing feature extractor during LP preserves quality but limits initial adaptation
  - Aggressive GMM threshold yields cleaner subsets but may discard useful data
  - Fewer FFT epochs saves compute but risks under-adaptation
- **Failure signatures**:
  - LP accuracy near random → feature extractor too noisy for clean detection
  - GMM yields very small clean set → aggressive threshold or high noise overlap
  - FFT performance drops vs LP → clean set too small or unrepresentative
- **First 3 experiments**:
  1. Run LP on CIFAR-100 with 90% symmetric noise; check clean/noisy separation accuracy
  2. Vary GMM threshold τ; measure clean set purity and size
  3. Compare FFT on full noisy set vs FFT on GMM-cleaned subset; measure accuracy gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TURN perform on other types of noisy labels beyond the ones explored in the paper (symmetric, asymmetric, instance-dependent)?
- Basis in paper: [explicit] The paper evaluates TURN on symmetric, asymmetric, and instance-dependent noise on CIFAR-100, but does not explore other types of noise.
- Why unresolved: The paper does not provide any analysis or results on TURN's performance with other types of noisy labels.
- What evidence would resolve it: Additional experiments on other types of noisy labels, such as class-dependent noise or real-world noise patterns, would provide insights into TURN's generalizability.

### Open Question 2
- Question: What is the impact of the GMM threshold τ on TURN's performance for different noise levels and datasets?
- Basis in paper: [explicit] The paper uses a fixed GMM threshold of τ = 0.6 for all experiments, but mentions that the threshold can be tuned.
- Why unresolved: The paper does not provide a systematic analysis of how the GMM threshold affects TURN's performance across different noise levels and datasets.
- What evidence would resolve it: A sensitivity analysis of TURN's performance with varying GMM thresholds for different noise levels and datasets would provide insights into the optimal threshold selection.

### Open Question 3
- Question: How does TURN compare to other methods for learning with noisy labels in terms of robustness to different noise levels and datasets?
- Basis in paper: [explicit] The paper compares TURN to several baseline methods on CIFAR-100, Clothing1M, and WebVision datasets with different noise levels, but does not provide a comprehensive comparison across all scenarios.
- Why unresolved: The paper does not provide a detailed analysis of TURN's performance relative to other methods in different noise scenarios and datasets.
- What evidence would resolve it: A thorough comparison of TURN with other methods on a wide range of datasets and noise levels, including statistical significance tests, would provide a more comprehensive understanding of TURN's strengths and weaknesses.

## Limitations
- The paper lacks detailed implementation specifications for the GMM-based clean sample selection process, particularly regarding the thresholding strategy and sampling methodology
- Computational efficiency claims relative to existing methods are not fully substantiated with runtime comparisons across different dataset sizes and noise levels
- Theoretical justification for why linear probing effectively protects feature extractors from noisy label corruption is primarily empirical rather than grounded in formal analysis

## Confidence
- High confidence in the core two-step LP-then-FFT mechanism for protecting feature extractors and enabling adaptation on cleaned subsets
- Medium confidence in the effectiveness of GMM clustering for clean sample identification, as the method relies on assumptions about loss distribution separation that may not hold for all noise types
- Low confidence in the claim about "few-shot clean-sample adaptation" being broadly generalizable across all PTM architectures and dataset domains

## Next Checks
1. **Clean set purity validation**: After GMM clustering, calculate the actual noise ratio in the selected clean subset across different noise levels (40%, 60%, 80%, 90%) to verify the effectiveness of the cleaning process.
2. **Feature extractor sensitivity analysis**: Compare t-SNE visualizations of features before LP, after LP, and after FFT on the clean subset to empirically demonstrate the protection mechanism.
3. **Threshold sensitivity study**: Systematically vary the GMM threshold τ across [0.4, 0.5, 0.6, 0.7] and measure the trade-off between clean set size and final model performance to identify optimal operating points.