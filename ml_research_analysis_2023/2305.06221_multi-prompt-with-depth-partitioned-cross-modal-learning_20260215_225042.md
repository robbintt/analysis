---
ver: rpa2
title: Multi-Prompt with Depth Partitioned Cross-Modal Learning
arxiv_id: '2305.06221'
source_url: https://arxiv.org/abs/2305.06221
tags:
- prompt
- pmpo
- prompts
- learning
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the limitation of single-prompt learning in
  large-scale vision-language models, which struggle to capture diverse attributes
  of categories. The authors propose PMPO, a multi-modal prompting technique that
  extends soft prompts to multiple prompts by partitioning the visual encoder depths
  and connecting learnable prompts to separated visual depths.
---

# Multi-Prompt with Depth Partitioned Cross-Modal Learning

## Quick Facts
- arXiv ID: 2305.06221
- Source URL: https://arxiv.org/abs/2305.06221
- Authors: 
- Reference count: 40
- Primary result: PMPO achieves 79.28 harmonic mean averaged over 11 diverse image recognition datasets

## Executive Summary
This work addresses the limitation of single-prompt learning in large-scale vision-language models, which struggle to capture diverse attributes of categories. The authors propose PMPO, a multi-modal prompting technique that extends soft prompts to multiple prompts by partitioning the visual encoder depths and connecting learnable prompts to separated visual depths. This enables different prompts to capture the hierarchical contextual depths of visual representations. PMPO achieves a 79.28 harmonic mean, averaged over 11 diverse image recognition datasets, demonstrating significant competitiveness compared to state-of-the-art prompting methods.

## Method Summary
PMPO extends soft prompt tuning by using multiple learnable prompts, each connected to a specific depth range of the visual transformer encoder. The method partitions the visual encoder depths and maps each learnable prompt to different levels of image encoder blocks via linear projections. The final text embedding is computed as the mean of all prompt embeddings plus a manually designed prompt. The model uses a ViT-B/16 backbone, trains with 16-shot samples per dataset, and employs SGD optimizer with learning rate 0.002 (0.01 for ImageNet), batch size 8 (32 for ImageNet), and 6 epochs (2 for domain generalization) with cosine annealing learning rate decay.

## Key Results
- PMPO achieves 79.28 harmonic mean averaged over 11 diverse image recognition datasets
- Outperforms state-of-the-art prompting methods including CLIP, CoOp, CoCoOp, MaPLe, and KgCoOp
- Demonstrates strong performance in both base accuracy and new class generalization tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Partitioned depth assignments force prompts to specialize on distinct hierarchical visual features
- Mechanism: By assigning each learnable prompt to a fixed range of transformer layers, the model prevents all prompts from converging to the same trivial solution
- Core assumption: Visual transformer layers capture progressively abstract visual representations
- Evidence anchors: [abstract] "partitions the visual encoder depths and connects learnable prompts to the separated visual depths"
- Break condition: If visual features at different depths are too similar, prompts may still converge trivially

### Mechanism 2
- Claim: Ensemble of multiple prompts plus a manually designed prompt improves both base and new class generalization
- Mechanism: The final text embedding is the mean of learnable prompts and a hand-crafted prompt
- Core assumption: Manual prompts encode robust, generalizable language priors
- Evidence anchors: [abstract] "incorporate prior information from manually designed templates"
- Break condition: If the manual prompt is poorly aligned with target tasks

### Mechanism 3
- Claim: Multi-prompt setup enables richer semantic descriptions per class
- Mechanism: Different prompts can encode distinct attribute descriptions (e.g., color vs. shape)
- Core assumption: Classes have multiple discriminative attributes
- Evidence anchors: [abstract] "describing diverse characteristics of each category may necessitate multiple descriptions"
- Break condition: If prompts learn redundant descriptions

## Foundational Learning

- Concept: **Vision-language pre-trained models (VLMs)** like CLIP learn joint embeddings from image-text pairs using contrastive objectives
  - Why needed here: PMPO builds on frozen CLIP encoders; understanding their frozen, contrastive setup is essential
  - Quick check question: In CLIP, what loss drives the image and text encoders to align embeddings of matching pairs?

- Concept: **Prompt learning / soft prompts** replaces hand-crafted text templates with learnable token vectors
  - Why needed here: PMPO extends this idea from a single prompt to multiple prompts
  - Quick check question: In standard soft prompt tuning, what remains frozen while the prompt tokens are updated?

- Concept: **Transformer layer depth and hierarchical feature abstraction** in vision transformers
  - Why needed here: PMPO assigns prompts to specific depth ranges
  - Quick check question: In a ViT, which layers typically encode object shapes vs. fine-grained textures?

## Architecture Onboarding

- Component map: Text encoder (frozen CLIP) -> Learnable prompts (multiple sets) -> Visual encoder (partitioned ViT blocks) -> Deep Visual Prompt Tuning (D-VPT) -> Ensemble (mean of prompt embeddings + manual prompt) -> Output (class probabilities)

- Critical path:
  1. Initialize multiple learnable prompts
  2. Map each prompt to assigned transformer depth via linear layer
  3. Forward pass: prompt → text embedding, visual → partitioned features, compute similarity
  4. Compute mean of prompt embeddings + manual prompt
  5. Apply similarity with visual embeddings → class scores
  6. Backpropagate only through prompt parameters

- Design tradeoffs:
  - More prompts → better base accuracy but higher GPU memory and risk of overfitting
  - Deeper depth partitions → richer feature interaction but more computation and potential redundancy
  - Manual prompt inclusion → better generalization but adds manual design burden

- Failure signatures:
  - All prompts collapse to identical outputs
  - Poor new-class performance despite good base accuracy
  - High GPU memory usage

- First 3 experiments:
  1. Ablate single vs. multi-prompt without depth partition
  2. Vary number of prompts to find optimal count
  3. Cross-dataset transfer (train on ImageNet, test on Caltech101/StanfordCars)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of prompts impact computational efficiency during inference?
- Basis: [explicit] PMPO demands more GPU memory than single-prompt learning
- Why unresolved: No specific strategies to mitigate computational burden are provided
- What evidence would resolve it: Experiments comparing inference times with different prompt counts and proposed mitigation strategies

### Open Question 2
- Question: How does PMPO perform in extremely few-shot scenarios (1-4 shots)?
- Basis: [explicit] PMPO requires more shots for training than other methods
- Why unresolved: No experiments with extremely low-shot settings or specific data augmentation techniques
- What evidence would resolve it: Experiments evaluating 1-4 shot performance with various data augmentation techniques

### Open Question 3
- Question: What is the optimal depth of partitioning for the visual encoder?
- Basis: [explicit] Default bridge depth is set to 12 but optimal configuration is not analyzed
- Why unresolved: No exploration of how varying depth impacts performance
- What evidence would resolve it: Experiments varying partitioned visual encoder depth and analyzing impact on performance

## Limitations

- Computational cost scales linearly with the number of prompts, increasing GPU memory requirements
- Manual prompt design adds a burden and lacks systematic evaluation across different template variations
- The optimal depth partitioning strategy for the visual encoder is not thoroughly explored

## Confidence

- **High**: Depth partitioning prevents prompt collapse (supported by ablation showing single-prompt failure)
- **Medium**: Multi-prompt ensemble improves base accuracy (demonstrated on 11 datasets with H=79.28)
- **Low**: Manual prompt inclusion improves generalization (based on internal ablation without systematic comparison)

## Next Checks

1. **Probe depth representations**: Visualize and quantitatively compare visual features at different ViT depths to verify hierarchical abstraction assumptions
2. **Ablate manual prompt**: Systematically compare PMPO with and without the manual prompt across multiple template variations
3. **Test prompt redundancy**: Measure cosine similarity between different learnable prompts during training to verify they learn complementary representations