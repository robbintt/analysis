---
ver: rpa2
title: Optimizing Retrieval-augmented Reader Models via Token Elimination
arxiv_id: '2310.13682'
source_url: https://arxiv.org/abs/2310.13682
tags:
- tokens
- answer
- input
- token
- chosen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the computational inefficiency of retrieval-augmented
  generative models like Fusion-in-Decoder (FiD), which suffer from high latency due
  to extensive cross-attention operations when processing multiple retrieved passages
  during decoding. The authors propose Token Filtering, a method that dynamically
  removes uninformative tokens from input passages during the decoding stage by using
  cross-attention scores to rank token relevance.
---

# Optimizing Retrieval-augmented Reader Models via Token Elimination

## Quick Facts
- **arXiv ID**: 2310.13682
- **Source URL**: https://arxiv.org/abs/2310.13682
- **Reference count**: 40
- **Primary result**: Reduces FiD model latency by up to 62.2% with only 2% performance drop by dynamically eliminating uninformative tokens during decoding

## Executive Summary
This paper addresses the computational inefficiency of retrieval-augmented generative models like Fusion-in-Decoder (FiD) by proposing Token Filtering, a method that dynamically removes uninformative tokens from input passages during decoding. The approach uses cross-attention scores to identify and eliminate irrelevant tokens, significantly reducing computational cost. When combined with the CALM decoder layer-skipping technique, the method achieves state-of-the-art performance on the ELI5 KILT leaderboard while maintaining a favorable performance-efficiency tradeoff.

## Method Summary
The method dynamically filters input tokens during FiD decoding by computing cross-attention scores between decoder and encoder representations. At specific decoder layers and token indices, tokens are ranked by their average cross-attention scores across attention heads, with the lowest-scoring tokens removed from the key/value states for subsequent steps. The authors combine this with CALM (decoder layer skipping) and perform hyperparameter search over filtering percentage, layer, and token index. The approach is evaluated on ELI5, MS MARCO, and NaturalQuestions datasets using both base and large T5 models.

## Key Results
- Achieves up to 62.2% reduction in end-to-end latency
- Maintains performance within 2% degradation compared to baseline FiD
- Achieves state-of-the-art results on ELI5 KILT leaderboard
- Combined Token Filtering + CALM approach outperforms either method alone

## Why This Works (Mechanism)

### Mechanism 1
Cross-attention scores during generation can identify and eliminate uninformative tokens without degrading performance. During decoding, compute average cross-attention scores across all attention heads for each input token. Keep only top p% scored tokens and filter out the rest from the key/value states for all subsequent generation steps. Core assumption: Cross-attention scores reflect the importance of input tokens for generating the correct answer.

### Mechanism 2
Combining token filtering with decoder layer skipping (CALM) achieves better performance vs efficiency tradeoff than either method alone. Apply token filtering to remove irrelevant input tokens, then apply CALM to dynamically skip redundant decoder layers based on confidence scores. Core assumption: The two methods address orthogonal sources of inefficiency - token filtering reduces input size while CALM reduces computational depth.

### Mechanism 3
Early decoder layers (2nd and 3rd) are optimal for computing cross-attention scores for token filtering. Compute cross-attention scores at specific decoder layers and token indices to determine which input tokens to filter. Core assumption: Cross-attention scores in early layers better reflect the importance of input tokens for the overall generation process.

## Foundational Learning

- **Cross-attention mechanism in encoder-decoder transformers**: Understanding how decoder attends to encoder representations is fundamental to grasping why cross-attention scores can indicate token importance. Quick check: What is the difference between self-attention and cross-attention in transformer architectures?

- **Token-level sparsity and its impact on computational efficiency**: The core optimization relies on identifying and removing uninformative tokens to reduce computation. Quick check: How does reducing the number of input tokens affect the computational complexity of attention operations?

- **Dynamic computation path optimization (layer skipping)**: CALM represents a complementary approach that needs to be understood alongside token filtering. Quick check: What are the potential risks of skipping decoder layers during generation?

## Architecture Onboarding

- **Component map**: Question → Retriever → FiD Encoder → Token Filtering → FiD Decoder → Answer
- **Critical path**: Question → Retriever (100 passages) → FiD Encoder (parallel processing) → Token Filtering (cross-attention scoring) → FiD Decoder (answer generation) → Answer
- **Design tradeoffs**: Precision vs efficiency (higher p% preserves more information but reduces latency gains), early vs late filtering (earlier filtering saves more computation but may remove tokens needed for later context)
- **Failure signatures**: Performance degradation >2%, latency reduction <30%, or inconsistent results across datasets
- **First 3 experiments**:
  1. Baseline FiD performance on ELI5 with 100 passages, measure ROUGE-L and latency
  2. Token Filtering with p=10% at layer 2, compare to baseline
  3. Combined Token Filtering (p=10% at layer 2) + CALM (threshold=0.5), compare to baseline

## Open Questions the Paper Calls Out

### Open Question 1
How do different cross-attention scoring methods affect the performance of token filtering, and is there a more optimal approach than using average attention scores over all heads? The paper only uses one method for computing cross-attention scores and does not explore alternatives or compare their effectiveness.

### Open Question 2
Can the token filtering mechanism be made more dynamic by learning when and how to filter tokens, rather than using predetermined hyperparameters? The current implementation uses fixed hyperparameters for filtering percentage, token index, and layer selection.

### Open Question 3
How does the token filtering approach scale to different types of retrieval-augmented generation tasks beyond long-form question answering, such as summarization or fact checking? The paper only evaluates the method on three long-form question answering datasets.

## Limitations

- **Empirical Generalization Gap**: Only validated on long-form QA datasets; effectiveness on other domains like structured data QA or multi-hop reasoning tasks is uncertain
- **Hyperparameter Sensitivity**: Optimal settings may not generalize across different model sizes, passage retrieval qualities, and question types
- **Trade-off Stability**: Performance vs latency tradeoff appears dataset-dependent, with inconsistent results across datasets

## Confidence

**High Confidence (8/10)**: Core mechanism of using cross-attention scores for token filtering is well-supported by empirical results and ablation studies.

**Medium Confidence (6/10)**: State-of-the-art claims on ELI5 KILT leaderboard are supported but lack comparison to full leaderboard range.

**Low Confidence (4/10)**: Theoretical justification for why cross-attention scores correlate with token importance is primarily empirical rather than mechanistic.

## Next Checks

1. **Cross-dataset Robustness Test**: Apply Token Filtering to multi-hop QA (HotpotQA) or structured data QA (TabFact) to validate generalization beyond long-form QA.

2. **Attention Score Ablation**: Randomize or invert cross-attention scores to test whether performance gains are specifically due to score-based filtering versus general pruning effects.

3. **Layer-wise Contribution Analysis**: Systematically evaluate Token Filtering at every decoder layer rather than just 2nd-3rd layers to validate claimed optimal layers.