---
ver: rpa2
title: 'BOIS: Bayesian Optimization of Interconnected Systems'
arxiv_id: '2311.11254'
source_url: https://arxiv.org/abs/2311.11254
tags:
- function
- bois
- optimization
- composite
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of applying Bayesian optimization
  (BO) to composite functions of the form f(x, y(x)), where f is a known performance
  function and y is an unknown intermediate function. The difficulty lies in propagating
  uncertainty from y to f when f is nonlinear.
---

# BOIS: Bayesian Optimization of Interconnected Systems

## Quick Facts
- arXiv ID: 2311.11254
- Source URL: https://arxiv.org/abs/2311.11254
- Authors: [Not specified in source]
- Reference count: 6
- One-line primary result: BOIS outperforms standard BO on chemical process optimization by efficiently propagating uncertainty through nonlinear performance functions using adaptive linearizations.

## Executive Summary
This paper addresses a fundamental challenge in Bayesian optimization (BO): optimizing composite functions f(x, y(x)) where f is a known performance function and y is an unknown intermediate function. The key difficulty is propagating uncertainty from y (typically modeled as Gaussian via GP) through nonlinear f. BOIS solves this by using adaptive linearizations of f around current iterates to obtain closed-form expressions for the statistical moments of the composite function. This approach is computationally more efficient than Monte Carlo sampling methods while maintaining accuracy. The method is demonstrated on a chemical process optimization case study, where it achieves accurate estimates of the performance function distribution at significantly lower computational cost than sampling-based approaches.

## Method Summary
BOIS is a Bayesian optimization method for composite functions that uses adaptive linearizations to efficiently propagate uncertainty. The method works by first training Gaussian process models for the unknown intermediate functions y(x). At each iteration, BOIS linearizes the known performance function f around the current estimate of y, using this linearization to generate a local Laplace approximation. This allows closed-form computation of the mean and variance of f(x,y(x)) without expensive Monte Carlo sampling. The acquisition function is then constructed from these moments to guide the search. The method is particularly useful for interconnected systems where structural knowledge about f can be exploited, and demonstrates significant computational advantages over sampling-based approaches while maintaining comparable accuracy.

## Key Results
- BOIS achieves accurate estimates of performance function distribution with O(1) GP queries versus O(S) evaluations required by Monte Carlo methods
- On a chemical process optimization case study, BOIS outperforms standard BO and achieves lower operating costs
- The method demonstrates significant computational efficiency gains while maintaining comparable accuracy to sampling-based approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linearizing f around current GP mean estimate enables closed-form computation of composite function's moments
- Mechanism: First-order Taylor expansion of f around reference point y₀ yields linear transformation of GP's Gaussian estimate of y(x), permitting exact calculation of mean and covariance
- Core assumption: f is once-differentiable and linearization error is small near current iterate
- Evidence anchors: [abstract] "adaptive linearizations of f to obtain closed-form expressions for the statistical moments" [section] "first-order Taylor series expansion to linearize f with respect to y around reference point y₀"
- Break condition: Highly nonlinear f causes large linearization errors, especially far from linearization point

### Mechanism 2
- Claim: BOIS is computationally cheaper than Monte Carlo sampling
- Mechanism: Instead of S samples from GP and S function evaluations (O(S³ + S) cost), BOIS computes mean/covariance directly and applies closed-form formulas (O(1) cost)
- Core assumption: Computing f(x,y₀) and gradient J is comparable to or less than single GP sample
- Evidence anchors: [abstract] "at significantly lower computational cost than sampling-based approaches" [section] "BOIS only samples from the GP once to determine ŷℓ and Σℓ and evaluates f once"
- Break condition: Expensive gradient computation (e.g., no automatic differentiation) reduces computational advantage

### Mechanism 3
- Claim: BOIS leverages known f structure to enable exploration respecting physical constraints
- Mechanism: Known f allows Jacobian computation with respect to y, capturing how internal variable uncertainty propagates to performance measure, guiding exploration
- Core assumption: Intermediate variables y(x) represent physically meaningful variables whose uncertainty impacts performance
- Evidence anchors: [abstract] "enables the exploitation of structural knowledge, such as that arising in interconnected systems" [section] "intermediate black-box functions, y(x), are set to model the flowrates and compositions"
- Break condition: Incorrect known structure of f or poor linearization approximation misleads exploration

## Foundational Learning

- Concept: Gaussian process regression and predictive mean/covariance formulas
  - Why needed here: BOIS relies on GP predictions of y(x) to linearize f; understanding predictive formulas is essential
  - Quick check question: Given GP with kernel k and data Dℓ, write predictive mean at new point x*

- Concept: First-order Taylor series expansion and Jacobian computation
  - Why needed here: Linearization of f around y₀ is core approximation step; computing Jacobian J is required for uncertainty propagation
  - Quick check question: What is Jacobian of f(x,y) = aᵀy + b with respect to y?

- Concept: Laplace approximation and propagation of uncertainty through linear maps
  - Why needed here: Method assumes linearized f yields Gaussian approximation to true distribution of f(x,y(x))
  - Quick check question: If y ~ N(μ, Σ) and z = Ay + b, what are mean and covariance of z?

## Architecture Onboarding

- Component map: GP surrogate for y(x) -> Taylor linearization module for f(x,y) -> Closed-form statistics calculator -> Acquisition function optimizer -> Data update loop

- Critical path:
  1. Query GP for ŷℓ and Σℓ at candidate x*
  2. Compute linearization point y₀ (often ŷℓ)
  3. Evaluate f(x*,y₀) and compute Jacobian J
  4. Compute mℓᵧ(x*) and σℓᵧ(x*) via linear transformation
  5. Pass to acquisition function and optimize

- Design tradeoffs:
  - Accuracy vs speed: tighter linearization (smaller |y - y₀|) improves accuracy but may require more frequent re-linearization
  - Exploration vs exploitation: BOIS's uncertainty propagation can favor exploration in regions where y is uncertain; tuning acquisition function can balance this

- Failure signatures:
  - Large residuals between predicted and sampled f(x,y(x)) indicate linearization error
  - Acquisition function consistently selecting points with high uncertainty but poor actual performance suggests over-exploration
  - Numerical instability in Jacobian computation indicates ill-conditioned problem

- First 3 experiments:
  1. Implement BOIS on simple 1D composite function (e.g., f(x,y) = sin(y) + x², y(x) = x + noise) and compare to MC-BO for accuracy
  2. Run BOIS on chemical process case study, tracking time per iteration and final cost distribution
  3. Test sensitivity to linearization point choice by varying y₀ = ŷℓ ± ε and measuring impact on acquisition quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does BOIS performance scale with increasing dimensionality of design space?
- Basis in paper: [inferred] Authors mention planning to "scale up BOIS and deploy it on high-dimensional systems where BO has traditionally not been applied" as future work
- Why unresolved: Current study only tests BOIS on relatively low-dimensional chemical process optimization problem
- What evidence would resolve it: Experiments comparing BOIS performance on problems with varying dimensions (e.g., 10, 50, 100+ dimensions) in terms of convergence rate, computational cost, and solution quality

### Open Question 2
- Question: How sensitive is BOIS to choice of linearization reference point (y₀)?
- Basis in paper: [explicit] Method uses adaptive linearizations around current iterates, but paper doesn't explore sensitivity to this choice
- Why unresolved: Paper only demonstrates using y₀ = ŷℓ (current iterate), but doesn't investigate how different choices affect accuracy or convergence
- What evidence would resolve it: Comparative experiments testing different strategies for choosing y₀ (e.g., fixed offset, multiple reference points, adaptive schemes) and their impact on performance

### Open Question 3
- Question: How does BOIS compare to sampling-based methods when f has non-Gaussian distributions?
- Basis in paper: [inferred] BOIS generates local Laplace approximation assuming Gaussian distributions, but paper doesn't explore scenarios where this assumption breaks down
- Why unresolved: All experiments assume composite function f(x,y(x)) results in approximately Gaussian distributions
- What evidence would resolve it: Tests on problems with known non-Gaussian composite functions (e.g., multimodal distributions, heavy tails) comparing BOIS accuracy to MC methods

## Limitations

- BOIS relies on first-order Taylor expansions which may break down for highly nonlinear f relative to y(x), causing inaccurate Laplace approximations
- The method assumes gradient computation of f is tractable, which may not hold for black-box simulations, negating computational advantages over Monte Carlo
- BOIS performance may degrade when GP uncertainty in y(x) is large, as linearization becomes less accurate far from the linearization point

## Confidence

- High confidence: Computational efficiency claims (O(1) vs O(S) evaluations) and general framework of using linearizations for uncertainty propagation
- Medium confidence: Accuracy of Laplace approximation in realistic chemical process scenarios, given limited empirical validation
- Low confidence: Method's robustness when intermediate function uncertainty is large or when f has discontinuities or sharp nonlinearities

## Next Checks

1. Test BOIS on benchmark functions with varying degrees of nonlinearity (e.g., f(y) = sin(y), f(y) = exp(y²)) to quantify linearization error across different regimes
2. Implement convergence analysis comparing BOIS to MC-BO as function of iteration count and GP model quality, tracking both optimization progress and moment estimation accuracy
3. Evaluate method's sensitivity to choice of linearization point y₀ by systematically varying this parameter and measuring impact on acquisition quality and optimization convergence