---
ver: rpa2
title: 'Dynamic ASR Pathways: An Adaptive Masking Approach Towards Efficient Pruning
  of A Multilingual ASR Model'
arxiv_id: '2309.13018'
source_url: https://arxiv.org/abs/2309.13018
tags:
- pruning
- multilingual
- training
- pathways
- monolingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient pruning of multilingual
  automatic speech recognition (ASR) models. The proposed method, Dynamic ASR Pathways,
  uses an adaptive masking approach that dynamically adjusts sub-networks during training
  to improve performance.
---

# Dynamic ASR Pathways: An Adaptive Masking Approach Towards Efficient Pruning of A Multilingual ASR Model

## Quick Facts
- **arXiv ID**: 2309.13018
- **Source URL**: https://arxiv.org/abs/2309.13018
- **Reference count**: 0
- **Key outcome**: Dynamic ASR Pathways achieves 5.3% relative WER reduction in monolingual pruning and 2-5.8% relative WER reduction in multilingual pruning compared to existing methods.

## Executive Summary
This paper introduces Dynamic ASR Pathways, an adaptive masking approach for efficient pruning of multilingual automatic speech recognition (ASR) models. The method dynamically adjusts sub-networks during training to improve performance, addressing the challenge of finding optimal sub-network structures in multilingual scenarios. By re-evaluating pruning decisions through adaptive masking, the approach demonstrates significant improvements over existing pruning methods for both monolingual and multilingual ASR models.

## Method Summary
The method builds upon existing ASR Pathways by introducing an adaptive masking approach that dynamically adjusts pruning masks during training. It uses iterative magnitude pruning (IMP) or lottery ticket hypothesis (LTH) initialization, followed by training with adaptive mask updates. During adaptation steps, the method re-prunes from all weights to re-evaluate the sub-network structure, then continues training with the updated mask. For multilingual scenarios, it uses a union of language-specific and residual masks to promote parameter sharing while maintaining language-specific pathways.

## Key Results
- Monolingual pruning: 5.3% relative WER reduction compared to IMP baseline
- Multilingual pruning: 2-5.8% relative WER reduction over ASR Pathways baseline
- LAP initialization with adaptation achieves comparable performance to LTH-70% masks while eliminating language-specific pruning rounds
- Consistent improvements across all four MLS languages (EN, FR, IT, NL)

## Why This Works (Mechanism)

### Mechanism 1
Adaptive masking allows the sub-network to align better with training data by re-evaluating pruning decisions dynamically during training. During the adaptation step, the method re-prunes from all weights (not just those in the current mask) and makes the newly pruned weights trainable. This allows the sub-network structure to adjust based on the evolving data distribution. The core assumption is that the optimal sub-network configuration is not fixed early in training and can be improved by allowing weights to re-enter the sub-network after being pruned.

### Mechanism 2
In multilingual pruning, dynamic adaptation promotes parameter sharing across languages by pruning from a union of language-specific and residual sub-networks. The pruning mask includes both the language-specific mask and the residual mask (1 - union of other language masks). This allows the method to adapt the language-specific pathway while still benefiting from shared parameters. The core assumption is that languages share underlying acoustic-phonetic patterns, and parameter sharing can improve performance without harming language-specific accuracy.

### Mechanism 3
Initializing from language-agnostic pruning masks (LAP) and adapting them can achieve performance comparable to language-specific masks while eliminating the need for language-specific pruning rounds. The adaptation step turns a language-agnostic mask into a more language-specific one by re-pruning with monolingual data, achieving similar performance to pre-computed language-specific masks. The core assumption is that language-agnostic masks contain some language-specific structure that can be enhanced through adaptation without starting from scratch.

## Foundational Learning

- **Concept**: Iterative Magnitude Pruning (IMP)
  - Why needed here: IMP forms the baseline pruning method that the adaptive approach builds upon and modifies.
  - Quick check question: What is the key difference between IMP and the adaptive masking approach in terms of when pruning decisions are made?

- **Concept**: Lottery Ticket Hypothesis (LTH)
  - Why needed here: LTH provides an alternative pruning baseline that uses re-winding to pre-trained weights, and the adaptive method is also tested with this initialization.
  - Quick check question: How does LTH differ from IMP in terms of weight initialization after pruning?

- **Concept**: Structured pruning with block-wise patterns
  - Why needed here: The method uses block-wise pruning (8x1) to maintain computational efficiency and hardware compatibility.
  - Quick check question: Why might block-wise pruning be preferred over unstructured pruning in on-device ASR deployment?

## Architecture Onboarding

- **Component map**: Dense multilingual RNN-T model (180M parameters) -> Language-specific pruning masks (ml) or language-agnostic masks (LAP) -> Adaptation step: re-prunes and makes pruned weights trainable -> Pruning step: increases sparsity and prunes from union mask -> Training loop with monolingual or multilingual batches

- **Critical path**: 1. Initialize with pre-trained dense weights and pruning mask 2. Train for n steps 3. Adapt mask by re-pruning and making weights trainable 4. Train for (T-n) steps with adapted mask 5. Prune to target sparsity using union mask

- **Design tradeoffs**:
  - Monolingual vs multilingual training: Monolingual gives better per-language performance but requires more models; multilingual is more efficient but may suffer from interference
  - Adaptation frequency: More frequent adaptation could improve performance but increases computational cost
  - Sparsity initialization: Starting from high sparsity (70%) vs medium sparsity (50%) affects the balance between adaptation and pruning

- **Failure signatures**:
  - No improvement over baseline: Adaptation step may be too early or too late
  - Performance degradation: Residual mask may be too large, causing interference
  - Inconsistent results across languages: Language-specific patterns not captured by adaptation

- **First 3 experiments**:
  1. Implement adaptive masking for monolingual pruning with IMP initialization and compare to baseline IMP
  2. Test adaptive masking with LTH initialization for monolingual pruning
  3. Extend to bilingual multilingual pruning with LAP initialization and measure performance vs ASR Pathways baseline

## Open Questions the Paper Calls Out
1. How does the adaptive masking approach perform when scaling to a larger number of languages (e.g., 10-20 languages)?
2. What is the impact of different pruning strategies (e.g., unstructured vs. structured pruning) on the effectiveness of the adaptive masking approach?
3. How does the adaptive masking approach affect the training time and computational resources required for multilingual ASR model pruning?

## Limitations
- Evaluation is constrained to a single multilingual dataset (MLS with 4 languages), limiting generalizability
- Computational overhead of the adaptation step is not quantified relative to baseline methods
- Block-wise pruning pattern (8x1) may limit achievable sparsity compared to unstructured pruning
- Potential sensitivity to hyperparameters such as adaptation timing or prune percentage is not explored

## Confidence
- **High confidence**: The relative WER improvements in monolingual pruning (5.3%) and the consistent outperformance of Dynamic ASR Pathways over ASR Pathways baselines in multilingual scenarios (2-5.8%)
- **Medium confidence**: The claim that LAP initialization with adaptation achieves comparable performance to LTH-70% masks while eliminating language-specific pruning rounds
- **Low confidence**: The assertion that adaptive masking fundamentally improves pruning decisions by avoiding premature sub-network structure decisions

## Next Checks
1. **Ablation study on adaptation timing**: Systematically vary the adaptation step timing (n) and prune percentage (p) across multiple runs to quantify their impact on final WER performance and identify optimal configurations for different languages and sparsity levels.
2. **Cross-dataset generalization test**: Evaluate Dynamic ASR Pathways on additional multilingual ASR datasets beyond MLS (such as CommonVoice or VoxPopuli) to assess whether the observed improvements generalize across different language families, data distributions, and acoustic conditions.
3. **Computational overhead measurement**: Quantify the wall-clock time and GPU memory overhead introduced by the adaptation step relative to baseline IMP and ASR Pathways methods, calculating the efficiency trade-off between performance gains and computational cost.