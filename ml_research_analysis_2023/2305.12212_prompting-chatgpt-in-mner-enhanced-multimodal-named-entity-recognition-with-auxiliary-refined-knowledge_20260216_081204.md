---
ver: rpa2
title: 'Prompting ChatGPT in MNER: Enhanced Multimodal Named Entity Recognition with
  Auxiliary Refined Knowledge'
arxiv_id: '2305.12212'
source_url: https://arxiv.org/abs/2305.12212
tags:
- text
- knowledge
- image
- chatgpt
- auxiliary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes PGIM, a two-stage framework that leverages
  ChatGPT to generate auxiliary refined knowledge for improved multimodal named entity
  recognition (MNER) on social media. The method involves using a Multimodal Similar
  Example Awareness module to select relevant examples from a small set of manually
  annotated samples, which are then incorporated into a formatted prompt template
  to guide ChatGPT in generating refined knowledge.
---

# Prompting ChatGPT in MNER: Enhanced Multimodal Named Entity Recognition with Auxiliary Refined Knowledge

## Quick Facts
- arXiv ID: 2305.12212
- Source URL: https://arxiv.org/abs/2305.12212
- Reference count: 27
- Primary result: Two-stage PGIM framework leveraging ChatGPT for auxiliary refined knowledge significantly outperforms state-of-the-art MNER methods on Twitter datasets

## Executive Summary
This paper introduces PGIM, a novel two-stage framework that enhances multimodal named entity recognition (MNER) on social media by leveraging ChatGPT to generate auxiliary refined knowledge. The approach uses a Multimodal Similar Example Awareness (MSEA) module to select relevant in-context examples from manually annotated samples, which are then incorporated into prompts to guide ChatGPT's knowledge generation. The acquired knowledge is combined with original text and processed by a downstream transformer-CRF model for final entity prediction. Experiments on Twitter-2015 and Twitter-2017 datasets demonstrate significant performance improvements over state-of-the-art methods while maintaining computational efficiency.

## Method Summary
PGIM operates in two stages: first, the MSEA module computes cosine similarity between multimodal fusion features of test inputs and pre-defined artificial samples to select top-N relevant examples. These examples are formatted into prompts along with test inputs and sent to ChatGPT to generate auxiliary refined knowledge containing entity classifications and explanations. In the second stage, this knowledge is concatenated with the original text and fed into a transformer-based encoder (XLM-RoBERTa-large) with a CRF decoder to produce final named entity predictions. The framework uses a Text+Text paradigm where images are converted to captions using BLIP-2, simplifying the multimodal fusion process.

## Key Results
- PGIM achieves state-of-the-art F1-scores on Twitter-2015 and Twitter-2017 datasets
- Significant improvements in entity recognition accuracy compared to previous methods
- Demonstrates stronger robustness and generalization capability
- Requires only a single GPU and reasonable number of ChatGPT API invocations

## Why This Works (Mechanism)

### Mechanism 1
ChatGPT generates refined auxiliary knowledge that better supports MNER than raw external knowledge retrieval. By providing carefully selected in-context examples via the MSEA module, ChatGPT's in-context learning ability is activated to generate explanations and entity classifications that are directly relevant to the specific text-image pair, reducing noise from irrelevant retrieved knowledge. The core assumption is that examples selected by MSEA are sufficiently similar to the test input in the fused multimodal feature space to trigger appropriate reasoning in ChatGPT.

### Mechanism 2
The MSEA module improves ChatGPT's performance by selecting relevant in-context examples based on multimodal feature similarity. It computes cosine similarity between the multimodal fusion features (H) of the test input and each predefined artificial sample. The top-N most similar examples are used as in-context demonstrations, providing ChatGPT with relevant context that aligns with the test case. The core assumption is that the multimodal fusion features extracted by the vanilla MNER model encode sufficient semantic information to determine example relevance for the MNER task.

### Mechanism 3
The Text+Text paradigm (captioning image to text) outperforms the Image+Text paradigm for MNER. By converting images to captions using a pretrained vision-language model (BLIP-2), the task is transformed into a single-modality sequence labeling problem, allowing transformer-based encoders to leverage their strong text modeling capabilities without the complexity of cross-modal attention. The core assumption is that image captions capture sufficient relevant information to disambiguate entities in the text, and the cross-modal attention limitations mentioned (different feature distributions, label misalignment) are significant enough to hinder performance.

## Foundational Learning

- **In-context learning (ICL) in large language models**: The method relies on ChatGPT's ability to perform MNER by providing it with a few relevant examples in the prompt, rather than fine-tuning the model. Why needed here: To generate auxiliary refined knowledge without model parameter updates. Quick check question: What is the difference between in-context learning and traditional fine-tuning in terms of model parameter updates?

- **Multimodal feature fusion and similarity measures**: The MSEA module requires computing similarity between multimodal features (text + image) to select relevant in-context examples. Why needed here: To identify which pre-defined artificial samples are most relevant to each test input. Quick check question: Why is cosine similarity used to compare multimodal fusion features in the MSEA module?

- **Sequence labeling with BIO tagging schema**: MNER is framed as a sequence labeling task where each token is assigned a label (B-PER, I-PER, O, etc.) to identify named entities. Why needed here: To format the final predictions in a standard NER output format. Quick check question: What does the "B-" prefix indicate in a BIO tagging schema for named entity recognition?

## Architecture Onboarding

- **Component map**: Multimodal Similar Example Awareness (MSEA) module → Prompt template generator → ChatGPT → Transformer-based encoder (XLM-RoBERTa) → CRF layer

- **Critical path**: MSEA selection → Prompt generation → ChatGPT knowledge generation → Text + knowledge concatenation → Transformer encoding → CRF prediction

- **Design tradeoffs**: Using ChatGPT for knowledge generation vs. training a dedicated model: Pros - leverages powerful pre-trained reasoning; Cons - API cost, dependency on external service, potential latency. Text+Text paradigm vs. Image+Text: Pros - simpler architecture, leverages strong text models; Cons - may lose fine-grained visual information. Number of in-context examples (N): Pros - more examples can provide better context; Cons - too many examples may introduce noise or overwhelm the model.

- **Failure signatures**: Poor MSEA selection leads to ChatGPT receiving irrelevant examples, resulting in incorrect or unhelpful auxiliary knowledge. Inadequate image captioning fails to capture entity-relevant visual information, reducing the effectiveness of the Text+Text approach. ChatGPT API errors or rate limits disrupt the knowledge generation pipeline. Mismatch between ChatGPT's output format and expected input for the downstream model causes errors in final prediction stage.

- **First 3 experiments**: 1) Ablation study on MSEA: Run PGIM with random in-context example selection (w/o MSEA) and compare performance to full PGIM to quantify MSEA impact. 2) Vary number of in-context examples (N): Test PGIM with N=1, N=3, N=5, N=10 to find optimal number maximizing performance without introducing noise. 3) Direct ChatGPT prediction vs. PGIM: Compare ChatGPT directly predicting named entities (without downstream model) to PGIM's two-stage approach to validate effectiveness of combining ChatGPT's knowledge with transformer-CRF model.

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality of the manually annotated examples affect the performance of the PGIM model? The paper mentions that the quality of dataset annotation has a certain influence on the effectiveness of integrating auxiliary refined knowledge to enhance NER tasks performance, but does not provide detailed analysis of how different quality levels affect model performance.

### Open Question 2
How does the number of in-context examples affect the performance of the PGIM model? The paper states that an appropriate number of in-context examples can further improve the quality of auxiliary refined knowledge generated by ChatGPT, but does not provide detailed analysis of how different numbers of examples affect model performance.

### Open Question 3
How does the PGIM model compare to other state-of-the-art methods in terms of computational efficiency? The paper mentions that PGIM is friendly to most researchers and requires only a single GPU and reasonable number of ChatGPT API invocations, but does not provide detailed comparison of computational resources required by each method.

## Limitations

- Reliance on manually annotated pre-defined artificial samples, which may not be representative of test distribution
- Text+Text paradigm may lose critical entity disambiguation information present only in visual features not captured by captions
- Computational efficiency claims lack concrete runtime or cost comparisons with alternative approaches

## Confidence

- **High confidence**: Two-stage framework architecture is sound and well-implemented; ablation studies demonstrating improvements over baseline methods are convincing
- **Medium confidence**: Superiority of Text+Text paradigm over Image+Text is supported by experiments but may not generalize to all MNER scenarios; MSEA module's effectiveness is demonstrated but underlying feature space assumptions warrant further validation
- **Low confidence**: Claim that approach is significantly more lightweight than alternatives lacks concrete evidence and depends on API costs and availability not fully explored

## Next Checks

1. **Error Analysis on Visual-Only Information**: Create diagnostic subset of Twitter-2015 and Twitter-2017 datasets where entity disambiguation relies critically on visual information not captured in captions. Compare PGIM's performance on this subset to overall performance to quantify limitations of Text+Text approach.

2. **MSEA Feature Space Validation**: Conduct qualitative analysis of multimodal fusion features (H) used by MSEA. Visualize or analyze whether these features actually capture task-relevant similarity between examples and test inputs by checking if examples selected by MSEA for similar test cases share semantic or contextual properties.

3. **API Cost and Reliability Assessment**: Calculate actual API costs and latency for ChatGPT invocations used in full PGIM pipeline across entire dataset. Compare these metrics to alternative approaches (fine-tuning dedicated model, using smaller open-source LLMs) to validate "lightweight" claim with concrete numbers.