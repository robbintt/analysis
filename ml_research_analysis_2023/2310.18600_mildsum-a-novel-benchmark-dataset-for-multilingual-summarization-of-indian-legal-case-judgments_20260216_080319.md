---
ver: rpa2
title: 'MILDSum: A Novel Benchmark Dataset for Multilingual Summarization of Indian
  Legal Case Judgments'
arxiv_id: '2310.18600'
source_url: https://arxiv.org/abs/2310.18600
tags:
- summaries
- dataset
- english
- summarization
- legal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MILDSum, a high-quality multilingual dataset
  of 3,122 Indian legal case judgments in English with expert summaries in both English
  and Hindi. It addresses the challenge of summarizing complex legal judgments in
  Indian languages to improve access to justice.
---

# MILDSum: A Novel Benchmark Dataset for Multilingual Summarization of Indian Legal Case Judgments

## Quick Facts
- arXiv ID: 2310.18600
- Source URL: https://arxiv.org/abs/2310.18600
- Reference count: 37
- A novel benchmark dataset for multilingual summarization of Indian legal case judgments

## Executive Summary
MILDSum is the first high-quality multilingual dataset for cross-lingual summarization of Indian legal case judgments, containing 3,122 English judgments with expert summaries in both English and Hindi. The dataset addresses the critical challenge of making complex legal documents accessible to non-English speaking populations in India. Through comprehensive benchmarking, the authors demonstrate that a summarize-then-translate pipeline approach significantly outperforms direct cross-lingual summarization, highlighting the need for specialized approaches in the legal domain.

## Method Summary
The dataset was constructed by extracting case judgments from LiveLaw articles, using metadata matching to pair English and Hindi articles with high Jaccard similarity. The authors benchmarked various summarization approaches including unsupervised/extractive methods (LexRank, LSA, Luhn), supervised extractive methods (SummaRuNNer, BERTSumExt), abstractive methods (Legal-Pegasus, LongT5, LED), and direct cross-lingual methods (CrossSum-mT5). A summarize-then-translate pipeline using IndicTrans for English-to-Hindi translation was also evaluated. The dataset is available for research purposes on GitHub.

## Key Results
- MILDSum dataset contains 3,122 case judgments with expert summaries in English and Hindi
- Summarize-then-translate pipeline achieves ROUGE-2 scores of 32.27 (English) and 24.87 (Hindi)
- Direct cross-lingual summarization achieves ROUGE-2 score of 21.76 (Hindi), significantly lower than pipeline approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual summarization of Indian legal documents requires domain-specific training data because legal language has unique terminology and structure.
- Mechanism: MILDSum provides paired English legal judgments with Hindi summaries, enabling models to learn the mapping between complex English legal text and simplified Hindi summaries.
- Core assumption: The legal domain requires specialized language understanding that general summarization datasets cannot provide.
- Evidence anchors:
  - [abstract] "While prior research primarily focuses on summarizing legal case judgments in their source languages, this study presents a pioneering effort toward cross-lingual summarization of English legal documents into Hindi"
  - [section] "MILDSum is the first dataset to enable cross-lingual summarization of Indian case judgments"
- Break condition: If the summaries are too extractive or lack legal domain understanding, models may not learn effective cross-lingual summarization.

### Mechanism 2
- Claim: A summarize-then-translate pipeline outperforms direct cross-lingual summarization for legal documents.
- Mechanism: First generate an English summary using a legal domain model, then translate to Hindi, avoiding the complexity of direct cross-lingual legal summarization.
- Core assumption: Direct cross-lingual summarization in the legal domain is more challenging than handling each language separately.
- Evidence anchors:
  - [abstract] "Experiments show that the pipeline approach (ROUGE-2: 32.27 English, 24.87 Hindi) outperforms direct cross-lingual summarization (ROUGE-2: 21.76 Hindi)"
  - [section] "We observe that the summarize-then-translate approach performs better... than the cross-lingual summarization approach"
- Break condition: If translation quality degrades significantly or legal terminology is lost in translation.

### Mechanism 3
- Claim: High-quality legal summaries can be constructed by matching English and Hindi articles from LiveLaw using metadata similarity.
- Mechanism: Use Bing Search API with metadata from Hindi articles to find matching English articles, ensuring high Jaccard similarity for reliable pairing.
- Core assumption: Hindi article metadata contains sufficient information to reliably match with corresponding English articles.
- Evidence anchors:
  - [section] "We leveraged this information to find the matching English article"
  - [section] "Only one data-point was found erroneous, where the English and Hindi summaries corresponded to different judgments"
- Break condition: If metadata similarity drops below 0.5 or manual verification fails frequently.

## Foundational Learning

- Concept: Cross-lingual summarization requires understanding both source and target languages in the same domain context.
  - Why needed here: Legal documents have specialized terminology that requires domain-specific understanding across languages.
  - Quick check question: What makes legal language particularly challenging for cross-lingual summarization?

- Concept: Pipeline approaches can reduce complexity by handling language-specific tasks separately.
  - Why needed here: Legal summarization involves both complex language understanding and translation, which may be easier to handle separately.
  - Quick check question: Why might a summarize-then-translate approach outperform direct cross-lingual summarization in the legal domain?

- Concept: Dataset quality depends on reliable alignment between source and target language pairs.
  - Why needed here: MILDSum's effectiveness depends on accurate pairing of English judgments with Hindi summaries.
  - Quick check question: How can you verify the quality of language alignment in a multilingual dataset?

## Architecture Onboarding

- Component map: Data extraction -> Metadata matching -> Document cleaning -> Model training -> Evaluation
- Critical path: Document extraction -> Hindi-English matching -> Summary generation -> Translation (for pipeline) -> Evaluation
- Design tradeoffs: High-quality manual matching vs. scalability; extractive vs. abstractive summaries; pipeline complexity vs. model performance
- Failure signatures: Low Jaccard similarity in matching, poor ROUGE scores, significant degradation in translation quality
- First 3 experiments:
  1. Test metadata matching algorithm on small sample to verify Jaccard similarity threshold
  2. Evaluate extractive vs. abstractive summary quality on validation set
  3. Compare pipeline translation quality vs. direct cross-lingual approach on sample documents

## Open Questions the Paper Calls Out

- Question: How do the performance differences between the summarize-then-translate pipeline and direct cross-lingual summarization approaches change when using more advanced machine translation models beyond IndicTrans?
  - Basis in paper: explicit
  - Why unresolved: The paper only tested a limited set of translation models and found IndicTrans to be the best, but did not explore more advanced or specialized translation models that could potentially improve the pipeline approach's performance.

- Question: What is the impact of using different chunking strategies on the performance of long document summarizers like LongT5 and LED when applied to legal judgments in the MILDSum dataset?
  - Basis in paper: inferred
  - Why unresolved: The paper mentions chunking was used to handle long documents, but does not explore or analyze how different chunking strategies affect summarization quality.

- Question: How would the inclusion of summaries in additional Indian languages beyond Hindi affect the performance of cross-lingual summarization models trained on the MILDSum dataset?
  - Basis in paper: explicit
  - Why unresolved: The paper acknowledges the lack of summaries in Indian languages other than Hindi as a limitation, but does not provide experimental data on how multilingual expansion would impact model performance.

## Limitations

- Evaluation relies primarily on ROUGE metrics, which measure lexical overlap rather than semantic quality or legal domain accuracy
- Manual verification covered only 300 samples (9.6% of total), though no significant errors were found
- The performance gap between pipeline and direct cross-lingual approaches is significant but underlying reasons are not fully explored

## Confidence

- **High confidence**: The dataset construction methodology using metadata matching is reliable based on low error rate in manual verification. The general finding that summarize-then-translate outperforms direct cross-lingual approaches is well-supported by quantitative results.
- **Medium confidence**: The claim that MILDSum is the first dataset enabling cross-lingual summarization of Indian case judgments is based on literature review but could benefit from more exhaustive search.
- **Low confidence**: The scalability of metadata matching approach for larger datasets and generalizability of pipeline approach to other legal domains are not thoroughly tested.

## Next Checks

1. **Error Analysis Validation**: Conduct detailed error analysis comparing pipeline vs direct cross-lingual approaches to identify specific failure modes that explain the performance gap.

2. **Manual Quality Assessment**: Perform comprehensive human evaluation of Hindi summaries from both approaches using legal experts to assess domain accuracy, coherence, and usefulness for access to justice.

3. **Temporal/Jurisdictional Coverage Analysis**: Analyze the dataset distribution across different courts and time periods to ensure representative coverage of Indian legal judgments and identify potential biases.