---
ver: rpa2
title: 'Open Gaze: Open Source eye tracker for smartphone devices using Deep Learning'
arxiv_id: '2308.13495'
source_url: https://arxiv.org/abs/2308.13495
tags:
- page
- 'true'
- 'false'
- mitsplit
- split
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present OpenGaze, an open-source smartphone-based gaze
  tracker inspired by Google's proprietary eye tracking method. The core idea is to
  train a deep neural network on the MIT GazeCapture dataset to predict gaze coordinates
  from facial features and eye regions without requiring additional hardware.
---

# Open Gaze: Open Source eye tracker for smartphone devices using Deep Learning

## Quick Facts
- arXiv ID: 2308.13495
- Source URL: https://arxiv.org/abs/2308.13495
- Reference count: 2
- Primary result: Smartphone-based gaze tracker achieving ~1.8-2.0 cm MED, competitive with expensive hardware

## Executive Summary
OpenGaze is an open-source smartphone-based gaze tracker inspired by Google's proprietary method. It trains a deep neural network on the MIT GazeCapture dataset to predict gaze coordinates from facial features and eye regions without requiring additional hardware. The model combines convolutional towers for eye crops, facial landmarks, and a fine-tuned regression head, with optional SVR personalization. It demonstrates accuracy comparable to state-of-the-art mobile eye trackers at a fraction of the cost.

## Method Summary
The method uses a multi-tower CNN architecture processing eye crops, facial landmarks, and their combination through regression layers to predict 2D gaze coordinates. The model is trained on the MIT GazeCapture dataset and can be personalized using support vector regression (SVR) on individual user data. Post-training quantization enables real-time mobile inference. The approach is evaluated on both MIT and Google data splits, showing mean Euclidean distances of ~1.8-2.0 cm.

## Key Results
- Achieves mean Euclidean distance (MED) of ~1.8-2.0 cm on test data
- Slightly higher than Google's 0.46 cm but on par with state-of-the-art mobile trackers
- Demonstrates accuracy without requiring supplementary hardware

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep learning on smartphone-based eye images can achieve comparable gaze accuracy to expensive dedicated hardware.
- Mechanism: A multi-tower CNN processes individual eye crops, facial landmarks, and combines them in a regression head to predict 2D gaze coordinates directly from video frames.
- Core assumption: The MIT GazeCapture dataset captures sufficient diversity in lighting, device type, and head pose to train a generalizable model.
- Evidence anchors:
  - [abstract] "Our approach demonstrates precision akin to the state-of-the-art mobile eye trackers, which are characterized by a cost that is two orders of magnitude higher."
  - [section] "The basemodel is trained, prioritizing the face and its landmarks, using the MIT GazeCapture dataset."
  - [corpus] Weak: No direct citations comparing this model to commercial hardware accuracy.
- Break condition: If test data includes severe occlusions or extreme lighting not present in training set, accuracy will degrade.

### Mechanism 2
- Claim: Personalization via SVR fine-tuning improves gaze predictions for individual users beyond the baseline CNN.
- Mechanism: The penultimate layer features (1,4) are extracted and used to train a user-specific SVR, calibrated on a small set of known gaze points.
- Core assumption: Individual differences in eye geometry and appearance are captured in the CNN features and can be linearly corrected by SVR.
- Evidence anchors:
  - [abstract] "A light-weight regression model, specifically support vector regression (SVR), is introduced... leveraging the outputs of the fine-tuned ConvNet."
  - [section] "The process of adjusting SVR parameters involved considering specific values: 'rbf' as the kernel, 'C' set to 20, and 'gamma' at 0.6."
  - [corpus] Weak: No direct citation of SVR effectiveness in gaze personalization literature.
- Break condition: If the calibration points are too few or not representative of the user's gaze range, personalization will fail.

### Mechanism 3
- Claim: Quantization and optimized model architecture enable real-time inference on mobile devices without significant accuracy loss.
- Mechanism: Post-training quantization reduces numerical precision while retaining accuracy; model uses lightweight MobileNet SSD for face detection and compact CNN towers.
- Core assumption: Modern mobile CPUs/GPUs can efficiently run quantized models and lightweight detection in real time.
- Evidence anchors:
  - [abstract] "Our focus is on attaining accuracy comparable to that attained through the GooglePaper's methodology, without the necessity for supplementary hardware."
  - [section] "However, this challenge is proactively addressed through the implementation of post-training quantization."
  - [corpus] Weak: No benchmark results showing inference speed on target devices.
- Break condition: If the device hardware cannot sustain the required frame rate, user experience degrades.

## Foundational Learning

- Concept: Convolutional neural networks for image feature extraction
  - Why needed here: Eye and face regions must be processed to extract discriminative visual cues for gaze prediction.
  - Quick check question: What is the role of the convolution layers in the eye towers versus the landmark processing path?

- Concept: Support Vector Regression for personalization
  - Why needed here: Individual anatomical differences cause systematic gaze prediction errors that can be corrected with user-specific regression.
  - Quick check question: How does the choice of kernel and hyperparameters (C, gamma) affect SVR's ability to fit individual gaze patterns?

- Concept: Dataset splitting strategies to avoid leakage
  - Why needed here: Ensures that personalization models are evaluated on truly unseen data, reflecting real-world use.
  - Quick check question: Why does the MITSplit exclude individuals from the base training set when training the SVR?

## Architecture Onboarding

- Component map:
  Face detection (MobileNet SSD) -> Eye region cropping (landmark-guided) -> Eye towers (3 conv layers each, 128x128x3 input) -> Landmark processing (FC layers) -> Feature fusion (concatenate eye towers + landmarks) -> Regression head (multiple FC layers) -> SVR (if used) -> Quantization (post-training)

- Critical path:
  Face detection → Eye crops → CNN towers → Landmark FC → Fusion → Regression → SVR (if used)

- Design tradeoffs:
  - Model size vs. accuracy: Larger models improve accuracy but slow inference.
  - Calibration effort vs. accuracy: More calibration points improve SVR personalization but increase user burden.
  - Quantization level vs. accuracy: Aggressive quantization speeds inference but risks accuracy loss.

- Failure signatures:
  - High MED on unseen users → model not generalizable.
  - SVR overfitting → low training error, high test error.
  - Model crashes on some devices → quantization or architecture incompatibility.

- First 3 experiments:
  1. Train baseline CNN on MIT split, measure MED on test set.
  2. Apply SVR personalization on a held-out subset, compare MED before/after.
  3. Quantize model to int8, verify MED and inference speed on target device.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact reason for the occasional negative impact of SVR personalization observed in the study?
- Basis in paper: [explicit] The paper notes that "there are instances where its impact isn't uniformly positive" but does not explain why.
- Why unresolved: The paper does not investigate or explain the underlying causes of these negative impacts, merely observing their occurrence.
- What evidence would resolve it: Detailed analysis of specific cases where SVR underperformed, including examination of input data characteristics, feature space distribution, and potential overfitting on certain data subsets.

### Open Question 2
- Question: How does the choice of epsilon value in BatchNormalization (1e-3 vs 1e-5) quantitatively affect gaze tracking accuracy across different hardware platforms?
- Basis in paper: [explicit] The paper mentions changing epsilon from 1e-5 to 1e-3 based on Google's approach but does not quantify the impact.
- Why unresolved: While the change was implemented, the paper does not provide comparative results showing the exact accuracy difference between the two epsilon values.
- What evidence would resolve it: Systematic ablation studies comparing model performance with both epsilon values across multiple device types and user populations.

### Open Question 3
- Question: What is the theoretical limit of accuracy achievable by smartphone-based gaze tracking without additional hardware, and how close does OpenGaze come to this limit?
- Basis in paper: [inferred] The paper achieves ~1.8-2.0 cm error, slightly higher than Google's 0.46 cm, but does not discuss theoretical limitations or bounds.
- Why unresolved: The paper benchmarks against existing methods but does not explore fundamental physical or algorithmic constraints on accuracy.
- What evidence would resolve it: Mathematical analysis of error sources (camera resolution, eye movement dynamics, lighting conditions) combined with empirical testing under controlled conditions to establish theoretical bounds.

## Limitations
- Reliance on MIT GazeCapture dataset which may not capture full real-world diversity
- Accuracy still an order of magnitude worse than Google's proprietary system
- Lack of runtime benchmarks leaves uncertainty about real-time performance

## Confidence
- High confidence in model architecture feasibility
- Medium confidence in claimed accuracy on MIT and Google splits
- Low confidence in practical utility of SVR personalization

## Next Checks
1. Test model generalization on a held-out dataset with diverse lighting and head pose conditions not represented in MIT GazeCapture.
2. Benchmark inference speed and MED on actual target smartphone hardware across different model quantization levels.
3. Validate SVR personalization with a larger, more diverse calibration set to assess robustness and prevent overfitting.