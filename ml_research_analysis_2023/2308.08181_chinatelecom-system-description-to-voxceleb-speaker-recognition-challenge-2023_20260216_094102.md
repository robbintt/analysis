---
ver: rpa2
title: ChinaTelecom System Description to VoxCeleb Speaker Recognition Challenge 2023
arxiv_id: '2308.08181'
source_url: https://arxiv.org/abs/2308.08181
tags:
- speaker
- system
- recognition
- cation
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This report describes the ChinaTelecom system for the VoxCeleb
  Speaker Recognition Challenge 2023, Track 1 (closed). The team employed a fusion
  of seven ResNet-based architectures with varying depths and loss functions, trained
  on VoxCeleb2 data.
---

# ChinaTelecom System Description to VoxCeleb Speaker Recognition Challenge 2023

## Quick Facts
- arXiv ID: 2308.08181
- Source URL: https://arxiv.org/abs/2308.08181
- Reference count: 0
- Primary result: EER of 1.980% and minDCF of 0.1066 on VoxSRC2023 test set using ResNet-based model fusion

## Executive Summary
This report describes the ChinaTelecom system for the VoxCeleb Speaker Recognition Challenge 2023 Track 1 (closed). The team developed a speaker verification system based on a fusion of seven ResNet-based architectures with varying depths and loss functions. The system was trained on VoxCeleb2 development data using extensive data augmentation and achieved state-of-the-art performance with an EER of 1.980% and minDCF of 0.1066 on the test set. The approach demonstrates the effectiveness of model fusion and large margin fine-tuning in improving speaker verification accuracy.

## Method Summary
The ChinaTelecom system employs a fusion of seven ResNet-based architectures including ResNet, Res2Net, and ERes2Net variants with depths of 152 and 293 layers. Models were trained using VoxCeleb2 development data with speed perturbation, noise addition, and utterance segmentation augmentation. Multi-query multi-head attention pooling (MQMHA) was used to extract speaker embeddings, followed by large margin fine-tuning with AAM and K-subcenter loss functions. Score fusion and calibration using QMF features were applied to combine model outputs, resulting in improved verification performance.

## Key Results
- EER of 1.980% and minDCF of 0.1066 on VoxSRC2023 test set
- Score fusion reduced EER from 3.597% to 2.136% on validation set
- ResNet152 with AAM loss achieved individual EER of 2.479% and minDCF of 0.1316
- ResNet293 with K-subcenter loss achieved individual EER of 3.016% and minDCF of 0.1673

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model fusion across ResNet variants improves speaker verification accuracy by leveraging complementary feature representations
- Mechanism: Different ResNet variants with varying depths capture different aspects of speaker characteristics. When fused, these complementary representations create a more robust speaker embedding
- Core assumption: The variations in architecture depth and type create sufficiently diverse feature representations that are complementary rather than redundant
- Evidence anchors:
  - [abstract] "The team employed a fusion of seven ResNet-based architectures with varying depths and loss functions"
  - [section] "Score fusion yielded a substantial enhancement in system performance, resulting in a remarkable reduction of EER to 2.136%, coupled with a decrease in minDCF to 0.1260 on the validation set"
  - [corpus] Weak evidence - corpus papers don't specifically discuss fusion mechanisms for speaker verification

### Mechanism 2
- Claim: Multi-query multi-head attention pooling (MQMHA) improves speaker embedding quality by focusing on discriminative features
- Mechanism: MQMHA assigns larger weights to more discriminative speaker characteristics through attention mechanisms, allowing the model to focus on relevant speaker information while suppressing irrelevant variation
- Core assumption: Speaker verification benefits from dynamic attention to different parts of the input rather than static pooling
- Evidence anchors:
  - [section] "We employed multi-query multi-head attention pooling method (MQMHA) [8] to alleviate model sticking in some certain patterns"
  - [abstract] "Models incorporated Res2Net and ERes2Net architectures, with MQMHA pooling"
  - [corpus] Weak evidence - corpus papers don't discuss attention pooling mechanisms in detail

### Mechanism 3
- Claim: Large margin fine-tuning increases discriminative ability of speaker embeddings by creating more separable feature spaces
- Mechanism: The second training step increases the margin from 0.2 to 0.5, forcing the model to create more distinctive embeddings that are easier to separate during verification
- Core assumption: Increasing the margin in the loss function during fine-tuning creates more robust speaker embeddings
- Evidence anchors:
  - [section] "Large margin fine-tuning [14] has been widely used for further increasing discriminative ability of speaker embeddings"
  - [section] "The margin was set to 0.5 from 0.2"
  - [corpus] Weak evidence - corpus papers don't discuss large margin fine-tuning specifically

## Foundational Learning

- Concept: Data augmentation techniques (speed perturbation, noise addition, utterance segmentation)
  - Why needed here: Increases effective training data size and robustness to real-world variations in speaker characteristics
  - Quick check question: What are the three data augmentation techniques used and how does each contribute to model robustness?

- Concept: Loss function selection and configuration (AAM, K-subcenter, Sphereface2)
  - Why needed here: Different loss functions optimize for different aspects of speaker verification - angular margins for discrimination, sub-centers for handling label noise, binary classification for separation
  - Quick check question: How do the different loss functions (AAM vs Sphereface2) differ in their approach to optimizing speaker embeddings?

- Concept: Score calibration and normalization (AS-Norm, QMF features)
  - Why needed here: Compensates for score distribution variations across different models and utterances, improving fusion performance
  - Quick check question: What are the six QMF features used for score calibration and what aspects of the utterances do they capture?

## Architecture Onboarding

- Component map: Input (80-dim Mel FBANKs) → Data Augmentation → ResNet/Res2Net/ERes2Net Backbone → MQMHA Pooling → Loss Function → Score Normalization → Fusion
- Critical path: Data augmentation → Model training (steps 1 and 2) → Score normalization → Fusion
- Design tradeoffs: Model depth vs. overfitting risk, complexity of attention mechanisms vs. training stability, fusion benefits vs. computational cost
- Failure signatures: Overfitting on small datasets (high validation loss), unstable attention weights, poor fusion performance despite strong individual models
- First 3 experiments:
  1. Test individual ResNet variants without fusion to establish baseline performance
  2. Implement MQMHA pooling with different head/query configurations to find optimal settings
  3. Compare different loss function combinations during fine-tuning to determine optimal margin settings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of data augmentation parameters (speed perturbation ratios, noise addition probability, segment length) affect speaker verification performance across different ResNet variants?
- Basis in paper: [explicit] The paper describes using speed perturbation with ratios 0.9, 1.0, 1.1, noise augmentation with probability 0.6, and fixed-length segments, but doesn't systematically explore parameter variations
- Why unresolved: The paper uses a fixed augmentation strategy without ablation studies or parameter sensitivity analysis
- What evidence would resolve it: Comparative results showing performance changes with different augmentation parameters (e.g., varying perturbation ratios, noise probability, segment lengths) across the 7 ResNet variants

### Open Question 2
- Question: What is the optimal pooling method configuration (head number, query number, scale factor) for different ResNet architectures in speaker verification?
- Basis in paper: [explicit] The paper uses MQMHA with fixed parameters (head number 8, query number 2, scale factor 2) but doesn't explore alternative configurations
- Why unresolved: No comparative analysis of different pooling configurations or justification for the chosen parameters
- What evidence would resolve it: Performance comparisons across different MQMHA parameter settings (varying heads, queries, scale factors) for each ResNet variant

### Open Question 3
- Question: How does model depth impact overfitting when training on limited VoxCeleb2 data, and what regularization strategies could mitigate this?
- Basis in paper: [inferred] The authors note that ResNet293 and Simam-ResNet293 "fell short of initial expectations" due to "inadequacy of the training data volume, potentially leading to overfitting"
- Why unresolved: The paper identifies overfitting as a problem but doesn't explore solutions or quantify the overfitting effect
- What evidence would resolve it: Systematic comparison of model performance versus depth with and without additional regularization techniques (dropout, weight decay, early stopping)

## Limitations
- The exact implementation details of WeSpeaker toolkit configuration and QMF feature extraction remain unspecified
- System's performance is evaluated only on VoxSRC2023 test set without cross-dataset validation
- Heavy reliance on data augmentation may indicate potential overfitting to training domain
- Complex fusion approach introduces significant computational overhead limiting practical deployment

## Confidence
- **High Confidence:** The core methodology of using ResNet variants with varying depths, MQMHA pooling, and score fusion is well-established in speaker verification literature and directly supported by experimental results showing EER reduction from 3.597% to 2.136% on validation data
- **Medium Confidence:** The specific combination of architectures (Res2Net, ERes2Net) and loss functions (AAM, K-subcenter) is effective but relies on empirical optimization without theoretical guarantees
- **Low Confidence:** The claim that this specific fusion configuration is optimal is based on a single test set evaluation without ablation studies or cross-validation

## Next Checks
1. **Ablation Study:** Systematically remove each ResNet variant from the fusion to quantify individual contributions and determine if all seven models are necessary for optimal performance
2. **Cross-Dataset Generalization:** Evaluate the trained models on different speaker verification datasets (e.g., Speakers in the Wild, SITW) to assess robustness beyond VoxCeleb data and validate claims of general speaker representation quality
3. **Computational Complexity Analysis:** Measure inference time and memory requirements for the full fusion system to evaluate practical deployment feasibility and compare against single-model alternatives