---
ver: rpa2
title: A Hybrid Architecture for Out of Domain Intent Detection and Intent Discovery
arxiv_id: '2303.04134'
source_url: https://arxiv.org/abs/2303.04134
tags:
- intent
- detection
- data
- intents
- discovery
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of detecting out-of-domain (OOD)
  and out-of-scope (OOS) inputs in task-oriented dialogue systems, and discovering
  new intents from these OOD/OOS inputs. The proposed method uses a Variational Autoencoder
  (VAE) to distinguish between known and unknown intents, independent of input data
  distribution.
---

# A Hybrid Architecture for Out of Domain Intent Detection and Intent Discovery

## Quick Facts
- arXiv ID: 2303.04134
- Source URL: https://arxiv.org/abs/2303.04134
- Reference count: 40
- Outperforms state-of-the-art methods for OOD detection and intent discovery on ATIS, SNIPS, and Persian-ATIS datasets

## Executive Summary
This paper presents a hybrid architecture for detecting out-of-domain (OOD) inputs and discovering new intents in task-oriented dialogue systems. The method combines a Variational Autoencoder (VAE) for OOD detection with kernel-PCA dimensionality reduction and HDBSCAN clustering for intent discovery. The VAE learns to reconstruct in-domain inputs with low loss, allowing OOD detection based on high reconstruction loss. For discovered OOD intents, kernel-PCA reduces the dimensionality of BERT embeddings before HDBSCAN clustering automatically determines the number of intent clusters.

## Method Summary
The method consists of three main stages: first, fine-tune BERT/ParsBERT on in-domain training data and extract CLS embeddings for all utterances; second, train a VAE on in-domain embeddings to detect OOD inputs based on reconstruction loss threshold; third, apply kernel-PCA with polynomial kernel to reduce OOD embedding dimensionality, then use HDBSCAN clustering to discover intent groups. The approach is evaluated on English (ATIS, SNIPS) and Persian (Persian-ATIS) datasets, achieving high F1-scores for OOD detection and good clustering results (ACC, NMI, ARI) for intent discovery.

## Key Results
- Achieves superior F1-scores for OOD detection compared to state-of-the-art methods
- Demonstrates strong clustering performance (ACC, NMI, ARI) for intent discovery on both English and Persian datasets
- Shows robustness when OOD intents semantically overlap with in-domain intents
- 32-dimensional latent space in VAE provides optimal performance among tested configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VAE reconstruction loss distinguishes in-domain from OOD/OOS inputs without distributional assumptions.
- Mechanism: The VAE is trained only on in-domain data, learning to reconstruct those inputs with low loss. OOD inputs have high reconstruction loss, creating a natural anomaly detection signal.
- Core assumption: In-domain and OOD data occupy sufficiently different latent spaces that reconstruction loss becomes a reliable classifier.
- Evidence anchors:
  - [abstract] "VAE is used to distinguish between known and unknown intents independent of input data distribution"
  - [section] "VAEs can detect inputs with novel contexts independent of their distribution"
- Break condition: If OOD inputs share latent structure with in-domain data, reconstruction loss differences diminish.

### Mechanism 2
- Claim: Kernel-PCA reduces dimensionality to make clustering distances meaningful for high-dimensional BERT embeddings.
- Mechanism: Kernel-PCA projects high-dimensional sentence embeddings into a lower-dimensional space where Euclidean distances better reflect semantic similarity.
- Core assumption: The polynomial kernel captures nonlinear relationships in the embedding space that PCA alone would miss.
- Evidence anchors:
  - [section] "kernel-PCA algorithm for intent discovery to overcome the problems that high-dimensional data may cause"
  - [section] "As we can see, the quality is better on the SNIPS dataset"
- Break condition: If the kernel transformation doesn't improve separability, clustering performance won't improve.

### Mechanism 3
- Claim: HDBSCAN's density-based clustering automatically determines the number of clusters without requiring this hyperparameter.
- Mechanism: HDBSCAN builds a hierarchy of clusters based on density reachability, eliminating the need to specify cluster count.
- Core assumption: OOD intents naturally form dense regions in the reduced-dimensional space that HDBSCAN can detect.
- Evidence anchors:
  - [section] "HDBSCAN does not ask for the number of clusters as a hyperparameter"
- Break condition: If OOD intents are uniformly distributed or form singleton clusters, HDBSCAN cannot find meaningful groupings.

## Foundational Learning

- Concept: Variational Autoencoders and their reconstruction loss property
  - Why needed here: VAE's ability to distinguish in-domain from OOD data based on reconstruction loss is central to the detection mechanism
  - Quick check question: What does high reconstruction loss from a VAE indicate about the input data?

- Concept: Kernel methods for dimensionality reduction
  - Why needed here: Kernel-PCA is used to project high-dimensional BERT embeddings into a space where clustering distances are meaningful
  - Quick check question: How does a polynomial kernel in Kernel-PCA differ from linear PCA?

- Concept: Density-based clustering algorithms
  - Why needed here: HDBSCAN is the clustering method that groups OOD utterances without requiring the number of clusters as input
  - Quick check question: What advantage does HDBSCAN have over traditional DBSCAN?

## Architecture Onboarding

- Component map: Input utterances -> BERT/ParsBERT encoder -> CLS embedding -> VAE -> Reconstruction loss; High loss -> OOD detected -> Kernel-PCA -> HDBSCAN -> Cluster labels; Low loss -> In-domain -> Classifier -> Intent label

- Critical path:
  1. BERT/ParsBERT fine-tuning on in-domain data
  2. VAE training on in-domain CLS embeddings
  3. OOD detection using reconstruction loss threshold
  4. Kernel-PCA dimensionality reduction on OOD embeddings
  5. HDBSCAN clustering of reduced embeddings

- Design tradeoffs:
  - VAE vs AE: VAE provides smoother latent representations but adds training complexity
  - Kernel-PCA vs PCA: Captures nonlinear relationships but increases computational cost
  - HDBSCAN vs K-means: No need to specify cluster count but requires careful parameter tuning

- Failure signatures:
  - Low F1 scores on OOD detection: VAE may not be learning sufficient separation between in-domain and OOD data
  - Poor clustering results: Kernel-PCA transformation may not be effective, or HDBSCAN parameters may need adjustment
  - High variance in results: Reconstruction loss threshold may be too sensitive

- First 3 experiments:
  1. Vary VAE latent dimension (8, 16, 32, 64) and measure impact on OOD detection F1
  2. Compare Kernel-PCA with polynomial kernel vs PCA on clustering performance (NMI, ARI)
  3. Test different HDBSCAN min_samples and eps values on in-domain data to find optimal clustering parameters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed method vary with different VAE latent space dimensions?
- Basis in paper: [explicit] The paper states "As shown in Figure 3, 32 has the best performance among values 8, 16, 32, and 64" and mentions the impact of latent space dimensions on VAE performance.
- Why unresolved: The paper only tests a limited range of dimensions (8, 16, 32, 64). The optimal dimension might vary depending on dataset characteristics or could be dataset-specific.
- What evidence would resolve it: Comprehensive experiments testing a wider range of latent dimensions across multiple datasets, potentially including a dimension selection method based on dataset properties.

### Open Question 2
- Question: How can the threshold selection for OOD detection be automated to make the method fully end-to-end?
- Basis in paper: [explicit] The paper mentions "The need to set the threshold manually is a con of the proposed method. In future works, we will try to make this approach end-to-end."
- Why unresolved: The paper acknowledges this as a limitation but doesn't propose a solution. Manual threshold selection may not be practical in real-world applications where data distributions change over time.
- What evidence would resolve it: Development and validation of an automated threshold selection method, possibly based on statistical properties of reconstruction losses or adaptive threshold adjustment mechanisms.

### Open Question 3
- Question: How does the proposed method perform on languages other than English and Persian?
- Basis in paper: [inferred] The paper evaluates the method on English and Persian datasets but doesn't test other languages. The performance difference between English and Persian suggests language-specific factors may affect results.
- Why unresolved: The paper only tests two languages with different language families and scripts. Performance on other languages (e.g., Romance languages, East Asian languages, or low-resource languages) remains unknown.
- What evidence would resolve it: Experiments on diverse language families with varying morphological complexity, script types, and available pre-trained language models to determine cross-linguistic generalizability.

## Limitations

- The controlled experimental setup using randomly selected OOD classes may not reflect real-world distribution shifts where OOD intents often share semantic overlap with in-domain intents
- The method requires manual threshold selection for OOD detection, making it not fully end-to-end and potentially sensitive to threshold choice
- Performance evaluation is limited to English and Persian datasets, leaving cross-linguistic generalizability uncertain

## Confidence

- High Confidence: VAE's fundamental capability to distinguish in-domain from OOD data based on reconstruction loss
- Medium Confidence: The effectiveness of kernel-PCA for dimensionality reduction in this specific clustering task
- Medium Confidence: HDBSCAN's ability to discover meaningful intent clusters without specifying cluster count

## Next Checks

1. Test VAE robustness by evaluating on OOD data that semantically overlaps with in-domain intents, measuring performance degradation compared to randomly selected OOD classes.

2. Compare kernel-PCA with polynomial kernel against simpler alternatives (standard PCA, t-SNE) to isolate the specific contribution of the kernel transformation to clustering performance.

3. Conduct ablation studies removing the VAE layer entirely and using raw BERT embeddings for OOD detection, to quantify the added value of the reconstruction loss mechanism.