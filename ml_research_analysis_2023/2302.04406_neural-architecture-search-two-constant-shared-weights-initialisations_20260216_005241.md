---
ver: rpa2
title: 'Neural Architecture Search: Two Constant Shared Weights Initialisations'
arxiv_id: '2302.04406'
source_url: https://arxiv.org/abs/2302.04406
tags:
- search
- metric
- architectures
- epsilon
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel zero-cost NAS metric, epsilon, which
  evaluates neural architectures by computing the dispersion of outputs between two
  distinct constant shared weight initialisations. The method normalises this dispersion
  by the average output magnitude, achieving strong correlations with trained accuracy
  across image classification and language tasks on multiple benchmark datasets including
  NAS-Bench-101, NAS-Bench-201, and NAS-Bench-NLP.
---

# Neural Architecture Search: Two Constant Shared Weights Initialisations

## Quick Facts
- arXiv ID: 2302.04406
- Source URL: https://arxiv.org/abs/2302.04406
- Reference count: 40
- One-line primary result: Zero-cost NAS metric epsilon computes output dispersion between two constant shared weight initialisations, achieving Spearman correlations up to 0.90+ on CIFAR-10/100

## Executive Summary
This paper introduces epsilon, a novel zero-cost neural architecture search metric that evaluates architectures by measuring the dispersion of outputs between two distinct constant shared weight initialisations. The metric normalises this dispersion by average output magnitude, enabling rapid architecture ranking without training, gradient computation, or label requirements. Epsilon achieves strong correlations with trained accuracy across multiple benchmark datasets and search spaces, outperforming existing zero-cost methods while requiring only a single forward pass on a minibatch.

## Method Summary
The epsilon metric computes the mean absolute difference between network outputs generated by two distinct constant weight initialisations, normalised by the mean output magnitude. It operates by forward propagating a fixed random minibatch through the network twice, once with each constant weight value, then computing the normalised dispersion of the resulting output distributions. This approach eliminates dependence on training hyperparameters, loss functions, and human-labelled data while requiring only a fraction of a GPU second per evaluation.

## Key Results
- Achieves Spearman correlations up to 0.90+ on CIFAR-10 and CIFAR-100 datasets
- Outperforms existing zero-cost NAS methods across NAS-Bench-101, NAS-Bench-201, and NAS-Bench-NLP
- Successfully improves random search and evolutionary algorithms when used as warm-up strategy
- Evaluates architectures in a fraction of a GPU second with no labels or gradient computation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dispersion between two constant shared weight initialisations correlates with trained accuracy
- Mechanism: The metric computes mean absolute difference between output distributions from two distinct constant weights, where higher dispersion indicates greater sensitivity to weight magnitude reflecting architectural expressiveness
- Core assumption: Network topology drives generalisation capacity more than individual weight values
- Evidence anchors: Abstract states dispersion "positively correlates with trained accuracy"; section describes epsilon as measuring "output distribution shape change due to geometry"
- Break condition: Architectures collapsing to identical outputs under different constant weights yield epsilon near zero regardless of true performance

### Mechanism 2
- Claim: Normalising by average output magnitude improves correlation by compensating for scale differences
- Mechanism: Dividing dispersion by mean output magnitude makes metric scale-invariant, focusing on relative variation rather than absolute signal strength
- Core assumption: Architectures producing similar output magnitudes but different patterns are more likely to generalise
- Evidence anchors: Section shows epsilon metric formula with normalisation by mean output magnitude; results demonstrate consistent performance across image classification and language tasks
- Break condition: Architectures consistently producing vanishing or exploding outputs may not be rescued by normalisation

### Mechanism 3
- Claim: Using single minibatch removes dependence on training hyperparameters and labels
- Mechanism: Forward propagation on fixed random batch yields deterministic outputs for given architecture and weight pair, making metric independent of loss functions, optimisers, or data annotations
- Core assumption: Single representative minibatch captures sufficient architectural characteristics for ranking
- Evidence anchors: Abstract states method "requires no data labels, operates on a single minibatch"; section confirms no gradient computation or labels needed
- Break condition: Chosen batch not representative of data distribution leads to unstable rankings

## Foundational Learning

- Concept: Neural Architecture Search (NAS)
  - Why needed here: Understanding NAS context explains why zero-cost proxies are valuable and how epsilon fits into NAS workflow
  - Quick check question: What distinguishes one-shot NAS from zero-cost NAS in terms of computational requirements?

- Concept: Constant shared weight initialisation
  - Why needed here: Epsilon relies on setting all weights to same constant value; knowing how this differs from random initialisation is critical
  - Quick check question: How does constant weight initialisation affect forward pass differently from randomly initialised network?

- Concept: Spearman rank correlation
  - Why needed here: Performance evaluation uses rank correlation to compare metric rankings against true trained accuracies
  - Quick check question: Why is Spearman correlation preferred over Pearson correlation when evaluating ranking metrics?

## Architecture Onboarding

- Component map:
  Input batch -> Network -> Two constant weight initialisations -> Forward pass (x2) -> Output normalisation -> Dispersion calculation -> Epsilon score

- Critical path:
  1. Load network and data batch
  2. Initialise with first constant weight
  3. Forward pass and flatten outputs
  4. Repeat with second constant weight
  5. Normalise both output sets
  6. Compute epsilon
  7. Return score

- Design tradeoffs:
  - Choice of constant weights: Too close → low sensitivity; too extreme → NaNs
  - Batch size: Larger → more stable but slower
  - Single batch vs. multiple batches: Single is fast but may be less robust

- Failure signatures:
  - All epsilon scores near zero → weights too close or architecture too shallow
  - Many NaNs → weights too large/small causing overflow/underflow
  - Random scatter in correlation plot → batch not representative

- First 3 experiments:
  1. Run epsilon on small NAS-Bench-201 subset with weights [1e-7, 1] and batch size 256; verify correlation with trained accuracy
  2. Vary weight pair to [1e-6, 10] and observe changes in NaN rate and correlation
  3. Replace CIFAR-10 batch with greyscale images and confirm epsilon still correlates reasonably

## Open Questions the Paper Calls Out

- Question: Why does epsilon perform significantly worse on NAS-Bench-NLP compared to image classification benchmarks, and what specific architectural differences between language models and CNNs cause this degradation?
  - Basis in paper: Paper notes epsilon's performance drops significantly on NAS-Bench-NLP with "noise level is beyond acceptable" and "the trend is visible enough to conclude that epsilon metric can apply to recurrent type architectures"
  - Why unresolved: Paper acknowledges poor performance but doesn't investigate architectural or data-specific reasons for failure on language tasks
  - What evidence would resolve it: Comparative analysis of epsilon behavior across RNN cells, embedding layers, attention mechanisms versus CNN components, plus controlled experiments varying sequence length, vocabulary size, and embedding initialization strategies

- Question: What is the theoretical foundation explaining why dispersion between two constant weight initializations correlates with trained accuracy, and what mathematical properties of neural networks create this relationship?
  - Basis in paper: Paper states "a coherent theoretical foundation of epsilon is missing and should be developed in future" and discusses epsilon as "difference in output distribution shapes between initialisations"
  - Why unresolved: Authors provide empirical observations and intuitions but don't establish rigorous mathematical proofs connecting constant weight dispersion to generalization or training dynamics
  - What evidence would resolve it: Formal mathematical proofs linking constant weight output dispersion to spectral properties of weight matrices, gradient flow characteristics, or information propagation in neural networks

- Question: How can selection of constant weight values be automated to eliminate need for manual tuning, and what optimization framework would determine optimal weight ranges for arbitrary search spaces?
  - Basis in paper: Paper states "The only significant disadvantage of the method is that it requires choice of constant weight values during initialisation" and "Our tests show that it must be set up individually for each search space"
  - Why unresolved: While paper demonstrates weight sensitivity through ablation studies, it doesn't propose algorithmic solution for automatically determining appropriate weight ranges for new problems
  - What evidence would resolve it: Development of meta-learning approach that learns weight selection policies from multiple search spaces, or self-tuning mechanism that adapts weight ranges based on architectural characteristics like depth, width, and activation patterns

## Limitations
- Weight sensitivity: Performance depends on careful selection of constant weight values, requiring individual tuning for each search space
- Language task degradation: Significantly worse performance on NAS-Bench-NLP compared to image classification benchmarks
- Theoretical foundation: Lacks rigorous mathematical proof connecting output dispersion to generalisation capacity

## Confidence

- High Confidence: Theoretical mechanism linking output dispersion to architectural expressiveness is well-founded
- Medium Confidence: Claim that epsilon eliminates dependence on training hyperparameters and labels is valid for forward pass computation
- Low Confidence: Assertion that epsilon "significantly outperforms existing zero-cost NAS methods" requires direct comparison with all competing metrics

## Next Checks

1. Test epsilon sensitivity by varying constant weight pairs across multiple orders of magnitude and measuring correlation stability
2. Evaluate ranking consistency by computing epsilon scores across multiple random minibatches and measuring rank correlation variance
3. Compare epsilon directly against all major zero-cost NAS metrics using identical benchmark datasets and evaluation protocols