---
ver: rpa2
title: 'Style Over Substance: Evaluation Biases for Large Language Models'
arxiv_id: '2307.03025'
source_url: https://arxiv.org/abs/2307.03025
tags:
- human
- answer
- evaluation
- language
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates biases in human and LLM evaluation of text
  generation quality. The authors find that human evaluators are inconsistent, fail
  to fact-check, and are biased towards longer answers.
---

# Style Over Substance: Evaluation Biases for Large Language Models

## Quick Facts
- arXiv ID: 2307.03025
- Source URL: https://arxiv.org/abs/2307.03025
- Reference count: 14
- Key outcome: Multi-dimensional Elo rating system improves LLM evaluation quality, particularly for factual accuracy, but doesn't improve human evaluation

## Executive Summary
This paper investigates biases in human and LLM evaluation of text generation quality, finding that both types of evaluators exhibit significant biases. Human evaluators are inconsistent, fail to fact-check, and show strong bias toward longer answers. LLM evaluators are more consistent and better at fact-checking but still exhibit length bias and first-answer preference. The authors propose a multi-dimensional Elo rating system that evaluates text along accuracy, helpfulness, and language dimensions independently. This approach significantly improves LLM evaluation quality, particularly for factual accuracy, but does not improve human evaluation due to crowd worker limitations.

## Method Summary
The study generates flawed answers using GPT-4 with intentional errors (factual, language, length variations) and collects pairwise comparisons from both human evaluators (via Amazon Mechanical Turk) and LLM judges (GPT-4 and Claude-1). Elo ratings are computed separately for each dimension (accuracy, helpfulness, language) to create a Multi-Elo Rating System. The approach compares single-dimensional versus multi-dimensional evaluation to assess the impact on evaluation quality and bias reduction.

## Key Results
- LLM evaluators are more consistent and fact-check more effectively than crowd-sourced humans
- Both human and LLM judges exhibit length bias, preferring longer answers regardless of quality
- Multi-dimensional Elo rating system significantly improves LLM evaluation quality for factual accuracy
- Human evaluation shows no improvement with multi-dimensional approach due to crowd worker limitations
- Evaluators are biased toward the first answer in pairwise comparisons

## Why This Works (Mechanism)

### Mechanism 1
Separating evaluation into multiple independent dimensions reduces bias from conflating different quality aspects. By evaluating "Accuracy", "Helpfulness", and "Language" separately using the Elo rating system, each dimension captures distinct aspects of text quality without interference from other factors. Core assumption: The three dimensions are sufficiently orthogonal that they can be evaluated independently without significant overlap.

### Mechanism 2
LLM judges are more consistent and fact-check more effectively than crowd-sourced humans. GPT-4 and Claude-1 systematically evaluate factual accuracy while crowd workers often skip fact-checking due to time constraints or lack of attention. Core assumption: LLMs have sufficient world knowledge to detect factual errors in the generated answers.

### Mechanism 3
Both human and LLM judges exhibit length bias, preferring longer answers regardless of quality. Evaluators conflate "level of detail" with text length, rating longer answers higher even when they contain factual errors. Core assumption: The evaluation criteria explicitly include "level of detail" which creates incentive to prefer longer text.

## Foundational Learning

- Concept: Elo rating system mechanics and K-factor adjustment
  - Why needed here: Understanding how Elo ratings update based on pairwise comparisons is essential for implementing Multi-Elo Rating System
  - Quick check question: If two models have ratings of 1200 and 1000, what is the expected score for the higher-rated model?

- Concept: Fact-checking methodology and error detection thresholds
  - Why needed here: Knowing how LLMs identify factual errors versus minor mistakes determines the effectiveness of accuracy dimension
  - Quick check question: What distinguishes "minor factual errors" from "major factual errors" in the answer generation process?

- Concept: Crowd-sourcing quality control and annotator reliability
  - Why needed here: Understanding limitations of crowd workers explains why human evaluation showed no improvement with multi-dimensional approach
  - Quick check question: What criteria were used to filter crowd workers in this study?

## Architecture Onboarding

- Component map: Answer generation pipeline (GPT-4 with error injection) -> Human evaluation interface (AMT platform) -> LLM evaluation system (GPT-4 and Claude-1 judges) -> Multi-Elo rating calculator (separate for each dimension) -> Analysis dashboard (comparison across dimensions)

- Critical path: Answer generation → Pairwise comparison collection → Elo rating calculation → Multi-dimensional analysis
- Design tradeoffs: Separate dimension evaluation vs. compound evaluation - separate gives better accuracy but requires 3x the LLM calls
- Failure signatures: Inconsistent Elo ratings across dimensions, crowd workers consistently choosing longer answers, LLMs failing to detect obvious factual errors
- First 3 experiments:
  1. Compare single Elo vs. Multi-Elo for a small subset of questions to validate dimensional independence
  2. Test LLM vs. human agreement rates on factual accuracy dimension only
  3. Measure length bias by creating controlled pairs with identical content but different lengths

## Open Questions the Paper Calls Out

### Open Question 1
How do crowd-sourced human annotators' evaluations differ from expert annotators' evaluations when assessing LLM outputs? The paper only uses crowd-sourced annotators and explicitly leaves the investigation of expert annotators to future work.

### Open Question 2
What additional dimensions beyond "Accuracy", "Helpfulness", and "Language" would be necessary for a comprehensive evaluation of LLM outputs? The paper acknowledges that their three dimensions may not be sufficient to encompass all aspects required for an ideal answer.

### Open Question 3
How does the order of answer presentation affect human evaluators' judgments, and can this bias be mitigated? While the bias is identified, the paper doesn't explore methods to mitigate this bias or fully understand its impact.

### Open Question 4
How do different LLM judges (e.g., GPT-4 vs Claude-1) compare in their ability to detect factual errors and evaluate outputs consistently? While differences are noted, the paper doesn't conduct a comprehensive comparison of different LLM judges' capabilities.

## Limitations
- The three evaluation dimensions may not be truly independent, potentially missing important quality trade-offs
- GPT-4-generated flawed answers may not accurately represent real-world error distributions
- The study uses only 40 questions, which may not capture full evaluation scenario diversity
- LLM judges, while improved, are not perfect and may miss nuanced errors or exhibit their own biases

## Confidence
- Multi-dimensional Elo effectiveness: Medium confidence - shows improvements for LLM evaluation but fails for human evaluation
- LLM vs. human fact-checking: High confidence - multiple comparisons consistently show LLMs detecting errors humans miss
- Length bias findings: High confidence - robust effect across both human and LLM evaluators
- Dimension independence assumption: Low confidence - insufficient analysis of inter-dimensional correlations

## Next Checks
1. Conduct a comprehensive correlation study between accuracy, helpfulness, and language Elo ratings to quantify dimensional independence and identify interaction effects.

2. Compare the distribution of error types in GPT-4-generated answers versus errors found in real-world deployed LLM applications to assess generalizability.

3. Replicate the study using a larger, more diverse question set (e.g., 200+ questions across multiple domains) to verify findings generalize beyond the original 40-question corpus.