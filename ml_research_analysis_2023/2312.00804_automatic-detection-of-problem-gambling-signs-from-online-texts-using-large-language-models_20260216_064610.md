---
ver: rpa2
title: Automatic detection of problem-gambling signs from online texts using large
  language models
arxiv_id: '2312.00804'
source_url: https://arxiv.org/abs/2312.00804
tags:
- gambling
- posts
- were
- data
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Using data from a German gambling discussion board, we trained
  a machine learning model to automatically detect signs of problem gambling from
  forum posts. Manual annotation of forum posts was performed based on diagnostic
  criteria for gambling disorder and on items of the Gambling Related Cognitions Scale
  (GRCS).
---

# Automatic detection of problem-gambling signs from online texts using large language models

## Quick Facts
- arXiv ID: 2312.00804
- Source URL: https://arxiv.org/abs/2312.00804
- Reference count: 40
- Using data from a German gambling discussion board, we trained a machine learning model to automatically detect signs of problem gambling from forum posts.

## Executive Summary
This study presents a machine learning approach to automatically detect problem-gambling signs from online forum posts using a fine-tuned BERT model. The researchers manually annotated 504 German forum posts based on DSM-5 diagnostic criteria and Gambling Related Cognitions Scale (GRCS) items, distinguishing between problem-gambling content and gambling-only content. Through 5-fold cross-validation, the best model achieved an F1 score of 0.71, demonstrating that high-quality manual annotation based on clinical criteria can yield satisfactory classification performance even with limited training data.

## Method Summary
The researchers scraped 205,385 posts from a major German gambling discussion board and manually annotated 504 posts as containing problem-gambling content or gambling content only. Annotation was based on DSM-5 diagnostic criteria for gambling disorder and GRCS items to identify gambling-related cognitive distortions. The annotated data was preprocessed (lowercasing and punctuation removal) and used to fine-tune a German BERT model (bert-base-german-uncased). Model performance was evaluated using 5-fold cross-validation across different training set sizes (69/69, 92/92, 138/138, 348/138, 348/348 gambling/problem-gambling posts).

## Key Results
- Manual annotation based on DSM-5 and GRCS criteria produced high-quality training labels
- Fine-tuned BERT model achieved F1 score of 0.71 with precision of 0.95
- Punctuation removal during preprocessing improved model performance by reducing noise
- Model struggled to distinguish casino complaints from genuine problem-gambling content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Manual annotation based on DSM-5 criteria and GRCS items provides high-quality training labels that improve classification performance.
- Mechanism: Human annotators use established diagnostic criteria and validated cognitive scales to label forum posts, ensuring that the model learns to detect clinically relevant problem-gambling content rather than relying on surface-level keyword matching.
- Core assumption: The presence of problem-gambling content can be reliably identified by humans using DSM-5 and GRCS criteria.
- Evidence anchors:
  - [abstract] "Training data were generated by manual annotation and by taking into account diagnostic criteria and gambling-related cognitive distortions."
  - [section] "Annotation of gambling forum posts [...] Posts were also coded as targets if none of the items were explicitly reported in the descriptions, but the person clearly self-identified as being addicted to gambling, was actively seeking help or treatment for gambling disorder, or mentioned currently undergoing treatment."
  - [corpus] Found 25 related papers; average neighbor FMR=0.37; related titles include detection of problem gambling with less features, indicating prior focus on feature engineering.

### Mechanism 2
- Claim: Fine-tuning a pre-trained BERT model on a small, high-quality dataset yields strong classification performance (F1=0.71).
- Mechanism: The transformer-based BERT model uses self-attention to capture contextual relationships between words, allowing it to generalize from limited annotated data by leveraging its pre-trained language understanding.
- Core assumption: Pre-trained language models retain sufficient domain-general knowledge to be effective after fine-tuning on domain-specific, small datasets.
- Evidence anchors:
  - [abstract] "Using k-fold cross-validation, our models achieved a precision of 0.95 and F1 score of 0.71, demonstrating that satisfactory classification performance can be achieved by generating high-quality training material through manual annotation based on diagnostic criteria."
  - [section] "BERT enables solving NLP tasks in a supervised fashion when the dataset labelled for training is not large enough for achieving satisfactory classification performance using a model trained from scratch."
  - [corpus] Related work used approximate nearest neighbor algorithm (F1=0.87) and SVM with BoW (F1=0.72), showing BERT performance is competitive.

### Mechanism 3
- Claim: Removing punctuation during preprocessing improves model performance by reducing noise related to emotional tone markers.
- Mechanism: Punctuation in forum posts often conveys urgency or emotional emphasis; removing it helps the model focus on semantic content rather than stylistic variations.
- Core assumption: Punctuation is not essential for distinguishing problem-gambling content from gambling-only content.
- Evidence anchors:
  - [section] "Performance was above the baseline for binary classification (above 50%, for the balanced data sets only) for all models with lowercasing only (keeping punctuation) and a training set size of 138/138 and larger. [...] Removing punctuations was not beneficial for model performance. On the contrary, it appears that punctuations conveyed some information relevant for distinguishing problem from non-problem gambling posts."
  - [corpus] Related studies did not explicitly address punctuation preprocessing, indicating this may be a novel insight.

## Foundational Learning

- Concept: DSM-5 diagnostic criteria for gambling disorder
  - Why needed here: The annotation process relies on DSM-5 criteria to identify problem-gambling posts.
  - Quick check question: What are the nine criteria listed in DSM-5 for diagnosing gambling disorder?

- Concept: Gambling Related Cognitions Scale (GRCS) and its subscales
  - Why needed here: GRCS items are used to identify gambling-related cognitive distortions in forum posts.
  - Quick check question: What are the five subscales of the GRCS?

- Concept: Transformer model architecture and self-attention mechanism
  - Why needed here: BERT's performance depends on its ability to capture contextual relationships via self-attention.
  - Quick check question: How does the self-attention mechanism in transformers differ from recurrent neural networks?

## Architecture Onboarding

- Component map: Web scraping -> SQLite storage -> Post selection -> Manual annotation -> Training set creation -> Text preprocessing -> BERT tokenization -> Fine-tuning -> Cross-validation -> Error analysis
- Critical path: High-quality manual annotation -> Model fine-tuning -> Cross-validation -> Error analysis -> Model refinement
- Design tradeoffs:
  - Small annotated dataset vs. larger but noisier dataset: Quality over quantity.
  - BERT model complexity vs. overfitting risk: Balance model capacity with dataset size.
  - Punctuation removal vs. information loss: Test impact on model performance.
- Failure signatures:
  - Low precision: Model over-generates false positives, possibly due to overlap between casino complaints and problem-gambling posts.
  - Low recall: Model misses problem-gambling posts, possibly due to reliance on explicit keywords.
  - High variance across folds: Model overfits to specific folds, indicating insufficient data or noise.
- First 3 experiments:
  1. Test model performance with and without punctuation removal to confirm preprocessing impact.
  2. Evaluate model performance using different training set sizes (e.g., 69/69, 92/92, 138/138) to identify optimal data volume.
  3. Analyze error patterns by manually reviewing false-positive and false-negative predictions to identify systematic biases.

## Open Questions the Paper Calls Out

- Question: How does the inclusion of inconclusive posts as a third category affect model performance compared to excluding them?
- Basis in paper: [inferred] The paper mentions that excluding inconclusive posts may limit training data and affect real-world performance, suggesting this as a potential future direction.
- Why unresolved: The authors did not test the impact of including inconclusive posts on model performance.
- What evidence would resolve it: Training and evaluating the model with inconclusive posts as a third category and comparing performance metrics to the current approach.

- Question: How does the model perform on data from different social media platforms compared to the gambling forum used in this study?
- Basis in paper: [explicit] The authors note that the model may not generalize to other platforms and suggest assessing its applicability to texts from other online communities.
- Why unresolved: The study only tested the model on data from a single German gambling forum.
- What evidence would resolve it: Evaluating the model's performance on data from various social media platforms and comparing the results.

- Question: What are the potential biases in the model's predictions, and how can they be mitigated?
- Basis in paper: [explicit] The authors discuss the risk of algorithmic bias in AI systems and emphasize the need to consider this when implementing such models.
- Why unresolved: The paper does not provide a detailed analysis of potential biases in the model's predictions or strategies to mitigate them.
- What evidence would resolve it: Conducting a thorough bias analysis of the model's predictions and proposing methods to reduce bias in future iterations.

## Limitations
- Manual annotation introduces potential subjectivity and inter-rater reliability concerns
- Small dataset (504 posts) and focus on German-language posts may limit generalizability
- Difficulty distinguishing casino complaints from problem-gambling content leads to false positives

## Confidence
- **High Confidence**: The effectiveness of manual annotation using DSM-5 and GRCS criteria for generating training labels
- **Medium Confidence**: The BERT model's ability to achieve satisfactory classification performance (F1=0.71) from a small, high-quality dataset
- **Low Confidence**: The generalizability of results to other languages, platforms, or for early detection of problem gambling

## Next Checks
1. Conduct inter-rater reliability analysis on the manual annotation process to quantify annotation consistency and potential biases
2. Test model performance on posts from different gambling platforms and languages to assess generalizability
3. Implement and evaluate the model's performance on longitudinal data to assess its potential for early detection of problem gambling signs