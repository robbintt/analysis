---
ver: rpa2
title: CNN based Cuneiform Sign Detection Learned from Annotated 3D Renderings and
  Mapped Photographs with Illumination Augmentation
arxiv_id: '2308.11277'
source_url: https://arxiv.org/abs/2308.11277
tags:
- sign
- renderings
- cuneiform
- bounding
- photographs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting cuneiform signs
  in images of clay tablets, a critical step in developing OCR for this ancient script.
  The authors propose a novel approach that leverages 3D renderings and photographs
  of tablets, using a RepPoints-based CNN to locate signs as bounding boxes.
---

# CNN based Cuneiform Sign Detection Learned from Annotated 3D Renderings and Mapped Photographs with Illumination Augmentation

## Quick Facts
- arXiv ID: 2308.11277
- Source URL: https://arxiv.org/abs/2308.11277
- Reference count: 30
- Primary result: Models trained on 3D renderings with MSII filtering and illumination augmentation outperform those trained solely on photographs for cuneiform sign detection.

## Executive Summary
This paper tackles the challenge of detecting cuneiform signs in images of clay tablets, a critical step in developing OCR for this ancient script. The authors propose a novel approach that leverages 3D renderings and photographs of tablets, using a RepPoints-based CNN to locate signs as bounding boxes. They introduce a method for transferring annotations between 3D renderings and photographs, and apply illumination augmentation to enhance the dataset. Their results show that models trained on 3D renderings, particularly those using Multi-Scale Integral Invariant (MSII) filtering and illumination augmentation, outperform models trained solely on photographs. When combining all image types, including photographs, the model achieves the best overall performance. On photographs, the model achieves an Average Precision (AP) of 0.57 at 50% IoU overlap, which is comparable to state-of-the-art methods. The study highlights the value of using 3D data and diverse image types for cuneiform sign detection, paving the way for improved OCR pipelines and further advancements in digital Assyriology.

## Method Summary
The authors use a RepPoints-based CNN with a ResNet18 backbone to detect cuneiform signs in images. They train the model on a combination of 3D renderings (VL, MSII, Mixed) and photographs of clay tablets from the HeiCuBeDa and MaiCuBeDa datasets. Annotations from 3D renderings are mapped to photographs using geometric alignment. Illumination augmentation is applied to VL renderings to improve robustness to varying lighting conditions. The model is trained using a combination of localization and classification losses with specific weights. The final model combines all image types and achieves the best overall performance, with an AP of 0.57 on photographs at 50% IoU overlap.

## Key Results
- Models trained on 3D renderings with MSII filtering and illumination augmentation outperform those trained solely on photographs.
- Illumination augmentation improves sign detection performance, particularly at higher IoU thresholds (75% and 90%).
- Training on a combination of different image types (renderings and photographs) leads to better overall performance than training on a single image type.
- On photographs, the model achieves an AP of 0.57 at 50% IoU overlap, which is comparable to state-of-the-art methods.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: 3D renderings, especially with Multi-Scale Integral Invariant (MSII) filtering, improve sign detection performance compared to photographs alone.
- Mechanism: MSII filtering enhances the contrast between the impressed cuneiform wedges and the clay surface by leveraging curvature information, making the 3D shape of the signs more pronounced in the rendered 2D images.
- Core assumption: The enhanced contrast from MSII filtering preserves sufficient information for the CNN to accurately localize signs.
- Evidence anchors:
  - [abstract]: "We use image data from GigaMesh's MSII (curvature, see https://gigamesh.eu) based rendering... The results show that using rendered 3D images for sign detection performs better than other work on photographs."
  - [section 4.2]: "Using the MSII filter on the meshes results in a higher contrast between the wedges and the clay surface based on the curvature. 3D renderings after MSII filtering are much more legible than photographs, which is consistent with human perception of cuneiform tablets."
  - [corpus]: Weak evidence; corpus contains no papers directly evaluating MSII filtering for cuneiform sign detection.

### Mechanism 2
- Claim: Illumination augmentation (IA) improves the model's robustness to varying lighting conditions, leading to better generalization.
- Mechanism: By rendering the 3D mesh under different virtual lighting conditions (varying azimuth angle), the model is exposed to a wider range of sign appearances during training, forcing it to learn features that are invariant to illumination changes.
- Core assumption: The variations in sign appearance due to illumination are significant enough to impact model performance, and the augmented dataset effectively covers this variation space.
- Evidence anchors:
  - [abstract]: "We use image data from GigaMesh's MSII (curvature, see https://gigamesh.eu) based rendering, Phong-shaded 3D models, and photographs as well as illumination augmentation."
  - [section 2.3]: "Similar to [20], we used this characteristic of Cuneiform to render our available tablets with different illumination and have augmented our data set with a huge set of virtual light renderings."
  - [section 4.2]: "Applying this method to augment the dataset has shown an improvement in the sign detector for VL renderings. Specifically, an increase of about 0.1 of AP@75 and 0.2 of AP@90..."
  - [corpus]: Weak evidence; corpus contains no papers directly evaluating illumination augmentation for cuneiform sign detection.

### Mechanism 3
- Claim: Training on a combination of different image types (renderings and photographs) leads to better overall performance than training on a single image type.
- Mechanism: Different image types capture different aspects of the cuneiform signs. Renderings emphasize 3D shape and curvature, while photographs capture texture and material properties. Combining them provides a richer, more diverse training set, forcing the model to learn a more robust and generalizable representation of cuneiform signs.
- Core assumption: The different image types contain complementary information about the signs, and the model can effectively integrate this information.
- Evidence anchors:
  - [abstract]: "In addition, our approach gives reasonably good results for photographs only, while it is best used for mixed datasets."
  - [section 4.2]: "The use of IA, as described in section 2.3, only as different VL renderings, has shown a slight improvement compared to the evaluation on VL renderings... However, the result on the mixed renderings is close to the combination without IA."
  - [section 4.3]: "As described in Section 4.2, the sign detection achieved sufficient results, especially considering our dataset's missing ground truth annotation... the best results have been achieved by the training with a combination of all datasets, including photographs."
  - [corpus]: Weak evidence; corpus contains no papers directly evaluating the combination of different image types for cuneiform sign detection.

## Foundational Learning

- Concept: Object Detection with Bounding Boxes
  - Why needed here: The task is to locate cuneiform signs in images, which is a classic object detection problem. The model needs to predict bounding boxes around each sign.
  - Quick check question: What is the difference between a one-stage and a two-stage object detector, and which type is used in this paper?

- Concept: Convolutional Neural Networks (CNNs)
  - Why needed here: CNNs are the backbone of the sign detection model. They are used to extract features from the input images that are then used to predict the locations of the signs.
  - Quick check question: What is the role of the ResNet18 backbone in the RepPoints architecture?

- Concept: Data Augmentation
  - Why needed here: Data augmentation is used to artificially increase the size and diversity of the training dataset. In this case, illumination augmentation is used to make the model more robust to varying lighting conditions.
  - Quick check question: What is the difference between illumination augmentation and other common data augmentation techniques like rotation or scaling?

## Architecture Onboarding

- Component map:
  Input: 512x512 pixel grayscale images (renderings or photographs) -> Backbone: ResNet18 (simplified from FPN) -> Feature Map: 64x64 resolution -> RepPoints Detector: Predicts k=9 representation points and confidence values -> Post-processing: Non-Maximum Suppression (NMS) with IoU threshold of 0.4 -> Output: Bounding boxes (xmin, ymin, xmax, ymax) with confidence scores

- Critical path:
  1. Image preprocessing (cropping, normalization)
  2. Feature extraction with ResNet18
  3. RepPoints prediction (classification and localization)
  4. Conversion of representation points to bounding boxes
  5. NMS to remove duplicate detections
  6. Evaluation against ground truth

- Design tradeoffs:
  - Simplified backbone (ResNet18 without FPN) for faster training and inference
  - Anchor-free approach (RepPoints) to avoid the need for predefined anchor boxes
  - Increased IoU thresholds (0.6 and 0.7) to account for dense sign placement
  - Dropout layers to prevent overfitting

- Failure signatures:
  - Low AP@50: Model is not detecting enough signs (high false negative rate)
  - Low AP@75: Model is detecting signs but with poor localization accuracy (high false positive rate or inaccurate bounding boxes)
  - High AP@90: Model is overfitting to the training data or ground truth bounding boxes are not accurate

- First 3 experiments:
  1. Train and evaluate the model on photographs only to establish a baseline performance.
  2. Train and evaluate the model on MSII renderings only to assess the impact of the preprocessing technique.
  3. Train and evaluate the model on a combination of photographs and renderings to test the benefit of using multiple image types.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the accuracy of cuneiform sign detection using 3D renderings compare to other imaging methods (e.g., photographs) when the dataset is fully annotated?
- Basis in paper: [explicit] The authors note that using rendered 3D images for sign detection performs better than other work on photographs, but the actual performance is better than the numbers due to missing ground truth annotations.
- Why unresolved: The dataset used in the study is incomplete, with many missing annotations, which may impact the accuracy of the results.
- What evidence would resolve it: A fully annotated dataset of cuneiform tablets with both 3D renderings and photographs would allow for a direct comparison of the accuracy of sign detection using different imaging methods.

### Open Question 2
- Question: How does the use of different preprocessing techniques (e.g., Ambient Occlusion) on 3D meshes affect the performance of the cuneiform sign detector?
- Basis in paper: [explicit] The authors mention that their research has shown that using preprocessing techniques like MSII filtering on 3D meshes can increase the performance of the sign detector, but they do not explore other preprocessing techniques.
- Why unresolved: The study only evaluates the impact of MSII filtering on the performance of the sign detector, leaving other preprocessing techniques unexplored.
- What evidence would resolve it: Evaluating the performance of the sign detector using various preprocessing techniques on 3D meshes would provide insights into which methods are most effective for improving sign detection accuracy.

### Open Question 3
- Question: How does the performance of the cuneiform sign detector change when using a combination of different imaging methods (e.g., 3D renderings, photographs, and other preprocessing techniques)?
- Basis in paper: [explicit] The authors note that the best results were obtained by training with a combination of all datasets, including photographs, suggesting that using multiple imaging methods can improve sign detection accuracy.
- Why unresolved: The study does not explore the impact of combining different imaging methods and preprocessing techniques on the performance of the sign detector.
- What evidence would resolve it: Experimenting with different combinations of imaging methods and preprocessing techniques would provide insights into the optimal approach for improving sign detection accuracy.

## Limitations

- The study uses a relatively small annotated dataset (approximately 500 tablets), which may limit the generalizability of the results.
- The transfer of annotations from 3D renderings to photographs assumes perfect geometric correspondence, which may not hold for all tablet surfaces.
- The evaluation focuses on sign detection rather than end-to-end OCR, leaving the impact on downstream tasks uncertain.

## Confidence

- The study's primary limitations include a relatively small annotated dataset, potential issues with annotation transfer, and a focus on sign detection rather than end-to-end OCR. Confidence in the core claims is Medium: while the methodology is sound and the improvements from MSII filtering and illumination augmentation are well-supported by quantitative results, the lack of extensive ablation studies and comparisons with state-of-the-art object detection methods for cuneiform signs prevents higher confidence. The success of combining different image types is plausible but requires further validation.

## Next Checks

1. Perform ablation studies to isolate the individual contributions of MSII filtering, illumination augmentation, and mixed training data to overall performance.
2. Evaluate the model on a held-out test set of photographs not used in any training or validation phase to assess true generalization.
3. Compare the RepPoints-based approach against other state-of-the-art object detection architectures (e.g., Faster R-CNN, RetinaNet) on the same dataset to benchmark performance.