---
ver: rpa2
title: Establishing Vocabulary Tests as a Benchmark for Evaluating Large Language
  Models
arxiv_id: '2310.14703'
source_url: https://arxiv.org/abs/2310.14703
tags:
- performance
- items
- llms
- test
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated seven Large Language Models (LLMs) including
  GPT-4, ChatGPT, Bard, Llama, and Mistral on vocabulary tests to assess their linguistic
  competence. Models were tested on English TOEFL and StuVoc tests, a Yes/No word/nonword
  discrimination test (LexTALE), and Spanish vocabulary tests.
---

# Establishing Vocabulary Tests as a Benchmark for Evaluating Large Language Models

## Quick Facts
- arXiv ID: 2310.14703
- Source URL: https://arxiv.org/abs/2310.14703
- Reference count: 5
- Models were tested on English TOEFL and StuVoc tests, Yes/No word/nonword discrimination (LexTALE), and Spanish vocabulary tests.

## Executive Summary
This study evaluated seven Large Language Models (LLMs) including GPT-4, ChatGPT, Bard, Llama, and Mistral on vocabulary tests to assess their linguistic competence. Models were tested on English TOEFL and StuVoc tests, a Yes/No word/nonword discrimination test (LexTALE), and Spanish vocabulary tests. Most LLMs achieved high performance on existing words (90-100% correct) but struggled significantly with nonwords, often providing false meanings. Spanish test performance was notably worse than English, especially for nonwords. The findings confirm vocabulary tests remain valuable for LLM evaluation and highlight persistent weaknesses in nonword processing across models and languages.

## Method Summary
The study evaluated seven LLMs using automated API-based testing with temperature set to zero for deterministic responses. Models were tested on TOEFL items (80 items), StuVoc items (English: 150 items, Spanish: 80 selected + 66 non-selected), and LexTALE items (English: 40 words + 20 nonwords, Spanish: 60 words + 28 nonwords). Simple prompts were used without context, asking models to identify whether items were existing words. Performance was measured as percentage correct for words versus nonwords across test types and languages.

## Key Results
- Most LLMs achieved 90-100% correct on existing English words but performed significantly worse on nonwords
- Spanish test performance was notably worse than English, especially for nonwords
- Larger Llama models showed better word performance but worse nonword performance compared to smaller versions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vocabulary tests reveal persistent gaps in LLM word knowledge, especially for nonwords.
- Mechanism: LLMs rely on co-occurrence patterns; nonwords lack training data, so models hallucinate meanings.
- Core assumption: LLM performance degrades predictably when encountering out-of-distribution inputs.
- Evidence anchors:
  - [abstract] "Models performed significantly worse on the nonword items, in line with other observations that current major language models provide non-existent information."
  - [section] "Interestingly, for Llama we see better performance on the words as the model becomes larger and at the same time worse performance on the nonwords."
- Break condition: If models begin to use explicit nonword rejection mechanisms or external validation, this gap could close.

### Mechanism 2
- Claim: Performance varies across languages due to training data imbalance.
- Mechanism: Models trained primarily on English data show degraded accuracy on non-English vocabulary tasks.
- Core assumption: Training corpus size and language-specific representation quality directly impact downstream task performance.
- Evidence anchors:
  - [abstract] "Spanish test performance was notably worse than English, especially for nonwords."
  - [section] "The Spanish results suggest that present-day LLMs are likely to perform less good in languages other than English, for which the training material was the most extensive."
- Break condition: If multilingual training corpora are balanced or if models incorporate language-agnostic representations.

### Mechanism 3
- Claim: Vocabulary test performance remains informative even for state-of-the-art LLMs.
- Mechanism: Even highly capable models make errors on divergent items, revealing limits in semantic representation.
- Core assumption: Vocabulary tests provide a consistent, interpretable metric for linguistic competence across model sizes and architectures.
- Evidence anchors:
  - [abstract] "The findings confirm vocabulary tests remain valuable for LLM evaluation and highlight persistent weaknesses in nonword processing across models and languages."
  - [section] "As expected, most models achieved high performance and the bigger Llama models did better than the basic 7B versions."
- Break condition: If models reach near-perfect performance across all test items, the discriminative power of these tests would diminish.

## Foundational Learning

- Concept: Co-occurrence-based semantic modeling
  - Why needed here: Vocabulary tests evaluate whether models can generalize word meanings from co-occurrence patterns.
  - Quick check question: How do LLMs infer word meanings without explicit definitions?

- Concept: Nonword detection and hallucination
  - Why needed here: Yes/No tests expose whether models can distinguish real words from plausible but non-existent sequences.
  - Quick check question: What cues do LLMs use to decide if a letter string is a real word?

- Concept: Cross-linguistic performance scaling
  - Why needed here: Spanish vs. English performance differences reveal the impact of training corpus size on linguistic competence.
  - Quick check question: Why might models perform worse on non-English vocabulary tests?

## Architecture Onboarding

- Component map: Test automation pipeline -> API interface -> LLM -> response parsing -> evaluation metrics
- Critical path: Prompt construction -> API call -> response interpretation -> correctness check
- Design tradeoffs: Using simple prompts (as in the study) vs. chain-of-thought prompting; language-specific vs. multilingual tests
- Failure signatures: False positives on nonwords, language-dependent degradation, inconsistent performance across models
- First 3 experiments:
  1. Run the same StuVoc items on GPT-4 and Llama-7B with and without sentence context to measure context sensitivity.
  2. Test GPT-4 on LexTALE words vs. nonwords to quantify hallucination rate.
  3. Compare English and Spanish StuVoc performance across all models to map language-dependent degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do vocabulary tests remain a valuable benchmark for evaluating Large Language Models (LLMs) in languages other than English and Spanish?
- Basis in paper: [explicit] The authors tested LLMs on vocabulary tests in English and Spanish, finding significant performance degradation in Spanish, especially for nonwords. They suggest performance might drop further for less studied languages.
- Why unresolved: The study only tested English and Spanish. Performance in other languages, particularly those with less training data, remains unknown.
- What evidence would resolve it: Testing LLMs on vocabulary tests in a variety of languages, especially those with limited training data, would provide evidence of performance degradation across different linguistic contexts.

### Open Question 2
- Question: Can vocabulary tests be used to identify and mitigate the issue of nonword hallucination in LLMs?
- Basis in paper: [explicit] The study found that LLMs often provide false meanings for nonwords, a phenomenon known as hallucination. The authors suggest that vocabulary tests can help identify these issues.
- Why unresolved: While the study identified the problem of nonword hallucination, it did not explore methods to mitigate this issue using vocabulary tests.
- What evidence would resolve it: Research demonstrating the use of vocabulary tests to systematically identify and reduce nonword hallucination in LLMs would provide evidence for their utility in this context.

### Open Question 3
- Question: How can vocabulary tests be used to improve the design and validation of new vocabulary tests?
- Basis in paper: [explicit] The authors suggest that LLM performance on vocabulary tests can help identify ambiguities in test item construction and avoid issues like nonwords that exist in dictionaries.
- Why unresolved: The study did not provide a detailed methodology for using LLM performance to guide the creation and validation of new vocabulary tests.
- What evidence would resolve it: A systematic approach for using LLM performance data to inform the design, validation, and refinement of vocabulary tests would provide evidence for their practical utility in test development.

## Limitations

- Single run of each test on each model without reported variance or confidence intervals
- Automated testing approach may not capture nuanced responses or contextual understanding
- Potential changes in model performance over time due to frequent updates not accounted for

## Confidence

- **High Confidence**: Models perform significantly worse on nonwords than words (supported by multiple test types and models)
- **Medium Confidence**: Spanish performance is notably worse than English (based on limited Spanish test items and potential sampling bias)
- **Medium Confidence**: Larger models show better word performance but worse nonword performance (observed in Llama family but not consistently across all models)

## Next Checks

1. **Statistical validation**: Run each test condition 10+ times across different model instances to establish performance variance and calculate confidence intervals for all reported metrics.
2. **Cross-linguistic expansion**: Test the same models on additional non-English languages (e.g., French, German, Chinese) to determine if the English advantage is consistent across different language families and orthographic systems.
3. **Context sensitivity assessment**: Repeat key tests using contextualized prompts (providing sentences or definitions) to determine whether the performance gaps on nonwords persist when models receive additional linguistic context.