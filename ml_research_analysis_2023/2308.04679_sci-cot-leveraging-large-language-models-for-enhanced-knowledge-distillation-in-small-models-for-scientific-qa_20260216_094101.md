---
ver: rpa2
title: 'Sci-CoT: Leveraging Large Language Models for Enhanced Knowledge Distillation
  in Small Models for Scientific QA'
arxiv_id: '2308.04679'
source_url: https://arxiv.org/abs/2308.04679
tags:
- reasoning
- sci-cot
- performance
- question
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of leveraging the reasoning capabilities
  of large language models (LLMs) for small models, which are limited by computational
  constraints. The authors propose Sci-CoT, a two-stage framework that separates the
  processes of generating rationales and inferring answers.
---

# Sci-CoT: Leveraging Large Language Models for Enhanced Knowledge Distillation in Small Models for Scientific QA

## Quick Facts
- arXiv ID: 2308.04679
- Source URL: https://arxiv.org/abs/2308.04679
- Authors: 
- Reference count: 19
- Primary result: Demonstrates that an 80-million parameter model can exceed BLOOM-176B performance on ARC-Easy dataset and achieve comparable accuracy to OPT-175B on ARC-Challenge dataset using a two-stage knowledge distillation framework

## Executive Summary
This paper addresses the challenge of transferring reasoning capabilities from large language models to smaller, more efficient models for scientific question answering. The authors propose Sci-CoT, a two-stage framework that separates rationale generation from answer inference, allowing small models to more effectively learn reasoning patterns. By using a large LLM (GPT-3.5-turbo) to generate high-quality rationales and then training a smaller student model (Flan-T5-small) through knowledge distillation, the approach achieves state-of-the-art performance on scientific question-answering benchmarks while requiring significantly fewer parameters than competing models.

## Method Summary
Sci-CoT is a two-stage knowledge distillation framework designed to transfer reasoning capabilities from large language models to smaller models for scientific question answering. In Stage 1, an LLM teacher (GPT-3.5-turbo) generates rationales for each question using the correct answer and a "Let's think step by step" prompt. These rationales are manually inspected and cleaned before being used to fine-tune a student model (Flan-T5-small) on the task of generating rationales. In Stage 2, the trained student model generates rationales for all questions, which are then used as input to fine-tune a second student model on answer inference. This two-stage approach separates the complex tasks of rationale generation and answer inference, allowing the small model to focus on each task independently and achieve better performance than one-stage approaches.

## Key Results
- Sci-CoT with an 80-million parameter model exceeds BLOOM-176B performance on ARC-Easy dataset under few-shot settings
- Achieves comparable accuracy to OPT-175B on ARC-Challenge dataset
- Requires less training data than conventional fine-tuning approaches
- Demonstrates clear performance improvements over baseline fine-tuning and one-stage CoT methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating rationale generation from answer inference enables more efficient use of rationales in small models
- Mechanism: The two-stage framework allows the model to focus on generating high-quality rationales in Stage 1, then use these rationales as structured input for answer inference in Stage 2, rather than trying to generate both simultaneously
- Core assumption: Small models with limited parameters cannot effectively learn to generate rationales and infer answers in a single stage
- Evidence anchors:
  - [abstract] "we propose Sci-CoT, a two-stage framework that separates the processes of generating rationales and inferring answers"
  - [section] "our experimental results indicate that this one-stage method does not effectively improve the performance of our small student model"
  - [corpus] Weak evidence - no direct citation about separation benefits, but related works focus on similar separation
- Break condition: If the model has sufficient capacity to handle both tasks simultaneously, the two-stage approach may add unnecessary complexity

### Mechanism 2
- Claim: Using correct answers as prompts improves rationale quality for scientific questions
- Mechanism: By providing the correct answer along with the "Let's think step by step" prompt, the LLM teacher is guided to generate accurate rationales that lead to the correct conclusion
- Core assumption: Scientific questions require precise reasoning paths, and knowing the answer helps generate more accurate intermediate steps
- Evidence anchors:
  - [section] "we provide LLMs with the correct answer accompanied by the prompt 'Let's think step by step'"
  - [section] "we undertake a rigorous process of manual data inspection and cleaning"
  - [corpus] Moderate evidence - similar approaches in Mentor-KD and other reasoning distillation papers
- Break condition: If the rationale generation becomes too dependent on the answer, it may not generalize to questions without provided answers

### Mechanism 3
- Claim: Knowledge distillation from large to small models transfers reasoning capabilities without requiring large model deployment
- Mechanism: The large LLM acts as a teacher to generate rationales and answers, which are then used to train a smaller student model that learns to mimic the reasoning process
- Core assumption: The student model can effectively learn the reasoning patterns demonstrated by the teacher without needing the same parameter count
- Evidence anchors:
  - [abstract] "we investigate the possibility of transferring the reasoning capabilities of LLMs to smaller models via knowledge distillation"
  - [section] "we introduce Sci-CoT, a method involving two distinct stages"
  - [corpus] Strong evidence - multiple related papers (Mentor-KD, Knowledge-Augmented Reasoning Distillation) use similar approaches
- Break condition: If the reasoning capabilities are too complex or emergent, the student model may not capture them effectively

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: CoT enables models to break down complex reasoning into intermediate steps, which is crucial for scientific question answering
  - Quick check question: What is the difference between Zero-Shot-CoT and Few-Shot-CoT prompting?

- Concept: Knowledge Distillation
  - Why needed here: KD allows transferring capabilities from large models to smaller, more efficient models without requiring the same computational resources
  - Quick check question: What are the two main components of a typical knowledge distillation setup?

- Concept: Cross-entropy loss for sequence generation
  - Why needed here: Cross-entropy loss is used to train the model to generate tokens that match the target sequences (rationales and answers)
  - Quick check question: How does cross-entropy loss work when training a model to generate sequences of tokens?

## Architecture Onboarding

- Component map:
  LLM Teacher (GPT-3.5-turbo) -> Generate rationales using correct answers as prompts -> Manual data cleaning -> Student Model (Flan-T5-small) -> Stage 1: Generate rationales, Stage 2: Infer answers -> Answer inference model

- Critical path:
  1. Input question to LLM teacher with correct answer and CoT prompt
  2. Generate and clean rationales
  3. Fine-tune student model on rationale generation (Stage 1)
  4. Generate rationales for all questions using trained student model
  5. Fine-tune second student model on answer inference using questions + rationales as input (Stage 2)
  6. Deploy answer inference model for inference

- Design tradeoffs:
  - Two-stage vs one-stage: Two-stage provides better performance but requires training two models
  - Model size: Smaller models are more efficient but may have limited reasoning capacity
  - Data cleaning: Manual inspection ensures quality but is time-consuming

- Failure signatures:
  - Stage 1 failure: Generated rationales are irrelevant or incorrect, leading to poor answer inference
  - Stage 2 failure: Model fails to effectively use provided rationales for answer inference
  - Overall failure: Performance worse than baseline fine-tuning

- First 3 experiments:
  1. Baseline comparison: Fine-tune Flan-T5-small directly on questions → answers
  2. One-stage CoT: Fine-tune Flan-T5-small on questions → rationales + answers simultaneously
  3. Two-stage Sci-CoT: Implement the full two-stage framework and compare performance on ARC-Easy and ARC-Challenge datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Sci-CoT scale with the parameter size of the student model?
- Basis in paper: [inferred] The paper mentions that previous studies used models with 1-11 billion parameters, while their model has only 80 million parameters.
- Why unresolved: The paper only evaluates one student model size, making it unclear how the approach would perform with larger or smaller models.
- What evidence would resolve it: Experiments comparing Sci-CoT performance across a range of student model sizes.

### Open Question 2
- Question: What is the impact of different prompt formulations on the quality of generated rationales?
- Basis in paper: [explicit] The paper discusses using two prompts ("Let's think step by step" and the correct answer) and mentions manual data cleaning, but doesn't systematically explore alternative prompt formulations.
- Why unresolved: The paper doesn't provide a comprehensive analysis of how different prompts affect rationale quality or downstream performance.
- What evidence would resolve it: Comparative experiments using different prompt formulations and their effects on rationale quality and final answer accuracy.

### Open Question 3
- Question: How does Sci-CoT perform on other types of reasoning tasks beyond scientific question answering?
- Basis in paper: [explicit] The paper mentions that future research could explore symbolic reasoning and logical reasoning tasks.
- Why unresolved: The paper only evaluates on scientific question-answering datasets (ARC-Easy and ARC-Challenge).
- What evidence would resolve it: Experiments applying Sci-CoT to different reasoning task domains and datasets.

### Open Question 4
- Question: What is the optimal ratio of training data for the two stages of Sci-CoT?
- Basis in paper: [inferred] The paper shows that Sci-CoT requires less training data than conventional fine-tuning, but doesn't explore different data allocation strategies between the two stages.
- Why unresolved: The paper uses the same training data for both stages but doesn't investigate whether different proportions would yield better results.
- What evidence would resolve it: Experiments varying the amount of training data allocated to each stage and measuring the impact on final performance.

## Limitations

- The exact prompts used for rationale generation with the LLM are not fully specified, making faithful reproduction challenging
- Manual data inspection and cleaning process is described but not quantified, raising questions about scalability and reproducibility
- Computational overhead of training two separate models versus potential benefits is not thoroughly analyzed
- Lack of detailed hyperparameter specifications makes it difficult to assess whether performance gains are method-specific

## Confidence

**High Confidence**: The claim that separating rationale generation from answer inference improves performance for small models is well-supported by experimental results showing clear improvements over baseline and one-stage approaches. The methodology is sound and the results are statistically significant.

**Medium Confidence**: The claim that knowledge distillation effectively transfers reasoning capabilities from large to small models is supported by results but limited by the lack of ablation studies. The performance gains could be partially attributed to factors not isolated in the experiments, such as data quality or training duration.

**Low Confidence**: The assertion that the 80-million parameter model exceeds BLOOM-176B performance is based on limited comparisons and lacks rigorous statistical analysis. The claim about achieving "comparable accuracy" to OPT-175B also requires more detailed benchmarking against other approaches.

## Next Checks

1. **Reproducibility Check**: Implement the full Sci-CoT pipeline with detailed logging of all prompts, hyperparameters, and data cleaning steps. Compare results with the reported performance on ARC-Easy and ARC-Challenge datasets using the same evaluation metrics and random seeds.

2. **Ablation Study**: Conduct controlled experiments to isolate the impact of each component: (a) compare one-stage vs two-stage approaches with identical training budgets, (b) test rationale quality without providing correct answers as prompts, and (c) evaluate performance when using rationales generated by the student model vs the LLM teacher.

3. **Generalization Test**: Evaluate the trained models on out-of-distribution scientific question datasets (e.g., different grade levels or subject areas) to assess whether the learned reasoning capabilities transfer beyond the ARC datasets. Measure both performance and rationale quality on these new datasets.