---
ver: rpa2
title: On the Robustness of Post-hoc GNN Explainers to Label Noise
arxiv_id: '2309.01706'
source_url: https://arxiv.org/abs/2309.01706
tags:
- explainers
- post-hoc
- graph
- label
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically investigates the robustness of post-hoc
  graph neural network (GNN) explainers under label noise. We evaluate two popular
  explainers, GNNExplainer and PGExplainer, across four datasets with varying levels
  of label noise.
---

# On the Robustness of Post-hoc GNN Explainers to Label Noise

## Quick Facts
- arXiv ID: 2309.01706
- Source URL: https://arxiv.org/abs/2309.01706
- Reference count: 30
- Key outcome: This study systematically investigates the robustness of post-hoc graph neural network (GNN) explainers under label noise. We evaluate two popular explainers, GNNExplainer and PGExplainer, across four datasets with varying levels of label noise. Results show that both explainers are highly sensitive to label perturbations, with explanation quality (measured by fidelity+) significantly declining even at minor noise levels. Interestingly, beyond 50% noise, explanation effectiveness gradually recovers as noise increases further. The study highlights the instability of post-hoc GNN explainers and underscores the need for dedicated efforts to improve their robustness against label noise.

## Executive Summary
This paper investigates the robustness of post-hoc GNN explainers under label noise, focusing on GNNExplainer and PGExplainer. The study evaluates these explainers across four datasets with varying levels of label noise, using fidelity+ and fidelity- metrics to measure explanation quality. Results show that both explainers are highly sensitive to label noise, with explanation quality significantly declining even at minor noise levels. However, beyond 50% noise, explanation effectiveness gradually recovers to levels comparable to those without noise.

## Method Summary
The study trains GCN and GIN models on graph datasets with varying levels of label noise, then generates explanations using GNNExplainer and PGExplainer. The quality of these explanations is evaluated using fidelity+ and fidelity- metrics. Experiments are conducted on four datasets: MUTAG, Graph-Twitter, BA-2motifs, and BA-Multi-Shapes, with noise levels ranging from 0% to 100%. The minimum viable reproduction plan involves setting up the environment, implementing label noise injection, training models, generating explanations, and computing evaluation metrics.

## Key Results
- Post-hoc GNN explainers are highly sensitive to label noise, with explanation quality significantly declining even at minor noise levels
- Beyond 50% noise, explanation effectiveness gradually recovers to levels comparable to those without noise
- The fidelity- metric becomes impractical for evaluating explainer robustness under high label noise

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Post-hoc GNN explainers are sensitive to label noise because they rely on predicted probabilities from the GNN model, which are affected by noisy labels.
- Mechanism: When label noise is introduced, the GNN model's predictions change, and these changes propagate to the explainer, which uses these predictions to generate explanations. The fidelity+ metric measures how much the GNN's prediction changes when relevant features are removed, so if the underlying predictions are noisy, the explanations will also be noisy.
- Core assumption: The fidelity+ metric accurately reflects explanation quality, and the GNN's predicted probabilities are directly used by the explainer to generate explanations.
- Evidence anchors:
  - [abstract] states "even minor levels of label noise, inconsequential to GNN performance, harm the quality of generated explanations substantially."
  - [section 3.1] discusses how "the predicted probabilities are affected by introduced noises, which are not represented in Acc" and that "these predicted probabilities would be passed to GNN explainers as essential inputs to generate explanations."
  - [corpus] has related work on "Graph Neural Network Explanations are Fragile" which suggests this sensitivity is a known issue.
- Break condition: If the explainer used a different mechanism to generate explanations that didn't rely on the GNN's predicted probabilities, or if the fidelity+ metric was not used to evaluate explanation quality.

### Mechanism 2
- Claim: The fidelity- metric becomes less meaningful with label noise because it measures the change in prediction when only relevant features are kept, and confusing labels lead to ambiguous predictions.
- Mechanism: With noisy labels, the GNN's predictions become ambiguous, and the fidelity- metric, which measures the change when only relevant features are kept, naturally approaches zero because the predictions are already confused. This makes the metric impractical for evaluating explainer robustness.
- Core assumption: The fidelity- metric is defined such that lower values indicate better explanations, and that confusing labels lead to ambiguous predictions that make the metric approach zero.
- Evidence anchors:
  - [abstract] states "we engage in a discourse regarding the progressive recovery of explanation effectiveness with escalating noise levels" and mentions the impracticality of one current metric.
  - [section 3.1] explicitly states "we find out that Fid− decreases as λ increases from 0% to 50%. However, the definition of Fid− indicating lower values as more satisfactory contradicts this outcome. In our scenario, confusing labels lead to ambiguous predictions, subsequently causing Fid− → 0."
- Break condition: If the fidelity- metric was defined differently, or if the GNN's predictions remained clear even with noisy labels.

### Mechanism 3
- Claim: Beyond a noise threshold of 50%, explanation effectiveness gradually recovers because inverted label signals enable GNN models to predict reverse labels while identifying important features.
- Mechanism: When the noise level is very high (beyond 50%), the label noise effectively inverts the true labels. The GNN model, trained on these inverted labels, learns to predict the opposite of what it would have predicted with clean labels. The explainer, using these inverted predictions, can then identify features that are important for the inverted labels, which may correspond to features that are actually important for the original labels.
- Core assumption: The GNN model can learn to predict the opposite of the true labels when the noise level is very high, and that the explainer can identify features that are important for these inverted predictions.
- Evidence anchors:
  - [abstract] states "beyond a noise threshold of 50%, explanation effectiveness gradually recovers to levels comparable to those without noise as noise levels continue escalating."
  - [section 3.1] discusses "inverted label signals enables GNN models to predict reverse labels while identifying important features" and provides an example where "at λ =50%, PGExplainer fails to identity key motifs (NO2), yet successfully does so at λ =100%."
- Break condition: If the GNN model cannot learn to predict the opposite of the true labels, or if the explainer cannot identify features that are important for the inverted predictions.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their black-box nature
  - Why needed here: The paper is about explaining the behavior of GNNs, so understanding what GNNs are and why they are considered black boxes is fundamental.
  - Quick check question: What is a GNN and why are they often considered black boxes?

- Concept: Post-hoc explainers and their evaluation metrics (fidelity+, fidelity-)
  - Why needed here: The paper evaluates the robustness of post-hoc GNN explainers, so understanding what these explainers are and how their performance is measured is crucial.
  - Quick check question: What are post-hoc GNN explainers and how are they typically evaluated?

- Concept: Label noise and its effects on machine learning models
  - Why needed here: The paper investigates the robustness of explainers to label noise, so understanding what label noise is and how it affects model performance is important.
  - Quick check question: What is label noise and how can it affect the performance of machine learning models?

## Architecture Onboarding

- Component map: GNN model (GCN or GIN) -> Post-hoc explainer (GNNExplainer or PGExplainer) -> Evaluation metrics (fidelity+, fidelity-)
- Critical path: 1. Train a GNN model on the graph data 2. Generate predictions using the trained GNN 3. Use the post-hoc explainer to generate explanations based on the GNN's predictions and the graph 4. Evaluate the quality of the explanations using fidelity+ and fidelity- metrics
- Design tradeoffs: The choice of GNN architecture (GCN vs GIN) may affect the explainer's performance; The choice of explainer (GNNExplainer vs PGExplainer) may affect the explainer's performance; The evaluation metrics (fidelity+, fidelity-) may not fully capture the quality of the explanations in the presence of label noise
- Failure signatures: If the GNN model's performance (accuracy) remains stable but the explainer's performance (fidelity+) degrades significantly with label noise, it indicates that the explainer is sensitive to label noise; If the fidelity- metric approaches zero with increasing label noise, it indicates that the metric is not suitable for evaluating explainer robustness in the presence of label noise
- First 3 experiments: 1. Train a GNN model (GCN or GIN) on a graph dataset (MUTAG, BA-2motifs, BA-Multi-Shapes, or Graph-Twitter) and evaluate its accuracy on a test set 2. Use a post-hoc explainer (GNNExplainer or PGExplainer) to generate explanations for the GNN's predictions on the test set, and evaluate the quality of these explanations using fidelity+ and fidelity- metrics 3. Introduce label noise to the training data at varying levels (e.g., 10%, 20%, 50%, 75%, 100%) and repeat steps 1 and 2 to observe how the GNN's accuracy and the explainer's fidelity+ and fidelity- metrics change with increasing noise levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the instability of post-hoc GNN explainers manifest in real-world applications with noisy labels?
- Basis in paper: [explicit] The paper states that post-hoc GNN explainers are susceptible to label noise and their explanation quality significantly declines even at minor noise levels.
- Why unresolved: The study provides empirical evidence on synthetic and controlled datasets but doesn't address real-world application scenarios where label noise is more complex and varied.
- What evidence would resolve it: Conducting experiments on real-world datasets with naturally occurring label noise and comparing explanation quality with controlled noise scenarios would provide insight into practical implications.

### Open Question 2
- Question: What mechanisms can be implemented to improve the robustness of post-hoc GNN explainers against label noise?
- Basis in paper: [inferred] The paper discusses the susceptibility of post-hoc GNN explainers to label noise and suggests that dedicated efforts are needed to bolster their robustness.
- Why unresolved: While the paper identifies the problem, it does not propose specific solutions or mechanisms to enhance the robustness of these explainers.
- What evidence would resolve it: Developing and testing new algorithms or techniques designed to mitigate the impact of label noise on explanation quality would provide concrete evidence of potential improvements.

### Open Question 3
- Question: How does the recovery of explanation effectiveness beyond 50% noise impact the reliability of post-hoc GNN explainers in practice?
- Basis in paper: [explicit] The paper notes that beyond a noise threshold of 50%, explanation effectiveness gradually recovers to levels comparable to those without noise.
- Why unresolved: The study observes this phenomenon but does not explore its implications for the reliability and trustworthiness of explanations in practical scenarios.
- What evidence would resolve it: Analyzing the practical scenarios where such high noise levels might occur and evaluating the trustworthiness of explanations in those contexts would clarify the real-world implications.

## Limitations
- The study focuses on graph classification tasks and may not generalize to other graph learning tasks or different types of explainers
- The fidelity+ and fidelity- metrics used to evaluate explanation quality may have limitations, especially in the presence of high label noise
- The observed recovery of explanation effectiveness beyond 50% noise is an intriguing finding, but the underlying mechanism is not fully understood and requires further investigation

## Confidence

**Confidence Labels:**
- High: GNN explainers are sensitive to label noise, and explanation quality degrades significantly even at minor noise levels
- Medium: Beyond 50% noise, explanation effectiveness gradually recovers to levels comparable to those without noise
- Medium: The fidelity- metric becomes impractical for evaluating explainer robustness under high label noise

## Next Checks

1. Replicate the experiments on additional graph datasets and with different GNN architectures (e.g., GAT, GraphSAGE) to assess the generalizability of the findings.
2. Investigate the behavior of other post-hoc explainers (e.g., GradCAM, LIME) under label noise to determine if the observed sensitivity is specific to GNNExplainer and PGExplainer.
3. Explore alternative metrics or evaluation methodologies that are more robust to label noise and can provide a more comprehensive assessment of explainer quality in the presence of noise.