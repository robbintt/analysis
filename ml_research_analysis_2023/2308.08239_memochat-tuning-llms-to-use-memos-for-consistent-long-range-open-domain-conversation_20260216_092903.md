---
ver: rpa2
title: 'MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation'
arxiv_id: '2308.08239'
source_url: https://arxiv.org/abs/2308.08239
tags:
- user
- memo
- conversation
- task
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes MemoChat, an instruction-tuning pipeline that
  enables large language models to use self-composed memos for consistent long-range
  open-domain conversations. MemoChat decomposes conversations into "memorization-retrieval-response"
  cycles, where the model writes structured memos of past dialogues, retrieves relevant
  evidence based on current queries, and generates responses.
---

# MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation

## Quick Facts
- arXiv ID: 2308.08239
- Source URL: https://arxiv.org/abs/2308.08239
- Reference count: 20
- MemoChat significantly improves response consistency in long-range conversations through instruction-tuned memo operations

## Executive Summary
MemoChat introduces an instruction-tuning pipeline that enables large language models to maintain consistency in long-range open-domain conversations by decomposing dialogue into "memorization-retrieval-response" cycles. The approach trains models to write structured memos of past dialogues, retrieve relevant evidence based on current queries, and generate coherent responses without external modules. Evaluated on a new expert-annotated consistency test set, MemoChat demonstrates significant improvements over baselines like ChatGPT-2k and MPC, with fine-tuned open-source models achieving competitive performance with or better than ChatGPT across different question types.

## Method Summary
MemoChat fine-tunes open-source LLMs (Fastchat-T5-3b, Vicuna-7b/13b/33b) on reconstructed instruction data from public dialogue datasets (TopicoQA, DialogSum, Alpaca-GPT4) using AdamW optimizer and WarmupDecayLR scheduler. The method trains models to partition conversations by topic, generate structured JSON memos, and retrieve relevant evidence for response generation. Evaluation uses an expert-annotated test set with "Retrospection," "Continuation," and "Conjunction" question types, scored by GPT-4 as judge for faithfulness (1-100 scale).

## Key Results
- MemoChat achieves average faithfulness scores of 70.76 on long-range questions
- Fine-tuned Vicuna-13B and Vicuna-33B perform competitively with or better than ChatGPT
- Instruction tuning improves open-source LLMs' memo writing ability (F1 scores increase from ~0.3 to ~0.8)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Decomposing long conversations into "memorization-retrieval-response" cycles enables LLMs to maintain contextual coherence across diverse topics.
- **Mechanism**: The system guides the model to first organize conversation history into structured memos, then retrieve topic-relevant evidence before generating responses, creating a self-contained loop without external modules.
- **Core assumption**: LLMs can learn to partition dialogue histories by topic and generate structured JSON-formatted memos through instruction tuning.
- **Evidence anchors**:
  - [abstract]: "MemoChat decomposes conversations into 'memorization-retrieval-response' cycles, where the model writes structured memos of past dialogues, retrieves relevant evidence based on current queries, and generates responses."
  - [section 3.2]: "MemoChat takes a step forward and proposes to decompose the long-range open-domain conversation as a three-stage 'memorization-retrieval-response' loop."
  - [corpus]: Weak - No direct corpus evidence for effectiveness of decomposition, but related work like MPC and MemoryBank show benefits of structured memory.
- **Break condition**: If the model fails to properly partition dialogue histories or retrieve relevant evidence, the response consistency degrades significantly.

### Mechanism 2
- **Claim**: Fine-tuning with reconstructed instructions from public datasets enables open-source LLMs to perform structured memo operations without external tools.
- **Mechanism**: The system reconstructs instruction data from existing dialogue datasets to teach models to partition conversations by topic, summarize sub-dialogues, and retrieve evidence based on query topics.
- **Core assumption**: LLMs can generalize from fine-tuning on reconstructed instructions to handle unseen long-range conversations.
- **Evidence anchors**:
  - [abstract]: "The instructions are reconstructed from a collection of public datasets to teach the LLMs to memorize and retrieve past dialogues with structured memos."
  - [section 3.2]: "We reconstruct 10k instruction data from three public dialogue datasets... to fine-tune open-source LLM-powered chatbots."
  - [corpus]: Moderate - Related work shows instruction tuning improves LLM task performance, but this specific application is novel.
- **Break condition**: If the fine-tuned model overfits to training patterns or cannot generalize to new conversation structures, memo operations fail.

### Mechanism 3
- **Claim**: Expert-annotated test sets enable rigorous evaluation of response consistency in long-range conversations.
- **Mechanism**: The system creates a specialized evaluation set with "Retrospection," "Continuation," and "Conjunction" question types to test different aspects of consistency across conversation turns.
- **Core assumption**: Human experts can create questions that effectively probe whether responses maintain consistency with conversation history.
- **Evidence anchors**:
  - [abstract]: "We invite experts to manually annotate a test set designed to evaluate the consistency of long-range conversations questions."
  - [section 4.1]: "We invite three experts to manually annotate a new long-range open-domain conversation dataset... adding long-range questions to the end of the chatting streams for 'Retrospection,' 'Continuation' and 'Conjunction'."
  - [corpus]: Moderate - Related work uses human evaluation for dialogue quality, but specialized long-range consistency evaluation is less common.
- **Break condition**: If expert annotations are inconsistent or questions don't effectively probe consistency, evaluation validity is compromised.

## Foundational Learning

- **Concept**: Instruction tuning methodology
  - Why needed here: The system relies on reconstructing instructions from public datasets to teach LLMs memo operations without external modules
  - Quick check question: Can you explain the difference between zero-shot, few-shot, and fine-tuning approaches in LLM adaptation?

- **Concept**: Named Entity Recognition (NER) metrics
  - Why needed here: The system uses NER-style evaluation for memo topic and summary generation, requiring understanding of precision, recall, and F1-score metrics
  - Quick check question: How would you calculate precision and recall for topic span detection in dialogue history?

- **Concept**: Chain-of-Thought (CoT) prompting
  - Why needed here: The system incorporates CoT in memo writing instructions to improve reasoning about dialogue partitioning and summarization
  - Quick check question: Can you describe how CoT prompting differs from standard prompting and why it might improve complex task performance?

## Architecture Onboarding

- **Component map**: Input processor -> Memo manager -> Evaluation module
- **Critical path**: Conversation → Memo writing → Memo retrieval → Response generation → Evaluation
- **Design tradeoffs**:
  - Text window size vs. computational cost: 2k tokens balance context richness and efficiency
  - Instruction volume balance: Equal distribution prevents catastrophic forgetting during fine-tuning
  - Model scale vs. performance: Larger models show better zero-shot and fine-tuned performance
- **Failure signatures**:
  - Memo writing fails: Model produces incomplete or incorrect topic partitions
  - Retrieval fails: Model cannot match queries to relevant memo entries
  - Response generation fails: Generated responses lack consistency with conversation history
- **First 3 experiments**:
  1. Test zero-shot performance on memo writing with different model scales
  2. Fine-tune Vicuna-13B on 1k instruction samples and evaluate memo usage ability
  3. Compare downstream consistency performance between MemoChat and baseline methods on expert-annotated test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MemoChat approach scale to extremely long conversations with hundreds or thousands of turns?
- Basis in paper: [inferred]
- Why unresolved: The paper only evaluates on conversations with up to 15 turns. It's unclear how the memo-based approach would handle conversations that are orders of magnitude longer. Issues like memo size, retrieval efficiency, and maintaining coherence over very long histories are not addressed.
- What evidence would resolve it: Experiments showing MemoChat's performance on conversations with 100+ turns, analysis of memo size growth, and timing data for memo retrieval on long conversations.

### Open Question 2
- Question: How robust is MemoChat to topic shifts and topic ambiguity in conversations?
- Basis in paper: [inferred]
- Why unresolved: The paper doesn't explore scenarios where topics are unclear or shift abruptly. It's unclear how well MemoChat can handle situations where the topic of a turn is ambiguous or where the conversation suddenly changes to an unrelated topic.
- What evidence would resolve it: Experiments testing MemoChat on conversations with ambiguous topics, sudden topic shifts, or multiple simultaneous topics. Analysis of how well the memo structure captures these complexities.

### Open Question 3
- Question: How does MemoChat perform on non-English conversations or code-switching scenarios?
- Basis in paper: [explicit]
- Why unresolved: The paper doesn't mention any experiments with non-English conversations or scenarios where multiple languages are used. It's unclear if the approach is language-agnostic or if it relies on language-specific features.
- What evidence would resolve it: Experiments testing MemoChat on conversations in different languages, including code-switching scenarios where multiple languages are used within the same conversation.

### Open Question 4
- Question: How does the performance of MemoChat compare to other memory-augmented approaches in the literature?
- Basis in paper: [explicit]
- Why unresolved: The paper only compares MemoChat to a few baselines (ChatGPT-2k, MPC-ChatGPT, MemoryBank-ChatGPT) and doesn't provide a comprehensive comparison with other memory-augmented dialogue systems.
- What evidence would resolve it: A thorough comparison of MemoChat with other memory-augmented approaches, including quantitative metrics and qualitative analysis of the strengths and weaknesses of each approach.

## Limitations
- Evaluation relies heavily on GPT-4 as judge, introducing potential biases
- Expert-annotated test set is relatively small (200 conversations) and may not capture full conversation diversity
- Reconstruction process from public datasets could introduce artifacts limiting generalization

## Confidence

**High confidence**: The claim that instruction tuning improves open-source LLMs' ability to perform structured memo operations (supported by quantitative metrics on F1, precision, recall for intermediate tasks and significant improvements in downstream consistency scores).

**Medium confidence**: The claim that MemoChat outperforms ChatGPT-2k and MPC on long-range consistency (based on expert evaluation, but limited by the scale of the test set and potential judge bias).

**Medium confidence**: The claim that Vicuna-33B achieves comparable or better performance than ChatGPT-4 on all question types (strong results but based on a single evaluation metric and dataset).

## Next Checks

1. **Generalization test**: Evaluate MemoChat-trained models on truly open-domain conversations outside the training dataset distribution to verify robustness to diverse topics and conversation structures.

2. **Human evaluation validation**: Conduct blind human evaluations comparing MemoChat responses against baseline methods to validate GPT-4 judge consistency scores and assess subjective quality metrics like coherence and engagement.

3. **Stress test for failure modes**: Systematically test model performance with varying conversation lengths, topic densities, and memo retrieval scenarios to identify and quantify the exact conditions under which memo operations fail.