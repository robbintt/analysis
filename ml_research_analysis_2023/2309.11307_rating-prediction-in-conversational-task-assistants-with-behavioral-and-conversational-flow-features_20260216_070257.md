---
ver: rpa2
title: Rating Prediction in Conversational Task Assistants with Behavioral and Conversational-Flow
  Features
arxiv_id: '2309.11307'
source_url: https://arxiv.org/abs/2309.11307
tags:
- user
- features
- task
- system
- rating
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles rating prediction in Conversational Task Assistants
  (CTA), where the goal is to predict user ratings based on dialog history. The authors
  propose TB-Rater, a Transformer model that combines conversational-flow features
  with user behavior features.
---

# Rating Prediction in Conversational Task Assistants with Behavioral and Conversational-Flow Features

## Quick Facts
- arXiv ID: 2309.11307
- Source URL: https://arxiv.org/abs/2309.11307
- Reference count: 38
- This paper tackles rating prediction in Conversational Task Assistants (CTA), where the goal is to predict user ratings based on dialog history. The authors propose TB-Rater, a Transformer model that combines conversational-flow features with user behavior features.

## Executive Summary
This paper addresses the challenge of predicting user ratings in Conversational Task Assistants (CTA) by proposing TB-Rater, a Transformer-based model that combines conversational-flow features with user behavior features. The authors leverage the Alexa TaskBot dataset and create CTA-specific behavioral features to improve rating prediction accuracy. Their approach achieves 69.6% accuracy in binary classification of ratings (1-3 vs 4-5), outperforming baseline methods. The study demonstrates the feasibility of rating prediction in this novel multimodal conversational setting and provides insights into the importance of different feature types through ablation studies.

## Method Summary
TB-Rater combines a Transformer-based language model for capturing conversational patterns with manually engineered behavioral features specific to the CTA domain. The model processes conversation turns with special tokens (device type, domain, intent, response generator) and extracts BERT embeddings for the conversational-flow features. These are combined with 70 CTA-specific behavioral features from the last conversation turn through separate feed-forward networks, concatenated, and passed through a final feed-forward network for binary classification of ratings.

## Key Results
- TB-Rater achieves 69.6% accuracy in binary classification of ratings (1-3 vs 4-5)
- Combining conversational-flow and behavioral features outperforms baselines that use only one feature type
- Left truncation (keeping beginning of conversation) is more effective than right truncation for rating prediction
- Ablation studies confirm the importance of both feature types for optimal performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TB-Rater combines conversational-flow and behavioral features to improve rating prediction accuracy.
- Mechanism: The model leverages a Transformer-based architecture to capture complex dialog patterns from conversational-flow features while behavioral features provide domain-specific context. These two information streams are combined via separate feed-forward networks before a final integration layer.
- Core assumption: Both conversational dynamics and user behavior patterns contribute independently meaningful signals for rating prediction.
- Evidence anchors:
  - [abstract]: "TB-Rater, a Transformer model which combines conversational-flow features with user behavior features"
  - [section]: "we use a Transformer-based [25] language model, which is able to capture various patterns in the language and derive a representation of the conversation's state"
  - [corpus]: Weak - corpus papers discuss related conversational systems but don't directly address this specific feature combination mechanism
- Break condition: If one feature type dominates the other or if the combination creates conflicting signals that confuse the model rather than enhance it.

### Mechanism 2
- Claim: Using special tokens for intent, response generator, device type, and domain enhances the model's understanding of conversation context.
- Mechanism: By encoding these meta-features as special tokens within the input sequence, the Transformer can learn relationships between these contextual elements and rating outcomes, going beyond raw utterance content.
- Core assumption: These additional tokens provide meaningful signals that improve the model's ability to distinguish between different types of conversations and their likely outcomes.
- Evidence anchors:
  - [section]: "we use additional special tokens denoting the type of device [ð·ð¸ð‘‰ ] (screen/screen-less) and the domain of the user's task [ð·ð‘‚ð‘€ ]"
  - [section]: "We use all turns of the conversation and perform left truncation of the input when it is over the maximum sequence length"
  - [corpus]: Weak - corpus papers don't discuss this specific token-augmented approach
- Break condition: If the model learns spurious correlations from these tokens or if the token vocabulary becomes too large relative to available training data.

### Mechanism 3
- Claim: Left truncation (keeping beginning of conversation) is more effective than right truncation (keeping end) for rating prediction.
- Mechanism: The model benefits from understanding the full conversational context, including how the interaction started and evolved, rather than just the final exchanges.
- Core assumption: Early conversation patterns and how they develop over time contain predictive information about final user satisfaction.
- Evidence anchors:
  - [section]: "Finally, we test the TB-Rater model but truncate inputs larger than the maximum input size from the right side (end of the conversation) instead of the left side. Here, we observe the worse results out of all methods"
  - [section]: "This result shows that focusing on the end of the conversation is more important to predict the rating"
  - [corpus]: Weak - corpus papers don't discuss truncation strategies
- Break condition: If the relationship between early conversation patterns and final ratings varies significantly across different task domains or user populations.

## Foundational Learning

- Concept: Transformer architectures and attention mechanisms
  - Why needed here: TB-Rater uses a Transformer-based language model to capture conversational patterns
  - Quick check question: How does the attention mechanism allow the model to weigh different parts of the conversation differently when predicting the rating?

- Concept: Feature engineering and manual feature creation
  - Why needed here: The behavioral features were manually engineered based on CTA-specific domain knowledge
  - Quick check question: What's the difference between CTA-specific features and general conversational features, and why are both needed?

- Concept: Binary classification and threshold-based rating conversion
  - Why needed here: The paper converts 1-5 ratings to binary 0-1 classification for the prediction task
  - Quick check question: Why might the authors have chosen to convert ratings to binary classification rather than predicting the full 5-point scale?

## Architecture Onboarding

- Component map: Input processing -> BERT encoding -> Conversational-flow FFNN -> Behavioral FFNN -> Concatenation -> Final FFNN -> Classification
- Critical path: Input â†’ BERT encoding â†’ Conversational-flow FFNN â†’ Behavioral FFNN â†’ Concatenation â†’ Final FFNN â†’ Classification
- Design tradeoffs:
  - Using separate FFNNs allows the model to learn different representations for each feature type before combining
  - Left truncation preserves conversation context but limits maximum conversation length
  - Manual feature engineering provides domain-specific signals but requires expert knowledge
- Failure signatures:
  - Overfitting to training data (check validation performance)
  - Poor generalization across different task domains
  - Failure to capture important conversational patterns (check attention visualization)
  - Behavioral features becoming less important than conversational features (check ablation study)
- First 3 experiments:
  1. Compare left vs right truncation with a small subset of data to confirm truncation direction matters
  2. Test with only conversational-flow features vs only behavioral features to understand individual contributions
  3. Vary the number of behavioral features to find the optimal subset that balances performance and complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do user demographics (age, gender, location, etc.) influence rating prediction accuracy in CTA systems?
- Basis in paper: [inferred] The paper focuses on behavioral and conversational features but does not consider user demographic factors.
- Why unresolved: User demographics are not collected or analyzed in the Alexa TaskBot dataset used in the study.
- What evidence would resolve it: Collecting user demographic information alongside ratings and testing if including these features improves prediction accuracy.

### Open Question 2
- Question: Can the proposed TB-Rater model effectively predict ratings in real-time during a conversation to enable dynamic system adaptation?
- Basis in paper: [explicit] The authors mention future work on applying the model in an online setting to change the course of a conversation.
- Why unresolved: The current work focuses on offline rating prediction using complete conversation histories, not real-time prediction.
- What evidence would resolve it: Implementing and testing the model in a live CTA system, measuring prediction accuracy and system performance changes.

### Open Question 3
- Question: How does the performance of TB-Rater compare to human experts in predicting user ratings for CTA interactions?
- Basis in paper: [inferred] The paper does not compare the model's predictions to human ratings or judgments.
- Why unresolved: The study only compares TB-Rater to other automated methods, not to human performance.
- What evidence would resolve it: Conducting a study where human experts rate the same CTA interactions and comparing their ratings to TB-Rater's predictions.

## Limitations
- Dataset size (1681 conversations) limits generalizability to other CTA systems and task domains
- Manual feature engineering approach may not capture all relevant patterns and requires significant human expertise
- Binary classification simplification loses granular information about user satisfaction

## Confidence
- **High Confidence**: The general architecture combining Transformer-based conversational-flow features with manually engineered behavioral features is sound and the experimental methodology is appropriate for the task.
- **Medium Confidence**: The specific implementation details (exact behavioral features, BERT variant, tokenization method) have some ambiguity that could affect reproducibility.
- **Low Confidence**: The generalizability of results to other CTA systems, different task domains, or larger-scale deployments remains uncertain due to the specialized dataset and task-specific feature engineering.

## Next Checks
1. Conduct a more extensive ablation study varying the number and types of behavioral features to identify the minimum effective feature set and understand feature redundancy.
2. Test the TB-Rater model on a different CTA dataset or conversational AI assistant to evaluate generalizability beyond the Alexa TaskBot domain.
3. Compare performance using the binary classification approach versus multi-class classification (predicting 1-5 ratings) to quantify information loss from the simplification.