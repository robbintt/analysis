---
ver: rpa2
title: 'CT-GAT: Cross-Task Generative Adversarial Attack based on Transferability'
arxiv_id: '2310.14265'
source_url: https://arxiv.org/abs/2310.14265
tags:
- adversarial
- attack
- samples
- tasks
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CT-GAT, a cross-task generative adversarial
  attack method that leverages the transferability of adversarial examples across
  different tasks. Unlike prior work that relies on training substitute models, CT-GAT
  directly learns universal adversarial features by training a sequence-to-sequence
  generative model on adversarial samples collected from multiple tasks.
---

# CT-GAT: Cross-Task Generative Adversarial Attack based on Transferability

## Quick Facts
- arXiv ID: 2310.14265
- Source URL: https://arxiv.org/abs/2310.14265
- Reference count: 21
- Key outcome: CT-GAT achieves superior attack performance with fewer queries by leveraging cross-task transferability, attaining highest attack success rates while reducing average queries by 31.9%-91.2%

## Executive Summary
CT-GAT is a cross-task generative adversarial attack method that exploits the transferability of adversarial examples across different NLP tasks. Unlike prior approaches that require training substitute models, CT-GAT directly learns universal adversarial features by training a sequence-to-sequence generative model on adversarial samples collected from multiple tasks. The key insight is that adversarial transferability extends beyond models of the same task to different tasks. Experiments on ten datasets demonstrate that CT-GAT achieves superior attack performance with significantly fewer queries compared to baselines, while preserving the original meaning of adversarial examples.

## Method Summary
CT-GAT trains a sequence-to-sequence generative model (BART) on adversarial samples from multiple tasks to acquire universal adversarial features. The method collects adversarial examples from various tasks and datasets into a training corpus (TCAB), then trains the generator to learn generalizable perturbation patterns. During attack, the trained generator produces adversarial examples for new victim models without requiring direct access to them. The approach implicitly learns perturbation constraints from the training data rather than enforcing them explicitly, and leverages cross-task transferability by exposing the generator to diverse adversarial patterns across different NLP tasks.

## Key Results
- Achieves highest attack success rate on most datasets compared to baselines
- Reduces average number of queries by 31.9%-91.2% compared to traditional methods
- Preserves semantic meaning of original text while maintaining attack effectiveness
- Outperforms state-of-the-art black-box attack methods including TextFooler, PWWS, and BERT-Attack

## Why This Works (Mechanism)

### Mechanism 1: Cross-Task Transferability
Adversarial transferability extends beyond models of the same task to different tasks because adversarial features capture universal perturbation patterns (word segmentation, synonym substitution, character-level noise) that are task-agnostic. This works because adversarial perturbations that exploit structural vulnerabilities tend to fool models across different tasks if those vulnerabilities are shared.

### Mechanism 2: Universal Feature Learning
Training a generative model on adversarial examples from multiple tasks produces a universal adversarial feature extractor by exposing it to diverse perturbation patterns. This broadens the generator's coverage of possible adversarial transformations, enabling it to generalize to unseen tasks by learning the commonalities in how adversarial perturbations work across different domains.

### Mechanism 3: Implicit Constraint Learning
Replacing explicit perturbation constraints with implicit constraints from training data improves imperceptibility and transferability. The adversarial examples in the training set already incorporate task-specific limits (e.g., synonym substitution rules), so the generator learns to respect these constraints through the distributional properties of the training data rather than explicit enforcement.

## Foundational Learning

- **Concept: Transferability in adversarial attacks**
  - Why needed here: Understanding how adversarial examples transfer across models and tasks is the foundation of the CT-GAT approach
  - Quick check question: What is the difference between white-box, black-box, and transferability-based attacks?

- **Concept: Sequence-to-sequence models for text generation**
  - Why needed here: CT-GAT uses a transformer-based encoder-decoder architecture to learn and generate adversarial examples
  - Quick check question: How does the self-attention mechanism in the encoder help capture context for adversarial perturbations?

- **Concept: Adversarial example evaluation metrics**
  - Why needed here: ASR, query count, perturbation degree, and semantic similarity are used to measure the effectiveness of CT-GAT
  - Quick check question: Why is semantic similarity (e.g., cosine similarity of USE vectors) important for evaluating adversarial attacks?

## Architecture Onboarding

- **Component map**: Training data collector -> Encoder-decoder model (BART) -> Generator -> Evaluator
- **Critical path**: 1) Collect adversarial examples from multiple tasks, 2) Train BART on mixed adversarial dataset, 3) Use trained generator to produce adversarial examples for new tasks, 4) Evaluate attack performance
- **Design tradeoffs**: 
  - Pros: No need for substitute models, leverages cross-task transferability, reduces query cost
  - Cons: Requires diverse training data, may not generalize to unseen domains, implicit constraints may not always preserve semantics
- **Failure signatures**: 
  - Low ASR: Training data lacks diversity or adversarial examples are too domain-specific
  - High perturbation: Generator overfits to aggressive perturbations in training data
  - Poor semantic preservation: Implicit constraints fail to maintain original meaning
- **First 3 experiments**:
  1. Train CT-GAT on a small set of tasks (e.g., hate speech and sentiment) and test on a held-out task
  2. Vary the proportion of different task data in training to see how it affects cross-task transferability
  3. Compare CT-GAT's performance with and without character-level adversarial examples in the training set

## Open Questions the Paper Calls Out

### Open Question 1
How do CT-GAT's adversarial examples perform against state-of-the-art defensive models compared to traditional attack methods? The paper mentions defensive experiments using generative models to clean adversarial inputs, but the defense performance against word-level attacks is not particularly outstanding. Additional experiments comparing CT-GAT's adversarial examples against various state-of-the-art defensive models would provide clearer understanding of its robustness.

### Open Question 2
What is the impact of dataset diversity on CT-GAT's ability to generalize and generate effective adversarial examples across different tasks and domains? The paper states that CT-GAT was trained on a limited dataset, which may constrain its ability to generalize to broader, real-world scenarios. The authors suggest that incorporating more diverse datasets could enhance the method's applicability, but empirical evidence is lacking.

### Open Question 3
How does CT-GAT's performance compare to other state-of-the-art black-box attack methods that do not rely on substitute models? The paper compares CT-GAT to several baseline methods but does not compare it to other innovative black-box attack approaches that may have similar characteristics. A comprehensive comparison would provide insights into its relative strengths and weaknesses.

## Limitations
- The core assumption of universal adversarial transferability across diverse NLP tasks remains theoretically underspecified
- Implicit constraint learning may fail to preserve semantics when training data contains aggressive or domain-specific perturbations
- Performance may degrade on tasks or domains not well-represented in the training corpus
- The method requires collecting adversarial examples from multiple tasks upfront, which can be resource-intensive

## Confidence

- **High confidence**: Experimental methodology and quantitative results (ASR improvements, query reductions) are well-documented and rigorously compared against baselines
- **Medium confidence**: The cross-task transferability claim is supported by results but the theoretical mechanism explaining universality across diverse tasks needs more substantiation
- **Low confidence**: The reliability of implicit constraint learning across different domains and the potential for semantic drift in generated adversarial examples

## Next Checks

1. **Domain transfer robustness**: Test CT-GAT on a completely different NLP task (e.g., medical text classification) that was not represented in the training data to assess true cross-task generalization
2. **Semantic drift measurement**: Conduct a more extensive human evaluation specifically measuring semantic preservation across different attack strengths, not just automatic metrics
3. **Perturbation diversity analysis**: Analyze the types of perturbations CT-GAT learns across different training task combinations to identify which perturbation patterns are most transferable versus task-specific