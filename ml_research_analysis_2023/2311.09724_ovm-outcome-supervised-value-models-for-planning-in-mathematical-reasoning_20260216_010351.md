---
ver: rpa2
title: OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning
arxiv_id: '2311.09724'
source_url: https://arxiv.org/abs/2311.09724
tags:
- supervision
- planning
- path
- reasoning
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of error propagation in multi-step
  mathematical reasoning with large language models (LLMs). The authors propose using
  outcome supervision to train a value model (OVM) that estimates the potential of
  intermediate reasoning paths to lead to correct final answers, rather than just
  checking per-step correctness.
---

# OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning

## Quick Facts
- arXiv ID: 2311.09724
- Source URL: https://arxiv.org/abs/2311.09724
- Authors: 
- Reference count: 8
- Key outcome: OVM-7B achieves 84.7% accuracy on GSM8K, state-of-the-art among LLMs up to 13B parameters

## Executive Summary
This paper addresses error propagation in multi-step mathematical reasoning by introducing Outcome-supervised Value Models (OVMs). The key insight is that outcome supervision - using only final answer correctness labels - can train models to estimate the value of intermediate reasoning paths, enabling better planning than traditional process supervision. The approach eliminates the need for expensive step-level annotations while achieving state-of-the-art performance on GSM8K and dramatically improving success rates on Game of 24 from 11% to 78.7%.

## Method Summary
The method trains a value model (OVM) using outcome supervision where only final answer correctness is known. The OVM learns to estimate whether partial reasoning paths will lead to correct final answers. During inference, beam search uses OVM scores to guide path selection. The approach contrasts with process supervision which requires per-step correctness annotations. Training involves generating multiple solution paths per question, labeling them with final answer correctness, and training the OVM with MSE loss for 1-10 epochs depending on the dataset.

## Key Results
- OVM-7B achieves 84.7% accuracy on GSM8K, state-of-the-art among LLMs up to 13B parameters
- Success rate on Game of 24 improves from 11% to 78.7% with fewer sampled paths
- OVM demonstrates superior planning capability compared to non-planning methods and traditional reward models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Outcome supervision accidentally trains the model to predict future correctness, enabling value estimation.
- Mechanism: When trained with outcome supervision, the model receives the final answer's correctness label for every token in the reasoning path. This forces the model to learn to predict whether the current step will lead to a correct final answer, effectively teaching it value estimation.
- Core assumption: The model can learn to generalize from final labels to intermediate step values through standard training objectives.
- Evidence anchors:
  - [abstract]: "We argue that in guided decoding, assessing the potential of an incomplete reasoning path can be more advantageous than simply ensuring per-step correctness"
  - [section 3.1.1]: "the model is trained to predict the correctness of the final answer at each step, irrespective of the fact that the final answer is yet to be produced"
  - [corpus]: Weak evidence - related papers focus on process supervision and RL approaches rather than outcome supervision for value estimation
- Break condition: If the model cannot generalize from final labels to intermediate step values, or if the correlation between step quality and final answer correctness is too weak.

### Mechanism 2
- Claim: Outcome supervision enables better planning than process supervision in partial path verification.
- Mechanism: In partial path verification, outcome supervision leads to value estimation (likelihood of reaching correct answer) while process supervision leads to reward estimation (immediate step correctness). Value estimation is more appropriate for planning as it considers future outcomes.
- Core assumption: Planning benefits more from value estimation than reward estimation when the goal is to reach correct final answers.
- Evidence anchors:
  - [abstract]: "We argue that assessing potentials of incomplete reasoning paths could be more advantageous as it guides towards correct final answers"
  - [section 2.2]: "Verifiers under process supervision assess per-step correctness during planning, similar to a reward model in reinforcement learning; whereas those under outcome supervision focus on the likelihood of achieving a correct final answer, similar to a value model in reinforcement learning"
  - [corpus]: Moderate evidence - related work on process supervision (PRM) vs outcome supervision (OVM) comparisons in Game of 24
- Break condition: If step-level correctness is a better predictor of final answer correctness than the value estimation approach.

### Mechanism 3
- Claim: Outcome supervision provides better scalability than process supervision by eliminating the need for step-level annotations.
- Mechanism: Process supervision requires expensive annotations at each step of reasoning paths, while outcome supervision only needs final answer correctness labels, which can be obtained automatically through string matching or simple rules.
- Core assumption: Step-level annotations are significantly more expensive and harder to obtain than final answer correctness labels.
- Evidence anchors:
  - [abstract]: "the OVM eliminates the need for labor-intensive annotations of step-level correctness, thereby significantly enhancing its scalability"
  - [section 2.2]: "Reward-based Models accesses only the correctness of the current steps, i.e. p(S(1:t) is correct|q). Obviously, annotating such step-level correctness labels are labor-intensive"
  - [corpus]: Limited evidence - related papers discuss automated process supervision but don't directly compare annotation costs
- Break condition: If step-level annotations become easily obtainable or if final answer labels are insufficient for training effective models.

## Foundational Learning

- Concept: Reinforcement Learning - Value vs Reward Functions
  - Why needed here: The paper draws an analogy between outcome supervision (value model) and process supervision (reward model) in RL, explaining why value estimation is better for planning
  - Quick check question: What is the key difference between a value function and a reward function in RL, and why does this matter for planning?

- Concept: Token-level vs Sequence-level Supervision
  - Why needed here: The paper explains how outcome supervision (sequence-level) can be adapted for token-level training, which is crucial for understanding the "accidental" value estimation mechanism
  - Quick check question: How does the paper achieve token-level supervision from sequence-level outcome labels, and why is this approach effective?

- Concept: Beam Search and Planning in Generation
  - Why needed here: The paper uses beam search guided by value estimation for planning, so understanding this search strategy is essential for grasping the implementation
  - Quick check question: How does value-guided beam search differ from traditional probability-guided beam search in language model decoding?

## Architecture Onboarding

- Component map:
  Generator -> OVM -> Beam Search -> Final Answer

- Critical path:
  1. Generator produces multiple reasoning paths for a question
  2. OVM evaluates each partial path's potential to lead to correct answer
  3. Beam search selects top candidates based on OVM scores
  4. Process repeats until paths are complete
  5. Final answer is selected from completed paths

- Design tradeoffs:
  - Outcome supervision vs process supervision: Simpler training but potentially less precise than step-level guidance
  - Value estimation vs reward estimation: Better for planning but may miss immediate errors
  - Beam search parameters (size, step count): Balance between computational cost and performance

- Failure signatures:
  - Poor performance on very long reasoning chains (>8 steps) due to insufficient training data
  - Over-reliance on linguistic patterns rather than mathematical reasoning
  - Failure to catch early errors that don't significantly impact final answer correctness

- First 3 experiments:
  1. Compare OVM planning vs vanilla sampling on GSM8K to verify planning effectiveness
  2. Compare OVM vs PRM on Game of 24 to validate value estimation vs reward estimation
  3. Vary problem difficulty (step count) to test when planning provides the most benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of OVM planning compare to other planning methods like Monte Carlo Tree Search (MCTS) or AlphaZero-like approaches in mathematical reasoning tasks?
- Basis in paper: [inferred] The paper mentions MCTS as a popular method for balancing exploration and exploitation during planning and discusses AlphaZero-like tree search approaches in related works.
- Why unresolved: The paper focuses on comparing OVM with non-planning methods and traditional reward models, but does not directly compare OVM to other planning methods like MCTS or AlphaZero-like approaches.
- What evidence would resolve it: Experiments comparing OVM planning with MCTS or AlphaZero-like approaches on mathematical reasoning benchmarks like GSM8K and Game of 24 would provide evidence for this comparison.

### Open Question 2
- Question: How does the performance of OVM vary with different generator capabilities, and is there a point of diminishing returns for planning?
- Basis in paper: [explicit] The paper discusses findings that show stronger models may benefit less from planning and weaker models may struggle to benefit from planning.
- Why unresolved: While the paper presents findings on the relationship between model capability and planning benefits, it does not explore the full range of generator capabilities or identify a specific point of diminishing returns for planning.
- What evidence would resolve it: Experiments testing OVM planning with a wide range of generator capabilities, including very strong and very weak models, would help identify the point of diminishing returns for planning.

### Open Question 3
- Question: How does the performance of OVM compare to other outcome-supervised methods like self-consistency or majority voting in mathematical reasoning tasks?
- Basis in paper: [explicit] The paper mentions self-consistency as a non-planning method for comparison and discusses majority voting as a method that has been shown to be outperformed by verifiers.
- Why unresolved: While the paper compares OVM to non-planning methods like self-consistency and majority voting, it does not directly compare OVM to other outcome-supervised methods that may exist or be developed in the future.
- What evidence would resolve it: Experiments comparing OVM to other outcome-supervised methods, including self-consistency and majority voting, on mathematical reasoning benchmarks would provide evidence for this comparison.

## Limitations
- Performance gain on GSM8K is modest (2-3% absolute improvement)
- Results are specialized to mathematical reasoning with clear structure
- OVM effectiveness degrades for longer reasoning chains (>8 steps)
- Claim of "accidental" value estimation lacks rigorous ablation studies

## Confidence

**High Confidence**: The core claim that outcome supervision can be used to train value models is well-supported by the experimental results, particularly the GSM8K performance improvement over non-planning methods.

**Medium Confidence**: The assertion that outcome supervision is more scalable than process supervision due to reduced annotation requirements is reasonable but lacks direct cost comparison data.

**Low Confidence**: The mechanistic claim that outcome supervision "accidentally" trains value estimation rather than just per-step correctness prediction needs more rigorous validation through controlled experiments.

## Next Checks

1. **Ablation Study on Supervision Type**: Train identical models with outcome supervision vs. process supervision on the same data, measuring not just final accuracy but also intermediate path quality to verify the claimed value estimation mechanism.

2. **Scalability Cost Analysis**: Quantify the actual annotation costs for process supervision (step-level) vs. outcome supervision (final answer only) across different problem types to empirically validate the claimed scalability advantage.

3. **Long Chain Performance Analysis**: Systematically test OVM performance across reasoning chains of varying lengths (2-10 steps) to identify the exact point where value estimation breaks down and whether this correlates with training data statistics.