---
ver: rpa2
title: 'GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary
  Path towards GPT-4 and Beyond'
arxiv_id: '2309.16583'
source_url: https://arxiv.org/abs/2309.16583
tags:
- llms
- shot
- gpt-3
- evaluation
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GPT-Fathom, an open-source and reproducible
  LLM evaluation suite that systematically benchmarks 10+ leading LLMs across 20+
  curated benchmarks in 7 capability categories. The key contribution is a retrospective
  study on OpenAI's evolutionary path from GPT-3 to GPT-4, shedding light on questions
  like the impact of code data on reasoning, the effectiveness of SFT/RLHF, and the
  alignment tax.
---

# GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond

## Quick Facts
- arXiv ID: 2309.16583
- Source URL: https://arxiv.org/abs/2309.16583
- Authors: 
- Reference count: 40
- Key outcome: Introduces GPT-Fathom, an open-source LLM evaluation suite that benchmarks 10+ models across 20+ benchmarks, revealing GPT-4's superior performance and identifying challenges like the "seesaw phenomenon" and model sensitivity to prompts.

## Executive Summary
This paper presents GPT-Fathom, a comprehensive and reproducible framework for benchmarking large language models (LLMs) across multiple capability categories. By systematically evaluating leading models like GPT-4, PaLM 2, and Claude 2 under consistent settings, the study offers insights into the evolutionary path from GPT-3 to GPT-4. Key findings include GPT-4's dominance in most capabilities, the impact of code data on reasoning, and the challenges posed by model sensitivity and non-monotonic capability progressions. The work highlights the importance of controlled experiments in AI benchmarking and provides a foundation for future LLM research.

## Method Summary
GPT-Fathom leverages the OpenAI Evals framework to evaluate 10+ leading LLMs, including OpenAI's models (GPT-3, GPT-3.5, GPT-4), PaLM 2, Claude 2, LLaMA, and Llama 2. The evaluation suite encompasses 20+ curated benchmarks across 7 capability categories, such as reasoning, coding, and mathematics. Consistent settings and prompts are used to ensure fair comparisons, and sensitivity testing is conducted to assess model brittleness. Results are analyzed to identify performance trends, the "seesaw phenomenon," and the impact of architectural changes on capabilities.

## Key Results
- GPT-4 significantly outperforms other models across most capability categories, with PaLM 2 and Claude 2 lagging behind.
- The inclusion of code data in pretraining enhances reasoning capabilities, as evidenced by code-davinci-002's strong performance.
- The study identifies a "seesaw phenomenon" where certain capabilities improve while others degrade with newer model versions, highlighting the complexity of LLM advancement.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Consistent evaluation settings and prompts across benchmarks enable apples-to-apples comparisons between LLMs.
- Mechanism: By fixing parameters like number of shots, Chain-of-Thought usage, and prompt templates, the evaluation suite removes confounding variables that could skew performance differences between models.
- Core assumption: Differences in benchmark scores primarily reflect true capability differences rather than variations in experimental setup.
- Evidence anchors:
  - [abstract] "Many existing LLM leaderboards reference scores from other papers without consistent settings and prompts, which may inadvertently encourage cherry-picking favored settings and prompts for better results."
  - [section] "To achieve reliable conclusions, it is crucial to make apples-to-apples LLM comparisons with consistent settings and prompts."
- Break Condition: If some benchmarks are inherently more sensitive to prompt variations than others, even consistent settings may not guarantee fair comparisons.

### Mechanism 2
- Claim: Retrospective evaluation of OpenAI's evolutionary path reveals key factors driving capability improvements.
- Mechanism: By evaluating legacy models alongside current ones under identical conditions, the study can isolate the effects of architectural changes, pretraining data shifts, and fine-tuning methods on specific capabilities.
- Core assumption: Performance trends across model generations can be attributed to identifiable technical changes rather than random variation.
- Evidence anchors:
  - [abstract] "Our retrospective study on OpenAI's earlier models offers valuable insights into the evolutionary path from GPT-3 to GPT-4."
  - [section] "Our analysis sheds light on many community-concerned questions (e.g., the gap between OpenAI / non-OpenAI models, whether adding code data improves reasoning capability, which aspects of LLM capability can be improved by SFT and RLHF, how much is the alignment tax, etc.)."
- Break Condition: If performance improvements are driven by factors not captured in the analysis (e.g., scale effects, emergent capabilities), the attribution to specific technical changes may be incomplete.

### Mechanism 3
- Claim: Identifying model sensitivity to prompts and the "seesaw phenomenon" highlights critical challenges for LLM advancement.
- Mechanism: Systematic variation of prompts and evaluation settings reveals models' brittleness and non-monotonic capability progressions, guiding research priorities.
- Core assumption: These phenomena are not isolated incidents but indicative of broader challenges facing the field.
- Evidence anchors:
  - [abstract] "We also study the impacts of model sensitivity with extensive experiments. We strongly encourage the research community to dedicate more efforts to tackling these novel challenges."
  - [section] "We discover the seesaw phenomenon of LLM capabilities, even on the latest GPT-4 model. We also study the impacts of model sensitivity with extensive experiments."
- Break Condition: If these phenomena prove to be artifacts of the specific evaluation suite rather than universal LLM properties, their significance as research priorities may be overstated.

## Foundational Learning

- Concept: Importance of controlled experiments in AI benchmarking
  - Why needed here: To ensure that observed performance differences between models are due to genuine capability variations rather than experimental artifacts
  - Quick check question: What are the key parameters that should be held constant across LLM evaluations to enable fair comparisons?

- Concept: Role of pretraining data composition in shaping LLM capabilities
  - Why needed here: To understand how factors like the inclusion of code data in pretraining can influence reasoning and other cognitive abilities
  - Quick check question: How might pretraining on code data differentially impact various LLM capabilities compared to pretraining on general web text?

- Concept: Trade-offs between model alignment and raw capability
  - Why needed here: To interpret findings about the "alignment tax" and understand the tension between instruction-following and benchmark performance
  - Quick check question: What are the potential downsides of aggressive alignment fine-tuning on benchmark performance, and how can they be mitigated?

## Architecture Onboarding

- Component map:
  OpenAI Evals framework -> Benchmark collection (20+ tasks) -> Model evaluation pipeline (10+ LLMs) -> Result analysis and visualization tools -> Sensitivity testing modules

- Critical path:
  1. Select and configure benchmarks
  2. Set up evaluation environment with consistent parameters
  3. Run evaluations across all models
  4. Collect and parse results
  5. Analyze trends and sensitivities
  6. Visualize findings and identify key insights

- Design tradeoffs:
  - Breadth vs. depth of benchmark coverage
  - Use of few-shot vs. zero-shot settings
  - Inclusion of Chain-of-Thought vs. answer-only prompting
  - Focus on closed-source vs. open-source models

- Failure signatures:
  - Inconsistent results across repeated runs (sampling variance)
  - Extreme sensitivity to minor prompt variations
  - Lack of improvement despite architectural advances (seesaw phenomenon)

- First 3 experiments:
  1. Run a single benchmark (e.g., MMLU) across all models with fixed 5-shot setting to verify basic functionality
  2. Compare results using 1-shot vs. 5-shot settings to quantify impact of in-context learning
  3. Evaluate a subset of models using both Chain-of-Thought and answer-only prompting to assess reasoning benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact impact of different types of code data (e.g., Python vs. other languages) on LLM reasoning capabilities?
- Basis in paper: [explicit] The paper mentions that OpenAI's model code-davinci-002, trained on a mixture of text and code data, showed strong performance on reasoning tasks compared to GPT-3 Series models.
- Why unresolved: The paper does not provide a detailed breakdown of how different types of code data specifically influence reasoning capabilities.
- What evidence would resolve it: Conducting ablation studies with LLMs trained on different types of code data (e.g., Python, Java, C++) and comparing their performance on reasoning benchmarks.

### Open Question 2
- Question: How does the seesaw phenomenon manifest across different model sizes and architectures?
- Basis in paper: [explicit] The paper identifies a "seesaw phenomenon" where certain capabilities improve while others degrade with newer model versions, but does not explore this across different model sizes or architectures.
- Why unresolved: The study focuses on OpenAI's models and does not compare the seesaw phenomenon across a broader range of model architectures.
- What evidence would resolve it: Evaluating a diverse set of LLMs with varying sizes and architectures (e.g., GPT, PaLM, LLaMA) on the same benchmarks to identify consistent patterns in the seesaw phenomenon.

### Open Question 3
- Question: What is the long-term impact of SFT and RLHF on the intrinsic problem-solving capabilities of LLMs?
- Basis in paper: [explicit] The paper discusses how SFT and RLHF models excel in the pass@1 metric but slightly underperform in pass@100, suggesting a trade-off between one-take coding and intrinsic problem-solving skills.
- Why unresolved: The paper does not explore the long-term effects of SFT and RLHF on LLM capabilities beyond coding tasks.
- What evidence would resolve it: Conducting longitudinal studies on LLMs with SFT and RLHF fine-tuning across various task types to assess the sustained impact on intrinsic problem-solving capabilities.

## Limitations

- The study's conclusions about the evolutionary path from GPT-3 to GPT-4 rely heavily on black-box evaluations without access to internal model details.
- The "seesaw phenomenon" and model sensitivity findings may be specific to the evaluation suite's benchmarks and prompts rather than universal LLM properties.
- The exact prompts and few-shot examples used for each benchmark are not fully specified, which may limit reproducibility.

## Confidence

- **High confidence**: GPT-4's superior performance across most capability categories compared to other leading models.
- **Medium confidence**: The impact of code data on reasoning capabilities and the effectiveness of SFT/RLHF in improving specific abilities.
- **Low confidence**: The identification of the "seesaw phenomenon" as a general challenge for LLM advancement and the universal importance of consistent evaluation settings.

## Next Checks

1. **Prompt sensitivity validation**: Conduct ablation studies by systematically varying prompt formats, few-shot examples, and Chain-of-Thought usage across a subset of benchmarks to quantify the true impact of prompt sensitivity on model rankings.

2. **Cross-suite reproducibility**: Replicate the evaluation using a different LLM benchmarking framework (e.g., HELM or BigBench) to verify whether the observed performance trends and phenomena are consistent across evaluation methodologies.

3. **Scale-adjusted analysis**: Control for model scale by evaluating scaled-down versions of GPT-4 and other models on the same benchmarks to determine whether performance differences can be attributed to architectural changes versus sheer model size.