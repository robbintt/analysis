---
ver: rpa2
title: Leveraging triplet loss for unsupervised action segmentation
arxiv_id: '2304.06403'
source_url: https://arxiv.org/abs/2304.06403
tags:
- action
- segmentation
- temporal
- video
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses unsupervised action segmentation in complex
  activity videos. It proposes a novel framework that learns temporal-semantic aware
  representations from a single video using a shallow neural network and triplet loss.
---

# Leveraging triplet loss for unsupervised action segmentation

## Quick Facts
- arXiv ID: 2304.06403
- Source URL: https://arxiv.org/abs/2304.06403
- Reference count: 33
- State-of-the-art performance on Breakfast and Youtube INRIA datasets for unsupervised action segmentation

## Executive Summary
This paper introduces a novel framework for unsupervised action segmentation in complex activity videos using a shallow neural network with triplet loss on similarity distributions. The method learns temporal-semantic aware representations from a single video without any labeled training data, then applies generic clustering algorithms for segmentation. By modeling both temporal proximity and semantic similarity through a novel triplet selection strategy, the approach achieves superior performance compared to existing unsupervised methods.

## Method Summary
The method employs a shallow MLP to transform initial video features into temporal-semantic aware (TSA) representations using triplet loss. Rather than using raw feature vectors, each frame is represented by a probability distribution of its similarity to all other frames. A novel triplet selection strategy combines temporal and semantic priors through a weighted similarity distribution (fts = α · ft + (1 − α) · fs), where ft captures temporal proximity and fs captures visual similarity. The learned representations are then segmented using K-means, Spectral, or FINCH clustering algorithms.

## Key Results
- Achieves state-of-the-art performance on Breakfast and Youtube INRIA Instructional datasets
- Outperforms existing unsupervised approaches in Mean over Frames (MoF), Intersection over Union (IoU), and F1-score
- Demonstrates effectiveness of triplet loss on similarity distributions for temporal action segmentation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Triplet loss on similarity distributions smooths and stabilizes representation learning for temporal action segmentation.
- Mechanism: Using probability distribution functions (PDFs) as feature vectors instead of initial features provides smoother and more robust representations that consider all video information for each frame.
- Core assumption: Similarity distributions capture both temporal continuity and semantic consistency better than raw features.
- Evidence anchors:
  - "Our method is a deep metric learning approach rooted in a shallow network with a triplet loss operating on similarity distributions"
  - "Using probability distribution functions (PDFs), fts as feature vectors, instead of initial features X can provide several benefits for action segmentation. Since PDFs consider all the information in the video for each frame, they are smoother and more robust compared to the initial features extracted from the data X"
- Break condition: If similarity distributions become too uniform or fail to capture discriminative information between actions, the KL divergence becomes uninformative and learning stalls.

### Mechanism 2
- Claim: Temporal-semantic weighted similarity distribution encodes both temporal proximity and semantic similarity.
- Mechanism: fts = α · ft + (1 − α) · fs combines temporal similarity (based on frame distance) and semantic similarity (based on visual similarity in feature space).
- Core assumption: Actions are temporally continuous and semantically homogeneous, so both temporal and semantic similarities are predictive of action boundaries.
- Evidence anchors:
  - "novel triplet selection strategy that effectively models temporal and semantic priors"
  - "We assume that the representational clustering grounding action segmentation encodes both temporal and semantic similarity, based on two observations: (i) an action in a video is a sequence of images temporally continuous, therefore temporal adjacent frames are likely to belong to the same action. (ii) frames corresponding to the same action (but not necessarily temporal adjacent) should have similar representation, encoding the common underlying semantic."
- Break condition: If the weight α is poorly chosen, the model may overemphasize either temporal smoothness or semantic similarity, leading to poor segmentation.

### Mechanism 3
- Claim: Shallow MLP architecture with triplet loss is sufficient for learning discriminative action representations.
- Mechanism: A single hidden layer MLP transforms initial features into TSA representations, with the triplet loss encouraging similar frames to be close and dissimilar frames to be far apart.
- Core assumption: Complex temporal dependencies can be captured by a shallow network when combined with appropriate metric learning.
- Evidence anchors:
  - "Our method is a deep metric learning approach rooted in a shallow network"
  - "Empirical experiments showed that using a single hidden layer was easier and faster to train than deeper models while achieving similar performance."
- Break condition: If the action space is too complex or the initial features are too noisy, a shallow network may not capture necessary non-linear transformations.

## Foundational Learning

- Concept: Triplet loss and metric learning
  - Why needed here: To learn a representation space where frames from the same action are close and frames from different actions are far apart, without labeled data.
  - Quick check question: What is the difference between triplet loss and contrastive loss in terms of the information they use?

- Concept: Temporal and semantic similarity modeling
  - Why needed here: To capture both the temporal continuity of actions and the semantic consistency across non-adjacent frames of the same action.
  - Quick check question: How would you define a temporal similarity function that decreases as frame distance increases?

- Concept: Probability distribution representations
  - Why needed here: To create smoother, more robust feature representations that consider global context rather than just local frame features.
  - Quick check question: Why might representing frames as similarity distributions be more robust than using raw feature vectors?

## Architecture Onboarding

- Component map: Input features (IDT or VGG conv5) → Shallow MLP (1 hidden layer, ReLU) → TSA representations Z → Clustering (K-means/Spectral/FINCH) → Segmentation output
- Critical path: Feature extraction → MLP transformation → Triplet loss optimization → Clustering → Segmentation evaluation
- Design tradeoffs: Shallow network trades expressiveness for training efficiency and reduced overfitting; triplet loss on distributions trades computational cost for smoother gradients
- Failure signatures: Poor MoF/F1 scores despite reasonable clustering suggest triplet loss not learning discriminative representations; high variance across runs suggests instability in triplet selection or learning rate
- First 3 experiments:
  1. Verify triplet selection strategy produces meaningful positive/negative pairs by visualizing fts distributions for random anchors
  2. Test different values of α to find optimal balance between temporal and semantic similarity weighting
  3. Compare performance using raw features vs. similarity distribution representations with identical network architecture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed approach generalize to videos with significantly different action durations or frame rates compared to the training data?
- Basis in paper: The paper focuses on complex activity videos with high temporal resolution but does not discuss performance on videos with varying frame rates or action durations.
- Why unresolved: The experimental evaluation is limited to two specific datasets with relatively consistent temporal characteristics, leaving the model's robustness to temporal variations unexplored.
- What evidence would resolve it: Testing the model on a diverse set of videos with varying frame rates, action durations, and temporal structures would demonstrate its generalization capabilities.

### Open Question 2
- Question: Can the learned action representations be effectively transferred to videos from entirely different domains or activities not seen during training?
- Basis in paper: The paper claims the approach can be applied to any video regardless of dataset, but does not provide evidence of cross-domain transfer learning.
- Why unresolved: The experiments are conducted within the same datasets, and there is no exploration of how well the learned representations transfer to unseen activities or domains.
- What evidence would resolve it: Evaluating the model on videos from completely different domains or activities (e.g., sports, medical procedures) would demonstrate its cross-domain applicability.

### Open Question 3
- Question: How does the performance of the model scale with the number of action classes in the video?
- Basis in paper: The paper mentions using the average number of action classes for hyperparameter L but does not analyze how performance varies with the number of action classes.
- Why unresolved: The experimental datasets have a limited number of action classes (6-9), and there is no investigation into how the model performs on videos with a larger number of action classes.
- What evidence would resolve it: Testing the model on videos with varying numbers of action classes (e.g., 10, 20, 30) would reveal its scalability and limitations in handling complex activities.

## Limitations
- Limited exploration of optimal weighting parameter α for combining temporal and semantic similarities across different video domains
- Computational efficiency claims relative to deeper architectures are not quantified
- Generalization to videos with significantly different temporal structures or semantic patterns remains untested

## Confidence

- High confidence: The empirical results showing state-of-the-art performance on Breakfast and Youtube INRIA datasets are well-documented and reproducible.
- Medium confidence: The mechanism that similarity distributions provide smoother gradients is theoretically sound but lacks direct empirical validation in the paper.
- Medium confidence: The claim that the shallow MLP architecture is sufficient is supported by ablation studies, but the comparison with deeper architectures is limited.

## Next Checks
1. Conduct ablation studies varying α across a wider range to determine sensitivity and optimal settings for different action types
2. Test the framework on videos with non-standard temporal patterns (rapid action changes, long-term dependencies) to assess robustness
3. Compare training times and convergence rates between the proposed shallow architecture and deeper alternatives using identical hardware and datasets