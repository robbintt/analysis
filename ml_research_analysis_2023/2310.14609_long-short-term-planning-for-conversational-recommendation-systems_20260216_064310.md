---
ver: rpa2
title: Long Short-Term Planning for Conversational Recommendation Systems
arxiv_id: '2310.14609'
source_url: https://arxiv.org/abs/2310.14609
tags:
- recommendation
- user
- conversation
- entity
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of integrating conversational
  and recommendation modules in conversational recommender systems (CRS). It proposes
  the Long Short-Term Planning (LSTP) framework, which connects these components through
  a feedback loop between a long-term planner (based on user history) and a short-term
  planner (driven by conversational context).
---

# Long Short-Term Planning for Conversational Recommendation Systems

## Quick Facts
- arXiv ID: 2310.14609
- Source URL: https://arxiv.org/abs/2310.14609
- Reference count: 20
- One-line primary result: LSTP outperforms existing methods on both recommendation and dialogue generation tasks, achieving state-of-the-art performance with improved accuracy and conversational coherence.

## Executive Summary
This paper addresses the challenge of integrating conversational and recommendation modules in conversational recommender systems (CRS). It proposes the Long Short-Term Planning (LSTP) framework, which connects these components through a feedback loop between a long-term planner (based on user history) and a short-term planner (driven by conversational context). The long-term planner predicts a target recommendation item, while the short-term planner guides the conversation toward that target by selecting relevant entities. Experiments on a newly collected dataset show that LSTP outperforms existing methods on both recommendation and dialogue generation tasks, achieving state-of-the-art performance with improved accuracy and conversational coherence.

## Method Summary
The LSTP framework consists of two planners connected by a feedback loop. The long-term planner uses time-aware self-attention on user profile data (items with timestamps) and conversational entities to predict a recommendation target. The short-term planner fuses the latest user utterance, dialog entity sequence, and LTP target through standard self-attention to select the next grounding entity. Knowledge search retrieves 1-2 hop entities around grounding entities, and T5 generates responses. The system continues until STP and LTP outputs match, at which point a recommendation is made.

## Key Results
- LSTP achieves state-of-the-art performance on both recommendation and dialogue generation tasks
- Improved recommendation accuracy with NDCG and HIT@ metrics
- Enhanced conversational coherence with better BLEU, Distinct, and F1 scores

## Why This Works (Mechanism)

### Mechanism 1
The long-term planner leverages historical user interactions with timestamps to predict a future recommendation target. User profile data (items + timestamps) is combined with conversational entities (timestamp=0) into a unified sequence. This sequence is processed by a time-aware self-attention layer that weights interactions by temporal intervals. The final hidden state predicts a target item via dot-product similarity with entity embeddings. Core assumption: The sequence representation captures long-term preference dynamics better than isolated embeddings.

### Mechanism 2
The short-term planner uses the LTP target to guide the conversational flow toward the recommendation. The short-term planner fuses the latest user utterance, the dialog entity sequence, and the LTP target through standard self-attention. It outputs the next grounding entity (item or attribute) that moves the conversation closer to the LTP target. Core assumption: Grounding decisions benefit from both immediate context and the long-term target.

### Mechanism 3
The feedback loop continues until the STP and LTP outputs match, signaling readiness for recommendation. Each turn, STP selects an entity; if it equals the LTP target, the system stops asking and issues the recommendation. Otherwise, the loop continues, with knowledge search enriching the next utterance. Core assumption: Convergence between STP and LTP implies sufficient user preference elicitation.

## Foundational Learning

- Concept: Knowledge Graph Embeddings (KGE)
  - Why needed here: Enables similarity-based reasoning between items, attributes, and topics in the same semantic space.
  - Quick check question: What distance metric does TransE use to learn entity and relation embeddings?

- Concept: Time-aware Self-Attention
  - Why needed here: Allows the model to weight historical interactions by recency and user-specific intervals, capturing dynamic preference shifts.
  - Quick check question: How does personalized interval modeling differ from standard self-attention?

- Concept: Entity Linking via Contrastive Learning
  - Why needed here: Maps conversational utterances to knowledge graph entities so that the planner can reason over grounded knowledge.
  - Quick check question: In the contrastive loss, what are the anchor, positive, and negative examples?

## Architecture Onboarding

- Component map: User profile + dialog history → LTP target → STP entity selection → Knowledge Search → T5 generation
- Critical path: User profile → LTP target → STP entity selection → Knowledge Search → T5 generation
- Design tradeoffs:
  - Stack more attention layers in LTP → higher accuracy but slower inference
  - Use 1-hop vs 2-hop knowledge search → simpler vs richer context
  - Align embeddings to 1024 dims → more expressive but larger model
- Failure signatures:
  - LTP predicts random items → check embedding alignment and temporal attention
  - STP keeps selecting irrelevant attributes → verify entity linking and fusion logic
  - Loop never converges → examine knowledge graph connectivity and grounding diversity
- First 3 experiments:
  1. Replace time-aware self-attention with standard self-attention in LTP; compare recommendation accuracy.
  2. Disable knowledge search; measure impact on BLEU and distinct metrics.
  3. Swap T5 with a smaller encoder-decoder; check if generation quality drops significantly.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the temporal interval information in user profiles improve recommendation accuracy compared to models that do not use this information?
- Basis in paper: The paper mentions that the long-term planner considers the timestamps of user interactions and that this information is used to improve recommendation accuracy. The ablation study shows that including history in the short-term planner improves recommendation outcomes.
- Why unresolved: While the paper shows that including temporal information improves performance, it does not quantify the exact contribution of this feature or compare it to other methods of incorporating temporal information.
- What evidence would resolve it: A controlled experiment comparing the performance of LSTP with and without temporal interval information, and a comparison with other methods that incorporate temporal information in different ways.

### Open Question 2
- Question: How does the choice of knowledge graph embedding method (e.g., TransE) affect the performance of the LSTP framework?
- Basis in paper: The paper uses TransE for knowledge graph embeddings, but does not explore other embedding methods or compare their performance.
- Why unresolved: The paper does not provide evidence that TransE is the optimal choice for knowledge graph embeddings in the context of conversational recommendation systems.
- What evidence would resolve it: An experiment comparing the performance of LSTP using different knowledge graph embedding methods (e.g., TransE, DistMult, ComplEx) and analyzing their impact on recommendation and conversation tasks.

### Open Question 3
- Question: How does the size and quality of the knowledge graph affect the performance of the LSTP framework?
- Basis in paper: The paper constructs a knowledge graph for movies, but does not discuss how the size or quality of the knowledge graph might impact the performance of the LSTP framework.
- Why unresolved: The paper does not provide evidence that the constructed knowledge graph is optimal for the LSTP framework or that the framework would perform similarly with a different knowledge graph.
- What evidence would resolve it: An experiment comparing the performance of LSTP with different sizes and qualities of knowledge graphs, and analyzing the impact on recommendation and conversation tasks.

## Limitations
- The TAP-Dial dataset construction methodology, particularly the clustering algorithm for determining recommendation targets and dialog flow construction, lacks sufficient detail for exact replication.
- The Multi-head Time-Aware Self-Attention mechanism's specific implementation details are underspecified, particularly how personalized temporal intervals are incorporated into the attention computation.
- Claims about the framework's ability to handle diverse recommendation domains beyond movies are not substantiated with evidence or experiments.

## Confidence

- High confidence: The core architectural concept of separating long-term preference prediction from short-term conversational guidance is sound and theoretically justified. The feedback loop mechanism for coordinating planners is clearly articulated.
- Medium confidence: The experimental results showing performance improvements over baselines are compelling, but the dataset-specific nature of the evaluation and lack of ablation studies on critical components reduce confidence in generalizability.
- Low confidence: Claims about the framework's ability to handle diverse recommendation domains beyond movies are not substantiated with evidence or experiments.

## Next Checks

1. Implement ablation studies comparing standard self-attention versus time-aware self-attention in the long-term planner to quantify the contribution of temporal modeling to recommendation accuracy.

2. Test the knowledge search component's impact by comparing 1-hop versus 2-hop retrieval on both recommendation performance and dialogue quality metrics, isolating the value of deeper knowledge integration.

3. Evaluate the framework's robustness by introducing noisy user histories or disconnected knowledge graph segments to assess how the planners handle incomplete information and whether the feedback loop can still converge.