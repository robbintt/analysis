---
ver: rpa2
title: 'ReLU to the Rescue: Improve Your On-Policy Actor-Critic with Positive Advantages'
arxiv_id: '2306.01460'
source_url: https://arxiv.org/abs/2306.01460
tags:
- learning
- policy
- function
- vsop
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces VSOP, a novel on-policy actor-critic method
  that improves upon existing approaches like PPO and A3C. VSOP addresses the issue
  of cautious interaction with the environment by incorporating three key modifications
  to the A3C algorithm: (1) applying a ReLU function to advantage estimates, (2) spectral
  normalization of actor-critic weights, and (3) using dropout as a Bayesian approximation
  for Thompson sampling.'
---

# ReLU to the Rescue: Improve Your On-Policy Actor-Critic with Positive Advantages

## Quick Facts
- arXiv ID: 2306.01460
- Source URL: https://arxiv.org/abs/2306.01460
- Reference count: 39
- Key outcome: VSOP improves on-policy actor-critic methods (PPO, A3C) through positive advantage clipping, spectral normalization, and dropout-based Thompson sampling

## Executive Summary
This paper introduces VSOP, a novel on-policy actor-critic method that addresses the issue of over-optimistic policy updates in reinforcement learning. VSOP incorporates three key modifications to the A3C algorithm: applying a ReLU function to advantage estimates, spectral normalization of actor-critic weights, and using dropout as a Bayesian approximation for Thompson sampling. The method is theoretically grounded, proving that restricting policy updates to positive advantages optimizes a lower bound on the true value function. Empirical results demonstrate significant improvements over baseline methods on continuous control and generalization benchmarks.

## Method Summary
VSOP is an on-policy actor-critic method that modifies the standard A3C algorithm with three key innovations: (1) ReLU function applied to advantage estimates to ensure only positive advantages contribute to policy updates, (2) spectral normalization of critic weights to regularize the Lipschitz constant of the value function, and (3) dropout as a Bayesian approximation to enable Thompson sampling for exploration. The method is designed to promote cautious interaction with the environment by bounding policy updates to improvements over the current value estimate.

## Key Results
- VSOP shows significant improvements in median and interquartile mean metrics over A3C, PPO, SAC, and TD3 on Mujoco continuous control benchmarks
- The method demonstrates better generalization on the ProcGen benchmark compared to PPO
- Theoretical analysis proves that VSOP optimizes a lower bound on the true value function plus a constant bounded by the Lipschitz constant

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Restricting policy updates to positive advantage estimates optimizes a lower bound on the true value function plus a constant, promoting cautious interaction.
- **Mechanism**: ReLU filters out negative advantage estimates, preventing over-optimistic policy updates by only improving actions better than the baseline.
- **Core assumption**: Policy gradient is conservative and rewards are non-negative
- **Evidence anchors**: Theorem 3.1 proves lower bound optimization; abstract states additive term is bounded by Lipschitz constant
- **Break condition**: If policy gradient is not conservative or rewards can be negative

### Mechanism 2
- **Claim**: Spectral normalization of critic weights regularizes the Lipschitz constant, promoting cautious estimates.
- **Mechanism**: Constrains spectral norm of critic network, bounding the difference between estimated and true values.
- **Core assumption**: Value function estimator is Lipschitz continuous
- **Evidence anchors**: Abstract mentions spectral normalization bounds the additive term; Equation 3 shows constant term bounded by Lipschitz constant
- **Break condition**: If value function is not Lipschitz continuous or normalization is improperly implemented

### Mechanism 3
- **Claim**: Dropout as Bayesian approximation enables Thompson sampling for adaptive state-aware exploration.
- **Mechanism**: Treats networks as Bayesian Neural Networks with dropout, enabling exploration by sampling from posterior distribution.
- **Core assumption**: Dropout rate and prior distribution are properly tuned
- **Evidence anchors**: Abstract describes dropout as Bayesian approximation for Thompson sampling; Algorithm 1 outlines dropout BNN procedure
- **Break condition**: If dropout rate is too high/low or prior is mis-specified

## Foundational Learning

- **Concept: Policy Gradient Methods**
  - Why needed here: VSOP is an on-policy actor-critic method using policy gradients for optimization
  - Quick check question: What is the difference between on-policy and off-policy methods in reinforcement learning?

- **Concept: Advantage Function**
  - Why needed here: VSOP uses advantage function to guide policy updates, requiring understanding of its properties
  - Quick check question: How does the advantage function relate to the value function and state-action value function?

- **Concept: Spectral Normalization**
  - Why needed here: Spectral normalization is used to regularize the critic network's Lipschitz constant
  - Quick check question: What is the effect of spectral normalization on the Lipschitz constant of a neural network?

## Architecture Onboarding

- **Component map**: Actor network -> Critic network -> ReLU layer -> Spectral normalization layer -> Dropout layer -> Optimizer
- **Critical path**:
  1. Interact with environment to collect state, action, reward tuples
  2. Compute advantage estimates using GAE
  3. Apply ReLU to advantage estimates
  4. Sample actor parameters (if using Thompson sampling)
  5. Update actor using policy gradient with positive advantages
  6. Update critic using MSE loss

- **Design tradeoffs**:
  - Dropout rate: Higher dropout increases exploration but may hurt performance if too high
  - Spectral normalization: Stronger normalization promotes caution but may slow learning
  - Update frequency: More frequent updates may lead to faster learning but require more compute

- **Failure signatures**:
  - Policy collapse: Consistently negative advantage estimates or too strong spectral normalization causes deterministic behavior
  - Unstable learning: Too high dropout rate or improper learning rate leads to instability

- **First 3 experiments**:
  1. Implement VSOP without Thompson sampling or spectral normalization to verify positive advantage mechanism
  2. Add Thompson sampling and verify it leads to more diverse action selection
  3. Add spectral normalization and verify it leads to more conservative value estimates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Lipschitz constant of the value function affect VSOP performance compared to other methods?
- Basis in paper: [explicit] Paper states additive term is bounded by Lipschitz constant and spectral normalization regulates this constant
- Why unresolved: Paper doesn't analyze how Lipschitz constant impacts VSOP performance across environments or compare to methods without spectral normalization
- What evidence would resolve it: Comparative experiments across environments showing relationship between Lipschitz constant and VSOP performance with control group without spectral normalization

### Open Question 2
- Question: What is the optimal dropout rate for VSOP in different environment types (high-dimensional vs low-dimensional action spaces)?
- Basis in paper: [explicit] Paper mentions small dropout rate (0.01-0.04) is beneficial but doesn't analyze optimal rate variation across environments
- Why unresolved: Paper doesn't provide systematic study of dropout rate impact on VSOP performance across various environments
- What evidence would resolve it: Comprehensive study varying dropout rate across environments comparing VSOP performance with different dropout rates

### Open Question 3
- Question: How does positive advantage clipping affect exploration-exploitation trade-offs in sparse reward environments?
- Basis in paper: [explicit] Paper introduces positive advantage clipping as key mechanism but doesn't analyze its impact on exploration-exploitation in sparse rewards
- Why unresolved: Paper doesn't analyze how positive advantage clipping affects exploration-exploitation trade-offs in sparse reward environments
- What evidence would resolve it: Experiments in sparse reward environments comparing VSOP with other methods, focusing on exploration-exploitation trade-off and impact of positive advantage clipping

## Limitations
- Theoretical guarantees rely on strong assumptions about conservative policy gradients and non-negative rewards that may not hold in practice
- The practical impact of spectral normalization on learning stability requires empirical validation beyond theoretical bounds
- Bayesian interpretation of dropout for Thompson sampling is an approximation that may not accurately capture parameter uncertainty in deep RL

## Confidence

**Confidence labels:**
- Theoretical claims (High): Mathematical proofs for lower bound optimization and spectral normalization effects are rigorous and well-established
- Empirical claims (Medium): Performance improvements are demonstrated but could be influenced by implementation details and hyperparameter tuning
- Mechanism claims (Medium): Proposed mechanisms are plausible but exact contribution of each component to overall performance is not fully isolated

## Next Checks

1. **Ablation study**: Remove each of the three key modifications (ReLU advantages, spectral normalization, dropout) individually to quantify their independent contributions to performance gains

2. **Reward negativity test**: Evaluate VSOP on environments with negative rewards to test theoretical assumption about non-negative rewards and observe if performance degrades as predicted

3. **Lipschitz constant monitoring**: Track spectral norms of critic weights during training to empirically verify they remain bounded as predicted by theory