---
ver: rpa2
title: 'LePaRD: A Large-Scale Dataset of Judges Citing Precedents'
arxiv_id: '2311.09356'
source_url: https://arxiv.org/abs/2311.09356
tags:
- legal
- retrieval
- passage
- dataset
- passages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present LePaRD, a large-scale dataset of U.S. federal judicial
  citations to precedent in context, to facilitate research on legal passage prediction.
---

# LePaRD: A Large-Scale Dataset of Judges Citing Precedents

## Quick Facts
- arXiv ID: 2311.09356
- Source URL: https://arxiv.org/abs/2311.09356
- Reference count: 11
- Key outcome: Classification-based methods outperform retrieval approaches for legal passage prediction, but significant improvement room remains

## Executive Summary
We present LePaRD, a large-scale dataset of U.S. federal judicial citations to precedent in context, to facilitate research on legal passage prediction. LePaRD contains around 4 million unique passages that have appeared in approximately 17 million contexts. We extensively evaluate various retrieval approaches on LePaRD, finding that classification-based methods work best, but legal precedent prediction remains a difficult task with significant room for improvement. We hope LePaRD will encourage legal NLP research on this practice-oriented task that promises to reduce the burden of legal research and expand access to justice.

## Method Summary
The paper constructs LePaRD by extracting quoted passages from U.S. federal court opinions and pairing them with the contexts in which they were cited. The dataset is split into 90% training, 5% development, and 5% test sets. Four retrieval approaches are evaluated: BM25, SBERT, fine-tuned SBERT, and classification using DistilBERT. The classification approach maps each target passage to a unique label and fine-tunes DistilBERT to predict the correct label given a context.

## Key Results
- Classification-based methods outperform traditional retrieval approaches (BM25, SBERT) for legal passage prediction
- Pre-trained SBERT variants transfer poorly to legal domain without fine-tuning
- Legal passage retrieval remains challenging with significant room for improvement despite classification approach gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset's construction from actual judicial citations ensures relevance and practical utility.
- Mechanism: By extracting quoted passages from judicial opinions and pairing them with the contexts in which they were cited, the dataset captures expert-annotated relevance signals.
- Core assumption: Judges cite passages that are legally relevant and binding, making their citations a reliable indicator of passage importance.
- Evidence anchors:
  - [abstract] "LePaRD is a massive collection of U.S. federal judicial citations to precedent in context."
  - [section] "We extract and validate around 4 million unique passages that have appeared in approximately 17 million contexts."
- Break condition: If judicial citations do not reliably indicate legal relevance or binding nature, the dataset's utility diminishes.

### Mechanism 2
- Claim: Transforming legal passage retrieval into a supervised classification task improves performance.
- Mechanism: Mapping each target passage to a unique label allows the use of powerful classification models like DistilBERT, which can better capture the nuanced legal reasoning required.
- Core assumption: Legal passage retrieval can be effectively modeled as a classification problem where the model learns to predict the correct passage label from a context.
- Evidence anchors:
  - [section] "We find, however, that results improve noticeably as soon as we incorporate training data and fine-tune models on the LePaRD dataset."
  - [section] "This setting works best consistently across splits and metrics chosen."
- Break condition: If the classification approach does not scale well with the number of labels or if the legal reasoning required cannot be captured by classification.

### Mechanism 3
- Claim: Domain-specific fine-tuning significantly enhances retrieval performance over generic models.
- Mechanism: Fine-tuning pre-trained models like SBERT on the LePaRD dataset allows the model to adapt to the specific language and reasoning patterns in legal texts.
- Core assumption: Pre-trained models do not inherently understand legal language and reasoning, and fine-tuning on domain-specific data is necessary for effective performance.
- Evidence anchors:
  - [section] "We find that a pre-trained SBERT variant also transfers poorly to the legal domain and especially to the legal passage retrieval task."
  - [section] "We can improve performance even more by turning legal precedent prediction into a supervised classification task and fine-tune a DistilBERT model to predict a unique label for each target passage."
- Break condition: If fine-tuning does not lead to significant performance improvements or if the model overfits to the training data.

## Foundational Learning

- Concept: Legal reasoning and precedent citation
  - Why needed here: Understanding how legal arguments are built on citations to precedents is crucial for grasping the dataset's construction and purpose.
  - Quick check question: Why are citations to precedent important in legal arguments?
- Concept: Information retrieval and text classification
  - Why needed here: The paper compares various retrieval algorithms and evaluates the task as a classification problem, requiring knowledge of these areas.
  - Quick check question: How does treating retrieval as a classification task differ from traditional retrieval methods?
- Concept: Dataset construction and evaluation
  - Why needed here: The paper details how the dataset was constructed from judicial opinions and evaluates various models on it.
  - Quick check question: What are the key steps in constructing a dataset from judicial opinions for legal passage retrieval?

## Architecture Onboarding

- Component map: Data extraction -> Preprocessing -> Model training -> Evaluation
- Critical path: Extract passages and contexts → Preprocess and split data → Train models → Evaluate performance
- Design tradeoffs: Classification approach offers better performance but may not scale well with the number of labels; retrieval models are more scalable but may underperform
- Failure signatures: Poor performance on retrieval tasks, overfitting on training data, inability to handle the long-tailed distribution of citations
- First 3 experiments:
  1. Evaluate a random baseline to establish the difficulty of the task
  2. Compare BM25 and SBERT retrieval methods to assess the impact of lexical overlap and semantic similarity
  3. Fine-tune a classification model on the dataset and compare its performance to retrieval models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can legal precedent prediction models be effectively combined with retrieval-augmented generation (RAG) to improve the accuracy and reliability of AI-generated legal documents?
- Basis in paper: [explicit] The paper discusses the potential of RAG to increase the correctness of citations and allow for quick updates as new cases come out and old cases are overturned.
- Why unresolved: While the paper highlights the potential benefits of RAG, it does not provide specific methods or experiments to demonstrate how legal precedent prediction models can be effectively integrated with RAG.
- What evidence would resolve it: Conducting experiments that combine legal precedent prediction models with RAG and evaluating their performance in generating accurate and reliable legal documents would provide evidence to resolve this question.

### Open Question 2
- Question: What are the specific challenges and limitations of legal language and entailment that hinder the transfer of general-domain pre-trained language models like SBERT to the legal domain?
- Basis in paper: [inferred] The paper mentions that pre-trained SBERT variants transfer poorly to the legal domain and legal passage retrieval task, attributing this finding to the domain shift and particular challenges of legal language and entailment.
- Why unresolved: The paper does not provide a detailed analysis of the specific challenges and limitations of legal language and entailment that hinder the transfer of general-domain pre-trained language models to the legal domain.
- What evidence would resolve it: Conducting a comprehensive analysis of the linguistic and entailment challenges in legal texts and comparing them with general-domain texts would provide evidence to resolve this question.

### Open Question 3
- Question: How can the long-tailed distribution of cited precedents be effectively addressed in legal passage retrieval models to improve their performance on less frequently cited passages?
- Basis in paper: [explicit] The paper mentions that citation frequency of passages obey a long-tailed distribution, with a few passages being cited with disproportionate frequency while most are rarely cited.
- Why unresolved: While the paper acknowledges the challenge of the long-tailed distribution, it does not provide specific methods or experiments to address this issue and improve the performance of legal passage retrieval models on less frequently cited passages.
- What evidence would resolve it: Conducting experiments that focus on improving the performance of legal passage retrieval models on less frequently cited passages and evaluating their effectiveness would provide evidence to resolve this question.

## Limitations

- Classification approach faces scalability challenges with the large number of unique labels (4 million passages)
- Long-tailed citation distribution creates data sparsity issues, with 93.3% of passages cited fewer than 20 times
- Current framework does not fully address the need for multi-passage reasoning in legal arguments

## Confidence

- High confidence: The dataset construction methodology and basic retrieval task formulation
- Medium confidence: The superiority of classification approach over retrieval methods
- Low confidence: The scalability of classification approach to full dataset size

## Next Checks

1. Test the classification approach on progressively larger subsets to empirically determine scalability limits
2. Evaluate whether multi-passage reasoning models can improve performance beyond single-passage retrieval
3. Compare performance across different legal domains (criminal, civil, administrative) to assess domain transfer within legal corpus