---
ver: rpa2
title: 'Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control'
arxiv_id: '2306.07863'
source_url: https://arxiv.org/abs/2306.07863
tags:
- tasks
- arxiv
- task
- prompting
- exemplars
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'SYNAPSE introduces a novel prompting approach for computer control
  using large language models, focusing on exemplar structure rather than self-correction.
  The method addresses context length limitations and compounding errors in multi-step
  tasks through three key components: state-conditional decomposition for temporal
  abstraction, structured prompting for state filtering and task reformulation, and
  exemplar retrieval for generalization.'
---

# Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control

## Quick Facts
- arXiv ID: 2306.07863
- Source URL: https://arxiv.org/abs/2306.07863
- Reference count: 40
- Primary result: Achieves 99.2% success rate on MiniWob++ and 56% relative improvement in Mind2Web's average step success rate over prior prompting schemes.

## Executive Summary
SYNAPSE introduces a novel prompting approach for computer control using large language models, focusing on exemplar structure rather than self-correction. The method addresses context length limitations and compounding errors in multi-step tasks through three key components: state-conditional decomposition for temporal abstraction, structured prompting for state filtering and task reformulation, and exemplar retrieval for generalization. Evaluated on MiniWob++ and Mind2Web benchmarks, SYNAPSE achieves 99.2% success rate on 64 MiniWob++ tasks using demonstrations from only 48 tasks, and shows 56% relative improvement in Mind2Web's average step success rate over prior prompting schemes.

## Method Summary
SYNAPSE uses in-context learning with three components: state-conditional decomposition breaks demonstrations into temporally abstracted action sets based on state needs, reducing context length and error compounding; structured prompting applies state filtering and task reformulation to provide clear, relevant inputs to the LLM; and exemplar retrieval uses similarity search on embedded task-state pairs to generalize to novel tasks. The method processes HTML states and natural language task descriptions through these components to generate action plans for computer control tasks.

## Key Results
- Achieves 99.2% success rate on 64 MiniWob++ tasks using demonstrations from only 48 tasks
- Shows 56% relative improvement in Mind2Web's average step success rate over prior prompting schemes
- Demonstrates effectiveness of trajectory-as-exemplar approach compared to traditional self-correction methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: State-conditional decomposition reduces context length pressure and minimizes error compounding by generating temporally abstracted action plans.
- Mechanism: Instead of generating atomic actions at every step, demonstrations are broken into sets based on when new state information is needed. This allows fewer exemplars in context while maintaining multi-step reasoning.
- Core assumption: Not every action in a task requires new environmental state; many can be planned as a group once the relevant state is available.
- Evidence anchors:
  - [abstract] "state-conditional decomposition for temporal abstraction"
  - [section 2.1] "We recognize that many computer control tasks do not require new state information for each atomic action... we adopt the principle of temporal abstraction"
- Break condition: If most tasks require state information at every step, decomposition provides no benefit and may lose granularity.

### Mechanism 2
- Claim: Structured prompting (state filtering + task reformulation) improves LLM understanding and reduces hallucination.
- Mechanism: Raw HTML states are filtered to relevant information; task descriptions are standardized into structured formats. This clarifies what the LLM needs to act on.
- Core assumption: LLMs perform better with concise, relevant state inputs and well-defined task objectives than with raw HTML and ambiguous instructions.
- Evidence anchors:
  - [abstract] "state abstraction, which filters out task-irrelevant information from raw states"
  - [section 2.2] "State filtering involves transforming complex HTML states into relevant information... task reformulation aims to standardize diverse and ambiguous natural language task descriptions."
- Break condition: If filtering removes necessary context or task reformulation over-constrains the LLM, performance may degrade.

### Mechanism 3
- Claim: Exemplar retrieval via similarity search enables generalization to novel tasks without explicit demonstration.
- Mechanism: Task-state pairs are embedded and stored in a vector database. New tasks are matched to relevant exemplars by similarity search, enabling zero-shot adaptation.
- Core assumption: There is sufficient semantic similarity between tasks that embeddings can retrieve useful exemplars for unseen tasks.
- Evidence anchors:
  - [abstract] "exemplar memory, which stores the embeddings of exemplars and retrieves them via similarity search for generalization to novel tasks."
  - [section 2.3] "When SYNAPSE encounters a new task, it first embeds the task description and state... perform a similarity search within the embedding space of the vector database to match relevant exemplars."
- Break condition: If embeddings fail to capture task semantics or the exemplar database is too sparse, retrieval will return irrelevant or no exemplars.

## Foundational Learning

- Concept: Temporal abstraction in hierarchical RL
  - Why needed here: The core idea of grouping actions based on state needs comes from temporal abstraction, allowing SYNAPSE to reduce the number of queries and context needed.
  - Quick check question: If a task requires a new state for every action, what happens to the benefit of state-conditional decomposition?

- Concept: Vector embeddings and similarity search
  - Why needed here: Embedding task-state pairs into a vector space enables efficient retrieval of relevant exemplars for novel tasks without manual specification.
  - Quick check question: If two tasks have similar surface descriptions but very different underlying actions, will their embeddings be similar?

- Concept: Prompt engineering and structured inputs
  - Why needed here: LLMs are sensitive to input format and clarity; structured prompting ensures the model receives only relevant information in a standardized format.
  - Quick check question: What happens if state filtering removes a crucial element needed for the next action?

## Architecture Onboarding

- Component map: Frontend (Task description + current state input) -> Embedding layer (Converts task-state pairs to vector embeddings) -> Vector database (Stores exemplar embeddings) -> Retrieval module (Finds nearest exemplar embeddings) -> Structured prompting pipeline (State filtering + task reformulation) -> LLM (Generates action plan based on filtered state, task reformulation, and exemplars) -> Output (Sequence of atomic actions)

- Critical path: Input → Embedding → Retrieval → Structured Prompting → LLM → Action Plan

- Design tradeoffs:
  - Embedding granularity: Finer embeddings capture more detail but increase storage/compute; coarser embeddings save resources but may miss task distinctions.
  - State filtering depth: More filtering reduces noise but risks losing context; less filtering preserves detail but may confuse the LLM.
  - Task reformulation strictness: Strict formats ensure clarity but may limit LLM flexibility; loose formats allow flexibility but risk ambiguity.

- Failure signatures:
  - No exemplars retrieved → Embedding similarity threshold too high or exemplar database too sparse
  - LLM generates irrelevant actions → State filtering removed critical context or task reformulation was too restrictive
  - Context overflow → Too many exemplars or overly long action plans in one set

- First 3 experiments:
  1. Validate that state-conditional decomposition reduces context length and maintains accuracy on a simple task (e.g., click-button-sequence).
  2. Test that state filtering removes irrelevant HTML without losing action-critical information on a complex task (e.g., click-collapsible-2).
  3. Verify exemplar retrieval returns relevant exemplars for a known novel task (e.g., email-inbox-forward-nl-turk using exemplars from email-inbox-nl-turk).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SYNAPSE perform on more complex real-world tasks beyond MiniWob++ and Mind2Web?
- Basis in paper: [inferred] The paper mentions potential for extension to multimodal prompting and direct device manipulation.
- Why unresolved: The current evaluation is limited to specific benchmark tasks. Real-world complexity may introduce new challenges.
- What evidence would resolve it: Testing SYNAPSE on diverse real-world computer control tasks, including multimodal inputs and complex UI interactions.

### Open Question 2
- Question: Can SYNAPSE's performance be further improved by integrating self-correction mechanisms?
- Basis in paper: [explicit] The paper states that SYNAPSE complements existing prompting approaches that enhance reasoning and planning, and acknowledges the potential of combining it with self-correction.
- Why unresolved: The current SYNAPSE design does not incorporate self-correction, leaving room for improvement.
- What evidence would resolve it: Experiments comparing SYNAPSE with and without self-correction modules on benchmark tasks.

### Open Question 3
- Question: How does the exemplar retrieval mechanism handle ambiguous or novel task-state pairs?
- Basis in paper: [explicit] The paper mentions that exemplar retrieval matches incoming tasks with demonstrations via similarity search but does not detail handling of ambiguous or novel pairs.
- Why unresolved: The effectiveness of exemplar retrieval in diverse scenarios is not fully explored.
- What evidence would resolve it: Analyzing the performance of exemplar retrieval on ambiguous or novel tasks not covered in the current dataset.

### Open Question 4
- Question: What is the impact of different embedding models on the accuracy of exemplar retrieval?
- Basis in paper: [inferred] The paper uses text-embedding-ada-002 but does not compare it with other embedding models.
- Why unresolved: The choice of embedding model could affect the quality of similarity search and generalization.
- What evidence would resolve it: Comparing SYNAPSE's performance using different embedding models for exemplar retrieval.

## Limitations

- The paper doesn't provide direct causal evidence for why each component works independently versus through interacting effects.
- Exemplar retrieval reliability depends heavily on embedding quality and exemplar database density, which aren't thoroughly analyzed for failure cases.
- Generalization claims are limited to two specific benchmarks without addressing scalability to more complex, open-ended computer control tasks.

## Confidence

High confidence in the core empirical results: The 99.2% success rate on MiniWob++ and 56% relative improvement on Mind2Web are well-documented through benchmark evaluations. These numbers appear robust and directly measurable.

Medium confidence in mechanism claims: While the three mechanisms are logically sound and supported by ablation studies, the paper doesn't provide direct causal evidence for why each component works. The improvements could result from multiple interacting effects rather than isolated mechanisms.

Low confidence in generalization claims: The paper demonstrates strong performance on two specific benchmarks but doesn't address scalability to more complex, open-ended computer control tasks. The exemplar database approach may face fundamental limits as task diversity increases.

## Next Checks

1. Error propagation analysis: Measure how errors compound across decomposed action sets versus atomic steps. Compare the probability of task failure given an error at each step position to determine if decomposition truly reduces compounding versus redistributing errors.

2. Ablation on filtering granularity: Systematically vary the depth of state filtering and measure the tradeoff between reduced noise and lost context. Identify the minimum filtering level that maintains performance while maximizing state information retention.

3. Embedding similarity validation: For a set of novel tasks, verify that retrieved exemplars are semantically relevant by human evaluation or task similarity metrics. Test whether surface-level similarity in embeddings correlates with functional similarity in required actions.