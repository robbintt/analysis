---
ver: rpa2
title: Art or Artifice? Large Language Models and the False Promise of Creativity
arxiv_id: '2309.14556'
source_url: https://arxiv.org/abs/2309.14556
tags:
- story
- stories
- writing
- ttcw
- creative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Researchers propose a creativity evaluation protocol for short\
  \ stories, inspired by the Torrance Test of Creative Thinking (TTCT). They adapt\
  \ the four dimensions of TTCT\u2014fluency, flexibility, originality, and elaboration\u2014\
  into 14 binary tests called the Torrance Test for Creative Writing (TTCW)."
---

# Art or Artifice? Large Language Models and the False Promise of Creativity

## Quick Facts
- arXiv ID: 2309.14556
- Source URL: https://arxiv.org/abs/2309.14556
- Reference count: 40
- Researchers propose a creativity evaluation protocol for short stories, showing LLM-generated stories pass only 9-30% of TTCW tests versus 84.7% for human-written stories.

## Executive Summary
This paper introduces the Torrance Test for Creative Writing (TTCW), a protocol for evaluating creativity in short stories based on the Torrance Test of Creative Thinking (TTCT). The researchers adapt TTCT's four dimensions into 14 binary tests and apply them to 48 stories (12 human-written, 36 LLM-generated). Results show that LLM-generated stories fail significantly more TTCW tests than human-written stories, with expert evaluators able to reliably distinguish between the two. The study also demonstrates that current LLMs cannot reliably assess creativity, as their evaluations show zero correlation with human experts.

## Method Summary
The researchers created TTCW by adapting TTCT's four dimensions (fluency, flexibility, originality, elaboration) into 14 binary tests for short stories. They collected 12 human-written stories from The New Yorker and generated 36 corresponding stories using GPT3.5, GPT4, and Claude v1.3 based on plot summaries. Ten creative writing experts evaluated each story using TTCW, with each story receiving three assessments. Experts provided binary labels with justifications, and agreement was measured using Fleiss Kappa for individual tests and Pearson correlation for aggregated scores. The researchers also tested whether LLMs could replicate expert evaluations using chain-of-thought reasoning.

## Key Results
- LLM-generated stories pass 3-10 times fewer TTCW tests than human-written stories
- Experts reach moderate agreement on individual TTCW tests (Fleiss Kappa 0.41) and strong agreement on aggregate scores (Pearson correlation 0.69)
- LLMs achieve zero correlation with expert evaluations when attempting to assess story creativity
- GPT4 and Claude outperform GPT3.5 in generating more creative stories according to TTCW metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TTCW provides reproducible creativity evaluation by aggregating multiple binary tests.
- Mechanism: Individual TTCW tests show moderate agreement (Fleiss Kappa 0.41), but aggregate scores achieve strong agreement (Pearson correlation 0.69).
- Core assumption: Creativity in short stories is multifaceted and cannot be captured by single tests.
- Evidence anchors:
  - [abstract]: "LLM-generated stories pass 3-10X less TTCW tests than stories written by professionals"
  - [section]: "experts reach moderate agreement on average (Fleiss Kappa 0.41) across the 14 TTCW tests, and reach strong agreement when considering the aggregated tests (Pearson correlation 0.69)"
- Break condition: If expert agreement on individual tests drops below 0.2, aggregate reliability becomes questionable.

### Mechanism 2
- Claim: LLMs fail at both creative writing and creativity assessment.
- Mechanism: LLM-generated stories pass only 9-30% of TTCW tests versus 84.7% for human-written stories; LLM assessors show zero correlation with expert judgments.
- Core assumption: Current LLMs lack the nuanced understanding required for creative writing evaluation.
- Evidence anchors:
  - [abstract]: "LLM-generated stories pass 3-10X less TTCW tests than stories written by professionals"
  - [section]: "LLMs are not capable of administering the TTCW tests, as the three LLMs we experiment with achieve correlations with experts that are close to zero"
- Break condition: If LLM-generated stories achieve >50% TTCW pass rate, current assessment methodology may be too lenient.

### Mechanism 3
- Claim: Expert differentiation between human and AI stories relies on specific creative elements.
- Mechanism: Experts identify AI stories through narrative ending issues, poor language proficiency, lack of rhetorical complexity, and inconsistent character development.
- Core assumption: Human writers possess distinct creative capabilities that LLMs cannot replicate.
- Evidence anchors:
  - [section]: "AI-written stories would forestall the ending by getting bigger in scope" and "characters in AI-written stories have poor rhetorical complexity"
  - [section]: "E1 highlighted one such example in a story - However, she managed to laugh louder and louder until her laughter transformed into an embrace of the sun's atmosphere"
- Break condition: If LLMs begin producing stories indistinguishable from human-written ones, current differentiation criteria become obsolete.

## Foundational Learning

- Concept: Torrance Test of Creative Thinking (TTCT) dimensions
  - Why needed here: TTCW is built on TTCT's four dimensions (fluency, flexibility, originality, elaboration) adapted for writing
  - Quick check question: Can you name all four TTCT dimensions and provide a writing-specific example of each?

- Concept: Consensual Assessment Technique (CAT)
  - Why needed here: TTCW evaluation relies on expert consensus rather than objective metrics
  - Quick check question: Why does CAT require domain experts rather than crowd workers for creative writing evaluation?

- Concept: Binary test aggregation
  - Why needed here: Individual TTCW tests show moderate agreement, but aggregate scores provide reliable creativity assessment
  - Quick check question: What statistical measure shows strong agreement when TTCW tests are aggregated?

## Architecture Onboarding

- Component map:
  - TTCW Framework: 14 binary tests organized across four Torrance dimensions
  - Expert Evaluation Pipeline: Human experts administer tests with binary labels and justifications
  - LLM Assessment Pipeline: LLMs attempt to replicate expert evaluations using chain-of-thought reasoning
  - Data Collection: 48 stories (12 human-written, 36 LLM-generated) with 2,016 binary labels

- Critical path:
  1. Story selection and generation (New Yorker stories + LLM-generated stories)
  2. Expert recruitment and training on TTCW framework
  3. Binary test administration with justifications
  4. Agreement analysis (Fleiss Kappa for individual tests, Pearson correlation for aggregates)
  5. LLM assessment implementation and correlation analysis

- Design tradeoffs:
  - Expert vs crowd worker recruitment: Experts provide domain knowledge but increase costs
  - Binary vs Likert scale: Binary simplifies aggregation but may lose nuance
  - Human vs LLM assessment: Humans provide reliable evaluation but are expensive to scale

- Failure signatures:
  - Low Fleiss Kappa (<0.2) indicates test ambiguity or expert disagreement
  - Zero Pearson correlation indicates LLM assessment failure
  - Inconsistent expert justifications suggest unclear test criteria

- First 3 experiments:
  1. Run TTCW on 5 diverse short stories to test agreement levels and identify ambiguous tests
  2. Compare expert vs LLM assessment on same stories to establish baseline correlation
  3. Test TTCW generalization by evaluating poetry or screenplays to identify framework limitations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the specific phrasing and wording of the TTCW questions affect the agreement levels among expert annotators?
- Basis in paper: Explicit - The paper discusses moderate agreement among experts on individual TTCW tests (Fleiss Kappa 0.41) and strong agreement when aggregated (Pearson correlation 0.69).
- Why unresolved: The paper does not explore whether different wordings or phrasings of the questions could lead to higher agreement levels. It is unclear if the current wording is optimal for achieving consensus among experts.
- What evidence would resolve it: Conducting experiments with different phrasings of the TTCW questions and measuring the resulting agreement levels among experts would provide evidence on the impact of question wording.

### Open Question 2
- Question: Can the TTCW framework be effectively adapted to evaluate creative writing in languages other than English?
- Basis in paper: Inferred - The paper focuses on English-language short stories and does not discuss the applicability of TTCW to other languages.
- Why unresolved: The cultural and linguistic nuances of different languages may affect the relevance and effectiveness of the TTCW tests. It is unclear if the framework can be seamlessly translated and applied to creative writing in other languages.
- What evidence would resolve it: Applying the TTCW framework to creative writing in various languages and evaluating its effectiveness and relevance in each context would provide evidence on its adaptability.

### Open Question 3
- Question: What is the impact of different generation parameters (e.g., temperature, prompt engineering) on the creativity of LLM-generated stories as evaluated by TTCW?
- Basis in paper: Explicit - The paper mentions that variations in generation parameters could potentially enhance the originality of generated content.
- Why unresolved: The paper does not explore the relationship between specific generation parameters and the creativity of the resulting stories. It is unclear how adjusting these parameters affects the performance of LLMs on TTCW tests.
- What evidence would resolve it: Systematically varying generation parameters and evaluating the resulting stories using TTCW would provide insights into the impact of these parameters on the creativity of LLM-generated content.

## Limitations
- Expert recruitment criteria and verification process are not fully specified
- Exact prompting strategies for LLM story generation are not detailed
- TTCW framework's applicability to other creative domains remains untested

## Confidence

- **High confidence**: Core finding that LLM-generated stories fail TTCW tests at significantly higher rates than human-written stories (3-10x difference)
- **Medium confidence**: Conclusion that LLMs cannot reliably assess creativity due to zero correlation with expert judgments
- **Medium confidence**: Expert differentiation criteria based on narrative patterns and character development

## Next Checks

1. Replicate with expanded story corpus - Test TTCW on a larger and more diverse set of stories (including different genres and publication sources) to verify the robustness of the 3-10x performance gap between human and AI-generated content.

2. Implement inter-rater reliability study - Have a subset of stories evaluated by both the original experts and an independent panel of creative writing professionals to quantify test-retest reliability and potential expert bias.

3. Test TTCW generalization - Apply the TTCW framework to other creative domains (poetry, screenplays, flash fiction) to determine if the 14 tests are domain-specific or can serve as a general creativity assessment tool.