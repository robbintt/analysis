---
ver: rpa2
title: Seq2seq is All You Need for Coreference Resolution
arxiv_id: '2310.13774'
source_url: https://arxiv.org/abs/2310.13774
tags:
- token
- coreference
- action
- mention
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a sequence-to-sequence (seq2seq) approach to
  coreference resolution, where the task is framed as a standard text-to-text problem.
  The model is trained to generate a tagged sequence representation of the coreference
  annotation, using either full linearization (encoding all tokens and mentions) or
  partial linearization (encoding only mentions).
---

# Seq2seq is All You Need for Coreference Resolution

## Quick Facts
- arXiv ID: 2310.13774
- Source URL: https://arxiv.org/abs/2310.13774
- Reference count: 21
- Key outcome: A sequence-to-sequence approach achieves state-of-the-art results on coreference resolution benchmarks

## Executive Summary
This paper presents a simple yet effective approach to coreference resolution by framing it as a standard sequence-to-sequence problem. The method uses a pretrained transformer to map input documents to tagged sequences encoding coreference annotations, achieving strong performance without specialized architectures. With large models (3B-11B parameters), the approach outperforms or closely matches the best existing systems on standard benchmarks like OntoNotes, PreCo, and LitBank. The work demonstrates that standard seq2seq models can directly generate valid coreference annotations when provided with appropriate linearization formats.

## Method Summary
The approach treats coreference resolution as a text-to-text problem where a pretrained seq2seq transformer (T5 or T0) is finetuned to map input documents to linearized coreference annotations. The method explores two linearization strategies: full linearization (encoding all tokens and mentions) and partial linearization (encoding only mentions). Two action sequence types are tested: token action (generating tokens directly) and copy action (using a special symbol to advance through the source document). The model generates a sequence of actions that are postprocessed into coreference clusters, with constrained beam search ensuring valid output generation.

## Key Results
- Achieves 82.9 F1 on English OntoNotes with a 3B-parameter T0 model, surpassing ASP (82.3)
- Reaches 83.2 F1 with an 11B-parameter T0, outperforming CorefQA (83.1) and approaching the best known result (83.3)
- Demonstrates competitive performance on PreCo and LitBank datasets
- Shows that model size, supervision amount, and sequence representation choices are critical factors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Standard seq2seq models can directly generate valid coreference annotations without task-specific architectures
- Mechanism: The model treats the document as a source sequence and the coreference annotation as a target sequence, using a linear representation of mentions and clusters
- Core assumption: Language model pretraining on standard text-to-text tasks is sufficient for understanding coreference structure when the output format is designed appropriately
- Evidence anchors:
  - [abstract] "We finetune a pretrained seq2seq transformer to map an input document to a tagged sequence encoding the coreference annotation. Despite the extreme simplicity, our model outperforms or closely matches the best coreference systems in the literature"
  - [section] "We treat the raw document as a source sequence and the coreference annotation as a target sequence, then finetune a pretrained encoder-decoder model like T5 (Raffel et al., 2020) or T0 (Sanh et al., 2022) without any modification to the architecture."

### Mechanism 2
- Claim: Full linearization with copy action sequence achieves the best performance by eliminating alignment ambiguity
- Mechanism: The model generates a sequence that includes all tokens from the source document interleaved with mention tags and cluster identifiers, using a special copy symbol to advance through the source
- Core assumption: Alignment ambiguity is a major source of error, and explicit alignment through full linearization improves performance significantly
- Evidence anchors:
  - [abstract] "We consider an especially simple seq2seq approach that generates only tagged spans rather than the spans interleaved with the original text. Our analysis shows that the model size, the amount of supervision, and the choice of sequence representations are key factors in performance."
  - [section] "The goal is to express S as a sequence z ∈ V^T for some length T. A minimal formulation is to literally predict the integer triples, for instance z = str(S) where str is the string conversion in Python. However, while short, such a non-linguistic representation was found to perform poorly likely because it is not compatible with language model pretraining."

### Mechanism 3
- Claim: Model size and amount of supervision are key factors in achieving state-of-the-art performance
- Mechanism: Larger pretrained models with more parameters can better capture the complex patterns in coreference resolution, and more training data improves generalization
- Core assumption: The capacity of the model and the quality/quantity of training data directly impact performance on structured prediction tasks like coreference resolution
- Evidence anchors:
  - [abstract] "Our analysis shows that the model size, the amount of supervision, and the choice of sequence representations are key factors in performance."
  - [section] "We observe an improvement in performance as the model size increases. For models of the same size, both FLAN-T5 and T0 surpass the performance of the original T5 model."

## Foundational Learning

- Concept: Linearization of coreference annotations
  - Why needed here: The model needs to convert the structured coreference information into a linear sequence format that can be generated autoregressively
  - Quick check question: What are the two main linearization strategies proposed, and how do they differ in handling alignment ambiguity?

- Concept: Action sequences and their role in structured prediction
  - Why needed here: The decoder needs to predict specific actions that build up the coreference annotation rather than just copying tokens
  - Quick check question: What is the difference between token action and copy action, and why does copy action generally perform better?

- Concept: Constrained beam search for valid output generation
  - Why needed here: The model must generate syntactically and semantically valid coreference annotations that respect the constraints of the task
  - Quick check question: How does constrained decoding prevent the model from generating invalid mention spans or cluster assignments?

## Architecture Onboarding

- Component map:
  - Input document → Encoder → Hidden states
  - Decoder generates action sequence step-by-step
  - Postprocessing converts action sequence to coreference clusters
  - Evaluation against gold annotations

- Critical path:
  1. Input document → Encoder → Hidden states
  2. Decoder generates action sequence step-by-step
  3. Postprocessing converts action sequence to coreference clusters
  4. Evaluation against gold annotations

- Design tradeoffs:
  - Full linearization vs partial linearization: Full provides better alignment but longer sequences; partial is faster but requires alignment step
  - Token action vs copy action: Copy action is more efficient but requires token-level postprocessing
  - Model size vs computational cost: Larger models perform better but are more expensive to train

- Failure signatures:
  - Poor mention detection: Model generates incorrect span boundaries
  - Cluster merging errors: Model fails to distinguish between separate clusters
  - Alignment failures: For partial linearization, mentions are assigned to wrong positions
  - Length limitations: Performance degrades on very long documents

- First 3 experiments:
  1. Train a small model (T5-base) with full linearization and token action on OntoNotes to establish baseline performance
  2. Switch to copy action to see improvement in mention detection accuracy
  3. Try partial linearization with oracle alignment to measure the impact of the alignment step on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model perform on coreference resolution tasks with significantly longer documents or more complex nested coreference structures?
- Basis in paper: [inferred] The paper mentions the model's performance on various datasets but does not extensively explore its behavior on extremely long documents or complex nested structures
- Why unresolved: The paper focuses on datasets with moderate document lengths and coreference complexity, leaving the model's scalability and robustness to more challenging scenarios untested
- What evidence would resolve it: Experiments on datasets with longer documents (e.g., legal or scientific texts) and more intricate coreference structures (e.g., highly nested mentions) would provide insights into the model's limitations and generalization capabilities

### Open Question 2
- Question: How does the model's performance compare to task-specific models when trained on smaller datasets or in low-resource settings?
- Basis in paper: [explicit] The paper discusses the model's performance on LitBank, a small dataset, but does not directly compare it to task-specific models in low-resource scenarios
- Why unresolved: While the paper shows the model's effectiveness on moderately sized datasets, its performance relative to task-specific models in low-resource settings remains unclear
- What evidence would resolve it: Comparative experiments on smaller datasets or synthetic low-resource settings would clarify the model's strengths and weaknesses in such scenarios

### Open Question 3
- Question: What is the impact of different sequence representations (e.g., integer-free vs. integer-based) on the model's ability to generalize to new, unseen clusters?
- Basis in paper: [explicit] The paper introduces an integer-free representation and shows its effectiveness, but does not explore its generalization to unseen clusters in depth
- Why unresolved: The paper demonstrates the integer-free representation's performance but does not analyze how well it handles clusters that were not present during training
- What evidence would resolve it: Experiments involving training and testing on datasets with disjoint cluster distributions would reveal the integer-free representation's generalization capabilities

### Open Question 4
- Question: How does the model's performance scale with larger model sizes beyond 11B parameters?
- Basis in paper: [explicit] The paper mentions computational constraints preventing training with models larger than 11B parameters but does not explore the potential performance gains from even larger models
- Why unresolved: The paper's analysis is limited to models up to 11B parameters, leaving the question of whether further scaling would yield significant improvements unanswered
- What evidence would resolve it: Training and evaluating the model with even larger transformer architectures (e.g., 20B+ parameters) would determine the upper limits of performance gains through scaling

## Limitations

- Relies on extremely large pretrained models (11B parameters), making it computationally expensive
- Performance gains over specialized architectures are modest, raising questions about practical deployment
- May struggle with extremely long documents or languages with complex morphology
- Limited exploration of cross-lingual and low-resource scenarios

## Confidence

**High Confidence:**
- The seq2seq formulation works as a viable alternative to specialized architectures
- Larger model sizes consistently improve performance
- The choice between full and partial linearization meaningfully affects results
- The method achieves competitive performance on standard benchmarks

**Medium Confidence:**
- The computational efficiency compared to specialized models
- Generalization to out-of-domain data
- The scalability to extremely long documents (beyond tested lengths)
- Performance on languages other than English

**Low Confidence:**
- The method's robustness to noisy or imperfect annotations
- Long-term performance trends as models scale beyond 11B parameters
- The approach's effectiveness for document-level coreference with hundreds of entities

## Next Checks

1. **Scalability Validation**: Test the approach on documents significantly longer than the typical 500-1000 token range (e.g., 2000-3000 tokens) to identify sequence length limitations and measure performance degradation

2. **Cross-Lingual Generalization**: Apply the same approach to non-English coreference datasets (e.g., AnCora for Spanish, CoNLL 2012 for Chinese) using the same model sizes and training procedures

3. **Resource Efficiency Analysis**: Compare wall-clock training and inference times against specialized coreference models (like CorefQA) on equivalent hardware, including memory usage measurements to quantify practical deployment costs