---
ver: rpa2
title: Improving Speech Recognition for African American English With Audio Classification
arxiv_id: '2309.09996'
source_url: https://arxiv.org/abs/2309.09996
tags:
- data
- speech
- classifier
- disparity
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of reducing word error rate (WER)
  disparity in ASR systems between African American English (AAE) and Mainstream American
  English (MAE). The core idea is to train an audio classifier using a small amount
  of out-of-domain AAE data to identify AAE utterances in a large corpus of untranscribed
  short-form speech.
---

# Improving Speech Recognition for African American English With Audio Classification

## Quick Facts
- arXiv ID: 2309.09996
- Source URL: https://arxiv.org/abs/2309.09996
- Reference count: 0
- One-line primary result: 38.5% relative WER disparity reduction between AAE and MAE achieved through dialect-aware semi-supervised learning

## Executive Summary
This work addresses the problem of reducing word error rate (WER) disparity in ASR systems between African American English (AAE) and Mainstream American English (MAE). The core approach uses a small amount of out-of-domain AAE data to train an audio classifier that identifies AAE utterances in a large corpus of untranscribed short-form speech. By combining classifier outputs with coarse geographic information, the system selects AAE-specific data for semi-supervised learning at scale. Fine-tuning the ASR model on this selected data yields significant WER disparity reduction without degrading MAE quality.

## Method Summary
The method involves training an audio classifier on out-of-domain AAE/MAE data to detect AAE utterances, then using this classifier combined with coarse geographic information to select AAE-specific utterances from a large untranscribed corpus. A teacher model generates pseudo-labels for the selected utterances, which are used to fine-tune the ASR model with reduced learning rates to prevent MAE regression. The approach leverages streaming HAT ASR architecture and evaluates performance using standard WER metrics along with matched n-gram analysis to isolate acoustic improvements.

## Key Results
- 38.5% relative WER disparity reduction between AAE and MAE achieved through fine-tuning on AAE-classified data
- 48.7% relative WER disparity reduction on matched n-grams containing common English words spoken in AAE
- Maintained MAE quality while improving AAE recognition through careful learning rate scheduling during fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining dialect classifier scores with coarse geographic information allows more precise selection of AAE utterances than either method alone.
- Mechanism: The dialect classifier provides linguistic variety labels while geographic data acts as a coarse filter to bias toward regions where AAE is prevalent. Together, they improve precision in identifying AAE speech.
- Core assumption: Geographic location correlates with linguistic variety, and classifier thresholds can effectively segment AAE from non-AAE speech.
- Evidence anchors:
  - [abstract] "By combining the classifier output with coarse geographic information, we can select a subset of utterances from a large corpus of untranscribed short-form queries for semi-supervised learning at scale."
  - [section] "We use the dialect classifier to further filter the Top70 and Southern sets... Classifying the utterances to be AAE when the classifier score is ≥ 0.7 and MAE when < 0.4 gave us a precision and recall of 89% and 94.7% on SF Verified Test Set."
  - [corpus] Weak - neighbor papers show similar geographic-linguistic mapping approaches but no direct validation of this combined method.
- Break condition: If the geographic-location correlation weakens (e.g., due to demographic shifts or data sparsity), the combined method's precision would degrade.

### Mechanism 2
- Claim: Fine-tuning the ASR model on AAE-classified semi-supervised data reduces the WER disparity between AAE and MAE.
- Mechanism: The ASR model learns acoustic and linguistic patterns specific to AAE by exposure to semi-supervised AAE utterances, improving its ability to recognize AAE speech.
- Core assumption: The semi-supervised AAE data captures sufficient linguistic diversity to generalize to unseen AAE speech patterns.
- Evidence anchors:
  - [abstract] "Fine-tuning on this data results in a 38.5% relative word error rate disparity reduction between AAE and MAE without reducing MAE quality."
  - [section] "Using this AAE data, for our best model, we were able to reduce the MAE/AAE disparity to 15.4% from 25%, achieving a relative disparity reduction of 38.9%."
  - [corpus] Weak - neighbor papers validate WER improvements but do not confirm the specific semi-supervised pipeline.
- Break condition: If the semi-supervised data quality is low or unrepresentative, fine-tuning may not improve (and could worsen) ASR performance.

### Mechanism 3
- Claim: Using a matched n-gram evaluation isolates acoustic modeling improvements from lexical differences.
- Mechanism: By evaluating WER on utterances containing only common n-grams between AAE and MAE, the evaluation focuses on acoustic similarity rather than lexical or syntactic differences.
- Core assumption: Matched n-grams are acoustically comparable across dialects, allowing fair acoustic-only comparison.
- Evidence anchors:
  - [section] "To estimate the improvement due to gains on dialects alone, we used the matched n-gram test to compare the WER of the baseline model on AAE and non-AAE audio of the same content."
  - [section] "Table 8 shows that the baseline model has a low WER, as expected due to its construction with common bi- and tri-grams across language varieties. However, there is still a large quality disparity, with a WER of 1.2% for non-AAE audio and 2.1% for AAE audio."
  - [corpus] Weak - neighbor papers reference n-gram matching but do not validate this exact acoustic-isolation claim.
- Break condition: If the matched n-gram extraction fails to control for lexical variation, the evaluation conflates acoustic and lexical effects.

## Foundational Learning

- Concept: Dialect classification fundamentals (audio features → linguistic variety prediction)
  - Why needed here: The dialect classifier is the core mechanism for selecting AAE data; understanding its design and limitations is critical.
  - Quick check question: What pooling method and classifier depth were found optimal for the AAE/MAE binary task?

- Concept: Semi-supervised learning in ASR (teacher-student framework, pseudo-labeling)
  - Why needed here: The AAE data is teacher-labeled, so understanding how teacher models generate pseudo-labels and how fine-tuning uses them is essential.
  - Quick check question: How does reducing the fine-tuning learning rate help prevent MAE quality regression?

- Concept: Fairness evaluation in ASR (disparity metrics, matched n-gram analysis)
  - Why needed here: The paper's contribution is framed in terms of fairness; knowing how to measure and interpret disparity is key.
  - Quick check question: What is the formula for WER disparity, and how does matched n-gram WER differ from standard WER?

## Architecture Onboarding

- Component map:
  Audio classifier (foundation model + pooling + classifier head) -> Data selection pipeline (geographic filter + classifier score threshold) -> ASR model (HAT encoder + decoder, streaming) -> Teacher model (bidirectional conformer) for pseudo-labeling -> Evaluation suite (WER, disparity, matched n-gram)

- Critical path:
  1. Train audio classifier on out-of-domain AAE/MAE data
  2. Apply classifier + geography to select AAE utterances from large corpus
  3. Use teacher model to generate pseudo-labels for selected utterances
  4. Fine-tune ASR model on pseudo-labeled AAE data
  5. Evaluate disparity reduction on test sets

- Design tradeoffs:
  - Out-of-domain classifier training vs. domain adaptation complexity
  - Coarse geography vs. fine-grained demographic data (privacy vs. precision)
  - Semi-supervised learning vs. fully supervised (cost vs. quality)
  - Streaming ASR constraints vs. accuracy (latency vs. performance)

- Failure signatures:
  - Classifier precision/recall drops → poor AAE data selection
  - MAE WER increases after fine-tuning → overfitting or negative transfer
  - Disparity reduction stalls → insufficient AAE data diversity or poor teacher model quality

- First 3 experiments:
  1. Validate classifier precision/recall on held-out SF data at chosen threshold (0.7/0.4)
  2. Run fine-tuning with varying learning rates (7.5 → 5.0 → 2.5) and monitor MAE WER
  3. Compare geographic-only vs. classifier+geography data selection on disparity reduction

## Open Questions the Paper Calls Out
The paper explicitly mentions evaluating on test sets not selected by the classifier to study how well this approach generalizes to diverse AAE varieties. The authors note this as a direction for future research, indicating uncertainty about the classifier's performance across the full range of AAE dialects beyond the specific varieties used in training and evaluation.

## Limitations
- Geographic correlation with dialect is treated as stable but may not hold across time or data subsets
- Classifier threshold optimization for WER disparity reduction is not established
- Generalizability of the approach to other language varieties beyond AAE and MAE is unknown

## Confidence
- **High confidence**: The core pipeline of using dialect classification + geographic filtering to select AAE data for fine-tuning is well-specified and reproducible. The 38.5% relative WER disparity reduction claim is directly supported by the described methodology and evaluation setup.
- **Medium confidence**: The matched n-gram WER disparity reduction (48.7%) isolates acoustic improvements, but the assumption that matched n-grams are acoustically comparable across dialects needs empirical validation. The precision/recall metrics (89%/94.7%) are reported but not independently verified.
- **Low confidence**: The generalizability of the geographic-linguistic correlation beyond the Top70 and Southern regions is untested. The impact of classifier overfitting on downstream WER disparity reduction is not quantified.

## Next Checks
1. Validate classifier precision/recall at multiple thresholds (0.6, 0.7, 0.8) on held-out SF data and measure corresponding WER disparity reduction to confirm the optimal threshold.

2. Run an ablation study comparing data selection using only geography vs. classifier+geography to quantify the added value of the dialect classifier in disparity reduction.

3. Cross-validate matched n-grams across AAE and MAE speakers using phonetic analysis to confirm acoustic comparability and rule out lexical confounds.