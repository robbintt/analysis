---
ver: rpa2
title: Cross Feature Selection to Eliminate Spurious Interactions and Single Feature
  Dominance Explainable Boosting Machines
arxiv_id: '2307.08485'
source_url: https://arxiv.org/abs/2307.08485
tags:
- feature
- features
- selection
- occurrence
- interpretability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses spurious interactions and single-feature dominance
  in Explainable Boosting Machines (EBMs). It proposes a multi-step cross-feature
  selection approach using various pre-selectors (SHAP, XGBoost, Random Forest, etc.)
  to eliminate redundant features and improve model interpretability.
---

# Cross Feature Selection to Eliminate Spurious Interactions and Single Feature Dominance Explainable Boosting Machines

## Quick Facts
- arXiv ID: 2307.08485
- Source URL: https://arxiv.org/abs/2307.08485
- Reference count: 0
- Single-line primary result: Cross-feature selection approach using ensemble methods improves EBM interpretability and performance by reducing spurious interactions and single-feature dominance.

## Executive Summary
This paper addresses two key interpretability challenges in Explainable Boosting Machines (EBMs): spurious feature interactions and single-feature dominance. The authors propose a multi-step cross-feature selection approach that uses various pre-selectors (SHAP, XGBoost, Random Forest, etc.) to eliminate redundant features and improve model interpretability. Tested on three benchmark datasets, the method outperforms vanilla EBM models in both predictive performance and interpretability by reducing feature dominance and spurious interactions.

## Method Summary
The approach involves a multi-stage feature selection pipeline where multiple feature selectors are applied to reduce the feature space before EBM training. The process includes preprocessing (imputation, encoding, scaling), applying diverse feature selectors (SHAP, AdaBoost, XGBoost, Random Forest, Correlation, VIF, Variance Threshold, Permutation Importance, Boruta), aggregating selected features using ensemble methods, and training an EBM model with configured thresholds for feature importance and interaction filtering. The method was tested on three benchmark datasets from Kaggle with 305-34 attributes and 20,000-82,000 instances.

## Key Results
- SHAP, boosting, correlation, variance thresholds, and ensemble pre-selectors improved F1 scores and reduced computational time while enhancing interpretability
- The approach effectively reduced feature dominance and spurious interactions compared to vanilla EBM models
- Ensemble feature selection identified features that were both relevant and complementary, reducing the risk of spurious interactions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-feature selection eliminates spurious interactions by removing redundant or noisy features before they can interact with important features in the EBM.
- Mechanism: A multi-stage pipeline first applies feature selectors to reduce feature space. By filtering out redundant or low-importance features early, the EBM is forced to interact only with meaningful features, preventing false interactions from noise.
- Core assumption: Spurious interactions arise because EBM's interaction detection will form relationships with redundant features that have no/low individual importance but somehow combine to high interaction scores.
- Evidence anchors:
  - [abstract] "Our approach involves a multi-step feature selection procedure that selects a set of candidate features, ensemble features and then benchmark the same using the EBM model."
  - [section] "A top-level overview clearly shows that by using a cross feature selector prior to EBM's both feature dominance and spurious Interactions are largely eliminated."
- Break condition: If redundant features are not identified accurately by the pre-selectors, or if the EBM's interaction detection still forms interactions with residual noise, spurious interactions will persist.

### Mechanism 2
- Claim: Ensemble feature selection improves interpretability and predictive performance by aggregating diverse feature importance signals.
- Mechanism: Multiple feature selection methods each rank features differently. By pooling or aggregating the top features from these diverse methods, the ensemble selects features that are both relevant and complementary, reducing the risk of spurious interactions and single-feature dominance.
- Core assumption: Different feature selectors capture different aspects of feature importance, and their combination yields a more robust and stable feature set than any single method.
- Evidence anchors:
  - [abstract] "By combining diverse feature selection methods, the ensemble can identify a set of features that are both relevant and complementary, reducing the risk of spurious interactions and single-feature dominance."
  - [section] "Overall SHAP, Boosting, Correlation, Variance Thresholds and ensemble preselectors is found to be a good choice a preselector for EBM's."
- Break condition: If the ensemble aggregation method fails to balance the diverse signals (e.g., if one method dominates), or if the underlying feature selectors are highly correlated in their selections, the ensemble may not provide added benefit.

### Mechanism 3
- Claim: Altered EBM configuration with feature importance thresholds and interaction filtering reduces single-feature dominance and improves interpretability.
- Mechanism: The EBM is configured to first select features with importance above a threshold (e.g., 0.05). Then, interactions are only retained if the individual main feature importance ranks better than the interaction itself. This ensures that interactions are meaningful and not dominated by a single highly correlated feature.
- Core assumption: Single-feature dominance occurs because one highly correlated feature appears in multiple interactions, making it difficult to interpret the contribution of other features. By setting thresholds and filtering interactions, the model focuses on meaningful relationships.
- Evidence anchors:
  - [abstract] "we identify two challenges with use of EBM's i.e. spurious feature interactions with redundant variables and Single feature dominance in interactions by using a multi-step cross feature selection to extract meaningful features to avoid interactions with redundant features and prevent a single feature dominance."
- Break condition: If the threshold is set too high, important features may be excluded; if too low, noise may remain. Also, if the interaction filtering logic is too strict, potentially meaningful interactions may be lost.

## Foundational Learning

- Concept: Feature selection methods (filter, wrapper, embedded)
  - Why needed here: The paper uses a variety of feature selection methods (SHAP, XGBoost, Random Forest, etc.) as pre-selectors. Understanding the differences between filter, wrapper, and embedded methods is crucial to grasp why the ensemble approach works.
  - Quick check question: What is the main difference between filter and wrapper feature selection methods, and why might an ensemble of both be beneficial?

- Concept: Explainable Boosting Machines (EBMs) and their interaction detection
  - Why needed here: The paper focuses on addressing spurious interactions and single-feature dominance in EBMs. Understanding how EBMs detect interactions and why these issues arise is essential to understanding the proposed solution.
  - Quick check question: How does EBM's automatic interaction detection work, and what are the potential drawbacks of this approach that the paper aims to address?

- Concept: Ensemble learning and aggregation strategies
  - Why needed here: The paper proposes using ensemble feature selection and altered EBM configuration. Understanding how ensemble learning works and different aggregation strategies (e.g., pooling top N features, selecting features that appear in at least k subsets) is crucial to understanding the proposed approach.
  - Quick check question: What are the advantages of using an ensemble of feature selection methods compared to a single method, and how does the aggregation strategy impact the final feature set?

## Architecture Onboarding

- Component map: Input Data → Preprocessing → Train-Test Split → Feature Selection Stage 1 (multiple selectors) → Feature Selection Stage 2 (ensemble aggregation) → EBM Model Configuration → Model Training and Evaluation
- Critical path: Preprocessing → Feature Selection (Stage 1) → Feature Selection (Stage 2) → EBM Configuration → Model Training and Evaluation
- Design tradeoffs: Using multiple feature selectors increases computational cost but improves robustness and reduces spurious interactions; setting thresholds and filtering criteria improves interpretability but may exclude some potentially important features
- Failure signatures: High computational time due to multiple feature selectors and ensemble aggregation; poor model performance if feature selectors are not diverse or if thresholds are set too high/low; persistent spurious interactions or single-feature dominance if pre-selectors fail to identify redundant features accurately
- First 3 experiments: 1) Compare EBM with and without cross-feature selection on benchmark dataset measuring F1 score, accuracy, computational time, feature dominance, and spurious interactions; 2) Evaluate impact of different ensemble aggregation strategies on model performance and interpretability; 3) Test sensitivity of EBM configuration on model performance and interpretability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different pre-feature selectors compare in terms of reducing spurious interactions and single-feature dominance across diverse datasets?
- Basis in paper: [explicit] The paper compares various pre-feature selectors in terms of performance, feature dominance, and spurious interactions.
- Why unresolved: While the paper shows that certain pre-feature selectors outperform others, it does not provide a detailed analysis of why specific selectors are more effective in reducing spurious interactions and single-feature dominance.
- What evidence would resolve it: A comprehensive study comparing the performance of different pre-feature selectors across a wide range of datasets, analyzing their impact on reducing spurious interactions and single-feature dominance.

### Open Question 2
- Question: What are the optimal parameters and configurations for the proposed multi-step cross-feature selection approach?
- Basis in paper: [inferred] The paper mentions using specific parameters for each pre-feature selector and adjusting the number of features selected, but it does not provide a systematic analysis of the optimal parameters and configurations.
- Why unresolved: The paper does not explore the sensitivity of the proposed approach to different parameter settings and configurations, which could impact its performance and generalizability.
- What evidence would resolve it: A thorough investigation of the impact of different parameter settings and configurations on the performance of the proposed approach, using techniques like grid search or Bayesian optimization.

### Open Question 3
- Question: How does the proposed approach compare to other state-of-the-art explainable boosting algorithms, such as Gami-Net?
- Basis in paper: [explicit] The paper mentions Gami-Net as a potential direction for future work, suggesting that it could be explored as an alternative to EBM.
- Why unresolved: The paper does not provide a direct comparison between the proposed approach and other state-of-the-art explainable boosting algorithms, limiting its ability to assess its relative performance and advantages.
- What evidence would resolve it: A comprehensive evaluation of the proposed approach against other state-of-the-art explainable boosting algorithms, using the same datasets and evaluation metrics.

## Limitations
- Lacks explicit details on parameter settings for individual feature selectors, making exact reproduction challenging
- No information provided about EBM implementation version or specific configuration parameters
- Effectiveness may vary depending on dataset characteristics and domain context

## Confidence

- **High Confidence**: The core mechanism of using ensemble feature selection to reduce spurious interactions is theoretically sound and supported by general ML principles
- **Medium Confidence**: The claim that the approach outperforms vanilla EBM is supported by benchmark results, but generalizability to other datasets and domains needs verification
- **Low Confidence**: The specific impact of different ensemble aggregation strategies on performance is not thoroughly explored

## Next Checks

1. Conduct ablation studies to isolate the contribution of each feature selector to overall performance improvement
2. Test the approach on additional diverse datasets (different domains, sizes, and feature distributions) to assess generalizability
3. Implement sensitivity analysis for threshold parameters (feature importance thresholds, interaction filtering criteria) to determine optimal settings for different scenarios