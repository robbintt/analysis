---
ver: rpa2
title: 'That was the last straw, we need more: Are Translation Systems Sensitive to
  Disambiguating Context?'
arxiv_id: '2310.14610'
source_url: https://arxiv.org/abs/2310.14610
tags:
- translation
- literal
- figurative
- ambiguous
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of translating semantically ambiguous
  idioms in machine translation (MT) systems. The authors create a new dataset called
  TIDE, consisting of 512 example triples of English sentences containing idioms with
  disambiguating context.
---

# That was the last straw, we need more: Are Translation Systems Sensitive to Disambiguating Context?

## Quick Facts
- arXiv ID: 2310.14610
- Source URL: https://arxiv.org/abs/2310.14610
- Reference count: 17
- MT-specific models consistently translate English idioms literally, even with disambiguating context; LMs show better context-awareness

## Executive Summary
This paper investigates how machine translation systems handle semantically ambiguous idioms by introducing the TIDE dataset of 512 English idiom triples with disambiguating context. The authors compare traditional MT-specific models with language models (LMs) on their ability to translate idioms both literally and figuratively based on context. While MT-specific models consistently produce literal translations regardless of context, LMs demonstrate significantly better sensitivity to disambiguating context, though this sensitivity decreases for low-resource target languages. The findings suggest that LMs, trained on next-token prediction rather than explicit translation supervision, may serve as stronger backbones for context-aware translation systems.

## Method Summary
The study constructs the TIDE dataset using GPT-4 to generate English sentences containing idioms in three forms: ambiguous subsentences, figurative sentences, and literal sentences. Human annotators then filter and validate these triples to ensure quality. The authors evaluate two MT-specific models (NLLB and Opus MT) and two LMs (ChatGPT and PaLM 2) on translating these idiom triples, using both automatic metrics (USW, NM, chrP, BERTScore-P) and human evaluation to assess translation quality and context sensitivity.

## Key Results
- MT-specific models consistently translate English idioms literally, even when context indicates a figurative meaning
- LMs demonstrate roughly balanced preferences between literal and figurative translations, showing better context awareness
- LM sensitivity to disambiguating context declines significantly for low-resource target languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 is effective at generating contextually disambiguated idiomatic triples because it leverages learned semantic patterns and contextual embeddings to understand the dual meanings of idioms.
- Mechanism: GPT-4's training on diverse linguistic data enables it to generate sentences where idioms can be used both figuratively and literally, by understanding the context that disambiguates the intended meaning.
- Core assumption: GPT-4 has internalized sufficient idiomatic expressions and their usage patterns to produce contextually appropriate sentences.
- Evidence anchors:
  - [abstract]: The paper states that GPT-4 is used to draft the triples, indicating its role in generating contextually rich idiomatic expressions.
  - [section]: The creation process leverages GPT-4's generative power to efficiently produce diverse and high-quality text.
  - [corpus]: The corpus signals show related work on idiom translation, supporting the assumption that GPT-4 has learned from idiomatic data.
- Break condition: If GPT-4's training data lacks sufficient idiomatic expressions or if it fails to understand the cultural nuances of idioms, it may produce ambiguous or contextually incorrect sentences.

### Mechanism 2
- Claim: Human annotation is crucial for ensuring the quality and correctness of the generated idiomatic triples.
- Mechanism: Human annotators filter out invalid triples by labeling sentences as figurative, literal, or ambiguous, ensuring that the context correctly disambiguates the idiom's meaning.
- Core assumption: Human judgment is necessary to identify nuanced uses of idioms that may not be apparent to automated systems.
- Evidence anchors:
  - [abstract]: The paper describes involving human annotators to filter out invalid triples, highlighting the importance of human oversight.
  - [section]: The annotation process involves crowdworkers who label each sentence, indicating the reliance on human evaluation.
  - [corpus]: The corpus does not provide direct evidence of human annotation, but the related work suggests the complexity of idiomatic language.
- Break condition: If human annotators are inconsistent or if there is a lack of consensus on the interpretation of idioms, the quality of the dataset may be compromised.

### Mechanism 3
- Claim: LMs demonstrate greater context-awareness than MT-specific models in handling idiomatic expressions.
- Mechanism: LMs, trained on next-token prediction without explicit supervision for translation, can adapt to disambiguating contexts more effectively due to their broader understanding of language patterns.
- Core assumption: The training of LMs on diverse and extensive text corpora enables them to capture contextual nuances better than models trained on parallel data.
- Evidence anchors:
  - [abstract]: The paper finds that LMs are far more context-aware, although there remain disparities across target languages.
  - [section]: Results show that LMs produce less literal translations and are more sensitive to disambiguating context compared to MT-specific models.
  - [corpus]: The corpus signals indicate research on the capabilities of LMs in translation, supporting the assumption of their contextual understanding.
- Break condition: If the training data for LMs is biased or lacks representation of certain idiomatic expressions, their performance may degrade, especially for low-resource languages.

## Foundational Learning

- Concept: Semantic ambiguity in language
  - Why needed here: Understanding how idioms can have multiple meanings is crucial for disambiguating them in translation.
  - Quick check question: Can you explain why the idiom "break the ice" can be interpreted both figuratively and literally?

- Concept: Context-aware translation
  - Why needed here: Evaluating how translation systems use surrounding context to resolve semantic ambiguities is central to this study.
  - Quick check question: How does the addition of context change the translation of an ambiguous idiom?

- Concept: Idiomatic expressions
  - Why needed here: Idioms are a key focus of the study, requiring knowledge of their figurative and literal meanings.
  - Quick check question: What makes idiomatic expressions challenging for machine translation systems?

## Architecture Onboarding

- Component map: GPT-4 -> Human annotators -> TIDE dataset -> MT-specific models and LMs -> Evaluation metrics
- Critical path: GPT-4 generates idiomatic triples → Human annotators filter/validate triples → Models translate triples → Evaluation of translation quality and context sensitivity
- Design tradeoffs: Using GPT-4 speeds up the generation process but requires human oversight to ensure quality. The choice of MT-specific models vs. LMs involves balancing dedicated training with broader contextual understanding.
- Failure signatures: Poor quality triples due to GPT-4's limitations, inconsistent human annotations, or translation models failing to handle idiomatic expressions correctly.
- First 3 experiments:
  1. Test GPT-4's ability to generate idiomatic triples with varying levels of complexity.
  2. Evaluate human annotators' consistency in labeling idiomatic sentences.
  3. Compare translation quality of MT-specific models and LMs on a small subset of idiomatic expressions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do translation systems handle semantic ambiguity in source languages other than English, particularly for languages with more complex or diverse idiomatic expressions?
- Basis in paper: [inferred] The paper focuses on English idioms and their translation to other languages, but does not explore ambiguity in non-English source languages.
- Why unresolved: The study is limited to English as the source language due to the availability of English idiom collections, leaving the behavior of translation systems for other source languages unexplored.
- What evidence would resolve it: A comprehensive study evaluating translation systems on idioms and ambiguous expressions in multiple source languages, including low-resource languages, would provide insights into how these systems handle semantic ambiguity beyond English.

### Open Question 2
- Question: What are the underlying reasons for the observed differences in translation quality between figurative and literal sentences, and how can translation systems be improved to handle figurative language more effectively?
- Basis in paper: [explicit] The paper finds that translation systems perform better on literal translations compared to figurative ones, with a more pronounced gap for MT-specific models compared to LMs.
- Why unresolved: While the paper identifies the performance disparity, it does not delve into the root causes of this difference or propose specific improvements for handling figurative language in translation systems.
- What evidence would resolve it: Detailed analysis of the internal workings of translation models when processing figurative language, along with experiments testing various techniques to improve figurative translation quality, would help uncover the reasons behind the performance gap and potential solutions.

### Open Question 3
- Question: How does the sensitivity of translation systems to disambiguating context vary across different types of idioms and figurative expressions, and what factors influence this sensitivity?
- Basis in paper: [inferred] The paper evaluates sensitivity to context for a specific set of idioms, but does not explore how this sensitivity varies across different types of figurative language or the factors that affect it.
- Why unresolved: The study focuses on a fixed set of idioms without considering the diversity of figurative expressions and the potential impact of various factors on the sensitivity of translation systems to disambiguating context.
- What evidence would resolve it: A broader evaluation of translation systems on a diverse range of figurative expressions, including metaphors, similes, and other forms of non-literal language, along with analysis of the factors influencing sensitivity to context (e.g., idiom frequency, cultural relevance), would provide a more comprehensive understanding of how translation systems handle ambiguity in different scenarios.

## Limitations

- Dataset construction relies heavily on GPT-4's capabilities, potentially introducing systematic biases
- Human annotation quality depends on annotator expertise and consistency in interpreting idiomatic meanings
- Study focuses only on English source language, limiting generalizability to other languages

## Confidence

- Confidence is Medium for the finding that MT-specific models consistently produce literal translations of idioms regardless of context, as this is demonstrated across multiple model comparisons but with a relatively small dataset (512 triples).
- Confidence is Medium-Low for the claim that LMs show balanced preferences between literal and figurative translations, as this observation is based on comparisons with limited target languages and may not generalize across diverse linguistic contexts.

## Next Checks

1. Test GPT-4's idiom generation capabilities on a diverse set of idioms beyond the 12 selected for TIDE, evaluating consistency and quality of generated triples.

2. Conduct human evaluation studies with multiple annotator groups to measure inter-annotator agreement and identify potential sources of interpretation variance in idiom labeling.

3. Expand evaluation to additional target languages, particularly focusing on low-resource languages identified as problematic, to better understand the limitations of LMs in context-aware translation.