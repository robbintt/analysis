---
ver: rpa2
title: In Defense of Softmax Parametrization for Calibrated and Consistent Learning
  to Defer
arxiv_id: '2311.01106'
source_url: https://arxiv.org/abs/2311.01106
tags:
- probability
- estimator
- loss
- softmax
- surrogate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning to defer in classification,
  where a model can choose to defer its prediction to an expert. Previous softmax-based
  methods suffer from unbounded probability estimates, leading to miscalibration.
---

# In Defense of Softmax Parametrization for Calibrated and Consistent Learning to Defer

## Quick Facts
- **arXiv ID**: 2311.01106
- **Source URL**: https://arxiv.org/abs/2311.01106
- **Reference count**: 40
- **Primary result**: Shows that softmax parameterization itself does not cause unbounded probability estimates in learning to defer; proposes an asymmetric softmax-based surrogate loss that is both calibrated and consistent

## Executive Summary
This paper addresses a critical issue in learning to defer (L2D) systems where previous softmax-based methods produced unbounded probability estimates, leading to miscalibration. The authors demonstrate that this problem stems not from softmax parameterization itself, but from the symmetric structure of the loss functions used. They propose a novel asymmetric softmax-based surrogate loss that maintains consistency while ensuring bounded probability estimates. Extensive experiments on benchmark datasets with both synthetic and real-world experts demonstrate significant improvements in calibration and deferral performance compared to existing methods.

## Method Summary
The authors propose an asymmetric softmax-based surrogate loss for learning to defer that addresses the unbounded probability estimation problem in prior work. The method modifies the standard softmax function to treat the K+1-th dimension (representing deferral to expert) differently, creating an asymmetric parameterization that maps directly to the target range of class probabilities and expert accuracy. This is combined with a consistent surrogate loss framework that preserves statistical consistency while ensuring bounded estimates. The approach can be viewed as a special case of general consistent surrogate formulations using asymmetric multi-class losses.

## Key Results
- The asymmetric softmax loss produces bounded probability estimates in ∆K × [0,1] range, unlike symmetric losses which become unbounded as expert accuracy approaches 1
- Experiments show improved Expected Calibration Error (ECE) of expert accuracy estimates compared to baseline softmax methods
- The method achieves competitive or superior performance in terms of misclassification error and coverage on CIFAR-100, CIFAR-10H, and HateSpeech datasets
- The proposed surrogate maintains statistical consistency while providing better calibration than One-vs-All (OvA) approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Symmetric loss functions cause unbounded probability estimates in learning to defer
- **Mechanism**: When the surrogate loss ϕ is symmetric (Pϕ(u) = ϕ(Pu) for any permutation matrix P), the induced probability estimator cannot distinguish between class probabilities and expert accuracy, leading to unboundedness when expert accuracy approaches 1
- **Core assumption**: The probability estimator needs to map from RK+1 to ∆K × [0,1] but symmetric losses treat this as a symmetric simplex
- **Evidence anchors**:
  - [abstract]: "we first show that the cause of the miscalibrated and unbounded estimator in prior literature is due to the symmetric nature of the surrogate losses used and not due to softmax"
  - [section 3]: "Theorem 1. (Impossibility of non-trivial bounded probability estimator with symmetric losses)"
  - [corpus]: Weak - the corpus doesn't discuss symmetric vs asymmetric losses in detail

### Mechanism 2
- **Claim**: Asymmetric softmax parameterization enables bounded and consistent probability estimation
- **Mechanism**: By modifying the standard softmax to treat the K+1-th dimension differently (Definition 2), the function maps directly to the target range ∆K × [0,1] while preserving the maximum-finding property needed for optimal deferral decisions
- **Core assumption**: The asymmetry in the softmax function can be exploited to separate the treatment of class probabilities and expert accuracy
- **Evidence anchors**:
  - [abstract]: "We then propose a novel statistically consistent asymmetric softmax-based surrogate loss that can produce valid estimates without the issue of unboundedness"
  - [section 4.1]: "Definition 2. (Asymmetric softmax parameterization)" and "Proposition 1. (Properties of ˜ψ)"
  - [corpus]: Moderate - related work discusses softmax gating but not this specific asymmetric modification

### Mechanism 3
- **Claim**: The proposed method connects to general consistent surrogate framework through asymmetric multi-class losses
- **Mechanism**: The asymmetric softmax loss can be expressed as a special case of the general consistent surrogate formulation (4) using novel asymmetric multi-class losses, showing that asymmetry is the key to bounded estimation
- **Core assumption**: The general consistent surrogate framework can be extended to include asymmetric losses
- **Evidence anchors**:
  - [abstract]: "Experiments on datasets with both synthetic experts and real-world experts are conducted to demonstrate the usefulness of our method"
  - [section 4.2]: "Corollary 1. We can get the consistent surrogates L ˜ψ (9) and LOvA (6) by setting ϕ in the consistent loss formulation (4) to specific consistent and asymmetric multi-class losses"
  - [corpus]: Weak - related work focuses on softmax gating but not the connection to asymmetric losses

## Foundational Learning

- **Concept**: Learning to defer framework
  - Why needed here: The entire paper addresses a specific problem in machine learning where models can choose to defer to experts
  - Quick check question: What is the 0-1-deferral loss and how does it differ from standard classification loss?

- **Concept**: Consistent surrogate losses
  - Why needed here: The paper builds on the idea that replacing discontinuous losses with continuous surrogates enables tractable optimization while preserving statistical consistency
  - Quick check question: What does it mean for a surrogate loss to be "classification-calibrated" and why is this important for L2D?

- **Concept**: Probability calibration and estimation
  - Why needed here: The core contribution addresses the miscalibration problem in existing L2D methods by providing a bounded probability estimator
  - Quick check question: Why is the range ∆K × [0,1] important for L2D probability estimation and what goes wrong when estimates fall outside this range?

## Architecture Onboarding

- **Component map**: Data → Scoring function g → Asymmetric softmax ˜ψ → Probability estimates → Deferral decision via φ → Expert or classifier output
- **Critical path**: Data → Scoring function g → Asymmetric softmax ˜ψ → Probability estimates → Deferral decision via φ → Expert or classifier output
- **Design tradeoffs**: The asymmetric softmax sacrifices the symmetric treatment of classes for boundedness and calibration; the method trades some theoretical simplicity for practical calibration improvements
- **Failure signatures**: Unbounded probability estimates, poor deferral coverage (either deferring too much or too little), miscalibration of expert accuracy estimates
- **First 3 experiments**:
  1. Test the asymmetric softmax on synthetic data with known expert accuracy to verify boundedness and calibration
  2. Compare coverage and ECE metrics against baseline symmetric softmax method on CIFAR-100 with synthetic experts
  3. Evaluate performance on real-world expert data (CIFAR10H, HateSpeech) to test generalization to realistic scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the unbounded probability estimation issue in softmax-based L2D methods be completely resolved by using alternative softmax parameterizations?
- **Basis in paper**: The authors argue that the unboundedness is not due to softmax parameterization itself but rather the symmetric structure of the surrogate losses. They propose an asymmetric softmax-based surrogate loss that induces a bounded probability estimator.
- **Why unresolved**: While the authors provide a solution using asymmetric softmax, it remains unclear if this is the only way to resolve the unboundedness issue or if other softmax parameterizations could also work.
- **What evidence would resolve it**: Further research exploring different softmax parameterizations and their effects on boundedness in L2D would help determine if asymmetric softmax is the only viable solution.

### Open Question 2
- **Question**: How does the proposed asymmetric softmax-based method compare to the OvA-based method in terms of performance and calibration across different datasets and expert scenarios?
- **Basis in paper**: The authors compare their proposed method with the OvA-based method and show that their method outperforms or is comparable to the OvA-based method in terms of classification accuracy, coverage, and ECE of expert accuracy estimates.
- **Why unresolved**: The comparison is limited to a few benchmark datasets and specific expert scenarios. It is unclear how the methods would perform in other settings or with different types of experts.
- **What evidence would resolve it**: Extensive experiments on a wider range of datasets and expert scenarios would provide a more comprehensive comparison of the two methods.

### Open Question 3
- **Question**: Can the proposed consistent surrogate loss be extended to handle multiple experts with different levels of expertise?
- **Basis in paper**: The authors mention that they provide a consistent multi-expert extension of their proposed surrogate in Appendix H, but they do not discuss it in detail in the main text.
- **Why unresolved**: The extension to multiple experts is only briefly mentioned, and it is unclear how the method would perform in practice with multiple experts of varying expertise levels.
- **What evidence would resolve it**: Further research and experiments on the multi-expert extension would help determine its effectiveness and potential limitations in handling multiple experts with different expertise levels.

## Limitations

- The theoretical guarantees depend on the assumption that expert accuracy is strictly less than 1, which may not hold for high-quality experts in practice
- The asymmetric softmax parameterization may be less intuitive and harder to optimize compared to standard symmetric approaches
- Performance in highly imbalanced datasets or with multiple experts remains unexplored

## Confidence

- **High Confidence**: The identification of symmetric loss functions as the root cause of unbounded probability estimates in L2D is well-supported by theoretical analysis and aligns with established principles in consistent surrogate theory
- **Medium Confidence**: The effectiveness of the proposed asymmetric softmax loss in improving calibration and performance is supported by experiments, but the sample size of datasets and comparison with more recent L2D methods is limited
- **Low Confidence**: The claim that the asymmetric softmax loss can be expressed as a special case of the general consistent surrogate formulation using asymmetric multi-class losses requires further validation, as the connection is not fully detailed in the paper

## Next Checks

1. **Theoretical Validation**: Rigorously verify the consistency of the proposed asymmetric softmax loss by proving that it satisfies the conditions for a classification-calibrated surrogate loss
2. **Empirical Validation**: Conduct extensive experiments on additional datasets, including highly imbalanced datasets and scenarios with multiple experts, to assess the method's robustness and generalization
3. **Comparison Validation**: Perform a comprehensive comparison with state-of-the-art L2D methods, including those based on recent advancements in the field, to establish the relative performance and advantages of the proposed approach