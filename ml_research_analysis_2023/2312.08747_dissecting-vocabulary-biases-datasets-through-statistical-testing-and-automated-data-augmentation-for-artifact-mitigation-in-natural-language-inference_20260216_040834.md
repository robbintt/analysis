---
ver: rpa2
title: Dissecting vocabulary biases datasets through statistical testing and automated
  data augmentation for artifact mitigation in Natural Language Inference
arxiv_id: '2312.08747'
source_url: https://arxiv.org/abs/2312.08747
tags:
- dataset
- language
- arxiv
- word
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates dataset biases in the SNLI natural language
  inference dataset, identifying vocabulary distribution in hypotheses as a key source
  of bias. The authors develop a statistical testing procedure to quantify this association
  between word choice and entailment labels.
---

# Dissecting vocabulary biases datasets through statistical testing and automated data augmentation for artifact mitigation in Natural Language Inference

## Quick Facts
- arXiv ID: 2312.08747
- Source URL: https://arxiv.org/abs/2312.08747
- Authors: 
- Reference count: 5
- Key outcome: Word replacement using word2vec similarity reduces hypothesis-only model accuracy by 1.14%, while word synonym augmentation from PPDB improves overall model accuracy by 0.66%.

## Executive Summary
This work investigates dataset biases in the SNLI natural language inference dataset, identifying vocabulary distribution in hypotheses as a key source of bias. The authors develop a statistical testing procedure to quantify this association between word choice and entailment labels. They then propose several automated data augmentation techniques at character and word levels to mitigate these biases. Fine-tuning ELECTRA on augmented data shows that word replacement using word2vec similarity reduces hypothesis-only model accuracy by 1.14%, while word synonym augmentation from PPDB improves overall model accuracy by 0.66%.

## Method Summary
The study uses χ² goodness-of-fit tests on POS-tagged nouns and verbs from SNLI hypotheses to identify vocabulary-label associations. Data augmentation is implemented using the nlpaug library with techniques including word2vec similarity replacement, WordNet and PPDB synonyms, and tf-idf sampling. ELECTRA-small is fine-tuned on the SNLI dataset with approximately 12,800 steps and batch size 256, evaluating both hypothesis-only and complete model performance.

## Key Results
- Statistical testing reveals significant associations between specific words and entailment labels in SNLI
- Word2vec-based replacement reduces hypothesis-only model accuracy by 1.14%
- PPDB synonym augmentation improves complete model accuracy by 0.66%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Statistical testing can reveal vocabulary bias in NLI datasets
- Mechanism: By performing χ² goodness-of-fit tests on POS-tagged nouns and verbs extracted from hypotheses, the study shows that certain words (e.g., "people", "wearing", "are") are statistically associated with specific labels (entailment, neutral, contradiction), indicating non-random vocabulary distributions.
- Core assumption: Vocabulary choices in hypotheses are not independent of their labels, implying annotator bias during dataset creation.
- Evidence anchors:
  - [abstract] "Through the utilization of a novel statistical testing procedure, we discover a significant association between vocabulary distribution and text entailment classes"
  - [section 4.2] χ² tests show extreme p-values (e.g., 1.0e-inf for "people" and "sitting"), indicating strong non-random associations
  - [corpus] Weak: Related papers focus on broader bias detection, not specific vocabulary-label associations
- Break condition: If annotators are instructed to avoid lexical cues or if the dataset generation process is fully randomized, the statistical association would disappear.

### Mechanism 2
- Claim: Word2vec-based replacement reduces hypothesis-only model performance
- Mechanism: Replacing words with similar embeddings disrupts the lexical patterns that hypothesis-only models exploit, forcing them to rely on premise information rather than vocabulary cues.
- Core assumption: Hypothesis-only models succeed by memorizing label-correlated vocabulary rather than learning true semantic inference.
- Evidence anchors:
  - [abstract] "word replacement using word2vec similarity reduces hypothesis-only model accuracy by 1.14%"
  - [section 5] Implementation uses nlpaug.augmenter.word.WordEmbsAug with word2vec model
  - [corpus] Weak: Related works mention bias mitigation but not specifically word2vec similarity
- Break condition: If the dataset contains no lexical bias or if word embeddings are not well-aligned with semantic similarity, the perturbation would have minimal effect.

### Mechanism 3
- Claim: PPDB synonym augmentation improves complete model accuracy
- Mechanism: By replacing words with synonyms from PPDB, the model is exposed to more diverse lexical representations, improving generalization and reducing overfitting to specific word-label patterns.
- Core assumption: Synonym replacement maintains semantic content while breaking spurious lexical associations.
- Evidence anchors:
  - [abstract] "word synonym augmentation from PPDB improves overall model accuracy by 0.66%"
  - [section 5] Uses nlpaug.augmenter.word.SynonymAug with PPDB database
  - [corpus] Weak: No direct corpus evidence linking PPDB to NLI accuracy improvements
- Break condition: If synonyms are not semantically equivalent in context or if the model overfits to specific paraphrase patterns, accuracy gains may not materialize.

## Foundational Learning

- Concept: Natural Language Inference (NLI) and dataset bias
  - Why needed here: The paper addresses biases in NLI datasets, so understanding how NLI works and what dataset artifacts are is foundational.
  - Quick check question: What are the three label types in NLI, and how can annotator bias create spurious patterns?

- Concept: Statistical hypothesis testing (χ² goodness-of-fit)
  - Why needed here: The study uses χ² tests to detect non-random associations between vocabulary and labels, so knowing how these tests work is essential.
  - Quick check question: What does a p-value of 1.0e-inf indicate about the null hypothesis in this context?

- Concept: Word embeddings and similarity
  - Why needed here: Word2vec similarity is used for data augmentation, so understanding how embeddings capture semantic similarity is key.
  - Quick check question: How does cosine similarity between word vectors relate to semantic similarity?

## Architecture Onboarding

- Component map: Data preprocessing → Statistical testing → Data augmentation → Model training → Evaluation
- Critical path: Statistical testing identifies biased words → Augmentation disrupts these patterns → Model fine-tuning on augmented data → Evaluation on hypothesis-only and complete models
- Design tradeoffs: Using word2vec vs. synonyms vs. tf-idf for augmentation balances semantic preservation with bias disruption; PPDB vs. WordNet affects synonym quality
- Failure signatures: No reduction in hypothesis-only accuracy suggests augmentation is ineffective; accuracy drop in complete model suggests semantic distortion
- First 3 experiments:
  1. Run χ² tests on extracted nouns/verbs to confirm bias patterns
  2. Apply word2vec replacement augmentation and measure hypothesis-only accuracy change
  3. Apply PPDB synonym augmentation and measure complete model accuracy change

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different data augmentation strategies compare in terms of their impact on reducing biases across various NLI datasets beyond SNLI?
- Basis in paper: [inferred] The paper primarily focuses on the SNLI dataset and proposes several data augmentation strategies to mitigate biases. However, it does not explore the effectiveness of these strategies on other NLI datasets like MNLI.
- Why unresolved: The study's scope is limited to SNLI, and it does not provide a comparative analysis of augmentation strategies across different datasets.
- What evidence would resolve it: Conducting experiments with the proposed augmentation strategies on multiple NLI datasets and comparing their effectiveness in reducing biases would provide insights into their generalizability.

### Open Question 2
- Question: What is the optimal combination of data augmentation techniques that maximizes both model accuracy and bias reduction in NLI tasks?
- Basis in paper: [inferred] The paper presents individual data augmentation strategies and their effects on model accuracy and bias reduction. However, it does not explore the potential synergistic effects of combining multiple augmentation techniques.
- Why unresolved: The study evaluates each augmentation strategy independently, leaving the question of their combined impact unanswered.
- What evidence would resolve it: Performing experiments that systematically combine different augmentation techniques and measuring their joint effects on model performance and bias reduction would help identify optimal combinations.

### Open Question 3
- Question: How do the proposed statistical testing procedures for identifying biases in NLI datasets compare to other bias detection methods in terms of sensitivity and specificity?
- Basis in paper: [explicit] The paper introduces a novel statistical testing procedure to uncover biases in the SNLI dataset, specifically focusing on vocabulary distribution. However, it does not compare this method to other existing bias detection techniques.
- Why unresolved: The study presents a new method for bias detection but lacks a comparative analysis with other established methods.
- What evidence would resolve it: Conducting a comparative study that evaluates the proposed statistical testing procedure against other bias detection methods in terms of their ability to accurately identify biases in NLI datasets would provide insights into their relative effectiveness.

## Limitations

- The study focuses only on SNLI dataset, limiting generalizability to other NLI datasets
- Only nouns and verbs are analyzed for bias detection, potentially missing other POS categories
- The magnitude of accuracy improvements (1.14% and 0.66%) suggests moderate rather than transformative impact

## Confidence

- Statistical testing methodology: Medium confidence (rigorous but narrow scope)
- Word2vec augmentation effectiveness: Low confidence (no comparative ablation studies)
- PPDB synonym augmentation: Low confidence (semantic preservation not validated)

## Next Checks

1. Conduct ablation studies comparing word2vec similarity augmentation against tf-idf sampling and synonym replacement across multiple NLI datasets to validate generalizability.
2. Perform human evaluation of synonym replacements from PPDB and WordNet to assess semantic preservation and identify potential degradation in meaning.
3. Test model performance on adversarial NLI examples to determine whether bias mitigation improves robustness to challenging inference scenarios beyond statistical improvements on standard benchmarks.