---
ver: rpa2
title: 'Towards On-device Learning on the Edge: Ways to Select Neurons to Update under
  a Budget Constraint'
arxiv_id: '2312.05282'
source_url: https://arxiv.org/abs/2312.05282
tags:
- learning
- selection
- training
- update
- neurons
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates efficient on-device learning under extreme
  memory and computation constraints. It proposes a dynamic neuron selection strategy
  that adapts to the target task during training, in contrast to static methods.
---

# Towards On-device Learning on the Edge: Ways to Select Neurons to Update under a Budget Constraint

## Quick Facts
- arXiv ID: 2312.05282
- Source URL: https://arxiv.org/abs/2312.05282
- Reference count: 40
- This paper proposes a dynamic neuron selection strategy for on-device learning under memory constraints, building on the NEq algorithm.

## Executive Summary
This paper addresses the challenge of efficient on-device learning under extreme memory and computation constraints. The authors propose a dynamic neuron selection strategy that adapts to the target task during training, in contrast to static methods like Sparse Update. The approach ranks neurons by their velocity (rate of change) and selects the fastest ones within a budget constraint. Experiments on multiple datasets and models demonstrate that this dynamic approach, on average, outperforms random selection and is competitive with static methods. The study highlights the potential for substantial gains through dynamic neuron selection and encourages further research in this area for efficient on-device learning.

## Method Summary
The paper proposes a dynamic neuron selection strategy for on-device learning, building on the NEq algorithm. The method ranks neurons by their absolute velocity (rate of change) and selects the fastest ones within a given memory budget. This adapts the subnetwork to the specific target task during training. The approach is compared against a random selection baseline and a static Sparse Update (SU) method. Experiments are conducted on pre-trained models (MobileNetV2, ResNet18, ResNet50) fine-tuned on various target datasets (Cifar10, Cifar100, VWW, Flowers, Food, Pets, CUB) under different memory budget constraints.

## Key Results
- Dynamic neuron selection based on velocity outperforms random selection on average across all datasets and models tested.
- The proposed approach is competitive with the static Sparse Update method while being more computationally efficient to find.
- Performance degrades as the memory budget decreases, but the dynamic approach maintains better accuracy than random selection even under tight constraints.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic neuron selection based on velocity outperforms static selection for on-device learning under memory constraints.
- Mechanism: The method dynamically selects neurons to update based on their velocity (rate of change), ranking neurons by their absolute velocity and selecting the fastest ones within a given budget. This adapts the subnetwork to the specific target task during training.
- Core assumption: Neurons with higher velocity are more critical for the target task and their update will have a larger impact on performance.
- Evidence anchors:
  - [abstract] "Our experiments demonstrate, in the average case, the superiority of a NEq-inspired approach over a random selection."
  - [section 3.3] "We evaluate the total parameterâ€™s cost including the j-th fastest neurons as Bw_j = sum_{i=1}^j Cw_i where |v_i| >= |v_j|, 1 <= i <= j <= M"
  - [corpus] Weak evidence: The related papers found do not directly address dynamic neuron selection based on velocity, suggesting this is a novel contribution.
- Break condition: If the correlation between neuron velocity and importance for the target task breaks down, or if the budget constraint becomes too tight to allow for effective adaptation.

### Mechanism 2
- Claim: Random neuron selection is a viable baseline for on-device learning under memory constraints.
- Mechanism: Neurons to update are chosen randomly until the budget Bw_max is met. This provides a simple, computationally cheap baseline to compare against more complex strategies.
- Core assumption: Randomly selected neurons will still provide some improvement over not updating any neurons, even if not as effective as a more targeted approach.
- Evidence anchors:
  - [section 3.3] "A random selection baseline. To the best of our knowledge, our proposal is the very first approach attempting to dynamically select a sub-network to update. A very intuitive baseline we can build consists of randomly selecting neurons to be updated, until the budget Bw_max is met."
  - [table 2] The Random selection method often achieves performance close to the dynamic Velocity method, suggesting it is a reasonable baseline.
  - [corpus] No direct evidence, but the concept of random selection is common in many optimization problems as a simple baseline.
- Break condition: If the memory budget becomes so small that even random selection provides negligible improvement, or if the random selection fails to include critical neurons for the task.

### Mechanism 3
- Claim: The Sparse Update (SU) approach, while effective, is computationally expensive to find and static for the entire training process.
- Mechanism: SU uses an evolutionary algorithm to find the optimal subset of layers and biases to update for a given memory budget. This configuration is then static for the entire training.
- Core assumption: There exists a static subnetwork that can be found offline and will provide good performance for the target task.
- Evidence anchors:
  - [section 3.1] "The SU approach is labor-intensive and costly, requiring multiple trainings for various layers and update combinations followed by an evolutionary search to find the optimal configuration."
  - [section 3.2] "Additionally, the SU configuration remains unchanged throughout network fine-tuning, leading to the same neurons being updated over and over until potential over-fitting while other neurons remain frozen even though they could require some training to improve performance."
  - [corpus] No direct evidence, but the concept of evolutionary algorithms for neural architecture search is well-established.
- Break condition: If the target task is too different from the tasks used to find the SU scheme, or if the static nature of the approach leads to overfitting or underfitting for the specific target task.

## Foundational Learning

- Concept: On-device learning
  - Why needed here: The paper focuses on efficient learning under extreme memory and computation constraints, which is a key challenge for on-device learning.
  - Quick check question: What are the main obstacles to on-device learning compared to on-device inference?

- Concept: Backpropagation and its memory cost
  - Why needed here: The paper highlights that the cost of backpropagation is the main obstacle to efficient on-device learning, as it often exceeds tight memory budgets.
  - Quick check question: Why is backpropagation more memory-intensive than forward propagation in neural networks?

- Concept: Subnetworks and parameter pruning
  - Why needed here: The paper builds on the idea that only a subset of neurons (a subnetwork) needs to be updated for effective on-device learning, inspired by the lottery ticket hypothesis.
  - Quick check question: What is the lottery ticket hypothesis, and how does it relate to the idea of subnetworks in neural networks?

## Architecture Onboarding

- Component map: Pre-trained model (MobileNetV2, ResNet18, ResNet50) -> Neuron selection strategy (SU, Velocity, Random) -> Memory budget constraint (Bw_max) -> Target datasets (Cifar10, Cifar100, VWW, Flowers, Food, Pets, CUB)
- Critical path: 1. Pre-train model on upstream task (e.g., ImageNet) 2. Load pre-trained model to target device 3. Select neurons to update based on chosen strategy and memory budget 4. Fine-tune model on target dataset 5. Evaluate performance
- Design tradeoffs:
  - Memory vs. performance: Tighter memory budgets lead to fewer neurons being updated, which may reduce performance but is necessary for on-device learning.
  - Static vs. dynamic selection: Static selection (SU) is computationally expensive to find but simple to implement, while dynamic selection (Velocity) is cheaper to find but requires more computation during training.
  - Velocity vs. Random selection: Velocity selection is more targeted and often performs better, but Random selection is simpler and can still provide reasonable performance.
- Failure signatures:
  - Underfitting: If too few neurons are updated, the model may not learn the target task effectively.
  - Overfitting: If too many neurons are updated, the model may overfit to the target dataset, especially if it is small.
  - Inefficient use of budget: If the neuron selection strategy does not effectively use the available memory budget, performance may suffer.
- First 3 experiments:
  1. Compare the performance of SU, Velocity, and Random selection on MobileNetV2 with a fixed memory budget (e.g., 8.8% of parameters).
  2. Vary the memory budget and observe how it affects the performance of each selection strategy.
  3. Test the selection strategies on different pre-trained models (e.g., ResNet18, ResNet50) and target datasets to assess their generalizability.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- The paper does not provide a detailed comparison of the computational cost between the dynamic (Velocity) and static (SU) neuron selection methods during the fine-tuning process, which is a critical factor for on-device learning.
- The experiments are conducted on a limited set of pre-trained models (MobileNetV2, ResNet18, ResNet50) and target datasets, which may not be representative of the full range of scenarios encountered in practice.
- The paper does not discuss the potential impact of the neuron selection strategy on the interpretability or robustness of the fine-tuned models.

## Confidence
- High confidence: The paper presents a novel approach to dynamic neuron selection for on-device learning, and the experimental results demonstrate its potential benefits over static methods and random selection.
- Medium confidence: The paper provides a reasonable comparison of the dynamic (Velocity) and static (SU) neuron selection methods, but the computational cost analysis is limited, and the generalizability of the results to other models and datasets is not fully explored.
- Low confidence: The paper does not address the potential impact of the neuron selection strategy on the interpretability or robustness of the fine-tuned models, which could be important factors in real-world applications.

## Next Checks
1. Conduct a detailed analysis of the computational cost of the dynamic (Velocity) and static (SU) neuron selection methods during the fine-tuning process, and compare their efficiency for on-device learning.
2. Extend the experiments to a wider range of pre-trained models and target datasets to assess the generalizability and robustness of the dynamic neuron selection approach.
3. Investigate the impact of the neuron selection strategy on the interpretability and robustness of the fine-tuned models, and explore potential techniques to mitigate any negative effects.