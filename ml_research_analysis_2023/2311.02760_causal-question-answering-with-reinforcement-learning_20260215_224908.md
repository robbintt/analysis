---
ver: rpa2
title: Causal Question Answering with Reinforcement Learning
arxiv_id: '2311.02760'
source_url: https://arxiv.org/abs/2311.02760
tags:
- causal
- agent
- learning
- question
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a reinforcement learning approach to answering
  causal questions using a large-scale causal knowledge graph (CauseNet). The authors
  model the problem as a sequential decision process where an agent learns to traverse
  the graph to find paths explaining causal relationships.
---

# Causal Question Answering with Reinforcement Learning

## Quick Facts
- arXiv ID: 2311.02760
- Source URL: https://arxiv.org/abs/2311.02760
- Reference count: 40
- The RL agent learns to traverse a causal knowledge graph, achieving 0.89 precision while visiting less than 30 nodes per question compared to over 3,000 for BFS.

## Executive Summary
This paper presents a reinforcement learning approach to answering binary causal questions by traversing a large-scale causal knowledge graph (CauseNet). The authors model the task as a sequential decision process where an agent learns to find paths explaining causal relationships. A key innovation is bootstrapping the RL agent with supervised learning from breadth-first search demonstrations, which significantly accelerates learning and improves performance. The approach achieves high precision (0.89) while dramatically reducing the search space compared to baseline methods.

## Method Summary
The authors implement a reinforcement learning agent based on the A2C algorithm with LSTM-based policy and value networks. The agent learns to traverse CauseNet to answer binary causal questions like "Does pneumonia cause anemia?" The key innovation is a two-phase training approach: first training the policy to imitate BFS demonstrations via supervised learning (REINFORCE with constant reward), then fine-tuning with policy gradients. During inference, beam search (width 50) is used to sample paths, and the answer is "yes" if the effect entity appears in any path. The approach significantly reduces search complexity while maintaining high precision.

## Key Results
- RL agent achieves 0.89 precision on binary causal question answering
- Search efficiency improves dramatically: visits <30 nodes per question vs. >3,000 for BFS
- Supervised learning phase is crucial, providing strong foundation that accelerates RL convergence
- Beam search decoding significantly outperforms greedy decoding, improving accuracy from 0.293 to 0.460 on MS MARCO

## Why This Works (Mechanism)

### Mechanism 1
Bootstrapping with supervised learning establishes a strong foundation that accelerates reinforcement learning convergence. The agent first learns to imitate expert demonstrations (BFS paths) via REINFORCE with constant reward, then continues training with policy gradients. This reduces the initial exploration burden and entropy of the policy distribution. If the BFS demonstrations contain many suboptimal paths or if the graph structure changes significantly, the supervised initialization may mislead the agent.

### Mechanism 2
The Actor-Critic architecture with generalized advantage estimation improves learning efficiency over pure policy gradient methods. The critic network estimates state values to reduce variance in policy updates via advantage computation, while GAE smooths advantage estimates over time steps. If the value network cannot learn accurate estimates due to high variance in rewards or complex state representations, the advantage estimates become unreliable.

### Mechanism 3
Beam search decoding effectively balances exploration and exploitation better than greedy decoding alone. Beam search maintains multiple high-probability partial paths, allowing the agent to recover from early suboptimal decisions while still focusing on likely successful trajectories. If the policy distribution becomes too peaked or too uniform, beam search loses its advantage over simpler decoding strategies.

## Foundational Learning

- Concept: Markov Decision Process formulation of graph traversal
  - Why needed here: Provides the mathematical framework for defining states, actions, rewards, and transitions in the causal question answering task
  - Quick check question: What are the four components of an MDP and how do they map to the causal question answering problem?

- Concept: Actor-Critic reinforcement learning algorithms
  - Why needed here: Enables stable learning by reducing variance through value function estimation while still allowing policy improvement
  - Quick check question: How does the critic network help reduce variance in policy gradient updates?

- Concept: Generalized Advantage Estimation (GAE)
  - Why needed here: Smooths advantage estimates over multiple time steps, balancing bias and variance in the learning signal
  - Quick check question: What hyperparameters control the bias-variance tradeoff in GAE?

## Architecture Onboarding

- Component map: State encoding → LSTM processing → Action distribution → Environment transition → Reward → Value estimation → Policy update
- Critical path: State encoding → LSTM processing → Action distribution → Environment transition → Reward → Value estimation → Policy update
- Design tradeoffs: Breadth-first search baseline vs. RL agent (accuracy vs. efficiency), LSTM vs. feedforward for path history, beam width selection for decoding
- Failure signatures: Low precision indicates poor path selection, low recall suggests insufficient exploration, high entropy indicates unstable learning
- First 3 experiments:
  1. Train agent with supervised learning only (no RL) to verify initialization effectiveness
  2. Compare greedy decoding vs. beam search with fixed RL-trained policy
  3. Test different LSTM vs. feedforward architectures on a small validation set

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the RL agent change when using different graph neural network architectures instead of an LSTM for encoding path history? The paper mentions that a pure feedforward network slightly decreases performance compared to an LSTM, and suggests that a GNN could capture an even larger context of nodes. This remains unresolved as the paper only experimented with LSTMs and simple feedforward networks.

### Open Question 2
What is the impact of adding inverse edges on the agent's performance when answering open-ended causal questions like "What causes X?" or "What are the effects of X?" The paper discusses that inverse edges can introduce false positives but also improve performance in practice for binary questions, and mentions that extending to open-ended questions only requires minor changes to the current code base. This remains unresolved as the paper only evaluated the agent on binary causal questions.

### Open Question 3
How does the agent's performance scale when applied to larger causal knowledge graphs beyond CauseNet? The paper uses CauseNet as the knowledge graph, which contains 80,222 entities. It would be valuable to know if the approach generalizes to much larger graphs. This remains unresolved as the paper only evaluated the agent on CauseNet.

## Limitations

- Entity linking relies on exact string matching without specifying normalization or handling of synonyms
- Evaluation focuses on binary questions with entities present in CauseNet, limiting generalizability
- LSTM-based policy network architecture lacks specification of hidden dimensions and sequence length handling

## Confidence

- **High confidence**: The supervised bootstrapping mechanism improves RL convergence (supported by ablation studies and learning curves)
- **Medium confidence**: Beam search decoding significantly improves accuracy over greedy decoding (supported by quantitative comparisons)
- **Medium confidence**: The A2C architecture with GAE provides stable learning (supported by training stability, but limited comparison to alternative RL methods)

## Next Checks

1. Implement and compare alternative entity linking strategies (fuzzy matching, entity disambiguation) to assess robustness of the reported performance
2. Conduct ablation studies testing different supervised learning step counts and RL fine-tuning durations to verify the optimal training schedule
3. Evaluate the model on multi-hop causal questions requiring longer path reasoning to test scalability beyond the reported average of 30 nodes visited