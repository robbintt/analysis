---
ver: rpa2
title: 'MegaWika: Millions of reports and their sources across 50 diverse languages'
arxiv_id: '2307.07049'
source_url: https://arxiv.org/abs/2307.07049
tags:
- source
- wikipedia
- passage
- question
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MegaWika is a large-scale multilingual dataset containing 13 million
  Wikipedia articles across 50 languages and their 71 million referenced source materials.
  The dataset includes 128 million automatically generated question-answer pairs.
---

# MegaWika: Millions of reports and their sources across 50 diverse languages

## Quick Facts
- arXiv ID: 2307.07049
- Source URL: https://arxiv.org/abs/2307.07049
- Reference count: 27
- 13 million Wikipedia articles across 50 languages with 71 million referenced sources

## Executive Summary
MegaWika is a large-scale multilingual dataset containing 13 million Wikipedia articles across 50 languages and their 71 million referenced source materials. The dataset includes 128 million automatically generated question-answer pairs. Passages are translated into English, semantically analyzed using FrameNet, and questions are generated and aligned with source documents. Manual evaluation found that 48% of cited sources contain the exact event described in the Wikipedia passage. The dataset enables multilingual QA and information retrieval tasks, outperforming existing models. It is the largest and most diverse resource for sentence-level report generation and multilingual report generation.

## Method Summary
The MegaWika dataset was constructed by extracting Wikipedia passages with citations, scraping source documents from cited URLs, translating non-English passages to English using M2M-100 or Google Translate, generating question-answer pairs using PAQ, performing semantic analysis with LOME FrameNet parser, and aligning answers with source documents. The pipeline creates a comprehensive resource for multilingual question answering and information retrieval tasks, with quality evaluated through manual annotation and baseline model performance.

## Key Results
- 13 million Wikipedia articles across 50 languages with 71 million referenced sources
- 128 million automatically generated question-answer pairs
- 48% of cited sources contain the exact event described in the Wikipedia passage
- mDPR model trained on MegaWika outperforms MiRACL's mDPR by 300% on MegaWika data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translating non-English Wikipedia passages into English enables cross-lingual question answering by aligning semantic content across languages.
- Mechanism: Machine translation creates parallel English and non-English representations, allowing models trained on English QA data to generalize to other languages via shared semantic space.
- Core assumption: The translation quality is high enough that semantic meaning is preserved across languages, enabling effective alignment of question-answer pairs.
- Evidence anchors:
  - [abstract] "We process this dataset for a myriad of applications, going beyond the initial Wikipedia citation extraction and web scraping of content, including translating non-English articles for cross-lingual applications"
  - [section 2] "In the case of non-English Wikipedia, translate the Wikipedia passage into English: this allows MegaWika to serve both as a monolingual resource in each of 50 languages, as well as a cross-lingual resource centered on English"
  - [corpus] Corpus contains translated passages alongside originals, but quality varies by language pair
- Break condition: If translations introduce semantic drift or lose key factual content, cross-lingual alignment breaks down and QA performance degrades.

### Mechanism 2
- Claim: FrameNet semantic parsing enables structured exploration and filtering of Wikipedia content by identifying events, processes, and states.
- Mechanism: LOME FrameNet parser tags spans of text that evoke semantic frames, creating structured annotations that can be used to sample passages by semantic type or filter for specific event types.
- Core assumption: FrameNet's semantic categories map well to the types of information people seek when reading Wikipedia, and the parser's precision is sufficient for meaningful stratification.
- Evidence anchors:
  - [abstract] "We process this dataset for a myriad of applications... including translating non-English articles for cross-lingual applications and providing FrameNet parses for automated semantic analysis"
  - [section 2] "Extract events using the LOME FrameNet parser... These annotations enable structured exploration and semantics based sampling of MegaWika"
  - [corpus] Corpus includes FrameNet parse annotations for all passages, with evaluation showing 48% of sources contain the highlighted event
- Break condition: If FrameNet parser precision is too low for certain frame types, stratified sampling becomes ineffective and semantic filtering yields poor results.

### Mechanism 3
- Claim: Question-answer pair generation from Wikipedia passages creates synthetic training data that improves multilingual QA models.
- Mechanism: PAQ system extracts answer spans from passages and generates natural questions, creating large-scale QA pairs that can be used to pretrain or fine-tune QA models across languages.
- Core assumption: Generated questions are natural and cover the types of information-seeking queries users actually ask, making the synthetic data useful for model training.
- Evidence anchors:
  - [abstract] "Finally, we provide baseline results and trained models for crucial steps in automated report generation: cross-lingual question answering and citation retrieval"
  - [section 2] "Automatically generate questions based on the English Wikipedia passage... In the case of question/answer pairs generated from a translated passage... we align the translated passage with its non-English original and determine the most likely non-English span corresponding to the English answer"
  - [corpus] Corpus contains 128 million QA pairs, but quality varies - some questions are unnatural or unanswerable
- Break condition: If generated questions are too artificial or don't match real user query distributions, models trained on this data won't generalize well to actual usage.

## Foundational Learning

- Concept: Cross-lingual information retrieval and question answering
  - Why needed here: The dataset spans 50 languages with citations to documents in various languages, requiring models that can handle cross-lingual retrieval and QA
  - Quick check question: What is the main challenge when retrieving documents in language A to answer questions in language B?

- Concept: Semantic role labeling and FrameNet
  - Why needed here: FrameNet parsing is used to identify and annotate events, processes, and states in Wikipedia passages for structured exploration
  - Quick check question: How does FrameNet represent the semantic structure of events differently from syntactic parsing?

- Concept: Web scraping and content extraction
  - Why needed here: Source documents are scraped from cited URLs and cleaned to extract readable content, requiring understanding of HTML structure and content extraction techniques
  - Quick check question: What are the main challenges in extracting meaningful content from arbitrary web pages?

## Architecture Onboarding

- Component map: Wikipedia dump processing -> passage extraction -> source scraping -> translation -> FrameNet parsing -> QA generation -> corpus assembly
- Critical path: Wikipedia passage extraction -> source document scraping -> translation -> QA generation -> corpus assembly
- Design tradeoffs: Used M2M-100 for most languages but Google Translate for lowest-resource languages due to quality differences. Chose FrameNet over other semantic representations for its event/state/process coverage. Generated QA pairs synthetically rather than collecting human annotations due to scale.
- Failure signatures: Low FrameNet precision on certain frame types -> poor semantic stratification. Translation errors -> broken cross-lingual alignment. Source scraping failures -> missing source documents. QA generation failures -> unanswerable or unnatural questions.
- First 3 experiments:
  1. Validate translation quality by sampling passages and checking semantic preservation across languages
  2. Test FrameNet parsing precision on a stratified sample of passages to ensure semantic stratification works
  3. Evaluate QA generation quality by checking if generated questions are answerable from passages and sources

## Open Questions the Paper Calls Out

- How does the semantic event distribution in MegaWika's 50 languages compare to the linguistic diversity and cultural contexts of those languages?
- What is the impact of machine translation quality on the downstream performance of cross-lingual question answering models trained on MegaWika?
- How does the citation retrieval performance of mDPR trained on MegaWika compare to models trained on other multilingual IR datasets like MiRACL when evaluated on the same test sets?

## Limitations
- Dataset quality heavily depends on translation accuracy across 50 languages, which varies significantly
- FrameNet parser's performance on non-English content is not evaluated, creating uncertainty about semantic analysis reliability
- 48% manual evaluation precision rate may limit downstream task performance
- Synthetic QA generation may not capture the full distribution of real user queries

## Confidence
- **High confidence**: The dataset construction methodology is well-specified and reproducible
- **Medium confidence**: The translation quality and its impact on cross-lingual alignment
- **Medium confidence**: The semantic parsing utility, as FrameNet performance varies by language and frame type
- **Low confidence**: The downstream task performance claims

## Next Checks
1. Sample passages from each of the 50 languages and verify translation quality by comparing original and translated versions for semantic preservation and factual accuracy
2. Evaluate FrameNet parsing precision across a stratified sample of passages in multiple languages to assess semantic analysis reliability for cross-lingual applications
3. Test QA generation quality by randomly sampling 100 generated questions and verifying that (a) they are answerable from the passage, (b) they are natural language questions, and (c) the answer spans align with source documents