---
ver: rpa2
title: 'Evaluating the Evaluators: Are Current Few-Shot Learning Benchmarks Fit for
  Purpose?'
arxiv_id: '2307.02732'
source_url: https://arxiv.org/abs/2307.02732
tags:
- performance
- learning
- few-shot
- accuracy
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of evaluating and selecting few-shot
  learning models at the task level, where only a small number of training examples
  are available. Current benchmarks focus on average performance over many tasks,
  but do not provide reliable estimates for individual tasks.
---

# Evaluating the Evaluators: Are Current Few-Shot Learning Benchmarks Fit for Purpose?

## Quick Facts
- arXiv ID: 2307.02732
- Source URL: https://arxiv.org/abs/2307.02732
- Reference count: 7
- Current few-shot learning benchmarks are not designed to provide reliable task-level performance estimates

## Executive Summary
This paper investigates the reliability of performance estimators in few-shot learning settings, where only limited training data is available per task. The authors find that standard evaluation methods like hold-out, cross-validation, and bootstrapping suffer from high bias and variance, making them unreliable for estimating true task-level performance. Through experiments on three datasets (miniImageNet, CIFAR-FS, Meta-Album) using four FSL algorithms, they demonstrate that 5-fold cross-validation offers the best trade-off for performance estimation, while bootstrapping or cross-validation with many folds is better suited for model selection. The paper concludes that current benchmarks are not designed to provide reliable task-level performance estimates, highlighting the need for new evaluation procedures specifically designed for few-shot learning.

## Method Summary
The authors evaluate four performance estimators (hold-out, 5-fold CV, leave-one-out CV, and bootstrapping) across three few-shot learning datasets and four algorithms (ProtoNet, MAML, R2D2, Baseline++). They compare estimator accuracy to oracle performance using mean absolute error (MAE) and rank correlation metrics. Experiments are conducted in 5-way 5-shot settings with standard meta-train/valid/test splits. The LibFewShot implementation is used with suggested hyperparameters, and the analysis focuses on task-level performance estimation rather than average performance across many tasks.

## Key Results
- All standard performance estimators (hold-out, CV, bootstrapping) have high bias and variance in few-shot settings
- 5-fold cross-validation provides the best trade-off between bias and variance for direct performance estimation
- Bootstrapping or cross-validation with many folds performs better for model selection tasks
- More stable algorithms (like ProtoNet) provide more reliable performance estimates when used with cross-validation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hold-out, cross-validation, and bootstrapping estimators all suffer from high bias and variance in the few-shot setting, making them unreliable for task-level performance estimation.
- Mechanism: These estimators sacrifice training data to estimate performance, leading to models that are poorer than models trained on all available data. In the few-shot regime, this effect is amplified because each data point is critical for learning.
- Core assumption: The few-shot learning setting inherently has very limited training data per task, so any data used for validation directly harms the model's ability to learn.
- Evidence anchors:
  - [abstract] "However, 5-fold cross-validation is the best option for directly estimating model performance, while bootstrapping or cross-validation with many folds is better for model selection."
  - [section] "Estimators with high bias and variance will exhibit a lack of co-linearity and be centred off the diagonal, respectively. We can see that all estimators have very high bias and variance, indicating that they do not provide reliable estimates of actual few-shot learning performance."
- Break condition: If the number of available training examples per task increases significantly, the bias and variance of these estimators may decrease to acceptable levels.

### Mechanism 2
- Claim: The stability of the underlying learning algorithm is an important factor when estimating performance in the few-shot setting.
- Mechanism: More stable algorithms (like ProtoNet) provide more reliable performance estimates when used with cross-validation because they are less sensitive to small changes in the training data.
- Core assumption: Algorithmic stability reduces the dependence on specific training data points, making performance estimates less sensitive to the particular data used for validation.
- Evidence anchors:
  - [section] "This line of reasoning is congruent with our empirical results, where we see that the most stable algorithm (Prototypical Networks) consistently has low MAE (relative to other approaches) when coupled with CV estimator."
- Break condition: If the algorithm's stability is compromised (e.g., through hyperparameter changes or architectural modifications), the reliability of performance estimates may degrade.

### Mechanism 3
- Claim: Cross-validation with a moderate number of folds (like 5-fold) provides the best trade-off between bias and variance for performance estimation in few-shot learning.
- Mechanism: 5-fold CV balances the need to use some data for validation (reducing bias) with the need to train on sufficient data (reducing variance), while avoiding the class imbalance issues that plague LOO-CV.
- Core assumption: There exists an optimal number of folds that balances the competing effects of bias and variance, and this optimal number is specific to the few-shot learning setting.
- Evidence anchors:
  - [section] "We find that 5-fold CV presents the ideal trade-off between these two effects."
  - [section] "We hypothesise that the negative effect that causes CV with a large number of folds to become less accurate is related to the class imbalance."
- Break condition: If the class distribution becomes more balanced or if the number of training examples per class increases significantly, the optimal number of folds may change.

## Foundational Learning

- Concept: Bias-variance tradeoff
  - Why needed here: Understanding how different evaluation methods balance bias and variance is crucial for interpreting the experimental results and choosing appropriate methods.
  - Quick check question: If an estimator has high bias but low variance, what does that tell you about its performance estimates compared to the true performance?

- Concept: Algorithmic stability
  - Why needed here: The stability of learning algorithms affects the reliability of performance estimates, particularly when using cross-validation.
  - Quick check question: How might a more stable algorithm affect the variance of performance estimates obtained through cross-validation?

- Concept: Cross-validation mechanics
  - Why needed here: Different forms of cross-validation (k-fold, LOO) have different properties that affect their suitability for few-shot learning.
  - Quick check question: What is the main difference between k-fold cross-validation and leave-one-out cross-validation in terms of how they use the training data?

## Architecture Onboarding

- Component map:
  Data pipeline -> Model zoo -> Evaluation framework -> Experiment runner -> Analysis tools

- Critical path:
  1. Load dataset and split into meta-train, meta-validation, and meta-test sets
  2. For each meta-test episode:
     a. Train model using support set
     b. Estimate performance using various evaluators
     c. Compare estimates to oracle performance
  3. Aggregate results across episodes
  4. Analyze performance of evaluators

- Design tradeoffs:
  - Number of folds in cross-validation: Balancing bias and variance vs. computational cost
  - Size of query set for oracle: Larger sets provide more accurate oracle estimates but may not reflect realistic few-shot scenarios
  - Choice of FSL algorithms: Different algorithms have different stability properties affecting evaluator performance

- Failure signatures:
  - High MAE between estimator and oracle: Indicates unreliable performance estimates
  - Low Spearman correlation between estimator rankings and oracle rankings: Suggests poor model selection capability
  - Large variance in estimator performance across episodes: Indicates estimator instability

- First 3 experiments:
  1. Compare oracle accuracy to hold-out, 5-fold CV, LOO-CV, and bootstrapping estimates for each FSL algorithm
  2. Analyze how estimator-oracle error varies with the number of shots per class
  3. Investigate the effect of class imbalance on LOO-CV performance by varying the number of ways while keeping total support set size constant

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms could be employed to improve the stability of few-shot learning algorithms and thereby enhance the reliability of performance estimators?
- Basis in paper: [explicit] The paper mentions that algorithmic stability is an important factor when estimating performance and that more stable algorithms have been shown to provide more reliable performance estimates when used in conjunction with cross validation.
- Why unresolved: While the paper identifies the importance of stability, it does not propose specific methods to improve the stability of few-shot learning algorithms.
- What evidence would resolve it: Experimental results demonstrating the impact of various algorithmic stability enhancements on the accuracy of performance estimators in few-shot learning settings.

### Open Question 2
- Question: How can Bayesian estimation procedures, informed by performance on the meta-training and meta-validation episodes, be effectively integrated into few-shot learning evaluation?
- Basis in paper: [explicit] The paper suggests that future work could design specialised evaluation procedures, including Bayesian estimation procedures that are informed by performance on the meta-training and meta-validation episodes.
- Why unresolved: The paper does not provide details on how such Bayesian estimation procedures could be implemented or their potential effectiveness.
- What evidence would resolve it: Development and experimental validation of Bayesian estimation procedures for few-shot learning, comparing their performance against existing methods.

### Open Question 3
- Question: What are the most effective ways to leverage side-information to reduce variance in few-shot learning performance estimates?
- Basis in paper: [explicit] The paper proposes that future work could leverage other side-information to reduce variance in estimates.
- Why unresolved: The paper does not specify what types of side-information could be used or how they could be incorporated into the evaluation process.
- What evidence would resolve it: Research demonstrating the impact of incorporating different types of side-information on the variance of few-shot learning performance estimates.

## Limitations

- The findings are primarily based on 5-way 5-shot settings and may not generalize to other FSL configurations
- The paper lacks theoretical justification for why 5-fold CV is optimal, presenting it as an empirical observation rather than a principled recommendation
- The analysis focuses on a limited set of FSL algorithms, and the observed superiority of ProtoNet may be algorithm-specific

## Confidence

- **High confidence:** The experimental methodology for comparing performance estimators (using MAE and rank correlation) is sound and well-executed. The observation that all estimators suffer from high bias and variance in few-shot settings is well-supported.
- **Medium confidence:** The recommendation of 5-fold CV for performance estimation and bootstrapping/CV with many folds for model selection is based on empirical evidence but lacks theoretical grounding. The observed superiority of ProtoNet for performance estimation is interesting but may be algorithm-specific.
- **Low confidence:** The extrapolation of these findings to all few-shot learning scenarios, including different algorithm families, task distributions, and evaluation protocols, requires further validation.

## Next Checks

1. Test the performance of the recommended estimators (5-fold CV for performance, bootstrapping for selection) across a wider range of FSL configurations, including 1-shot, 20-way, and cross-domain settings, to assess their robustness.
2. Conduct ablation studies to understand the impact of support set size, class imbalance, and algorithm stability on estimator performance, potentially leading to more nuanced recommendations.
3. Develop and evaluate alternative evaluation protocols specifically designed for few-shot learning, such as hierarchical validation or task-level meta-learning, to address the fundamental limitations identified in current benchmarks.