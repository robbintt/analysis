---
ver: rpa2
title: Adversarial Imitation Learning On Aggregated Data
arxiv_id: '2311.08568'
source_url: https://arxiv.org/abs/2311.08568
tags:
- agent
- data
- reward
- learning
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'AILAD introduces a novel adversarial imitation learning approach
  that addresses key limitations of existing IRL methods: the need for full trajectories,
  the assumption of homogeneous expert data, and the requirement for complex inner-loop
  RL solvers. The method learns a non-linear reward function and optimal policy jointly
  using aggregated data only, without requiring complete state-action trajectories.'
---

# Adversarial Imitation Learning On Aggregated Data

## Quick Facts
- arXiv ID: 2311.08568
- Source URL: https://arxiv.org/abs/2311.08568
- Authors: 
- Reference count: 36
- Key outcome: AILAD outperforms CARMI on Jensen-Shannon divergence metrics (0.25 train, 0.52 test vs 0.33 and 0.59) for matching expert distributions using aggregated data only

## Executive Summary
AILAD introduces a novel adversarial imitation learning approach that addresses key limitations of existing IRL methods: the need for full trajectories, the assumption of homogeneous expert data, and the requirement for complex inner-loop RL solvers. The method learns a non-linear reward function and optimal policy jointly using aggregated data only, without requiring complete state-action trajectories. It introduces a latent space vector to enable diverse behaviors that match the distribution of expert demonstrations across multiple metrics.

The approach was evaluated in a complex 3D turn-based strategy game environment using data from 25 human players across 9 training levels and 1 test level. Metrics included offensive actions, defensive actions, and special ability usage. AILAD significantly outperformed the baseline CARMI method on Jensen-Shannon divergence metrics for matching expert distributions, achieving values of 0.25 (train) and 0.52 (test) versus CARMI's 0.33 and 0.59 respectively. The method also generalized better to test levels and matched additional unmeasured game behaviors more closely than CARMI.

## Method Summary
AILAD learns a reward function and optimal policy jointly using only aggregated data from expert trajectories. The method introduces a latent space vector z sampled from a standard normal distribution as input to the policy network, enabling diverse behaviors that match the expert's aggregated metrics distribution. A discriminator network classifies expert vs agent-generated aggregated metrics, providing a reward signal for the policy. The approach uses ACER (Actor-Critic with Experience Replay) as the RL algorithm and implements stability improvements including pre-training the discriminator, using off-policy updates, and updating the discriminator less frequently than the policy. The method is evaluated on a 3D turn-based strategy game with 7414-dimensional state space and 1258 possible actions.

## Key Results
- Achieved Jensen-Shannon divergence of 0.25 (train) and 0.52 (test) on aggregated metrics versus CARMI's 0.33 and 0.59
- Generalizes better to test levels than CARMI
- Matches additional unmeasured game behaviors more closely than baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The latent space z enables diversity by allowing the agent to sample different behavior modes while still optimizing for the same reward structure.
- Mechanism: By injecting a random vector z sampled from a standard normal distribution into the policy network as an additional input, the agent can produce a distribution of trajectories that match the expert's aggregated metrics distribution rather than converging to a single deterministic policy.
- Core assumption: The expert demonstrations contain a variety of behaviors that can be captured in the aggregated metric space, and a continuous latent space can span this behavioral space effectively.
- Evidence anchors:
  - [abstract] "Moreover, building from both the GAN framework and the goal-conditioned approaches applied to behavioral diversity, we introduce a latent space, given as input to the agent. This latent space, absent from existing IRL approaches, allows the agent to introduce diversity in its behavior, thus producing not a single optimal policy, but rather the full distribution of the experts on the space of aggregated data."
  - [section] "In addition to training the reward model on aggregated data only, we introduce a latent space z ~ Z. This latent space serves the same purpose as in the GAN framework: generate diversity."
- Break condition: If the expert data is too homogeneous or the metric space doesn't capture the diversity well, the latent space may not be able to generate meaningful diverse behaviors.

### Mechanism 2
- Claim: The GAN-GCL correction stabilizes training by ensuring the discriminator outputs are well-calibrated probabilities rather than unbounded values.
- Mechanism: The discriminator outputs a probability using the softmax-like function exp(f_φ(ψ(τ)))/(exp(f_φ(ψ(τ))) + π_θ(ψ(τ))), which ensures convergence to a fixed point and provides stable gradients for both the discriminator and generator.
- Core assumption: The equilibrium point of this discriminator formulation is stable and leads to meaningful gradients for policy improvement.
- Evidence anchors:
  - [abstract] "Using neural networks as R and training the RL agent and the reward estimator in an adversarial manner has been the focus of recent work [4] [7]."
  - [section] "Because our approach relies on aggregated data and not full paths, computing q is quite straightforward, and requires little computational power."
- Break condition: If the agent's distribution π_θ doesn't overlap well with the expert distribution, the discriminator may become too confident, leading to vanishing gradients.

### Mechanism 3
- Claim: Training on aggregated metrics rather than full trajectories makes the approach scalable to complex environments where trajectory data is unavailable or expensive to collect.
- Mechanism: By summarizing each trajectory into a fixed-length vector of normalized metrics, the discriminator only needs to process low-dimensional data, avoiding the computational burden of processing high-dimensional state-action sequences.
- Core assumption: The aggregated metrics contain sufficient information to distinguish between expert and agent behaviors, and the normalization ensures fair comparison across different metric scales.
- Evidence anchors:
  - [abstract] "However, current methods are constrained by at least one of the following requirements... The second one is the need for full trajectories from the experts, which might not be easily available."
  - [section] "As stated before, we assume that full expert trajectories are not available, instead the agent can access data aggregated over each trajectory."
- Break condition: If critical behavioral information is lost during aggregation, the agent may learn to optimize for the metrics without actually matching the expert's true behavior.

## Foundational Learning

- Concept: Maximum Entropy Inverse Reinforcement Learning
  - Why needed here: Provides the theoretical framework for learning a reward function that explains expert behavior while maximizing entropy to avoid overfitting to specific trajectories.
  - Quick check question: How does the entropy regularization term affect the learned reward function's sensitivity to variations in expert demonstrations?

- Concept: Generative Adversarial Networks
  - Why needed here: The adversarial framework between the discriminator (reward learner) and generator (policy) enables learning without requiring explicit reward specification.
  - Quick check question: What role does the discriminator play in shaping the agent's policy, and how does this differ from traditional RL reward functions?

- Concept: Goal-Conditioned Reinforcement Learning
  - Why needed here: The latent space z functions similarly to goal conditioning, allowing the agent to produce diverse behaviors conditioned on different latent vectors.
  - Quick check question: How does conditioning on latent vectors compare to conditioning on explicit goals in terms of behavioral diversity and controllability?

## Architecture Onboarding

- Component map:
  - Environment: Turn-based strategy game with 7414-dimensional state and 1258 action space
  - Aggregator: Function ψ that converts trajectories to normalized metrics
  - Discriminator: Neural network D_φ that outputs probability of being expert data
  - Policy: ACER-based agent π_θ with latent input z
  - Replay buffer: Stores both state-action tuples and aggregated metrics

- Critical path: z ~ N(0,I) → Policy π_θ(z) → Environment → Trajectory → ψ → Aggregated metrics → Discriminator D_φ → Reward r = log D - log(1-D) → Policy update

- Design tradeoffs: Aggregated data reduces computational cost but may lose temporal information; latent space enables diversity but adds stochasticity to training; adversarial training provides scalability but requires careful tuning for stability.

- Failure signatures: Poor metric matching (high JSD), lack of behavioral diversity (metrics cluster around single point), training instability (oscillating metrics), or failure to generalize to test levels.

- First 3 experiments:
  1. Verify that the aggregator ψ correctly computes and normalizes metrics from expert trajectories.
  2. Test pre-training of the discriminator with synthetic data to ensure it outputs reasonable probabilities.
  3. Run a short training loop with a simplified environment to verify the adversarial feedback loop functions correctly.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AILAD scale with increasing complexity of the reward function and state space dimensionality?
- Basis in paper: [inferred] The paper focuses on a specific 3D turn-based strategy game environment and does not explore the scalability of AILAD to more complex environments or reward functions.
- Why unresolved: The experimental evaluation is limited to a single game environment with a fixed set of metrics, making it difficult to generalize the results to other domains.
- What evidence would resolve it: Experiments evaluating AILAD on a range of environments with varying complexity, including high-dimensional state spaces and complex reward functions, would provide insights into its scalability.

### Open Question 2
- Question: How does the introduction of the latent space vector z affect the interpretability of the learned reward function and policy?
- Basis in paper: [explicit] The paper introduces a latent space vector z to enable diverse behaviors, but does not explore the interpretability of the learned reward function and policy in relation to z.
- Why unresolved: The latent space vector is used to generate diversity, but its relationship to the underlying reward function and policy is not well understood.
- What evidence would resolve it: Analysis of the learned reward function and policy, examining how they vary with different values of z, would provide insights into the interpretability of the model.

### Open Question 3
- Question: How does AILAD compare to other imitation learning methods that use aggregated data, such as CARMI, in terms of sample efficiency and computational cost?
- Basis in paper: [explicit] The paper compares AILAD to CARMI, but does not provide a detailed analysis of their sample efficiency and computational cost.
- Why unresolved: The comparison between AILAD and CARMI is limited to performance metrics, without considering the resources required for training and inference.
- What evidence would resolve it: Experiments measuring the sample efficiency and computational cost of AILAD and CARMI, including the number of interactions with the environment and the time required for training and inference, would provide a comprehensive comparison.

## Limitations
- Lack of specific neural network architecture details makes exact reproduction challenging
- Limited evaluation to single game environment restricts generalizability claims
- Missing analysis of computational cost and sample efficiency compared to baseline methods

## Confidence

**High confidence** in the core theoretical contribution: The integration of latent space for behavioral diversity with adversarial imitation learning on aggregated data represents a novel and technically sound approach.

**Medium confidence** in experimental results: The results show clear improvements over CARMI on the specified metrics, but the lack of architecture details and potential hyperparameter tuning effects reduce reproducibility confidence.

**Low confidence** in generalization claims: The paper claims better generalization to test levels, but only one test level was used, and the extent of behavioral matching beyond measured metrics remains unclear.

## Next Checks

1. **Architecture replication study**: Implement the method with multiple neural network architectures (varying layer sizes and activation functions) to test robustness of the reported improvements.

2. **Metric ablation analysis**: Systematically remove individual metrics from the aggregated data to determine which contribute most to successful imitation and whether all 7 metrics are necessary.

3. **Latent space analysis**: Visualize the behavioral space by sampling multiple z vectors and plotting the resulting metric distributions to verify that the latent space actually induces meaningful behavioral diversity.