---
ver: rpa2
title: 'Dialogue Shaping: Empowering Agents through NPC Interaction'
arxiv_id: '2307.15833'
source_url: https://arxiv.org/abs/2307.15833
tags:
- agent
- game
- chatgpt
- dialogue
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of slow convergence in reinforcement
  learning agents when playing text-based games, particularly due to extensive action
  spaces and sparse rewards. The authors propose Dialogue Shaping, a framework that
  uses large language models (LLMs) to interact with non-player characters (NPCs)
  to extract key game information, which is then incorporated into the agent's training
  process via knowledge graphs and Story Shaping.
---

# Dialogue Shaping: Empowering Agents through NPC Interaction

## Quick Facts
- arXiv ID: 2307.15833
- Source URL: https://arxiv.org/abs/2307.15833
- Reference count: 11
- Key outcome: Dialogue Shaping accelerates RL convergence in text games using LLM-extracted NPC information via knowledge graphs

## Executive Summary
This paper addresses the challenge of slow convergence in reinforcement learning agents when playing text-based games, particularly due to extensive action spaces and sparse rewards. The authors propose Dialogue Shaping, a framework that uses large language models (LLMs) to interact with non-player characters (NPCs) to extract key game information, which is then incorporated into the agent's training process via knowledge graphs and Story Shaping. The approach involves using ChatGPT as both an NPC to provide game information and as an agent to retrieve this information through conversation, generating a target knowledge graph that guides the RL agent's learning.

## Method Summary
The method involves running dialogue between a ChatGPT agent and NPC to extract game-critical information, which is converted into a target knowledge graph. This graph is used by Story Shaping to provide additional reward signals during KGA2C training, encouraging the agent's internal knowledge graph to align with the target. The framework aims to accelerate learning by providing the agent with key information before it learns it through trial-and-error exploration.

## Key Results
- Dialogue Shaping significantly accelerates learning, with KGA2C agents converging to optimal policies much faster (e.g., 10,000 steps vs. 90,000 steps in one game)
- Consistently achieves higher scores throughout training compared to baseline
- Demonstrates effectiveness across three custom text games in the LIGHT environment

## Why This Works (Mechanism)

### Mechanism 1
Dialogue shaping accelerates RL convergence by providing the agent with game-critical information before it learns it through trial-and-error. ChatGPT (acting as both NPC and agent) retrieves key game information via conversation, which is then converted into a target knowledge graph. This graph is used by Story Shaping to provide additional reward signals that guide the RL agent toward optimal actions faster than standard exploration. Core assumption: The key information needed to solve the game can be extracted through structured dialogue with an NPC simulated by ChatGPT.

### Mechanism 2
Story Shaping uses the target knowledge graph to provide additional reward signals that guide the RL agent's internal knowledge graph toward the target state. At each training step, the RL agent receives an extra reward based on the similarity between its current internal knowledge graph (representing its understanding of the game state) and the target knowledge graph (representing the goal state). This encourages the agent to perform actions that make its internal KG more similar to the target KG. Core assumption: The internal knowledge graph accurately represents the agent's current state and understanding of the game.

### Mechanism 3
Using ChatGPT as both NPC and agent creates a closed-loop information retrieval system that can be controlled through prompting. The same LLM (ChatGPT) is prompted in two different roles - as an NPC providing game information, and as an agent asking questions to retrieve that information. This dual-role setup allows for controlled experimentation with dialogue strategies. Core assumption: ChatGPT can reliably act as both NPC and agent when given appropriate prompts, and the information flow between these roles can be controlled.

## Foundational Learning

- **Concept: Reinforcement Learning (RL) and Q-learning**
  - Why needed here: The paper uses KGA2C, which is an RL agent that combines Advantage Actor Critic methods with knowledge graph guidance. Understanding RL basics is crucial for understanding how the agent learns and how the additional reward signals from Story Shaping affect learning.
  - Quick check question: What is the difference between on-policy and off-policy RL algorithms, and which category does KGA2C fall into?

- **Concept: Knowledge Graphs and Graph Embeddings**
  - Why needed here: The paper uses knowledge graphs to represent both the agent's current state (internal KG) and the goal state (target KG). Understanding how to represent and compare KGs is crucial for understanding how Story Shaping works.
  - Quick check question: How would you measure the similarity between two knowledge graphs, and what graph embedding techniques could be used for this purpose?

- **Concept: Prompt Engineering for LLMs**
  - Why needed here: The paper relies heavily on prompting ChatGPT to act as both NPC and agent, and to generate the target knowledge graph. Understanding prompt engineering techniques is crucial for replicating and extending this work.
  - Quick check question: What are some best practices for prompting LLMs to maintain a consistent persona or role, and how would you structure prompts to elicit specific types of information?

## Architecture Onboarding

- **Component map**: Text-based game environment -> ChatGPT sessions (NPC and agent) -> Dialogue module -> Knowledge graph module -> KGA2C RL agent -> Story Shaping module

- **Critical path**: 
  1. Initialize game and knowledge graphs
  2. Run dialogue between ChatGPT agent and NPC to generate target KG
  3. Train KGA2C agent with standard rewards and Story Shaping rewards
  4. Evaluate agent performance

- **Design tradeoffs**:
  - Using ChatGPT for dialogue vs. using a simpler rule-based system: ChatGPT provides more natural and potentially more informative dialogue, but is slower and more expensive.
  - Generating target KG through dialogue vs. hard-coding it: Dialogue allows for more flexible and potentially more complete information gathering, but introduces variability and potential errors.
  - Using Story Shaping vs. relying solely on game rewards: Story Shaping can accelerate learning, but requires careful design of the KG similarity measure and reward scaling.

- **Failure signatures**:
  - Agent fails to learn: Check if target KG is complete and accurate, if Story Shaping rewards are properly scaled, and if the game itself is solvable.
  - Dialogue fails to extract key information: Check if ChatGPT prompts are well-designed, if the NPC has the necessary knowledge, and if the agent is asking good questions.
  - Inconsistent performance across seeds: Check if the dialogue generation is introducing too much variability, or if the KG similarity measure is unstable.

- **First 3 experiments**:
  1. Run baseline KGA2C agent without Story Shaping to establish performance without dialogue assistance.
  2. Run full Dialogue Shaping pipeline on a simple game with a clear solution path to verify the basic mechanism works.
  3. Test the robustness of the dialogue module by running multiple dialogue sessions for the same game and comparing the generated target KGs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we optimize the dialogue between ChatGPT agent and NPC to minimize the number of exchanges while retrieving the most relevant game information?
- Basis in paper: The paper mentions that the authors aim to minimize the number of exchanges with ChatGPT NPC while retrieving hints on winning the game. They found that ChatGPT agent is more likely to hallucinate without explicit instructions on how to ask optimal questions.
- Why unresolved: The paper does not provide a detailed analysis of the optimal question-asking strategy or a comparison of different prompting techniques to achieve this goal.
- What evidence would resolve it: A study comparing the efficiency of different prompting techniques and question-asking strategies in terms of the number of exchanges and the relevance of information retrieved.

### Open Question 2
- Question: Can the dialogue module be further improved to handle more complex text games with larger action spaces and more intricate storylines?
- Basis in paper: The paper demonstrates the effectiveness of the dialogue module in simple custom text games created in the LIGHT environment. However, it does not explore the module's performance in more complex games.
- Why unresolved: The paper does not provide any evidence of the dialogue module's scalability or its ability to handle more complex game environments.
- What evidence would resolve it: Experiments testing the dialogue module's performance in more complex text games with larger action spaces and intricate storylines.

### Open Question 3
- Question: How does the performance of the Dialogue Shaping approach compare to other methods that incorporate external knowledge or information retrieval techniques in reinforcement learning?
- Basis in paper: The paper introduces the Dialogue Shaping approach and demonstrates its effectiveness in accelerating RL agent convergence. However, it does not compare this approach to other methods that incorporate external knowledge or information retrieval techniques.
- Why unresolved: The paper does not provide a comparative analysis of the Dialogue Shaping approach with other state-of-the-art methods in the field.
- What evidence would resolve it: A comparative study of the Dialogue Shaping approach with other methods that incorporate external knowledge or information retrieval techniques in terms of convergence speed, final performance, and robustness.

## Limitations

- The effectiveness depends heavily on prompt quality and consistency of ChatGPT responses, with no evidence of reproducibility across different sessions or versions
- The knowledge graph similarity measure used in Story Shaping is not specified, making it unclear whether additional reward signals are meaningful or could mislead the agent
- Experiments were conducted on only three custom games, limiting generalizability to other text-based environments

## Confidence

- **High confidence**: The general framework of using LLM-facilitated dialogue to extract game information and incorporating it into RL training is sound and well-explained
- **Medium confidence**: The empirical results showing faster convergence with Story Shaping, as the exact prompts, knowledge graph construction details, and similarity measures are not specified
- **Low confidence**: Claims about the specific mechanisms by which Dialogue Shaping accelerates learning, particularly the quality and completeness of information extracted through ChatGPT dialogues

## Next Checks

1. **Reproducibility check**: Run multiple dialogue sessions for the same game using different ChatGPT instances and compare the generated target knowledge graphs for consistency and completeness.

2. **Ablation study**: Test variants of the approach with different knowledge graph similarity measures (e.g., graph edit distance, node overlap, edge overlap) to determine which most effectively guides the RL agent.

3. **Generalization test**: Apply the Dialogue Shaping framework to a different text-based game environment (e.g., Jericho) to assess whether the approach transfers beyond the custom LIGHT games used in the current experiments.