---
ver: rpa2
title: 'Entity Embeddings : Perspectives Towards an Omni-Modality Era for Large Language
  Models'
arxiv_id: '2310.18390'
source_url: https://arxiv.org/abs/2310.18390
tags:
- modality
- language
- tokens
- arxiv
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper discusses the evolution of Large Language Models (LLMs)
  towards integrating multiple modalities, such as text, image, and audio, into a
  unified linguistic space. The authors introduce the concept of "entity embeddings,"
  where conceptual entities defined in sequences of text can be imagined as modalities.
---

# Entity Embeddings : Perspectives Towards an Omni-Modality Era for Large Language Models

## Quick Facts
- arXiv ID: 2310.18390
- Source URL: https://arxiv.org/abs/2310.18390
- Reference count: 40
- The paper introduces the concept of "entity embeddings," where conceptual entities defined in sequences of text can be imagined as modalities to overcome cognitive and computational limitations of current LLMs.

## Executive Summary
This paper explores the evolution of Large Language Models (LLMs) towards integrating multiple modalities into a unified linguistic space. The authors propose treating conceptual entities as modalities, enabling context compression and richer semantic representation. They introduce the concept of "entity embeddings" and discuss two recent architectures, AnyMAL and Kosmos-I, that conform to this tokenized, multi-modal context. The paper highlights the potential benefits and challenges of an omni-modal era for LLMs, emphasizing the need for further research and development in this area.

## Method Summary
The paper proposes an "omni-modal" approach for LLMs where conceptual entities defined in sequences of text can be imagined as modalities. This involves developing modality-specific encoders for various entity types (geography, numbers, dates, corporations, etc.) that project information into a common linguistic embedding space. The approach leverages recent architectures like AnyMAL and Kosmos-I, which use projection layers or interleavable token sequences for multi-modal data. The method requires mechanisms for entity classification, encoder design, and handling nested modality compositions to achieve richer semantic representation while managing computational feasibility.

## Key Results
- Entity embeddings enable context compression by treating semantic entities as modalities rather than individual tokens.
- Nested and iterative encoding of entities increases cognitive power without exploding context length.
- Treating numbers and dates as modalities offers a potential solution to numerical reasoning limitations in current LLMs.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entity embeddings enable context compression by treating semantic entities as modalities.
- Mechanism: Instead of encoding individual tokens, modality-specific encoders transform entire entities (e.g., a geographic region, a corporation) into compact embeddings that carry dense semantic information.
- Core assumption: LLMs can learn to decode rich, high-level information from modality tokens if trained jointly on both entity and linguistic embeddings.
- Evidence anchors:
  - [abstract] "conceptual entities defined in sequences of text can also be imagined as modalities."
  - [section] "Rather than encoding individual tokens of the raw text, a pre-trained geographical encoder can generate modality tokens to feed in the joint linguistic space."
  - [corpus] Weak - corpus focuses on multi-modal models but does not directly discuss entity-as-modality compression.
- Break condition: If the modality encoder fails to capture sufficient semantic depth, the LLM cannot recover meaningful information, defeating the compression purpose.

### Mechanism 2
- Claim: Nested and iterative encoding of entities increases cognitive power without exploding context length.
- Mechanism: Entity embeddings can themselves contain tokens of other modalities, allowing recursive composition (e.g., a "company" entity embedding includes location, leadership, and date tokens).
- Core assumption: Modality encoders and LLMs can be jointly trained to handle arbitrarily deep nesting without loss of coherence.
- Evidence anchors:
  - [section] "The possibility of recursive and interconnected utilization of the entity embeddings is discussed."
  - [section] "The data given to a geospatial encoder can contain other geospatial entities along with other modalities such as imagery."
  - [corpus] Weak - corpus neighbors do not discuss nested modality encoding.
- Break condition: Excessive nesting may cause training instability or create semantic ambiguity that the LLM cannot resolve.

### Mechanism 3
- Claim: Treating numbers and dates as modalities solves numerical reasoning limitations in LLMs.
- Mechanism: Specialized encoders convert numeric and temporal expressions into embeddings that the LLM can manipulate algebraically or chronologically.
- Core assumption: Temporal and numeric encoders can preserve relational structure in the embedding space.
- Evidence anchors:
  - [section] "Approaching numbers as an entity might solve this problem under this omni-modal context."
  - [section] "Date and time inherently have algebraic nature, so numerical limitation of current LLMs to construct relationships exists for them as well."
  - [corpus] Weak - corpus does not mention numeric or temporal modality encoding.
- Break condition: If the encoder fails to encode relational structure, the LLM will still perform poorly on arithmetic or chronological tasks.

## Foundational Learning

- Concept: Modality-specific encoders
  - Why needed here: To convert diverse entity types (geography, numbers, dates, corporations, etc.) into tokens aligned with the LLM's embedding space.
  - Quick check question: How does a geospatial encoder differ from a CLIP image encoder in its output space alignment?

- Concept: Joint embedding alignment
  - Why needed here: Ensures that tokens from different modalities are comparable and combinable in the same latent space.
  - Quick check question: What training objective ensures modality tokens are properly aligned with language tokens?

- Concept: Recursive modality composition
  - Why needed here: Enables embedding of complex entities that themselves contain other modalities, increasing representational power.
  - Quick check question: How would you represent a "company" entity that includes nested "location" and "date" modality tokens?

## Architecture Onboarding

- Component map:
  - Modality-specific encoders (geography, numbers, dates, corporations, etc.)
  - LLM backbone (e.g., transformer)
  - Token alignment layers (projection or joint embedder)
  - Classification mechanism for entity extraction
  - Nested modality handling pipeline
  - Decoder modules (if generation is required)

- Critical path:
  1. Extract entity spans from text.
  2. Classify entity type.
  3. Route to appropriate modality encoder.
  4. Align encoded tokens to LLM embedding space.
  5. Interleave with regular language tokens.
  6. Process through LLM layers.
  7. Decode response or entity information.

- Design tradeoffs:
  - Fine-grained vs. coarse entity categories (granularity vs. encoder complexity).
  - Static vs. dynamic entity classification (simpler but less flexible vs. more capable but complex).
  - Pre-training vs. joint training of encoders (efficiency vs. tighter alignment).

- Failure signatures:
  - Poor entity classification → irrelevant or wrong modality tokens.
  - Misaligned embeddings → degraded LLM performance.
  - Overly nested structures → training instability or incoherent outputs.
  - Insufficient encoder capacity → loss of important entity details.

- First 3 experiments:
  1. Train a simple geographic encoder on OpenStreetMap data and verify that the LLM can answer distance queries.
  2. Implement number modality encoding and test basic arithmetic reasoning.
  3. Combine geographic and number encoders in a nested setting and measure the LLM's ability to reason over mixed entity types.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can a universal entity classification system be designed to automatically identify and categorize entity types (e.g., geospatial, temporal, corporate) in multi-modal datasets for training omni-modal LLMs?
- Basis in paper: [explicit] The paper discusses the challenge of classifying entities in datasets for omni-modal LLMs, noting that unlike explicit modalities like images, proposed entity-based modalities are not explicit and need to be extracted from text or multi-modal data.
- Why unresolved: Current NER algorithms may not suffice for a universal, scalable system that can handle diverse and intricate entities across various modalities.
- What evidence would resolve it: A demonstrated framework or algorithm that can accurately and efficiently classify a wide range of entity types in diverse multi-modal datasets, validated on real-world data.

### Open Question 2
- Question: What are the optimal design principles for modality-specific encoders that can project complex entity information (e.g., geospatial, corporate) into the joint linguistic embedding space while preserving the ability to perform entity-specific reasoning tasks?
- Basis in paper: [explicit] The paper highlights the need for carefully designed and trained encoders for conceptual entities like geolocations or numbers to project information into the common linguistic dimension of the LLM.
- Why unresolved: The paper does not provide specific design principles or architectures for these encoders, and the optimal balance between information preservation and reasoning capability is unclear.
- What evidence would resolve it: Empirical studies comparing different encoder architectures and training approaches on benchmark tasks requiring entity-specific reasoning, demonstrating superior performance for a proposed design.

### Open Question 3
- Question: How can the trade-off between the granularity of entity categorization (finer vs. coarser concepts) and the feasibility of managing a large number of modality encoders and training datasets be effectively navigated in omni-modal LLMs?
- Basis in paper: [explicit] The paper discusses the challenge of the inclusiveness-feasibility trade-off on the definition of modalities, noting that finer concepts would require an exponentially larger number of modality encoders, dataset curation, and training regimen.
- Why unresolved: The paper does not provide a clear methodology for determining the optimal granularity of entity categorization, and the impact of this choice on model performance and scalability is unknown.
- What evidence would resolve it: A comprehensive analysis of the relationship between entity granularity, the number of required encoders and datasets, and model performance across various tasks, providing guidelines for making informed decisions.

## Limitations
- The concept of entity embeddings as modalities remains largely theoretical with limited empirical validation.
- The recursive and nested encoding approach introduces significant complexity that could lead to training instability or semantic ambiguity.
- The computational overhead of maintaining multiple modality-specific encoders and the trade-offs between model inclusiveness and practical feasibility are not thoroughly quantified.

## Confidence
- High confidence in the observation that current LLMs face limitations in handling complex entity relationships and numerical reasoning.
- Medium confidence in the proposed mechanism of treating entities as modalities, as this is conceptually sound but lacks direct empirical support.
- Low confidence in the practical implementation details for recursive modality composition and the specific architectural requirements for nested encoding, as these remain largely speculative.

## Next Checks
1. Implement a simple geographic encoder trained on OpenStreetMap data and conduct controlled experiments to measure whether LLM performance on location-based queries improves when using entity embeddings versus traditional token-based approaches. Track both accuracy and computational efficiency metrics.

2. Design and execute a numerical reasoning benchmark comparing LLMs with standard token encoding against those using dedicated number modality encoders. Test across arithmetic operations, date calculations, and temporal reasoning tasks to quantify the claimed benefits of numeric modality encoding.

3. Create a prototype system that combines two or three different modality encoders (e.g., geographic, numeric, and temporal) and evaluate the model's ability to reason across these combined entities. Measure performance degradation or improvement as nesting depth increases, and identify the breaking point where the approach becomes unstable or incoherent.