---
ver: rpa2
title: The NUS-HLT System for ICASSP2024 ICMC-ASR Grand Challenge
arxiv_id: '2312.16002'
source_url: https://arxiv.org/abs/2312.16002
tags:
- speech
- data
- system
- track
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes the NUS-HLT team's systems for the ICASSP2024
  ICMC-ASR Grand Challenge, which focuses on in-car multi-channel automatic speech
  recognition. The team's approach includes multi-channel front-end enhancement and
  diarization, training data augmentation, and speech recognition modeling with multi-channel
  branches.
---

# The NUS-HLT System for ICASSP2024 ICMC-ASR Grand Challenge

## Quick Facts
- arXiv ID: 2312.16002
- Source URL: https://arxiv.org/abs/2312.16002
- Reference count: 0
- Primary result: Best system achieves 34.3% relative CER improvement and 56.5% relative cpCER improvement over baseline

## Executive Summary
This paper presents the NUS-HLT team's approach to the ICASSP2024 ICMC-ASR Grand Challenge, focusing on in-car multi-channel automatic speech recognition. The system employs multi-channel front-end enhancement and diarization, training data augmentation, and speech recognition modeling with multi-channel branches. For track I (automatic speech recognition), the system uses various speech enhancement and separation models (DCCRN, BSRNN, GSS, IVA) combined with data augmentation techniques like room impulse response simulation and speed perturbation. The back-end ASR model utilizes a joint HuBERT-E-Branchformer architecture. For track II (speech diarization and recognition), the system combines clustering-based methods with a modified transformer-based TS-VAD framework leveraging multi-channel audio inputs and speaker embeddings from CAM++. Tested on official Eval1 and Eval2 sets, the best system demonstrates significant performance improvements over the baseline.

## Method Summary
The NUS-HLT system employs a multi-stage approach for in-car multi-channel ASR. First, noisy multi-channel audio is processed through several speech enhancement and separation models (DCCRN, BSRNN, GSS, IVA) trained on far-field data mixed with official noise. Next, training data is augmented through Room Impulse Response (RIR) simulation using pyroomacoustic, real noise addition with SNR 0-10dB, and speed perturbation (0.9-1.1) combined with Spec-Augmentation. The back-end ASR model uses a joint HuBERT-E-Branchformer architecture with CTC-Attention hybrid modeling. For diarization in track II, the system employs clustering-based methods with CAM++ speaker embeddings followed by a modified TS-VAD framework that replaces BLSTM with transformer encoders.

## Key Results
- 34.3% relative improvement in Character Error Rate (CER) on Eval1 set
- 56.5% relative improvement in confusion pair Character Error Rate (cpCER) on Eval1 set
- 31.4% relative improvement in CER on Eval2 set
- 49.8% relative improvement in cpCER on Eval2 set

## Why This Works (Mechanism)

### Mechanism 1
Multi-channel front-end enhancement improves ASR performance by mitigating noise and reverberation in in-car recordings. The system applies several speech enhancement and separation models (DCCRN, BSRNN, GSS, IVA) to process noisy multi-channel audio before ASR. These models use far-field training data mixed with official noise to learn noise suppression and speaker separation, improving the quality of input to the ASR model.

### Mechanism 2
Data augmentation through RIR simulation and speed perturbation increases ASR robustness. The system simulates far-field speech from close-talk data using Room Impulse Response (RIR) with pyroomacoustic, adds real noise with SNR 0-10dB, and applies speed perturbation (0.9-1.1) and Spec-Augmentation during training. This creates diverse training data that matches the characteristics of the target in-car environment.

### Mechanism 3
Multi-channel diarization using modified TS-VAD with CAM++ embeddings improves cpCER by better speaker separation. For track II, the system uses a clustering-based method followed by a modified TS-VAD framework that replaces BLSTM with transformer encoders and uses CAM++ speaker embeddings instead of i-vectors. This approach better handles overlapping speech and multiple speakers in multi-channel recordings.

## Foundational Learning

- **Speech enhancement and separation fundamentals**
  - Why needed: Understanding how DCCRN, BSRNN, GSS, and IVA models work is crucial for diagnosing front-end processing issues
  - Quick check: What's the difference between speech enhancement (noise reduction) and speech separation (source separation), and when would you use each?

- **Data augmentation techniques for ASR**
  - Why needed: RIR simulation, speed perturbation, and Spec-Augmentation are key components of the training pipeline
  - Quick check: How does Room Impulse Response simulation affect the frequency and temporal characteristics of speech, and why is this useful for far-field ASR?

- **Speaker diarization and VAD principles**
  - Why needed: The modified TS-VAD and CAM++ embedding approach is central to track II performance
  - Quick check: What are the main challenges in speaker diarization for overlapping speech, and how does the modified TS-VAD approach address them?

## Architecture Onboarding

- **Component map**: Multi-channel audio input → Front-end enhancement (DCCRN, BSRNN, GSS, IVA) → RIR simulation and data augmentation → HuBERT-E-Branchformer ASR model (or modified TS-VAD for diarization)
- **Critical path**: Audio preprocessing → Enhancement → ASR model inference
- **Design tradeoffs**: Multiple enhancement models provide robustness but increase computational complexity; the modified TS-VAD offers better diarization but may be slower than simpler approaches
- **Failure signatures**: High CER/cpCER on Eval sets despite good Dev performance suggests overfitting or data mismatch; poor diarization results indicate issues with speaker embedding quality or TS-VAD configuration
- **First 3 experiments**:
  1. Ablation study: Test ASR performance with and without each front-end enhancement model to identify the most beneficial components
  2. Data augmentation analysis: Compare ASR performance using different RIR simulation parameters and noise levels to find optimal augmentation settings
  3. Diarization refinement: Experiment with different iterations of TS-VAD and speaker embedding extraction methods to improve segmentation accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different multi-channel enhancement techniques compare in effectiveness for in-car speech recognition, particularly when handling overlapping speech from multiple speakers?
- Basis in paper: The paper employs multiple enhancement techniques (DCCRN, BSRNN, GSS, IVA) but doesn't systematically compare their relative performance or investigate optimal combinations for overlapping speech scenarios
- Why unresolved: The paper mentions using these techniques but doesn't provide comparative analysis of their individual or combined effectiveness, particularly in challenging overlapping speech conditions
- What evidence would resolve it: Systematic ablation studies comparing each enhancement technique's performance both individually and in combination, with detailed analysis of their effectiveness in handling overlapping speech scenarios

### Open Question 2
- Question: What is the optimal approach for integrating speaker diarization with speech recognition in multi-channel in-car environments?
- Basis in paper: The paper combines clustering-based methods with modified TS-VAD but doesn't explore alternative integration strategies or investigate the impact of different diarization approaches on overall recognition performance
- Why unresolved: While the paper presents a working integration method, it doesn't explore the design space of different diarization-recognition integration strategies or their relative effectiveness
- What evidence would resolve it: Comparative studies of different diarization-recognition integration approaches, including end-to-end systems, with detailed analysis of their performance trade-offs in multi-channel in-car scenarios

### Open Question 3
- Question: How does the choice of training data augmentation techniques impact model robustness across different in-car acoustic environments?
- Basis in paper: The paper employs various augmentation techniques (RIR simulation, speed perturbation, SpecAugment) but doesn't systematically evaluate their individual contributions to model robustness
- Why unresolved: The paper uses multiple augmentation techniques but doesn't provide detailed analysis of their individual or combined effects on model performance across different acoustic conditions
- What evidence would resolve it: Controlled experiments isolating the impact of individual augmentation techniques on model performance across diverse in-car acoustic scenarios, including analysis of their effectiveness in different noise conditions

## Limitations

- Specific training recipes and hyperparameters for front-end enhancement models (DCCRN, BSRNN, GSS, IVA) are not provided
- Implementation details of the modified TS-VAD framework with CAM++ embeddings and cross-attention module are insufficient
- Limited analysis of individual component contributions through ablation studies
- No systematic evaluation of different diarization-recognition integration strategies

## Confidence

- **High Confidence**: The overall system architecture and general approach are well-described and align with established practices in multi-channel ASR
- **Medium Confidence**: The effectiveness of the multi-channel front-end enhancement pipeline is supported by reported performance improvements, but specific contributions of each model remain unclear
- **Low Confidence**: The specific implementation details for front-end enhancement models and modified TS-VAD framework are insufficient for exact reproduction

## Next Checks

1. **Ablation study on front-end enhancement**: Systematically evaluate ASR performance with individual front-end models (DCCRN, BSRNN, GSS, IVA) disabled to quantify their individual contributions and identify the most critical components

2. **Data augmentation sensitivity analysis**: Conduct experiments varying RIR simulation parameters (room size, absorption coefficients) and noise levels (SNR ranges) to determine optimal augmentation settings for the in-car environment

3. **Diarization pipeline validation**: Test different iterations of TS-VAD and compare CAM++ embeddings against alternative speaker embedding methods (x-vectors, ECAPA-TDNN) to isolate the sources of diarization performance gains