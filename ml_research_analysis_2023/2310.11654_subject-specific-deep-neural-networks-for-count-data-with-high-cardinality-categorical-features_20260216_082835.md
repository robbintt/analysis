---
ver: rpa2
title: Subject-specific Deep Neural Networks for Count Data with High-cardinality
  Categorical Features
arxiv_id: '2310.11654'
source_url: https://arxiv.org/abs/2310.11654
tags:
- data
- random
- effects
- subject-specific
- proposed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a hierarchical likelihood approach to introduce
  gamma random effects into Poisson deep neural networks (DNNs) for clustered count
  data with high-cardinality categorical features. The method improves prediction
  by capturing both nonlinear effects of input variables and subject-specific cluster
  effects.
---

# Subject-specific Deep Neural Networks for Count Data with High-cardinality Categorical Features

## Quick Facts
- arXiv ID: 2310.11654
- Source URL: https://arxiv.org/abs/2310.11654
- Reference count: 14
- Key outcome: Proposes hierarchical likelihood approach with gamma random effects in Poisson DNNs for clustered count data with high-cardinality categorical features, achieving improved prediction by capturing both nonlinear effects and subject-specific cluster effects

## Executive Summary
This paper introduces a hierarchical likelihood framework that incorporates gamma random effects into Poisson deep neural networks for clustered count data with high-cardinality categorical features. The method enables simultaneous maximum likelihood estimation of fixed parameters and best unbiased prediction of random effects through a single optimization objective. The approach addresses the challenge of subject-specific prediction in count data by combining the nonlinear modeling power of deep neural networks with the ability to capture cluster-specific effects through gamma-distributed random effects. Experimental results demonstrate improved prediction accuracy compared to standard Poisson DNNs and traditional GLMMs.

## Method Summary
The method extends Poisson deep neural networks by introducing gamma-distributed random effects to capture subject-specific multiplicative effects in clustered count data. The hierarchical likelihood framework simultaneously optimizes fixed parameters and random effects through a single objective function, incorporating an adjustment procedure to maintain identifiability constraints. The model uses a multi-layer perceptron architecture with separate output nodes for marginal predictions and random effects, employing a negative h-likelihood loss function that includes the Poisson likelihood, gamma prior, and adjustment terms. The approach also includes a method-of-moments pretraining step for variance components to accelerate convergence.

## Key Results
- The proposed Poisson-gamma DNN achieves lower root mean squared Pearson error (RMSPE) compared to standard Poisson DNNs and GLMMs on both simulated and real datasets
- The adjustment procedure for random effect predictors prevents poor predictions due to local minima violating the identifiability constraint E(ui) = 1
- The method maintains prediction advantages even when random effects are mis-specified as normal rather than gamma distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The h-likelihood framework enables exact maximum likelihood estimation (MLE) of fixed parameters and best unbiased prediction (BUP) of random effects simultaneously by optimizing a single objective function.
- Mechanism: The method introduces a transformation of random effects combined with an adjustment term ci(θ; yi) that makes the log density of the transformed random effects independent of the fixed parameters, thus recovering the classical marginal likelihood when the random effects are at their mode.
- Core assumption: The sufficient condition that log f(ev*|y) = 0 holds for the proposed transformation and adjustment term.
- Evidence anchors:
  - [abstract] "The proposed method simultaneously yields maximum likelihood estimators for fixed parameters and best unbiased predictors for random effects by optimizing a single objective function."
  - [section] "A sufficient condition for h(θ, v) to yield exact MLEs of all the fixed parameters in θ is that fθ(ev*|y) is independent of θ"
  - [corpus] Weak - corpus papers discuss random effects in neural networks but don't directly address the h-likelihood approach or exact MLE/BUP properties.
- Break condition: If the sufficient condition log f(ev*|y) = 0 fails, or if the transformation doesn't properly account for the Jacobian terms, the h-likelihood may not yield exact MLEs.

### Mechanism 2
- Claim: The Poisson-gamma DNN captures both nonlinear effects of input variables and subject-specific cluster effects, improving prediction accuracy over models that only capture one or the other.
- Mechanism: The model combines a deep neural network (capturing nonlinear marginal effects) with gamma-distributed random effects (capturing subject-specific multiplicative effects), allowing the conditional mean to be expressed as the product of the marginal mean and the random effect.
- Core assumption: The gamma distribution is appropriate for modeling the subject-specific multiplicative effects, and the random effects are independent across subjects.
- Evidence anchors:
  - [abstract] "The method improves prediction by capturing both nonlinear effects of input variables and subject-specific cluster effects."
  - [section] "Thus, Poisson-gamma DNNs allow highly nonlinear relationship between input and output variables, but only provide the marginal predictions for µm_ij. Thus, Poisson DNN can be viewed as an extension of Poisson GLM with ηm_ij = xT_ijβ."
  - [corpus] Weak - corpus papers discuss random effects in neural networks but don't specifically address the combination of nonlinear DNN effects with gamma random effects for count data.
- Break condition: If the gamma distribution is misspecified, or if the random effects are not independent, the model's performance may degrade.

### Mechanism 3
- Claim: The adjustment procedure for random effect predictors prevents poor predictions due to local minima that violate the identifiability constraint E(ui) = 1.
- Mechanism: The adjustment shifts the random effect predictions to ensure their average is 1, and compensates by adjusting the intercept, thus maintaining the identifiability constraint while improving the h-likelihood.
- Core assumption: The local minimum found by the optimization algorithm violates the constraint E(ui) = 1, and the adjustment improves the h-likelihood.
- Evidence anchors:
  - [abstract] "To resolve this issue, we propose an adjustment to the random effect prediction that prevents from violation of the constraints for identifiability."
  - [section] "Theorem 1 shows that the adjustment (7) improves the random effect prediction."
  - [corpus] Weak - corpus papers discuss random effects in neural networks but don't specifically address the issue of local minima violating identifiability constraints or propose an adjustment procedure.
- Break condition: If the optimization consistently finds the global minimum that satisfies the constraint, or if the adjustment itself introduces bias, the procedure may not improve predictions.

## Foundational Learning

- Concept: Hierarchical generalized linear models (HGLMs) and their extension to include random effects from conjugate distributions.
  - Why needed here: The Poisson-gamma DNN is an extension of HGLMs to deep neural networks, so understanding HGLMs is crucial for grasping the proposed method.
  - Quick check question: What is the key difference between a standard GLM and an HGLM?

- Concept: The h-likelihood approach and its use for obtaining exact MLEs and BUPs simultaneously.
  - Why needed here: The proposed method relies on the h-likelihood to achieve its theoretical properties, so understanding the h-likelihood is essential.
  - Quick check question: What is the sufficient condition for the h-likelihood to yield exact MLEs of all fixed parameters?

- Concept: Gamma distribution and its properties, especially in the context of random effects for count data.
  - Why needed here: The random effects in the proposed model follow a gamma distribution, so understanding this distribution is important for interpreting the results.
  - Quick check question: What is the relationship between the gamma random effect ui and its log-transformed version vi in the model?

## Architecture Onboarding

- Component map:
  - Input layer: High-cardinality categorical features (zij) and other input features (xij)
  - Hidden layers: Multi-layer perceptron with leaky ReLU activation
  - Output nodes: Two separate nodes for marginal predictor (µm_ij) and random effect (vi)
  - Loss function: Negative h-likelihood incorporating Poisson likelihood, gamma prior, and adjustment term
  - Adjustment procedure: Post-processing step to ensure E(ui) = 1

- Critical path:
  1. Forward pass through the neural network to compute marginal predictor and random effects
  2. Compute the negative h-likelihood loss
  3. Backpropagate the loss and update parameters using stochastic optimization
  4. Apply the adjustment procedure to the random effect predictions

- Design tradeoffs:
  - Using gamma random effects allows multiplicative subject-specific effects but requires careful handling of the identifiability constraint
  - The adjustment procedure improves predictions but adds complexity to the training pipeline
  - The method-of-moments estimator for pretraining variance components accelerates convergence but may not always provide the optimal starting point

- Failure signatures:
  - Poor predictions due to local minima that violate the identifiability constraint
  - Slow convergence of the variance component estimation
  - Overfitting when the number of subjects is small relative to the number of parameters

- First 3 experiments:
  1. Compare the performance of the Poisson-gamma DNN with and without the adjustment procedure on a simulated dataset with known subject-specific effects.
  2. Investigate the impact of different network architectures (e.g., number of hidden layers, activation functions) on the model's performance.
  3. Evaluate the model's ability to handle high-cardinality categorical features by varying the number of subjects and cluster sizes in the experiments.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed adjustment to random effect predictors (ensuring Pn_i=1 bu_i / n = 1) quantitatively impact prediction accuracy across different cluster sizes and variance components?
- Basis in paper: [explicit] The paper presents Theorem 1 proving the adjustment improves local h-likelihood and shows visual comparisons in Figure 2 for n=100, q=100.
- Why unresolved: The paper provides limited experimental validation (only one scenario in Figure 2). The quantitative impact across varying cluster sizes, variance components, and distributions of random effects remains unclear.
- What evidence would resolve it: Systematic experiments varying n, q, λ, and random effect distributions showing RMSPE differences with and without adjustment.

### Open Question 2
- Question: Does the proposed PG-NN framework maintain prediction advantages when the gamma random effects assumption is severely violated (e.g., highly skewed or multimodal distributions)?
- Basis in paper: [inferred] The paper notes in Table 1 that PG-NN performs best even when random effects are mis-specified as normal rather than gamma, but this is only one alternative distribution tested.
- Why unresolved: The paper only compares gamma vs. normal random effects. Real-world data may exhibit more complex distributions that could challenge the framework's robustness.
- What evidence would resolve it: Experiments with simulated data using various non-gamma random effect distributions (e.g., lognormal, bimodal, heavy-tailed) and real data where random effect distributions are empirically examined.

### Open Question 3
- Question: What is the computational complexity of the PG-NN framework compared to traditional DNNs and GLMMs, and how does it scale with high-cardinality categorical features?
- Basis in paper: [explicit] The paper claims the framework enables "fast end-to-end algorithm" and "easily incorporates state-of-the-art architectures," but provides no computational benchmarks or complexity analysis.
- Why unresolved: No runtime comparisons are provided between PG-NN, traditional DNNs, and GLMM implementations across datasets of varying size and complexity.
- What evidence would resolve it: Benchmarking studies measuring training/inference times and memory usage for PG-NN versus competing methods across datasets with different n, p, q, and cardinality of categorical features.

## Limitations
- The sufficient condition for exact MLE properties (log f(ev*|y) = 0) is verified empirically rather than proven for the specific transformation used
- The adjustment procedure for random effects adds complexity and may introduce bias if the optimization consistently finds the global minimum
- Performance with extremely high-cardinality features (thousands of categories) or very small cluster sizes remains unclear

## Confidence
- Mechanism 1 (Exact MLE/BUP via h-likelihood): **Medium** - Theoretical framework is sound but sufficient condition verification is empirical
- Mechanism 2 (Combined nonlinear and random effects): **High** - Well-established statistical principles, empirical results support the claim
- Mechanism 3 (Adjustment procedure for identifiability): **Medium** - Theoretical justification exists but practical implementation details and potential biases are not fully explored

## Next Checks
1. Verify the sufficient condition log f(ev*|y) = 0 holds numerically across multiple datasets and network architectures to confirm exact MLE properties
2. Conduct ablation studies comparing models with and without the adjustment procedure across datasets with varying cluster sizes to assess practical impact
3. Test model performance with high-cardinality categorical features containing 1,000+ categories to evaluate scalability limits