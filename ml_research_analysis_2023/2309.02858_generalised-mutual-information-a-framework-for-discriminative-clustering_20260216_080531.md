---
ver: rpa2
title: 'Generalised Mutual Information: a Framework for Discriminative Clustering'
arxiv_id: '2309.02858'
source_url: https://arxiv.org/abs/2309.02858
tags:
- clustering
- data
- clusters
- distribution
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses limitations of mutual information (MI) maximization
  as a clustering objective by proposing the Generalised Mutual Information (GEMINI),
  which replaces the Kullback-Leibler divergence with other distances. GEMINI offers
  two variants: One-vs-All (OvA) and One-vs-One (OvO), utilizing f-divergences, Maximum
  Mean Discrepancy (MMD), or Wasserstein distances.'
---

# Generalised Mutual Information: a Framework for Discriminative Clustering

## Quick Facts
- arXiv ID: 2309.02858
- Source URL: https://arxiv.org/abs/2309.02858
- Authors: 
- Reference count: 40
- Key outcome: GEMINI framework outperforms MI-based clustering on MNIST/CIFAR10 using MMD and Wasserstein distances, with automatic cluster number selection

## Executive Summary
This paper addresses limitations of mutual information maximization for clustering by introducing Generalised Mutual Information (GEMINI), which replaces the KL divergence with alternative distances like MMD and Wasserstein. The framework offers two variants: One-vs-All (OvA) comparing cluster distributions to the data distribution, and One-vs-One (OvO) comparing cluster distributions directly to each other. Experiments demonstrate that GEMINI variants achieve higher Adjusted Rand Index scores than MI-based methods on MNIST and CIFAR10, while also automatically selecting relevant numbers of clusters during training.

## Method Summary
GEMINI extends mutual information by replacing the KL divergence with other statistical distances including f-divergences (Hellinger, Total Variation), Maximum Mean Discrepancy (MMD), and Wasserstein distances. The framework is implemented in a Python package (gemclus) and tested using MLP and LeNet-5 architectures on MNIST and CIFAR10. Training uses stochastic gradient descent to maximize GEMINI objectives, with performance evaluated using Adjusted Rand Index and analysis of cluster selection behavior through entropy maps and counting non-empty clusters.

## Key Results
- MMD-GEMINI and Wasserstein-GEMINI outperform KL-MI and KL-GEMINI variants on MNIST and CIFAR10
- One-vs-One (OvO) variants generally achieve higher ARI scores than One-vs-All (OvA) variants
- GEMINI automatically selects relevant numbers of clusters, with MLP discovering more clusters than LeNet-5 on MNIST
- The method achieves ARI scores of 0.47 (OvO-MMD) and 0.51 (OvO-Wasserstein) on MNIST, compared to 0.45 for KL-MI

## Why This Works (Mechanism)

### Mechanism 1
Replacing the KL divergence in mutual information with other distances improves clustering because the KL divergence can lead to poor decision boundaries when the clustering model converges to a Dirac distribution. The KL divergence measures the difference between conditional and marginal distributions, and when the model becomes overconfident, it maximizes regardless of actual data geometry. Distances like MMD or Wasserstein incorporate data space geometry through kernels or distances, leading to more meaningful clustering. This works when the data has clear geometric structure and the chosen distance captures relevant features.

### Mechanism 2
The One-vs-One (OvO) GEMINI is better than One-vs-All (OvA) for clustering because it compares cluster distributions directly against each other rather than against the entire data distribution. In OvA, if one cluster distribution resembles the data distribution, its distance to the data distribution could be zero, making it unnoticed when maximizing the OvA GEMINI. OvO ensures all clusters are considered by comparing them pairwise, which is more meaningful for clustering when the number of clusters is moderate.

### Mechanism 3
GEMINI automatically selects a relevant number of clusters during training through the optimization process and properties of the chosen distance. When clusters become empty during training, their contribution to the GEMINI objective may become negligible, leading to fewer active clusters. This behavior emerges naturally from the optimization landscape, though the paper does not provide a rigorous mathematical explanation for why this occurs.

## Foundational Learning

- **Discriminative Clustering**: Direct inference of cluster assignments from data without distributional assumptions. Needed because GEMINI is a discriminative clustering framework that maps data to cluster assignments without modeling data distribution.
- **Information Theory**: Concepts like mutual information and divergence measures. Needed because GEMINI is based on mutual information and requires understanding of information-theoretic measures.
- **Statistical Distances and Divergences**: KL divergence, MMD, Wasserstein distances, and f-divergences. Needed because GEMINI replaces KL divergence with these alternatives, and understanding their properties is crucial for implementation and interpretation.

## Architecture Onboarding

- **Component map**: Data -> Discriminative Model -> Distance/Divergence -> GEMINI Objective -> Optimizer
- **Critical path**: 1) Prepare data 2) Choose discriminative model 3) Select distance/divergence 4) Implement GEMINI objective 5) Train model with optimizer 6) Evaluate clustering
- **Design tradeoffs**: Computational complexity varies by distance (Wasserstein most expensive), interpretability differs across distances, and performance depends on data characteristics and model choice
- **Failure signatures**: Poor performance indicates inappropriate distance/data mismatch, numerical instability suggests Wasserstein implementation issues, convergence problems may indicate optimization or architecture issues
- **First 3 experiments**: 1) Implement GEMINI with logistic regression and KL divergence on synthetic data to verify basic functionality 2) Implement GEMINI with neural network and MMD on MNIST to evaluate realistic performance 3) Compare different distances (MMD vs Wasserstein) on same dataset to identify optimal choice

## Open Questions the Paper Calls Out

### Open Question 1
What causes some clusters to become empty after GEMINI convergence, and how can this be prevented or understood theoretically? The paper observes this empirically but provides no theoretical explanation for why certain cluster assignments lead to empty clusters or proposes solutions to prevent it.

### Open Question 2
How do different kernel choices in MMD-GEMINI affect clustering quality, and what properties make a kernel effective for clustering? The paper suggests using OvA MMD or OvA Wasserstein as default but doesn't systematically compare different kernels or provide guidance on selecting appropriate kernels for different data types.

### Open Question 3
How does the choice of architecture affect the number of clusters discovered by GEMINI, and what architectural properties influence this? The paper shows MLP finds more clusters than LeNet-5 on MNIST but doesn't explain why different architectures lead to different numbers of clusters or identify specific architectural properties driving this behavior.

## Limitations
- Claims about automatic cluster number selection lack rigorous theoretical explanation and are based on limited experimental evidence
- Computational scalability concerns for large datasets, particularly for Wasserstein distances requiring optimal transport solutions
- Experimental evaluation limited to image datasets (MNIST, CIFAR10) without demonstration on text, tabular, or other data types

## Confidence
- **High Confidence**: KL divergence causing poor decision boundaries in MI maximization (supported by theoretical analysis and empirical evidence)
- **Medium Confidence**: MMD and Wasserstein variants outperforming KL-based MI (supported by MNIST experiments but limited to specific datasets)
- **Low Confidence**: Automatic cluster number selection (limited experimental validation and unclear mechanism)

## Next Checks
1. Conduct ablation studies on synthetic datasets with varying geometries to validate whether geometry-aware distances consistently outperform KL divergence when data has clear geometric structure versus when it does not.
2. Evaluate GEMINI's computational requirements on larger datasets (e.g., CIFAR100, ImageNet subsets) to quantify the trade-off between clustering quality and computational cost, particularly for Wasserstein variants.
3. Test GEMINI on non-image datasets (e.g., text documents, gene expression data, or tabular data) to verify whether observed advantages extend beyond visual data where distance metrics are more intuitive.