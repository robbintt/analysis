---
ver: rpa2
title: 'Everyone Deserves A Reward: Learning Customized Human Preferences'
arxiv_id: '2309.03126'
source_url: https://arxiv.org/abs/2309.03126
tags:
- training
- preference
- general
- data
- customized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work studies how to train reward models that align with human
  preferences in customized scenarios. To simulate domain-specific preferences, the
  authors collect a dataset with preferred responses from four application domains:
  Academy, Business, Entertainment, and Literature&Art.'
---

# Everyone Deserves A Reward: Learning Customized Human Preferences

## Quick Facts
- arXiv ID: 2309.03126
- Source URL: https://arxiv.org/abs/2309.03126
- Authors: 
- Reference count: 40
- Key outcome: This work studies how to train reward models that align with human preferences in customized scenarios. To simulate domain-specific preferences, the authors collect a dataset with preferred responses from four application domains: Academy, Business, Entertainment, and Literature&Art. They propose a three-stage training scheme (base LM training, general RM fine-tuning, customized RM fine-tuning) and empirically test multiple training and data strategies. They find that general preference enrichment in the fine-tuning stage and customized preference imitation learning in the fine-tuning stage are effective for preserving general preferring ability while training the customized reward models.

## Executive Summary
This paper addresses the challenge of aligning reward models with human preferences in specialized application domains while maintaining general preference alignment. The authors propose a three-stage training framework that first establishes general preference understanding, then adapts to domain-specific preferences through fine-tuning. They collect a novel dataset spanning four application domains and demonstrate that combining general preference data enrichment with imitation learning on preferred responses effectively preserves the model's general preferring ability while enabling customization to specific domains.

## Method Summary
The approach involves training reward models in three stages: base language model training (using LLaMA-7B or Alpaca-7B), general reward model fine-tuning on datasets like Helpful&Harmless and WebGPT, and customized reward model fine-tuning on domain-specific preference data. The model uses a linear reward head on top of the base LLM, with ranking loss for preference learning and optional imitation learning loss on preferred responses to preserve general performance. Training uses a learning rate of 1e-6 and batch size of 64, with data augmentation through preferred response generation using ChatGPT.

## Key Results
- Domain-specific fine-tuning with general preference enrichment preserves general preference accuracy while improving domain-specific alignment
- Imitation learning on preferred responses helps maintain general preference performance during customized fine-tuning
- LLaMA-7B outperforms Alpaca-7B as a base model for reward learning, contrary to initial expectations
- Three-stage training effectively mitigates catastrophic forgetting compared to direct domain-specific fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific fine-tuning preserves specialized preference alignment while retaining general alignment through staged learning
- Mechanism: Three-stage training (base LM → general RM fine-tuning → customized RM fine-tuning) allows the model to first learn broad preference patterns, then specialize to domain-specific preferences without catastrophic forgetting
- Core assumption: General preferences are sufficiently diverse to provide a stable foundation for domain adaptation
- Evidence anchors:
  - [abstract]: "We proposed a three-stage customized RM learning scheme, whose effectiveness is empirically verified on both general preference datasets and our DSP set."
  - [section 4.3]: "GRFT data enrichment can better preserve the general RM performance decay during the CRFT stage"
  - [corpus]: Weak - related papers discuss multi-stage alignment but don't specifically validate this three-stage approach
- Break condition: If domain-specific preferences are too contradictory to general preferences, the staged approach may fail to preserve general alignment

### Mechanism 2
- Claim: Imitation learning on preferred responses improves transfer learning and data efficiency
- Mechanism: Adding language modeling loss on preferred responses (LLM_good) in the general fine-tuning stage provides additional supervision that helps the model internalize preference patterns beyond binary ranking
- Core assumption: Preferred responses contain implicit information about human preferences that can be extracted through language modeling
- Evidence anchors:
  - [abstract]: "We discovered that imitation learning on customized preference and general preference data enrichment are two effective ways to preserve RMs' general preferring ability"
  - [section 4.4]: "RM can better preserve the general performance (H&H) uniformly in terms of the preference accuracy difference"
  - [corpus]: Moderate - papers like "CHARM" and "Q-Adapter" discuss calibration and forgetting mitigation which align with imitation learning benefits
- Break condition: If the preferred responses are too diverse or noisy, imitation learning may introduce conflicting signals

### Mechanism 3
- Claim: Base model selection significantly impacts customized preference learning capability
- Mechanism: Using a general pre-trained model (LLaMA) as base performs better than a specialized fine-tuned model (Alpaca) for reward modeling, likely due to reduced overfitting and better generalization
- Core assumption: Pre-trained language models have better feature representations for preference learning than supervised fine-tuned variants
- Evidence anchors:
  - [section 4.2]: "Out of our expectations, Alpaca-based (with GPT-3 data fine-tuning) RM performs worse than the LLaMA-based one on all the testing sets"
  - [section 4.6]: "With GRFT on H&H data, LLaMA-based RM achieves higher ranking accuracy than the Alpaca-based"
  - [corpus]: Weak - no direct comparison of base model types in related papers
- Break condition: If the base model is too general and lacks domain-specific knowledge, it may struggle with specialized preferences

## Foundational Learning

- Concept: Reinforcement learning from human feedback (RLHF)
  - Why needed here: Reward models are core components of RLHF pipelines, providing preference signals for LLM alignment
  - Quick check question: How does a reward model differ from a standard classification model when handling preference data?

- Concept: Transfer learning and fine-tuning strategies
  - Why needed here: The paper explores how to effectively transfer general preference knowledge to domain-specific contexts
  - Quick check question: What are the risks of catastrophic forgetting when fine-tuning on domain-specific data?

- Concept: Language model pretraining and base model selection
  - Why needed here: The choice between general pre-trained models and supervised fine-tuned variants significantly impacts reward model performance
  - Quick check question: How might supervised fine-tuning on task-specific data affect a model's ability to learn general preferences?

## Architecture Onboarding

- Component map: Base LLM → Reward Head (linear layer) → Reward Score output. Optional imitation learning head for language modeling loss
- Critical path: Input prompt + response → Base LLM → Pooled hidden states → Reward head → Score. For imitation learning: Base LLM → Language modeling head → Next token prediction
- Design tradeoffs: Base model choice (general vs specialized) vs performance on general vs domain-specific preferences. Pooling strategy (last token vs average vs max) vs computational efficiency vs accuracy
- Failure signatures: General preference performance drops significantly during domain fine-tuning (forgetting). Domain-specific accuracy plateaus early (insufficient specialization). Base model overfits to training data
- First 3 experiments:
  1. Compare different base models (LLaMA vs Alpaca) on general RM fine-tuning to establish baseline performance
  2. Test different pooling strategies (last token, EOS token, average, max) on general RM fine-tuning to optimize architecture
  3. Implement three-stage training with varying data sizes in general fine-tuning stage to determine optimal data enrichment strategy

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- Empirical findings rely on a single domain-specific dataset (DSP) collected by the authors, limiting generalizability
- Base model comparison (LLaMA vs Alpaca) is based on a single model size (7B parameters), and results may not extend to larger or smaller models
- Three-stage training scheme is empirically effective but lacks theoretical justification for why this specific architecture outperforms alternatives

## Confidence
- High confidence: General observation that domain-specific fine-tuning can improve alignment while preserving general preferences when using the three-stage approach
- Medium confidence: Specific mechanisms (imitation learning, base model selection) due to limited ablation studies and single dataset evaluation
- Low confidence: Robustness of results across different domains, model sizes, and preference data sources

## Next Checks
1. Test the three-stage training scheme on additional domain-specific datasets beyond the four collected (e.g., medical, legal, or technical domains) to validate generalizability of the approach

2. Conduct systematic ablation studies varying the number of general preference data pairs in the fine-tuning stage (beyond the 1.5x, 2.5x, 4.5x, 6.5x tested) to identify optimal data enrichment ratios

3. Compare the three-stage approach against alternative continuous learning methods (like elastic weight consolidation or rehearsal) to determine if staged learning provides unique benefits or if similar results could be achieved through other forgetting mitigation techniques