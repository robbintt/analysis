---
ver: rpa2
title: Learning Semantic Role Labeling from Compatible Label Sequences
arxiv_id: '2305.14600'
source_url: https://arxiv.org/abs/2305.14600
tags:
- joint
- propbank
- verbnet
- data
- semlink
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework for jointly modeling two semantic
  role labeling (SRL) label sets (VerbNet and PropBank) as a single sequence, addressing
  the issue of structurally inconsistent label sequences in prior multi-task setups.
  The approach involves converting multitask modeling into a single sequence labeling
  task using a joint Conditional Random Field (CRF) with SEMLINK constraints to ensure
  compatibility.
---

# Learning Semantic Role Labeling from Compatible Label Sequences

## Quick Facts
- arXiv ID: 2305.14600
- Source URL: https://arxiv.org/abs/2305.14600
- Authors: 
- Reference count: 12
- Key outcome: Achieves state-of-the-art F1 scores on CoNLL05 dataset by jointly modeling VerbNet and PropBank SRL labels with SEMLINK constraints

## Executive Summary
This paper addresses the challenge of structurally inconsistent label sequences in multi-task semantic role labeling (SRL) by proposing a framework that jointly models VerbNet and PropBank labels as a single sequence. The approach uses a joint Conditional Random Field (CRF) with SEMLINK constraints to ensure compatibility between the two label sets, converting the multitask problem into a single sequence labeling task. This framework achieves significant improvements over prior models, with 3.5 points higher F1 for VerbNet and 0.8 points for PropBank in-domain, while also demonstrating strong out-of-domain generalization. Additionally, the constrained marginal CRF variant enables effective semi-supervised learning from PropBank-only data, further improving performance.

## Method Summary
The method involves converting multitask SRL modeling into a single sequence labeling task using a joint CRF model that cross-products the label spaces of VerbNet and PropBank while enforcing SEMLINK constraints. The approach uses a ROBERTA encoder for text representation, predicate-specific feature extraction layers, and a CRF layer for sequence modeling. SEMLINK constraints are applied during both training and inference to ensure label compatibility. A constrained marginal CRF variant is also introduced for semi-supervised learning, which maximizes the marginal distribution of observed PropBank labels while restricting possible VerbNet labels using SEMLINK mappings.

## Key Results
- Achieves state-of-the-art F1 scores on CoNLL05 dataset: 3.5 points higher for VerbNet and 0.8 points for PropBank in-domain
- Strong out-of-domain generalization: 3.4 points higher for VerbNet and 0.2 points for PropBank
- Can infer VerbNet arguments from PropBank arguments with over 99% accuracy
- Constrained marginal CRF improves performance by efficiently utilizing PropBank-only data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint modeling of VerbNet and PropBank as a single sequence with SEMLINK constraints reduces structural inconsistencies in predicted labels.
- Mechanism: The joint CRF model cross-products the label spaces while enforcing compatibility constraints from SEMLINK, ensuring only valid label combinations are considered during decoding.
- Core assumption: SEMLINK mappings accurately capture semantic relationships between VerbNet and PropBank arguments.
- Evidence anchors:
  - [abstract] "enforcing SEMLINK constraints during decoding constantly improves the overall F1."
  - [section 6] "if a VerbNet argument presents in the predicate u's SEMLINK, we prevent it from aligning to any PropBank counterparts not defined in SEMLINK."
  - [corpus] Weak evidence - only 25 related papers found, none specifically addressing SEMLINK constraint mechanisms.

### Mechanism 2
- Claim: Marginal CRF with SEMLINK constraints enables effective semi-supervised learning from PropBank-only data.
- Mechanism: The marginal CRF maximizes the marginal distribution of observed PropBank labels while using SEMLINK to restrict the space of possible VerbNet labels, allowing the model to learn from partially labeled data.
- Core assumption: SEMLINK mappings provide sufficient coverage to effectively constrain the label space during semi-supervised learning.
- Evidence anchors:
  - [abstract] "The constrained marginal CRF further enhances performance by efficiently utilizing PropBank-only data."
  - [section 5.4] "We can further narrow it down to legitimate VN-PB argument pairs defined in SEMLINK."
  - [corpus] Weak evidence - no direct corpus evidence found for marginal CRF with SEMLINK constraints.

### Mechanism 3
- Claim: Special input construction using predicate senses improves model performance.
- Mechanism: Appending PropBank sense information to the input sentence provides additional contextual information that helps the model better disambiguate between different semantic roles.
- Core assumption: PropBank sense information is informative and relevant to determining the correct semantic roles.
- Evidence anchors:
  - [section 4] "we propose a simple solution that appends the PropBank senses of potential predicates to the original sentence x."
  - [section 7.4] "For the predicate word, we use the predicate feature (i.e. lemma and sense)."
  - [corpus] No direct evidence found in related papers.

## Foundational Learning

- Concept: Semantic Role Labeling (SRL)
  - Why needed here: The paper is specifically about improving SRL by jointly modeling VerbNet and PropBank labels.
  - Quick check question: Can you explain the difference between VerbNet and PropBank labeling schemes and why they might be compatible?

- Concept: Conditional Random Fields (CRFs)
  - Why needed here: The proposed models use CRFs to handle the sequence labeling task and incorporate constraints.
  - Quick check question: How does a CRF differ from a standard classifier, and why is it useful for sequence labeling tasks?

- Concept: Semi-supervised Learning
  - Why needed here: The paper explores using partially labeled PropBank-only data to improve model performance.
  - Quick check question: What are the challenges of semi-supervised learning, and how might SEMLINK constraints help address them?

## Architecture Onboarding

- Component map: ROBERTA encoder -> Predicate-specific feature extraction -> Joint label space scoring -> CRF layer -> SEMLINK constraint application -> Final predictions

- Critical path:
  1. Input text and predicate information → ROBERTA encoder
  2. Encoded representations → Predicate-specific feature extraction
  3. Combined features → Joint label space scoring
  4. Scores → CRF layer for sequence modeling
  5. CRF output → SEMLINK constraint application
  6. Constrained output → Final predictions

- Design tradeoffs:
  - Joint vs. separate modeling: Joint modeling reduces structural inconsistencies but increases label space complexity.
  - Constraint application: Applying constraints during inference improves accuracy but may limit flexibility.
  - Input construction: Including predicate senses helps disambiguation but adds complexity and potential noise.

- Failure signatures:
  - High SEMLINK violation rate: Indicates the model is not respecting compatibility constraints.
  - Poor performance on out-of-domain data: Suggests overfitting to in-domain patterns or insufficient generalization.
  - Large gap between development and test performance: May indicate data leakage or overfitting to development set.

- First 3 experiments:
  1. Implement the basic joint CRF model without constraints and evaluate on development set.
  2. Add SEMLINK constraints during inference and measure improvement in F1 scores and reduction in violation rate.
  3. Implement the marginal CRF variant and compare performance on PropBank-only data to the joint CRF baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the results of the proposed models compare to those obtained using large language models (LLMs) for semantic role labeling?
- Basis in paper: [explicit] The paper mentions that recent advancements in large language models (LLMs) have shown promising performance in semantic parsing tasks and suggests that there could be shared advantages between their work and LLM-based approaches.
- Why unresolved: The paper does not provide a direct comparison between the proposed models and LLM-based models for semantic role labeling.
- What evidence would resolve it: Conducting experiments that compare the performance of the proposed models and LLM-based models on the same semantic role labeling tasks and datasets.

### Open Question 2
- Question: How does the joint modeling of VerbNet and PropBank labels affect the performance on other semantic role labeling tasks or datasets?
- Basis in paper: [inferred] The paper focuses on the CoNLL05 dataset and does not explore the generalization of the proposed joint modeling approach to other datasets or semantic role labeling tasks.
- Why unresolved: The paper does not provide evidence on the applicability and performance of the proposed joint modeling approach on other datasets or semantic role labeling tasks.
- What evidence would resolve it: Conducting experiments that apply the proposed joint modeling approach to other semantic role labeling tasks or datasets and comparing the performance to state-of-the-art models.

### Open Question 3
- Question: How does the constrained marginal CRF model perform when applied to other sequence labeling tasks with disjoint label sets?
- Basis in paper: [inferred] The paper introduces the constrained marginal CRF model for the specific case of VerbNet and PropBank SRL and does not explore its applicability to other sequence labeling tasks with disjoint label sets.
- Why unresolved: The paper does not provide evidence on the performance of the constrained marginal CRF model when applied to other sequence labeling tasks with disjoint label sets.
- What evidence would resolve it: Conducting experiments that apply the constrained marginal CRF model to other sequence labeling tasks with disjoint label sets and comparing the performance to state-of-the-art models.

## Limitations

- The approach relies heavily on the quality and completeness of SEMLINK mappings, with no thorough analysis of robustness to mapping errors.
- Semi-supervised learning claims are evaluated on a limited scale with only 32% of training data being partially labeled.
- The paper doesn't thoroughly analyze what happens when constraints fail or how robust the approach is to mapping errors.

## Confidence

- **High confidence** in the joint CRF framework's ability to improve SRL performance through structural consistency
- **Medium confidence** in the semi-supervised learning claims, as evaluation is limited to specific experimental conditions
- **Medium confidence** in the generalizability of results, particularly for the out-of-domain experiments

## Next Checks

1. **Constraint Robustness Analysis**: Systematically evaluate model performance when SEMLINK mappings contain errors or are incomplete, measuring how violations affect F1 scores and overall model reliability.

2. **Semi-supervised Scaling**: Test the marginal CRF approach with varying proportions of partially labeled data (0%, 25%, 50%, 75%) to establish the minimum effective ratio for performance gains.

3. **Cross-framework Transfer**: Evaluate whether the joint modeling approach improves SRL performance when applied to other semantic frameworks beyond VerbNet and PropBank, testing generalizability of the core mechanism.