---
ver: rpa2
title: 'AlignDet: Aligning Pre-training and Fine-tuning in Object Detection'
arxiv_id: '2307.11077'
source_url: https://arxiv.org/abs/2307.11077
tags:
- pre-training
- aligndet
- detection
- backbone
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the data, model, and task discrepancies between
  pre-training and fine-tuning in object detection. It proposes AlignDet, a unified
  framework that decouples pre-training into image-domain and box-domain stages to
  bridge these gaps.
---

# AlignDet: Aligning Pre-training and Fine-tuning in Object Detection

## Quick Facts
- arXiv ID: 2307.11077
- Source URL: https://arxiv.org/abs/2307.11077
- Reference count: 40
- Key outcome: Improves FCOS by 5.3 mAP, RetinaNet by 2.1 mAP, Faster R-CNN by 3.3 mAP, and DETR by 2.3 mAP under fewer epochs.

## Executive Summary
AlignDet addresses the data, model, and task discrepancies between pre-training and fine-tuning in object detection by decoupling pre-training into two stages: image-domain and box-domain. The image-domain stage optimizes the backbone for holistic visual abstraction using large-scale classification datasets, while the box-domain stage learns object-level semantics and task-aware concepts to initialize the neck and head modules. This unified framework enables fully unsupervised pre-training for various detectors and significantly improves detection performance with fewer training epochs.

## Method Summary
AlignDet proposes a two-stage pre-training paradigm that decouples the pre-training process into image-domain and box-domain stages. In the image-domain stage, the backbone is pre-trained on large-scale image classification datasets to capture holistic visual abstraction. The box-domain stage uses multi-object datasets and constructs detection-oriented pretext tasks (contrastive learning + regression) to initialize the neck and head modules. By freezing the backbone during box-domain pre-training and using selective search proposals as pseudo-labels, AlignDet enables fully unsupervised pre-training that bridges the gap between pre-training and fine-tuning for various detection architectures.

## Key Results
- Improves FCOS by 5.3 mAP with fewer epochs
- Improves RetinaNet by 2.1 mAP with fewer epochs
- Improves Faster R-CNN by 3.3 mAP with fewer epochs
- Improves DETR by 2.3 mAP with fewer epochs

## Why This Works (Mechanism)

### Mechanism 1
Decoupling pre-training into image-domain and box-domain stages resolves data, model, and task discrepancies simultaneously. The image-domain stage uses large-scale classification datasets to pre-train the backbone for holistic visual abstraction, while the box-domain stage uses multi-object datasets and detection-oriented pretext tasks to initialize the neck and head modules. This separation allows efficient pre-training of modules with different parameter scales and task requirements.

### Mechanism 2
Box-domain contrastive learning with unsupervised proposals provides detection-oriented supervision without requiring labeled data. Selective search generates unsupervised proposals as pseudo-labels. Two augmented views of each image are created, and the detector predicts boxes for each view. Box-level contrastive learning maximizes similarity between features of boxes corresponding to the same proposal across views while minimizing similarity for different proposals. Coordinate-related regression losses are also applied based on the pseudo-labels.

### Mechanism 3
Pre-training all modules (backbone, neck, head) rather than just the backbone significantly improves downstream detection performance. The box-domain pre-training stage updates the neck and head modules while keeping the backbone frozen. This allows the detection-specific modules to learn from detection-oriented tasks using the already-pretrained backbone features. The ablation study shows that pre-training each module individually provides incremental improvements, with full pre-training yielding the best results.

## Foundational Learning

- **Concept: Contrastive learning**
  - Why needed here: Box-domain contrastive learning is a core component that enables the model to learn detection-relevant features from unlabeled data by maximizing similarity between features of the same object across different views while minimizing similarity between different objects.
  - Quick check question: How does box-level contrastive learning differ from image-level contrastive learning, and why is this difference important for object detection pre-training?

- **Concept: Target assignment in object detection**
  - Why needed here: Understanding how different detectors assign targets (e.g., IoU-based for anchor-based methods, point-based for FCOS) is crucial for understanding how AlignDet adapts to different detector architectures.
  - Quick check question: What are the different target assignment strategies used by anchor-based, point-based, and query-based detectors, and how does AlignDet accommodate these differences?

- **Concept: Selective search for proposal generation**
  - Why needed here: Selective search is used to generate unsupervised proposals that serve as pseudo-labels for box-domain pre-training. Understanding how selective search works and its limitations is important for understanding the quality of the pre-training data and potential failure modes.
  - Quick check question: What are the advantages and limitations of using selective search for generating unsupervised proposals compared to other proposal generation methods?

## Architecture Onboarding

- **Component map:**
  - Backbone -> Classification head -> Image-domain contrastive loss
  - Backbone (frozen) -> Neck & Head -> Box-domain contrastive loss + Regression loss
  - All pre-trained weights (except projection module) -> Standard detection loss

- **Critical path:**
  1. Image-domain pre-training: Backbone → Classification head → Contrastive loss
  2. Box-domain pre-training: Backbone (frozen) → Neck & Head → Box-level contrastive loss + Regression loss
  3. Fine-tuning: All pre-trained weights (except projection module) → Standard detection loss

- **Design tradeoffs:**
  - Freezing backbone during box-domain pre-training vs. fine-tuning all parameters: Freezing prevents overfitting to noisy pseudo-labels but may limit adaptation to detection-specific features.
  - Using selective search proposals vs. other proposal generation methods: Selective search is simple and effective but may not capture all objects or may generate redundant proposals.
  - Decoupling pre-training into two stages vs. end-to-end pre-training: Decoupling is more efficient and leverages existing pre-trained backbones but requires careful coordination between stages.

- **Failure signatures:**
  - Poor quality selective search proposals leading to noisy pseudo-labels and degraded pre-training
  - Overfitting to the small COCO dataset during box-domain pre-training, especially for the neck and head modules
  - Mismatch between pre-training tasks and downstream fine-tuning tasks causing poor knowledge transfer
  - Insufficient diversity in the box-domain pre-training data leading to poor generalization

- **First 3 experiments:**
  1. Verify that the image-domain pre-training stage effectively learns meaningful visual representations by evaluating the backbone on a downstream classification task.
  2. Test the box-domain pre-training with different proposal generation methods (e.g., selective search vs. random boxes) to understand the importance of quality proposals.
  3. Evaluate the transfer learning performance with different fine-tuning learning rates and weight decay values to find the optimal transfer settings for AlignDet pre-trained models.

## Open Questions the Paper Calls Out
The paper discusses the potential of extending the decoupled pre-training paradigm to other vision tasks, allowing the integration of general-purpose pre-trained backbones with task-aware pre-trained necks and heads. However, it does not provide empirical evidence or detailed analysis of how this approach would perform on tasks beyond object detection, such as semantic segmentation, instance segmentation, or image classification.

## Limitations
- The effectiveness of AlignDet heavily depends on the quality of unsupervised proposals generated by selective search, which may not capture all objects or may generate redundant proposals.
- The computational overhead of generating proposals and running dual-view training may limit practical adoption, especially for real-time detection scenarios.
- The approach requires careful coordination between the two pre-training stages and may not generalize well to datasets with significantly different characteristics from COCO.

## Confidence
- **High Confidence**: Claims about improved mAP scores across multiple detectors (FCOS, RetinaNet, Faster R-CNN, DETR) with fewer epochs.
- **Medium Confidence**: Claims about decoupling pre-training into image-domain and box-domain stages resolving data, model, and task discrepancies.
- **Medium Confidence**: Claims about the effectiveness of box-level contrastive learning with unsupervised proposals.

## Next Checks
1. Systematically evaluate the impact of selective search parameters (IoU thresholds, number of proposals) on pre-training quality and downstream detection performance. Compare against alternative proposal generation methods to isolate the importance of proposal quality.
2. Test AlignDet's transfer learning performance on a dataset with significantly different characteristics from COCO (e.g., Pascal VOC, Open Images, or a specialized domain like medical imaging) to validate the robustness of the pre-trained representations to domain shifts.
3. Conduct a more granular ablation study that isolates the contribution of image-domain vs. box-domain pre-training for each detector type, including testing scenarios where only the backbone is pre-trained, only the neck and head are pre-trained, or where different combinations of modules are pre-trained.