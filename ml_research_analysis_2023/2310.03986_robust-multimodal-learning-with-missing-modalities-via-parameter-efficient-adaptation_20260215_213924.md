---
ver: rpa2
title: Robust Multimodal Learning with Missing Modalities via Parameter-Efficient
  Adaptation
arxiv_id: '2310.03986'
source_url: https://arxiv.org/abs/2310.03986
tags:
- modalities
- multimodal
- missing
- performance
- segmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of multimodal learning with missing
  modalities, where the performance of existing multimodal networks significantly
  deteriorates if one or multiple modalities are absent at test time. The authors
  propose a parameter-efficient adaptation procedure for pretrained multimodal networks
  that exploits modulation of intermediate features to compensate for missing modalities.
---

# Robust Multimodal Learning with Missing Modalities via Parameter-Efficient Adaptation

## Quick Facts
- arXiv ID: 2310.03986
- Source URL: https://arxiv.org/abs/2310.03986
- Reference count: 40
- Primary result: Parameter-efficient adaptation via scale-and-shift feature modulation can partially recover missing modality performance without retraining entire model.

## Executive Summary
This paper addresses the challenge of multimodal learning when one or more modalities are missing at test time. The authors propose a parameter-efficient adaptation approach that inserts learnable scale-and-shift feature (SSF) layers after each linear, convolutional, and normalization layer in a pretrained multimodal network. By freezing the original weights and learning only the adaptation parameters for different modality combinations, the method can compensate for missing modalities while requiring fewer than 0.7% of total parameters. Extensive experiments across five multimodal tasks and seven datasets demonstrate that this approach can partially bridge the performance gap caused by missing modalities and even outperform independent networks trained for specific modality combinations in some cases.

## Method Summary
The proposed method involves pretraining a multimodal network on all available modalities, then freezing its weights and inserting SSF layers after each linear, convolutional, and normalization layer. For each possible modality combination, the SSF parameters are trained separately while setting missing modalities to zero. The adaptation learns to remap intermediate features to compensate for missing information, requiring only a small fraction of parameters compared to the full model. An alternative LoRA-based adaptation is also explored. The method is evaluated on semantic segmentation tasks (using MFNet, NYUDv2, MCubeS datasets) and multimodal sentiment analysis tasks (CMU-MOSI, CMU-MOSEI), showing effective performance across diverse scenarios.

## Key Results
- Adaptation requires fewer than 0.7% of total parameters while maintaining competitive performance
- Method outperforms independent dedicated networks for specific modality combinations in some cases
- Shows versatility across five different multimodal tasks spanning seven datasets
- Can partially bridge performance drop due to missing modalities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parameter-efficient adaptation via scale-and-shift feature modulation can partially recover missing modality performance without retraining entire model.
- Mechanism: Insert SSF layers after each linear, convolutional, and normalization layer in a pretrained multimodal network. Freeze original weights and train only SSF parameters for each modality combination, allowing feature space remapping to compensate for missing modalities.
- Core assumption: Linear transformations of intermediate features are sufficient to approximate the effect of missing modalities for downstream task performance.
- Evidence anchors: [abstract] "we exploit modulation of intermediate features to compensate for the missing modalities"; [section 3.2] "SSF applies a linear transformation to the given token/feature with learnable scale (γ) and shift parameters (β)"

### Mechanism 2
- Claim: Low-rank adaptation (LoRA) provides parameter-efficient alternative to SSF for multimodal missing modality robustness.
- Mechanism: Learn low-rank update matrices WS for weight matrices W in pretrained model, where update parameters are a small fraction of total parameters but can approximate missing modality compensation.
- Core assumption: Missing modality effects can be approximated by low-rank transformations of existing feature representations.
- Evidence anchors: [section 3.2] "LoRA in transformers [19] and KAdaptation learn low-rank factors for task/domain adaptation"; [abstract] "We primarily selected the SSF technique... LoRA [19]"

### Mechanism 3
- Claim: Adaptation requires minimal additional parameters (<0.7% of total) while maintaining or improving performance compared to dedicated models.
- Mechanism: By freezing pretrained weights and only training adaptation parameters (SSF or LoRA), the method achieves parameter efficiency while learning modality-specific transformations.
- Core assumption: The pretrained multimodal model captures sufficient shared representations that small adaptation parameters can effectively compensate for missing modalities.
- Evidence anchors: [abstract] "The proposed adaptation requires extremely small number of parameters (e.g., fewer than 1% of the total parameters)"; [section 4.4] "SSF introduces only 0.789M learnable parameters which is less than 0.7% of the total model parameters"

## Foundational Learning

- Concept: Multimodal learning fundamentals and modality fusion strategies
  - Why needed here: Understanding how different modalities contribute to task performance and how fusion strategies affect robustness to missing modalities
  - Quick check question: What are the key differences between early, late, and hybrid fusion approaches in multimodal learning?

- Concept: Parameter-efficient fine-tuning methods (LoRA, BitFit, SSF)
  - Why needed here: The adaptation methods rely on understanding how to efficiently modify pretrained models without full retraining
  - Quick check question: How does LoRA's low-rank approximation differ from BitFit's bias-only adaptation in terms of parameter efficiency and expressiveness?

- Concept: Cross-modal attention and feature interaction mechanisms
  - Why needed here: Understanding how modalities interact in transformer-based multimodal models helps explain why simple linear transformations can compensate for missing modalities
  - Quick check question: In multimodal transformers, how does the attention mechanism enable cross-modal information flow that could be approximated by adaptation parameters?

## Architecture Onboarding

- Component map: Pretrained multimodal backbone (frozen) -> SSF/LoRA adaptation layers -> Decoder/prediction head (frozen)

- Critical path:
  1. Load pretrained multimodal model with all modalities
  2. Insert adaptation layers at specified positions
  3. For each modality combination, train adaptation parameters with missing modalities set to zero
  4. At inference, select appropriate adaptation parameters based on available modalities

- Design tradeoffs:
  - SSF vs LoRA: SSF is simpler and more interpretable but may require more parameters; LoRA is more compact but potentially less expressive
  - Adaptation layer placement: After every layer provides maximum flexibility but increases parameters; selective placement reduces parameters but may limit effectiveness
  - Frozen vs finetuned decoder: Keeping decoder frozen ensures consistency but may miss adaptation opportunities

- Failure signatures:
  - Performance drops when modality combinations are too dissimilar from training combinations
  - Overfitting to specific modality combinations if training data is insufficient
  - Computational overhead during inference if many adaptation parameter sets are loaded

- First 3 experiments:
  1. Validate adaptation effectiveness on MFNet dataset with RGB-thermal pairs (simplest case)
  2. Test adaptation with single modality missing vs multiple modalities missing
  3. Compare SSF vs LoRA parameter efficiency and performance tradeoffs on NYUDv2 dataset

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several important areas remain unexplored based on the experimental scope and discussion:

1. How does the proposed adaptation method perform when applied to more than three modalities, and what are the scalability limitations?

2. What are the theoretical bounds on performance improvement when adapting a pretrained multimodal network for missing modalities, and how do these bounds compare to specialized models?

3. How does the adaptation method handle gradual degradation of modalities (e.g., noisy or corrupted inputs) rather than complete absence, and what are the implications for real-world applications?

4. What are the potential negative societal impacts of deploying multimodal systems that can adapt to missing modalities, particularly in critical applications like autonomous driving or medical diagnosis?

## Limitations

- The adaptation mechanism's reliance on linear feature transformations may not capture complex nonlinear relationships between modalities in certain tasks
- Evaluation focuses primarily on structured datasets (segmentation and sentiment analysis) without testing on more complex multimodal scenarios
- Performance gap remains significant when multiple modalities are missing, suggesting limitations in compensating for extensive information loss
- Claims of versatility across "various tasks and datasets" are based on only seven datasets across two task types

## Confidence

**High Confidence**: The parameter efficiency claims are well-supported by the experimental results showing adaptation parameters consistently below 0.7% of total model parameters across all tested tasks.

**Medium Confidence**: The performance claims showing the adaptation can "partially bridge" the performance drop are supported but show significant variance across datasets and modality combinations.

**Low Confidence**: The generalizability claims across "various tasks and datasets" are based on only seven datasets across two task types (segmentation and sentiment analysis).

## Next Checks

1. **Cross-task validation**: Test the adaptation method on a third modality type (e.g., text-vision-audio fusion for VQA) to validate claims of versatility across diverse multimodal tasks beyond the current segmentation and sentiment analysis focus.

2. **Multiple modality missing analysis**: Systematically evaluate the adaptation's performance degradation curve as more modalities are simultaneously missing, particularly testing the theoretical limit where single-modality adaptation fails to match dedicated single-modality models.

3. **Generalization to out-of-distribution scenarios**: Evaluate the adaptation on modality combinations not seen during training (e.g., train on RGB+Depth, test on RGB-only) to assess robustness to distribution shifts and validate the claim that learned transformations generalize beyond training combinations.