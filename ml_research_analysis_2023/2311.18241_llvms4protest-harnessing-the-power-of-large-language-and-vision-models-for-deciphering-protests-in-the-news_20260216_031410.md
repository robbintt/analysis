---
ver: rpa2
title: 'LLVMs4Protest: Harnessing the Power of Large Language and Vision Models for
  Deciphering Protests in the News'
arxiv_id: '2311.18241'
source_url: https://arxiv.org/abs/2311.18241
tags:
- protest
- data
- social
- articles
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LLVMs4Protest, a framework for fine-tuning
  large language and vision models to identify and analyze protests in news articles
  and images. The approach uses transfer learning to adapt pretrained transformer
  models (Longformer for text, Swin-Transformer V2 for images) to the specific task
  of protest detection.
---

# LLVMs4Protest: Harnessing the Power of Large Language and Vision Models for Deciphering Protests in the News

## Quick Facts
- arXiv ID: 2311.18241
- Source URL: https://arxiv.org/abs/2311.18241
- Reference count: 3
- Key outcome: Fine-tuned transformer models achieve over 94% accuracy in identifying protest events from news articles and images

## Executive Summary
LLVMs4Protest presents a framework for fine-tuning large language and vision models to detect and analyze protests in news media. The approach adapts pretrained transformer models (Longformer for text, Swin-Transformer V2 for images) using labeled datasets from the DoCA corpus and UCLA-Protest dataset. The fine-tuned models achieve high accuracy in identifying protest events, providing social movement scholars with accessible AI tools for multimodal protest analysis. The authors release both models and training code to support research in this domain.

## Method Summary
The framework uses transfer learning to adapt pretrained transformer models to protest detection tasks. Longformer is fine-tuned on 11,902 labeled NYT articles from the DoCA corpus, while Swin-Transformer V2 is trained on 11,659 labeled protest images from the UCLA-Protest dataset. Both models leverage their pretrained knowledge of language and vision to identify protest-related patterns, achieving over 94% accuracy. The approach combines specialized transformer architectures with labeled training data to enable efficient and accurate protest event detection.

## Key Results
- Fine-tuned Longformer model achieves over 94% accuracy in identifying protest-related news articles
- Fine-tuned Swin-Transformer V2 model achieves over 94% accuracy in analyzing protest imagery
- Framework provides accessible AI tools for social movement scholars to analyze multimodal protest data
- Authors release both models and training code for community use

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer learning from large pretrained transformer models to protest detection tasks is effective because these models have already learned general language and vision representations that can be adapted to specific social movement analysis tasks.
- Mechanism: The fine-tuned Longformer and Swin-Transformer V2 models leverage their pretrained knowledge to identify protest-related patterns in text and imagery, achieving over 94% accuracy on the downstream tasks.
- Core assumption: Pretrained transformer models contain transferable knowledge relevant to protest event detection that can be efficiently adapted through fine-tuning.
- Evidence anchors: [abstract] "This article documents how we fine-tuned two large pretrained transformer models, including longformer and swin-transformer v2, to infer potential protests in news articles using textual and imagery data."

### Mechanism 2
- Claim: The combination of textual and visual data improves protest event detection accuracy compared to using either modality alone.
- Mechanism: By fine-tuning both Longformer (text) and Swin-Transformer V2 (images) models and potentially combining their outputs, the framework can capture complementary information from news articles and accompanying images.
- Core assumption: Protest events have distinctive textual and visual signatures that can be detected by specialized models for each modality.
- Evidence anchors: [abstract] "Large language and vision models have transformed how social movement scholars identify protest and extract key protest attributes from multi-modal data such as texts, images, and videos."

### Mechanism 3
- Claim: The use of specialized transformer architectures (Longformer for text, Swin-Transformer for images) is more effective than general-purpose models for this specific task.
- Mechanism: Longformer's ability to handle long documents and Swin-Transformer's hierarchical vision processing are well-suited for analyzing news articles and protest imagery respectively.
- Core assumption: The architectural features of these specialized transformers directly address the challenges of processing long news articles and complex protest images.
- Evidence anchors: [section] "Instead of utilizing the pretrained BERT model, we fine-tune the longformer model, which is specifically used for the long documents (Beltagy, Peters, & Cohan, 2020)."

## Foundational Learning

- Concept: Transfer learning
  - Why needed here: The paper relies on fine-tuning pretrained transformer models rather than training from scratch, which requires understanding how knowledge from one domain can be adapted to another.
  - Quick check question: What is the key difference between training a model from scratch and fine-tuning a pretrained model, and why is fine-tuning typically more efficient?

- Concept: Multimodal learning
  - Why needed here: The framework processes both textual and visual data to detect protests, requiring understanding of how different data modalities can be combined or processed separately.
  - Quick check question: What are the main challenges in multimodal learning, and how does processing text and images separately (as this paper does) address these challenges?

- Concept: Transformer architectures
  - Why needed here: The paper uses Longformer and Swin-Transformer V2, which are transformer-based models with specific architectural features for handling long sequences and hierarchical vision tasks.
  - Quick check question: How do the attention mechanisms in transformers differ from traditional recurrent or convolutional neural networks, and what advantages do they offer for processing sequential or spatial data?

## Architecture Onboarding

- Component map: Longformer (text processing) -> Swin-Transformer V2 (image processing) -> Protest detection outputs
- Critical path: Data preparation and labeling -> Fine-tuning of Longformer and Swin-Transformer V2 models -> Evaluation and release of models
- Design tradeoffs: Specialized transformer architectures offer better performance but require more domain-specific knowledge; separate text and image models simplify architecture but may miss cross-modal interactions
- Failure signatures: Poor performance could indicate insufficient labeled training data, poor quality of pretrained models, or mismatch between pretrained knowledge and protest detection tasks
- First 3 experiments:
  1. Evaluate Longformer model on held-out test set from DoCA corpus to verify protest article detection
  2. Test Swin-Transformer V2 model on UCLA-Protest dataset subset to confirm protest image identification
  3. Compare fine-tuned models against baseline models (BERT for text, ResNet for images) to quantify benefits of specialized architectures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of fine-tuned Longformer and Swin-Transformer models compare to GPT-4 for protest detection and attribute extraction tasks?
- Basis in paper: [explicit] The authors note that "a comparison between these fine-tuned models and GPT-4 annotation can offer social movement scholars more nuances in terms of choosing generative AIs or transfer learning with pretrained models."
- Why unresolved: The authors state that cross-validation comparing their models to GPT-4 is needed but has not yet been conducted.
- What evidence would resolve it: Systematic evaluation of both approaches on the same datasets using standardized metrics like accuracy, precision, recall, and F1-score for both protest detection and attribute extraction tasks.

### Open Question 2
- Question: How well do the fine-tuned models generalize to protest data from time periods beyond 1960-1995?
- Basis in paper: [explicit] The authors note that "the training data used to fine-tune longformer model has time constraints... this warrants further studies" and that most positive news articles are from 1960-1995.
- Why unresolved: The training data's temporal limitations are acknowledged but not tested against more recent data.
- What evidence would resolve it: Evaluation of model performance on protest articles and images from 1995 to present, comparing accuracy across different decades.

### Open Question 3
- Question: What is the optimal balance between model size, computational requirements, and accuracy for protest detection tasks?
- Basis in paper: [inferred] The authors discuss both the computational barriers of using large models and the need to make models "easily operable on standard computing devices."
- Why unresolved: While the authors mention that their models achieve over 94% accuracy, they don't explore whether smaller, more efficient models could achieve comparable results with less computational overhead.
- What evidence would resolve it: Systematic comparison of model performance across different architectures and sizes (e.g., Longformer-base vs. Longformer-large, Swin-Tiny vs. Swin-Base) with corresponding computational requirements and accuracy metrics.

## Limitations
- High accuracy reported but lacks detailed precision-recall tradeoffs and class imbalance handling information
- Performance on real-world, unlabeled data from diverse news sources remains uncertain
- Computational requirements for fine-tuning and deployment may limit accessibility for researchers with constrained resources

## Confidence

**High Confidence Claims:**
- Transfer learning with pretrained transformer models is an established approach for adapting large models to specific tasks
- Fine-tuning Longformer for text processing and Swin-Transformer V2 for image analysis are appropriate architectural choices for their respective modalities
- Framework addresses genuine need in social movement scholarship for accessible AI tools to analyze protest data

**Medium Confidence Claims:**
- Reported 94% accuracy reflects true model performance, pending verification of evaluation methodology
- Combination of textual and visual modalities improves protest detection compared to single-modality approaches
- Specialized transformer architectures outperform general-purpose models for protest detection tasks

## Next Checks

1. **Cross-dataset validation**: Test the fine-tuned models on protest data from news sources outside the NYT corpus and UCLA-Protest dataset to assess generalizability to different media outlets and protest contexts.

2. **Bias and fairness audit**: Analyze model predictions across different demographic groups, protest types, and geographical regions to identify potential systematic biases in detection or classification accuracy.

3. **Computational efficiency benchmarking**: Measure inference time and memory requirements for both fine-tuned models to determine the minimum hardware specifications needed for researchers to effectively deploy these tools in practice.