---
ver: rpa2
title: A Reference Architecture for Designing Foundation Model based Systems
arxiv_id: '2304.11090'
source_url: https://arxiv.org/abs/2304.11090
tags:
- foundation
- architecture
- components
- systems
- design
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a pattern-oriented reference architecture for
  designing responsible foundation model-based AI systems, addressing the challenges
  of architecture evolution and responsible AI. The architecture evolution of AI systems
  in the era of foundation models is discussed, transitioning from "foundation-model-as-a-connector"
  to "foundation-model-as-a-monolithic architecture".
---

# A Reference Architecture for Designing Foundation Model based Systems

## Quick Facts
- **arXiv ID**: 2304.11090
- **Source URL**: https://arxiv.org/abs/2304.11090
- **Reference count**: 13
- **Primary result**: Proposes a pattern-oriented reference architecture for designing responsible and adaptable foundation model-based AI systems

## Executive Summary
This paper presents a comprehensive reference architecture for designing responsible foundation model-based AI systems, addressing the critical challenges of architecture evolution and responsible AI implementation. The architecture evolves from traditional distributed AI systems to a foundation-model-centric approach, recognizing that foundation models are increasingly absorbing external capabilities. The proposed three-layer architecture (system, operation, and supply chain) provides a structured framework that enables adaptability while maintaining responsible AI practices through embedded governance patterns.

The paper identifies key design decisions and patterns that allow organizations to harness the potential of foundation models while mitigating associated risks. By decomposing responsibilities into capability-aligned components and implementing verification mechanisms throughout the system layers, the architecture ensures both adaptability over time and adherence to responsible AI principles. The reference architecture serves as a template that can be customized based on specific organizational needs, balancing trade-offs between cost, control, modularity, and performance.

## Method Summary
The paper systematically analyzes the evolution of AI system architectures in the foundation model era, transitioning from distributed architectures to monolithic foundation model approaches. Through literature review and project experience, the authors identify critical design decisions and patterns, then propose a pattern-oriented reference architecture consisting of three layers: system layer (deployed components), operation layer (responsible AI tooling), and supply chain layer (development and procurement). The methodology focuses on creating a responsible-AI-by-design template that anticipates architectural evolution and ensures adaptability.

## Key Results
- Proposes a three-layer pattern-oriented reference architecture for foundation model-based AI systems
- Identifies key design decisions including pre-trained vs. fine-tuned vs. sovereign foundation models, and automatic vs. verifier-in-the-loop response generation
- Introduces specific patterns such as microkernel, adapter, verifier-in-the-loop, and RAI black box to ensure responsible AI and adaptability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed architecture enables adaptability by anticipating that external components will be absorbed into foundation models over time.
- Mechanism: By decomposing responsibilities into smaller, capability-aligned components (microkernel pattern), changes can be isolated to specific components that may later be absorbed by the foundation model, minimizing system-wide impact.
- Core assumption: Foundation models will continue to evolve and absorb external capabilities, making component boundaries fluid.
- Evidence anchors:
  - [abstract] "The paper first presents an architecture evolution of AI systems in the era of foundation models, transitioning from 'foundation-model-as-a-connector' to 'foundation-model-as-a-monolithic architecture'."
  - [section] "Responsibility is a concept in a software context that comes from object-oriented design[9]. A responsibility can be an action, a piece of knowledge to be maintained, or a decision to be carried out by a software component."
  - [corpus] Weak evidence - the corpus does not directly address the microkernel pattern or component absorption mechanism.
- Break condition: If foundation models stop absorbing external capabilities or if the rate of absorption exceeds the system's ability to decompose responsibilities, the adaptability benefit diminishes.

### Mechanism 2
- Claim: The reference architecture addresses responsible AI concerns by incorporating governance and verification patterns throughout the system layers.
- Mechanism: Verifier-in-the-loop pattern ensures responses are accurate and responsible before being sent to users, while RAI black box pattern maintains accountability through immutable logging of critical data.
- Core assumption: Verification mechanisms can effectively catch and correct responsible AI issues in foundation model outputs.
- Evidence anchors:
  - [abstract] "The patterns can enable the potential of foundation models while ensuring associated risks."
  - [section] "Veriﬁer-in-the-loop pattern is particularly useful when accuracy and trustworthiness are critical. In this pattern, a veriﬁer is responsible for verifying or modifying the responses returned by the foundation model, or providing feedback to agree or disagree with the responses."
  - [corpus] Weak evidence - the corpus mentions AI safety and guardrails but does not specifically discuss verification patterns.
- Break condition: If verification mechanisms cannot keep pace with the complexity and volume of foundation model outputs, or if they introduce unacceptable latency.

### Mechanism 3
- Claim: The three-layer architecture (system, operation, supply chain) provides a structured approach to managing foundation model-based AI systems.
- Mechanism: Each layer has distinct responsibilities - the system layer contains deployed components, the operation layer provides responsible AI tooling, and the supply chain layer manages development and procurement, creating clear separation of concerns.
- Core assumption: A layered architecture can effectively manage the complexity of foundation model-based systems while supporting evolution.
- Evidence anchors:
  - [abstract] "Finally, we propose a pattern-oriented reference architecture, which provides a responsible-AI-by-design template architectural solution for designing foundation model-based AI systems and considers the evolution of architecture to ensure adaptability over time."
  - [section] "Fig. 2 illustrates a pattern-oriented reference architecture for designing responsible and adaptable foundation model-based AI systems. The architecture comprises three layers: the system layer, which includes the components of the deployed AI system, the operation layer, which provides responsible AI tooling functions to the AI system, and the supply chain layer, which generates the software components that compose the AI system."
  - [corpus] Weak evidence - the corpus discusses architecture options but does not detail a three-layer structure.
- Break condition: If the boundaries between layers become unclear or if communication overhead between layers becomes prohibitive.

## Foundational Learning

- Concept: Software Architecture Evolution Patterns
  - Why needed here: Understanding how AI system architectures evolve from distributed components to monolithic foundation models is crucial for designing adaptable systems.
  - Quick check question: What are the three stages of architecture evolution described in the paper, and how do they differ in terms of foundation model integration?

- Concept: Design Patterns for Responsible AI
  - Why needed here: The paper introduces multiple patterns (verifier-in-the-loop, RAI black box, microkernel, etc.) that address responsible AI concerns specific to foundation models.
  - Quick check question: How does the verifier-in-the-loop pattern differ from traditional validation approaches in conventional software systems?

- Concept: Trade-off Analysis in Architecture Design
  - Why needed here: The paper presents multiple design decisions with associated trade-offs (cost vs. control, modularity vs. performance), requiring understanding of how to evaluate competing priorities.
  - Quick check question: What are the trade-offs between using a chain of foundation models versus an ultra-large foundation model in terms of maintainability and vendor lock-in?

## Architecture Onboarding

- Component map: User query → interaction components (with prompt patterns) → foundation model (fine-tuned/sovereign/chain/ultra-large) → verifier-in-the-loop (if enabled) → domain-specific knowledge base → response to user, with continuous monitoring and risk assessment throughout.
- Critical path: User query → interaction components (with prompt patterns) → foundation model (fine-tuned/sovereign/chain/ultra-large) → verifier-in-the-loop (if enabled) → domain-specific knowledge base → response to user, with continuous monitoring and risk assessment throughout.
- Design tradeoffs: Pre-trained vs. fine-tuned vs. sovereign foundation models (cost vs. control), single bot vs. bot team (complexity vs. performance), automatic response vs. verifier-in-the-loop (efficiency vs. trustworthiness), think aloud vs. think silently (transparency vs. privacy).
- Failure signatures: Inaccurate responses despite verification (verifier-in-the-loop failure), privacy breaches despite privacy-preserving techniques (security failure), component absorption causing system instability (adaptability failure), vendor lock-in preventing component substitution (maintainability failure).
- First 3 experiments:
  1. Implement a simple chatbot using a pre-trained foundation model with prompt patterns and basic interaction components, measuring response quality and latency.
  2. Add verifier-in-the-loop functionality using human verification for a subset of responses, comparing accuracy and user trust metrics against the basic implementation.
  3. Implement microkernel pattern by decomposing a complex component into smaller capabilities, testing component substitution and system adaptability when foundation model capabilities change.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the trade-offs between responsible AI and cost-effectiveness be quantified when choosing between fine-tuning foundation models versus using sovereign foundation models?
- Basis in paper: [explicit] The paper discusses the trade-offs between using pre-trained foundation models (which save costs but pose responsible AI issues) and sovereign foundation models (which ensure responsible AI but require high investment).
- Why unresolved: The paper identifies these trade-offs but does not provide a framework for quantifying them in specific contexts.
- What evidence would resolve it: A decision model that incorporates cost metrics, responsible AI risk assessments, and organizational capabilities to evaluate different foundation model options.

### Open Question 2
- Question: What specific mechanisms can ensure the long-term maintainability of foundation model-based AI systems as components are absorbed by the foundation models over time?
- Basis in paper: [explicit] The paper discusses the moving boundary and interface evolution challenges as foundation models absorb external components, suggesting the use of microkernel and adapter patterns.
- Why unresolved: While patterns are proposed, the paper does not detail how to ensure these patterns remain effective as the architecture evolves.
- What evidence would resolve it: Case studies or empirical research demonstrating the effectiveness of these patterns in maintaining system architecture over extended periods.

### Open Question 3
- Question: How can the effectiveness of verifiers in the verifi-er-in-the-loop pattern be measured and improved over time?
- Basis in paper: [explicit] The paper mentions the verifi-er-in-the-loop pattern for ensuring response accuracy but does not discuss how to measure or improve verifi-er effectiveness.
- Why unresolved: The paper introduces the concept but lacks details on implementation and evaluation of verifi-er performance.
- What evidence would resolve it: Metrics for verifi-er accuracy, benchmarks for different verifi-er types (human, AI, knowledge-based), and methods for continuous improvement of verifi-ers.

## Limitations

- The proposed architecture lacks empirical validation through implementation or case studies, relying instead on theoretical pattern analysis and project experience.
- The paper does not provide specific mechanisms for quantifying trade-offs between responsible AI and cost-effectiveness when selecting foundation model approaches.
- The effectiveness of proposed verification and logging patterns is not evaluated against real-world responsible AI failure cases.

## Confidence

- **High confidence**: The identification of architecture evolution stages from distributed components to monolithic foundation models is well-supported by industry trends and observable patterns.
- **Medium confidence**: The three-layer architectural structure provides a reasonable framework for organizing foundation model-based systems, though the boundaries between layers may become blurred in practice.
- **Low confidence**: The specific patterns proposed (verifier-in-the-loop, RAI black box, etc.) may require significant adaptation as foundation models continue to evolve and new responsible AI challenges emerge.

## Next Checks

1. **Implementation Pilot**: Develop a minimal prototype implementing the three-layer architecture with one foundation model pattern (e.g., fine-tuned model with verifier-in-the-loop) to test the practicality of the reference architecture and identify integration challenges.

2. **Evolution Simulation**: Create a simulation model that tests the architecture's adaptability when foundation models absorb external capabilities at different rates, measuring the impact on system stability and maintenance overhead.

3. **Responsible AI Benchmark**: Design a test suite that evaluates the effectiveness of the proposed verification and logging patterns against real-world responsible AI failure cases, measuring detection accuracy and response time.