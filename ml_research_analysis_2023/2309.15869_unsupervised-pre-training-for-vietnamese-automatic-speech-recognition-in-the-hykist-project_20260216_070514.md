---
ver: rpa2
title: Unsupervised Pre-Training for Vietnamese Automatic Speech Recognition in the
  HYKIST Project
arxiv_id: '2309.15869'
source_url: https://arxiv.org/abs/2309.15869
tags:
- data
- loss
- in-house
- speech
- vietnamese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents automatic speech recognition (ASR) systems for
  low-resource Vietnamese telephone speech in the medical domain, addressing the language
  barriers between doctors and patients in emergency rooms. We build upon the hybrid
  HMM/Neural Network framework and investigate unsupervised pre-training methods using
  wav2vec 2.0, focusing on cross-lingual speech representation learning.
---

# Unsupervised Pre-Training for Vietnamese Automatic Speech Recognition in the HYKIST Project

## Quick Facts
- arXiv ID: 2309.15869
- Source URL: https://arxiv.org/abs/2309.15869
- Reference count: 0
- Primary result: Achieved 23.9% WER on Vietnamese medical domain telephone speech using XLSR-53 pre-training

## Executive Summary
This work presents automatic speech recognition (ASR) systems for low-resource Vietnamese telephone speech in the medical domain, addressing language barriers between doctors and patients in emergency rooms. The study investigates unsupervised pre-training methods using wav2vec 2.0, focusing on cross-lingual speech representation learning through multilingual pre-training on 56k hours from 53 languages. By incorporating various training schedules, data augmentation, and intermediate loss techniques, the authors achieve significant improvements over supervised-only baselines, reducing WER from 35.1% to 23.9% on in-domain HYKIST test data.

## Method Summary
The authors build upon the hybrid HMM/Neural Network framework and investigate unsupervised pre-training methods using wav2vec 2.0 for Vietnamese ASR. They leverage multilingual pre-training on 56k hours of speech data from 53 languages (XLSR-53), followed by fine-tuning on limited Vietnamese in-house data. The approach incorporates various training schedules, data augmentation techniques (SpecAugment, speed and pitch perturbations, reverberation), and intermediate loss techniques (including focal loss). A novel "on-off regularization" technique is proposed, where regularization methods are temporarily disabled during initial training epochs to allow better weight initialization before being reapplied to prevent overfitting.

## Key Results
- Reduced word error rate from 35.1% to 23.9% on HYKIST test data
- XLSR-53 pre-training with intermediate focal loss achieved best performance
- On-off regularization technique further improved results by allowing better weight initialization
- Cross-lingual pre-training proved effective for low-resource Vietnamese ASR

## Why This Works (Mechanism)

### Mechanism 1: Cross-lingual Speech Representation Learning via XLSR-53
The multilingual pre-training on 56k hours from 53 languages provides diverse acoustic patterns that generalize better than monolingual pretraining. The model learns shared latent speech representations across languages, allowing knowledge transfer to low-resource Vietnamese through common phonetic structures. This works because phonetic units are partially shared across languages, so multilingual exposure improves model robustness to Vietnamese acoustic variations.

### Mechanism 2: Intermediate Focal Loss for Class Imbalance
Focal loss in intermediate layers forces the model to focus on hard examples during training by down-weighting easy examples and emphasizing misclassified samples. This reduces overfitting to common patterns while improving learning of rare acoustic patterns, which is particularly beneficial for the challenging HYKIST test data containing difficult acoustic patterns.

### Mechanism 3: On-off Regularization for Robust Convergence
Temporarily disabling regularization techniques during initial training epochs allows better weight initialization through aggressive weight updates without regularization constraints. After initial convergence, regularization is applied to fine-tune without overfitting. This approach works because early training benefits from unconstrained updates while later training requires regularization to generalize.

## Foundational Learning

- **Concept: Hybrid HMM/Neural Network Framework**
  - Why needed here: The thesis explicitly uses hybrid models combining acoustic models with language models for Vietnamese ASR
  - Quick check question: What are the three main components of the hybrid framework described in section 3.1?

- **Concept: Unsupervised Pre-training with wav2vec 2.0**
  - Why needed here: The entire thesis focuses on using wav2vec 2.0 for unsupervised pre-training to address low-resource Vietnamese ASR challenges
  - Quick check question: According to section 3.3.1, what are the three main components of the wav2vec 2.0 architecture?

- **Concept: Contrastive Learning**
  - Why needed here: The thesis relies on contrastive loss for unsupervised pre-training
  - Quick check question: What is the main purpose of contrastive learning in the wav2vec 2.0 framework?

## Architecture Onboarding

- **Component map**: Raw waveform → Feature extraction (7 CNN layers) → Transformer encoding → Quantization → Contrastive loss (pretraining) → Fine-tuning with softmax output → Decoding with language model
- **Critical path**: The model processes raw waveform through CNN layers for feature extraction, applies transformer encoding, quantizes representations, and uses contrastive loss during pretraining before fine-tuning with cross-entropy loss
- **Design tradeoffs**: Large transformer (24 layers) offers better accuracy but higher computational cost vs. Base (12 layers). XLSR-53 initialization provides faster convergence but may not capture 8kHz characteristics perfectly
- **Failure signatures**: Poor performance on HYKIST data suggests domain mismatch or insufficient diversity in pretraining data. Degraded performance on read speech indicates over-specialization to telephone domain
- **First 3 experiments**:
  1. Train supervised-only Transformer baseline on Vietnamese in-house data to establish reference WER
  2. Pretrain wav2vec 2.0 Large1-8 on Vietnamese in-house data from scratch, then fine-tune and evaluate on HYKIST
  3. Fine-tune XLSR-53 1-8 directly on Vietnamese in-house data without pretraining to assess transfer learning effectiveness

## Open Questions the Paper Calls Out

- **Open Question 1**: How effective would continued pre-training be on medical-domain speech data specifically, rather than the general conversational telephone speech used in this study? The paper notes domain mismatch as a key challenge and that continued pre-training on diverse data benefits less diverse data more, but did not investigate medical-domain pre-training.

- **Open Question 2**: Would applying data augmentation techniques during fine-tuning improve performance, as it did during pre-training? The study only applied SpecAugment during fine-tuning, not the speed, pitch, and reverberation perturbations used during pretraining.

- **Open Question 3**: How effective would the On-off Regularization technique be when applied to different pre-training schedules, such as multilingual pre-training or pre-training with random initialization? The paper only applied this to raw waveform from scratch training and noted this as a direction for future work.

- **Open Question 4**: Would a Wav2vec 2.0-Conformer architecture outperform the Wav2vec 2.0-Transformer architecture for Vietnamese ASR in the medical domain? The paper noted that Wav2vec 2.0-Conformer has been popular recently but its effectiveness on Vietnamese has not been investigated.

## Limitations
- Data availability constraints: HYKIST project data remains private (only 3 hours total), limiting reproducibility and independent validation
- Architecture specification gaps: Critical implementation details remain unspecified, making precise replication challenging
- Cross-lingual transfer limitations: Effectiveness depends on phonetic overlap with pretraining languages without analysis of phonetic similarity

## Confidence
- **High confidence**: Baseline supervised-only performance (35.1% WER) and general methodology of using wav2vec 2.0 pretraining for low-resource ASR
- **Medium confidence**: Specific improvements from unsupervised pretraining (23.9% WER) are credible but sensitive to data quality and implementation details
- **Low confidence**: "On-off regularization" technique's effectiveness is demonstrated only on this specific dataset without ablation studies

## Next Checks
1. Independent replication on public Vietnamese ASR data: Test the same pretraining and fine-tuning approach on publicly available Vietnamese speech datasets to verify if similar WER improvements can be achieved without access to private HYKIST data.

2. Phonetic overlap analysis: Conduct an analysis of phonetic similarities between Vietnamese and the 53 languages in XLSR-53 to quantify expected cross-lingual transfer benefits and identify which languages contribute most to Vietnamese ASR performance.

3. Ablation study on proposed techniques: Systematically remove individual components (intermediate focal loss, on-off regularization, data augmentation) to quantify their independent contributions to the final 23.9% WER, establishing which techniques are essential versus complementary.