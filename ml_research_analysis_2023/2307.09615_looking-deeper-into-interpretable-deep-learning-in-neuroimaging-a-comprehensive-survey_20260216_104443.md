---
ver: rpa2
title: 'Looking deeper into interpretable deep learning in neuroimaging: a comprehensive
  survey'
arxiv_id: '2307.09615'
source_url: https://arxiv.org/abs/2307.09615
tags:
- learning
- deep
- brain
- methods
- interpretability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews interpretable deep learning
  models in neuroimaging, focusing on methods, evaluation approaches, and usage trends.
  The paper categorizes interpretability methods into visualization (e.g., Grad-CAM,
  gradients), distillation (e.g., LIME, SHAP), intrinsic (e.g., attention, joint training),
  counterfactual, and influence functions.
---

# Looking deeper into interpretable deep learning in neuroimaging: a comprehensive survey

## Quick Facts
- arXiv ID: 2307.09615
- Source URL: https://arxiv.org/abs/2307.09615
- Reference count: 40
- This survey comprehensively reviews interpretable deep learning models in neuroimaging, focusing on methods, evaluation approaches, and usage trends.

## Executive Summary
This survey provides a comprehensive review of interpretable deep learning models in neuroimaging, categorizing interpretability methods into visualization, distillation, intrinsic, counterfactual, and influence functions. The paper analyzes over 300 neuroimaging studies to identify usage trends and evaluates various evaluation metrics for interpretability. The review emphasizes the importance of interpretability for advancing scientific understanding of brain disorders and provides practical recommendations for future research, including the need for strong out-of-distribution tests, stability checks, and expert validation of explanations.

## Method Summary
The paper conducts a systematic literature review of over 300 neuroimaging studies, categorizing interpretability methods into five main types: visualization (Grad-CAM, gradients), distillation (LIME, SHAP), intrinsic (attention, joint training), counterfactual, and influence functions. The analysis focuses on identifying usage trends, evaluating axiomatic properties of different methods, and discussing evaluation metrics for interpretability. The authors synthesize findings to provide recommendations for improving interpretability practices in neuroimaging research.

## Key Results
- Grad-CAM, SHAP, and integrated gradients are the most frequently used interpretability methods in neuroimaging studies
- Visualization methods are preferred due to their intuitive spatial heatmaps that align with clinicians' expectations
- Intrinsic interpretability methods remain underexplored despite their potential for more reliable explanations
- No neuroimaging studies have yet used counterfactual explanations to understand model decision-making processes

## Why This Works (Mechanism)

### Mechanism 1
Visualization methods (Grad-CAM, gradients) are most frequently used because they produce intuitive, spatial heatmaps that align with clinicians' expectations for anatomical interpretability. These methods highlight regions in input space that most influenced the model prediction, allowing direct comparison with known biomarkers in neuroimaging. Core assumption: Clinicians can validate the highlighted regions against established anatomical knowledge, creating trust in model predictions. Evidence: The review analyzes over 300 neuroimaging studies, revealing that Grad-CAM, SHAP, and integrated gradients are the most frequently used. Break Condition: If the highlighted regions do not align with established anatomical biomarkers, clinicians lose trust in the model regardless of predictive performance.

### Mechanism 2
Distillation methods (LIME, SHAP) are gaining traction because they provide feature-level importance scores that can be aggregated across patients to identify consistent disease patterns. These methods train separate interpretable models on perturbed samples to estimate feature contributions, which can be averaged to find population-level biomarkers. Core assumption: Consistent feature importance across patients indicates true disease mechanisms rather than spurious correlations. Evidence: The paper highlights the importance of interpretability for advancing scientific understanding of brain disorders. Break Condition: If feature importance varies wildly across patients with similar diagnoses, the method fails to identify true biomarkers.

### Mechanism 3
Intrinsic methods (attention mechanisms, joint training) are less common but potentially more reliable because they integrate interpretability into the model architecture from the start. These methods force the model to learn interpretable representations during training, rather than trying to explain a black-box after the fact. Core assumption: Models trained to be interpretable from the beginning will produce more reliable explanations than post-hoc methods applied to black-box models. Evidence: Intrinsic methods focus on interpretation as part of the model design or training rather than doing a separate post hoc analysis. Very few neuroimaging studies so far have considered interpretability as part of the algorithmic aspect of the model from its inception. Break Condition: If the intrinsic interpretability mechanisms interfere with model performance, practitioners will prefer post-hoc methods despite their limitations.

## Foundational Learning

- Concept: Axiomatic properties of attribution methods
  - Why needed here: Different attribution methods satisfy different mathematical properties, and understanding these helps choose appropriate methods for specific neuroimaging tasks
  - Quick check question: Which axiom ensures that if a feature doesn't affect the model output mathematically, the attribution to that feature is always zero?

- Concept: Evaluation metrics for interpretability
  - Why needed here: Neuroimaging studies need objective ways to validate that generated explanations correspond to true biological mechanisms rather than artifacts
  - Quick check question: What metric measures the proportion of total attributions that reside within the relevance area defined by ground truth?

- Concept: Transfer learning in neuroimaging
  - Why needed here: Limited training data in neuroimaging studies makes transfer learning essential, but understanding what knowledge transfers is crucial for interpretability
  - Quick check question: Why might transferring knowledge from natural images to medical images provide only marginal performance improvements?

## Architecture Onboarding

- Component map: Literature search -> Method classification -> Evidence extraction -> Trend analysis -> Synthesis of recommendations
- Critical path: Literature search -> Method classification -> Evidence extraction -> Trend analysis -> Synthesis of recommendations
- Design tradeoffs: Comprehensive coverage vs. focus on most impactful methods; theoretical rigor vs. practical applicability
- Failure signatures: Over-reliance on single interpretation method; lack of validation against clinical knowledge; failure to account for distribution shifts
- First 3 experiments:
  1. Replicate usage trend analysis on a subset of 50 recent neuroimaging papers to validate methodology
  2. Compare explanations from Grad-CAM vs. SHAP on a common Alzheimer's dataset to understand method disagreement
  3. Test counterfactual explanations on a small neuroimaging dataset to explore underlying biological mechanisms

## Open Questions the Paper Calls Out

### Open Question 1
How can we develop a unified framework for comparing interpretability methods across neuroimaging studies to establish standardized benchmarks? Basis: The paper mentions that "a unified framework [32] in interpretable neuroimaging research may be useful so that the findings across the studies can be directly compared to share advancement benchmarks." Why unresolved: While various interpretability methods are used in neuroimaging studies, there is no agreed-upon standard for comparing their effectiveness or reliability across different research contexts and datasets. What evidence would resolve it: Development and validation of a standardized set of evaluation metrics and protocols that can be consistently applied across different neuroimaging studies and interpretability methods.

### Open Question 2
What are the most effective strategies for designing out-of-distribution (o.o.d) tests in neuroimaging to ensure model generalization to real-world clinical scenarios? Basis: The paper suggests that "we must design suitable o.o.d tests as suggested by Geirhos et al. [44]" and acknowledges that "building strong o.o.d test cases may not be so easy for neuroimaging." Why unresolved: Neuroimaging data is complex and varied, making it challenging to create representative out-of-distribution test cases that accurately reflect the diversity of real-world clinical scenarios. What evidence would resolve it: Empirical studies demonstrating the effectiveness of different o.o.d test strategies in improving model generalization and clinical applicability across diverse neuroimaging datasets.

### Open Question 3
How can counterfactual explanations be effectively integrated into neuroimaging research to uncover underlying biological mechanisms of brain disorders? Basis: The paper states that "we believe counterfactual explanations may help understand the underlying biological mechanism that potentially caused the specific disorder in the first place" and notes that "no neuroimaging study has ever used counterfactuals to understand the model's decision-making process." Why unresolved: While counterfactual explanations show promise in other domains, their application to neuroimaging research is unexplored, and the specific methods and validation approaches for this domain are unknown. What evidence would resolve it: Successful implementation and validation of counterfactual explanation methods in neuroimaging studies, demonstrating their ability to reveal biologically meaningful insights into brain disorders.

## Limitations

- Lack of standardized evaluation metrics for interpretability methods makes cross-study comparisons challenging
- Insufficient validation that frequently used methods (Grad-CAM, SHAP) produce clinically meaningful explanations rather than spurious correlations
- Intrinsic interpretability methods remain underexplored despite their potential for more reliable explanations

## Confidence

- **High confidence**: The methodology for categorizing interpretability methods and analyzing usage trends is well-documented and follows established survey practices. The identification of Grad-CAM, SHAP, and integrated gradients as the most frequently used methods is supported by clear evidence from the analyzed studies.
- **Medium confidence**: The recommendations for future research directions are reasonable but may be influenced by the authors' perspectives on what constitutes valuable interpretability. The emphasis on axiomatic properties and evaluation metrics is well-founded but may not capture all relevant considerations.
- **Low confidence**: The claim that intrinsic methods are "potentially more reliable" lacks empirical validation in the neuroimaging context. While theoretically sound, this assertion requires further testing to determine if the trade-offs with model performance are acceptable in practice.

## Next Checks

1. **Cross-validation of usage trends**: Replicate the analysis of interpretability method usage on an independent sample of 50 recent neuroimaging papers to verify the reported trends for Grad-CAM, SHAP, and integrated gradients.
2. **Clinical validation study**: Conduct a small-scale study where clinicians evaluate explanations generated by different interpretability methods (visualization vs. distillation) on the same neuroimaging dataset to assess their clinical utility and alignment with established biomarkers.
3. **Intrinsic vs. post-hoc comparison**: Compare model performance and explanation quality between intrinsically interpretable architectures and post-hoc methods on a common neuroimaging task to empirically test whether intrinsic methods provide more reliable explanations despite potential performance trade-offs.