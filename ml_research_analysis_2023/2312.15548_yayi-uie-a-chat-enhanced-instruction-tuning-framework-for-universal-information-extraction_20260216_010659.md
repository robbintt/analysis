---
ver: rpa2
title: 'YAYI-UIE: A Chat-Enhanced Instruction Tuning Framework for Universal Information
  Extraction'
arxiv_id: '2312.15548'
source_url: https://arxiv.org/abs/2312.15548
tags:
- extraction
- chinese
- datasets
- information
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes YAYI-UIE, a chat-enhanced instruction tuning
  framework for universal information extraction that supports both Chinese and English.
  The key idea is to first fine-tune a base LLM on dialogue data to obtain a chat
  model with common understanding abilities, and then fine-tune this chat model on
  a comprehensive Chinese IE benchmark dataset combined with an existing English benchmark.
---

# YAYI-UIE: A Chat-Enhanced Instruction Tuning Framework for Universal Information Extraction

## Quick Facts
- arXiv ID: 2312.15548
- Source URL: https://arxiv.org/abs/2312.15548
- Reference count: 10
- Primary result: Achieves state-of-the-art performance on Chinese information extraction tasks with 91.19% average F1 on Chinese NER datasets

## Executive Summary
YAYI-UIE introduces a two-stage instruction tuning framework for universal information extraction that supports both Chinese and English. The approach first fine-tunes a base LLM on dialogue data to develop common understanding abilities, then fine-tunes the resulting chat model on a comprehensive Chinese IE benchmark combined with existing English datasets. The framework demonstrates state-of-the-art performance on Chinese datasets under supervised settings and remarkable zero-shot generalization capabilities, outperforming previous methods on Chinese relation extraction and event extraction tasks.

## Method Summary
The YAYI-UIE framework employs a two-step instruction tuning approach. First, a pre-trained LLM (Baichuan2-13B) is fine-tuned on dialogue instruction data to create a chat model with enhanced instruction-following abilities. Second, this chat model is fine-tuned on a comprehensive dataset combining 16 Chinese IE datasets (covering NER, RE, and EE tasks) with existing English benchmarks. All tasks use a standardized JSON output format, enabling uniform modeling across different IE tasks. The framework leverages instruction tuning rather than traditional fine-tuning to improve both task-specific performance and cross-lingual generalization.

## Key Results
- Achieves 91.19% average F1 on Chinese NER datasets compared to 88.48% for BERT-base
- Demonstrates state-of-the-art performance on Chinese information extraction tasks under supervised settings
- Shows remarkable generalization under zero-shot settings, outperforming previous methods on Chinese RE and EE tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-step instruction tuning approach improves both task-specific performance and generalization across languages.
- Mechanism: First fine-tuning on dialogue data builds common understanding abilities in the model, which is then leveraged in the second step when fine-tuning on task-specific IE data. This sequential process allows the model to first learn general instruction-following capabilities before specializing in information extraction tasks.
- Core assumption: Dialogue data contains diverse linguistic patterns that can be transferred to improve understanding of IE instructions.
- Evidence anchors:
  - [abstract]: "Specifically, we utilize dialogue data and information extraction data to enhance the information extraction performance jointly."
  - [section]: "We first fine-tune a pre-trained LLM on the dialogue instruction corpus to enhance the instruction-following ability."
- Break condition: If dialogue data doesn't contain sufficient diversity or if the second fine-tuning step overfits to specific schemas.

### Mechanism 2
- Claim: The comprehensive Chinese IE benchmark dataset addresses the language imbalance in existing methods.
- Mechanism: By constructing 16 Chinese datasets across multiple domains and combining them with existing English benchmarks, the model receives balanced training data that improves performance on Chinese IE tasks while maintaining English capabilities.
- Core assumption: Language-specific data is necessary for achieving high performance in that language.
- Evidence anchors:
  - [abstract]: "we construct the largest and most comprehensive Chinese IE benchmark dataset and combined it with the existing English benchmark."
  - [section]: "Due to the lack of Chinese datasets in existing information extraction benchmarks, we collect 16 Chinese datasets for NER, RE, and EE tasks from diverse domains."
- Break condition: If the constructed Chinese datasets don't represent the full diversity of Chinese IE tasks or if the combination ratio with English data is suboptimal.

### Mechanism 3
- Claim: The JSON output format standardization enables uniform modeling across different IE tasks.
- Mechanism: By using a consistent JSON format for all tasks (NER, RE, EE), the model learns to generate structured outputs in a unified way, reducing the complexity of handling multiple output formats.
- Core assumption: A single output format can effectively represent all types of structured information extraction tasks.
- Evidence anchors:
  - [abstract]: "Specifically, our YAYI-UIE chooses JSON as the output format for all the IE tasks."
  - [section]: "Ouput = YAYI-UIE (Instruction, Input)" and "Instruction = {task, option, format}"
- Break condition: If JSON format becomes too verbose for certain tasks or if some IE tasks require output structures that JSON cannot efficiently represent.

## Foundational Learning

- Concept: Instruction Tuning
  - Why needed here: The paper's core contribution relies on instruction tuning to adapt LLMs for specific IE tasks rather than traditional fine-tuning approaches.
  - Quick check question: What's the difference between instruction tuning and traditional fine-tuning in the context of adapting LLMs?

- Concept: Multi-task Learning
  - Why needed here: The framework handles multiple IE tasks (NER, RE, EE) simultaneously, requiring understanding of how to train models on diverse but related tasks.
  - Quick check question: How does multi-task learning help with generalization compared to single-task training?

- Concept: Zero-shot Learning
  - Why needed here: The paper evaluates performance on unseen datasets, which requires understanding of how models can generalize to new tasks without specific training.
  - Quick check question: What characteristics of the instruction tuning approach enable zero-shot performance on new datasets?

## Architecture Onboarding

- Component map:
  - Base LLM (Baichuan2-13B) -> Dialogue instruction corpus -> Chat model -> Chinese IE benchmark (16 datasets) + English benchmark -> Universal IE model

- Critical path:
  1. Load base LLM
  2. First instruction tuning with dialogue data
  3. Construct combined IE dataset
  4. Second instruction tuning with IE data
  5. Evaluation on supervised and zero-shot tasks

- Design tradeoffs:
  - Language balance: Chinese vs English data ratio
  - Output format: JSON vs other structured formats
  - Training stages: Single vs multi-stage fine-tuning
  - Dataset diversity: Domain coverage vs task coverage

- Failure signatures:
  - Poor performance on one language suggests data imbalance
  - Degradation in zero-shot settings indicates overfitting
  - Inconsistent output formats suggest issues in the JSON formatting step

- First 3 experiments:
  1. Test baseline performance with single-stage fine-tuning vs two-stage approach
  2. Evaluate impact of Chinese dataset size on overall performance
  3. Measure zero-shot generalization on held-out datasets from different domains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of YAYI-UIE on Chinese information extraction tasks compare to models specifically trained on Chinese data without the English instruction tuning component?
- Basis in paper: [explicit] The paper mentions that YAYI-UIE achieves state-of-the-art performance on Chinese datasets under supervised settings, with an average F1 of 91.19% on Chinese NER datasets compared to 88.48% for BERT-base. However, it does not provide a direct comparison to models trained solely on Chinese data.
- Why unresolved: The paper does not provide a direct comparison to models trained solely on Chinese data, making it difficult to determine the extent to which the English instruction tuning component contributes to the model's performance on Chinese tasks.
- What evidence would resolve it: A direct comparison of YAYI-UIE's performance on Chinese tasks to models trained solely on Chinese data, with and without the English instruction tuning component, would help determine the impact of the English instruction tuning on Chinese task performance.

### Open Question 2
- Question: How does the chat-enhanced instruction tuning approach impact the model's ability to handle out-of-distribution or unseen data compared to traditional instruction tuning methods?
- Basis in paper: [inferred] The paper mentions that YAYI-UIE displays remarkable generalization performance under zero-shot settings, outperforming previous methods on Chinese RE and EE tasks. This suggests that the chat-enhanced instruction tuning approach may improve the model's ability to handle unseen data.
- Why unresolved: While the paper provides evidence of YAYI-UIE's strong performance on zero-shot tasks, it does not directly compare the chat-enhanced instruction tuning approach to traditional instruction tuning methods in terms of handling out-of-distribution or unseen data.
- What evidence would resolve it: A direct comparison of YAYI-UIE's performance on zero-shot tasks to models trained using traditional instruction tuning methods, as well as a systematic evaluation of the model's ability to handle out-of-distribution data, would help determine the impact of the chat-enhanced instruction tuning approach on handling unseen data.

### Open Question 3
- Question: How does the size and quality of the dialogue data used in the chat-enhanced instruction tuning process impact the model's performance on information extraction tasks?
- Basis in paper: [inferred] The paper mentions that dialogue data is used to fine-tune the base LLM to obtain a chat model with common understanding abilities, which is then used for information extraction instruction tuning. However, it does not provide specific details on the size or quality of the dialogue data used.
- Why unresolved: The paper does not provide information on the size or quality of the dialogue data used in the chat-enhanced instruction tuning process, making it difficult to determine the impact of these factors on the model's performance.
- What evidence would resolve it: An analysis of the relationship between the size and quality of the dialogue data used in the chat-enhanced instruction tuning process and the model's performance on information extraction tasks, as well as a comparison to models trained with different amounts or qualities of dialogue data, would help determine the impact of these factors on the model's performance.

## Limitations
- Reliance on extensive dataset construction, particularly the 16 Chinese IE datasets, creates scalability challenges for extending to other languages or domains
- Two-stage instruction tuning approach requires significant computational resources for training large language models
- JSON output format may not be optimal for all IE tasks and could introduce overhead for simpler extraction scenarios

## Confidence
**High Confidence**: The reported performance improvements on Chinese datasets (91.19% F1 average vs 88.48% for BERT-base) are supported by the comprehensive benchmark construction and standard evaluation metrics.

**Medium Confidence**: The zero-shot generalization claims are supported by experimental results but would benefit from testing on a broader range of unseen datasets across different domains.

**Low Confidence**: The long-term stability of the JSON output format across evolving IE task requirements and the model's ability to handle edge cases in complex extraction scenarios are not thoroughly evaluated.

## Next Checks
1. **Dataset Generalization Test**: Evaluate YAYI-UIE on held-out Chinese IE datasets from domains not represented in the training corpus (e.g., medical or legal domains) to assess true zero-shot capabilities beyond the reported tasks.

2. **Resource Efficiency Analysis**: Measure the computational overhead of the two-stage instruction tuning approach compared to single-stage fine-tuning on the same data, including training time, memory usage, and parameter updates required.

3. **Output Format Scalability Test**: Test YAYI-UIE's performance when applied to IE tasks with output structures that deviate significantly from the JSON format, such as hierarchical or graph-based representations, to evaluate the flexibility limitations of the standardized output approach.