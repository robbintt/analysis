---
ver: rpa2
title: Outlier detection using flexible categorisation and interrogative agendas
arxiv_id: '2312.12010'
source_url: https://arxiv.org/abs/2312.12010
tags:
- outlier
- algorithm
- detection
- agendas
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel framework for outlier detection using
  formal concept analysis (FCA) and interrogative agendas. The key idea is to represent
  different categorization strategies as "interrogative agendas" (sets of features)
  and learn their relative importance for outlier detection.
---

# Outlier detection using flexible categorisation and interrogative agendas

## Quick Facts
- arXiv ID: 2312.12010
- Source URL: https://arxiv.org/abs/2312.12010
- Authors: [Not specified in source]
- Reference count: 40
- One-line primary result: Novel framework using formal concept analysis and interrogative agendas that performs at par with or better than common outlier detection methods while providing interpretable explanations

## Executive Summary
This paper introduces a novel outlier detection framework that leverages formal concept analysis (FCA) and interrogative agendas to provide both high performance and interpretability. The approach represents different categorization strategies as sets of features ("interrogative agendas") and learns their relative importance for identifying outliers. Two algorithms are developed: an unsupervised FCA-based method that assigns outlier scores based on concept lattice sizes across different agendas, and a supervised meta-learning algorithm that learns optimal weights for these agendas. The framework is evaluated on 26 benchmark datasets and shows performance comparable to or better than existing methods while offering interpretable explanations for its decisions.

## Method Summary
The method converts datasets into formal contexts using interval scaling, then computes closures and concept lattice sizes for different feature sets (agendas). The unsupervised algorithm assigns outlier scores inversely proportional to concept closure sizes across all agendas. The supervised meta-learning algorithm learns optimal weights for different agendas using gradient descent on labeled training data, minimizing a loss function that combines outlier scores for known outliers and inliers. The framework provides both local explanations (identifying which agendas contributed most to specific outlier scores) and global explanations (identifying which agendas are most important overall).

## Key Results
- The proposed algorithms perform at par with or better than existing outlier detection methods on 26 benchmark datasets
- The framework provides both local and global explanations for outlier detection decisions
- The supervised meta-learning algorithm improves performance by learning optimal weights for different feature combinations
- The approach successfully balances performance with interpretability, addressing a key limitation of many black-box outlier detection methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The unsupervised algorithm works by assigning outlier scores inversely proportional to the size of the concept generated by each object under different feature sets.
- Mechanism: For each object and each agenda (set of features), the algorithm computes the closure size in the corresponding concept lattice and transforms it into an outlier score via an exponential decay function. Smaller closure sizes indicate more outliers.
- Core assumption: Objects that are outliers under a given agenda will generate smaller concept closures because fewer other objects share the same feature combinations.
- Evidence anchors:
  - [abstract] "Results show that the proposed methods perform at par with or better than existing approaches, while providing interpretable explanations for their decisions."
  - [section] "For any formal context P = (A, X, I), and any object a ∈ A, the closure of a in P... represents the set of objects which are 'similar' or 'almost indistinguishable from a'."
- Break condition: If the dataset contains many objects with identical feature combinations, closure sizes will be artificially large and the algorithm will fail to distinguish outliers.

### Mechanism 2
- Claim: The supervised meta-learning algorithm improves performance by learning optimal weights for different agendas based on labeled training data.
- Mechanism: The algorithm uses gradient descent to learn a weight function that minimizes a loss function combining outlier scores for known outliers and inliers. These weights represent the relative importance of different feature combinations for the specific task.
- Core assumption: Different feature combinations are more or less useful for outlier detection depending on the specific dataset and task, and these optimal combinations can be learned from labeled examples.
- Evidence anchors:
  - [abstract] "We then present a supervised meta-learning algorithm to learn suitable (fuzzy) agendas for categorization as sets of features with different weights or masses."
  - [section] "For a given weight function w : T → R, the outlier degree of a (resp. the membership of a in the class k) assigned by the algorithm Alg acting on a non-crisp categorization described by w is..."
- Break condition: If the training data is too small or not representative, the learned weights may not generalize well to new data.

### Mechanism 3
- Claim: The interpretability of the algorithm comes from the ability to trace outlier scores back to specific feature combinations and their learned importance.
- Mechanism: The algorithm provides both local explanations (which agendas contributed most to a specific object's outlier score) and global explanations (which agendas are most important overall), using the weights learned by the meta-learning algorithm.
- Core assumption: Human users can understand and benefit from explanations that show which feature combinations are driving outlier detection decisions.
- Evidence anchors:
  - [abstract] "These algorithms provide both local and global explanations of their results."
  - [section] "For any object a which is assigned a high outlier degree, the algorithm can provide explanation of the following form: 'The algorithm assigns outlier degree outdeg(a) to a which is contributed mainly by a having high outlier degrees outdeg(a, Y1), outdeg(a, Y2), ..., outdeg(a, Yn) w.r.t. agendas Y1, Y2, ..., Yn.'"
- Break condition: If the number of features is very large, the number of possible agendas grows exponentially, making it difficult to provide meaningful explanations.

## Foundational Learning

- Concept: Formal Concept Analysis (FCA) and concept lattices
  - Why needed here: The entire outlier detection approach is built on FCA, using concept lattices to represent categorizations under different feature sets.
  - Quick check question: What is the relationship between a formal context, its concept lattice, and the closures of objects?

- Concept: Dempster-Shafer theory and mass functions
  - Why needed here: Fuzzy agendas are represented as mass functions that assign importance scores to different feature sets, allowing for more nuanced representations of epistemic stances.
  - Quick check question: How does a mass function on feature sets differ from a simple probability distribution, and why is this distinction important for representing interrogative agendas?

- Concept: Gradient descent and meta-learning
  - Why needed here: The supervised algorithm uses gradient descent to learn optimal weights for different agendas based on labeled training data, requiring understanding of optimization techniques.
  - Quick check question: What is the loss function used in the meta-learning algorithm, and how does it balance the contributions of outliers and inliers?

## Architecture Onboarding

- Component map:
  Data preprocessing -> FCA engine -> Unsupervised algorithm / Meta-learning component -> Explanation generator

- Critical path:
  1. Preprocess data using interval scaling
  2. For each object and agenda, compute closure size in corresponding concept lattice
  3. Transform closure sizes into outlier scores using exponential decay
  4. (Supervised only) Use meta-learning to optimize agenda weights based on training data
  5. Aggregate scores using learned weights (supervised) or equal weights (unsupervised)
  6. Generate explanations showing agenda contributions

- Design tradeoffs:
  - Computational complexity vs. explainability: Using all possible agendas provides better explanations but increases computation time exponentially
  - Granularity of discretization: More bins provide finer distinctions but may lead to overfitting or sparse data issues
  - Balance between local and global explanations: Focusing on local explanations may miss broader patterns, while focusing on global explanations may miss important individual cases

- Failure signatures:
  - High variance in performance across different datasets: May indicate sensitivity to dataset characteristics or need for parameter tuning
  - Poor performance on datasets with many categorical features: May indicate issues with interval scaling or need for different preprocessing
  - Slow computation on datasets with many features: May indicate need for agenda space reduction strategies

- First 3 experiments:
  1. Run unsupervised algorithm on a small dataset with known outliers to verify basic functionality and check that outlier scores correlate with ground truth
  2. Test supervised algorithm on the same dataset with a small training set to verify that learned weights improve performance over equal weighting
  3. Generate explanations for a few outlier cases and verify that the identified contributing agendas make intuitive sense given the data characteristics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the choice of crisp agenda space (e.g., small agendas vs. designated agendas based on expert knowledge) impact the performance of the supervised outlier detection algorithm?
- Basis in paper: [explicit] The paper discusses different strategies for choosing the space of crisp agendas in Section 4.1, including choosing small agendas, using designated agendas based on prior knowledge, and adaptively choosing agendas.
- Why unresolved: The paper does not provide a comparative study of the effectiveness of these different strategies on different outlier detection datasets.
- What evidence would resolve it: A systematic evaluation of the supervised algorithm's performance using different strategies for choosing the crisp agenda space on a variety of benchmark datasets.

### Open Question 2
- Question: Can the learned agendas from the meta-learning algorithm be effectively transferred to other classification tasks beyond outlier detection?
- Basis in paper: [explicit] The paper mentions in Section 7 that it intends to study transferability analysis of learned agendas in future work.
- Why unresolved: The paper does not provide any empirical results or theoretical analysis on the transferability of learned agendas to other tasks.
- What evidence would resolve it: Experimental results showing the performance of the supervised algorithm on classification tasks using agendas learned from outlier detection tasks, and vice versa.

### Open Question 3
- Question: How can the interpretability of the supervised algorithm be improved beyond the current level of explainability provided by the learned weights of different agendas?
- Basis in paper: [explicit] The paper discusses the explainability of both the unsupervised and supervised algorithms in Section 6, but notes that the supervised algorithm is not fully white-box due to the use of stochastic gradient descent.
- Why unresolved: The paper does not propose any specific methods or techniques to further enhance the interpretability of the supervised algorithm.
- What evidence would resolve it: A new interpretable variant of the supervised algorithm, along with empirical results demonstrating improved explainability while maintaining comparable performance.

## Limitations
- The exponential decay function for outlier scoring may be sensitive to the choice of γ parameter, which could affect performance across different datasets
- The meta-learning algorithm requires labeled training data, which may not be available in unsupervised outlier detection scenarios
- Computational complexity grows exponentially with the number of features due to the need to consider all possible agendas

## Confidence

- **High confidence**: The basic FCA-based outlier detection mechanism using concept lattice sizes (Mechanism 1) is well-established and theoretically sound
- **Medium confidence**: The meta-learning algorithm's ability to learn optimal agenda weights (Mechanism 2) is supported by the theoretical framework but requires empirical validation across diverse datasets
- **Medium confidence**: The interpretability claims (Mechanism 3) are reasonable given the framework structure, but user studies would strengthen this aspect

## Next Checks

1. Test the algorithm's sensitivity to different binning strategies and γ parameters on a small, controlled dataset to establish optimal parameter ranges
2. Compare the meta-learning algorithm's performance with and without labeled training data on datasets where ground truth outliers are known
3. Conduct a user study to evaluate the practical utility and understandability of the local and global explanations provided by the framework