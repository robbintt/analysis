---
ver: rpa2
title: Pre or Post-Softmax Scores in Gradient-based Attribution Methods, What is Best?
arxiv_id: '2306.13197'
source_url: https://arxiv.org/abs/2306.13197
tags:
- post-softmax
- scores
- gradients
- grad-cam
- pre-softmax
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares gradient-based attribution methods using pre-softmax
  versus post-softmax scores in neural network classifiers. The key insight is that
  post-softmax gradients better capture the impact of inputs on classification outcomes
  because they directly relate to information gain and are less affected by training
  variations.
---

# Pre or Post-Softmax Scores in Gradient-based Attribution Methods, What is Best?

## Quick Facts
- arXiv ID: 2306.13197
- Source URL: https://arxiv.org/abs/2306.13197
- Authors: 
- Reference count: 9
- Key outcome: Post-softmax gradients better capture input impact on classification outcomes through direct information gain relationship and provide more consistent results across equivalent models

## Executive Summary
This paper investigates whether gradient-based attribution methods should use pre-softmax or post-softmax scores when explaining neural network predictions. The authors demonstrate that post-softmax gradients provide more robust explanations because they directly relate to information gain through the cross-entropy loss function and are invariant to constant shifts in pre-softmax scores. While pre-softmax gradients avoid vanishing gradient issues, they can produce inconsistent results across equivalent models that have the same functional output. The study suggests post-softmax scores are preferable for most gradient-based attribution methods, with log-softmax scores proposed as a potential third alternative.

## Method Summary
The study compares gradient-based attribution methods (Grad-CAM, Grad-CAM++, RSI Grad-CAM) using pre-softmax versus post-softmax scores in a VGG19 network pretrained on ImageNet. The authors derive mathematical relationships showing how pre-softmax gradients transform through the softmax layer to become post-softmax gradients. They evaluate attribution quality and consistency across equivalent models by adding constant offsets to pre-softmax scores and comparing attribution maps. The core comparison involves implementing both gradient variants and measuring their stability and interpretability across multiple training runs and network architectures.

## Key Results
- Post-softmax gradients directly relate to information gain through the equation ∂L/∂x = -∂log yc/∂x, providing better interpretability
- Post-softmax gradients are invariant to constant shifts in pre-softmax scores, preventing inconsistent attribution maps across equivalent models
- Pre-softmax gradients can vary significantly across models with identical outputs, leading to unreliable explanations
- Log-softmax scores show promise as a third alternative but require further validation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Post-softmax gradients directly capture information gain in classification tasks
- Mechanism: The cross-entropy loss function reaches minimum when predicted class distribution matches ground truth. Post-softmax gradients relate directly to the loss gradient through the equation ∂L/∂x = -∂log yc/∂x, where yc is the predicted probability for the correct class c. This creates a direct mapping between gradient magnitude and information gain.
- Core assumption: The classification task uses cross-entropy loss and one-hot encoded ground truth labels
- Evidence anchors:
  - [abstract] states that post-softmax gradients "better capture the impact of inputs on classification outcomes because they directly relate to information gain"
  - [section 2.3] provides the mathematical derivation showing ∂L/∂x = -∂log yc/∂x
  - [corpus] shows related work on gradient-based attribution methods (FMR=0.611) but lacks direct evidence for information gain claims
- Break condition: If loss function differs from cross-entropy or if multi-label classification uses non-one-hot targets, the information gain relationship breaks down

### Mechanism 2
- Claim: Post-softmax gradients are invariant to constant shifts in pre-softmax scores
- Mechanism: Adding a constant t to all pre-softmax scores z'i = zi + t doesn't change the softmax outputs yc or their gradients. This means post-softmax gradients remain consistent across models that are functionally equivalent in their output behavior but may have different internal representations.
- Core assumption: The softmax function properly normalizes inputs and maintains gradient relationships under constant shifts
- Evidence anchors:
  - [section 2.2] demonstrates mathematically that z'i = zi + t produces identical yc and ∂yc/∂x values
  - [section 2.2] explicitly states this prevents situations where "equally well trained networks may lead to radically different saliency maps"
  - [corpus] lacks direct evidence but related works on gradient-based attribution methods support the general framework
- Break condition: If the model architecture introduces non-linear transformations between pre-softmax and post-softmax layers, the invariance property may not hold

### Mechanism 3
- Claim: Post-softmax gradients isolate the impact of inputs on individual class predictions
- Mechanism: Unlike pre-softmax gradients which require summing over all classes to compute loss gradients, post-softmax gradients for a single class yc directly capture its contribution to the loss. This simplifies attribution by focusing on the relevant class output.
- Core assumption: The classification task has a single correct class per input
- Evidence anchors:
  - [section 2.3] shows that ∂L/∂x = -∂log yc/∂x for the correct class c, while gradients for other classes don't contribute
  - [section 2.3] contrasts this with pre-softmax gradients requiring summation over all classes: ∂L/∂x = -Σ(tc - yc)∂zc/∂x
  - [corpus] evidence is weak, with related works not directly addressing this isolation property
- Break condition: In multi-label classification or when using non-one-hot targets, the isolation property fails as multiple classes contribute to the loss

## Foundational Learning

- Concept: Chain rule for composite functions
  - Why needed here: Essential for deriving how gradients propagate through the softmax layer and relate pre-softmax to post-softmax gradients
  - Quick check question: If f(y) = y² and y = x², what is df/dx using the chain rule?

- Concept: Information theory and cross-entropy
  - Why needed here: The paper's core argument relies on interpreting cross-entropy loss as measuring information gain between predicted and actual distributions
  - Quick check question: For a binary classification with predicted probabilities [0.8, 0.2] and true labels [1, 0], what is the cross-entropy loss?

- Concept: Softmax function properties
  - Why needed here: Understanding softmax's behavior (normalization, gradient properties, saturation) is crucial for comparing pre- and post-softmax gradient approaches
  - Quick check question: What happens to the gradient of softmax outputs as input values become very large or very small?

## Architecture Onboarding

- Component map: Input → Convolutional blocks → Fully connected layer (pre-softmax scores z) → Softmax activation (post-softmax scores y) → Cross-entropy loss
- Critical path: For attribution analysis, the critical path is: Input features → pre-softmax gradients ∂zi/∂x → post-softmax gradients ∂yc/∂x (via softmax derivative) → loss gradient ∂L/∂x
- Design tradeoffs: Post-softmax gradients provide better interpretability and consistency but may suffer from vanishing gradients in saturated regions; pre-softmax gradients avoid saturation issues but can vary significantly across equivalent models
- Failure signatures: Inconsistent attribution maps across equivalent models (pre-softmax issue), attribution maps that don't align with prediction confidence (post-softmax saturation), attribution maps that don't correlate with information gain
- First 3 experiments:
  1. Compare Grad-CAM attribution maps using pre-softmax vs post-softmax scores on a VGG network for correctly classified images
  2. Test gradient invariance by adding constant offsets to pre-softmax scores and measuring changes in attribution maps
  3. Evaluate attribution map consistency across multiple training runs of the same architecture on the same dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the precise mathematical conditions under which post-softmax gradients provide more consistent explanations than pre-softmax gradients across different training runs?
- Basis in paper: [explicit] The paper discusses how two different trainings of the network may produce two different models for which outputs and post-softmax gradients are the same, while the pre-softmax gradients are very different
- Why unresolved: The paper provides an example of inconsistency but doesn't establish general conditions for when this occurs
- What evidence would resolve it: Mathematical proofs showing when pre-softmax gradients diverge across equivalent models, and experimental validation across different architectures

### Open Question 2
- Question: How do log-softmax scores perform compared to pre-softmax and post-softmax scores across different gradient-based attribution methods and network architectures?
- Basis in paper: [explicit] The authors suggest log-softmax scores as a third alternative but note their preliminary tests didn't show noticeable differences from post-softmax scores
- Why unresolved: The authors only performed limited testing with Grad-CAM and RSI Grad-CAM methods
- What evidence would resolve it: Systematic comparison of log-softmax gradients across multiple attribution methods, network architectures, and datasets

### Open Question 3
- Question: Are there specific network architectures or training scenarios where pre-softmax gradients consistently outperform post-softmax gradients?
- Basis in paper: [inferred] The authors note that methods vulnerable to vanishing gradients may benefit from pre-softmax scores
- Why unresolved: The paper identifies Grad-CAM as potentially benefiting from pre-softmax scores but doesn't explore other scenarios
- What evidence would resolve it: Controlled experiments varying network depth, activation functions, and training procedures to identify conditions favoring pre-softmax gradients

### Open Question 4
- Question: How does the choice between pre-softmax and post-softmax gradients affect attribution method performance on multi-label classification tasks?
- Basis in paper: [inferred] The paper mentions that cross-entropy loss plays a role in cases where ground truth cannot be expressed as a 1-hot vector
- Why unresolved: The paper focuses on single-label classification scenarios
- What evidence would resolve it: Comparative studies of attribution methods using both gradient types on multi-label datasets with varying degrees of label overlap

## Limitations
- The practical impact of gradient vanishing in post-softmax approaches for saturated regions remains unclear
- The log-softmax alternative proposed needs more empirical validation across diverse architectures
- The analysis assumes standard cross-entropy loss and one-hot encoding, which may not generalize to all classification scenarios

## Confidence
- High confidence: The mathematical derivation showing ∂L/∂x = -∂log yc/∂x for post-softmax gradients (Mechanism 1)
- Medium confidence: The invariance property of post-softmax gradients under constant shifts (Mechanism 2) - supported by mathematical proof but limited empirical validation
- Medium confidence: The isolation property for individual class contributions (Mechanism 3) - mathematically sound but not extensively tested across different loss functions

## Next Checks
1. Empirical validation of information gain: Compare attribution map quality using information-theoretic metrics (mutual information between input features and predictions) across pre-softmax, post-softmax, and log-softmax approaches on multiple datasets.
2. Cross-architecture consistency test: Evaluate the invariance properties across different network architectures (CNNs, Transformers) and training conditions to verify that post-softmax gradients maintain consistency as claimed.
3. Saturation region analysis: Systematically test attribution performance in gradient-saturated regions for both pre- and post-softmax approaches using synthetic inputs with controlled saturation levels.