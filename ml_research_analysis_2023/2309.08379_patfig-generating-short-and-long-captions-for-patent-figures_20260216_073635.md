---
ver: rpa2
title: 'PatFig: Generating Short and Long Captions for Patent Figures'
arxiv_id: '2309.08379'
source_url: https://arxiv.org/abs/2309.08379
tags:
- patent
- figure
- image
- captions
- short
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Qatent PatFig, a novel dataset for patent
  figure captioning, comprising 30,000+ patent figures from over 11,000 European patent
  applications. The dataset includes short and long captions, reference numerals,
  their corresponding terms, and the minimal claim set for each figure.
---

# PatFig: Generating Short and Long Captions for Patent Figures

## Quick Facts
- arXiv ID: 2309.08379
- Source URL: https://arxiv.org/abs/2309.08379
- Reference count: 40
- Primary result: Long caption generation benefits from adding patent-specific terms, while short caption generation is disturbed by them.

## Executive Summary
This paper introduces Qatent PatFig, a novel dataset for patent figure captioning containing over 30,000 patent figures from 11,000+ European patent applications. The dataset uniquely includes short and long captions, reference numerals with their corresponding terms, and minimal claim sets for each figure. The authors demonstrate that finetuning an LVLM model on this dataset can generate patent-relevant captions, with the key finding that incorporating patent-specific terminology improves long caption generation but may degrade short caption performance. The work addresses a gap in patent-specific vision-language modeling and provides insights into the role of domain knowledge in figure captioning tasks.

## Method Summary
The method involves creating the Qatent PatFig dataset by scraping patent figures and associated textual data from Espacenet, extracting reference numerals using OCR (docTR), and matching them to patent description terms. The LVLM model MiniGPT-4 is then fine-tuned on this dataset with separate models for short and long captions. The training uses a projection layer fine-tuning approach with different input configurations: Vision-only, Vision+Title, and Vision+Title+Terms. Evaluation is performed using automated metrics including BLEU, ROUGE, METEOR, and CIDEr scores.

## Key Results
- Long caption generation benefits from adding patent-specific terms, particularly improving CIDEr scores
- Short caption generation is disturbed by the inclusion of term lists in the input
- Model performance improves when reference numerals are excluded from evaluation due to LVLM limitations with OCR
- The dataset enables effective fine-tuning of LVLM models for patent-specific figure captioning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Long caption generation benefits from adding terms to the input because the terms provide explicit semantic grounding that bridges visual content with technical domain language.
- Mechanism: LVLM receives both visual features and textual cues (patent title + reference numeral terms). The additional text acts as a semantic scaffold, allowing the model to align image regions with precise technical vocabulary rather than relying on visual-only context.
- Core assumption: The model's text-encoder can effectively integrate and contextualize patent-specific terminology within the image captioning process.
- Evidence anchors:
  - [abstract] "long caption generation benefits from adding the terms to the input"
  - [section] "long caption generation generally benefits from adding the terms to the input, in particular for the CIDEr score"
  - [corpus] Weak corpus evidence; related work focuses on scientific figure captioning, not patent-specific term integration.
- Break condition: If the terms are noisy, ambiguous, or poorly matched to the image, the model may misinterpret them, leading to hallucinated or incorrect captions.

### Mechanism 2
- Claim: Short caption generation is disturbed by the term list in the input, likely because short captions are formulaic and the model overfits to the terms rather than the image content.
- Mechanism: Short captions follow a template-like structure (e.g., "FIGX is a schematic diagram..."). When additional terms are provided, the model may prioritize them over visual grounding, producing captions that are less coherent or off-topic.
- Core assumption: The model treats short captions as a distinct generation task requiring different conditioning strategies than long captions.
- Evidence anchors:
  - [abstract] "short caption generation seems to be disturbed by the term list in the input"
  - [section] "Interestingly, the short caption generation seems to be disturbed by the term list in the input."
  - [corpus] Weak corpus evidence; similar observations in scientific image captioning but not specifically for patent short captions.
- Break condition: If the model is fine-tuned with explicit short-caption templates and visual-only conditioning, it may regain performance without term interference.

### Mechanism 3
- Claim: Removing reference numerals from evaluation improves model scores because LVLM models are not inherently trained for OCR, and numerals are better handled as post-processing additions.
- Mechanism: The model generates captions based on visual and textual features, but numerals in images are not reliably extracted by the visual encoder. By excluding them from evaluation, the focus shifts to semantic accuracy rather than exact numeral matching.
- Core assumption: The visual encoder (ViT) lacks robust OCR capability, and the projection layer is insufficient to learn numeral extraction during fine-tuning.
- Evidence anchors:
  - [section] "It is well-known that neural vision models do not implicitly learn OCR... the model's performance is generally better when evaluating without taking the numerals into account."
  - [section] "Training only a single projection layer may limit the model's ability to learn comprehensive visual-text alignment effectively."
  - [corpus] Weak corpus evidence; no direct mention of OCR limitations in related datasets.
- Break condition: If the OCR pipeline is improved or the model is augmented with explicit numeral detection, the need to exclude numerals from evaluation would diminish.

## Foundational Learning

- Concept: Vision-Language Model Fine-tuning
  - Why needed here: PatFig uses an LVLM (MiniGPT-4) that requires fine-tuning on domain-specific data to generate patent-relevant captions.
  - Quick check question: What is the difference between pre-training and fine-tuning in the context of LVLM models?

- Concept: OCR (Optical Character Recognition) for Patent Figures
  - Why needed here: Patent figures contain reference numerals that must be extracted and matched to terms for captioning.
  - Quick check question: Why might OCR performance vary across different patent figure types?

- Concept: Patent Figure Structure and Captioning Norms
  - Why needed here: Patent figures follow specific formatting rules (short/long captions, reference numerals, claims) that differ from general image captioning.
  - Quick check question: How do short captions in patents differ structurally from long captions?

## Architecture Onboarding

- Component map:
  - Patent Figure Dataset (PatFig) -> OCR Pipeline -> Reference Numeral Extraction -> Term Matching -> LVLM Model (MiniGPT-4) -> Caption Generation -> Evaluation Metrics

- Critical path:
  1. Extract reference numerals from patent images using OCR
  2. Match numerals to terms from patent descriptions
  3. Fine-tune MiniGPT-4 on image-caption pairs with optional text cues (title, terms)
  4. Generate captions using prompts with/without additional text context
  5. Evaluate using automated metrics

- Design tradeoffs:
  - Fine-tuning only the projection layer reduces computational cost but may limit alignment learning
  - Using text cues (title, terms) improves long captions but can degrade short captions
  - Excluding numerals from evaluation improves scores but hides numeral extraction limitations

- Failure signatures:
  - Low BLEU/CIDEr scores may indicate poor visual-text alignment or noisy OCR terms
  - Mismatched terms and image content suggest term extraction errors
  - Short captions disturbed by terms indicate overfitting to text cues

- First 3 experiments:
  1. Compare caption quality with and without reference numerals in the input to test term impact
  2. Evaluate model performance on patent figure subsets grouped by type (e.g., schematic vs. plot)
  3. Test alternative OCR models (e.g., EasyOCR vs. docTR) to measure impact on caption accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal threshold at which textual information becomes sufficient for figure captioning, making the visual input redundant?
- Basis in paper: [inferred] from the conclusion section discussing future research directions on identifying when a text-only large language model can accurately predict figure content without analyzing the image.
- Why unresolved: The paper does not provide any experiments or quantitative analysis on this threshold, only mentions it as a potential area for future research.
- What evidence would resolve it: Empirical studies comparing caption quality when using text-only vs. text+image inputs across different levels of image-text information density, identifying the point where additional visual information no longer improves caption quality.

### Open Question 2
- Question: How does the performance of MiniGPT-4 compare to traditional CNN+RNN approaches for patent figure captioning?
- Basis in paper: [explicit] mentioned in the experimental setup section stating that while traditional approaches have limitations, the paper chose MiniGPT-4 following recent trends without providing direct comparisons.
- Why unresolved: The paper does not include any experiments or results comparing MiniGPT-4 to CNN+RNN models on the patent figure captioning task.
- What evidence would resolve it: Head-to-head comparison experiments evaluating both MiniGPT-4 and CNN+RNN models on the same patent figure captioning task using the PatFig dataset, measuring performance across different metrics.

### Open Question 3
- Question: What is the impact of including or excluding reference numerals in the training data on caption generation quality?
- Basis in paper: [explicit] discussed in the discussion section noting that models perform better when evaluated without reference numerals, but also mentioning the possibility of removing numbers from images.
- Why unresolved: While the paper observes performance differences, it doesn't systematically study the impact of reference numeral inclusion/exclusion in training data on final caption quality.
- What evidence would resolve it: Controlled experiments training models with and without reference numerals in the training data, measuring the effect on caption quality and analyzing whether the model learns to generate or ignore reference numerals.

## Limitations
- The dataset's representativeness is unclear with uneven coverage across 11,000+ patent applications
- Evaluation relies solely on automated metrics without human assessment of factual accuracy
- The model's inability to handle reference numerals effectively limits its practical utility for patent applications

## Confidence
- Mechanism linking term inclusion to long caption improvement: Medium
- Claim that short captions are disturbed by terms: Medium
- Assertion about LVLM limitations with OCR: Medium

## Next Checks
1. Conduct human evaluation of generated captions focusing on factual accuracy and technical coherence, particularly for captions generated with and without term inclusion.

2. Test the model on patent figures from different jurisdictions (US, Chinese, Japanese) to assess cross-corpus generalization and identify potential domain-specific limitations.

3. Compare the docTR OCR pipeline with alternative approaches (EasyOCR, Tesseract) on a subset of patent figures to quantify the impact of numeral extraction quality on caption generation performance.