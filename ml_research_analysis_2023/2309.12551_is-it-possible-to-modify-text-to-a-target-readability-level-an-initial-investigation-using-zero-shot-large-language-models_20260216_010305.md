---
ver: rpa2
title: Is it Possible to Modify Text to a Target Readability Level? An Initial Investigation
  Using Zero-Shot Large Language Models
arxiv_id: '2309.12551'
source_url: https://arxiv.org/abs/2309.12551
tags:
- text
- readability
- target
- source
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new task of readability-controlled text
  modification, which aims to generate multiple versions of a text at different target
  readability levels. The proposed zero-shot approaches using ChatGPT and Llama-2
  can push the readability of the paraphrases in the desired direction, but the final
  readability remains correlated with the original text's readability.
---

# Is it Possible to Modify Text to a Target Readability Level? An Initial Investigation Using Zero-Shot Large Language Models

## Quick Facts
- arXiv ID: 2309.12551
- Source URL: https://arxiv.org/abs/2309.12551
- Reference count: 16
- Key outcome: Zero-shot large language models can push readability in the desired direction but struggle to completely decouple from source text readability.

## Executive Summary
This paper introduces readability-controlled text modification as a new task where the goal is to generate multiple versions of a text at different target readability levels while maintaining semantic content. The authors investigate zero-shot approaches using ChatGPT and Llama-2 with explicit prompts specifying desired Flesch Reading Ease Scores. While the models can push readability in the desired direction, the generated text readability remains significantly correlated with the source text readability, indicating fundamental limitations in zero-shot readability control.

## Method Summary
The authors use the CLEAR dataset (4,724 passages) and generate 8 paraphrases for each passage at target readability levels (5, 20, 40, 55, 65, 75, 85, 95) using zero-shot prompting with ChatGPT (gpt-3.5-turbo) and Llama-2 (Llama-2-7b-chat-hf). They evaluate performance using individual-scale metrics (Spearman correlation, RMSE, classification accuracy) and population-scale metrics (Pearson correlation, linear regression) to assess readability control, plus paraphrasing metrics (self-WER, BERTScore F1) to evaluate content preservation. A two-step sequential process is also explored where generated paraphrases are passed through the model again.

## Key Results
- Zero-shot approaches using ChatGPT and Llama-2 can push readability in the desired direction
- Generated text readability remains highly correlated with source text readability
- Individual-scale metrics show effectiveness in readability control while population-scale metrics reveal persistent source-text influence
- ChatGPT outperforms Llama-2 on most metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models can generate text at different readability levels through zero-shot prompting.
- Mechanism: By providing explicit prompts that describe the desired readability level, the model adjusts its language complexity accordingly.
- Core assumption: The model has been exposed to enough varied text during training to understand and reproduce different readability levels.
- Evidence anchors:
  - [abstract] "The zero-shot approaches using ChatGPT and Llama-2 can push the readability of the paraphrases in the desired direction"
  - [section] "Large-scale generative foundation models... have demonstrated state-of-the-art performance across a large range of natural language tasks in zero-shot and few-shot settings"

### Mechanism 2
- Claim: Sequential prompting (two-step process) can incrementally improve readability control by allowing the model to refine its output.
- Mechanism: First pass generates a paraphrase at the target readability; second pass uses this output as input, allowing further adjustment toward the target readability.
- Core assumption: The readability adjustment is not a one-time transformation but can be improved through iterative refinement.
- Evidence anchors:
  - [abstract] "with an extension approach introducing a two-step process (generating paraphrases by passing through the language model twice)"
  - [section] "paraphrasing based zero-shot approaches to control readability using large language models can sequentially be applied multiple times on a source text"

### Mechanism 3
- Claim: Readability-controlled outputs remain correlated with source text readability due to inherent model biases.
- Mechanism: Even with explicit prompts, the model's generation process is influenced by the input text's complexity, causing the output to partially inherit the source's readability characteristics.
- Core assumption: The model's generation process cannot completely decouple from the input text's features, even with targeted prompts.
- Evidence anchors:
  - [abstract] "the final readability remains correlated with the original text's readability"
  - [section] "the measured readability of the generated texts, albeit correctly ordered, is highly correlated with the source text readability"

## Foundational Learning

- Concept: Readability metrics (specifically Flesch Reading Ease Score)
  - Why needed here: The task requires precise control over text readability, which must be measured and evaluated.
  - Quick check question: What are the two main components of the Flesch Reading Ease formula, and how do they affect the final score?

- Concept: Zero-shot learning with large language models
  - Why needed here: The baseline approaches rely on prompting models without task-specific fine-tuning.
  - Quick check question: How does zero-shot prompting differ from few-shot or fine-tuned approaches in terms of task performance expectations?

- Concept: Paraphrasing metrics (semantic similarity and lexical divergence)
  - Why needed here: Evaluating whether generated text maintains meaning while changing readability requires understanding these metrics.
  - Quick check question: Why would a high-quality paraphrase typically show high lexical divergence but high semantic similarity with the source text?

## Architecture Onboarding

- Component map:
  Input preprocessing -> FRES calculation -> Prompt generation -> LLM interface -> Output validation -> FRES calculation -> Evaluation metrics

- Critical path:
  Generate prompts → Call LLM → Validate output → Calculate FRES → Compare to target → Record metrics
  The most time-sensitive component is LLM inference (45 seconds per example for Llama-2)

- Design tradeoffs:
  Zero-shot vs fine-tuned: Zero-shot is faster to implement but less precise; fine-tuning would require parallel corpora
  Single vs two-step generation: Two-step offers marginal improvement but doubles inference time
  Dataset choice: CLEAR provides passage-level data but may not generalize to all domains

- Failure signatures:
  Outputs are incoherent strings (Llama-2 specific issue)
  Generated FRES remains tightly correlated with source FRES despite target specification
  Significant drops in semantic similarity when readability shifts are large

- First 3 experiments:
  1. Run zero-shot generation with ChatGPT for a small subset of CLEAR data and verify FRES calculation works correctly
  2. Implement the two-step process for ChatGPT and compare individual-scale metrics (Spearman, RMSE, accuracy)
  3. Generate heatmaps for BERTScore and WER to visualize the relationship between source/target readability classes and paraphrasing quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the readability-controlled text modification metrics proposed in this paper compare to existing readability assessment tools in terms of accuracy and reliability?
- Basis in paper: [explicit] The paper introduces novel readability-controlled text modification metrics to assess the quality of readability-controlled text modification.
- Why unresolved: The paper does not provide a direct comparison between the proposed metrics and existing readability assessment tools.
- What evidence would resolve it: A comparative study between the proposed metrics and existing readability assessment tools, evaluating their accuracy and reliability.

### Open Question 2
- Question: Can the two-step process for readability-controlled text modification using large language models be further optimized to achieve better results?
- Basis in paper: [explicit] The paper explores a two-step process for readability-controlled text modification using ChatGPT and Llama-2, observing incremental improvements compared to the one-step process.
- Why unresolved: The paper does not investigate further optimization techniques for the two-step process.
- What evidence would resolve it: An investigation into additional optimization techniques for the two-step process, such as fine-tuning the language models or exploring different prompt strategies.

### Open Question 3
- Question: How do the readability-controlled text modification results generalize to other datasets and domains beyond the CLEAR dataset used in this study?
- Basis in paper: [explicit] The paper acknowledges that the insights drawn from the study are based on a single dataset, CLEAR, and some observations may not generalize to datasets in other domains.
- Why unresolved: The paper does not provide evidence of the generalizability of the results to other datasets and domains.
- What evidence would resolve it: A study that applies the readability-controlled text modification approach to other datasets and domains, evaluating the consistency of the results.

## Limitations
- Task definition ambiguity between readability control and general paraphrasing
- Persistent correlation between source and generated readability despite explicit target specifications
- Limited to passage-level data from a single dataset (CLEAR)

## Confidence
**High Confidence**:
- Large language models can push readability in the desired direction using zero-shot prompting
- Sequential prompting provides marginal improvements over single-step approaches
- Individual-scale metrics (Spearman correlation, RMSE, classification accuracy) effectively measure readability control ability

**Medium Confidence**:
- Population-scale metrics indicate persistent correlation between source and generated readability
- ChatGPT outperforms Llama-2 on most metrics
- Two-step process provides consistent but small improvements across metrics

**Low Confidence**:
- The task definition as distinct from readability simplification or paraphrasing
- The claim that "readability-controlled text modification is a different task from readability simplification" due to the bidirectional nature of the task

## Next Checks
**Validation Check 1**: Test the robustness of readability control across diverse text domains by applying the zero-shot approaches to multiple datasets beyond CLEAR (e.g., news articles, academic texts, social media content) and compare performance consistency.

**Validation Check 2**: Implement a controlled experiment where source text readability is artificially manipulated (e.g., through synonym replacement or sentence splitting) to determine whether the observed correlation between source and generated readability is due to inherent model limitations or dataset characteristics.

**Validation Check 3**: Compare zero-shot approaches against a simple baseline where readability is controlled through explicit vocabulary restriction (e.g., using word frequency lists) to determine whether large language models provide meaningful advantages for readability control versus rule-based methods.