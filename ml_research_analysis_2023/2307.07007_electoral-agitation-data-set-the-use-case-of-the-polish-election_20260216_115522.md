---
ver: rpa2
title: 'Electoral Agitation Data Set: The Use Case of the Polish Election'
arxiv_id: '2307.07007'
source_url: https://arxiv.org/abs/2307.07007
tags:
- election
- data
- agitation
- polish
- media
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first publicly available dataset for
  detecting electoral agitation in Polish social media, containing 6,112 human-annotated
  tweets across four legally-defined categories (Inducement, Encouragement, Voting
  turnout, Normal). The dataset was created by collecting tweets from the 2020 Polish
  presidential election campaign and annotating them through a multi-pass process
  by native speakers, achieving a moderate inter-annotator agreement of 0.66 Cohen's
  kappa.
---

# Electoral Agitation Data Set: The Use Case of the Polish Election

## Quick Facts
- arXiv ID: 2307.07007
- Source URL: https://arxiv.org/abs/2307.07007
- Reference count: 0
- Primary result: 68% F1 score for electoral agitation classification

## Executive Summary
This paper introduces the first publicly available dataset for detecting electoral agitation in Polish social media, containing 6,112 human-annotated tweets across four legally-defined categories (Inducement, Encouragement, Voting turnout, Normal). The dataset was created through a multi-pass annotation process by native speakers during the 2020 Polish presidential election campaign. The authors fine-tuned a Polish language model (HerBERT) on this dataset, achieving 68% F1 score for agitation classification. They demonstrate practical applications through analysis of the 2020 election campaign, finding that over one-third of tweets contained electoral agitation with significant increases during key campaign periods. The dataset and model are made publicly available to support research on election integrity and social media manipulation.

## Method Summary
The authors collected 6,112 tweets from the 2020 Polish presidential election campaign and annotated them through a multi-pass process by native Polish speakers using four mutually exclusive categories: Inducement, Encouragement, Voting turnout, and Normal. They achieved moderate inter-annotator agreement (0.66 Cohen's kappa) and fine-tuned the HerBERT Polish language model using a stratified 80/20 train-test split. The model was evaluated using F1 score (68% macro-average) and applied to analyze temporal patterns in electoral agitation behavior during the election campaign.

## Key Results
- Achieved 68% F1 score for electoral agitation classification on Polish tweets
- Identified that over one-third of tweets contained electoral agitation during the 2020 election campaign
- Found significant increases in agitation during key campaign periods, particularly around the pre-election silence period

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset enables quantitative analysis of electoral agitation compliance by mapping legal categories to machine-readable labels
- Mechanism: Legal definitions (e.g., Article 105 EC's "public encouraging") are operationalized into four mutually exclusive annotation categories (Inducement, Encouragement, Voting turnout, Normal), creating a bridge between legal compliance requirements and NLP classification
- Core assumption: The four-category framework adequately captures the complexity of electoral agitation as defined by Polish election law
- Evidence anchors:
  - [abstract] "contains 6,112 human-annotated tweets tagged with four legally conditioned categories"
  - [section] "adopting four mutually exclusive categories for the annotation: Inducement... Encouragement... Voting turnout... Normal"
  - [corpus] Weak - corpus shows related work on election claim classification but no direct legal-operationalization evidence
- Break condition: If legal definitions evolve or new forms of digital electioneering emerge that don't fit these categories

### Mechanism 2
- Claim: The HerBERT model achieves reasonable classification performance for practical monitoring applications
- Mechanism: Fine-tuning a Polish-specific language model (HerBERT) on the annotated dataset creates a classifier that can process large volumes of tweets and identify agitation patterns beyond manual review capacity
- Core assumption: 68% F1 score provides sufficient accuracy for trend analysis and identifying compliance violations at scale
- Evidence anchors:
  - [abstract] "achieved 68% F1 score for agitation classification"
  - [section] "achieved a 68% F1 score for the stratified train-test split"
  - [corpus] Weak - corpus shows election content analysis work but no specific evidence about F1 threshold sufficiency for legal monitoring
- Break condition: If precision drops below threshold needed for reliable enforcement decisions or recall becomes insufficient for comprehensive monitoring

### Mechanism 3
- Claim: The dataset reveals temporal patterns in electoral agitation behavior, particularly pre-election silence violations
- Mechanism: Time-stamped classification of historical tweet data allows detection of increased agitation during campaign peaks and identification of illegal pre-election silence violations through comparative analysis
- Core assumption: Temporal patterns in classified data accurately reflect real-world campaign behavior and legal violations
- Evidence anchors:
  - [abstract] "significant increases during key campaign periods" and "analysis of the 2020 election campaign, finding that over one-third of tweets contained electoral agitation"
  - [section] "we performed a study over agitation during the 2020 Presidential Election... The pre-election silence ended at 9 p.m (21:00)"
  - [corpus] Weak - corpus shows temporal analysis work but no specific evidence about pre-election silence violation detection
- Break condition: If temporal patterns don't correlate with actual campaign events or if classification errors create false violation signals

## Foundational Learning

- Concept: Cohen's kappa inter-annotator agreement
  - Why needed here: To validate annotation quality and reliability for legal compliance monitoring
  - Quick check question: What kappa score would indicate "substantial" agreement rather than "moderate"?

- Concept: Stratified train-test split
  - Why needed here: To ensure balanced representation of all agitation categories in both training and evaluation, critical for reliable performance measurement
  - Quick check question: Why is stratified splitting particularly important when class distribution is imbalanced?

- Concept: Legal-operationalization framework
  - Why needed here: To translate abstract legal concepts into concrete annotation guidelines that annotators and models can consistently apply
  - Quick check question: What risks arise if legal definitions and operational categories diverge?

## Architecture Onboarding

- Component map: Data collection → Annotation pipeline (2+1 annotator process) → HerBERT fine-tuning → Classification deployment → Analysis dashboard
- Critical path: Annotation quality → Model training → Deployment performance → Analytical insights
- Design tradeoffs: Four-category granularity vs. simplicity; high kappa vs. annotation efficiency; model complexity vs. interpretability
- Failure signatures: Low kappa scores (annotation inconsistency); poor F1 scores (model inadequacy); time-series anomalies (data quality issues)
- First 3 experiments:
  1. Re-run annotation on a subset to verify kappa stability across different annotator teams
  2. Test model performance on temporal slices to identify when classification degrades
  3. Compare legal expert review of classified samples against model outputs to validate operationalization accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does electoral agitation content differ across social media platforms in Poland beyond Twitter?
- Basis in paper: [inferred] The paper focuses exclusively on Twitter data collection and analysis, with no comparison to other platforms like Facebook, Instagram, or TikTok where electoral agitation may also occur.
- Why unresolved: The dataset and analysis are limited to Twitter data, preventing understanding of platform-specific patterns or cross-platform differences in electoral agitation.
- What evidence would resolve it: Comparative datasets from multiple social media platforms annotated with the same legal categories, enabling analysis of platform-specific agitation patterns.

### Open Question 2
- Question: What are the long-term effects of electoral agitation on voter turnout and election outcomes in Polish elections?
- Basis in paper: [explicit] The authors mention this could help courts and NEC verify election compliance, but the dataset only provides snapshot analysis of one election without longitudinal studies.
- Why unresolved: The dataset captures a single election cycle and focuses on detection rather than impact assessment, lacking data on voter behavior changes over time.
- What evidence would resolve it: Longitudinal studies tracking voter behavior, turnout rates, and election outcomes across multiple election cycles with corresponding agitation data.

### Open Question 3
- Question: How effective are current Polish legal regulations at curbing electoral agitation during pre-election silence periods?
- Basis in paper: [explicit] The authors note that agitation still exists during pre-election silence (though reduced threefold), raising questions about regulatory effectiveness.
- Why unresolved: The paper identifies the problem but doesn't evaluate enforcement mechanisms, penalties, or compliance rates across different elections or regions.
- What evidence would resolve it: Comparative analysis of agitation levels, enforcement actions, and penalties across multiple elections and regions, combined with compliance data from election authorities.

## Limitations
- Moderate inter-annotator agreement (0.66 Cohen's kappa) suggests potential subjectivity in legal-operationalization process
- 68% F1 score may be insufficient for high-stakes legal enforcement decisions where false positives/negatives carry significant consequences
- Dataset limited to single election cycle and Twitter platform, restricting generalizability to other contexts

## Confidence

- **High confidence**: The dataset contains 6,112 human-annotated tweets across four legally-defined categories (confirmed by dataset statistics and annotation methodology)
- **Medium confidence**: The temporal patterns showing increased agitation during campaign periods (supported by time-series analysis but could be influenced by classification errors)
- **Low confidence**: The sufficiency of 68% F1 score for practical legal monitoring applications (no evidence provided about threshold requirements for enforcement decisions)

## Next Checks

1. **Legal expert validation**: Have Polish election law experts review 100 randomly selected model classifications to assess whether the operationalized categories accurately capture legal definitions of electoral agitation

2. **Temporal robustness testing**: Perform cross-validation on temporal slices of the data to determine if model performance degrades during different campaign phases, particularly around the pre-election silence period

3. **Alternative model comparison**: Test whether other Polish language models (e.g., PolBERT, KB-BERT) achieve comparable or better performance on this task to establish whether HerBERT's architecture is optimal for electoral agitation detection