---
ver: rpa2
title: The inverse problem for neural networks
arxiv_id: '2308.14093'
source_url: https://arxiv.org/abs/2308.14093
tags:
- preimage
- neural
- networks
- affine
- inverse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses computing the preimage of a set under piecewise-affine
  neural networks. It recalls that the preimage of a polyhedral set under such networks
  is a union of polyhedral sets and can be computed exactly using linear programming.
---

# The inverse problem for neural networks

## Quick Facts
- arXiv ID: 2308.14093
- Source URL: https://arxiv.org/abs/2308.14093
- Reference count: 38
- Primary result: Presents exact algorithm for computing preimages of polyhedral sets under piecewise-affine neural networks using linear programming

## Executive Summary
This paper addresses the inverse problem of computing the preimage of a set under neural networks with piecewise-affine activation functions. The authors demonstrate that the preimage of a polyhedral set under such networks can be computed exactly as a union of polyhedral sets using an alternating algorithm that computes preimages of activation functions and affine maps. The work brings these theoretical results to the formal methods community's attention, showing applications in interpretability, robustness analysis, and adversarial attacks.

## Method Summary
The paper presents an algorithm that computes the preimage of a polyhedral set under a piecewise-affine neural network by alternating between computing preimages of activation functions and affine maps. For affine maps, the preimage is obtained by transforming the polyhedron's constraints. For piecewise-affine activations, the preimage is computed by partitioning the space into regions where the activation is affine and taking the union of preimages for each region. The authors also discuss approximation schemes for scalability and extensions to non-piecewise-affine activations using forward-backward computation.

## Key Results
- Exact preimage computation is possible for piecewise-affine neural networks using linear programming
- The preimage of a polyhedral set is a union of (potentially exponentially many) polyhedral sets
- Applications demonstrated in interpretability and function approximation analysis
- Approximation schemes can trade exactness for computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The preimage of a polyhedral set under a piecewise-affine neural network can be computed exactly as a union of polyhedral sets.
- Mechanism: The algorithm alternates between computing preimages of activation functions and affine maps. For affine maps, the preimage is obtained by transforming the polyhedron's constraints (Equation 6). For piecewise-affine activations, the preimage is computed by partitioning the space into regions where the activation is affine and then taking the union of preimages for each region (Equations 7-10).
- Core assumption: The activation functions are piecewise-affine and the network architecture is known.
- Evidence anchors:
  - [abstract] "the preimage of a polyhedral set is again a union of polyhedral sets and can be effectively computed using linear programming"
  - [section] "For this class, the preimage of a polyhedral set is again a union of (potentially exponentially many) polyhedral sets and can be effectively computed using linear programming."
- Break condition: If the activation functions are not piecewise-affine or the network has loops/recurrence, the exact algorithm no longer applies.

### Mechanism 2
- Claim: The preimage computation can be used for interpretability by revealing what inputs lead to specific outputs.
- Mechanism: By computing the preimage of a set of outputs (e.g., all inputs that classify as a certain class), we obtain a geometric description of the input space that maps to those outputs. This reveals the network's decision boundaries and function approximation capabilities.
- Core assumption: The preimage can be represented and visualized in the input space.
- Evidence anchors:
  - [abstract] "We show several applications of computing the preimage for analysis and interpretability of neural networks."
  - [section] "Wecanuse thepreimage computationtolearnabout thefunction-approximation capabilities of a DNN."
- Break condition: If the preimage is too complex to visualize or compute exactly, interpretability benefits may be lost.

### Mechanism 3
- Claim: Approximation schemes can trade exactness for scalability in preimage computation.
- Mechanism: Underapproximation selects one partition at each activation, potentially with backtracking. Overapproximation uses interval domains to compute bounds on the preimage. These methods are faster but coarser than the exact algorithm.
- Core assumption: Approximate solutions are acceptable for the application at hand.
- Evidence anchors:
  - [section] "A simple approach to compute an underapproximation of the preimage considers the partitioning of the sets (as, e.g., in Fig. 3(b)) as a search space and selects only one of the sets to continue with."
  - [section] "Instead of underapproximations, we can also consider overapproximations. Using abstract interpretation, we can choose an abstract domain to simplify the calculations."
- Break condition: If the approximation is too coarse to be useful for the intended analysis.

## Foundational Learning

- Concept: Piecewise-affine functions and their properties
  - Why needed here: The algorithm relies on the piecewise-affine nature of the activation functions to decompose the preimage computation into manageable pieces.
  - Quick check question: What is the defining characteristic of a piecewise-affine function, and why is it important for this algorithm?

- Concept: Polyhedral sets and linear programming
  - Why needed here: The preimage is computed as a union of polyhedral sets, and linear programming is used to find these polyhedra.
  - Quick check question: How is a polyhedron represented in matrix-vector form, and what is the role of linear programming in finding the preimage?

- Concept: Forward and backward image computation
  - Why needed here: The algorithm alternates between computing the preimage of activation functions (backward) and affine maps (backward), building up the full preimage from the output set.
  - Quick check question: What is the difference between forward and backward image computation in the context of neural networks?

## Architecture Onboarding

- Component map:
  - Preimage computation module -> Affine map inverter -> Piecewise-affine activation inverter -> Polyhedral set representation -> Linear programming solver

- Critical path:
  1. Input: A neural network and a set of outputs
  2. Initialize the output set
  3. For each layer (from output to input):
     a. Compute the preimage under the activation function
     b. Compute the preimage under the affine map
  4. Output: The preimage as a union of polyhedra

- Design tradeoffs:
  - Exact vs. approximate computation: Exact computation is more precise but can be slower and produce exponentially many polyhedra. Approximate computation is faster but less precise.
  - Representation of polyhedra: Different representations (e.g., H-representation, V-representation) have different tradeoffs in terms of computational efficiency and expressiveness.

- Failure signatures:
  - Empty preimage: The output set is not in the range of the network.
  - Exponentially many polyhedra: The preimage is very complex, which may indicate a need for approximation.
  - Inconsistent constraints: The linear programming solver fails to find a feasible solution, indicating an error in the computation.

- First 3 experiments:
  1. Compute the preimage of a single output point under a simple network with one ReLU layer.
  2. Compute the preimage of a small interval of outputs under a network with two ReLU layers.
  3. Compute the preimage of a half-space of outputs under the example network from the paper, and visualize the result.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact computational complexity of computing the preimage of a piecewise-affine neural network in terms of the number of layers and neurons?
- Basis in paper: [explicit] The paper mentions that a DNN with k layers each of dimension n can map a polyhedron to O(bkn) polyhedra, where b is the number of affine pieces in the activation function.
- Why unresolved: While the paper provides an upper bound on the number of polyhedra in the preimage, it doesn't explicitly analyze the computational complexity of the algorithm itself.
- What evidence would resolve it: A detailed analysis of the algorithm's runtime, including the number of linear programming problems solved and the complexity of each step.

### Open Question 2
- Question: How does the approximation scheme using interval arithmetic compare to other abstraction methods in terms of accuracy and computational efficiency?
- Basis in paper: [explicit] The paper discusses using interval approximation as an overapproximation scheme and mentions that it is â‰ˆ100x faster than the exact algorithm.
- Why unresolved: The paper only provides a comparison between the exact algorithm and the interval approximation scheme. It doesn't compare the interval approximation to other potential abstraction methods.
- What evidence would resolve it: Experiments comparing the interval approximation scheme to other abstraction methods, such as zonotopes or polytopes, in terms of accuracy and computational efficiency.

### Open Question 3
- Question: Can the preimage computation be extended to neural networks with non-piecewise-affine activation functions, and if so, what are the limitations and trade-offs?
- Basis in paper: [explicit] The paper mentions that the approach in [26] uses forward-backward computation for sigmoid activations, but doesn't provide a general method for non-piecewise-affine activations.
- Why unresolved: The paper doesn't explore methods for handling non-piecewise-affine activation functions beyond the specific case of sigmoid activations.
- What evidence would resolve it: A proposed method for handling non-piecewise-affine activation functions, along with experiments demonstrating its effectiveness and limitations.

## Limitations
- The exact algorithm only works for piecewise-affine activation functions and known feedforward network architectures
- Preimage computation can result in exponentially many polyhedra, limiting scalability
- Lack of extensive experimental validation on real-world networks

## Confidence
- High confidence in the theoretical framework and core algorithm for piecewise-affine networks
- Medium confidence in the practical scalability of the exact algorithm due to potential exponential growth in the number of polyhedra
- Low confidence in the empirical evaluation, as the paper lacks extensive experimental validation on real-world networks

## Next Checks
1. Implement the exact algorithm and benchmark its performance on networks of increasing depth to empirically verify the exponential complexity claim
2. Apply the preimage computation to a simple classification task and verify that the computed preimages correctly correspond to the decision boundaries
3. Extend the framework to handle common non-piecewise-affine activations (e.g., sigmoid) and evaluate the accuracy of the approximation methods on benchmark datasets