---
ver: rpa2
title: Enhancing Instance-Level Image Classification with Set-Level Labels
arxiv_id: '2311.05659'
source_url: https://arxiv.org/abs/2311.05659
tags:
- learning
- labels
- dataset
- latexit
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FACILE, a novel representation learning framework
  that enhances instance-level image classification by leveraging set-level coarse-grained
  labels. The method addresses the challenge of limited fine-grained labels by utilizing
  easily accessible coarse-grained annotations.
---

# Enhancing Instance-Level Image Classification with Set-Level Labels

## Quick Facts
- **arXiv ID**: 2311.05659
- **Source URL**: https://arxiv.org/abs/2311.05659
- **Reference count**: 40
- **Primary result**: 13% improvement in histopathology image classification accuracy using coarse-grained set-level labels

## Executive Summary
This paper introduces FACILE, a representation learning framework that leverages coarse-grained set-level labels to enhance instance-level image classification, particularly for fine-grained tasks with limited labeled data. The method addresses the challenge of scarce fine-grained annotations by utilizing easily accessible coarse-grained labels, employing a two-step approach: pretraining a feature map using coarse-grained labels and then fine-tuning a classifier for downstream tasks. Theoretical analysis demonstrates that the availability of coarse-grained labels can lead to fast excess risk rates for fine-grained label prediction, while empirical results on natural and histopathology image datasets showcase improved classification performance compared to traditional single-instance label-based methods.

## Method Summary
FACILE employs a two-step approach to enhance instance-level image classification using set-level coarse-grained labels. First, it pretrains a feature map using coarse-grained set-level labels (e.g., "beach vacation" as a label for a set of images) through fully supervised pretraining or supervised contrastive learning. The framework uses set-input models like Set Transformer or Deep Set to encode relationships within sets and extract instance-level features. Second, it fine-tunes a classifier on the extracted features using fine-grained labels for downstream tasks. The method is evaluated on natural image datasets (CIFAR-100) and histopathology image datasets (TCGA, GTEx, LC25000, PAIP, NCT, PDAC), demonstrating improved classification performance, particularly a 13% improvement on histopathology image classification benchmarks.

## Key Results
- FACILE achieves 13% improvement in classification accuracy compared to strongest baseline on histopathology image classification benchmarks
- Theoretical analysis demonstrates fast excess risk rates (O(1/n)) for fine-grained label prediction under Lipschitzness conditions
- Set-input models (Set Transformer, Deep Set) are essential for encoding relationships within sets when learning from coarse-grained set-level labels

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Coarse-grained set-level labels accelerate downstream fine-grained classification by providing a fast excess risk rate of O(1/n) under a Lipschitzness condition.
- **Mechanism**: The framework learns a feature map from coarse-grained set-level labels and uses that feature map to extract instance-level features for fine-grained classification. The theoretical analysis shows that when the feature map's loss is Lipschitz relative to the fine-grained loss, the excess risk can be bounded efficiently, enabling rapid learning even with few fine-grained labels.
- **Core assumption**: The Lipschitzness assumption that small changes in the feature map do not harm pretraining performance and do not affect downstream fine-grained loss much either. The availability of coarse-grained labels is tightly coupled with the fine-grained task via this property.
- **Evidence anchors**:
  - [abstract] "Theoretical analysis demonstrates that the availability of coarse-grained labels can lead to fast excess risk rates for fine-grained label prediction."
  - [section 2.2] "We say that f is L-Lipschitz relative to E if for all s ∈ S , x ∈ s, y ∈ Y , and e, e′ ∈ E , |ℓfg(f ◦ e(x), y) − ℓfg(f ◦ e′(x), y)| ≤ Lℓcg(ge ◦ ϕe(s), ge′ ◦ ϕe′ (s))"
  - [corpus] Weakly related: no direct mention of Lipschitzness; only related titles about semi-supervised learning or few-shot learning, no mechanism detail.
- **Break condition**: If the coarse-grained labels are not sufficiently correlated with the fine-grained labels, or if the Lipschitz condition does not hold, the fast rate is lost.

### Mechanism 2
- **Claim**: Pretraining with coarse-grained labels can outperform ImageNet pretraining on histopathology image classification tasks, even with fewer samples per class.
- **Mechanism**: FACILE pretrains a feature map on histopathology patches using coarse-grained labels like tissue or tumor origin, then fine-tunes a classifier on small labeled fine-grained tasks (e.g., tissue subtypes). The framework leverages large histopathology datasets (e.g., TCGA) that are easier to annotate at set level than at instance level.
- **Core assumption**: The coarse-grained labels (e.g., tumor origin) provide sufficient semantic structure for the fine-grained task (e.g., tissue classification), and the set-input model can learn useful representations from set-level supervision.
- **Evidence anchors**:
  - [abstract] "Notably, our algorithm achieves 13% improvement in classification accuracy compared to the strongest baseline on the histopathology image classification benchmarks."
  - [section 3.4] "Our method get roughly 13% improvement compared to Yang et al. (2022) on the LC dataset."
  - [corpus] Weakly related: one neighbor paper "Libra-MIL: Multimodal Prototypes Stereoscopic Infused with Task-specific Language Priors for Few-shot Whole Slide Image Classification" also works on WSI classification but with a different method.
- **Break condition**: If the coarse-grained label space is too coarse or semantically distant from the fine-grained target, pretraining gains will diminish.

### Mechanism 3
- **Claim**: Set-input models (e.g., Set Transformer, Deep Set) are essential for encoding relationships within sets when learning from coarse-grained set-level labels.
- **Mechanism**: The input to FACILE is a set of images, and the model uses set-input functions g to generate set-level features. The set-input model (e.g., Set Transformer with attention) allows permutation-invariant processing and modeling of interactions among instances in a set, enabling the extraction of discriminative features for downstream fine-grained classification.
- **Core assumption**: The set-input architecture (e.g., Set Transformer) can learn to aggregate instance features in a way that preserves semantic structure relevant to both coarse and fine labels.
- **Evidence anchors**:
  - [section 2.1] "We define instance feature maps E = {e : X → Z} , set-input functions G = {g : M → W} where M = {{z1, . . . , za} : zj ∈ Z for j ∈ [a]"
  - [section F.1] "Set Transformer adapted the Transformer model for set data. It leverages the attention mechanism (Vaswani et al., 2017) to capture interactions between instances of the input set."
  - [corpus] No direct evidence; neighbor papers focus on graph or few-shot learning, not set-input models.
- **Break condition**: If the set size is too small or the set-input model is too simple, the aggregation may fail to capture meaningful semantic relationships.

## Foundational Learning

- **Concept**: Few-shot learning (FSL)
  - Why needed here: FACILE is evaluated in an FSL setting where only a few labeled examples are available for the fine-grained downstream task.
  - Quick check question: In FSL, how many examples per class are typically used during meta-testing?

- **Concept**: Weakly supervised learning
  - Why needed here: FACILE uses coarse-grained set-level labels rather than instance-level fine-grained labels, which is a form of weak supervision.
  - Quick check question: What distinguishes coarse-grained set-level supervision from instance-level supervision in terms of label granularity?

- **Concept**: Self-supervised learning
  - Why needed here: FACILE compares against self-supervised baselines (e.g., SimCLR, SimSiam) that ignore labels during pretraining.
  - Quick check question: In contrastive self-supervised learning, what defines a positive pair?

## Architecture Onboarding

- **Component map**: Input images -> Set of images with coarse-grained labels -> Feature extractor (e) -> Set-input model (g) -> Projection head -> Fine-grained classifier (f)

- **Critical path**:
  1. Sample sets of images from pretraining dataset with coarse-grained labels.
  2. Train feature extractor e and set-input model g using coarse-grained supervision (e.g., cross-entropy loss).
  3. Extract features from support set with e.
  4. Train fine-grained classifier f on support set features.
  5. Evaluate f on query set.

- **Design tradeoffs**:
  - **Set size**: Larger sets may capture richer semantic structure but increase computational cost; experiments suggest 5 is near-optimal for histopathology.
  - **Set-input model complexity**: Simple pooling (e.g., mean) is fast but may lose interactions; Set Transformer is expressive but heavier.
  - **Loss function**: SupCon vs. FSP—SupCon uses contrastive learning for better representations, FSP uses fully supervised pretraining with instance-level pseudo-labels.

- **Failure signatures**:
  - **No improvement over ImageNet**: Coarse-grained labels are too semantically distant from fine-grained task.
  - **Performance drops with larger set size**: Set-input model cannot handle interactions in large sets effectively.
  - **Instability in few-shot regime**: Feature extractor overfits to coarse-grained task, hurting generalization.

- **First 3 experiments**:
  1. **Pretrain on CIFAR-100 with unique class number labels**: Verify that FACILE-FSP improves over self-supervised baselines on 5-way 5-shot tasks.
  2. **Ablation on set size (2, 5, 10, 15)**: Measure impact on LC dataset 5-shot accuracy to find optimal set size.
  3. **Comparison on histopathology**: Pretrain on TCGA with coarse-grained tumor labels and test on LC, PAIP, NCT datasets to confirm 13% improvement over baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FACILE vary when using different set-input models (e.g., attention-based MIL pooling, Deep Set, Set Transformer) across diverse histopathology datasets?
- Basis in paper: [explicit] The paper mentions that none of the three set-input models (attention-based MIL pooling, Deep Set, and Set Transformer) consistently outperforms the others across all tasks.
- Why unresolved: The paper provides performance results for each set-input model on specific datasets but does not offer a comprehensive analysis of how these models perform across a wider range of histopathology datasets or tasks.
- What evidence would resolve it: Comparative studies on FACILE using different set-input models across multiple histopathology datasets with varying characteristics (e.g., tissue types, magnification levels) would clarify which models are more effective in different scenarios.

### Open Question 2
- Question: What is the impact of input set size on the performance of FACILE, particularly in balancing computational efficiency and classification accuracy?
- Basis in paper: [explicit] The paper mentions an ablation study on input set size, finding that models with an input set size of 5 perform well for LC and PAIP datasets, while larger sizes do not significantly improve performance.
- Why unresolved: The paper does not explore the trade-off between computational efficiency and classification accuracy for different input set sizes or investigate the optimal set size for other types of datasets.
- What evidence would resolve it: Detailed experiments comparing FACILE's performance and computational requirements across various input set sizes on multiple datasets would help determine the optimal balance between efficiency and accuracy.

### Open Question 3
- Question: How does FACILE's performance compare to state-of-the-art weakly supervised learning methods when applied to histopathology images?
- Basis in paper: [explicit] The paper compares FACILE to self-supervised and weakly supervised models but does not directly compare it to other state-of-the-art weakly supervised learning methods specifically designed for histopathology images.
- Why unresolved: The paper focuses on comparing FACILE to general self-supervised and weakly supervised models without considering specialized methods that might be more effective for histopathology tasks.
- What evidence would resolve it: Comparative studies between FACILE and recent weakly supervised learning methods tailored for histopathology images would clarify FACILE's relative performance and potential areas for improvement.

## Limitations

- The theoretical Lipschitzness condition may not hold for all real-world datasets, limiting the generalizability of the fast excess risk rate analysis.
- The performance improvement on histopathology datasets is based on limited benchmarks and may not generalize to all medical imaging tasks.
- The practical implementation details of set-input models (e.g., Set Transformer) are not fully specified, which could impact reproducibility.

## Confidence

- **High**: The core mechanism of pretraining with coarse-grained set-level labels to improve downstream fine-grained classification is well-supported by empirical results on multiple datasets.
- **Medium**: The theoretical analysis showing fast excess risk rates under Lipschitzness assumptions is promising but relies on idealized conditions that may not hold in practice.
- **Low**: The claim of superior performance over ImageNet pretraining on histopathology tasks requires further validation across more diverse datasets and settings.

## Next Checks

1. **Dataset diversity test**: Evaluate FACILE on additional histopathology datasets (e.g., TCGA-SARC) to confirm robustness beyond the reported benchmarks.
2. **Hyperparameter sensitivity analysis**: Systematically vary set size, batch size, and augmentation strength to identify optimal configurations and failure modes.
3. **Cross-dataset pretraining**: Pretrain on one histopathology dataset (e.g., TCGA) and evaluate on a different one (e.g., PAIP) to test generalization of coarse-grained label utility.