---
ver: rpa2
title: 'A U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning'
arxiv_id: '2310.18988'
source_url: https://arxiv.org/abs/2310.18988
tags:
- error
- descent
- double
- regression
- leaf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the claim that double descent in non-deep
  learning models truly extends beyond traditional U-shaped complexity-generalization
  curves. The authors argue that the second descent in test error as parameters grow
  past sample size arises from implicit changes in the model class when transitioning
  between distinct complexity axes, rather than from the interpolation threshold itself.
---

# A U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning

## Quick Facts
- **arXiv ID:** 2310.18988
- **Source URL:** https://arxiv.org/abs/2310.18988
- **Reference count:** 40
- **Primary result:** Double descent in non-deep learning models arises from transitions between distinct complexity mechanisms, not the interpolation threshold itself

## Executive Summary
This paper challenges the conventional understanding of double descent in non-deep learning models. The authors argue that what appears to be a second descent in test error beyond the interpolation threshold is actually an artifact of plotting performance against a composite axis that combines multiple distinct complexity mechanisms. When these mechanisms are properly separated and measured using effective parameters on test data, the double descent curves fold back into traditional U-shapes. This reframing suggests that the phenomenon is not unique to overparameterization but rather reflects fundamental changes in model class when switching between different ways of increasing parameters.

## Method Summary
The paper analyzes tree-based models, gradient boosting, and linear regression with random Fourier features to demonstrate how double descent emerges from transitions between complexity axes. For trees and boosting, the authors vary both leaf complexity and ensemble size to show that double descent appears only when transitioning between these mechanisms. For linear regression, they use random Fourier features and show that min-norm solutions implicitly perform dimensionality reduction, creating two distinct mechanisms for increasing parameter count. The key innovation is a generalized effective parameter measure that counts smoothing applied to test inputs, which reveals that apparent double descent curves are actually traditional U-shapes when viewed through this lens.

## Key Results
- Individual complexity axes in trees, boosting, and linear regression exhibit traditional convex U-shaped curves, not double descent
- Double descent shapes emerge only when transitioning between distinct complexity mechanisms
- Effective parameter measures computed on test inputs reveal that apparent double descent curves fold back into U-shapes
- Min-norm solutions in linear regression implicitly perform dimensionality reduction, creating two distinct mechanisms for increasing parameters

## Why This Works (Mechanism)

### Mechanism 1
The second descent in test error appears when transitioning between distinct complexity axes (e.g., leaf complexity to ensemble size), not at the interpolation threshold itself.

### Mechanism 2
Min-norm solutions implicitly perform dimensionality reduction, creating two distinct mechanisms for increasing parameter count that are not directly observable.

### Mechanism 3
Effective parameter measures on test inputs reveal that apparent double descent curves fold back into traditional U-shapes because effective parameter counts don't increase past transition thresholds.

## Foundational Learning

- **Singular Value Decomposition (SVD) and PCA**: Understanding how min-norm solutions implicitly perform dimensionality reduction requires knowledge of SVD and its connection to PCA. *Quick check:* How does the SVD factorization X = UΣV^T relate to the eigenvectors of X^TX in PCA?

- **Smoothers and effective parameters**: The paper's resolution relies on interpreting ML methods as smoothers and using effective parameter measures. *Quick check:* For a k-nearest neighbor smoother with k neighbors, what is the effective number of parameters relative to the number of training examples?

- **Bias-variance tradeoff in fixed design**: The impossibility result for double descent in fixed design settings requires understanding how bias and variance behave differently when inputs are deterministic. *Quick check:* In a fixed design setting with interpolating models, what happens to the in-sample prediction error as model complexity increases past the interpolation threshold?

## Architecture Onboarding

- **Component map:** MNIST/SVHN/CIFAR-10 datasets → normalization → train/test split → Trees (CART), Gradient Boosting (AdaBoost-style), Linear Regression (RFF + min-norm) → Complexity control (P_leaf, P_boost, P_P, P_ex) → Squared loss computation, effective parameter calculation, plotting

- **Critical path:** 1) Generate random Fourier features, 2) Train models with varying complexity, 3) Compute test errors and effective parameters, 4) Plot results to verify curves, 5) Analyze transitions

- **Design tradeoffs:** Using squared loss vs. classification loss (squared loss reveals double descent more clearly but is less standard); Fixed vs. random design (fixed design simplifies analysis but may not capture all aspects of generalization); One-vs-rest vs. multiclass (one-vs-rest is simpler but may not be optimal)

- **Failure signatures:** Double descent not appearing (check if correct solution method is being used); U-shaped curves not appearing (verify effective parameters are computed correctly for test inputs); Transitions not occurring at expected points (ensure parameter-increasing mechanisms are properly sequenced)

- **First 3 experiments:** 1) Linear regression with RFF: Vary P_φ from 1 to 50,000, compute test error and ptest, plot both against raw parameters and effective parameters; 2) Tree experiments: Fix P_leaf at 500, vary P_ens from 1 to 100, compute test error and observe L-shaped curve; 3) Peak-moving experiment: For trees, vary when transitioning from P_leaf to P_ens, observe how peak location shifts

## Open Questions the Paper Calls Out

### Open Question 1
Can the insights from this paper regarding multiple complexity axes and effective parameter counting be extended to explain double descent in deep neural networks? The authors note this is a "very natural next question" and suggest combining their insights with known connections between random feature models and two-layer neural networks.

### Open Question 2
How robust are effective parameter measures like ptest_s to different choices of basis expansions or feature representations in linear models? The authors show that in RFF regression, increasing excess features improves basis quality, but only tested random Fourier features.

### Open Question 3
Can effective parameter measures like ptest_s be effectively used for automated model selection in practice? The authors provide anecdotal evidence that ptest_s could help select between interpolating gradient boosting models with different hyperparameters.

## Limitations
- Analysis is limited to specific model classes (trees, boosting, linear regression) and may not generalize to all ML methods exhibiting double descent
- The effective parameter measure p0 requires careful implementation to compute correctly for each model type
- The claim that all non-deep learning double descent can be explained by this mechanism is based on analysis of only three model families

## Confidence
- **High confidence**: Individual complexity axes exhibit convex U-shaped curves rather than double descent
- **Medium confidence**: Min-norm solutions implicitly perform dimensionality reduction, as this relies on specific implementation choices
- **Medium confidence**: All non-deep learning double descent can be explained by this mechanism, as the analysis is limited to three model families

## Next Checks
1. Test the effective parameter framework on additional model classes (e.g., kernel methods, neural networks) to verify generalizability
2. Implement the framework for the min-norm solution computation to confirm the implicit dimensionality reduction claim
3. Conduct ablation studies by varying the transition points between complexity mechanisms to confirm their causal role in producing double descent shapes