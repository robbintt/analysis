---
ver: rpa2
title: Large Language Model as a Policy Teacher for Training Reinforcement Learning
  Agents
arxiv_id: '2311.13373'
source_url: https://arxiv.org/abs/2311.13373
tags:
- agent
- teacher
- student
- llms
- llm-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a framework that uses large language models
  (LLMs) as policy teachers to train reinforcement learning (RL) agents. The LLM provides
  high-level instructions to guide the RL agent, enabling it to learn faster and more
  efficiently.
---

# Large Language Model as a Policy Teacher for Training Reinforcement Learning Agents

## Quick Facts
- arXiv ID: 2311.13373
- Source URL: https://arxiv.org/abs/2311.13373
- Authors: 
- Reference count: 10
- Key outcome: LLM-based policy teachers improve sample efficiency in RL, with students learning to outperform teachers by correcting their mistakes through environmental feedback

## Executive Summary
This paper introduces a framework where pre-trained large language models serve as policy teachers to guide reinforcement learning agents. The LLM provides high-level instructions that help bootstrap the student agent's learning, enabling faster acquisition of effective policies. The key innovation is a two-stage training approach where the student learns from both teacher instructions and environmental feedback, with a decaying temperature parameter that gradually shifts emphasis from imitation to independent learning. Experiments on MiniGrid environments demonstrate that this approach achieves superior sample efficiency and enables students to correct teacher errors and generalize to previously unseen scenarios.

## Method Summary
The method uses a pre-trained LLM (ChatGLM) as a teacher agent that provides soft action distributions based on state observations. A PPO-based student agent learns from both the teacher's guidance and environment rewards through a two-objective loss function. The teacher influence is controlled by a temperature parameter λ that decays from an initial value to zero, allowing the student to gradually transition from imitation to independent learning. The framework incorporates uncertainty estimation from the LLM to weight the teacher's instructions, and uses fixed option policies (explore, go to object, pickup, drop, open) to structure the LLM's action space.

## Key Results
- Student agents achieve superior sample efficiency compared to baseline RL methods
- Students learn to outperform their LLM teachers by leveraging environmental feedback to correct mistakes
- The approach enables generalization to previously unseen scenarios like avoiding hazardous lava grids
- Optimal performance achieved with decay schedule that maintains small teacher influence after initial decay

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based policy distillation improves sample efficiency by bootstrapping the student agent's exploration phase
- Mechanism: The teacher agent (LLM) provides high-level action guidance that steers the student agent toward promising states early in training, reducing the need for extensive random exploration
- Core assumption: The LLM's general world knowledge aligns sufficiently with the specific task domain to provide useful guidance
- Evidence anchors: [abstract] "By incorporating the guidance from the teacher agent, the student agent can distill the prior knowledge of the LLM into its own model"
- Break condition: LLM knowledge is misaligned with task-specific dynamics, leading to consistently misleading guidance

### Mechanism 2
- Claim: Student agent learns to correct teacher errors through environmental feedback, achieving performance beyond the teacher
- Mechanism: During training, the student agent experiences a two-stage loss: minimizing the RL loss (environment feedback) while also aligning with teacher instructions. As training progresses, the weight on teacher instructions decays, allowing the student to override poor teacher decisions
- Core assumption: The environment provides reliable feedback signals that can override incorrect teacher instructions
- Evidence anchors: [abstract] "the student agent learns to outperform the LLM-based teacher by leveraging environmental feedback"
- Break condition: Teacher errors are systematic and aligned with environmental rewards, making correction difficult

### Mechanism 3
- Claim: Decaying temperature schedule enables smooth transition from imitation to independent learning
- Mechanism: The temperature λ controlling the balance between teacher guidance and environmental feedback is scheduled to decay from a high initial value to zero, allowing gradual independence
- Core assumption: The decay schedule is properly tuned to match the student's learning pace
- Evidence anchors: [section] "We linearly decay the temperature λ, encouraging the student agent to learn from RL loss and deviate from the teacher agent as the training proceeds"
- Break condition: Decay is too rapid (causing instability) or too slow (preventing independence)

## Foundational Learning

- Concept: Reinforcement Learning with Policy Gradient Methods
  - Why needed here: The student agent uses RL to learn from environmental feedback, specifically through PPO-style updates
  - Quick check question: Can you explain the difference between on-policy and off-policy RL, and why on-policy methods like PPO are used here?

- Concept: Knowledge Distillation
  - Why needed here: The framework distills knowledge from the LLM teacher to the student agent by aligning their action distributions
  - Quick check question: How does soft-label distillation differ from hard-label imitation, and why is uncertainty-aware soft instruction important?

- Concept: Prompt Engineering for LLMs
  - Why needed here: The teacher LLM requires carefully designed prompts (Chain-of-Thought, example demonstrations) to generate useful action guidance
  - Quick check question: What is the purpose of including an example in the LLM prompt, and how does Chain-of-Thought reasoning improve the quality of generated instructions?

## Architecture Onboarding

- Component map: State observation -> Student agent -> Query teacher LLM -> Receive soft action distribution -> Execute action -> Environment reward -> Store transition -> Two-loss optimization (RL + distillation) -> Update student parameters -> Decay teacher influence

- Critical path: Student observes state → Queries teacher for action distribution → Executes action and receives reward → Stores transition → Samples batch and computes joint loss → Updates student policy parameters → Decays teacher influence coefficient

- Design tradeoffs: Teacher choice (pre-trained LLM vs task-specific model) vs generalization; Option policies (human-designed vs learned) vs flexibility; KL divergence vs other distillation metrics; Linear vs adaptive decay schedules

- Failure signatures: Student performance plateaus below teacher (teacher errors dominate); Student performance degrades after removing teacher (decay too aggressive); Student never surpasses teacher (environmental rewards don't provide corrective signal)

- First 3 experiments: 1) Ablation: Train student with RL only (no teacher) to establish baseline; 2) Fixed teacher: Train student with constant teacher influence (no decay) to test error correction; 3) Decay schedule sweep: Test different decay rates and plateau durations to find optimal schedule

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different methods of estimating LLM uncertainty (Statistical Estimates, Prompt Design, Softmax of Logits) affect the student agent's performance across various environments?
- Basis in paper: [explicit] The paper compares three methods for calculating P rLLM(k|c(s)) and shows that Statistical Estimates and Softmax of Logits perform better than Prompt Design in SimpleDoorKey
- Why unresolved: The paper only presents results for one environment. It is unclear if these findings generalize to more complex tasks or environments with different characteristics
- What evidence would resolve it: Conducting experiments with different uncertainty estimation methods across a diverse set of environments and comparing the student agent's performance would provide insights into their effectiveness

### Open Question 2
- Question: What is the optimal decay schedule for the kickstarting coefficient λ in the LLM4Teach framework?
- Basis in paper: [explicit] The paper presents an ablation study comparing different decay schedules for λ and shows that a schedule with an initial linear decay followed by a constant small value yields the best performance
- Why unresolved: The optimal decay schedule might depend on factors such as the complexity of the task, the quality of the LLM teacher, and the student agent's architecture
- What evidence would resolve it: Conducting extensive experiments with different decay schedules across various tasks and analyzing the impact on the student agent's learning dynamics and final performance would provide insights into the optimal decay schedule design

### Open Question 3
- Question: How does the quality of the LLM teacher affect the student agent's performance in the LLM4Teach framework?
- Basis in paper: [explicit] The paper mentions that LLMs may provide incorrect, inefficient, or inconsistent strategies, which can hinder the student agent's learning
- Why unresolved: The effectiveness of the LLM4Teach framework relies on the LLM teacher's ability to provide accurate and useful instructions
- What evidence would resolve it: Conducting experiments with LLMs of varying quality or using different prompting strategies to elicit better instructions from the LLM teacher would provide insights into the relationship between LLM quality and the student agent's performance

## Limitations
- Experimental scope restricted to MiniGrid environments with small observation/action spaces
- Limited ablation studies on critical design choices (decay schedule, temperature initialization)
- No analysis of teacher error types or conditions under which correction fails
- Performance comparison with GPT-4 raises questions about whether smaller LLMs can generalize

## Confidence

**High confidence**: The basic teacher-student framework and two-loss optimization approach are sound and technically well-founded
**Medium confidence**: The claim of improved sample efficiency is supported by experiments but limited to three relatively simple MiniGrid tasks
**Low confidence**: The assertion that students can reliably "correct" teacher errors through environmental feedback needs more rigorous validation across diverse error types

## Next Checks

1. **Error Correction Validation**: Systematically inject known incorrect teacher instructions (e.g., 10-20% random action corruption) and measure whether the student agent successfully corrects these errors across multiple training runs.

2. **Teacher Size Sensitivity**: Compare performance using different LLM sizes (from smaller models like Llama-7B up to GPT-4) to determine the minimum viable teacher capability for effective instruction.

3. **Generalization Stress Test**: Evaluate the student agent on completely unseen MiniGrid variants (different layouts, object configurations) to verify whether teacher-guided learning provides robust generalization beyond the training distribution.