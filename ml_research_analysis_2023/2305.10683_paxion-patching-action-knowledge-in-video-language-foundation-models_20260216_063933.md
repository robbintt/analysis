---
ver: rpa2
title: 'Paxion: Patching Action Knowledge in Video-Language Foundation Models'
arxiv_id: '2305.10683'
source_url: https://arxiv.org/abs/2305.10683
tags:
- knowledge
- action
- video
- paxion
- patcher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces the Action Dynamics Benchmark (ActionBench)\
  \ to probe video-language models\u2019 understanding of action knowledge. The benchmark\
  \ contains two tasks\u2014Action Antonym and Video Reversal\u2014that reveal state-of-the-art\
  \ models\u2019 surprising inability to grasp action semantics and temporal dynamics."
---

# Paxion: Patching Action Knowledge in Video-Language Foundation Models

## Quick Facts
- arXiv ID: 2305.10683
- Source URL: https://arxiv.org/abs/2305.10683
- Authors: 
- Reference count: 40
- This work introduces the Action Dynamics Benchmark (ActionBench) to probe video-language models’ understanding of action knowledge.

## Executive Summary
This work introduces the Action Dynamics Benchmark (ActionBench) to probe video-language models’ understanding of action knowledge. The benchmark contains two tasks—Action Antonym and Video Reversal—that reveal state-of-the-art models’ surprising inability to grasp action semantics and temporal dynamics. To address this gap, the authors propose PAXION, a framework that patches frozen video-language models with action knowledge using a Perceiver-based Knowledge Patcher and a cross-attention Knowledge Fuser. A key innovation is the Discriminative Video Dynamics Modeling (DVDM) objective, which replaces or augments the widely used Video-Text Contrastive loss to better capture the relationship between action text and video frame ordering. Experiments show that PAXION trained with DVDM significantly improves performance on both ActionBench tasks (from near-random to ~80% accuracy) and a variety of downstream video-language tasks, while maintaining or enhancing object recognition capabilities.

## Method Summary
PAXION addresses the gap in action knowledge understanding in video-language foundation models through a two-component approach. The Knowledge Patcher (KP) uses a Perceiver architecture to extract action-centric features from a frozen VidLM backbone, acting as an information bottleneck that distills relevant visual information. The DVDM training objective replaces or augments standard VTC loss with Video-Action Contrastive (VAC) and Action-Temporal Matching (ATM) losses to better capture action-text correlations and temporal dynamics. The Knowledge Fuser (KF) integrates the KP-extracted features with the backbone's object-centric representations through cross-attention, creating a balanced representation for downstream tasks.

## Key Results
- State-of-the-art video-language models perform near-random on ActionBench tasks (Action Antonym and Video Reversal)
- PAXION trained with DVDM improves ActionBench performance from near-random to ~80% accuracy
- PAXION maintains or enhances object recognition capabilities while significantly improving action understanding
- The model achieves strong performance on downstream video-language tasks including Video-Text Retrieval, Video-to-Action Retrieval, and Causal-Temporal VQA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Perceiver-based Knowledge Patcher effectively learns action-centric representations by acting as an information bottleneck between frozen VidLM features and action-specific knowledge.
- Mechanism: The Perceiver module performs cross-attention between a small set of learnable latent queries and the higher-dimensional visual embeddings from the frozen VidLM backbone. This dimensionality reduction forces the model to distill only the most relevant visual information for action understanding.
- Core assumption: The learned latents can effectively capture the essential features needed for action understanding without being overwhelmed by irrelevant visual information.
- Evidence anchors:
  - [abstract] "The Knowledge Patcher (KP) is a Perceiver-based [20, 19] lightweight module attached to a frozen VidLM backbone used to augment the VidLM with action-aware representations."
  - [section 3] "As shown in Figure 3 Knowledge Patcher, we use a lightweight Perceiver which performs cross-attention between a sequence of lower-dimensional, learnable latents Q and the higher-dimensional visual embedding V* from a frozen, pretrained VidLM backbone."
  - [corpus] Weak - no direct corpus evidence for Perceiver's effectiveness in this specific action knowledge context.
- Break condition: If the latent dimension is too small, the model may lose critical information needed for action understanding. If too large, it may fail to act as an effective bottleneck.

### Mechanism 2
- Claim: The Discriminative Video Dynamics Modeling (DVDM) objective successfully addresses the limitations of standard Video-Text Contrastive (VTC) loss for learning action knowledge.
- Mechanism: DVDM introduces two new losses - Video-Action Contrastive (VAC) and Action-Temporal Matching (ATM) - that force the model to learn the correlation between action text and video frame ordering. VAC adds action antonym texts as hard negatives, while ATM distinguishes original videos from reversed ones.
- Core assumption: The correlation between action text and temporal video dynamics is learnable through discriminative tasks rather than generative approaches.
- Evidence anchors:
  - [abstract] "Due to limitations of the widely-used Video-Text Contrastive (VTC) loss for learning action knowledge, we introduce the DVDM objective to train the Knowledge Patcher. DVDM forces the model to encode the correlation between the action text and the correct ordering of video frames."
  - [section 3.1] "We propose a novel 'relaxed' formulation of dynamics modeling, dubbed Discriminative Video Dynamics Modeling (DVDM), which contains two losses: Video-Action Contrastive (VAC) and Action-Temporal Matching (ATM)."
  - [corpus] Weak - no direct corpus evidence for DVDM's effectiveness in this specific context.
- Break condition: If the hard negative mining strategy fails to provide meaningful contrastive pairs, or if the temporal ordering information is too subtle to learn through the ATM loss.

### Mechanism 3
- Claim: The Knowledge Fuser component successfully integrates action-centric and object-centric representations while maintaining general VL capabilities.
- Mechanism: The Knowledge Fuser uses cross-attention where the backbone's pooled visual feature serves as the query and the Knowledge Patcher's extracted visual tokens serve as keys and values. This allows the model to selectively attend to both object and action information based on task requirements.
- Core assumption: Cross-attention can effectively balance the contributions of action-centric and object-centric representations without one dominating the other.
- Evidence anchors:
  - [abstract] "The Knowledge Fuser (KF) aims to obtain a balanced representation for general downstream tasks by fusing the KP with the backbone."
  - [section 4] "The KF takes the pooled visual feature (v*) from the frozen VL backbone as the input query, and performs cross-attention with the extracted visual tokens (V) from the Knowledge Patcher."
  - [corpus] Weak - no direct corpus evidence for this specific cross-attention fusion approach.
- Break condition: If the backbone representation dominates the fused output, the action knowledge benefits may be lost. If the KP representation dominates, object understanding may suffer.

## Foundational Learning

- Concept: Video-language foundation models
  - Why needed here: Understanding the baseline capabilities and limitations of existing models is crucial for identifying the specific gap in action knowledge that PAXION addresses.
  - Quick check question: What are the key components of a typical video-language foundation model, and how do they process multimodal inputs?

- Concept: Contrastive learning
  - Why needed here: The paper builds upon and extends contrastive learning approaches, making it essential to understand how these methods work and their limitations.
  - Quick check question: How does the Video-Text Contrastive (VTC) loss function, and what are its limitations for learning action-specific knowledge?

- Concept: Temporal reasoning in video understanding
  - Why needed here: Action understanding inherently requires temporal reasoning, which is a key focus of the paper's proposed solutions.
  - Quick check question: What are the challenges in modeling temporal dynamics in videos, and how do different approaches address these challenges?

## Architecture Onboarding

- Component map: Frozen VidLM backbone -> Knowledge Patcher (Perceiver) -> DVDM training objectives (VTC + VAC + ATM) -> Knowledge Fuser (cross-attention) -> Downstream tasks

- Critical path:
  1. Extract visual features from frozen VidLM backbone
  2. Process visual features through Knowledge Patcher to obtain action-centric representation
  3. Train Knowledge Patcher using VTC + DVDM objectives
  4. Fuse backbone and KP representations using Knowledge Fuser
  5. Fine-tune the integrated model on downstream tasks

- Design tradeoffs:
  - Perceiver vs. Transformer for Knowledge Patcher: Perceiver offers computational efficiency but may lose some representational power
  - VTC only vs. VTC + DVDM: DVDM improves action understanding but requires more complex training
  - Direct fine-tuning vs. fusion approach: Fusion maintains backbone capabilities but adds complexity

- Failure signatures:
  - Poor performance on ActionBench tasks despite good DVDM training: Indicates issues with KP architecture or training data
  - Degradation in object-centric tasks after fusion: Suggests imbalance in the Knowledge Fuser
  - No improvement from DVDM objectives: Points to issues with hard negative mining or ATM task formulation

- First 3 experiments:
  1. Train Knowledge Patcher with VTC loss only and evaluate on ActionBench to establish baseline
  2. Add DVDM objectives (VAC + ATM) to Knowledge Patcher training and compare ActionBench performance
  3. Implement Knowledge Fuser and evaluate on a mix of object-centric and action-centric downstream tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of PAXION change if the Knowledge Patcher were trained on a more diverse and larger dataset beyond the ActionBench tasks?
- Basis in paper: [explicit] The paper discusses training the Knowledge Patcher on ActionBench data, which is constructed from Ego4D and SSv2 datasets.
- Why unresolved: The paper does not explore the impact of using a larger or more diverse dataset for training the Knowledge Patcher.
- What evidence would resolve it: Training PAXION with a Knowledge Patcher trained on a larger, more diverse dataset and comparing its performance to the current model on ActionBench and downstream tasks.

### Open Question 2
- Question: Can the DVDM objective be further improved by incorporating additional types of hard negatives or by using a different formulation of dynamics modeling?
- Basis in paper: [explicit] The paper introduces DVDM with Video-Action Contrastive (VAC) and Action-Temporal Matching (ATM) losses, which are improvements over the widely-used VTC loss.
- Why unresolved: The paper does not explore alternative formulations or additional types of hard negatives for the DVDM objective.
- What evidence would resolve it: Experimenting with different hard negative mining strategies or alternative dynamics modeling formulations and evaluating their impact on the performance of PAXION.

### Open Question 3
- Question: How would the performance of PAXION change if it were integrated with a larger, more capable video-language model?
- Basis in paper: [explicit] The paper evaluates PAXION on three state-of-the-art video-language models: InternVideo, CLIP-ViP, and Singularity-temporal.
- Why unresolved: The paper does not explore the impact of integrating PAXION with larger or more capable video-language models that may emerge in the future.
- What evidence would resolve it: Integrating PAXION with a larger, more capable video-language model and evaluating its performance on ActionBench and downstream tasks.

## Limitations

- The ActionBench benchmark is newly proposed and its ability to comprehensively evaluate action knowledge understanding remains to be validated by the broader research community
- The reliance on frozen VidLM backbones while only training the KP and KF modules may limit the extent of action knowledge integration that can be achieved
- The paper lacks direct comparison to alternative action knowledge patching methods, making it difficult to assess the relative effectiveness of the proposed approach

## Confidence

- **High confidence**: The experimental results demonstrating improved performance on ActionBench tasks and downstream video-language tasks are well-documented and reproducible.
- **Medium confidence**: The mechanism by which the Perceiver-based Knowledge Patcher distills action-centric features is theoretically sound but lacks extensive ablation studies to confirm its necessity over simpler approaches.
- **Medium confidence**: The DVDM objective's effectiveness in capturing temporal dynamics is demonstrated through improved ActionBench performance, but the specific contribution of each component (VAC vs ATM) is not clearly isolated.

## Next Checks

1. Conduct an ablation study comparing the Perceiver-based Knowledge Patcher against simpler alternatives (e.g., direct linear projection or attention-based pooling) to validate the choice of architecture.
2. Perform cross-dataset evaluation of the ActionBench benchmark to ensure it generalizes beyond the Ego4D and SSv2 datasets used in the paper.
3. Test the model's ability to handle novel action-antonym pairs not seen during training to assess the generalization of the learned action knowledge.