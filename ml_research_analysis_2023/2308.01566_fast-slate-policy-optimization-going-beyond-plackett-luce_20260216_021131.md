---
ver: rpa2
title: 'Fast Slate Policy Optimization: Going Beyond Plackett-Luce'
arxiv_id: '2308.01566'
source_url: https://arxiv.org/abs/2308.01566
tags:
- gradient
- learning
- policy
- action
- variance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of optimizing large-scale online
  decision systems that return ordered lists of items (slates) given a query. The
  authors propose a new policy class, Latent Gaussian Perturbation (LGP), which relaxes
  the decision function by adding noise in the latent space rather than the action
  space.
---

# Fast Slate Policy Optimization: Going Beyond Plackett-Luce

## Quick Facts
- arXiv ID: 2308.01566
- Source URL: https://arxiv.org/abs/2308.01566
- Reference count: 40
- Key outcome: Proposes LGP policy class for slate optimization that achieves faster convergence and better results than Plackett-Luce methods on problems with action space sizes in the order of millions

## Executive Summary
This paper addresses the challenge of optimizing large-scale online decision systems that return ordered lists of items (slates) given a query. The authors propose a new policy class, Latent Gaussian Perturbation (LGP), which relaxes the decision function by adding noise in the latent space rather than the action space. This approach results in a simple, efficient learning algorithm that scales to massive action spaces. The proposed method demonstrates superior performance compared to the commonly adopted Plackett-Luce policy class, achieving faster convergence and better results on problems with action space sizes in the order of millions.

## Method Summary
The paper introduces the Latent Gaussian Perturbation (LGP) policy class for slate optimization. Instead of perturbing the action space as in Plackett-Luce methods, LGP adds Gaussian noise to the latent space representation of queries. This allows for tractable sampling and gradient estimation using Maximum Inner Product Search (MIPS) algorithms, reducing computational complexity from O(P log K) to O(log P). The method is evaluated on three collaborative filtering datasets with action space sizes up to 100,000, showing superior performance compared to baseline methods.

## Key Results
- LGP achieves faster convergence than Plackett-Luce-based methods across different datasets
- LGP-MIPS variant outperforms baseline methods consistently across different Monte Carlo sample sizes
- The method demonstrates superior performance on problems with action space sizes in the order of millions
- LGP shows better scalability to large action spaces compared to traditional Plackett-Luce approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LGP policy reduces gradient variance by perturbing the latent space instead of the action space.
- Mechanism: Adding noise in the latent space (R^L) creates a continuous sampling distribution, avoiding the combinatorial explosion of the action space (P>>L).
- Core assumption: The induced distribution on user embeddings remains tractable and smooth.
- Evidence anchors:
  - [abstract] "Latent Gaussian Perturbation, a new policy class based on smoothing the latent space, that is perfectly suitable to optimize decision functions of the form described in (2)."
  - [section 3] "This means that the induced distribution on the user embeddings Qθ(x) is a normal distribution with mean parameter hθ(x), making the evaluation of its the log density gradient easy for any h"
- Break condition: If the latent space dimensionality L is too small to capture meaningful variations, the smoothing effect may be insufficient for good gradient estimates.

### Mechanism 2
- Claim: The LGP policy achieves sublinear time complexity (O(log P)) for sampling and gradient estimation.
- Mechanism: By adding noise in the latent space, sampling becomes compatible with approximate Maximum Inner Product Search (MIPS) algorithms, reducing complexity from O(P log K) to O(log P).
- Core assumption: The action embeddings β are fixed and a MIPS index is precomputed.
- Evidence anchors:
  - [abstract] "This approach results in a simple, efficient learning algorithm that scales to massive action spaces."
  - [section 3] "This class of policies present desirable properties: Fast Sampling... making the sampling achievable in O(log P ), better than the sampling complexity O(P logK) of the Placett-Luce family."
- Break condition: If the MIPS index becomes outdated or the action embeddings β change frequently, the speedup advantage may be lost.

### Mechanism 3
- Claim: The LGP policy does not suffer from variance growth with slate size K, unlike Plackett-Luce.
- Mechanism: The gradient estimate variance for LGP is O(1/σ²) and does not depend on K, while Plackett-Luce variance grows with K.
- Core assumption: The standard deviation σ is fixed appropriately (σ=1/L in experiments).
- Evidence anchors:
  - [abstract] "LGP-MIPS, an accelerated variant of LGP, outperforms Plackett-Luce-based methods consistently across different datasets and Monte Carlo sample sizes"
  - [section 3] "This gradient is a sum over the latent space of size L and does not depend on the size of the action space P nor the slate size K."
- Break condition: If σ is not set properly, the gradient variance could still be high, reducing the advantage over Plackett-Luce.

## Foundational Learning

- Concept: Maximum Inner Product Search (MIPS) algorithms
  - Why needed here: To enable fast sampling and decision making in large action spaces (P in millions)
  - Quick check question: What is the time complexity of MIPS algorithms compared to brute-force search in large action spaces?

- Concept: Policy optimization and score function gradients
  - Why needed here: To understand how policies are learned and why gradient variance is critical for convergence
  - Quick check question: How does the variance of score function gradients affect the step size and convergence rate in stochastic optimization?

- Concept: Gumbel trick and Plackett-Luce distribution
  - Why needed here: To understand the baseline method and why its gradient variance grows with slate size K
  - Quick check question: How does the Gumbel trick enable sampling from the Plackett-Luce distribution, and what is its computational complexity?

## Architecture Onboarding

- Component map:
  - Context encoder h_θ(x) → Latent space embedding
  - Fixed action embeddings β → Action space
  - MIPS index on β → Fast approximate nearest neighbor search
  - LGP sampling (add Gaussian noise to h_θ(x)) → Slate generation
  - Gradient estimation (score function + log density of Gaussian) → Policy update

- Critical path:
  1. Compute context embedding h_θ(x)
  2. Sample Gaussian noise ε ~ N(0, σ²I_L)
  3. Perturb embedding: h = h_θ(x) + ε
  4. Generate slate using MIPS on (h^T β_a)
  5. Compute reward and gradient estimate
  6. Update policy parameters θ

- Design tradeoffs:
  - Fixed vs. learnable action embeddings β: Fixed embeddings reduce variance and memory usage but may limit expressiveness
  - Choice of noise distribution: Gaussian noise provides tractability but other distributions might offer better exploration
  - MIPS index accuracy vs. speed: Approximate MIPS is faster but may introduce errors in slate selection

- Failure signatures:
  - High gradient variance despite LGP: Likely due to poor choice of σ or insufficiently diverse action embeddings
  - Slow convergence: Could indicate need for better MIPS index or more Monte Carlo samples S
  - Degraded performance with large K: May suggest the latent space is too constrained to handle complex interactions

- First 3 experiments:
  1. Compare LGP gradient variance vs. Plackett-Luce for varying K (e.g., K=2,5,10) on a small dataset
  2. Measure sampling time for LGP with/without MIPS index on a dataset with P=100K actions
  3. Ablation study: Train LGP with fixed vs. learnable action embeddings β on a medium-scale dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of noise distribution Q in the Latent Random Perturbation (LRP) policy affect the optimization performance and convergence properties?
- Basis in paper: [explicit] The paper discusses using a Centered Independent Gaussian distribution but mentions that future research could explore the impact of different noise distributions.
- Why unresolved: The paper only experiments with a Gaussian noise distribution and does not compare it with other potential distributions (e.g., Laplace, uniform) to assess their impact on learning performance.
- What evidence would resolve it: Conducting experiments comparing LGP performance using different noise distributions (Gaussian, Laplace, uniform) while keeping other factors constant would reveal which distribution yields the best optimization results.

### Open Question 2
- Question: Can the LGP approach be effectively extended to non-linear user embedding functions beyond the linear case explored in the paper?
- Basis in paper: [explicit] The paper mentions that experiments with deep policies can be found in the appendix, suggesting that LGP was tested with neural network backbones, but the main results focus on linear policies.
- Why unresolved: While the paper briefly mentions deep policies in the appendix, it does not provide a comprehensive analysis of how LGP performs with various non-linear (e.g., multi-layer neural networks, attention-based) user embedding functions compared to linear ones.
- What evidence would resolve it: Extensive experiments comparing LGP performance with linear and various non-linear (e.g., multi-layer neural networks, attention-based) user embedding functions would determine if LGP's advantages extend beyond linear policies.

### Open Question 3
- Question: How does the variance of gradient estimates in LGP scale with the latent space dimension L, and what is the optimal L for balancing computational efficiency and learning performance?
- Basis in paper: [inferred] The paper discusses that LGP has a variance that does not grow with the slate size K, but it does not explicitly analyze how the variance scales with the latent space dimension L, which is a crucial factor in the method's design.
- Why unresolved: The paper fixes L to 100 in experiments without exploring how different values of L affect the gradient variance and overall performance, leaving the optimal choice of L unclear.
- What evidence would resolve it: Conducting a systematic study varying L across a wide range (e.g., 10, 50, 100, 200) and measuring both gradient variance and policy performance would identify the optimal L for different problem scales.

## Limitations

- The method's performance on non-collaborative filtering domains remains unexplored
- The paper assumes fixed action embeddings, which may limit expressiveness in some applications
- The optimal choice of latent space dimensionality L is not systematically studied

## Confidence

- Computational complexity claims: High
- Variance reduction benefits: Medium-High
- Overall performance superiority: Medium

## Next Checks

1. **Domain Transferability**: Evaluate LGP performance on non-collaborative filtering domains (e.g., search ranking, display advertising) with different reward structures and action embedding methods.

2. **Parameter Sensitivity Analysis**: Systematically study the impact of σ choice, latent space dimensionality L, and MIPS index parameters on both convergence speed and final performance across multiple problem scales.

3. **Robustness to Action Embedding Quality**: Compare LGP performance when using learned vs. fixed action embeddings, and assess degradation when action embeddings are noisy or outdated.