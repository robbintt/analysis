---
ver: rpa2
title: When can transformers reason with abstract symbols?
arxiv_id: '2310.09753'
source_url: https://arxiv.org/abs/2310.09753
tags:
- template
- transformer
- loss
- kernel
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies whether transformers can learn abstract relational
  reasoning tasks, where the goal is to generalize beyond training symbols to new
  unseen symbols. It analyzes two settings: regression tasks with real-valued labels
  and next-token prediction tasks with symbolic labels.'
---

# When can transformers reason with abstract symbols?

## Quick Facts
- arXiv ID: 2310.09753
- Source URL: https://arxiv.org/abs/2310.09753
- Reference count: 40
- One-line primary result: Transformers can learn abstract relational reasoning but require large training data for real-valued tasks and fail on symbolic tasks as embedding dimension increases

## Executive Summary
This paper investigates whether transformers can learn abstract relational reasoning tasks that require generalizing beyond training symbols to new unseen symbols. The authors analyze two settings: regression tasks with real-valued labels and next-token prediction tasks with symbolic labels. They prove that transformers can generalize on real-valued template tasks but require large amounts of training data. For symbolic-label tasks, they show that transformers fail as embedding dimension increases due to orthogonality issues. Based on these theoretical insights, the paper proposes architectural modifications—adding trainable identity scaling parameters to attention heads—that improve data efficiency for learning to reason.

## Method Summary
The authors analyze transformer generalization on template tasks using kernel methods and theoretical analysis. They implement synthetic template tasks where templates contain wildcard tokens that are filled with variable names. The training involves standard gradient descent with weight decay on these synthetic datasets. The key methodological innovation is analyzing the training dynamics through the neural tangent kernel framework, examining how the attention mechanism's evolution relates to generalization on unseen symbols. The proposed architectural modifications involve adding trainable identity scaling parameters to each attention head's computation.

## Key Results
- Transformers can learn abstract relations and generalize to unseen symbols when trained on sufficiently large quantities of data for real-valued template tasks
- Transformers fail to generalize on unseen symbols for symbolic-label tasks as embedding dimension increases
- Adding trainable identity scaling parameters to attention heads improves data efficiency for learning to reason, validated on template tasks and real language modeling data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers can learn abstract relational reasoning by treating variable names as symbols whose meaning comes from their relationships rather than their identities
- Mechanism: The attention mechanism captures relational structure through the attention matrix, which encodes how tokens interact. When trained on template tasks, the transformer learns to use these interactions to classify rather than memorizing specific symbol identities
- Core assumption: The attention mechanism can effectively capture the relational structure needed for template tasks
- Evidence anchors: [abstract] "we prove that transformers learn the abstract relations and generalize to the test set when trained by gradient descent on sufficiently large quantities of training data"

### Mechanism 2
- Claim: Adding trainable identity scaling parameters to attention heads improves data efficiency for learning to reason
- Mechanism: By adding parameters a to each attention head so that WKW^T_Q is replaced by WKW^T_Q + aI, the transformer emphasizes the incidence matrix XX^T, which contains relational information about which tokens appear together
- Core assumption: The incidence matrix XX^T contains sufficient relational information for the template tasks
- Evidence anchors: [abstract] "adding trainable identity scaling parameters to attention heads—that improve data efficiency for learning to reason"

### Mechanism 3
- Claim: Transformers fail at copying unseen symbols as embedding dimension increases due to orthogonality issues
- Mechanism: As embedding dimension grows, the evolution of W^T_O W V,h weights during early training lies approximately in the span of {W^T_E e_xi e^T_xi W_E}_i, which becomes orthogonal to the direction W^T_E e_xtest e^T_xtest W_E needed to copy unseen symbols
- Core assumption: The orthogonality between training and test directions becomes more pronounced as embedding dimension increases
- Evidence anchors: [abstract] "transformers fail to generalize as their embedding dimension increases... We prove that transformers with large embedding dimension fail to generalize on unseen symbols for the copy-task"

## Foundational Learning

- Concept: Template tasks as a framework for measuring reasoning with abstract symbols
  - Why needed here: Provides the theoretical foundation for understanding when and how transformers can perform abstract reasoning
  - Quick check question: Can you explain the difference between real-label template tasks and symbolic-label template tasks?

- Concept: Kernel methods and their role in analyzing neural network generalization
  - Why needed here: The paper uses kernel analysis to prove when transformers can generalize on unseen symbols
  - Quick check question: What is the relationship between the transformer random features kernel and the final layer training dynamics?

- Concept: Permutation invariance in neural network training
  - Why needed here: Explains why MLPs fail at template tasks while transformers succeed
  - Quick check question: How does permutation invariance prevent MLPs from distinguishing between unseen tokens?

## Architecture Onboarding

- Component map: Input → Embedding (W_E) → Positional Embeddings (P) → Attention Heads (W_K,h, W_Q,h, W_V,h, W_O,h) → MLP Layer (W_A, W_B) → Unembedding Layer (w_U) → Output
- Critical path: Input → Embedding → Attention → MLP → Unembedding → Output
- Design tradeoffs:
  - Width vs depth: Wider networks converge to kernel regime, deeper networks may capture more complex patterns
  - Embedding dimension: Higher dimensions improve representational capacity but can hurt generalization on symbolic tasks
  - Number of attention heads: More heads capture diverse patterns but increase computational cost
- Failure signatures:
  - Poor generalization on unseen symbols: Check if identity scaling parameters are properly implemented
  - Overfitting to training symbols: May need more data or architectural modifications
  - Training instability: Check learning rates and batch sizes
- First 3 experiments:
  1. Implement basic template task (e.g., αα vs αβ) and test transformer performance
  2. Add identity scaling parameters and compare data efficiency
  3. Test copying task with varying embedding dimensions to observe inverse scaling law

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed architectural modification (adding trainable identity scalings) improve data efficiency for more complex reasoning tasks beyond the template tasks studied in this paper?
- Basis in paper: Explicit - The paper proposes adding one trainable parameter to each attention head and mentions this is validated experimentally on template tasks and real language modeling data
- Why unresolved: The paper focuses on specific template tasks and does not explore whether these modifications generalize to more complex, real-world reasoning tasks
- What evidence would resolve it: Testing the modified architecture on diverse reasoning benchmarks (e.g., mathematical word problems, logical syllogisms, or multi-step reasoning tasks) and comparing data efficiency against standard transformers

### Open Question 2
- Question: How does the transformer's inductive bias for relational reasoning change with different activation functions beyond the cosine function used in the theoretical analysis?
- Basis in paper: Explicit - The paper analyzes the transformer with ϕ(t) = cos(b1t + b2) out of technical convenience but conjectures that most non-polynomial activation functions should succeed
- Why unresolved: The analysis is limited to cosine activation for mathematical tractability, leaving open the question of how other activation functions affect the model's ability to learn relational reasoning
- What evidence would resolve it: Empirical comparison of transformer performance on template tasks using various activation functions, measuring generalization on unseen symbols and data efficiency

### Open Question 3
- Question: Can the failure of transformers to generalize on symbolic-label tasks (e.g., copying unseen symbols) be mitigated by other architectural changes beyond the proposed attention-modulated skip connection?
- Basis in paper: Explicit - The paper shows transformers fail on symbolic-label tasks as embedding dimension grows and proposes adding trainable parameters to WV W T
O to fix this
- Why unresolved: The paper proposes one specific fix but does not explore alternative architectural modifications or training strategies that might address this failure mode
- What evidence would resolve it: Experimenting with different architectural changes (e.g., different skip connections, attention mechanisms, or embedding strategies) and testing their effectiveness on symbolic-label tasks

## Limitations

- The theoretical analysis relies heavily on the neural tangent kernel regime, which may not accurately capture practical transformer behavior
- Empirical validation is limited to synthetic template tasks without thorough demonstration on complex real-world reasoning benchmarks
- The copying task results showing inverse scaling with embedding dimension need further investigation to determine if this is a fundamental limitation

## Confidence

**High Confidence**: The proof that transformers can learn to generalize on real-valued template tasks with sufficient data, and the theoretical analysis of why standard transformers fail on symbolic-label tasks as embedding dimension increases.

**Medium Confidence**: The proposed architectural modifications (identity scaling parameters) will consistently improve data efficiency across different reasoning tasks and scales, based on limited empirical validation.

**Low Confidence**: The claim that transformers cannot learn to copy unseen symbols in high-dimensional spaces without modifications, as this requires further empirical validation on diverse copying tasks.

## Next Checks

1. **Scaling Experiments**: Systematically vary embedding dimension and model size to verify the inverse scaling relationship in copying tasks across multiple copying benchmarks, including both synthetic and natural language tasks.

2. **Real-World Reasoning Tasks**: Evaluate the identity scaling modification on established abstract reasoning benchmarks like Raven's Progressive Matrices or analogical reasoning tasks to assess practical utility beyond template tasks.

3. **Alternative Kernel Analysis**: Investigate whether transformers with non-standard initialization or different attention mechanisms (e.g., sparse attention) can overcome the symbolic generalization limitations identified in the kernel analysis.