---
ver: rpa2
title: 'Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design'
arxiv_id: '2305.13035'
source_url: https://arxiv.org/abs/2305.13035
tags:
- scaling
- shape
- arxiv
- compute
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for optimizing the shape of vision
  transformers, focusing on depth, width, and MLP size. The authors derive scaling
  laws that predict how to scale these dimensions optimally for a given compute budget.
---

# Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design

## Quick Facts
- **arXiv ID**: 2305.13035
- **Source URL**: https://arxiv.org/abs/2305.13035
- **Authors**: 
- **Reference count**: 40
- **Primary result**: Proposed method optimizes ViT shapes for compute budgets, achieving 90.3% ImageNet accuracy with less than half the inference cost of larger models

## Executive Summary
This paper presents a method for optimizing vision transformer architectures by deriving scaling laws that predict optimal depth, width, and MLP size dimensions for a given compute budget. The authors introduce a star sweep methodology that efficiently estimates scaling exponents with significantly fewer experiments than prior approaches. They validate their method by optimizing a ViT architecture to match the compute of a larger model, resulting in a smaller, faster model with comparable performance. The shape-optimized model achieves state-of-the-art results across multiple vision tasks while reducing inference costs substantially.

## Method Summary
The method involves three key steps: first, conducting a star sweep to estimate scaling exponents for individual shape dimensions by varying one dimension at a time while fixing others at large values; second, performing a grid sweep on small models trained for short compute to identify a compute-optimal architecture in the Pareto frontier; and third, scaling the identified architecture jointly for different compute budgets using the derived scaling laws. The approach leverages quasiconvex relationships between shape dimensions and performance under fixed compute, allowing efficient identification of optimal model shapes without exhaustive grid search.

## Key Results
- Shape-optimized ViT achieves 90.3% fine-tuning accuracy on ImageNet, surpassing much larger models
- SoViT model has less than half the inference cost of larger baseline models while maintaining comparable performance
- The method demonstrates effectiveness across multiple tasks including image classification, captioning, VQA, and zero-shot transfer
- Star sweep methodology reduces required experiments by an order of magnitude compared to prior work

## Why This Works (Mechanism)

### Mechanism 1
The proposed functional form captures the non-monotonic relationship between shape dimensions and performance under fixed compute. The term involving $x_k^{-a_k}$ represents performance degradation as dimensions become too small, while $x_k^{b_k} t^{-c}$ captures performance improvement with diminishing returns. This creates a quasiconvex curve with a unique optimum. The core assumption is that other shape dimensions are sufficiently large such that they do not constitute bottlenecks when estimating parameters.

### Mechanism 2
The star sweep method efficiently estimates scaling exponents by varying single dimensions at a time in exponentially-spaced grids while fixing other dimensions to large values. This isolates each dimension's effect on performance, enabling accurate exponent estimation without exhaustive grid search. The key assumption is that scaling exponents are robust to the choice of evaluation metric, which is supported by empirical evidence showing relative stability across different shot settings.

### Mechanism 3
The derived scaling exponents accurately predict optimal model shapes for different compute budgets by scaling all dimensions jointly by a factor of $\tau^{s_k/D}$ for each increment in compute by a factor of $\tau$. This maintains optimal model shape as compute increases. The method assumes the functional form accurately captures relationships between model shape, compute, and performance across scales, which is demonstrated through successful extrapolation from small to large models.

## Foundational Learning

- **Concept**: Power laws in machine learning
  - **Why needed here**: The method relies on power law relationships between model size, compute, and performance
  - **Quick check**: What is the general form of a power law relationship in machine learning?

- **Concept**: Quasiconvex functions
  - **Why needed here**: The method depends on quasiconvexity of the performance function with respect to individual shape dimensions under fixed compute
  - **Quick check**: What is the defining property of a quasiconvex function?

- **Concept**: Scaling laws
  - **Why needed here**: The method uses scaling laws to predict how model shapes should change as compute budgets increase
  - **Quick check**: What is the general form of a scaling law in machine learning?

## Architecture Onboarding

- **Component map**: Vision Transformer backbone with optimized depth, width, and MLP size dimensions → Pretrained on JFT-3B dataset → Evaluated on downstream tasks (ImageNet, COCO, VQA, OCR, GQA)

- **Critical path**: (1) Conduct star sweep to estimate scaling exponents, (2) Perform grid sweep to identify small compute-optimal architecture, (3) Scale architecture jointly for target compute budget and evaluate performance

- **Design tradeoffs**: Trades accuracy of estimated scaling exponents for efficiency of star sweep approach; assumes scaling exponents are robust to evaluation metrics

- **Failure signatures**: Inaccurate scaling exponent estimation leading to suboptimal model shapes, overfitting/underfitting during shape optimization, violation of bottleneck assumptions

- **First 3 experiments**:
  1. Conduct star sweep to estimate scaling exponents for width, depth, and MLP size dimensions
  2. Perform grid sweep to identify a small compute-optimal architecture
  3. Scale the architecture jointly for a target compute budget and evaluate its performance on downstream tasks

## Open Questions the Paper Calls Out

### Open Question 1
How sensitive is the scaling strategy to different vision transformer architectures beyond ViT, such as Swin or MLP-Mixer? The paper focuses on optimizing ViT shapes and mentions potential applicability to other domains but does not test other transformer architectures. This could be resolved by conducting similar scaling experiments on other vision transformer architectures.

### Open Question 2
How do the scaling laws change when using different pre-training datasets or objectives? The paper uses JFT-3B dataset and standard ViT pre-training objective, but scaling laws might be dataset and objective dependent. This could be resolved by repeating scaling experiments with different pre-training datasets and objectives.

### Open Question 3
What is the impact of optimizing model shape on inference speed and memory usage in practical deployment scenarios? While the paper mentions SoViT has less than half the inference cost of larger models, it doesn't provide detailed measurements or analysis of inference speed and memory usage. This could be resolved through detailed benchmarking on various hardware platforms.

## Limitations

- The quasiconvexity assumption remains theoretical rather than empirically validated across all regimes
- Efficiency claims rely on assumptions about fixing other dimensions at large values without affecting exponent estimation
- Extrapolation capability from small to large models is demonstrated but not extensively validated with error bounds

## Confidence

**High confidence**: Empirical results demonstrating SoViT's performance gains across multiple benchmarks are well-supported by presented experiments and the methodology is clearly specified and reproducible.

**Medium confidence**: Theoretical framework connecting shape optimization to scaling laws is internally consistent but some mathematical derivations require deeper verification, and efficiency gain claims could benefit from more direct comparisons.

**Low confidence**: Claims about universality of scaling exponents across different tasks and model families are not thoroughly tested, and the assumption about exponent stability during pretraining to fine-tuning transitions is not explicitly validated.

## Next Checks

1. **Quasiconvexity validation**: Conduct systematic experiments varying two shape dimensions simultaneously under fixed compute to empirically verify the quasiconvex property, particularly near optimal points.

2. **Robustness to baseline architecture size**: Repeat star sweep methodology using different center architectures (varying from ViT-Huge to smaller variants) to quantify sensitivity of estimated scaling exponents to baseline model size.

3. **Cross-task exponent stability**: Evaluate whether scaling exponents estimated from ImageNet pretraining transfer accurately to non-vision tasks like VQA and OCR, or if task-specific exponent estimation is necessary.