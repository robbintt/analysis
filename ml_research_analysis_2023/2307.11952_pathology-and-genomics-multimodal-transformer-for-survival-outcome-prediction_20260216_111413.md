---
ver: rpa2
title: Pathology-and-genomics Multimodal Transformer for Survival Outcome Prediction
arxiv_id: '2307.11952'
source_url: https://arxiv.org/abs/2307.11952
tags:
- image
- data
- finetuning
- multimodal
- cancer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes PathOmics, a pathology-and-genomics multimodal
  transformer for survival outcome prediction in cancer. The key idea is to leverage
  unsupervised pretraining to capture the intrinsic interaction between tissue microenvironments
  in whole slide images and a wide range of genomics data, followed by task-specific
  model finetuning that can handle both multi- and single-modal data.
---

# Pathology-and-genomics Multimodal Transformer for Survival Outcome Prediction

## Quick Facts
- arXiv ID: 2307.11952
- Source URL: https://arxiv.org/abs/2307.11952
- Authors: 
- Reference count: 40
- Primary result: PathOmics achieves C-index of 0.7 on TCGA-COAD and TCGA-READ datasets, outperforming state-of-the-art studies

## Executive Summary
This paper introduces PathOmics, a multimodal transformer framework that integrates whole slide images (WSIs) and genomics data for cancer survival outcome prediction. The approach uses unsupervised pretraining to align pathology and genomics embeddings through MSE loss, followed by flexible finetuning that can handle both multi- and single-modal data. The method demonstrates competitive performance on TCGA colon and rectum cancer cohorts, with particular effectiveness when using limited finetuning samples. The framework shows that cross-modal pretraining knowledge transfers effectively to single-modal tasks.

## Method Summary
PathOmics combines pathology images and genomics data through a multimodal transformer architecture. The method first extracts group-wise embeddings from both modalities (WSIs processed through CNN feature extractors, genomics organized into 8 functional groups). An unsupervised pretraining phase aligns image and genomics embeddings using MSE loss to capture intrinsic interactions. The model then undergoes supervised finetuning with three possible modes: multimodal, image-only, or genomics-only. Global attention pooling aggregates patient-level representations before risk classification. The framework was evaluated on TCGA-COAD and TCGA-READ datasets.

## Key Results
- Achieved C-index of 0.7 on TCGA-COAD and TCGA-READ datasets
- Outperformed state-of-the-art studies in survival outcome prediction
- Demonstrated effectiveness with limited data, achieving comparable performance with only 50% of finetuning samples
- Showed superior performance when combining different types of genomics data versus using them in isolation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The unsupervised pretraining step aligns multimodal embeddings (pathology and genomics) into the same latent space using MSE loss, enabling effective cross-modal interaction.
- Mechanism: By minimizing the mean squared error between paired image and genomics embeddings for the same patient, the model learns to map different modalities into a shared space where semantically related features are close, regardless of modality differences.
- Core assumption: The paired image-genomics samples contain sufficient intrinsic correlation that minimizing MSE will align semantically similar features across modalities.
- Evidence anchors:
  - [abstract]: "unsupervised pretraining to capture the intrinsic interaction between tissue microenvironments in gigapixel whole slide images (WSIs) and a wide range of genomics data"
  - [section]: "we develop an unsupervised data fusion strategy by decreasing the mean square error (MSE) loss to map images and genomics embeddings into the same space"
  - [corpus]: Weak - no direct mention of MSE-based pretraining in neighbor papers

### Mechanism 2
- Claim: The group-wise feature extraction approach enables handling heterogeneous genomics data by organizing genes into clinically relevant functional groups.
- Mechanism: Genomics data is partitioned into 8 functional groups based on clinical relevance, with each group processed separately using SNN and attention pooling, allowing the model to capture group-specific patterns while maintaining overall structure.
- Core assumption: The 8 functional groups defined in [22] adequately capture the clinically relevant structure of cancer genomics data.
- Evidence anchors:
  - [section]: "We define the group-wise genomics representation by referring to N = 8 major functional groups obtained from [22]"
  - [abstract]: "wide range of genomics data (e.g., mRNA-sequence, copy number variant, and methylation)"
  - [corpus]: Missing - neighbor papers don't discuss group-wise genomics processing

### Mechanism 3
- Claim: The flexible modality finetuning allows single-modal data to benefit from multimodal pretraining through cross-modal information aggregation.
- Mechanism: After pretraining on both modalities, the model contains cross-modal knowledge that can be applied even when finetuning on single-modal data, because the single-modality embeddings were generated through the same multimodal architecture.
- Core assumption: The cross-modal knowledge learned during pretraining transfers effectively to single-modal tasks.
- Evidence anchors:
  - [abstract]: "task-specific model finetuning could expand the scope of data utility applicable to both multi- and single-modal data"
  - [section]: "our approach enables three types of finetuning modal modes (i.e., multimodal, image-only, and genomics-only)"
  - [corpus]: Weak - neighbor papers mention multimodal approaches but don't discuss single-modal finetuning benefits

## Foundational Learning

- Concept: Self-normalizing neural networks (SNN) with SeLU activation
  - Why needed here: To extract stable, normalized features from heterogeneous genomics data groups
  - Quick check question: What property of SeLU activation helps maintain mean and variance across layers in deep networks?

- Concept: Multi-head self-attention mechanism
  - Why needed here: To capture complex interactions between different feature groups within each modality
  - Quick check question: How does multi-head attention enable the model to focus on different types of relationships simultaneously?

- Concept: Global attention pooling
  - Why needed here: To aggregate patient-wise representations from group-wise embeddings while weighting important groups more heavily
  - Quick check question: What advantage does attention-based pooling have over simple averaging for combining group representations?

## Architecture Onboarding

- Component map: Feature extractors (CNN for images, SNN for genomics) -> Group-wise embedding layers with attention refiners -> Two parallel transformer streams (image and genomics) -> Global attention pooling layers -> Multimodal fusion layer (concatenation) -> Risk classifier (FC layers)

- Critical path: 1. Extract group-wise embeddings from both modalities 2. Process through modality-specific transformers 3. Apply global attention pooling 4. Concatenate for multimodal or use single modality 5. Risk classification

- Design tradeoffs:
  - MSE vs cosine similarity for pretraining: MSE directly minimizes distance, cosine focuses on angular similarity
  - Group size vs number: Larger groups capture more context but may lose specificity
  - Transformer layers vs simpler models: Transformers capture complex interactions but increase computational cost

- Failure signatures:
  - Poor C-index performance: Check if pretraining converged properly or if data preprocessing introduced errors
  - Mode collapse during pretraining: Verify if MSE loss is too aggressive, causing embeddings to collapse to similar values
  - Single-modal finetuning not benefiting from pretraining: Check if cross-modal knowledge transfer is working by comparing embeddings before and after pretraining

- First 3 experiments:
  1. Verify group-wise embedding extraction works by visualizing attention weights and checking embedding dimensions
  2. Test pretraining convergence by monitoring MSE loss and checking if embeddings are actually aligning
  3. Validate single-modal finetuning by comparing performance with and without pretraining on held-out data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the PathOmics model change when using different types of genomics data (mRNA-sequence, copy number variant, and methylation) in isolation versus combined?
- Basis in paper: [explicit] The paper mentions that combining different types of genomics data leads to distinguishing survival outcomes and that the model performs better with a single-modal finetuning strategy compared to multimodal finetuning.
- Why unresolved: The paper does not provide a detailed comparison of the model's performance with different combinations of genomics data types in isolation and in combination.
- What evidence would resolve it: Conducting experiments with different combinations of genomics data types (e.g., mRNA-sequence alone, copy number variant alone, methylation alone, and all combinations) and comparing the results would provide insights into the importance of each data type and their interactions.

### Open Question 2
- Question: How does the PathOmics model's performance compare to other state-of-the-art models when using a smaller dataset for finetuning?
- Basis in paper: [explicit] The paper mentions that the model achieves comparable performance even with fewer finetuned data, but it does not provide a direct comparison with other state-of-the-art models under the same conditions.
- Why unresolved: The paper does not provide a detailed comparison of the model's performance with other state-of-the-art models when using a smaller dataset for finetuning.
- What evidence would resolve it: Conducting experiments with other state-of-the-art models using the same smaller dataset for finetuning and comparing the results would provide insights into the model's efficiency and performance under data-constrained conditions.

### Open Question 3
- Question: How does the PathOmics model's performance change when using different types of pathology images (e.g., different staining methods or image resolutions)?
- Basis in paper: [inferred] The paper mentions that the model uses whole slide images (WSIs) for pathology information, but it does not discuss the impact of different image types or resolutions on the model's performance.
- Why unresolved: The paper does not provide a detailed analysis of the model's performance with different types of pathology images.
- What evidence would resolve it: Conducting experiments with different types of pathology images (e.g., different staining methods or image resolutions) and comparing the results would provide insights into the model's robustness and generalizability to different image types.

## Limitations
- Evaluated only on two cancer types (TCGA-COAD and TCGA-READ), limiting generalizability
- Pretraining relies heavily on MSE loss, which may not capture complex semantic relationships if paired samples have weak correlation
- Group-wise genomics processing depends on predefined functional groups that may not optimally represent underlying data structure

## Confidence
- Multimodal pretraining via MSE alignment: Medium
- Single-modal finetuning benefits: Medium
- Group-wise genomics approach: Medium

## Next Checks
1. Cross-cancer validation: Evaluate the pretrained model on additional TCGA cancer types to assess generalizability beyond colon and rectum cancers
2. Pretraining objective ablation: Compare MSE pretraining against alternative objectives (cosine similarity, contrastive loss) to determine if the alignment mechanism is optimal for capturing cross-modal relationships
3. Functional group sensitivity: Systematically vary the number and composition of genomics functional groups to identify whether the current 8-group structure is optimal or if alternative groupings yield better performance