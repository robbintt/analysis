---
ver: rpa2
title: 'Herd: Using multiple, smaller LLMs to match the performances of proprietary,
  large LLMs via an intelligent composer'
arxiv_id: '2310.19902'
source_url: https://arxiv.org/abs/2310.19902
tags:
- herd
- performance
- source
- open
- able
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Herd, a method that uses multiple smaller
  open-source language models, intelligently routed to match or exceed the performance
  of larger proprietary models like ChatGPT. The approach employs a router trained
  to allocate incoming queries to the most suitable model in the herd based on expected
  performance.
---

# Herd: Using multiple, smaller LLMs to match the performances of proprietary, large LLMs via an intelligent composer

## Quick Facts
- arXiv ID: 2310.19902
- Source URL: https://arxiv.org/abs/2310.19902
- Reference count: 22
- Primary result: A herd of open-source models can match or exceed ChatGPT's performance on MMLU despite being 2.5x smaller

## Executive Summary
This paper introduces Herd, a method that uses multiple smaller open-source language models intelligently routed to match or exceed the performance of larger proprietary models like ChatGPT. The approach employs a router trained to allocate incoming queries to the most suitable model in the herd based on expected performance. Despite being composed of models 2.5x smaller in effective size, the Herd achieves comparable or better accuracy than ChatGPT on the MMLU benchmark, correctly answering questions that ChatGPT fails on 69.3% of the time.

## Method Summary
The Herd system trains a router model (bert-medium) to predict which model in a diverse pool of open-source LLMs will perform best on each query. The router is trained on 12,000 examples from MMLU using a loss function comparing predicted vs actual F1 scores. Incoming queries are routed to the model most likely to answer correctly, allowing smaller models to handle queries they're good at while larger models handle harder ones. The system uses a mix of quantized and non-quantized models (7B, 15B, 30B, 70B parameters) to balance performance and computational cost.

## Key Results
- Herd matches or surpasses ChatGPT's performance on 74% of MMLU queries
- Herd correctly answers questions that ChatGPT fails on 69.3% of the time
- Herd can identify a correct-answering model 40% of the time when ChatGPT is wrong
- All models in the herd are open-source and can be easily expanded or interchanged

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Herd system can match or exceed ChatGPT performance by intelligently routing queries to the most suitable open-source model from a diverse pool.
- Mechanism: A trained router model evaluates incoming queries and assigns them to the open-source model most likely to answer correctly, based on learned performance associations.
- Core assumption: There exists a sufficient diversity of open-source models such that at least one model in the herd can correctly answer any given query that ChatGPT could handle.
- Evidence anchors:
  - [abstract] "We show that a Herd of open source models is able to match the accuracy of ChatGPT, despite being composed of models that are effectively 2.5x smaller."
  - [section] "We show that in cases where GPT is not able to answer the query, Herd is able to identify a model that can, at least 40% of the time."
- Break condition: If the diversity or quality of open-source models is insufficient to cover the query space, or if the router fails to accurately predict which model will perform best.

### Mechanism 2
- Claim: The Herd system can correctly answer questions that ChatGPT fails on approximately 69.3% of the time.
- Mechanism: By leveraging the collective knowledge of multiple models, the Herd can find a model that knows the answer when ChatGPT does not, effectively covering ChatGPT's blind spots.
- Core assumption: Different models have different strengths and weaknesses, and the collective knowledge of the herd exceeds that of any single model including ChatGPT.
- Evidence anchors:
  - [abstract] "In cases where GPT is not able to answer the query, Herd is able to identify a model that can, at least 40% of the time."
  - [section] "In the cases where ChatGPT was wrong... Herd was able to achieve a correct answer... 69.3% of the time."
- Break condition: If the models in the herd have overlapping weaknesses or if the router fails to identify the correct model for the query.

### Mechanism 3
- Claim: The Herd system offers a cost-effective and scalable alternative to proprietary models by using smaller, open-source models.
- Mechanism: By intelligently routing queries to smaller models when possible, the system reduces the overall computational cost while maintaining performance.
- Core assumption: Smaller models can be used effectively for many queries, reducing the need for large, expensive models for all queries.
- Evidence anchors:
  - [abstract] "despite being composed of models that are effectively 2.5x smaller."
  - [section] "Herd was able to surpass ChatGPT's performance. Further, all the models are open source, and the herd can be seamlessly expanded, contracted or interchanged for other models."
- Break condition: If the cost savings from using smaller models are outweighed by the complexity of the routing system or if the performance degradation of smaller models is too significant.

## Foundational Learning

- Concept: Large Language Model (LLM) Performance Benchmarks
  - Why needed here: Understanding how LLM performance is measured and compared is crucial for interpreting the results of the Herd system.
  - Quick check question: What is the MMLU benchmark and why is it used to evaluate LLM performance?

- Concept: Model Quantization and Size Optimization
  - Why needed here: The paper discusses using quantized models to reduce computational cost while maintaining performance.
  - Quick check question: How does model quantization affect LLM performance and what are the trade-offs involved?

- Concept: Multi-Armed Bandit Problem and Exploration-Exploitation Trade-off
  - Why needed here: The router in the Herd system must balance exploring different models to find the best performer for each query type while exploiting known good models.
  - Quick check question: How does the router in the Herd system balance the need to explore different models with the need to exploit known good models?

## Architecture Onboarding

- Component map: Query Interface -> Router (bert-medium) -> Model Herd (diverse open-source LLMs) -> Performance Monitor
- Critical path:
  1. Incoming query is received by the query interface.
  2. Router evaluates the query and predicts which model will perform best.
  3. Query is routed to the selected model.
  4. Model generates a response.
  5. Response is returned to the user.

- Design tradeoffs:
  - Model diversity vs. computational cost: Including more diverse models increases the chances of finding a good match for each query but also increases computational overhead.
  - Router complexity vs. accuracy: A more complex router may make better predictions but also requires more resources to train and run.
  - Quantization level vs. performance: More aggressive quantization reduces computational cost but may also degrade model performance.

- Failure signatures:
  - Consistently poor performance across all queries: This may indicate that the models in the herd are not diverse enough or that the router is not functioning correctly.
  - High variance in performance across different query types: This may indicate that the router is not effectively learning the associations between query types and model performance.
  - High computational cost: This may indicate that the system is not effectively leveraging smaller models and is defaulting to larger, more expensive models too often.

- First 3 experiments:
  1. Evaluate the performance of the Herd system on a subset of the MMLU benchmark to validate that it can match or exceed ChatGPT's performance.
  2. Test the system's ability to correctly answer questions that ChatGPT fails on by comparing the Herd's performance to ChatGPT's on a set of known failure cases.
  3. Measure the computational cost of the Herd system compared to using a single large model like ChatGPT to validate the cost-effectiveness claim.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of Herd scale with the number of models in the herd?
  - Basis in paper: [inferred] The paper mentions that Herd can be "seamlessly expanded, contracted or interchanged for other models" but does not explore how performance scales with herd size.
  - Why unresolved: The paper only presents results for a specific herd configuration and does not analyze how adding or removing models affects overall performance.
  - What evidence would resolve it: Experiments varying the number of models in the herd while measuring accuracy and F1 scores on MMLU would show the scaling relationship.

- **Open Question 2**: What is the computational cost of using a router to select the optimal model for each query compared to using a single large model?
  - Basis in paper: [explicit] The paper mentions that "evaluating Mj(z) for all j, is expensive" and introduces a router to avoid this, but does not quantify the router's computational overhead.
  - Why unresolved: While the paper argues for using multiple smaller models instead of one large model, it doesn't compare the total computational cost including the router.
  - What evidence would resolve it: Detailed measurements of inference time and energy consumption for both the router-based approach and a single large model on the same hardware would provide a direct comparison.

- **Open Question 3**: How does Herd perform on other benchmarks beyond MMLU?
  - Basis in paper: [explicit] The paper states that "Herd of open source models is able to match the accuracy of ChatGPT, despite being composed of models that are effectively 2.5x smaller" but only demonstrates this on MMLU.
  - Why unresolved: MMLU is a comprehensive benchmark, but performance on other tasks and domains may vary significantly.
  - What evidence would resolve it: Testing Herd on other popular benchmarks like HumanEval, Big-Bench, or domain-specific datasets would show the generalizability of the approach.

- **Open Question 4**: How does the performance of quantized models in the herd compare to their non-quantized counterparts?
  - Basis in paper: [explicit] The paper mentions using "a mix of quantized and non-quantized models" but does not analyze the performance difference between them.
  - Why unresolved: The paper notes that quantized models can run on edge/cloud compute, but doesn't quantify the performance trade-off.
  - What evidence would resolve it: Measuring and comparing the accuracy and F1 scores of quantized vs non-quantized models in the herd on the same queries would quantify the performance impact of quantization.

## Limitations

- The evaluation focuses primarily on a single benchmark (MMLU), which may not generalize to other domains or real-world deployment scenarios.
- The system's performance advantage of 74% matching or exceeding ChatGPT depends heavily on the specific composition of the herd and may vary with different model selections or query distributions.
- The computational cost savings are theoretically sound but not empirically validated against actual deployment scenarios.

## Confidence

- High confidence in the core finding that diverse model ensembles can match proprietary model performance
- Medium confidence in the specific performance metrics and routing efficiency claims
- Medium confidence in the cost-effectiveness argument, pending real-world validation

## Next Checks

1. Evaluate the Herd system across multiple benchmarks beyond MMLU to assess generalizability
2. Conduct A/B testing comparing the Herd's real-world deployment costs versus a single large model endpoint
3. Test the system's robustness to query distribution shifts and adversarial examples