---
ver: rpa2
title: Experimental Results regarding multiple Machine Learning via Quaternions
arxiv_id: '2308.01946'
source_url: https://arxiv.org/abs/2308.01946
tags:
- quaternions
- quaternion
- learning
- machine
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an experimental study on the application of
  quaternions in several machine learning algorithms. Quaternion is a mathematical
  representation of rotation in three-dimensional space, which can be used to represent
  complex data transformations.
---

# Experimental Results regarding multiple Machine Learning via Quaternions

## Quick Facts
- arXiv ID: 2308.01946
- Source URL: https://arxiv.org/abs/2308.01946
- Reference count: 17
- Primary result: Quaternion-based models show higher accuracy and improved performance in prediction tasks compared to traditional matrix representations

## Executive Summary
This experimental study explores the application of quaternions in machine learning algorithms for handling spatial and rotational data. The research demonstrates that quaternion representations, which encode three-dimensional rotations more compactly than matrices, lead to improved classification performance across multiple algorithms. The study provides empirical evidence that quaternion-based approaches can enhance model accuracy, precision, recall, and F1-scores in rotation-based classification tasks, establishing a foundation for further exploration of quaternion applications in machine learning.

## Method Summary
The study generates 1000 random quaternion data points with binary labels, converts quaternions to rotation matrices, and splits data into 80/20 train/test sets. Five machine learning algorithms (SVM, Logistic Regression, FLD, Naive Bayes, and KNN) are trained and evaluated using eight performance metrics including accuracy, precision, recall, F1-score, MSE, MAE, HMAE, and HMSE. The experimental design allows direct comparison between quaternion-based and traditional matrix-based approaches across the same algorithms and datasets.

## Key Results
- Quaternion-based models achieved higher accuracy rates compared to matrix-based approaches
- Improved precision, recall, and F1-scores were observed across multiple ML algorithms
- Performance enhancements were consistent across SVM, Logistic Regression, FLD, Naive Bayes, and KNN models
- Quaternion representations demonstrated better capacity to capture spatial interactions in geometric transformations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quaternion representations improve spatial data handling in ML tasks compared to matrix representations
- Mechanism: Quaternions avoid gimbal lock and provide more compact rotation representation (4 parameters vs 9 for 3x3 matrix), leading to better feature learning and improved accuracy in rotation-based classification tasks
- Core assumption: The dataset contains spatial/rotational information that benefits from non-commutative representation
- Evidence anchors:
  - [abstract] "Quaternion is a mathematical representation of rotation in three-dimensional space, which can be used to represent complex data transformations."
  - [section 4.3] "The different model indicators have essentially all somewhat improved from the usage of quaternions alone."

### Mechanism 2
- Claim: Quaternion operations enable more efficient computation of rotation-related features
- Mechanism: The quaternion multiplication and conversion to rotation matrices preserves rotational relationships while reducing computational complexity compared to matrix operations
- Core assumption: The ML algorithms can effectively learn from quaternion-encoded features without significant preprocessing overhead
- Evidence anchors:
  - [section 3.1] "For the imaginary part, we can apply all normal vector arithmetic operations, such as addition, scaling, dot product, cross product, etc."
  - [section 4.3] "They have high precision rate and F1 score, and they can better detect true positive samples."

### Mechanism 3
- Claim: Quaternion-based models better capture spatial interactions in geometric transformations
- Mechanism: Quaternions naturally represent 3D rotations without singularities, allowing models to learn rotation-invariant features more effectively
- Core assumption: The task involves geometric transformations where rotational information is crucial
- Evidence anchors:
  - [abstract] "Quaternion is a mathematical representation of rotation in three-dimensional space, which can be used to represent complex data transformations."
  - [section 5] "The better capacity of quaternion-based models to capture and describe spatial interactions may be a contributing factor to their higher performance."

## Foundational Learning

- Concept: Quaternion algebra and its properties (non-commutative multiplication, conversion to rotation matrices)
  - Why needed here: Essential for understanding how quaternion features differ from traditional matrix representations and why they might improve ML performance
  - Quick check question: How does quaternion multiplication differ from matrix multiplication, and what implications does this have for feature representation?

- Concept: Rotation matrix conversion from quaternions
  - Why needed here: Critical for using quaternion data as input features in ML algorithms that expect matrix-like structures
  - Quick check question: What is the mathematical relationship between a quaternion q=<w,x,y,z> and its corresponding rotation matrix?

- Concept: Loss functions for regression tasks (MAE, MSE, HMAE, HMSE)
  - Why needed here: Used to evaluate model performance and compare quaternion vs matrix approaches
  - Quick check question: How do the heteroscedastic loss functions (HMAE, HMSE) differ from standard MAE and MSE, and when would you use each?

## Architecture Onboarding

- Component map: Data generation -> Quaternion-to-rotation matrix conversion -> Train/test split -> Model training -> Evaluation -> Learning curve analysis
- Critical path: Data generation → Quaternion-to-rotation matrix conversion → Train/test split → Model training → Evaluation → Learning curve analysis
- Design tradeoffs:
  - Quaternion representation provides better spatial handling but requires conversion overhead
  - Random data generation ensures controlled experiments but may not reflect real-world distributions
  - Multiple ML algorithms tested for generalizability but increase experimental complexity
- Failure signatures:
  - No performance improvement over matrix baseline
  - Learning curves showing overfitting or underfitting
  - Inconsistent results across different ML algorithms
  - High computational overhead without accuracy gains
- First 3 experiments:
  1. Compare quaternion SVM vs matrix SVM on the same random dataset, measuring all eight metrics
  2. Test quaternion Naive Bayes vs matrix Naive Bayes with different K values in KNN distance metric
  3. Analyze learning curves for quaternion vs matrix FLD to check for overfitting patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do quaternion-based machine learning models perform on datasets with different spatial properties and characteristics?
- Basis in paper: [explicit] The paper mentions that the performance enhancements provided by quaternion-based models may not be equal for all datasets and workloads, and that the properties of the data and the needs of the current task have a significant impact on how well quaternions perform in machine learning
- Why unresolved: The paper only presents experimental results using randomly generated quaternion data and corresponding labels, which may not capture the full range of possible spatial properties and characteristics found in real-world datasets
- What evidence would resolve it: Conducting experiments with a diverse set of real-world datasets, including those with varying spatial properties and characteristics, to compare the performance of quaternion-based and matrix-based models

### Open Question 2
- Question: How interpretable and explainable are quaternion-based machine learning models, and how can their interpretability and explainability be improved?
- Basis in paper: [inferred] The paper suggests that analyzing the interpretability and explainability of quaternion-based models could offer insightful knowledge into how they function and contribute to the development of confidence in their predictions
- Why unresolved: The paper does not provide any specific insights or methods for improving the interpretability and explainability of quaternion-based models
- What evidence would resolve it: Developing and evaluating methods for interpreting and explaining the decisions made by quaternion-based models, such as visualizing the learned representations or using techniques like LIME or SHAP

### Open Question 3
- Question: How can the training methods and architectures of quaternion-based machine learning models be optimized to further improve their performance?
- Basis in paper: [inferred] The paper mentions that efforts may be made to optimize the training methods and architectures created for quaternion-based models to further enhance their performance
- Why unresolved: The paper does not provide any specific suggestions or results related to optimizing the training methods and architectures of quaternion-based models
- What evidence would resolve it: Experimenting with different training techniques, such as regularization, data augmentation, or transfer learning, and exploring various architectural designs, such as incorporating attention mechanisms or using different activation functions, to identify the most effective approaches for quaternion-based models

## Limitations
- Experiments rely on randomly generated quaternion data rather than real-world datasets
- Only binary classification tasks were tested with 1000 samples
- Limited exploration of different quaternion representations and conversion methods
- No comparison with other compact rotation representations like Euler angles

## Confidence

- **High Confidence**: The mathematical foundations of quaternion operations and their conversion to rotation matrices are well-established
- **Medium Confidence**: The observed performance improvements across multiple ML algorithms are reproducible with the described methodology
- **Low Confidence**: Generalizability to real-world datasets and other types of spatial data transformations beyond rotations

## Next Checks

1. Test quaternion-based models on established spatial datasets (e.g., molecular structure data, robotics sensor data) to validate real-world applicability
2. Conduct ablation studies comparing quaternion representations against other compact rotation representations like Euler angles and axis-angle
3. Scale experiments to larger datasets (10K+ samples) and multi-class classification tasks to evaluate performance at scale