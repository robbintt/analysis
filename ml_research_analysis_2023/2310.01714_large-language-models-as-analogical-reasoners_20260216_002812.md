---
ver: rpa2
title: Large Language Models as Analogical Reasoners
arxiv_id: '2310.01714'
source_url: https://arxiv.org/abs/2310.01714
tags:
- problem
- exemplars
- prefix
- problems
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces analogical prompting, a new approach for prompting
  large language models to reason by self-generating relevant exemplars and knowledge
  in context before solving a problem. This method eliminates the need for labeled
  exemplars and tailors the exemplars to each problem.
---

# Large Language Models as Analogical Reasoners

## Quick Facts
- arXiv ID: 2310.01714
- Source URL: https://arxiv.org/abs/2310.01714
- Reference count: 40
- Key outcome: Analogical prompting achieves +4% average accuracy gain over 0-shot CoT and manual few-shot CoT across math, code generation, and logical reasoning tasks

## Executive Summary
This paper introduces analogical prompting, a novel approach for prompting large language models to reason by self-generating relevant exemplars and knowledge in context before solving a problem. The method eliminates the need for labeled exemplars while tailoring guidance to each specific problem. Experimental results demonstrate consistent performance improvements across various reasoning tasks, with the approach being particularly effective for larger-scale language models.

## Method Summary
Analogical prompting prompts language models to self-generate relevant exemplars and knowledge in context before solving a problem. The approach leverages the model's pretraining knowledge to create problem-specific guidance dynamically, rather than relying on fixed exemplars. The method involves generating high-level knowledge about core problem concepts followed by self-generated exemplars that align with the problem's fundamental solving approaches, creating a tailored reasoning framework for each task.

## Key Results
- Outperforms 0-shot CoT by +4% average accuracy across multiple reasoning tasks
- Shows particular effectiveness with larger-scale models (text-davinci-002/003)
- Demonstrates adaptability across diverse domains including math problem solving, code generation, and logical/temporal reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-generated exemplars enable in-context learning by leveraging the LLM's prior knowledge acquired during pretraining
- Core assumption: The LLM has been exposed to diverse problem-solving patterns during pretraining that can be retrieved through targeted prompting
- Evidence: [abstract] "Our approach prompts language models to self-generate relevant exemplars or knowledge in the context, before proceeding to solve the given problem"
- Break condition: If the model lacks relevant pretraining data or cannot generalize from generated exemplars to the target problem structure

### Mechanism 2
- Claim: Generating high-level knowledge before exemplars improves problem abstraction and guides exemplar generation
- Core assumption: The model can effectively identify and articulate core concepts that govern problem structure
- Evidence: [section 4.2] "By generating knowledge first, LLMs identify the core concepts of the problem"
- Break condition: If the model fails to generate coherent or relevant high-level knowledge, or if the knowledge does not meaningfully influence exemplar quality

### Mechanism 3
- Claim: Self-generation eliminates the need for labeled exemplars while providing problem-specific guidance
- Core assumption: Dynamic exemplar generation provides better guidance than static exemplars for diverse problem types
- Evidence: [abstract] "it obviates the need for labeling or retrieving exemplars, offering generality and convenience"
- Break condition: If the model cannot generate useful exemplars or if the computational cost outweighs the performance gains

## Foundational Learning

- Concept: Chain-of-thought reasoning
  - Why needed here: Understanding CoT is essential to grasp why analogical prompting improves upon it by providing relevant exemplars
  - Quick check question: What is the key limitation of 0-shot CoT that analogical prompting addresses?

- Concept: In-context learning
  - Why needed here: The mechanism relies on the model's ability to learn from generated exemplars in the context window
  - Quick check question: How does in-context learning differ from fine-tuning in terms of knowledge application?

- Concept: Prompt engineering principles
  - Why needed here: Effective analogical prompting requires careful prompt design to elicit relevant exemplar generation
  - Quick check question: What role do separator tokens (like "#") play in structuring model responses?

## Architecture Onboarding

- Component map: Problem statement -> Prompt generator -> LLM generation -> Solution extraction -> Evaluation
- Critical path: Problem statement → Prompt construction → LLM generation → Solution extraction → Evaluation
- Design tradeoffs:
  - Single-pass vs. multi-pass generation: Single-pass is more convenient but may have lower quality
  - Number of exemplars (K): More exemplars provide better coverage but increase computational cost
  - Knowledge generation: Improves abstraction but adds complexity to prompt
- Failure signatures:
  - Generated exemplars are irrelevant or incorrect
  - Model overfits to specific exemplars without generalizing
  - Knowledge generation produces vague or unhelpful concepts
  - Prompt structure confuses the model's response format
- First 3 experiments:
  1. Compare performance on GSM8K with K=1, K=3, and K=5 exemplars to find optimal number
  2. Test knowledge-before-exemplars vs. knowledge-after-exemplars on Codeforces to validate ordering
  3. Compare against retrieval-based exemplar methods on a small dataset to assess self-generation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of self-generated exemplars change as the complexity of the target problem increases?
- Basis in paper: Inferred from the error analysis section, which notes that LLMs often struggle to solve new problems when there's a generalization gap between the exemplars and the target problem
- Why unresolved: The paper doesn't systematically vary problem complexity to measure exemplar quality
- What evidence would resolve it: Experiments testing self-generated exemplars on problems of increasing complexity, measuring both exemplar relevance and the LLM's ability to solve the target problem

### Open Question 2
- Question: What is the optimal balance between self-generated exemplars and self-generated knowledge for different types of reasoning tasks?
- Basis in paper: The paper notes that self-generated knowledge provides additional performance gains for complex tasks like code generation, but less so for simpler tasks like GSM8K
- Why unresolved: The paper doesn't explore the optimal ratio or conditions for using knowledge vs. exemplars across different task types
- What evidence would resolve it: Systematic experiments varying the proportion of knowledge vs. exemplars across a diverse set of reasoning tasks, measuring performance to find optimal balances

### Open Question 3
- Question: How does the size and diversity of the LLM's training data affect the quality of self-generated exemplars?
- Basis in paper: The paper shows that analogical prompting performs better with larger-scale LLMs, suggesting that training data size/diversity impacts exemplar generation quality
- Why unresolved: The paper doesn't directly analyze the relationship between training data characteristics and exemplar generation quality
- What evidence would resolve it: Experiments training LLMs of varying sizes on datasets of different sizes and diversities, then testing their ability to generate relevant exemplars for a range of problems

## Limitations

- Generalization gap uncertainty: The method's effectiveness for truly novel problem types outside pretraining data remains unclear
- Computational overhead: Self-generation of multiple exemplars adds processing time and API costs
- Evaluation scope: Focuses on accuracy metrics without analyzing solution quality or robustness

## Confidence

**High Confidence**: The claim that analogical prompting outperforms 0-shot CoT by +4% average accuracy is well-supported by the experimental results across multiple datasets and model scales.

**Medium Confidence**: The mechanism that self-generated exemplars improve problem-solving by leveraging pretraining knowledge is plausible but relies on the untested assumption that generated exemplars are truly relevant and useful for each specific problem.

**Medium Confidence**: The assertion that knowledge generation before exemplars improves abstraction and exemplar quality is supported by qualitative observations but lacks quantitative validation comparing different ordering strategies.

**Low Confidence**: The claim that analogical prompting eliminates the need for labeled exemplars is overstated, as the method still requires careful prompt engineering and may fail when the model cannot generate relevant exemplars.

## Next Checks

1. Cross-Domain Transfer Evaluation: Test analogical prompting on a dataset from a domain with minimal overlap to pretraining data (e.g., specialized scientific domains or novel puzzle types) to assess whether the method can generate relevant exemplars for truly novel problem types.

2. Exemplar Quality Analysis: Implement an automated system to evaluate the relevance and correctness of generated exemplars, measuring the correlation between exemplar quality and final problem-solving performance. This would validate the core assumption that exemplar generation quality drives performance improvements.

3. Cost-Benefit Analysis: Measure the wall-clock time and computational resources required for analogical prompting versus baseline methods across different problem complexities, establishing whether the performance gains justify the additional overhead for practical deployment scenarios.