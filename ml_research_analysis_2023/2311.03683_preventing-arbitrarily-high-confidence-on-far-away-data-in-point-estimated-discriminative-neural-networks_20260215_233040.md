---
ver: rpa2
title: Preventing Arbitrarily High Confidence on Far-Away Data in Point-Estimated
  Discriminative Neural Networks
arxiv_id: '2311.03683'
source_url: https://arxiv.org/abs/2311.03683
tags:
- data
- neural
- training
- class
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of arbitrarily high confidence
  predictions in neural networks for out-of-distribution (OOD) data. The authors propose
  a method called PreLoad that adds an extra OOD class to the output layer of the
  network.
---

# Preventing Arbitrarily High Confidence on Far-Away Data in Point-Estimated Discriminative Neural Networks

## Quick Facts
- arXiv ID: 2311.03683
- Source URL: https://arxiv.org/abs/2311.03683
- Reference count: 9
- One-line primary result: Proposes PreLoad method that adds OOD class to output layer with positive-constrained weights to prevent high confidence on far-away data while maintaining discriminative training simplicity

## Executive Summary
This paper addresses the problem of neural networks producing arbitrarily high confidence predictions on out-of-distribution (OOD) data, particularly for data that is far from the training distribution. The authors propose PreLoad, a simple method that adds an extra OOD class to the output layer with constrained positive weights. This approach ensures that as inputs move away from training data, the OOD class logit grows faster than other class logits, dominating the softmax and preventing high confidence predictions on far-away OOD data while maintaining standard discriminative training.

## Method Summary
PreLoad adds an extra output neuron representing an OOD class to standard classification networks. The key innovation is constraining the weights of this OOD class to be positive, ensuring its logit grows quadratically with input magnitude (via Gψ(x)²) while other logits grow linearly. The method is trained using standard cross-entropy loss with an auxiliary OOD dataset, requiring no generative modeling or Bayesian methods. During inference, the probability of the OOD class is used for detection. The method can be applied through fine-tuning a pre-trained model without retraining from scratch.

## Key Results
- Achieves perfect FPR-95 on far-away OOD data across multiple benchmarks
- Maintains competitive performance on realistic OOD detection compared to state-of-the-art baselines
- Preserves standard discriminative training simplicity while providing theoretical guarantees
- Works across multiple datasets (MNIST, Fashion MNIST, SVHN, CIFAR-10, CIFAR-100)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The OOD class logit grows faster than in-distribution logits as input magnitude increases, dominating the softmax and preventing high confidence on far-away data.
- Mechanism: By constraining OOD class weights to be positive and using Gψ(x)² in the logit, the OOD logit increases quadratically with input magnitude while in-distribution logits grow linearly.
- Core assumption: The neural network embedding Gψ(x) grows unbounded as input magnitude increases, and the ReLU network is piecewise affine.
- Evidence anchors:
  - [abstract]: "We overcome this problem by adding a term to the output of the neural network that corresponds to the logit of an extra class, that we design to dominate the logits of the original classes as we move away from the training data."
  - [section]: "by using Gψ(x)² in the logit of the (k+1)-st class we make sure that it grows faster than other logits and therefore, eventually dominates."
- Break condition: If the embedding Gψ(x) does not grow with input magnitude, or if the weights for the OOD class are not constrained to be positive.

### Mechanism 2
- Claim: The method provably prevents arbitrarily high confidence on far-away data while maintaining discriminative training simplicity.
- Mechanism: The additional OOD class is trained using standard cross-entropy loss on auxiliary OOD data, requiring no generative modeling or Bayesian methods.
- Core assumption: The network embedding can be represented as a piecewise affine function, and the OOD dataset is sufficiently representative of far-away data.
- Evidence anchors:
  - [abstract]: "This technique provably prevents arbitrarily high confidence on far-away test data while maintaining a simple discriminative point-estimate training."
  - [section]: "Theorem 4. Let Gψ be any neural network embedding... Then limt→∞ P (y = c|tx∗) < 1 for all c ≠ k + 1."
- Break condition: If the OOD dataset is not representative or if the network embedding does not grow with input magnitude.

### Mechanism 3
- Claim: The method performs well on realistic OOD detection benchmarks while preventing far-away overconfidence.
- Mechanism: The OOD class is trained on a large auxiliary dataset (300,000 tiny images), allowing it to learn features that distinguish realistic OOD data from in-distribution data.
- Core assumption: The auxiliary OOD dataset captures realistic OOD distribution, and the neural network architecture can effectively learn to distinguish OOD features.
- Evidence anchors:
  - [section]: "We rely on an auxiliary OOD dataset like previous methods... Evaluation on various benchmarks demonstrates strong performance against competitive baselines on both far-away and realistic OOD data."
- Break condition: If the auxiliary OOD dataset is too small or unrepresentative, or if fine-tuning does not properly initialize the OOD class weights.

## Foundational Learning

- Concept: Softmax function and probability calibration
  - Why needed here: Understanding how logits map to probabilities and why unbounded logits cause overconfidence is crucial for grasping the core problem.
  - Quick check question: If one logit is much larger than others, what happens to the softmax probability of that class?

- Concept: ReLU networks and piecewise affine functions
  - Why needed here: The theoretical proof relies on ReLU networks being piecewise affine, which determines how the network behaves for large inputs.
  - Quick check question: What happens to ReLU activations when inputs become very large?

- Concept: Cross-entropy loss and discriminative training
  - Why needed here: The method maintains standard discriminative training, so understanding how the loss function works with the additional class is important.
  - Quick check question: How does adding an additional class affect the cross-entropy loss computation?

## Architecture Onboarding

- Component map:
  Standard neural network -> Classification head with k classes -> Additional OOD class with positive-constrained weights -> Cross-entropy loss (in-distribution + OOD data)

- Critical path:
  1. Train standard network on in-distribution data
  2. Initialize OOD class weights to small positive values
  3. Fine-tune with combined loss (standard + OOD)
  4. During inference, use OOD probability for detection

- Design tradeoffs:
  - Positive weight constraint vs. unrestricted weights: ensures faster growth but may limit expressiveness
  - Auxiliary OOD dataset size: larger datasets improve realistic OOD detection but increase training cost
  - Fine-tuning vs. training from scratch: fine-tuning is faster but may not fully optimize OOD detection

- Failure signatures:
  - High OOD confidence on in-distribution data (false positives)
  - Low OOD confidence on realistic OOD data (false negatives)
  - Poor calibration under dataset shift
  - Far-away data still showing high confidence in in-distribution classes

- First 3 experiments:
  1. Train on MNIST and evaluate on far-away data to verify zero confidence in original classes
  2. Train on CIFAR-10 and evaluate on CIFAR-100 to test realistic OOD detection
  3. Fine-tune PreLoad on a pre-trained model and compare calibration under rotation to baseline methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PreLoad perform on other types of data shifts beyond rotation and corruption, such as adversarial examples or domain adaptation?
- Basis in paper: [inferred] The paper evaluates PreLoad on rotated MNIST and corrupted CIFAR10 for calibration, but does not explore other types of data shifts.
- Why unresolved: The experiments only cover specific types of data shifts. Performance on other shifts like adversarial examples or domain adaptation is unknown.
- What evidence would resolve it: Experiments testing PreLoad on a wider range of data shifts, including adversarial examples and domain adaptation, would show its robustness to different types of shifts.

### Open Question 2
- Question: Can PreLoad be extended to handle OOD detection in language models or other non-image domains?
- Basis in paper: [inferred] The paper focuses on OOD detection for image classification tasks. There is no discussion of applying PreLoad to other domains like language.
- Why unresolved: The methodology is demonstrated only for image data. Its applicability and effectiveness for other types of data is unknown.
- What evidence would resolve it: Applying PreLoad to OOD detection tasks in non-image domains, such as language models, and evaluating its performance would show its generalizability.

### Open Question 3
- Question: How does the performance of PreLoad scale with the number of classes in the classification problem?
- Basis in paper: [inferred] The experiments are conducted on datasets with a limited number of classes (e.g., MNIST with 10 classes). Scaling to datasets with hundreds or thousands of classes is not explored.
- Why unresolved: The experiments only cover relatively small-scale classification problems. Performance on large-scale problems is unknown.
- What evidence would resolve it: Evaluating PreLoad on datasets with a wide range of class numbers, especially very large-scale problems, would show how its performance scales with the number of classes.

## Limitations

- Theoretical proof relies on specific assumptions about ReLU networks that may not hold for all architectures
- Method depends on availability of representative auxiliary OOD training data
- Positive weight constraint may limit expressiveness and generalizability to different network architectures

## Confidence

- High confidence: Empirical results showing perfect FPR-95 on far-away OOD data are convincing and reproducible
- Medium confidence: Theoretical proof is sound within stated assumptions, but practical applicability depends on those assumptions
- Low confidence: Generalizability to different network architectures and sensitivity to OOD training dataset choice are not well-established

## Next Checks

1. Conduct ablation study on OOD dataset size by systematically varying the size and composition (50k, 100k, 300k images) and measuring impact on both far-away and realistic OOD detection performance

2. Test architectural robustness by applying PreLoad to networks with different activation functions (LeakyReLU, ELU) and architectures (Vision Transformers) to verify theoretical assumptions

3. Analyze fine-tuning stability by varying OOD class weight initialization and fine-tuning schedule to determine method sensitivity and identify failure modes