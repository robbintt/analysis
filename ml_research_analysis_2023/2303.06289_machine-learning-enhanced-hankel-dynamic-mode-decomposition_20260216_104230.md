---
ver: rpa2
title: Machine Learning Enhanced Hankel Dynamic-Mode Decomposition
arxiv_id: '2303.06289'
source_url: https://arxiv.org/abs/2303.06289
tags:
- which
- time
- dynamics
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of generating accurate dynamical
  models from time series, especially for chaotic systems. The core idea is to combine
  Takens' Embedding Theorem with a deep learning approach to improve upon standard
  Dynamic Mode Decomposition (DMD).
---

# Machine Learning Enhanced Hankel Dynamic-Mode Decomposition

## Quick Facts
- arXiv ID: 2303.06289
- Source URL: https://arxiv.org/abs/2303.06289
- Reference count: 31
- Primary result: DLHDMD achieves ~1% relative accuracy on forecasts up to characteristic time scales for chaotic systems like Lorenz-63 and Rossler.

## Executive Summary
This paper introduces Deep Learning Hankel DMD (DLHDMD), a novel method that combines Takens' Embedding Theorem with deep learning to improve Dynamic Mode Decomposition for chaotic systems. The approach uses an auto-encoder to embed dynamics in a higher-dimensional space and adaptively orders embedded coordinates based on mutual information. DLHDMD successfully generates accurate reconstructions and forecasts for several chaotic systems, achieving approximately 1% relative accuracy on forecasts up to the characteristic time scales of strange attractors. The method demonstrates significant changes in mutual information between dimensions in latent variables, which appears to be a key feature in enhancing DMD performance.

## Method Summary
DLHDMD uses an auto-encoder (encoder E, decoder D) to map original dynamics to a latent space where Hankel DMD is applied. The method employs a combined loss function balancing reconstruction loss, prediction loss, DMD loss, and regularization. Training uses ADAM optimizer with batch size 256 and regularization weight 1e-14. The window size Nob is adaptively updated during training to optimize prediction performance. The approach is tested on chaotic systems including Lorenz-63, Rossler, and 12-dimensional projections of the Kuramoto-Sivashinsky equation.

## Key Results
- DLHDMD achieves ~1% relative accuracy on forecasts up to characteristic time scales for chaotic systems
- The method successfully handles chaotic time series from Lorenz-63, Rossler, and Kuramoto-Sivashinsky systems
- Encoder significantly changes mutual information between dimensions in latent variables, enhancing DMD performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The auto-encoder transforms the original time series into a higher-dimensional latent space where the dynamics are more linear and stable for DMD.
- Mechanism: The encoder maps the original variables into a new coordinate system (latent variables) where the Hankel DMD can approximate the Koopman operator more accurately. The decoder then maps back to the original space, preserving reconstruction and forecasting capabilities.
- Core assumption: The encoder can learn a coordinate transformation that makes the dynamics more amenable to linear Koopman operator approximation.
- Evidence anchors:
  - [abstract] "We call this method the Deep Learning Hankel DMD (DLHDMD). We show that the DLHDMD is able to generate accurate dynamics for chaotic time series"
  - [section] "This is done by first making the EDMD over embedded coordinates global as opposed to the local approach of [6]"
  - [corpus] Weak - no direct mentions of auto-encoder in related papers, suggesting this is a novel contribution
- Break condition: If the encoder fails to learn a transformation that linearizes the dynamics sufficiently, the DMD approximation will remain inaccurate for chaotic systems.

### Mechanism 2
- Claim: The adaptive Hankel matrix ordering based on mutual information improves the quality of the embedded coordinates.
- Mechanism: By using mutual information to order the embedded coordinates, the method prioritizes dimensions that capture the most relevant dynamical information, reducing redundancy and improving DMD accuracy.
- Core assumption: Mutual information between dimensions can guide the construction of more informative observables for the Koopman operator approximation.
- Evidence anchors:
  - [abstract] "we explore how our method learns mappings which tend, after successful training, to significantly change the mutual information between dimensions in the dynamics"
  - [section] "we study how the fully trained encoder changes the information content of the dynamics"
  - [corpus] Weak - no mentions of mutual information in related papers, indicating this is a unique analytical approach
- Break condition: If the mutual information analysis doesn't correlate with improved DMD performance, the adaptive ordering may not provide meaningful benefits.

### Mechanism 3
- Claim: The loss function design balances reconstruction accuracy, prediction accuracy, and DMD approximation quality.
- Mechanism: The total loss combines reconstruction loss (Lrecon), prediction loss (Lpred), DMD loss (Ldmd), and regularization (Lreg) to train the auto-encoder to preserve essential dynamics while enabling accurate DMD.
- Core assumption: The weighted combination of these losses will guide the auto-encoder to learn transformations that satisfy all three objectives simultaneously.
- Evidence anchors:
  - [section] "we use the following loss function Ltot = Lrecon + Lpred + Ldmd + αLreg"
  - [section] "We collect the details of our learning method in Algorithm 1, which we call the Deep Learning Hankel DMD (DLHDMD)"
  - [corpus] Weak - no direct discussion of this specific loss formulation in related papers
- Break condition: If the loss function weights are poorly chosen, the auto-encoder may prioritize one objective at the expense of others, degrading overall performance.

## Foundational Learning

- Concept: Dynamic Mode Decomposition (DMD)
  - Why needed here: DMD is the foundational algorithm being enhanced by machine learning techniques to handle chaotic systems
  - Quick check question: What is the relationship between DMD and the Koopman operator?

- Concept: Takens' Embedding Theorem
  - Why needed here: Provides the theoretical justification for using embedded coordinates to reconstruct higher-dimensional dynamics from time series
  - Quick check question: How does Takens' theorem justify the use of Hankel matrices for dynamical systems analysis?

- Concept: Mutual Information
  - Why needed here: Used to analyze how the encoder changes the statistical dependencies between dimensions in the original and latent variables
  - Quick check question: What does it mean when mutual information between two dimensions approaches zero?

## Architecture Onboarding

- Component map: Auto-encoder (E, D) → Hankel DMD → Loss computation → Parameter updates
- Critical path: Data → Encoder → Hankel DMD → Decoder → Loss → Backpropagation → Updated encoder/decoder
- Design tradeoffs: Number of observables (Nob) vs. prediction horizon (Nst) - increasing Nob improves reconstruction but limits forecasting capability
- Failure signatures: Ldmd collapse when Nob = Nst indicates overfitting; poor reconstruction accuracy suggests encoder isn't learning useful transformations
- First 3 experiments:
  1. Test HDMD alone on Lorenz-63 system to establish baseline performance
  2. Implement DLHDMD with fixed Nob and monitor loss convergence
  3. Vary Nob during training and observe impact on reconstruction vs. forecasting accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Deep Learning HDMD method perform compared to other equation-free modeling techniques like SINDy, RC, and HAVOK for chaotic time series forecasting?
- Basis in paper: [explicit] The authors state: "There is a clear need for a comparison across SINDy, RC, and our DLHDMD methods."
- Why unresolved: The paper acknowledges this comparison is needed but defers it to future work, leaving the relative performance of DLHDMD across different methods unknown.
- What evidence would resolve it: Systematic experiments comparing DLHDMD against SINDy, RC, and HAVOK on the same chaotic systems (Lorenz-63, Rossler, KS) with metrics for reconstruction accuracy, forecast horizon, and computational efficiency.

### Open Question 2
- Question: What is the theoretical basis for the adaptive Hankel matrix ordering in DLHDMD and how does it improve Koopman operator approximation?
- Basis in paper: [explicit] The authors state: "We develop an adaptive Hankel matrix based ordering of the embedded coordinates which adds more expressive power for approximating dynamics to the deep learning framework."
- Why unresolved: While the authors describe the adaptive Hankel matrix ordering, they do not provide a rigorous mathematical analysis of why this particular ordering improves Koopman operator approximation.
- What evidence would resolve it: Mathematical proof showing how the adaptive Hankel matrix ordering reduces approximation error in the Koopman operator compared to fixed Hankel matrices.

### Open Question 3
- Question: How does the encoder network in DLHDMD specifically modify the mutual information structure of the dynamics, and can this be predicted or controlled?
- Basis in paper: [explicit] The authors analyze how the encoder changes mutual information between dimensions and state: "This appears to be a key feature in enhancing the DMD overall."
- Why unresolved: While the authors observe changes in mutual information, they do not explain the underlying mechanism or provide methods to predict or control these changes during training.
- What evidence would resolve it: Analysis connecting specific encoder architecture choices (layer sizes, activation functions) to predictable changes in mutual information structure across dimensions.

## Limitations

- The adaptive Nob update procedure lacks detailed specification, making it difficult to reproduce the exact parameter adaptation strategy
- The choice of lag parameter Nlg for the DMD loss is not fully justified, and its impact on training stability and forecast horizon remains unclear
- The method's performance on more complex, higher-dimensional, or noisy real-world datasets has not been demonstrated

## Confidence

- **High Confidence**: The core mechanism of using an auto-encoder to transform dynamics into a more linear space for DMD approximation is well-supported by the experimental results on Lorenz-63, Rossler, and Kuramoto-Sivashinsky systems.
- **Medium Confidence**: The claim that mutual information analysis significantly enhances DMD performance is supported by the observed changes in latent variable correlations, but the direct causal link between MI reduction and improved forecasting needs further investigation across diverse systems.
- **Low Confidence**: The generalization of the method to highly nonlinear or noisy real-world systems is not yet established.

## Next Checks

1. **Noise Robustness Test**: Apply DLHDMD to noisy versions of the Lorenz-63 and Rossler systems (additive Gaussian noise at 1-10% of signal amplitude) and measure degradation in reconstruction and forecast accuracy compared to clean data.

2. **Real-World Dataset Application**: Test the method on a real-world chaotic or complex system (e.g., weather data, fluid flow measurements) to assess generalization beyond synthetic benchmarks.

3. **Hyperparameter Sensitivity Analysis**: Systematically vary the auto-encoder architecture (number of layers, neurons), learning rate, and regularization weight to quantify their impact on convergence and final accuracy, identifying stable operating regions.