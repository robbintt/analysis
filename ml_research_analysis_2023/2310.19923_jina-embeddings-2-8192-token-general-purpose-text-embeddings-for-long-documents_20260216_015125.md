---
ver: rpa2
title: 'Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents'
arxiv_id: '2310.19923'
source_url: https://arxiv.org/abs/2310.19923
tags:
- text
- tasks
- training
- embedding
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Jina Embeddings 2 introduces a novel text embedding model with
  a 8192-token context window, significantly exceeding the 512-token limit of existing
  open-source models. The model leverages Attention with Linear Biases (ALiBi) and
  a modified BERT architecture to encode long documents without truncation.
---

# Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents

## Quick Facts
- arXiv ID: 2310.19923
- Source URL: https://arxiv.org/abs/2310.19923
- Reference count: 11
- Introduces an 8192-token context text embedding model achieving state-of-the-art MTEB performance

## Executive Summary
Jina Embeddings 2 introduces a novel text embedding model with an 8192-token context window, significantly exceeding the 512-token limit of existing open-source models. The model leverages Attention with Linear Biases (ALiBi) and a modified BERT architecture to encode long documents without truncation. Training involves pre-training on a masked language modeling objective, followed by fine-tuning on text pairs and hard negatives to improve retrieval and classification tasks. The model achieves state-of-the-art performance on the MTEB benchmark, outperforming other open-source models and matching the performance of OpenAI's proprietary text-embedding-ada-002.

## Method Summary
The model pre-trains a modified BERT architecture with ALiBi on the C4 dataset using a masked language modeling objective. It then undergoes two-stage fine-tuning: first on text pairs using InfoNCE loss, then with hard negatives to sharpen discrimination. The final embedding is generated using mean pooling of token-level representations. The model uses gated linear units in feedforward sub-layers and post-layer normalization in attention blocks.

## Key Results
- Achieves state-of-the-art performance on the MTEB benchmark across 8 tasks and 58 datasets
- Outperforms other open-source models and matches OpenAI's text-embedding-ada-002
- Demonstrates improved performance on long-document tasks like NarrativeQA with extended context lengths

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ALiBi allows BERT to handle long sequences without increasing computational cost.
- Mechanism: ALiBi replaces learned positional embeddings with fixed linear biases in attention scores, avoiding the quadratic blowup in memory and computation that comes with longer sequences.
- Core assumption: The linear bias terms can sufficiently capture positional information for long-range dependencies.
- Evidence anchors:
  - [abstract]: "The model leverages Attention with Linear Biases (ALiBi) and a modified BERT architecture to encode long documents without truncation."
  - [section 4.1]: "ALiBi forgoes the use of positional embeddings. Instead, it encodes positional information directly within the self-attention layer by introducing a constant bias term to the attention score matrix of each layer..."
  - [corpus]: Weak. Corpus mentions related models but not the ALiBi mechanism in detail.
- Break condition: If the fixed bias cannot represent complex positional patterns, long-range accuracy degrades.

### Mechanism 2
- Claim: Multi-stage contrastive fine-tuning improves both retrieval and classification performance.
- Mechanism: First fine-tuning aligns semantically related text pairs using InfoNCE loss; second stage adds hard negatives to sharpen discrimination between relevant and irrelevant passages.
- Core assumption: Hard negatives are correctly labeled and representative of real retrieval noise.
- Evidence anchors:
  - [abstract]: "Training involves pre-training on a masked language modeling objective, followed by fine-tuning on text pairs and hard negatives to improve retrieval and classification tasks."
  - [section 5.2]: "The model is further fine-tuned using text pairs complemented with hard negatives... This stage is crucial for enabling the model to better distinguish between relevant passages and related, but irrelevant text passages."
  - [corpus]: Weak. No direct mention of contrastive fine-tuning in neighbor papers.
- Break condition: If hard negatives are noisy or not truly negative, the model overfits to false negatives.

### Mechanism 3
- Claim: Mean pooling converts token-level representations into a single embedding without introducing extra parameters.
- Mechanism: The mean of token embeddings is used as the sentence/document embedding, preserving semantic content while keeping the model size small.
- Core assumption: Averaging captures sufficient semantic information for downstream tasks.
- Evidence anchors:
  - [abstract]: "The model achieves state-of-the-art performance on the MTEB benchmark..."
  - [section 5]: "To enable a model to perform a text operation, we augment it with a mean pooling layer... without introducing additional trainable parameters."
  - [corpus]: Weak. Neighbor papers do not discuss pooling strategies.
- Break condition: If token-level representations are not homogeneous, mean pooling may dilute important features.

## Foundational Learning

- Concept: Attention with Linear Biases (ALiBi)
  - Why needed here: Enables long-context processing without the memory cost of positional embeddings.
  - Quick check question: What is the main advantage of ALiBi over learned positional embeddings for long sequences?

- Concept: Contrastive Learning (InfoNCE loss)
  - Why needed here: Forces the model to pull similar texts together and push dissimilar ones apart, improving retrieval quality.
  - Quick check question: In InfoNCE, what role does the temperature parameter Ï„ play in the loss calculation?

- Concept: Mean Pooling for Embeddings
  - Why needed here: Aggregates token-level features into a fixed-size vector for downstream tasks without adding parameters.
  - Quick check question: Why might mean pooling be insufficient for tasks requiring emphasis on specific tokens?

## Architecture Onboarding

- Component map: ALiBi bias layers -> Gated Linear Units -> Post-layer normalization -> Mean pooling layer
- Critical path:
  1. Pre-training on C4 with MLM objective (512 tokens)
  2. First fine-tuning on text pairs (unsupervised InfoNCE)
  3. Second fine-tuning with hard negatives (supervised InfoNCE+)
- Design tradeoffs:
  - Fixed ALiBi biases vs. learned positional embeddings: faster training, less memory, but limited positional expressiveness.
  - Mean pooling vs. CLS token: simpler, fewer parameters, but may lose token-specific nuance.
  - Large batch sizes for contrastive loss: better gradients but higher memory demand.
- Failure signatures:
  - Training instability: check GLU activation choice and weight decay settings.
  - Poor retrieval performance: verify hard negative quality and InfoNCE loss scaling.
  - Long-context underperformance: ensure ALiBi slopes are correctly mirrored for bidirectional attention.
- First 3 experiments:
  1. Train a small ALiBi BERT on C4, test MLM accuracy at 512 vs 8192 tokens.
  2. Apply mean pooling to the trained model, run unsupervised contrastive fine-tuning on text pairs.
  3. Add hard negatives from a retrieval dataset, measure nDCG@10 on a validation set.

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- ALiBi Generalization: No empirical evidence that ALiBi maintains accuracy for sequences beyond 8192 tokens or for highly structured documents.
- Hard Negative Quality: Construction pipeline for hard negatives is not detailed, raising concerns about reproducibility and label noise.
- Evaluation Scope: MTEB benchmark does not include domain-specific tasks where embedding performance can diverge significantly.

## Confidence
- High Confidence:
  - Model architecture (ALiBi + modified BERT) and training pipeline are well-specified and align with published results.
  - State-of-the-art MTEB benchmark performance claims are supported by direct comparison to known open-source baselines.
- Medium Confidence:
  - Claims of matching OpenAI's proprietary embedding-ada-002 are based on public benchmark comparisons but lack independent verification.
  - The generalization benefit of 8192 tokens over shorter contexts is demonstrated in NarrativeQA but not systematically across all retrieval tasks.
- Low Confidence:
  - The precise impact of mean pooling versus CLS-based pooling on downstream classification is not directly compared within the paper.

## Next Checks
1. **Long-Document Robustness Test**: Evaluate model performance on synthetic long documents (10K+ tokens) with complex structure to verify ALiBi positional encoding remains effective beyond 8192 tokens.
2. **Hard Negative Ablation**: Reproduce retrieval performance with and without hard negatives on MSMarco to quantify their contribution and check for overfitting to synthetic negatives.
3. **Pooling Strategy Comparison**: Implement and compare mean pooling, CLS pooling, and max pooling on classification tasks to measure sensitivity to pooling choice and identify potential performance trade-offs.