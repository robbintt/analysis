---
ver: rpa2
title: Remember what you did so you know what to do next
arxiv_id: '2311.01468'
source_url: https://arxiv.org/abs/2311.01468
tags:
- gpt-j
- actions
- games
- training
- scienceworld
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates whether large language models can effectively
  plan and execute tasks in ScienceWorld, a simulated text-based environment for elementary
  science experiments. While prior work found that language models performed poorly
  in this setting, this study shows that by conditioning on sequences of prior actions
  rather than just the most recent one, a 6B parameter GPT-J model achieves 3.5x better
  performance than reinforcement learning baselines.
---

# Remember what you did so you know what to do next

## Quick Facts
- arXiv ID: 2311.01468
- Source URL: https://arxiv.org/abs/2311.01468
- Reference count: 19
- Key outcome: GPT-J 6B achieves 3.5x better performance than RL baselines by conditioning on full action histories in ScienceWorld

## Executive Summary
This paper demonstrates that large language models can effectively plan and execute tasks in a text-based science simulation environment by conditioning on sequences of prior actions rather than just the most recent step. The study shows that GPT-J 6B outperforms reinforcement learning baselines by 3.5x when using full action histories, and maintains a 2.2x advantage even with only 6.5% of training data. Performance varies widely across task types, and when combined with a pre-condition checker, GPT-J matches the performance of a two-stage architecture using a model 29 times larger.

## Method Summary
The study fine-tunes GPT-J 6B on ScienceWorld game transcripts formatted as natural language dialog, training with up to two epochs using AdamW optimizer, weight decay 0.01, batch size 16, and learning rate 1e-4 with DeepSpeed offloading. The model is evaluated on 1,819 games across 30 elementary science tasks with a 100-step limit, comparing performance against a reinforcement learning baseline (DRRN) and testing various training data sizes (7,359, 3,589, and 480 games).

## Key Results
- GPT-J achieves 62.57 average score vs 17.95 for RL baseline when conditioning on full action histories
- Performance remains strong (39.78 average score) even with only 6.5% of training data
- Adding a pre-condition checker to the single-stage GPT-J matches the performance of a two-stage architecture using a model 29x larger

## Why This Works (Mechanism)

### Mechanism 1
GPT-J outperforms DRRN by conditioning on longer action histories rather than just the most recent step. The model uses prior action-observation pairs as context, effectively turning the task into a sequence modeling problem where the LLM learns to generate the next action based on a richer dialogue history. Core assumption: The ScienceWorld environment has enough state persistence that prior actions provide meaningful context for the next decision.

### Mechanism 2
GPT-J generalizes well from small training sets due to pre-training knowledge. The LLM's pre-training on broad web text provides background scientific knowledge that compensates for limited in-domain training data. Core assumption: Elementary science concepts and procedures are sufficiently represented in GPT-J's pre-training corpus.

### Mechanism 3
External precondition checking reduces redundant actions without harming overall performance. A lightweight parser tracks open/closed states and intercepts redundant commands, preventing the LLM from wasting turns on unnecessary actions. Core assumption: Redundant actions are frequent enough to matter but do not constitute the core decision-making challenge.

## Foundational Learning

- **Sequence-to-sequence modeling in dialogue format**: Why needed here: GPT-J learns by predicting the next action given a history of (action, observation) pairs, so understanding the autoregressive generation pattern is critical. Quick check: If the input ends with "A: open door", what does the model generate next?

- **Fine-tuning vs. pre-training**: Why needed here: The study relies on fine-tuning a pre-trained model, so knowing how fine-tuning differs from full pre-training (smaller dataset, task-specific) is essential. Quick check: Does fine-tuning overwrite all pre-trained weights or just adapt them?

- **Reinforcement learning vs. sequence modeling**: Why needed here: The comparison to DRRN frames the problem as RL vs. sequence modeling; understanding the key difference (policy search vs. next-token prediction) is needed to interpret results. Quick check: In RL, how is the action chosen versus how it is chosen in sequence modeling?

## Architecture Onboarding

- **Component map**: GPT-J model → precondition checker (optional) → ScienceWorld simulator → score/metrics
- **Critical path**: Input buffer filling → model inference → action validation (by precondition checker) → simulator step → score accumulation
- **Design tradeoffs**: Longer histories improve accuracy but risk hitting the 2048 token limit; precondition checker adds latency but reduces redundant actions
- **Failure signatures**: High invalid object rates suggest hallucination; low scores on specific tasks indicate domain knowledge gaps; flat performance curves suggest overfitting
- **First 3 experiments**:
  1. Compare Markov-only vs. full history conditioning on a small subset of tasks to measure gain
  2. Add precondition checker to the Markov model to see if it reduces redundant actions without hurting score
  3. Reduce training data to ~10% and evaluate whether performance degrades gracefully or catastrophically

## Open Questions the Paper Calls Out

1. How does the performance of GPT-J change when the training data is augmented with additional external knowledge sources, such as knowledge graphs or text from other domains? The paper only experimented with using a pre-condition checker and did not explore the use of additional knowledge sources.

2. How does the performance of GPT-J vary when the input buffer size is increased beyond the current limit of 2048 word pieces? The paper discusses filling the LLM's input buffer with as much prior history as possible, but does not explore the effects of increasing the buffer size beyond its current limit.

3. How does the performance of GPT-J compare to other large language models of similar size when applied to the ScienceWorld environment? The paper focuses on GPT-J and does not compare its performance to other large language models of similar size in the ScienceWorld environment.

## Limitations
- Limited task diversity: Only 30 elementary science tasks in a single text-based environment, limiting generalization to other domains
- Sparse hyperparameter details: Critical details like learning rate schedule, gradient clipping thresholds, and token truncation strategies are omitted
- Baseline comparison gaps: The reinforcement learning baseline (DRRN) is described only briefly, making it difficult to assess architectural differences

## Confidence
- High confidence: The core finding that conditioning on longer action histories improves GPT-J performance is well-supported by controlled experiments
- Medium confidence: The claim that GPT-J's pre-training provides sufficient scientific knowledge for generalization is plausible but not directly tested
- Medium confidence: The assertion that the precondition checker becomes redundant when GPT-J learns well enough is supported, but the mechanism is not fully explained

## Next Checks
1. Test Markov-only vs. full history conditioning on a small task subset to measure the exact performance gain from longer action histories
2. Implement and evaluate the precondition checker on the Markov-only model to verify whether it reduces redundant actions by ~50% without hurting overall scores
3. Reduce training data to ~10% and assess whether performance degrades gracefully or catastrophically, testing the claimed pre-training knowledge transfer effect