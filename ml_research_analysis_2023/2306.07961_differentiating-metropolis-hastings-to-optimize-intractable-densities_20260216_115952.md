---
ver: rpa2
title: Differentiating Metropolis-Hastings to Optimize Intractable Densities
arxiv_id: '2306.07961'
source_url: https://arxiv.org/abs/2306.07961
tags:
- algorithm
- metropolis-hastings
- alternative
- gradient
- differentiable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a method to automatically differentiate Metropolis-Hastings
  (MH) samplers, enabling gradient-based optimization of intractable densities. The
  key idea is to use smoothed perturbation analysis (SPA) and stochastic automatic
  differentiation to unbiasedly estimate gradients through the discrete accept/reject
  steps in MH.
---

# Differentiating Metropolis-Hastings to Optimize Intractable Densities

## Quick Facts
- arXiv ID: 2306.07961
- Source URL: https://arxiv.org/abs/2306.07961
- Reference count: 4
- Key outcome: Method to automatically differentiate Metropolis-Hastings samplers for optimizing intractable densities using smoothed perturbation analysis and coupled chains

## Executive Summary
This paper introduces a method to automatically differentiate through Metropolis-Hastings (MH) samplers, enabling gradient-based optimization of intractable densities. The key innovation is using smoothed perturbation analysis (SPA) combined with Markov chain coupling to create unbiased gradient estimators for discrete accept/reject steps. The approach couples two MH chains in parallel, tracks alternative trajectories, and prunes them when they recouple to maintain computational efficiency. Experiments demonstrate successful optimization of posterior entropy in a Gaussian mixture model and specific heat in an Ising model.

## Method Summary
The method builds on smoothed perturbation analysis to create unbiased gradient estimators for discrete random programs like MH samplers. It couples two chains in parallel using a proposal coupling scheme where if the chains are in the same state, they remain together. Alternative trajectories are tracked and pruned when they recouple to the primal chain, maintaining O(1) computational overhead. The approach uses stochastic automatic differentiation to compute weighted differences between chain outcomes, providing gradients for optimization. The method is demonstrated on optimizing posterior entropy for a Gaussian mixture model and specific heat for an Ising model.

## Key Results
- Successfully differentiated MH sampler to optimize posterior entropy in a 3-component Gaussian mixture model
- Found critical temperature in Ising model by maximizing specific heat through gradient-based optimization
- Maintained unbiased gradient estimates while keeping computational overhead constant through trajectory pruning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Coupling two Metropolis-Hastings chains allows unbiased gradient estimation through discrete accept/reject steps.
- Mechanism: By running two chains in parallel with a coupled proposal, the algorithm can track alternative trajectories and compute stochastic derivatives that weight differences between the two chains' outcomes. When chains recouple, the alternative contribution becomes zero, ensuring unbiasedness.
- Core assumption: The coupled proposal qxy must satisfy that when x = y, then x' = y' (primal and alternative states stay identical when they match).
- Evidence anchors:
  - [abstract] "We develop a methodology for unbiased differentiation of the Metropolis-Hastings sampler, allowing us to differentiate through probabilistic inference, even if the model has discrete components within it."
  - [section] "Theorem 2.1. Suppose that for all x, y, it holds that if x', y' ~ qxy(·, · | x, y) then x' ~ q(· | x) and y' ~ q(· | y) (i.e. qxy is a proposal coupling), and furthermore that if x = y then x' = y'."
  - [corpus] Weak evidence - no direct corpus support for coupling-based gradient estimation, though related work exists on MCMC coupling.
- Break condition: If the coupling assumption fails (x = y but x' ≠ y'), the estimator becomes biased because alternative trajectories could diverge even when chains match.

### Mechanism 2
- Claim: Smoothed perturbation analysis (SPA) provides an unbiased gradient estimator for discrete random programs like MH.
- Mechanism: SPA estimates gradients by taking differences between program outputs under discrete perturbations, weighted by the probability of those perturbations. For MH, this means tracking alternative samples that branch from the primal chain and computing weighted differences.
- Core assumption: The program (MH sampler) can be modified to produce alternative samples while maintaining the same marginal distribution as the original.
- Evidence anchors:
  - [abstract] "By fusing recent advances in stochastic differentiation with Markov chain coupling schemes, the procedure can be made unbiased, low-variance, and automatic."
  - [section] "In this work, we opt for an SPA-based estimator, which for a purely discrete random variable X θ assumes the following form: ∂/∂θ E[f(X θ)] = E[wθ(f(Y θ) - f(X θ))]"
  - [corpus] Weak evidence - no direct corpus support for SPA in MH context, though SPA is known for discrete optimization.
- Break condition: If the perturbation probability wθ becomes zero or undefined, the estimator fails to provide a valid gradient signal.

### Mechanism 3
- Claim: Pruning alternative trajectories when they recouple to the primal chain maintains O(1) computational overhead.
- Mechanism: When alternative samples yi+1 = xi+1, the algorithm sets wi = 0, effectively dropping that alternative trajectory. This prevents exponential growth in the number of alternatives tracked while preserving unbiasedness.
- Core assumption: Once chains recouple, they remain coupled for all future steps under the coupling scheme used.
- Evidence anchors:
  - [abstract] "The method couples two MH chains and prunes alternative trajectories to maintain low variance while keeping computational overhead constant."
  - [section] "We employ the pruning strategy given in Arya et al. (2022) to stochastically select a single alternative between the currently tracked alternative and the new possible alternative... This ensures that Algorithm 2 has the same time complexity as Algorithm 1 while remaining unbiased."
  - [corpus] Weak evidence - no direct corpus support for pruning in gradient estimation, though MCMC coupling literature exists.
- Break condition: If recoupling doesn't guarantee permanent coupling, pruning could remove trajectories that would later contribute to the gradient.

## Foundational Learning

- Concept: Markov Chain Monte Carlo and Metropolis-Hastings algorithm
  - Why needed here: The entire method builds on differentiating through MH sampling, so understanding how MH works is fundamental.
  - Quick check question: What are the two key components of a Metropolis-Hastings step, and how do they depend on the parameter θ?

- Concept: Automatic differentiation through discrete random programs
- Why needed here: The method extends stochastic automatic differentiation to handle discrete accept/reject steps that are typically non-differentiable.
- Quick check question: How does smoothed perturbation analysis differ from score function estimators when dealing with discrete randomness?

- Concept: Markov chain coupling theory
- Why needed here: The variance reduction technique relies on coupling two chains to minimize recoupling time and explore alternative trajectories efficiently.
- Quick check question: What property must a coupling have to ensure that once two chains meet, they stay together for all future steps?

## Architecture Onboarding

- Component map:
  Primal MH chain -> Alternative chain -> Coupling mechanism -> Pruning logic -> Stochastic derivative computation -> Gradient accumulator

- Critical path:
  1. Initialize primal and alternative chains at same state
  2. Propose coupled moves for both chains
  3. Perform accept/reject steps using common random numbers
  4. Compute stochastic derivative weight based on accept/reject outcome
  5. Prune alternative if chains have recoupled
  6. Accumulate weighted difference for gradient estimate
  7. Return average gradient over all steps

- Design tradeoffs:
  - Coupling strength vs. variance: Stronger coupling reduces recoupling time but may limit exploration of alternative trajectories
  - Pruning frequency vs. computational cost: More aggressive pruning reduces overhead but may increase variance
  - Proposal design vs. gradient quality: Different coupling schemes affect the quality of gradient estimates

- Failure signatures:
  - High gradient variance: Indicates poor coupling or excessive pruning
  - Biased gradients: Suggests coupling assumption violation or implementation error
  - Slow convergence: May indicate suboptimal coupling scheme or learning rate issues

- First 3 experiments:
  1. Implement basic differentiable MH with common random numbers coupling on a simple Gaussian target to verify unbiasedness
  2. Compare variance of common random numbers vs. maximal reflection coupling on a 2D Gaussian mixture
  3. Test optimization performance on a small Ising model with known critical temperature

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the differentiable MH approach scale with increasing numbers of parameters in the target distribution?
- Basis in paper: [explicit] The authors mention that future work should consider applying their scheme to settings with a high number of parameters, where the advantage of gradient-based optimization is more apparent.
- Why unresolved: The current experiments focus on relatively low-dimensional problems (Gaussian mixture with 3 components, Ising model with 144 spins). The authors note that scaling to high-dimensional parameter spaces is an open direction.
- What evidence would resolve it: Experiments applying the differentiable MH approach to target distributions with thousands or millions of parameters, comparing convergence speed and computational cost to baseline methods.

### Open Question 2
- Question: What are the optimal coupling strategies for the proposal distribution in differentiable MH across different problem domains?
- Basis in paper: [explicit] The authors discuss the importance of the choice of coupling between primal and alternative chains for variance performance, and mention using maximal reflection coupling for the Gaussian case and maximal independent coupling for the Gaussian mixture. They note that the choice of coupling is crucial for variance performance.
- Why unresolved: The paper demonstrates the impact of coupling choices on two specific examples but does not provide a systematic analysis of optimal coupling strategies across different types of target distributions or problem domains.
- What evidence would resolve it: A comprehensive study comparing different coupling strategies (maximal coupling, reflection coupling, independent coupling, etc.) across various target distributions, with quantitative analysis of their impact on variance and computational efficiency.

### Open Question 3
- Question: How can the differentiable MH approach be extended to differentiate through parameters of the proposal distribution rather than just the target distribution?
- Basis in paper: [explicit] The authors mention that while their current work focuses on differentiating the target distribution, in other applications they may wish to extend their scheme to instead differentiate parameters of the proposal.
- Why unresolved: The current framework and theoretical guarantees are developed for differentiating target distribution parameters. Extending to proposal parameters would require new theoretical analysis and potentially different coupling strategies.
- What evidence would resolve it: A theoretical extension of the unbiased gradient estimator framework to handle proposal distribution parameters, along with empirical validation on problems where proposal optimization is beneficial (e.g., adaptive MCMC).

## Limitations
- The coupling assumptions are critical for unbiasedness but may be difficult to satisfy in complex, high-dimensional models
- Performance scaling to high-dimensional parameter spaces remains untested and uncertain
- Optimal coupling strategy selection across different problem domains is not systematically addressed

## Confidence

- **High Confidence**: The core theoretical framework for unbiased gradient estimation through coupling is well-established in the coupling literature and the mathematical derivations appear sound.
- **Medium Confidence**: The pruning strategy's ability to maintain O(1) complexity while preserving unbiasedness is theoretically justified but lacks extensive empirical validation across different coupling schemes.
- **Low Confidence**: The generalizability of the method to complex, high-dimensional models beyond the demonstrated Gaussian mixture and Ising examples remains uncertain.

## Next Checks

1. **Coupling Robustness Test**: Systematically vary coupling strength in the Gaussian mixture model and measure the impact on gradient variance and optimization convergence to identify the sensitivity to coupling quality.

2. **Dimensionality Scaling Analysis**: Apply the method to higher-dimensional Gaussian mixtures (d > 3) and measure how gradient variance scales with dimension to test the claimed computational efficiency.

3. **Non-Gaussian Proposal Evaluation**: Replace Gaussian proposals with non-Gaussian alternatives (e.g., Student's t-distribution) in both the Gaussian mixture and Ising models to assess the method's robustness to different proposal distributions.