---
ver: rpa2
title: 'Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language
  Models in Knowledge Conflicts'
arxiv_id: '2305.13300'
source_url: https://arxiv.org/abs/2305.13300
tags:
- memory
- evidence
- llms
- answer
- parametric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) can be easily misled by external evidence,
  even when it conflicts with their parametric memory. This occurs when the evidence
  is coherent and convincing, contradicting prior assumptions about LLM stubbornness.
---

# Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts

## Quick Facts
- arXiv ID: 2305.13300
- Source URL: https://arxiv.org/abs/2305.13300
- Reference count: 19
- Large language models (LLMs) can be easily misled by external evidence, even when it conflicts with their parametric memory.

## Executive Summary
Large language models (LLMs) exhibit a complex behavior when faced with external evidence that conflicts with their parametric memory. This study reveals that LLMs can be surprisingly receptive to coherent and convincing external evidence, even when it contradicts their internal knowledge. However, they also demonstrate a strong confirmation bias, favoring evidence that aligns with their parametric memory when both supportive and contradictory evidence are present. These findings highlight the challenges in developing unbiased tool-augmented LLMs that can effectively integrate conflicting information from external sources.

## Method Summary
The study investigates LLM behavior in knowledge conflict scenarios using question answering (QA) datasets POPQA and STRATEGY QA, along with LLMs like ChatGPT and GPT-4. The researchers elicit parametric memory from LLMs in a closed-book QA fashion, construct counter-memory by reframing memory answers and generating coherent supporting evidence, and conduct controlled experiments to analyze LLM behavior in single-source and multi-source evidence settings. The focus is on evaluating memorization ratio and answer consistency to understand how LLMs handle external evidence that conflicts with their parametric memory.

## Key Results
- LLMs are highly receptive to coherent external evidence that conflicts with parametric memory when it's the sole source.
- LLMs exhibit strong confirmation bias when presented with both supportive and contradictory evidence, preferring parametric memory-aligned information.
- Evidence format (length, order, coherence) significantly influences LLM's evidence preference.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs are highly receptive to external evidence when it is the sole source, even if it conflicts with parametric memory.
- Mechanism: When no parametric memory is explicitly activated, the model relies on the provided external evidence to generate responses. Coherent, convincing counter-memory can override default parametric knowledge if the evidence is strong enough and presented without contradictory context.
- Core assumption: The model treats external evidence as the primary source of truth when parametric memory is not explicitly triggered or recalled.
- Evidence anchors:
  - [abstract]: "LLMs can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing."
  - [section]: "On the one hand, surprisingly, LLMs are actually highly receptive to external evidence if it is presented in a coherent way, even though it conflicts with their parametric memory."
  - [corpus]: Weak evidence. Only general knowledge conflict papers in corpus, no specific study on single-source external evidence overriding parametric memory.
- Break condition: If the external evidence is incoherent, fragmented, or contains conflicting information within itself, the model may distrust it and revert to parametric memory or abstain from answering.

### Mechanism 2
- Claim: LLMs exhibit strong confirmation bias when both supportive and contradictory evidence to their parametric memory are present.
- Mechanism: When presented with multiple pieces of evidence, the model weighs them against its internal parametric memory. Evidence that aligns with parametric memory is given higher weight, leading to a preference for the memory answer even when contradictory evidence is present.
- Core assumption: The model has an inherent bias to trust its own parametric memory over external evidence, especially when the external evidence is conflicting.
- Evidence anchors:
  - [abstract]: "LLMs also demonstrate a strong confirmation bias when the external evidence contains some information that is consistent with their parametric memory, despite being presented with conflicting evidence at the same time."
  - [section]: "However, when both supportive and contradictory evidence to their parametric memory are present, LLMs show a strong confirmation bias and tend to cling to their parametric memory."
  - [corpus]: Weak evidence. While the paper cites studies on knowledge conflicts, specific evidence on confirmation bias in multi-source scenarios is limited in the corpus.
- Break condition: If the quantity or quality of contradictory evidence significantly outweighs the supportive evidence, or if the supportive evidence is fragmented or irrelevant, the model may shift its preference.

### Mechanism 3
- Claim: The format and presentation of external evidence (length, order, coherence) significantly influences LLM's evidence preference.
- Mechanism: LLMs are sensitive to the characteristics of the evidence presented. Longer, more coherent evidence is more likely to be trusted, especially if it aligns with parametric memory. The order of evidence presentation also matters, with evidence presented first having a higher chance of being favored.
- Core assumption: The model uses heuristics based on evidence characteristics (length, coherence, order) to determine its trustworthiness and relevance, in addition to content-based evaluation.
- Evidence anchors:
  - [abstract]: Not directly stated, but implied by the comprehensive investigation into behavior under different evidence scenarios.
  - [section]: "LLMs demonstrate a noticeable sensitivity to the evidence order... LLMs trust long counter-memory only, while they trust any-length parametric memory evidence."
  - [corpus]: Weak evidence. While the paper mentions studies on evidence order sensitivity in other models, specific evidence on LLM's sensitivity to evidence format is limited in the corpus.
- Break condition: If the evidence format is manipulated in a way that the model's heuristics are exploited (e.g., artificially lengthening irrelevant evidence), the model's preference may be skewed away from the most accurate information.

## Foundational Learning

- Concept: Knowledge Conflict Resolution
  - Why needed here: Understanding how LLMs handle situations where external evidence contradicts their parametric memory is crucial for developing effective tool-augmented LLMs.
  - Quick check question: What is the primary factor that influences an LLM's preference when presented with both supportive and contradictory evidence to its parametric memory?

- Concept: Evidence Quality Assessment
  - Why needed here: The quality of external evidence (coherence, relevance, length) significantly impacts an LLM's willingness to accept it over its parametric memory.
  - Quick check question: How does the coherence of external evidence affect an LLM's receptiveness to it, especially when it conflicts with parametric memory?

- Concept: Confirmation Bias in AI
  - Why needed here: Recognizing the presence of confirmation bias in LLMs is essential for mitigating its effects and ensuring unbiased use of external evidence.
  - Quick check question: What is confirmation bias, and how does it manifest in the behavior of LLMs when presented with conflicting evidence?

## Architecture Onboarding

- Component map: External evidence → Evidence Processor → LLM Core → Response Generation
- Critical path: External evidence → Evidence Processor → LLM Core → Response Generation
- Design tradeoffs:
  - Balancing the use of parametric memory and external evidence to optimize for accuracy and relevance.
  - Deciding on the level of bias mitigation to apply without overly constraining the model's ability to use its parametric memory.
  - Determining the optimal evidence format and presentation to maximize the model's receptiveness to accurate external information.
- Failure signatures:
  - Overreliance on parametric memory, leading to resistance to accurate external evidence.
  - Susceptibility to disinformation when external evidence is coherent and convincing.
  - Inconsistent behavior based on evidence order or format rather than content.
- First 3 experiments:
  1. Single-source evidence test: Present coherent counter-memory as the sole evidence and measure the model's receptiveness.
  2. Multi-source evidence test: Present both supportive and contradictory evidence and measure the strength of confirmation bias.
  3. Evidence format test: Manipulate the length, order, and coherence of evidence to determine their impact on the model's preference.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop methods to prevent LLMs from being deceived by disinformation from external tools?
- Basis in paper: [explicit] The paper mentions that LLMs can be easily deceived by disinformation from external tools and that exploring methods to prevent such attacks warrants significant attention in future research.
- Why unresolved: The paper identifies this as a potential safety risk but does not propose specific solutions or methods to address it.
- What evidence would resolve it: Research demonstrating effective methods or techniques to prevent LLMs from being deceived by disinformation from external tools would resolve this question.

### Open Question 2
- Question: How can we improve LLMs' ability to integrate fragmented evidence from different sources?
- Basis in paper: [explicit] The paper shows that LLMs have limited abilities to integrate fragments of evidence, which can affect their performance in complex queries requiring information from multiple sources.
- Why unresolved: The paper highlights this limitation but does not propose solutions or techniques to enhance LLMs' evidence integration capabilities.
- What evidence would resolve it: Research demonstrating improved methods or techniques for LLMs to effectively integrate fragmented evidence from different sources would resolve this question.

### Open Question 3
- Question: How can we ensure that the presentation of evidence to LLMs is in an easy-to-use format for tool-augmented systems?
- Basis in paper: [explicit] The paper suggests that the same external evidence in different formats (fragmented or whole) may have different effects on LLMs and that it is worth exploring the presentation of evidence in an easy-to-use format for LLMs in the future.
- Why unresolved: The paper raises this issue but does not provide specific recommendations or techniques for optimizing the presentation of evidence to LLMs.
- What evidence would resolve it: Research demonstrating effective methods or techniques for presenting evidence to LLMs in a format that enhances their performance in tool-augmented systems would resolve this question.

## Limitations
- The study's conclusions are limited by the specific QA tasks and LLMs used, which may not generalize to other domains or model architectures.
- Evidence quality assessment relies on entailment checking, which may not fully capture nuanced contradictions or the model's interpretation of coherence.
- The strength of confirmation bias observed could be influenced by the specific evidence presentation formats used in the experiments.

## Confidence
- High Confidence: LLMs are receptive to coherent external evidence that conflicts with parametric memory when it's the sole source.
- Medium Confidence: LLMs exhibit confirmation bias when presented with both supportive and contradictory evidence, preferring parametric memory-aligned information.
- Medium Confidence: Evidence format (length, order, coherence) influences LLM's evidence preference, though the strength of these effects may vary.

## Next Checks
1. **Cross-domain validation:** Test the observed behaviors in knowledge conflicts across different task types (e.g., fact-checking, reasoning, creative writing) to assess generalizability.
2. **Model architecture comparison:** Investigate whether the observed behaviors are consistent across different LLM architectures (e.g., encoder-decoder vs. decoder-only models) and training paradigms.
3. **Bias mitigation effectiveness:** Evaluate the effectiveness of various bias mitigation techniques (e.g., debiasing training, evidence weighting mechanisms) in reducing confirmation bias while maintaining accurate use of parametric memory.