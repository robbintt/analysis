---
ver: rpa2
title: 'LightSAGE: Graph Neural Networks for Large Scale Item Retrieval in Shopee''s
  Advertisement Recommendation'
arxiv_id: '2310.19394'
source_url: https://arxiv.org/abs/2310.19394
tags:
- graph
- items
- item
- lightsage
- shopee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of applying Graph Neural Networks
  (GNNs) for large-scale item retrieval in Shopee''s e-commerce recommendation system.
  It focuses on three critical challenges: graph construction, model architecture,
  and handling data sparsity and long-tail items.'
---

# LightSAGE: Graph Neural Networks for Large Scale Item Retrieval in Shopee's Advertisement Recommendation

## Quick Facts
- arXiv ID: 2310.19394
- Source URL: https://arxiv.org/abs/2310.19394
- Reference count: 17
- Primary result: 1% increase in ads orders and 5% increase in revenue from online A/B tests

## Executive Summary
This paper introduces LightSAGE, a GNN-based architecture designed for large-scale item retrieval in Shopee's e-commerce advertisement recommendation system. The authors address three key challenges: constructing high-quality item graphs, designing efficient GNN models, and handling data sparsity for long-tail items. LightSAGE combines user behavior edges with collaborative filtering-based edges to create a dense, high-precision graph. The simplified architecture removes unnecessary transformations for scalability while maintaining performance. The system achieves significant improvements in both offline evaluations and online A/B tests, with practical deployment to Shopee's main traffic.

## Method Summary
The method constructs item graphs by combining strong-signal user behaviors (clicks on product detail pages) with high-precision collaborative filtering algorithms (Swing and Search CF). LightSAGE, the proposed GNN model, uses neighborhood sampling from PinSAGE and linear aggregation from LightGCN, removing feature transformations and nonlinear activations for efficiency. For long-tail items, embeddings are derived through content-based averaging with seed items or weighted neighbor aggregation in a daily-updated inference graph. The model is trained using cross-entropy loss on cosine-similarity scores between target and positive/negative nodes.

## Key Results
- Achieves 1% increase in ads orders and 5% increase in revenue in online A/B tests
- Improves offline AUC and unique recall metrics compared to baseline models
- Successfully handles long-tail items through content-based and inference-graph-based embedding propagation

## Why This Works (Mechanism)

### Mechanism 1
LightSAGE achieves high-quality embeddings for both seed and long-tail items by combining direct user behavior edges with CF-based edges. The model constructs a homogeneous directed graph where seed items (popular items with sufficient training data) are connected via high-precision user behaviors and CF-derived edges. During inference, long-tail items inherit embeddings either through content-based averaging or weighted neighbor aggregation in a relaxed inference graph.

### Mechanism 2
LightSAGE's simplified architecture improves scalability and performance compared to PinSAGE and LightGCN. The model removes feature transformation and nonlinear activation layers from PinSAGE, reducing computational cost while retaining neighborhood sampling. It adopts LightGCN's linear aggregation, making it memory-efficient and faster for large-scale e-commerce retrieval.

### Mechanism 3
Multi-stage handling of long-tail items via content-based and inference-graph-based embedding propagation increases recall without harming precision. For long-tail items not in the training graph, embeddings are first derived from content similarity to seed items; for those in the daily-updated inference graph, embeddings are computed by weighted averaging of neighbor seed embeddings, capturing recent behavioral shifts.

## Foundational Learning

- Concept: Graph Neural Networks and neighborhood aggregation
  - Why needed here: LightSAGE's core operation is propagating and aggregating neighbor embeddings to form item representations; understanding this is essential for model tuning and debugging.
  - Quick check question: In LightSAGE, what is the aggregation formula used to compute a node's next-layer embedding from its neighbors?

- Concept: Approximate Nearest Neighbor (ANN) search for large-scale retrieval
  - Why needed here: After generating embeddings, ANN is used to perform scalable item-to-item retrieval; understanding ANN tradeoffs is crucial for latency and recall optimization.
  - Quick check question: Why is ANN preferred over exact nearest neighbor search in this production setup?

- Concept: Collaborative Filtering (CF) algorithms (Swing, Search CF)
  - Why needed here: CF edges supplement user behavior edges to densify the graph; knowing how Swing and Search CF work is necessary for graph construction and quality assessment.
  - Quick check question: What is the key difference between Swing and Search CF in generating CF edges?

## Architecture Onboarding

- Component map: User behavior extraction -> spam filtering -> edge weighting -> CF edge addition -> graph finalization -> dynamic sampling -> projection layer -> GNN blocks (neighborhood sampling + linear aggregation) -> embedding generation -> loss computation -> ANN retrieval -> online serving

- Critical path: Graph construction -> model training -> inference embedding generation -> ANN retrieval -> online serving

- Design tradeoffs:
  - Precision vs. recall in graph construction: strict user behavior criteria improve precision but reduce graph density; CF edges increase recall but risk noise.
  - Memory vs. performance: LightSAGE removes nonlinear activations for speed but may lose representational power.
  - Freshness vs. stability: daily inference graph updates capture trends but introduce variation.

- Failure signatures:
  - Low AUC and recall: likely graph sparsity or noisy edges; check edge filtering and CF precision.
  - High latency: inefficient neighbor sampling or excessive embedding dimensions; profile GNN block and ANN indexing.
  - Cold-start failure: insufficient content features or missing inference graph updates; verify content-based fallback logic.

- First 3 experiments:
  1. Train LightSAGE on a small graph (10k nodes) and validate AUC on held-out edges; compare to baseline Node2vec.
  2. Perform ablation: remove CF edges and measure change in unique recall and tail unique recall.
  3. Test long-tail embedding propagation: compare content-based vs. inference-graph-based embeddings on a sample of cold-start items using similarity to seed embeddings.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of LightSAGE compare to other state-of-the-art GNN architectures like GAT or PinSAGE when applied to similar large-scale e-commerce recommendation systems? The paper only compares to a limited set of baselines and doesn't explore how LightSAGE performs against newer or more specialized GNN architectures.

### Open Question 2
How does the performance of LightSAGE scale with the size of the item graph and the number of user interactions? The paper focuses on a single dataset and doesn't investigate how the model's performance scales with different sizes of item graphs or user interaction volumes.

### Open Question 3
What is the impact of using different high-precision collaborative filtering algorithms for constructing supplementary edges in the item graph? The paper only uses two specific CF algorithms and doesn't investigate how using different algorithms or combinations might affect the quality of the item graph and subsequent model performance.

## Limitations
- Key methodological details are underspecified, including exact LightSAGE architecture parameters and collaborative filtering implementation details
- Limited comparative analysis with more recent GNN architectures
- Lack of statistical significance reporting for online A/B test results

## Confidence

| Claim | Confidence |
|-------|------------|
| 1% order increase and 5% revenue increase | Medium |
| Improvements in offline AUC and recall metrics | Medium |
| Successful handling of long-tail items | Medium |

## Next Checks

1. Request architecture specification: layer dimensions, neighborhood sampling strategy, and training hyperparameters for faithful reproduction
2. Conduct ablation study: train PinSAGE and LightGCN on identical graph data to validate architectural simplifications
3. Verify statistical significance: obtain confidence intervals and p-values for the reported 1% order and 5% revenue improvements in online tests