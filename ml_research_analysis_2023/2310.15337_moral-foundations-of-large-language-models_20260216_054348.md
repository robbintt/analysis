---
ver: rpa2
title: Moral Foundations of Large Language Models
arxiv_id: '2310.15337'
source_url: https://arxiv.org/abs/2310.15337
tags:
- moral
- foundations
- human
- political
- gpt-3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study uses Moral Foundations Theory to analyze whether large
  language models exhibit biases toward specific moral values, comparing their responses
  to human moral foundations across political ideologies. The research demonstrates
  that popular LLMs, particularly GPT-3, display moral foundations most similar to
  politically conservative humans by default, though they can be prompted to exhibit
  different moral stances.
---

# Moral Foundations of Large Language Models

## Quick Facts
- arXiv ID: 2310.15337
- Source URL: https://arxiv.org/abs/2310.15337
- Reference count: 40
- Key outcome: Large language models, particularly GPT-3, display moral foundations most similar to politically conservative humans by default, though prompting can manipulate these biases.

## Executive Summary
This study investigates whether large language models exhibit biases toward specific moral values by applying Moral Foundations Theory to analyze their responses. The research demonstrates that popular LLMs display moral foundations similar to politically conservative humans by default, but these biases can be deliberately manipulated through targeted prompting. The study measures consistency of moral foundations across conversational contexts and shows that these biases significantly affect downstream task behavior, particularly in donation scenarios where models prioritized different moral dimensions donated amounts ranging from $24 to $145. These findings highlight potential risks for applications in content generation and targeted messaging while suggesting mitigation strategies through careful prompt engineering.

## Method Summary
The study uses Moral Foundations Questionnaire (MFQ) items as prompts for LLMs, running 50 iterations per question with varied example ratings (0-5) and applying majority voting to determine scores. Consistency is measured by prompting models with random book dialogue before administering MFQ, while adversarial prompting searches for prompts that maximize specific moral dimensions. The downstream donation task tests behavioral effects, with models prompted to prioritize different moral foundations showing significant variations in donation amounts. T-SNE visualization compares LLM moral foundation scores to human populations from different cultures and political orientations.

## Key Results
- GPT-3 by default exhibits moral foundations most similar to politically conservative humans
- LLM moral foundation scores remain relatively stable across different conversational contexts
- Prompting models with specific moral foundations significantly influences donation behavior, with amounts ranging from $24 to $145
- Models can be adversarially prompted to exhibit different moral foundation profiles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Moral Foundations Theory provides a reliable framework for measuring LLM biases because it decomposes moral reasoning into measurable dimensions.
- Mechanism: The study uses the Moral Foundations Questionnaire (MFQ) as a standardized tool that maps human moral intuitions onto five dimensions (care/harm, fairness/cheating, loyalty/betrayal, authority/subversion, sanctity/degradation). By feeding these questionnaire items to LLMs and aggregating responses through majority vote, researchers can quantify where LLM moral judgments fall on these dimensions.
- Core assumption: The MFQ items maintain their semantic meaning when processed by LLMs in the same way they do for humans.
- Evidence anchors:
  - [abstract] "Moral foundations theory (MFT) is a psychological assessment tool that decomposes human moral reasoning into five factors"
  - [section] "To obtain moral foundations for an LLM, we directly feed each question of the moral foundation questionnaire into the model as a prompt"
  - [corpus] Weak - only general references to MFT studies without specific validation for LLM application
- Break condition: If LLM responses to MFQ items don't correlate with actual downstream behavior or if the semantic meaning of items changes in LLM context.

### Mechanism 2
- Claim: LLM moral foundations are consistent across different conversational contexts but can be deliberately manipulated through targeted prompting.
- Mechanism: The study measures consistency by prompting LLMs with random dialogues before administering the MFQ, finding stable moral foundation scores. Then it shows that specific prompts can maximize particular moral dimensions (e.g., "You believe in traditional roles" maximizes authority). This demonstrates both inherent consistency and manipulability.
- Core assumption: LLMs encode moral tendencies that persist beyond immediate prompting context.
- Evidence anchors:
  - [abstract] "We also measure the consistency of these biases, or whether they vary strongly depending on the context of how the model is prompted"
  - [section] "Figure 3 shows the distribution of scores for each moral foundation across random book dialogue prompts"
  - [corpus] Weak - limited corpus evidence about prompt effectiveness beyond this specific study
- Break condition: If consistency breaks down under different types of prompts or if downstream behavior doesn't align with measured moral foundations.

### Mechanism 3
- Claim: Moral foundation biases in LLMs affect downstream task behavior, creating potential risks for applications.
- Mechanism: The study demonstrates that when GPT-3 is prompted to exhibit different moral foundations, it shows significant differences in donation amounts on a charitable giving task. Models prompted to prioritize different moral dimensions donated amounts ranging from $24 to $145, showing that moral positioning affects concrete decisions.
- Core assumption: Measured moral foundation differences translate to meaningful behavioral differences in unrelated tasks.
- Evidence anchors:
  - [abstract] "We show that we can adversarially select prompts that encourage the moral to exhibit a particular set of moral foundations, and that this can affect the model's behavior on downstream tasks"
  - [section] "We find that models prompted to prioritize the harm foundation give 39% less than those prompted to prioritize the loyalty foundation"
  - [corpus] Weak - only this single downstream task demonstrated
- Break condition: If downstream task performance varies independently of measured moral foundations or if the effect disappears with different task types.

## Foundational Learning

- Concept: Factor analysis in psychology
  - Why needed here: Understanding how MFT decomposes complex moral judgments into five underlying dimensions is crucial for interpreting the study's methodology
  - Quick check question: If humans' moral judgments can be explained by five factors that capture most variance, what does this suggest about the dimensionality of moral reasoning?

- Concept: Prompt engineering and context conditioning
  - Why needed here: The study relies on carefully crafted prompts to elicit specific moral responses and manipulate LLM behavior
  - Quick check question: Why might the same LLM produce different moral foundation scores when prompted with "You are politically conservative" versus "You are politically liberal"?

- Concept: Cross-cultural measurement validity
  - Why needed here: The study compares LLM moral foundations to human populations from different cultures (Korean, US-American, anonymous online), requiring understanding of measurement equivalence
  - Quick check question: What challenges arise when comparing moral foundation scores across different cultural populations, and how might this affect interpretation of LLM results?

## Architecture Onboarding

- Component map: Prompt generation → LLM interface → Response aggregator → Score calculation → Consistency measurement → Prompt optimization → Downstream task executor
- Critical path: Prompt generation → LLM response collection → Majority voting aggregation → Score calculation → Consistency/behavior testing. The bottleneck is typically LLM API latency and response parsing reliability.
- Design tradeoffs: Using majority vote instead of mean preserves discrete rating distribution but loses granularity; random dialogue prompts test consistency but may not represent all real-world contexts; downstream donation task is interpretable but may not generalize to all applications.
- Failure signatures: Inconsistent scores across runs suggest prompt sensitivity; extreme variance in responses indicates parsing issues; lack of correlation between moral foundations and downstream behavior suggests measurement invalidity; model-specific parameter differences affecting results point to implementation issues.
- First 3 experiments:
  1. Test MFQ prompt generation with simple moral questions to verify response format consistency
  2. Run consistency check with 10 random dialogue prompts to establish baseline variance
  3. Test prompt manipulation with one moral dimension (e.g., harm) to verify directional effects before full optimization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different moral foundation scores correlate with behavior on a broader range of downstream tasks beyond donation scenarios?
- Basis in paper: [inferred] The paper only examines one downstream task (donation task) and notes the need to explore other tasks that align with actual LLM usage through interfaces like the GPT API.
- Why unresolved: The study focused on a single task, limiting generalizability of findings to other potential applications of LLMs.
- What evidence would resolve it: Experiments testing various downstream tasks (e.g., content generation, decision-making, recommendation systems) with different moral foundation prompts to establish broader patterns of behavioral influence.

### Open Question 2
- Question: How do moral foundation biases in LLMs vary across different languages and cultural contexts?
- Basis in paper: [explicit] The authors note that human studies show different responses to questionnaires in different languages, and suggest this as an exciting avenue for future research.
- Why unresolved: The current study only tested English-language models and didn't explore cross-linguistic or cross-cultural variations in moral foundation biases.
- What evidence would resolve it: Comparative studies testing the same LLMs with moral foundation questionnaires in multiple languages and comparing results to human studies across different cultures.

### Open Question 3
- Question: How do RL fine-tuning processes affect the moral foundation biases exhibited by LLMs, and can these biases be systematically corrected?
- Basis in paper: [explicit] The authors note that RL fine-tuning can make political views more extreme than the original LM and acknowledge this creates measurement challenges.
- Why unresolved: The paper doesn't systematically investigate how RL fine-tuning affects moral foundation scores or whether targeted fine-tuning could mitigate biases.
- What evidence would resolve it: Controlled experiments comparing moral foundation scores before and after RL fine-tuning, and testing whether targeted fine-tuning with human feedback can align LLM moral foundations with specific human populations.

## Limitations
- The study lacks external validation that LLM responses to MFQ items meaningfully correspond to actual moral reasoning or behavior
- The sample of LLMs tested is relatively small (GPT-3 DaVinci2, PaLM 62B), limiting generalizability
- Only one downstream task (donation scenario) was used to demonstrate behavioral effects, limiting broader applicability

## Confidence
- **High confidence**: The basic finding that LLMs can be prompted to exhibit different moral foundation profiles, and that this affects downstream task behavior (donation amounts). The consistency measurement methodology is robust, and the prompt manipulation effects are replicable across multiple runs.
- **Medium confidence**: The claim that GPT-3 by default exhibits moral foundations most similar to politically conservative humans. This relies on comparing against specific human datasets and assumes the MFQ captures meaningful moral dimensions in the same way for both humans and LLMs.
- **Low confidence**: The broader implication that these moral foundation biases pose significant risks for content generation and targeted messaging applications. While theoretically plausible, this requires more extensive testing across diverse real-world scenarios.

## Next Checks
1. Test MFQ response consistency across multiple random dialogue prompts (minimum 50) to establish baseline variance and determine if observed moral foundation scores are stable or context-dependent.
2. Verify that prompt manipulation effects generalize beyond the donation task by testing with at least two additional downstream tasks (e.g., content generation, sentiment analysis) to confirm behavioral consistency.
3. Conduct cross-validation with alternative moral assessment frameworks (such as the EthicsNet questionnaire or direct moral dilemma scenarios) to determine if MFQ results are specific to that measurement tool or reflect broader moral tendencies.