---
ver: rpa2
title: Error Feedback Can Accurately Compress Preconditioners
arxiv_id: '2306.06098'
source_url: https://arxiv.org/abs/2306.06098
tags:
- uni00000013
- uni00000011
- gradient
- uni00000014
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a compression method for full-matrix preconditioners
  that leverages error feedback to avoid divergence. The approach compresses the gradient
  history used by preconditioners like GGT and M-FAC via sparsification or low-rank
  approximation, feeding the compression error back into future iterations.
---

# Error Feedback Can Accurately Compress Preconditioners

## Quick Facts
- arXiv ID: 2306.06098
- Source URL: https://arxiv.org/abs/2306.06098
- Reference count: 40
- Key outcome: Compresses full-matrix preconditioners via error feedback to enable 99% sparsity or 9x memory reduction without accuracy loss

## Executive Summary
This paper introduces Error Feedback Compressed Preconditioners (EFCP), a method that compresses full-matrix preconditioners like GGT and M-FAC using sparsification or low-rank approximation while maintaining convergence through error feedback. By accumulating and feeding back compression errors into future iterations, EFCP enables up to 99% sparsity or 9x memory reduction without accuracy degradation. Experiments demonstrate that compressed preconditioners achieve the same or better accuracy than dense versions while using significantly less memory across vision and language models.

## Method Summary
EFCP compresses the gradient history used by full-matrix preconditioners (GGT and M-FAC) before feeding it into the preconditioner update. At each iteration, the method accumulates the new gradient and previous error into an accumulator, compresses this using sparsification (Top-k) or low-rank approximation (power iteration), and feeds the compression error back into future iterations. This error feedback mechanism ensures that lossy compression doesn't accumulate unbounded errors, allowing the method to maintain convergence guarantees while achieving substantial memory savings.

## Key Results
- Achieves 99% sparsity on ImageNet without accuracy loss, reducing memory from 59GB to 15GB for M-FAC
- On BERT-tiny, sparse M-FAC matches Adam baseline while dense M-FAC runs out of memory
- Low-rank compression achieves 9x memory improvement while maintaining accuracy on BERT models
- Memory savings of up to 20x for GGT and 30x for M-FAC while preserving or improving accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Error feedback enables lossy compression of gradient history without losing convergence guarantees.
- **Mechanism**: At each iteration, compression error from the previous step is added to the current gradient before compression, ensuring error accumulates in the "error accumulator" rather than being lost.
- **Core assumption**: Compression method is applied consistently and error feedback is properly integrated.
- **Break condition**: If compression error grows unbounded or feedback loop fails, convergence may be lost.

### Mechanism 2
- **Claim**: Compressed preconditioners achieve memory savings of up to 20x for GGT and 30x for M-FAC while maintaining accuracy.
- **Mechanism**: By storing only k ≪ d entries per gradient (e.g., 1% density), memory required for gradient history drops dramatically through sparse or low-rank representations.
- **Core assumption**: Compression rate is sufficient to preserve information needed for accurate preconditioning.
- **Break condition**: If compression rate is too aggressive (>99% sparsity), preconditioner may lose critical information.

### Mechanism 3
- **Claim**: Low-rank compression of per-layer gradients achieves 9x memory improvement while maintaining accuracy.
- **Mechanism**: Each gradient tensor is decomposed into smaller matrices via power iteration, computing inner products needed for M-FAC using low-rank factors.
- **Core assumption**: Rank ρ is chosen appropriately (e.g., ρ=4) to balance compression and accuracy.
- **Break condition**: If rank is too low, low-rank approximation may not capture essential gradient information.

## Foundational Learning

- **Concept**: Stochastic gradient descent (SGD) and its variants
  - Why needed here: EFCP builds upon SGD by adding preconditioning and compression. Understanding SGD is essential to grasp how EFCP modifies the optimization process.
  - Quick check question: What is the update rule for standard SGD?

- **Concept**: Preconditioning in optimization
  - Why needed here: EFCP introduces full-matrix preconditioning (GGT and M-FAC) to accelerate convergence. Understanding how preconditioning works is crucial to understanding the benefits and challenges of EFCP.
  - Quick check question: How does preconditioning affect the convergence rate of optimization algorithms?

- **Concept**: Error feedback in distributed optimization
  - Why needed here: EFCP uses error feedback to compensate for lossy compression. Understanding how error feedback works in distributed optimization is essential to understanding how EFCP maintains convergence despite compression.
  - Quick check question: How does error feedback ensure convergence in distributed SGD with gradient compression?

## Architecture Onboarding

- **Component map**: at → COMPRESS → ξt → A → ut → θt+1
- **Critical path**: The accumulator (at) is compressed, the error is updated, the preconditioner is updated using the compressed gradient, and the parameters are updated using the preconditioned gradient.
- **Design tradeoffs**:
  - Compression rate vs. accuracy: Higher compression rates save more memory but may degrade accuracy
  - Sparsification vs. low-rank compression: Sparsification is simpler but may be less effective for some gradient distributions; low-rank compression can be more effective but requires more complex implementation
- **Failure signatures**:
  - Accuracy degradation: If compression rate is too high or error feedback is not working properly
  - Memory usage: If compression is not effective, memory usage may not be reduced as expected
  - Training instability: If error feedback is not integrated properly
- **First 3 experiments**:
  1. Linear probing on ImageNet features with different compression rates to validate accuracy preservation
  2. ResNet-18 training on CIFAR-10 with sparse M-FAC to compare memory usage and accuracy against dense M-FAC
  3. BERT-tiny training on MNLI with low-rank M-FAC to demonstrate memory savings and accuracy recovery

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can error feedback compressed preconditioners maintain theoretical convergence guarantees for full-matrix preconditioners?
- Basis in paper: The paper states "we can adapt a recent argument by [24] to show that the method should guarantee standard rates of O(1/√T + σ2/√T + d/T) under standard assumptions" but this is only for the diagonal case, not the full-matrix case discussed in the main paper.
- Why unresolved: The paper explicitly notes that "obtaining general bounds appears challenging, as even in the case of standard SGD understanding error feedback required significant technical effort" and plans to investigate provable convergence in the full-matrix case in future work.
- What evidence would resolve it: A formal proof showing that error feedback compressed GGT or M-FAC preconditioners maintain convergence rates comparable to their uncompressed versions under standard assumptions.

### Open Question 2
- Question: What is the fundamental limit to gradient compression in full-matrix preconditioners before accuracy degradation becomes inevitable?
- Basis in paper: The paper shows that up to 99% sparsity can be achieved without accuracy loss, but notes "accuracy decreases very gradually" as compression increases further. They demonstrate this experimentally with Figure 1 showing accuracy trends at different densities.
- Why unresolved: The paper demonstrates empirical compression limits but doesn't establish theoretical bounds on how much compression is possible before preconditioner quality degrades below a threshold.
- What evidence would resolve it: Theoretical analysis establishing bounds on how compression affects the quality of preconditioner estimates, or experimental results showing the point where accuracy degradation becomes significant across different model architectures and tasks.

### Open Question 3
- Question: How does error feedback compressed preconditioning affect the relationship between learning rate, dampening parameter, and effective scaling of gradients?
- Basis in paper: Section 6.3 discusses quantifying scaling and notes "the scale of values in the empirical distribution R is directly dependent to the learning rate η and the dampening λ and it is extremely difficult to state the relationship between these three quantities."
- Why unresolved: The paper observes that compressed preconditioners induce much larger scaling factors (Q25 ∈ [50, 330], Q50 ∈ [100, 800], Q75 ∈ [250, 2000]) compared to SGD (Q25 ≈ 1, Q50 ∈ [2, 2.5], Q75 ∈ [4.5, 6]) but doesn't explain the theoretical relationship or provide guidelines for tuning.
- What evidence would resolve it: Analysis establishing how error feedback compression interacts with learning rate and dampening to affect gradient scaling, with practical guidelines for hyperparameter tuning that account for compression effects.

## Limitations

- The paper lacks rigorous theoretical convergence guarantees for compressed full-matrix preconditioners, only providing proofs for the diagonal case
- Experimental evaluation is limited to specific model architectures (ResNet, BERT) and datasets, potentially missing edge cases or failure modes
- The relationship between hyperparameters (learning rate, dampening) and compression-induced gradient scaling is not well understood, making hyperparameter tuning challenging

## Confidence

- **High confidence**: The basic error feedback mechanism and its implementation are correct, as demonstrated by stable training curves and memory savings across multiple experiments
- **Medium confidence**: The accuracy preservation claims are supported by experiments, but the evaluation could be more comprehensive across different model scales and tasks
- **Low confidence**: The theoretical guarantees for convergence under compression are not fully established, and the paper does not explore failure modes or sensitivity to hyperparameters like compression rate

## Next Checks

1. **Convergence analysis**: Perform a detailed study of training stability across different compression rates (e.g., 99%, 95%, 90% sparsity) to identify the threshold where accuracy degradation begins.

2. **Scalability testing**: Evaluate the approach on larger models (e.g., ResNet-50, BERT-base) and more diverse datasets to confirm the memory savings and accuracy preservation scale as claimed.

3. **Robustness validation**: Test the sensitivity of the method to hyperparameters such as the number of power iterations for low-rank compression and the choice of compression method (sparsification vs. low-rank) for different gradient distributions.