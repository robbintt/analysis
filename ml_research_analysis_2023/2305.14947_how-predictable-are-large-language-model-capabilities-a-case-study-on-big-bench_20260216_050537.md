---
ver: rpa2
title: How Predictable Are Large Language Model Capabilities? A Case Study on BIG-bench
arxiv_id: '2305.14947'
source_url: https://arxiv.org/abs/2305.14947
tags:
- json
- subtask
- performance
- lite
- big-bench
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work studies how well we can predict large language model
  (LLM) performance based on records of past experiments. We formulate the problem
  as a machine learning task: given model family, number of parameters, task, and
  number of in-context examples, predict normalized performance.'
---

# How Predictable Are Large Language Model Capabilities? A Case Study on BIG-bench

## Quick Facts
- **arXiv ID**: 2305.14947
- **Source URL**: https://arxiv.org/abs/2305.14947
- **Reference count**: 40
- **Primary result**: Strong predictability (R² > 0.95) on random splits degrades when holding out entire model families or tasks

## Executive Summary
This work studies the predictability of large language model (LLM) performance using BIG-bench experiment records. The authors formulate the problem as a regression task, predicting normalized performance from model family, parameter count, task, and number of in-context examples. On random train-test splits, their MLP-based predictor achieves R² > 0.95, demonstrating strong learnable patterns. However, performance degrades significantly on more challenging splits that hold out entire model families or tasks. The paper also introduces the concept of "small-bench" - finding compact task subsets that maximize recoverability of full-task performance, achieving subsets 3× smaller than BIG-bench Hard with similar effectiveness.

## Method Summary
The authors filter BIG-bench records to create a dataset of approximately 56,000 entries, then featurize configurations using binary encoding for model families/tasks and scaled numerical features for parameters and shots. They train multiple regression models including Random Forest, Gradient Boosted Trees, and MLP, comparing their performance using R² and RMSE metrics. The study evaluates different data splitting strategies including random splits, L2.1/L2.2 (holding out tasks or model families), L3 (holding out model-task combinations), and L3 Composition (holding out entire model families across all tasks). Additionally, they implement search algorithms (randomized, greedy, beam search) to find informative "small-bench" task subsets that maximize recoverability of full-task performance.

## Key Results
- MLP-based predictor achieves R² > 0.95 and RMSE < 0.05 on random train-test splits
- Performance degrades significantly when holding out entire model families or tasks (L3 splits)
- Randomized search finds "small-bench" subsets 3× smaller than BIG-bench Hard with similar recoverability
- No single search algorithm consistently outperforms others across different budget sizes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Learnable patterns exist in BIG-bench experiment records enabling accurate performance prediction
- **Mechanism**: Model family, parameter count, task, and shot count form a structured feature space where regression models can learn mappings from configurations to performance outcomes
- **Core assumption**: The performance surface is smooth and continuous enough for interpolation between known configurations
- **Evidence anchors**: R² > 0.95 on random splits demonstrates learnable patterns
- **Break condition**: Performance becomes highly non-linear or discontinuous across configurations

### Mechanism 2
- **Claim**: Task and model family similarities enable transfer learning within the predictor
- **Mechanism**: The predictor leverages similarities among model families and tasks to generalize performance predictions across configurations
- **Core assumption**: Similar model families or tasks have correlated performance patterns
- **Evidence anchors**: Clustering of task representations and observation of subtask relationships in high-performing predictions
- **Break condition**: Model families or tasks are too dissimilar, breaking shared performance patterns

### Mechanism 3
- **Claim**: Search algorithms can identify informative task subsets that maximize recoverability of full-task performance
- **Mechanism**: By evaluating different task subsets, search algorithms find combinations that retain maximal information about performance on remaining tasks
- **Core assumption**: A compact subset of tasks exists that is sufficiently representative of the full task set's performance distribution
- **Evidence anchors**: Successful discovery of 3× smaller subsets compared to BIG-bench Hard
- **Break condition**: No subset adequately represents the full performance distribution

## Foundational Learning

- **Concept**: Regression modeling and evaluation metrics (R², RMSE)
  - **Why needed here**: To quantify how well the predictor can estimate LLM performance based on configurations
  - **Quick check question**: If a predictor achieves R² = 0.95, what percentage of variance in the target variable is explained by the model?

- **Concept**: Feature engineering and featurization
  - **Why needed here**: To convert categorical and numerical inputs into a format suitable for machine learning models
  - **Quick check question**: Why is it beneficial to include both total parameters and non-embedding parameters as separate features in the model?

- **Concept**: Data splitting strategies and their impact on generalization
  - **Why needed here**: To simulate different practical scenarios and assess the predictor's robustness
  - **Quick check question**: How does holding out entire model families (L3 Composition) differ from random splitting in terms of the challenge it poses to the predictor?

## Architecture Onboarding

- **Component map**: Data pipeline -> Feature encoder -> Model zoo -> Evaluation engine -> Search module
- **Critical path**: 1. Load and filter BIG-bench records 2. Preprocess and featurize data 3. Train and evaluate baseline models on random splits 4. Test models on challenging splits (L2.1, L2.2, L3, L3 Composition) 5. Implement search algorithms for "small-bench" discovery 6. Evaluate and compare task subsets for recoverability
- **Design tradeoffs**: Model complexity vs. interpretability; Search efficiency vs. solution quality; Subset size vs. recoverability
- **Failure signatures**: High RMSE or low R² on challenging splits; Search algorithms returning poor recoverability subsets; Large performance drops indicating overfitting
- **First 3 experiments**: 1. Train Random Forest on default random split and evaluate R²/RMSE 2. Test Random Forest on L3 split (holding out entire model-task combinations) 3. Run randomized search to find 16-task subset and compare to BIG-bench Hard

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do specific model architecture details (beyond parameter count) influence predictability of performance across tasks?
- **Basis in paper**: The paper uses only model family and parameter count as features, omitting architectural details
- **Why unresolved**: Current featurization abstracts away important architectural differences that likely affect task performance patterns
- **What evidence would resolve it**: Experiments incorporating detailed architectural features and measuring improvements in R2 scores

### Open Question 2
- **Question**: What are the fundamental properties that make certain BIG-bench tasks inherently unpredictable by current models?
- **Basis in paper**: The paper identifies specific tasks with lowest R2 scores and notes they have "hard to explain" performance curves
- **Why unresolved**: Analysis identifies unpredictable tasks but doesn't systematically characterize what makes them different
- **What evidence would resolve it**: Detailed analysis of task characteristics correlated with prediction difficulty

### Open Question 3
- **Question**: What is the optimal balance between task diversity and redundancy in "small-bench" construction?
- **Basis in paper**: Random search finds 3x smaller subsets than BIG-bench Hard with similar recoverability, suggesting current methods may be suboptimal
- **Why unresolved**: Search algorithms don't explicitly optimize for diversity, yet results suggest redundancy in existing benchmarks
- **What evidence would resolve it**: Systematic comparison of diversity metrics across different "small-bench" constructions and their prediction performance

## Limitations
- High predictability on random splits doesn't translate to challenging splits holding out model families or tasks
- "Small-bench" search shows inconsistent results across different algorithms and budget sizes
- Limited generalizability to truly novel configurations not represented in training data

## Confidence
- **High**: Strong R² > 0.95 performance on random splits is well-supported by experimental results
- **Medium**: Generalizability claims are tentative given performance degradation on challenging splits
- **Medium**: Task subset selection methodology shows promise but lacks consistent superiority across approaches

## Next Checks
1. **Cross-dataset validation**: Test predictors on performance records from different LLM benchmarks (HELM, GLUE) to assess generalizability beyond BIG-bench
2. **Temporal validation**: Train predictors on older LLM data and test ability to predict capabilities of more recent models to evaluate forward-looking predictability
3. **Ablation study on feature importance**: Systematically remove individual features to quantify their relative contribution to prediction accuracy and identify critical configuration aspects