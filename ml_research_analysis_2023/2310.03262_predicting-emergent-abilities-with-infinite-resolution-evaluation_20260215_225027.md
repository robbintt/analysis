---
ver: rpa2
title: Predicting Emergent Abilities with Infinite Resolution Evaluation
arxiv_id: '2310.03262'
source_url: https://arxiv.org/abs/2310.03262
tags:
- scaling
- task
- performance
- loss
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of understanding and predicting
  the scaling behavior of large language models (LLMs) on real tasks. The key challenge
  is that while the loss scales predictably with model size, task performance often
  exhibits emergent abilities - sudden jumps in performance that are not captured
  by conventional evaluation methods.
---

# Predicting Emergent Abilities with Infinite Resolution Evaluation

## Quick Facts
- arXiv ID: 2310.03262
- Source URL: https://arxiv.org/abs/2310.03262
- Authors: 
- Reference count: 40
- The paper introduces PASS UNTIL, an evaluation strategy that enables prediction of 2.4B model performance with 0.05% deviation before training starts, while mathematically defining emergent abilities as super-scaling law growth.

## Executive Summary
This paper addresses the challenge of understanding and predicting scaling behavior of large language models on real tasks, particularly focusing on emergent abilities - sudden jumps in performance that conventional evaluation methods miss. The authors introduce PASS UNTIL, a massive sampling evaluation strategy that estimates task success probability with theoretically infinite resolution by continuing sampling until r successes occur. They demonstrate that task performance follows scaling laws analogous to loss scaling laws, enabling accurate prediction of model performance before training. The paper also proposes a "circuit competition" hypothesis to explain the genesis of emergent abilities, refuting the hypothesis that they stem from multi-step reasoning.

## Method Summary
The authors train models ranging from 0.03B to 2.4B parameters on code and text-code mixtures, then evaluate them on four tasks using PASS UNTIL, which involves massive sampling until r successes are found, providing an unbiased estimate of task success probability. They fit scaling laws to the PASS UNTIL scores by observing the linear relationship between log(-log(PU)) and log(N), and predict 2.4B model performance from smaller models with high accuracy. The method involves both dataset-level and instance-level fitting, with instance-level providing improved predictions by capturing individual instance scaling behaviors. The evaluation requires significant computational resources due to the massive sampling required for infinite resolution.

## Key Results
- Predicts 2.4B model performance on code generation with only 0.05% deviation before training starts
- Identifies super-scaling law growth in some tasks, providing mathematical definition of emergent abilities
- Refutes multi-step reasoning hypothesis for emergent abilities, proposing "circuit competition" alternative
- Demonstrates instance-level fitting provides improved prediction accuracy over dataset-level approaches

## Why This Works (Mechanism)

### Mechanism 1: Infinite Resolution Evaluation (PASS UNTIL)
- **Claim:** By massively sampling decoding outputs and stopping at first success, PASS UNTIL estimates task success probability with theoretically infinite resolution, revealing improvements invisible to traditional metrics.
- **Mechanism:** Instead of fixed K samples, PASS UNTIL continues sampling until r successes occur, recording stopping time K. This gives PU = r/E[K], an unbiased estimator of true success probability P(s) for each instance.
- **Core assumption:** The underlying success probability P(s) exists and is estimable through repeated sampling, with negative binomial distribution accurately modeling the sampling process.
- **Evidence anchors:** Theorem 1 proves PU is unbiased estimate for P(s) using negative binomial distribution; PASS UNTIL is described as having "theoretically infinite resolution" through massive sampling.
- **Break condition:** If P(s) is extremely close to zero making sampling computationally infeasible, or if output distribution changes non-stationarily with scale.

### Mechanism 2: Task Scaling Law Extension from Loss Scaling Law
- **Claim:** Task performance PU follows scaling law analogous to loss scaling law, specifically PU ~ exp(-cN^(-α)), enabling prediction from model parameters alone.
- **Mechanism:** Assuming token-level test loss follows scaling law L ~ cN^(-α) + L0, and answers composed of |y| tokens, overall task success probability becomes PU ~ exp(-cN^(-α)), where c aggregates token-level coefficients.
- **Core assumption:** Test loss for each token in answer sequence decreases with same scaling exponent α across tokens, and irreducible loss L0 approaches zero for passable tasks.
- **Evidence anchors:** Derivation of task scaling law from loss scaling law is shown; empirical validation demonstrates strong linear relationship between log(-log(PU)) and log(N) for all three tasks.
- **Break condition:** If different tokens scale at different rates, or if irreducible loss doesn't approach zero, or if task difficulty varies too much across instances.

### Mechanism 3: Instance-Level Scaling Curve Fit
- **Claim:** Individual instances have different scaling behaviors, and fitting scaling curves at instance level improves prediction accuracy compared to dataset-level averaging.
- **Mechanism:** Instead of averaging PU across instances, fit individual PU curves for each instance IPU(cs, as; N) then aggregate: PU({cs, as}; N) = (1/|S|) Σ_s IPU(cs, as; N). This captures that easy instances saturate early while hard ones continue improving.
- **Core assumption:** Each instance's success probability follows same functional form as overall task but with instance-specific parameters, and instances are independent enough for separate fitting.
- **Evidence anchors:** Instance-level fitting proposed to improve accuracy; fitted instance-level scaling laws demonstrate performances originate from unique starting points and scale at varying rates; Table 2 shows improved accuracy of instance-level predictions.
- **Break condition:** If instance difficulty is too correlated or if too few samples per instance to fit reliable curves.

## Foundational Learning

- **Concept: Negative Binomial Distribution**
  - Why needed here: The proof that PU is an unbiased estimator relies on understanding that number of failures before r successes follows negative binomial distribution.
  - Quick check question: If you sample until 2 successes with success probability p, what is the expected number of trials needed?

- **Concept: Scaling Laws in Neural Networks**
  - Why needed here: The paper extends established loss scaling law framework to task performance, so understanding original loss scaling law is essential.
  - Quick check question: What is the mathematical form of the original loss scaling law (Kaplan et al. 2020)?

- **Concept: Log-Log Linear Relationships**
  - Why needed here: The paper identifies scaling laws by observing linearity in log(-log(PU)) vs log(N) space, which is common pattern in power-law relationships.
  - Quick check question: If y = x^α, what is the relationship between log(y) and log(x)?

## Architecture Onboarding

- **Component map:** Pre-training pipeline (0.03B → 2.4B) -> PASS UNTIL evaluator -> Scaling curve fitter -> Instance-level processor -> Predictor
- **Critical path:** 1. Train scaling curve models (0.03B → 1.5B) 2. Evaluate on test tasks using PASS UNTIL 3. Fit scaling curves (dataset and instance level) 4. Predict 2.4B performance 5. Train and evaluate 2.4B model to verify predictions
- **Design tradeoffs:** Sampling budget vs resolution (more samples give better PU estimates but increase computation); Dataset-level vs instance-level (simpler vs more accurate but computationally heavier); Model size range (wider range gives better curve fitting but requires more training resources)
- **Failure signatures:** Non-linear log(-log(PU)) vs log(N) relationship indicates deviation from assumed scaling law; Very high variance in PU estimates suggests insufficient sampling; Poor prediction accuracy suggests model architecture or training differs significantly from scaling curve models
- **First 3 experiments:** 1. Reproduce PASS UNTIL on simple task with known ground truth to verify unbiased estimation 2. Train small scaling curve (0.03B, 0.1B, 0.3B) and verify scaling law fit on HumanEval 3. Compare dataset-level vs instance-level predictions on single task to quantify accuracy improvement

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can we accurately predict task performance for much larger models (e.g., 10B+ parameters)?
- **Basis in paper:** [explicit] The paper states "we currently do not extend the prediction of task performance to much larger models (e.g., 10B and more). We will try to scale up the experiment in the future."
- **Why unresolved:** Scaling up models and experiments requires significant computational resources and time.
- **What evidence would resolve it:** Successful prediction of task performance on models larger than 10B parameters using proposed PASS UNTIL evaluation strategy and scaling law fitting methods.

### Open Question 2
- **Question:** Can we develop a more rigorous validation method for the "circuit competition" hypothesis explaining emergent abilities?
- **Basis in paper:** [inferred] The paper proposes "circuit competition" hypothesis as alternative explanation for emergent abilities but states "our validation for the hypothesis is superficial. We satisfactorily fit the scaling curve under this hypothesis. However, whether this hypothesis is true from the underlying mechanism remains unknown."
- **Why unresolved:** The hypothesis is based on indirect evidence and curve fitting, not direct observation of circuit behavior within LLMs.
- **What evidence would resolve it:** Experimental or analytical evidence demonstrating existence of multiple competing circuits within LLMs and their role in emergent abilities.

### Open Question 3
- **Question:** How can we establish scaling laws for tasks with non-zero random baselines?
- **Basis in paper:** [explicit] The paper mentions "We consider random sampling as a viable target decoding strategy due to its widespread use in LLMs. Using greedy decoding and beam search as target decoding strategies and their relationship with random sampling poses an interesting avenue for future exploration and study." It also states "In the context of multiple-choice grade as the evaluation metric, evaluations tend to exhibit a biased high score relative to the true performance of the model."
- **Why unresolved:** Current evaluation strategies may overestimate performance when random guessing can yield non-zero scores.
- **What evidence would resolve it:** Development and validation of evaluation strategies that accurately measure performance improvements beyond what can be achieved by random guessing.

## Limitations
- **Sampling Resource Constraints:** PASS UNTIL requires massive sampling with reported times reaching 15.24 hours for 2.4B model evaluation on HumanEval, limiting practical applicability.
- **Task Generalization Uncertainty:** Analysis covers only four specific tasks (HumanEval, Emoji Movie, Date Understanding, Unnatural In-context Learning), may not generalize to other domains or task types.
- **Model Architecture Constraints:** All experiments use decoder-only transformer architectures with specific configurations, scaling law predictions may not transfer to encoder-decoder architectures or different attention mechanisms.

## Confidence
**High Confidence:**
- PASS UNTIL provides unbiased estimation of task success probability (Theorem 1 proof is mathematically rigorous)
- Scaling laws exist for task performance when measured with sufficient resolution
- The mathematical relationship between log(-log(PU)) and log(N) holds across tested tasks

**Medium Confidence:**
- Prediction accuracy of 0.05% deviation for 2.4B model performance
- The "circuit competition" hypothesis as alternative explanation for emergent abilities
- Instance-level fitting provides meaningful improvement over dataset-level predictions

**Low Confidence:**
- Universal applicability of PASS UNTIL across all task types
- The proposed mathematical definition of emergent abilities captures all relevant phenomena
- Extrapolation beyond the 0.03B-2.4B parameter range maintains accuracy

## Next Checks
1. **Cross-Domain Task Validation:** Apply PASS UNTIL and scaling law analysis to tasks from completely different domains (e.g., mathematical reasoning, multilingual translation, scientific question answering) to test generalizability of framework beyond the four tested tasks.

2. **Architecture Transfer Experiment:** Train and evaluate models with different architectural configurations (varying feed-forward dimensions, attention heads, or using encoder-decoder transformers) to verify whether scaling laws and emergent ability predictions transfer across architectural variations.

3. **Extreme Scale Extrapolation:** Use fitted scaling curves to predict performance for models significantly larger than 2.4B parameters (e.g., 10B+ parameters) and validate against actual measurements when feasible, testing limits of extrapolation methodology.