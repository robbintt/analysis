---
ver: rpa2
title: 'MEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities,
  Models and Tasks'
arxiv_id: '2311.07463'
source_url: https://arxiv.org/abs/2311.07463
tags:
- languages
- language
- dataset
- datasets
- gpt4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks large language models across 81 languages,
  including low-resource African languages, using 22 datasets covering tasks like
  classification, question answering, and summarization. The research evaluates models
  like GPT-4, PaLM2, and Llama2, comparing their performance on multilingual tasks.
---

# MEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks

## Quick Facts
- arXiv ID: 2311.07463
- Source URL: https://arxiv.org/abs/2311.07463
- Reference count: 15
- Key outcome: GPT-4 outperforms smaller models like Llama2 on multilingual tasks, especially low-resource African languages, but dataset contamination significantly impacts evaluation reliability.

## Executive Summary
This study benchmarks large language models across 81 languages using 22 datasets covering tasks like classification, question answering, and summarization. The research evaluates models including GPT-4, PaLM2, and Llama2, with a focus on low-resource languages. Results show that larger models outperform smaller ones, particularly on low-resource languages, with GPT-4 generally achieving the best performance. However, dataset contamination is identified as a significant issue affecting the reliability of evaluations, highlighting the need for improved multilingual evaluation methods and models.

## Method Summary
The study evaluates multiple LLMs (GPT-3.5-Turbo, GPT4, PaLM2, Llama2 variants, and LLaVA-v1.5) on 22 multilingual datasets covering 81 languages. The approach uses zero-shot monolingual prompting with task-specific instructions and templates, incorporating few-shot examples in the target language where available. Models are compared across various tasks including classification, question answering, summarization, and multimodal reasoning. Performance is measured using standard metrics like accuracy, BLEU, chrF++, and F1 scores.

## Key Results
- GPT-4 and PaLM2 outperform Llama models, especially on low-resource languages
- Dataset contamination is identified as a critical issue affecting evaluation reliability
- LLaVA-v1.5 shows poor performance on reasoning tasks despite being trained on image-caption pairs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GPT-4 outperforms smaller models like Llama2 on multilingual tasks, especially low-resource African languages.
- **Mechanism:** Larger parameter count and pretraining diversity allow GPT-4 to generalize better across languages, capturing patterns in underrepresented languages.
- **Core assumption:** Pretraining data includes sufficient low-resource language examples.
- **Evidence anchors:**
  - [abstract]: "larger models such as GPT-4, Gemini-Pro and PaLM2 outperform smaller models on various tasks, notably on low-resource languages, with GPT-4 outperforming PaLM2 and Gemini-Pro on more datasets."
  - [section]: "GPT4 and PaLM2 outperform the Llama models, particularly on low-resource languages."
  - [corpus]: Weak - no explicit data provenance in corpus.
- **Break condition:** If pretraining corpus lacks low-resource language data, generalization fails.

### Mechanism 2
- **Claim:** Multimodal LLaVA-v1.5 shows poor performance on reasoning tasks despite being trained on image-caption pairs.
- **Mechanism:** Multimodal reasoning requires joint reasoning over vision and language; mere image-caption pairing is insufficient for tasks like MaRVL.
- **Core assumption:** Visual instruction tuning transfers to cross-modal reasoning.
- **Evidence anchors:**
  - [section]: "Llava model performs well on the image captioning task for some high-resource languages, but performs poorly on the reasoning dataset."
  - [section]: "F1 scores are low overall, with the lowest score on Tamil."
  - [corpus]: Weak - no details on MaRVL data diversity.
- **Break condition:** If reasoning requires more complex reasoning beyond image-caption grounding.

### Mechanism 3
- **Claim:** Dataset contamination significantly impacts benchmark reliability, especially for GPT-4.
- **Mechanism:** Training data overlaps with test sets, inflating performance metrics.
- **Core assumption:** Overlap between pretraining corpus and evaluation datasets.
- **Evidence anchors:**
  - [abstract]: "However, issues such as data contamination must be addressed to obtain an accurate assessment of LLM performance on non-English languages."
  - [section]: "Dataset contamination is a critical issue that affects English and non-English language benchmarking studies."
  - [corpus]: Weak - no direct contamination detection method described.
- **Break condition:** If contamination detection and filtering are not performed.

## Foundational Learning

- **Concept:** Multilingual evaluation design
  - **Why needed here:** To fairly compare models across 81 languages, including low-resource ones, ensuring representative benchmarking.
  - **Quick check question:** What metrics are used to ensure evaluation fairness across languages with vastly different resource availability?

- **Concept:** Prompt engineering and few-shot learning
  - **Why needed here:** Different prompting strategies (monolingual vs. translate-test) affect performance; understanding their impact is crucial for valid comparisons.
  - **Quick check question:** How does the choice of prompt language and few-shot examples influence cross-lingual transfer?

- **Concept:** Multimodal task formulation
  - **Why needed here:** Tasks like MaRVL and XM-3600 require joint reasoning over vision and language; understanding task structure is essential for model development.
  - **Quick check question:** What are the key differences in model requirements between image captioning and visual reasoning tasks?

## Architecture Onboarding

- **Component map:** Dataset ingestion → Prompt generation → Model inference (GPT-4, PaLM2, Llama2, LLaVA) → Metric computation (accuracy, BLEU, chrF++, F1) → Contamination analysis
- **Critical path:** Dataset loading → Prompt formatting → Model API call (or local inference) → Result aggregation → Contamination check
- **Design tradeoffs:** API-based models (GPT-4, PaLM2) offer ease of use but limited control; local models (Llama2, LLaVA) provide flexibility but require more resources
- **Failure signatures:** Zero or near-zero scores across languages (possible contamination or model incompatibility), high variance in low-resource languages (data scarcity), poor multimodal reasoning (insufficient visual instruction tuning)
- **First 3 experiments:**
  1. Run a small multilingual dataset (e.g., XNLI) on all models to verify API connectivity and prompt formatting
  2. Compare monolingual vs. translate-test prompting on a high-resource language pair to validate prompt strategy impact
  3. Test a multimodal dataset (e.g., XM-3600) on LLaVA to confirm multimodal pipeline functionality

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can we effectively detect and prevent dataset contamination in multilingual LLM evaluation?
- **Basis in paper:** [explicit] The paper discusses dataset contamination as a significant issue affecting the reliability of evaluations, particularly in multilingual contexts, and highlights the need for approaches to detect and handle contamination.
- **Why unresolved:** Dataset contamination is a complex issue that requires developing new methods to identify and mitigate its effects on evaluation reliability, which is not yet fully addressed in the research.
- **What evidence would resolve it:** Development and validation of robust methods for detecting dataset contamination, along with strategies to mitigate its impact on LLM evaluation, would provide evidence to resolve this question.

### Open Question 2
- **Question:** What are the key factors affecting the performance of multilingual LLMs across different languages and tasks?
- **Basis in paper:** [inferred] The paper suggests that factors such as pre-training data quantity, fine-tuning data, tokenizer fertility, and other relevant factors affect LLM performance, but does not extensively study these aspects.
- **Why unresolved:** The influence of these factors on multilingual LLM performance is complex and requires further empirical investigation to understand their impact across diverse languages and tasks.
- **What evidence would resolve it:** Comprehensive studies examining the effects of pre-training and fine-tuning data, tokenizer fertility, and other factors on LLM performance across multiple languages and tasks would help resolve this question.

### Open Question 3
- **Question:** How can we develop better metrics for evaluating generative models in multilingual contexts?
- **Basis in paper:** [explicit] The paper highlights the insufficiency of standard metrics like ROUGE-L for evaluating generative models, especially in multilingual settings, indicating a need for improved evaluation metrics.
- **Why unresolved:** Existing metrics may not capture the nuances of generative model performance across diverse languages, necessitating the development of more comprehensive and language-agnostic evaluation methods.
- **What evidence would resolve it:** Creation and validation of new evaluation metrics that accurately assess generative model performance across multiple languages and tasks would provide evidence to resolve this question.

## Limitations
- Dataset contamination assessment lacks detailed methodology for contamination detection
- Low-resource language evaluation depth may be limited by data availability
- Multimodal reasoning capability assessment doesn't fully explore architectural vs. training data limitations

## Confidence
- **GPT-4 vs smaller models performance:** High confidence - claims are supported by direct comparisons across multiple datasets with clear performance gaps
- **Low-resource language performance:** Medium confidence - supported by results but limited by data availability and contamination concerns
- **Dataset contamination impact:** Medium confidence - identified as a problem but without detailed contamination detection methodology
- **Multimodal reasoning assessment:** Medium confidence - results are clear but mechanism attribution is speculative

## Next Checks
1. Implement and run a systematic contamination detection pipeline (e.g., n-gram overlap analysis, embedding similarity checks) on all evaluation datasets to quantify contamination levels for each model
2. For each low-resource language, analyze the number of training examples available in each dataset and correlate with model performance to distinguish between data scarcity effects and true model capability gaps
3. Design a controlled experiment where LLaVA-v1.5 is tested on tasks requiring only visual grounding (not complex reasoning) to determine whether its multimodal architecture or its training data is the limiting factor for MaRVL performance