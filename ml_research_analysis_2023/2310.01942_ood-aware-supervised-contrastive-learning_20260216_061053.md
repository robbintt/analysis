---
ver: rpa2
title: OOD Aware Supervised Contrastive Learning
arxiv_id: '2310.01942'
source_url: https://arxiv.org/abs/2310.01942
tags:
- features
- data
- auroc
- aupr
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes an OOD-aware supervised contrastive learning
  method that improves OOD detection performance by extending SupCon loss with two
  additional contrastive terms. The method leverages powerful representations learned
  with supervised contrastive training and introduces class prototypes to replace
  softmax classifiers.
---

# OOD Aware Supervised Contrastive Learning

## Quick Facts
- arXiv ID: 2310.01942
- Source URL: https://arxiv.org/abs/2310.01942
- Authors: 
- Reference count: 40
- Primary result: Achieves 12.01% FPR on CIFAR-10 and 50.97% FPR on CIFAR-100 with real auxiliary OOD data

## Executive Summary
This work introduces OOD-aware Prototypical Supervised Contrastive Learning (OPSupCon), a method that improves out-of-distribution detection by extending standard supervised contrastive learning with two additional contrastive terms. The approach replaces softmax classifiers with prototype-based classification and leverages either auxiliary or synthesized OOD data to create stronger separation between in-distribution and out-of-distribution features. Extensive experiments demonstrate state-of-the-art OOD detection performance with significant reductions in false positive rates across multiple benchmark datasets.

## Method Summary
The method extends SupCon loss with two additional contrastive terms: one pushing OOD features away from ID features at the projection head level, and another pulling OOD features away from class prototypes at the encoder level. It uses prototype-based classification instead of softmax, where class prototypes are learned through a tightness term that penalizes features falling far from others of the same class. When auxiliary OOD data is unavailable, the method generates pseudo-OOD features through feature mixing techniques like Mixup applied at the feature level, creating samples that span the space between different class clusters.

## Key Results
- Achieves 12.01% FPR on CIFAR-10 with real auxiliary OOD data
- Achieves 50.97% FPR on CIFAR-100 with real auxiliary OOD data
- Outperforms energy-based fine-tuning and other contrastive learning approaches on multiple benchmarks
- Significant reduction in false positive rates compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The OOD-aware contrastive loss improves separation between ID and OOD features in the embedding space.
- Mechanism: Two additional contrastive terms push OOD features away from both ID features and class prototypes while pulling ID features toward their corresponding prototypes. This creates a decision boundary that is more robust to out-of-distribution samples.
- Core assumption: The auxiliary or synthesized OOD data is representative enough of realistic OOD cases to create meaningful separation.
- Evidence anchors:
  - [abstract]: "We extend SupCon loss with two additional contrast terms. The first term pushes auxiliary OOD representations away from ID representations without imposing any constraints on similarities among auxiliary data."
  - [section 3.1]: "The first loss term is applied at the projection head, similar to SupCon, but targets minimizing the pairwise similarities of ID and OOD features."
  - [corpus]: Weak correlation (FMR 0.356) with related OOD detection methods suggests novelty but limited direct evidence for this specific mechanism.

### Mechanism 2
- Claim: The prototype-based classifier reduces overconfidence on OOD data compared to softmax classifiers.
- Mechanism: By learning class prototypes in the embedding space and using nearest prototype classification, the model naturally assigns lower confidence scores to OOD samples that are far from all prototypes.
- Core assumption: Features of the same class are more similar to each other than to features of other classes, making prototype learning effective.
- Evidence anchors:
  - [abstract]: "We replace the CE-based training with learning randomly initialized class prototypes θ using a tightness term that penalizes features f falling far from others of the same class."
  - [section 3.1]: "With that assumption, we use a nearest prototype classifier i.e., assigning a test sample to the class of the nearest prototype in the feature space."
  - [corpus]: Limited evidence (FMR 0.356) for prototype-based approaches in OOD detection literature.

### Mechanism 3
- Claim: The pseudo-OOD generation technique creates realistic OOD-like features by interpolating between ID samples of different classes.
- Mechanism: Mixup is applied at the feature level, generating samples in the space between different class clusters, which mimics realistic OOD scenarios.
- Core assumption: OOD data commonly appears in between class clusters in the embedding space.
- Evidence anchors:
  - [section 3.2]: "Based on the observation that OOD data are commonly projected in between class categories in the embedding space... our idea is to generate features spanning this space between ID samples of different classes."
  - [corpus]: No direct evidence for this specific feature mixing approach in the related papers corpus.

## Foundational Learning

- Concept: Supervised Contrastive Learning (SupCon)
  - Why needed here: SupCon provides a strong representation learning foundation that the OOD-aware modifications build upon.
  - Quick check question: What is the key difference between SupCon loss and standard cross-entropy loss in terms of how they handle sample relationships?

- Concept: Prototype-based classification
  - Why needed here: Prototypes provide a natural way to assign lower confidence to OOD samples by measuring distance to the nearest class prototype.
  - Quick check question: How does the nearest prototype classifier differ from softmax classification in terms of decision boundary shape?

- Concept: Contrastive learning with negative samples
  - Why needed here: The additional contrastive terms rely on pushing apart dissimilar samples to create separation between ID and OOD features.
  - Quick check question: In the context of OOD detection, what role do the negative samples play in the contrastive loss formulation?

## Architecture Onboarding

- Component map: Input image -> Encoder (ResNet18/50) -> Encoder features -> Projection head (MLP) -> Projection features -> Prototypes -> Classification decision

- Critical path:
  1. Encode input image → get encoder features
  2. Project encoder features → get projection head features
  3. Apply SupCon loss on projection head features
  4. Apply tightness loss to pull encoder features toward class prototypes
  5. Apply OOD contrastive losses to push OOD features away from ID features and prototypes
  6. Update encoder, projection head, and prototypes

- Design tradeoffs:
  - Using prototypes vs. softmax classifier: Prototypes provide natural OOD detection but may be less discriminative for ID classification
  - Real vs. pseudo OOD data: Real OOD data provides better representation but may be unavailable; pseudo OOD is always available but may be less realistic
  - Training epochs: More epochs improve representation quality but may cause pseudo OOD to become too similar to ID data

- Failure signatures:
  - ID classification accuracy drops significantly: Indicates prototypes are not learning useful representations
  - OOD detection performance doesn't improve with auxiliary OOD: Suggests contrastive terms are not effective
  - Pseudo OOD generation creates features too close to ID clusters: Indicates λ sampling needs adjustment

- First 3 experiments:
  1. Train with only SupCon and prototype losses (no OOD terms) to establish baseline
  2. Add real OOD auxiliary data with contrastive terms to measure improvement
  3. Switch to pseudo OOD generation and compare performance to real OOD baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform when using other backbone architectures beyond ResNet18 and ResNet50?
- Basis in paper: [explicit] The paper mentions that the method was evaluated with ResNet18 and ResNet50 backbones, but does not explore other architectures.
- Why unresolved: The paper only provides results for two specific backbone architectures, leaving the performance on other architectures unknown.
- What evidence would resolve it: Conducting experiments with various other backbone architectures (e.g., EfficientNet, MobileNet) and comparing the OOD detection performance to establish the method's generalizability.

### Open Question 2
- Question: How does the method perform on larger-scale datasets with more classes?
- Basis in paper: [inferred] The paper evaluates the method on CIFAR-10 and CIFAR-100 datasets, which have 10 and 100 classes respectively. It does not test the method on larger-scale datasets with thousands of classes.
- Why unresolved: The scalability of the method to larger datasets with more classes is not explored, leaving its performance in such scenarios unknown.
- What evidence would resolve it: Evaluating the method on large-scale datasets like ImageNet-1k (1000 classes) or ImageNet-22k (22,000 classes) and comparing the OOD detection performance to establish the method's scalability.

### Open Question 3
- Question: How does the method perform when using different scoring functions for OOD detection?
- Basis in paper: [explicit] The paper mentions that Maximum Logit is used as the scoring function, but does not explore the use of other scoring functions like Maximum Softmax Probability or Energy Score.
- Why unresolved: The paper only uses one scoring function, leaving the performance of the method with other scoring functions unknown.
- What evidence would resolve it: Conducting experiments using different scoring functions (e.g., Maximum Softmax Probability, Energy Score) and comparing the OOD detection performance to establish the optimal scoring function for the method.

## Limitations

- The paper lacks detailed implementation specifications for the feature mixing technique, particularly the sampling strategy for the mixup parameter λ and how "most similar different-class features" are selected.
- Evaluation focuses heavily on FPR metrics without providing calibration curves or uncertainty estimates to assess the reliability of confidence scores.
- The effectiveness of pseudo-OOD generation is not thoroughly validated, as results are only shown in supplementary materials with limited analysis.

## Confidence

**High Confidence**: The core conceptual framework of extending SupCon with OOD contrastive terms is well-founded and logically coherent. The mechanism of using prototypes for OOD detection is theoretically sound given existing prototype-based approaches.

**Medium Confidence**: The empirical results showing 12.01% FPR on CIFAR-10 and 50.97% FPR on CIFAR-100 are compelling, but the lack of statistical significance testing and limited ablation studies reduces confidence in the magnitude of improvement.

**Low Confidence**: The effectiveness of the pseudo-OOD generation technique is the most uncertain component, as the paper provides minimal detail on implementation and only shows results in supplementary materials.

## Next Checks

1. **Feature Space Visualization**: Generate t-SNE plots of the embedding space for both ID and OOD samples to visually verify that the contrastive terms are creating the intended separation between classes and pushing OOD samples away from ID clusters.

2. **Prototype Stability Analysis**: Track prototype movement during training and measure their final distances from ID cluster centroids. This would validate whether prototypes are learning meaningful class representations or drifting toward outlier regions.

3. **Hyperparameter Sensitivity**: Conduct experiments varying the weighting factors (alpha, gamma) for the OOD contrastive losses to determine if the reported performance is robust across different hyperparameter settings or specific to the chosen values.