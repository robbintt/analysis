---
ver: rpa2
title: Manifold Learning by Mixture Models of VAEs for Inverse Problems
arxiv_id: '2303.15244'
source_url: https://arxiv.org/abs/2303.15244
tags:
- manifold
- inverse
- gradient
- arxiv
- chart
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces mixture models of variational autoencoders
  (VAEs) to represent manifolds of arbitrary topology for inverse problems. Unlike
  traditional VAEs which assume a global parameterization, the proposed method learns
  multiple local charts of the manifold using a mixture model of VAEs.
---

# Manifold Learning by Mixture Models of VAEs for Inverse Problems

## Quick Facts
- **arXiv ID**: 2303.15244
- **Source URL**: https://arxiv.org/abs/2303.15244
- **Reference count**: 40
- **Key outcome**: Mixture models of VAEs improve inverse problem reconstruction, achieving 1.12 dB average PSNR improvement for EIT example

## Executive Summary
This paper introduces mixture models of variational autoencoders (VAEs) to represent manifolds of arbitrary topology for inverse problems. Unlike traditional VAEs which assume a global parameterization, the proposed method learns multiple local charts of the manifold using a mixture model of VAEs. The authors derive a loss function for maximum likelihood estimation of model weights and propose a Riemannian gradient descent algorithm for optimization on the learned manifold. Numerical experiments on low-dimensional toy examples and real-world applications demonstrate the effectiveness of the method.

## Method Summary
The method trains a mixture of K VAEs, where each decoder-encoder pair represents one chart of the data manifold. The loss function combines ELBOs weighted by mixing probabilities, approximated using Bayes' theorem. After training, an overlap procedure refines chart boundaries. For inverse problems, Riemannian gradient descent optimizes on the learned manifold using two types of retractions - one based on projection and another using chart information. The approach is evaluated on synthetic manifolds and real applications including deblurring and electrical impedance tomography.

## Key Results
- Mixture of VAEs successfully learns disconnected manifolds (e.g., two circles) where single VAEs fail
- Riemannian gradient descent on learned manifold provides effective solution for inverse problems
- Average PSNR improvement of 1.12 dB for EIT example compared to single-generator approaches
- Method works for both low-dimensional toy examples and real-world inverse problems

## Why This Works (Mechanism)

### Mechanism 1
The mixture of VAEs represents manifolds of arbitrary topology by using multiple local charts, each modeled by a separate VAE. Each decoder-encoder pair in the mixture acts as a chart that parameterizes a subset of the manifold. The mixture model allows disconnected components and holes to be modeled by separate charts.

### Mechanism 2
The loss function approximates the negative log-likelihood of the mixture model by combining ELBOs weighted by mixing probabilities. Using Bayes' theorem, the probability that a data point belongs to a specific chart is computed, then the ELBO of that chart is weighted by this probability.

### Mechanism 3
The Riemannian gradient descent scheme enables optimization on the learned manifold by projecting the Euclidean gradient onto the tangent space. For each chart, the gradient is projected onto the tangent space using the Jacobian of the decoder, then a retraction moves along this projected direction.

## Foundational Learning

- **Concept**: Variational Autoencoders (VAEs) and Evidence Lower Bound (ELBO)
  - Why needed here: The paper builds on VAEs as the basic building block for each chart in the mixture model.
  - Quick check question: What is the ELBO and why is it used instead of the true log-likelihood in VAE training?

- **Concept**: Manifolds and Charts
  - Why needed here: The paper's core idea is to represent a data manifold using multiple charts, each parameterized by a VAE.
  - Quick check question: What is a chart in the context of manifold theory, and how does it relate to the decoder-encoder pair in this paper?

- **Concept**: Riemannian Geometry and Gradient Descent on Manifolds
  - Why needed here: The optimization algorithm for inverse problems operates on the learned manifold, requiring Riemannian gradient descent.
  - Quick check question: How does the Riemannian gradient differ from the Euclidean gradient, and why is it necessary for optimization on manifolds?

## Architecture Onboarding

- **Component map**: Mixture of K VAEs (decoder Dk, encoder Ek, normalizing flow Tk) -> Loss function (ELBOs weighted by mixing probabilities) -> Riemannian gradient descent (projection-based and chart-based retractions) -> Overlap post-processing

- **Critical path**: 1. Train mixture of VAEs using loss function L(Θ) 2. Apply overlap procedure to refine chart boundaries 3. Use Riemannian gradient descent for inverse problem optimization

- **Design tradeoffs**: More charts (higher K) improve topology coverage but increase computational cost; narrower charts (smaller latent dimension) improve local parameterization but may require more charts; choice of retraction affects numerical stability vs computational efficiency

- **Failure signatures**: Reconstructions contain artifacts or unrealistic structures; gradient descent trajectories diverge or get stuck; ELBO values plateau early during training

- **First 3 experiments**: 1. Train mixture of VAEs on two disconnected circles dataset; visualize learned charts 2. Apply Riemannian gradient descent to minimize distance to a point on the learned manifold 3. Compare reconstruction quality of inverse problems using one vs multiple generators on simple test cases

## Open Questions the Paper Calls Out

### Open Question 1
How can the dimension of the learned manifold be automatically determined from the data without requiring prior knowledge? The authors note that several methods exist in the literature to estimate manifold dimension, but do not implement or compare any with their approach.

### Open Question 2
How would the proposed method perform on real-world image datasets rather than synthetic examples with clear manifold structures? The authors explicitly state that application to real-world data is left to future research.

### Open Question 3
How does the number of charts (K) affect the performance and what is the optimal way to determine K? The authors use different numbers of charts for different manifolds but do not provide a systematic method for choosing K.

## Limitations

- The approach assumes finite mixture models can represent manifolds requiring potentially infinite charts, without theoretical bounds on the number of charts needed
- ELBO approximation may introduce significant bias when individual VAE components have poor encoder-decoder fits
- Riemannian gradient descent relies on Jacobian approximations that may become numerically unstable for high-dimensional latent spaces

## Confidence

- **High confidence** in the core mathematical framework and algorithmic structure
- **Medium confidence** in the experimental results and PSNR improvements
- **Low confidence** in the generalizability to highly complex real-world manifolds without extensive tuning

## Next Checks

1. **Topology Coverage Analysis**: Systematically vary the number of mixture components K and measure reconstruction quality on increasingly complex manifold topologies.

2. **ELBO Approximation Error Quantification**: Compute the gap between true log-likelihood and ELBO approximation during training, and analyze correlation with reconstruction quality.

3. **Riemannian Gradient Stability Test**: Implement monitoring of the condition number of (JTJ)⁻¹ during optimization and test algorithm behavior with ill-conditioned Jacobians.