---
ver: rpa2
title: Data Selection for Language Models via Importance Resampling
arxiv_id: '2302.03169'
source_url: https://arxiv.org/abs/2302.03169
tags:
- data
- classi
- dsir
- target
- importance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of selecting a suitable pretraining
  dataset for language models, both for general-domain (e.g., GPT-3) and domain-specific
  (e.g., Codex) models. The authors formalize this as selecting a subset of a large
  raw unlabeled dataset to match a desired target distribution, given unlabeled target
  samples.
---

# Data Selection for Language Models via Importance Resampling

## Quick Facts
- arXiv ID: 2302.03169
- Source URL: https://arxiv.org/abs/2302.03169
- Reference count: 40
- Primary result: DSIR improves over random selection and heuristic filtering by 2-2.5% on GLUE when pretraining general-domain models

## Executive Summary
This paper addresses the challenge of selecting optimal pretraining datasets for language models when given a large unlabeled corpus and a target domain. The authors propose Data Selection with Importance Resampling (DSIR), which uses importance resampling in a reduced feature space to efficiently select data that matches a target distribution. By projecting text onto hashed n-gram features, DSIR makes high-dimensional data selection tractable while achieving strong performance improvements on both general-domain and domain-specific pretraining tasks.

## Method Summary
DSIR extends importance resampling to high-dimensional text by working in a reduced feature space. The method uses hashed n-gram features (10k dimensions) to extract tractable representations of raw and target distributions. A generative bag-of-n-grams model estimates importance weights as likelihood ratios in this space, and data is selected via Gumbel top-k resampling according to these weights. The approach is efficient enough to select 100M documents from The Pile in 4.5 hours, and the authors define KL reduction as a metric that correlates strongly (r=0.82) with downstream task performance.

## Key Results
- DSIR improves over random selection and heuristic filtering baselines by 2-2.5% on GLUE benchmark for general-domain models
- When performing continued pretraining toward specific domains, DSIR performs comparably to expert-curated data across 8 target distributions
- KL reduction in n-gram feature space highly correlates with average downstream accuracy (r=0.82), providing a reliable data quality metric

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Importance resampling in a reduced feature space enables tractable selection from high-dimensional text.
- Mechanism: By projecting raw text onto hashed n-gram features, the algorithm reduces dimensionality from exponential to fixed 10k buckets, making density estimation feasible.
- Core assumption: Target distribution's n-gram statistics are well-captured by hashing collisions and the fixed 10k-dimensional space.
- Evidence anchors: [abstract] "DSIR estimates importance weights in a reduced feature space for tractability"; [section] "we work in a reduced feature space which allows us to efficiently estimate a density over a featurization."
- Break Condition: If hashing introduces too much collision noise, importance weights become unreliable.

### Mechanism 2
- Claim: KL reduction in n-gram space correlates strongly with downstream task performance.
- Mechanism: Selecting data that minimizes KL divergence to target in n-gram feature space indirectly optimizes for transfer to tasks depending on similar n-gram patterns.
- Core assumption: Downstream performance depends largely on matching n-gram statistics between pretraining and target data.
- Evidence anchors: [abstract] "KL reduction... has high correlation with average downstream accuracy (r=0.82)"; [section] "strong correlation (Pearson r = 0.89) between KL reduction and average downstream performance."
- Break Condition: If downstream tasks rely on higher-order linguistic structures poorly represented by n-grams, KL reduction won't predict performance.

### Mechanism 3
- Claim: Generative importance weight estimation outperforms discriminative approaches for DSIR.
- Mechanism: Bag-of-ngrams generative models separately estimate raw and target distributions, producing better-calibrated likelihood ratios than discriminative classifier probabilities.
- Core assumption: Generative approach yields better-calibrated likelihood ratios for importance resampling than discriminative classifier probabilities.
- Evidence anchors: [section] "We consider a discriminative version of DSIR... This underperforms DSIR by 0.7%"; [section] "generative approach requires less tuning and could also be better when the number of target examples is small."
- Break Condition: If target dataset is very large and discriminative classifier can be well-calibrated, performance gap may disappear.

## Foundational Learning

- Concept: Importance resampling (importance sampling)
  - Why needed here: Core algorithmic framework for selecting data according to target distribution without knowing it directly
  - Quick check question: What is the mathematical condition under which importance resampling produces unbiased samples from the target distribution?

- Concept: KL divergence and its reduction
  - Why needed here: Metric used to measure how much selected data matches target distribution in feature space
  - Quick check question: If KL(target||selected) decreases by X compared to random selection, what does that say about the overlap between the two distributions?

- Concept: Hashing for dimensionality reduction
  - Why needed here: Enables tractable n-gram feature extraction from massive text corpora by mapping to fixed-size vectors
  - Quick check question: What is the trade-off introduced by hash collisions in the n-gram feature space?

## Architecture Onboarding

- Component map: Raw data ingestion → Preprocessing (tokenization, length filtering) → Hashed n-gram feature extraction → Generative model training (target and raw) → Importance weight computation → Gumbel top-k resampling → Output selected subset

- Critical path: Feature extraction → Model training → Weight computation → Resampling. Any delay in these stages blocks the pipeline.

- Design tradeoffs: Fixed 10k-dimensional hashed n-grams vs. larger vocabularies (accuracy vs. memory/scale). Generative vs. discriminative importance estimation (simplicity vs. potential accuracy).

- Failure signatures: If selected data is too homogeneous, likely importance weights are too peaked; if too diverse, weights may be too flat. Poor downstream performance despite high KL reduction suggests n-gram space misses relevant features.

- First 3 experiments:
  1. Run DSIR with small subset of The Pile (1M examples) and simple target (Wikipedia) to verify end-to-end flow and timing
  2. Compare KL reduction vs. random selection on held-out validation set to confirm metric behavior
  3. Fine-tune small language model on DSIR-selected data and measure GLUE performance to confirm downstream impact

## Open Questions the Paper Calls Out

- How do different choices of feature space (beyond n-grams) impact the effectiveness of importance resampling for data selection? The authors note that while n-grams are effective, they "don't provide the full story" and suggest exploring other feature spaces.

- What is the optimal target distribution for pretraining data selection across diverse downstream tasks? The authors state that "the optimal target distribution should ideally take the performance of the base model into account" and that "what the right target distribution is for the best downstream performance across many tasks is left for future work."

- How can we effectively handle the computational challenges of importance resampling on extremely large datasets like The Pile? While the paper demonstrates efficiency with n-gram features, it doesn't explore other potential optimization strategies or address scalability challenges for even larger datasets.

## Limitations
- The paper relies heavily on hashed n-gram features as a proxy for semantic content, but there is limited empirical validation that hash collisions do not distort the target distribution across diverse domains in The Pile.
- The claimed strong correlation (r=0.82) between KL reduction and downstream accuracy is based on experiments with 8 downstream datasets, which may not be representative of broader NLP tasks.
- The comparison with manual curation is based on expert-selected data from 8 target distributions, but the selection criteria for these distributions and the specific manual curation process are not detailed.

## Confidence
- High confidence: The basic DSIR algorithm framework and its implementation using hashed n-gram features is technically sound and reproducible.
- Medium confidence: The performance improvements over random selection (2-2.5% on GLUE) are measurable but the causal mechanism linking KL reduction to these improvements is not fully established.
- Medium confidence: The claim that DSIR performs comparably to expert curation is supported by the experimental results, but the lack of transparency about the curation process introduces uncertainty.

## Next Checks
1. Validate hash collision behavior: Conduct ablation study with different hash table sizes (1k, 10k, 100k) and measure impact on both KL reduction and downstream performance.
2. Test correlation robustness: Apply DSIR to new set of diverse downstream tasks and recompute correlation between KL reduction and average downstream accuracy.
3. Analyze feature space sufficiency: Replace hashed n-grams with simple pretrained embedding-based feature space and run DSIR to determine if performance gains generalize.