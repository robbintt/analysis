---
ver: rpa2
title: Ensemble Value Functions for Efficient Exploration in Multi-Agent Reinforcement
  Learning
arxiv_id: '2302.03439'
source_url: https://arxiv.org/abs/2302.03439
tags:
- value
- agents
- ensemble
- exploration
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EMAX, a framework that uses ensemble value
  functions to improve exploration and training stability in value-based multi-agent
  reinforcement learning. EMAX guides agents to explore cooperation-requiring state-action
  pairs by leveraging disagreement across ensemble value estimates via an upper confidence
  bound (UCB) policy.
---

# Ensemble Value Functions for Efficient Exploration in Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2302.03439
- Source URL: https://arxiv.org/abs/2302.03439
- Reference count: 40
- Improves sample efficiency and final returns by 54%, 55%, and 844% for independent DQN, VDN, and QMIX respectively

## Executive Summary
This paper introduces EMAX, a framework that uses ensemble value functions to improve exploration and training stability in value-based multi-agent reinforcement learning. EMAX guides agents to explore cooperation-requiring state-action pairs by leveraging disagreement across ensemble value estimates via an upper confidence bound (UCB) policy. It also uses averaged value estimates across the ensemble to reduce variance in target values, improving gradient stability. Experiments with 21 tasks across four environments show EMAX consistently improves performance over baseline algorithms.

## Method Summary
EMAX trains an ensemble of K value functions per agent, using bootstrap sampling from experience replay to maintain diversity. The framework employs UCB exploration based on ensemble disagreement (standard deviation) to target cooperation-requiring states. Target values are computed as the average across all ensemble estimates, eliminating the need for target networks. During evaluation, a majority vote across ensemble greedy actions reduces miscoordination. The method is compatible with value decomposition approaches like VDN and QMIX, and can be applied to any value-based MARL algorithm.

## Key Results
- EMAX improves sample efficiency by 54%, 55%, and 844% for independent DQN, VDN, and QMIX respectively across 21 tasks
- Training stability is enhanced through reduced variance in target estimates compared to target networks
- Evaluation performance benefits from majority vote policy reducing miscoordination across ensemble predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: UCB exploration guided by ensemble disagreement focuses exploration on cooperation-requiring state-action pairs
- Mechanism: Disagreement across ensemble value estimates is high in states requiring cooperation because agents receive highly variant rewards (sometimes fail, sometimes succeed), which guides UCB policy toward these parts of the environment
- Core assumption: Variance in received rewards correlates with disagreement across ensemble value functions
- Evidence anchors: [abstract] "The uncertainty of value estimates across the ensemble is used in a UCB policy to guide the exploration of agents to parts of the environment which require cooperation"
- Break condition: If value functions converge too quickly to similar estimates, disagreement disappears and exploration loses its targeting capability

### Mechanism 2
- Claim: Average value estimates across ensemble reduce variance in target values and stabilize training
- Mechanism: Computing target values as the average across all ensemble value estimates eliminates the need for target networks and produces lower-variance target estimates
- Core assumption: Averaging across ensemble models reduces variance in target estimates, leading to more stable gradients
- Evidence anchors: [abstract] "Average value estimates across the ensemble serve as target values. These targets exhibit lower variance compared to commonly applied target networks"
- Break condition: If ensemble models are not sufficiently diverse, averaging may not reduce variance and could even increase bias

### Mechanism 3
- Claim: Majority vote across ensemble greedy actions reduces miscoordination during evaluation
- Mechanism: During evaluation, agent selects action based on majority vote across greedy actions of all ensemble models
- Core assumption: Individual value functions can make errors in value estimation, but majority voting across ensemble reduces impact of these errors
- Evidence anchors: [abstract] "During evaluation, EMAX selects actions following a majority vote across the ensemble to reduce the likelihood of miscoordination"
- Break condition: If ensemble models are highly correlated in their errors, majority voting may not provide robustness benefits

## Foundational Learning

- Concept: Upper Confidence Bound (UCB) policy
  - Why needed here: UCB balances exploration and exploitation by adding uncertainty-weighted bonus to value estimates
  - Quick check question: What does the uncertainty coefficient β control in UCB policy and how does it affect exploration behavior?

- Concept: Value decomposition in multi-agent RL
  - Why needed here: Value decomposition methods like VDN and QMIX aggregate individual agent value functions into a centralized value function
  - Quick check question: How does VDN's linear decomposition differ from QMIX's monotonic mixing function in terms of representational capacity?

- Concept: Ensemble learning and bootstrap sampling
  - Why needed here: Ensemble diversity is maintained through separate random initializations and bootstrap sampling from experience replay
  - Quick check question: Why is bootstrap sampling important for maintaining diversity in ensemble value functions?

## Architecture Onboarding

- Component map: Ensemble of K value functions per agent → UCB exploration policy → Value decomposition (optional) → Target value computation via ensemble averaging → Majority vote evaluation policy
- Critical path: Experience collection → Ensemble training with bootstrap samples → UCB action selection → Value aggregation → Target computation → Policy evaluation
- Design tradeoffs: Larger ensembles provide better exploration and stability but increase computational cost; simpler decomposition (VDN) is more sample efficient but less expressive than complex mixing (QMIX)
- Failure signatures: Low ensemble disagreement across all states (exploration not targeted), high variance in target values (stability issues), majority voting frequently splits (evaluation coordination problems)
- First 3 experiments:
  1. Implement EMAX with K=2 ensemble size on simple matrix game to verify UCB exploration focuses on cooperation-requiring actions
  2. Compare training stability metrics (gradient norm variance) between EMAX and baseline with target networks on cooperative task
  3. Evaluate evaluation policy performance with majority voting vs. single model greedy policy on coordination task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of EMAX scale with larger ensemble sizes beyond K=8, and what is the optimal ensemble size for different MARL tasks?
- Basis in paper: [explicit] The paper mentions that "the benefits of larger ensemble models saturate at K=5" and that "larger ensemble value functions may require more data to train, thus leading to diminishing benefits."
- Why unresolved: The paper only experiments with ensemble sizes up to K=8 and does not explore the effects of significantly larger ensembles
- What evidence would resolve it: Conducting experiments with ensemble sizes larger than K=8 in various MARL tasks would help determine if there is an optimal ensemble size

### Open Question 2
- Question: How does EMAX perform in competitive and mixed cooperative-competitive MARL environments?
- Basis in paper: [inferred] The paper focuses on cooperative MARL environments and does not evaluate EMAX in competitive or mixed settings
- Why unresolved: The paper does not provide any experimental results or theoretical analysis of how EMAX would perform in non-cooperative MARL settings
- What evidence would resolve it: Evaluating EMAX in competitive and mixed cooperative-competitive MARL environments would provide insights into its performance and potential adaptations needed

### Open Question 3
- Question: How does the choice of UCB uncertainty coefficient β affect the exploration-exploitation trade-off in EMAX?
- Basis in paper: [explicit] The paper mentions that "UCB uncertainty coefficient hyperparameter β > 0 chosen in consideration of the scale of rewards and the amount of exploration required for a task"
- Why unresolved: The paper does not provide a systematic analysis of how different β values affect the performance of EMAX across various MARL tasks
- What evidence would resolve it: Conducting a sensitivity analysis of EMAX's performance with respect to different β values in a diverse set of MARL tasks would help identify the optimal range of β

## Limitations

- Limited ablation studies to isolate the contribution of each mechanism (UCB exploration, ensemble averaging, majority voting)
- Claims about disagreement targeting cooperation-requiring states rely on intuition rather than direct measurement
- Majority vote evaluation mechanism's robustness benefits are not empirically validated through systematic error analysis

## Confidence

- High Confidence: EMAX improves sample efficiency and final returns compared to baselines (supported by statistically significant results across 21 tasks)
- Medium Confidence: Ensemble disagreement guides exploration to cooperation-requiring states (supported by task performance but lacks direct correlation analysis)
- Medium Confidence: Ensemble averaging reduces target variance and stabilizes training (supported by gradient variance claims but lacks direct comparison)
- Low Confidence: Majority vote evaluation significantly reduces miscoordination (claimed but not empirically validated)

## Next Checks

1. **Ablation Study**: Remove UCB exploration while keeping ensemble averaging to isolate the exploration benefit; measure if performance drops specifically on coordination tasks
2. **Variance Analysis**: Quantify and compare the variance of target values from ensemble averaging versus target networks across multiple training runs
3. **Disagreement Correlation**: Measure the correlation between ensemble disagreement and actual task coordination requirements across states to validate the exploration targeting claim