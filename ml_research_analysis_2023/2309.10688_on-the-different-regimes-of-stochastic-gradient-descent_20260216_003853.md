---
ver: rpa2
title: On the different regimes of Stochastic Gradient Descent
arxiv_id: '2309.10688'
source_url: https://arxiv.org/abs/2309.10688
tags:
- training
- learning
- batch
- size
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes the regimes of stochastic gradient descent\
  \ (SGD) for training deep neural networks. The key parameters are the batch size\
  \ B and learning rate \u03B7."
---

# On the different regimes of Stochastic Gradient Descent

## Quick Facts
- arXiv ID: 2309.10688
- Source URL: https://arxiv.org/abs/2309.10688
- Reference count: 0
- Key outcome: This paper analyzes the regimes of stochastic gradient descent (SGD) for training deep neural networks, showing a phase diagram in the batch size-learning rate plane separating noise-dominated, first-step-dominated, and gradient descent regimes.

## Executive Summary
This paper provides a comprehensive analysis of stochastic gradient descent (SGD) regimes for training deep neural networks, identifying three distinct dynamical phases: noise-dominated SGD, large-first-step-dominated SGD, and gradient descent. The authors use a teacher-student perceptron model to derive theoretical predictions about the critical batch size B* scaling with training set size P as a power law, with an exponent that characterizes problem hardness. They validate these predictions empirically on both perceptron models and deep networks, demonstrating that weight changes during training depend on both temperature T=η/B and training set size P.

## Method Summary
The authors analyze SGD regimes using a teacher-student perceptron model with data distribution ρ(x₁) = |x₁|ᵡe⁻ˣ₁²/2/Z, training with hinge loss until zero training error is reached. They derive an online stochastic differential equation (SDE) approximation for the weight dynamics in the small-batch, high-temperature regime, and solve for the asymptotic weight evolution. The critical batch size B* separating noise-dominated and first-step-dominated regimes is predicted to scale as P^γ where γ = 1/(1+χ). For deep networks, they use fully-connected networks and CNNs trained on MNIST and CIFAR10 datasets with the same hinge loss formulation, varying batch sizes and learning rates to map out the phase diagram and verify the theoretical predictions.

## Key Results
- Identified three dynamical phases of SGD: noise-dominated (T-controlled), first-step-dominated, and gradient descent (GD)
- Derived critical batch size B* ~ P^γ with γ = 1/(1+χ) separating noise-dominated and first-step-dominated regimes
- Showed that weight norms scale as T·P^γ in the noise-dominated regime due to orthogonal component growth
- Verified predictions empirically on both perceptron models and deep networks (fully-connected and CNN architectures)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SGD noise creates an orthogonal weight component that scales with temperature T and is necessary to fit all training points.
- Mechanism: In the small-batch, high-temperature regime, SGD noise adds a perpendicular component to the student weight vector. This component's magnitude is proportional to T, and the number of unfitted points scales as λ^-(χ+1), where λ is the alignment ratio. To fit all data, the orthogonal component must grow, increasing total weight magnitude as T·P^γ.
- Core assumption: Data distribution near the decision boundary follows ρ(x₁) ~ x₁^χ, creating a hard margin that requires larger weights as P increases.
- Evidence anchors:
  - [abstract] "For small B and large η, SGD corresponds to a stochastic evolution of the parameters, whose noise amplitude is governed by the 'temperature' T≡η/B."
  - [section] "In the limit λ → ∞, r → 0, from Eq. C8 we have gt₁ = c₁√d λ^-(χ+2), gt⊥ = −c₂√d λ^-(χ+1)"
  - [corpus] Weak evidence - no direct citations discussing this specific noise-induced orthogonal component mechanism
- Break condition: When T drops below κ (margin), the noise becomes negligible and fitting is constrained by margin rather than noise-induced orthogonal growth.

### Mechanism 2
- Claim: The critical batch size B* separating noise-dominated and first-step-dominated regimes scales as P^γ, where γ depends on problem hardness.
- Mechanism: For fixed temperature T=η/B, increasing batch size increases learning rate η. The first gradient step creates weight change proportional to η, which becomes larger than the noise-dominated asymptotic value ηP^γ/B when B exceeds P^γ. This creates a phase transition where weight dynamics shift from being noise-controlled to step-size controlled.
- Core assumption: The asymptotic solution from online SDE remains valid until the number of unfitted points reaches O(d/P), after which finite-sample effects dominate.
- Evidence anchors:
  - [abstract] "the batch size Bc separating regimes (i) and (ii) scale with the size P of the training set, with an exponent that characterizes the hardness of the classification problem."
  - [section] "The condition wη₁ ~ wt₁* gives: B* ~ P^γ, (14) with γ = 1/(1+χ)"
  - [corpus] Weak evidence - no direct citations discussing this specific P-dependent critical batch size scaling
- Break condition: When B << B*, SGD follows online-SDE dynamics; when B >> B*, first-step dominates and weights depend only on η, not T.

### Mechanism 3
- Claim: When temperature is too small (T << κ), SGD reduces to gradient descent with no temperature dependence.
- Mechanism: For small T, the margin κ dominates the fitting constraint. The condition for fitting a data point becomes w₁/∥w⊥∥ ≥ 1/|x₁|(κ/√d∥w⊥∥ + cμ), where the margin term is no longer negligible. This shifts the dynamics from being noise-controlled to margin-controlled, eliminating the temperature dependence.
- Core assumption: The margin κ sets the scale below which SGD noise becomes irrelevant for the dynamics.
- Evidence anchors:
  - [abstract] "simplifies to gradient descent (GD) when the temperature is sufficiently small"
  - [section] "For T ≪ κ, fitting (xμ,yμ) is constrained by the margin κ and the SGD noise is negligible in Eq. 12, implying that the temperature delimiting the noise-dominated regime of SGD follows: Tc ∝ κ"
  - [corpus] Weak evidence - no direct citations discussing this specific temperature-margin relationship
- Break condition: When T > κ, noise dominates; when T < κ, gradient descent dynamics emerge.

## Foundational Learning

- Concept: Stochastic differential equations (SDEs) for SGD
  - Why needed here: The paper uses SDEs to model SGD dynamics in the small-batch regime, deriving how noise affects weight evolution
  - Quick check question: What is the relationship between batch size B, learning rate η, and temperature T in the SDE formulation?

- Concept: Teacher-student perceptron model
  - Why needed here: This simple model allows analytical treatment of SGD dynamics while capturing key phenomena like weight growth and alignment
  - Quick check question: How does the data distribution parameter χ affect the hardness of the classification problem in this model?

- Concept: Critical phenomena and phase diagrams
  - Why needed here: The paper identifies three distinct dynamical regimes of SGD and their boundaries, requiring understanding of phase transitions
  - Quick check question: What physical quantities define the phase boundaries between noise-dominated, first-step-dominated, and gradient descent regimes?

## Architecture Onboarding

- Component map: Data generation (teacher perceptron with margin κ) -> Model architecture (student perceptron or deep network) -> Training algorithm (SGD with hinge loss) -> Analysis framework (online SDE approximation + finite-sample corrections)
- Critical path: Data generation → Model initialization → Training loop (SGD updates) → Monitoring weight evolution and alignment → Phase diagram construction → Performance analysis
- Design tradeoffs: Small batch sizes give better SDE approximation but slower training; large batch sizes enable faster computation but may skip noise-dominated dynamics; margin κ controls lazy vs feature learning regime.
- Failure signatures: Diverging training when η is too large; poor generalization when batch size exceeds critical B*; breakdown of online SDE approximation when finite-sample effects become important.
- First 3 experiments:
  1. Reproduce the perceptron phase diagram by varying B and η at fixed P, measuring alignment ⟨y(x)f(x)⟩x and weight norms
  2. Verify the P^γ scaling of critical batch size B* by training at different training set sizes
  3. Test the temperature-margin relationship by varying κ and measuring when temperature effects disappear

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the critical batch size B* scale with training set size P for different data distributions (varying χ)?
- Basis in paper: Explicit - The paper shows B* ~ P^γ with γ = 1/(1+χ), but the exact scaling for different χ values and datasets is not fully characterized.
- Why unresolved: The paper only provides limited empirical data for specific χ values and datasets. More comprehensive testing across different data distributions is needed.
- What evidence would resolve it: Empirical measurements of B* for a wide range of χ values and datasets, confirming the predicted power-law scaling.

### Open Question 2
- Question: What are the specific mechanisms by which SGD noise affects generalization performance in different learning regimes?
- Basis in paper: Inferred - The paper shows that different SGD regimes correspond to different generalization behaviors, but the underlying mechanisms are not fully explained.
- Why unresolved: The paper identifies correlations between SGD regimes and performance, but does not provide a complete theoretical explanation of the causal mechanisms.
- What evidence would resolve it: Detailed analysis of the relationship between SGD noise characteristics and generalization metrics across different architectures and tasks.

### Open Question 3
- Question: How do the three identified SGD regimes (noise-dominated, first-step-dominated, gradient descent) manifest in other deep learning architectures and tasks beyond those studied?
- Basis in paper: Explicit - The paper demonstrates these regimes in perceptrons, fully-connected networks, and CNNs, but acknowledges the need for broader validation.
- Why unresolved: The paper only examines a limited set of architectures and tasks. The generalizability of the phase diagram to other settings is unknown.
- What evidence would resolve it: Experimental validation of the three regimes and their phase boundaries in diverse architectures (e.g., transformers, recurrent networks) and tasks (e.g., regression, reinforcement learning).

## Limitations
- The theoretical analysis is rigorously derived only for the teacher-student perceptron model in the high-dimensional limit, with empirical validation needed for deep networks
- The online SDE approximation may break down for very large batch sizes or when finite-sample effects become important
- The role of the margin parameter κ in deep networks is not fully characterized and may differ from the perceptron case

## Confidence

| Claim | Confidence | Evidence |
|-------|------------|----------|
| Phase diagram predictions for perceptron | High | Rigorous analytical derivation with explicit solutions |
| B* ∝ P^γ scaling for deep networks | Medium | Empirical verification on limited architectures and datasets |
| Mechanisms extend to deep networks | Medium | Correlation observed but causal mechanisms not fully established |

## Next Checks

1. Test the B* ∝ P^γ scaling across multiple datasets and architectures with systematic variation of P to verify the power-law relationship holds beyond the perceptron model.

2. Measure the actual temperature T = η/B at which SGD transitions to gradient descent behavior across different network depths to determine if the κ-dependent temperature scaling generalizes.

3. Analyze weight alignment dynamics in the three regimes to verify the proposed mechanisms (noise-induced orthogonal components, first-step dominance, and margin-controlled dynamics) hold for deep networks beyond the simple perceptron case.