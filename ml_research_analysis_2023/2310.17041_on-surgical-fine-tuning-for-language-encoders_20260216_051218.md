---
ver: rpa2
title: On Surgical Fine-tuning for Language Encoders
arxiv_id: '2310.17041'
source_url: https://arxiv.org/abs/2310.17041
tags:
- layers
- fine-tuning
- tasks
- language
- glue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently fine-tuning pre-trained
  language encoders for downstream NLP tasks by proposing to selectively fine-tune
  only a subset of layers rather than the full model. The authors introduce a Fisher
  Information Matrix (FIM) score-based method to rank layers by their importance for
  a given task, using only a small subset of target data samples.
---

# On Surgical Fine-tuning for Language Encoders

## Quick Facts
- arXiv ID: 2310.17041
- Source URL: https://arxiv.org/abs/2310.17041
- Reference count: 9
- Key outcome: Selective fine-tuning of top-ranked layers identified by Fisher Information Matrix achieves comparable or better performance than full fine-tuning on GLUE/SuperGLUE tasks.

## Executive Summary
This paper addresses the challenge of efficiently fine-tuning pre-trained language encoders for downstream NLP tasks by proposing to selectively fine-tune only a subset of layers rather than the full model. The authors introduce a Fisher Information Matrix (FIM) score-based method to rank layers by their importance for a given task, using only a small subset of target data samples. Empirical results on GLUE and SuperGLUE benchmarks show that fine-tuning only the top-ranked layers identified by FIM achieves comparable or even better performance than full fine-tuning in most cases. The layer rankings remain stable across the optimization trajectory, validating the approach's effectiveness.

## Method Summary
The method uses Fisher Information Matrix (FIM) scores to identify the most important layers for a given task by computing the sensitivity of the likelihood function to parameter changes. The process involves computing FIM scores on a small subset (~100 samples) of target data to rank layer importance, then selectively fine-tuning only the top-ranked layers (typically top 5) using standard fine-tuning hyperparameters. The approach was evaluated against full fine-tuning baselines on BERT-base-cased across GLUE and SuperGLUE tasks, measuring performance via accuracy metrics.

## Key Results
- Selective fine-tuning of top 5 layers achieves comparable or better performance than full fine-tuning on most GLUE and SuperGLUE tasks
- Layer rankings identified by FIM scores remain stable across the optimization trajectory during fine-tuning
- Tasks requiring localized linguistic knowledge (discourse understanding, syntactic understanding, semantic understanding) can be effectively fine-tuned with fewer localized parameters
- The approach is particularly effective for tasks sensitive to domain shifts rather than temporal or environmental variations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning only the top-ranked layers identified by FIM achieves comparable or better performance than full fine-tuning.
- Mechanism: The Fisher Information Matrix quantifies how parameter changes affect model output, allowing identification of the most influential layers for a given task. By selectively fine-tuning these high-impact layers, the model adapts efficiently without overfitting or losing generalization.
- Core assumption: Task-specific information is localized in a few layers, and fine-tuning these layers is sufficient for strong downstream performance.
- Evidence anchors:
  - [abstract] "empirical results on GLUE and SuperGLUE benchmarks show that fine-tuning only the top-ranked layers identified by FIM achieves comparable or even better performance than full fine-tuning in most cases."
  - [section] "Results in Figure 1 (synthesized from Table 4 and Table 5) show that for GLUE and SuperGLUE tasks, surgical fine-tuning of identified most important layers results in comparable performance and sometimes outperforms tuning all parameters in all layers."
- Break condition: If task-specific information is distributed across all layers rather than localized, or if the FIM score fails to accurately identify the most influential layers.

### Mechanism 2
- Claim: The FIM score-based layer rankings remain stable across the optimization trajectory.
- Mechanism: The FIM score measures the sensitivity of the likelihood function to parameter changes. This sensitivity is task-dependent and remains constant throughout the fine-tuning process, ensuring that the layer rankings identified at the start remain valid.
- Core assumption: The importance of layers for a given task does not change significantly during fine-tuning.
- Evidence anchors:
  - [section] "we compute the rank of distinct layers leveraging the Fisher Information Score across the fine-tuning process of BERT at different epochs. Across tasks including WSC and WIC, we find that the rank of the different layers remain more or less consistent across the entire optimization trajectory during fine-tuning."
- Break condition: If the optimization process significantly alters the importance of layers for a given task, or if the FIM score is not a reliable indicator of layer importance.

### Mechanism 3
- Claim: Selective fine-tuning using FIM scores is effective for tasks that rely on simpler linguistic features and are sensitive to domain, environmental, or demographic shifts.
- Mechanism: Tasks that depend on simpler linguistic features can be effectively fine-tuned by adjusting a subset of layers identified by FIM scores. Additionally, FIM scores can efficiently select layers for tasks that are sensitive to certain types of distributional shifts.
- Core assumption: The complexity of the task and the type of distributional shift influence the effectiveness of selective fine-tuning.
- Evidence anchors:
  - [section] "Across the GLUE and SuperGLUE benchmarks, we observed that tasks requiring localized linguistic knowledge, such as discourse understanding (MNLI, MRPC, WNLI, WSC, and MultiRC), syntactic understanding (CoLA), semantic understanding, and commonsense/world knowledge (SST-2, QNLI), can be effectively fine-tuned with fewer localized parameters identified through ranked layer importance from the FIM scores."
- Break condition: If tasks require complex linguistic features or are sensitive to temporal drifts, selective fine-tuning using FIM scores may not be effective.

## Foundational Learning

- Concept: Fisher Information Matrix (FIM)
  - Why needed here: FIM quantifies the impact of parameter changes on a model's prediction, allowing identification of the most influential layers for a given task.
  - Quick check question: What does a higher Fisher information score indicate about the precision of parameter estimation?

- Concept: Layer-wise knowledge localization
  - Why needed here: Understanding that task-specific information is often localized in a few layers is crucial for the effectiveness of selective fine-tuning.
  - Quick check question: Why might fine-tuning only a subset of layers be sufficient for strong downstream performance?

- Concept: Distributional shifts
  - Why needed here: Recognizing the different types of distributional shifts (domain, environmental, temporal, demographic) helps explain when selective fine-tuning using FIM scores is effective.
  - Quick check question: How might the type of distributional shift in a task influence the effectiveness of selective fine-tuning?

## Architecture Onboarding

- Component map: Pre-trained language encoder (BERT) -> FIM computation module -> Layer ranking module -> Selective fine-tuning module -> Evaluation module

- Critical path:
  1. Compute FIM scores for each layer using a small subset of target data samples
  2. Rank layers based on their FIM scores
  3. Select the top-ranked layers for fine-tuning
  4. Perform selective fine-tuning on the chosen layers
  5. Evaluate the performance of the fine-tuned model on the target task

- Design tradeoffs:
  - Selective fine-tuning vs. full fine-tuning: Selective fine-tuning is more efficient but may not be effective for all tasks
  - Number of layers to fine-tune: Fine-tuning more layers may improve performance but increase computational cost
  - Size of the subset used for FIM computation: A larger subset may provide more accurate rankings but increase computation time

- Failure signatures:
  - Performance degradation compared to full fine-tuning
  - Inconsistent layer rankings across optimization epochs
  - FIM scores fail to identify the most influential layers for a given task

- First 3 experiments:
  1. Reproduce the results on a GLUE or SuperGLUE task using BERT and compare the performance of selective fine-tuning (top 5 layers) against full fine-tuning
  2. Analyze the stability of layer rankings across optimization epochs by computing FIM scores at different checkpoints
  3. Investigate the effectiveness of selective fine-tuning for tasks with different types of distributional shifts (e.g., domain, environmental, temporal, demographic)

## Open Questions the Paper Calls Out
- How does the FIM score method generalize to other transformer-based language models beyond BERT and RoBERTa?
- How does surgical fine-tuning using FIM compare to parameter-efficient fine-tuning methods like LoRA or adapters?
- What is the optimal number of layers to fine-tune for different task categories, and how does this vary with dataset size?

## Limitations
- The FIM score computation methodology lacks precise specification regarding which data samples are used and how diagonal elements are aggregated
- Evaluation only considers BERT-base-cased, leaving uncertainty about whether results generalize to other architectures or larger models
- Analysis focuses on GLUE/SuperGLUE tasks, which may not represent the full diversity of NLP applications, particularly those requiring complex reasoning or handling temporal drifts

## Confidence

- **High confidence**: The empirical observation that selective fine-tuning achieves comparable performance to full fine-tuning on many GLUE/SuperGLUE tasks is well-supported by the presented results
- **Medium confidence**: The claim that task-specific information is localized in a few layers is supported by results but relies on the FIM metric's validity, which isn't fully validated against alternative importance measures
- **Medium confidence**: The effectiveness for tasks sensitive to domain shifts is demonstrated but the analysis could benefit from more diverse distributional shift types and tasks requiring temporal reasoning

## Next Checks
1. Replicate FIM computation methodology using the specified small data subset approach and verify that it produces consistent layer rankings across multiple random seeds and data samples

2. Apply the selective fine-tuning approach to other transformer architectures (BERT-large, RoBERTa, DeBERTa) and evaluate whether layer importance rankings and performance benefits remain consistent

3. Design experiments with tasks that exhibit temporal distributional shifts (e.g., sentiment analysis on social media posts across different years) to test whether selective fine-tuning maintains effectiveness when the underlying data distribution changes over time