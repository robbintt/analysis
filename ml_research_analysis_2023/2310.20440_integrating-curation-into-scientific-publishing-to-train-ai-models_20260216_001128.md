---
ver: rpa2
title: Integrating curation into scientific publishing to train AI models
arxiv_id: '2310.20440'
source_url: https://arxiv.org/abs/2310.20440
tags:
- entities
- experimental
- cell
- entity
- variable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SourceData-NLP integrates data curation into the scientific publishing
  process to extract structured data from figure captions. By combining natural language
  processing with human-in-the-loop feedback from authors, the dataset annotates 8
  bioentity classes and their roles in experimental designs across 18,689 figures
  from 3,223 articles.
---

# Integrating curation into scientific publishing to train AI models

## Quick Facts
- arXiv ID: 2310.20440
- Source URL: https://arxiv.org/abs/2310.20440
- Reference count: 11
- Key outcome: SourceData-NLP integrates data curation into scientific publishing to extract structured data from figure captions, enabling training of AI models for biomedical entity recognition and semantic interpretation.

## Executive Summary
SourceData-NLP introduces a novel approach to dataset creation by integrating data curation directly into the scientific publishing workflow. The system combines natural language processing with human-in-the-loop feedback from authors to annotate 8 bioentity classes and their roles in experimental designs across 18,689 figures from 3,223 articles. By leveraging author expertise during validation, the dataset achieves high annotation accuracy while creating a valuable resource for training AI models in biomedical named-entity recognition and semantic interpretation tasks.

## Method Summary
The methodology combines automated NLP techniques with human validation to create a high-quality annotated dataset. Figure captions are extracted from published papers and segmented into panels, then professional curators annotate bioentities and their experimental roles. Authors are engaged to validate uncertain annotations, providing domain-specific corrections. The resulting dataset is used to fine-tune transformer-based language models (PubMedBERT and BioLinkBERT) for named-entity recognition and a novel semantic interpretation task that distinguishes controlled intervention targets from measurement objects in experimental designs.

## Key Results
- Dataset contains 620,000+ annotated entities across 8 bioentity classes from 18,689 figures
- Models fine-tuned on SourceData-NLP achieve strong performance on biomedical entity recognition tasks
- Novel semantic interpretation task successfully learns to classify entities as controlled vs measured variables from context alone
- Large BioLinkBERT models perform slightly better than PubMedBERT on entity recognition tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating data curation into the scientific publishing workflow improves annotation accuracy by leveraging author expertise during validation.
- Mechanism: Authors review curated annotations of their own figures, providing context-specific corrections that professional annotators might miss. This human-in-the-loop approach reduces ambiguity and increases precision of entity normalization and role assignment.
- Core assumption: Authors possess domain knowledge sufficient to validate complex experimental design annotations and entity identifications in their figures.
- Evidence anchors:
  - [abstract] "Natural language processing (NLP) was combined with human-in-the-loop feedback from the original authors to increase annotation accuracy."
  - [section 2.1.1] "When curators are uncertain, or in some systematically predefined cases, additional information or identifier validation is requested from the authors"
  - [corpus] Weak evidence - corpus doesn't provide direct validation data on accuracy improvements
- Break condition: Author feedback becomes inconsistent or unavailable, or if authors lack expertise to validate specific experimental designs.

### Mechanism 2
- Claim: Training models on SourceData-NLP dataset enables strong performance on biomedical named-entity recognition tasks.
- Mechanism: Large language models fine-tuned on SourceData-NLP learn to recognize and categorize biomedical entities across multiple classes with high precision due to the dataset's scale and diversity.
- Core assumption: The 620,000+ annotated entities provide sufficient coverage and examples for models to learn robust entity recognition patterns.
- Evidence anchors:
  - [abstract] "Models fine-tuned on SourceData-NLP achieve strong performance on biomedical entity recognition tasks"
  - [section 3.2.1] "The large versions tend to perform slightly better, a tendency that is more marked for BioLinkBERT than for PubMedBERT"
  - [corpus] Weak evidence - corpus shows related work but no direct performance benchmarks
- Break condition: Entity classes are too imbalanced or sparse, preventing effective learning for underrepresented categories.

### Mechanism 3
- Claim: SourceData-NLP enables a novel semantic interpretation task that distinguishes controlled intervention targets from measurement objects in experimental designs.
- Mechanism: Models learn from contextual patterns in figure captions to classify entities as either "controlled variables" (targets of intervention) or "measured variables" (objects of measurement), even when entities are masked to force context-only learning.
- Core assumption: Experimental figure captions contain sufficient contextual information for models to infer the causal role of entities in the experimental design.
- Evidence anchors:
  - [abstract] "a novel context-dependent semantic task assessing whether an entity is a controlled intervention target or a measurement object"
  - [section 3.4] "This task, which uses token classification, assigns the 'measured variable' or 'controlled variable' roles to entities based on context"
  - [corpus] Weak evidence - corpus shows related work on entity recognition but not causal role interpretation
- Break condition: Context is insufficient to disambiguate entity roles, leading to random or incorrect classifications.

## Foundational Learning

- Concept: Named Entity Recognition (NER)
  - Why needed here: NER is the fundamental task of identifying and categorizing entities in text, which is the core function of SourceData-NLP for biomedical literature.
  - Quick check question: What is the difference between identifying "gene products" versus "cell lines" in biomedical text?

- Concept: Token Classification
  - Why needed here: SourceData-NLP uses token classification to assign entity labels to individual tokens, which is essential for both NER and the semantic role interpretation tasks.
  - Quick check question: How does the IOB schema help in token classification for entity recognition?

- Concept: Transformer-based Language Models
  - Why needed here: The performance evaluation uses PubMedBERT and BioLinkBERT, which are transformer models pre-trained on biomedical text and fine-tuned for specific tasks.
  - Quick check question: Why do biomedical language models with biomedical vocabulary outperform those with general vocabulary on biomedical NER tasks?

## Architecture Onboarding

- Component map:
  - Figure extraction from published papers → Panel segmentation → Entity tagging → Normalization → Role assignment → Validation → Model training → Evaluation

- Critical path:
  1. Figure extraction from published papers
  2. Manual annotation of panels by professional curators
  3. Author validation of uncertain annotations
  4. Model fine-tuning on curated dataset
  5. Performance evaluation and model deployment

- Design tradeoffs:
  - Single vs multi-class training: Multi-class models are more computationally efficient but may sacrifice some accuracy compared to ensemble single-class models
  - Context-only vs identity-aware learning: Context-only approaches force models to learn from patterns, while identity-aware approaches can leverage entity recognition
  - Scale vs quality: Large dataset provides more examples but may include more noise without multiple annotators

- Failure signatures:
  - Low F1 scores on specific entity classes indicate insufficient training examples or ambiguous entity definitions
  - Poor performance on novel entities suggests memorization rather than generalization
  - Inconsistent role assignment indicates context is insufficient to disambiguate experimental designs

- First 3 experiments:
  1. Fine-tune PubMedBERT base model on SourceData-NLP for multi-class NER and evaluate F1 scores
  2. Fine-tune BioLinkBERT large model on SourceData-NLP for semantic role classification with masked entities
  3. Compare single-class vs multi-class training approaches on a subset of entity classes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of noise in the training data affect the performance of the models in SourceData-NLP?
- Basis in paper: [inferred] The paper mentions that adding noise to the dataset did not improve the generalization capabilities of the models, despite previous studies suggesting that noise can improve generalization.
- Why unresolved: The paper only tested the effect of noise on the overall performance of the models, not on specific entity categories or the context-dependent semantic interpretation task.
- What evidence would resolve it: Conducting experiments to test the effect of noise on specific entity categories and the context-dependent semantic interpretation task, as well as comparing the results to the overall performance of the models.

### Open Question 2
- Question: How does the performance of the models in SourceData-NLP vary across different entity categories?
- Basis in paper: [explicit] The paper states that the performance of the models varies significantly across different entity categories, with the best performances obtained for gene products, cell lines, organisms, and small molecules, and the lowest performance for subcellular components, cell types, diseases, and experimental assays.
- Why unresolved: The paper does not provide a detailed analysis of the reasons behind the performance differences across entity categories.
- What evidence would resolve it: Conducting a detailed analysis of the reasons behind the performance differences across entity categories, such as the frequency of annotation, the complexity of the terminology, and the availability of training data.

### Open Question 3
- Question: How does the inclusion of the experimental roles in the annotation process affect the performance of the models in SourceData-NLP?
- Basis in paper: [explicit] The paper introduces a novel NLP task to determine the empirical roles of entities within biomedical experiments and shows that this task can be efficiently learned to determine whether a gene product is measured or whether it is the target of an experimental perturbation.
- Why unresolved: The paper does not provide a comparison of the performance of the models with and without the inclusion of the experimental roles in the annotation process.
- What evidence would resolve it: Conducting experiments to compare the performance of the models with and without the inclusion of the experimental roles in the annotation process, as well as analyzing the impact of the experimental roles on the overall performance of the models.

## Limitations

- Performance claims lack external validation on independent biomedical datasets
- No quantification of author response rates or analysis of potential biases in human-in-the-loop validation
- Limited analysis of class imbalance and its impact on model performance for rare entity categories

## Confidence

- High Confidence: Dataset creation methodology and technical implementation are well-documented and reproducible
- Medium Confidence: Performance claims for fine-tuned models on biomedical entity recognition tasks are supported but need external validation
- Low Confidence: Semantic interpretation task claims have limited validation and minimal evidence of real-world applicability

## Next Checks

1. Test fine-tuned models on independent biomedical NER benchmarks (BC5CDR or NCBI Disease) to assess generalization beyond SourceData-NLP corpus
2. Analyze author validation dataset to quantify response rates, validation consistency, and potential biases in human-in-the-loop approach
3. Conduct detailed analysis of model performance across entity classes, focusing on rare categories like "organism" or "disease" to identify class imbalance issues