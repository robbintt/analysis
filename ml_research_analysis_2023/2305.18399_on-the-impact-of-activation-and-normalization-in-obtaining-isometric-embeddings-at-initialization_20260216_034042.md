---
ver: rpa2
title: On the impact of activation and normalization in obtaining isometric embeddings
  at initialization
arxiv_id: '2305.18399'
source_url: https://arxiv.org/abs/2305.18399
tags:
- normalization
- isometry
- layer
- gram
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the structure of Gram matrices in deep neural
  networks, which measure the similarity between outputs for a batch of inputs. It
  is observed that these matrices can become degenerate with depth at initialization,
  slowing training.
---

# On the impact of activation and normalization in obtaining isometric embeddings at initialization

## Quick Facts
- arXiv ID: 2305.18399
- Source URL: https://arxiv.org/abs/2305.18399
- Authors: 
- Reference count: 40
- Primary result: Layer normalization combined with activation layers biases Gram matrices toward isometry at an exponential rate with depth

## Executive Summary
This paper studies how deep neural networks' Gram matrices, which measure similarity between outputs for a batch of inputs, can become degenerate with depth at initialization, potentially slowing training. The authors prove that layer normalization, when combined with activation layers, biases the Gram matrix toward isometry at an exponential rate with depth. This isometry improvement is quantified using the Hermite expansion of the activation function, where higher-order Hermite coefficients promote isometry. The results suggest that certain activations can achieve isometry without normalization, shedding light on the role of layer normalization in training deep networks.

## Method Summary
The paper analyzes multilayer perceptrons with layer normalization and activation functions at initialization. It computes Gram matrices for each layer using random weights and input batches, then measures isometry using the formula I(G) = det(G)^(1/n) / (1/n tr(G)). The theoretical analysis uses Hermite expansions to characterize activation functions and prove exponential convergence rates to isometry. Experiments validate these predictions by comparing isometry across different activation functions and network depths.

## Key Results
- Layer normalization combined with activation layers achieves isometric embeddings at an exponential rate with depth
- The rate of isometry improvement depends on the Hermite expansion coefficients of the activation function
- Certain self-normalizing activations (like SELU, hyperbolic tangent) can achieve isometry without explicit normalization
- The order of activation and normalization layers critically impacts the ability to achieve isometry

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layer normalization combined with activation layers biases the Gram matrix toward isometry at an exponential rate with depth.
- Mechanism: Layer normalization projects representations onto a sphere, and activation functions with higher-order Hermite coefficients introduce non-linearity that pushes the Gram matrix toward the identity matrix. The isometry improvement is quantified by the non-linearity strength β0, which depends on the Hermite expansion of the activation.
- Core assumption: The input Gram matrix is non-degenerate and the network is sufficiently deep for the exponential rate to dominate.
- Evidence anchors:
  - [abstract] "we prove that layer normalization, in conjunction with activation layers, biases the Gram matrix of a multilayer perceptron towards the identity matrix at an exponential rate with depth at initialization."
  - [section] "Theorem 4 confirms the main claim of this paper: MLPs with normalization layers and non-linear activations can achieve isometric embedding by increasing depth at initialization."
  - [corpus] Weak - no direct citations on isometry rates with depth.

### Mechanism 2
- Claim: The centering step in layer normalization is crucial for achieving isometry by eliminating the constant term c0 in the Hermite expansion.
- Mechanism: Layer normalization includes a centering step that removes the mean of the pre-activation values. This centering ensures that the constant term c0 in the Hermite expansion of the activation function disappears, which is necessary for the isometry bias to occur.
- Core assumption: The Hermite expansion of the activation function includes a non-zero constant term c0 that needs to be removed.
- Evidence anchors:
  - [section] "Our Theorem 4 proof underscores the pivotal role of two elements of layer normalization in controlling the Hermite expansion: (i) centering and (ii) projection onto the √d-sphere."
  - [section] "The proof of Theorem 4 necessitates centering in layer normalization to ensure that the c0 term in the Hermite expansion disappears."
  - [corpus] Weak - no direct citations on the role of centering in Hermite expansions.

### Mechanism 3
- Claim: The projection step in layer normalization onto the √d-sphere is essential for achieving isometry by normalizing the Hermite coefficients.
- Mechanism: Layer normalization includes a normalization step that projects the centered activations onto a sphere of radius √d. This projection ensures that the variance of the activations is normalized, which is crucial for the isometry bias to occur.
- Core assumption: The variance of the activations needs to be normalized to achieve isometry.
- Evidence anchors:
  - [section] "Our mean-field theory emphasizes the role of projection onto the √d-sphere in addition to centering to achieve isometry."
  - [section] "This projection ensures that the variance of the activations is normalized, which is crucial for the isometry bias to occur."
  - [corpus] Weak - no direct citations on the role of projection in normalizing Hermite coefficients.

## Foundational Learning

- Concept: Hermite polynomials and their role in characterizing neural network activations
  - Why needed here: The paper uses Hermite expansions to quantify the non-linearity strength of activation functions and to prove the exponential rate of isometry convergence
  - Quick check question: How does the Hermite expansion of an activation function relate to its non-linearity strength β0?

- Concept: Gram matrices and their role in measuring similarity between neural network representations
  - Why needed here: The paper studies the structure of Gram matrices to understand how they become degenerate with depth and how normalization prevents this
  - Quick check question: How does the isometry of a Gram matrix relate to the preservation of distances and angles between input data points?

- Concept: Layer normalization and its components (centering and normalization)
  - Why needed here: The paper proves that both the centering and normalization steps in layer normalization are crucial for achieving isometry
  - Quick check question: What is the difference between the centering and normalization steps in layer normalization, and why are both necessary for isometry?

## Architecture Onboarding

- Component map: Input batch X ∈ ℝᵈ×ⁿ -> Layer normalization LN(X) -> Activation σ -> Weight matrices Wℓ -> Gram matrix Gℓ -> Isometry I(Gℓ)

- Critical path:
  1. Initialize input batch X
  2. Apply layer normalization LN(X)
  3. Apply activation σ to obtain hidden representations
  4. Compute Gram matrix Gℓ
  5. Measure isometry I(Gℓ)
  6. Repeat steps 2-5 for each layer

- Design tradeoffs:
  - Depth vs. width: Deeper networks achieve higher isometry but may be more prone to optimization difficulties
  - Activation choice: Activations with higher-order Hermite coefficients achieve faster isometry rates but may be harder to optimize
  - Normalization type: Layer normalization achieves isometry through centering and projection, while batch normalization may have different effects

- Failure signatures:
  - Degenerate Gram matrices: If the isometry I(Gℓ) is close to 0, the Gram matrix has become degenerate and training may be slowed
  - Insufficient depth: If the network depth is too shallow, the exponential rate of isometry convergence may not have manifested
  - Missing centering/normalization: If either the centering or normalization step is omitted from layer normalization, isometry may not be achieved

- First 3 experiments:
  1. Measure the isometry I(Gℓ) of a deep MLP with layer normalization and ReLU activation as a function of depth ℓ
  2. Compare the isometry I(Gℓ) of MLPs with different activation functions (e.g., ReLU, tanh, sigmoid) and measure their non-linearity strength β0
  3. Measure the isometry I(Gℓ) of an MLP with layer normalization but without the centering step, to demonstrate the importance of centering for isometry

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the normalization bias evolve during training beyond the initial 3 epochs observed in Figure 6?
- Basis in paper: [explicit] The paper observes that the normalization bias persists and increases during training with SGD on CIFAR10, but only over 3 epochs.
- Why unresolved: The paper does not explore the long-term evolution of the normalization bias during training.
- What evidence would resolve it: Extended training experiments showing the normalization bias over many more epochs, potentially revealing convergence or further increase.

### Open Question 2
- Question: Can certain "self-normalizing" activations (e.g., SELU, hyperbolic tangent) achieve isometry without layer normalization?
- Basis in paper: [explicit] The paper mentions that certain activations can achieve isometry with only offset and scale adjustments, as shown in Figure 7.
- Why unresolved: The paper does not provide a comprehensive analysis of which activations can achieve isometry without normalization and under what conditions.
- What evidence would resolve it: Systematic experiments comparing isometry for various activations with and without normalization, identifying the conditions for achieving isometry.

### Open Question 3
- Question: What is the precise impact of the order of activation and normalization layers on the isometry of Gram matrices?
- Basis in paper: [explicit] The paper demonstrates that the ordering of activation and normalization layers has a critical impact on isometry, as shown in Figure 8.
- Why unresolved: The paper does not provide a detailed theoretical analysis of why this ordering matters or explore all possible orderings.
- What evidence would resolve it: A theoretical analysis explaining the impact of ordering, along with experiments exploring all possible orderings of activation and normalization layers.

## Limitations
- The paper assumes the input Gram matrix is non-degenerate, but real-world datasets may contain duplicate or highly correlated samples that violate this assumption
- The exponential rate of isometry convergence depends on the non-linearity strength β0, but the paper does not provide guidance on what constitutes a sufficient β0 for practical networks
- The role of centering in layer normalization is crucial for isometry, but the paper does not explore the effects of omitting centering in detail

## Confidence

### Confidence Labels
- High confidence in the mathematical proofs of Theorem 4 and its supporting lemmas
- Medium confidence in the experimental validation, as the specific hyperparameters (batch size, width) are not fully specified
- Low confidence in the practical implications, as the paper focuses on theoretical analysis rather than extensive empirical studies on real-world datasets

## Next Checks
1. Test the isometry bias with degenerate input Gram matrices (e.g., containing duplicate samples) to verify the robustness of the theoretical results
2. Measure the isometry convergence rate for a wider range of activation functions and quantify their non-linearity strength β0
3. Compare the isometry achieved by layer normalization with and without the centering step to isolate the effects of each component