---
ver: rpa2
title: Label-Retrieval-Augmented Diffusion Models for Learning from Noisy Labels
arxiv_id: '2305.19518'
source_url: https://arxiv.org/abs/2305.19518
tags:
- labels
- latexit
- label
- learning
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reformulates the noisy label problem from a generative
  perspective, using diffusion models to progressively denoise labels by modeling
  the stochastic process of conditional label generation. To mitigate the impact of
  noisy labels, the authors propose a label-retrieval-augmented diffusion model that
  leverages neighbor consistency to construct pseudo-clean labels for training.
---

# Label-Retrieval-Augmented Diffusion Models for Learning from Noisy Labels

## Quick Facts
- arXiv ID: 2305.19518
- Source URL: https://arxiv.org/abs/2305.19518
- Reference count: 40
- Key outcome: Achieves new state-of-the-art results on all standard real-world noisy label benchmark datasets, with 20% absolute accuracy improvement in many cases

## Executive Summary
This paper introduces a novel approach to learning from noisy labels by reformulating the problem through diffusion models. The authors propose a label-retrieval-augmented (LRA) diffusion model that progressively denoises labels by modeling the stochastic process of conditional label generation. By incorporating auxiliary conditional information from large pre-trained models like CLIP and leveraging neighbor consistency to construct pseudo-clean labels, the method significantly improves robustness to label noise. The approach achieves state-of-the-art performance on standard real-world noisy label benchmarks.

## Method Summary
The proposed method treats label denoising as a conditional generation problem using diffusion models. A forward process gradually corrupts clean labels into noisy versions using Gaussian transitions, while a reverse diffusion process learns to denoise by reconstructing clean labels from noisy inputs. The LRA component retrieves k nearest neighbors in feature space using a pre-trained encoder and uses their labels as targets for diffusion training. The model incorporates conditional information from powerful pre-trained models like CLIP to guide the denoising process. Training uses the DDIM sampling strategy for faster inference while maintaining high-quality label generation.

## Key Results
- Achieves new SOTA results on all standard real-world noisy label benchmark datasets
- 20% absolute accuracy improvement compared to previous methods in many cases
- Demonstrates robust performance across different noise levels and dataset types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion models can progressively denoise labels by modeling the stochastic process of conditional label generation
- Mechanism: The forward process gradually corrupts clean labels into noisy versions using Gaussian transitions, while the reverse process learns to denoise by reconstructing clean labels from noisy inputs
- Core assumption: Label corruption can be modeled as a stochastic Gaussian diffusion process
- Evidence anchors: The CARD model transforms deterministic classification into a conditional label generation process, allowing for more flexible uncertainty modeling in the labeling process

### Mechanism 2
- Claim: Label-retrieval-augmentation improves training by leveraging neighbor consistency to construct pseudo-clean labels
- Mechanism: Using a pre-trained encoder, the method retrieves k nearest neighbors in feature space and uses their labels as targets for diffusion training
- Core assumption: In a well-structured feature space, the majority of a sample's neighbors share the same true label
- Evidence anchors: The main assumption is that in a latent space, data points from different classes form distinctive clusters, so the majority of a data point's neighbors are expected to have the same label as the point itself

### Mechanism 3
- Claim: Incorporating large pre-trained models like CLIP significantly boosts performance by providing high-quality conditional information
- Mechanism: The fp encoder extracts rich, high-dimensional features that guide the reverse diffusion process, while the diffusion model learns to denoise labels conditioned on these features
- Core assumption: Pre-trained models like CLIP capture meaningful semantic relationships that can guide label denoising
- Evidence anchors: The method can boost current SOTA accuracy by 10-20 absolute points in many cases by using pre-trained encoders

## Foundational Learning

- Concept: Denoising Diffusion Probabilistic Models (DDPMs)
  - Why needed here: The paper builds directly on diffusion model architecture to model label generation as a reverse denoising process
  - Quick check question: In DDPM, what is the relationship between the forward process variance schedule βt and the noise added at each timestep?

- Concept: Conditional Generation and Classifier-Free Guidance
  - Why needed here: The model uses conditional diffusion where labels are generated conditioned on image features
  - Quick check question: How does classifier-free guidance modify the sampling process in conditional diffusion models?

- Concept: Neighborhood Consistency and Semi-Supervised Learning
  - Why needed here: The LRA component relies on the assumption that nearby samples in feature space share labels
  - Quick check question: What is the key assumption behind label propagation methods in semi-supervised learning?

## Architecture Onboarding

- Component map:
  Pre-trained encoder fp -> Feature vector -> Diffusion model network -> Noise prediction -> Denoised label estimate
  KNN retrieval in fp feature space -> Augmented label distribution

- Critical path:
  1. Image → fp encoder → feature vector
  2. Feature vector + time embedding → diffusion model → noise prediction
  3. Noise prediction + yt → denoised label estimate
  4. Neighbor retrieval in fp feature space → augmented label distribution

- Design tradeoffs:
  - Using CLIP vs SimCLR: CLIP provides richer semantic features but is larger and slower; SimCLR is faster but may capture less semantic information
  - k value in KNN: Higher k provides more label diversity but may include more noisy labels; lower k is faster but may not capture enough label distribution information
  - DDIM steps S: Fewer steps = faster inference but potentially lower quality; more steps = slower but better quality

- Failure signatures:
  - Training divergence: Likely caused by improper learning rate or gradient explosion in diffusion model
  - Poor performance despite training: May indicate feature space is not well-structured for KNN retrieval or diffusion model is not learning effective denoising
  - Slow inference: Usually due to large fp encoder (e.g., CLIP ViT-L) or too many DDIM steps

- First 3 experiments:
  1. Train a basic conditional diffusion model without LRA or pre-trained encoders on a small dataset (e.g., CIFAR-10 with synthetic noise) to verify the core diffusion framework works
  2. Add KNN retrieval with a simple pre-trained encoder (e.g., SimCLR) to test the LRA component in isolation
  3. Replace the fp encoder with CLIP and compare performance to understand the impact of richer conditional information

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of k (number of neighbors) impact the trade-off between label noise reduction and diversity of the label distribution?
- Basis in paper: The paper states "We retrieve labels of the k nearest neighbors... Empirically, we find that the model can still achieve satisfactory performance when the magnitude of fq(x) is small... We set k=10 based on our tests using a range of k values from 1 to 100 on the validation sets."
- Why unresolved: The paper only mentions testing k values from 1 to 100 and settling on k=10, without providing a detailed analysis of the impact of different k values on model performance across different noise levels or datasets
- What evidence would resolve it: A systematic study varying k across different datasets and noise levels, showing how k affects the balance between label noise reduction and label distribution diversity

### Open Question 2
- Question: How does the proposed LRA-diffusion method compare to other semi-supervised learning approaches when dealing with noisy labels?
- Basis in paper: The paper mentions "Data recalibration. Data recalibration techniques progressively remove or correct mislabeled data during training to improve the reliability of training data."
- Why unresolved: The paper does not directly compare LRA-diffusion to other semi-supervised learning approaches like MixMatch, FixMatch, or UDA in the context of noisy labels
- What evidence would resolve it: Experimental results comparing LRA-diffusion to other semi-supervised learning methods on noisy label datasets

### Open Question 3
- Question: What is the impact of using different pre-trained encoders (e.g., other CLIP variants, Vision Transformers) on the performance of LRA-diffusion?
- Basis in paper: The paper states "Our method is flexible and general, allowing easy incorporation of different types of conditional information, e.g., use of pre-trained models, to further boost model performance... We use the SimCLR model trained on the training images... and the pre-trained CLIP model."
- Why unresolved: While the paper uses SimCLR and CLIP encoders, it does not explore the impact of using other pre-trained encoders like different CLIP variants or other vision transformers
- What evidence would resolve it: Experiments using different pre-trained encoders, such as other CLIP variants or other vision transformers

## Limitations

- Heavy reliance on pre-trained encoders means performance may vary significantly depending on domain alignment and feature space quality
- Computational cost is substantial, requiring training multiple large models and performing expensive neighbor searches during training
- KNN-based label retrieval assumes well-clustered feature spaces, which may not hold for all datasets or noise types

## Confidence

- **High confidence**: The diffusion model framework for label denoising is technically sound and well-grounded in existing DDPM literature
- **Medium confidence**: The LRA component's effectiveness depends heavily on feature space quality, which varies by dataset and pre-trained model choice
- **Medium confidence**: The 20% absolute improvement claims are impressive but should be interpreted cautiously given the complexity of the method

## Next Checks

1. **Ablation study on feature extractors**: Compare performance using CLIP, SimCLR, and no pre-trained encoder to quantify the exact contribution of semantic features to label denoising performance

2. **Noise type sensitivity analysis**: Test the method across different noise types (symmetric, asymmetric, instance-dependent) and noise rates to understand robustness boundaries

3. **Computational efficiency evaluation**: Measure wall-clock training time, memory usage, and inference latency compared to baseline methods to assess practical deployment feasibility