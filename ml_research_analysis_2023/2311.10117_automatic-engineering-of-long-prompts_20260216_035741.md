---
ver: rpa2
title: Automatic Engineering of Long Prompts
arxiv_id: '2311.10117'
source_url: https://arxiv.org/abs/2311.10117
tags:
- prompt
- arxiv
- search
- algorithm
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of automatic engineering of long
  prompts for large language models (LLMs). Existing methods focus on short prompts,
  but long prompts with thousands of tokens are common for complex tasks.
---

# Automatic Engineering of Long Prompts

## Quick Facts
- **arXiv ID**: 2311.10117
- **Source URL**: https://arxiv.org/abs/2311.10117
- **Reference count**: 6
- **Key outcome**: Greedy algorithm with beam search achieves 9.2% accuracy gain over human-written prompts on Big Bench Hard with only 50 training evaluations

## Executive Summary
This paper addresses the challenge of automatic engineering for long prompts (thousands of tokens) used with large language models (LLMs). While existing methods focus on short prompts, this work proposes a greedy algorithm with beam search that efficiently optimizes long prompts by replacing sentences while preserving semantic meaning. The method introduces two novel techniques using search history to guide the mutation process, achieving significant performance improvements over human-written prompts with minimal training evaluations.

## Method Summary
The method employs a greedy algorithm with beam search to optimize long prompts by iteratively replacing sentences. At each step, it selects a sentence to mutate using a contextual bandit approach, generates variations using an LLM-based mutator, evaluates performance on a limited training set, and updates a beam search pool of top candidates. Two key innovations guide the mutation process: (1) using search history to provide in-context examples for the LLM-mutator, and (2) using sentence similarity to retrieve relevant historical mutations. The approach is evaluated on the Big Bench Hard benchmark, demonstrating significant accuracy improvements over human-written prompts and baseline methods.

## Key Results
- 9.2% average accuracy gain over human-written prompts on Big Bench Hard benchmark
- Outperforms genetic algorithms and evolving single sentences baselines
- Achieves strong results with only 50 training evaluations
- Greedy algorithm with beam search converges faster than genetic algorithms

## Why This Works (Mechanism)

### Mechanism 1: Greedy Algorithm with Beam Search
The greedy algorithm with beam search maintains a pool of top-performing prompts and updates it immediately after generating each new candidate, leading to faster convergence compared to genetic algorithms that wait to evaluate an entire generation. This approach is more efficient for long prompt engineering because it keeps the solution pool more up-to-date and exploits the fact that human-written initial prompts are reasonably good.

### Mechanism 2: History-Guided Mutation
The algorithm maintains a history of previous mutations and their rewards, then uses in-context learning to guide new mutations. When mutating a sentence, it retrieves similar historical entries and uses them as examples to guide the LLM-mutator. This allows the system to learn from past successful and unsuccessful mutations, improving the quality of future mutations.

### Mechanism 3: Contextual Bandit for Sentence Selection
The algorithm treats each sentence as an arm in a contextual bandit problem, using features from sentence embeddings to predict which sentences are likely to benefit from modification. The Lin-UCB algorithm guides sentence selection by balancing exploration and exploitation, helping the search focus on sentences most likely to yield performance improvements when modified.

## Foundational Learning

- **Concept: Greedy algorithms vs Genetic algorithms**
  - Why needed here: Understanding the fundamental differences between these search strategies is crucial for appreciating why the proposed beam search approach works better for long prompt engineering.
  - Quick check question: What is the main difference in how greedy algorithms and genetic algorithms update their solution pools during search?

- **Concept: In-context learning with LLMs**
  - Why needed here: The history-guided mutation mechanism relies on LLMs' ability to learn from examples provided in the prompt context.
  - Quick check question: How does providing historical mutation examples as in-context examples help guide the LLM-mutator?

- **Concept: Contextual bandit problems**
  - Why needed here: The sentence selection mechanism uses contextual bandit algorithms to balance exploration and exploitation when choosing which sentences to modify.
  - Quick check question: What is the role of the exploration parameter in the Lin-UCB algorithm used for sentence selection?

## Architecture Onboarding

- **Component map**: Initial prompt -> Sentence tokenizer -> LLM-mutator -> Performance evaluator -> Beam search manager -> History database -> Sentence selection module -> Search loop controller

- **Critical path**: 1. Initialize with human-written prompt, 2. Tokenize into sentences, 3. Select sentence to mutate using contextual bandit, 4. Generate variations using LLM-mutator with history guidance, 5. Evaluate performance on training set, 6. Update beam search pool, 7. Store mutation in history, 8. Repeat until budget exhausted

- **Design tradeoffs**: Beam size vs computational budget (larger beam sizes provide better exploration but increase computational cost), History retention vs relevance (keeping more history provides more guidance but may include less relevant examples), Exploration vs exploitation (the contextual bandit must balance trying new sentences vs modifying known good ones)

- **Failure signatures**: Slow convergence (may indicate poor initial prompt quality or ineffective mutation strategy), High variance in results (could suggest unstable LLM-mutator or noisy performance evaluations), Overfitting (when training accuracy significantly exceeds test accuracy)

- **First 3 experiments**: 1. Baseline comparison: Run original prompt vs greedy algorithm without beam search to demonstrate the value of beam search, 2. Ablation study: Test with and without history-guided mutation to quantify its contribution, 3. Sentence selection comparison: Compare random sentence selection vs contextual bandit approach to show its effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed greedy algorithm with beam search compare to genetic algorithms in terms of search efficiency and effectiveness for automatic long prompt engineering?
- Basis in paper: The paper compares the performance of the proposed greedy algorithm with beam search against genetic algorithms in terms of search efficiency and effectiveness. The results show that the greedy algorithm with beam search outperforms genetic algorithms in terms of search efficiency.
- Why unresolved: While the paper provides experimental results comparing the two algorithms, it does not delve into the underlying reasons for the superior performance of the greedy algorithm with beam search.
- What evidence would resolve it: Additional analysis and theoretical explanations could shed light on the reasons behind the superior performance of the greedy algorithm with beam search.

### Open Question 2
- Question: How can the search space of long prompts be further explored and optimized to improve the performance of automatic prompt engineering?
- Basis in paper: The paper mentions that the search space of long prompts is immense, and optimizing it remains a challenging problem. It suggests that manipulating multiple sentences simultaneously or consolidating multiple sentences into a single one could potentially enhance the search process.
- Why unresolved: While the paper acknowledges the importance of exploring the search space, it does not provide specific techniques or strategies for optimizing it.
- What evidence would resolve it: Experimental studies comparing different search space exploration techniques, such as multi-sentence manipulation or sentence consolidation, could provide insights into their effectiveness.

### Open Question 3
- Question: How can overfitting be mitigated in automatic prompt engineering, especially when using a limited training set?
- Basis in paper: The paper mentions that overfitting can occur during the search process, leading to higher training accuracy than testing accuracy. It suggests that applying additional regularization techniques, such as imposing sparsity constraints by reducing the number of modified sentences, could potentially alleviate overfitting.
- Why unresolved: While the paper acknowledges the issue of overfitting, it does not provide specific regularization techniques or strategies for mitigating it.
- What evidence would resolve it: Experimental studies comparing different regularization techniques, such as L1 or L2 regularization, dropout, or early stopping, could provide insights into their effectiveness in mitigating overfitting.

## Limitations
- The method's effectiveness depends heavily on the quality of the human-written initial prompt and the LLM's ability to generate meaningful sentence variations
- The current approach cannot correct mistakes introduced by the LLM-mutator, which is a significant limitation given that incorrect mutations are more likely for intricate sentences
- The evaluation is limited to the Big Bench Hard benchmark, and performance on other datasets or real-world applications remains unknown

## Confidence
- **High Confidence**: The claim that greedy algorithm with beam search converges faster than genetic algorithms is well-supported by the provided learning curve comparisons (Figure 3) and theoretical reasoning about immediate pool updates.
- **Medium Confidence**: The effectiveness of history-guided mutation relies on LLMs' in-context learning capabilities, which are well-established but may vary across different LLM models and prompt engineering tasks.
- **Medium Confidence**: The contextual bandit approach for sentence selection is theoretically sound, but its practical effectiveness depends on the quality of sentence embeddings and the stability of the reward signal.

## Next Checks
1. **Ablation Study**: Run experiments with different beam sizes (k=1, 3, 5, 10) to quantify the trade-off between search quality and computational cost, and determine the optimal beam size for different prompt lengths and task complexities.

2. **Generalization Test**: Apply the proposed method to additional benchmarks beyond Big Bench Hard, such as MMLU or SuperGLUE, to evaluate its effectiveness across different task domains and difficulty levels.

3. **Mutation Quality Analysis**: Implement a mechanism to detect and correct LLM-mutator errors by using a separate LLM to validate sentence semantic equivalence before accepting mutations, and measure the impact on overall performance.