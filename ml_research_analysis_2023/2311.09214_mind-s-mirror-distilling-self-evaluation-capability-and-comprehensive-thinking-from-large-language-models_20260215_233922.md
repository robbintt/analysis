---
ver: rpa2
title: 'Mind''s Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking
  from Large Language Models'
arxiv_id: '2311.09214'
source_url: https://arxiv.org/abs/2311.09214
tags:
- self-evaluation
- cots
- reasoning
- distillation
- slms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to distill self-evaluation capability
  and comprehensive thinking from large language models (LLMs) into small language
  models (SLMs). The key idea is to first obtain multiple diverse chain-of-thought
  (CoT) rationales and corresponding self-evaluation outputs from the LLM, and then
  use multi-task learning to train the SLM on both the CoT rationales and self-evaluation
  outputs.
---

# Mind's Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking from Large Language Models

## Quick Facts
- arXiv ID: 2311.09214
- Source URL: https://arxiv.org/abs/2311.09214
- Reference count: 12
- Distills self-evaluation and comprehensive thinking from LLMs into SLMs using multi-task learning on diverse CoTs

## Executive Summary
This paper proposes a novel method for distilling self-evaluation capability and comprehensive thinking from large language models (LLMs) into small language models (SLMs). The key idea is to first obtain multiple diverse chain-of-thought (CoT) rationales and corresponding self-evaluation outputs from the LLM, and then use multi-task learning to train the SLM on both the CoT rationales and self-evaluation outputs. This allows the SLM to learn the self-evaluation capability of the LLM to identify flawed reasoning, as well as a more comprehensive understanding from the diverse CoTs.

## Method Summary
The proposed method involves two steps: (1) obtaining multiple diverse CoTs and corresponding self-evaluation outputs from the LLM using few-shot prompting, and (2) training the SLM using multi-task learning on the CoTs and self-evaluation outputs. The SLM is trained to generate both rationales and labels, with different task prefixes ("predict:" for labels and "explain:" for rationales). The experiments use gpt-3.5-turbo as the LLM teacher and T5-Base (220M) as the SLM student, evaluating on three NLP tasks: Math Word Problems (SVAMP), Commonsense Question Answering (CQA), and Natural Language Inference (ANLI).

## Key Results
- The proposed method significantly improves the performance of distilled SLMs compared to standard distillation and CoT distillation baselines
- Consistent improvements are observed across different label types and tasks
- The method demonstrates the ability to mitigate flawed reasoning and reduce hallucinations in SLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-evaluation capability from LLMs allows SLMs to detect and correct flawed reasoning chains.
- Mechanism: By training SLMs to generate self-evaluation outputs that assess the correctness of CoT rationales, the SLM learns to recognize when reasoning is faulty and can avoid propagating hallucinations.
- Core assumption: The LLM's self-evaluation capability is sufficiently reliable and transferable to smaller models.
- Evidence anchors:
  - [abstract] "we propose a novel method for distilling the self-evaluation capability inherent in LLMs into SLMs, which aims to mitigate the adverse effects of erroneous reasoning and reduce hallucinations."
  - [section 3.1.2] Describes the process of obtaining multiple self-evaluation outputs from the LLM, where the LLM evaluates the correctness of its generated CoTs.
- Break condition: If the LLM's self-evaluation is unreliable or biased, the distilled capability in the SLM will also be flawed, potentially worsening performance.

### Mechanism 2
- Claim: Distilling multiple diverse CoTs and self-evaluation outputs leads to more comprehensive knowledge transfer.
- Mechanism: By exposing the SLM to multiple CoTs and corresponding self-evaluation outputs, the SLM learns a broader range of reasoning paths and can better understand when different approaches are correct or incorrect.
- Core assumption: Multiple diverse CoTs capture a wider range of the LLM's reasoning capabilities than a single CoT.
- Evidence anchors:
  - [abstract] "we advocate for a comprehensive distillation process that incorporates multiple distinct chain-of-thought and self-evaluation paradigms and ensures a more holistic and robust knowledge transfer into SLMs."
  - [section 3.1.1] Describes obtaining multiple different CoTs for each training instance from the LLM.
- Break condition: If the multiple CoTs are not sufficiently diverse or if the self-evaluation outputs do not accurately assess the CoTs, the benefit of comprehensive distillation may be limited.

### Mechanism 3
- Claim: Multi-task learning with both rationales and labels in CoTs and self-evaluation outputs improves SLM performance.
- Mechanism: The SLM is trained to predict both the label and generate the rationale, allowing it to learn the reasoning process more effectively than just predicting labels.
- Core assumption: Joint training on rationales and labels provides more effective learning than single-task training.
- Evidence anchors:
  - [section 3.2] "we employ multi-task learning to train the SLM for self-evaluation capability and CoT reasoning capability."
  - [section 3.2.1] Describes the multi-task loss function used for training the SLM with self-evaluation data.
- Break condition: If the multi-task learning does not converge or if the model struggles to balance the two tasks, performance may degrade.

## Foundational Learning

- Concept: Chain-of-thought (CoT) reasoning
  - Why needed here: Understanding CoT is crucial because the paper's method builds upon CoT distillation, extending it with self-evaluation and multiple diverse CoTs.
  - Quick check question: What is the difference between standard distillation and CoT distillation?

- Concept: Knowledge distillation
  - Why needed here: The paper's core contribution is a new method for distilling knowledge from LLMs to SLMs, so understanding the basics of knowledge distillation is essential.
  - Quick check question: What is the role of the teacher model and the student model in knowledge distillation?

- Concept: Self-evaluation in language models
  - Why needed here: The paper introduces a novel approach of distilling the self-evaluation capability of LLMs into SLMs, so understanding what self-evaluation means in this context is important.
  - Quick check question: How does the LLM's self-evaluation capability help in mitigating flawed reasoning?

## Architecture Onboarding

- Component map: LLM (teacher) -> Generates multiple CoTs and self-evaluation outputs -> SLM (student) -> Trained using multi-task learning on CoTs and self-evaluation outputs

- Critical path:
  1. Generate multiple CoTs and self-evaluation outputs from the LLM.
  2. Train the SLM using multi-task learning on both the CoTs and self-evaluation outputs.
  3. Evaluate the SLM's performance on downstream tasks.

- Design tradeoffs:
  - Number of CoTs and self-evaluation outputs: More diversity can improve performance but increases computational cost.
  - Balancing rationale and label loss in multi-task learning: The hyperparameter Î» controls the trade-off between generating rationales and predicting labels.

- Failure signatures:
  - SLM performance does not improve over standard distillation: May indicate issues with the quality of self-evaluation outputs or insufficient diversity in CoTs.
  - SLM generates irrelevant or incorrect rationales: May suggest the multi-task learning is not effectively balancing the two tasks.

- First 3 experiments:
  1. Train the SLM using only a single CoT without self-evaluation and compare performance to standard distillation.
  2. Train the SLM using multiple CoTs but without self-evaluation and assess the impact on performance.
  3. Vary the number of self-evaluation outputs per CoT and observe how it affects the SLM's ability to correct flawed reasoning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method's performance vary across different types of large language models (LLMs) as teachers, such as GPT-4, Llama 2, or other state-of-the-art models?
- Basis in paper: [inferred] The paper mentions that the experiments were conducted using gpt-3.5-turbo as the LLM, but it also acknowledges the rapid evolution of LLMs and suggests that the distillation method may manifest differently when paired with other architectures or models.
- Why unresolved: The study primarily utilized a single teacher model, gpt-3.5-turbo, and did not explore the performance variations when using different types of LLMs.
- What evidence would resolve it: Conducting experiments with various LLMs as teachers and comparing the performance of the distilled SLMs across different tasks and datasets.

### Open Question 2
- Question: What is the impact of the number of self-evaluation outputs on the model's ability to mitigate hallucinations and flawed reasoning in tasks beyond the three NLP benchmarks tested?
- Basis in paper: [explicit] The paper discusses the effect of varying the number of self-evaluation outputs on model performance, but this exploration was limited to the SV AMP dataset.
- Why unresolved: The study did not extensively evaluate the impact of self-evaluation outputs on a broader range of tasks, leaving the generalizability of the findings uncertain.
- What evidence would resolve it: Performing experiments with different numbers of self-evaluation outputs on a wider variety of NLP tasks and analyzing the consistency of the results.

### Open Question 3
- Question: How does the proposed method perform when the LLM-generated pseudo-labels are of significantly lower quality compared to human-annotated labels?
- Basis in paper: [explicit] The paper acknowledges a performance gap between models trained with LLM-generated pseudo-labels and those trained with human-annotated labels, suggesting that the method's benefits are robust to variations in label quality.
- Why unresolved: The paper does not provide detailed insights into how the method performs under extreme cases of low-quality pseudo-labels, which could be a common scenario in real-world applications.
- What evidence would resolve it: Conducting experiments with intentionally degraded pseudo-labels and assessing the distilled SLM's performance to determine the threshold of label quality that still allows for effective distillation.

## Limitations

- Unknown Impact of Self-Evaluation Quality: The paper assumes the LLM's self-evaluation capability is sufficiently reliable, but this is not empirically validated.
- Generalization Across Tasks and Datasets: The results may not generalize to other domains or more complex reasoning tasks.
- Scalability Concerns: The method requires generating multiple CoTs and self-evaluation outputs from the LLM for each training instance, which can be computationally expensive.

## Confidence

- High Confidence: The core mechanism of using multi-task learning to train SLMs on both CoTs and self-evaluation outputs is well-established and clearly described.
- Medium Confidence: The claim that distilling self-evaluation capability significantly improves SLM performance is supported by the experimental results, but the underlying reasons for this improvement are not thoroughly investigated.
- Low Confidence: The assertion that the proposed method leads to more "comprehensive thinking" in SLMs is somewhat vague and not directly measured or validated in the paper.

## Next Checks

1. Evaluate Self-Evaluation Quality: Conduct a study to assess the accuracy and reliability of the LLM's self-evaluations. Compare the LLM's self-evaluations against ground truth labels or human annotations to quantify the quality of the self-evaluation capability being distilled.

2. Test on Additional Tasks and Datasets: Apply the proposed method to a wider range of NLP tasks and datasets, including those with different levels of complexity and reasoning requirements. This will help determine the generalizability and robustness of the approach.

3. Analyze Computational Costs: Measure and analyze the computational costs associated with generating multiple CoTs and self-evaluation outputs from the LLM. Investigate techniques to reduce these costs, such as using smaller or more efficient LLMs, or sampling a subset of CoTs for training.