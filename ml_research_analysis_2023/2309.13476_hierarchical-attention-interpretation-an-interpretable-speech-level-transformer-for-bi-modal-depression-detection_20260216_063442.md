---
ver: rpa2
title: 'Hierarchical attention interpretation: an interpretable speech-level transformer
  for bi-modal depression detection'
arxiv_id: '2309.13476'
source_url: https://arxiv.org/abs/2309.13476
tags:
- depression
- attention
- each
- detection
- speech-level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a bi-modal speech-level transformer for depression
  detection that avoids segment-level labelling noise and provides hierarchical model
  interpretability. The model processes natural sentences with both audio (Mel-spectrogram)
  and text (BERT embeddings), fuses them via cross-attention, and produces a single
  depression prediction per speech.
---

# Hierarchical attention interpretation: an interpretable speech-level transformer for bi-modal depression detection

## Quick Facts
- arXiv ID: 2309.13476
- Source URL: https://arxiv.org/abs/2309.13476
- Reference count: 20
- Outperforms segment-level baseline with precision 0.854, recall 0.947, F1 0.897

## Executive Summary
This paper proposes a bi-modal speech-level transformer for depression detection that avoids segment-level labelling noise and provides hierarchical model interpretability. The model processes natural sentences with both audio (Mel-spectrogram) and text (BERT embeddings), fuses them via cross-attention, and produces a single depression prediction per speech. It outperforms a segment-level baseline (p=0.854, r=0.947, F1=0.897 vs. p=0.732, r=0.808, F1=0.768). Hierarchical attention interpretation reveals which sentences, text tokens, and spectrogram regions are most relevant to depression detection, enabling clinicians to verify predictions. This interpretability and improved performance make the approach promising for clinical deployment.

## Method Summary
The model uses sentence-level segmentation to process entire speeches as sequences of natural sentences, avoiding the noise introduced by arbitrary segment boundaries. Each sentence's audio (Mel-spectrogram) and text (BERT embeddings) are encoded separately and fused via cross-attention. A speech-level transformer encoder aggregates sentence representations into a final depression prediction. The model is trained on the D-Vlog dataset with gradient accumulation to handle memory constraints, and hierarchical attention interpretation provides both sentence-level and speech-level explanations for predictions.

## Key Results
- Outperforms segment-level baseline with precision 0.854, recall 0.947, F1 0.897
- Hierarchical attention interpretation identifies relevant sentences and features within speeches
- Model processes natural sentence boundaries rather than arbitrary segments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model avoids segment-level labelling noise by processing entire speeches as sequences of natural sentences.
- Mechanism: By segmenting audio into natural sentences and using a speech-level transformer, the model learns from coherent linguistic units rather than arbitrary segments that may mix depressed and non-depressed content.
- Core assumption: Natural sentence boundaries preserve the semantic coherence needed for accurate depression detection.
- Evidence anchors:
  - [abstract] "We propose a bi-modal speech-level transformer to avoid segment-level labelling"
  - [section] "To avoid labelling noise, [4] applied a post-level encoder to first encode social media posts... Inspired by [4] on social media analysis, the current work performs a sentence-level segmentation for speech analysis"
  - [corpus] Found 25 related papers; average neighbor FMR=0.401 suggests moderate semantic relevance to this approach
- Break condition: If sentence boundaries do not align with depression-relevant content, or if sentences are too short to capture depression patterns, the noise reduction benefit diminishes.

### Mechanism 2
- Claim: Hierarchical attention interpretation provides both speech-level and sentence-level interpretability for clinical verification.
- Mechanism: The model uses gradient-weighted attention maps across all attention layers to track feature interactions, creating relevancy scores for both sentences within a speech and specific tokens/Mel-spectrogram regions within those sentences.
- Core assumption: Attention scores at different layers can be weighted by gradients to accurately represent feature importance for the final prediction.
- Evidence anchors:
  - [abstract] "we introduce a hierarchical interpretation approach to provide both a speech-level and a sentence-level interpretation"
  - [section] "we design the proposed model to be based entirely on the attention mechanism so that we can apply the method introduced in [9] to track interactions between input features"
  - [corpus] Weak corpus evidence for this specific hierarchical interpretation approach
- Break condition: If gradient-weighted attention maps do not accurately reflect feature importance, or if the hierarchical structure introduces interpretation artifacts.

### Mechanism 3
- Claim: Cross-attention fusion between audio and text modalities improves depression detection performance.
- Mechanism: The transformer decoder uses cross-attention layers to fuse encoded audio (Mel-spectrogram) and text (BERT embeddings) representations, allowing the model to leverage both acoustic and linguistic depression indicators.
- Core assumption: Depression-relevant information exists in both audio and text modalities and their interaction provides complementary signals.
- Evidence anchors:
  - [abstract] "fuses them via cross-attention"
  - [section] "we deploy the transformer decoder as the cross-attention module... using its cross-attention layers to fuse the encoded text representations with the encoded audio representations"
  - [corpus] Moderate corpus support with related work on multimodal depression detection
- Break condition: If one modality dominates the fusion process, or if the cross-attention fails to align relevant audio-text pairs.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The entire model is based on attention mechanisms, requiring understanding of self-attention, cross-attention, and multi-head attention
  - Quick check question: What is the difference between self-attention and cross-attention in transformer architectures?

- Concept: Gradient-weighted attention interpretation
  - Why needed here: The hierarchical interpretation approach relies on weighting attention scores by gradients to determine feature importance
  - Quick check question: How do gradient-weighted attention maps differ from raw attention scores in terms of interpretability?

- Concept: Multimodal learning and fusion strategies
  - Why needed here: The model fuses audio (Mel-spectrogram) and text (BERT embeddings) modalities using cross-attention, requiring understanding of how different modalities can be combined effectively
  - Quick check question: What are the advantages of using cross-attention for multimodal fusion compared to simple concatenation?

## Architecture Onboarding

- Component map: Audio waveform → Mel-spectrogram → AST encoding → Cross-attention with BERT → Sentence embeddings → Speech-level transformer → Depression prediction
- Critical path: The model processes each sentence's audio and text through separate encoders, fuses them via cross-attention, aggregates sentence representations with a speech-level transformer, and outputs a depression classification
- Design tradeoffs:
  - Memory vs. performance: Using first 42 sentences to fit GPU memory limits potentially missing longer speeches
  - Interpretability vs. complexity: Entirely attention-based design enables interpretation but may sacrifice some modeling power
  - Modality balance: Cross-attention fusion must effectively combine audio and text without one modality dominating
- Failure signatures:
  - Poor performance despite good interpretability: Indicates model may be capturing correct patterns but lacks predictive power
  - High interpretability but incorrect predictions: Suggests interpretation mechanism may be misleading or model overfits to spurious patterns
  - Training instability: Could indicate gradient accumulation issues or learning rate problems
- First 3 experiments:
  1. Compare sentence-level vs. segment-level baselines with identical architectures except for the speech-level processing block
  2. Ablation study removing cross-attention fusion to test modality independence
  3. Interpretation validation by comparing highlighted regions to known depression acoustic markers (e.g., reduced pitch, pause time)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific acoustic features are present in the highlighted Mel-spectrogram regions that are most relevant to depression detection, and how do they differ from non-highlighted regions?
- Basis in paper: [explicit] The paper states that future work may investigate what specific acoustic features are present in these regions and whether they are associated with depression, such as reduced pitch.
- Why unresolved: The paper acknowledges that while the model highlights relevant regions, it does not explicitly analyze the acoustic features within these regions to determine their specific characteristics or their correlation with depression.
- What evidence would resolve it: Analyzing the acoustic features (e.g., pitch, intensity, formants) in the highlighted regions and comparing them to non-highlighted regions, and correlating these features with clinical assessments of depression.

### Open Question 2
- Question: Does training an audio-only model, without the cross-attention module, highlight different Mel-spectrogram regions compared to the bi-modal model, and would these regions include features unrelated to speech content, such as pause time?
- Basis in paper: [explicit] The paper suggests exploring training an audio-only model to investigate if acoustic features unrelated to speech content, such as pause time, would be highlighted.
- Why unresolved: The current model uses cross-attention to learn interactions between spectral features and speech content, which may bias the highlighted regions towards articulated speech. An audio-only model would isolate acoustic features.
- What evidence would resolve it: Training an audio-only model and comparing the highlighted regions with those from the bi-modal model, specifically looking for regions corresponding to non-speech content features like pauses.

### Open Question 3
- Question: How do the sentence-level relevancy scores obtained from the hierarchical attention interpretation correlate with clinical assessments of depression severity within the speech?
- Basis in paper: [explicit] The paper provides interpretations at both speech-level and sentence-level, showing which sentences are most relevant to depression detection.
- Why unresolved: While the model identifies relevant sentences, it does not validate these findings against clinical depression severity scores or expert annotations within the same speech.
- What evidence would resolve it: Correlating the sentence-level relevancy scores with clinician-rated depression severity scores for the same speech samples to assess the model's alignment with clinical expertise.

### Open Question 4
- Question: To what extent does the segment-level labelling noise affect the performance of the baseline model compared to the proposed speech-level model, and can this be quantified by varying the segmentation granularity?
- Basis in paper: [explicit] The paper states that the baseline model's comparatively low performance might have been caused by noise introduced by segment-level labelling.
- Why unresolved: The paper compares a segment-level baseline to a speech-level model but does not quantify how different levels of segmentation granularity impact performance or the introduction of labelling noise.
- What evidence would resolve it: Conducting experiments with varying segmentation lengths (e.g., 2-second, 5-second, 10-second segments) and measuring the performance of a segment-level model at each granularity to quantify the impact of labelling noise.

## Limitations
- Dataset generalization: The D-Vlog dataset consists of YouTube vlogs which may not generalize to clinical settings
- Sample size concerns: The sample size (637 downloaded videos) may be insufficient for robust depression detection
- Sentence boundary assumptions: The approach assumes natural sentence boundaries align with depression-relevant content

## Confidence

- High Confidence: The sentence-level processing architecture and cross-attention fusion mechanism are well-established approaches with clear implementation details.
- Medium Confidence: The hierarchical attention interpretation methodology is described but lacks specific implementation details and validation against known depression markers.
- Low Confidence: The claim that this approach significantly outperforms segment-level baselines may be dataset-specific and not generalizable to other depression detection scenarios.

## Next Checks

1. Cross-dataset validation: Test the model on a clinically-annotated depression dataset (e.g., DAIC-WOZ) to assess generalizability beyond YouTube vlogs and verify if the performance advantage over segment-level approaches persists.

2. Ablation study with ground truth alignment: Remove the hierarchical attention interpretation and compare predictions against known acoustic markers of depression (pitch variation, pause duration, speech rate) to validate whether the model is actually learning depression-relevant patterns.

3. Modality dependency analysis: Conduct experiments where either audio or text modalities are systematically degraded or removed to quantify the actual contribution of each modality to the model's performance and determine if the cross-attention fusion provides meaningful improvements over unimodal approaches.