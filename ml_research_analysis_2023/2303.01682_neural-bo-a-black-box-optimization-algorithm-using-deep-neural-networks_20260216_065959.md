---
ver: rpa2
title: 'Neural-BO: A Black-box Optimization Algorithm using Deep Neural Networks'
arxiv_id: '2303.01682'
source_url: https://arxiv.org/abs/2303.01682
tags:
- lemma
- neural
- optimization
- algorithm
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Neural-BO, a novel black-box optimization
  algorithm that leverages deep neural networks (DNNs) to model complex, high-dimensional
  functions without requiring Bayesian neural networks for uncertainty estimation.
  The method utilizes advances in Neural Tangent Kernel (NTK) theory to estimate confidence
  intervals and employs Thompson Sampling for exploration-exploitation trade-off.
---

# Neural-BO: A Black-box Optimization Algorithm using Deep Neural Networks

## Quick Facts
- arXiv ID: 2303.01682
- Source URL: https://arxiv.org/abs/2303.01682
- Authors: Not specified in input
- Reference count: 40
- Key outcome: Neural-BO achieves sub-linear regret bounds and outperforms existing methods on synthetic and real-world optimization tasks using deep neural networks with NTK theory

## Executive Summary
Neural-BO introduces a novel black-box optimization algorithm that leverages deep neural networks to model complex, high-dimensional functions without requiring Bayesian neural networks for uncertainty estimation. The method utilizes advances in Neural Tangent Kernel (NTK) theory to estimate confidence intervals and employs Thompson Sampling for exploration-exploitation trade-off. Theoretical analysis shows Neural-BO achieves sub-linear regret bounds, improving upon existing neural-based contextual bandit methods. Experiments on synthetic benchmarks and real-world tasks demonstrate superior performance compared to Gaussian process-based, random forest, and other neural network-based approaches.

## Method Summary
Neural-BO uses a two-layer neural network with ReLU activation trained via SGD to model the black-box function. The algorithm employs Thompson Sampling for acquisition, using NTK-based confidence intervals for exploration-exploitation trade-off. Unlike traditional Bayesian optimization methods, Neural-BO doesn't require Bayesian neural networks for uncertainty estimation, making it computationally favorable. The method scales linearly with data points by avoiding kernel matrix inversion required in Gaussian processes while maintaining similar theoretical guarantees. Neural-BO handles high-dimensional structured data by leveraging neural networks' feature extraction capabilities.

## Key Results
- Achieves sub-linear regret bounds on synthetic benchmarks (Ackley, Levy, Michalewics functions) improving upon existing neural-based contextual bandit methods
- Outperforms Gaussian process-based methods (GP-UCB, GP-TS, GP-EI), random forest (SMAC), and other neural approaches (DNGO, NeuralGreedy) on real-world tasks
- Scales linearly with data points and performs well on complex structured data like images and text
- Shows 2-3x improvement in detection rate for sensitive sample generation on MNIST compared to GP-UCB

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural-BO achieves sub-linear regret by using over-parameterized neural networks with Neural Tangent Kernel (NTK) theory to estimate confidence intervals without requiring Bayesian neural networks.
- Mechanism: The algorithm trains a deep neural network to model the black-box function, then uses Thompson Sampling with NTK-based confidence intervals for exploration-exploitation trade-off.
- Core assumption: The true function lies in the Reproducing Kernel Hilbert Space (RKHS) corresponding to the NTK with bounded norm, and the neural network width satisfies Condition 1 for generalization guarantees.
- Evidence anchors:
  - [abstract]: "Theoretical analysis shows Neural-BO achieves sub-linear regret bounds, improving upon existing neural-based contextual bandit methods."
  - [section 5]: "We analyze the theoretical behavior of our algorithm in terms of regret bound using advances in NTK theory showing its efficient convergence."
  - [corpus]: Weak evidence - corpus neighbors focus on Bayesian neural networks and black-box optimization but don't specifically address NTK-based confidence intervals without BNNs.

### Mechanism 2
- Claim: Neural-BO scales linearly with data points by avoiding kernel matrix inversion required in Gaussian processes.
- Mechanism: Uses neural networks with gradient descent for training, avoiding the cubic complexity of kernel inversion in GPs while maintaining similar theoretical guarantees.
- Core assumption: The computational efficiency gain from avoiding kernel inversion outweighs any potential loss in modeling accuracy for complex structured data.
- Evidence anchors:
  - [abstract]: "The algorithm scales linearly with data points and performs well on complex structured data like images and text."
  - [section 1]: "GP-based Bayesian optimization is that the computational complexity grows cubically in the number of observations, as it necessitates the inversion of the covariance matrix."
  - [section 4]: "Our algorithm does not need a Bayesian Neural Network to model predictive uncertainty and is therefore computationally favorable."

### Mechanism 3
- Claim: Neural-BO handles high-dimensional structured data (images, text) effectively by leveraging neural networks' feature extraction capabilities.
- Mechanism: Neural networks can learn powerful representations from complex structured data, unlike kernel methods that struggle with curse of dimensionality.
- Core assumption: The neural network architecture can effectively learn representations that capture the relevant structure in high-dimensional data.
- Evidence anchors:
  - [abstract]: "perform well on complex structured data like images and text"
  - [section 1]: "kernel methods are usually not effective on complex structured high dimensional data due to curse of dimensionality"
  - [section 2]: "The emergence of DNNs proves their power to scale and generalize to most kinds of data, including high dimensional and structured data as they can extract powerful features"

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) theory
  - Why needed here: Provides theoretical foundation for analyzing over-parameterized neural networks and their generalization properties in the context of Bayesian optimization
  - Quick check question: What is the key insight of NTK theory that allows us to analyze infinitely wide neural networks as linear models?

- Concept: Reproducing Kernel Hilbert Space (RKHS) with NTK kernel
  - Why needed here: Defines the function class that the black-box function is assumed to belong to, enabling the theoretical regret analysis
  - Quick check question: How does the RKHS norm bound B relate to the smoothness assumptions on the objective function?

- Concept: Thompson Sampling in continuous action spaces
  - Why needed here: Provides the exploration-exploitation mechanism adapted for continuous optimization rather than discrete action spaces
  - Quick check question: How does the discretization technique used in Neural-BO address the challenge of continuous action spaces in Thompson Sampling?

## Architecture Onboarding

- Component map: Neural network model -> NTK-based confidence interval estimator -> Thompson Sampling module -> Gradient descent trainer -> Regret calculation and analysis framework

- Critical path:
  1. Initialize neural network with He initialization
  2. At each iteration: compute confidence intervals using NTK theory
  3. Sample function from posterior using Thompson Sampling
  4. Optimize sampled function to select next evaluation point
  5. Update network parameters with new observation
  6. Update NTK-based statistics for next iteration

- Design tradeoffs:
  - Over-parameterization vs. generalization: Requires wide networks for NTK analysis but may overfit if too wide
  - Fixed vs. dynamic feature maps: Uses g(x; θ0) instead of g(x; θt) for simpler regret analysis but may be less adaptive
  - Discretization granularity vs. computational cost: Finer discretization improves approximation but increases computation

- Failure signatures:
  - Poor performance on structured data: May indicate insufficient network width or inappropriate architecture
  - Degrading regret bounds: Could signal violation of NTK assumptions or insufficient exploration
  - High computational cost: May indicate inefficient implementation or need for architectural optimization

- First 3 experiments:
  1. Test on synthetic Ackley function in 50D with varying network widths to validate Condition 1 requirements
  2. Compare GPU memory usage vs. GP-UCB on 14D robot pushing task to verify linear scaling claim
  3. Test on MNIST-based sensitive sample generation to verify structured data handling capability

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided content.

## Limitations
- Theoretical regret analysis assumes the true function lies in the NTK RKHS with bounded norm, which may not hold for all black-box functions
- Performance claims on complex structured data are based on limited experiments, primarily MNIST-based sensitive sample generation
- The computational scaling advantage assumes efficient neural network training, which may not hold for extremely large models or datasets

## Confidence

- **High confidence**: Neural-BO achieves better performance than GP-based methods on the tested synthetic and real-world benchmarks
- **Medium confidence**: The theoretical regret bound analysis using NTK theory provides valid justification for the algorithm's convergence properties
- **Low confidence**: Neural-BO effectively handles complex structured data like images and text

## Next Checks

1. **Regret validation**: Design experiments that explicitly measure and compare cumulative regret over time for Neural-BO versus GP-based methods on standard optimization benchmarks.

2. **Scaling verification**: Conduct controlled experiments measuring GPU memory usage and wall-clock time as a function of data points for both Neural-BO and GP-UCB on identical hardware.

3. **Structured data breadth**: Test Neural-BO on diverse high-dimensional structured datasets including CIFAR-10 images and natural language processing tasks to validate the structured data handling claims.