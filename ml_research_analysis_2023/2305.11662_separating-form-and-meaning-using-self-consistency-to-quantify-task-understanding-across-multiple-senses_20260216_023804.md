---
ver: rpa2
title: 'Separating form and meaning: Using self-consistency to quantify task understanding
  across multiple senses'
arxiv_id: '2305.11662'
source_url: https://arxiv.org/abs/2305.11662
tags:
- sentence
- consistency
- language
- task
- translations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method to evaluate language model
  understanding by testing consistency across different linguistic senses (here, languages).
  Instead of requiring fixed multilingual test sets, the method uses the model itself
  to generate translations of both data and instructions, then measures how consistently
  the model responds to the original and translated versions.
---

# Separating form and meaning: Using self-consistency to quantify task understanding across multiple senses

## Quick Facts
- arXiv ID: 2305.11662
- Source URL: https://arxiv.org/abs/2305.11662
- Reference count: 19
- Key outcome: Language models show low consistency across languages even with high-quality translations, suggesting form-dependent task understanding.

## Executive Summary
This paper introduces a novel method to evaluate language model understanding by testing consistency across different linguistic senses (here, languages). Instead of requiring fixed multilingual test sets, the method uses the model itself to generate translations of both data and instructions, then measures how consistently the model responds to the original and translated versions. Applied to ChatGPT on paraphrase detection and natural language inference across English, German, and Chinese, the study finds that consistency is low across all language pairs—particularly when translating from English to Chinese—even though translation quality is high and task accuracy is reasonable. Accuracy can even improve when translating inputs to English before task completion. The findings indicate that current models do not maintain language-independent task understanding, and that consistency across senses is a valuable complement to standard benchmarks.

## Method Summary
The study evaluates multilingual consistency in large language models by comparing responses on original and translated inputs. Using GPT-3.5-Turbo-0301, the method generates model-internal translations of both instructions and data across English, German, and Chinese. The researchers measure consistency by comparing standardized responses (binary/ternary classification) between original and translated inputs, while also evaluating translation quality (BLEU, ROUGE, COMET-22) and task accuracy. They conduct ablation experiments translating only instructions, only data, or both to isolate whether inconsistencies stem from interpretation, execution, or both. The approach leverages PAWS-X and XNLI benchmark datasets with English task prompts translated by native speakers.

## Key Results
- Consistency is low across all language pairs (0.44-0.66), with English→Chinese showing the lowest values
- Even high-quality translations (BLEU > 50) show significant inconsistencies, with correlations between translation quality and consistency being very low (≤ 0.09)
- Translating inputs to English before task completion improves accuracy, suggesting language-dependent understanding
- Inconsistencies are driven by both differences in task interpretation and execution, with execution differences being more pronounced
- Consistency drops further when only instructions or only data are translated, indicating form-dependent understanding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Consistency across languages is a valid proxy for genuine task understanding because correct understanding should yield invariant outputs across different linguistic representations of the same meaning.
- Mechanism: The method generates alternative linguistic senses (translations) via the model itself, then compares responses on original and translated inputs. If task understanding is language-independent, responses should be consistent across these senses.
- Core assumption: The model's translations are meaning-preserving and the benchmark task is insensitive to subtle meaning changes.
- Evidence anchors:
  - [abstract] "We measure understanding not in terms of correctness but by evaluating consistency across multiple senses that are generated by the model itself."
  - [section 2] "we are thus using multilingual self-consistency as a litmus test for their understanding"
- Break condition: If translations introduce meaning shifts or the task is sensitive to subtle semantic nuances, inconsistencies may reflect translation quality rather than task understanding.

### Mechanism 2
- Claim: The model's task understanding varies with the linguistic form (language) used to represent the task, not just with translation quality or execution ability.
- Mechanism: By ablating the translation process (translating only instructions, only data, or both), the study isolates whether inconsistencies stem from interpretation, execution, or both. Consistent drops across conditions indicate form-dependent understanding.
- Core assumption: The model can execute the task reasonably well in at least one language, isolating the form effect.
- Evidence anchors:
  - [section 4.2] "inconsistencies seem to be driven by differences in both task interpretation and execution, although differences in execution are more pronounced."
  - [section 4.4] "translations from German or Chinese to English lead to an increase in accuracy"
- Break condition: If the model cannot execute the task well in any language, the form-dependent differences cannot be isolated.

### Mechanism 3
- Claim: Low consistency is not due to poor translation quality because BLEU/ROUGE/COMET scores are high across most language pairs, and consistency remains low even for high-quality translations.
- Mechanism: High translation quality metrics indicate meaning preservation, so inconsistencies must reflect genuine differences in task understanding across languages.
- Core assumption: Translation quality metrics adequately capture meaning preservation for the task.
- Evidence anchors:
  - [section 4.3] "All metrics indicate that the model's translations are of high quality across tasks and languages"
  - [section 4.3] "correlations between BLEU score and consistency are very low (≤ 0.09)"
- Break condition: If translation quality metrics fail to capture task-relevant meaning differences, low consistency could still reflect translation issues.

## Foundational Learning

- Concept: Fregean senses and meaning invariance
  - Why needed here: The paper's core assumption is that correct understanding should yield consistent responses across different senses (languages) of the same meaning.
  - Quick check question: If a person understands a concept, should their response change when the same question is asked in a different language?

- Concept: Self-consistency in language models
  - Why needed here: The method evaluates consistency by comparing the model's responses on original and translated inputs, assuming consistency indicates understanding.
  - Quick check question: If a model gives different answers to the same question asked in different languages, does this necessarily indicate poor understanding?

- Concept: Translation quality metrics (BLEU, ROUGE, COMET)
  - Why needed here: These metrics are used to verify that inconsistencies are not due to poor translation quality.
  - Quick check question: If a translation has a high BLEU score, does this guarantee that the meaning is perfectly preserved for any possible task?

## Architecture Onboarding

- Component map:
  Model (GPT-3.5-Turbo-0301) -> Translation pipeline (model-internal zero-shot translations) -> Data (PAWS-X and XNLI in English, German, Chinese) -> Instructions (English prompts with native translations) -> Evaluation (accuracy + consistency between original and translated responses)

- Critical path:
  1. Load benchmark data and instructions
  2. Generate model-internal translations (instructions and/or data)
  3. Query model with original and translated inputs
  4. Standardize responses and calculate accuracy
  5. Calculate consistency between original and translated responses
  6. Analyze results across translation conditions

- Design tradeoffs:
  - Using model-internal translations ensures meaning preservation but introduces potential self-consistency issues
  - Focusing on binary/ternary classification simplifies response standardization but limits task diversity
  - Zero-shot approach avoids fine-tuning but may miss task-specific nuances

- Failure signatures:
  - High translation quality but low consistency suggests form-dependent understanding
  - Low accuracy on original language indicates model cannot execute task, invalidating form-dependent analysis
  - Inconsistent responses even on identical inputs suggest model instability

- First 3 experiments:
  1. Baseline: Run model on original English data with English instructions, record accuracy and self-consistency
  2. Translation of both data and instructions: Translate English data and instructions to German, run model, compare accuracy and consistency to baseline
  3. Ablation: Translate only instructions to German while keeping original English data, compare consistency to baseline to isolate interpretation vs execution effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does ChatGPT exhibit cross-lingual consistency on other tasks beyond paraphrase detection and natural language inference?
- Basis in paper: [explicit] The paper evaluates consistency on two specific tasks (PAWS-X and XNLI) across three languages and finds low consistency, but does not test other tasks.
- Why unresolved: The study is limited to a narrow set of tasks and languages; it's unclear whether inconsistency is task-specific or a general property of the model.
- What evidence would resolve it: Testing ChatGPT on additional tasks (e.g., question answering, summarization, reasoning) and languages to measure cross-lingual consistency.

### Open Question 2
- Question: Are inconsistencies in ChatGPT's cross-lingual task understanding due to lack of shared representations or differences in how the model processes different languages?
- Basis in paper: [inferred] The paper shows inconsistencies persist even with high-quality translations and rules out translation errors, suggesting the issue may lie in how the model represents meaning across languages.
- Why unresolved: The paper does not analyze internal representations or investigate whether language-specific processing contributes to inconsistency.
- What evidence would resolve it: Analyzing internal representations (e.g., via probing tasks or alignment studies) to see if task-relevant information is shared across languages.

### Open Question 3
- Question: Would using model-internal paraphrases instead of translations yield similar or better consistency in task understanding?
- Basis in paper: [explicit] The paper suggests paraphrasing as an alternative way to generate different senses of the same meaning.
- Why unresolved: The study only uses translation to create alternative senses; paraphrasing has not been tested as a method for generating consistent inputs.
- What evidence would resolve it: Conducting consistency experiments using model-generated paraphrases instead of translations and comparing the results.

### Open Question 4
- Question: Does improving translation quality beyond current levels (BLEU > 50) significantly reduce inconsistencies in cross-lingual task performance?
- Basis in paper: [explicit] The paper finds that even among high-quality translations (BLEU > 50), inconsistencies persist, but the effect of further improving translation quality is not explored.
- Why unresolved: The analysis stops at BLEU > 50, leaving open whether even higher translation quality could reduce inconsistency.
- What evidence would resolve it: Testing consistency with translations achieving even higher BLEU scores or using human-verified translations to see if inconsistency decreases further.

## Limitations
- The method relies on model-internal translations, which may introduce self-consistency issues or meaning shifts not captured by standard translation quality metrics
- Focus on binary/ternary classification tasks limits generalizability to more complex reasoning tasks where consistency is harder to define
- The study doesn't fully disentangle whether inconsistencies arise from genuine differences in understanding versus execution difficulties

## Confidence
- Core claim (consistency indicates understanding): Medium
- Form-dependent understanding conclusion: Medium
- Translation quality metrics adequacy: Medium

## Next Checks
1. **Cross-model validation**: Replicate the consistency analysis using different LLMs (e.g., Claude, LLaMA) to determine if the observed patterns are specific to ChatGPT or reflect broader phenomena in language model architectures.

2. **Task complexity expansion**: Test the consistency framework on tasks with continuous or multi-dimensional outputs (e.g., sentiment strength ratings, numerical reasoning) to assess whether the method generalizes beyond binary/ternary classification.

3. **Controlled meaning perturbation**: Systematically introduce controlled semantic variations in translations to map the relationship between translation quality, meaning preservation, and consistency, helping isolate whether inconsistencies reflect true understanding gaps or translation artifacts.