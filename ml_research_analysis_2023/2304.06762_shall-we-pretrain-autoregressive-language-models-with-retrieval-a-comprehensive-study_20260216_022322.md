---
ver: rpa2
title: Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive
  Study
arxiv_id: '2304.06762'
source_url: https://arxiv.org/abs/2304.06762
tags:
- retro
- retrieval
- evaluation
- text
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the potential of pretraining autoregressive
  language models with retrieval by conducting a comprehensive comparison between
  RETRO and standard GPT. RETRO is a retrieval-augmented language model that scales
  to trillions of tokens and reduces model parameters while achieving lower perplexity.
---

# Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study

## Quick Facts
- **arXiv ID:** 2304.06762
- **Source URL:** https://arxiv.org/abs/2304.06762
- **Reference count:** 21
- **Primary result:** RETRO (retrieval-augmented autoregressive language model) outperforms GPT on text generation quality, factual accuracy, and knowledge-intensive tasks while using fewer parameters

## Executive Summary
This comprehensive study investigates whether autoregressive language models should be pretrained with retrieval by comparing RETRO against standard GPT across multiple dimensions. RETRO, which incorporates a retrieval mechanism that scales to trillions of tokens while reducing model parameters, demonstrates superior performance in several key areas. The model shows significantly less text degeneration (repetition), moderately higher factual accuracy, and slightly lower toxicity when using a non-toxic retrieval database. Most notably, RETRO substantially outperforms GPT on knowledge-intensive tasks while maintaining comparable performance on other benchmarks. The study also introduces RETRO++, a variant that further improves open-domain QA results by prioritizing relevant evidence for the decoder and using additional evidence for the encoder.

## Method Summary
The study pretrains both GPT and RETRO models of varying sizes (148M to 9.5B parameters) using identical transformer architectures and pretraining schedules on a corpus of 330B tokens. RETRO implements chunk-wise retrieval by splitting both input sequences and the retrieval datastore into chunks (size=64), using Faiss with IVF+HNSW and PQ encoding for fast approximate nearest neighbor search. The models are evaluated on text generation quality (repetition percentage, self-BLEU, Zipf coefficient), factual accuracy (NEER, EntailR, MC1/MC2), toxicity (Expected Max Toxicity, Toxicity Probability), and downstream tasks including the LM Evaluation Harness benchmark and open-domain QA datasets. RETRO++ is introduced as a variant that improves evidence prioritization for the decoder and encoder.

## Key Results
- RETRO outperforms GPT on text generation with significantly less repetition (up to 90% reduction) across different model sizes
- RETRO achieves moderately higher factual accuracy and slightly lower toxicity when using a non-toxic retrieval database
- On LM Evaluation Harness benchmark, RETRO substantially outperforms GPT on knowledge-intensive tasks (Hellaswag, BoolQ) while maintaining parity on other tasks
- RETRO++ improves open-domain QA results, achieving +8.6 EM score improvement on Natural Questions compared to original RETRO

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** RETRO reduces text repetition compared to GPT by leveraging retrieval
- **Mechanism:** Chunk-wise retrieval allows RETRO to reference external knowledge during generation, reducing reliance on memorized content and enabling more diverse outputs
- **Core assumption:** The retrieval database contains relevant and accurate information
- **Evidence anchors:** RETRO reduces repetition percentage compared to GPT by a large margin across different sizes
- **Break condition:** If the retrieval database is noisy or irrelevant, benefits may not materialize

### Mechanism 2
- **Claim:** RETRO significantly outperforms GPT on knowledge-intensive tasks in zero-shot evaluation
- **Mechanism:** The retrieval module provides access to factual knowledge during inference, allowing answers to questions requiring specific information not in model parameters
- **Core assumption:** The retrieval database contains necessary knowledge for the tasks
- **Evidence anchors:** RETRO largely outperforms GPT on knowledge-intensive tasks (Hellaswag, BoolQ) with 6 of 8 cases showing improvements
- **Break condition:** If tasks don't require external knowledge or database lacks relevant information, no significant improvements expected

### Mechanism 3
- **Claim:** RETRO++ improves open-domain QA by prioritizing relevant evidence
- **Mechanism:** RETRO++ feeds most relevant evidence into the decoder and additional evidence into the encoder, focusing on pertinent information while leveraging context
- **Core assumption:** Retrieved evidence is relevant and can be effectively utilized by the model architecture
- **Evidence anchors:** RETRO++ improves EM score by +8.6 on Natural Questions compared to original RETRO
- **Break condition:** If retrieved evidence is not relevant or architecture doesn't utilize prioritized evidence effectively, no improvements expected

## Foundational Learning

- **Concept:** Autoregressive Language Models
  - **Why needed here:** Understanding autoregressive models is crucial for grasping how RETRO extends this architecture with retrieval
  - **Quick check question:** What is the key difference between autoregressive and masked language models?

- **Concept:** Retrieval-Augmented Generation
  - **Why needed here:** Central to understanding how RETRO incorporates external knowledge into generation
  - **Quick check question:** How does retrieval-augmented generation differ from traditional language model generation?

- **Concept:** Chunk-wise Retrieval
  - **Why needed here:** RETRO's scalability relies on retrieving at the chunk level
  - **Quick check question:** Why does RETRO split both input sequence and retrieval datastore into chunks?

## Architecture Onboarding

- **Component map:** Transformer-based decoder (similar to GPT) augmented with retrieval module containing dense retriever (Faiss) and chunked text database. RETRO++ modifies architecture by prioritizing evidence for decoder and encoder.
- **Critical path:** During generation, RETRO retrieves relevant chunks based on previous chunk, then uses chunk-wise cross-attention to incorporate information into next chunk's generation. RETRO++ prioritizes most relevant evidence for decoder and additional evidence for encoder.
- **Design tradeoffs:** RETRO trades increased inference complexity and storage requirements for improved performance and reduced parameter count. RETRO++ further trades architectural complexity for improved QA performance.
- **Failure signatures:** Poor performance may indicate issues with retrieval database (noise, irrelevance), retrieval quality (incorrect nearest neighbors), or model architecture (ineffective incorporation of retrieved information).
- **First 3 experiments:**
  1. Compare RETRO and GPT on simple text generation task to verify reduction in repetition
  2. Evaluate RETRO and GPT on knowledge-intensive task (BoolQ) to confirm zero-shot performance improvement
  3. Test RETRO++ on open-domain QA task (Natural Question) to verify EM score improvement

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does chunk-wise retrieval mechanism in RETRO affect factual accuracy and text quality compared to GPT models?
- **Basis in paper:** The paper discusses impact of chunk-wise retrieval on text generation quality and factual accuracy
- **Why unresolved:** The paper doesn't provide detailed comparison of factual accuracy and text quality between RETRO and GPT with varying chunk sizes
- **What evidence would resolve it:** Comprehensive study comparing factual accuracy and text quality of RETRO and GPT with different chunk sizes

### Open Question 2
- **Question:** What are specific factors contributing to improved performance of RETRO in knowledge-intensive tasks?
- **Basis in paper:** The paper mentions RETRO significantly outperforms GPT on knowledge-intensive tasks
- **Why unresolved:** The paper doesn't delve into specific factors contributing to this improved performance
- **What evidence would resolve it:** In-depth analysis of factors contributing to RETRO's improved performance in knowledge-intensive tasks

### Open Question 3
- **Question:** How does retrieval-augmented generation approach in RETRO impact scalability and efficiency of the model?
- **Basis in paper:** The paper discusses scalability of RETRO due to chunk-wise retrieval mechanism
- **Why unresolved:** The paper doesn't provide detailed analysis of impact on scalability and efficiency
- **What evidence would resolve it:** Comprehensive study on scalability and efficiency of RETRO with varying retrieval strategies and database sizes

## Limitations

- The study shows inconsistent performance across different tasks and model sizes, with some improvements appearing marginal
- The exact contribution of retrieval versus other factors (model size, pretraining data) is not fully isolated
- Limited evaluation of toxicity reduction, with insufficient evidence to rule out alternative explanations

## Confidence

**High Confidence:** RETRO reduces text repetition compared to GPT, well-supported by quantitative evidence showing significant reductions across different model sizes.

**Medium Confidence:** Improvements in factual accuracy and knowledge-intensive tasks are supported by benchmark results, but exact contribution of retrieval versus other factors is not fully isolated.

**Low Confidence:** Toxicity reduction claim with non-toxic retrieval database is based on limited evaluation and could be influenced by factors beyond retrieval mechanism.

## Next Checks

1. **Cross-database validation:** Evaluate RETRO using multiple retrieval databases with varying quality and domain coverage to determine robustness to database quality and relevance.

2. **Ablation study on retrieval frequency:** Systematically vary frequency of retrieval steps during generation to quantify optimal balance between computational cost and performance gains.

3. **Domain-specific evaluation:** Test RETRO and RETRO++ on specialized domains (medical, legal, technical) where pretraining corpus may have limited coverage to assess how well retrieval mechanism compensates for knowledge gaps.