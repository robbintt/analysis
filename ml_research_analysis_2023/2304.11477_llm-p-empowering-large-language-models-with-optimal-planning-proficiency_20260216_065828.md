---
ver: rpa2
title: 'LLM+P: Empowering Large Language Models with Optimal Planning Proficiency'
arxiv_id: '2304.11477'
source_url: https://arxiv.org/abs/2304.11477
tags:
- planning
- problem
- language
- pddl
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling large language models
  (LLMs) to solve complex planning problems that require long-horizon reasoning. The
  authors propose LLM+P, a framework that combines LLMs with classical planners to
  leverage the strengths of both.
---

# LLM+P: Empowering Large Language Models with Optimal Planning Proficiency

## Quick Facts
- arXiv ID: 2304.11477
- Source URL: https://arxiv.org/abs/2304.11477
- Reference count: 40
- Key outcome: LLM+P achieves 56.7-95% success rates vs 0-40% for LLM-only baselines on seven planning domains

## Executive Summary
LLM+P addresses the challenge of enabling large language models to solve complex planning problems requiring long-horizon reasoning. The framework combines LLMs with classical planners, leveraging LLMs' natural language processing capabilities to convert problem descriptions into PDDL format, using classical planners to find optimal solutions, and translating results back into natural language. Experiments demonstrate significantly higher success rates compared to LLM-only approaches across seven benchmark planning domains.

## Method Summary
LLM+P is a framework that converts natural language problem descriptions into PDDL format using LLMs, solves the PDDL problems with classical planners, and translates the solutions back into natural language. The approach uses in-context learning to generate correct PDDL problem encodings without fine-tuning. The system is evaluated across seven planning domains (BlocksWorld, Barman, FloorTile, Grippers, Storage, Termes, TyreWorld) with 20 problems each, comparing four methods: LLM-AS-P without context, LLM-AS-P with context, LLM+P without context, and LLM+P with context.

## Key Results
- LLM+P achieves 56.7-95% success rates across seven planning domains
- LLM-only approaches achieve 0-40% success rates
- In-context learning significantly improves LLM+P performance
- Most failures occur due to mis-specified problem files missing initial conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM+P leverages LLMs' strength in natural language processing to convert problem descriptions into PDDL format.
- Mechanism: LLMs excel at text generation and understanding, allowing them to translate natural language descriptions into the structured PDDL format required by classical planners.
- Core assumption: LLMs can accurately translate natural language problem descriptions into PDDL format.
- Evidence anchors:
  - [abstract] "LLM+P takes in a natural language description of a planning problem, then returns a correct (or optimal) plan for solving that problem in natural language. LLM+P does so by first converting the language description into a file written in the planning domain definition language (PDDL)"
  - [section] "LLMs are bad at planning (or long-horizon reasoning) [9] but they are good at describing and translating textual inputs, including re-writing planning prompts in the PDDL format."
- Break condition: If LLMs cannot accurately translate natural language descriptions into PDDL format, the entire pipeline breaks down.

### Mechanism 2
- Claim: Classical planners are used to find optimal solutions for the translated PDDL problems.
- Mechanism: Once the problem is converted to PDDL, classical planners can use their efficient search algorithms to find optimal solutions.
- Core assumption: Classical planners can efficiently find optimal solutions for the translated PDDL problems.
- Evidence anchors:
  - [abstract] "LLM+P does so by first converting the language description into a file written in the planning domain definition language (PDDL), then leveraging classical planners to quickly find a solution"
  - [section] "There exists a rich set of symbolic planners that implement efficient search algorithms to solve planning problems formalized in PDDL."
- Break condition: If the classical planners cannot find a solution for the translated PDDL problem, the pipeline fails.

### Mechanism 3
- Claim: LLMs are used again to convert the planner's output back into natural language.
- Mechanism: LLMs can translate the structured output of classical planners back into natural language, making the solution understandable to users.
- Core assumption: LLMs can accurately translate the structured output of classical planners back into natural language.
- Evidence anchors:
  - [abstract] "and then translating the found solution back into natural language."
  - [section] "In the end, the LLM translates the PDDL plan back into the natural language to finish up the LLM+P pipeline."
- Break condition: If LLMs cannot accurately translate the planner's output back into natural language, the final solution may be incomprehensible to users.

## Foundational Learning

- Concept: PDDL (Planning Domain Definition Language)
  - Why needed here: PDDL is the standard format for describing planning problems and solutions in classical planning.
  - Quick check question: What are the two main components of a PDDL problem description?

- Concept: In-context learning
  - Why needed here: In-context learning allows LLMs to perform unseen tasks by conditioning on a few input-label pairs without fine-tuning.
  - Quick check question: How does in-context learning enable LLMs to generate correct PDDL problem encodings?

- Concept: Classical planning algorithms
  - Why needed here: Classical planning algorithms are used to find optimal solutions for the translated PDDL problems.
  - Quick check question: What are the key properties of classical planning algorithms that make them suitable for this task?

## Architecture Onboarding

- Component map:
  Natural language problem description -> LLM for PDDL translation -> Classical planner for solution finding -> LLM for natural language translation of solution

- Critical path:
  1. Convert natural language problem to PDDL
  2. Solve PDDL problem with classical planner
  3. Convert planner's output back to natural language

- Design tradeoffs:
  - Accuracy vs. speed: Using more complex LLMs or planners may improve accuracy but slow down the process.
  - Dependency on human-provided domain knowledge: The system relies on human experts to provide PDDL domain files.

- Failure signatures:
  - Incorrect PDDL problem encoding by LLM
  - Classical planner unable to find a solution
  - LLM unable to accurately translate planner's output back to natural language

- First 3 experiments:
  1. Test LLM's ability to translate a simple natural language problem to PDDL.
  2. Verify that the classical planner can find a solution for a basic PDDL problem.
  3. Check if the LLM can accurately translate a simple planner output back to natural language.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLM+P be extended to handle domains where the PDDL domain file is not provided by a human expert?
- Basis in paper: [explicit] The authors mention that automatically generating the domain description is a valuable research question but assume it is available in this work.
- Why unresolved: This requires developing methods for LLMs to learn and represent domain knowledge from natural language descriptions or examples.
- What evidence would resolve it: A demonstration of LLM+P working without a pre-provided domain file, either by generating the domain file from natural language or by directly reasoning about the problem.

### Open Question 2
- Question: What is the impact of the complexity and size of the PDDL problem on the performance of LLM+P?
- Basis in paper: [inferred] The paper evaluates LLM+P on problems with up to 20 tasks per domain, but does not explore how performance scales with larger or more complex problems.
- Why unresolved: Scaling to real-world problems with hundreds or thousands of objects and constraints is a significant challenge for LLMs and planners alike.
- What evidence would resolve it: Empirical results showing the success rate and planning time of LLM+P on problems with varying sizes and complexities.

### Open Question 3
- Question: How can LLM+P be improved to handle failures due to mis-specified problem files, such as missing initial conditions?
- Basis in paper: [explicit] The authors note that most failed cases of LLM+P are due to mis-specified problem files.
- Why unresolved: Detecting and correcting errors in the generated PDDL files requires additional reasoning and validation steps.
- What evidence would resolve it: A method for LLM+P to identify and fix common errors in the generated PDDL files, leading to improved success rates on previously failed problems.

## Limitations
- Domain file dependency: The framework requires manually crafted PDDL domain files, limiting its ability to handle novel domains without human intervention
- Context sensitivity: Performance heavily depends on the quality and relevance of in-context examples provided to the LLM
- Scalability concerns: The approach may not scale well to domains with complex or large state spaces that are difficult to describe in natural language

## Confidence
- High confidence: The experimental results showing LLM+P outperforming LLM-only baselines (56.7-95% vs 0-40% success rates) are well-supported by the data
- Medium confidence: The claim that LLMs can reliably translate natural language to PDDL format, as this depends on the quality of in-context examples and the complexity of the domain
- Medium confidence: The assertion that classical planners can efficiently solve all translated PDDL problems, as some complex domains may still pose challenges

## Next Checks
1. **Generalization test**: Evaluate LLM+P on domains not included in the original seven benchmarks to assess its ability to handle novel planning problems
2. **Ablation study**: Systematically remove the in-context learning component to quantify its contribution to overall performance
3. **Robustness analysis**: Test the framework with deliberately ambiguous or incomplete natural language descriptions to evaluate its error handling and recovery capabilities