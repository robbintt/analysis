---
ver: rpa2
title: 'GEMRec: Towards Generative Model Recommendation'
arxiv_id: '2308.02205'
source_url: https://arxiv.org/abs/2308.02205
tags:
- generative
- images
- image
- prompts
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for generative model recommendation,
  addressing the challenge of personalizing recommendations when items are infinitely
  created by generative models. The proposed two-stage approach consists of Prompt-Model
  Retrieval and Generated Item Ranking.
---

# GEMRec: Towards Generative Model Recommendation

## Quick Facts
- arXiv ID: 2308.02205
- Source URL: https://arxiv.org/abs/2308.02205
- Reference count: 33
- Key outcome: Novel framework for generative model recommendation with two-stage approach (Prompt-Model Retrieval and Generated Item Ranking) and GEMRec-18K dataset

## Executive Summary
This paper introduces a novel framework for personalized generative model recommendation, addressing the challenge of exploring infinitely created items in text-to-image generation. The authors propose a two-stage approach that first retrieves candidate models based on pre-defined prompts, then allows users to rank generated images through interactive feedback. They release GEMRec-18K, an 18K-image dataset spanning 200 models and 90 prompts, enabling research on generative model recommendation. The work identifies limitations of existing evaluation metrics and proposes GRE-Score, a weighted combination that balances relevance and diversity.

## Method Summary
The proposed two-stage framework addresses generative model recommendation through Prompt-Model Retrieval and Generated Item Ranking. The system first generates images for all prompt-model pairs, then uses retrieval metrics (CLIP-Score, mCos, popularity) to identify candidate models. Users interact with an interface to explore and rank models based on their preferences. The GRE-Score metric combines multiple evaluation measures using learned weights to provide comprehensive assessment. The GEMRec-18K dataset provides the foundation for this research, containing 18K images generated by 200 models across 90 prompts with associated metadata.

## Key Results
- Demonstrated two-stage framework effectiveness for exploring generative models through user interaction
- Released GEMRec-18K dataset with 18K images generated by 200 models on 90 prompts
- Proposed GRE-Score metric that balances relevance and diversity better than individual metrics
- Identified limitations of existing evaluation metrics through qualitative and quantitative analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage framework enables effective exploration by separating coarse retrieval from fine-grained ranking
- Mechanism: First stage retrieves candidate models based on relevance, popularity, and distinctiveness; second stage allows detailed pairwise feedback on generated images
- Core assumption: Users can effectively judge model quality through visual inspection
- Evidence anchors: Framework description in abstract and section, but weak corpus evidence about user effectiveness
- Break condition: If users cannot distinguish between models visually, approach fails

### Mechanism 2
- Claim: GRE-Score effectively balances relevance and diversity
- Mechanism: Aggregates CLIP-Score, mCos, and popularity using learned weights
- Core assumption: Combining multiple metrics overcomes individual limitations
- Evidence anchors: Metric ensemble description and qualitative results showing diverse high-quality image retrieval
- Break condition: If learned weights don't balance metrics effectively, GRE-Score may not improve recommendations

### Mechanism 3
- Claim: GEMRec-18K enables research through dense prompt-model interactions
- Mechanism: 18K images from 200 models on 90 prompts with metadata and generation configurations
- Core assumption: Large dataset of interactions necessary to understand user preferences
- Evidence anchors: Dataset description in abstract and section, but weak corpus evidence about sufficiency
- Break condition: If dataset lacks real-world representativeness or diversity, research effectiveness limited

## Foundational Learning

- Recommender systems and information retrieval: Understanding classical recommender concepts applied to generative models
  - Why needed: Framework builds on recommender system concepts for generative model context
  - Quick check: How do traditional recommender systems differ from generative model recommendation in candidate item space?

- Text-to-image generation and diffusion models: Knowledge of how these models work and limitations
  - Why needed: Paper focuses on image generation as use case
  - Quick check: What are key components of text-to-image diffusion model and how do they relate to generated image quality?

- Evaluation metrics for image generation: Understanding existing metrics and their limitations
  - Why needed: Paper proposes new metrics (GRE-Score) for generative recommendations
  - Quick check: What are key differences between CLIP-Score, mCos, and popularity as evaluation metrics?

## Architecture Onboarding

- Component map: User Interface -> Prompt-Model Retrieval -> Generated Item Ranking -> User Feedback
- Critical path: Generate images for all prompt-model pairs → Build retrieval interface → Implement ranking algorithm → Collect user feedback for model training
- Design tradeoffs: Computational cost (18K images) vs comprehensive analysis; diversity vs quality in recommendations
- Failure signatures: Users cannot distinguish models visually; GRE-Score doesn't balance metrics effectively; dataset lacks representativeness
- First 3 experiments:
  1. Compare user satisfaction with full corpus vs subset recommendations
  2. Compare GRE-Score performance against individual metrics across different domains
  3. Analyze diversity of recommendations across different prompt domains

## Open Questions the Paper Calls Out

- How do users' aesthetic preferences differ across various domains (illustration, abstract concepts, people)?
  - Basis: Paper discusses heterogeneity across domains but lacks user studies
  - Why unresolved: Preliminary analysis only, no large-scale user preference data
  - Resolution: User studies and large-scale data collection on domain-specific preferences

- How can evaluation metrics be standardized to capture individual aesthetic tastes?
  - Basis: Paper highlights metric limitations and proposes GRE-Score but needs more rigorous validation
  - Why unresolved: New metric proposed but comprehensive user preference alignment studies needed
  - Resolution: Rigorous studies comparing metrics with user feedback and preferences

- How can GEMRec dataset be extended with more comprehensive prompts and models?
  - Basis: Paper mentions plans for more comprehensive datasets including LoRAs and different samplers
  - Why unresolved: Initial dataset presented but extensions not implemented
  - Resolution: Extended dataset with comprehensive prompts, models, LoRAs, and sampler combinations

## Limitations

- Limited empirical evidence about user effectiveness in judging model quality through visual inspection
- GRE-Score lacks comparative validation against established baselines in real-world scenarios
- Dataset may not capture full diversity of user preferences and prompt domains

## Confidence

- **High Confidence**: Technical implementation of two-stage framework and dataset construction methodology
- **Medium Confidence**: Theoretical framework for combining multiple evaluation metrics
- **Low Confidence**: Claims about user preference modeling and real-world applicability

## Next Checks

1. Conduct controlled user study comparing two-stage framework against traditional single-stage approaches, measuring satisfaction and diversity
2. Perform ablation studies on GRE-Score by varying weights and comparing against individual metrics across different domains
3. Test framework performance on prompts and models outside original 90 prompts and 200 models to assess generalization capability