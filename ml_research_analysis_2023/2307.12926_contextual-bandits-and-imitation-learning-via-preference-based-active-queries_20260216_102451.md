---
ver: rpa2
title: Contextual Bandits and Imitation Learning via Preference-Based Active Queries
arxiv_id: '2307.12926'
source_url: https://arxiv.org/abs/2307.12926
tags:
- learning
- regret
- lemma
- algorithm
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses contextual bandits and imitation learning with
  preference-based feedback, where an expert provides binary comparisons between actions
  instead of direct rewards. The proposed algorithms, AURORA and AURORAE, use an online
  regression oracle to estimate a preference function and actively query the expert
  only when necessary.
---

# Contextual Bandits and Imitation Learning via Preference-Based Active Queries

## Quick Facts
- arXiv ID: 2307.12926
- Source URL: https://arxiv.org/abs/2307.12926
- Reference count: 40
- One-line primary result: Achieves best-of-both-worlds regret bounds while minimizing expert queries in contextual bandits and imitation learning with preference feedback

## Executive Summary
This paper introduces algorithms AURORA and AURORAE for contextual bandits and imitation learning with preference-based feedback. The key innovation is an active query strategy that only asks the expert to compare actions when necessary, achieving optimal regret bounds while minimizing query complexity. The algorithms use an online regression oracle to estimate a preference function and selectively query the expert based on the cardinality of the candidate action set. The imitation learning extension can even learn to outperform a suboptimal expert, which prior algorithms like DAgger cannot guarantee.

## Method Summary
The paper proposes AURORA for contextual bandits, which uses an online regression oracle to estimate a preference function and maintains a version space of plausible functions. It only queries the expert when the candidate action set has more than one element, otherwise playing the single candidate action. The algorithm uses a cumulative regret estimator to switch between uniform exploration and inverse gap weighting strategies. AURORAE extends this to imitation learning by running H instances of AURORA across time steps, leveraging preference feedback that reflects reward-to-go.

## Key Results
- AURORA achieves best-of-both-worlds regret bound of $\tilde{O}(\min\{\sqrt{T}, d/\Delta\})$
- Query complexity is $\tilde{O}(\min\{T, d^2/\Delta^2\})$ where $d$ is eluder dimension and $\Delta$ is preference gap
- AURORAE extends to imitation learning with similar regret bounds up to factor H
- Can learn to outperform a suboptimal expert, unlike prior imitation learning algorithms

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The algorithm minimizes regret while minimizing expert queries by selectively querying only when the candidate action set has more than one element.
- **Mechanism:** At each round, the learner constructs a version space of functions close to past predictors on observed data. If this version space yields only one candidate arm, no query is needed because that arm must be optimal. Otherwise, a comparison query is made to disambiguate between multiple candidate arms.
- **Core assumption:** The online regression oracle provides a predictor close enough to the true preference function that the version space contains the true function with high probability.
- **Evidence anchors:**
  - [abstract]: "our algorithm does not require the knowledge of Δ, and the obtained regret bound is comparable to what can be achieved in the standard contextual bandits setting"
  - [section]: "When |At|=1, the only arm in the set is the optimal arm since f⋆∈Ft, and thus no query is needed (Zt=0)."
- **Break condition:** If the online regression oracle's regret guarantee fails or the confidence parameter is set too low, the version space may not contain the true function, leading to incorrect non-query decisions.

### Mechanism 2
- **Claim:** The algorithm achieves best-of-both-worlds regret bounds by balancing exploration and exploitation based on cumulative regret estimation.
- **Mechanism:** The algorithm uses a cumulative regret estimator (λt) to determine when to switch from uniform exploration (λt=0) to inverse gap weighting (λt=1). This allows optimal worst-case performance initially and instance-dependent performance later.
- **Core assumption:** The cumulative regret can be accurately estimated through the sum of weighted widths (Ztwt) of the version space.
- **Evidence anchors:**
  - [abstract]: "our algorithm achieves a regret bound that combines the best of both worlds, scaling as O(min{√T, d/Δ})"
  - [section]: "The strategy to choose the actions (to be queried) for different values of λt are as follows: If λt=0, the cumulative reward has not yet exceeded √AT/β..."
- **Break condition:** If the gap Δ is underestimated or the eluder dimension is very large, the algorithm may delay the transition to exploitation too long, harming instance-dependent performance.

### Mechanism 3
- **Claim:** The imitation learning extension can learn to outperform a suboptimal expert by leveraging preference feedback structure.
- **Mechanism:** By running H instances of the contextual bandit algorithm across time steps and using preference feedback that reflects reward-to-go, the algorithm can identify actions that maximize the expert's value function even when the expert doesn't always take them.
- **Core assumption:** The preference feedback provides enough information to identify the action that maximizes Qπe(x,a) even without observing the expert's actions or reward signals directly.
- **Evidence anchors:**
  - [abstract]: "Interestingly, our algorithm for imitation learning via preference-feedback can even learn to outperform the underlying expert thus highlighting a practical benefit"
  - [section]: "This means that our algorithm not only competes with the expert policy but can also surpass it to some extent."
- **Break condition:** If the preference feedback is too noisy or the expert's policy has very small gaps in Q-values, the algorithm may struggle to identify the optimal actions.

## Foundational Learning

- **Concept: Online regression oracle with sublinear regret**
  - Why needed here: The algorithm relies on an online regression oracle to estimate the preference function from noisy binary feedback. The oracle's sublinear regret guarantee ensures that the estimated function stays close to the true function over time.
  - Quick check question: What happens to the algorithm's performance if the online regression oracle has linear regret instead of sublinear regret?

- **Concept: Eluder dimension as a complexity measure**
  - Why needed here: The eluder dimension characterizes the complexity of the function class and appears in both the regret and query complexity bounds. It measures how many points need to be observed to distinguish between functions in the class.
  - Quick check question: How does the algorithm's performance change if the function class has infinite eluder dimension?

- **Concept: Strong convexity and link functions**
  - Why needed here: The strong convexity of the link function's generating function ensures that the associated loss function has favorable properties for online learning, enabling the use of standard online regression techniques.
  - Quick check question: What would happen if we used a non-strongly convex link function in the algorithm?

## Architecture Onboarding

- **Component map:** Online regression oracle -> Version space constructor -> Candidate action set generator -> Query decision module -> Action selection module -> Update module

- **Critical path:**
  1. Receive context xt
  2. Construct version space Ft
  3. Generate candidate arm set At
  4. Decide whether to query (Zt = 1{|At|>1})
  5. If querying: select actions and receive feedback, update oracle
  6. If not querying: play the single action in At
  7. Repeat for next round

- **Design tradeoffs:**
  - Query frequency vs. regret: Making more queries reduces uncertainty but increases cost
  - Confidence parameter β: Higher values reduce query frequency but may increase regret
  - Function class complexity: More complex classes can represent more problems but require more queries

- **Failure signatures:**
  - High regret despite many queries: Likely indicates online regression oracle is not performing well
  - Very few queries but poor performance: Version space construction may be too conservative
  - Oscillating performance: λt switching too frequently between exploration and exploitation

- **First 3 experiments:**
  1. **Sanity check:** Run algorithm on a simple contextual bandit problem where the optimal action is known and verify that it queries only when necessary and achieves low regret.
  2. **Eluder dimension test:** Compare algorithm performance on function classes with different eluder dimensions to verify the theoretical bounds.
  3. **Gap sensitivity:** Test algorithm on problems with varying gap sizes to observe the transition between worst-case and instance-dependent performance regimes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the algorithm's performance scale when the eluder dimension is infinite or extremely large?
- Basis in paper: [explicit] The paper provides regret and query complexity bounds that scale with the eluder dimension (dim_E(F, ∆)), but does not analyze cases where this dimension is infinite or extremely large.
- Why unresolved: The paper focuses on the case where the eluder dimension is finite and small, but does not explore scenarios where this assumption may not hold. This leaves open the question of how the algorithm would perform in more complex function classes.
- What evidence would resolve it: Empirical results or theoretical analysis demonstrating the algorithm's performance on function classes with large or infinite eluder dimensions would help answer this question.

### Open Question 2
- Question: Can the dependence on the number of actions (A) in the query complexity bound be improved?
- Basis in paper: [inferred] The paper mentions that the query complexity bound has a dependence on A^3, which arises from the fact that two actions are queried at each round. The authors suggest this as a potential area for future investigation.
- Why unresolved: The authors acknowledge this as a limitation but do not provide any concrete methods or theoretical results to improve this dependence.
- What evidence would resolve it: A modified algorithm or theoretical proof showing a reduced dependence on A in the query complexity bound would address this open question.

### Open Question 3
- Question: How does the algorithm perform in the stochastic contextual bandit setting?
- Basis in paper: [explicit] The authors mention that their result on contextual dueling bandits can be extended to the stochastic setting, where the eluder dimension could be replaced by the value function disagreement coefficient.
- Why unresolved: While the authors suggest this as a potential extension, they do not provide any theoretical results or empirical evidence for this setting.
- What evidence would resolve it: Theoretical analysis or experimental results demonstrating the algorithm's performance in the stochastic contextual bandit setting would help answer this question.

### Open Question 4
- Question: What are the practical implementation challenges and potential solutions for the proposed algorithms?
- Basis in paper: [explicit] The authors mention that developing practical implementations of their algorithms is an interesting direction for future work.
- Why unresolved: The paper focuses on theoretical guarantees and does not discuss practical implementation details or potential challenges.
- What evidence would resolve it: A practical implementation of the algorithms, along with a discussion of implementation challenges and potential solutions, would address this open question.

## Limitations

- Theoretical analysis assumes realizable setting where true preference function lies within chosen function class
- Query complexity and regret bounds depend heavily on eluder dimension, which can be difficult to compute for complex function classes
- Requirement for online regression oracle with sublinear regret may be restrictive in some applications

## Confidence

- Confidence in main theoretical claims: High
- Confidence in practical applicability: Medium
- Confidence in imitation learning extension: Medium-High

## Next Checks

1. **Empirical validation**: Implement the algorithm on benchmark contextual bandit problems to verify that it achieves low regret while minimizing expert queries compared to baselines like AIL and DAgger.

2. **Gap sensitivity analysis**: Systematically vary the preference gap Δ in synthetic problems to empirically verify the transition between worst-case and instance-dependent regret regimes predicted by the theory.

3. **Function class complexity study**: Compare algorithm performance on function classes with different eluder dimensions (e.g., linear vs. kernel-based) to validate the theoretical dependence on this complexity measure.