---
ver: rpa2
title: Skill Reinforcement Learning and Planning for Open-World Long-Horizon Tasks
arxiv_id: '2303.16563'
source_url: https://arxiv.org/abs/2303.16563
tags:
- skill
- skills
- tasks
- uni00000048
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Plan4MC, a method for learning and planning
  basic skills to accomplish long-horizon tasks in the open-world game Minecraft.
  The authors define three types of fine-grained basic skills (Finding, Manipulation,
  and Crafting) and train policies to learn them using reinforcement learning with
  intrinsic rewards.
---

# Skill Reinforcement Learning and Planning for Open-World Long-Horizon Tasks

## Quick Facts
- arXiv ID: 2303.16563
- Source URL: https://arxiv.org/abs/2303.16563
- Reference count: 40
- Primary result: Plan4MC achieves high success rates on 24 diverse Minecraft tasks by combining skill learning and planning

## Executive Summary
This paper introduces Plan4MC, a method for learning and planning basic skills to accomplish long-horizon tasks in the open-world game Minecraft. The authors define three types of fine-grained basic skills (Finding, Manipulation, and Crafting) and train policies to learn them using reinforcement learning with intrinsic rewards. A skill graph is constructed in advance using a large language model to capture the relationships between skills. During task execution, a skill search algorithm walks on the skill graph to generate executable skill plans. Experiments on 24 diverse Minecraft tasks show that Plan4MC significantly outperforms baselines, achieving high success rates by effectively combining skill learning and planning.

## Method Summary
Plan4MC converts multi-task learning into learning basic skills and planning over those skills. The method trains three types of basic skills (Finding, Manipulation, Crafting) using hierarchical RL with intrinsic rewards. A skill graph is generated offline using a large language model to capture skill dependencies, then corrected by humans. During execution, a skill search algorithm uses DFS to find executable skill sequences given the current inventory. The method alternates between planning and skill execution until task completion.

## Key Results
- Plan4MC significantly outperforms baselines on 24 diverse Minecraft tasks
- Achieves high success rates by effectively combining skill learning and planning
- Demonstrates the effectiveness of decomposing long-horizon tasks into shorter atomic skills

## Why This Works (Mechanism)

### Mechanism 1
Learning fine-grained basic skills improves sample efficiency compared to learning full tasks directly. Decomposing long-horizon tasks into shorter atomic skills reduces exploration complexity, with each skill having a shorter horizon and smaller state space, making RL feasible. The core assumption is that skill decomposition preserves task solvability when skills are executed sequentially.

### Mechanism 2
Hierarchical policy with intrinsic rewards improves exploration efficiency for Finding-skills. High-level policy maximizes state coverage by visiting different grid regions, while low-level policy navigates to specific goals. This guides exploration more effectively than random exploration. The core assumption is that target items are uniformly distributed, so maximizing visited area increases chance of finding them.

### Mechanism 3
Pre-generating skill graph with LLM and using DFS search provides reliable planning without real-time LLM errors. LLM generates skill dependencies offline where mistakes can be manually corrected. DFS algorithm then uses this graph to find executable skill sequences given current inventory. The core assumption is that LLM can accurately capture Minecraft crafting dependencies and skill prerequisites.

## Foundational Learning

- **Concept: Hierarchical reinforcement learning**
  - Why needed here: Enables learning complex behaviors by decomposing them into simpler sub-policies that can be trained separately
  - Quick check question: What is the difference between high-level and low-level policies in the Finding-skill architecture?

- **Concept: Intrinsic motivation in reinforcement learning**
  - Why needed here: Provides dense rewards for exploration when sparse extrinsic rewards are insufficient for learning
  - Quick check question: How does state count reward encourage exploration differently than distance-based rewards?

- **Concept: Skill dependency graphs and planning**
  - Why needed here: Enables decomposing tasks into executable sequences by representing prerequisites between skills
  - Quick check question: What information must be tracked during DFS to ensure generated plans are executable?

## Architecture Onboarding

- **Component map**: MineCLIP encoder -> Basic skill policies (12×3 actions) -> High-level Finding-skill policy (LSTM) -> Skill graph (LLM-generated) -> Skill search algorithm (DFS) -> Task execution loop

- **Critical path**: Task → Skill Search → Skill Execution → Inventory Update → Repeat until success

- **Design tradeoffs**:
  - Fine-grained skills vs. coarse skills: Better RL sample efficiency but requires more policies and planning complexity
  - Offline vs. online planning: More reliable but less adaptive to runtime changes
  - Intrinsic rewards vs. pure extrinsic: Better exploration but may learn suboptimal behaviors

- **Failure signatures**:
  - Low skill success rates indicate exploration or reward design issues
  - Planning failures suggest incorrect skill dependencies or insufficient inventory tracking
  - High variance across seeds suggests sensitivity to initialization

- **First 3 experiments**:
  1. Train basic Manipulation-skill (e.g., Place) and verify success rate exceeds 90%
  2. Generate skill graph for a small subset of skills and verify DFS produces correct sequences
  3. Test complete pipeline on a simple 3-skill task and measure end-to-end success rate

## Open Questions the Paper Calls Out

The paper identifies several open questions:
- How Plan4MC performs on tasks requiring exploration in underground environments
- The impact of different skill planning algorithms on Plan4MC's performance
- How Plan4MC scales to more complex tasks requiring longer skill sequences

## Limitations

- Reliance on LLM-generated skill graphs may not generalize to domains without comprehensive documentation
- Manual correction of LLM errors limits scalability to larger skill spaces
- Intrinsic reward design for Finding-skills assumes uniform item distribution, which may not hold in more complex environments

## Confidence

- **High confidence**: The hierarchical skill learning approach with intrinsic rewards improves sample efficiency compared to direct task learning (supported by experimental results showing Plan4MC's superior performance)
- **Medium confidence**: The LLM-generated skill graph provides reliable planning without runtime errors (supported by claims of "few mistakes" but limited quantitative validation)
- **Low confidence**: The method generalizes to other open-world environments beyond Minecraft (no experiments outside Minecraft environment)

## Next Checks

1. Test Plan4MC on a completely different open-world environment (e.g., robotic manipulation or another game) to assess cross-domain generalizability
2. Compare online LLM planning vs. offline graph planning on tasks where dependencies may change dynamically during execution
3. Conduct ablation studies varying skill granularity (finer vs. coarser skills) to quantify the tradeoff between learning efficiency and planning complexity