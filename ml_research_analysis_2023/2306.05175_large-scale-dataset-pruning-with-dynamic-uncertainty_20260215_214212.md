---
ver: rpa2
title: Large-scale Dataset Pruning with Dynamic Uncertainty
arxiv_id: '2306.05175'
source_url: https://arxiv.org/abs/2306.05175
tags:
- training
- pruning
- dataset
- samples
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of large-scale dataset pruning,
  aiming to reduce the computational cost of training sophisticated deep models by
  removing less-informative samples from large datasets. The authors propose a simple
  yet effective method based on prediction uncertainty and training dynamics, which
  outperforms state-of-the-art approaches.
---

# Large-scale Dataset Pruning with Dynamic Uncertainty

## Quick Facts
- arXiv ID: 2306.05175
- Source URL: https://arxiv.org/abs/2306.05175
- Reference count: 40
- Primary result: Achieves 75% lossless compression on ImageNet while maintaining comparable model performance

## Executive Summary
This paper addresses the challenge of large-scale dataset pruning by proposing a dynamic uncertainty-based method that effectively reduces training data size while preserving model performance. The approach leverages prediction uncertainty and training dynamics to identify and remove less-informative samples from large datasets. Through extensive experiments on ImageNet-1K and ImageNet-21K using advanced architectures like Swin Transformer and ConvNeXt, the method demonstrates significant computational savings with minimal accuracy loss.

## Method Summary
The proposed method computes dynamic uncertainty for each sample by measuring prediction variation over a sliding window of training epochs. During training, predictions and losses are recorded for each sample, and uncertainty is calculated using the standard deviation of predictions within the window. Samples are then ranked by their uncertainty scores, with the lowest uncertainty samples (easy and hard samples) being pruned. The method is designed to be scalable to large datasets and models through parallel computation on GPUs.

## Key Results
- Achieves 75% lossless compression ratio on ImageNet datasets
- Maintains comparable performance to models trained on full datasets
- Demonstrates good cross-architecture generalization and out-of-distribution detection capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Removing easy and hard samples while retaining uncertain ones preserves model performance
- **Mechanism**: Easy samples have high mean accuracy and low variance; hard samples have low mean accuracy and low variance. Uncertain samples have moderate mean accuracy and high variance. By measuring dynamic uncertainty over a sliding window of J epochs, the method captures training dynamics and retains samples with high uncertainty, which are most informative for learning
- **Core assumption**: The uncertainty measured via standard deviation over predictions correlates with sample informativeness
- **Evidence anchors**: [abstract]: "We propose a simple yet effective dataset pruning method by exploring both the prediction uncertainty and training dynamics."

### Mechanism 2
- **Claim**: Dynamic uncertainty captures training dynamics better than static uncertainty
- **Mechanism**: Static uncertainty is computed using predictions at a single epoch, while dynamic uncertainty averages standard deviations over a sliding window of J epochs. This captures how uncertainty evolves during training, making the pruning more robust
- **Core assumption**: Training dynamics affect the informativeness of samples, and a sliding window captures this effect
- **Evidence anchors**: [abstract]: "Since the uncertainty may vary throughout the training process, we introduce the training dynamics into the metric, which measure the uncertainty in a sliding window throughout all training epochs."

### Mechanism 3
- **Claim**: The method scales to large datasets and models due to its O(n) complexity
- **Mechanism**: The algorithm computes predictions and losses for each sample in parallel on GPUs, and uncertainty is computed using a sliding window without requiring pairwise comparisons or complex optimization
- **Core assumption**: Parallel computation and simple arithmetic operations allow scalability
- **Evidence anchors**: [abstract]: "We design a simple yet effective metric for dataset pruning, which is scalable to large datasets and models."

## Foundational Learning

- **Concept**: Prediction uncertainty and its measurement via standard deviation
  - **Why needed here**: The method relies on measuring prediction uncertainty to identify informative samples
  - **Quick check question**: How is prediction uncertainty defined in this method?

- **Concept**: Training dynamics and their role in sample informativeness
  - **Why needed here**: The method incorporates training dynamics to improve uncertainty measurement
  - **Quick check question**: Why is a sliding window used instead of a single epoch for uncertainty measurement?

- **Concept**: Dataset pruning and coreset selection
  - **Why needed here**: The method is a form of dataset pruning, aiming to retain a subset of informative samples
  - **Quick check question**: What is the goal of dataset pruning in this context?

## Architecture Onboarding

- **Component map**: Model training module -> Uncertainty computation module -> Pruning module
- **Critical path**: 
  1. Train model on full dataset while recording predictions for each sample
  2. Compute dynamic uncertainty for each sample using a sliding window of J epochs
  3. Sort samples by dynamic uncertainty and retain the top (1-r) fraction
- **Design tradeoffs**: 
  - Using a larger J window may capture more training dynamics but increases computation time
  - Pruning more samples (higher r) reduces training cost but may degrade model performance
- **Failure signatures**: 
  - Performance degradation: May indicate too many informative samples were pruned
  - High variance in results: May indicate instability in uncertainty measurement
- **First 3 experiments**:
  1. Verify dynamic uncertainty computation by comparing static vs. dynamic uncertainty on a small dataset
  2. Test pruning with different r values to find the optimal compression ratio
  3. Evaluate cross-architecture generalization by pruning with one model and testing with another

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the proposed method perform on datasets with highly imbalanced class distributions?
- **Basis in paper**: [inferred] The paper focuses on balanced datasets like ImageNet-1K and ImageNet-21K, but does not address imbalanced datasets
- **Why unresolved**: The paper does not provide experiments or analysis on imbalanced datasets, which are common in real-world applications
- **What evidence would resolve it**: Conducting experiments on imbalanced datasets and comparing the performance with other methods would provide insights into the method's effectiveness in such scenarios

### Open Question 2
- **Question**: What is the impact of the window size J on the performance of the dynamic uncertainty metric?
- **Basis in paper**: [explicit] The paper mentions that J = 10 is used as the window's length, but does not explore the impact of different window sizes
- **Why unresolved**: The choice of window size is a hyperparameter that could significantly affect the method's performance, but its impact is not investigated in the paper
- **What evidence would resolve it**: Conducting experiments with different window sizes and analyzing the performance would help determine the optimal window size for various datasets and tasks

### Open Question 3
- **Question**: How does the proposed method handle noisy labels in the training data?
- **Basis in paper**: [inferred] The paper mentions that hard samples with low mean accuracy and low deviation might be mislabeled, but does not explicitly address the issue of noisy labels
- **Why unresolved**: Noisy labels can negatively impact the performance of deep learning models, and it is crucial to understand how the proposed method handles such cases
- **What evidence would resolve it**: Conducting experiments on datasets with noisy labels and comparing the performance with other methods would provide insights into the method's robustness to label noise

## Limitations
- The method requires training on the full dataset initially to compute uncertainties, which partially defeats the purpose of dataset pruning for computational efficiency
- Limited evaluation of cross-dataset generalization beyond the two ImageNet variants
- No ablation study on the impact of training hyperparameters on uncertainty measurement

## Confidence
- **High**: The theoretical framework connecting uncertainty to sample informativeness is well-established in the literature
- **Medium**: The experimental results showing 75% compression with minimal performance loss are compelling but need independent verification
- **Medium**: Claims about cross-architecture generalization are supported but limited to three model families

## Next Checks
1. Replicate the dynamic uncertainty computation independently and verify it produces consistent rankings across multiple training runs
2. Test the method on a dataset from a different domain (e.g., medical imaging or speech) to evaluate generalizability
3. Perform an ablation study varying the sliding window size J to determine optimal settings for different dataset sizes