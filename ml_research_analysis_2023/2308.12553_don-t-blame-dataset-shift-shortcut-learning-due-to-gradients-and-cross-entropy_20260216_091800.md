---
ver: rpa2
title: Don't blame Dataset Shift! Shortcut Learning due to Gradients and Cross Entropy
arxiv_id: '2308.12553'
source_url: https://arxiv.org/abs/2308.12553
tags:
- shortcut
- group
- feature
- loss
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies shortcut learning in perception tasks, where
  models rely on unstable correlations instead of stable features that determine the
  label. The authors analyze why gradient-based optimization of cross-entropy (default-ERM)
  exhibits shortcut learning even when the stable feature perfectly predicts the label.
---

# Don't blame Dataset Shift! Shortcut Learning due to Gradients and Cross Entropy

## Quick Facts
- arXiv ID: 2308.12553
- Source URL: https://arxiv.org/abs/2308.12553
- Reference count: 40
- Primary result: MARG-CTRL mitigates shortcut learning by encouraging uniform margins, outperforming default-ERM and two-stage methods on vision and language tasks.

## Executive Summary
This paper challenges the common assumption that dataset shift causes shortcut learning in perception tasks. Instead, it argues that the default-ERM's implicit bias toward maximizing margins leads models to depend more on unstable shortcut features than stable features, even when the stable feature perfectly predicts the label. The authors develop margin control (MARG-CTRL) loss functions that encourage uniform margins, thereby reducing dependence on shortcuts. Empirical results on synthetic and real datasets demonstrate that MARG-CTRL outperforms both default-ERM and two-stage shortcut-mitigating methods, suggesting that better inductive biases can replace expensive mitigation techniques.

## Method Summary
The method introduces margin control (MARG-CTRL) loss functions that encourage uniform margins across training samples. These losses have the property that per-sample loss decreases for margins up to a threshold and increases beyond it, pushing all margins toward the threshold value. This uniform-margin solution forces the model to depend only on the stable feature rather than shortcuts. The approach is evaluated on a synthetic linear perception task and real datasets including Waterbirds, CelebA, and CivilComments, comparing against default-ERM and two-stage methods like JTT and CNC.

## Key Results
- MARG-CTRL outperforms default-ERM and two-stage methods on worst-group accuracy for perception tasks
- Uniform-margin solutions depend only on stable features, while max-margin solutions can depend on shortcuts
- The method works even in nuisance-free settings where group annotations are unavailable during training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Default-ERM exhibits shortcut learning even when the stable feature perfectly predicts the label
- **Mechanism**: Default-ERM's implicit bias toward maximizing margins leads to models that depend more on shortcut features than stable features
- **Core assumption**: The stable feature is a perfect predictor of the label (y = d ∘ s(x))
- **Evidence anchors**: [abstract] default-ERM still exhibits shortcut learning even with perfect stable features; [section] max-margin classification leads to shortcut dependence
- **Break condition**: If the stable feature is not a perfect predictor of the label

### Mechanism 2
- **Claim**: Inductive biases toward uniform margins mitigate shortcut learning
- **Mechanism**: Models with uniform margins depend only on stable features, while max-margin classifiers can depend on shortcuts
- **Core assumption**: The stable feature is a perfect predictor of the label
- **Evidence anchors**: [abstract] uniform margins guarantee dependence only on the stable feature; [section] uniform margins can be achieved with stable feature
- **Break condition**: If the stable feature is not a perfect predictor of the label

### Mechanism 3
- **Claim**: MARG-CTRL loss functions encourage uniform margins and mitigate shortcut learning
- **Mechanism**: MARG-CTRL losses decrease for margins up to a threshold and increase beyond it, pushing all margins to the threshold value
- **Core assumption**: The stable feature is a perfect predictor of the label
- **Evidence anchors**: [abstract] MARG-CTRL losses encourage uniform-margin solutions; [section] similar margins across samples reduces shortcut dependence
- **Break condition**: If the stable feature is not a perfect predictor of the label

## Foundational Learning

- **Concept**: Linear separability
  - Why needed here: The paper studies a linear perception task where the stable feature perfectly predicts the label, making the data linearly separable
  - Quick check question: In a linearly separable dataset, what is the relationship between the Bayes-optimal predictor and the max-margin classifier?

- **Concept**: Inductive bias
  - Why needed here: The paper discusses how the inductive bias of default-ERM (max-margin) is unsuitable for perception tasks
  - Quick check question: What is the difference between the inductive bias of default-ERM and the inductive bias proposed in the paper for perception tasks?

- **Concept**: Gradient descent optimization
  - Why needed here: The paper studies the behavior of gradient-based optimization of cross-entropy (default-ERM) and how it leads to shortcut learning
  - Quick check question: How does the choice of loss function and optimization algorithm affect the inductive bias of a machine learning model?

## Architecture Onboarding

- **Component map**: Model parameters w = [wz, wy, we] for shortcut, stable, and noise features → MARG-CTRL loss functions → gradient-based optimization → trained model
- **Critical path**: Define MARG-CTRL loss functions → modify training loop to use these losses → evaluate on perception tasks with known shortcuts
- **Design tradeoffs**: Default-ERM is simpler and more efficient but exhibits shortcut learning; MARG-CTRL mitigates shortcut learning but requires careful hyperparameter tuning
- **Failure signatures**: Poor performance when stable feature is not a perfect predictor; degraded results with improper hyperparameter tuning
- **First 3 experiments**:
  1. Implement synthetic linear perception task and train with default-ERM and MARG-CTRL, comparing shortcut vs stable feature dependence
  2. Implement MARG-CTRL losses and test on Waterbirds, CelebA, and CivilComments datasets, comparing worst-group accuracy
  3. Apply MARG-CTRL in nuisance-free setting, tuning hyperparameters using label-balanced accuracy and comparing worst-group test accuracy

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain regarding the approach's performance in overparameterized settings, with label noise, and compared to other shortcut mitigation methods.

## Limitations
- Theoretical analysis is limited to underparameterized linear models and doesn't extend to overparameterized deep networks
- Empirical evaluation is primarily on relatively simple perception tasks, leaving questions about scalability
- Assumes perfect linear separability and binary stable features, which may not generalize to more complex real-world scenarios

## Confidence
- **High Confidence**: The theoretical framework linking margin maximization to shortcut dependence is well-supported
- **Medium Confidence**: Empirical results on real datasets demonstrate effectiveness but with limited task diversity
- **Low Confidence**: Claims about highly overparameterized deep networks require further investigation

## Next Checks
1. Apply MARG-CTRL to a large-scale perception task (e.g., ImageNet) to assess effectiveness in highly overparameterized settings
2. Evaluate MARG-CTRL's performance under varying degrees of label noise and imperfect stable features
3. Compare MARG-CTRL's inductive bias to other uniform-margin methods to identify potential synergies or conflicts