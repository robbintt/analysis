---
ver: rpa2
title: 'Towards Better Evaluation of Instruction-Following: A Case-Study in Summarization'
arxiv_id: '2310.08394'
source_url: https://arxiv.org/abs/2310.08394
tags:
- instruction
- methods
- output
- evaluation
- rating
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces riSum, a new short-form dataset of 300 document-instruction
  pairs with 3 answers each, rated by 3 human annotators, to evaluate the instruction-following
  abilities of large language models (LLMs). The authors propose new LLM-based reference-free
  evaluation methods that improve upon established baselines and perform on par with
  costly reference-based metrics.
---

# Towards Better Evaluation of Instruction-Following: A Case-Study in Summarization

## Quick Facts
- arXiv ID: 2310.08394
- Source URL: https://arxiv.org/abs/2310.08394
- Reference count: 28
- The paper introduces riSum, a new short-form dataset of 300 document-instruction pairs with 3 answers each, rated by 3 human annotators, to evaluate the instruction-following abilities of large language models (LLMs).

## Executive Summary
This paper addresses the challenge of evaluating instruction-following abilities in large language models, focusing on the summarization domain. The authors introduce riSum, a new dataset of 300 document-instruction pairs with multiple human-annotated answers, and propose new LLM-based reference-free evaluation methods that outperform traditional reference-based metrics. The key insight is that LLM-based evaluators can directly assess instruction adherence without needing reference summaries, achieving results comparable to costly reference-based methods while avoiding their limitations.

## Method Summary
The authors collected the riSum dataset by sampling documents from diverse sources, generating instructions using GPT-4, and producing answers using multiple LLMs (F-PaLM 2-S, F-PaLM 2-Sc, and GPT-3.5). Human annotators rated each answer on two dimensions: whether it follows the instruction and how well it does so. The evaluation methods compared include traditional reference-based approaches (ROUGE, BLEURT), simple reference-free baselines (length heuristics, BARTScore, T5ANLI), and new LLM-based reference-free methods (Constrained Softmax, Self-Agreement, Multi-LLM Agreement). The methods were evaluated using AUC ROC for instruction-following classification and rank-based metrics (Kendall's Tau-b) plus Pearson correlation for quality assessment.

## Key Results
- LLM-based reference-free evaluation methods improve upon established baselines and perform on par with costly reference-based metrics.
- When using the same underlying model, reference-free methods based on F-PaLM 2-Lc outperform their reference-based counterparts.
- Larger LLM evaluators consistently improve evaluation performance across all methods tested.
- Traditional metrics like ROUGE and BLEURT are less effective at quantifying LLMs' instruction-following ability compared to the proposed LLM-based approaches.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based reference-free evaluation methods can match or outperform reference-based methods when the underlying model is sufficiently capable.
- Mechanism: The proposed LLM-based evaluators directly assess whether a generated answer follows the instruction without needing reference summaries, bypassing the need for expensive reference summaries.
- Core assumption: The underlying LLM (e.g., F-PaLM 2-Lc) has strong instruction-following and reasoning capabilities to accurately evaluate instruction adherence.
- Evidence anchors: The paper shows that LLM-based methods achieve comparable performance to reference-based methods across multiple evaluation metrics.

### Mechanism 2
- Claim: Reference-based methods relying on n-gram overlap (ROUGE) or semantic similarity (BLEURT) are less effective than LLM-based methods for evaluating instruction-following in summarization.
- Mechanism: ROUGE and BLEURT measure surface form or semantic similarity between candidate and reference summaries but don't directly assess instruction adherence.
- Core assumption: Instruction-following is a distinct dimension of summary quality not well-captured by traditional similarity-based metrics.
- Evidence anchors: The paper finds that established metrics like ROUGE and BLEURT are not effective at quantifying LLMs' instruction-following ability.

### Mechanism 3
- Claim: Using larger LLMs as the underlying evaluator improves the performance of LLM-based evaluation methods.
- Mechanism: Larger models have more parameters and training data, enabling better understanding of complex instructions and reasoning about instruction adherence.
- Core assumption: Model size is a strong indicator of instruction-following and reasoning ability in LLMs.
- Evidence anchors: The paper observes that performance of each evaluation method improves with model size.

## Foundational Learning

- Concept: Understanding of instruction-following and its dimensions (coherence, faithfulness, style, alignment).
  - Why needed here: The paper's main contribution is evaluating how well LLMs follow instructions. A clear understanding of what instruction-following entails is crucial for interpreting results and designing evaluation methods.
  - Quick check question: What are the four dimensions of instruction-following mentioned in the paper, and how do they differ from each other?

- Concept: Reference-based vs. reference-free evaluation methods.
  - Why needed here: The paper compares reference-based methods (ROUGE, BLEURT) with reference-free methods (LLM-based evaluators). Understanding the pros and cons of each approach is essential for appreciating the paper's contributions.
  - Quick check question: What is the key difference between reference-based and reference-free evaluation methods, and what are the main advantages of reference-free methods in this context?

- Concept: Meta-evaluation and rank-based evaluation.
  - Why needed here: The paper uses meta-evaluation to assess how well different evaluation methods agree with human judgment. Rank-based evaluation (Kendall's Tau-b) is used to compare the ordering of answers produced by each method with the ordering preferred by humans.
  - Quick check question: What is the purpose of meta-evaluation in this paper, and how does Kendall's Tau-b rank distance help in comparing the performance of different evaluation methods?

## Architecture Onboarding

- Component map:
  Data Collection -> Human Evaluation -> Reference-based Methods (ROUGE, BLEURT) + Reference-free Methods (BARTScore, T5ANLI) + LLM-based Methods (Constrained Softmax, Self-Agreement, Multi-LLM Agreement) -> Analysis (AUC ROC, Kendall's Tau-b, Pearson correlation)

- Critical path:
  1. Collect riSum dataset with human ratings
  2. Implement reference-based and reference-free evaluation methods
  3. Compute AUC ROC, Kendall's Tau-b, and Pearson correlation between evaluation methods and human ratings
  4. Analyze the results to determine which methods best predict human judgment

- Design tradeoffs:
  - Using GPT-4 to generate instructions ensures high-quality instructions but introduces potential bias if GPT-4 is also used as a reference in evaluation
  - Using LLM-based evaluation methods avoids the need for reference summaries but relies on the underlying LLM's instruction-following ability
  - Meta-evaluation using rank-based metrics (Kendall's Tau-b) is robust to ties but doesn't capture the magnitude of differences between answers

- Failure signatures:
  - If AUC ROC is close to 0.5, the evaluation method is no better than random at predicting human "Follows Instruction?" ratings
  - If Kendall's Tau-b rank distance is close to 0.5, the evaluation method's ranking of answers is independent of the human ranking
  - If Pearson correlation is close to 0, the evaluation method's ratings are not linearly related to human ratings

- First 3 experiments:
  1. Compute AUC ROC for each evaluation method using the "Follows Instruction?" human ratings as ground truth
  2. Compute Kendall's Tau-b rank distance for each evaluation method using the "How Well?" human ratings as ground truth
  3. Compute Pearson correlation between each evaluation method's ratings and the mean human ratings for "How Well?"

## Open Questions the Paper Calls Out

- How do the proposed reference-free evaluation methods perform on other NLP tasks beyond summarization?
- How do the evaluation methods perform on non-English languages?
- How do the evaluation methods perform when annotators have different backgrounds or expertise levels?

## Limitations

- The riSum dataset contains only 300 document-instruction pairs with three model-generated answers each, which may limit generalizability.
- The study focuses exclusively on short-form summarization, and findings may not extend to longer documents or different summarization paradigms.
- The proposed LLM-based evaluation methods depend heavily on the underlying model's instruction-following capabilities, with uncertain performance on models of varying abilities.

## Confidence

High confidence: The core finding that LLM-based reference-free methods outperform or match reference-based methods (ROUGE, BLEURT) for instruction-following evaluation.

Medium confidence: The claim that larger models consistently improve evaluation performance across all methods tested.

Medium confidence: The assertion that instruction-following is a distinct dimension from semantic similarity.

## Next Checks

1. Validate the findings on a larger, more diverse dataset that includes different document types, instruction complexities, and languages.

2. Quantify the actual computational and annotation costs of reference-based versus reference-free evaluation methods in practice.

3. Systematically test the relationship between evaluator model size and evaluation quality across a wider range of model sizes.