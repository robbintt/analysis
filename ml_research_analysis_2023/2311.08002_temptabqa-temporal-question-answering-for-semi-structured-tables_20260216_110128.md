---
ver: rpa2
title: 'TempTabQA: Temporal Question Answering for Semi-Structured Tables'
arxiv_id: '2311.08002'
source_url: https://arxiv.org/abs/2311.08002
tags:
- temporal
- table
- human
- questions
- tables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the task of temporal question answering on
  semi-structured tables and presents a new dataset, TEMP TABQA, containing 11,454
  question-answer pairs extracted from 1,208 Wikipedia Infobox tables. The dataset
  focuses on answering complex temporal questions that require reasoning over time-related
  information in entity-centric tables.
---

# TempTabQA: Temporal Question Answering for Semi-Structured Tables

## Quick Facts
- arXiv ID: 2311.08002
- Source URL: https://arxiv.org/abs/2311.08002
- Reference count: 39
- Primary result: Introduces TEMP TABQA dataset for temporal reasoning over semi-structured tables; even top models lag humans by >13.5 F1 points

## Executive Summary
This paper introduces TEMP TABQA, a new dataset for temporal question answering on semi-structured tables containing 11,454 question-answer pairs extracted from 1,208 Wikipedia Infobox tables. The dataset focuses on complex temporal questions requiring reasoning over time-related information in entity-centric tables. The authors evaluate several state-of-the-art models including fine-tuned models, zero-shot LLMs, and few-shot LLMs with chain-of-thought prompting. Results show significant performance gaps between models and human benchmarks, highlighting the challenges of temporal reasoning in semi-structured tables.

## Method Summary
The authors created TEMP TABQA by extracting temporal questions from Wikipedia Infobox tables across more than 90 domains. Tables were preprocessed into linearized JSON format using delimiters like ":" and ";" for columns and rows. The dataset was split into training, development, head test, and tail test sets. Various models were evaluated including fine-tuned decoder-only models (BART, T5, Flan-T5) and zero/few-shot large language models (GPT-3.5, GPT-4, LLaMA, PaLM) with chain-of-thought prompting. Performance was measured using F1 score, Exact Match, Rouge 1, Rouge 2, and Meteor metrics.

## Key Results
- Even top-performing LLMs lag behind human performance by more than 13.5 F1 points on temporal table QA
- Fine-tuning medium-scale models on TEMP TABQA provides significant advantages over zero-shot approaches
- Chain-of-thought prompting provides additional performance boosts in few-shot settings
- Fine-tuned models show improved performance when incorporating temporal metadata extracted from dates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TempTabQA introduces temporal reasoning as a distinct challenge for semi-structured table QA
- Mechanism: By filtering tables to include only those with high temporal content and crafting questions that require time-based inference, the dataset forces models to learn temporal dynamics
- Core assumption: Temporal reasoning is orthogonal to standard numerical reasoning in tables
- Evidence anchors: Abstract states current NLP systems struggle with temporal reasoning in semi-structured tables; dataset aims to help train and evaluate models for this task

### Mechanism 2
- Claim: Chain-of-thought prompting improves performance on temporal questions
- Mechanism: CoT provides intermediate reasoning steps that help models bridge implicit temporal relationships and numerical calculations
- Core assumption: Models lack inherent temporal common sense and benefit from explicit reasoning scaffolding
- Evidence anchors: Abstract notes top LLMs lag human performance by 13.5 F1 points; incorporating chain-of-thought prompting provides additional boost

### Mechanism 3
- Claim: Fine-tuning on TempTabQA closes performance gap vs zero-shot
- Mechanism: Domain-specific fine-tuning adapts model parameters to temporal table structures and question patterns
- Core assumption: Pretrained models lack sufficient exposure to temporal semi-structured data
- Evidence anchors: Findings highlight significant advantages of fine-tuning medium-scale models; dataset evaluates state-of-the-art models for temporal reasoning

## Foundational Learning

- Concept: Temporal interval reasoning (before/after/during)
  - Why needed here: Many questions require understanding event ordering and duration
  - Quick check question: Given "event A happened in 2000, event B in 2010", how many years between them?

- Concept: Implicit vs explicit temporal references
  - Why needed here: Dataset includes questions where time must be inferred from context
  - Quick check question: "How old was she when she won?" - where does the "when" time come from?

- Concept: Numerical reasoning over temporal data
  - Why needed here: Questions combine date arithmetic with counting, comparison, etc.
  - Quick check question: If someone was born in 1980 and won in 2000, how many years elapsed?

## Architecture Onboarding

- Component map: Table parser → Temporal extractor → Question encoder → Reasoning module → Answer generator
- Critical path: Parse table → Identify temporal cells → Generate question embedding → Apply reasoning (possibly CoT) → Produce answer
- Design tradeoffs: Structured vs linearized table representation; explicit temporal metadata vs inferred from text
- Failure signatures: Incorrect date parsing; conflating event order; missing implicit time references; arithmetic errors on dates
- First 3 experiments:
  1. Fine-tune T5-Large on TempTabQA with linearized tables, measure F1 on dev set
  2. Add temporal metadata (year extracted from dates) to table representation, compare performance
  3. Implement chain-of-thought few-shot prompts, evaluate gain over zero-shot baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GPT-4 on TEMP TABQA compare to other state-of-the-art models like LLaMA 2 or PaLM 2?
- Basis in paper: The paper mentions that GPT-4 outperforms other models like LLaMA 2 and PaLM 2 in both zero-shot and few-shot settings
- Why unresolved: The paper doesn't provide a detailed comparison of GPT-4's performance with other state-of-the-art models on TEMP TABQA
- What evidence would resolve it: A detailed comparison of GPT-4's performance with other state-of-the-art models on TEMP TABQA would resolve this question

### Open Question 2
- Question: How does the performance of models on TEMP TABQA change when using different table representations like knowledge graphs instead of linearization?
- Basis in paper: The paper mentions that turning tables into knowledge graphs results in better performance compared to conventional linearization methods
- Why unresolved: The paper doesn't provide a detailed analysis of how different table representations affect model performance on TEMP TABQA
- What evidence would resolve it: A detailed analysis of model performance on TEMP TABQA using different table representations would resolve this question

### Open Question 3
- Question: How does the performance of models on TEMP TABQA change when incorporating external temporal knowledge or fine-tuning on other temporal knowledge datasets?
- Basis in paper: The paper suggests incorporating external temporal knowledge and fine-tuning on other temporal knowledge datasets as future directions for enhancing model performance
- Why unresolved: The paper doesn't provide any experimental results on how incorporating external temporal knowledge or fine-tuning on other temporal knowledge datasets affects model performance on TEMP TABQA
- What evidence would resolve it: Experimental results on how incorporating external temporal knowledge or fine-tuning on other temporal knowledge datasets affects model performance on TEMP TABQA would resolve this question

## Limitations
- The dataset construction methodology relies on extracting temporal questions from Wikipedia Infobox tables, which may introduce domain-specific biases
- The filtering criteria for temporal content are not fully specified, potentially limiting generalizability to other table formats
- The performance gap between models and human benchmarks may partly reflect differences in evaluation methodology rather than pure reasoning capability

## Confidence
- High Confidence: The dataset construction approach and evaluation methodology are sound, with clear procedures for table linearization and metric computation
- Medium Confidence: Claims about temporal reasoning challenges are supported by results, but the extent to which this isolates temporal vs. general reasoning remains unclear
- Medium Confidence: Chain-of-thought prompting benefits are demonstrated, but the specific templates and their impact on temporal vs. non-temporal questions are not fully analyzed

## Next Checks
1. **Cross-domain generalization test**: Evaluate models on TempTabQA questions transferred to tables from different domains (e.g., scientific tables) to assess whether performance degradation stems from domain shift or temporal reasoning challenges

2. **Temporal reasoning ablation**: Create a parallel dataset with identical table structures but non-temporal questions to quantify the specific contribution of temporal reasoning to the observed performance gap

3. **Error analysis framework**: Conduct a systematic error categorization (e.g., parsing errors, arithmetic mistakes, temporal inference failures) to identify whether model weaknesses are concentrated in specific temporal reasoning aspects or distributed across multiple failure modes