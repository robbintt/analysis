---
ver: rpa2
title: 'NarrativePlay: Interactive Narrative Understanding'
arxiv_id: '2310.01459'
source_url: https://arxiv.org/abs/2310.01459
tags:
- character
- narrative
- response
- characters
- event
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NarrativePlay is a system that allows users to role-play characters
  and interact with them in narratives like novels. It uses Large Language Models
  to generate human-like responses guided by personality traits extracted from the
  narratives.
---

# NarrativePlay: Interactive Narrative Understanding

## Quick Facts
- arXiv ID: 2310.01459
- Source URL: https://arxiv.org/abs/2310.01459
- Reference count: 9
- Primary result: System allows users to role-play characters and interact with them in narratives using LLM-generated responses guided by extracted personality traits

## Executive Summary
NarrativePlay is an interactive narrative understanding system that enables users to role-play characters within existing narratives like novels. The system leverages Large Language Models (LLMs) to extract character traits, events, and environments from narrative text, then uses this structured information to generate human-like responses during user interactions. By focusing on main storyline events from a user-selected character's perspective rather than predefined sandbox environments, NarrativePlay creates an immersive experience enhanced by auto-generated visuals and character speech. The system was evaluated on detective and adventure stories, demonstrating users' ability to explore narrative worlds and build relationships with characters through conversations.

## Method Summary
NarrativePlay uses GPT-3.5-turbo to extract main storyline events, characters, conversations, and environments from narrative text. The extracted character traits and narrative elements are stored as memories for each agent. Users select a character and progress through the main storyline events, with the system retrieving relevant memories based on character, input, and importance scores to guide agent responses. GPT-3.5 generates responses based on retrieved memories and current context. Stable Diffusion models generate environment visuals, and TTS models provide character speech. The system was evaluated through human assessments of agent response quality on aspects like coherence, relevance, empathy, and commonsense.

## Key Results
- Successfully extracts character traits and narrative elements using GPT-3.5-turbo from detective and adventure stories
- Generates coherent agent responses guided by retrieved memories from character and narrative context
- Creates immersive user experience through auto-generated visuals of settings and character portraits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using LLMs to extract character traits from narratives creates more authentic interactive agents.
- Mechanism: LLMs parse narrative text to extract structured information like character traits, objectives, and appearances. This structured data initializes agent memory and guides behavior during interactions.
- Core assumption: Narrative text contains sufficient explicit and implicit information about characters to allow LLMs to accurately extract defining traits.
- Evidence anchors:
  - [abstract]: "We leverage Large Language Models (LLMs) to generate human-like responses, guided by personality traits extracted from narratives."
  - [section 2.1]: Describes using GPT-3.5 to extract character details like core traits, appearance, and quotes from narrative text.
  - [corpus]: Weak evidence - no directly relevant papers found in corpus that specifically address using LLMs to extract character traits for interactive agents.

### Mechanism 2
- Claim: Focusing on main storyline events from user-selected character's perspective simplifies environment extraction and generation.
- Mechanism: Instead of extracting detailed environments throughout narrative, system extracts only locations associated with main storyline events and generates images using text-to-image models like Stable Diffusion.
- Core assumption: Main storyline events are most important for user experience, and detailed environments can be reasonably inferred from event descriptions.
- Evidence anchors:
  - [abstract]: "Our approach eschews predefined sandboxes, focusing instead on main storyline events extracted from narratives from perspective of a user-selected character."
  - [section 2.1]: Explains focus on extracting locations associated with main events and using text-to-image models to generate environment visuals.
  - [section 2.2]: Mentions using Stable Diffusion models as external knowledge to generate scenarios when information is lacking.

### Mechanism 3
- Claim: Retrieving relevant memories based on character, input, and importance scores enables context-aware agent responses.
- Mechanism: System assigns weights to each memory based on relevance to current input, recency, and importance to character. Top memories are retrieved and included in prompt to guide agent response generation.
- Core assumption: Relevant memories significantly improve quality and coherence of agent responses in interactive narratives.
- Evidence anchors:
  - [abstract]: "The system incorporates auto-generated visual display of narrative settings, character portraits, and character speech, greatly enhancing user experience."
  - [section 2.3]: Describes memory retrieval process, including weight formula that combines relevance, recency, and importance.
  - [section 3.2]: Shows agent responses with memory retrieval performed better than those without in human evaluations.

## Foundational Learning

- Concept: Narrative Understanding
  - Why needed here: To accurately extract character traits, events, and environments from input narrative text.
  - Quick check question: What are the key elements of a narrative that need to be extracted to create an interactive experience?

- Concept: Information Extraction
  - Why needed here: To parse narrative text and extract structured information like character details, event descriptions, and location data.
  - Quick check question: What are some common challenges in extracting structured information from unstructured text?

- Concept: Text-to-Image Generation
  - Why needed here: To create visual representations of narrative settings and characters based on extracted descriptions.
  - Quick check question: What are the key components of a text-to-image generation model?

## Architecture Onboarding

- Component map:
  Narrative Input -> Information Extraction -> Memory Initialization -> Event Progression -> Memory Retrieval -> Response Generation -> Multimodal Synthesis

- Critical path: Narrative Input -> Information Extraction -> Memory Initialization -> Event Progression -> Memory Retrieval -> Response Generation -> Multimodal Synthesis

- Design tradeoffs:
  - Using GPT-3.5 for information extraction allows flexible parsing but may introduce formatting errors
  - Focusing on main storyline events simplifies environment extraction but may miss some narrative details
  - Retrieving memories based on multiple factors improves context-awareness but increases computational complexity

- Failure signatures:
  - Incomplete or incorrect information extraction leading to inaccurate agent behavior
  - Poor quality environment images due to insufficient event descriptions or text-to-image model limitations
  - Contextually inappropriate agent responses due to ineffective memory retrieval or incorporation

- First 3 experiments:
  1. Test information extraction on simple narrative with clear character descriptions and event sequences
  2. Evaluate quality of generated environment images for single event with detailed location description
  3. Assess impact of memory retrieval on agent response quality by comparing responses with and without memory incorporation for simple dialogue scenario

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance of NarrativePlay compare when extracting events and environments from narratives with complex, non-linear storylines (e.g., those with time jumps or flashbacks)?
- Basis in paper: [inferred] Paper mentions current work assumes linear event timeline in input narrative, implying handling more complex storylines is left for future work.
- Why unresolved: Paper does not provide any evaluation or results for narratives with non-linear storylines, leaving performance of NarrativePlay in such scenarios unknown.
- What evidence would resolve it: Evaluating NarrativePlay on set of narratives with complex, non-linear storylines and comparing its performance in extracting events and environments to its performance on linear narratives would provide necessary evidence.

### Open Question 2
- Question: How does quality of images generated by NarrativePlay using HotPot API compare to those generated using Midjourney, and what impact does this have on user experience?
- Basis in paper: [explicit] Paper mentions HotPot API is used as automatic substitution for Midjourney when users have limited compute resources, but notes quality of its generated pictures is inferior to those from Midjourney.
- Why unresolved: Paper does not provide direct comparison of image quality between two APIs or any user experience studies to assess impact of using HotPot API.
- What evidence would resolve it: Conducting user study comparing image quality and user experience between images generated using Midjourney and HotPot API would provide necessary evidence.

### Open Question 3
- Question: How does inclusion of retrieved memories impact coherence, relevance, empathy, and commonsense of agent responses in interactive narratives?
- Basis in paper: [explicit] Paper discusses use of retrieved memories to enhance coherence and context-awareness of agent responses, and evaluates quality of agent responses with and without memory usage.
- Why unresolved: While paper provides some evaluation results comparing responses with and without memory usage, it does not directly measure impact of retrieved memories on specific aspects of coherence, relevance, empathy, and commonsense.
- What evidence would resolve it: Conducting detailed analysis of agent responses with and without retrieved memories, focusing on aspects of coherence, relevance, empathy, and commonsense, would provide necessary evidence.

## Limitations

- Information extraction quality relies heavily on GPT-3.5's ability to parse narrative text, with no quantitative validation of extraction accuracy
- Memory retrieval mechanism uses weighted combination of factors but lacks detailed analysis of how weights were determined or validated
- Multimodal generation quality (images and speech) is mentioned as enhancing user experience but lacks rigorous evaluation

## Confidence

- High Confidence: System architecture and overall approach to interactive narrative understanding
- Medium Confidence: Effectiveness of information extraction pipeline and memory retrieval mechanism
- Low Confidence: Quality and impact of multimodal synthesis (images and speech) on user experience

## Next Checks

1. Conduct controlled study comparing GPT-3.5's extracted character traits against human annotations on set of narratives with known character descriptions. Measure precision, recall, and F1-score for trait extraction.

2. Systematically vary weights of relevance, recency, and importance in memory retrieval formula and measure impact on agent response quality across different narrative scenarios. Include statistical significance testing.

3. Evaluate quality of generated environment images and speech using both automated metrics (e.g., CLIP similarity for images, speech intelligibility scores) and human ratings on relevance and alignment with narrative descriptions.