---
ver: rpa2
title: 'State Machine of Thoughts: Leveraging Past Reasoning Trajectories for Enhancing
  Problem Solving'
arxiv_id: '2312.17445'
source_url: https://arxiv.org/abs/2312.17445
tags:
- state
- reasoning
- smot
- machine
- plagt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces State Machine of Thoughts (SMoT), a novel
  framework that enhances Large Language Models' problem-solving abilities by leveraging
  predefined state machines to guide reasoning. SMoT employs a multi-agent mechanism
  with a Planning Agent (PlAgt) and an Action Agent (ActAgt) to decompose complex
  problems into discrete state transitions, improving accuracy and efficiency.
---

# State Machine of Thoughts: Leveraging Past Reasoning Trajectories for Enhancing Problem Solving

## Quick Facts
- arXiv ID: 2312.17445
- Source URL: https://arxiv.org/abs/2312.17445
- Reference count: 22
- Primary result: Achieves 95% accuracy on 24-point game task with improved efficiency over Chain-of-Thoughts and Tree of Thoughts

## Executive Summary
This paper introduces State Machine of Thoughts (SMoT), a novel framework that enhances Large Language Models' problem-solving abilities by leveraging predefined state machines to guide reasoning. SMoT employs a multi-agent mechanism with a Planning Agent (PlAgt) and an Action Agent (ActAgt) to decompose complex problems into discrete state transitions, improving accuracy and efficiency. The PlAgt manages overall flow and state updates, while the ActAgt handles specific task execution based on predefined state machine rules. Experiments on tasks like the 24-point game and taxi navigation show that SMoT achieves a remarkable 95% accuracy and significantly improves efficiency compared to state-of-the-art baselines like Chain-of-Thoughts and Tree of Thoughts.

## Method Summary
SMoT uses a two-agent architecture where the Planning Agent extracts states and events from observations, updates the state machine, and coordinates with the Action Agent, which determines specific state transitions and actions. The system relies on predefined state machines constructed from successful reasoning trajectories, either through human expertise or LLM reflection. For the 24-point game, the state machine tracks sums modulo 3, while for taxi navigation, it encodes optimal moves toward pickup/drop-off locations derived from ToT trajectories.

## Key Results
- Achieves 95% success rate on 24-point game task
- Significantly reduces LLM inference counts compared to ToT baseline on taxi navigation
- Demonstrates improved accuracy over CoT, CoT-SC, and ToT baselines across tested domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SMoT achieves 95% accuracy by replacing exploration-based reasoning with predefined state machine guidance
- Mechanism: The system uses a state machine to encode successful reasoning trajectories from past problems, allowing the LLM to follow optimal paths rather than exploring randomly
- Core assumption: Successful reasoning trajectories can be effectively captured and encoded in state machine format
- Evidence anchors:
  - [abstract]: "SMoT selects the most optimal sub-solutions and avoids incorrect ones"
  - [section 3.1.2]: "Utilizing the experience from the state machine, our proposed State Machine of Thoughts (SMoT) selects the most optimal sub-solutions and avoids incorrect ones"
  - [corpus]: No direct evidence about state machine effectiveness in corpus

### Mechanism 2
- Claim: Multi-agent architecture improves reasoning accuracy by dividing responsibilities
- Mechanism: The Planning Agent (PlAgt) manages state tracking and flow control while the Action Agent (ActAgt) handles specific task execution, allowing each to specialize
- Core assumption: Separating state management from action execution reduces cognitive load on each agent
- Evidence anchors:
  - [section 3]: "The multi-agent mechanism includes a planning agent responsible for controlling the overall flow and recording states and a state transition agent responsible for implementing state transitions"
  - [section 6.1.4]: "The PlAgt is responsible for recording states and events, while the ActAgt analysis events and states to carry out particular state transitions and actions"
  - [corpus]: No corpus evidence about multi-agent effectiveness

### Mechanism 3
- Claim: State transition optimization through human expertise and reflection on successful trajectories improves accuracy
- Mechanism: SMoT uses either human-provided state transition rules or automatically reflects on successful ToT trajectories to determine optimal actions
- Core assumption: Either human expertise or LLM reflection can accurately identify optimal state transitions
- Evidence anchors:
  - [section 3.1.2]: "Human expertise has better understanding for in-domain problems" and "we propose utilizing an LLM to reflect on the successful trajectories"
  - [section 6.2.3]: "We first reflect on the successful trajectories of ToT, and then utilize the external function to memorize which moves can be taken from each state"
  - [corpus]: No corpus evidence about reflection effectiveness

## Foundational Learning

- Concept: State machines as computational models
  - Why needed here: SMoT is fundamentally built on state machine theory to model problem-solving as state transitions
  - Quick check question: Can you describe the five-tuple definition of a state machine (S, E, A, s0, μ) and explain what each component represents?

- Concept: Reinforcement learning and exploration-exploitation tradeoff
  - Why needed here: The paper positions SMoT as solving the inefficiency of exploration-based methods like ToT
  - Quick check question: What is the fundamental problem with exploration-based reasoning methods that SMoT claims to solve, and how does the state machine approach address this?

- Concept: Multi-agent systems and task decomposition
  - Why needed here: SMoT uses a two-agent architecture (PlAgt and ActAgt) to divide reasoning responsibilities
  - Quick check question: How does separating planning and action responsibilities between agents improve reasoning accuracy compared to a single-agent approach?

## Architecture Onboarding

- Component map:
  - Planning Agent (PlAgt) -> State tracker, event extractor, flow controller
  - Action Agent (ActAgt) -> State transition executor, action determiner
  - State Machine -> Predefined knowledge base of optimal transitions
  - Environment/User Interface -> Observation provider and action receiver

- Critical path: PlAgt receives observation → extracts state/event → updates state → calls ActAgt → ActAgt determines transition → PlAgt executes action → provides feedback to environment/user

- Design tradeoffs:
  - Single vs. multi-agent: Single agent simpler but less accurate; multi-agent more accurate but with communication overhead
  - Hard-coded vs. learned state machines: Hard-coded more reliable but less flexible; learned more adaptable but potentially less accurate
  - Exploration vs. guidance: Exploration more general but inefficient; guidance more efficient but requires prior knowledge

- Failure signatures:
  - State machine mismatch: LLM cannot interpret or apply state machine rules correctly
  - Agent communication breakdown: PlAgt and ActAgt cannot coordinate effectively
  - Over-specialization: Agents become too narrowly focused and miss important context

- First 3 experiments:
  1. Implement SMoT for the greatest sum divisible by three task and compare with CoT baseline
  2. Test SMoT on taxi navigation with manually designed state machine vs. ToT exploration
  3. Evaluate performance degradation when removing state machine guidance (forcing LLM to explore)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the reasoning ability of Large Language Models (LLMs) be leveraged to assist in the design of state machines for unfamiliar problems?
- Basis in paper: [explicit] The paper mentions that existing SMoT solutions rely on manual design of state machines and highlights the need to utilize LLMs' reasoning ability to assist in designing state machines suitable for LLMs.
- Why unresolved: The paper acknowledges the importance of utilizing LLMs to design state machines but does not provide specific methods or techniques for doing so.
- What evidence would resolve it: Empirical studies demonstrating the effectiveness of using LLMs to automatically design state machines for various problem domains, along with comparisons to manually designed state machines.

### Open Question 2
- Question: How can the modular architecture of SMoT be further optimized to enhance problem-solving accuracy and efficiency?
- Basis in paper: [inferred] The paper discusses the modular approach of SMoT, which partitions responsibilities between the Planning Agent (PlAgt) and the Action Agent (ActAgt), resulting in improved accuracy and efficiency.
- Why unresolved: While the paper demonstrates the effectiveness of the current modular architecture, it does not explore potential optimizations or alternative designs for further enhancing problem-solving capabilities.
- What evidence would resolve it: Comparative studies evaluating the performance of different modular architectures and optimization techniques, along with quantitative metrics measuring accuracy and efficiency improvements.

### Open Question 3
- Question: How can SMoT be extended to handle problems that do not involve state transitions?
- Basis in paper: [explicit] The paper acknowledges that SMoT relies on state transitions and may be less applicable to situations without state transitions.
- Why unresolved: The paper does not provide specific approaches or strategies for adapting SMoT to handle problems that do not involve state transitions.
- What evidence would resolve it: Empirical studies demonstrating the effectiveness of adapting SMoT or developing alternative methods to handle problems without state transitions, along with quantitative metrics comparing performance to existing approaches.

## Limitations

- Task Domain Specificity: The evaluation focuses on two narrow domains without evidence of generalization to broader problem classes
- State Machine Construction: The method for determining optimal state transitions is underspecified and critical to reproducibility
- Baseline Fairness: Comparisons may be unfair since SMoT uses predefined state machine knowledge that baselines don't have access to

## Confidence

- High Confidence: The multi-agent architecture and basic functionality is well-specified and reproducible
- Medium Confidence: The claim of improved efficiency is supported but depends on state machine construction
- Low Confidence: Generalizability claims and 95% accuracy figure lack evidence beyond tested domains

## Next Checks

1. Implement SMoT on a third, distinct problem domain (e.g., Sokoban puzzle) to verify generalizability of the 95% accuracy claim

2. Reconstruct the state machine for taxi navigation from first principles using ToT trajectories, documenting exactly how optimal moves are identified and encoded

3. Implement a modified ToT baseline that receives the same state machine knowledge as SMoT to isolate the contribution of the multi-agent architecture versus the guidance information itself