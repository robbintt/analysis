---
ver: rpa2
title: Dual input stream transformer for vertical drift correction in eye-tracking
  reading data
arxiv_id: '2311.06095'
source_url: https://arxiv.org/abs/2311.06095
tags:
- fixation
- line
- dataset
- datasets
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a Dual Input Stream Transformer (DIST) model
  for correcting vertical drift in eye-tracking data during reading. DIST uses two
  input streams: fixation features and rendered page information.'
---

# Dual input stream transformer for vertical drift correction in eye-tracking reading data

## Quick Facts
- arXiv ID: 2311.06095
- Source URL: https://arxiv.org/abs/2311.06095
- Reference count: 38
- Primary result: Dual Input Stream Transformer (DIST) achieves 98.5% average accuracy on nine diverse eye-tracking datasets for vertical drift correction

## Executive Summary
This paper introduces DIST, a transformer-based model for correcting vertical drift in eye-tracking data during reading. DIST uses two input streams: fixation features (normalized x-y coordinates and line overlap) and rendered page information. Evaluated on nine diverse datasets, DIST outperforms classical algorithms with an average accuracy of 98.5% when used in an ensemble. Key success factors include line overlap features and the second input stream. The model shows robustness across different experimental setups, making it a reliable choice for practitioners.

## Method Summary
DIST combines fixation features and rendered page information through two parallel input streams. The model normalizes fixation coordinates using line height and width, then processes both streams through a CoAtNet feature extractor and BERT encoder. A Conditional Ordinal Regression for Neural Networks (CORN) loss function handles the ordinal nature of line assignments. The ensemble variant (E-DIST) combines multiple DIST instances with different normalization schemes to improve robustness across diverse datasets.

## Key Results
- DIST achieves 98.5% average accuracy across nine diverse eye-tracking datasets
- Dual input stream architecture outperforms classical algorithms
- Line overlap features and second input stream are critical success factors
- Ensemble approach (E-DIST) improves robustness across datasets with varying characteristics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual input stream enables richer contextual encoding
- Mechanism: The DIST model combines fixation features (x-y coordinates, line overlap) with rendered page information, allowing the model to leverage both temporal gaze patterns and spatial text layout
- Core assumption: Fixation assignments depend not just on where fixations fall, but also on the surrounding text structure
- Evidence anchors:
  - [abstract] "DIST uses two input streams: fixation features and rendered page information"
  - [section] "Our model incorporates two input streams: one consisting of normalized fixation features... the other input stream contains the rendered page with coarsely depicted fixation point information"
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.379, average citations=0.0. Weak corpus evidence for dual stream benefits specifically.
- Break condition: If page information is irrelevant (e.g., single line reading), dual stream adds unnecessary complexity

### Mechanism 2
- Claim: Line overlap features improve accuracy by capturing vertical drift patterns
- Mechanism: Including a feature indicating which line a fixation overlaps with helps the model distinguish between true fixations on a line and drift-induced misplacements
- Core assumption: Vertical drift creates fixations that spatially overlap with incorrect lines, and this overlap is a strong signal
- Evidence anchors:
  - [abstract] "incorporation of line overlap features... contribute to DIST's success"
  - [section] "The model incorporates two input streams... fixation related features namely, the x-y coordinates of each fixation point and the line number with which the fixation point overlaps with"
  - [corpus] Weak corpus evidence for line overlap features specifically.
- Break condition: If drift is minimal or uniformly distributed, line overlap feature may not provide discriminative power

### Mechanism 3
- Claim: Rank-consistent ordinal regression loss better handles ordered line assignments
- Mechanism: CORN loss function accounts for the ordinal nature of line indices, penalizing predictions based on rank distance rather than treating line assignments as unordered categories
- Core assumption: The difference between line 1 and line 2 is more meaningful than between line 1 and line 5, and this should influence the loss calculation
- Evidence anchors:
  - [abstract] "We use a Conditional Ordinal Regression for Neural Networks(CORN) loss"
  - [section] "The main encoder model uses several blocks... A Conditional Ordinal Regression for Neural Networks(CORN) loss is used to train the network"
  - [corpus] Weak corpus evidence for CORN loss specifically in eye-tracking applications.
- Break condition: If line assignments are truly unordered or if classification accuracy is sufficient, ordinal loss may be unnecessary

## Foundational Learning

- Concept: Transformer architectures and self-attention
  - Why needed here: DIST uses BERT-style transformer as main encoder to process long fixation sequences
  - Quick check question: Why are transformers preferred over RNNs for this task?

- Concept: Ensemble methods and model averaging
  - Why needed here: E-DIST combines multiple DIST instances with different normalization schemes to improve robustness
  - Quick check question: What's the key difference between E-DIST and a single trained DIST model?

- Concept: Eye-tracking data characteristics and preprocessing
  - Why needed here: Understanding fixation sequences, vertical drift, and normalization requirements is essential for implementing DIST
  - Quick check question: Why does DIST normalize fixation coordinates by line height and width?

## Architecture Onboarding

- Component map: Input → Dual streams (fixation features + rendered page) → CoAtNet feature extractor → Main BERT encoder → Linear head → CORN loss
- Critical path: Fixation normalization → Dual stream concatenation → BERT encoding → Line prediction → CORN loss computation
- Design tradeoffs: Dual streams add complexity but improve accuracy; CORN loss better handles ordinal targets but is more complex than cross-entropy
- Failure signatures: Poor normalization leads to inconsistent performance; missing line overlap features degrades accuracy; single normalization scheme fails on diverse datasets
- First 3 experiments:
  1. Train DIST with fixation features only (no second stream) to verify dual stream benefit
  2. Compare CORN loss vs cross-entropy loss to validate ordinal regression approach
  3. Test different normalization schemes to identify which works best for each dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DIST compare to other transformer-based architectures specifically designed for sequence-to-sequence tasks?
- Basis in paper: [inferred] The paper evaluates DIST against classical algorithms and mentions using a BERT-based architecture, but does not compare it to other transformer architectures like GPT or T5
- Why unresolved: The study focuses on comparing DIST to classical algorithms and does not explore its performance relative to other transformer-based models
- What evidence would resolve it: A comparative study evaluating DIST against other transformer architectures on the same datasets would provide insights into its relative performance

### Open Question 2
- Question: What is the impact of using different normalization schemes for the fixation data on the model's performance?
- Basis in paper: [explicit] The paper mentions that the choice of normalization scheme has a significant effect on model performance for some datasets
- Why unresolved: The paper does not provide a detailed analysis of how different normalization schemes affect the model's performance
- What evidence would resolve it: A comprehensive study comparing the performance of DIST using different normalization schemes would provide insights into the optimal approach

### Open Question 3
- Question: How does the model's performance change when using synthetic data from different sources or with different characteristics?
- Basis in paper: [inferred] The paper mentions using synthetic data during training, but does not explore the impact of using data from different sources or with varying characteristics
- Why unresolved: The study uses a specific synthetic dataset and does not investigate the effects of using alternative sources or characteristics
- What evidence would resolve it: A comparative study using synthetic data from different sources or with varying characteristics would provide insights into the model's robustness

## Limitations
- Reliance on rendered page information assumes access to original stimulus text
- Performance on extremely short lines or non-text stimuli is not evaluated
- Ensemble approach increases computational overhead during inference

## Confidence
- **High confidence**: The dual stream architecture improves accuracy over classical algorithms (supported by direct comparison on nine datasets with 98.5% average accuracy)
- **Medium confidence**: Line overlap features are a key success factor (mechanism is plausible but specific ablation studies are not detailed)
- **Medium confidence**: CORN loss function meaningfully improves ordinal line prediction (theoretically sound but empirical comparison with simpler losses is limited)

## Next Checks
1. **Ablation study**: Train DIST variants without the second input stream and without line overlap features to quantify their individual contributions to the 98.5% accuracy

2. **Generalization test**: Evaluate DIST on datasets with extreme line lengths (very short or very long) and non-text stimuli to assess robustness beyond the nine evaluated datasets

3. **Computational efficiency analysis**: Compare inference time and memory usage between DIST and classical algorithms to quantify the practical trade-off between accuracy gains and computational cost