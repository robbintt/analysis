---
ver: rpa2
title: First Attempt at Building Parallel Corpora for Machine Translation of Northeast
  India's Very Low-Resource Languages
arxiv_id: '2312.04764'
source_url: https://arxiv.org/abs/2312.04764
tags:
- languages
- translation
- low-resource
- language
- india
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the creation of initial bilingual corpora for
  thirteen very low-resource languages of India, all from Northeast India. It also
  presents the results of initial translation efforts in these languages.
---

# First Attempt at Building Parallel Corpora for Machine Translation of Northeast India's Very Low-Resource Languages

## Quick Facts
- arXiv ID: 2312.04764
- Source URL: https://arxiv.org/abs/2312.04764
- Reference count: 10
- Key outcome: First-ever parallel corpora for 13 low-resource Sino-Tibetan Indian languages paired with English; fine-tuning m2m100-48 outperformed transformer from scratch in both translation directions.

## Executive Summary
This paper presents the creation of initial bilingual corpora for thirteen very low-resource languages of Northeast India, all from the Sino-Tibetan family, paired with English. The authors collected Bible-based parallel text for languages including Adi, Angami, Ao, Apatani, Chokri, Dimasa, Karbi, Kokborok, Konyak, Mising, Paite, Tangkhul, and Thado, creating the first-ever parallel corpora for these languages. They evaluated translation models using both a transformer trained from scratch and a fine-tuned multilingual model (m2m100-48), finding that fine-tuning significantly outperformed the transformer approach in both translation directions.

## Method Summary
The authors collected Bible-based parallel text from bible.com for 13 low-resource Sino-Tibetan Indian languages paired with English. The datasets ranged from 7K-30K sentence pairs per language. They trained two types of models: a transformer model from scratch and a fine-tuned m2m100-48 multilingual model. The data was split into 70:10:20 train:dev:test sets for each language pair, and translation quality was evaluated using Sacrebleu BLEU scores in both English-to-Indian language and Indian language-to-English directions.

## Key Results
- Fine-tuning m2m100-48 outperformed transformer trained from scratch in both translation directions
- Languages with larger datasets showed better translation performance
- Translation quality was higher when translating from low-resource languages to English (BLEU 22.67) than vice versa (BLEU 13.93)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning a multilingual model outperforms training a transformer from scratch for low-resource languages.
- Mechanism: The multilingual model has already learned language-agnostic representations across many languages, which can be adapted to new low-resource languages with limited data.
- Core assumption: Pre-trained multilingual models capture transferable linguistic features that benefit low-resource language pairs.
- Evidence anchors:
  - [abstract] "Fine-tuning a multilingual model outperformed the transformer model trained from scratch in both translation directions."
  - [section] "Fine-tuning the multilingual model produced better results than the model built from scratch for English to Indian language translation."
- Break condition: If the pre-trained model lacks exposure to similar language families or scripts, the transfer may be ineffective.

### Mechanism 2
- Claim: Larger corpora lead to better translation performance in low-resource settings.
- Mechanism: More training data allows the model to learn more robust patterns and reduce overfitting, which is critical when data is scarce.
- Core assumption: The quality of the corpus is consistent and representative of the language pair.
- Evidence anchors:
  - [abstract] "The results indicate that languages with larger datasets tend to perform better."
  - [section] "Fine-tuning the m2m100 model performs better than using a transformer trained from scratch."
- Break condition: If the additional data is noisy or unrepresentative, performance may degrade despite increased volume.

### Mechanism 3
- Claim: Translation quality is higher when translating from low-resource languages to English than vice versa.
- Mechanism: English has more available resources and is often the pivot language in multilingual models, giving it an advantage in understanding and generating English.
- Core assumption: The multilingual model is more heavily trained or fine-tuned on English-centric data.
- Evidence anchors:
  - [abstract] "The average BLEU scores were 4.30 and 13.93 for translating from English to low-resource Indian languages, and 7.63 and 22.67 for translating from low-resource Indian languages to English."
  - [section] "the fine-tuned model exhibits better Bleu scores while translating to English than when translating to low-resource Indian languages."
- Break condition: If the model is explicitly balanced or fine-tuned to treat both directions equally, this asymmetry may not hold.

## Foundational Learning

- Concept: Sentence alignment in parallel corpora
  - Why needed here: Accurate sentence-level alignment is crucial for training translation models; misalignment leads to poor quality translations.
  - Quick check question: What happens if a source sentence is aligned with the wrong target sentence in the corpus?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The transformer is the baseline model used; understanding its self-attention and encoder-decoder structure is essential for interpreting results.
  - Quick check question: How does the self-attention mechanism in transformers help capture long-range dependencies in language?

- Concept: BLEU score calculation and interpretation
  - Why needed here: BLEU is the primary metric used to evaluate translation quality; understanding its strengths and limitations is key for result analysis.
  - Quick check question: What does a BLEU score of 22.67 indicate about translation quality compared to a score of 4.30?

## Architecture Onboarding

- Component map: Corpus collection -> Sentence alignment -> Preprocessing -> Model training (Transformer / Fine-tuned M2M100) -> Evaluation (BLEU)
- Critical path: Corpus -> Alignment -> Model -> Evaluation
- Design tradeoffs:
  - Transformer from scratch: More control, but requires more data and is less effective for low-resource languages.
  - Fine-tuning multilingual model: Better performance with less data, but depends on the quality and coverage of the pre-trained model.
- Failure signatures:
  - Low BLEU scores across all languages: Likely corpus quality or alignment issues.
  - Large variance in BLEU scores between languages: May indicate imbalance in corpus size or language family representation.
- First 3 experiments:
  1. Train a transformer from scratch on the Adi-English corpus and evaluate BLEU.
  2. Fine-tune M2M100 on the same Adi-English corpus and compare BLEU.
  3. Repeat experiments for a high-resource (larger corpus) language pair and analyze performance differences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do these newly created parallel corpora compare to existing low-resource language corpora in terms of size, quality, and domain coverage?
- Basis in paper: [explicit] The paper mentions that these are the first-ever parallel corpora for these languages and compares the dataset sizes in Table 1.
- Why unresolved: The paper does not provide a direct comparison with other existing low-resource language corpora.
- What evidence would resolve it: A comparative analysis of the new corpora against other established low-resource language corpora, considering factors like size, quality, and domain coverage.

### Open Question 2
- Question: What is the impact of corpus size on translation quality for these low-resource languages, and is there a threshold beyond which additional data does not significantly improve performance?
- Basis in paper: [inferred] The paper mentions that languages with larger datasets tend to perform better, but it does not provide a detailed analysis of the relationship between corpus size and translation quality.
- Why unresolved: The paper does not conduct experiments to determine the relationship between corpus size and translation quality or identify any potential thresholds.
- What evidence would resolve it: Experiments varying the size of the training data and measuring the impact on translation quality, potentially revealing a relationship or threshold.

### Open Question 3
- Question: How well do the translation models generalize to different domains beyond religious texts, and what are the challenges in adapting these models to other domains?
- Basis in paper: [explicit] The paper mentions that the datasets were collected from religious domains and expresses an intention to extend the corpora to include other domains.
- Why unresolved: The paper does not evaluate the translation models on different domains or discuss the challenges of adapting the models to other domains.
- What evidence would resolve it: Evaluating the translation models on different domains and analyzing the performance differences, along with identifying the challenges and potential solutions for domain adaptation.

## Limitations
- All corpora are derived from Bible texts, limiting generalizability to other domains
- Results may not transfer to non-Sino-Tibetan language families
- Fine-tuning hyperparameters and exact preprocessing pipeline are not fully detailed

## Confidence
**High Confidence:**
- Fine-tuning multilingual models outperforms training from scratch for these language pairs
- Larger corpus sizes correlate with better translation performance
- Translation quality is consistently higher when translating into English versus from English

**Medium Confidence:**
- The specific performance advantage of m2m100-48 over other multilingual models
- The magnitude of performance differences between language pairs
- Domain-specific performance (Bible text) generalizing to other domains

**Low Confidence:**
- Whether these results would hold for non-Sino-Tibetan language families
- Long-term stability of these models with additional data
- Impact of different preprocessing approaches on final results

## Next Checks
1. **Cross-domain validation**: Test the trained models on non-Bible parallel text (news, dialogue, etc.) for 2-3 languages to assess domain transfer capability.

2. **Corpus quality audit**: Randomly sample 100 sentence pairs per language and manually verify alignment accuracy and translation quality to quantify potential noise in the datasets.

3. **Alternative model comparison**: Implement a second fine-tuning approach using mT5 or NLLB to determine if the performance advantage is specific to m2m100 or a general fine-tuning phenomenon.