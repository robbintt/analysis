---
ver: rpa2
title: Adversarial Attacks on Fairness of Graph Neural Networks
arxiv_id: '2310.13822'
source_url: https://arxiv.org/abs/2310.13822
tags:
- fairness
- attack
- loss
- g-fairattack
- gnns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes G-FairAttack, a general framework for attacking
  the fairness of graph neural networks (GNNs) with unnoticeable effects on prediction
  utility. G-FairAttack addresses two challenges: designing a surrogate loss function
  that can represent various types of fairness-aware GNNs and optimizing the attack
  while ensuring unnoticeable utility changes.'
---

# Adversarial Attacks on Fairness of Graph Neural Networks

## Quick Facts
- arXiv ID: 2310.13822
- Source URL: https://arxiv.org/abs/2310.13822
- Reference count: 40
- Primary result: Proposes G-FairAttack, a general framework for attacking the fairness of GNNs while keeping prediction utility unnoticeable.

## Executive Summary
This paper introduces G-FairAttack, a novel framework for attacking the fairness of graph neural networks (GNNs) with minimal impact on prediction utility. The key innovation is a surrogate loss function based on total variation that can represent various types of fairness-aware GNN losses. The attack employs a non-gradient optimization method with an unnoticeable utility change constraint to effectively corrupt fairness metrics while maintaining prediction accuracy. Experiments on three real-world datasets demonstrate G-FairAttack's success in compromising fairness across different GNN architectures while remaining undetected through utility monitoring.

## Method Summary
G-FairAttack addresses two main challenges in attacking GNN fairness: creating a universal surrogate loss for diverse fairness mechanisms and optimizing the attack while keeping utility changes unnoticeable. The method uses a total variation loss as a common upper bound for various fairness loss terms including demographic parity, mutual information, and Wasserstein distance. A non-gradient optimization approach with a sequential edge-flipping strategy is employed, constrained by utility change thresholds. The framework includes a fast computation technique to reduce time complexity when evaluating potential edge modifications.

## Key Results
- Successfully corrupts fairness metrics (Δdp, Δeo) of multiple victim models while maintaining high prediction accuracy
- Achieves lower utility loss compared to baseline attacks on all three datasets
- Demonstrates effectiveness against both vanilla GCN and fairness-aware GNN variants

## Why This Works (Mechanism)

### Mechanism 1
- Claim: G-FairAttack can corrupt the fairness of various fairness-aware GNNs by designing a surrogate loss that represents different fairness loss terms.
- Mechanism: The surrogate loss function is designed as a total variation loss, which serves as a common upper bound for different fairness loss terms like demographic parity, mutual information, and Wasserstein distance. This allows the surrogate model to learn from various types of fairness-aware GNNs.
- Core assumption: The total variation loss is a valid upper bound for the different fairness loss terms used by various fairness-aware GNNs.
- Evidence anchors:
  - [abstract]: "we propose a novel surrogate loss function to help the surrogate model learn from all types of fairness-aware GNNs with theoretical analysis."
  - [section 3.1]: "we propose a novel surrogate loss function T V ( ˆY , S) = R 1 0 |P ˆY |S=0(z) − P ˆY |S=1(z)|dz, i.e., the total variation of ˆY on different sensitive groups."
- Break condition: If the total variation loss is not a valid upper bound for the different fairness loss terms, or if the surrogate model cannot effectively learn from the total variation loss.

### Mechanism 2
- Claim: G-FairAttack can achieve unnoticeable fairness attacks by introducing a utility change constraint.
- Mechanism: The attack formulation includes a constraint that limits the absolute value of the utility change over the training set, making the attack unnoticeable in terms of prediction utility.
- Core assumption: A change in prediction utility can be a strong clue of being manipulated, so limiting utility change makes the attack unnoticeable.
- Evidence anchors:
  - [abstract]: "we propose a novel unnoticeable constraint in utility change to make the fairness attack unnoticeable."
  - [section 3.2]: "we propose a novel unnoticeable constraint in utility change to make the fairness attack unnoticeable."
- Break condition: If the utility change constraint is not effective in making the attack unnoticeable, or if the attacker can find other ways to detect the manipulation.

### Mechanism 3
- Claim: G-FairAttack can efficiently solve the constrained optimization problem using a non-gradient optimization method.
- Mechanism: The attack algorithm uses a sequential attack method based on a scoring function that can provably increase the attacker's objective. It also uses a fast computation technique to reduce the time complexity.
- Core assumption: The scoring function can effectively identify the target edge to flip in each iteration, and the fast computation technique can significantly reduce the time complexity.
- Evidence anchors:
  - [section 3.2]: "we propose a non-gradient attack algorithm to solve the constrained optimization problem, which is verified to have a better performance than previous gradient-based methods."
  - [section 3.3]: "we propose a fast computation approach to reduce its time complexity."
- Break condition: If the scoring function is not effective in identifying the target edge, or if the fast computation technique does not significantly reduce the time complexity.

## Foundational Learning

- Concept: Adversarial attacks on graph neural networks
  - Why needed here: The paper investigates the problem of adversarial attacks on the fairness of GNNs, which is a specific type of attack on GNNs.
  - Quick check question: What are the different types of adversarial attacks on GNNs, and how do they differ from attacks on fairness?

- Concept: Fairness-aware graph neural networks
  - Why needed here: The paper focuses on attacking the fairness of fairness-aware GNNs, so understanding how these models work is crucial.
  - Quick check question: What are the different strategies used by fairness-aware GNNs to improve fairness, and how do they differ from each other?

- Concept: Optimization techniques for discrete problems
  - Why needed here: The attack problem is formulated as a bilevel optimization problem in a discrete domain, which is challenging to solve. The paper proposes a non-gradient optimization method to address this challenge.
  - Quick check question: What are the main challenges in solving optimization problems in discrete domains, and how do different optimization techniques address these challenges?

## Architecture Onboarding

- Component map: Surrogate model -> Attack algorithm -> Fast computation -> Utility change constraint
- Critical path: Train the surrogate model with the surrogate loss function -> Iteratively flip edges based on the scoring function and fast computation technique -> Update the surrogate model if necessary -> Check the utility change constraint after each edge flip
- Design tradeoffs:
  - Surrogate loss design: The total variation loss is a valid upper bound for different fairness loss terms, but it may not be the optimal choice for all cases.
  - Optimization method: The non-gradient optimization method can provably increase the attacker's objective, but it may be slower than gradient-based methods in some cases.
  - Fast computation: The fast computation technique can significantly reduce the time complexity, but it may introduce some approximation error.
- Failure signatures:
  - The attack fails to corrupt the fairness of the victim model.
  - The attack is detectable by the victim due to a large change in utility or other clues.
  - The attack is too slow to be practical for large graphs.
- First 3 experiments:
  1. Verify that the surrogate loss function can effectively represent different fairness loss terms by testing it on a small synthetic dataset with known fairness properties.
  2. Evaluate the effectiveness of the non-gradient optimization method compared to gradient-based methods on a small graph dataset.
  3. Test the fast computation technique on a medium-sized graph dataset to measure the reduction in time complexity and any potential approximation errors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed surrogate loss function generalize to fairness metrics beyond demographic parity, such as equalized odds or individual fairness?
- Basis in paper: [explicit] The paper mentions that the total variation loss can represent different types of fairness loss functions, but only provides theoretical analysis for demographic parity, mutual information, and Wasserstein distance.
- Why unresolved: The paper does not provide empirical evidence or theoretical analysis for the effectiveness of the surrogate loss on other fairness metrics.
- What evidence would resolve it: Experimental results on datasets with different fairness metrics, or theoretical proofs showing the surrogate loss bounds other fairness metrics.

### Open Question 2
- Question: What is the impact of the choice of kernel function and bandwidth parameter in the kernel density estimation for computing the total variation loss?
- Basis in paper: [explicit] The paper mentions using kernel density estimation to compute the total variation loss, but does not discuss the impact of different kernel functions or bandwidth parameters.
- Why unresolved: The paper does not provide sensitivity analysis or ablation studies on the choice of kernel function and bandwidth.
- What evidence would resolve it: Experimental results comparing the performance of the attack with different kernel functions and bandwidth parameters, or theoretical analysis on the impact of these choices.

### Open Question 3
- Question: How does the proposed fast computation approach scale to larger graphs with millions of nodes and edges?
- Basis in paper: [inferred] The paper proposes a fast computation approach to reduce the time complexity, but does not provide experimental results on large-scale graphs.
- Why unresolved: The paper does not discuss the practical limitations of the fast computation approach or provide scalability analysis.
- What evidence would resolve it: Experimental results on large-scale graphs, or theoretical analysis on the scalability of the fast computation approach.

## Limitations
- The effectiveness of the total variation loss as a universal upper bound for all fairness loss functions remains theoretically unproven beyond specific cases
- The unnoticeable constraint assumes utility change is the primary detection signal, which may not hold in all real-world scenarios
- Scalability to massive graphs with millions of nodes and edges has not been empirically validated

## Confidence
- **High Confidence**: The paper's experimental results demonstrating G-FairAttack's effectiveness across multiple datasets and victim models.
- **Medium Confidence**: The theoretical framework connecting total variation loss to various fairness metrics.
- **Low Confidence**: The generalizability of the unnoticeable constraint to real-world deployment scenarios where adversaries might employ additional detection methods.

## Next Checks
1. Cross-fairness mechanism validation: Test G-FairAttack's effectiveness against fairness-aware GNNs using fairness metrics beyond demographic parity, mutual information, and Wasserstein distance (e.g., equalized opportunity, predictive parity).
2. Real-world deployment simulation: Evaluate attack detection rates when the victim employs monitoring systems that track multiple indicators beyond prediction utility, such as feature importance shifts or temporal patterns.
3. Scalability stress test: Systematically measure attack performance and computation time as graph size increases beyond the three real-world datasets used, particularly focusing on the fast computation technique's effectiveness for massive graphs.