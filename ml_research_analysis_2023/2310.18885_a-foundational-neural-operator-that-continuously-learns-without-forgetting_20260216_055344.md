---
ver: rpa2
title: A foundational neural operator that continuously learns without forgetting
arxiv_id: '2310.18885'
source_url: https://arxiv.org/abs/2310.18885
tags:
- ncwno
- wavelet
- operator
- learning
- pdes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a foundational neural operator (NCWNO) for
  scientific computing that can learn and generalize solution operators for multiple
  parametric partial differential equations (PDEs) without catastrophic forgetting.
  The NCWNO uses a gated structure with local wavelet experts to acquire shared features
  across different physical systems and employs a memory-based ensembling approach
  for rapid adaptation to new tasks.
---

# A foundational neural operator that continuously learns without forgetting

## Quick Facts
- arXiv ID: 2310.18885
- Source URL: https://arxiv.org/abs/2310.18885
- Authors: 
- Reference count: 40
- Primary result: Foundational neural operator that learns multiple parametric PDEs without catastrophic forgetting

## Executive Summary
This paper introduces NCWNO (Neural Continuous Wavelet Neural Operator), a foundational operator learning framework that can sequentially learn multiple parametric partial differential equations (PDEs) without forgetting previously learned tasks. The key innovation is a gated architecture with local wavelet experts that preserves task-specific features while acquiring shared representations across different physical systems. The model demonstrates robust performance on diverse time-dependent PDEs including Allen-Cahn, Burgers, and Navier-Stokes equations, achieving state-of-the-art results with minimal hyperparameter tuning.

## Method Summary
NCWNO uses a gated structure with local wavelet experts to learn solution operators for multiple parametric PDEs. The architecture consists of lifting layers (P), expert wavelet integral blocks containing multiple local wavelet experts parameterized with different wavelet basis functions, a linear skip connection (g), and output lifting (Q). During training, the foundation model learns on multiple PDEs simultaneously. For new tasks, only the gating function parameters are fine-tuned while expert wavelet kernels remain frozen, enabling rapid adaptation. The gating mechanism conditions on both the input function and PDE label to output probabilities for each expert, creating a channel-level ensemble that preserves task-specific performance.

## Key Results
- First foundational operator learning algorithm robust against catastrophic forgetting
- Maintains positive transfer for new parametric PDEs while preserving old task performance
- Accurately learns new parametric PDEs with minimal fine-tuning compared to task-specific baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NCWNO avoids catastrophic forgetting through local wavelet experts and gating network
- Mechanism: Multiple local wavelet experts with different wavelet basis functions process inputs in parallel, with gating network outputting probabilities for each expert based on input function and PDE label
- Core assumption: Different physical systems share common features captured by different wavelet basis functions
- Evidence anchors:
  - [abstract] "gated structure with local wavelet experts to acquire shared features across different physical systems"
  - [section] "gating mechanism designed as function of actual parametric inputs and corresponding PDE label"
  - [corpus] Weak evidence; no similar continual learning claims found in corpus

### Mechanism 2
- Claim: Foundation model generalizes to new PDEs with minimal fine-tuning
- Mechanism: Pre-trained universal solution operator allows rapid adaptation by fine-tuning only gating network while keeping expert kernels fixed
- Core assumption: Universal solution operator captures enough shared structure across diverse PDEs
- Evidence anchors:
  - [abstract] "swiftly generalize to new parametric PDEs with minimal fine-tuning"
  - [section] "adapt foundation model by fine-tuning only parameters of gating functions"
  - [corpus] No direct evidence in corpus; assumption based on architectural design

### Mechanism 3
- Claim: Channel-level ensembling more effective than model averaging
- Mechanism: Element-wise weighted averaging of expert outputs at channel level provides finer-grained control over feature combinations
- Core assumption: Channel-level interactions capture task-specific dependencies better than simple averaging
- Evidence anchors:
  - [abstract] "ensembling probabilities vary across channels, NCWNO can consider interactions between channels more effectively than simple model averaging"
  - [section] "ensembling probabilities vary across channels, NCWNO can consider interactions between channels more effectively than simple model averaging"
  - [corpus] No direct evidence; claim based on architectural comparison

## Foundational Learning

- Concept: Neural operators learn mappings between infinite-dimensional function spaces rather than finite-dimensional vectors
  - Why needed here: Problem requires learning solution operators for parametric PDEs, which are inherently infinite-dimensional
  - Quick check question: What is the key difference between learning a function versus learning an operator in this context?

- Concept: Wavelet transforms provide multi-scale representations that can capture both local and global features
  - Why needed here: Different PDEs may have features at different scales, and wavelets allow model to represent these effectively
  - Quick check question: How does the undecimated wavelet transform help preserve spatial resolution in this architecture?

- Concept: Catastrophic forgetting occurs when sequential learning causes performance degradation on previous tasks
  - Why needed here: Paper aims to sequentially learn multiple PDEs without forgetting previously learned ones
  - Quick check question: What architectural feature in NCWNO specifically addresses catastrophic forgetting?

## Architecture Onboarding

- Component map: Input → Lifting (P) → Expert Wavelet Integral Blocks (with local experts and gating) → Linear Skip (g) → Output Lifting (Q)
- Critical path: Data flows through each expert block where local wavelet experts process input in parallel, outputs are weighted by gating probabilities, summed, passed through linear skip connection, and transformed to output space
- Design tradeoffs: Multiple local experts increase model capacity and robustness but also increase computation time per epoch; fixing expert kernels during fine-tuning reduces adaptation time but may limit performance on very dissimilar PDEs
- Failure signatures: Gating network failure to learn appropriate probabilities causes all experts to contribute equally and model loses ability to distinguish between tasks; too aggressive wavelet compression causes loss of important features
- First 3 experiments:
  1. Train foundation model on two simple PDEs (Burgers and Allen-Cahn) and verify it can predict both without catastrophic forgetting
  2. Test zero-shot prediction by varying gating probabilities to see if model can approximate PDEs it wasn't explicitly trained on
  3. Measure effect of number of local experts on both accuracy and computation time to find optimal tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can NCWNO architecture be extended to perform zero-shot super-resolution predictions, given that current feedforward gating function prevents this capability?
- Basis in paper: [explicit] Paper explicitly states while kernel convolutions in local wavelet experts preserve zero-shot super-resolution properties, feedforward gating function prevents this capability
- Why unresolved: Paper mentions instantiating gating function as wavelet operator could be solution but notes devising appropriate wavelet operator algorithm for estimating model probabilities is unclear
- What evidence would resolve it: Development and demonstration of wavelet operator-based gating function successfully enabling zero-shot super-resolution predictions without compromising NCWNO's performance on other tasks

### Open Question 2
- Question: What mechanisms can be introduced to enable NCWNO to discover task identities (PDE labels) automatically rather than requiring them as a priori knowledge?
- Basis in paper: [explicit] Paper states NCWNO assumes task identities/PDE labels are known a priori and suggests introducing reflexive mechanism to track experts' reflexes and discover task identities as future direction
- Why unresolved: While paper proposes idea of reflexive mechanism, it does not provide concrete implementation or methodology for achieving this capability
- What evidence would resolve it: Demonstration of NCWNO variant that can accurately identify and adapt to different PDE tasks without explicit task labels, maintaining or improving upon original model's performance

### Open Question 3
- Question: How can NCWNO be extended to handle multiscale PDEs where full microscale simulations become computationally expensive for large domains?
- Basis in paper: [explicit] Paper suggests developing multiscale NCWNO as possible extension to accelerate modeling of multiscale parametric PDEs by learning multiscale operators
- Why unresolved: Paper does not provide implementation details or theoretical framework for how such multiscale NCWNO would function or how it would integrate with existing architecture
- What evidence would resolve it: Successful implementation and validation of multiscale NCWNO on benchmark problems, demonstrating improved computational efficiency and accuracy compared to traditional methods for multiscale PDEs

### Open Question 4
- Question: How can NCWNO be extended to perform multiphysics simulations where interactions between different physical processes need to be modeled?
- Basis in paper: [explicit] Paper proposes developing physics-informed NCWNO by biasing local wavelet experts to satisfy physics of individual aspects of multi-physics processes as possible extension
- Why unresolved: Paper does not provide details on how to bias local wavelet experts to satisfy specific physics or how to integrate multiple physics into existing NCWNO framework
- What evidence would resolve it: Demonstration of physics-informed NCWNO that can accurately simulate multiphysics problems such as fluid-structure interaction or coupled thermal-mechanical systems with improved accuracy and efficiency compared to existing methods

## Limitations

- Limited evidence for generalization to completely unseen PDEs with fundamentally different physics
- Sparse implementation details for wavelet transforms and kernel parameterization
- Incomplete computational efficiency analysis lacking comprehensive comparison of inference latency and total resources

## Confidence

- **High Confidence:** Architecture design and implementation details clearly specified; sequential learning approach with frozen expert kernels is technically sound
- **Medium Confidence:** Claims about avoiding catastrophic forgetting supported by experimental results but test PDE diversity is limited; mechanism for shared feature learning through wavelets is plausible but not rigorously proven
- **Low Confidence:** Assertion that NCWNO can generalize to completely unseen PDEs with minimal fine-tuning is largely theoretical, as paper only tests on PDEs similar to training distribution

## Next Checks

1. Test zero-shot generalization on PDEs with fundamentally different physics (e.g., Schrödinger equation, Maxwell's equations) to verify universality claim
2. Compare inference latency and memory usage against baseline models to provide complete computational efficiency analysis
3. Perform ablation studies removing gating mechanism to quantify its contribution to preventing catastrophic forgetting versus other architectural components