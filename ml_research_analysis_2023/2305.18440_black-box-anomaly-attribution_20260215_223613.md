---
ver: rpa2
title: Black-Box Anomaly Attribution
arxiv_id: '2305.18440'
source_url: https://arxiv.org/abs/2305.18440
tags:
- uni00000013
- uni00000014
- attribution
- uni00000010
- anomaly
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a new approach for anomaly attribution in
  the doubly black-box regression setting, where neither the model''s internal parameters
  nor the training data are available. The authors identify a key limitation of existing
  methods like LIME, Shapley values, and integrated gradients: they are "deviation-agnostic,"
  meaning they explain the model''s prediction rather than the deviation from the
  true observation.'
---

# Black-Box Anomaly Attribution

## Quick Facts
- arXiv ID: 2305.18440
- Source URL: https://arxiv.org/abs/2305.18440
- Reference count: 10
- Primary result: Novel likelihood compensation approach for doubly black-box anomaly attribution outperforms existing methods

## Executive Summary
This paper addresses the challenge of attributing anomalies in regression models when both the model internals and training data are unavailable. The authors identify a critical limitation in existing attribution methods: they explain predictions rather than deviations from true observations. They propose likelihood compensation (LC), a novel approach that reframes anomaly attribution as a maximum likelihood optimization problem, finding perturbations to input that maximize the likelihood of observed outputs.

## Method Summary
The authors introduce likelihood compensation (LC) for anomaly attribution in doubly black-box regression settings. LC formulates attribution as finding a local perturbation δ that maximizes the likelihood of the observed output under a Gaussian observation model. The method uses elastic net regularization to balance sparsity and stability, and employs proximal gradient optimization to handle the black-box nature of the regression function. The approach is validated against existing methods including LIME, Shapley values, and integrated gradients on multiple benchmark datasets.

## Key Results
- LC demonstrates superior deviation sensitivity compared to existing attribution methods
- The approach outperforms LIME, IG, SV, EIG, and Z-score methods on Boston Housing, California Housing, Diabetes, and Building Energy datasets
- LC shows improved interpretability by directly attributing the anomaly rather than the model prediction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Likelihood compensation (LC) works because it reframes anomaly attribution as a maximum likelihood optimization problem, finding the perturbation δ that makes the observed output most probable under the model.
- Mechanism: Instead of explaining the model prediction or increment from baseline, LC finds a horizontal deviation δ such that f(x+δ) best explains y, making the deviation-sensitive attribution directly interpretable.
- Core assumption: The model's regression function f(x) can be queried at arbitrary inputs and the variance σ²(x) can be estimated from held-out data.
- Evidence anchors:
  - [abstract]: "LC seeks a local perturbation to the input that maximizes the likelihood of the observed output"
  - [section]: "δ∗= arg max {ln p(yt ∣xt+ δ)} subject to xt+ δ ∈vic(xt)" and "δ∗is defined through p(y ∣x)"
- Break condition: If f(x) is non-smooth or has many local maxima, gradient estimation becomes unreliable and LC may converge to suboptimal perturbations.

### Mechanism 2
- Claim: The equivalence between SV and EIG (Theorem 4) reveals that both are fundamentally based on integrated gradients, explaining why they are deviation-agnostic.
- Mechanism: By expanding the power series of f around the test point, SV's combinatorial definition reduces to EIG's expectation over baselines, showing they compute similar differential increments rather than explaining deviations.
- Core assumption: The regression function f is sufficiently smooth for Taylor expansion to be valid.
- Evidence anchors:
  - [section]: "The Shapley value is equivalent to the expected integrated gradient up to the second order"
  - [section]: Power series expansion in Eq. (4) and (9) showing SV ≈ EIG
- Break condition: If f has discontinuous derivatives or sharp gradients, the Taylor expansion breaks down and the equivalence no longer holds.

### Mechanism 3
- Claim: The elastic net regularization in LC prevents the selection of correlated variables that plague ℓ1 methods like LIME.
- Mechanism: By combining ℓ1 and ℓ2 penalties, LC balances sparsity with stability, avoiding random selection among correlated features while still producing sparse attribution scores.
- Core assumption: The correlation structure among input variables is not pathological (e.g., all variables perfectly correlated).
- Evidence anchors:
  - [section]: "We propose to use the elastic net regularization" and "lasso tends to pick one at random [ Roy et al., 2017]"
  - [corpus]: No direct evidence, but standard regularization theory supports this claim
- Break condition: If variables are perfectly correlated or nearly so, even elastic net may struggle to provide meaningful attribution.

## Foundational Learning

- Concept: Gaussian observation model p(y|x) = N(y|f(x), σ²(x))
  - Why needed here: Provides the likelihood framework that LC optimizes, connecting regression output to probability of observed y
  - Quick check question: Why does assuming Gaussian noise allow us to use maximum likelihood for attribution?

- Concept: Elastic net regularization (combination of ℓ1 and ℓ2 penalties)
  - Why needed here: Balances sparsity with stability when estimating perturbations δ, avoiding issues with correlated variables
  - Quick check question: How does elastic net differ from pure ℓ1 or ℓ2 regularization in feature selection?

- Concept: Proximal gradient method for optimization with ℓ1 penalty
  - Why needed here: Enables iterative computation of LC when the regression function is black-box and gradients must be estimated numerically
  - Quick check question: What is the role of the soft-thresholding operation in the proximal gradient update?

## Architecture Onboarding

- Component map: Black-box regression model f(x) → Gradient estimation module → Likelihood computation → Elastic net optimization → Attribution score δ
- Critical path: Input perturbation → Smooth gradient estimation → Log-likelihood evaluation → Regularized optimization → Convergence check
- Design tradeoffs: Smooth gradient estimation (Ns, η) vs computational cost; regularization strength (ν, λ) vs attribution sparsity; held-out data size for σ² estimation vs accuracy
- Failure signatures: Non-convergence of iterative updates; high variability in attribution scores across bootstrap samples; LC scores contradicting domain knowledge
- First 3 experiments:
  1. Run LC on 2D sinusoidal data with known analytical solution to verify deviation-sensitivity
  2. Compare LC vs LIME/SV on Boston Housing with varying regularization strengths
  3. Test gradient estimation stability on non-smooth regression surfaces

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the likelihood compensation approach be extended to handle categorical variables in the input data?
- Basis in paper: [explicit] The paper mentions that the input variable x is assumed to be noisy real-valued, but does not explicitly address categorical variables.
- Why unresolved: The paper does not provide a detailed discussion on how to handle categorical variables, which are common in many real-world datasets.
- What evidence would resolve it: A detailed explanation or algorithm for handling categorical variables in the likelihood compensation framework would resolve this question.

### Open Question 2
- Question: Can the likelihood compensation approach be adapted to handle non-Gaussian distributions in the observation model?
- Basis in paper: [explicit] The paper uses a Gaussian-based observation model in Eq. (26), but acknowledges that the distribution of f(xt) is not always Gaussian in general.
- Why unresolved: The paper does not explore alternative distributions for the observation model, which could be more appropriate for certain datasets or applications.
- What evidence would resolve it: An analysis of the performance of the likelihood compensation approach with different observation models, such as Laplace or Student's t-distribution, would help answer this question.

### Open Question 3
- Question: How does the performance of the likelihood compensation approach scale with the dimensionality of the input space?
- Basis in paper: [inferred] The paper does not explicitly discuss the scalability of the approach, but mentions that the optimization problem (27) can be challenging due to the black-box nature of f and the potential non-smoothness of the objective function.
- Why unresolved: The paper does not provide a detailed analysis of the computational complexity or scalability of the approach for high-dimensional input spaces.
- What evidence would resolve it: A comprehensive empirical study evaluating the performance of the likelihood compensation approach on datasets with varying input dimensions would help address this question.

## Limitations

- Performance may degrade if the true error distribution deviates significantly from Gaussian assumptions
- The equivalence between Shapley values and expected integrated gradients holds only under second-order Taylor expansion assumptions
- Computational cost increases with the number of iterations needed for gradient estimation and convergence

## Confidence

- **High confidence**: The mathematical formulation of LC as maximum likelihood optimization is sound and the proximal gradient method for elastic net regularization is well-established
- **Medium confidence**: Empirical superiority over baselines on benchmark datasets, though the choice of comparison methods and evaluation metrics could be more comprehensive
- **Medium confidence**: The claim about LC's advantage over ℓ1 methods in handling correlated variables, as this depends on specific correlation structures not fully explored

## Next Checks

1. **Distribution sensitivity test**: Evaluate LC performance when the black-box model's residuals follow non-Gaussian distributions (e.g., heavy-tailed or skewed) to assess robustness beyond assumed Gaussian noise

2. **High-dimensional correlation stress test**: Systematically vary correlation structures among input variables (from independent to perfectly correlated) to quantify when and how LC outperforms pure ℓ1 methods

3. **Non-smooth model evaluation**: Apply LC to black-box models with known discontinuities or sharp gradients (e.g., decision trees with deep splits) to test the validity of Taylor expansion assumptions and gradient estimation stability