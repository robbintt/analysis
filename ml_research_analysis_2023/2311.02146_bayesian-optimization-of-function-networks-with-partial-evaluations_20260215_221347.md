---
ver: rpa2
title: Bayesian Optimization of Function Networks with Partial Evaluations
arxiv_id: '2311.02146'
source_url: https://arxiv.org/abs/2311.02146
tags:
- function
- optimization
- node
- network
- bayesian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KGFN, a novel knowledge gradient acquisition
  function for Bayesian optimization of function networks that enables cost-aware
  partial evaluations. The key innovation is an acquisition function that decides
  which node and inputs to evaluate in a function network, allowing partial evaluations
  to reduce costs.
---

# Bayesian Optimization of Function Networks with Partial Evaluations

## Quick Facts
- arXiv ID: 2311.02146
- Source URL: https://arxiv.org/abs/2311.02146
- Reference count: 40
- Primary result: KGFN outperforms existing BOFN methods and benchmarks across synthetic and real-world problems by intelligently allocating budget to learn early nodes before evaluating later, more expensive nodes

## Executive Summary
This paper introduces KGFN, a novel knowledge gradient acquisition function for Bayesian optimization of function networks that enables cost-aware partial evaluations. The key innovation is an acquisition function that decides which node and inputs to evaluate in a function network, allowing partial evaluations to reduce costs. KGFN outperforms existing BOFN methods and other benchmarks across synthetic and real-world problems by intelligently allocating budget to learn early nodes before evaluating later, more expensive nodes. The method shows strong performance improvements when downstream nodes have high evaluation costs and strong correlations with parent nodes, which is common in real applications.

## Method Summary
KGFN extends the knowledge gradient framework to function networks by defining an acquisition function that maximizes the expected improvement of the final node's posterior mean divided by the evaluation cost. The method uses Monte Carlo estimation with reparameterization to handle the nested expectations in the knowledge gradient computation. To make the optimization computationally tractable, KGFN employs a discretization approach that samples around the current maximizer and uniformly across the domain. The acquisition function is optimized over all nodes in the network to decide which node to evaluate next and at what inputs.

## Key Results
- KGFN achieves significant cost reduction by selectively evaluating high-value nodes in function networks
- The method shows 2-3x improvement in solution quality compared to full-evaluation baselines on synthetic problems
- KGFN outperforms EIFN and TSFN on real-world problems including Manu-GP, FreeSolv, and Pharm datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KGFN reduces total evaluation cost by selectively evaluating only high-value nodes in the function network
- Mechanism: The knowledge gradient acquisition function explicitly divides expected improvement by evaluation cost, creating a cost-aware decision rule that favors cheaper nodes when they provide comparable information gain
- Core assumption: The posterior mean maximizer doesn't shift significantly after a single evaluation (used to discretize the search space)
- Evidence anchors:
  - [abstract]: "This approach can dramatically reduce query costs by allowing the evaluation of part of the network at a lower cost relative to evaluating the entire network"
  - [section 4.4]: "we incorporate a discretization method inspired by Frazier et al. (2009); Ungredda et al. (2022) in the algorithm"
- Break condition: When evaluation costs across nodes are similar, KGFN loses its cost advantage and behaves similarly to full-evaluation methods

### Mechanism 2
- Claim: KGFN achieves better solution quality by learning early nodes thoroughly before evaluating expensive downstream nodes
- Mechanism: The acquisition function allocates budget to explore upstream nodes that have high uncertainty and strong downstream influence, creating better-informed models for expensive evaluations
- Core assumption: Strong correlations exist between parent and child nodes in the function network
- Evidence anchors:
  - [abstract]: "The method shows strong performance improvements when downstream nodes have high evaluation costs and strong correlations with parent nodes"
  - [section 5.3]: "KGFN shows more outstanding performance when downstream nodes in the function network exhibit strong correlations with their parent nodes"
- Break condition: When parent-child correlations are weak, the benefit of partial evaluations diminishes

### Mechanism 3
- Claim: The Monte Carlo estimation with reparameterization enables efficient computation of the nested expectations in KGFN
- Mechanism: Standard normal samples are reparameterized to generate fantasy observations, which are then used to compute updated posterior means through recursive sampling along the network
- Core assumption: The GP posterior distributions remain well-behaved under fantasy observations
- Evidence anchors:
  - [section 4.2]: "We first generate a standard normal random vector W(j) = (W₁(j), W₂(j), ..., W_K(j))ᵀ ~ N(0, I_K) for a single j"
  - [section 4.1]: "We utilize a method called sample average approximation (SAA) (Kim et al., 2015)"
- Break condition: When the number of fantasy samples I is too small, the MC estimates become unreliable

## Foundational Learning

- Concept: Gaussian Process posterior updates with new observations
  - Why needed here: KGFN requires computing posterior means and variances after hypothetical observations (fantasy samples)
  - Quick check question: How does the posterior mean of a GP change when a new observation is added at a specific input?

- Concept: Knowledge gradient acquisition function for sequential decision making
  - Why needed here: KGFN extends the knowledge gradient to function networks with partial evaluations
  - Quick check question: What is the difference between expected improvement and knowledge gradient in terms of what they optimize?

- Concept: Function network structure and evaluation dependencies
  - Why needed here: KGFN must respect the network topology when deciding which nodes to evaluate
  - Quick check question: In a function network with nodes 1→2→3, which nodes must be evaluated before node 3 can be evaluated?

## Architecture Onboarding

- Component map: Acquisition optimizer -> Fantasy sampler -> Posterior mean estimator -> Cost-aware selector
- Critical path: Acquisition optimization (outer loop) -> Fantasy sampling (MC estimation) -> Posterior mean computation (recursive GP evaluation)
- Design tradeoffs: Computational efficiency (discretization) vs. solution accuracy (one-shot optimization)
- Failure signatures: Poor performance when costs are uniform, failure to converge when network has strong overlapping inputs
- First 3 experiments:
  1. Run KGFN on the Ackley6D problem with varying cost ratios (c2/c1 = 1, 10, 50) to observe cost sensitivity
  2. Compare KGFN vs EIFN on the FreeSolv dataset with the same budget allocation
  3. Test KGFN on a simple 2-node sequential network where f2(y) = -y*sin(5y/6π) and f1 is the negated Ackley function

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the computational overhead of optimizing KGFN scale with the number of nodes in the function network?
- Basis in paper: [explicit] The paper states that optimizing KGFN consumes substantial computational resources as it considers all nodes and available outputs at each iteration, which could be challenging for larger networks.
- Why unresolved: The paper acknowledges this limitation but does not provide quantitative analysis of how the computational cost scales with network size.
- What evidence would resolve it: Empirical studies showing the relationship between the number of nodes and the computational time required to optimize KGFN, ideally across multiple network structures and sizes.

### Open Question 2
- Question: How would KGFN perform on function networks with non-Gaussian observation noise or heavy-tailed distributions?
- Basis in paper: [inferred] The paper assumes Gaussian process models with Gaussian noise for all function nodes, which is a common but restrictive assumption in Bayesian optimization.
- Why unresolved: The paper does not test KGFN on non-Gaussian noise models or discuss how robust it is to violations of this assumption.
- What evidence would resolve it: Comparative experiments of KGFN against baselines when using non-Gaussian likelihood functions (e.g., Student-t, Laplace) or when the true observation noise is non-Gaussian.

### Open Question 3
- Question: Can the discretization method for optimizing KGFN be extended to high-dimensional design spaces?
- Basis in paper: [explicit] The paper uses a discretization approach that constructs a discrete set A by sampling uniformly across the domain and around the current maximizer. It notes that discretization will lead to less accurate solutions, especially in higher dimensions.
- Why unresolved: The paper does not explore how well this discretization method scales to high-dimensional spaces or whether alternative discretization strategies might be more effective.
- What evidence would resolve it: Empirical studies comparing KGFN's performance with different discretization strategies (e.g., low-discrepancy sequences, adaptive discretization) across problems with varying input dimensions.

## Limitations
- The method's performance relies heavily on the presence of strong parent-child correlations in the function network
- Computational complexity scales poorly with the number of nodes due to the MC-based optimization
- The discretization approach may struggle with high-dimensional input spaces

## Confidence
- **High confidence**: The theoretical foundation of KGFN and its cost-aware knowledge gradient formulation is sound and well-justified
- **Medium confidence**: The empirical results showing KGFN's superiority over existing methods are robust across multiple problem types
- **Medium confidence**: The discretization approach provides a practical compromise between optimization quality and computational efficiency

## Next Checks
1. Test KGFN on function networks with 5+ nodes and varying topological structures to assess scalability and robustness to network complexity
2. Evaluate KGFN's performance when parent-child correlations follow different functional forms (beyond the linear relationships in the current experiments)
3. Compare KGFN's computational overhead against EIFN across a range of budget sizes to quantify the cost-benefit tradeoff in practice