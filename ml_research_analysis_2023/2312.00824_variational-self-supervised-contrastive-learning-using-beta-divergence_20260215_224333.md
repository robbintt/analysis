---
ver: rpa2
title: Variational Self-Supervised Contrastive Learning Using Beta Divergence
arxiv_id: '2312.00824'
source_url: https://arxiv.org/abs/2312.00824
tags:
- learning
- self-supervised
- contrastive
- variational
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a variational contrastive learning method
  (VCL) that incorporates beta-divergence to enhance robustness against noise in unlabelled
  and noisy datasets. The proposed method leverages a contrastive learning framework
  augmented with a Gaussian sampling head that learns the distribution parameters
  of embeddings.
---

# Variational Self-Supervised Contrastive Learning Using Beta Divergence

## Quick Facts
- arXiv ID: 2312.00824
- Source URL: https://arxiv.org/abs/2312.00824
- Reference count: 40
- Achieves 89.23% accuracy on CelebA and 88.12% on YFCC-CelebA in linear evaluation scenarios

## Executive Summary
This paper introduces a variational contrastive learning method (VCL) that incorporates beta-divergence to enhance robustness against noise in unlabelled and noisy datasets. The proposed method leverages a contrastive learning framework augmented with a Gaussian sampling head that learns the distribution parameters of embeddings. By employing beta-divergence, the approach effectively mitigates the impact of outliers, ensuring improved performance in multi-label settings, particularly with face attribute datasets like CelebA and YFCC-CelebA.

## Method Summary
The VCL framework combines contrastive learning with variational inference using beta-divergence. The architecture consists of a VGG16bn backbone that extracts features, followed by a Gaussian sampling head that learns mean and log-variance parameters for embedding distributions. The model creates two augmentations of each input image, extracts features, learns distribution parameters, samples from these distributions, and computes a multi-term loss combining beta-NT-Xent, distribution similarity, and distribution normalizing losses. The beta-divergence formulation downweights outliers in the loss function, while the variational approach learns robust distribution parameters for embeddings.

## Key Results
- Achieves 89.23% accuracy on CelebA and 88.12% on YFCC-CelebA in linear evaluation scenarios
- Outperforms state-of-the-art self-supervised methods on noisy multi-label datasets
- Demonstrates effectiveness in handling outliers through beta-divergence weighting mechanism

## Why This Works (Mechanism)

### Mechanism 1
The beta-divergence formulation modifies the likelihood weighting such that data points with lower probability density under the model contribute less to the loss, thereby reducing the impact of noisy samples. This works under the assumption that noisy samples have significantly different distributions from true data, making them identifiable through lower probability density.

### Mechanism 2
The Gaussian sampling head learns mean and log-variance parameters of embedding distributions for each augmented view. This enables more robust representation learning by sampling from learned distributions rather than using point estimates, capturing the underlying structure better than deterministic approaches.

### Mechanism 3
The combination of distribution similarity (Jensen-Shannon divergence) and distribution normalizing (KL divergence to standard Gaussian) losses ensures that augmentations of the same image map to similar distributions while maintaining well-behaved standard Gaussian properties, preventing degenerate solutions.

## Foundational Learning

- Concept: Contrastive Learning
  - Why needed here: Provides framework for learning representations by pulling together positive pairs and pushing apart negative pairs
  - Quick check question: What is the difference between positive and negative pairs in contrastive learning, and why is this distinction important for representation learning?

- Concept: Beta-Divergence
  - Why needed here: Provides robust alternative to KL-divergence by downweighting outliers in the loss function
  - Quick check question: How does beta-divergence differ from KL-divergence in terms of sensitivity to outliers, and why is this important for noisy datasets?

- Concept: Variational Autoencoders (VAEs)
  - Why needed here: Adapted to learn distribution parameters for embeddings rather than pixel-level reconstructions
  - Quick check question: What is the key difference between a standard autoencoder and a variational autoencoder, and how does this difference apply to the Gaussian sampling head in this work?

## Architecture Onboarding

- Component map: Input image → Augmentations → Backbone feature extraction → Gaussian sampling head → Distribution sampling → Contrastive loss computation → Backpropagation
- Critical path: input image → augmentations → backbone feature extraction → Gaussian sampling head → distribution sampling → contrastive loss computation → backpropagation
- Design tradeoffs: VGG16bn provides computational efficiency but may limit compared to modern architectures; beta-divergence parameter requires careful tuning; Gaussian assumption simplifies model but may not capture complex distributions
- Failure signatures: Poor convergence indicates incorrect temperature/beta parameters; degenerate solutions suggest issues with distribution parameters; overfitting to noise indicates weak distribution normalizing loss
- First 3 experiments:
  1. Train model without beta-divergence component on clean dataset for baseline performance
  2. Train with different beta-divergence values (β = 0.001, 0.005, 0.01) on noisy dataset to find optimal robustness
  3. Train with only beta-divergence component (no distribution similarity/normalizing losses) to understand individual loss contributions

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided content.

## Limitations

- The Gaussian sampling head introduces additional complexity and may struggle with highly non-Gaussian data distributions
- The beta-divergence parameter (β) requires careful tuning and may vary significantly across datasets and noise levels
- Computational overhead of learning distribution parameters could be prohibitive for large-scale applications

## Confidence

- **High Confidence**: General framework of combining variational inference with contrastive learning is well-founded
- **Medium Confidence**: Specific architectural choices and hyper-parameter settings appear reasonable but may not be optimal
- **Low Confidence**: Effectiveness of distribution similarity and normalizing losses is less certain with limited empirical evidence

## Next Checks

1. **Ablation study on loss components**: Systematically remove each loss term to quantify individual contributions to performance
2. **Robustness across noise types**: Test model performance with different types of label noise (random vs. systematic bias)
3. **Comparison with modern backbones**: Evaluate whether VGG16bn backbone limits performance compared to ResNet or Vision Transformers