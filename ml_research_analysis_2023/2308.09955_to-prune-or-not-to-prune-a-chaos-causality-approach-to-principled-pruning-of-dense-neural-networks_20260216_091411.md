---
ver: rpa2
title: 'To prune or not to prune : A chaos-causality approach to principled pruning
  of dense neural networks'
arxiv_id: '2308.09955'
source_url: https://arxiv.org/abs/2308.09955
tags:
- network
- pruning
- dense
- neural
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a principled neural network pruning method
  that combines chaos theory (via Lyapunov exponents) and Granger causality to identify
  and remove non-causal weights that do not impact model accuracy. The approach trains
  a dense network to estimate weight update chaos and measure which weight updates
  Granger-cause accuracy changes.
---

# To prune or not to prune : A chaos-causality approach to principled pruning of dense neural networks

## Quick Facts
- arXiv ID: 2308.09955
- Source URL: https://arxiv.org/abs/2308.09955
- Authors: 
- Reference count: 34
- One-line primary result: Combines Lyapunov exponents and Granger causality to identify non-causal weights for principled pruning of dense neural networks

## Executive Summary
This paper proposes LEGCNet, a principled neural network pruning method that combines chaos theory and causality to identify and remove non-causal weights without harming model performance. The approach trains a dense network to estimate weight update chaos via Lyapunov exponents and measures which weight updates Granger-cause accuracy changes. Weights whose updates do not causally influence accuracy are pruned. The method is tested across multiple tabular datasets and achieves similar or better accuracy and F1-scores with significantly fewer parameters, reduced FLOPs, and faster convergence compared to random and magnitude-based pruning. The pruned networks maintain feature explainability (verified via SHAP) and are well-trained (validated via WeightWatcher diagnostics), supporting the lottery ticket hypothesis.

## Method Summary
LEGCNet combines Lyapunov exponents and Granger causality to identify non-causal weights for pruning. During dense network training, weight updates are recorded and windowed Lyapunov exponents are computed to quantify chaotic dynamics. Granger causality tests determine whether these chaotic signatures cause changes in classification accuracy. Weights whose updates do not Granger-cause accuracy changes are identified as non-causal and pruned. Two variants are proposed: LEGCNet-FT (full training) prunes after complete training, while LEGCNet-PT (partial training) makes pruning decisions based on early training epochs (10%) for faster convergence. The pruned sparse networks are retrained and validated using SHAP for feature importance and WeightWatcher for training quality diagnostics.

## Key Results
- LEGCNet-FT achieves significant reductions in FLOPs without compromising accuracy on multiple tabular datasets
- LEGCNet-PT converges in half the epochs required by dense networks while maintaining comparable accuracy
- Pruned networks maintain feature explainability (SHAP) and are well-trained (WeightWatcher alpha > 2.0), supporting the lottery ticket hypothesis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pruning guided by Lyapunov exponents and Granger causality identifies non-causal weights without harming performance.
- Mechanism: The method computes windowed Lyapunov exponents from weight time series during training. Granger causality tests determine whether these chaotic signatures cause changes in classification accuracy. Weights whose updates do not Granger-cause accuracy changes are considered non-causal and pruned.
- Core assumption: Chaotic behavior in weight updates (positive Lyapunov exponents) reliably signals important learning dynamics that can be causally linked to accuracy changes.
- Evidence anchors:
  - [abstract] "The approach trains a dense network to estimate weight update chaos and measure which weight updates Granger-cause accuracy changes."
  - [section 3.4] "We then computed the Granger causality between the windowed Lyapunov exponents and the misclassification rate to identify weight connections that Granger caused misclassification."
- Break condition: If weight updates are not sufficiently chaotic (Lyapunov exponents near zero) or if Granger causality tests fail to detect meaningful relationships between chaos and accuracy, the pruning decisions become unreliable.

### Mechanism 2
- Claim: Sparse subnetworks found by LEGCNet perform comparably to dense networks, supporting the lottery ticket hypothesis.
- Mechanism: By pruning only non-causal weights, the method retains a sparse subnetwork that preserves the essential learning dynamics. This subnetwork, when trained, achieves accuracy and F1-scores similar to the original dense network.
- Core assumption: The sparse subnetwork retains the "winning ticket" that can be trained to match dense network performance.
- Evidence anchors:
  - [abstract] "The pruned networks maintain feature explainability (verified via SHAP) and are well-trained (validated via WeightWatcher diagnostics), supporting the lottery ticket hypothesis."
  - [section 5] "LEGCNet-FT achieves notable reductions in Flops without compromising accuracy... both network achieves a similar level of performance - accuracy, and f1-score- without significant differences, thus validating the lottery ticket hypothesis."
- Break condition: If the pruned subnetwork loses critical feature representations or if the remaining weights cannot be trained effectively, performance will degrade.

### Mechanism 3
- Claim: Early pruning decisions based on partial training (LEGCNet-PT) yield sparse networks that converge faster and use fewer FLOPs.
- Mechanism: By computing Lyapunov exponents and Granger causality on weight updates from only the first 10% of training epochs, the method identifies pruning candidates early. The sparse network then trains for the remaining epochs, achieving faster convergence and reduced computational cost.
- Core assumption: Early weight update dynamics are representative enough to identify non-causal weights for effective pruning.
- Evidence anchors:
  - [abstract] "The approach is effective even when pruning decisions are based on early training epochs, offering practical advantages for resource-constrained deployment."
  - [section 5] "LEGCNet-FT converges significantly faster consuming a few epochs compared to the dense network... Specifically, for the Titanic, Vowel, and Cancer datasets, LEGCNet-FT achieves convergence in just half the number of epochs required by the dense network."
- Break condition: If early training dynamics differ significantly from later dynamics, pruning decisions may be suboptimal, leading to reduced accuracy or slower convergence.

## Foundational Learning

- Concept: Lyapunov exponents and chaos theory
  - Why needed here: Used to quantify the chaotic nature of weight updates, which informs which weights are important for learning.
  - Quick check question: What does a positive Lyapunov exponent indicate about a dynamical system?

- Concept: Granger causality
  - Why needed here: Determines whether chaotic weight updates causally influence classification accuracy, guiding pruning decisions.
  - Quick check question: In Granger causality, what does it mean if variable X Granger-causes variable Y?

- Concept: Lottery ticket hypothesis
  - Why needed here: Provides theoretical justification that sparse subnetworks can match dense network performance after training.
  - Quick check question: What is the key claim of the lottery ticket hypothesis regarding overparameterized networks?

## Architecture Onboarding

- Component map:
  Dense neural network training -> Lyapunov exponent computation -> Granger causality testing -> Pruning -> Sparse network training -> Validation (SHAP + WeightWatcher)

- Critical path:
  1. Train dense network and record weight updates
  2. Compute windowed Lyapunov exponents
  3. Test Granger causality between LE and accuracy
  4. Prune non-causal weights
  5. Retrain sparse network
  6. Validate performance and diagnostics

- Design tradeoffs:
  - Early vs. full training for pruning decisions (LEGCNet-PT vs. LEGCNet-FT)
  - Computational cost of chaos and causality analysis vs. pruning benefits
  - Granularity of windowed LE computation (window size affects detection of chaotic patterns)

- Failure signatures:
  - Positive Lyapunov exponents but no Granger causality detected (chaos not linked to accuracy)
  - Poor performance after pruning (critical weights mistakenly identified as non-causal)
  - WeightWatcher diagnostics show poor layer training (alpha < 2.0)
  - SHAP feature importance changes significantly after pruning (loss of explainability)

- First 3 experiments:
  1. Run LEGCNet-FT on a simple tabular dataset (e.g., Iris) and compare accuracy/FLOPs to dense network.
  2. Run LEGCNet-PT on the same dataset and verify early pruning decisions yield similar results.
  3. Apply WeightWatcher and SHAP diagnostics to both pruned networks and compare to dense network baselines.

## Open Questions the Paper Calls Out

- Question: How does the LEGCNet approach scale to deeper and more complex architectures like ResNets or DenseNets?
  - Basis in paper: [explicit] The authors state "Our pruning approach is yet to be tested on baseline architectures (Resnet, Densenet), and Large Language Models..."
  - Why unresolved: The paper only tests on single-hidden-layer MLPs. Extending to deeper architectures with skip connections or dense connections may change the dynamics of chaos and causality detection.
  - What evidence would resolve it: Testing LEGCNet on ResNet/DenseNet architectures with performance metrics and diagnostic tools (WW, SHAP) comparable to the tabular datasets.

- Question: Can the pruning efficiency (number of parameters removed) be improved while maintaining performance, and what is the theoretical limit?
  - Basis in paper: [inferred] The authors note "the percentage of pruned weights is significantly less" because only non-causal weights are pruned, suggesting room for improvement.
  - Why unresolved: The method prioritizes maintaining performance over aggressive pruning, but the relationship between pruning aggressiveness and performance degradation is not characterized.
  - What evidence would resolve it: Systematic ablation studies varying pruning thresholds and analyzing the Pareto frontier of parameter reduction vs. accuracy.

- Question: What is the computational overhead of computing Lyapunov exponents and Granger causality during training, and how does it compare to the savings from reduced FLOPs?
  - Basis in paper: [explicit] The method requires computing LE and GC, but no computational cost analysis is provided.
  - Why unresolved: While the pruned networks have fewer FLOPs, the additional computations for pruning decisions could offset these savings, especially for large-scale models.
  - What evidence would resolve it: Empirical comparison of wall-clock training time and energy consumption (carbon footprint) between dense training, pruning baseline methods, and LEGCNet.

- Question: Does the LEGCNet approach transfer to domains beyond tabular data, such as vision, NLP, or reinforcement learning?
  - Basis in paper: [explicit] The authors state "Our pruning approach is yet to be tested on baseline architectures (Resnet, Densenet), and Large Language Models..."
  - Why unresolved: The method relies on chaos and causality detection in weight updates, which may manifest differently in different domains or architectures.
  - What evidence would resolve it: Applying LEGCNet to vision tasks (e.g., CIFAR-10), NLP models (e.g., BERT), or RL agents and verifying performance retention and explainability.

## Limitations
- The method relies on chaotic dynamics in weight updates being detectable and causally linked to accuracy changes; if these conditions fail, pruning decisions become unreliable
- Effectiveness on non-tabular datasets (images, sequences) remains untested
- Hyperparameter choices for Lyapunov exponent computation (window size, embedding dimension) and Granger causality testing are not fully specified

## Confidence
- High confidence in the mathematical framework combining Lyapunov exponents and Granger causality for pruning decisions
- Medium confidence in the empirical results across the six tabular datasets tested
- Low confidence in generalization to other architectures (CNNs, transformers) and non-tabular data

## Next Checks
1. Apply LEGCNet to a standard image classification benchmark (e.g., CIFAR-10) and compare pruning efficiency and accuracy retention against magnitude-based pruning baselines
2. Perform ablation studies varying the window size for Lyapunov exponent computation and Granger causality testing to identify optimal parameters
3. Test the method on recurrent neural networks (LSTMs/GRUs) to evaluate effectiveness on sequential data architectures