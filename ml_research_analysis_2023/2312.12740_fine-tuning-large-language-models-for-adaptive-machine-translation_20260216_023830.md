---
ver: rpa2
title: Fine-tuning Large Language Models for Adaptive Machine Translation
arxiv_id: '2312.12740'
source_url: https://arxiv.org/abs/2312.12740
tags:
- translation
- mistral
- one-shot
- zero-shot
- ne-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the outcomes of fine-tuning Mistral 7B, a general-purpose
  large language model (LLM), for adaptive machine translation (MT). The fine-tuning
  process involves utilising a combination of zero-shot and one-shot translation prompts
  within the medical domain.
---

# Fine-tuning Large Language Models for Adaptive Machine Translation

## Quick Facts
- arXiv ID: 2312.12740
- Source URL: https://arxiv.org/abs/2312.12740
- Reference count: 10
- Fine-tuning Mistral 7B for adaptive MT in medical domain shows improvements in both zero-shot and one-shot translation

## Executive Summary
This paper presents a fine-tuning approach for Mistral 7B to improve adaptive machine translation capabilities, particularly for Spanish-to-English medical translation. The researchers demonstrate that fine-tuning on a relatively small dataset (20,000 segments) using QLoRA significantly enhances the model's zero-shot and one-shot translation quality. The results show that the fine-tuned Mistral outperforms ChatGPT in zero-shot translation and matches NLLB 3.3B's performance while surpassing it in one-shot translation quality.

## Method Summary
The researchers fine-tuned Mistral 7B using QLoRA with 4-bit quantization and LoRA adapters on a dataset of 20,000 segments (10,000 zero-shot + 10,000 one-shot prompts) from medical domain OPUS datasets. They used semantic search with SentenceTransformers and Faiss to retrieve fuzzy matches for one-shot prompts. The fine-tuning process employed a learning rate of 2e-3, batch size of 32, and 1 epoch training. After fine-tuning, models were converted to CTranslate2 format for efficient inference. Evaluation was performed using standard MT metrics (BLEU, chrF++, TER, COMET) on a test set of 10,000 segments, comparing against baseline Mistral 7B, ChatGPT "gpt-3.5-turbo", and NLLB 3.3B.

## Key Results
- Fine-tuned Mistral outperforms ChatGPT "gpt-3.5-turbo" in zero-shot translation quality
- Zero-shot translation of fine-tuned Mistral matches NLLB 3.3B performance
- One-shot translation quality of fine-tuned Mistral surpasses NLLB 3.3B
- Small dataset (20,000 segments) sufficient to achieve significant improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning Mistral 7B on a small, domain-specific dataset (20,000 segments) significantly improves its zero-shot and one-shot translation quality.
- Mechanism: The fine-tuning process adapts the general-purpose model's parameters to better capture domain-specific patterns and terminology, effectively reducing the gap between zero-shot and one-shot performance.
- Core assumption: Mistral 7B's architecture allows for efficient fine-tuning that generalizes to both zero-shot and one-shot scenarios without catastrophic forgetting.
- Evidence anchors:
  - [abstract] "Our experiments demonstrate that, with a relatively small dataset of 20,000 segments that incorporate a mix of zero-shot and one-shot prompts, fine-tuning significantly enhances Mistral's in-context learning ability, especially for real-time adaptive MT."
  - [section 2.1] "The whole dataset includes 1,868,505 segments before filtering, and 922,343 segments after filtering. However, we used only part of it for this preliminary experiment."
- Break condition: If the dataset is too small to capture domain-specific nuances, or if fine-tuning causes catastrophic forgetting of general translation capabilities.

### Mechanism 2
- Claim: Fine-tuning improves Mistral's in-context learning ability, making it competitive with task-oriented models like NLLB 3.3B and commercial models like ChatGPT.
- Mechanism: By exposing the model to both zero-shot and one-shot prompts during fine-tuning, it learns to better utilize contextual information at inference time, reducing the performance gap between different prompting strategies.
- Core assumption: In-context learning is a learned behavior that can be enhanced through fine-tuning on appropriate examples.
- Evidence anchors:
  - [abstract] "Moreover, the zero-shot translation of the fine-tuned Mistral matches NLLB 3.3B's performance, and its one-shot translation quality surpasses that of NLLB 3.3B."
  - [section 3] "Fine-tuning an efficient LLM like Mistral 7B helps to produce a high-quality zero-shot translation comparable to that of MT task-oriented models such as NLLB 3.3B, while achieving adaptive gains of one-shot translation on par with commercial LLMs such as ChatGPT 'gpt-3.5-turbo'."
- Break condition: If the model's architecture limits its ability to effectively learn in-context patterns, or if the fine-tuning data is not representative of real-world use cases.

### Mechanism 3
- Claim: Fine-tuning with QLoRA on a small dataset provides efficiency gains without sacrificing translation quality.
- Mechanism: QLoRA's 4-bit quantization and LoRA-based adaptation allow for memory-efficient fine-tuning that still captures domain-specific knowledge, making the approach practical for resource-constrained environments.
- Core assumption: The combination of quantization and LoRA is sufficient to maintain model performance while reducing computational requirements.
- Evidence anchors:
  - [section 2.3] "We used QLoRA (Hu et al., 2021; Dettmers et al., 2023) for efficient fine-tuning with 4bit quantization, with Hugging Face Transformers."
  - [section 2.4] "For inference (translation), we converted both the baseline and our fine-tuned models of Mistral to the CTranslate2 format (with 8int quantization) for more efficiency."
- Break condition: If quantization artifacts significantly degrade translation quality, or if LoRA adaptation is insufficient for the target domain.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: The paper's core contribution is enhancing Mistral's in-context learning ability for adaptive MT, which requires understanding how LLMs can adapt to new contexts without explicit fine-tuning at inference time.
  - Quick check question: What is the difference between zero-shot and one-shot translation in the context of LLMs?

- Concept: Fine-tuning vs. in-context learning
  - Why needed here: The paper investigates how fine-tuning can improve in-context learning, which requires understanding the distinction between these two approaches and how they interact.
  - Quick check question: How does fine-tuning a model differ from using its in-context learning capabilities at inference time?

- Concept: Retrieval-augmented generation
  - Why needed here: The paper uses semantic search to retrieve fuzzy matches for one-shot prompts, which is a form of retrieval-augmented generation that requires understanding how to effectively combine retrieved information with LLM outputs.
  - Quick check question: How does the retrieval process for fuzzy matches work in this paper, and why is it important for one-shot translation?

## Architecture Onboarding

- Component map: Mistral 7B base model -> QLoRA fine-tuning with LoRA adapters -> Semantic search (SentenceTransformers + Faiss) -> CTranslate2 conversion -> Inference pipeline with prompt formatting -> Evaluation metrics (BLEU, chrF++, TER, COMET)

- Critical path: 1. Data preparation and prompt generation 2. Model fine-tuning with QLoRA 3. Retrieval of fuzzy matches for one-shot prompts 4. Inference with both zero-shot and one-shot prompts 5. Evaluation and comparison with baselines

- Design tradeoffs:
  - Small dataset vs. comprehensive domain coverage
  - Efficient fine-tuning (QLoRA) vs. full fine-tuning
  - Generic model vs. domain-specific fine-tuning
  - One-shot adaptation vs. zero-shot generalization

- Failure signatures:
  - Degradation in zero-shot performance after fine-tuning
  - Inability to effectively utilize retrieved fuzzy matches
  - Overfitting to the small fine-tuning dataset
  - Computational inefficiency despite QLoRA optimization

- First 3 experiments:
  1. Fine-tune Mistral 7B on zero-shot prompts only and evaluate zero-shot performance
  2. Fine-tune Mistral 7B on one-shot prompts only and evaluate one-shot performance
  3. Fine-tune Mistral 7B on a balanced mix of zero-shot and one-shot prompts and compare to the previous experiments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the size of the fine-tuning dataset affect the quality of the fine-tuned model for adaptive MT?
- Basis in paper: [inferred] The paper mentions that they used a relatively small dataset of 20,000 segments for fine-tuning and achieved significant improvements. However, they also state that they would like to increase the size of the training data in the future to compare performance.
- Why unresolved: The paper does not provide any experiments or analysis on how the size of the fine-tuning dataset impacts the quality of the fine-tuned model. It is unclear whether the current results are due to the small dataset size or if further improvements could be achieved with more data.
- What evidence would resolve it: Experiments comparing the performance of the fine-tuned model using different dataset sizes would provide insights into the impact of dataset size on model quality.

### Open Question 2
- Question: How does the choice of the retrieval model and re-ranking process affect the quality of the retrieved fuzzy matches and, consequently, the adaptive MT performance?
- Basis in paper: [explicit] The paper describes the use of Sentence-Transformers and Faiss for indexing and retrieval of fuzzy matches, but mentions that they did not apply a re-ranking step based on a cross-encoder. It also suggests that this dual process could improve both efficiency and quality of the retrieved documents.
- Why unresolved: The paper does not explore the impact of different retrieval models or re-ranking processes on the quality of the retrieved fuzzy matches. It is unclear whether the current retrieval and indexing approach is optimal for the adaptive MT task.
- What evidence would resolve it: Experiments comparing the performance of the adaptive MT system using different retrieval models and re-ranking processes would provide insights into the impact of these choices on the quality of the retrieved fuzzy matches and the overall system performance.

### Open Question 3
- Question: How does the fine-tuning process affect the efficiency of the model in terms of inference time and resource usage?
- Basis in paper: [explicit] The paper mentions that Mistral 7B is much more efficient than ChatGPT, especially when used with CTranslate2. However, it does not provide any specific analysis or experiments on how the fine-tuning process affects the efficiency of the model.
- Why unresolved: The paper focuses on the quality improvements achieved through fine-tuning but does not explore the impact of fine-tuning on the efficiency of the model. It is unclear whether the fine-tuning process introduces any overhead or changes in resource usage during inference.
- What evidence would resolve it: Experiments measuring the inference time and resource usage of the baseline and fine-tuned models would provide insights into the efficiency impact of the fine-tuning process.

## Limitations
- The small dataset size (20,000 segments) raises concerns about generalizability to other domains or language pairs beyond Spanish-to-English medical translation.
- Comparison with commercial models and task-oriented models is limited to specific metrics and may not capture qualitative differences in translation quality or domain-specific terminology accuracy.
- The retrieval mechanism for one-shot prompts relies on fuzzy matches, but the quality and relevance of these matches to the input context are not thoroughly evaluated.

## Confidence

- High confidence: The experimental results demonstrating improved zero-shot and one-shot translation quality compared to the baseline Mistral 7B model.
- Medium confidence: The claim that fine-tuned Mistral outperforms ChatGPT in zero-shot translation and matches NLLB 3.3B's performance.
- Low confidence: The assertion that fine-tuning with a small dataset (20,000 segments) is sufficient for achieving competitive performance.

## Next Checks

1. **Dataset Size Ablation Study**: Evaluate the performance of the fine-tuned model on progressively larger subsets of the training data (e.g., 5,000, 10,000, 15,000 segments) to determine the optimal dataset size and assess whether 20,000 segments is indeed sufficient.

2. **Cross-Domain Evaluation**: Test the fine-tuned model on medical translation tasks in different languages (e.g., French-to-English, German-to-English) and non-medical domains (e.g., legal, technical) to assess the generalizability of the approach.

3. **Retrieval Quality Analysis**: Conduct a detailed analysis of the fuzzy match retrieval process, including precision, recall, and relevance scores, to ensure that the one-shot prompts are effectively leveraging the retrieved context.