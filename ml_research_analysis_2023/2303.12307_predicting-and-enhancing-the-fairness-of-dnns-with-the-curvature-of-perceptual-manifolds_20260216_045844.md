---
ver: rpa2
title: Predicting and Enhancing the Fairness of DNNs with the Curvature of Perceptual
  Manifolds
arxiv_id: '2303.12307'
source_url: https://arxiv.org/abs/2303.12307
tags:
- perceptual
- curvature
- manifold
- manifolds
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates model bias in deep neural networks through\
  \ a geometric analysis of perceptual manifolds in feature space. The authors systematically\
  \ propose geometric measurements for perceptual manifolds and explore how their\
  \ properties\u2014such as separation and curvature\u2014affect classification difficulty\
  \ and model bias."
---

# Predicting and Enhancing the Fairness of DNNs with the Curvature of Perceptual Manifolds

## Quick Facts
- arXiv ID: 2303.12307
- Source URL: https://arxiv.org/abs/2303.12307
- Reference count: 40
- This paper proposes curvature regularization (CR) to reduce model bias in deep neural networks by balancing the curvature of perceptual manifolds in feature space.

## Executive Summary
This paper investigates model bias in deep neural networks through a geometric analysis of perceptual manifolds in feature space. The authors systematically propose geometric measurements for perceptual manifolds and explore how their properties—such as separation and curvature—affect classification difficulty and model bias. An unexpected finding is that during training, the correlation between class accuracy and separation degree of perceptual manifolds decreases, while the negative correlation with curvature increases, indicating that curvature imbalance leads to model bias. To address this, the authors propose curvature regularization (CR), which encourages flatter and curvature-balanced feature manifolds, reducing model bias and improving overall performance. Evaluations on long-tailed and non-long-tailed datasets show significant performance improvements, especially for state-of-the-art methods, and demonstrate the approach's generality across backbone networks. The method also reduces model bias by improving tail class performance without sacrificing head class accuracy.

## Method Summary
The paper introduces a curvature regularization method that reduces model bias by balancing the curvature of perceptual manifolds in feature space. During training, perceptual manifolds are constructed from feature embeddings, and their geometric properties (separation and curvature) are computed. The curvature regularization term is added to the loss function, penalizing high-curvature manifolds more strongly. This encourages flatter, more balanced manifolds across all classes, reducing bias and improving classification performance, particularly for tail classes in long-tailed datasets.

## Key Results
- Curvature regularization (CR) significantly improves tail class performance in long-tailed datasets without sacrificing head class accuracy.
- CR reduces model bias as measured by variance of accuracy across classes.
- The method demonstrates generality across multiple backbone networks and state-of-the-art methods.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Curvature imbalance among perceptual manifolds directly causes model bias, even in balanced datasets.
- Mechanism: The paper shows that during training, the negative correlation between class accuracy and curvature of perceptual manifolds increases, while the correlation with separation decreases. This indicates that perceptual manifolds with higher curvature are harder to classify, leading to bias.
- Core assumption: Perceptual manifolds with higher curvature are inherently harder to decode by the classifier network.
- Evidence anchors:
  - [abstract] "An unanticipated finding is that the correlation between the class accuracy and the separation degree of perceptual manifolds gradually decreases during training, while the negative correlation with the curvature gradually increases, implying that curvature imbalance leads to model bias."
  - [section 4.3] "The correlation between curvature and accuracy increases with training."
- Break condition: If curvature regularization does not reduce model bias variance, the mechanism is invalid.

### Mechanism 2
- Claim: Learning naturally reduces both separation and curvature of perceptual manifolds, but not equally across classes.
- Mechanism: Training separates perceptual manifolds (increasing separation) and flattens them (reducing curvature). However, curvature reduction is uneven across classes, leading to residual bias that separation alone cannot fix.
- Core assumption: Training dynamics inherently flatten manifolds but do not balance curvature across classes.
- Evidence anchors:
  - [section 4.1] "The correlation between separation degree and class accuracy decreases with training."
  - [section 4.2] "The curvature of the perceptual manifold decreases faster in the early stage of training, and it gradually becomes flat with further training."
- Break condition: If curvature regularization causes worse separation or fails to improve tail class performance, the mechanism is invalid.

### Mechanism 3
- Claim: Curvature regularization (CR) effectively balances curvature across classes without harming separation.
- Mechanism: CR penalizes high-curvature manifolds more strongly, driving all curvatures toward the minimum value. This balances the manifolds and improves overall classification fairness.
- Core assumption: Balancing curvature improves classifier performance across all classes without degrading separation.
- Evidence anchors:
  - [section 5.2] "The greater the curvature of a perceptual manifold, the stronger the penalty for it... When the curvature is balanced, the penalty strength is the same for each perceptual manifold."
  - [section 6] "Evaluations on multiple long-tailed and non-long-tailed datasets show the excellent performance and exciting generality of our approach."
- Break condition: If CR causes a drop in head class accuracy or fails to reduce bias variance, the mechanism is invalid.

## Foundational Learning

- Concept: Geometric analysis of data manifolds
  - Why needed here: The paper relies on understanding how data is distributed in high-dimensional feature space as low-dimensional manifolds. This is foundational to interpreting separation and curvature measurements.
  - Quick check question: What is a perceptual manifold in the context of deep learning?

- Concept: Covariance matrix and its geometric interpretation
  - Why needed here: Volume and separation measures are computed from covariance matrices of feature embeddings. Understanding eigenvalues and eigenvectors is essential.
  - Quick check question: How does the determinant of a covariance matrix relate to the volume of a point cloud?

- Concept: Gaussian curvature estimation on discrete point clouds
  - Why needed here: Curvature regularization depends on accurately estimating curvature at each point of a manifold using neighboring points and quadratic surface fitting.
  - Quick check question: Why is the smallest non-zero eigenvalue of the local covariance matrix used to estimate the normal vector?

## Architecture Onboarding

- Component map: Feature extractor (CNN backbone) → Feature embeddings → Perceptual manifold construction → Curvature & separation computation → Regularization term → Loss function → Classifier → Accuracy evaluation
- Critical path: Feature extraction → Manifold property computation → Curvature regularization → Training loop update
- Design tradeoffs:
  - Computational cost of real-time curvature computation vs. storage of historical features in FIFO pool
  - Balancing original loss vs. curvature regularization strength (τ hyperparameter)
  - Choice of neighbor count k for curvature estimation (affects accuracy vs. speed)
- Failure signatures:
  - If variance of class accuracies does not decrease after CR, curvature estimation may be inaccurate
  - If training diverges, τ may be too large or curvature penalty too aggressive
  - If no improvement on tail classes, curvature may not be the dominant bias factor
- First 3 experiments:
  1. Implement curvature computation on a simple 2D synthetic dataset and visualize manifold flattening during training
  2. Add curvature regularization to a baseline CE loss on CIFAR-10 and measure bias variance reduction
  3. Test τ hyperparameter sweep on CIFAR-10-LT to find optimal balance between original loss and CR

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the geometric characteristics of perceptual manifolds affect the performance of tail classes in long-tailed classification beyond curvature?
- Basis in paper: [explicit] The paper discusses separation degree and curvature as geometric characteristics affecting classification difficulty and model bias, but acknowledges other factors may exist.
- Why unresolved: The study primarily focuses on curvature and separation degree, leaving open the question of how other geometric properties of perceptual manifolds might influence tail class performance.
- What evidence would resolve it: Conducting experiments to analyze the impact of additional geometric properties (e.g., volume, shape, density) of perceptual manifolds on tail class performance and model bias.

### Open Question 2
- Question: How does the proposed curvature regularization method perform on other imbalanced learning tasks beyond long-tailed classification, such as semantic segmentation or object detection?
- Basis in paper: [explicit] The paper mentions the potential application of the geometric perspective to object detection, highlighting the challenge of non-fixed dimensionality of instances.
- Why unresolved: The paper focuses on long-tailed classification, leaving open the question of how curvature regularization generalizes to other imbalanced learning tasks with different data structures and complexities.
- What evidence would resolve it: Extending the curvature regularization method to semantic segmentation and object detection tasks and evaluating its performance on imbalanced datasets in these domains.

### Open Question 3
- Question: What is the relationship between the geometric characteristics of perceptual manifolds and the interpretability of deep neural networks?
- Basis in paper: [inferred] The paper explores the geometric properties of perceptual manifolds, which could provide insights into the decision-making process of deep neural networks.
- Why unresolved: While the paper discusses the impact of geometric characteristics on classification difficulty, it does not explicitly address the interpretability aspect of deep neural networks.
- What evidence would resolve it: Investigating how the geometric properties of perceptual manifolds relate to the decision boundaries and feature representations learned by deep neural networks, potentially providing insights into their interpretability.

## Limitations
- The geometric assumptions about perceptual manifolds may not hold for all network architectures or data distributions
- The causal relationship between curvature imbalance and model bias needs further validation through ablation studies
- The computational overhead of curvature estimation may limit scalability to larger models/datasets

## Confidence

- **High confidence**: The empirical results showing CR's effectiveness across multiple datasets and architectures
- **Medium confidence**: The theoretical connection between curvature imbalance and model bias
- **Low confidence**: The generalizability of findings to domains outside image classification

## Next Checks

1. Conduct ablation studies to isolate the effect of curvature regularization from other training factors
2. Test CR on additional backbone architectures (e.g., Vision Transformers) to verify generality
3. Evaluate computational overhead and scalability to larger models/datasets like Swin-Large on ImageNet-22K