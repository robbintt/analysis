---
ver: rpa2
title: Stable Differentiable Causal Discovery
arxiv_id: '2311.10263'
source_url: https://arxiv.org/abs/2311.10263
tags:
- sdcd
- constraint
- causal
- stage
- variables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of differentiable causal discovery
  (DCD) in high dimensions. While DCD reformulates the problem as a continuous optimization
  over adjacency matrices, existing methods are numerically unstable and fail to scale
  beyond tens of variables.
---

# Stable Differentiable Causal Discovery

## Quick Facts
- arXiv ID: 2311.10263
- Source URL: https://arxiv.org/abs/2311.10263
- Authors: [Not specified in source]
- Reference count: 40
- Key outcome: Introduces SDCD, a stable differentiable causal discovery method that scales to thousands of variables using a spectral acyclicity constraint and two-stage optimization

## Executive Summary
Differentiable causal discovery methods face numerical instability when optimizing acyclicity constraints in high dimensions. Existing methods using polynomial spectral constraints either explode or vanish during training, preventing convergence beyond tens of variables. This paper proposes SDCD, which introduces a stable spectral acyclicity constraint based on the spectral radius and employs a two-stage optimization procedure that first prunes edges before performing causal discovery on the reduced graph.

The method demonstrates superior performance across observational and interventional datasets, achieving lower structural Hamming distances while scaling efficiently to 1000 variables. By addressing the fundamental stability issues in differentiable causal discovery, SDCD provides a practical solution for learning causal structures in high-dimensional settings where previous methods fail.

## Method Summary
SDCD reformulates causal discovery as a continuous optimization problem over adjacency matrices using differentiable acyclicity constraints. The method employs a spectral radius-based constraint (â„ğœŒ) that measures the largest eigenvalue magnitude of the adjacency matrix, providing numerical stability where traditional polynomial spectral transformations fail. SDCD uses a two-stage approach: first performing edge preselection without acyclicity constraints to prune unlikely causal edges, then applying the stable spectral constraint to the reduced graph. Neural networks parameterize conditional distributions, and gradient-based optimization minimizes a composite objective including L1/L2 regularization and the acyclicity penalty.

## Key Results
- SDCD achieves lower structural Hamming distances than existing DCD methods across all tested scenarios
- The method scales to 1000 variables while maintaining stability and accuracy
- Two-stage optimization provides faster convergence and better performance than single-stage alternatives
- â„ğœŒ constraint maintains numerical stability throughout training, unlike exponential or logarithmic PST constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The spectral acyclicity constraint (â„ğœŒ) prevents numerical instability during optimization by avoiding the exponential or vanishing behavior seen in traditional PST constraints.
- Mechanism: â„ğœŒ measures the spectral radius (largest eigenvalue magnitude) of the adjacency matrix. Since eigenvalues are bounded, â„ğœŒ remains stable even as the number of variables ğ‘‘ grows.
- Core assumption: The largest eigenvalue magnitude of a cyclic graph is nonzero, and for acyclic graphs it is zero.
- Evidence anchors:
  - [abstract] "They propose a stable spectral acyclicity constraint based on the spectral radius, which is both theoretically and empirically robust."
  - [section 3.3] "We propose to use another type of constraint, one based on the spectrum of ğ´... â„ğœŒ is stable."
- Break condition: If the graph contains nodes with very strong feedback loops, the spectral radius could become large enough to challenge numerical precision.

### Mechanism 2
- Claim: The two-stage optimization in SDCD (edge preselection followed by DCD) dramatically improves scalability and accuracy.
- Mechanism: Stage 1 prunes edges unlikely to be causal using a non-acyclic regression task. This reduces the number of variables the second stage must handle, making the spectral constraint optimization more efficient and accurate.
- Core assumption: Real-world causal graphs are sparse, so most edges can be safely removed without harming accuracy.
- Evidence anchors:
  - [abstract] "It uses a training procedure tailored for sparse causal graphs... first pre-selects edges via feature selection..."
  - [section 4.1] "stage 1 is likely to remove many false edges and facilitate stage 2."
- Break condition: If the true causal graph is dense or if Stage 1 threshold is set too aggressively, true edges may be removed.

### Mechanism 3
- Claim: The stability of â„ğœŒ allows the use of simple penalty-based optimization instead of more complex methods like augmented Lagrangian.
- Mechanism: â„ğœŒ does not vanish or explode during training, so a straightforward penalty term â„ğœŒ(ğ´ğœƒ) in the objective suffices to guide the solution toward acyclicity without requiring delicate tuning of multipliers.
- Core assumption: â„ğœŒ is both E-stable (bounded growth) and V-stable (does not vanish rapidly), as proven in Theorem 4.
- Evidence anchors:
  - [section 3.2] "E-stable â„(ğ‘  ğ´) = ğ‘‚ğ‘ â†’âˆ(ğ‘ ) ... V-stable â„(ğ´) â‰  0 â‡’ â„(ğœ€ ğ´) = Î© ğœ€â†’0+ (ğœ€)"
  - [section 5.2] "The computation of â„ğœŒ can be done in ğ‘‚ (ğ‘‘2) time... contrary to the PST constraints whose computations scale in ğ‘‚ (ğ‘‘3)."
- Break condition: If the graph contains extremely large cycles, the spectral radius may become large enough to destabilize the optimization.

## Foundational Learning

- Concept: Directed Acyclic Graph (DAG)
  - Why needed here: Causal discovery seeks to learn a DAG that encodes the true cause-effect relationships among variables.
  - Quick check question: In a DAG, can any node be its own ancestor?

- Concept: Spectral Radius
  - Why needed here: The spectral radius is used as a differentiable proxy for acyclicity, replacing unstable PST constraints.
  - Quick check question: What is the spectral radius of a DAG adjacency matrix?

- Concept: Markov Boundary
  - Why needed here: Stage 1 of SDCD identifies Markov boundaries to avoid removing true causal parents.
  - Quick check question: In a causal graph, what variables are included in the Markov boundary of a node?

## Architecture Onboarding

- Component map:
  - Data â†’ Model (neural net for conditional distributions) â†’ Adjacency Matrix (ğ´ğœƒ) â†’ Spectral Radius (â„ğœŒ) â†’ Objective (L1+L2+Penalty) â†’ Optimizer (Adam)
  - Stage 1: No acyclicity constraint, only edge pruning
  - Stage 2: Acyclicity constraint enforced, restricted edge set

- Critical path:
  1. Forward pass to compute predictions and log-likelihood
  2. Compute â„ğœŒ via power iteration on ğ´ğœƒ
  3. Compute gradient of loss (including â„ğœŒ penalty)
  4. Adam update step

- Design tradeoffs:
  - â„ğœŒ vs PST: â„ğœŒ is more stable but requires eigenvalue estimation; PST is faster per iteration but can explode/vanish
  - Two-stage vs single-stage: Two-stage is more robust and faster on sparse graphs but adds complexity
  - Neural net depth: Deeper nets can model complex conditionals but increase training time and overfitting risk

- Failure signatures:
  - NaNs in adjacency matrix â†’ â„ğœŒ underflows/overflows or initialization is too extreme
  - Convergence to cyclic graph â†’ ğ›¾ increment too small or stage 1 threshold too low
  - Very dense predicted graph â†’ L1 regularization too weak or ğ›¼ too small

- First 3 experiments:
  1. Run SDCD on a small synthetic dataset (ğ‘‘=10, ğ‘ =2) with observational data; verify SHD < 10
  2. Increase ğ‘‘ to 50 and compare training time and SHD vs DCDI baseline
  3. Test interventional variant: simulate interventions on 25% of nodes, compare precision/recall

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the spectral acyclicity constraint (â„ğœŒ) perform on causal graphs with a high proportion of directed cycles or near-cycles, where traditional PST constraints might still be applicable?
- Basis in paper: [inferred] The paper primarily compares â„ğœŒ to PST constraints (â„exp, â„log, â„inv) on acyclic graphs, but does not explore its behavior on graphs with cycles or near-cycles.
- Why unresolved: The paper focuses on the theoretical and empirical stability of â„ğœŒ for acyclic graph discovery, leaving its performance on cyclic or near-cyclic graphs unexplored.
- What evidence would resolve it: Experiments comparing â„ğœŒ to PST constraints on datasets containing graphs with varying degrees of cyclicity, measuring accuracy, convergence speed, and stability.

### Open Question 2
- Question: What is the impact of the choice of neural network architecture (e.g., depth, width, activation functions) on the performance of SDCD, and how does it compare to other model classes like Gaussian processes or kernel methods?
- Basis in paper: [explicit] The paper mentions using neural networks to parameterize the conditional distributions but does not explore the impact of different architectures or compare to other model classes.
- Why unresolved: The paper assumes a specific neural network architecture without investigating its influence on SDCD's performance or exploring alternative model classes.
- What evidence would resolve it: Ablation studies varying the neural network architecture and comparing SDCD's performance to other model classes on the same datasets, measuring accuracy, convergence speed, and scalability.

### Open Question 3
- Question: How does SDCD's performance scale with the dimensionality of the variables (ğ‘‘) and the number of interventions (ğ¾), and what are the practical limitations in terms of computational resources and data requirements?
- Basis in paper: [explicit] The paper demonstrates SDCD's scalability to thousands of variables but does not explicitly explore the relationship between ğ‘‘, ğ¾, and performance, nor does it discuss practical limitations.
- Why unresolved: The paper focuses on SDCD's scalability but does not provide a detailed analysis of its performance as a function of ğ‘‘ and ğ¾, nor does it discuss practical limitations in terms of computational resources and data requirements.
- What evidence would resolve it: Experiments varying ğ‘‘ and ğ¾ while measuring SDCD's performance, computational time, and memory usage, along with a discussion of the practical limitations and potential solutions.

## Limitations
- The method assumes sparse causal graphs, which may not hold in domains with dense connections like gene regulation
- Two-stage approach can fail when Stage 1 removes true causal edges, especially with aggressive pruning thresholds
- Performance on real-world high-dimensional data with complex conditional distributions remains untested despite synthetic success

## Confidence
- **High confidence**: The numerical stability improvements from â„ğœŒ over traditional PST constraints are well-supported by both theoretical proofs (E-stability and V-stability in Theorem 4) and empirical convergence speed comparisons
- **Medium confidence**: The claim that SDCD outperforms existing methods across diverse datasets is supported by SHD comparisons, but synthetic data generation differs from real-world complexity
- **Low confidence**: The paper's assertion that â„ğœŒ enables simpler penalty-based optimization (vs. augmented Lagrangian) is plausible but lacks direct comparative experiments

## Next Checks
1. **Ablation on Stage 1 Threshold**: Systematically vary the stage 1 pruning threshold and measure the trade-off between computational efficiency and accuracy on graphs with different densities (s=2, s=5, s=10) to identify break points where true edges are lost

2. **Real-World Data Transfer**: Apply SDCD to a high-dimensional real dataset (e.g., gene expression or brain connectivity) and compare performance against synthetic benchmarks, focusing on whether the sparsity assumption holds and how â„ğœŒ behaves with empirically complex conditionals

3. **Cycle Robustness Test**: Construct synthetic graphs with known feedback loops of varying strength and measure whether â„ğœŒ maintains stability or whether the spectral radius becomes numerically problematic, potentially requiring modified initialization or constraint formulations