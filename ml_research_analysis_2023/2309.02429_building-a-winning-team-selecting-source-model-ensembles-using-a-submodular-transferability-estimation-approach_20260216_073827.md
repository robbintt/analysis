---
ver: rpa2
title: 'Building a Winning Team: Selecting Source Model Ensembles using a Submodular
  Transferability Estimation Approach'
arxiv_id: '2309.02429'
source_url: https://arxiv.org/abs/2309.02429
tags:
- source
- ensemble
- target
- transferability
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of selecting an optimal ensemble
  of pre-trained models for a target dataset in transfer learning. The authors propose
  OSBORN, a novel transferability estimation metric that accounts for domain difference,
  task difference, and inter-model cohesiveness.
---

# Building a Winning Team: Selecting Source Model Ensembles using a Submodular Transferability Estimation Approach

## Quick Facts
- **arXiv ID**: 2309.02429
- **Source URL**: https://arxiv.org/abs/2309.02429
- **Reference count**: 40
- **Key outcome**: OSBORN outperforms state-of-the-art transferability metrics by 20-96% on correlation metrics across image classification, semantic segmentation, and domain adaptation tasks.

## Executive Summary
This paper addresses the challenge of selecting optimal ensembles of pre-trained models for transfer learning tasks. The authors propose OSBORN, a novel transferability estimation metric that combines domain difference, task difference, and inter-model cohesiveness. By leveraging submodularity theory, OSBORN enables efficient greedy selection of model ensembles that consistently outperform existing methods. Extensive experiments across multiple tasks and datasets demonstrate significant improvements in correlation with actual fine-tuned accuracy compared to baseline metrics.

## Method Summary
OSBORN estimates transferability by combining three components: Wasserstein distance (WD) for domain mismatch, conditional entropy-based task difference (WT), and conditional entropy-based model cohesiveness (WC). The method treats transferability as a submodular set function, enabling efficient greedy maximization for ensemble selection. The metric is computed by first extracting features from source models, then calculating pairwise distances and entropy measures between source and target datasets. The greedy algorithm iteratively selects models that maximize the gain in the OSBORN score until the desired ensemble size is reached.

## Key Results
- OSBORN achieves 96.36%, 66.06%, and 58.62% improvements in PCC, KT, and WKT correlation metrics respectively over MS-LEEP and E-LEEP baselines
- The method consistently outperforms single-source transferability metrics across image classification, semantic segmentation, and domain adaptation tasks
- Greedy selection using OSBORN reaches near-optimal ensemble performance due to submodularity properties
- OSBORN demonstrates practical efficiency with computation time in seconds even for larger ensembles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OSBORN outperforms single-source transferability metrics by incorporating domain difference, task difference, and inter-model cohesiveness.
- Mechanism: The metric combines three components: WD (Wasserstein distance for domain mismatch), WT (conditional entropy for task mismatch), and WC (conditional entropy for model disagreement). By considering these three factors together, OSBORN provides a more holistic and accurate assessment of transferability for model ensembles.
- Core assumption: The three components (domain, task, and cohesiveness) are independent and can be combined additively to form a reliable transferability estimate.
- Evidence anchors:
  - [abstract] "OSBORN collectively accounts for image domain difference, task difference, and cohesiveness of models in the ensemble to provide reliable estimates of transferability."
  - [section 4] "Our metric collectively accounts for domain difference, task difference and model cohesion."
  - [corpus] Weak evidence; related papers focus on single-source transferability or ensemble robustness but not the specific combination of these three factors.
- Break condition: If the three components are not independent or if their relative importance varies significantly across different target datasets, the additive combination may not be optimal.

### Mechanism 2
- Claim: Viewing OSBORN as a submodular set function allows for efficient selection of model ensembles via greedy maximization.
- Mechanism: The scoring function f(Me) defined in Equation 9 is submodular, satisfying the property of diminishing returns. This allows the use of a greedy algorithm to select an ensemble of size k that achieves at least 63% of the optimal value (Nemhauser's theorem).
- Core assumption: The set function representing transferability is submodular, enabling the use of greedy optimization.
- Evidence anchors:
  - [section 4] "Theorem 4.1. The scoring function f(Me), as defined in Equation 9, is a submodular function."
  - [section 4] "Since our set function f(Me) (mentioned in Eq. 9) is submodular, it exhibits monotonicity..."
  - [corpus] Weak evidence; related papers discuss submodularity in different contexts (e.g., subset selection) but not specifically for transferability estimation.
- Break condition: If the submodularity assumption is violated, the greedy algorithm may not provide a near-optimal solution.

### Mechanism 3
- Claim: The conditional entropy-based measures (WT and WC) effectively capture task difference and model disagreement.
- Mechanism: WT measures the mismatch between model outputs and ground truth labels, while WC measures the disagreement between models in the ensemble. Both are based on conditional entropy, which quantifies the uncertainty or information gain.
- Core assumption: Conditional entropy is a suitable measure for quantifying task difference and model disagreement.
- Evidence anchors:
  - [section 4] "To understand the cohesiveness of an ensemble, we use Conditional Entropy to capture the amount of disagreement between models in the subset of models Me."
  - [section 4] "In order to measure the difference between a source task and the given target task, we use the mismatch between the model/classifier's outputs..."
  - [corpus] Weak evidence; related papers discuss conditional entropy in different contexts but not specifically for transferability estimation or ensemble selection.
- Break condition: If conditional entropy is not an appropriate measure for task difference or model disagreement, the resulting estimates may be inaccurate.

## Foundational Learning

- Concept: Submodularity and greedy optimization
  - Why needed here: To efficiently select model ensembles from a large source pool without exhaustive search.
  - Quick check question: Can you explain why a submodular function allows for efficient greedy optimization and what the theoretical guarantee is?

- Concept: Optimal transport and Wasserstein distance
  - Why needed here: To measure the domain difference between source and target datasets in the latent feature space.
  - Quick check question: How does the Wasserstein distance capture the geometry of the underlying data distribution, and why is it preferred over other distance measures?

- Concept: Conditional entropy
  - Why needed here: To quantify task difference and model disagreement.
  - Quick check question: What does conditional entropy measure, and how does it relate to uncertainty or information gain?

## Architecture Onboarding

- Component map: Target dataset -> Source models -> Feature extraction -> WD/WT/WC computation -> Submodular optimization -> Ensemble selection
- Critical path:
  1. Precompute pairwise WD and WT between source and target datasets
  2. Initialize an empty ensemble set
  3. Iteratively add the model that maximizes the gain in OSBORN score until the desired ensemble size is reached
- Design tradeoffs:
  - Computational cost vs. accuracy: Using a stratified representative set for optimal transport computation reduces cost but may introduce some approximation error
  - Model diversity vs. cohesiveness: Selecting models that are both diverse (to cover different aspects of the target task) and cohesive (to reinforce each other's predictions) is crucial for good ensemble performance
- Failure signatures:
  - Poor correlation between OSBORN scores and fine-tuned ensemble accuracy
  - Ensembles selected by OSBORN perform worse than random ensembles or ensembles selected by simpler metrics
  - High computational cost or memory usage during optimal transport computation
- First 3 experiments:
  1. Reproduce the correlation results on a subset of the classification datasets (e.g., Caltech101, Oxford102Flowers) and compare OSBORN with MS-LEEP and E-LEEP
  2. Study the impact of different weighting schemes for the three components of OSBORN on a specific target dataset (e.g., Caltech101)
  3. Investigate the scalability of OSBORN by increasing the number of source models and target datasets and measuring the computational cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal weighting strategy for the three components (WD, WT, WC) of OSBORN across different target datasets and tasks?
- Basis in paper: [inferred] The authors mention that while simple addition of the three quantities outperforms existing methods, they observed that these can be fine-tuned through grid search over a larger range of values to get even better transferability estimates. They also note that this varies with the target dataset, as seen in their study of Caltech101 where giving more weightage to WD compared to WT and WC achieved higher correlation scores.
- Why unresolved: The paper does not provide a systematic approach to determine optimal weights for the components of OSBORN across different datasets and tasks. The authors suggest this as an interesting direction for future work but do not explore it further.
- What evidence would resolve it: A comprehensive study that tests OSBORN with different weighting strategies across a diverse set of target datasets and tasks, identifying patterns or rules for optimal weight selection based on dataset characteristics or task types.

### Open Question 2
- Question: How does OSBORN perform when applied to other tasks beyond image classification, semantic segmentation, and domain adaptation?
- Basis in paper: [explicit] The authors state that future directions include studying the applicability of OSBORN to other tasks and problem settings. They also mention conducting additional experiments on tasks like multi-domain/domain adaptation and semantic segmentation in the appendix.
- Why unresolved: The paper focuses primarily on three computer vision tasks (image classification, semantic segmentation, and domain adaptation). While the authors mention potential for broader applicability, they do not provide experimental results or theoretical justification for extending OSBORN to other domains or tasks.
- What evidence would resolve it: Experimental results applying OSBORN to a diverse set of tasks beyond computer vision, such as natural language processing, speech recognition, or graph-based problems. Additionally, theoretical analysis demonstrating how OSBORN's components (domain difference, task difference, and model cohesiveness) can be adapted to other task types.

### Open Question 3
- Question: What is the computational complexity of OSBORN, and how does it scale with the number of source models and target dataset size?
- Basis in paper: [inferred] The authors mention that OSBORN is practical and relevant because the time taken for model selection is still in the order of seconds, even as the ensemble size increases. They also state that the quantities in their metric can be computed independently for each source model after forward-propagating target samples, making computations parallelizable.
- Why unresolved: While the authors provide some insight into the computational efficiency of OSBORN, they do not provide a detailed analysis of its computational complexity. The paper does not discuss how the method scales with increasing numbers of source models or larger target datasets, which is crucial for understanding its practical limitations and potential bottlenecks.
- What evidence would resolve it: A formal analysis of OSBORN's time and space complexity, including how it scales with the number of source models, target dataset size, and ensemble size. Empirical studies showing the computational requirements for different problem sizes, possibly including comparisons with other transferability estimation methods in terms of computational efficiency.

## Limitations
- The theoretical foundation for combining the three components additively is not rigorously established
- Submodularity proof is stated but not fully detailed, making verification difficult
- Computational complexity of optimal transport may become prohibitive for very large datasets or model ensembles

## Confidence
- **High confidence**: Empirical correlation results showing OSBORN outperforming MS-LEEP and E-LEEP on multiple datasets and tasks
- **Medium confidence**: Submodularity assumption enabling greedy optimization and the specific formulation of the three components
- **Low confidence**: Theoretical justification for additive combination of components and generalizability to domains beyond computer vision

## Next Checks
1. **Ablation study on component importance**: Systematically vary the weights of WD, WT, and WC components on a held-out dataset to determine if the additive combination is optimal or if different tasks require different weightings.

2. **Scalability analysis**: Measure wall-clock time and memory usage of OSBORN as the number of source models increases from 10 to 100+ models, and compare against alternative metrics to identify practical bottlenecks.

3. **Cross-domain transferability**: Apply OSBORN to a non-image domain (e.g., natural language processing or tabular data) to test whether the metric generalizes beyond computer vision tasks, particularly focusing on whether conditional entropy remains an effective measure for task difference and model cohesiveness in different data modalities.