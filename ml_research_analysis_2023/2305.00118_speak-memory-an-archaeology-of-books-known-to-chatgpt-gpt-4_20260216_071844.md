---
ver: rpa2
title: 'Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4'
arxiv_id: '2305.00118'
source_url: https://arxiv.org/abs/2305.00118
tags:
- books
- data
- gpt-4
- language
- name
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the memorization of books in ChatGPT and
  GPT-4 models. The authors use a name cloze task, where they mask a character's name
  in a passage and ask the model to predict it, to assess memorization.
---

# Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4

## Quick Facts
- arXiv ID: 2305.00118
- Source URL: https://arxiv.org/abs/2305.00118
- Reference count: 40
- Key finding: ChatGPT and GPT-4 have memorized extensive collections of copyrighted books, with memorization strength correlating to web frequency.

## Executive Summary
This paper investigates book memorization in ChatGPT and GPT-4 through a name cloze task where character names are masked in passages. The authors find that both models have memorized a wide collection of copyrighted books, with strong preferences for science fiction/fantasy and bestsellers. Memorization is tied to web frequency, creating disparities in downstream task performance where memorized books yield significantly better results than non-memorized ones. The study highlights how this memorization threatens the validity of cultural analytics research by contaminating evaluation benchmarks.

## Method Summary
The authors construct a name cloze task using 571 works of fiction published between 1749-2020, including public domain texts, Pulitzer nominees, bestsellers, and genre fiction. Using the BookNLP pipeline, they extract passages and mask single character names. They prompt GPT-4 and ChatGPT to predict the masked names, comparing predictions to true names to assess memorization. They also measure web frequency using Google, Bing, C4, and Pile datasets, and evaluate downstream task performance on predicting publication years and narrative time.

## Key Results
- GPT-4 achieves 98% accuracy on name cloze for "Alice's Adventures in Wonderland" while BERT achieves near 0% for most books
- Strong correlation (ρ=0.74-0.84) between GPT-4 name cloze accuracy and web search results for public domain works
- Downstream task performance shows stark disparities: 0.6 year MAE for memorized vs 14.5 years for non-memorized books in GPT-4

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Name cloze task is a valid proxy for exact memorization in LLMs
- Mechanism: Masking character names in passages with no other named entities forces reliance on memorized knowledge
- Core assumption: Models that have seen books during training will have stronger character name associations
- Evidence: GPT-4 98% accuracy on "Alice's Adventures in Wonderland" vs BERT near 0% on most books
- Break condition: If models learn to guess common names based on genre patterns

### Mechanism 2
- Claim: Web presence frequency directly correlates with model memorization
- Mechanism: Books with more web copies appear more often in training data, leading to higher memorization
- Core assumption: Training data includes substantial web-scraped content
- Evidence: Strong correlation (ρ=0.74-0.84) for public domain works between web frequency and name cloze accuracy
- Break condition: If training data sampling is non-uniform or deduplication removes duplicates

### Mechanism 3
- Claim: Memorization creates validity threats for cultural analytics benchmarks
- Mechanism: Better performance on memorized books contaminates benchmark results
- Core assumption: Cultural analytics should measure linguistic features, not encyclopedic knowledge
- Evidence: Year prediction MAE shows 0.6 years for memorized vs 14.5 years for non-memorized books
- Break condition: If tasks explicitly require encyclopedic knowledge

## Foundational Learning

- **Membership inference attack**: Why needed - The study uses name cloze accuracy as a membership inference query to determine which books are in training data. Quick check: If a model achieves 95% accuracy on name cloze for a book, what can we infer about that book's presence in training data?

- **Exact memorization vs. generalization**: Why needed - The paper distinguishes between memorized content versus generalized patterns. Quick check: If BERT achieves near 0% accuracy on name cloze except for one book, what does this suggest about its training data?

- **Data contamination in evaluation**: Why needed - The study shows how memorized books contaminate evaluation benchmarks. Quick check: If a benchmark contains books that were in training data, how might this affect the validity of performance measurements?

## Architecture Onboarding

- **Component map**: BookNLP preprocessing → Name cloze evaluation → Web frequency correlation → Downstream task validation
- **Critical path**: Passage extraction → Name cloze evaluation → Web frequency correlation → Downstream task validation
- **Design tradeoffs**: Open-ended name cloze provides stronger memorization signal but requires more sophisticated prompt engineering
- **Failure signatures**: High name cloze accuracy but poor downstream performance suggests memorization without understanding
- **First 3 experiments**:
  1. Replicate name cloze accuracy on a small subset (10 books) to verify measurement reliability
  2. Test correlation between web frequency and accuracy on pre-1923 vs post-1928 books separately
  3. Run downstream year prediction task on top/bottom decile books by name cloze accuracy

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of GPT-4 and ChatGPT compare to other large language models (LLMs) on the name cloze task?
  - Basis: The paper only compares GPT-4/ChatGPT to BERT, not other LLMs
  - Why unresolved: No comprehensive comparison to other LLMs provided
  - What evidence would resolve it: Additional experiments comparing GPT-4/ChatGPT to other LLMs on the name cloze task

- **Open Question 2**: How does the memorization of books by GPT-4 and ChatGPT impact their performance on other downstream tasks beyond those explored in the paper?
  - Basis: Paper explores only year prediction and narrative time prediction tasks
  - Why unresolved: No comprehensive analysis of impact on other downstream tasks
  - What evidence would resolve it: Additional experiments exploring impact on a wider range of downstream tasks

- **Open Question 3**: How can the biases in book memorization by GPT-4 and ChatGPT be mitigated?
  - Basis: Paper identifies biases including preference for science fiction/fantasy and bestsellers
  - Why unresolved: No solutions proposed for mitigating these biases
  - What evidence would resolve it: Research into methods for mitigating biases in book memorization

## Limitations

- The core claims about memorization are based on indirect inference methods rather than direct access to model weights or training data
- The strong correlation between web frequency and memorization cannot be verified given the models' closed nature
- The corpus selection may not be representative of all books that could appear in training data

## Confidence

- **High confidence**: Name cloze task successfully distinguishes memorized vs non-memorized books (GPT-4 vs BERT accuracy differences)
- **Medium confidence**: Downstream task performance disparities are real but interpretation as validity threat needs external validation
- **Low confidence**: Exact training data composition and frequency-to-memorization causation cannot be verified

## Next Checks

1. Replicate name cloze accuracy with controlled passage selection on 10 pre-1923 public domain works
2. Web frequency manipulation experiment: artificially increase web presence for low-presence books and re-measure accuracy
3. Cross-model generalization test: apply methodology to open-weight models with known training data