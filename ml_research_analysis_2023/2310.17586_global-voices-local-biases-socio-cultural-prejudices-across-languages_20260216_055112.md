---
ver: rpa2
title: 'Global Voices, Local Biases: Socio-Cultural Prejudices across Languages'
arxiv_id: '2310.17586'
source_url: https://arxiv.org/abs/2310.17586
tags:
- weat
- languages
- biases
- bias
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces WEATHub, a multilingual dataset for evaluating
  bias across 24 languages, extending the WEAT test to include culturally relevant
  and human-centered bias dimensions. It compares static and contextualized embeddings,
  human and machine translations, and multilingual vs monolingual models.
---

# Global Voices, Local Biases: Socio-Cultural Prejudices across Languages

## Quick Facts
- arXiv ID: 2310.17586
- Source URL: https://arxiv.org/abs/2310.17586
- Reference count: 40
- Primary result: Human translations yield larger bias effect sizes than machine translations, with monolingual models often detecting stronger biases than multilingual ones across 24 languages.

## Executive Summary
This paper introduces WEATHub, a multilingual dataset for evaluating bias across 24 languages, extending the WEAT test to include culturally relevant and human-centered bias dimensions. The study compares static and contextualized embeddings, human and machine translations, and multilingual vs monolingual models to understand how bias manifests across languages. Results show that human translations capture more nuanced cultural context, leading to more accurate bias detection with larger effect sizes. The research identifies significant biases in ableism, sexuality, education, and immigration, while highlighting that Indian languages show inconsistent bias patterns that require careful interpretation.

## Method Summary
The study employs the Word Embedding Association Test (WEAT) metric, modified to handle different target set lengths, to quantify bias in word embeddings across 24 languages. The WEATHub dataset provides target-attribute pairs for evaluation, using both human translations (for cultural relevance) and machine translations for comparison. Contextualized embeddings are extracted from multiple layers of multilingual models (XLM-RoBERTa, DistilmBERT) and monolingual models where available. The research introduces a bias sensitivity heuristic that measures variance of pairwise cosine distances within word sets to compare different embedding methods. The analysis examines cross-linguistic patterns and compares monolingual versus multilingual model performance.

## Key Results
- Human translations yield larger absolute effect sizes than machine translations for bias detection
- Monolingual models often detect stronger biases than multilingual models across multiple languages
- Significant biases were found in ableism, sexuality, education, and immigration dimensions
- Indian languages showed inconsistent bias patterns requiring careful interpretation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human translations capture nuanced cultural context that machine translations miss, leading to more accurate bias detection.
- Mechanism: Human translators provide language-specific culturally relevant data and correct automatic translations, ensuring that WEAT categories accurately reflect local contexts and stereotypes.
- Core assumption: Human translators have deep cultural knowledge and can identify subtle biases that automated systems cannot detect.
- Evidence anchors:
  - [abstract]: "human translations yield larger effect sizes than machine translations"
  - [section]: "human translations provide a more comprehensive evaluation of bias, yielding larger absolute effect sizes than MT"
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.444, average citations=0.0. Top related titles include bias evaluation studies.
- Break condition: If human translators are not native speakers or lack cultural awareness, translations may miss important nuances.

### Mechanism 2
- Claim: Monolingual models detect language-specific biases better than multilingual models in certain cases.
- Mechanism: Monolingual models are trained on language-specific corpora that may contain culturally aligned biases not present in multilingual training data.
- Core assumption: Language-specific training data contains unique cultural references and stereotypes that influence bias patterns.
- Evidence anchors:
  - [section]: "monolingual models do reflect human biases more closely than multilingual ones across multiple methods"
  - [section]: "Thai-specific monolingual model robustly and statistically significantly contradicted" traditional associations found by multilingual models
  - [corpus]: Related papers like "Cross-Lingual Probing and Community-Grounded Analysis of Gender Bias in Low-Resource Bengali" suggest language-specific bias evaluation is important.
- Break condition: If training data for monolingual models lacks diversity or contains its own biases, results may be skewed.

### Mechanism 3
- Claim: Contextually-aware embeddings from hidden layers capture different bias patterns than static embeddings.
- Mechanism: Different encoding methods (static vs contextualized, different layers, subword averaging) extract different representations that vary in their ability to detect bias.
- Core assumption: Bias manifests differently in static word representations versus context-dependent representations, and within different layers of transformer models.
- Evidence anchors:
  - [section]: "contextualized embeddings often encode biases differently than static representations like FastText"
  - [section]: "embedding layer from BERT, providing static word representations, exhibits the highest sensitivity" for bias detection
  - [corpus]: Related research on fairness in embeddings suggests different encoding methods yield different bias measurements.
- Break condition: If the WEAT metric itself is unreliable for contextualized embeddings, different encoding methods may not yield meaningful differences.

## Foundational Learning

- Concept: Word Embedding Association Test (WEAT)
  - Why needed here: WEAT is the primary metric used to quantify bias in word embeddings across languages
  - Quick check question: What does a negative effect size in WEAT indicate about the association between target and attribute groups?

- Concept: Cultural relevance in bias measurement
  - Why needed here: Bias manifests differently across cultures, so measurements must account for local contexts
  - Quick check question: Why might a bias found in Hindi not necessarily appear in other Indian languages?

- Concept: Static vs contextualized embeddings
  - Why needed here: The paper compares these embedding types to understand how bias detection varies
  - Quick check question: How might static embeddings from the embedding layer differ from contextualized embeddings from hidden layers in bias detection?

## Architecture Onboarding

- Component map: WEATHub dataset → encoding methods (FastText, BERT layers, etc.) → WEAT metric calculation → bias analysis across languages and models
- Critical path: Dataset creation → encoding method selection → bias calculation → cross-linguistic comparison
- Design tradeoffs: Human translation vs machine translation (accuracy vs scalability), monolingual vs multilingual models (cultural specificity vs broader coverage)
- Failure signatures: Inconsistent p-values across languages, minimal effect sizes suggesting insufficient bias, significant differences between encoding methods indicating metric sensitivity issues
- First 3 experiments:
  1. Compare effect sizes for a single WEAT category across all 24 languages using the same encoding method
  2. Test the same language with both human and machine translations to quantify the translation impact
  3. Compare monolingual and multilingual models for a high-resource language to identify systematic differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed human-centered bias dimensions (toxicity, ableism, sexuality, education, immigration) compare in terms of their cultural relevance and validity across different language communities?
- Basis in paper: [explicit] The paper introduces these new dimensions and discusses the process of selecting target and attribute pairs based on insights from intersectional bias research and consultations with native speakers.
- Why unresolved: The paper does not provide a systematic evaluation of the cultural relevance and validity of these new dimensions across the 24 languages. It only mentions discussions with native speakers and selection of cross-culturally applicable words.
- What evidence would resolve it: A study that systematically evaluates the cultural relevance and validity of these dimensions across multiple language communities, potentially through surveys, interviews, or analysis of language-specific corpora.

### Open Question 2
- Question: How does the choice of embedding method (e.g., static vs. contextualized, monolingual vs. multilingual) impact the detection of bias across different languages and bias dimensions?
- Basis in paper: [explicit] The paper extensively compares different embedding methods (FastText, BERT layers, multilingual vs. monolingual models) and finds significant variations in bias detection across languages and dimensions.
- Why unresolved: While the paper identifies patterns and differences, it does not provide a comprehensive framework for understanding the underlying reasons for these variations or for guiding the selection of appropriate embedding methods for specific bias detection tasks.
- What evidence would resolve it: A study that systematically investigates the relationship between embedding method characteristics (e.g., training data, architecture) and their ability to detect bias across different languages and dimensions, potentially through controlled experiments or analysis of model internals.

### Open Question 3
- Question: What are the implications of the observed inconsistencies in bias patterns across languages for the development of fair and equitable language technologies?
- Basis in paper: [explicit] The paper highlights the need for culturally sensitive bias evaluation methods and discusses the limitations of relying solely on machine translations or high-resource language models.
- Why unresolved: The paper does not provide concrete recommendations or guidelines for developing language technologies that account for these inconsistencies and ensure fairness across diverse linguistic communities.
- What evidence would resolve it: A study that explores the practical implications of these inconsistencies for real-world language technology applications and proposes strategies for mitigating bias and ensuring fairness across different languages and cultural contexts.

## Limitations
- The study's reliance on WEAT as the primary bias detection metric may not capture all forms of bias, particularly subtle or intersectional biases
- The WEATHub dataset depends on human judgments for culturally relevant translations, which may introduce subjective biases or inconsistencies
- The analysis focuses on word-level associations rather than contextualized sentence-level biases

## Confidence
- **High Confidence**: Human translations yield larger effect sizes than machine translations is well-supported and aligns with established linguistic research
- **Medium Confidence**: Contextually-aware embeddings capture different bias patterns requires more careful interpretation due to complex relationships
- **Low Confidence**: Specific bias patterns in Indian languages being "inconsistent" is based on limited data and may reflect dataset limitations

## Next Checks
1. Replicate the human vs. machine translation comparison using an independent human translation team to verify effect size differences are reproducible
2. Conduct a sensitivity analysis by systematically varying the number of human translators per language to determine minimum number needed for reliable bias detection
3. Test the WEAT metric's reliability by comparing results across different WEAT category formulations for the same cultural concepts to assess whether the metric itself introduces variability