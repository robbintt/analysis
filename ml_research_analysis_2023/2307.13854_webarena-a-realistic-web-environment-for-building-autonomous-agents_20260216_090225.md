---
ver: rpa2
title: 'WebArena: A Realistic Web Environment for Building Autonomous Agents'
arxiv_id: '2307.13854'
source_url: https://arxiv.org/abs/2307.13854
tags:
- agent
- tasks
- action
- page
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces WebArena, a realistic web environment for
  building autonomous agents. The environment contains fully functional websites from
  four common domains: e-commerce, social forums, collaborative software development,
  and content management.'
---

# WebArena: A Realistic Web Environment for Building Autonomous Agents

## Quick Facts
- arXiv ID: 2307.13854
- Source URL: https://arxiv.org/abs/2307.13854
- Reference count: 40
- Autonomous agents achieve 7.38% (GPT-3.5) and 10.59% (GPT-4) success rates on 812 diverse web tasks

## Executive Summary
WebArena introduces a realistic web environment for evaluating autonomous agents on complex, long-horizon tasks. The environment contains four fully functional websites from common domains (e-commerce, social forums, collaborative development, and content management) deployed in Docker containers with deterministic initial states. A benchmark of 812 diverse tasks with programmatic evaluation functions enables fair assessment of functional correctness rather than just action sequences. Experiments with GPT-3.5 and GPT-4-based agents show limited success rates, highlighting the challenges of autonomous web navigation and the need for improved agent capabilities.

## Method Summary
The WebArena environment provides Docker-based deployment of four self-contained websites (OneStopShop, GitLab, Reddit, CMS) along with supporting tools like maps and external knowledge bases. Agents interact through flexible observation modalities (HTML DOM, screenshots, accessibility trees) and execute actions like click, type, and navigate. The benchmark includes 812 tasks with annotated evaluation programs that programmatically verify task completion. Two GPT-based agent architectures (direct and reasoning) use few-shot in-context learning with GPT-3.5 and GPT-4 models. The minimum viable reproduction involves deploying the environment, loading the benchmark dataset, and running baseline agents with provided configurations.

## Key Results
- GPT-3.5-based agents achieve 7.38% success rate on 812 WebArena tasks
- GPT-4-based agents achieve 10.59% success rate on the same benchmark
- Error analysis reveals early task infeasibility determination and over-reliance on available context as major failure modes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-contained web environment enables reproducible agent evaluation
- Mechanism: Docker-based deployment with deterministic initial state ensures identical conditions across experiments
- Core assumption: Website behavior is fully encapsulated within containers without external dependencies
- Evidence anchors:
  - [abstract] "Our code, data, environment reproduction resources... are publicly available"
  - [section] "We deliver the environment as dockers and provide scripts to reset the environment to a deterministic initial state"
  - [corpus] "Average neighbor FMR=0.509" - weak correlation, no direct evidence of reproducibility
- Break condition: External dependencies (e.g., API rate limits, CAPTCHAs) break isolation

### Mechanism 2
- Claim: Multi-modal observations improve agent performance through flexibility
- Mechanism: Agents can choose between HTML DOM, screenshots, and accessibility trees for input representation
- Core assumption: Different modalities capture complementary information for decision-making
- Evidence anchors:
  - [section] "We provide flexible configuration to render the page content in many modes: (1) the raw web page HTML... (2) the screenshot... (3) the accessibility tree"
  - [section] "Although we provide careful instructions on avoiding common failures, the actions are not flawless... we incorporate these error messages in the observation"
  - [corpus] No direct evidence - weak support
- Break condition: Model architecture cannot handle chosen modality or context length limits are exceeded

### Mechanism 3
- Claim: Functional correctness evaluation enables more meaningful agent assessment
- Mechanism: Programmatic validation of task outcomes rather than surface-level action sequence comparison
- Core assumption: Task completion can be verified by checking concrete results rather than matching reference trajectories
- Evidence anchors:
  - [abstract] "We focus on evaluating the functional correctness of these tasks... does the result of the execution actually achieve the desired goal"
  - [section] "To assess these, we establish reward functions rprog(s) that programmatically examine the intermediate states s"
  - [corpus] No direct evidence - weak support
- Break condition: Evaluation programs cannot capture all valid task completion strategies

## Foundational Learning

- Concept: POMDP formulation of web navigation
  - Why needed here: Web environments are partially observable and require sequential decision-making
  - Quick check question: What information does an agent have at each step and what can it observe?

- Concept: Accessibility tree structure
  - Why needed here: Element selection via IDs requires understanding tree-based representation
  - Quick check question: How do you locate an element's ID in the accessibility tree?

- Concept: Programmatic task validation
  - Why needed here: Functional correctness requires checking concrete outcomes rather than action sequences
  - Quick check question: What properties would you check to verify an order was successfully placed?

## Architecture Onboarding

- Component map: Docker containers -> Observation renderer -> Action executor -> Evaluation engine -> LLM agent interface
- Critical path: Task intent → Observation rendering → Agent decision → Action execution → State update → Evaluation
- Design tradeoffs:
  - Reproducibility vs realism: Self-contained containers sacrifice some real-world complexity
  - Modality choice: HTML is structured but verbose; screenshots are universal but require vision models
  - Evaluation granularity: Programmatic checks are precise but may miss creative solutions
- Failure signatures:
  - Early termination with "N/A" indicates infeasibility determination errors
  - Repeated identical actions suggest observation interpretation failures
  - Stuck in loops indicates exploration limitations
- First 3 experiments:
  1. Run single task through entire pipeline with GPT-3.5 to verify basic functionality
  2. Test different observation modalities on same task to compare performance
  3. Validate evaluation programs by running known successful trajectories through them

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of autonomous agents on WebArena tasks change when using different large language models beyond GPT-3.5 and GPT-4, such as Claude or LLaMA?
- Basis in paper: [inferred] The paper primarily evaluates GPT-3.5 and GPT-4-based agents, leaving the potential performance of other large language models unexplored.
- Why unresolved: The paper does not experiment with other large language models, focusing only on GPT-3.5 and GPT-4 for agent implementation.
- What evidence would resolve it: Experimenting with different large language models like Claude or LLaMA on the WebArena benchmark and comparing their success rates with GPT-3.5 and GPT-4.

### Open Question 2
- Question: How does the performance of autonomous agents on WebArena tasks vary when using different observation modalities, such as raw HTML, screenshots, or accessibility trees?
- Basis in paper: [explicit] The paper mentions that the observation space can be configured to render the page content in different modes, including raw HTML, screenshots, and accessibility trees.
- Why unresolved: The paper does not provide a comparative analysis of agent performance across different observation modalities.
- What evidence would resolve it: Conducting experiments using different observation modalities (raw HTML, screenshots, accessibility trees) and comparing the success rates of autonomous agents on the WebArena benchmark.

### Open Question 3
- Question: How does the performance of autonomous agents on WebArena tasks change when incorporating more advanced reasoning techniques, such as tree-of-thoughts or procedural skill libraries?
- Basis in paper: [inferred] The paper discusses the challenges faced by current agents, including early task infeasibility determination and over-reliance on available context, suggesting the need for more advanced reasoning techniques.
- Why unresolved: The paper does not explore the potential benefits of advanced reasoning techniques like tree-of-thoughts or procedural skill libraries in improving agent performance.
- What evidence would resolve it: Implementing and evaluating autonomous agents on the WebArena benchmark using advanced reasoning techniques like tree-of-thoughts or procedural skill libraries, and comparing their success rates with baseline agents.

## Limitations
- Reproducibility may be affected by external dependencies despite Docker encapsulation claims
- Effectiveness of multi-modal observations lacks direct experimental validation
- Programmatic evaluation may miss valid alternative solutions to tasks

## Confidence
- High confidence in basic WebArena environment functionality
- Medium confidence in claimed success rates due to potential implementation variations
- Low confidence in completeness of evaluation programs

## Next Checks
1. Verify reproducibility by running identical tasks across multiple independent WebArena deployments
2. Test agent performance using different observation modalities on the same task set to quantify modality impact
3. Validate evaluation programs by having human experts review successful trajectories that were incorrectly scored as failures