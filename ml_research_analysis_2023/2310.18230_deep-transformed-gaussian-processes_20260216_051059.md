---
ver: rpa2
title: Deep Transformed Gaussian Processes
arxiv_id: '2310.18230'
source_url: https://arxiv.org/abs/2310.18230
tags:
- dtgp
- gaussian
- processes
- dtgps
- normalizing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Deep Transformed Gaussian Processes (DTGPs),
  a novel generalization of Transformed Gaussian Processes (TGPs) that increases flexibility
  through hierarchical concatenation of layers, each being a TGP. Unlike standard
  GPs, DTGPs transform the prior distribution using invertible normalizing flows between
  layers, enabling better modeling of complex data.
---

# Deep Transformed Gaussian Processes

## Quick Facts
- arXiv ID: 2310.18230
- Source URL: https://arxiv.org/abs/2310.18230
- Reference count: 3
- Key outcome: DTGPs achieve competitive or better performance compared to Deep Gaussian Processes (DGPs) and Transformed Gaussian Processes (TGPs) on regression datasets.

## Executive Summary
This paper introduces Deep Transformed Gaussian Processes (DTGPs), a novel generalization of Transformed Gaussian Processes (TGPs) that increases flexibility through hierarchical concatenation of layers, each being a TGP. Unlike standard GPs, DTGPs transform the prior distribution using invertible normalizing flows between layers, enabling better modeling of complex data. Exact inference in DTGPs is intractable, so the authors use variational inference with Monte Carlo sampling to approximate the required computations, extending the popular DSVI algorithm. Experiments on multiple regression datasets show that DTGPs achieve competitive or better performance compared to Deep Gaussian Processes (DGPs) and TGPs, with good scalability.

## Method Summary
DTGPs build on TGPs by stacking multiple layers where each layer applies a normalizing flow to transform the output of a Gaussian Process (GP), which then serves as input to the next GP layer. This hierarchical structure allows DTGPs to model increasingly complex non-linear relationships. Since exact inference is intractable, the authors use variational inference with Monte Carlo sampling to approximate the ELBO, avoiding the need for exact posterior computations while still capturing multi-layer dependencies. The method also supports input-dependent normalizing flows, where flow parameters are generated by a neural network, allowing the model to adapt transformations based on input structure.

## Key Results
- DTGPs achieve competitive or better performance compared to DGPs and TGPs on multiple regression datasets
- The method demonstrates good scalability while maintaining interpretable uncertainty estimates
- Experiments show effectiveness on both UCI regression datasets and a toy step-function dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hierarchical composition of TGPs into DTGPs increases model expressiveness by stacking multiple layers of non-Gaussian processes.
- Mechanism: Each layer applies an invertible transformation (normalizing flow) to the output of a GP, and the next layer treats this transformed output as its input. This allows the model to capture increasingly complex non-linear relationships.
- Core assumption: Normalizing flows are flexible enough to approximate the true data-generating process when composed across multiple layers.
- Evidence anchors:
  - [abstract] "we propose a generalization of TGPs named Deep Transformed Gaussian Processes (DTGPs), which follows the trend of concatenating layers of stochastic processes. More precisely, we obtain a multi-layer model in which each layer is a TGP."
  - [section 3] "Following the fashion of DGPs, the output of the TGP is used as the input of another TGP, recursively defining a stochastic process"
- Break condition: If the normalizing flows are too simple or the composition becomes too deep, the model may overfit or become intractable to train.

### Mechanism 2
- Claim: The use of variational inference with Monte Carlo sampling allows tractable approximate inference in DTGPs.
- Mechanism: By approximating the ELBO with Monte Carlo samples from each layer's variational distribution, the algorithm avoids the need for exact inference while still capturing the multi-layer dependencies.
- Core assumption: The ELBO approximation with MC samples is accurate enough for learning useful representations in DTGPs.
- Evidence anchors:
  - [abstract] "Exact inference in DTGPs is intractable, so the authors use variational inference with Monte Carlo sampling to approximate the required computations"
  - [section 3] "we approximate this term using Monte Carlo samples from QL l=1 q(f l K | f l−1 K ), Thus, we achieve computational tractability using two sources of stochasticity"
- Break condition: If the number of MC samples is too small, the ELBO estimate becomes too noisy, leading to poor optimization.

### Mechanism 3
- Claim: Using input-dependent normalizing flows allows DTGPs to model non-stationary processes with input-dependent inductive biases.
- Mechanism: The parameters of the normalizing flows are generated by a neural network that takes both the input and flow weights, allowing the transformation to adapt to the input structure.
- Core assumption: The neural network used to generate flow parameters is expressive enough to capture the necessary input-dependent transformations.
- Evidence anchors:
  - [section 2] "parameters are given by functions θ : W × X → R which we denote as θθθ(W, X)" and "Input-Dependent ( ID) normalizing flows, which yield a non-stationary process with well-inductive biases"
  - [corpus] No direct evidence in corpus about input-dependent flows in DTGPs; this is a novel extension.
- Break condition: If the neural network is poorly initialized or regularized, the input-dependent flows may not learn useful transformations.

## Foundational Learning

- Concept: Gaussian Processes and their limitations
  - Why needed here: DTGPs build on GPs by adding layers and transformations; understanding GP basics is essential.
  - Quick check question: What is the computational complexity of exact GP inference and why does it motivate sparse approximations?

- Concept: Normalizing Flows and invertible transformations
  - Why needed here: Each layer in DTGPs applies a normalizing flow to transform the GP output; knowing how flows work is critical.
  - Quick check question: How does the change of variables formula apply when transforming a GP with a normalizing flow?

- Concept: Variational Inference and Evidence Lower Bound (ELBO)
  - Why needed here: DTGPs use variational inference to approximate the intractable posterior; understanding ELBO derivation is key.
  - Quick check question: In the ELBO for DTGPs, what terms cancel due to the choice of variational distribution and why is this beneficial?

## Architecture Onboarding

- Component map:
  - Input layer → GP layer → Normalizing Flow → Next GP layer → ... → Output layer (identity flow)
  - Each GP layer uses sparse variational inference with inducing points
  - Normalizing flows can be input-dependent (parameterized by NN) or fixed
  - ELBO computed with MC sampling across layers

- Critical path:
  1. Sample from variational distribution at layer 1
  2. Apply normalizing flow to samples
  3. Propagate through next GP layer
  4. Repeat until final layer
  5. Compute ELL term with MC samples from all layers
  6. Compute KL terms analytically
  7. Backpropagate through entire path

- Design tradeoffs:
  - More layers → higher expressiveness but harder optimization and risk of overfitting
  - Input-dependent flows → more flexible but higher parameter count and training cost
  - Number of MC samples → ELBO accuracy vs. computational cost
  - Inducing point count → approximation quality vs. scalability

- Failure signatures:
  - Training loss plateaus early → flows not flexible enough or initialization poor
  - NLL on validation worse than training → overfitting, try fewer layers or stronger regularization
  - Extremely slow training → too many MC samples or layers, consider reducing
  - Posterior collapses to prior → KL term dominates, increase ELL weight or reinitialize

- First 3 experiments:
  1. Implement 2-layer DTGP with identity flows only (reduces to DGP) to verify inference code
  2. Add single fixed (non-input-dependent) arcsinh flow in first layer, compare to DGP
  3. Try input-dependent flows with small NN, monitor overfitting with early stopping on validation NLL

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of normalizing flow architecture (e.g. residual flows vs. coupling flows) impact the performance of DTGPs?
- Basis in paper: [inferred] The paper mentions that input-dependent flows can be used and that the choice of flow affects the flexibility of the model, but does not explore different flow architectures.
- Why unresolved: The paper only uses arcsinh flows in the experiments, leaving the impact of other flow types unexplored.
- What evidence would resolve it: Systematic comparison of DTGPs using different normalizing flow architectures (e.g. residual flows, coupling flows, autoregressive flows) on benchmark datasets.

### Open Question 2
- Question: What is the optimal number of layers and inducing points for DTGPs to balance performance and computational cost?
- Basis in paper: [explicit] The paper experiments with 2-5 layers but notes that performance can degrade with too many layers due to overfitting. Computational cost grows with more layers and inducing points.
- Why unresolved: The paper does not provide guidance on how to select these hyperparameters, and the optimal choice likely depends on the dataset and problem.
- What evidence would resolve it: Detailed study of DTGP performance and computational cost as a function of number of layers and inducing points across multiple datasets.

### Open Question 3
- Question: How does the performance of DTGPs compare to other hierarchical Bayesian models like Bayesian neural networks and implicit processes?
- Basis in paper: [explicit] The paper compares DTGPs to DGPs and TGPs but notes that other hierarchical models exist.
- Why unresolved: The paper does not benchmark DTGPs against these alternative models.
- What evidence would resolve it: Fair comparison of DTGPs, Bayesian neural networks, and implicit processes on the same datasets using the same training and evaluation protocols.

## Limitations
- Scalability claims are not well-supported by experimental evidence on large datasets
- The choice of normalizing flows (arcsinh, LeakyReLU) lacks justification for why these specific transformations were selected
- No sensitivity analysis provided for critical hyperparameters like number of layers, MC samples, or inducing points

## Confidence
- **High confidence**: The core theoretical framework of DTGPs (hierarchical composition of TGPs with normalizing flows) is mathematically sound and builds on established GP and flow-based modeling principles
- **Medium confidence**: The variational inference approach with MC sampling is a reasonable approximation method, though its effectiveness for very deep architectures remains unproven
- **Low confidence**: Claims about scalability and practical performance on large datasets are not well-supported by the experimental evidence

## Next Checks
1. **Scalability test**: Evaluate DTGPs on large-scale regression datasets (100k+ points) to verify the claimed computational advantages over standard DGPs, measuring both training time and memory usage
2. **Flow sensitivity analysis**: Systematically compare different normalizing flow families (RealNVP, MAF, Glow) and architectures (fixed vs. input-dependent) to determine which configurations provide the best trade-off between expressiveness and trainability
3. **Depth vs. performance study**: Conduct controlled experiments varying the number of layers (1-5) on multiple datasets to identify the optimal depth and detect overfitting patterns, particularly for input-dependent flows