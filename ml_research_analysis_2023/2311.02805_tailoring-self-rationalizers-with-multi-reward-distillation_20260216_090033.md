---
ver: rpa2
title: Tailoring Self-Rationalizers with Multi-Reward Distillation
arxiv_id: '2311.02805'
source_url: https://arxiv.org/abs/2311.02805
tags:
- rationales
- rationale
- mario
- answer
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of enabling small language models
  to generate high-quality free-text rationales for question answering tasks. While
  large models like GPT-3 can generate such rationales, they are computationally expensive.
---

# Tailoring Self-Rationalizers with Multi-Reward Distillation

## Quick Facts
- arXiv ID: 2311.02805
- Source URL: https://arxiv.org/abs/2311.02805
- Authors: 
- Reference count: 40
- This paper proposes MaRio, a multi-reward distillation method to train small language models to generate high-quality rationales for question answering tasks.

## Executive Summary
This paper addresses the challenge of enabling small language models to generate high-quality free-text rationales for question answering tasks. While large models like GPT-3 can generate such rationales, they are computationally expensive. The proposed approach, MaRio (Multi-rewArd RatIOnalization), addresses this by using multi-reward distillation to train smaller models. MaRio uses rationales from GPT-3 as initial supervision and then optimizes the smaller model using multiple reward functions that assess properties like plausibility, diversity, and consistency of the generated rationales. Experiments on five datasets show that MaRio improves both task accuracy and the quality of generated rationales compared to a supervised fine-tuning baseline. Human evaluations further confirm the superiority of MaRio-generated rationales.

## Method Summary
The paper proposes MaRio, a multi-reward distillation method to train small language models to generate high-quality rationales for question answering tasks. The method first uses GPT-3-generated rationales as initial supervision to train the small LM. Then, it employs a modified QUARK algorithm to optimize the LM using multiple reward functions that assess properties like plausibility, diversity, and consistency of the generated rationales. The approach uses control tokens to condition the LM on different rewards during training. The method is evaluated on five QA datasets, comparing the performance of the MaRio-trained LM with a supervised fine-tuning baseline.

## Key Results
- MaRio improves both task accuracy and the quality of generated rationales compared to supervised fine-tuning baseline.
- Human evaluations confirm the superiority of MaRio-generated rationales.
- The multi-reward optimization leads to rationales that are more plausible, diverse, and consistent.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Multi-reward distillation improves small LMs' rationale quality by training them to optimize for multiple properties simultaneously.
- **Mechanism**: The approach extends QUARK to a multi-reward setup, where rationales are binned based on multiple reward scores (plausibility, diversity, consistency) and control tokens are used to condition the LM on each reward. This allows the model to learn to generate rationales that score highly across all desired properties.
- **Core assumption**: Improving these individual properties will lead to better overall rationale quality and task performance.
- **Evidence anchors**:
  - [abstract] "Our method, MaRio (Multi-rewArd RatIOnalization), is a multi-reward conditioned self-rationalization algorithm that optimizes multiple distinct properties like plausibility, diversity and consistency."
  - [section 3] "To improve an LMs' rationalization across multiple properties, we leverage QUARK... We propose Multi-rewArd RatIOnalization (MARIO), an extension of QUARK to multiple rewards concurrently."
- **Break condition**: If the individual reward metrics are not reliable or strongly correlated with overall rationale quality, optimizing for them may not lead to the desired improvements.

### Mechanism 2
- **Claim**: Using GPT-3 generated rationales as initial supervision provides high-quality training data for small LMs.
- **Mechanism**: The approach first trains the small LM to mimic GPT-3's rationales, providing a strong starting point. The multi-reward optimization then refines this further. This leverages the strong rationale generation capabilities of large LMs to bootstrap training of smaller ones.
- **Core assumption**: GPT-3 generated rationales are of higher quality than those from smaller LMs, and this quality difference is sufficient to improve downstream training.
- **Evidence anchors**:
  - [abstract] "MARIO first starts with training a small LM to self-rationalize, with the help of GPT-3 generated rationales as initial supervision, which are shown to be of higher quality."
  - [section 3] "MARIO uses rationales generated by a larger LM like GPT-3 as initial supervision..."
- **Break condition**: If GPT-3 rationales are not consistently high quality, or if the quality difference is not sufficient to provide a strong training signal, this approach may not be effective.

### Mechanism 3
- **Claim**: Optimizing for task accuracy as an additional reward leads to the best performance.
- **Mechanism**: The approach includes task correctness (answering the question correctly) as an additional reward in the multi-reward optimization. This ensures that the rationales not only have good properties but also lead to correct answers.
- **Core assumption**: Improving task accuracy will lead to better rationales, as the two are correlated.
- **Evidence anchors**:
  - [abstract] "We also consider task correctness as a necessary property of rationales, that they should try to improve over as a byproduct."
  - [section 4.2] "As our last reward, we add task correctness of the answer that is generated following the rationale."
- **Break condition**: If task accuracy is not well-correlated with rationale quality, optimizing for it may lead to worse rationales in terms of the other properties.

## Foundational Learning

- **Concept**: Reinforcement learning from human feedback (RLHF)
  - **Why needed here**: The multi-reward optimization is a form of RLHF, where the rewards are automatically computed rather than provided by humans. Understanding RLHF is key to understanding how the approach works.
  - **Quick check question**: How does RLHF differ from standard supervised learning, and what are the benefits and challenges of using it?

- **Concept**: Chain-of-thought prompting
  - **Why needed here**: The approach leverages GPT-3's ability to generate rationales via chain-of-thought prompting as the initial supervision. Understanding how chain-of-thought works is important for understanding the quality of the training data.
  - **Quick check question**: What is chain-of-thought prompting, and how does it enable large LMs to generate rationales?

- **Concept**: Reward hacking
  - **Why needed here**: The discussion section mentions the risk of reward hacking, where the model learns to optimize the reward in unintended ways. Being aware of this risk is important for properly designing and evaluating the approach.
  - **Quick check question**: What is reward hacking, and how can it be detected and prevented in the context of multi-reward optimization?

## Architecture Onboarding

- **Component map**: GPT-3 -> Small LM (T5-Large) -> Reward Models -> Control Tokens

- **Critical path**:
  1. Sample GPT-3 rationales for training data
  2. Train small LM to mimic GPT-3 rationales (supervised learning)
  3. Iteratively sample LM rationales, score with reward models, bin based on scores
  4. Train LM to associate control tokens with reward bins (RLHF)
  5. Use control tokens to generate high-quality rationales

- **Design tradeoffs**:
  - Using GPT-3 vs. other large LMs for supervision
  - Number and choice of reward metrics
  - Single vs. multi-reward optimization approach
  - KL divergence penalty strength in training

- **Failure signatures**:
  - Reward hacking (e.g. repetitive rationales to game diversity metric)
  - Degraded task performance despite improved rationale metrics
  - Inconsistent or implausible rationales
  - Slow or unstable training

- **First 3 experiments**:
  1. Train small LM to mimic GPT-3 rationales, evaluate rationale and task performance
  2. Add multi-reward optimization with plausibility and diversity, evaluate improvements
  3. Include task accuracy as additional reward, evaluate end-to-end performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the multi-reward optimization process be improved to prevent reward hacking and generate higher-quality rationales?
- Basis in paper: Explicit - The paper discusses the challenge of reward hacking, where models optimize for numerical reward scores without necessarily improving the quality of the rationales qualitatively.
- Why unresolved: The paper presents an initial attempt at multi-reward optimization but acknowledges that finding the right balance of rewards and preventing reward hacking remains an open problem.
- What evidence would resolve it: Further research into designing stronger reward functions, exploring different optimization techniques, and conducting more in-depth qualitative analysis of the generated rationales could help address this issue.

### Open Question 2
- Question: What is the impact of using different control token orders in the CANONICAL and ADDITIVE MARIO methods?
- Basis in paper: Explicit - The paper mentions that the order of control tokens is a design choice but does not explore its impact on the performance of the model.
- Why unresolved: The paper focuses on the overall effectiveness of the MARIO framework but does not investigate how the specific ordering of control tokens affects the quality of the generated rationales.
- What evidence would resolve it: Conducting experiments with different control token orders and analyzing their impact on the plausibility, diversity, and consistency of the rationales could provide insights into this aspect.

### Open Question 3
- Question: How does the performance of MARIO compare to other methods that use large language models for self-rationalization?
- Basis in paper: Inferred - The paper mentions that large language models are used for self-rationalization but does not provide a direct comparison with MARIO.
- Why unresolved: The paper focuses on improving the rationalization capabilities of smaller language models but does not explicitly compare its performance to methods that rely on large language models.
- What evidence would resolve it: Conducting experiments that compare the performance of MARIO with other methods that use large language models for self-rationalization, such as chain-of-thought prompting or few-shot learning, would provide a clearer understanding of its relative strengths and weaknesses.

## Limitations

- The success of the approach depends critically on the reliability of the reward metrics used, and there is a risk of the model learning to "game" these metrics rather than genuinely improving rationale quality.
- The method assumes GPT-3 rationales are consistently high-quality, but this is not empirically validated in the paper.
- The choice of specific reward functions and their relative weights appears to be based on intuition rather than systematic optimization.

## Confidence

- **High confidence**: The core multi-reward distillation approach (extending QUARK to multiple rewards) is technically sound and the experimental methodology is clearly specified.
- **Medium confidence**: The claim that GPT-3 rationales are of higher quality than smaller LMs is plausible but not directly validated in the paper.
- **Medium confidence**: The reported improvements in rationale quality metrics are likely accurate, but the connection to genuine reasoning improvements is less certain.
- **Medium confidence**: The human evaluation results support the claims, but the sample size and methodology details are limited.

## Next Checks

1. Conduct an ablation study removing each reward metric to determine which ones are most critical for the observed improvements and whether optimizing for multiple metrics is better than optimizing for just task accuracy.

2. Perform a direct comparison between rationales generated by the fine-tuned small LM and GPT-3 rationales using the same evaluation metrics to verify the claim that small LMs can match GPT-3 quality.

3. Design a study where human evaluators assess whether the rationales demonstrate genuine reasoning improvements beyond what the automated metrics capture, particularly looking for cases of reward hacking.