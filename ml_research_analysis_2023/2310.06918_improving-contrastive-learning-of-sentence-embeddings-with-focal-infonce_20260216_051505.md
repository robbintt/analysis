---
ver: rpa2
title: Improving Contrastive Learning of Sentence Embeddings with Focal-InfoNCE
arxiv_id: '2310.06918'
source_url: https://arxiv.org/abs/2310.06918
tags:
- learning
- sentence
- contrastive
- focal-infonce
- negative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the limitations of SimCSE, a state-of-the-art
  framework for learning sentence embeddings, which does not fully exploit the potential
  of hard negative samples in contrastive learning. To address this issue, the authors
  propose Focal-InfoNCE, a novel loss function that introduces self-paced modulation
  terms to downweight the loss associated with easy negatives and encourage the model
  to focus on hard negatives.
---

# Improving Contrastive Learning of Sentence Embeddings with Focal-InfoNCE

## Quick Facts
- arXiv ID: 2310.06918
- Source URL: https://arxiv.org/abs/2310.06918
- Reference count: 10
- Key outcome: Focal-InfoNCE improves unsupervised SimCSE by 1.64% on BERT-base, 0.82% on BERT-large, 1.51% on RoBERTa-base, and 0.75% on RoBERTa-large on STS benchmarks

## Executive Summary
This paper addresses the limitations of SimCSE in exploiting hard negative samples for contrastive learning of sentence embeddings. The authors propose Focal-InfoNCE, a novel loss function that introduces self-paced modulation terms to downweight easy negatives and focus on hard negatives. Experimental results demonstrate significant improvements in Spearman's correlation and representation uniformity across multiple model architectures and STS benchmarks.

## Method Summary
Focal-InfoNCE modifies the standard InfoNCE loss by introducing modulation terms for both positive and negative pairs. The method uses a hardness-aware hyperparameter to dynamically adjust the contribution of each sample based on its similarity score. Easy negatives are downweighted while hard negatives receive higher weights, encouraging the model to focus on more informative samples. The approach is evaluated by fine-tuning pre-trained BERT and RoBERTa models on 1 million English Wikipedia sentences and testing on STS benchmarks.

## Key Results
- Improves unsupervised SimCSE by 1.64%, 0.82%, 1.51%, and 0.75% on BERT-base, BERT-large, RoBERTa-base, and RoBERTa-large respectively
- Enhances representation uniformity and alignment as measured by standard metrics
- Demonstrates consistent improvements across multiple STS benchmarks (STS12-16, STS-B, SICK-R)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Focal-InfoNCE improves representation uniformity by downweighting easy negatives and upweighting hard negatives during training.
- Mechanism: The focal-InfoNCE loss includes a modulation term (s_i,j_n + m) for negative pairs that scales the contribution of each negative based on its similarity score. High-similarity negatives (hard negatives) receive higher weights, while low-similarity negatives (easy negatives) are downweighted. This shifts the optimization focus toward challenging pairs that better separate classes in the embedding space.
- Core assumption: High-similarity negative pairs indicate hard negatives that are more informative for improving representation uniformity and reducing anisotropy in pre-trained models.
- Evidence anchors:
  - [abstract]: "The proposed focal-InfoNCE function introduces self-paced modulation terms in the contrastive objective, downweighting the loss associated with easy negatives and encouraging the model focusing on hard negatives."
  - [section]: "To this end, we propose to upweight the corresponding term si,j n /τ by a modulation factor si,j n + m... According to Eq. (4), comparing to easy negatives, hard negative samples that associates with higher similarity score si,j n contribute more to the loss function."
- Break condition: If similarity scores do not correlate with actual semantic difficulty (e.g., due to poor embedding quality or degenerate representations), the modulation may incorrectly prioritize irrelevant pairs, harming uniformity.

### Mechanism 2
- Claim: Focal-InfoNCE improves alignment by reweighting positive pairs to reduce the impact of semantic information loss from dropout augmentation.
- Mechanism: The loss includes a modulation term (s_i_p)^2 on positive pairs that downweights positive pairs with low similarity scores. Since SimCSE uses dropout as minimal augmentation, low similarity often reflects semantic information loss rather than poor model quality. Reducing their contribution prevents the model from being penalized for dropout-induced dissimilarity.
- Core assumption: Dropout-induced positive pairs with low similarity reflect semantic information loss rather than poor embedding quality, so they should be downweighted during optimization.
- Evidence anchors:
  - [abstract]: "the proposed focal-InfoNCE function introduces self-paced modulation terms in the contrastive objective, downweighting the loss associated with easy negatives and encouraging the model focusing on hard negatives."
  - [section]: "In SimCSE, the positive pair is formed by dropout with random masking. Thus a low similarity score sp indicates semantic information loss introduced by dropout... The partial derivative with respect to si,j n is... which suggests that positive pairs with lower similarity scores in SimCSE contributes less to model optimization."
- Break condition: If dropout augmentation reliably preserves semantic content (e.g., with very small dropout rates), the assumption that low similarity indicates information loss breaks down, and positive reweighting may degrade alignment.

### Mechanism 3
- Claim: Focal-InfoNCE reduces the chance of the model getting stuck in sub-optimal solutions dominated by easy pairs by balancing the influence of hard and easy samples.
- Mechanism: By upweighting hard negatives and downweighting easy negatives, the loss function prevents the model from converging to a solution where easy negatives dominate the gradient signal. This encourages the model to explore more discriminative decision boundaries.
- Core assumption: Easy negatives dominate the gradient signal in standard InfoNCE, leading to sub-optimal solutions where the model fails to learn fine-grained distinctions.
- Evidence anchors:
  - [abstract]: "With a large number of easy negative samples, the contribution of hard negatives is thus prone to being overwhelmed" and "Our experiments also show that Focal-InfoNCE improves uniformity in sentence embeddings."
  - [section]: "Due to the modulation terms on both positive and negative samples, Focal-InfoNCE reduces the chances of the model getting stuck in sub-optimal solutions dominated by easy pairs."
- Break condition: If the dataset or batch composition naturally provides sufficient hard negative signals without explicit reweighting, the additional complexity of Focal-InfoNCE may not yield benefits and could overfit to batch-specific patterns.

## Foundational Learning

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: Focal-InfoNCE builds directly on InfoNCE by modifying its loss function with self-paced modulation terms. Understanding InfoNCE is essential to grasp how Focal-InfoNCE changes the optimization dynamics.
  - Quick check question: What is the core objective of InfoNCE loss in contrastive learning, and how does it use positive and negative pairs?

- Concept: Self-paced learning and hard example mining
  - Why needed here: Focal-InfoNCE applies self-paced modulation to dynamically adjust the importance of samples based on their difficulty, a concept borrowed from hard example mining in other domains.
  - Quick check question: How does self-paced learning differ from static reweighting, and why is it particularly useful for contrastive learning with sentence embeddings?

- Concept: Dropout as data augmentation in SimCSE
  - Why needed here: Focal-InfoNCE incorporates a positive modulation term that specifically addresses the semantic information loss introduced by dropout-based positive pair construction in SimCSE.
  - Quick check question: Why does SimCSE use dropout as augmentation, and what are the implications for positive pair similarity scores?

## Architecture Onboarding

- Component map:
  Pre-trained encoder (BERT/RoBERTa) -> Dropout layer for positive pair construction -> Cosine similarity computation -> Focal-InfoNCE loss computation with modulation terms -> Optimizer and training loop

- Critical path:
  Forward pass through encoder → Compute similarities → Apply focal-InfoNCE modulation → Compute loss → Backward pass → Update weights

- Design tradeoffs:
  - Hard negative mining vs. computational cost: Focal-InfoNCE increases per-sample computation due to modulation terms but avoids explicit hard negative mining overhead.
  - Hyperparameter sensitivity: Temperature τ and hardness m require tuning; poor choices can degrade performance.
  - Positive pair reweighting: Assumes dropout-induced similarity loss is undesirable; may not hold for all augmentation strategies.

- Failure signatures:
  - Degraded uniformity: Model produces embeddings with high anisotropy despite training.
  - Overfitting to batch: Performance drops significantly on held-out data, suggesting batch-specific hard negative patterns.
  - Unstable training: Loss oscillates or diverges due to extreme modulation values.

- First 3 experiments:
  1. Ablation study: Compare Focal-InfoNCE vs. standard InfoNCE on STS-B benchmark with BERT-base to verify performance gains.
  2. Hyperparameter sweep: Test different τ and m values to identify optimal settings for each model variant.
  3. Uniformity analysis: Measure representation uniformity (e.g., using the uniformity metric from Wang & Isola 2020) before and after applying Focal-InfoNCE to confirm the mechanism's effect.

## Open Questions the Paper Calls Out

- The paper does not explicitly call out open questions, but several potential research directions emerge from the limitations and implications of the work.

## Limitations

- Limited evaluation scope: Experiments focus primarily on English STS benchmarks without exploring cross-lingual or domain-specific applications.
- Lack of theoretical analysis: The effectiveness of the hardness-aware parameter m is demonstrated empirically but not theoretically analyzed.
- Computational overhead: The additional modulation terms increase per-sample computation compared to standard InfoNCE.

## Confidence

- Mechanism 1 (Negative modulation for uniformity): Medium
- Mechanism 2 (Positive modulation for dropout compensation): Low
- Mechanism 3 (Prevention of sub-optimal convergence): Low

## Next Checks

1. Conduct ablation studies to separately evaluate the impact of positive vs. negative modulation terms on STS benchmark performance
2. Test the method on cross-lingual STS benchmarks to assess generalization beyond English
3. Analyze training dynamics with different hardness-aware parameter values to identify potential stability issues or optimal ranges