---
ver: rpa2
title: 'CMMLU: Measuring massive multitask language understanding in Chinese'
arxiv_id: '2306.09212'
source_url: https://arxiv.org/abs/2306.09212
tags:
- chinese
- performance
- language
- llms
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CMMLU, a comprehensive Chinese benchmark
  designed to evaluate the knowledge and reasoning capabilities of large language
  models (LLMs) across diverse subjects. The benchmark covers 67 topics spanning natural
  sciences, social sciences, engineering, humanities, and China-specific knowledge.
---

# CMMLU: Measuring massive multitask language understanding in Chinese

## Quick Facts
- arXiv ID: 2306.09212
- Source URL: https://arxiv.org/abs/2306.09212
- Reference count: 21
- Most models struggle to achieve average accuracy above 40% on CMMLU benchmark

## Executive Summary
This paper introduces CMMLU, a comprehensive Chinese benchmark designed to evaluate large language models across 67 subjects spanning natural sciences, social sciences, engineering, humanities, and China-specific knowledge. The benchmark contains 11,528 multiple-choice questions and evaluates 18 advanced multilingual and Chinese-oriented LLMs. Results show that most models struggle to achieve average accuracy above 40%, while ChatGPT leads with 55.51% accuracy. The study reveals systematic performance disparities across subject categories and explores factors affecting model performance including question length, negation, and sub-options.

## Method Summary
The evaluation uses 11,528 multiple-choice questions across 67 subjects, with each question having four options. Models are evaluated in both zero-shot and few-shot settings, with five examples provided for few-shot scenarios. The standard prompting format includes a subject introduction followed by either zero-shot or few-shot examples, then the question, ending with "Answer is:". Performance is measured by average accuracy across subjects, broken down by categories including STEM, Humanities, Social Science, Other, and China-specific content.

## Key Results
- ChatGPT achieves highest average accuracy of 55.51% across all models
- Most models struggle to exceed 40% average accuracy even with in-context examples
- Chain-of-thought prompts improve STEM reasoning but decrease performance on humanities/social sciences
- Chinese-oriented models show reduced performance gaps on China-specific content compared to general models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark reveals systematic limitations in LLMs' Chinese language and cultural understanding due to subject-specific performance disparities.
- Mechanism: CMMLU's 67 subject coverage includes both universally translatable and China-specific domains, allowing performance gaps to emerge when models encounter culturally nuanced content. The format of multiple-choice questions with single correct answers provides a clear signal of knowledge gaps.
- Core assumption: LLMs encode knowledge in a way that generalizes poorly across culturally specific contexts unless explicitly trained on relevant data.
- Evidence anchors:
  - [abstract] "reveals that most models struggle to achieve an average accuracy above 40%"
  - [section 4.1] "all models exhibit better performance in humanities, social sciences, and other subjects compared to STEM subjects"
  - [section 4.1] "models with Chinese-specific pre-training or fine-tuning show smaller performance gaps between China-specific and other categories"
- Break condition: If models achieve consistent >90% accuracy across all subjects, the mechanism fails to explain the observed performance distribution.

### Mechanism 2
- Claim: Chain-of-thought prompts improve STEM reasoning but not humanities/social sciences due to the nature of analytical vs. recall-based questions.
- Mechanism: The prompt modification to "Analyze step by step and select the correct answer" activates sequential reasoning processes that benefit computational problem-solving but disrupt straightforward knowledge recall in humanities.
- Core assumption: STEM questions require explicit reasoning steps while humanities questions rely more on direct knowledge retrieval.
- Evidence anchors:
  - [section 4.2] "chain-of-thought prompts do assist these models with answering STEM-related questions" with systematic errors of +1.82% for STEM
  - [section 4.2] "Chinese-oriented models exhibit a decrease in overall performance by approximately 3%" with systematic errors of -3.21%
- Break condition: If chain-of-thought prompts show consistent improvement or degradation across all subject categories, the mechanism is invalid.

### Mechanism 3
- Claim: Model size improvements follow diminishing returns with consistent ~5 point accuracy gains per doubling of parameters.
- Mechanism: Larger parameter counts enable better representation of knowledge patterns, but quality of training data and task alignment become limiting factors beyond certain model sizes.
- Core assumption: Parameter scaling follows predictable performance curves until data quality constraints dominate.
- Evidence anchors:
  - [section 4.2] "as the model size approximately doubles, there is a consistent increase of around 5 points"
  - [section 4.2] "an LLaMA model with 500B parameters could potentially achieve performance on par with ChatGPT"
- Break condition: If parameter scaling shows non-linear or saturating behavior before reaching practical limits, the mechanism breaks down.

## Foundational Learning

- Concept: Cross-cultural benchmarking
  - Why needed here: Understanding how to design evaluations that capture culturally specific knowledge versus universal knowledge
  - Quick check question: What distinguishes a culturally specific question from a universal one in multilingual benchmarking?

- Concept: Prompt engineering for reasoning
  - Why needed here: Different prompt formats (direct vs. chain-of-thought) produce varying results across subject domains
  - Quick check question: When would you choose chain-of-thought prompting over direct prompting for multiple-choice tasks?

- Concept: Zero-shot vs. few-shot evaluation
  - Why needed here: Understanding how demonstration examples affect model performance and whether they help or hinder different model architectures
  - Quick check question: Why might few-shot examples degrade performance for models already trained on similar data distributions?

## Architecture Onboarding

- Component map: Data collection pipeline → Quality checking → Prompt generation → Model inference → Result aggregation → Analysis
- Critical path: Load question dataset with subject categorization → Generate appropriate prompts (zero-shot vs. few-shot) → Run inference across all target models → Extract and parse model responses → Calculate accuracy metrics per subject and overall → Generate analysis reports
- Design tradeoffs: Multiple-choice format provides clear evaluation but may not capture nuanced understanding; few-shot examples can help or hurt depending on model training; chain-of-thought prompts may improve reasoning but reduce direct knowledge recall; question length normalization affects difficulty assessment
- Failure signatures: Random baseline performance across all subjects indicates evaluation design issues; consistent accuracy below random suggests parsing or inference problems; subject-specific failures indicate knowledge gaps rather than model limitations; prompt sensitivity suggests over-reliance on specific formatting
- First 3 experiments: Run zero-shot evaluation across all 18 models to establish baseline performance; Compare few-shot vs. zero-shot performance for each model to identify optimization needs; Test chain-of-thought prompting on STEM vs. humanities subjects to validate reasoning mechanisms

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific training data sources and methodologies could significantly improve Chinese-specific LLMs' performance on China-specific subjects?
- Basis in paper: [explicit] The paper notes that ChatGPT outperforms Chinese-oriented models on most subjects, but Chinese-oriented models perform better on China-specific subjects, suggesting that different data sources may be key.
- Why unresolved: The paper identifies the gap but doesn't explore what specific data sources or training methodologies would bridge it.
- What evidence would resolve it: Comparative studies of different training data sources (e.g., government documents, cultural materials, local exam data) and their impact on performance across China-specific subjects.

### Open Question 2
- Question: What is the optimal balance between model size and training data quality for Chinese language models?
- Basis in paper: [explicit] The paper shows that larger models generally perform better, but also notes that Falcon-40B (smaller than LLaMA-65B) achieves competitive results, suggesting data quality may be as important as size.
- Why unresolved: While the paper demonstrates the importance of both factors, it doesn't determine the optimal trade-off between them.
- What evidence would resolve it: Systematic experiments varying both model size and training data quality across the same tasks to identify optimal combinations.

### Open Question 3
- Question: How can chain-of-thought prompting be optimized for Chinese language models to improve STEM subject performance?
- Basis in paper: [explicit] The paper finds that chain-of-thought prompting shows mixed results, sometimes improving STEM performance but also causing difficulties in answer extraction.
- Why unresolved: The paper identifies the problem but doesn't explore solutions for better integrating chain-of-thought prompting with Chinese language models.
- What evidence would resolve it: Experiments testing different prompt formats, answer extraction methods, and prompt engineering techniques specifically for Chinese STEM questions.

## Limitations

- Benchmark relies on multiple-choice format which may not capture nuanced understanding
- Few-shot examples can degrade performance for certain models due to training methodology mismatch
- Results may not generalize to non-multiple-choice evaluation formats

## Confidence

**High Confidence Claims:**
- ChatGPT achieves significantly higher accuracy (55.51%) than other evaluated models
- Chain-of-thought prompts consistently improve STEM performance across model families by approximately 1.82%
- Chinese-oriented models show systematically lower performance when using chain-of-thought prompts (-3.21% average)
- Parameter scaling follows approximately 5-point accuracy gains per doubling of model size

**Medium Confidence Claims:**
- Humanities and social sciences show higher performance than STEM subjects across all models
- Models with Chinese-specific pre-training show reduced performance gaps on China-specific content
- Question length and negation features correlate with decreased model accuracy

**Low Confidence Claims:**
- The specific mechanisms by which few-shot examples degrade performance for certain models
- The generalizability of results to non-multiple-choice evaluation formats
- The long-term scalability of parameter-based performance improvements

## Next Checks

1. Cross-Validation of Baseline Performance: Conduct additional evaluations using scrambled answer choices to verify that the 25% random baseline accurately represents chance performance and investigate whether systematic biases exist in question design or model response patterns.

2. Prompt Sensitivity Analysis: Systematically test variations in prompt formatting, example selection, and chain-of-thought instructions across multiple model families to determine whether the observed performance differences are robust to prompt engineering choices.

3. Knowledge Transfer Assessment: Evaluate models on subset questions that can be translated between Chinese and English to quantify the contribution of language-specific versus general knowledge to the observed performance differences between Chinese-oriented and multilingual models.