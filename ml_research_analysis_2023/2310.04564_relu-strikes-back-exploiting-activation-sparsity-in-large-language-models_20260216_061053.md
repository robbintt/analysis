---
ver: rpa2
title: 'ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models'
arxiv_id: '2310.04564'
source_url: https://arxiv.org/abs/2310.04564
tags:
- sparsity
- activation
- relu
- tokens
- speculative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper advocates for the use of ReLU activation in large language
  models (LLMs) to improve inference efficiency. The authors show that ReLU has a
  negligible impact on convergence and performance compared to other activation functions,
  but significantly reduces computation and weight transfer during inference.
---

# ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models

## Quick Facts
- arXiv ID: 2310.04564
- Source URL: https://arxiv.org/abs/2310.04564
- Reference count: 40
- Primary result: ReLU activation can reduce LLM inference computation by up to 3x with minimal performance trade-offs

## Executive Summary
This paper demonstrates that ReLU activation functions can significantly improve inference efficiency of large language models (LLMs) by exploiting activation sparsity. Through systematic relufication of pretrained models, the authors show that ReLU has negligible impact on convergence and performance while reducing computation and weight transfer during inference. The paper proposes practical strategies including staged relufication and shifted ReLU to maximize sparsity benefits, achieving up to 32% computation savings on OPT models and demonstrating neuron reuse patterns that enable faster speculative decoding.

## Method Summary
The authors fine-tune pretrained LLMs (OPT, Llama, Falcon) by replacing GELU/SiLU activations with ReLU and inserting additional ReLU layers after normalization layers. The process occurs in stages: first fine-tuning with ReLU, then adding ReLU after normalization layers and fine-tuning again. Models are trained on RefinedWeb dataset for 30B tokens (stage 1) and 50B tokens (stage 2) using AdamW optimizer with learning rate 1.5e-5. Performance is evaluated on zero-shot accuracy and MMLU benchmarks while measuring activation sparsity and FLOPS per token.

## Key Results
- ReLU activation reduces computation from 6.6G FLOPS to 4.5G FLOPS per token (32% saving) for OPT models
- Reluified models achieve similar or slightly better accuracy compared to original models (e.g., 67.6% vs 67.5% on MMLU for Llama)
- Activation sparsity improves significantly after relufication, with potential for further gains using shifted ReLU
- Aggregated sparsity enables neuron reuse across tokens, reducing I/O for speculative decoding

## Why This Works (Mechanism)

### Mechanism 1: Activation Sparsity Enables I/O Reduction
ReLU produces sparse activations where zero activations eliminate entire rows of weight matrices from computation and transfer. This reduces both FLOPS and memory I/O during inference. The effectiveness depends on hardware's ability to exploit structured sparsity.

### Mechanism 2: ReLU Reluification Recovers Performance After Activation Replacement
Pretrained models maintain stable pre-activation distributions that can be adapted to ReLU through short finetuning. This rapid recovery works because the underlying representational capacity remains intact despite activation function changes.

### Mechanism 3: Aggregated Sparsity Enables Speculative Decoding Speedup
Consistent neuron activation patterns across tokens (aggregated sparsity) allows weight reuse, reducing memory transfers in speculative decoding. Previously loaded weights for active neurons can be reused for subsequent tokens.

## Foundational Learning

- **Activation functions and their impact on sparsity**: ReLU produces sparse activations while GELU/SiLU remain dense. Understanding this difference is crucial for comparing efficiency gains.
  - Quick check: What property of ReLU makes it produce sparse activations compared to GELU or SiLU?

- **Transformer architecture (FFN and Attention layers)**: Reluification modifies activation placement in both feed-forward and attention sublayers. Understanding weight flow and normalization placement is essential.
  - Quick check: In a standard transformer layer, which sublayers produce activations that can be sparsified by ReLU?

- **Speculative decoding**: Aggregated sparsity is leveraged to improve speculative decoding speed. Understanding how the small and large model interact is necessary for speedup calculations.
  - Quick check: How does speculative decoding reduce inference latency compared to autoregressive decoding?

## Architecture Onboarding

- **Component map**: Token embeddings → LayerNorm → ReLU → Attention (QKV projection) → FFN (Up/Down projection) → Output

- **Critical path**: 1) Token generation loop with ReLU after normalization sparsifying both attention and FFN inputs; 2) For speculative decoding, aggregated sparsity determines which neurons to keep loaded across tokens

- **Design tradeoffs**: ReLU offers sparsity but may require longer training; GELU/SiLU converge faster but are denser. Stage 1 targets down projection sparsity; stage 2 adds sparsity to QKV and up projection at slight accuracy cost. Shifted ReLU offers higher sparsity but needs careful tuning.

- **Failure signatures**: Loss of sparsity if pre-activation distribution shifts during finetuning; performance degradation if sparsity is too aggressive or finetuning insufficient; speculative decoding slowdown if aggregated sparsity is inconsistent.

- **First 3 experiments**: 1) Compare FLOPS and accuracy of relufied vs original model on fixed dataset; 2) Measure aggregated sparsity across tokens and test weight reuse impact on perplexity; 3) Test shifted ReLU with different shift values to find optimal sparsity-performance tradeoff.

## Open Questions the Paper Calls Out

- How does activation sparsity of ReLU-based LLMs change across different tasks and datasets? The paper only provides results for WikiText dataset and doesn't explore variation across tasks.

- What is the impact of shifted ReLU activation function on performance and sparsity of different LLM architectures? The paper only tests shifted ReLU on Llama model, not OPT or Falcon.

- How does choice of activation function affect convergence speed and final performance of LLMs during training? The paper states activation choice doesn't significantly impact performance but doesn't analyze convergence differences.

## Limitations

- Hardware-specific gains are uncertain as actual wall-clock speedups depend on architecture and optimization, not just FLOPS reduction.
- Pre-activation distribution stability is only visually inspected, not rigorously quantified, creating a validation gap.
- Aggregated sparsity consistency across diverse text patterns remains unproven, potentially limiting practical applicability.

## Confidence

- **High confidence**: ReLU activation induces sparsity in LLM activations; FLOPS reduction calculations are mathematically sound.
- **Medium confidence**: Reluification can be done in stages with each stage improving efficiency while maintaining performance, though depends on hyperparameter tuning.
- **Low confidence**: Practical speedup claims for speculative decoding based on aggregated sparsity are promising but not rigorously validated with end-to-end latency measurements.

## Next Checks

1. **Hardware measurement validation**: Implement relufied model on GPU/CPU hardware and measure actual inference latency and throughput, comparing wall-clock time for sequences of varying lengths.

2. **Pre-activation distribution stability test**: Quantitatively measure KL divergence or other statistical distance metrics between pre-activation distributions before and after each stage of relufication across different layers and attention heads.

3. **Aggregated sparsity consistency evaluation**: Test neuron reuse patterns across diverse text domains (code, dialogue, formal writing) and measure how consistent aggregated sparsity is across different contexts.