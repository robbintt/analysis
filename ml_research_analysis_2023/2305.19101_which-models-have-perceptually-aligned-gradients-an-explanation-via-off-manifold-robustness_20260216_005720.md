---
ver: rpa2
title: Which Models have Perceptually-Aligned Gradients? An Explanation via Off-Manifold
  Robustness
arxiv_id: '2305.19101'
source_url: https://arxiv.org/abs/2305.19101
tags:
- robustness
- robust
- gradients
- off-manifold
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the mechanisms behind perceptually-aligned
  gradients (PAGs) in robust machine learning models. The authors propose that PAGs
  arise from off-manifold robustness, where models are more robust to noise off the
  data manifold than on it.
---

# Which Models have Perceptually-Aligned Gradients? An Explanation via Off-Manifold Robustness

## Quick Facts
- arXiv ID: 2305.19101
- Source URL: https://arxiv.org/abs/2305.19101
- Reference count: 40
- Models with off-manifold robustness exhibit perceptually-aligned gradients

## Executive Summary
This paper investigates why robust machine learning models produce gradients that align with human perception. The authors propose that perceptually-aligned gradients (PAGs) arise from off-manifold robustness - where models are more robust to noise off the data manifold than on it. They demonstrate theoretically that off-manifold robustness leads to on-manifold gradient alignment, and empirically show that robust models trained with various methods (gradient norm regularization, noise augmentation, randomized smoothing) exhibit this property. The paper identifies three regimes of robustness - weak, Bayes-aligned, and excessive - each affecting both perceptual alignment and model accuracy differently.

## Method Summary
The authors train ResNet18 models using CIFAR-10, ImageNet, and ImageNet-64x64 datasets with various robust training objectives including gradient norm regularization, smoothness penalty, randomized smoothing, and adversarial training. They estimate tangent spaces using autoencoders and generate on-manifold and off-manifold noise perturbations. Robustness is measured by perturbing inputs with generated noise and computing L1-norm of output changes. Perceptual alignment is quantified using LPIPS metric comparing model gradients with score estimates from diffusion models.

## Key Results
- Off-manifold robustness directly causes input gradients to lie approximately on the signal manifold
- Bayes optimal classifiers are off-manifold robust because their gradients naturally lie on the signal manifold
- Robust training objectives implicitly encourage off-manifold robustness due to the imbalance between cross-entropy loss on data samples and isotropic robustness penalties

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Off-manifold robustness directly causes input gradients to lie approximately on the signal manifold.
- Mechanism: When a model is more robust to noise off the data manifold than on it, the gradients align with the tangent space of the signal manifold due to the equivalence between relative off-manifold robustness and on-manifold gradient alignment.
- Core assumption: The signal manifold is a lower-dimensional subspace of the data manifold, and the model's gradients reflect this structure when trained robustly.
- Evidence anchors:
  - [abstract] "We first demonstrate theoretically that off-manifold robustness leads input gradients to lie approximately on the data manifold, explaining their perceptual alignment."
  - [section 3.1] "A function f : Rd → R exhibits on-manifold gradient alignment if and only if it is off-manifold robust wrt normal noise u ∼ N (0, σ2) for σ → 0 (with ρ1 = ρ2)."
- Break condition: If the signal manifold and data manifold are identical (no distractors), the distinction vanishes and the mechanism loses explanatory power for perceptual alignment.

### Mechanism 2
- Claim: Bayes optimal classifiers are off-manifold robust because their gradients naturally lie on the signal manifold.
- Mechanism: The gradients of the Bayes optimal classifier are linear combinations of class-conditional generative model gradients, which are tangent to the data manifold; the distractor component vanishes, leaving only the signal manifold alignment.
- Core assumption: The signal-distractor decomposition exists and is known for the classification task, with the distractor distribution independent of the label.
- Evidence anchors:
  - [section 3.2] "The input-gradients of Bayes optimal classifiers lie on the signal manifold ⇔ Bayes optimal classifiers are relative off-manifold robust."
  - [section 3.2] "The distractor is independent of the label, thus the distractor component is zero, and the input-gradient of the Bayes optimal model lies entirely on the signal manifold."
- Break condition: If no meaningful signal-distractor decomposition exists (e.g., all features are correlated with the label), the Bayes optimal gradients are not necessarily perceptually aligned.

### Mechanism 3
- Claim: Robust training objectives implicitly encourage off-manifold robustness because the cross-entropy loss on data samples biases the model toward robustness in directions orthogonal to the data manifold.
- Mechanism: The cross-entropy term applies only to on-manifold directions (data samples), while the robustness penalty (e.g., gradient norm regularization) applies equally in all directions; this imbalance makes off-manifold robustness easier to minimize.
- Core assumption: The model can be decomposed into on-manifold and off-manifold components, and these components are optimized independently by the training objective.
- Evidence anchors:
  - [section 3.3] "While robustness penalties itself are isotropic and do not prefer robustness in any direction, they are combined with the cross-entropy loss on data samples, which lie on the data manifold."
  - [section 4.1] "As we increase the importance of the robustness term in the training objective, the models become increasingly robust to off-manifold perturbations."
- Break condition: If the signal manifold and data manifold are identical, the bias toward off-manifold robustness disappears.

## Foundational Learning

- Concept: Manifold learning and tangent spaces
  - Why needed here: The paper's core mechanism relies on understanding how data lies on low-dimensional manifolds and how gradients project onto tangent spaces.
  - Quick check question: Given a point x on a k-dimensional manifold in d-dimensional space, what is the dimension of the tangent space at x?

- Concept: Signal-distractor decomposition
  - Why needed here: The distinction between signal and distractor distributions is crucial for explaining why Bayes optimal gradients and robust model gradients align with human perception.
  - Quick check question: If a masking function m(x) isolates the signal component, what property must the distractor distribution satisfy relative to the label y?

- Concept: Adversarial robustness and noise augmentation
  - Why needed here: The paper evaluates multiple robustness training methods, and understanding their mechanisms is essential for interpreting the experimental results.
  - Quick check question: How does gradient norm regularization relate to training with normal noise in the limit of infinite noise samples?

## Architecture Onboarding

- Component map: Data Preprocessing -> Manifold Estimation (Autoencoder) -> Robust Model Training -> Noise Generation -> Gradient Computation -> Perceptual Alignment Evaluation
- Critical path: Data → Manifold estimation → Robust model training → Noise perturbation → Gradient alignment measurement → Perceptual similarity evaluation
- Design tradeoffs: Using diffusion models for score estimation provides good perceptual alignment but is computationally expensive; simpler score estimators could be used for faster iteration but may reduce alignment accuracy
- Failure signatures: If the autoencoder fails to estimate the tangent space accurately, on- and off-manifold noise will be incorrectly generated, leading to misleading robustness measurements. If the LPIPS metric is not well-calibrated, perceptual alignment scores may not reflect true human perception.
- First 3 experiments:
  1. Train a standard Resnet18 on CIFAR-10 and measure on- and off-manifold robustness to verify that weakly robust models are not off-manifold robust.
  2. Train a Resnet18 with gradient norm regularization and varying λ, then measure perceptual alignment via LPIPS to find the bayes-aligned robustness regime.
  3. Create the MNIST with distractor dataset, train both standard and robust models, and measure relative noise robustness to confirm signal manifold alignment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical basis for why off-manifold robustness leads to on-manifold gradient alignment, and under what conditions does this equivalence break down?
- Basis in paper: [explicit] The paper states that off-manifold robustness leads to on-manifold gradient alignment in Theorem 1, but the proof relies on small noise levels and may not hold for larger perturbations or non-linear manifolds.
- Why unresolved: The paper only provides a theoretical proof for small noise levels and mentions that the equivalence may break down for larger noise or highly curved manifolds, but does not provide a rigorous analysis of these cases.
- What evidence would resolve it: A more general theoretical proof that characterizes the conditions under which off-manifold robustness implies on-manifold gradient alignment for larger noise levels and non-linear manifolds. Empirical studies that test the relationship between off-manifold robustness and on-manifold gradient alignment across a wider range of noise levels and manifold curvatures.

### Open Question 2
- Question: How does the signal-distractor decomposition affect the perceptual alignment of gradients in real-world datasets where such a decomposition may not be clean or may not exist?
- Basis in paper: [explicit] The paper assumes the existence of a signal-distractor decomposition and uses it to explain the perceptual alignment of gradients, but acknowledges that this decomposition may not exist for all datasets.
- Why unresolved: The paper does not provide a concrete method for identifying the signal and distractor components in real-world datasets, nor does it explore the implications of the absence of such a decomposition on the perceptual alignment of gradients.
- What evidence would resolve it: A method for automatically identifying the signal and distractor components in real-world datasets, along with empirical studies that test the relationship between the existence of a signal-distractor decomposition and the perceptual alignment of gradients.

### Open Question 3
- Question: What is the precise mechanism by which robust training objectives lead to off-manifold robustness, and why does this not occur in standard training?
- Basis in paper: [explicit] The paper provides two arguments for why robust models exhibit off-manifold robustness, but acknowledges that these arguments are not rigorous and that empirical evidence is the main support.
- Why unresolved: The paper does not provide a rigorous theoretical explanation for why robust training objectives lead to off-manifold robustness, nor does it explain why standard training does not produce this effect.
- What evidence would resolve it: A rigorous theoretical analysis of the relationship between robust training objectives and off-manifold robustness, along with empirical studies that compare the behavior of robust and standard models in terms of their off-manifold robustness and perceptual alignment of gradients.

## Limitations
- The empirical evidence for off-manifold robustness causing perceptual alignment relies heavily on synthetic MNIST-with-distractor experiments.
- The paper assumes the signal manifold is lower-dimensional than the data manifold, but doesn't verify this assumption rigorously for real datasets like CIFAR-10.
- The relationship between relative noise robustness and perceptual alignment, while strongly correlated in experiments, is not proven to be causative in all cases.

## Confidence
- High confidence in the theoretical connection between off-manifold robustness and on-manifold gradient alignment
- Medium confidence in the empirical demonstration across natural image datasets
- Medium confidence in the claim that Bayes optimal classifiers are off-manifold robust (relies on specific signal-distractor assumptions)

## Next Checks
1. **Natural Image Manifold Validation**: Use the MNIST-with-distractor experimental setup but replace the synthetic distractor with real image patches from other classes in CIFAR-10. Measure whether off-manifold robustness still correlates with perceptual alignment.

2. **Manifold Dimensionality Verification**: For CIFAR-10 and ImageNet, estimate the intrinsic dimensionality of both the full data manifold and the signal manifold (using techniques like two-photon microscopy analysis [39]). Verify that the signal manifold is indeed lower-dimensional.

3. **Robustness Break Condition Test**: Systematically vary the strength of the signal-distractor correlation in the synthetic dataset. Measure at what point the Bayes optimal classifier loses off-manifold robustness and whether perceptual alignment degrades accordingly.