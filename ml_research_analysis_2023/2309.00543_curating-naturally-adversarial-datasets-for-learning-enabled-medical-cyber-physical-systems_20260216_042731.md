---
ver: rpa2
title: Curating Naturally Adversarial Datasets for Learning-Enabled Medical Cyber-Physical
  Systems
arxiv_id: '2309.00543'
source_url: https://arxiv.org/abs/2309.00543
tags:
- labeling
- adversarial
- data
- datasets
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for curating datasets with natural
  adversarial examples to evaluate model robustness in medical and non-medical domains.
  The approach uses weakly-supervised labeling with labeling functions to generate
  probabilistic labels and confidence intervals for samples.
---

# Curating Naturally Adversarial Datasets for Learning-Enabled Medical Cyber-Physical Systems

## Quick Facts
- **arXiv ID**: 2309.00543
- **Source URL**: https://arxiv.org/abs/2309.00543
- **Reference count**: 40
- **Primary result**: Introduces a method for curating naturally adversarial datasets using weakly-supervised labeling to evaluate model robustness in medical and non-medical domains

## Executive Summary
This paper presents a method for curating naturally adversarial datasets to evaluate the robustness of machine learning models in medical and non-medical domains. The approach uses weakly-supervised labeling functions to generate probabilistic labels and confidence intervals for unlabeled samples. By ordering samples based on confidence interval lower bounds, the method creates progressively more adversarial datasets. The technique is evaluated on six medical and three non-medical case studies, demonstrating statistically valid adversarially ordered datasets in 10 out of 18 experiments. This approach addresses the critical need for realistic adversarial examples that capture real-world variations and uncertainties in data, which is essential for building trustworthy AI systems in healthcare applications.

## Method Summary
The method works by first applying labeling functions to unlabeled time-series data to obtain weak labels, then pruning dependent labeling functions based on correlation thresholds. Independent labeling functions are combined using either majority vote or generative models to produce probabilistic labels with confidence scores. Confidence intervals (using Clopper-Pearson method) are constructed around these labels to quantify uncertainty. Samples are then ordered by the lower bounds of their confidence intervals, creating a sequence of 10 progressively adversarial datasets. Statistical validity is assessed using Spearman's rank correlation between dataset adversarialness and accuracy, with valid datasets showing negative correlation with p-value ≤ 0.01.

## Key Results
- Method produces statistically valid adversarially ordered datasets in 10 out of 18 experiments
- Outperforms comparative approaches in creating challenging natural datasets
- Effective at capturing real-world challenging examples that can deceive models
- Demonstrates broad applicability across 6 medical and 3 non-medical case studies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Weakly-supervised labeling functions can identify natural adversarial examples by their low confidence scores.
- **Mechanism**: The method uses labeling functions to assign probabilistic labels with confidence scores to unlabeled data. Natural adversarial examples are identified as samples with low confidence scores, indicating high uncertainty in classification.
- **Core assumption**: Natural adversarial examples are inherently difficult to classify, resulting in low confidence scores from the probabilistic labeling process.
- **Evidence anchors**: [abstract]: "Natural adversarial examples capture the inherent variations and uncertainties present in real-world medical data that effectively deceive models." [section]: "We identify adversarial examples as those with high label uncertainty. As indication of label uncertainty, we will use label confidence scores."
- **Break condition**: If the probabilistic labeling process is poorly calibrated or if the labeling functions don't capture the true difficulty of classifying natural adversarial examples, this mechanism could fail.

### Mechanism 2
- **Claim**: Confidence intervals can quantify the uncertainty in probabilistic labels, making the adversarial ordering more reliable.
- **Mechanism**: The method constructs confidence intervals around the probabilistic labels to account for the unreliability of confidence scores from weak supervision techniques. The lower bounds of these intervals are used to order samples by their "natural adversarialness".
- **Core assumption**: The confidence intervals accurately reflect the true uncertainty in the probabilistic labels, with the lower bounds indicating the minimum possible confidence.
- **Evidence anchors**: [section]: "We quantify the potential inaccuracy in the estimated labels by providing confidence intervals. Our confidence intervals contain the likely true confidences in the estimated labels." [section]: "Our confidence interval lower bounds indicate the smallest possible certainty in the estimated label."
- **Break condition**: If the confidence intervals are poorly calibrated or if they don't accurately capture the true uncertainty in the labels, the adversarial ordering could be unreliable.

### Mechanism 3
- **Claim**: Independent labeling functions lead to more reliable probabilistic labels and confidence intervals.
- **Mechanism**: The method selects a subset of independent labeling functions to use in the probabilistic labeling process. This reduces bias and duplicate information that could arise from dependent labeling functions.
- **Core assumption**: Labeling functions are conditionally independent, and removing dependent functions improves the quality of the probabilistic labels.
- **Evidence anchors**: [section]: "Labeling functions can contradict each other or abstain in different combinations. Our second challenge is that probabilistic labelers are generally overconfident in their estimated labels." [section]: "We opt to use weakly-supervised data labeling techniques under the independent labeling function assumption. Therefore, it is crucial to identify a subset of the labeling functions Λ′ ⊆ Λ where the labeling functions are independent of each other."
- **Break condition**: If the method for selecting independent labeling functions is flawed or if the assumption of conditional independence doesn't hold, the quality of the probabilistic labels could suffer.

## Foundational Learning

- **Concept**: Weakly-supervised learning
  - **Why needed here**: The method relies on weak supervision techniques to generate probabilistic labels from noisy labeling functions, which is essential for identifying natural adversarial examples without expensive manual labeling.
  - **Quick check question**: How does weak supervision differ from traditional supervised learning, and why is it particularly useful for medical time-series data?

- **Concept**: Confidence intervals and statistical significance
  - **Why needed here**: Confidence intervals are used to quantify the uncertainty in the probabilistic labels, and statistical significance tests (Spearman's rank correlation) are used to validate the adversarial ordering of datasets.
  - **Quick check question**: How do confidence intervals help in assessing the reliability of probabilistic labels, and why is statistical significance important for validating the adversarial ordering?

- **Concept**: Spearman's rank correlation
  - **Why needed here**: Spearman's rank correlation is used to measure the strength and direction of the monotonic relationship between dataset adversarialness and accuracy, which is crucial for determining the statistical validity of the adversarial ordering.
  - **Quick check question**: What does a negative Spearman's rank correlation coefficient indicate in the context of adversarial ordering, and how does the p-value help in assessing the statistical significance?

## Architecture Onboarding

- **Component map**: Unlabeled dataset X and LFs Λ → LF pruning → Probabilistic labeling → Confidence intervals → Adversarial ordering → Datasets D1, ..., DN
- **Critical path**: Labeling Function Pruning → Probabilistic Labeling → Confidence Intervals → Adversarial Dataset Design
- **Design tradeoffs**:
  - Tradeoff between using all labeling functions (potentially more information but with dependencies) vs. using only independent functions (less information but potentially more reliable)
  - Tradeoff between using simple weak supervision techniques (e.g., majority vote) vs. more complex ones (e.g., generative models) in terms of calibration and reliability
  - Tradeoff between using narrower confidence intervals (more precise but potentially less reliable) vs. wider ones (less precise but potentially more reliable)
- **Failure signatures**:
  - Datasets with statistically significant increasing accuracy (violation of requirements)
  - Datasets with no statistically significant trend in accuracy
  - Poorly calibrated confidence intervals that don't accurately reflect the uncertainty in the labels
- **First 3 experiments**:
  1. Apply the method to a small, well-understood dataset (e.g., a simple medical time-series dataset) to verify that it produces the expected adversarial ordering.
  2. Compare the performance of the method using all labeling functions vs. only independent ones on a dataset with known dependencies.
  3. Evaluate the calibration of the confidence intervals by comparing them to the true accuracy of the labels on a dataset with known ground truth labels.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can we calibrate weakly-supervised data labeling techniques to provide more reliable confidence scores?
- **Basis in paper**: [explicit] The paper mentions that weak supervision techniques are poorly calibrated, with confidence scores not reflecting the true accuracy of labels, and that this is an area for future work.
- **Why unresolved**: While the paper proposes using confidence intervals to quantify uncertainty in probabilistic labels, this is a workaround rather than a solution to the underlying calibration issue. The effectiveness and limitations of this approach are not fully explored.
- **What evidence would resolve it**: Experiments comparing the performance of calibrated vs. uncalibrated weak supervision techniques on various datasets, along with analysis of the impact on downstream tasks like model robustness evaluation.

### Open Question 2
- **Question**: Can feature-based approaches, such as outlier detection or out-of-distribution detectors, be effectively combined with label uncertainty to identify natural adversarial examples?
- **Basis in paper**: [explicit] The paper mentions that natural adversarial examples typically have a different underlying data distribution than non-adversarial examples, and suggests that feature-based approaches could be explored in future work.
- **Why unresolved**: The paper focuses on selecting natural adversarial examples based on label uncertainty, leaving the investigation of feature-based approaches for future work. The potential benefits and limitations of combining these approaches are not explored.
- **What evidence would resolve it**: Experiments comparing the performance of label uncertainty-based, feature-based, and combined approaches for identifying natural adversarial examples, along with analysis of the impact on model robustness evaluation.

### Open Question 3
- **Question**: How can we extend weakly-supervised methods for evaluating additional properties of machine learning models beyond robustness?
- **Basis in paper**: [explicit] The paper mentions that developing weakly-supervised methods for evaluating additional properties of machine learning models is a direction for future work.
- **Why unresolved**: The paper focuses on using weakly-supervised methods for curating adversarial datasets to evaluate model robustness. The potential applications of these methods to other model properties, such as fairness or interpretability, are not explored.
- **What evidence would resolve it**: Examples of weakly-supervised methods applied to evaluate various model properties, along with experiments demonstrating their effectiveness compared to traditional evaluation methods.

## Limitations

- **LF quality dependency**: The method's effectiveness heavily depends on the quality and independence of the labeling functions, which are not fully specified in the paper
- **Statistical validation scope**: Spearman's rank correlation only captures monotonic relationships and may miss more complex patterns in dataset adversarialness
- **Calibration uncertainty**: The confidence intervals are used as a workaround for poorly calibrated weak supervision techniques, but their reliability across different domains is not fully established

## Confidence

- **High confidence**: The core mechanism of using confidence intervals for ordering samples is well-founded and mathematically sound
- **Medium confidence**: The LF pruning approach for selecting independent functions is theoretically justified but depends heavily on the quality of the initial LFs
- **Medium confidence**: The statistical validation framework using Spearman's correlation is appropriate but may not capture all relevant failure modes

## Next Checks

1. **LF Quality Assessment**: Conduct an ablation study removing different LFs to quantify their individual contributions to adversarial dataset quality, helping identify which LFs are most critical for capturing natural adversarial examples.

2. **Calibration Analysis**: Compare the confidence intervals against empirical accuracy on a held-out validation set to assess whether the intervals accurately reflect uncertainty and whether they are well-calibrated across different difficulty levels.

3. **Domain Transferability**: Test the method on a dataset from a different domain (e.g., image classification) to evaluate whether the approach generalizes beyond time-series medical data and maintains statistical validity.