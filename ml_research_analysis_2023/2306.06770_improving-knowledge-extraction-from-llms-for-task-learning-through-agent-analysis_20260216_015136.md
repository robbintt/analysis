---
ver: rpa2
title: Improving Knowledge Extraction from LLMs for Task Learning through Agent Analysis
arxiv_id: '2306.06770'
source_url: https://arxiv.org/abs/2306.06770
tags:
- goal
- task
- robot
- responses
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces STARS, a cognitive-agent approach that improves
  knowledge extraction from large language models (LLMs) for robotic task learning.
  STARS addresses key challenges in using LLMs for embodied agents by combining search
  tree generation of LLM responses, analysis and repair of unviable responses, and
  LLM-based selection of optimal responses.
---

# Improving Knowledge Extraction from LLMs for Task Learning through Agent Analysis

## Quick Facts
- arXiv ID: 2306.06770
- Source URL: https://arxiv.org/abs/2306.06770
- Reference count: 40
- Primary result: 77-94% task completion in one-shot learning without user oversight, reaching 100% with human confirmation

## Executive Summary
This paper introduces STARS, a cognitive-agent approach that improves knowledge extraction from large language models (LLMs) for robotic task learning. STARS addresses key challenges in using LLMs for embodied agents by combining search tree generation of LLM responses, analysis and repair of unviable responses, and LLM-based selection of optimal responses. The method achieves 77-94% task completion in one-shot learning without user oversight, reaching 100% completion when human confirmation is provided.

## Method Summary
STARS is a cognitive-agent approach that improves knowledge extraction from LLMs for robotic task learning by combining search tree generation, analysis and repair, and selection mechanisms. The method generates diverse LLM responses using beam search, validates them through internal simulation checking language interpretability, object grounding, and action affordances, repairs identified issues through targeted prompting, and selects optimal responses using LLM-based ranking. This framework significantly reduces human oversight requirements while achieving high task completion rates in simulated kitchen environments.

## Key Results
- 77-94% task completion rates in one-shot learning without user oversight
- 100% task completion when human confirmation is provided
- Significant reduction in human oversight requirements by shifting from natural language instruction to simple yes/no confirmation
- Effective mitigation of LLM limitations while leveraging cognitive agent reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Analysis and Repair prevents robot from using unviable LLM responses by proactively detecting mismatches.
- Mechanism: The robot simulates using each candidate response and checks for language interpretability, grounding to observable objects, and affordance feasibility before execution.
- Core assumption: Internal simulation using the robot's own language processor and object knowledge accurately predicts execution failures.
- Evidence anchors:
  - [abstract]: "Analysis and Repair detects and categorizes mismatches, drawing on the cognitive agent's knowledge and capabilities to identify problems, and then attempts to repair responses with identifiable mismatches."
  - [section 3.3]: "Analysis includes a linguistic evaluation (orange: whether the robot can parse and interpret the language and terms), a grounding analysis (purple: whether each referent in the response can be grounded to an object observable in the environment), and an analysis of affordances (green: whether the actions on objects implied by clauses in the goal response can be achieved by the robot)."
  - [corpus]: Weak - related papers discuss uncertainty estimation and multi-agent frameworks but don't directly address this specific proactive analysis mechanism.
- Break condition: If the robot's internal simulation model differs significantly from actual execution environment, mismatches will be missed and unviable responses may be used.

### Mechanism 2
- Claim: Search Tree generates diverse high-probability responses by recursively expanding low-probability tokens.
- Mechanism: Beam search recursively generates completions for tokens with log probability below 90%, expanding up to 3 levels of recursion with alternative tokens above 5% probability.
- Core assumption: Expanding low-probability tokens captures diverse viable responses that wouldn't appear with temperature-based sampling alone.
- Evidence anchors:
  - [section 3.2]: "Similar to others [9, 7], we enable the robot to use a beam-search strategy to generate a breadth of high probability responses from a single prompt."
  - [section B.3]: Detailed example showing beam search expanding tokens like 'dish', 'washer', 'and', 'turned' with alternative tokens and probabilities.
  - [corpus]: Weak - related papers discuss multi-agent approaches and reliability but don't specifically address this beam search mechanism for response diversity.
- Break condition: If threshold parameters (90% for initial, 5% for alternatives) are poorly chosen, search may generate too many irrelevant responses or miss viable ones.

### Mechanism 3
- Claim: LLM-based Selection chooses optimal response by evaluating all viable candidates in context.
- Mechanism: Robot constructs prompt with all viable candidates and asks LLM which is most reasonable for the task context, receiving single integer response.
- Core assumption: LLM can accurately rank candidate responses when provided with full context and comparison set.
- Evidence anchors:
  - [abstract]: "The robot constructs a prompt with the candidates and asks which is the most reasonable goal given task context."
  - [section B.6]: Example prompt showing how robot presents candidates to LLM for selection with single token response requested.
  - [section 5]: "Selection was not distinguishable from the baseline (mean log probability) choice strategy" indicating some limitations.
- Break condition: If LLM lacks context about robot's embodiment or environment, selection may favor human-centric responses over robot-viable ones.

## Foundational Learning

- Concept: Beam search and token probability analysis
  - Why needed here: Understanding how Search Tree generates diverse responses by expanding low-probability tokens
  - Quick check question: What thresholds control when beam search expands tokens vs. accepting them as final responses?

- Concept: Internal simulation for validation
  - Why needed here: Robot must simulate using responses before execution to catch mismatches in language, grounding, and affordances
  - Quick check question: How does the robot determine if a referent in a response can be grounded to an observable object?

- Concept: Prompt engineering for repair
  - Why needed here: Analysis identifies specific mismatches that require targeted repair prompts to the LLM
  - Quick check question: What three types of mismatches can be repaired through additional LLM prompting?

## Architecture Onboarding

- Component map: Search Tree → Analyze → Repair → Select → (Oversight) → Execution → Learning
  - First generate responses, then analyze and repair them, select best option, optionally confirm with user, then execute and learn

- Critical path: ST → AR → S → (O) → Execution → Learning
  - First generate responses, then analyze and repair them, select best option, optionally confirm with user, then execute and learn

- Design tradeoffs:
  - Token cost vs. response quality: More responses (ST) increases tokens but improves chances of finding viable ones
  - Analysis depth vs. speed: Thorough internal simulation catches more issues but takes longer
  - Repair attempts vs. efficiency: Multiple repair iterations can fix issues but may waste tokens on unrepairable responses

- Failure signatures:
  - High token usage with low task completion suggests poor search parameters or ineffective repairs
  - Many ungrounded objects indicates environment perception issues
  - Unknown words in responses suggests language model limitations or insufficient training data

- First 3 experiments:
  1. Compare task completion with/without Search Tree to measure impact of response diversity
  2. Test different beam search thresholds to optimize token usage vs. viable response yield
  3. Evaluate repair effectiveness by measuring how many mismatches are successfully fixed vs. abandoned

## Open Questions the Paper Calls Out

- How would STARS perform on tasks requiring more complex reasoning or planning beyond simple object placement?
- What is the impact of different prompt engineering strategies on STARS performance compared to the baseline template-based approach?
- How does the token cost of STARS scale with task complexity and number of objects?
- How robust is STARS to variations in the robot's perceptual capabilities and environment uncertainty?
- What is the optimal balance between search tree depth and token efficiency in STARS?

## Limitations
- Performance with human oversight (100% completion) vs. without (77-94%) shows the system still requires significant improvement for complete autonomy
- The study relies on simulated kitchen environments which may not capture the full complexity of real-world robotic interactions
- While the approach reduces user oversight from instruction-following to yes/no confirmation, it doesn't eliminate the need for human validation entirely

## Confidence

- **High Confidence**: The core mechanism of using beam search to generate diverse responses and LLM-based selection for choosing optimal candidates is well-supported by experimental results
- **Medium Confidence**: The analysis and repair mechanism's effectiveness is demonstrated, but the study doesn't fully explore edge cases where internal simulation might fail to predict execution problems
- **Medium Confidence**: The claim about reducing human oversight requirements is supported, but the comparison between different oversight levels could be more granular

## Next Checks

1. Test STARS in a real-world robotic environment with physical objects to validate performance outside simulation
2. Systematically vary beam search parameters (90% initial threshold, 5% alternative threshold) across different task types to identify optimal configurations
3. Conduct ablation studies removing individual components (Search Tree, Analysis/Repair, Selection) to quantify each mechanism's specific contribution to task completion rates