---
ver: rpa2
title: 'Think before you speak: Training Language Models With Pause Tokens'
arxiv_id: '2310.02226'
source_url: https://arxiv.org/abs/2310.02226
tags:
- pause
- tokens
- token
- finetuning
- downstream
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper explores delaying a language model\u2019s answer generation\
  \ by appending dummy <pause tokens to the input during training and inference. This\
  \ allows the model to process additional intermediate vectors before producing the\
  \ next token, potentially improving representation quality."
---

# Think before you speak: Training Language Models With Pause Tokens

## Quick Facts
- arXiv ID: 2310.02226
- Source URL: https://arxiv.org/abs/2310.02226
- Reference count: 40
- Key outcome: 18% EM score improvement on SQuAD with pause-pretrained models

## Executive Summary
This paper introduces delay training for language models by appending dummy <pause> tokens during both training and inference. The approach allows models to manipulate additional hidden vectors before generating each output token, potentially improving representation quality. Experiments on a 1B-parameter decoder-only model trained on C4 show significant gains on select downstream tasks when pauses are used consistently across pretraining and finetuning stages, though benefits vary substantially by task type.

## Method Summary
The method involves appending M learnable pause tokens to the input sequence during both pretraining and finetuning. During training, the model processes K+M intermediate vectors before outputting the (K+1)th token, with loss calculated only on non-pause tokens. Pause-pretraining randomly inserts pauses at 10% of sequence positions, while pause-finetuning appends fixed Mft pause tokens. At inference, Minf pause tokens are appended, with Mft tuned in {10, 50} and best downstream performance reported.

## Key Results
- 18% EM score improvement on SQuAD with PausePT PauseFT approach
- 8% improvement on CommonSenseQA when pauses used consistently across pretraining and finetuning
- 1% accuracy gain on GSM8k with optimal pause token count
- No improvement or degradation when pauses introduced only during finetuning (StdPT PauseFT)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pausing allows the model to perform more intermediate computations per layer, effectively increasing computational width
- Core assumption: Additional hidden vectors provide more information than standard fixed computations
- Evidence anchors: [abstract] models manipulate K+10 hidden vectors; [section] describes K+M intermediate vectors; related work suggests pauses improve reasoning
- Break condition: If pause tokens don't provide additional information and are skipped, mechanism fails

### Mechanism 2
- Claim: Pause-pretraining biases the model to utilize pauses effectively during inference
- Core assumption: Models learn to leverage pauses for better downstream performance
- Evidence anchors: [abstract] gains observed when both pre-trained and finetuned with delays; [section] explains pause-pretraining improves representation
- Break condition: If model doesn't learn to utilize pauses effectively or pauses distract from informative signals

### Mechanism 3
- Claim: Optimal number of pause tokens varies by task, indicating task-specific computational needs
- Core assumption: Different downstream tasks benefit from different numbers of pause tokens
- Evidence anchors: [section] shows GSM8k optimal at 10 pauses while SQuAD optimal differs; related studies vary pauses to find optimal performance
- Break condition: If model cannot adapt to different pause counts or optimal number is too high for practical use

## Foundational Learning

- Concept: Transformer architecture and attention mechanism
  - Why needed here: Understanding how Transformers process tokens is crucial for grasping how pauses affect computation
  - Quick check question: How does the attention mechanism allow each token to attend to all previous tokens?

- Concept: Causal language modeling and next-token prediction
  - Why needed here: The approach modifies standard next-token prediction by introducing pauses
  - Quick check question: In causal language modeling, how is the next token predicted based on previous tokens?

- Concept: Fine-tuning and transfer learning
  - Why needed here: Experiments involve fine-tuning pre-trained models on downstream tasks with and without pauses
  - Quick check question: What is the difference between pre-training and fine-tuning in language models?

## Architecture Onboarding

- Component map: Input layer -> Transformer layers -> Output layer (after last pause token) -> Loss function (excluding pause tokens)

- Critical path: 1. Append M pause tokens to input prefix, 2. Process extended sequence through Transformer layers, 3. Ignore outputs until last pause token, 4. Generate next token autoregressively

- Design tradeoffs: More pauses can improve performance but increase computation time; optimal pause count varies by task requiring hyperparameter tuning; pause-pretraining is crucial but adds to pre-training cost

- Failure signatures: No improvement or degradation with pauses; model ignores pause tokens; performance drops when inference pause count differs from finetuning count

- First 3 experiments: 1. Compare standard training vs. pause-training on SQuAD, 2. Vary pause tokens during finetuning to find optimal value, 3. Test robustness to different inference pause counts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different positions of pause tokens (appending vs prepending) affect performance on various downstream tasks?
- Basis in paper: [explicit] Section 5.3 compares appending and prepending pause tokens, finding appending generally more optimal
- Why unresolved: Only preliminary comparison provided without exploring underlying reasons for positional differences
- What evidence would resolve it: Comprehensive study varying pause token positions across diverse tasks with attention pattern analysis

### Open Question 2
- Question: What is the optimal number of pause tokens for different task types, and how does this scale with model size?
- Basis in paper: [explicit] Section 5.1 shows optimal pause count varies by dataset but doesn't explore systematic variation across task types or model sizes
- Why unresolved: Experiments only vary Mft between 10-50 for 1B model without exploring wider range or other model sizes
- What evidence would resolve it: Large-scale study varying Mft across wider range and multiple model sizes correlating with task characteristics

### Open Question 3
- Question: Why does pause-pretraining appear crucial for downstream gains, and what representations or biases does it induce?
- Basis in paper: [explicit] Section 4.3 notes PausePT PauseFT outperforms StdPT PauseFT but doesn't fully explain why
- Why unresolved: Paper lacks rigorous analysis of how pause-pretraining affects representations or why standard pretraining hinders inference-time pauses
- What evidence would resolve it: Detailed analysis of model representations comparing pause-pretrained and standard-pretrained models with targeted interventions

## Limitations

- Inconsistent benefits across tasks - significant improvements on only 3 of 9 downstream tasks tested
- Computational efficiency claims are qualified - task-specific optimization requirements may offset parameter savings
- Mechanism remains incompletely explained - unclear whether pause tokens provide meaningful computation or are simply skipped

## Confidence

**High Confidence**: Experimental methodology is sound and results are reproducible; clearly demonstrates task-specific improvements with consistent pause usage across pretraining and finetuning

**Medium Confidence**: Computational benefits claim is partially supported but inconsistent task performance and need for extensive hyperparameter tuning suggests more limited advantages than implied

**Low Confidence**: Mechanistic explanation for why pause tokens work remains speculative; lacks empirical validation of core assumption that additional hidden vectors translate to richer representations

## Next Checks

1. **Ablation study on pause token utility**: Measure whether pause tokens are actually utilized by analyzing attention patterns and hidden state changes during pause token processing to determine if they contribute meaningful computation

2. **Cross-task transfer analysis**: Systematically vary pause token counts across all nine downstream tasks to map relationship between task complexity and optimal pause numbers, validating claims about task-specific computational needs

3. **Mechanism isolation test**: Compare pause tokens against alternative computation extension methods like increasing hidden dimension size or adding attention heads to determine if benefits come specifically from pause token approach or general principle of extended computation time