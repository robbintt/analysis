---
ver: rpa2
title: Improving Zero-Shot Text Matching for Financial Auditing with Large Language
  Models
arxiv_id: '2308.06111'
source_url: https://arxiv.org/abs/2308.06111
tags:
- text
- financial
- segments
- requirement
- relevant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of matching text segments from
  financial reports to legal requirements in auditing. The authors propose ZeroShotALI,
  a system that combines a domain-adapted SentenceBERT model with GPT-4 to retrieve
  and filter relevant text segments.
---

# Improving Zero-Shot Text Matching for Financial Auditing with Large Language Models

## Quick Facts
- arXiv ID: 2308.06111
- Source URL: https://arxiv.org/abs/2308.06111
- Reference count: 20
- Key outcome: ZeroShotALI achieves 57.62% mean sensitivity, 44.65% MAP, and 30.57% F1 score, outperforming plain SentenceBERT and other baselines

## Executive Summary
This paper addresses the challenge of matching text segments from financial reports to legal requirements in auditing. The authors propose ZeroShotALI, a system that combines a domain-adapted SentenceBERT model with GPT-4 to retrieve and filter relevant text segments. ZeroShotALI significantly outperforms other methods, including a plain SentenceBERT model, vector database retrieval models, and vector database models combined with GPT-3.5 Turbo or GPT-4. The authors attribute the superior performance of ZeroShotALI to the domain-specific fine-tuning of SentenceBERT and the use of GPT-4 for filtering.

## Method Summary
ZeroShotALI combines a domain-adapted SentenceBERT model with GPT-4 filtering. First, SentenceBERT retrieves top 15 relevant segments per requirement based on cosine similarity; then GPT-4 filters these to top 5. The system is evaluated on 10 IFRS-compliant financial reports with 7097 text segments, annotated by auditors to map text segments to 1214 IFRS requirements. Performance is measured using mean sensitivity, mean average precision (MAP), and F1 score with K=5 recommendations per requirement.

## Key Results
- ZeroShotALI achieves 57.62% mean sensitivity, 44.65% MAP, and 30.57% F1 score
- Outperforms plain SentenceBERT (52.12% sensitivity, 39.00% MAP, 27.69% F1)
- Closed-format prompts for GPT-4 yield better performance than open-ended formats

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuned SentenceBERT with domain-specific training improves text matching for financial auditing.
- Mechanism: The SentenceBERT model is trained on domain-specific data from financial auditing tasks, allowing it to learn the nuances and terminology of financial reports and accounting standards. This enables more accurate semantic matching between report segments and legal requirements.
- Core assumption: Domain-specific fine-tuning is necessary to capture the specialized vocabulary and context of financial auditing.
- Evidence anchors:
  - [abstract]: "We present ZeroShotALI, a novel recommender system that leverages a state-of-the-art large language model (LLM) in conjunction with a domain-specifically optimized transformer-based text-matching solution."
  - [section]: "The SentenceBERT model was fine-tuned specifically for the task of retrieving semantically similar text passages within an auditing context."
  - [corpus]: Weak evidence - no direct mention of domain-specific fine-tuning in corpus neighbors.
- Break condition: If the SentenceBERT model is not fine-tuned on domain-specific data, the matching performance may degrade significantly.

### Mechanism 2
- Claim: GPT-4 filtering improves the relevance of the top-K recommendations.
- Mechanism: After the SentenceBERT model retrieves the top 15 most relevant segments, GPT-4 is used to further filter and select the top 5 most relevant segments based on the legal requirement. This two-step process refines the recommendations and improves overall performance.
- Core assumption: GPT-4 can effectively filter and rank the retrieved segments based on their relevance to the legal requirement.
- Evidence anchors:
  - [abstract]: "We find that a two-step approach of first retrieving a number of best matching document sections per legal requirement with a custom BERT-based model and second filtering these selections using an LLM yields significant performance improvements over existing approaches."
  - [section]: "To further improve the final matching performance, we augment the described SentenceBERT model with the state-of-the-art generative language model, GPT-4."
  - [corpus]: Weak evidence - no direct mention of GPT-4 filtering in corpus neighbors.
- Break condition: If GPT-4 is unable to effectively filter the retrieved segments, the performance gains from the two-step approach may be minimal.

### Mechanism 3
- Claim: The closed-format prompt design for GPT-4 yields improved performance compared to open-ended formats.
- Mechanism: When prompting GPT-4 for filtering, restricting the output to the IDs of the top 5 most relevant segments (closed format) results in better performance than allowing the model to provide explanations (open-ended format).
- Core assumption: The closed format provides clearer and more concise instructions to GPT-4, leading to more focused and relevant outputs.
- Evidence anchors:
  - [section]: "Interestingly, we find that the 'closed' format yields improved performance compared to the 'open-ended' format."
  - [corpus]: Weak evidence - no direct mention of prompt design in corpus neighbors.
- Break condition: If the closed format is not more effective than the open-ended format, the performance gains from the prompt design may be limited.

## Foundational Learning

- Concept: Text matching using semantic similarity
  - Why needed here: The core task is to match relevant text segments from financial reports to legal requirements based on semantic similarity.
  - Quick check question: What is the difference between semantic similarity and lexical similarity, and why is semantic similarity more important for this task?

- Concept: Transformer-based language models
  - Why needed here: Both SentenceBERT and GPT-4 are transformer-based models that can capture complex relationships and semantics in text.
  - Quick check question: How do transformer-based models differ from traditional recurrent neural networks in handling long-range dependencies and parallel processing?

- Concept: Prompt engineering for language models
  - Why needed here: The effectiveness of GPT-4 in filtering depends on the design of the prompts used to guide its output.
  - Quick check question: What are the key factors to consider when designing prompts for language models, and how can they impact the quality and relevance of the outputs?

## Architecture Onboarding

- Component map: Financial report text segments and legal requirements -> SentenceBERT encoding -> Cosine similarity computation -> Top 15 retrieval -> GPT-4 filtering -> Top 5 recommendations

- Critical path:
  1. Encode text segments and requirements using SentenceBERT
  2. Compute cosine similarity scores between segments and requirements
  3. Retrieve the top 15 most relevant segments for each requirement
  4. Prompt GPT-4 with the requirement and the top 15 segments
  5. Filter and rank the segments using GPT-4
  6. Select the top 5 most relevant segments as the final recommendations

- Design tradeoffs:
  - Using a two-step approach (SentenceBERT + GPT-4) vs. a single-step approach (e.g., using GPT-4 alone)
  - Tradeoff between computational efficiency (two-step approach) and potential performance gains (single-step approach)
  - Impact of prompt design on GPT-4's filtering effectiveness

- Failure signatures:
  - Low sensitivity or precision scores
  - Irrelevant or off-topic segments being recommended
  - Inconsistent performance across different requirements or report segments

- First 3 experiments:
  1. Compare the performance of ZeroShotALI with and without GPT-4 filtering on a subset of the data.
  2. Evaluate the impact of different prompt designs (closed vs. open-ended) on GPT-4's filtering effectiveness.
  3. Analyze the performance of ZeroShotALI on different types of legal requirements (e.g., simple vs. complex) to identify potential weaknesses or areas for improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would ZeroShotALI perform with different SentenceBERT models fine-tuned on various financial datasets or accounting standards beyond IFRS?
- Basis in paper: [explicit] The paper discusses using a domain-adapted SentenceBERT model fine-tuned for auditing tasks, suggesting potential for exploring other fine-tuned models.
- Why unresolved: The paper only evaluates one specific SentenceBERT model fine-tuned for IFRS compliance. Testing with models fine-tuned on different financial datasets or accounting standards could reveal performance variations and generalizability.
- What evidence would resolve it: Experimental results comparing ZeroShotALI's performance using different SentenceBERT models fine-tuned on various financial datasets or accounting standards, measuring sensitivity, MAP, and F1 scores across these variations.

### Open Question 2
- Question: What is the impact of different LLM filtering strategies (e.g., chain-of-thought, tree-of-thoughts) on ZeroShotALI's performance?
- Basis in paper: [explicit] The paper mentions future work exploring advanced prompt tuning methodologies like Chain-of-Thought and Tree-of-Thoughts approaches, indicating potential performance improvements.
- Why unresolved: The paper only uses a simple filtering approach with GPT-4 and doesn't explore more advanced LLM strategies that could potentially enhance performance.
- What evidence would resolve it: Comparative experiments showing performance metrics (sensitivity, MAP, F1) of ZeroShotALI using different LLM filtering strategies, including chain-of-thought and tree-of-thoughts approaches, against the current simple filtering method.

### Open Question 3
- Question: How does ZeroShotALI's performance compare to human auditors in terms of accuracy and efficiency?
- Basis in paper: [inferred] The paper discusses the tedious and time-consuming nature of manual auditing, suggesting a need for AI-based solutions like ZeroShotALI to improve the process.
- Why unresolved: While the paper demonstrates ZeroShotALI's superior performance compared to other AI methods, it doesn't provide a direct comparison with human auditors, which would be valuable for understanding its practical utility.
- What evidence would resolve it: A study comparing ZeroShotALI's accuracy (sensitivity, MAP, F1 scores) and efficiency (time taken to complete the task) against a group of human auditors performing the same matching task on financial reports and legal requirements.

## Limitations

- Data source dependency: Results depend on the quality and representativeness of the 10 IFRS-compliant financial reports used, which may not capture full real-world variability.
- Model access constraints: GPT-4 is proprietary and expensive, and performance gains may not be replicable with open-source alternatives.
- Implementation specificity: Exact SentenceBERT fine-tuning procedure and domain adaptation data are referenced but not fully detailed, making exact replication challenging.

## Confidence

- High confidence: Domain-specific fine-tuning improves text matching performance (sensitivity improved from 52.12% to 57.62%)
- Medium confidence: GPT-4 filtering significantly improves top-K recommendation relevance
- Low confidence: Closed-format prompts yield superior performance compared to open-ended formats (lacks broader validation)

## Next Checks

1. **Ablation study**: Remove the GPT-4 filtering step while keeping all other components identical to quantify the exact contribution of the LLM filtering to overall performance.

2. **Cross-dataset validation**: Test ZeroShotALI on financial reports from different accounting frameworks (e.g., US GAAP instead of IFRS) or from different industries to assess whether the domain adaptation generalizes beyond the original training data.

3. **Cost-benefit analysis**: Measure the computational cost (API calls, latency) of the two-step approach versus single-step alternatives, and calculate the break-even point where the performance gains justify the additional expense and complexity.