---
ver: rpa2
title: 'FAIRLABEL: Correcting Bias in Labels'
arxiv_id: '2311.00638'
source_url: https://arxiv.org/abs/2311.00638
tags:
- data
- bias
- label
- datasets
- minority
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces FAIRLABEL, an algorithm to detect and correct
  bias in labels within machine learning datasets. Unlike existing fairness approaches
  that assume unbiased ground truth, FAIRLABEL addresses the issue of biased labels
  in real-world data by proposing two complementary processes: FAIRMIN for correcting
  minority group labels and FAIRMAJ for majority group labels.'
---

# FAIRLABEL: Correcting Bias in Labels

## Quick Facts
- arXiv ID: 2311.00638
- Source URL: https://arxiv.org/abs/2311.00638
- Reference count: 31
- Key outcome: FAIRLABEL detects and corrects bias in labels by flipping labels in minority and majority groups, improving Disparate Impact by up to 54.2% and Correct Flip Rate to 86.7%.

## Executive Summary
FAIRLABEL addresses the critical problem of biased labels in machine learning datasets, which existing fairness approaches typically assume don't exist. The algorithm proposes two complementary processes: FAIRMIN corrects minority group labels by flipping 0→1 where a majority-trained classifier predicts 1, while FAIRMAJ corrects majority group labels by flipping 1→0 where a minority-trained classifier predicts 0. Experiments on synthetic and benchmark datasets demonstrate significant improvements in fairness metrics while maintaining or improving prediction accuracy.

## Method Summary
FAIRLABEL is an algorithm that detects and corrects bias in labels within machine learning datasets. It operates by first training a classifier on the majority group, then using this classifier to flip minority group labels from 0→1 where the model predicts 1 (FAIRMIN process). Optionally, it trains a classifier on the minority group and flips majority group labels from 1→0 where the model predicts 0 (FAIRMAJ process). The algorithm assumes known directional bias (minority group receives negative bias, majority receives positive bias) and uses synthetic data generation with controlled bias injection to validate its effectiveness by measuring Correct Flip Rate and Missed Flip Rate against ground truth.

## Key Results
- FAIRLABEL increases Disparate Impact Ratio by up to 54.2% on benchmark datasets
- Achieves Correct Flip Rate of 86.7% versus 71.9% for baseline model
- Improves F1-score from 0.78 to 0.88 on synthetic datasets while reducing bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The bias correction works because it exploits asymmetric directional bias — minority group receives negative bias, majority receives positive bias.
- Mechanism: The algorithm trains a classifier on the majority group, then flips minority labels from 0→1 where the model predicts 1, correcting for systematic under-recognition of qualified minority candidates. It optionally trains on minority group and flips majority labels from 1→0 where model predicts 0, correcting for over-recognition of majority candidates.
- Core assumption: The direction of bias is known and consistent (i.e., minority group is systematically disadvantaged).
- Evidence anchors:
  - [abstract] "Unlike existing fairness approaches that assume unbiased ground truth, FAIRLABEL addresses the issue of biased labels in real-world data by proposing two complementary processes: FAIRMIN for correcting minority group labels and FAIRMAJ for majority group labels."
  - [section] "The intuition behind FAIR LABEL is simple. In the minority class, a biased decision occurs when the decision maker makes a negative decision despite the person having the requisite qualifications. The decision maker in this case would make the opposite decision if the person belonged to the majority class."
- Break condition: If bias direction is unknown or non-existent, the algorithm may introduce more noise than it corrects, leading to degraded model performance.

### Mechanism 2
- Claim: The two-stage debiasing (FAIRMIN then FAIRMAJ) increases Disparate Impact Ratio (DIR) while maintaining or improving F1-score.
- Mechanism: By correcting labels in both directions, the algorithm balances the predicted positive rates between groups. The paper reports up to 54.2% increase in DIR and F1-score improvement from 0.78 to 0.88 on synthetic datasets.
- Core assumption: The debiased dataset, when used to train a new model, will produce predictions with reduced disparity between groups.
- Evidence anchors:
  - [abstract] "Experiments on synthetic and benchmark datasets... show that FAIRLABEL increases DI by up to 54.2% and achieves a Correct Flip Rate (CFR) of 86.7% versus 71.9% for a baseline model."
  - [section] "We ran F AIR LABEL on the datasets Adult, German Credit Risk, and Compass... Across each dataset, the DIR improved from +0.356 (UCI) to +0.542 (Compas), showing that F AIR LABEL has the ability to reduce disparity between groups."
- Break condition: If the original data has very low bias or the bias direction is opposite to what the algorithm assumes, DIR may not improve and could even decrease.

### Mechanism 3
- Claim: The synthetic data generation framework enables controlled validation of the debiasing algorithm by injecting known bias.
- Mechanism: The framework generates clean data, adds noise independent of protected attributes, then injects systematic label noise (bias) that depends on the protected attribute. This allows measurement of Correct Flip Rate (CFR) and Missed Flip Rate (MFR) against ground truth.
- Core assumption: The synthetic bias injection accurately mimics real-world bias patterns.
- Evidence anchors:
  - [section] "By injecting bias into synthetic data, we have the ability to track where the bias was added, an attribute of the synthetic data that is not possible in real-world datasets."
  - [section] "We validate the label flipping using synthetically generated data... We quantify noise as ϵ. Bias can be considered noise except that bias is based on the protected attribute and is unidirectional."
- Break condition: If the synthetic bias model does not reflect real-world bias mechanisms, the validation results may not generalize to actual applications.

## Foundational Learning

- Concept: Disparate Impact (DI) and Demographic Parity
  - Why needed here: These are the fairness metrics used to evaluate whether the debiased model treats groups equally. DI is the ratio of positive prediction rates between minority and majority groups.
  - Quick check question: If a model predicts positive for 40% of the majority group and 20% of the minority group, what is the Disparate Impact Ratio? (Answer: 0.5)

- Concept: Correct Flip Rate (CFR) and Missed Flip Rate (MFR)
  - Why needed here: These metrics measure the quality of the debiasing process by comparing flipped labels against known ground truth in synthetic data.
  - Quick check question: In a dataset with 100 biased labels, if the algorithm correctly flips 80 and misses 10, what are the CFR and MFR? (Answer: CFR = 80%, MFR = 10%)

- Concept: Synthetic data generation with controlled bias injection
  - Why needed here: Real-world datasets lack ground truth for biased labels, so synthetic data with known bias is essential for validating the debiasing algorithm.
  - Quick check question: What are the three steps in the synthetic data generation process described in the paper? (Answer: Generate clean data, add independent noise, inject systematic bias based on protected attribute)

## Architecture Onboarding

- Component map:
  Data ingestion → Synthetic data generator (optional) → FAIRMIN module → FAIRMAJ module → Debiased dataset → Model training → Fairness evaluation

- Critical path:
  1. Split data by protected attribute into majority and minority groups
  2. Train classifier on majority group only
  3. Predict on minority group and flip labels where conditions met (FAIRMIN)
  4. Train classifier on minority group only
  5. Predict on majority group and flip labels where conditions met (FAIRMAJ)
  6. Concatenate debiased groups into final dataset

- Design tradeoffs:
  - Using separate classifiers for each group ensures the model learns patterns specific to that group's distribution, but may require more computation and data.
  - Flipping labels in only one direction (0→1 for minority, 1→0 for majority) assumes known bias direction; bidirectional flipping could be more general but risk over-correction.
  - Synthetic data validation provides ground truth but may not capture all real-world bias complexities.

- Failure signatures:
  - If CFR is low and MFR is high, the algorithm is missing many biased labels or flipping too many correct ones.
  - If DIR does not improve after debiasing, the bias may be in features rather than labels, or the algorithm assumptions may not hold.
  - If F1-score drops significantly, the label flipping is introducing too much noise relative to bias correction.

- First 3 experiments:
  1. Run FAIRLABEL on a small synthetic dataset with known bias injection rate of 0.1, measure CFR, MFR, and DIR before/after.
  2. Compare FAIRLABEL to the Naive baseline (flip minority labels based on full-data model) on the same synthetic data, record F1-score difference.
  3. Apply FAIRLABEL to the UCI Adult dataset, use aif360 to compute DIR on the original vs. debiased training data, verify improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the synthetic data generation framework be extended to handle more complex data distributions and dependencies?
- Basis in paper: [explicit] The paper mentions that synthetic data generation has been used to mitigate the problem of bias in real-world datasets, but these methods care more about privacy than bias. The authors propose a synthetic data generation framework to validate FAIRLABEL in which they inject bias into the synthetic data and measure the algorithm's ability to find and correct the bias.
- Why unresolved: The paper only provides a basic example of synthetic data generation using a linear model. Real-world data often exhibits more complex patterns and dependencies that may not be captured by simple models.
- What evidence would resolve it: Extending the synthetic data generation framework to include more complex data distributions, such as those with non-linear relationships, interactions between features, and multi-modal distributions, would provide a more comprehensive evaluation of FAIRLABEL's performance.

### Open Question 2
- Question: How can FAIRLABEL be adapted to handle fairness issues in non-tabular data, such as text and images?
- Basis in paper: [explicit] The paper mentions that the authors' goal is to extend FAIRLABEL to other modalities like text and images.
- Why unresolved: The current implementation of FAIRLABEL is designed for tabular data. Adapting it to handle fairness issues in non-tabular data would require significant modifications to the algorithm.
- What evidence would resolve it: Developing and testing FAIRLABEL on non-tabular datasets, such as text and image datasets, would demonstrate its effectiveness in addressing fairness issues in these modalities.

### Open Question 3
- Question: How can FAIRLABEL be integrated with existing fairness-aware machine learning algorithms?
- Basis in paper: [explicit] The paper mentions that there are several algorithms for measuring fairness of ML models, and FAIRLABEL is complementary to these approaches by addressing bias in labels.
- Why unresolved: The paper does not discuss how FAIRLABEL can be integrated with existing fairness-aware ML algorithms. Combining FAIRLABEL with other fairness techniques could potentially lead to more robust and effective fairness mitigation.
- What evidence would resolve it: Conducting experiments that combine FAIRLABEL with other fairness-aware ML algorithms and comparing their performance would provide insights into the potential benefits and limitations of such integrations.

## Limitations
- The algorithm assumes known directional bias, which may not hold in all real-world scenarios.
- Synthetic data validation may not capture the full complexity of real-world bias patterns.
- The paper lacks detailed implementation specifications for synthetic data generation and threshold determination.

## Confidence

**Mechanism 1 (asymmetric directional bias correction):** Medium - The theoretical framework is sound, but validation is limited to synthetic data where ground truth is known.

**Mechanism 2 (two-stage debiasing improving DIR and F1):** Medium - Results show improvements, but the magnitude may be dataset-dependent and not generalizable.

**Mechanism 3 (synthetic data validation framework):** Low - While the approach is valid, the paper doesn't demonstrate that synthetic bias patterns reflect real-world scenarios.

## Next Checks

1. Apply FAIRLABEL to a real-world dataset with documented label bias (e.g., hiring decisions) and measure both fairness metrics and actual decision outcomes.

2. Test the algorithm's robustness by applying it to datasets where bias direction is unknown or varies across subgroups.

3. Compare FAIRLABEL's performance against other debiasing approaches that address feature bias or model bias, not just label bias.