---
ver: rpa2
title: Generalised Scale-Space Properties for Probabilistic Diffusion Models
arxiv_id: '2303.07900'
source_url: https://arxiv.org/abs/2303.07900
tags:
- usion
- probabilistic
- image
- osmosis
- scale-space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates probabilistic diffusion models from a scale-space
  perspective, establishing generalised scale-space properties on evolving probability
  distributions. The author shows that probabilistic diffusion models satisfy properties
  analogous to classical scale-spaces, including a training data distribution as initial
  state, semigroup property, increasing differential and conditional entropy as Lyapunov
  sequences, permutation invariance, and convergence to a standard normal distribution
  as steady state.
---

# Generalised Scale-Space Properties for Probabilistic Diffusion Models

## Quick Facts
- arXiv ID: 2303.07900
- Source URL: https://arxiv.org/abs/2303.07900
- Reference count: 30
- Primary result: Probabilistic diffusion models satisfy scale-space properties including semigroup property, increasing entropy, and convergence to normal distribution

## Executive Summary
This paper establishes that probabilistic diffusion models satisfy generalised scale-space properties on evolving probability distributions. The author demonstrates that these models preserve key scale-space characteristics including initial state matching, semigroup property, increasing differential and conditional entropy, permutation invariance, and convergence to a standard normal distribution. The work bridges deep learning approaches with classical model-based scale-space methods, providing a theoretical foundation that connects probabilistic diffusion to deterministic osmosis filters through drift-diffusion concepts.

## Method Summary
The paper uses a Markov process formulation where training images are gradually corrupted with Gaussian noise over discrete time steps. The forward process maps the unknown training data distribution to a simple standard normal distribution through a series of Gaussian transitions. The method proves theoretical properties about the evolving probability distributions and establishes connections to partial differential equations via the Fokker-Planck formulation. The approach is validated through theoretical analysis rather than empirical experiments.

## Key Results
- Probabilistic diffusion models satisfy scale-space properties including semigroup property and convergence to steady state
- Differential entropy increases monotonically during the diffusion process, acting as Lyapunov sequence
- The forward and backward processes are connected through Fokker-Planck equations
- Probabilistic diffusion models share conceptual similarities with deterministic osmosis filters while maintaining distinct stochastic characteristics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Probabilistic diffusion models satisfy scale-space properties on evolving probability distributions
- Mechanism: The Markov process defining the diffusion maps training data distribution through noise addition steps, preserving semigroup property and increasing entropy
- Core assumption: The transition probabilities are Gaussian and satisfy Markov property
- Evidence anchors:
  - [abstract] "we investigate probabilistic diffusion models from the viewpoint of scale-space research and show that they fulfil generalised scale-space properties on evolving probability distributions"
  - [section 3.1] "Probabilistic diffusion maps this unknown p(F) to a simple, well-known distribution such as the standard normal distribution"
- Break condition: If transition probabilities deviate from Gaussian assumptions or Markov property fails

### Mechanism 2
- Claim: Increasing differential entropy acts as Lyapunov sequence
- Mechanism: Adding Gaussian noise at each step increases the differential entropy of the evolving distribution, measuring information content growth
- Core assumption: Beta parameters satisfy constraints in Eq. (10)
- Evidence anchors:
  - [section 3.1] "the differential entropy H(Ui) increases with ti under the following assumptions for βi"
  - [section 3.1] "the differential entropy H(Ui) := −∫ p(ui) ln p(ui) dui (9) increases with ti"
- Break condition: If beta parameters violate constraints or noise addition doesn't follow specified form

### Mechanism 3
- Claim: The forward and backward processes are connected via Fokker-Planck equations
- Mechanism: The forward process solves a drift-diffusion PDE, while the backward process solves the corresponding backward equation with learned parameters
- Core assumption: Feller's conditions on stochastic moments are satisfied
- Evidence anchors:
  - [section 3.2] "Feller [5] has shown a connection of a Markov process... if the stochastic moments mk(ut,t) exist for k∈{1, 2}"
  - [section 3.2] "probabilistic diffusion can also be expressed in a PDE formulation"
- Break condition: If stochastic moments don't exist or PDE formulation breaks down

## Foundational Learning

- Concept: Markov processes and transition probabilities
  - Why needed here: The entire diffusion model framework relies on Markov property and Gaussian transitions
  - Quick check question: Can you explain why p(ui|ui-1,...,u0) = p(ui|ui-1) is crucial for the model?

- Concept: Entropy and information theory
  - Why needed here: Lyapunov sequences are defined using differential and conditional entropy to measure information content
  - Quick check question: How does adding Gaussian noise affect the differential entropy of a distribution?

- Concept: Partial differential equations and Fokker-Planck equation
  - Why needed here: The connection between Markov processes and PDEs provides the mathematical foundation for the reverse process
  - Quick check question: What is the relationship between the forward and backward Fokker-Planck equations?

## Architecture Onboarding

- Component map:
  - Forward process: Markov chain with Gaussian transitions
  - Reverse process: Learned denoising network
  - Entropy calculation: Numerical integration for differential entropy
  - PDE solver: For theoretical analysis and comparison

- Critical path: Training data → forward process (Markov chain) → noise distribution → reverse process (learned network) → generated samples

- Design tradeoffs:
  - Number of diffusion steps vs. quality: More steps → better quality but higher computational cost
  - Beta schedule: Linear vs. cosine vs. learned schedules affect convergence and stability
  - Network architecture: Depth and width affect generation quality and training time

- Failure signatures:
  - Mode collapse: Training data distribution not properly learned
  - Mode mixing: Generated samples don't match training data characteristics
  - Instability: Numerical issues in entropy calculation or PDE solving

- First 3 experiments:
  1. Implement forward process with simple linear beta schedule and verify semigroup property
  2. Calculate differential entropy at each step and verify monotonic increase
  3. Compare generated samples from reverse process with training data distribution using statistical tests

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can probabilistic diffusion models leverage their scale-space properties for tasks beyond generating final images, such as progressive image analysis or multi-scale feature extraction?
- Basis in paper: [explicit] The paper discusses that probabilistic diffusion models constitute a scale-space and mentions potential applications that make use of intermediate results of the evolution instead of only relying on steady states.
- Why unresolved: The paper focuses on establishing the theoretical connection between probabilistic diffusion and scale-spaces but does not explore practical applications that exploit the intermediate scales of the diffusion process.
- What evidence would resolve it: Empirical studies demonstrating improved performance in tasks like image denoising, segmentation, or feature extraction when utilizing intermediate diffusion states, compared to using only the final generated image.

### Open Question 2
- Question: What are the precise mathematical relationships between the Lyapunov sequences in probabilistic diffusion (differential entropy, conditional entropy) and the Lyapunov sequence in osmosis filtering (relative entropy with respect to the steady state)?
- Basis in paper: [explicit] The paper establishes Lyapunov sequences for probabilistic diffusion (increasing differential and conditional entropy) and mentions the increasing relative entropy for osmosis filtering, noting their conceptual similarity.
- Why unresolved: While the paper highlights the conceptual resemblance, it does not provide a rigorous mathematical comparison or unification of these Lyapunov sequences across the two approaches.
- What evidence would resolve it: A formal mathematical framework that relates the entropy-based Lyapunov sequences in probabilistic diffusion to the relative entropy in osmosis, potentially revealing deeper connections or generalizations.

### Open Question 3
- Question: Can the deterministic nature of osmosis filtering be effectively combined with the stochastic nature of probabilistic diffusion to create hybrid models that benefit from both approaches?
- Basis in paper: [inferred] The paper extensively compares and contrasts probabilistic diffusion and osmosis filtering, highlighting their similarities (drift-diffusion foundation) and differences (stochastic vs. deterministic, applications). This suggests potential for hybrid approaches.
- Why unresolved: The paper focuses on analyzing the two approaches separately and does not explore the possibility of combining their strengths in a unified model.
- What evidence would resolve it: Development and evaluation of hybrid models that integrate deterministic osmosis-like components with stochastic diffusion processes, demonstrating improved performance or new capabilities compared to pure probabilistic diffusion or osmosis models.

## Limitations
- Scale-space connection formalization: The precise mathematical relationship between diffusion models and classical scale-space theory remains somewhat informal
- Practical implications: Limited empirical validation of how theoretical scale-space properties translate to practical improvements
- Reverse process specifics: The paper mentions the reverse process but doesn't fully specify how it leverages scale-space properties

## Confidence

- High confidence: Mathematical proofs showing probabilistic diffusion satisfies semigroup property, initial state matching, and convergence to normal distribution
- Medium confidence: Entropy increase claims depend on specific β parameter constraints that may be challenging to satisfy in practice
- Medium confidence: Permutation invariance property is theoretically valid but may be difficult to verify empirically due to numerical precision issues

## Next Checks

1. Implement the forward diffusion process with varying β schedules and empirically verify that differential entropy increases monotonically across all time steps, checking the constraint in Eq. (10).

2. Create a controlled experiment where pixel permutations are applied to training images and verify that the resulting diffusion trajectories are correspondingly permuted, measuring the correlation between original and permuted trajectories.

3. Implement both the forward diffusion process and the Fokker-Planck PDE solver to verify that numerical solutions match theoretical predictions, testing different stochastic moment conditions to validate Feller's theorem applicability.