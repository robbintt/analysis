---
ver: rpa2
title: Implicit Variational Inference for High-Dimensional Posteriors
arxiv_id: '2310.06643'
source_url: https://arxiv.org/abs/2310.06643
tags:
- implicit
- variational
- approximation
- neural
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new method for implicit variational inference
  that can handle high-dimensional posteriors without requiring adversarial objectives.
  The key idea is to use a local linearisation of the neural sampler to obtain a tractable
  approximation to the entropy of the implicit distribution.
---

# Implicit Variational Inference for High-Dimensional Posteriors

## Quick Facts
- arXiv ID: 2310.06643
- Source URL: https://arxiv.org/abs/2310.06643
- Reference count: 35
- Key outcome: New implicit VI method using local linearization achieves competitive performance on MNIST/CIFAR10 with better uncertainty quantification than deep ensembles and Laplace approximations

## Executive Summary
This paper introduces a novel approach to implicit variational inference that enables scalable Bayesian inference for high-dimensional neural networks without requiring adversarial objectives. The key innovation is a local linearization of the neural sampler that provides a tractable approximation to the entropy of the implicit distribution, making the evidence lower bound (ELBO) computationally feasible. The method demonstrates strong performance on regression and classification tasks, achieving lower negative log-likelihoods and better calibration than state-of-the-art baselines while maintaining computational efficiency.

## Method Summary
The method uses a hypernetwork (generator) to produce weights for a Bayesian neural network (BNN) by sampling from a base distribution. The generator's output is locally linearized around the sampled point to approximate the entropy term in the ELBO. Gaussian noise is added to the generator output to ensure the KL divergence is well-defined. Two entropy estimation approaches are proposed: one using the full Jacobian determinant (LIVI) and another using only the largest singular value for computational efficiency. A novel generator architecture (CMMNN) enables scaling to models with millions of parameters.

## Key Results
- Achieves lower negative log-likelihoods and expected calibration errors than deep ensembles and last-layer Laplace approximations on MNIST and CIFAR10
- Maintains computational efficiency while handling models with up to 2.7 million parameters
- Demonstrates superior out-of-distribution detection performance with high AUROC scores
- Shows competitive RMSE and NLL on UCI regression benchmarks compared to state-of-the-art VI methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local linearisation of the neural sampler enables tractable entropy approximation
- Mechanism: By linearising the generator around the sampled latent point, the method replaces the intractable implicit density with a tractable Gaussian approximation whose entropy can be computed analytically
- Core assumption: The generator's Jacobian exists and the linearisation is accurate in the neighbourhood of sampled latent points
- Evidence anchors:
  - [abstract]: "We propose using neural samplers that specify implicit distributions... by locally linearising the neural sampler"
  - [section]: "We consider an analytical approximation of Eq. (7) to use during training obtained via a local linearisation of the generator/neural sampler around z′"
  - [corpus]: Weak evidence; corpus focuses on related methods like normalizing flows and phylogenetic inference but not on linearisation of samplers

### Mechanism 2
- Claim: Adding Gaussian noise to the sampler output makes the KL divergence well-defined
- Mechanism: The Gaussian perturbation smooths the implicit density so that it has full support in parameter space, avoiding measure-zero issues on low-dimensional manifolds
- Core assumption: The variance σ² is small enough to preserve the structure of the implicit distribution while ensuring continuity
- Evidence anchors:
  - [section]: "To make the KL in Eq. (1) well-defined, we add Gaussian noise to the output of our neural sampler"
  - [abstract]: "Our approach introduces novel bounds for approximate inference using implicit distributions by locally linearising the neural sampler"
  - [corpus]: No direct support in corpus for this noise-adding trick; corpus discusses VI methods but not this specific regularisation

### Mechanism 3
- Claim: Efficient entropy estimation via low-rank Jacobian approximation scales to millions of dimensions
- Mechanism: By estimating only the largest singular value of the Jacobian and using it in a lower bound, the method avoids computing the full determinant while preserving most of the entropy's information
- Core assumption: The largest singular value captures enough of the Jacobian's spectrum to provide a useful entropy lower bound
- Evidence anchors:
  - [section]: "We can further approximate the differential entropy by... we can lower-bound Eq. (16) as... We defer the details... to Appendix C"
  - [abstract]: "Furthermore, we present a new sampler architecture that, for the first time, enables implicit distributions over tens of millions of latent variables, addressing computational concerns"
  - [corpus]: Weak; corpus papers discuss VI and flows but do not mention low-rank Jacobian tricks for entropy

## Foundational Learning

- Concept: Variational inference and evidence lower bound (ELBO)
  - Why needed here: The method builds on VI theory to derive a tractable objective for implicit distributions
  - Quick check question: What are the two terms that make up the ELBO in standard VI?

- Concept: Reparameterization trick for gradient estimation
  - Why needed here: Required to backpropagate through the sampling process from the base distribution to the implicit parameters
  - Quick check question: How does the reparameterization trick allow gradients to flow through stochastic nodes?

- Concept: Jacobians and singular value decomposition
  - Why needed here: The linearisation step and entropy approximation depend on the Jacobian of the generator and its singular values
  - Quick check question: Why is the Jacobian of the generator evaluated at the latent sample point?

## Architecture Onboarding

- Component map: Hypernetwork (generator) -> BNN (primary model) -> Entropy estimator -> Training loop

- Critical path:
  1. Sample latent vector z from base distribution
  2. Generate BNN parameters via generator gγ(z)
  3. Forward pass through BNN to compute log p(D|θ)
  4. Compute Jacobian Jg(z) and largest singular value s₁(z)
  5. Evaluate entropy lower bound using s₁(z)
  6. Combine likelihood and entropy terms to form ELBO
  7. Backpropagate to update generator parameters

- Design tradeoffs:
  - Accuracy vs. speed: Full Jacobian determinant (LIVI bound) is more accurate but expensive; single singular value bound is faster but potentially less precise
  - Expressiveness vs. tractability: More complex generators can model richer posteriors but may yield ill-conditioned Jacobians
  - Noise variance σ²: Balances between well-defined KL and preserving posterior structure

- Failure signatures:
  - High variance in gradient estimates → check Jacobian conditioning or learning rate
  - Model collapses to prior → entropy term may dominate; reduce weight on entropy
  - Poor OOD performance → generator may not be expressive enough; consider deeper or alternative architecture

- First 3 experiments:
  1. Toy sinusoidal regression with a small MLP BNN to verify epistemic uncertainty estimates
  2. UCI regression dataset with a single-hidden-layer MLP to compare RMSE and NLL against baselines
  3. MNIST classification with LeNet to test OOD detection and calibration metrics

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided content. The authors focus on presenting their method and experimental results without identifying specific limitations or future research directions that need to be addressed.

## Limitations
- Local linearization approximation may fail for complex generators with non-smooth regions or ill-conditioned Jacobians
- Gaussian noise addition introduces a hyperparameter that requires careful tuning and may trade off expressivity for tractability
- Computational cost of Jacobian and singular value estimation may still be prohibitive for extremely large models with hundreds of millions of parameters

## Confidence
- Confidence in core claims is Medium: The experimental results demonstrate competitive performance on standard benchmarks, but the lack of ablation studies on the entropy approximation method and the limited comparison to newer deep ensemble variants reduces confidence in the claimed advantages

## Next Checks
1. Perform ablation studies comparing different entropy approximation methods (full determinant vs. singular value bounds) on the same tasks to quantify the trade-off between accuracy and computational cost
2. Test the method on larger-scale vision tasks (e.g., ImageNet) to verify scalability claims and evaluate whether the generator architecture maintains performance with millions of parameters
3. Analyze the sensitivity of results to the Gaussian noise variance hyperparameter across multiple datasets to establish robust default settings and understand the trade-off between numerical stability and posterior fidelity