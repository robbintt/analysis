---
ver: rpa2
title: Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved
  Vision-Language Compositionality
arxiv_id: '2305.13812'
source_url: https://arxiv.org/abs/2305.13812
tags:
- text
- graph
- image
- scene
- mosaiclip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving compositional reasoning
  capabilities in vision-language models, which struggle with tasks like attribute
  binding, relation understanding, and systematic generalization. The authors propose
  a novel approach called MosaiCLIP that uses scene graph-based text decomposition
  and augmentation to match images with multiple text sub-graphs of varying complexity.
---

# Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality

## Quick Facts
- arXiv ID: 2305.13812
- Source URL: https://arxiv.org/abs/2305.13812
- Authors: 
- Reference count: 14
- Primary result: MosaiCLIP improves compositionality in VLMs by up to 18% on systematic generalization and 16.5% on relation understanding

## Executive Summary
This paper addresses the critical challenge of compositional reasoning in vision-language models, which struggle with tasks requiring attribute binding, relation understanding, and systematic generalization. The authors propose MosaiCLIP, a novel approach that leverages scene graph decomposition and augmentation to match images with multiple text sub-graphs of varying complexity. Through extensive experiments, MosaiCLIP demonstrates significant improvements on compositionality benchmarks while maintaining or exceeding CLIP's performance on general multimodal tasks.

## Method Summary
MosaiCLIP extends contrastive learning by decomposing scene graphs into sub-graphs representing different semantic granularities and matching them to the same image. The approach introduces hard negative graph mining through minimally perturbed sub-graphs to improve fine-grained semantic distinctions. A two-stage curriculum learning strategy gradually introduces complexity, first learning to handle hard negatives, then multiple positives and negatives, to prevent feature distortion during fine-tuning.

## Key Results
- Up to 18% improvement in systematic generalization on CREPE benchmark
- 16.5% improvement in relation understanding on SVO benchmark
- Maintains or exceeds CLIP performance on general multimodal tasks
- Significant improvements across COCO, CC-100K, and YFCC-100K fine-tuning datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Matching multiple text sub-graphs of varying complexity to the same image improves hierarchical understanding
- Mechanism: By decomposing scene graphs into sub-graphs representing different semantic granularities and matching them all to the same image, the model learns to recognize both coarse and fine-grained features simultaneously
- Core assumption: The text scene graph parsed from language is a sub-graph of the image scene graph, and multiple text sub-graphs can represent different aspects of the same image
- Evidence anchors:
  - [abstract] "we consider the scene graph parsed from text as a proxy for the image scene graph and propose a graph decomposition and augmentation framework"
  - [section 4.2] "we implicitly bring the representation of the image scene graph close to one of its sub-graph (the text scene graph)"
  - [corpus] Weak - corpus mentions "coarse-to-fine" but doesn't explain the mechanism
- Break condition: If text scene graphs don't accurately represent image content or if the decomposition creates semantically inconsistent sub-graphs

### Mechanism 2
- Claim: Hard negative graph mining improves attribute binding and relation understanding
- Mechanism: Creating minimally perturbed negative sub-graphs by swapping/replacing nodes and edges forces the model to distinguish between subtle semantic differences
- Core assumption: Small perturbations to positive sub-graphs create challenging negative examples that help the model learn fine-grained distinctions
- Evidence anchors:
  - [abstract] "we propose novel negative mining techniques in the scene graph space for improving attribute binding and relation understanding"
  - [section 4.4] "We swap nodes in the sub-graph, these can be swaps of nodes which are attributes or objects"
  - [corpus] Weak - corpus mentions "compositionality" but doesn't detail negative mining strategies
- Break condition: If perturbations are too large (creating unrelated concepts) or too small (not challenging enough)

### Mechanism 3
- Claim: Curriculum learning strategy prevents feature distortion during fine-tuning
- Mechanism: Gradually introducing complexity by first learning to handle hard negatives, then multiple positives and negatives, mimics pre-training distribution more closely
- Core assumption: The gap between pre-training (one positive text per image) and fine-tuning (multiple texts) creates a distribution shift that harms performance
- Evidence anchors:
  - [section 4.6] "our coarse-to-fine contrastive learning objective naturally deviates from the pre-training objective in two ways"
  - [abstract] "we develop a two-stage curriculum learning strategy for improved fine-tuning performance"
  - [corpus] Weak - corpus doesn't mention curriculum learning
- Break condition: If the curriculum stages don't align with the model's learning capacity or if the gap is too small to matter

## Foundational Learning

- Concept: Scene graph representation
  - Why needed here: Provides structured semantic representation of images that captures objects, attributes, and relations
  - Quick check question: Can you explain how a scene graph differs from a bounding box representation of an image?

- Concept: Graph decomposition and sub-graph sampling
  - Why needed here: Enables matching multiple semantic granularities to the same image
  - Quick check question: What's the difference between a spanning tree and a sub-graph in this context?

- Concept: Contrastive learning with multiple positives
  - Why needed here: Standard CLIP uses one positive text per image; this work extends it to multiple
  - Quick check question: How does the loss function change when you have multiple positive texts for one image?

## Architecture Onboarding

- Component map: Image encoder -> Graph decomposition module -> Text encoder -> Contrastive loss (multiple positives/negatives)
- Critical path: Scene graph parsing -> Graph decomposition -> Text template conversion -> Embedding -> Contrastive matching
- Design tradeoffs: More sub-graphs = better compositionality but higher computational cost and potential training instability
- Failure signatures: Poor performance on CREPE systematic generalization indicates graph decomposition isn't capturing compositional structure correctly
- First 3 experiments:
  1. Verify that text scene graphs accurately represent image content by comparing human annotations
  2. Test different numbers of positive sub-graphs (1, 3, 5) to find optimal balance between performance and cost
  3. Compare different negative graph transformation strategies (node swap vs edge replacement) on attribute vs relation understanding tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different graph transformation strategies (fobj, frel, fattr) contribute differently to improvements in attribute binding vs relation understanding?
- Basis in paper: [explicit] The authors mention that "fobj, fattr broadly aims at improving the model's attribute understanding, while frel broadly targets improved relation understanding" and show ablation results in Table 7
- Why unresolved: While the authors show overall performance differences, they don't provide detailed analysis of how each transformation type specifically impacts different aspects of compositionality or quantify the relative contribution of each strategy
- What evidence would resolve it: Controlled experiments isolating each transformation type's impact on attribute vs relation understanding tasks, along with ablation studies showing performance degradation when specific transformations are removed

### Open Question 2
- Question: What is the optimal number of positive and negative sub-graphs per image, and how does this vary across different types of compositionality tasks?
- Basis in paper: [explicit] The authors show in Figures 5 and 6 that performance varies with the number of sub-graphs but don't identify an optimal point or explain why different datasets respond differently
- Why unresolved: The authors only explore up to 3 positive and 6 negative sub-graphs, and observe performance plateaus or declines without investigating the underlying reasons or task-specific requirements
- What evidence would resolve it: Systematic analysis testing larger numbers of sub-graphs, identifying the point of diminishing returns for different tasks, and correlating sub-graph complexity with task difficulty

### Open Question 3
- Question: How does the tree-score improvement in language encoders translate to specific compositional reasoning capabilities?
- Basis in paper: [explicit] The authors observe in Figure 8 that MosaiCLIP has higher tree-scores than NegCLIP, and hypothesize this explains improved compositionality
- Why unresolved: The authors establish a correlation between tree-score and performance but don't demonstrate causation or explain which specific compositional reasoning tasks benefit most from hierarchical processing
- What evidence would resolve it: Detailed analysis linking specific tree-score components to performance on individual compositionality tasks, or controlled experiments modifying tree-score while holding other factors constant

## Limitations
- Relies heavily on accurate scene graph parsing without thorough evaluation of parsing fidelity
- Negative mining strategies validated only through downstream task performance rather than direct semantic distinction measurement
- Curriculum learning benefits demonstrated empirically without theoretical grounding for optimal design

## Confidence
- High confidence: Significant improvements on compositionality benchmarks (CREPE, ARO, SVO)
- High confidence: Maintains or exceeds CLIP performance on general multimodal tasks
- Medium confidence: Curriculum learning strategy prevents feature distortion
- Low confidence: Claims about "unlocking productivity" and "best of both worlds" are subjective interpretations

## Next Checks
1. **Scene Graph Fidelity Analysis**: Conduct human evaluation studies to verify that automatically generated scene graphs accurately capture semantic content of images and text pairs, particularly for complex compositional structures.

2. **Negative Mining Ablation Study**: Systematically test different negative graph transformation strategies (varying perturbation magnitudes and types) to determine which specific transformations contribute most to improved attribute binding and relation understanding.

3. **Curriculum Learning Sensitivity**: Evaluate performance impact of different curriculum progression speeds and stage compositions to identify whether current two-stage approach is optimal or if simpler alternatives could achieve similar results with less computational overhead.