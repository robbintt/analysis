---
ver: rpa2
title: 'Large Language Model-Enhanced Algorithm Selection: Towards Comprehensive Algorithm
  Representation'
arxiv_id: '2311.13184'
source_url: https://arxiv.org/abs/2311.13184
tags:
- algorithm
- problem
- selection
- features
- problemset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a new framework, AS-LLM, for algorithm selection
  by leveraging pre-trained large language models (LLMs) to extract features from
  algorithm code text. The method models the matching degree between problem and algorithm
  representations using cosine similarity.
---

# Large Language Model-Enhanced Algorithm Selection: Towards Comprehensive Algorithm Representation

## Quick Facts
- arXiv ID: 2311.13184
- Source URL: https://arxiv.org/abs/2311.13184
- Reference count: 26
- The AS-LLM framework leverages pre-trained LLMs to extract algorithm features from code text, outperforming state-of-the-art algorithm selection methods, especially with limited training data.

## Executive Summary
This paper introduces AS-LLM, a novel algorithm selection framework that utilizes pre-trained large language models to extract semantic features from algorithm code. Instead of traditional one-way mapping from problems to algorithms, AS-LLM directly models the matching degree between algorithm and problem representations using cosine similarity. The framework demonstrates superior performance on continuous optimization problems compared to existing methods, particularly when training data is limited. The approach leverages the LLM's ability to comprehend code structure, semantics, and library functions to create comprehensive algorithm representations.

## Method Summary
AS-LLM extracts features from algorithm code using a pre-trained LLM (specifically CodeBERT), then processes these features through pooling layers and MLPs to create comparable representations with problem features. Problem instances are represented through tree traversal of objective functions. The framework computes cosine similarity between problem and algorithm representations to quantify their matching degree, which is then used for algorithm selection. Training involves negative sampling guided by theoretical analysis of CVaR-divergence. The method is evaluated on continuous optimization problems using performance data from existing benchmarks.

## Key Results
- AS-LLM achieves higher algorithm selection accuracy than state-of-the-art methods across multiple problem sets
- The framework demonstrates particular advantage when training data is limited, showing better data efficiency
- Ablation studies confirm the effectiveness of algorithm feature extraction and similarity-based matching over direct classification approaches

## Why This Works (Mechanism)

### Mechanism 1
LLMs can extract meaningful semantic features from algorithm code that capture algorithmic intent and structure. Pre-trained LLMs learn distributed representations of code tokens encoding semantic relationships, control flow patterns, and library usage, which preserve algorithmic semantics relevant to optimization performance.

### Mechanism 2
Combining problem features with algorithm features enables bidirectional modeling of the algorithm selection task. By computing similarity between problem and algorithm representations, AS-LLM captures the bidirectional relationship between problems and algorithms that determines good matches, rather than learning a unidirectional mapping.

### Mechanism 3
Feature selection and pooling reduce high-dimensional LLM embeddings to manageable representations while preserving relevant information. The two-stage pooling and MLP layers compress potentially millions of dimensions into comparable sizes with problem features while removing noise and irrelevant dimensions.

## Foundational Learning

- Concept: Tree traversal for objective function representation
  - Why needed here: Objective functions in continuous optimization can be represented as expression trees, and traversing these trees provides a structured way to extract problem features that preserve operator precedence and variable relationships
  - Quick check question: Why is postorder traversal preferred over inorder traversal for objective function representation?

- Concept: Large Language Model code comprehension
  - Why needed here: LLMs trained on code learn semantic representations that go beyond syntax, capturing algorithmic patterns, control flow, and library usage that are essential for understanding algorithm capabilities
  - Quick check question: What distinguishes code-specific LLMs like CodeBERT from general-purpose LLMs when applied to algorithm feature extraction?

- Concept: Cosine similarity for matching degree calculation
  - Why needed here: Cosine similarity provides a normalized measure of alignment between problem and algorithm representations, allowing the model to quantify how well an algorithm's characteristics match a problem's structure
  - Quick check question: How does cosine similarity handle the comparison between problem and algorithm representations of different semantic spaces?

## Architecture Onboarding

- Component map:
  - Problem track: Expression tree traversal → Embedding layer → LSTM encoder → Linear layer → Problem representation
  - Algorithm track: Code text → Pre-trained LLM → Pooling layers → MLP → Algorithm representation
  - Matching module: Cosine similarity computation → Concatenation with features → Output MLP
  - Training: Negative sampling with uniform distribution based on theoretical analysis

- Critical path: Problem representation extraction → Algorithm representation extraction → Similarity calculation → Selection decision
- Design tradeoffs: High-dimensional LLM features vs. computational efficiency (handled through pooling), general vs. code-specific LLMs (accuracy vs. availability), similarity-based vs. direct classification approaches (interpretability vs. potential accuracy)
- Failure signatures: Poor performance on problems with uncommon structures, sensitivity to code preprocessing, degradation when algorithm set is too diverse
- First 3 experiments:
  1. Verify that different LLMs produce distinguishable performance on algorithm selection task (validate Mechanism 1)
  2. Test ablation of algorithm track to confirm contribution of algorithm features (validate Mechanism 2)
  3. Compare different pooling strategies to optimize compression of high-dimensional features (validate Mechanism 3)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal sampling probability for negative instances in algorithm selection tasks, balancing performance and computational efficiency?
- Basis in paper: Explicit. The paper discusses negative sampling in Section 3.4 and presents experimental results in Figure 3 showing the relationship between sampling probability and performance/time.
- Why unresolved: The paper shows that increasing sampling probability doesn't consistently improve performance and may even degrade it. The optimal balance depends on specific scenarios and training data characteristics.
- What evidence would resolve it: Systematic experiments varying problem types, algorithm sets, and training data sizes to identify consistent patterns in optimal sampling probabilities across different conditions.

### Open Question 2
- Question: How does the performance of AS-LLM compare to traditional algorithm selection methods when applied to non-continuous optimization problems?
- Basis in paper: Inferred. The paper focuses on continuous optimization problems but mentions in Section 3 that the method's applicability is not restricted to this scenario.
- Why unresolved: The paper only demonstrates AS-LLM's effectiveness on continuous optimization problems. Its performance on other problem types (e.g., combinatorial optimization, classification) remains unexplored.
- What evidence would resolve it: Experiments applying AS-LLM to various problem domains with appropriate problem feature extraction methods and comparing results to established algorithm selection techniques for each domain.

### Open Question 3
- Question: Which architectural components of AS-LLM contribute most significantly to its performance advantage over traditional algorithm selection methods?
- Basis in paper: Explicit. The paper presents ablation experiments in Section 4.2 that systematically remove different components (algorithm track, cosine similarity, pooling layer, MLP layer) to assess their impact.
- Why unresolved: While the ablation study identifies that algorithm features and matching degree calculation are crucial, the relative importance of each component and potential interactions between components remain unclear.
- What evidence would resolve it: More extensive ablation studies with additional variants, including combinations of removed components, and possibly statistical analysis of component importance across multiple datasets.

## Limitations

- The framework's performance depends heavily on the quality and coverage of the pre-trained LLM's code corpus, potentially limiting effectiveness with poorly represented algorithms
- Current experiments are restricted to continuous optimization problems with MATLAB implementations, raising questions about generalization to other domains or programming languages
- The negative sampling strategy may not fully capture real-world complexity where multiple algorithms can perform similarly well

## Confidence

- **High confidence**: The general framework design and its ability to outperform traditional algorithm selection methods when training data is limited
- **Medium confidence**: The specific contributions of each module (pooling, MLP layers) based on ablation studies, as improvements may be sensitive to hyperparameter choices
- **Medium confidence**: The scalability to very large algorithm sets, as experiments used a relatively small set of 12 algorithms

## Next Checks

1. Test AS-LLM on algorithm sets with diverse characteristics (e.g., including meta-heuristics, exact methods, and hybrid approaches) to evaluate robustness
2. Evaluate performance when algorithm code is in different programming languages or when documentation is used instead of code
3. Conduct sensitivity analysis on the pooling dimensions and MLP architecture to determine optimal configurations for different problem scales