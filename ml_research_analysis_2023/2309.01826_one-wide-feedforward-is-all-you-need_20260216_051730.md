---
ver: rpa2
title: One Wide Feedforward is All You Need
arxiv_id: '2309.01826'
source_url: https://arxiv.org/abs/2309.01826
tags:
- transformer
- decoder
- self-attn
- encoder
- sharing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The Transformer\u2019s Feed Forward Network (FFN) consumes two-thirds\
  \ of model parameters but is highly redundant across layers. Sharing a single wide\
  \ FFN across the encoder and removing FFNs from the decoder maintains BLEU score\
  \ while reducing parameters by ~40%."
---

# One Wide Feedforward is All You Need

## Quick Facts
- arXiv ID: 2309.01826
- Source URL: https://arxiv.org/abs/2309.01826
- Reference count: 16
- One-line primary result: Sharing a single wide FFN across encoder layers and removing decoder FFNs yields up to +1.2 BLEU improvement with 22% faster inference

## Executive Summary
This paper demonstrates that the Feed Forward Network (FFN) in Transformers is highly redundant across layers, consuming two-thirds of model parameters while contributing less to accuracy than expected. By sharing a single wide FFN across all encoder layers and removing FFNs from the decoder, the authors maintain BLEU score while reducing parameters by ~40%. Increasing the shared FFN width to match original parameter count yields up to +1.2 BLEU improvement over baseline with 22% faster inference. The work challenges the conventional wisdom that per-layer FFNs are essential for Transformer performance.

## Method Summary
The authors systematically eliminate and share FFNs in Transformer models across three datasets (WMT22 EN→DE, WMT16 EN→RO, and multilingual translation). They implement five architectures: baseline per-layer FFNs, shared encoder FFN, no decoder FFN, combined shared encoder/no decoder FFN, and a widened shared FFN matching original parameter count. Training uses Transformer Big configuration with Adam optimizer, fp16 training, and specific dropout rates per dataset. Model performance is evaluated using BLEU score, inference latency, and representational similarity analysis via Centered Kernel Alignment (CKA) and Linear Centered Kernel Alignment (LNS).

## Key Results
- Sharing a single FFN across all encoder layers reduces parameters by ~40% with minimal BLEU loss
- Removing decoder FFNs improves inference latency by 20% with only 0.4 BLEU point degradation
- Widening the shared FFN to match original parameter count achieves up to +1.2 BLEU improvement over baseline
- CKA analysis shows adjacent FFN layers learn highly correlated patterns (0.8-0.9 similarity), validating redundancy
- The One Wide FFN architecture maintains stable accuracy while improving efficiency metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sharing a single wide FFN across all encoder layers reduces redundancy without significant accuracy loss
- Mechanism: Adjacent FFN layers learn overlapping key-value patterns, so parameter sharing leverages this redundancy to cut parameters while preserving learned transformations
- Core assumption: FFN keys and values learned at adjacent layers are highly correlated, making one shared FFN sufficient to represent the combined function
- Evidence anchors:
  - [abstract] "We are able to eliminate the decoder FFN and share a single FFN across the encoder without significantly compromising the model's accuracy"
  - [section 2.2] "there's a substantial overlap between patterns captured by adjacent layers, indicating that there are redundancies in the FFNs"
  - [corpus] Weak - no direct citation but aligns with Transformer efficiency literature
- Break condition: If layer-wise semantic patterns diverge strongly, sharing would collapse unique transformations and degrade accuracy

### Mechanism 2
- Claim: Removing decoder FFNs improves inference latency while maintaining BLEU score
- Mechanism: Decoder FFNs are more redundant than encoder FFNs; dropping them eliminates unnecessary computation in the autoregressive pass, yielding latency gains without accuracy penalty
- Core assumption: Decoder-side transformations are less critical than encoder-side ones for final translation quality
- Evidence anchors:
  - [abstract] "sharing a single wide FFN on the encoder and no FFN on the decoder... substantial gains in both accuracy and latency"
  - [section 4.2] "Dropping the FFN on the decoder... causes a degradation of only 0.4 BLEU points while increasing the inference speed by 20%"
  - [corpus] Weak - no direct latency-FFN literature, but consistent with autoregressive inference constraints
- Break condition: If decoder-side representations carry unique semantic information not captured by encoder, accuracy will drop sharply

### Mechanism 3
- Claim: Widening the shared FFN restores lost accuracy while keeping parameter count constant
- Mechanism: The redundancy savings from sharing free up parameter budget; reallocating them to a wider shared FFN recovers capacity for complex transformations
- Core assumption: A wider FFN can approximate the combined effect of multiple narrower ones when redundancy is removed
- Evidence anchors:
  - [abstract] "scale this architecture back to its original size by increasing the hidden dimension of the shared FFN, achieving substantial gains in both accuracy and latency"
  - [section 4.3] "increasing the dimension of the shared FFN to match the number of parameters of the original (fully-parameterized) model"
  - [corpus] Weak - no explicit parameter reallocation studies, but aligns with width-depth tradeoffs in Transformers
- Break condition: If width alone cannot capture multi-layer depth interactions, accuracy gains will plateau or reverse

## Foundational Learning

- Concept: Residual connections and layer normalization
  - Why needed here: FFN sharing removes some normalization and residual links; understanding their role clarifies why dropping decoder FFN is safe
  - Quick check question: What happens to gradient flow if you drop the residual connection associated with a removed FFN?

- Concept: Attention vs FFN functional separation
  - Why needed here: The paper shows FFN is more redundant than attention; distinguishing their roles helps decide what to prune
  - Quick check question: In a Transformer block, which component (attention or FFN) contributes more to pairwise token dependencies?

- Concept: Parameter counting and model footprint estimation
  - Why needed here: Sharing and dropping FFNs change parameter counts; accurate accounting is needed to keep model size comparable across experiments
  - Quick check question: How many parameters does a single FFN layer consume in a Transformer Big (dmodel=1024, dff=4096)?

## Architecture Onboarding

- Component map: Token → Embedding → Encoder self-attention → Shared wide FFN → Cross-attention → Output
- Critical path: Token → Embedding → Encoder self-attention → Shared wide FFN → Cross-attention → Output
- Design tradeoffs: Parameter efficiency vs. depth of representation; Inference speed vs. model expressivity; Encoder capacity vs. decoder simplicity
- Failure signatures: BLEU drops > 1 point after sharing/dropping FFNs; Layer-wise CKA similarity diverges sharply from baseline; Latency improvement not materializing due to memory-bound operations
- First 3 experiments:
  1. Replace all encoder FFNs with a single shared FFN; keep decoder FFNs; measure BLEU and parameter count
  2. Drop all decoder FFNs; keep per-layer encoder FFNs; measure BLEU and latency
  3. Combine steps 1 and 2, then widen shared FFN to match original parameter count; measure BLEU, latency, and compare to baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the One Wide FFN model's performance change when applied to other sequence-to-sequence tasks beyond machine translation?
- Basis in paper: [explicit] The authors acknowledge that their focus was on machine translation and expect results to generalize to other tasks but note that further experiments are needed
- Why unresolved: The paper explicitly states that the model's effectiveness across different sequence-to-sequence tasks remains untested, leaving a gap in understanding its broader applicability
- What evidence would resolve it: Empirical results demonstrating the One Wide FFN model's performance on a diverse set of sequence-to-sequence tasks, such as summarization, question answering, or text generation, would provide clarity on its generalizability

### Open Question 2
- Question: What is the impact of the One Wide FFN model on energy consumption and carbon footprint compared to traditional Transformer models?
- Basis in paper: [inferred] The authors mention the importance of energy consumption for model training and its relation to greenhouse emissions, but do not provide specific data on the One Wide FFN model's energy efficiency
- Why unresolved: While the paper discusses the potential benefits of the One Wide FFN model in terms of parameter efficiency and inference speed, it lacks quantitative analysis of its energy consumption during training and inference
- What evidence would resolve it: Comparative studies measuring the energy consumption and carbon footprint of the One Wide FFN model versus traditional Transformer models during training and inference would address this gap

### Open Question 3
- Question: How does the One Wide FFN model's performance vary with different tokenization strategies and vocabulary sizes?
- Basis in paper: [inferred] The authors use specific tokenization strategies (SENTENCE PIECE and BPE) and vocabulary sizes for their experiments, but do not explore how the model's performance might change with alternative tokenization approaches
- Why unresolved: The choice of tokenization strategy can significantly impact a model's performance, especially in multilingual settings. The paper does not investigate the robustness of the One Wide FFN model to different tokenization choices
- What evidence would resolve it: Experiments comparing the One Wide FFN model's performance across various tokenization strategies and vocabulary sizes would provide insights into its sensitivity to tokenization choices and its potential for adaptation to different languages and domains

## Limitations

- The CKA similarity analysis lacks statistical validation - 0.8-0.9 values are presented without confidence intervals or significance testing
- Latency improvements are measured on unspecified hardware, making verification of the claimed 22% speedup difficult
- Multilingual experiments use a single dataset without ablation studies across different language pairs or families, limiting generalizability claims
- The paper does not explore the impact of tokenization strategies on model performance, which could affect multilingual applicability

## Confidence

**High confidence**: The parameter reduction claims are mathematically straightforward and verifiable through architecture modifications. The BLEU degradation patterns (< 0.4 points) are well-documented and reproducible given the training setup.

**Medium confidence**: The representational similarity findings depend on the choice of similarity metrics and reference points. The claim that FFNs are more redundant than attention mechanisms is supported by CKA analysis but lacks ablation studies isolating attention's role.

**Low confidence**: The generalization to multilingual settings and the claim that dropping decoder FFNs universally improves efficiency are based on single-dataset experiments without systematic cross-linguistic validation.

## Next Checks

1. **Statistical validation of redundancy**: Compute confidence intervals for CKA/LNS similarity scores across multiple training runs to establish whether 0.8-0.9 values are statistically significant indicators of redundancy

2. **Hardware-specific latency verification**: Replicate the latency measurements on different GPU/CPU configurations to confirm the 22% speedup claim is not hardware-dependent

3. **Attention-FFN ablation study**: Train models with attention removed while keeping FFNs, and vice versa, to quantify the relative contributions of each component to translation quality and identify true redundancy patterns