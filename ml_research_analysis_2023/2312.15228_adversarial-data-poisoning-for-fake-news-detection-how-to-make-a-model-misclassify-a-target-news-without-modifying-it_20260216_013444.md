---
ver: rpa2
title: 'Adversarial Data Poisoning for Fake News Detection: How to Make a Model Misclassify
  a Target News without Modifying It'
arxiv_id: '2312.15228'
source_url: https://arxiv.org/abs/2312.15228
tags:
- data
- news
- target
- poisoning
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This position paper investigates adversarial data poisoning attacks
  in online fake news detection. The authors propose a method to make models misclassify
  true news as false without modifying the target article, by injecting poisoned data
  into training.
---

# Adversarial Data Poisoning for Fake News Detection: How to Make a Model Misclassify a Target News without Modifying It

## Quick Facts
- arXiv ID: 2312.15228
- Source URL: https://arxiv.org/abs/2312.15228
- Reference count: 18
- The study reveals that linear logistic regression models are more vulnerable to Most Confidence Mislabeling attacks, while quadratic models are more susceptible to Target Label Flipping attacks.

## Executive Summary
This position paper explores adversarial data poisoning attacks on online fake news detection systems. The authors demonstrate how attackers can manipulate model behavior by injecting poisoned data into training without modifying the target article itself. Through experiments with two logistic regression models (linear and quadratic) against two poisoning attacks, the study reveals that model architecture significantly influences vulnerability patterns. The research highlights critical security vulnerabilities in online learning frameworks and lays groundwork for understanding poisoning attacks on more sophisticated fake news detection systems.

## Method Summary
The study employs synthetic data generation (10,000 real values x ∈ [0,1] with binary class labels based on separation value p=0.5) to test two logistic regression models against two poisoning attacks. The linear model uses logit(ˆp) = β0 + β1x, while the quadratic model uses logit(ˆp) = β0 + β1x + β2x2. The researchers apply Most Confidence Mislabeling and Target Label Flipping attacks to evaluate how many poisoned samples are required to misclassify a target news article. The online learning framework continuously updates the model with new data, including injected poisoned samples.

## Key Results
- Linear logistic regression models require fewer poisoned samples to be compromised by Most Confidence Mislabeling attacks compared to quadratic models
- Quadratic logistic regression models show greater resilience to Most Confidence Mislabeling but are more significantly affected by Target Label Flipping attacks
- Model complexity (number of parameters) directly correlates with susceptibility to specific types of poisoning attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data poisoning attacks can cause an online fake news detection model to misclassify true news as false without modifying the target article.
- Mechanism: By injecting poisoned data samples into the training set, the attacker shifts the decision boundary of the logistic regression model. This boundary shift causes the model to incorrectly classify the true target news article as fake when it would otherwise be classified correctly.
- Core assumption: The attacker has access to the training data stream and can inject carefully crafted adversarial examples that influence the model's decision boundary.
- Evidence anchors:
  - [abstract]: "we analyze how an attacker can compromise the performance of an online learning detector on specific news content without being able to manipulate the original target news"
  - [section]: "we seek to understand the vulnerabilities of the online learning model [5] and the potential risks associated with data poisoning attacks"
  - [corpus]: Weak evidence - related papers discuss adversarial attacks but not specifically this targeted poisoning mechanism
- Break condition: The attack fails if the model architecture is robust to the specific poisoning method, or if the model uses strong regularization or defense mechanisms that detect and reject poisoned samples.

### Mechanism 2
- Claim: The complexity of the logistic regression model determines its vulnerability to different types of poisoning attacks.
- Mechanism: Linear LR models are more susceptible to Most Confidence Mislabeling attacks because they have fewer parameters and cannot adapt well to poisoned data patterns. Quadratic LR models, with more parameters, can better follow poisoned data patterns but are more vulnerable to Target Label Flipping attacks that directly contradict the model's learned decision boundary.
- Core assumption: Model complexity (number of parameters) directly correlates with the ability to adapt to poisoned data patterns.
- Evidence anchors:
  - [abstract]: "Our initial findings reveal varying susceptibility of logistic regression models based on complexity and attack type"
  - [section]: "Figure 2(a) reveals that in the linear model, the Most Confidence Mislabeling attack requires a lower number of samples to misclassify the target news article. Conversely, Figure 2(b) shows that the Quadratic LR model is resilient to Most Confidence Mislabeling, but more significantly affected by the Target Label Flipping attack"
  - [corpus]: Weak evidence - related papers discuss model vulnerability but not specifically the relationship between model complexity and attack type susceptibility
- Break condition: The attack fails if the model uses techniques like ensemble methods, adversarial training, or robust optimization that make it less sensitive to poisoning regardless of complexity.

### Mechanism 3
- Claim: Online learning frameworks are vulnerable to data poisoning because they continuously update their models with new data, including potential poisoned samples.
- Mechanism: In the online learning framework, new incoming news articles are combined with existing ones, poisoned data is injected, and the model is updated. This continuous update process allows poisoned data to gradually shift the model's decision boundary over time, eventually causing misclassification of the target article.
- Core assumption: The online learning framework continuously incorporates new training data without robust validation mechanisms to detect poisoned samples.
- Evidence anchors:
  - [abstract]: "we show how an attacker could potentially introduce poisoning data into the training data to manipulate the behavior of an online learning method"
  - [section]: "Figure 1 depicts the iterative process of an online learning fake news detector... at each time t, previously unseen news articles are combined with already existing ones; then, some poisoned data are generated and added to the collected data. Finally, the model is trained and updated using all the aggregated data"
  - [corpus]: Weak evidence - related papers discuss online learning but not specifically its vulnerability to data poisoning attacks
- Break condition: The attack fails if the online learning framework includes mechanisms to detect and filter poisoned data, such as anomaly detection, robust aggregation methods, or periodic retraining on verified clean data.

## Foundational Learning

- Concept: Logistic Regression fundamentals
  - Why needed here: The paper tests two types of logistic regression models (linear and quadratic) against poisoning attacks, so understanding how logistic regression works is essential to grasp the results.
  - Quick check question: How does the logit function transform probabilities into log-odds in logistic regression?

- Concept: Adversarial machine learning
  - Why needed here: The paper investigates adversarial data poisoning attacks, which are a specific type of adversarial machine learning technique aimed at manipulating model behavior through training data manipulation.
  - Quick check question: What is the difference between evasion attacks and poisoning attacks in adversarial machine learning?

- Concept: Online learning vs. batch learning
  - Why needed here: The paper specifically uses an online learning framework where the model continuously updates with new data, making it more vulnerable to poisoning attacks compared to traditional batch learning.
  - Quick check question: How does the continuous update process in online learning create vulnerabilities that batch learning does not have?

## Architecture Onboarding

- Component map:
  Data collection pipeline -> Poison generation module -> Model training component -> Decision boundary evaluation

- Critical path:
  1. New news articles arrive and are collected
  2. Poisoned data is generated and injected into the training set
  3. Model is trained on the combined clean and poisoned data
  4. Model's decision boundary shifts, causing misclassification of target article

- Design tradeoffs:
  - Model complexity vs. attack vulnerability: More complex models (quadratic LR) are more robust to some attacks but vulnerable to others
  - Real-time updates vs. security: Online learning enables rapid adaptation but increases poisoning vulnerability
  - Detection capability vs. performance: Strong poison detection mechanisms may reduce false positives but could also block legitimate data

- Failure signatures:
  - Sudden drops in model accuracy on clean validation data
  - Gradual shift in model decision boundaries toward poisoned data distributions
  - Increased misclassification of specific target articles that match poisoning patterns

- First 3 experiments:
  1. Test Most Confidence Mislabeling attack on linear LR model with varying numbers of poisoned samples to find the minimum required for misclassification
  2. Test Target Label Flipping attack on quadratic LR model to measure vulnerability compared to linear model
  3. Compare model performance on clean vs. poisoned data to quantify the impact of each attack type on different model architectures

## Open Questions the Paper Calls Out
- How effective are other types of data poisoning attacks (e.g., Gradient Maximization) on online fake news detection models?
- How do more sophisticated fake news detection models (e.g., transformer-based models, graph neural networks) compare in their vulnerability to data poisoning attacks?
- How does the effectiveness of data poisoning attacks vary across different types of fake news and sources of misinformation?

## Limitations
- The study uses synthetic data rather than real-world news datasets, limiting generalizability to practical applications
- The evaluation focuses only on logistic regression models, excluding more sophisticated architectures commonly used in fake news detection
- The paper lacks detailed technical specifications for the poisoning attack implementations, making exact reproduction difficult

## Confidence
- High confidence in the theoretical framework of data poisoning attacks on online learning systems
- Medium confidence in the specific vulnerability findings for logistic regression models
- Low confidence in the practical applicability to real-world fake news detection systems

## Next Checks
1. Implement the Most Confidence Mislabeling and Target Label Flipping attacks on real news datasets to verify if the vulnerability patterns hold beyond synthetic data
2. Test the same poisoning attacks against more complex fake news detection models (e.g., transformer-based architectures, graph neural networks) to assess whether the linear-quadratic LR vulnerability pattern generalizes
3. Evaluate potential defense mechanisms such as anomaly detection, robust optimization, or ensemble methods to determine if they can effectively mitigate the identified poisoning vulnerabilities