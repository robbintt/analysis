---
ver: rpa2
title: 'Deep de Finetti: Recovering Topic Distributions from Large Language Models'
arxiv_id: '2312.14226'
source_url: https://arxiv.org/abs/2312.14226
tags:
- topic
- llms
- language
- latent
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates whether large language models (LLMs) encode
  the latent topic structure of documents. Motivated by an extension of de Finetti's
  theorem, the authors hypothesize that the autoregressive objective used to train
  LLMs implicitly performs Bayesian inference on the underlying topic distributions
  of text.
---

# Deep de Finetti: Recovering Topic Distributions from Large Language Models

## Quick Facts
- **arXiv ID:** 2312.14226
- **Source URL:** https://arxiv.org/abs/2312.14226
- **Reference count:** 18
- **Primary result:** LLM embeddings contain information allowing accurate recovery of LDA topic distributions, with autoregressive models outperforming masked language models

## Executive Summary
This work investigates whether large language models encode the latent topic structure of documents. Building on an extension of de Finetti's theorem, the authors hypothesize that the autoregressive objective used to train LLMs implicitly performs Bayesian inference on underlying topic distributions. Through experiments with topic probes trained on LLM embeddings, they demonstrate that these models indeed capture information about topic mixtures, with autoregressive models showing superior performance compared to masked language models. The findings suggest that next-word prediction encourages LLMs to implicitly learn topic models, providing insight into what information LLMs encode about text structure.

## Method Summary
The authors train topic probes (linear classifiers with softmax) on top of LLM embeddings to predict topic mixture vectors from LDA models. They use both synthetic exchangeable text and natural corpora (20Newsgroups and WikiText-103) as testbeds. For synthetic data, they generate bags-of-words using manually initialized topic models. For natural corpora, they train LDA models to obtain ground truth topic mixtures. The probes are evaluated using accuracy, cross-entropy loss, L2 loss, and total variation loss metrics.

## Key Results
- LLM embeddings contain sufficient information to accurately predict LDA topic mixtures
- Autoregressive models (GPT-2 variants) outperform masked language models (BERT variants) in topic recovery
- Topic information is most accessible in the final layer embeddings across different model architectures
- Performance generalizes from synthetic exchangeable data to natural corpus data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Autoregressive language models trained on next-word prediction implicitly perform Bayesian inference on latent topic distributions
- **Mechanism**: Under de Finetti's theorem, exchangeable sequences can be represented as mixtures with respect to latent generating distributions. While text is not fully exchangeable, topic structure depends little on word order, making this approximation reasonable for topic modeling
- **Core assumption**: Topic structure exhibits partial exchangeability - the distribution of topics in a document remains approximately invariant under word order permutations within certain contexts
- **Evidence anchors**:
  - [abstract] "De Finetti's theorem shows that exchangeable probability distributions can be represented as a mixture with respect to a latent generating distribution"
  - [section] "Although text is not exchangeable at the level of syntax, exchangeability is a reasonable starting assumption for topic structure"
  - [corpus] Weak - the corpus evidence primarily comes from successful decoding experiments rather than direct exchangeability tests
- **Break condition**: If topic distributions show strong dependence on word order (e.g., in highly structured narratives or technical documentation where word sequence is semantically crucial)

### Mechanism 2
- **Claim**: LLM internal representations encode topic mixture vectors that can be decoded using linear probes
- **Mechanism**: The embedding of a document sequence f(x1:N) contains sufficient information to reconstruct the topic mixture distribution p(θ|x1:n) through a simple linear transformation
- **Core assumption**: Topic mixture vectors are linearly separable in the LLM embedding space
- **Evidence anchors**:
  - [abstract] "the representations formed by LLMs encode both the topics used to generate synthetic data and those used to explain natural corpus data"
  - [section] "we define the input as the LLM-learned document embedding f(x1:N)i. We define the target as a topic mixture θi that is sampled from p(θ|x1:n) learned by LDA"
  - [corpus] Strong - the paper shows successful decoding with linear probes across multiple datasets and model architectures
- **Break condition**: If topic mixture vectors require non-linear transformations for separation, or if they are distributed across multiple layers in a non-additive way

### Mechanism 3
- **Claim**: Autoregressive models encode topic structure more effectively than masked language models
- **Mechanism**: The autoregressive objective forms a more expressive Bayesian inference objective than MLM, allowing better recovery of latent variables
- **Core assumption**: The joint distribution modeling capability of autoregressive models is crucial for capturing topic structure
- **Evidence anchors**:
  - [abstract] "autoregressive models outperforming masked language models"
  - [section] "The difference between the MLM objective and the autoregressive objective is that in the summation, the prediction of each token xn uses the same posterior over the latent variable θ. In other words, each token xn is predicted independently from the latent variable θ"
  - [corpus] Strong - synthetic experiments show AT significantly outperforming BERT, and natural corpus experiments show consistent advantage for autoregressive models
- **Break condition**: If MLM performance improves with architectural modifications that enable joint distribution modeling

## Foundational Learning

- **Concept**: de Finetti's theorem and its extension to autoregressive models
  - Why needed here: Provides the theoretical foundation for why next-word prediction implicitly performs Bayesian inference on latent distributions
  - Quick check question: What does de Finetti's theorem state about the relationship between exchangeable distributions and latent generating processes?

- **Concept**: Partial exchangeability in language
  - Why needed here: Explains why de Finetti's theorem can be applied to language despite its non-exchangeability at the syntactic level
  - Quick check question: How does partial exchangeability differ from full exchangeability, and why is it relevant for topic modeling?

- **Concept**: Topic modeling with LDA
  - Why needed here: Provides the target framework for measuring how well LLMs encode topic structure
  - Quick check question: In LDA, what do the variables θ and β represent, and how are they related to document-topic and topic-word distributions?

## Architecture Onboarding

- **Component map**: Pre-trained LLM (GPT-2, LLAMA 2, BERT) -> Document embedding extraction (token embeddings, averaged or last token) -> Topic probe (linear classifier with softmax) -> Target generation (LDA training on corpus) -> Evaluation metrics (accuracy, L2 loss, total variation loss)

- **Critical path**: 
  1. Train LDA on corpus to obtain topic mixtures
  2. Extract LLM embeddings for each document
  3. Train linear probe to map embeddings to topic mixtures
  4. Evaluate probe performance

- **Design tradeoffs**:
  - Linear vs. non-linear probes: Linear probes keep statistical learning in the LLM but may miss complex relationships
  - Embedding choice: Last token vs. average vs. multiple layers affects information capture
  - Autoregressive vs. MLM: Autoregressive models show better performance but are computationally more expensive

- **Failure signatures**:
  - Probe performance close to random guessing indicates topic information is not encoded
  - High variance in probe performance across seeds suggests instability in topic encoding
  - Discrepancy between synthetic and natural corpus performance may indicate model-specific encoding strategies

- **First 3 experiments**:
  1. Train a small autoregressive transformer on synthetic exchangeable data and attempt to decode topic mixtures
  2. Compare autoregressive vs. MLM performance on the same synthetic dataset
  3. Test different embedding strategies (last token, average, multiple layers) on a small natural corpus

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs implicitly represent topic distributions, and what is the relationship between the learned embeddings and the actual topic distributions?
- Basis in paper: [explicit] The paper discusses the connection between LLMs and implicit Bayesian inference, suggesting that LLMs learn to represent latent topic distributions.
- Why unresolved: The paper provides theoretical justification and empirical evidence for the hypothesis, but it does not delve into the specific mechanisms by which LLMs represent these distributions. Understanding the internal workings of LLMs is an ongoing area of research.
- What evidence would resolve it: Further research could involve analyzing the internal representations of LLMs to understand how they encode topic distributions, potentially using techniques like probing classifiers or examining the attention mechanisms within the models.

### Open Question 2
- Question: Can the findings be generalized to other types of latent variables beyond topics, such as author attributes or sentiment?
- Basis in paper: [explicit] The paper mentions that the analysis could potentially be extended to other latent variables that do not depend heavily on word order, such as the author of the document or the author's sentiment.
- Why unresolved: The paper focuses on topic structure as a specific example, but it acknowledges that other latent variables might exhibit similar properties. However, empirical validation for these other variables is needed.
- What evidence would resolve it: Future research could involve applying the same analysis framework to other latent variables and testing whether LLMs encode these variables in a similar manner.

### Open Question 3
- Question: How does the performance of LLMs in recovering topic distributions compare to other topic modeling approaches, such as traditional LDA models?
- Basis in paper: [explicit] The paper compares the performance of LLMs to LDA models in recovering topic distributions, finding that LLMs achieve comparable or even superior performance in some cases.
- Why unresolved: While the paper provides a comparison, it does not explore the reasons for the differences in performance or the potential advantages and disadvantages of each approach.
- What evidence would resolve it: Further research could involve conducting more comprehensive comparisons between LLMs and other topic modeling approaches, considering factors such as computational efficiency, interpretability, and robustness to noise in the data.

## Limitations

- The theoretical connection to de Finetti's theorem remains informal, as text is not fully exchangeable
- The superiority of autoregressive models is empirically demonstrated but not fully explained mechanistically
- Synthetic experiments use artificially constructed topic models that may not reflect real-world complexity

## Confidence

**Confidence Level: Medium** - While empirical results demonstrate that LLM embeddings contain information about topic distributions, the theoretical connection to de Finetti's theorem remains somewhat informal. The paper acknowledges that text is not fully exchangeable, and the extension to autoregressive models is not rigorously proven.

**Confidence Level: Medium** - The superiority of autoregressive models over masked language models is well-demonstrated empirically, but the paper does not fully explore why this difference exists. The claim that autoregressive models perform "more expressive Bayesian inference" is plausible but not rigorously established.

**Confidence Level: Low** - The synthetic experiments, while providing controlled conditions, use artificially constructed topic models that may not reflect the complexity of real-world topic distributions.

## Next Checks

**Check 1: Layer-wise Analysis** - Conduct a systematic analysis of topic information across different layers of the LLM. This would involve training probes on intermediate layer embeddings to determine where and how topic information is encoded, potentially revealing whether different layers capture different aspects of topic structure.

**Check 2: Cross-Architecture Probe Transfer** - Test whether probes trained on one LLM architecture (e.g., GPT-2) can effectively decode topic information from embeddings of a different architecture (e.g., LLAMA 2). This would help determine whether topic encoding is architecture-specific or follows general principles across LLMs.

**Check 3: Temporal Stability Assessment** - Evaluate the stability of topic encoding over time by testing whether LLM embeddings from different checkpoints (during training) show consistent topic information. This would help understand when and how topic structure emerges during LLM training and whether it remains stable across model iterations.