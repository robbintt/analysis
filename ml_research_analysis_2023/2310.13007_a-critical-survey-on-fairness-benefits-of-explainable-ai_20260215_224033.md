---
ver: rpa2
title: A Critical Survey on Fairness Benefits of Explainable AI
arxiv_id: '2310.13007'
source_url: https://arxiv.org/abs/2310.13007
tags:
- fairness
- conference
- pages
- learning
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts a systematic review of 175 scientific articles
  to analyze claims about the fairness benefits of explainable AI (XAI). It identifies
  seven archetypal claims, including XAI's role in achieving fairness, enabling fairness
  reporting and analysis, mitigating unfairness, informing fairness judgments, increasing
  fairness perceptions, and implementing subjective fairness notions.
---

# A Critical Survey on Fairness Benefits of Explainable AI

## Quick Facts
- arXiv ID: 2310.13007
- Source URL: https://arxiv.org/abs/2310.13007
- Reference count: 40
- Primary result: Systematic review of 175 papers identifies seven archetypal claims about XAI's fairness benefits, finding most are vague and poorly aligned with XAI's actual capabilities

## Executive Summary
This paper conducts a systematic review of 175 scientific articles to critically examine claims about the fairness benefits of explainable AI (XAI). The authors identify seven archetypal claims about how XAI relates to fairness, including enabling fairness reporting, analysis, mitigation, judgments, and perceptions. Through careful analysis, they find that many claims are poorly defined, lack normative grounding, and are not well-supported by evidence. The review emphasizes the need for greater specificity about which XAI methods enable which fairness dimensions for which stakeholders, and argues that XAI should be viewed as one of many tools for addressing algorithmic fairness rather than a silver bullet solution.

## Method Summary
The study employed a systematic literature review using database searches across Scopus and arXiv with specific search strings combining XAI and fairness terms. An initial seed set of papers was expanded through iterative backward and forward snowballing using Citationchaser. The authors then performed qualitative content analysis using MAXQDA to extract and categorize claims about XAI and fairness, organizing them into seven archetypal claims. Each claim was critically evaluated for conceptual support, applied ML support, and human subject study evidence.

## Key Results
- Identified seven archetypal claims about XAI's fairness benefits across formal and perceived fairness dimensions
- Found that claims about XAI enabling fairness reporting and analysis have the strongest empirical support
- Demonstrated that many claims are vague and lack specificity about which XAI methods serve which stakeholders
- Highlighted the misalignment between XAI's capabilities and the complex, multidimensional nature of algorithmic fairness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: XAI enables humans to report on formal fairness by revealing how models use sensitive attributes
- Mechanism: XAI methods like LIME, SHAP, and counterfactual explanations provide feature importance scores that indicate reliance on protected attributes, allowing stakeholders to audit models for compliance with fairness metrics
- Core assumption: Low feature importance of sensitive attributes indicates fair treatment and absence of discrimination
- Evidence anchors: Identifies "formal fairness" as distinct dimension; discusses feature importance methods; corpus shows limited external validation
- Break condition: If XAI methods cannot reliably measure feature importance due to input perturbation or model approximation limitations

### Mechanism 2
- Claim: XAI increases human perceptions of fairness by providing transparency and enabling better understanding of model decisions
- Mechanism: Explanations help users acknowledge formal unfairness, support contestability, and provide guidance on recourse, which positively affects perceived informational and procedural fairness
- Core assumption: More information about model reasoning leads to more positive fairness perceptions and trust
- Evidence anchors: Distinguishes between formal and perceived fairness; cites studies showing explanations increase perceived fairness; limited external validation
- Break condition: If explanations reveal properties that conflict with people's fairness beliefs or if manipulative explanations are provided

### Mechanism 3
- Claim: XAI enables humans to implement subjective notions of fairness by incorporating domain expertise and human judgment into model adjustments
- Mechanism: Stakeholders use XAI feedback to make tradeoff decisions between fairness and accuracy, exclude predictors deemed unfair, or directly incorporate interpretable constraints into the model
- Core assumption: Human-in-the-loop adjustments based on XAI insights lead to fairer outcomes that reflect domain-specific ethical standards
- Evidence anchors: Mentions XAI enabling implementation of subjective fairness notions; discusses domain expert tradeoff decisions; limited external support
- Break condition: If humans lack adequate tools or knowledge to make informed adjustments, or if their input makes fairness worse

## Foundational Learning

- Concept: Distinguish between formal fairness (compliance with mathematical fairness criteria) and perceived fairness (human judgments about fairness)
  - Why needed here: The paper organizes claims along these two dimensions, and understanding the difference is crucial for evaluating XAI's role in each
  - Quick check question: If a model achieves statistical parity but provides no explanations, which fairness dimension is satisfied?

- Concept: Understand the difference between epistemic and substantial facets of fairness
  - Why needed here: The paper distinguishes between measuring fairness properties (epistemic) and the actual model properties themselves (substantial)
  - Quick check question: When XAI methods like feature importance are integrated into training algorithms, which facet (epistemic or substantial) is being addressed?

- Concept: Recognize the power dynamics in XAI-enabled fairness reporting
  - Why needed here: The paper highlights how stakeholders producing fairness reports shape perceptions of other stakeholders
  - Quick check question: Who controls the design choices on transparency of a model, and how might this influence downstream fairness perceptions?

## Architecture Onboarding

- Component map: Literature review pipeline (database search → filtering → snowballing) → Claim extraction and coding system (MAXQDA-based qualitative analysis) → Categorization framework (seven archetypal claims organized by fairness dimensions) → Evidence evaluation matrix (conceptual support, applied ML support, human subject studies)

- Critical path: 1. Systematic literature review to identify claims 2. Inductive coding to categorize claims 3. Critical evaluation of each claim type 4. Synthesis into archetypal claims with caveats

- Design tradeoffs: Breadth vs. depth (comprehensive coverage of 175 papers vs. detailed analysis of fewer claims), Automation vs. manual review (database search efficiency vs. nuanced claim identification), Technical vs. normative focus (balancing methodological critique with ethical considerations)

- Failure signatures: Claims lacking specific evidence or relying solely on intuition, Misalignment between fairness desiderata and XAI capabilities, Vagueness about which XAI method enables which fairness dimension for which stakeholder

- First 3 experiments: 1. Replicate the literature review with a different search string to test claim saturation 2. Apply the seven archetypal claim framework to a new domain (e.g., healthcare XAI) 3. Design a survey to measure how stakeholders interpret different types of fairness claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we measure and ensure that fairness perceptions enabled by XAI are "appropriate" or calibrated rather than inappropriately positive?
- Basis in paper: The paper emphasizes the need for "appropriate, i.e., calibrated, fairness perceptions" rather than just positive ones, citing concerns that XAI can create unwarranted trust in unfair models
- Why unresolved: Current research shows XAI can increase perceived fairness even for unfair models, but there's no established framework for measuring or ensuring calibrated fairness perceptions across different stakeholders and contexts
- What evidence would resolve it: Studies demonstrating reliable methods to measure the calibration between actual model fairness and user perceptions, along with interventions that successfully align these perceptions with reality

### Open Question 2
- Question: What are the technical limitations of current XAI methods in detecting and explaining intersectional unfairness (when multiple protected attributes interact)?
- Basis in paper: The paper notes that "there are facets to fairness that conventional XAI methods will never be able to reveal, such as contextual and societal factors" and mentions intersectional fairness as an underrepresented area
- Why unresolved: Existing XAI methods like feature importance and counterfactual explanations are designed for single attributes and struggle with the complex interactions between multiple protected characteristics
- What evidence would resolve it: Development and validation of XAI methods specifically designed to detect and explain intersectional unfairness, with demonstrated effectiveness across diverse real-world datasets

### Open Question 3
- Question: How can we design XAI methods that are resistant to manipulation (fairwashing) while still providing useful insights for fairness evaluation?
- Basis in paper: The paper discusses how explanations can be manipulated to conceal biases and introduces the concept of "fairwashing" where deceptive explanations are provided to hide unfair practices
- Why unresolved: Current post-hoc explanation methods like LIME and SHAP can be manipulated, yet they remain popular tools for fairness evaluation, creating a tension between usability and security
- What evidence would resolve it: Empirical studies comparing different XAI methods' vulnerability to manipulation and development of robust explanation methods that balance transparency with resistance to adversarial attacks

## Limitations
- Systematic review may have missed relevant papers due to database coverage gaps or keyword limitations
- Qualitative coding process relies on subjective interpretation of claim types
- Evidence evaluation is constrained by publication bias toward positive results

## Confidence
- High confidence in claims about XAI enabling fairness reporting and analysis, supported by multiple applied ML studies
- Medium confidence in claims about XAI mitigating unfairness, limited by lack of rigorous empirical validation
- Low confidence in claims about XAI increasing fairness perceptions, with minimal human subject evidence

## Next Checks
1. Replicate the literature review using alternative search terms and databases to assess claim saturation
2. Conduct empirical studies measuring the actual impact of XAI on different stakeholder groups' fairness perceptions
3. Develop and validate a taxonomy of fairness stakeholders to better specify which XAI methods serve which fairness dimensions for which groups