---
ver: rpa2
title: A Data Source for Reasoning Embodied Agents
arxiv_id: '2309.07974'
source_url: https://arxiv.org/abs/2309.07974
tags:
- property
- agent
- location
- pitch
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a data generation framework for training embodied
  agents in a 3D gridworld environment. The framework generates context-query-answer
  triples where the context represents dynamic world states, and queries test various
  reasoning capabilities including temporal, spatial, and property-based reasoning.
---

# A Data Source for Reasoning Embodied Agents

## Quick Facts
- arXiv ID: 2309.07974
- Source URL: https://arxiv.org/abs/2309.07974
- Reference count: 22
- Key outcome: Framework generates context-query-answer triples for training embodied agents; GPT-2 excels at property queries but struggles with geometric reasoning, while graph-structured transformer shows promise for structured reasoning

## Executive Summary
This work introduces a data generation framework for training embodied agents in a 3D gridworld environment. The framework creates context-query-answer triples where the context represents dynamic world states, and queries test various reasoning capabilities including temporal, spatial, and property-based reasoning. Two baseline models are evaluated: a pre-trained GPT-2 fine-tuned on text-formatted world states, and a graph-structured Transformer operating directly on a knowledge-graph representation. Experiments show that GPT-2 performs well on property queries but struggles with geometric reasoning, while the graph-structured model shows promise for structured reasoning tasks. The authors emphasize that the data generator is a flexible resource for researchers to create custom datasets and train reasoning models, rather than a fixed benchmark.

## Method Summary
The method involves generating dynamic world states in a 3D gridworld with NPCs, then creating context-query-answer triples through automated query generation. Two baseline models are trained: GPT-2 is fine-tuned on text-formatted world states, while a graph-structured Transformer uses relational embeddings in its attention mechanism to operate on knowledge-graph representations. Both models are trained using Adam optimizer for 5,000 epochs with linear warmup and cosine decay. The evaluation uses exact match error as the primary metric across different query types (properties, spatial, temporal, geometric) and data complexity levels.

## Key Results
- GPT-2 achieves near-zero test loss on property queries but struggles with geometric reasoning tasks
- Graph-structured Transformer with relational embeddings outperforms GPT-2 on geometric queries when context exceeds 1024 tokens
- Performance degrades for both models as world complexity increases, but sequence models are more severely affected by token limits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretrained language models provide strong inductive bias for property-based reasoning in structured worlds.
- Mechanism: GPT-2's pretraining on vast text corpora enables it to recognize and recall object-property associations quickly, making it highly effective at answering queries about static attributes (e.g., "what is the color of bob?").
- Core assumption: The pretraining distribution overlaps sufficiently with the vocabulary and structure of the generated world-state text.
- Evidence anchors:
  - [abstract] "GPT-2 performs well on property queries but struggles with geometric reasoning"
  - [section] "GPT-2 has a near zero test set loss for the properties queries"
  - [corpus] Weak; corpus contains newer works on embodied agents but not specifically on pretraining effects.
- Break condition: If world-state text uses novel tokens or syntactic structures not seen during pretraining, or if property queries require complex multi-hop reasoning.

### Mechanism 2
- Claim: Graph-structured transformers can explicitly model relational dependencies in the world-state.
- Mechanism: The model uses relational embeddings (rij) in the attention mechanism to encode direct relationships between objects and their properties, enabling reasoning over structured knowledge graphs.
- Core assumption: The relational structure of the world-state is sufficiently informative and accurately represented in the graph.
- Evidence anchors:
  - [section] "We propose a novel way of encoding relational information directly into the self-attention mechanism"
  - [section] "relational embeddings result in a slightly lower test loss than random-hash"
  - [corpus] Weak; no corpus papers directly evaluate relational embeddings for this exact task.
- Break condition: If the relational graph becomes too large or sparse, or if the relational embeddings are not adequately learned.

### Mechanism 3
- Claim: Increasing data complexity (NPC count, world size) degrades performance for both models but exposes limitations of the sequence model.
- Mechanism: Larger worlds and more entities increase the sequence length, causing GPT-2 to clip information beyond its 1024-token limit, while the graph model scales more gracefully.
- Core assumption: Sequence length is a hard constraint for the GPT-2 model.
- Evidence anchors:
  - [section] "the sequence length becomes longer than 1024 tokens... it is not able to answer queries where the relevant information is outside of the 1024 tokens"
  - [section] "the performance does decrease when we increase the number of NPCs... the Structured+Transformer model begins to outperform"
  - [corpus] Weak; corpus does not include papers analyzing sequence length limitations for embodied agents.
- Break condition: If a long-context language model is used, or if the world complexity is managed through hierarchical abstraction.

## Foundational Learning

- Concept: Structured knowledge graph representation.
  - Why needed here: The world-state is naturally relational (objects, properties, spatial relations). Representing it as a graph allows explicit modeling of these relations.
  - Quick check question: What are the two types of nodes in the graph-structured representation?

- Concept: Pretrained language model fine-tuning.
  - Why needed here: GPT-2 is not trained on structured world-state text; fine-tuning adapts it to this new domain while leveraging its strong generalization from pretraining.
  - Quick check question: Why does GPT-2 perform better on property queries than geometric queries?

- Concept: Relational embeddings in attention.
  - Why needed here: Vanilla transformers treat inputs as a set; relational embeddings inject explicit structural information about how tokens relate to each other.
  - Quick check question: How do relational embeddings differ from positional embeddings?

## Architecture Onboarding

- Component map:
  Data generator -> World state (text/structured) -> Query generator -> (C, Q, A) triples
  Sequence model: GPT-2 (fine-tuned) -> Predict text answer
  Structured model: Transformer encoder + relational embeddings -> Predict text answer and memid
  Training loop: Adam optimizer, cross-entropy loss, optional memid loss

- Critical path:
  Generate scene -> Flatten to text or graph -> Encode query -> Model processes context+query -> Predict answer -> Compute loss -> Update parameters

- Design tradeoffs:
  GPT-2: Simple to implement, strong on properties, limited by sequence length, no explicit relational reasoning
  Structured: Explicit relational modeling, better on complex queries, requires careful featurization, larger model

- Failure signatures:
  GPT-2: Struggles with geometric/spatial queries, fails when relevant info is beyond 1024 tokens
  Structured: Lower accuracy on property queries, needs more training data, sensitive to featurization choices

- First 3 experiments:
  1. Train both models on Properties-only dataset, compare exact match error
  2. Train both models on All dataset, plot train/val loss curves
  3. Vary number of NPCs (4 vs 8) and retrain, measure degradation in each model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does increasing the number of timesteps affect the performance of sequence context models when the context exceeds 1024 tokens?
- Basis in paper: [explicit] The paper mentions that when the number of timesteps increases, the sequence length becomes longer than 1024 tokens, which is the length GPT-2 is trained on, and that the model clips the context at 1024 tokens.
- Why unresolved: The paper does not provide experimental results showing the performance degradation of sequence context models when the context length exceeds 1024 tokens.
- What evidence would resolve it: Experiments showing the performance of sequence context models with varying context lengths, particularly when the context exceeds 1024 tokens, would resolve this question.

### Open Question 2
- Question: How well do the proposed models generalize to out-of-domain scenarios where new objects or properties are introduced?
- Basis in paper: [explicit] The paper mentions that in real-world scenes, new objects and properties may arise at any given time, and they provide a small experiment showing that the Sequence+GPT-2 model generalizes significantly better than the Structured+Transformer model when new color properties are introduced.
- Why unresolved: The paper does not provide a comprehensive evaluation of the models' ability to generalize to various out-of-domain scenarios, such as new object types or combinations of properties.
- What evidence would resolve it: Experiments evaluating the models' performance on a diverse set of out-of-domain scenarios, including new object types, properties, and combinations, would resolve this question.

### Open Question 3
- Question: How does the proposed relational embedding approach compare to other methods for encoding relational information in transformer models?
- Basis in paper: [explicit] The paper proposes a novel way of encoding relational information directly into the self-attention mechanism of the transformer model and compares it to a random hash embedding approach.
- Why unresolved: The paper does not provide a comprehensive comparison of the proposed relational embedding approach with other methods for encoding relational information, such as relative position embeddings or other graph-based approaches.
- What evidence would resolve it: Experiments comparing the proposed relational embedding approach with other methods for encoding relational information in transformer models would resolve this question.

## Limitations
- GPT-2's 1024-token limit significantly impacts performance on larger world states, but this limitation is not addressed through architectural modifications
- The graph-structured model's performance gains are modest and may be sensitive to specific featurization choices
- The data generation framework introduces potential biases through its randomization parameters that are not fully characterized

## Confidence
- **High confidence**: GPT-2 performs well on property queries but struggles with geometric reasoning (supported by direct experimental results)
- **Medium confidence**: Graph-structured transformers with relational embeddings show promise for structured reasoning (supported by modest performance improvements, but limited ablation studies)
- **Medium confidence**: Increasing data complexity degrades performance for both models (supported by experiments, but degradation patterns could be more systematically characterized)

## Next Checks
1. **Featurization sensitivity analysis**: Systematically vary the featurization choices for the graph-structured model (node features, edge features, embedding dimensions) and measure the impact on performance across all query types.
2. **Long-context evaluation**: Replace GPT-2 with a long-context language model (e.g., Longformer or BigBird) and retrain on the same datasets to quantify the exact contribution of the 1024-token limitation.
3. **Novel property generalization**: Generate out-of-domain test sets with novel object properties not present in the training data and evaluate both models' ability to generalize their reasoning capabilities.