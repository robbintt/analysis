---
ver: rpa2
title: Manifold-based Verbalizer Space Re-embedding for Tuning-free Prompt-based Classification
arxiv_id: '2309.04174'
source_url: https://arxiv.org/abs/2309.04174
tags:
- space
- lle-inc
- language
- embeddings
- tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method called LLE-INC for prompt-based
  classification, which re-embeds the verbalizer space using manifold learning. The
  key idea is to preserve local properties within the same class as guidance for classification,
  addressing the challenge of high-dimensional verbalizer embeddings.
---

# Manifold-based Verbalizer Space Re-embedding for Tuning-free Prompt-based Classification

## Quick Facts
- **arXiv ID:** 2309.04174
- **Source URL:** https://arxiv.org/abs/2309.04174
- **Reference count:** 14
- **Primary result:** Introduces LLE-INC, a tuning-free method that re-embeds verbalizer space using manifold learning, achieving state-of-the-art performance on GLUE, CLUE, and CBLUE datasets without parameter tuning.

## Executive Summary
This paper presents LLE-INC, a novel approach for prompt-based classification that re-embeds verbalizer space using manifold learning. The method addresses the challenge of high-dimensional verbalizer embeddings by preserving local properties within the same class, enabling more effective distance metrics for classification. LLE-INC achieves state-of-the-art performance on various benchmark datasets while remaining completely tuning-free, leveraging the implicit information in pre-trained model output embeddings.

## Method Summary
LLE-INC re-embeds the verbalizer space by collecting [MASK] token embeddings from training instances and fitting a Locally Linear Embedding model that preserves intra-class neighborhood relationships. The method then transforms test instance embeddings into this re-embedded space and classifies them using k-nearest neighbors. The approach can be enhanced with optional contrastive learning to update the pre-trained model with task-specific knowledge. LLE-INC operates without parameter tuning by treating pre-trained models as knowledge bases and reconstructing the representation space based on training data relationships.

## Key Results
- Achieves state-of-the-art performance on GLUE, CLUE, and CBLUE benchmark datasets
- Outperforms automated verbalizers that require parameter tuning
- Demonstrates effectiveness across multiple pre-trained models (RoBERTa-Large, LLaMA-7B, LLaMA-13B)
- Successfully handles both English and Chinese classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLE-INC preserves intra-class neighborhood relationships to improve classification accuracy.
- Mechanism: The method re-embeds verbalizer embeddings into a new Euclidean space where linear relationships between data points within the same class are consistent with the original manifold space.
- Core assumption: Embeddings of [MASK] tokens from instances sharing the same class are proximate on a specific manifold in the original high-dimensional space.
- Evidence anchors:
  - [abstract] "preserves local properties within the same class as guidance for classification"
  - [section] "we posit that the embeddings of the [MASK] token from the instances that share the same class should be close to one another on a particular manifold"
  - [corpus] Weak evidence - no directly relevant papers found on this specific mechanism
- Break condition: If intra-class neighborhoods are not preserved in the original space, the re-embedding will not improve classification.

### Mechanism 2
- Claim: Re-embedding verbalizer space improves distance metrics compared to Euclidean distance in original high-dimensional space.
- Mechanism: Manifold learning estimates distances between nearby points with local neighbors and distant points with multi-hop neighbors along the manifold shape, creating a more accurate distance metric.
- Core assumption: High-dimensional verbalizer embeddings are distributed on highly distorted manifolds where Euclidean distance is inadequate.
- Evidence anchors:
  - [abstract] "the distance between high-dimensional verbalizer embeddings should not be measured by Euclidean distance due to the potential for non-linear manifolds"
  - [section] "verbalizer embeddings with even higher dimensions can be also distributed on highly distorted manifolds"
  - [corpus] Weak evidence - no directly relevant papers found on this specific mechanism
- Break condition: If verbalizer embeddings are actually distributed in Euclidean space without manifold distortion, re-embedding provides no benefit.

### Mechanism 3
- Claim: Tuning-free approach leverages implicit information in pre-trained model output embeddings.
- Mechanism: The method uses frozen pre-trained model parameters and reconstructs the representation space based on training data relationships, avoiding computational costs of parameter tuning.
- Core assumption: Output embeddings from frozen pre-trained models contain sufficient implicit information for effective classification without additional tuning.
- Evidence anchors:
  - [abstract] "even without tuning any parameters, our LLE-INC is on par with automated verbalizers with parameter tuning"
  - [section] "the PTMs are regarded as a 'knowledge base' to some extent"
  - [corpus] Weak evidence - no directly relevant papers found on this specific mechanism
- Break condition: If pre-trained models lack sufficient task-specific knowledge in their output embeddings, tuning-free approach will underperform.

## Foundational Learning

- Concept: Locally Linear Embedding (LLE)
  - Why needed here: LLE forms the basis for preserving local linear relationships between embeddings in the re-embedding process
  - Quick check question: How does LLE determine the weights for reconstructing each data point from its neighbors?

- Concept: Manifold learning
  - Why needed here: Manifold learning addresses the limitation of Euclidean distance in high-dimensional spaces by estimating distances along the manifold structure
  - Quick check question: What is the key assumption that makes manifold learning applicable to high-dimensional data?

- Concept: Contrastive learning
  - Why needed here: Contrastive learning updates the pre-trained model with task-specific knowledge by creating positive and negative instance pairs
  - Quick check question: How does the InfoNCE loss function distinguish between positive and negative instance pairs in contrastive learning?

## Architecture Onboarding

- Component map:
  Input [MASK] embeddings -> LLE-INC module -> Optional contrastive learning -> kNN classifier -> Output labels

- Critical path:
  1. Collect [MASK] token embeddings from training data
  2. Apply LLE-INC to re-embed original space
  3. (Optional) Update PTM with contrastive learning
  4. Transform test instance embeddings using re-embedded space
  5. Classify test instances using kNN in re-embedded space

- Design tradeoffs:
  - Tuning-free vs. parameter updating: Tuning-free saves computational resources but may underperform when PTM lacks task-specific knowledge
  - Intra-class neighbors vs. k-nearest neighbors: Intra-class constraint improves classification but requires sufficient data within each class
  - Dimension of re-embedded space: Higher dimensions preserve more information but increase computational cost

- Failure signatures:
  - Poor performance with very few training instances per class
  - Degradation when classes have overlapping manifold structures
  - Computational issues when re-embedding into very high dimensions

- First 3 experiments:
  1. Compare LLE-INC performance with and without parameter updating on a simple binary classification task
  2. Test sensitivity to re-embedding dimension by varying it from 20 to 400 on a multi-class dataset
  3. Evaluate performance degradation when training data per class is reduced below 5 instances

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal dimension for the re-embedded space vary across different tasks and model architectures?
- Basis in paper: [inferred] The paper mentions that the optimal dimensions fall into the range of 20 to 400 for RoBERTa-large and LLaMA models, but does not explore how this varies across tasks or models.
- Why unresolved: The paper does not provide a systematic analysis of how the optimal dimension varies with different factors, leaving this as an open hyperparameter tuning problem.
- What evidence would resolve it: A comprehensive study varying the re-embedding dimension across different tasks, model sizes, and architectures to determine optimal settings.

### Open Question 2
- Question: How does LLE-INC perform on other types of NLP tasks beyond classification, such as sequence labeling or generation?
- Basis in paper: [inferred] The paper focuses exclusively on classification tasks, leaving open the question of whether the method generalizes to other NLP problem types.
- Why unresolved: The paper does not explore the applicability of LLE-INC to other task types that also use prompt-based approaches.
- What evidence would resolve it: Experiments applying LLE-INC to sequence labeling tasks (e.g., NER, POS tagging) and generation tasks (e.g., summarization, translation) with prompt-based formulations.

### Open Question 3
- Question: What is the theoretical relationship between the manifold structure of verbalizer embeddings and task difficulty or dataset characteristics?
- Basis in paper: [explicit] The paper acknowledges that high-dimensional verbalizer embeddings can form non-linear manifolds but does not explore how this relates to task difficulty or dataset properties.
- Why unresolved: The paper demonstrates the practical effectiveness of addressing manifold issues but does not investigate the underlying relationship between manifold structure and task characteristics.
- What evidence would resolve it: A theoretical analysis or empirical study correlating manifold complexity metrics with task difficulty, dataset size, or other characteristics.

## Limitations
- Performance heavily depends on having sufficient training instances per class for reliable intra-class neighborhood preservation
- May fail when class manifolds overlap significantly, as the re-embedding process cannot distinguish between classes with similar manifold structures
- Effectiveness of tuning-free approach is contingent on pre-trained models containing sufficient implicit task-specific knowledge

## Confidence
- **High confidence**: The core mechanism of using manifold learning (LLE-INC) to re-embed verbalizer space and preserve intra-class neighborhood relationships is well-established theoretically and the experimental results demonstrate consistent performance improvements
- **Medium confidence**: The claim that tuning-free approach is "on par with" parameter-updating methods, as this depends on the specific task and whether the pre-trained model contains adequate implicit knowledge
- **Low confidence**: The generalizability of results to datasets and tasks not covered in the evaluation, particularly those with highly overlapping classes or very limited training data per class

## Next Checks
1. **Neighboring Class Overlap Test**: Evaluate LLE-INC performance on a dataset with deliberately overlapping class manifolds to determine the breaking point where the method fails to separate classes effectively.

2. **Training Data Threshold Analysis**: Systematically vary the number of training instances per class (k) from 1 to 50 and measure performance degradation to identify the minimum k required for reliable classification.

3. **Pre-trained Knowledge Sufficiency Test**: Compare tuning-free LLE-INC against the same method with parameter updating on tasks known to require specialized knowledge not present in general pre-trained models, to quantify when tuning becomes necessary.