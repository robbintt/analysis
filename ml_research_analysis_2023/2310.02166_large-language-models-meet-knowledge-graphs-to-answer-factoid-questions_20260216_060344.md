---
ver: rpa2
title: Large Language Models Meet Knowledge Graphs to Answer Factoid Questions
arxiv_id: '2310.02166'
source_url: https://arxiv.org/abs/2310.02166
tags:
- question
- answer
- subgraphs
- knowledge
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to enhance Large Language Models
  for factoid question answering by integrating structured knowledge from Knowledge
  Graphs. The approach involves generating answer candidates using a pre-trained text-to-text
  language model, extracting relevant subgraphs from a Knowledge Graph based on question
  entities and answer candidates, and ranking these subgraphs using linearized Transformer-based
  models.
---

# Large Language Models Meet Knowledge Graphs to Answer Factoid Questions

## Quick Facts
- arXiv ID: 2310.02166
- Source URL: https://arxiv.org/abs/2310.02166
- Reference count: 21
- Improves Hits@1 scores by 4-6% for factoid QA by integrating Knowledge Graph subgraphs

## Executive Summary
This paper presents a novel approach to enhance Large Language Models (LLMs) for factoid question answering by integrating structured knowledge from Knowledge Graphs (KGs). The method generates answer candidates using a pre-trained T5 model, extracts relevant subgraphs from Wikidata based on question entities and answer candidates, and re-ranks these candidates using linearized Transformer-based models. The approach demonstrates significant improvements in accuracy, particularly for complex question types, by leveraging the structural properties of subgraphs to distinguish correct from incorrect answers.

## Method Summary
The approach involves three main stages: (1) Generating answer candidates using a fine-tuned T5-SSM model with Diverse Beam Search, (2) Extracting subgraphs from Wikidata containing shortest paths from question entities to answer candidates, and (3) Re-ranking candidates using Transformer Encoders (MPNet or DistilBERT) on linearized subgraphs. The linearized subgraphs preserve structural information while being compatible with standard transformer architectures. The rankers are trained to distinguish "correct" subgraphs (containing the right answer) from "incorrect" ones based on structural properties like density, node count, and edge connectivity.

## Key Results
- Achieves 4-6% improvement in Hits@1 scores compared to base T5-SSM models
- Linearized subgraphs with MPNet outperform raw graph representations with Graphormer
- Correct subgraphs show lower density, fewer nodes/edges, and less complex cycles compared to incorrect ones
- Effectiveness varies by question type, with better performance on complex questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linearized subgraph representation enables transformer encoders to effectively capture relational patterns between question entities and answer candidates.
- Mechanism: The graph linearization process converts the subgraph into a sequence format where nodes and edges are ordered row-by-row from the adjacency matrix. Special tokens [unused1] and [unused2] highlight the answer candidate within the sequence, allowing the transformer to distinguish it from other entities.
- Core assumption: The linearized representation preserves enough structural information for the transformer to learn meaningful patterns about correct vs incorrect answer paths.
- Evidence anchors: [section]: "As a result of our analysis, we discover that 12.8% of subgraphs in the test split of the dataset contain the correct answer but are not identified as potential candidates. To address this issue, we emphasize the answer candidate entities. To achieve this, we include special tokens before and after the answer candidate's label in the final linearized sequence."
- Break condition: If the linearization process loses critical structural information or the transformer cannot effectively learn from the sequential representation of graph data.

### Mechanism 2
- Claim: Diverse Beam Search generates sufficiently varied answer candidates to enable effective re-ranking through subgraph analysis.
- Mechanism: The Diverse Beam Search algorithm uses a diversity penalty and dissimilarity term to produce answer candidates with higher variance than standard beam search. This creates a broader pool of candidates for subgraph extraction and ranking.
- Core assumption: The answer candidates generated by Diverse Beam Search include both correct and incorrect options that can be distinguished by their subgraph characteristics.
- Evidence anchors: [section]: "To solve the problem, we apply Diverse Beam Search (Vijayakumar et al., 2018), which produces a larger number of candidates and generates them with higher variance. Diverse Beam Search is formulated as follows: [equation showing diversity penalty and dissimilarity term]."
- Break condition: If Diverse Beam Search fails to generate enough diverse candidates or the generated candidates don't include the correct answer among top candidates.

### Mechanism 3
- Claim: Structural differences between correct and incorrect subgraphs (density, number of nodes/edges) provide discriminative features for re-ranking.
- Mechanism: The subgraph extraction algorithm creates paths from question entities to answer candidates. Analysis shows "incorrect" subgraphs tend to have higher density, more nodes, edges, simple cycles, and bridges compared to "correct" subgraphs. This geometric difference allows the ranker to distinguish correct answers.
- Core assumption: The structural properties of subgraphs correlate with answer correctness in a way that can be learned by the ranking model.
- Evidence anchors: [section]: "Table 3 displays the average of the number of nodes, edges, density, simple cycles (elementary circuits), and bridges (isthmus) for the 'incorrect' and 'correct' subgraphs. We analyze these metrics on our subgraph dataset of 13,491 'correct' subgraphs and 94,615 'incorrect' subgraphs."
- Break condition: If the structural differences between correct and incorrect subgraphs are not consistent or the ranker cannot effectively learn from these features.

## Foundational Learning

- Concept: Graph linearization and sequence modeling
  - Why needed here: The approach converts graph structures into sequences that can be processed by transformer models, which are inherently designed for sequential data.
  - Quick check question: Can you explain how the adjacency matrix is converted into a linear sequence and why this transformation preserves relevant information?

- Concept: Knowledge Graph Question Answering (KGQA) fundamentals
  - Why needed here: Understanding how questions are mapped to entities in knowledge graphs and how answers are extracted from graph structures is essential for implementing the subgraph extraction algorithm.
  - Quick check question: What are the key differences between semantic parsing and retrieval-based approaches in KGQA, and which one does this method use?

- Concept: Transformer encoder architectures and attention mechanisms
  - Why needed here: The ranking models use transformer encoders to process the linearized subgraphs, so understanding how self-attention works and how positional information is encoded is crucial.
  - Quick check question: How does the transformer encoder process the linearized subgraph sequence, and what role do the special tokens [unused1] and [unused2] play?

## Architecture Onboarding

- Component map:
  - Input: Natural language question
  - Question type classifier → Determines yes/no, count, or other question types
  - Entity linker → Extracts question entities (gold entities used in experiments)
  - LLM (T5-SSM) → Generates answer candidates using Diverse Beam Search
  - Subgraph extractor → Creates subgraphs from Wikidata using question entities and candidates
  - Graph linearization → Converts subgraphs to sequential format with answer highlighting
  - Ranker (MPNet/DistilBERT) → Re-ranks candidates based on subgraph analysis
  - Output: Ranked answer candidates with improved Hits@1 scores

- Critical path: Question → Entity extraction → Candidate generation → Subgraph extraction → Linearization → Ranking → Final answer

- Design tradeoffs:
  - Using linearized graphs vs raw graph structures (Graphormer): Linearized approach works better with transformer encoders and includes question context
  - Diverse Beam Search vs standard beam search: Higher variance in candidates but potentially more computational cost
  - Subgraph size vs completeness: Larger subgraphs may contain more information but increase computational complexity

- Failure signatures:
  - Hits@1 scores not improving despite re-ranking: May indicate the subgraph features are not discriminative enough or the ranker cannot learn from them
  - Pipeline execution time too long: Subgraph extraction from Wikidata for many candidates can be time-consuming
  - Low precision on specific question types: May indicate the approach works better for certain complexity types than others

- First 3 experiments:
  1. Implement and test the question type classifier on the Mintaka dataset to verify it correctly identifies yes/no, count, and other question types with high accuracy.
  2. Generate answer candidates using T5-SSM with Diverse Beam Search and verify that the candidates include both correct and incorrect options for a sample of questions.
  3. Extract subgraphs for a small set of question-answer pairs and manually inspect the structural differences between correct and incorrect subgraphs to validate the hypothesis about density and connectivity differences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed approach perform on multilingual factoid question answering datasets beyond Mintaka?
- Basis in paper: [inferred] The paper explicitly mentions that the approach was tested only on the English-language Mintaka dataset and states that "Mintaka possesses suites in other languages and our approach should definitely be evaluated on them."
- Why unresolved: The authors acknowledge this as a limitation and did not conduct experiments on other languages, leaving the performance on multilingual datasets unexplored.
- What evidence would resolve it: Testing the proposed approach on multilingual factoid QA datasets like LC-QuAD 2.0 (which has multilingual versions) or RuBQ 2.0 would provide concrete evidence of its cross-lingual performance.

### Open Question 2
- Question: How would incorporating entity linking affect the overall performance of the proposed system?
- Basis in paper: [inferred] The paper states "we have not proofed the full pipeline performance as we do not embed an Entity Linker and test our model on gold question entities," suggesting that entity linking was not part of the tested pipeline.
- Why unresolved: The authors acknowledge this limitation and did not evaluate the complete pipeline including entity linking, leaving the impact of this component on performance unclear.
- What evidence would resolve it: Implementing and testing the full pipeline with an entity linker (such as mGENRE) on the factoid question answering task would provide evidence of how entity linking affects the overall system performance.

### Open Question 3
- Question: Would using other generative Transformer models instead of T5 improve the performance of the proposed approach?
- Basis in paper: [inferred] The authors mention "we have not tested other Generative Transformers" as a limitation of their study, implying that the approach was only tested with T5 models.
- Why unresolved: The authors did not explore the performance of the proposed approach with different generative Transformer models, leaving the question of whether T5 is the optimal choice unanswered.
- What evidence would resolve it: Testing the proposed approach with other generative Transformer models like BART, GPT-3, or BLOOM and comparing their performance to the T5-based results would provide evidence of the impact of the choice of generative model on the overall system performance.

## Limitations
- Uses gold entity annotations rather than automatic entity linking, which may overestimate real-world performance
- Does not evaluate the complete pipeline with entity linking integrated
- Only tested on English-language Mintaka dataset, limiting cross-lingual generalization
- Subgraph linearization may lose critical structural information that could be preserved in raw graph representations

## Confidence

- **Medium Confidence**: The core mechanism of using subgraph structural differences for re-ranking (based on density and connectivity analysis)
- **Medium Confidence**: The linearization approach's effectiveness, as Graphormer with raw subgraphs shows worse performance despite being designed for graph data
- **Low Confidence**: The scalability of the approach to larger KGs or more complex question types beyond those tested

## Next Checks

1. **Structural preservation test**: Compare ranker performance using linearized vs. raw subgraph representations on a subset of questions to quantify information loss from linearization
2. **Entity linking robustness**: Implement automatic entity linking and measure performance degradation compared to gold entities across different entity linking systems
3. **Cross-KG generalization**: Test the complete pipeline on a different Knowledge Graph (e.g., DBpedia) with the same question set to validate KG independence assumptions