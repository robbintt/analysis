---
ver: rpa2
title: Converting Depth Images and Point Clouds for Feature-based Pose Estimation
arxiv_id: '2310.14924'
source_url: https://arxiv.org/abs/2310.14924
tags:
- images
- flexion
- image
- depth
- point
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Flexion images, a novel method for converting
  depth data into images that highlight spatial details more effectively than traditional
  depth images or Bearing Angle (BA) images. Flexion images are created by computing
  surface normals from horizontal/vertical and diagonal/antidiagonal neighboring points
  and encoding the difference between these normals into a grayscale image.
---

# Converting Depth Images and Point Clouds for Feature-based Pose Estimation

## Quick Facts
- arXiv ID: 2310.14924
- Source URL: https://arxiv.org/abs/2310.14924
- Authors: 
- Reference count: 24
- One-line primary result: Flexion images improve feature-based pose estimation on depth data, outperforming Bearing Angle images and matching color image performance.

## Executive Summary
This paper introduces Flexion images, a novel method for converting depth data into high-contrast grayscale images that highlight spatial details for feature-based pose estimation. By computing surface normals from orthogonal and diagonal point neighborhoods and encoding their angular difference, Flexion images capture geometric curvature while being rotation-invariant. Experimental results on synthetic and real-world datasets demonstrate that Flexion images consistently outperform Bearing Angle images and achieve comparable results to color images in visual odometry and RGB-D SLAM tasks.

## Method Summary
Flexion images are generated by computing two surface normal vectors from local point neighborhoods in a depth image: one from horizontal/vertical neighbors and another from diagonal/antidiagonal neighbors. The Flexion value at each pixel is the absolute cosine of the angle between these normals, scaled to 8-bit grayscale. The method includes noise removal via median filtering and produces rotation-invariant images that emphasize geometric features. Flexion images are evaluated using feature descriptors (AKAZE, ORB, SIFT, SURF) for visual odometry and integrated into ORB-SLAM3 for RGB-D SLAM, with performance measured against Bearing Angle images and color images.

## Key Results
- Flexion images achieved ATE of 3.99-7.13 m in visual odometry versus BA's 11.01-22.53 m on synthetic data
- In TUM RGB-D SLAM, Flexion 5×5 achieved ATE of 21.25 cm on desk scene versus BA's 67.64 cm
- Larger neighborhood sizes (7×7, 9×9) improved SLAM performance by reducing noise and widening edges
- Flexion images generally matched or exceeded color image performance in feature-based pose estimation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Flexion images encode spatial curvature by computing the angle between two surface normal vectors derived from different point neighborhoods, producing higher-contrast images than depth or Bearing Angle (BA) images.
- Mechanism: Surface normals are calculated using horizontal/vertical neighbors (n1) and diagonal/antidiagonal neighbors (n2). The Flexion value F is the absolute cosine of the angle between n1 and n2, scaled to 8-bit grayscale. This captures fine geometric changes that traditional depth maps miss.
- Core assumption: The difference between normals from orthogonal and diagonal neighborhoods robustly represents surface curvature and is invariant to rotation.
- Evidence anchors:
  - [abstract] "After noise removal, a neighborhood of points forms two normal vectors whose difference is encoded into this new conversion."
  - [section] "the first normal ⃗n1,i,j is based on its horizontal and vertical neighbors... and the second normal ⃗n2,i,j is based on its diagonal and antidiagonal neighbors... Finally, the Flexion Fi,j at point Pi,j is defined as Fi,j = | (⃗n1,i,j)ᵀ⃗n2,i,j |."
  - [corpus] No direct corpus evidence; method is novel.
- Break condition: If surface normals become unreliable due to severe noise or if point density is too low to compute meaningful differences between the two normal sets.

### Mechanism 2
- Claim: Flexion images are rotation-invariant because swapping horizontal/vertical and diagonal/antidiagonal normal sets does not change the final Flexion value.
- Mechanism: Rotating the image swaps the roles of the two normal sets, but since Flexion uses the absolute cosine of the angle between them, the result is invariant.
- Core assumption: The surface geometry is preserved under rotation, so the angle between the two normal sets remains the same.
- Evidence anchors:
  - [section] "Contrary to BA images, Flexion images use all neighboring points for calculation which makes a Flexion image rotation invariant. A roll rotation of more than 45° only swaps ⃗n1,i,j and ⃗n2,i,j which does not change the final value of F."
  - [section] "Fig. 7 demonstrates rotation invariance, which becomes obvious when comparing to Fig. 3."
  - [corpus] No corpus evidence for rotation invariance claim; stated as novel result.
- Break condition: If the point cloud is sparse or contains large gaps, rotation invariance may break down because the normal estimation becomes unstable.

### Mechanism 3
- Claim: Larger neighborhood sizes (e.g., 7×7, 9×9) in Flexion images reduce noise and widen edges, improving performance in RGB-D SLAM tasks.
- Mechanism: Increasing the distance to the central point smooths local noise and makes edges more prominent by integrating over larger surface patches.
- Core assumption: Noise reduction outweighs the loss of fine detail, and the larger patches still capture meaningful curvature.
- Evidence anchors:
  - [section] "Using a larger distance to calculate Flexion values results in less noise and wider edges. Figs. 9b to 9f show less noise, smoother surfaces, and thicker edges respectively."
  - [section] "Probably due to noise, a larger n in Flexion images has a positive effect on ATE and RPE."
  - [corpus] No direct corpus evidence; based on paper's synthetic and TUM RGB-D experiments.
- Break condition: If the neighborhood becomes too large relative to the object scale, fine details may be lost, harming feature detection.

## Foundational Learning

- Concept: Pinhole camera model and depth image formation
  - Why needed here: Flexion images are computed from depth images, so understanding how depth maps are generated from 3D points is essential for implementation.
  - Quick check question: Given a 3D point (X, Y, Z) and camera intrinsics (fx, fy, cx, cy), what pixel coordinates does it project to?

- Concept: Surface normal estimation from point clouds
  - Why needed here: Flexion images rely on accurate normal vectors from local neighborhoods; without this, the method fails.
  - Quick check question: How do you compute a surface normal from three neighboring points in a depth image?

- Concept: Feature descriptors (AKAZE, ORB, SIFT, SURF) and matching
  - Why needed here: Flexion images are evaluated using feature-based pose estimation, so familiarity with how these descriptors work on grayscale images is required.
  - Quick check question: What distinguishes ORB from SIFT in terms of computational efficiency and robustness to rotation?

## Architecture Onboarding

- Component map: Depth image -> Median filter -> Flexion image generation -> Feature extraction -> Feature matching -> Pose estimation -> Error evaluation
- Critical path: 1. Read depth image -> 2. Apply median filter -> 3. Compute Flexion image -> 4. Extract features -> 5. Match features -> 6. Estimate pose -> 7. Evaluate error
- Design tradeoffs:
  - Neighborhood size: Larger -> smoother, less noise, but coarser edges; smaller -> sharper details, but more noise
  - Feature choice: AKAZE/ORB faster, SIFT/SURF more robust but slower
  - Pre-filtering: Median preserves edges but may not remove all noise; Gaussian smooths but blurs edges
- Failure signatures:
  - Very low-contrast Flexion images: Likely due to low surface variation or too large neighborhood
  - Feature matching failures: Could be from noise, insufficient texture, or poor normal estimation
  - Unstable pose estimates: May indicate insufficient overlap or repetitive geometry
- First 3 experiments:
  1. Generate Flexion images with 3×3, 5×5, and 7×7 neighborhoods on a synthetic depth map; visually inspect edge preservation and noise reduction.
  2. Run AKAZE feature matching on raw depth vs Flexion images; compare number of matches and matching quality.
  3. Integrate Flexion images into a simple visual odometry pipeline; measure ATE vs using raw depth or Bearing Angle images.

## Open Questions the Paper Calls Out

- Open Question 1: How do Flexion images compare to other depth-based representations like RGB-D feature descriptors or learned depth embeddings in terms of computational efficiency and accuracy for SLAM tasks?
- Open Question 2: How does the performance of Flexion images degrade in environments with low geometric complexity or highly repetitive structures?
- Open Question 3: Can Flexion images be effectively integrated with deep learning-based SLAM frameworks, and how do they compare to raw depth inputs in such systems?
- Open Question 4: How sensitive are Flexion images to sensor noise and calibration errors, and what preprocessing steps can mitigate these effects?

## Limitations
- Implementation details for edge cases in normal computation are not fully specified
- Method's robustness to varying point cloud densities and noise levels is not thoroughly explored
- Performance generalization to other sensors or scenarios beyond tested datasets is not demonstrated

## Confidence
- Core mathematical formulation: High confidence (explicitly described with formulas)
- Rotation invariance claim: Medium confidence (supported by visual comparison but lacks quantitative validation)
- Performance improvements: Medium confidence (based on specific datasets and feature descriptors)

## Next Checks
1. Conduct quantitative experiments to validate the rotation invariance claim by measuring feature matching consistency across rotated depth images
2. Test Flexion images on additional datasets with varying point cloud densities and noise levels to assess robustness
3. Compare Flexion images with other state-of-the-art depth-to-image conversion methods (e.g., normal maps, depth gradients) on the same tasks to establish relative performance