---
ver: rpa2
title: Convergence of SGD for Training Neural Networks with Sliced Wasserstein Losses
arxiv_id: '2307.11714'
source_url: https://arxiv.org/abs/2307.11714
tags:
- assumption
- which
- wasserstein
- convergence
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work provides theoretical guarantees for convergence of Stochastic
  Gradient Descent (SGD) when training generative Neural Networks using Sliced Wasserstein
  (SW) losses. While convergence has been observed in practice, there were no prior
  theoretical explanations for this behavior.
---

# Convergence of SGD for Training Neural Networks with Sliced Wasserstein Losses

## Quick Facts
- arXiv ID: 2307.11714
- Source URL: https://arxiv.org/abs/2307.11714
- Reference count: 5
- This work provides theoretical guarantees for convergence of SGD when training generative Neural Networks using Sliced Wasserstein losses.

## Executive Summary
This paper bridges the gap between empirical observations and theoretical understanding of SGD convergence when training generative neural networks with Sliced Wasserstein (SW) losses. While practitioners have observed good convergence behavior in practice, there were no prior theoretical explanations. The authors prove that under realistic assumptions—including compactly supported distributions and reasonable network regularity—interpolated SGD trajectories converge to sub-gradient flow solutions as learning rate decreases. Under stricter assumptions with discrete input measures, noised and projected SGD schemes converge to sets of generalized critical points of the loss function.

## Method Summary
The paper analyzes SGD convergence for generative neural networks trained with Sliced Wasserstein losses. The method involves implementing the SW distance computation through random projections and sorting, defining neural networks satisfying specific regularity conditions (piecewise C²-smooth and jointly Lipschitz), and implementing SGD training with optional noise and projection. The theoretical analysis uses Clarke subdifferential theory to handle non-smoothness from sorting operations, proving convergence results for interpolated trajectories to sub-gradient flow equations and for noised/projected schemes to generalized critical points.

## Key Results
- Interpolated SGD trajectories converge to sub-gradient flow solutions as learning rate decreases
- Noised and projected SGD schemes converge to generalized critical points under stricter assumptions
- Theoretical foundation explains empirical success of SW-based training in generative modeling

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Interpolated SGD trajectories converge to sub-gradient flow solutions as the learning rate decreases.
- **Mechanism:** The piecewise affine interpolation of discrete SGD steps approximates the continuous-time Clarke differential inclusion. As the step size goes to zero, the discrete dynamics align with the gradient flow of the Sliced Wasserstein loss.
- **Core assumption:** The loss function F is locally Lipschitz and satisfies conditions for non-smooth non-convex optimization.
- **Break condition:** If the learning rate does not decay or if the loss violates the local Lipschitz assumption, the trajectory may diverge.

### Mechanism 2
- **Claim:** Noised and projected SGD schemes converge to generalized critical points of the loss.
- **Mechanism:** Additive noise improves exploration while projection prevents divergence, ensuring long-run limits approach KKT points.
- **Core assumption:** The input and data measures are discrete, and the neural network is path differentiable.
- **Break condition:** If measures are not discrete or the network lacks path differentiability, the result does not apply.

### Mechanism 3
- **Claim:** SW distance is computationally tractable for gradient-based optimization.
- **Mechanism:** SW is computed by projecting high-dimensional measures onto random 1D lines, sorting projections, and summing squared differences, making it differentiable almost everywhere.
- **Core assumption:** The network and loss are locally Lipschitz, allowing almost-everywhere differentiability.
- **Break condition:** If projection or sorting becomes non-differentiable at ambiguous points, the gradient is undefined.

## Foundational Learning

- **Concept:** Clarke subdifferential for non-smooth functions
  - Why needed here: SW loss is non-smooth due to sorting and projection operations
  - Quick check question: If F is not differentiable at u, what set replaces the gradient in the flow equation?

- **Concept:** Path differentiability and conservative fields
  - Why needed here: To ensure SGD trajectories correspond to meaningful flow solutions
  - Quick check question: What property must F have so that the chain rule holds for any absolutely continuous curve u(s)?

- **Concept:** Karush-Kuhn-Tucker (KKT) conditions for constrained optimization
  - Why needed here: Projected SGD turns the problem into a constrained one
  - Quick check question: How does projecting onto a ball change the optimality conditions compared to unconstrained optimization?

## Architecture Onboarding

- **Component map:** Input measures x and y → Neural network T(u,x) → Sliced Wasserstein loss SW²(Tu#x, y) → SGD update (with optional noise+projection) → Convergence monitoring

- **Critical path:** 1. Sample projections θ and data batches → 2. Compute SW loss via sorting and summation → 3. Backpropagate to get almost-everywhere gradient → 4. Update parameters with SGD → 5. Track interpolation and/or KKT residual

- **Design tradeoffs:**
  - Projection count vs. gradient variance: more projections give lower SW variance but higher compute
  - Noise level vs. convergence speed: noise aids exploration but may slow down convergence to critical points
  - Projection radius vs. expressiveness: tighter radius enforces Lipschitzness but may limit model capacity

- **Failure signatures:**
  - Gradient explosion or NaNs → check Lipschitzness and projection clipping
  - No decrease in SW loss → increase projection count or reduce learning rate
  - Stuck at high loss → increase noise level or relax projection radius

- **First 3 experiments:**
  1. Verify that the interpolated trajectory approaches the SW flow by plotting SW loss vs. interpolation time for decreasing learning rates
  2. Check that projected SGD stays within the ball by logging parameter norms and SW loss
  3. Compare convergence speed and final loss for with/without noise to assess exploration benefit

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the convergence results be extended to cases where the input data measure x is not discrete?
  - Basis in paper: The paper explicitly states this is a core limitation, requiring discrete input measures for the stronger convergence result
  - Why unresolved: The stability of path differentiability through integration for non-discrete measures is not known
  - What evidence would resolve it: A proof showing composition with integration over continuous measures remains path differentiable, or a counterexample

- **Open Question 2:** Can the flow approximation result from Theorem 1 be tied to Sliced Wasserstein Flows?
  - Basis in paper: The paper mentions this as another avenue for future study
  - Why unresolved: The differential inclusion cannot be directly seen as a flow of F due to non-differentiable nature
  - What evidence would resolve it: A mathematical framework connecting the differential inclusion to a flow in Sliced Wasserstein space

- **Open Question 3:** What are the practical implications of the convergence results for training generative neural networks with SW losses?
  - Basis in paper: The paper shows theoretical convergence but doesn't discuss practical implications
  - Why unresolved: While theoretical results provide guarantees, practical implications are not explicitly discussed
  - What evidence would resolve it: Experimental results comparing standard SGD vs modified SGD for training generative NNs with SW losses

## Limitations

- The convergence results depend heavily on stringent regularity assumptions about neural network architecture that may not hold for common architectures
- Results require compact support for input and data distributions, which may not reflect real-world applications with unbounded data
- The stronger convergence to critical points specifically requires discrete input measures, limiting applicability to continuous data distributions

## Confidence

- **High confidence:** The convergence of interpolated SGD to sub-gradient flow solutions - follows from standard results in non-smooth optimization
- **Medium confidence:** The convergence of noised and projected SGD to generalized critical points - theoretically sound but depends on specific assumptions
- **Medium confidence:** The computational tractability of SW as a proxy for Wasserstein distance - empirically validated but theoretical analysis focuses on convergence

## Next Checks

1. **Architecture verification:** Test whether common neural network architectures (e.g., standard ReLU networks) satisfy the required piecewise C²-smooth and jointly Lipschitz assumptions for various input distributions

2. **Discrete vs continuous measures:** Empirically compare convergence behavior when training with discrete vs continuous input measures to validate when the stronger convergence guarantees apply

3. **Projection and noise sensitivity:** Systematically vary the projection radius and noise level in SGD to determine their impact on both convergence speed and the quality of the final solution, particularly for different data distributions