---
ver: rpa2
title: Improving grapheme-to-phoneme conversion by learning pronunciations from speech
  recordings
arxiv_id: '2307.16643'
source_url: https://arxiv.org/abs/2307.16643
tags:
- pronunciation
- speech
- words
- languages
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to improve Grapheme-to-Phoneme (G2P)
  conversion systems by learning pronunciation examples from audio recordings. The
  authors propose a pipeline that bootstraps a G2P model with a small set of annotated
  examples, trains a multilingual phone recognition system, decodes speech recordings
  with phonetic representations, and learns pronunciation dictionaries for out-of-vocabulary
  words.
---

# Improving grapheme-to-phoneme conversion by learning pronunciations from speech recordings

## Quick Facts
- arXiv ID: 2307.16643
- Source URL: https://arxiv.org/abs/2307.16643
- Reference count: 0
- Primary result: Method improves G2P performance by learning pronunciations from speech, reducing phone error rates by 2-6% across five languages in low-resource scenarios.

## Executive Summary
This paper presents a novel approach to improving Grapheme-to-Phoneme (G2P) conversion systems by leveraging speech recordings to learn pronunciation examples, particularly beneficial for low-resource languages. The authors propose a pipeline that bootstraps a G2P model with a small set of annotated examples, trains a multilingual phone recognition system, and uses lexicon learning to extract pronunciation dictionaries from decoded speech. These learned pronunciations are then used to retrain the G2P system, resulting in consistent improvements across multiple languages and data scenarios. The method is particularly effective when limited annotated data is available, achieving up to 10% absolute reduction in phone error rate for languages like French, Danish, and Polish with only 50 seed words.

## Method Summary
The proposed method follows a bootstrapping approach to improve G2P conversion by learning pronunciation examples from audio recordings. It begins with a baseline G2P model trained on a small set of manually-annotated pronunciations. This model is used to generate pronunciations for the vocabulary in the target language speech corpus. A multilingual phone recognition system is then trained using speech data pooled from multiple languages, including the target language. This phone recognizer decodes the audio data in the target language, producing phoneme sequences that are aligned to graphemes using lexicon learning with Viterbi alignment. The resulting pronunciation dictionaries are pooled with the original seed set to retrain the G2P model. The authors also investigate iterative self-training, where the retrained G2P model is used to re-annotate audio, generating increasingly accurate pronunciation examples over multiple cycles.

## Key Results
- The proposed approach consistently improves phone error rates across five languages (English, French, Danish, Polish, and Turkish) and amounts of available data.
- In low-resource scenarios with 500 seed words, the method reduces phone error rates by 2-6% on average across all languages.
- For languages like French, Danish, and Polish with only 50 annotated words, the method achieves a 6-10% absolute reduction in phone error rate over the baseline system.
- Iterative self-training leads to further performance gains, though with diminishing returns after several iterations.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low-resource G2P models benefit from additional pronunciation examples learned from speech, even if those examples contain errors.
- Mechanism: The phone recognition system decodes audio using hypothesized phoneme labels from the G2P system, and lexicon learning aligns these decoded sequences to graphemes, generating novel <word, pronunciation> pairs. These pairs, despite potential errors, increase training data diversity.
- Core assumption: Quantity of pronunciation examples is more valuable than quality in low-resource scenarios, because the model can learn to generalize from noisy data.
- Evidence anchors:
  - [abstract] "we propose a method to improve the G2P conversion task by learning pronunciation examples from audio recordings"
  - [section] "we observe a positive impact across all tested values of k, with the best results occurring when k = 1. This is an interesting observation, as we have noted that, on average, learned dictionaries at k = 1 underperform when compared with corresponding G2P-generated dictionaries"
  - [corpus] Weak evidence: no direct citations on quantity-vs-quality trade-offs in low-resource G2P, but related work on self-training suggests similar patterns.
- Break condition: If the phone recognition system produces too many errors or the lexicon learning fails to align phoneme sequences to graphemes, the learned pronunciations become too noisy to be useful.

### Mechanism 2
- Claim: Multilingual phone recognition generalizes across languages, allowing a single model to decode pronunciations in a target language with minimal target-language data.
- Mechanism: The phone recognizer is trained on multilingual speech data pooled with target-language data, learning universal phonetic representations that transfer to the target language.
- Core assumption: Acoustic-phonetic patterns are sufficiently shared across languages that a multilingual model can decode pronunciations for an unseen language.
- Evidence anchors:
  - [abstract] "we train a multilingual phone recognition system, which then decodes speech recordings with a phonetic representation"
  - [section] "When decoding speech samples in the target language, we use a 5gram language model learned on the phone-level generated transcripts for the 'Train Set'. The language model enforces the system to decode plausible pronunciations restricted to the target language's phonotactics"
  - [corpus] Moderate evidence: recent work on universal phone recognition [25, 26] shows multilingual models can learn shared phonetic representations.
- Break condition: If the target language has a significantly different phonetic inventory or phonotactic constraints, the multilingual model may fail to generalize.

### Mechanism 3
- Claim: Iterative self-training further improves G2P performance by refining pronunciation hypotheses over multiple cycles.
- Mechanism: After learning new pronunciations from speech, the G2P system is retrained and used to re-annotate audio, generating increasingly accurate pronunciation examples in each iteration.
- Core assumption: Each iteration improves pronunciation quality, so the G2P benefits from seeing progressively more accurate examples.
- Evidence anchors:
  - [abstract] "we investigate additional improvements as we iterate in a type of 'self-training' scenario"
  - [section] "we always observe improvements by self-training. For example, in the English system with a seed set of 100 words, we reduce PER relative to the baseline from 17.14% (iteration 1) to 20.96% (iteration 5)"
  - [corpus] Limited evidence: self-training is common in ASR but less studied in G2P; results here suggest it works but diminishing returns occur.
- Break condition: If pronunciation errors compound over iterations instead of being corrected, performance may degrade after a few cycles.

## Foundational Learning

- Concept: Grapheme-to-phoneme correspondence
  - Why needed here: G2P systems must map characters to phonemes; understanding language-specific correspondence rules is essential for both baseline models and interpreting learned pronunciations.
  - Quick check question: What is the main difference between languages with regular orthography (e.g., Turkish) and irregular orthography (e.g., English) in terms of G2P mapping?

- Concept: Phoneme recognition and alignment
  - Why needed here: The phone recognizer decodes audio to phonemes, and lexicon learning must align these to graphemes; understanding HMMs and Viterbi alignment is crucial.
  - Quick check question: How does the lexicon learning step use Viterbi alignment to discover word boundaries from sentence-level phoneme sequences?

- Concept: Multilingual transfer learning
  - Why needed here: The approach relies on transferring phonetic knowledge from multiple languages to a target language; understanding how multilingual models generalize is key.
  - Quick check question: Why might a multilingual phone recognizer be more effective than a monolingual one for low-resource languages?

## Architecture Onboarding

- Component map:
  - G2P model (transformer encoder-decoder) → Phone recognizer (Kaldi TDNN) → Lexicon learner (HMM + Viterbi) → Updated G2P model
  - Data flow: seed pronunciations → G2P hypotheses → multilingual phone recognizer → decoded phonemes → lexicon learning → new pronunciations → retrain G2P

- Critical path:
  - The phone recognition step is most sensitive to data quality; errors here propagate to lexicon learning and degrade G2P retraining.

- Design tradeoffs:
  - Higher lexicon learning threshold (k) → fewer, higher-quality pronunciations but smaller dictionary
  - Lower k → more pronunciations but higher error rate; better for low-resource scenarios
  - Multilingual phone recognizer vs. monolingual: trade-off between generalization and language-specific accuracy

- Failure signatures:
  - G2P error rate increases after retraining → lexicon learning producing too many errors
  - Phone recognizer fails to decode target language → multilingual model not generalizing
  - Lexicon learner cannot align phonemes to graphemes → poor acoustic-phonetic mapping

- First 3 experiments:
  1. Run pipeline with k=1 and k=8; compare PER/WER to baseline to confirm quantity-over-quality hypothesis.
  2. Test multilingual phone recognizer on target language alone vs. with multilingual data to measure transfer benefit.
  3. Perform one iteration of self-training; check if PER improves and by how much to validate iterative gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform on high-resource languages with large pronunciation dictionaries?
- Basis in paper: [explicit] The paper focuses on low-resource scenarios and notes that "the impact of our approach is small when a large amount of data is available."
- Why unresolved: The paper only provides experimental results for low-resource languages and does not evaluate the method on high-resource languages with extensive pronunciation dictionaries.
- What evidence would resolve it: Experimental results comparing the proposed method's performance on high-resource languages (e.g., English, Mandarin) with large pronunciation dictionaries to existing state-of-the-art G2P systems.

### Open Question 2
- Question: What is the impact of different speech recognition model architectures on the proposed pronunciation learning method?
- Basis in paper: [explicit] The paper uses a Kaldi-based TDNN acoustic model for phone recognition, but mentions that "more robust speech recognition systems, perhaps pre-trained in a self-supervised fashion" could improve performance.
- Why unresolved: The paper only evaluates one speech recognition architecture and does not explore the impact of alternative architectures or pre-trained models on the pronunciation learning process.
- What evidence would resolve it: Comparative experiments using different speech recognition architectures (e.g., end-to-end models, self-supervised pre-trained models) to assess their impact on the proposed pronunciation learning method.

### Open Question 3
- Question: How does the proposed method handle out-of-vocabulary words with irregular grapheme-to-phoneme correspondence?
- Basis in paper: [inferred] The paper mentions that future work should investigate "solutions to learn novel pronunciations for infrequent words with irregular grapheme-to-phoneme correspondence, such as proper names, loan words, or domain-specific tokens."
- Why unresolved: The paper does not provide experimental evidence on the method's performance for handling out-of-vocabulary words with irregular correspondences.
- What evidence would resolve it: Experiments evaluating the proposed method's ability to learn pronunciations for out-of-vocabulary words with irregular grapheme-to-phoneme correspondences, comparing it to existing approaches for handling such words.

## Limitations
- The method's effectiveness is limited by the amount of target-language speech data available, with diminishing returns when substantial annotated data already exists.
- The iterative self-training approach shows diminishing returns after several iterations, suggesting a practical limit to how much improvement can be gained through repeated cycles.
- The approach relies heavily on the quality of the phone recognition system and lexicon learning alignment, with errors potentially propagating through the pipeline.

## Confidence
- **High Confidence**: The core finding that learning pronunciation examples from speech recordings improves G2P performance across multiple languages and data scenarios.
- **Medium Confidence**: The mechanism by which multilingual phone recognition generalizes to target languages through shared phonetic representations.
- **Medium Confidence**: The effectiveness of iterative self-training for further G2P improvements.

## Next Checks
1. **Error Analysis on Learned Pronunciations**: Conduct detailed analysis comparing the quality of learned pronunciations versus G2P-generated ones at different threshold values (k=1, k=3, k=8) to better understand the quantity-vs-quality trade-off and identify failure patterns in lexicon learning.

2. **Multilingual Model Ablation Study**: Test the phone recognizer with varying numbers of languages in the multilingual training set to determine the optimal configuration and identify the point of diminishing returns for cross-linguistic transfer.

3. **Iteration Limit Determination**: Systematically test the self-training pipeline for more iterations (beyond iteration 5) on a subset of languages to precisely identify when performance plateaus or begins to degrade, establishing practical guidelines for iteration limits.