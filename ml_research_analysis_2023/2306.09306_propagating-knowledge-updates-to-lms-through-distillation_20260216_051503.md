---
ver: rpa2
title: Propagating Knowledge Updates to LMs Through Distillation
arxiv_id: '2306.09306'
source_url: https://arxiv.org/abs/2306.09306
tags:
- distillation
- definition
- entity
- transfer
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies how to inject and propagate knowledge about new
  entities into pretrained language models. While existing methods for updating model
  knowledge can inject specific facts, they fail to teach the model to make inferences
  based on these facts.
---

# Propagating Knowledge Updates to LMs Through Distillation

## Quick Facts
- arXiv ID: 2306.09306
- Source URL: https://arxiv.org/abs/2306.09306
- Reference count: 40
- Key outcome: Context distillation approach more effective at propagating knowledge updates than fine-tuning and other gradient-based knowledge-editing methods without compromising performance in other contexts, even when injecting the definitions of up to 150 entities at once.

## Executive Summary
This work introduces a context distillation approach to inject and propagate knowledge about new entities into pretrained language models. Unlike existing methods that can inject specific facts but fail to teach models to make inferences, this approach generates continuations from entity definitions and updates the model to match the distribution of a definition-conditioned model on these continuations. The method demonstrates superior performance in propagating knowledge updates while preserving model specificity on unrelated contexts.

## Method Summary
The approach generates a transfer set by prompting a generator model (GPT-3.5 or base LM) with entity definitions to create continuations that include the entity name. For each continuation, it computes KL divergence between the base model conditioned on the definition ("teacher") and unconditioned ("student") over tokens after the entity mention. The student model is then updated using gradient descent on this loss. This process is repeated for multiple epochs and all continuations, allowing the model to learn contextual patterns and make inferences beyond simple fact memorization while keeping updates localized to entity-specific knowledge.

## Key Results
- Context distillation outperforms fine-tuning and other gradient-based knowledge-editing methods for propagating knowledge updates
- The approach maintains specificity on unrelated contexts even when injecting up to 150 entities simultaneously
- Generated continuations from entity definitions enable the model to make broader inferences beyond simple fact memorization

## Why This Works (Mechanism)

### Mechanism 1: Context Distillation as Teacher-Student Matching
The model learns to match the distribution of a definition-conditioned LM without requiring the definition to be present through KL divergence minimization between student and teacher model predictions on continuation tokens after the entity mention.

### Mechanism 2: Transfer Set Generation Enables Broader Inference
Generated continuations from entity definitions teach the model to make inferences beyond simple fact memorization by containing tokens that are predictable from the definition even when the definition isn't present.

### Mechanism 3: Targeted Updates Preserve Specificity
Distillation updates are localized to entity-specific knowledge without degrading performance on unrelated contexts by computing KL loss only on tokens after the entity mention.

## Foundational Learning

- **Concept: KL Divergence as a Loss Function**
  - Why needed here: The distillation process relies on KL divergence to measure the difference between student and teacher distributions
  - Quick check question: What does KL divergence measure between two probability distributions?

- **Concept: Autoregressive Language Models**
  - Why needed here: The method works with autoregressive LMs that generate tokens sequentially
  - Quick check question: How do autoregressive LMs generate text token by token?

- **Concept: Knowledge Distillation in Machine Learning**
  - Why needed here: The approach uses context distillation, a variant of knowledge distillation
  - Quick check question: What is the difference between standard knowledge distillation and context distillation?

## Architecture Onboarding

- **Component map**: Base LM (MBase) -> Generator model (Mg) -> Transfer set (Te) -> Student model (Ms) <- Teacher model (Mt)

- **Critical path**: 1. Generate transfer set by prompting generator model with entity definitions 2. For each continuation, compute teacher distributions and student distributions 3. Calculate KL divergence on tokens after entity mention 4. Update student model parameters using gradient descent 5. Repeat for multiple epochs and all continuations

- **Design tradeoffs**: Generator model choice (GPT-3.5 vs base model), transfer set size (coverage vs computational cost), update frequency (knowledge propagation vs overfitting risk)

- **Failure signatures**: No improvement in target perplexity but improved definition perplexity (model memorized definition rather than making inferences), decreased specificity (updates affected unrelated contexts), increased perplexity on target (distillation harmed knowledge about entity)

- **First 3 experiments**: 1. Test distillation with single entity and simple definition to verify basic functionality 2. Compare distillation using base model vs GPT-3.5 as generator to understand impact of continuation quality 3. Vary number of unique continuations in transfer set to find optimal diversity vs redundancy tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does the distillation approach generalize to models with billions of parameters, such as GPT-3 or GPT-4?
- Basis in paper: The paper states that due to computational constraints, experiments were limited to models with fewer than 10 billion parameters, and it is unknown how the techniques would generalize to larger models.

### Open Question 2
- Question: Can the distillation method effectively inject and propagate knowledge about millions of new entities simultaneously, or are there scalability limits?
- Basis in paper: The paper shows promising results for injecting knowledge about up to 150 entities at once, but the constraints of the datasets used limit experiments to hundreds of entities.

### Open Question 3
- Question: How does the distillation approach perform when applied to models that have been instruction-tuned, compared to base language models?
- Basis in paper: The paper mentions that it is unknown whether the techniques generalize to models that have been instruction-tuned.

## Limitations

- The paper doesn't fully specify the exact prompts used for GPT-3.5 continuation generation, creating uncertainty about reproducibility
- Hyperparameter sensitivity is not fully characterized, with only ranges provided for learning rates and epoch counts
- Scalability limits are not explored beyond 150 entities, leaving uncertainty about performance with millions of entities

## Confidence

- **High Confidence (4/5)**: The core distillation mechanism is well-specified and theoretically sound with rigorous comparison to baseline methods
- **Medium Confidence (3/5)**: The claim of superior performance depends on specific implementation choices (generator model, prompt, hyperparameters) that aren't fully specified
- **Low Confidence (2/5)**: Scalability claims are demonstrated but not thoroughly tested, and the performance-specificity trade-off with large entity sets needs further validation

## Next Checks

- **Check 1: Generator Prompt Ablation Study** - Systematically test different prompts for GPT-3.5 and the base LM to quantify how prompt design affects continuation quality and downstream distillation performance

- **Check 2: Hyperparameter Sensitivity Analysis** - Conduct experiments varying learning rates, epoch counts, and KL divergence temperature parameters across the full range used in the paper to identify which parameters most affect the performance-specificity trade-off

- **Check 3: Scalability Boundary Testing** - Test the distillation method with increasing numbers of entities (25, 50, 100, 200, 300) to identify at what point performance degrades significantly and whether the method scales linearly or exhibits diminishing returns