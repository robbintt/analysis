---
ver: rpa2
title: Unlocking Pre-trained Image Backbones for Semantic Image Synthesis
arxiv_id: '2312.13314'
source_url: https://arxiv.org/abs/2312.13314
tags:
- image
- loss
- images
- table
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DP-SIMS, a new GAN-based approach for semantic
  image synthesis that leverages pre-trained image backbones in the discriminator
  to improve image quality and consistency with input label maps. The key innovations
  include a UNet discriminator with a pre-trained encoder and trained decoder, a novel
  generator architecture with cross-attention for noise injection, and additional
  loss terms for diversity and consistency.
---

# Unlocking Pre-trained Image Backends for Semantic Image Synthesis

## Quick Facts
- arXiv ID: 2312.13314
- Source URL: https://arxiv.org/abs/2312.13314
- Authors: 
- Reference count: 40
- Key outcome: State-of-the-art semantic image synthesis using pre-trained backbones, outperforming diffusion models in both FID and mIoU while being significantly faster

## Executive Summary
This paper introduces DP-SIMS, a GAN-based approach for semantic image synthesis that leverages pre-trained image backbones in the discriminator to improve image quality and consistency with input label maps. The method achieves state-of-the-art performance on ADE-20K, COCO-Stuff, and Cityscapes datasets, outperforming recent diffusion models in both image quality (FID) and consistency with input masks (mIoU) while being two orders of magnitude faster at inference. The key innovations include a UNet discriminator with a pre-trained encoder and trained decoder, a novel generator architecture with cross-attention for noise injection, and additional loss terms for diversity and consistency.

## Method Summary
DP-SIMS is a GAN-based semantic image synthesis approach that incorporates pre-trained image backbones into the discriminator architecture. The method uses a UNet discriminator with a fixed pre-trained encoder (ConvNeXt-L) and trained decoder, paired with a generator that employs cross-attention for noise injection. The training includes contrastive loss for multi-instance images and diversity loss to encourage variation. The approach is trained on ADE-20K, COCO-Stuff, and Cityscapes datasets using ADAM optimizer with learning rate 1e-3, batch size 64, and EMA with beta 0.9999 for 240 hours on 8 GPUs.

## Key Results
- Achieves state-of-the-art FID and mIoU scores on ADE-20K, COCO-Stuff, and Cityscapes datasets
- Outperforms recent diffusion models in both image quality and consistency with input masks
- Demonstrates two orders of magnitude faster inference speed compared to diffusion-based approaches
- Shows critical importance of pre-trained backbones, cross-attention noise injection, and novel loss terms through ablation studies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using pre-trained feature backbones as fixed encoders in the discriminator improves image quality and consistency.
- Mechanism: Pre-trained backbones provide rich, multi-scale feature representations learned from large datasets, which the discriminator's trained decoder can leverage to better distinguish real from generated images.
- Core assumption: The pre-trained features are sufficiently general to be useful for semantic image synthesis, and the discriminator can effectively learn to use them.
- Evidence anchors:
  - [abstract] "Our model, which we dub DP-SIMS, achieves state-of-the-art results in terms of image quality and consistency with the input label maps on ADE-20K, COCO-Stuff, and Cityscapes..."
  - [section] "We use a UNet architecture (Ronneberger et al., 2015) for the discriminator, similar to OASIS (Schönfeld et al., 2021). The discriminator is trained to classify pixels in real training images with the corresponding ground-truth labels, and the pixels in generated images as 'fake'."
- Break condition: If the pre-trained backbone features are too specific to the original task (e.g., image classification) and do not generalize well to semantic image synthesis, or if the discriminator cannot effectively learn to use the fixed features.

### Mechanism 2
- Claim: Feature conditioning with batch normalization and sigmoid activation improves the utilization of all features from the pre-trained backbone.
- Mechanism: Batch normalization normalizes features to have similar contributions, and the sigmoid activation reduces the dynamic range of feature activations, preventing a few strong features from dominating the discriminator's decision.
- Core assumption: Normalizing the features from the pre-trained backbone will lead to more balanced utilization of all features by the discriminator.
- Evidence anchors:
  - [section] "We choose batch normalization, yielding ˜Fl = (F′l − µl)/σl, where µl and σl are the batch statistics. In this manner, all features are in a similar range and therefore the decoder does not prioritize features with a high variance or amplitude."
- Break condition: If the normalization and activation introduce too much information loss or if the discriminator still finds a way to prioritize certain features despite the conditioning.

### Mechanism 3
- Claim: Cross-attention in the generator allows for better noise injection and diversity in the generated images.
- Mechanism: Cross-attention between the label map features and the noise vector at different scales allows the generator to incorporate noise in a spatially-aware manner, leading to more diverse and globally coherent images.
- Core assumption: Cross-attention is more effective than simple concatenation or other noise injection methods for incorporating noise into the generator.
- Evidence anchors:
  - [section] "Let hi ∈ RCi×Hi×Wi be the downsampled feature representation from the previous scale, hi first goes through a convolution to provide an embedding of the label map, then the spatial dimensions are flattened and projected via a linear layer to obtain the queries Q ∈ RHiWi×dq. The transformed noise vector w is projected via two linear layers to obtain the keys and the values K, V ∈ Rnk×dq, then the cross-attention is computed as: A = SoftMax(QK⊤/√dq)V."
- Break condition: If cross-attention is too computationally expensive or if it does not significantly improve diversity compared to simpler noise injection methods.

## Foundational Learning

- Concept: Understanding of convolutional neural networks (CNNs) and their architectures.
  - Why needed here: The paper relies heavily on CNN architectures for both the generator and discriminator.
  - Quick check question: What are the key differences between a CNN and a fully connected neural network?

- Concept: Familiarity with generative adversarial networks (GANs) and their training dynamics.
  - Why needed here: The proposed method is based on a GAN framework, and understanding the minimax game between the generator and discriminator is crucial.
  - Quick check question: In a GAN, what is the objective of the generator and what is the objective of the discriminator?

- Concept: Knowledge of pre-trained models and transfer learning.
  - Why needed here: The method leverages pre-trained feature backbones, and understanding how to effectively use pre-trained models is key.
  - Quick check question: What are the benefits and potential drawbacks of using pre-trained models in a new task?

## Architecture Onboarding

- Component map: Pre-trained backbone features → Discriminator UNet → Generator UNet → Generated images

- Critical path: The discriminator uses the pre-trained backbone to extract features, which are then processed by the trained decoder to classify real vs. generated images. The generator uses the label map and noise to produce images that can fool the discriminator.

- Design tradeoffs:
  - Using a pre-trained backbone vs. training the discriminator from scratch: Pre-trained backbones provide rich features but may not be perfectly aligned with the task. Training from scratch allows for task-specific features but requires more data and computation.
  - Cross-attention vs. simpler noise injection: Cross-attention can lead to better diversity but is more computationally expensive.

- Failure signatures:
  - If the generated images are not diverse enough, it could indicate that the noise injection mechanism is not working properly.
  - If the generated images are not consistent with the input label maps, it could indicate that the discriminator is not effectively using the pre-trained features.

- First 3 experiments:
  1. Train a baseline model without using a pre-trained backbone in the discriminator to establish a performance baseline.
  2. Experiment with different pre-trained backbone architectures (e.g., ResNet, EfficientNet) to find the best one for the task.
  3. Compare the performance of cross-attention noise injection with simpler methods (e.g., concatenation) to validate its effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can transformer-based models like Swin be stabilized as discriminator backbones in semantic image synthesis GANs?
- Basis in paper: [explicit] The paper mentions that transformer-based models like Swin can lead to instability when used as discriminator backbones, requiring gradient clipping to converge.
- Why unresolved: The authors only observe this issue but do not investigate the underlying causes or propose solutions to stabilize these models.
- What evidence would resolve it: Developing techniques to mitigate the instability of transformer-based models in this context, such as architectural modifications or training strategies, and demonstrating improved performance and stability.

### Open Question 2
- Question: What is the impact of using larger pre-trained encoders, such as ConvNext-XL, on the performance of semantic image synthesis models?
- Basis in paper: [explicit] The authors experimented with a ConvNext-XL encoder (350M parameters) on COCO-Stuff and observed improved FID and mIoU scores compared to ConvNext-L.
- Why unresolved: The authors only provide results for one large encoder on a single dataset, leaving open the question of how much larger encoders can improve performance across different datasets and tasks.
- What evidence would resolve it: Conducting extensive experiments with various large pre-trained encoders on multiple semantic image synthesis datasets and tasks, analyzing the trade-offs between model size, performance, and computational requirements.

### Open Question 3
- Question: How does the use of instance-level annotations affect the performance of semantic image synthesis models compared to using only semantic label maps?
- Basis in paper: [explicit] The authors performed an ablation study where they trained models on COCO-Stuff and Cityscapes without instance masks and observed performance degradation.
- Why unresolved: The study only compared the use of instance masks versus semantic label maps on two datasets, and the authors did not investigate the reasons behind the performance differences or explore potential improvements.
- What evidence would resolve it: Conducting a comprehensive analysis of the impact of instance-level annotations on semantic image synthesis performance across multiple datasets, investigating the reasons for performance differences, and proposing methods to better leverage instance information for improved results.

## Limitations

- The paper does not thoroughly validate whether pre-trained features generalize well across different semantic image synthesis tasks
- The computational overhead of cross-attention versus simpler noise injection methods is not explicitly quantified
- The stability issues with transformer-based discriminator backbones are observed but not resolved

## Confidence

- Mechanism 1 (Pre-trained backbones): Medium confidence - while state-of-the-art results are achieved, the generalization of pre-trained features needs more validation
- Mechanism 2 (Feature conditioning): Low-Medium confidence - theoretical justification exists but lacks strong empirical validation
- Mechanism 3 (Cross-attention): Medium confidence - described in detail but computational tradeoffs not quantified

## Next Checks

1. **Ablation study on pre-trained backbone importance**: Remove the pre-trained backbone from the discriminator and measure the degradation in both FID and mIoU scores to quantify its actual contribution.

2. **Computational overhead analysis**: Measure wall-clock time and GPU memory usage of cross-attention versus simpler noise injection methods (concatenation, spatial transformation) to validate the efficiency claims.

3. **Generalization across backbone architectures**: Test the same approach with different pre-trained backbones (ResNet, EfficientNet, Vision Transformer) to verify that the improvements are not specific to ConvNeXt or the particular dataset domains.