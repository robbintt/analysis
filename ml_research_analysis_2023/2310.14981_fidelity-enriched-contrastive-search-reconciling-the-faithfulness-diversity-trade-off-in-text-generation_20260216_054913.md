---
ver: rpa2
title: 'Fidelity-Enriched Contrastive Search: Reconciling the Faithfulness-Diversity
  Trade-Off in Text Generation'
arxiv_id: '2310.14981'
source_url: https://arxiv.org/abs/2310.14981
tags:
- fecs
- search
- contrastive
- association
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Fidelity-Enriched Contrastive Search (FECS),
  a decoding method designed to address the hallucination problem in natural language
  generation. FECS extends the Contrastive Search framework by incorporating context-aware
  regularization terms that promote tokens semantically similar to the provided source
  while penalizing repetitiveness.
---

# Fidelity-Enriched Contrastive Search: Reconciling the Faithfulness-Diversity Trade-Off in Text Generation

## Quick Facts
- arXiv ID: 2310.14981
- Source URL: https://arxiv.org/abs/2310.14981
- Reference count: 16
- Key outcome: FECS improves faithfulness in text generation while maintaining diversity

## Executive Summary
Fidelity-Enriched Contrastive Search (FECS) is a decoding method designed to address the hallucination problem in natural language generation. The method extends Contrastive Search by incorporating context-aware regularization terms that promote tokens semantically similar to the provided source while penalizing repetitiveness. FECS is evaluated on abstractive summarization and dialogue generation tasks across various language model sizes, showing consistent improvements in faithfulness metrics while maintaining comparable diversity to other decoding algorithms.

## Method Summary
FECS extends Contrastive Search by adding a faithfulness reward term to the scoring function. The method computes three terms for each candidate token: model confidence, degeneration penalty, and faithfulness reward. The faithfulness reward is calculated as the maximum cosine similarity between the candidate token's embedding and all tokens in the source content. The scoring function uses weighted hyperparameters (1-α-β) for model confidence, α for degeneration penalty, and β for faithfulness reward. FECS is evaluated on CNN-DailyMail for abstractive summarization and Wizard of Wikipedia for dialogue generation, using language models of various scales (1.3B, 2.7B, 6.7B parameters).

## Key Results
- FEQA scores increased by up to 27.63% on CNN-DailyMail dataset
- FEQA scores increased by up to 63.88% on Wizard of Wikipedia dataset
- Human evaluation showed FECS received more than twice the votes for faithfulness compared to Contrastive Search

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FECS improves faithfulness by rewarding tokens with high semantic similarity to source content.
- Mechanism: The model computes cosine similarity between candidate token embeddings and all tokens in the provided source, then adds this maximum similarity value as a reward term in the scoring function.
- Core assumption: Higher cosine similarity between token embeddings and source tokens indicates greater semantic faithfulness to the source content.
- Evidence anchors: [abstract] "FECS promotes tokens that are semantically similar to the provided source while penalizing repetitiveness"

### Mechanism 2
- Claim: FECS maintains diversity by inheriting the degeneration penalty from Contrastive Search.
- Mechanism: The method penalizes candidate tokens that are semantically similar to previously generated tokens, preventing repetitive output patterns.
- Core assumption: Semantic similarity between consecutive tokens indicates potential degeneration in generated text.
- Evidence anchors: [abstract] "FECS promotes tokens that are semantically similar to the provided source while penalizing repetitiveness"

### Mechanism 3
- Claim: FECS achieves better performance by balancing faithfulness and diversity through weighted hyperparameters.
- Mechanism: The scoring function uses weighted terms (1-α-β) for model confidence, α for degeneration penalty, and β for faithfulness reward.
- Core assumption: The three components can be linearly combined with fixed weights to achieve optimal balance between faithfulness and diversity.
- Evidence anchors: [abstract] "FECS consistently enhances faithfulness across various language model sizes while maintaining output diversity comparable to well-performing decoding algorithms"

## Foundational Learning

- Concept: Cosine similarity in embedding space
  - Why needed here: Used to measure semantic similarity between candidate tokens and both source content and previously generated tokens
  - Quick check question: If token A has embedding [0.5, 0.2] and token B has embedding [0.5, 0.3], what is their cosine similarity?

- Concept: Contrastive Search framework
  - Why needed here: FECS builds upon Contrastive Search, inheriting its degeneration penalty mechanism while adding faithfulness rewards
  - Quick check question: What is the degeneration penalty term in Contrastive Search, and how does it prevent repetitive output?

- Concept: Decoding algorithm hyperparameters
  - Why needed here: Understanding how α and β weights affect the balance between model confidence, degeneration penalty, and faithfulness reward
  - Quick check question: What happens to FECS behavior when β=0 versus when α=0?

## Architecture Onboarding

- Component map: Input preprocessor -> Token scorer -> Similarity calculator -> Argmax selector -> Output token
- Critical path: Input → Embedding lookup → Similarity computation → Term combination → Argmax selection → Output token
- Design tradeoffs:
  - Fixed vs. adaptive hyperparameters: Fixed values simplify implementation but may not optimize for all scenarios
  - Top-k candidate selection: Larger k increases computational cost but may improve token quality
  - Embedding source: Using last hidden state vs. dedicated token embeddings affects similarity calculation quality
- Failure signatures:
  - Faithfulness degrades when source content is ambiguous or contains errors
  - Diversity decreases significantly if β is too large relative to α
  - Computational overhead increases linearly with sequence length due to similarity calculations
- First 3 experiments:
  1. Compare FEQA scores with and without faithfulness reward term (β=0 vs β=0.3) on CNN-DailyMail
  2. Measure diversity impact by varying β from 0.1 to 0.5 while keeping α=0.3
  3. Test different α values (0.1, 0.3, 0.5) with fixed β=0.3 to find optimal balance for dialogue generation task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FECS perform when the source content is ambiguous, incomplete, or erroneous?
- Basis in paper: [inferred] The paper mentions that FECS's performance could be influenced by the quality of the source content and assumes source content is always correct and complete, but does not test this assumption explicitly.
- Why unresolved: The paper only evaluates FECS on well-formed source content and does not explore scenarios with noisy or ambiguous inputs.
- What evidence would resolve it: Experiments testing FECS on corrupted or ambiguous source data to measure faithfulness degradation.

### Open Question 2
- Question: What is the effect of varying the β hyperparameter on FECS's performance across different tasks?
- Basis in paper: [explicit] The paper uses a fixed β=0.3 without hyperparameter tuning and mentions exploring β in future work.
- Why unresolved: The impact of different β values on faithfulness-diversity trade-off is not systematically studied.
- What evidence would resolve it: A hyperparameter sensitivity analysis showing performance curves for different β values across multiple tasks.

### Open Question 3
- Question: How does FECS compare to retrieval-augmented generation approaches for reducing hallucinations?
- Basis in paper: [inferred] The paper focuses solely on decoding strategies without comparing to other hallucination mitigation methods like retrieval-augmentation.
- Why unresolved: The relative effectiveness of FECS versus content-grounded approaches remains unknown.
- What evidence would resolve it: Head-to-head comparisons between FECS and retrieval-augmented baselines on the same datasets and metrics.

## Limitations
- The evaluation focuses primarily on abstractive summarization and dialogue generation tasks, with limited testing across diverse generation scenarios
- Performance gains show substantial variation across model scales, suggesting potential lack of generalization
- The method's reliance on cosine similarity assumes semantic similarity directly correlates with factual accuracy, which may not hold for all types of hallucinations

## Confidence

- **High Confidence:** The core mechanism of FECS (adding faithfulness reward and maintaining degeneration penalty) is well-defined and reproducible
- **Medium Confidence:** The reported improvements in FEQA scores are supported by experimental results, but human evaluation requires careful interpretation
- **Low Confidence:** The generalization of fixed hyperparameters (α=0.3, β=0.3) across different tasks and model scales is questionable

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Systematically vary α and β values across a wider range (0.1 to 0.5) for each task and model scale to identify optimal parameter configurations and assess the stability of FECS performance.

2. **Cross-Domain Evaluation:** Test FECS on additional generation tasks beyond summarization and dialogue, such as story generation or technical writing, to evaluate whether faithfulness improvements generalize to diverse content types.

3. **Embedding Quality Impact:** Compare FECS performance using different embedding sources (last hidden state vs. dedicated token embeddings) to determine whether embedding quality affects the reliability of similarity-based faithfulness rewards.