---
ver: rpa2
title: Learning the Visualness of Text Using Large Vision-Language Models
arxiv_id: '2305.10434'
source_url: https://arxiv.org/abs/2305.10434
tags:
- text
- visual
- image
- clip
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a new task of predicting sentence-level\
  \ visualness\u2014the extent to which a sentence evokes a mental image\u2014and\
  \ proposes a dataset of 3,620 sentences with human visualness ratings. To tackle\
  \ this, the authors fine-tune large vision-language models (like CLIP) with a contrastive\
  \ objective that maps visual sentences to their corresponding images and non-visual\
  \ sentences to a NULL image."
---

# Learning the Visualness of Text Using Large Vision-Language Models

## Quick Facts
- arXiv ID: 2305.10434
- Source URL: https://arxiv.org/abs/2305.10434
- Reference count: 18
- This paper introduces a new task of predicting sentence-level visualness and proposes a dataset of 3,620 sentences with human visualness ratings.

## Executive Summary
This paper introduces the task of predicting sentence-level visualness—the extent to which a sentence evokes a mental image—and proposes a dataset of 3,620 sentences with human visualness ratings. To tackle this, the authors fine-tune large vision-language models (like CLIP) with a contrastive objective that maps visual sentences to their corresponding images and non-visual sentences to a NULL image. This enables the model to classify text as visual or non-visual based on text input alone. Experiments show that the proposed approach outperforms several baselines, including heuristics, word-level imageability aggregation, and fine-tuned BERT. The learned embeddings are also effective for downstream text-to-image retrieval.

## Method Summary
The authors propose a two-stage fine-tuning approach for CLIP. First, CLIP is fine-tuned on a large corpus of automatically labeled sentence-image pairs extracted from PDFs, where visual sentences are paired with their corresponding images and non-visual sentences with a NULL image. Then, the model is further fine-tuned on a smaller, human-labeled dataset (TIMED) with 3,620 sentences. The modified contrastive objective maps visual text to corresponding images and non-visual text to a NULL image, enabling the model to distinguish between visual and non-visual text and generate reusable embeddings for downstream tasks like text-to-image retrieval.

## Key Results
- TIP-CLIP outperforms baselines (heuristics, word-level imageability aggregation, fine-tuned BERT) on sentence-level visualness classification.
- The two-stage fine-tuning strategy (automatic labels → human labels) yields the best performance.
- TIP-CLIP embeddings preserve retrieval performance (MRR of 0.937 vs. CLIP's 0.989) and better attend to visually evocative words.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP-style contrastive training with a NULL image creates a discriminative embedding space for visual vs non-visual text.
- Mechanism: During training, visual sentences are paired with their corresponding images and pushed closer in embedding space, while non-visual sentences are paired with a fixed NULL image and pushed away from all image embeddings. This forces the model to learn a visualness dimension.
- Core assumption: The NULL image embedding acts as a reference point that reliably separates visual from non-visual text embeddings.
- Evidence anchors:
  - [abstract]: "Our strategy involves modifying the model's contrastive learning objective to map text identified as non-visual to a common NULL image while matching visual text to their corresponding images in the document."
  - [section]: "Matching visual text with its corresponding image while non-visual text to a NULL image not only encourages the model to distinguish between visual and non-visual text, but also allows it to anchor non-visual text in the common NULL image that can be used during inference without having access to a potentially paired image."
  - [corpus]: Limited. No direct ablation on different NULL image choices beyond Appendix A.1.
- Break condition: If the NULL image embedding drifts or becomes semantically similar to some visual images, the separation breaks down.

### Mechanism 2
- Claim: Two-stage fine-tuning (large-scale automatic labels → small human-labeled set) yields better visualness classifiers than either stage alone.
- Mechanism: The first stage adapts CLIP to the visualness task at scale, capturing broad patterns. The second stage refines the model with high-quality human labels, correcting systematic errors from noisy automatic labels.
- Core assumption: Automatically labeled data captures enough signal to prime the model before human supervision.
- Evidence anchors:
  - [abstract]: "Our proposed two-stage fine-tuning strategy leads to the most accurate visual and non-visual text classifier."
  - [section]: "We observe that our proposed two-stage fine-tuning strategy leads to the best-performing model (TIP-CLIP). In comparison, the pre-trained CLIP model demonstrates notably weaker performance on the task of distinguishing visual text from non-visual text."
  - [corpus]: No ablation comparing one-stage vs two-stage training directly in the main paper, but implied by the results in Table 4.
- Break condition: If automatic labels are too noisy or human labels too small, second stage gains vanish.

### Mechanism 3
- Claim: Preserving the original CLIP embedding space for visual text enables downstream text-to-image retrieval.
- Mechanism: By keeping the original image-text matching for visual text, TIP-CLIP embeddings remain semantically aligned with CLIP image embeddings, allowing retrieval to work with minimal degradation.
- Core assumption: CLIP's retrieval performance is tied to the geometry of its original embedding space.
- Evidence anchors:
  - [abstract]: "Our strategy involves modifying the model's contrastive learning objective to map text identified as non-visual to a common NULL image while matching visual text to their corresponding images in the document."
  - [section]: "Matching visual text with its corresponding image instead of a common image for all visual text affords text embeddings that can be used for downstream tasks like text-to-image retrieval."
  - [corpus]: Direct evidence in the paper: "As expected, CLIP achieves a near-perfect MRR of 0.989. The proposed fine-tuning objective does not severely impact the reusability of embeddings obtained from TIP-CLIP for retrieval, and results in an MRR of 0.937."
- Break condition: If the visual-text embedding space is distorted, retrieval MRR drops significantly.

## Foundational Learning

- Concept: Contrastive learning with a shared embedding space.
  - Why needed here: CLIP's core is aligning text and image embeddings via contrastive loss; this paper extends that to include a NULL image for non-visual text.
  - Quick check question: In the CLIP loss, what happens to the similarity of mismatched text-image pairs?
- Concept: Fine-tuning vs. training from scratch.
  - Why needed here: Large vision-language models like CLIP have strong pre-trained representations; fine-tuning is cheaper and leverages prior knowledge.
  - Quick check question: What is the key difference between CLIP's original training and TIP-CLIP's two-stage fine-tuning?
- Concept: Word-level vs. sentence-level imageability.
  - Why needed here: Aggregating word-level imageability scores is insufficient for sentence-level prediction; the paper shows this empirically.
  - Quick check question: According to the paper, why doesn't averaging MRC word imageability scores work well for sentences?

## Architecture Onboarding

- Component map:
  - CLIP encoder (pre-trained ViT/B-32) -> NULL image embedding (fixed or learnable) -> Two-stage training pipeline -> Evaluation and inference modules
- Critical path:
  1. Load CLIP model.
  2. Generate automatic labels from PDFs.
  3. Fine-tune CLIP on automatic labels.
  4. Fine-tune CLIP on human-labeled TIMED.
  5. Evaluate and deploy for visualness scoring.
- Design tradeoffs:
  - Using a fixed NULL image vs. learnable NULL embedding.
  - Matching visual text to its image vs. a common image for all visual text (affects downstream retrieval).
  - Two-stage vs. single-stage fine-tuning (accuracy vs. efficiency).
- Failure signatures:
  - MRR for text-to-image retrieval drops significantly.
  - Visual vs. non-visual classification accuracy degrades.
  - Attention weights no longer correlate with word-level imageability.
- First 3 experiments:
  1. Ablation: Train with common image for all visual text vs. original image for each visual text (affects retrieval).
  2. Ablation: Use learnable NULL image vs. fixed random NULL image (affects separation).
  3. Ablation: Single-stage fine-tuning vs. two-stage (affects final accuracy).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of TIP-CLIP on text visualness prediction generalize to languages other than English?
- Basis in paper: [explicit] The authors note that TIMED only covers English and acknowledge that visualness may vary across languages.
- Why unresolved: The paper does not include experiments or analysis on non-English text.
- What evidence would resolve it: Experiments evaluating TIP-CLIP on multilingual datasets or cross-lingual transfer learning results.

### Open Question 2
- Question: How does the choice of NULL image affect the learned text embeddings and downstream task performance?
- Basis in paper: [explicit] The authors test different NULL images (random RGB, natural image) and find no performance difference, but do not analyze the impact on embedding space geometry.
- Why unresolved: The paper only reports classification accuracy and MRR, not embedding space properties.
- What evidence would resolve it: Analysis of embedding distributions and downstream task performance for different NULL images.

### Open Question 3
- Question: Can word-level visualness scores be composed to predict sentence-level visualness more effectively?
- Basis in paper: [inferred] The authors find that word-level imageability aggregation performs poorly, suggesting a compositionality gap.
- Why unresolved: The paper does not explore compositional models or analyze which linguistic features contribute to sentence-level visualness.
- What evidence would resolve it: Experiments with compositional models or linguistic feature analysis showing improved sentence-level predictions.

## Limitations
- Evaluation is based on a single sentence-level dataset (TIMED), so generalizability to other languages, domains, or paragraph-level visualness is unclear.
- The automatic labeling process depends on an assumed threshold-based heuristic from PDF images, but the robustness of this heuristic is not deeply tested.
- The NULL image is a fixed random image; there is no ablation showing if a learned NULL image or a different type of NULL reference would perform better.

## Confidence
**High confidence**: The two-stage fine-tuning strategy (automatic labels → human labels) improves classification accuracy over a single stage. The claim that matching visual text to its corresponding image (instead of a common image) preserves retrieval performance is directly supported by MRR scores. The assertion that averaging word-level imageability scores does not work for sentence-level prediction is well supported by experiments.

**Medium confidence**: The claim that TIP-CLIP's attention is more correlated with word-level imageability than CLIP is based on reported correlations but lacks ablation on how attention is computed or whether the difference is statistically significant. The suggestion that CLIP's NULL-image-based embeddings could be reused for downstream tasks like text-to-image generation is speculative, as this is not empirically tested in the paper.

**Low confidence**: The robustness of the PDF-based automatic labeling pipeline (450k PDFs, Fitz library) is not verified; there may be systematic biases or noise. The claim that CLIP's original retrieval MRR (0.989) is "near-perfect" is based on a small visual subset and may not generalize.

## Next Checks
1. Reproduce the two-stage training pipeline on the TIMED dataset, comparing single-stage vs. two-stage fine-tuning to confirm the reported accuracy improvements.

2. Evaluate the effect of NULL image choice by ablating between a fixed random NULL image, a learned NULL embedding, or using a common image for all visual text, and measure both classification and retrieval performance.

3. Test the robustness of the automatic labeling pipeline by varying the similarity thresholds (Tpos, Tneg) and document sources, and assess how much the downstream classifier accuracy changes.