---
ver: rpa2
title: 'Universal Domain Adaptation from Foundation Models: A Baseline Study'
arxiv_id: '2305.11092'
source_url: https://arxiv.org/abs/2305.11092
tags:
- clip
- methods
- photo
- uniot
- dance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates universal domain adaptation (UniDA) using
  foundation models. The authors find that while foundation models improve baseline
  performance, existing UniDA methods fail to outperform the baseline.
---

# Universal Domain Adaptation from Foundation Models: A Baseline Study

## Quick Facts
- arXiv ID: 2305.11092
- Source URL: https://arxiv.org/abs/2305.11092
- Authors: 
- Reference count: 40
- Primary result: CLIP zero-shot baseline outperforms existing UniDA methods under proposed UCR metric

## Executive Summary
This paper investigates universal domain adaptation (UniDA) using foundation models like CLIP and DINOv2. The authors find that existing UniDA methods fail to outperform the strong CLIP zero-shot baseline, prompting them to propose a new evaluation metric called Universal Classification Rate (UCR) and a simple target data distillation method called CLIP distillation. Their method achieves state-of-the-art results on UniDA benchmarks without using source data, demonstrating the effectiveness of foundation models in domain adaptation tasks while highlighting the need for better evaluation metrics and adaptation strategies.

## Method Summary
The paper proposes a CLIP distillation method for universal domain adaptation that leverages foundation models by freezing their encoders and training only the classifier head. The core technique involves self-calibration with automatic temperature scaling, where the model is fine-tuned on target data using cross-entropy loss between target logits from the zero-shot model and fine-tuned model outputs. The temperature parameter τ is fixed at 0.3, and the method does not utilize source data, focusing instead on improving the strong CLIP zero-shot baseline through target data distillation.

## Key Results
- CLIP zero-shot baseline outperforms existing SOTA UniDA methods under UCR metric
- Proposed CLIP distillation method achieves state-of-the-art results without using source data
- New UCR metric provides fairer evaluation than H-score/H3-score by being threshold- and ratio-free

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP zero-shot baseline outperforms complex UniDA methods under UCR metric
- Mechanism: Zero-shot CLIP leverages large-scale pretraining to achieve strong out-of-distribution detection without adaptation
- Core assumption: CLIP's language-vision alignment provides robust semantic understanding for unknown class detection
- Evidence anchors:
  - [abstract] "CLIP zero-shot outperforms the SOTAs under the CLIP model"
  - [section] "CLIP zero-shot even outperforms some SOTA methods that proposed for improving the adaptation of foundation models"
  - [corpus] Weak - no direct citations about CLIP zero-shot performance in UniDA
- Break condition: When target domain has significantly different semantic structure than pretraining data

### Mechanism 2
- Claim: Self-calibration with automatic temperature scaling improves out-of-distribution detection
- Mechanism: Temperature scaling adjusts the confidence calibration of softmax outputs to better separate in-class from out-class samples
- Core assumption: Cross-entropy loss with temperature scaling can learn optimal confidence thresholds
- Evidence anchors:
  - [abstract] "The core of our CLIP distillation lies in a self-calibration technique for automatic temperature scaling"
  - [section] "loss of the CLIP distillation to each target example xt is: loss(xt) = H(σ(ct/τ), σ(φ(xt)))"
  - [corpus] Weak - no direct citations about temperature scaling for calibration
- Break condition: When target data distribution is too different from source for effective calibration

### Mechanism 3
- Claim: UCR metric provides fairer evaluation than H-score/H3-score
- Mechanism: Area under CCR vs FPR curve is threshold-free and ratio-free, avoiding sensitivity to threshold selection
- Core assumption: Varying threshold values should not dramatically affect method comparison fairness
- Evidence anchors:
  - [abstract] "UCR is threshold- and ratio-free and thus is not sensitive to different tasks"
  - [section] "Unlike previous evaluation metrics, UCR is threshold- and ratio-free, thus is much fairer"
  - [corpus] Weak - no direct citations about UCR metric design
- Break condition: When evaluation requires specific operating points rather than overall performance

## Foundational Learning

- **Concept**: Domain adaptation and domain shift
  - Why needed here: UniDA specifically addresses domain shift between source and target domains
  - Quick check question: What is the key difference between universal domain adaptation and standard domain adaptation?

- **Concept**: Out-of-distribution detection
  - Why needed here: UniDA requires detecting samples from unknown classes (out-class samples)
  - Quick check question: How does out-of-distribution detection differ from standard classification?

- **Concept**: Cross-entropy loss and temperature scaling
  - Why needed here: CLIP distillation uses temperature-scaled cross-entropy for target data distillation
  - Quick check question: What effect does temperature scaling have on softmax probability distributions?

## Architecture Onboarding

- **Component map**: CLIP encoder (frozen) -> Feature extractor -> Linear classifier (trainable) -> In-class prediction -> Temperature scaling parameter τ -> Calibration adjustment -> Cross-entropy loss -> Target data distillation

- **Critical path**:
  1. Extract features from target data using frozen CLIP encoder
  2. Compute logits using CLIP zero-shot model
  3. Apply temperature scaling to logits
  4. Calculate cross-entropy loss with target features
  5. Update classifier parameters

- **Design tradeoffs**:
  - Freezing CLIP encoder vs. fine-tuning: Better baseline performance but less adaptation flexibility
  - Temperature scaling: Automatic calibration vs. fixed threshold methods
  - Source data exclusion: Simpler method vs. potential performance gains from source information

- **Failure signatures**:
  - Poor performance on tasks with many unknown classes
  - Calibration failure when target distribution differs significantly from source
  - Temperature scaling getting stuck at extreme values

- **First 3 experiments**:
  1. Compare CLIP zero-shot baseline vs. CLIP distillation on Office dataset (10/10 split)
  2. Test temperature scaling sensitivity by varying τ from 0.1 to 1.0
  3. Evaluate performance drop when target domain has no shared classes with source

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific improvements can be achieved by fine-tuning foundation models (e.g., CLIP or DINOv2) using surgical fine-tuning techniques or pre-train method adaptations, compared to freezing the encoder as done in the current study?
- Basis in paper: [explicit] The paper mentions that surgical fine-tuning [15] and fine-tuning used pre-train method [10] were not explored due to high computing costs, leaving room for future work.
- Why unresolved: The paper only considers freezing the encoder due to worse results with full fine-tuning, but recent works suggest better full-tuning methods exist.
- What evidence would resolve it: Conducting experiments comparing surgical fine-tuning and pre-train method adaptations with the current approach, and measuring performance on UniDA benchmarks.

### Open Question 2
- Question: How would the proposed CLIP distillation method perform if source data were utilized in addition to target unlabeled data?
- Basis in paper: [explicit] The paper suggests that future work could improve the method by further utilizing source data, as the current method does not use source data.
- Why unresolved: The paper focuses on improving the strong CLIP zero-shot baseline without using source data, leaving the potential of combining source data unexplored.
- What evidence would resolve it: Implementing and evaluating the CLIP distillation method with source data, comparing results to the current approach and other state-of-the-art methods.

### Open Question 3
- Question: What are the specific reasons for the poor performance of fine-tuning full models compared to freezing the encoder, and how can these be addressed?
- Basis in paper: [explicit] The paper states that full fine-tuning of foundation models yields significantly poorer results, sometimes worse than training from scratch, but does not explore the underlying reasons.
- Why unresolved: The paper does not investigate the causes of the poor performance when fine-tuning full models, which could lead to better strategies for adapting foundation models.
- What evidence would resolve it: Analyzing the differences in performance between fine-tuning full models and freezing the encoder, identifying the causes of poor performance, and developing targeted solutions.

### Open Question 4
- Question: How does the proposed Universal Classification Rate (UCR) metric compare to other evaluation metrics in terms of sensitivity to threshold values and ratio of out-class samples?
- Basis in paper: [explicit] The paper introduces UCR as a threshold- and ratio-free metric to address the sensitivity issues of existing metrics like H-score and H3-score.
- Why unresolved: The paper does not provide a comprehensive comparison of UCR with other metrics in terms of their sensitivity to threshold values and ratio of out-class samples.
- What evidence would resolve it: Conducting experiments comparing UCR with H-score and H3-score under various threshold values and ratios of out-class samples, and analyzing the sensitivity of each metric.

## Limitations
- Limited exploration of fine-tuning foundation models, which recent works suggest could yield better results
- Fixed temperature scaling parameter (τ=0.3) without justification for why this specific value was chosen
- Novel UCR metric introduced without extensive validation against established threshold-free metrics

## Confidence
- High confidence: CLIP zero-shot baseline performance claims (well-established in literature)
- Medium confidence: CLIP distillation method effectiveness (single method, limited ablation)
- Low confidence: UCR metric superiority claims (novel metric without extensive validation)

## Next Checks
1. Test the CLIP distillation method with different temperature values (0.1, 0.5, 1.0) to verify the chosen value is optimal
2. Compare UCR metric results with established metrics (AUC, H-score) on the same tasks to validate fairness claims
3. Evaluate performance when target domain has no shared classes with source to test true out-of-distribution detection capability