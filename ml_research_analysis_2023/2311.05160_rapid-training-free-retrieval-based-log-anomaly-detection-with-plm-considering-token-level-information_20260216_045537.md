---
ver: rpa2
title: 'RAPID: Training-free Retrieval-based Log Anomaly Detection with PLM considering
  Token-level information'
arxiv_id: '2311.05160'
source_url: https://arxiv.org/abs/2311.05160
tags:
- detection
- logs
- anomaly
- normal
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RAPID is a training-free log anomaly detection model that leverages
  pre-trained language models and token-level information to achieve real-time performance.
  It treats logs as natural language, extracts representations using PLM, and employs
  a retrieval-based technique to compare test logs with the most similar normal logs.
---

# RAPID: Training-free Retrieval-based Log Anomaly Detection with PLM considering Token-level information

## Quick Facts
- arXiv ID: 2311.05160
- Source URL: https://arxiv.org/abs/2311.05160
- Reference count: 16
- RAPID achieves training-free log anomaly detection using pre-trained language models with token-level information

## Executive Summary
RAPID is a novel training-free log anomaly detection model that leverages pre-trained language models (PLMs) and token-level information to achieve real-time performance. By treating logs as natural language and employing a retrieval-based technique, RAPID compares test logs with the most similar normal logs using PLM representations. The core set technique reduces computational cost while maintaining detection accuracy. Experimental results demonstrate that RAPID achieves competitive performance compared to prior models without requiring log-specific training, and shows particular strength on certain datasets. The model is capable of real-time detection without delay, even with limited data and various pre-trained models.

## Method Summary
RAPID reformulates log anomaly detection as a retrieval task using PLM representations. The method treats logs as natural language, extracts representations using a pre-trained language model, and performs anomaly detection by comparing test logs to the most similar normal logs. A core set technique reduces computational cost by limiting comparisons to a subset of known normal logs. The model uses maxSim distance to capture token-level information, which helps detect subtle anomalies that sequence-level methods might miss. RAPID achieves training-free operation by leveraging PLM representations rather than requiring log-specific training.

## Key Results
- RAPID demonstrates competitive F1-scores compared to prior models without requiring log-specific training
- The core set technique reduces computational cost while maintaining detection accuracy
- RAPID achieves real-time detection capability even with limited data accumulation
- The model shows robust performance across different pre-trained models (BERT, RoBERTa, ELECTRA)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAPID achieves training-free log anomaly detection by reformulating the problem as a retrieval task using PLM representations
- Mechanism: RAPID treats logs as natural language, extracts representations using a pre-trained language model (PLM), and performs anomaly detection by comparing test logs to the most similar normal logs. The core set technique reduces computational cost by limiting comparisons to a subset of known normal logs
- Core assumption: Logs generated in similar system situations will have similar PLM representations, allowing nearest-neighbor retrieval to identify normal log types
- Evidence anchors: [abstract] "RAPID treats logs as natural language, extracting representations using pre-trained language models. Given that logs can be categorized based on system context, we implement a retrieval-based technique to contrast test logs with the most similar normal logs."
- Break condition: If log data lacks sufficient semantic structure or PLM fails to capture meaningful log context, retrieval-based anomaly detection would fail

### Mechanism 2
- Claim: RAPID leverages token-level information through maxSim distance to capture subtle anomalies missed by sequence-level methods
- Mechanism: RAPID uses maxSim distance, which computes similarity between all tokens in query and document log sequences, rather than just sequence-level CLS embeddings. This captures detailed semantic information within logs
- Core assumption: Subtle differences in individual tokens can indicate anomalies, and these differences would be missed by sequence-level methods that only use CLS embeddings
- Evidence anchors: [abstract] "This strategy not only obviates the need for log-specific training but also adeptly incorporates token-level information, ensuring refined and robust detection, particularly for unseen logs."
- Break condition: If token-level information doesn't correlate with anomaly status or if token variations are random noise rather than meaningful patterns

### Mechanism 3
- Claim: RAPID achieves real-time performance by using a core set technique that limits comparisons to relevant log types
- Mechanism: RAPID uses K-nearest neighbor (KNN) with CLS token embeddings to identify a core set of candidate normal logs for each query, then computes maxSim distance only within this core set. This dramatically reduces computational cost
- Core assumption: The nearest log types (identified via CLS distance) will be the most relevant for anomaly detection, so computing maxSim distance on all known normal logs is unnecessary
- Evidence anchors: [abstract] "We also propose the core set technique, which can reduce the computational cost needed for comparison."
- Break condition: If core set selection fails to include the true nearest normal type, or if the core set is too small to capture log type diversity

## Foundational Learning

- **Concept**: Pre-trained Language Models (PLMs)
  - Why needed here: RAPID relies on PLMs to extract meaningful representations from log sequences, enabling the retrieval-based anomaly detection approach
  - Quick check question: Why does RAPID use PLMs instead of training a custom model on log data?

- **Concept**: Information Retrieval (IR) principles
  - Why needed here: RAPID reformulates anomaly detection as an IR problem, requiring understanding of how retrieval systems work with vector representations
  - Quick check question: How does RAPID's retrieval-based approach differ from traditional anomaly detection methods?

- **Concept**: Token-level vs. sequence-level embeddings
  - Why needed here: RAPID uses maxSim distance to capture token-level information, requiring understanding of when token-level information is more valuable than sequence-level summaries
  - Quick check question: What advantage does maxSim distance have over using only CLS token embeddings for log anomaly detection?

## Architecture Onboarding

- **Component map**: Preprocess logs → Encode with PLM → Build lookup database → For each test period: Select core set → Compute maxSim distances → Assign scores → Detect anomalies

- **Critical path**: Preprocess logs → Encode with PLM → Build lookup database → For each test period: Select core set → Compute maxSim distances → Assign scores → Detect anomalies

- **Design tradeoffs**:
  - Training-free vs. accuracy: RAPID sacrifices potential accuracy gains from training-specific models for deployment speed and flexibility
  - Token-level vs. sequence-level: RAPID uses both but emphasizes token-level information, increasing computational cost but improving detection of subtle anomalies
  - Core set size: Smaller core sets improve speed but risk missing relevant normal types; larger core sets improve accuracy but reduce real-time capability

- **Failure signatures**:
  - Poor core set selection: High false positive rates when query logs are misclassified to wrong normal types
  - Insufficient token coverage: High false negative rates when anomalies are subtle and only detectable at token level
  - PLM representation failure: Inconsistent performance across different log types or systems

- **First 3 experiments**:
  1. Vary core set size (K=2, 5, 10, ALL) and measure F1-score and inference time on BGL dataset to verify core set technique effectiveness
  2. Compare maxSim distance using all tokens vs. CLS-only embeddings on Thunderbird dataset to validate token-level information importance
  3. Test RAPID on limited known normal log ratios (0.001, 0.01, 0.1, 1.0) to verify real-time data accumulation capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RAPID's performance scale with increasing log vocabulary size and complexity in real-world systems?
- Basis in paper: [inferred] The paper discusses RAPID's robust performance across datasets with varying vocabulary sizes (BGL: 888, Thunderbird: 3137, HDFS: 229) and mentions that log vocabulary size may increase over time due to system updates
- Why unresolved: While the paper demonstrates competitive performance across current benchmark datasets, it doesn't explore scenarios with significantly larger and more complex vocabulary sizes that would be encountered in long-running, evolving production systems
- What evidence would resolve it: Systematic evaluation of RAPID on datasets with progressively larger vocabulary sizes, including synthetic data with controlled vocabulary growth, and real-world enterprise logs with years of accumulation

### Open Question 2
- Question: What is the theoretical limit of RAPID's core set technique in terms of the ratio between core set size and total database size while maintaining performance?
- Basis in paper: [explicit] The paper demonstrates that RAPID maintains performance with core set ratios as low as 0.01, and shows that inference time decreases significantly with smaller core sets while performance remains stable
- Why unresolved: The experiments show performance stability at 0.01 ratio but don't establish the lower bound or determine if there's a critical threshold below which performance degrades
- What evidence would resolve it: Extensive experimentation across multiple datasets varying the core set ratio from 0.01 down to near-zero, measuring the point where performance begins to significantly degrade

### Open Question 3
- Question: How does RAPID handle log streams with rapidly changing patterns or concept drift over time?
- Basis in paper: [inferred] The paper emphasizes RAPID's training-free nature and real-time capabilities, suggesting it could adapt to new log patterns without retraining, but doesn't explicitly test this scenario
- Why unresolved: While the paper demonstrates robust performance with limited training data and mentions scalability, it doesn't test RAPID's behavior when log patterns shift significantly over time or when new log types emerge that weren't present in the initial known normal set
- What evidence would resolve it: Longitudinal studies tracking RAPID's performance over time on evolving log streams, or experiments introducing synthetic concept drift by gradually modifying log patterns during testing periods

## Limitations

- The exact threshold value used for anomaly detection is not specified in the paper
- Optimal core set size K for different datasets is not explicitly determined
- Performance generalization to log types and systems not included in the evaluation remains unverified

## Confidence

**High Confidence Claims**:
- RAPID achieves competitive F1-scores compared to prior models without requiring log-specific training
- The core set technique provides computational efficiency improvements
- Token-level information captured by maxSim distance contributes to improved detection performance

**Medium Confidence Claims**:
- RAPID demonstrates real-time capability with limited data accumulation
- The approach is effective across multiple pre-trained language models (BERT, RoBERTa, ELECTRA)
- RAPID's training-free nature provides advantages in deployment flexibility

**Low Confidence Claims**:
- The specific impact of varying core set size K on different metrics
- Performance guarantees for log types not represented in the training data
- Scalability to very large log datasets with millions of entries

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically vary the anomaly detection threshold δ across a range of values (0.1 to 0.9) and measure the resulting precision-recall trade-off on all three datasets to understand the robustness of RAPID's detection performance to threshold selection

2. **Core Set Size Optimization**: Conduct experiments varying K (core set size) from 2 to 20 in increments of 2, measuring both F1-score and inference time on the BGL dataset to identify the optimal trade-off between detection accuracy and real-time performance

3. **Cross-System Generalization**: Apply RAPID to a fourth, previously unseen log dataset from a different system domain (e.g., web server logs) to evaluate how well the approach generalizes beyond the three systems used in the original evaluation, measuring performance degradation compared to the original datasets