---
ver: rpa2
title: 'auto-sktime: Automated Time Series Forecasting'
arxiv_id: '2312.08528'
source_url: https://arxiv.org/abs/2312.08528
tags:
- time
- series
- hourly
- forecasting
- duration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'auto-sktime automates time series forecasting by adapting AutoML
  techniques to time series data. It uses Bayesian optimization to construct forecasting
  pipelines from statistical, ML, and DNN models, with three key improvements: pipeline
  templates for different model types, a novel warm-starting technique using historic
  runs, and a multi-fidelity budget using reverse expanding windows.'
---

# auto-sktime: Automated Time Series Forecasting

## Quick Facts
- arXiv ID: 2312.08528
- Source URL: https://arxiv.org/abs/2312.08528
- Reference count: 40
- Primary result: Auto-sktime automates time series forecasting using Bayesian optimization with three key improvements, outperforming existing AutoML frameworks on 64 real-world datasets

## Executive Summary
auto-sktime automates time series forecasting by adapting AutoML techniques to time series data. It constructs forecasting pipelines from statistical, ML, and DNN models using Bayesian optimization with three key improvements: pipeline templates for different model types, a novel warm-starting technique using historic runs, and a multi-fidelity budget using reverse expanding windows. Experiments on 64 real-world datasets show auto-sktime achieves significantly better performance (MASE 1.68) than baselines like pmdarima (2.62) and AutoGluon (6.00).

## Method Summary
The framework adapts AutoML to time series forecasting by creating a unified pipeline system that combines statistical, ML, and DNN approaches. It uses Bayesian optimization with three novel components: (1) template-based pipeline construction where each model type has appropriate preprocessing steps, (2) warm-starting via DTW-based similarity to initialize optimization from historical runs, and (3) multi-fidelity approximation using reverse expanding windows to efficiently evaluate models. The system is evaluated on 64 diverse real-world datasets against multiple baseline approaches.

## Key Results
- auto-sktime achieves MASE 1.68 on 64 datasets, significantly outperforming baselines (pmdarima 2.62, AutoGluon 6.00)
- Ablation study confirms combining all three improvements yields optimal performance
- The framework successfully handles diverse dataset types including univariate, multivariate, and panel data
- Computational efficiency is improved through multi-fidelity approximation without sacrificing accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: auto-sktime improves forecasting accuracy by combining diverse model types (statistical, ML, DNN) into a unified pipeline framework.
- Mechanism: The templating approach allows each model type to be configured with appropriate preprocessing steps, ensuring optimal data preparation for the model's assumptions.
- Core assumption: Different time series forecasting problems require different modeling approaches, and no single model type is universally optimal.
- Evidence anchors:
  - [abstract] "auto-sktime combines statistical, ML, and DNN state-of-the-art (SOTA) techniques for time series forecasting"
  - [section] "Historically, three general approaches for time series forecasting have been proposed: statistical, ML, and DNN models"
  - [corpus] No direct evidence found in corpus
- Break condition: If preprocessing steps are not properly matched to model requirements, the unified framework could introduce bias or miss critical data transformations.

### Mechanism 2
- Claim: Multi-fidelity approximation using reverse expanding windows enables efficient evaluation of time series models without distorting temporal patterns.
- Mechanism: Instead of random sampling or iterative training, the method uses progressively larger historical windows from the end of the series, preserving temporal relationships while reducing computation.
- Core assumption: Many time series models cannot be trained iteratively and require complete historical context for accurate forecasts.
- Evidence anchors:
  - [abstract] "we adapt multi-fidelity optimizations to make them applicable to a search space containing statistical, ML and DNN models"
  - [section] "We propose a reverse expanding window interpretation for multi-fidelity budgets"
  - [corpus] No direct evidence found in corpus
- Break condition: If time series are too short, the multi-fidelity benefits become negligible, and the overhead may outweigh the gains.

### Mechanism 3
- Claim: Warm-starting optimization using DTW-based similarity improves sampling efficiency by initializing the search in promising regions.
- Mechanism: The system calculates DTW distances between the target time series and historical datasets, then weights and combines the best configurations from similar datasets using KDE.
- Core assumption: Time series with similar patterns benefit from similar model configurations and hyperparameters.
- Evidence anchors:
  - [abstract] "we propose a novel warm-starting technique to start the optimization from prior optimization runs"
  - [section] "Instead of using a distance based on hard-to-select meta-features, we propose to use native distance metrics for time series"
  - [corpus] No direct evidence found in corpus
- Break condition: If the historical dataset collection lacks diversity or contains poorly performing configurations, the warm-starting could mislead the optimization.

## Foundational Learning

- Concept: Bayesian optimization and acquisition functions
  - Why needed here: The framework uses BO to efficiently search the high-dimensional pipeline configuration space
  - Quick check question: What is the role of the acquisition function in Bayesian optimization?

- Concept: Dynamic Time Warping (DTW)
  - Why needed here: DTW is used to measure similarity between time series for warm-starting
  - Quick check question: How does DTW differ from Euclidean distance when comparing time series?

- Concept: Time series preprocessing techniques
  - Why needed here: Different model types require different preprocessing (detrending, smoothing, feature generation)
  - Quick check question: Why is detrending important for statistical models but not necessarily for DNNs?

## Architecture Onboarding

- Component map:
  Input data handler -> Template selector -> Pipeline builder -> Bayesian optimizer -> Model fitter (with multi-fidelity) -> Ensemble builder
  Each template contains preprocessing steps specific to model type
  Warm-starting module maintains historical configuration database

- Critical path:
  Data preprocessing -> Template selection -> Configuration generation -> Model evaluation (multi-fidelity) -> Configuration update -> Ensemble creation

- Design tradeoffs:
  Template rigidity vs. flexibility: Fixed templates ensure proper preprocessing but limit experimentation
  Warm-starting overhead vs. efficiency: DTW calculations add computational cost but improve search efficiency
  Multi-fidelity window size vs. accuracy: Larger windows improve forecast quality but increase computation time

- Failure signatures:
  Poor performance on specific dataset types -> Check template selection logic
  Slow convergence -> Verify warm-starting is functioning and DTW calculations are correct
  Memory issues -> Monitor pipeline construction and ensemble building

- First 3 experiments:
  1. Run auto-sktime on a single univariate dataset with all components disabled to verify basic functionality
  2. Enable multi-fidelity approximation only and compare runtime vs. accuracy
  3. Enable warm-starting only and verify configuration selection improves over random search

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of auto-sktime scale with increasing dataset size and forecasting horizon length?
- Basis in paper: [inferred] The paper evaluates auto-sktime on 64 datasets with varying sizes and horizons, but does not systematically analyze performance trends as these factors change.
- Why unresolved: The experiments provide aggregate performance metrics across diverse datasets but lack a controlled study varying dataset size and horizon independently.
- What evidence would resolve it: Experiments measuring MASE/RMSE as functions of time series length and forecasting horizon, ideally on datasets with controlled variations in these parameters.

### Open Question 2
- Question: What is the impact of different distance metrics for warm-starting (besides DTW) on the optimization efficiency and final forecasting performance?
- Basis in paper: [explicit] The paper uses DTW for warm-starting based on time series similarity, but mentions "Similar distance measures would also be possible" without testing alternatives.
- Why unresolved: Only DTW is evaluated as the distance metric for warm-starting, leaving open whether other metrics could yield better or worse results.
- What evidence would resolve it: Comparative experiments using alternative time series distance metrics (e.g., Euclidean, Edit Distance on Real sequences) for warm-starting, measuring both optimization convergence speed and final model performance.

### Open Question 3
- Question: How do the three proposed improvements (templates, multi-fidelity approximations, warm-starting) interact with each other in terms of performance and computational efficiency?
- Basis in paper: [explicit] The ablation study shows each improvement contributes to better performance, but does not analyze interactions between them.
- Why unresolved: The ablation study tests each improvement in isolation but does not examine whether combining certain improvements yields diminishing returns or unexpected synergies.
- What evidence would resolve it: Experiments testing all possible combinations of the three improvements to identify optimal combinations and quantify interaction effects on both performance and computation time.

## Limitations

- The framework's effectiveness may be limited for time series with extreme or irregular characteristics not well-handled by the fixed pipeline templates
- DTW-based warm-starting could become computationally prohibitive when dealing with very large historical datasets
- Reverse expanding window multi-fidelity may not be optimal for non-stationary time series where recent patterns differ significantly from historical patterns

## Confidence

- Pipeline templates: Medium-High - Well-supported by literature on time series diversity, though empirical validation across all dataset types remains limited
- Multi-fidelity approximation: Medium - Limited discussion of edge cases with very short time series
- Warm-starting: Medium-High - DTW similarity has proven effective in related domains, but KDE implementation details require validation

## Next Checks

1. Test auto-sktime on datasets with known challenging characteristics (high noise, structural breaks, multiple seasonalities) to stress-test the template system
2. Measure the overhead of DTW calculations versus the warm-starting benefits across different dataset sizes
3. Compare reverse expanding window multi-fidelity performance against traditional cross-validation on datasets with varying degrees of stationarity