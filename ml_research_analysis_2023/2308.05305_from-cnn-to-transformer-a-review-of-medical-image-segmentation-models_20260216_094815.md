---
ver: rpa2
title: 'From CNN to Transformer: A Review of Medical Image Segmentation Models'
arxiv_id: '2308.05305'
source_url: https://arxiv.org/abs/2308.05305
tags:
- segmentation
- image
- medical
- u-net
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Medical image segmentation using deep learning models like U-Net
  and its variants has become a prevalent trend. This paper conducts a survey of the
  four most representative medical image segmentation models in recent years: U-Net,
  UNet++, TransUNet, and Swin-Unet.'
---

# From CNN to Transformer: A Review of Medical Image Segmentation Models

## Quick Facts
- arXiv ID: 2308.05305
- Source URL: https://arxiv.org/abs/2308.05305
- Authors: [Not specified in source]
- Reference count: 31
- Primary result: Transformer-based models (TransUNet, Swin-Unet) outperform CNN-based models on medical image segmentation tasks

## Executive Summary
This paper reviews four representative medical image segmentation models: U-Net, UNet++, TransUNet, and Swin-Unet. The study evaluates these models on two benchmark datasets - Tuberculosis Chest X-rays and ovarian tumors - using standard segmentation metrics including Dice coefficient, HD95, IoU, accuracy, precision, and recall. The results demonstrate that Transformer-based models, particularly TransUNet, achieve superior performance compared to traditional CNN architectures. The introduction of Transformer modules enables better capture of long-range dependencies and global information, leading to improved segmentation accuracy.

## Method Summary
The study implements four segmentation models (U-Net, UNet++, TransUNet, Swin-Unet) using PyTorch and evaluates them on Tuberculosis Chest X-rays and Ovarian Tumors datasets. The models are trained with binary cross-entropy and Dice loss using Adam optimizer for U-Net and UNet++, and SGD for TransUNet and Swin-Unet. The evaluation metrics include Dice coefficient, HD95, IoU, accuracy, precision, and recall. The study follows a standard encoder-decoder architecture with skip connections, with Transformer modules added to enhance long-range dependency modeling in TransUNet and Swin-Unet.

## Key Results
- TransUNet achieves the best performance with 96.45% Dice coefficient on the Tuberculosis Chest X-rays dataset
- On ovarian tumors dataset, TransUNet achieves 89.18% Dice coefficient
- Swin-Unet's shifted window mechanism provides computational efficiency while preserving positional information
- Transformer modules significantly improve the ability to capture long-range dependencies and global information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer modules improve the model's ability to capture long-range dependencies and global information in medical images, leading to better segmentation performance.
- Mechanism: The Transformer module uses self-attention mechanisms to calculate the similarity between each position and other positions in the input sequence, creating a weight vector that allows for weighted representations of each position. This facilitates the interaction and integration of global information across the entire image.
- Core assumption: The self-attention mechanism can effectively capture the correlations between different positions in the input sequence, which is particularly important for medical images where contextual information from distant regions can be crucial for accurate segmentation.
- Evidence anchors:
  - [abstract] "The introduction of Transformer modules improves the model's ability to capture long-range dependencies and global information, leading to better segmentation performance."
  - [section] "TransUNet is a segmentation model that introduces Transformer modules [10] to improve the model's ability to model long-range dependencies. The Transformer module adopts a self-attention mechanism, which calculates the similarity between each position and other positions in the input sequence, resulting in a weight vector."
  - [corpus] No direct evidence found in corpus, but related work on ScribFormer and Channel Boosted CNN-Transformer-based models supports the claim that Transformers enhance medical image segmentation.
- Break condition: If the input image resolution is too high for self-attention computation, causing memory overflow or prohibitive computational cost.

### Mechanism 2
- Claim: The hybrid encoder structure combining CNN and Transformer achieves better performance than using pure Transformers or pure CNNs for medical image segmentation.
- Mechanism: CNN layers first extract local features and generate feature maps, which are then divided into patches and fed into Transformer modules. This combination leverages the strengths of both architectures: CNNs' ability to capture local spatial patterns and Transformers' ability to model global dependencies.
- Core assumption: The feature maps generated by CNN provide a suitable representation that can be effectively processed by Transformer modules for capturing long-range dependencies.
- Evidence anchors:
  - [section] "TransUNet builds upon the U-Net model by introducing a hybrid encoder that combines CNN and Transformer to address the limitations of traditional convolutional neural networks in modeling long-range dependencies and handling large-sized images."
  - [section] "This hybrid structure combines convolutional neural networks' feature extraction capability with effective global information modeling using Transformer modules, yielding better performance than using pure Transformers as encoders."
  - [corpus] No direct evidence found in corpus, but the general trend in related papers suggests hybrid approaches are gaining traction.
- Break condition: If the CNN feature extraction is insufficient or if the patch size is not optimally chosen for the Transformer input.

### Mechanism 3
- Claim: Swin-Unet's shifted window mechanism reduces computational complexity while preserving positional information, making it more efficient for medical image segmentation.
- Mechanism: Swin-Unet decomposes the input feature map into multiple small patches, with each patch independently computing attention weights within a window. The shifted window mechanism then alternates the window positions, allowing for cross-window connection while maintaining computational efficiency.
- Core assumption: Limiting attention calculation to local windows significantly reduces computational complexity without sacrificing the ability to capture relevant global information for segmentation tasks.
- Evidence anchors:
  - [section] "Swin-Unet, on the other hand, is another novel segmentation model that introduces the Swin Transformer module [11] to improve computational efficiency. The Swin Transformer is a hierarchical self-attention mechanism that decomposes the input feature map into multiple small patches, with each patch independently computing attention weights, thus reducing computational complexity."
  - [section] "Additionally, Swin Transformer introduces the mechanism of shifting windows on top of the self-attention mechanism. By limiting the attention calculation to windows in the vicinity of the current region, Swin-Unet better preserves positional information and further improves the model's performance."
  - [corpus] No direct evidence found in corpus, but the computational efficiency claim aligns with the Swin Transformer's design principles.
- Break condition: If the window size is too small to capture necessary contextual information, or if the shifting mechanism introduces artifacts in the segmentation.

## Foundational Learning

- Concept: Self-attention mechanism in Transformers
  - Why needed here: Understanding how self-attention works is crucial to grasping how Transformer modules capture long-range dependencies in medical images, which is the key innovation in TransUNet.
  - Quick check question: How does the self-attention mechanism in Transformers differ from the local receptive field of convolutional layers?

- Concept: Encoder-decoder architecture with skip connections
  - Why needed here: All four models (U-Net, UNet++, TransUNet, Swin-Unet) use variations of the U-shaped encoder-decoder architecture. Understanding this architecture is essential for understanding how features are extracted and combined at different scales.
  - Quick check question: What is the purpose of skip connections in U-Net-like architectures, and how do they help preserve spatial information?

- Concept: Evaluation metrics for image segmentation (Dice coefficient, HD95, IoU, accuracy, precision, recall)
  - Why needed here: The paper uses these metrics to quantitatively evaluate model performance. Understanding what each metric measures and their relative importance is crucial for interpreting the results.
  - Quick check question: How does the Dice coefficient differ from IoU, and in what scenarios might one be preferred over the other for evaluating segmentation performance?

## Architecture Onboarding

- Component map:
  Input layer -> CNN backbone (TransUNet) -> Transformer modules -> U-Net-like encoder-decoder structure -> Output layer

- Critical path:
  1. Image preprocessing and normalization
  2. Feature extraction using CNN backbone (TransUNet) or patch embedding (Swin-Unet)
  3. Self-attention computation in Transformer modules
  4. Feature fusion through skip connections
  5. Upsampling and final segmentation prediction

- Design tradeoffs:
  - Transformer vs CNN: Transformers capture global context but are computationally expensive; CNNs are efficient but limited to local features
  - Window size in Swin-Unet: Larger windows capture more context but increase computation; smaller windows are efficient but may miss global information
  - Model complexity vs performance: More complex models (TransUNet) achieve better performance but require more computational resources

- Failure signatures:
  - Poor segmentation at image boundaries: May indicate issues with padding or window shifting in Swin-Unet
  - Inconsistent segmentation across similar structures: Could suggest insufficient training data or issues with the attention mechanism
  - High computational cost or memory errors: Likely due to inefficient Transformer implementation or inappropriate batch sizes

- First 3 experiments:
  1. Reproduce U-Net results on the Tuberculosis Chest X-rays dataset to establish a baseline
  2. Implement TransUNet with a reduced number of Transformer layers to understand the impact of Transformer depth on performance
  3. Compare Swin-Unet with different window sizes to find the optimal balance between computational efficiency and segmentation accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of TransUNet and Swin-Unet compare to other recent transformer-based models like Segment Anything Model (SAM) when applied to medical image segmentation tasks?
- Basis in paper: [explicit] The paper mentions SAM and MedSAM as future trends and promising research directions, but does not compare their performance to the models studied.
- Why unresolved: The paper focuses on comparing U-Net, UNet++, TransUNet, and Swin-Unet, without including newer transformer-based models like SAM in the comparison.
- What evidence would resolve it: Conducting experiments to compare the performance of SAM and other recent transformer-based models with TransUNet and Swin-Unet on the same medical image segmentation datasets.

### Open Question 2
- Question: How does the segmentation performance of the models vary across different types of medical images (e.g., CT, MRI, X-ray) and anatomical regions?
- Basis in paper: [explicit] The paper evaluates the models on Tuberculosis Chest X-rays and ovarian tumors datasets, but does not explore performance across a wider range of medical image types and anatomical regions.
- Why unresolved: The study is limited to two specific datasets, and it is unclear how the models would perform on other types of medical images or anatomical regions.
- What evidence would resolve it: Conducting experiments to evaluate the models' performance on a diverse set of medical image types (e.g., CT, MRI, X-ray) and anatomical regions (e.g., brain, liver, heart).

### Open Question 3
- Question: What are the computational requirements and limitations of the models when applied to large-scale medical image segmentation tasks?
- Basis in paper: [explicit] The paper mentions that TransUNet increases the difficulty of model training due to the introduction of Transformer modules and requires reducing the batch size to meet GPU training requirements.
- Why unresolved: The paper does not provide detailed information on the computational requirements and limitations of the models when applied to large-scale medical image segmentation tasks.
- What evidence would resolve it: Conducting experiments to evaluate the computational requirements (e.g., memory usage, training time) and limitations of the models when applied to large-scale medical image segmentation tasks with high-resolution images and extensive datasets.

## Limitations
- Narrow scope focusing only on four specific models without comprehensive coverage of the rapidly evolving field
- Evaluation based on only two benchmark datasets, which may not represent the diversity of medical imaging modalities
- Lack of ablation studies to isolate the specific contributions of Transformer modules versus architectural changes

## Confidence
- High confidence: The general claim that Transformer modules improve long-range dependency modeling in medical image segmentation
- Medium confidence: The comparative performance rankings between models, as they depend on specific datasets and hyperparameters
- Low confidence: The mechanism explanations for why hybrid CNN-Transformer architectures work better, as the paper doesn't provide sufficient empirical evidence

## Next Checks
1. Implement U-Net with and without Transformer modules on the same datasets to isolate the specific contribution of Transformers to performance improvements
2. Test the best-performing model (TransUNet) on additional medical image segmentation datasets (e.g., LUNA16 for lung nodules, BraTS for brain tumors) to assess generalizability
3. Measure and compare the actual computational costs (memory usage, inference time) of TransUNet versus pure CNN baselines under identical hardware conditions to verify the claimed efficiency gains of Swin-Unet's window mechanism