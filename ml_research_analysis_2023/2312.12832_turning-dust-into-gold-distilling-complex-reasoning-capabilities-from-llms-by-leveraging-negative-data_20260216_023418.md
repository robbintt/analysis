---
ver: rpa2
title: 'Turning Dust into Gold: Distilling Complex Reasoning Capabilities from LLMs
  by Leveraging Negative Data'
arxiv_id: '2312.12832'
source_url: https://arxiv.org/abs/2312.12832
tags:
- negative
- reasoning
- knowledge
- data
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of distilling complex reasoning
  capabilities from large language models (LLMs) to smaller models, particularly for
  arithmetic reasoning tasks. The core method involves leveraging negative data (incorrect
  reasoning chains) alongside positive data to enhance model specialization.
---

# Turning Dust into Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative Data

## Quick Facts
- arXiv ID: 2312.12832
- Source URL: https://arxiv.org/abs/2312.12832
- Reference count: 28
- Primary result: 75.75% accuracy improvement over fine-tuning on MATH dataset

## Executive Summary
This work introduces a novel framework for distilling complex reasoning capabilities from large language models (LLMs) to smaller models by leveraging negative data (incorrect reasoning chains). The approach addresses the challenge of transferring sophisticated reasoning patterns while avoiding contamination from incorrect outputs. The framework consists of three components: Negative Assistant Training (NAT) dynamically integrates knowledge from negative samples using a dual LoRA structure, Negative Calibrated Enhancement (NCE) calibrates self-distillation using negative knowledge, and Adaptive Self-Consistency (ASC) employs a ranking model to weight answer aggregation. Experiments on the MATH dataset demonstrate significant improvements over traditional fine-tuning and chain-of-thought knowledge distillation methods.

## Method Summary
The proposed framework addresses the challenge of distilling complex reasoning capabilities from LLMs to smaller models by exploiting negative data alongside positive samples. The three-step approach begins with Negative Assistant Training (NAT), which uses a dual LoRA structure to dynamically integrate knowledge from incorrect reasoning chains without inheriting their undesirable behaviors. The second component, Negative Calibrated Enhancement (NCE), improves self-distillation by using the negative model as a baseline to identify critical knowledge differences through KL divergence calibration. Finally, Adaptive Self-Consistency (ASC) enhances answer aggregation by weighting candidates based on rationale quality using a ranking model trained on both positive and negative data. The framework is evaluated on the MATH dataset with LLaMA-7B as the student model and demonstrates significant improvements over baseline methods.

## Key Results
- NAT improves accuracy by 75.75% over fine-tuning and 27.8% over chain-of-thought knowledge distillation on MATH dataset
- NCE further improves performance by 10% when applied after NAT
- ASC enhances self-consistency with a 4.06% accuracy improvement on MATH test set
- The framework shows effective exploitation of negative data to boost reasoning capabilities in smaller models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NAT can extract useful knowledge from negative samples without inheriting their undesirable behaviors.
- Mechanism: NAT uses a dual LoRA structure where a negative LoRA is first trained on incorrect samples, then dynamically integrated with a positive LoRA through a corrected attention mechanism that can both add and subtract the negative contribution based on token context.
- Core assumption: The negative LoRA captures generalizable patterns that can improve reasoning even though the final answers are wrong.
- Evidence anchors:
  - [abstract]: "Negative Assistant Training (NAT) dynamically integrates knowledge from negative samples using a dual LoRA structure"
  - [section]: "we design Dynamic Integrate Unit... By adding a correction term [0.5;-0.5] on [ αpos; αneg], the attention weights for hneg are constrained to the range of [-0.5, 0.5]"
- Break condition: If the negative LoRA overfits to spurious patterns unique to incorrect answers, the corrected attention may amplify noise rather than useful signals.

### Mechanism 2
- Claim: NCE improves self-distillation by using the negative model as a baseline to identify critical knowledge differences.
- Mechanism: NCE computes the KL divergence between the negative model and the current model, using this as a scaling factor β to emphasize samples where the models differ most, indicating potentially valuable reasoning steps.
- Core assumption: Samples where the NAT model differs from the negative model contain reasoning steps that are uniquely valuable for learning.
- Evidence anchors:
  - [abstract]: "Negative Calibrated Enhancement (NCE) uses negative knowledge to calibrate self-distillation"
  - [section]: "we use KL divergence to measure such inconsistency and maximize the expectation... By introducing β to adjust the loss weights of different samples"
- Break condition: If the negative model and NAT model become too similar, β approaches zero and NCE provides no additional benefit.

### Mechanism 3
- Claim: ASC improves answer aggregation by weighting candidates based on rationale quality rather than just frequency.
- Mechanism: ASC trains a ranking model on both positive and negative samples to predict whether a rationale-answer pair is correct, then uses these scores to reweight votes during self-consistency.
- Core assumption: A rationale's quality correlates with whether it leads to the correct answer, and this can be learned from the binary labels of positive/negative samples.
- Evidence anchors:
  - [abstract]: "Adaptive Self-Consistency (ASC) employs a ranking model trained on both positive and negative data to weight answer aggregation"
  - [section]: "we construct training samples... and use MSE loss to train Mrank"
- Break condition: If the ranking model accuracy is too low (as noted in the paper at ~60%), the weighting may introduce more noise than signal.

## Foundational Learning

- Concept: Chain-of-thought reasoning
  - Why needed here: The entire distillation process relies on transferring step-by-step reasoning chains from LLMs to smaller models
  - Quick check question: What is the difference between chain-of-thought prompting and chain-of-thought distillation?

- Concept: Knowledge distillation principles
  - Why needed here: The framework builds on standard distillation but extends it to handle both correct and incorrect outputs
  - Quick check question: How does hard distillation differ from soft distillation in the context of LLMs?

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: The dual LoRA structure is central to NAT's parameter-efficient approach to integrating negative knowledge
  - Quick check question: Why is LoRA preferred over full fine-tuning in this architecture?

## Architecture Onboarding

- Component map:
  - Teacher LLMs (GPT-3.5 Turbo, GPT-4) → generate reasoning chains
  - Student LLaMA-7B → base model to be specialized
  - Dual LoRA modules → positive and negative adapters
  - Dynamic Integrate Unit → corrected attention fusion
  - Ranking model → for ASC weighting
  - Training pipeline → NAT → NCE → ASC stages

- Critical path:
  1. Generate positive/negative samples from LLMs
  2. Train negative LoRA on Dneg
  3. Train positive LoRA with dynamic integration of negative LoRA
  4. Generate augmented data with NAT model
  5. Train NCE model with KL-based calibration
  6. Train ranking model for ASC
  7. Apply ASC during inference

- Design tradeoffs:
  - Using LoRA vs full fine-tuning: parameter efficiency vs potential representational limitations
  - Two-stage NAT training: prevents negative contamination but requires more compute
  - ASC ranking model: adds inference overhead but improves accuracy

- Failure signatures:
  - NAT model performs worse than baseline → negative LoRA may be overfitting
  - NCE provides no improvement → KL divergence may not capture relevant differences
  - ASC hurts performance → ranking model accuracy too low or miscalibrated

- First 3 experiments:
  1. Train NAT without the corrected attention mechanism (use simple averaging instead) to measure its contribution
  2. Train NCE with constant β=1 (no calibration) to verify the KL-based scaling helps
  3. Test ASC with random weights vs learned weights to confirm the ranking model adds value

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed framework scale with increasing model size for the student model?
- Basis in paper: [inferred] The paper focuses on LLaMA-7b as the student model and demonstrates improvements over baselines, but does not explore how performance changes with different model sizes.
- Why unresolved: The paper does not provide experiments or analysis on how the effectiveness of the proposed framework varies when applied to student models of different sizes, such as LLaMA-13b or LLaMA-33b.
- What evidence would resolve it: Experiments showing the performance of the proposed framework across a range of student model sizes, with a focus on how the improvements over baselines scale with model capacity.

### Open Question 2
- Question: What is the impact of using different chain-of-thought generation strategies for the LLM on the effectiveness of the proposed framework?
- Basis in paper: [explicit] The paper mentions using few-shot CoT and zero-shot CoT prompting strategies to generate rationales from LLMs, but does not explore how different generation strategies affect the framework's performance.
- Why unresolved: The paper does not investigate how the quality or diversity of the generated rationales, influenced by the prompting strategy, impacts the effectiveness of the proposed framework in distilling reasoning capabilities.
- What evidence would resolve it: Experiments comparing the performance of the proposed framework when using rationales generated with different chain-of-thought prompting strategies, such as least-to-most prompting or self-consistency, and analyzing the impact on the distilled model's reasoning abilities.

### Open Question 3
- Question: How does the proposed framework perform on non-mathematical reasoning tasks, such as commonsense reasoning or symbolic reasoning?
- Basis in paper: [explicit] The paper demonstrates the framework's effectiveness on arithmetic reasoning tasks using the MATH dataset, but does not evaluate its performance on other types of reasoning tasks.
- Why unresolved: The paper does not provide evidence of the framework's generalization to reasoning tasks beyond arithmetic, such as commonsense reasoning, logical reasoning, or symbolic reasoning tasks.
- What evidence would resolve it: Experiments applying the proposed framework to non-mathematical reasoning datasets, such as StrategyQA for commonsense reasoning or datasets involving symbolic reasoning tasks, and comparing the performance to state-of-the-art methods in those domains.

### Open Question 4
- Question: What is the impact of incorporating additional forms of negative data, such as incorrect answers without rationales or partially correct rationales, on the proposed framework's performance?
- Basis in paper: [inferred] The paper focuses on using rationales with incorrect answers as negative data, but does not explore the potential benefits of incorporating other forms of negative data, such as incorrect answers without rationales or rationales with partially correct reasoning.
- Why unresolved: The paper does not investigate how the inclusion of different types of negative data, beyond rationales with incorrect answers, affects the effectiveness of the proposed framework in distilling reasoning capabilities from LLMs.
- What evidence would resolve it: Experiments comparing the performance of the proposed framework when incorporating various forms of negative data, such as incorrect answers without rationales or partially correct rationales, and analyzing the impact on the distilled model's reasoning abilities and robustness to incorrect reasoning patterns.

## Limitations
- The framework's effectiveness critically depends on the quality of negative samples generated by LLMs, with no quantification of partially correct samples
- The ranking model accuracy for ASC is only ~60%, marginally better than random guessing
- The framework is primarily evaluated on MATH dataset problems and may not generalize to diverse real-world reasoning tasks

## Confidence
- **High Confidence**: The core observation that negative data can be leveraged to improve reasoning capabilities (75.75% improvement over fine-tuning baseline)
- **Medium Confidence**: The specific mechanisms (NAT, NCE, ASC) and their individual contributions are supported by the data
- **Low Confidence**: The scalability of this approach to much larger models or different reasoning domains

## Next Checks
1. **Ablation on Negative Sample Quality**: Systematically vary the quality of negative samples (e.g., partially correct vs completely wrong) to determine the threshold at which negative data becomes detrimental rather than helpful.

2. **Cross-Domain Transfer Evaluation**: Apply the complete NAT-NCE-ASC pipeline to a reasoning domain completely disjoint from mathematics (e.g., commonsense reasoning or scientific reasoning) to assess whether the negative data exploitation strategy transfers across domains.

3. **Ranking Model Calibration Analysis**: Conduct a detailed analysis of the ranking model's confidence calibration on the MATH test set, examining whether high-confidence ASC weights actually correlate with correct answers across different question difficulty levels.