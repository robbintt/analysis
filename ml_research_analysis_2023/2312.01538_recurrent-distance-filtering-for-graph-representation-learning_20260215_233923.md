---
ver: rpa2
title: Recurrent Distance Filtering for Graph Representation Learning
arxiv_id: '2312.01538'
source_url: https://arxiv.org/abs/2312.01538
tags:
- graph
- gred
- node
- linear
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel Graph Recurrent Encoding by Distance
  (GRED) model that leverages linear recurrent neural networks (RNNs) interleaved
  with permutation-invariant neural networks to effectively utilize information from
  distant nodes in graph representation learning. The key idea is to aggregate nodes
  based on their shortest distances to a target node and encode the sequence of hop
  representations using a linear RNN.
---

# Recurrent Distance Filtering for Graph Representation Learning

## Quick Facts
- **arXiv ID**: 2312.01538
- **Source URL**: https://arxiv.org/abs/2312.01538
- **Reference count**: 40
- **Primary result**: GRED achieves comparable or better performance than state-of-the-art graph transformers on various benchmarks with significantly reduced computational cost.

## Executive Summary
This paper introduces GRED, a novel graph representation learning model that leverages linear recurrent neural networks interleaved with permutation-invariant neural networks to effectively utilize information from distant nodes. The key innovation is aggregating nodes based on their shortest distances to a target node and encoding the sequence of hop representations using a linear RNN. This approach achieves competitive or superior performance to state-of-the-art graph transformers while requiring significantly less computation, as it eliminates the need for positional encoding and enables parallel processing.

## Method Summary
GRED operates by first computing shortest distances between all node pairs, then grouping nodes by their distance from each target node. For each distance level, node features are aggregated via a permutation-invariant function (two-layer MLP), and the resulting sequence of hop representations is encoded by a linear RNN parameterized in diagonal complex form. The model uses LayerNorm, GLU activations, and skip connections between layers. The linear RNN enables parallel computation and stable long-range signal propagation through its eigenvalue properties.

## Key Results
- GRED achieves comparable or better performance than state-of-the-art graph transformers on node classification and graph classification benchmarks
- The model requires significantly less computational resources than attention-based approaches
- Theoretical analysis proves GRED is strictly more expressive than one-hop message passing neural networks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Linear RNN with diagonal parameterization enables stable long-range signal propagation without vanishing/exploding gradients.
- **Mechanism**: The diagonal complex-form recurrence sₖ = Λsₖ₋₁ + Winxₖ allows eigenvalues in the unit disk, ensuring stable propagation. The parallel scan implementation avoids sequential bottlenecks.
- **Core assumption**: The diagonal form with polar parametrization of eigenvalues is sufficient for stability and expressiveness.
- **Evidence anchors**:
  - [abstract] "The linear RNN is parameterized in a particular diagonal form for stable long-range signal propagation"
  - [section 2] "The linearity of the recurrence and the diagonal state transition matrix enable a parallel scan over the sequences, which further speeds up the computation"
  - [corpus] Weak - no direct evidence about diagonal RNN stability in corpus
- **Break condition**: If eigenvalues escape the unit disk during training, stability is lost and gradients explode/vanish.

### Mechanism 2
- **Claim**: Permutation-invariant aggregation followed by linear RNN encoding preserves the hierarchical structure of neighborhoods better than one-hop message passing.
- **Mechanism**: Nodes are grouped by shortest distance, aggregated via MLP2(ΣMLP1(hu)), then the sequence of hop representations is encoded by the linear RNN. This naturally encodes distance ordering without positional encoding.
- **Core assumption**: The injectivity of the linear RNN (Lemma 4.1) guarantees that distance information is preserved in the encoding.
- **Evidence anchors**:
  - [abstract] "for a given target node, our model aggregates other nodes by their shortest distances to the target and uses a parallelizable linear recurrent network over the chain of distances"
  - [section 3] "The sequence of the set representations is then encoded by a linear recurrent neural network... starting from x(ℓ)v,K which is the farthest away from the target node to the target node itself"
  - [corpus] Weak - corpus mentions distance encoding but not this specific mechanism
- **Break condition**: If the RNN hidden state dimension is too small relative to K, injectivity is lost and distance information is corrupted.

### Mechanism 3
- **Claim**: GRED is strictly more expressive than one-hop MPNNs because it can distinguish graphs that 1-WL test cannot.
- **Mechanism**: By encoding the full sequence of hop representations through the linear RNN, GRED can capture fine-grained neighborhood structures that one-hop MPNNs miss. The example in Figure 3 shows two graphs indistinguishable by 1-WL but distinguishable by GRED.
- **Core assumption**: The combination of injective multiset aggregation and linear RNN is sufficient to break the 1-WL expressiveness barrier.
- **Evidence anchors**:
  - [section 4.2] "Theorem 4.3. One wide enough GRED layer... is strictly more powerful than one iteration of any 1-hop message passing algorithm"
  - [section 4] "GRED provides distinct updates for the two graphs above. Such graphs, however, are indistinguishable under the 1-WL isomorphism test"
  - [corpus] Weak - corpus mentions expressiveness but not this specific comparison
- **Break condition**: If either the MLP or RNN components are not sufficiently wide, the theoretical expressiveness advantage may not materialize in practice.

## Foundational Learning

- **Concept**: Shortest path distances and graph diameter
  - **Why needed here**: GRED relies on categorizing nodes by their shortest distances to the target node, which requires computing the graph diameter and all-pairs shortest paths.
  - **Quick check question**: What is the time complexity of computing all-pairs shortest paths in an unweighted graph with n nodes and m edges?

- **Concept**: Permutation-invariant functions
  - **Why needed here**: The aggregation of nodes at the same distance must be permutation-invariant to handle arbitrary node orderings.
  - **Quick check question**: Why is it important that the aggregation function AGG is injective, and how does the MLP2(ΣMLP1(hu)) construction ensure this?

- **Concept**: Diagonalization of linear operators and complex-valued recurrences
  - **Why needed here**: The linear RNN is implemented in diagonal complex form for stability and parallelizability, requiring understanding of eigenvalue decomposition and complex arithmetic.
  - **Quick check question**: What is the advantage of using complex eigenvalues over real eigenvalues in the diagonal RNN formulation?

## Architecture Onboarding

- **Component map**: Input node features → LayerNorm → MLP1 (per node) → Multiset aggregation by distance → MLP2 (per distance) → Linear RNN (diagonal complex) → LayerNorm → MLP (GLU) → Output
- **Critical path**: The linear RNN recurrence from the farthest nodes back to the target node is the core information flow that distinguishes GRED from standard GNNs.
- **Design tradeoffs**:
  - Larger K captures more distant information but increases computation and memory
  - Complex-valued RNN vs real-valued: complex enables better conditioning but requires more parameters
  - Diagonal RNN vs structured SSM: diagonal is simpler and faster but may be less expressive
- **Failure signatures**:
  - Performance plateaus or degrades with increasing K → indicates over-squashing or insufficient model capacity
  - Training instability → eigenvalues escaping unit disk in diagonal RNN
  - Poor generalization on long-range tasks → insufficient K or inadequate eigenvalue initialization
- **First 3 experiments**:
  1. Ablation: Replace LRU with vanilla RNN and measure performance drop on Peptides-func
  2. Sensitivity: Vary K from 1 to max diameter on CIFAR10 and plot accuracy curve
  3. Efficiency: Compare training time and memory usage of GRED vs Graphormer on ZINC dataset

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the optimal value of K (maximum hop distance) for different types of graph datasets and tasks?
- **Basis in paper**: [explicit] The paper shows that performance varies with K on CIFAR10, ZINC, and Peptides-func datasets, with optimal values lying between 1 and Kmax.
- **Why unresolved**: The paper demonstrates that the optimal K depends on the specific task and dataset, but does not provide a general method for determining the best K value for new datasets or tasks.
- **What evidence would resolve it**: Empirical studies testing various K values across diverse graph datasets and tasks, potentially leading to a heuristic or method for estimating the optimal K value based on dataset characteristics.

### Open Question 2
- **Question**: How does the performance of GRED compare to state-of-the-art methods on larger, more complex graph datasets?
- **Basis in paper**: [inferred] The paper evaluates GRED on several benchmark datasets but does not test it on larger or more complex graph datasets.
- **Why unresolved**: The paper focuses on smaller benchmark datasets, leaving questions about GRED's scalability and performance on larger, real-world graph datasets.
- **What evidence would resolve it**: Extensive testing of GRED on large-scale graph datasets, such as social networks or knowledge graphs, comparing its performance to current state-of-the-art methods.

### Open Question 3
- **Question**: Can the theoretical expressiveness of GRED be further improved by modifying its architecture or using different linear recurrent networks?
- **Basis in paper**: [explicit] The paper proves that GRED is more expressive than one-hop message passing neural networks but does not explore potential architectural modifications or alternative linear recurrent networks.
- **Why unresolved**: The paper establishes a lower bound on GRED's expressiveness but does not investigate upper bounds or potential improvements through architectural changes.
- **What evidence would resolve it**: Theoretical analysis and empirical experiments testing various modifications to GRED's architecture, such as different types of linear recurrent networks or alternative ways of encoding hop representations, to determine if its expressiveness can be further improved.

## Limitations

- The Floyd-Warshall pre-processing step has O(n³) complexity, which may become prohibitive for very large graphs despite the overall efficiency gains.
- The theoretical expressiveness claims rely on injectivity of the linear RNN, but practical implementations may not achieve this due to finite precision and limited width.
- The diagonal RNN parameterization assumes complex eigenvalues are necessary for stability, but empirical evidence for this assumption is limited.

## Confidence

- **High confidence**: The basic mechanism of distance-based aggregation followed by linear recurrence encoding is well-founded and the experimental results on standard benchmarks are reproducible.
- **Medium confidence**: The claim about superior expressiveness compared to one-hop MPNNs is theoretically sound but may not translate to consistent practical advantages across all tasks.
- **Medium confidence**: The efficiency claims relative to graph transformers are reasonable given the parallel implementation of the linear RNN, but depend heavily on the specific transformer architecture being compared.

## Next Checks

1. **Expressiveness validation**: Test GRED on graphs where the key distinguishing examples (like Figure 3) appear to verify it can actually differentiate structures that 1-WL cannot.
2. **Stability analysis**: Monitor eigenvalue magnitudes during training across different datasets to empirically verify the stability claims of the diagonal RNN parameterization.
3. **Efficiency profiling**: Measure actual wall-clock time and memory usage for GRED vs Graphormer on large-scale graphs (e.g., >10K nodes) to validate the claimed efficiency advantages.