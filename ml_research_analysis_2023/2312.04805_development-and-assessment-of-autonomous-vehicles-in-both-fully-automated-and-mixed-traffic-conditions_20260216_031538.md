---
ver: rpa2
title: Development and Assessment of Autonomous Vehicles in Both Fully Automated and
  Mixed Traffic Conditions
arxiv_id: '2312.04805'
source_url: https://arxiv.org/abs/2312.04805
tags:
- driving
- vehicle
- track
- lane
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multi-stage approach to develop and assess
  autonomous vehicles (AVs) in both fully automated and mixed traffic conditions.
  The core method involves using deep reinforcement learning (DRL) to train AVs in
  a Unity-based urban environment, progressing from a single AV to connected AVs with
  vehicle-to-vehicle (V2V) communication.
---

# Development and Assessment of Autonomous Vehicles in Both Fully Automated and Mixed Traffic Conditions

## Quick Facts
- arXiv ID: 2312.04805
- Source URL: https://arxiv.org/abs/2312.04805
- Authors: 
- Reference count: 35
- Key outcome: AVs trained with DRL achieve driving performance comparable to human drivers, with 90% and 100% safety rates in unidirectional and bidirectional V2V communication, respectively.

## Executive Summary
This paper presents a multi-stage approach to develop and assess autonomous vehicles (AVs) in both fully automated and mixed traffic conditions. The core method involves using deep reinforcement learning (DRL) to train AVs in a Unity-based urban environment, progressing from a single AV to connected AVs with vehicle-to-vehicle (V2V) communication. The study also includes a human-in-the-loop survey to evaluate AV performance against human drivers in mixed traffic scenarios. Key results show that AVs trained with DRL can achieve driving performance comparable to or better than human drivers, with 90% and 100% safety rates in unidirectional and bidirectional V2V communication, respectively. However, mixed traffic safety cannot be guaranteed due to unpredictable human driver behavior.

## Method Summary
The study employs a multi-stage approach to develop and assess AVs using deep reinforcement learning (DRL) with Proximal Policy Optimization (PPO). The method involves training AVs in a Unity-based urban environment, starting with a single AV and progressing to connected AVs with V2V communication. The AV agents interact with the environment through a Markov Decision Process (MDP) loop, receiving rewards for safe driving, smooth control, and reaching the finish line efficiently. The study also incorporates a human-in-the-loop survey to evaluate AV performance against human drivers in mixed traffic scenarios.

## Key Results
- AVs trained with DRL achieved driving performance comparable to or better than human drivers.
- Safety rates of 90% and 100% were achieved in unidirectional and bidirectional V2V communication, respectively.
- Mixed traffic safety could not be guaranteed due to unpredictable human driver behavior.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep reinforcement learning (DRL) enables autonomous vehicles (AVs) to learn driving behavior that matches or exceeds human driving performance.
- Mechanism: The DRL model uses Proximal Policy Optimization (PPO) to iteratively update the AV's policy based on rewards for safe driving, smooth control, and reaching the finish line efficiently. The agent receives feedback from the environment through a Markov Decision Process (MDP) loop of actions, states, and rewards.
- Core assumption: The reward function is properly designed to capture the key aspects of good driving behavior, and the DRL algorithm can effectively learn from the environment's feedback.
- Evidence anchors:
  - [abstract] "Results show that using deep reinforcement learning, the AV acquired driving behavior that reached human driving performance."
  - [section 2.3.4] "The structuring of reward signals is based on the linguistic criteria outlined below, while Table 1 presents the numerical values for these signals."
- Break condition: The DRL model fails to converge or produces unsafe driving behavior if the reward function is poorly designed or the training environment is not sufficiently realistic.

### Mechanism 2
- Claim: Vehicle-to-vehicle (V2V) communication enhances cooperation between AVs and improves their driving performance in mixed traffic scenarios.
- Mechanism: V2V communication allows AVs to share perception data (e.g., sensor readings, velocity, position) and coordinate their actions to avoid collisions and optimize traffic flow. The "sharing and caring" concept pairs AVs into supportive units that exchange data and provide mutual aid.
- Core assumption: AVs can reliably share accurate perception data and make coordinated decisions based on this shared information.
- Evidence anchors:
  - [abstract] "The adoption of sharing and caring based V2V communication within AV networks enhances their driving behavior, aids in more effective action planning, and promotes collaborative behavior amongst the AVs."
  - [section 2.3.3] "The shared information consists of the blue vehicle's 16 raycast sensor readings, velocity, and position."
- Break condition: V2V communication fails or becomes unreliable if there are technical issues, interference, or malicious actors attempting to disrupt the communication.

### Mechanism 3
- Claim: Mixed traffic safety cannot be guaranteed due to unpredictable human driver behavior, but AVs can still cooperate with human drivers by adopting a conservative driving approach.
- Mechanism: The AV's DRL model learns to anticipate and respond to human driver actions, prioritizing safety and cooperation over speed or efficiency. In the survey experiment, the AV demonstrated a tendency to slow down or avoid aggressive maneuvers when interacting with human drivers.
- Core assumption: Human drivers exhibit some level of predictability in their actions, and the AV can learn to anticipate and respond to these actions through its DRL model.
- Evidence anchors:
  - [abstract] "The survey shows that safety in mixed traffic cannot be guaranteed, as we cannot control human ego-driven actions if they decide to compete with AV."
  - [section 3.5.1] "In the observations from the mixed traffic experiment, it was noted that many human drivers prioritized their own performance over cooperation."
- Break condition: The AV's DRL model fails to adequately anticipate or respond to highly unpredictable or aggressive human driver behavior, leading to unsafe interactions.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: MDP is the fundamental framework for modeling the AV's interaction with the environment in the DRL algorithm.
  - Quick check question: What are the key components of an MDP, and how do they relate to the AV's decision-making process?

- Concept: Proximal Policy Optimization (PPO)
  - Why needed here: PPO is the specific DRL algorithm used to train the AV's policy in this study.
  - Quick check question: How does PPO differ from other policy gradient methods, and what are its key advantages for training AVs?

- Concept: Vehicle-to-Vehicle (V2V) Communication
  - Why needed here: V2V communication is a critical component of the cooperative driving system between AVs.
  - Quick check question: What types of information can be shared through V2V communication, and how does this information enable AVs to cooperate and improve traffic safety?

## Architecture Onboarding

- Component map: AV agents (using DRL with PPO) -> Unity-based urban environment -> V2V communication modules -> human-in-the-loop survey component
- Critical path: 1) Design and implement the AV agent with DRL and PPO, 2) Create the Unity-based urban environment with V2V communication capabilities, 3) Train the AV agent in the environment using the DRL algorithm, 4) Conduct the human-in-the-loop survey to evaluate the AV's performance in mixed traffic scenarios, and 5) Analyze the results and iterate on the design as needed.
- Design tradeoffs: The main tradeoffs involve balancing the complexity of the AV's DRL model and the realism of the training environment against the computational resources available and the time required for training. Additionally, the V2V communication system must balance the benefits of cooperation against the potential risks of relying on external communication.
- Failure signatures: Potential failures include: the AV agent exhibiting unsafe or unpredictable behavior due to issues with the DRL model or training environment, V2V communication failures leading to a lack of cooperation between AVs, or the AV agent failing to adequately anticipate or respond to human driver behavior in mixed traffic scenarios.
- First 3 experiments:
  1. Train a single AV agent in the Unity environment using DRL and PPO, and evaluate its performance against a human driver in a solo race scenario.
  2. Train two AV agents with and without V2V communication, and compare their cooperation and safety performance in a multi-AV scenario.
  3. Conduct a human-in-the-loop survey where human drivers interact with the AV agent in a mixed traffic scenario, and evaluate the AV's ability to cooperate and maintain safety.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do autonomous vehicles trained with deep reinforcement learning handle sudden, unexpected obstacles or maneuvers from human drivers in mixed traffic scenarios?
- Basis in paper: [inferred] The paper mentions that the survey shows safety in mixed traffic cannot be guaranteed due to unpredictable human driver behavior.
- Why unresolved: The paper does not provide specific data on AV performance in handling sudden, unexpected maneuvers by human drivers in mixed traffic.
- What evidence would resolve it: Experimental data or simulations showing AV responses to sudden, unexpected maneuvers by human drivers in mixed traffic.

### Open Question 2
- Question: What is the impact of increased AV-to-AV communication on traffic flow efficiency and safety in fully automated environments?
- Basis in paper: [explicit] The paper discusses the adoption of sharing and caring based V2V communication within AV networks, which enhances their driving behavior and promotes collaborative behavior among AVs.
- Why unresolved: The paper does not provide quantitative data on traffic flow efficiency and safety improvements due to increased AV-to-AV communication.
- What evidence would resolve it: Quantitative data comparing traffic flow efficiency and safety metrics in fully automated environments with varying levels of AV-to-AV communication.

### Open Question 3
- Question: How does the presence of AVs affect the driving behavior and safety of human drivers in mixed traffic conditions?
- Basis in paper: [inferred] The paper notes that dealing with mixed traffic poses challenges for human drivers and requires professional drivers and a conservative driving approach to avoid aggressive maneuvers that could lead to accidents.
- Why unresolved: The paper does not provide detailed analysis of changes in human driving behavior and safety in the presence of AVs.
- What evidence would resolve it: Observational studies or simulations showing changes in human driving behavior and safety metrics in mixed traffic conditions with varying AV penetration rates.

## Limitations
- The Unity-based simulation environment may not fully capture the complexity and unpredictability of real-world driving conditions.
- The human-in-the-loop survey involves a limited sample size of 11 participants, which may not represent the full spectrum of human driving behaviors and reactions to AVs.
- The study's findings on mixed traffic safety are particularly uncertain, as they rely on simulated human driver behavior rather than real-world interactions.

## Confidence
**High Confidence:**
- The effectiveness of DRL (specifically PPO) in training AVs to achieve driving performance comparable to human drivers in controlled environments.
- The positive impact of V2V communication on AV cooperation and safety in multi-AV scenarios.

**Medium Confidence:**
- The ability of AVs to cooperate with human drivers in mixed traffic scenarios, given the conservative approach adopted.
- The general applicability of the study's findings to real-world driving conditions, considering the limitations of the simulation environment.

**Low Confidence:**
- The guarantee of safety in mixed traffic conditions due to unpredictable human driver behavior.
- The long-term effects of widespread AV adoption on traffic patterns and human driver behavior.

## Next Checks
1. **Expand Simulation Scenarios**: Conduct further simulations with a wider variety of road types, traffic densities, and environmental conditions to test the robustness of the AV system beyond the single intersection scenario.
2. **Increase Human Participant Diversity**: Expand the human-in-the-loop survey to include a more diverse participant pool in terms of age, driving experience, and cultural background to better understand the range of human responses to AVs in mixed traffic.
3. **Real-world Pilot Study**: Implement a small-scale real-world pilot study with AVs and human-driven vehicles in a controlled urban environment to validate the simulation results and identify any discrepancies between simulated and real-world performance.