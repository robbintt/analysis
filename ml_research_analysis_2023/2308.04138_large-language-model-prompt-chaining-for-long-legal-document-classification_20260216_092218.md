---
ver: rpa2
title: Large Language Model Prompt Chaining for Long Legal Document Classification
arxiv_id: '2308.04138'
source_url: https://arxiv.org/abs/2308.04138
tags:
- legal
- language
- prompt
- arxiv
- chaining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a prompt chaining method for classifying
  long legal documents, a challenging task due to their complexity and length. The
  method involves generating a concise summary of the document, performing semantic
  similarity search to retrieve relevant training samples, and then using few-shot
  prompting to generate a label.
---

# Large Language Model Prompt Chaining for Long Legal Document Classification

## Quick Facts
- arXiv ID: 2308.04138
- Source URL: https://arxiv.org/abs/2308.04138
- Reference count: 37
- Key outcome: Prompt chaining improves legal document classification over zero-shot prompting and matches ChatGPT on micro-F1 for ECHR task

## Executive Summary
This paper introduces a prompt chaining method for classifying long legal documents, addressing the challenge of their complexity and length. The approach involves generating document summaries, performing semantic similarity search to retrieve relevant training samples, and using few-shot prompting to generate labels. Evaluated on ECHR (binary) and SCOTUS (multi-class) datasets, the method improves performance over zero-shot prompting and surpasses zero-shot ChatGPT on ECHR micro-F1 score. However, it shows lower performance than ChatGPT on SCOTUS, though it's more effective at retrieving higher frequency classes. The results demonstrate prompt chaining's potential for legal document classification.

## Method Summary
The method uses a three-stage prompt chaining approach: first generating concise summaries of long legal documents using task-specific models (PRIMERA for legal), then performing semantic similarity search to retrieve relevant training samples based on these summaries, and finally using few-shot prompting to generate classification labels. The approach leverages in-context learning capabilities of large language models like GPT-NeoX-20B and Flan-UL2. Summaries preserve legal context better than generic models, and retrieved examples guide the classification decision through semantic similarity.

## Key Results
- Prompt chaining improves over zero-shot prompting on ECHR binary classification task
- Method surpasses zero-shot ChatGPT on ECHR micro-F1 score
- Performance on SCOTUS multi-class classification is lower than zero-shot ChatGPT but more effective at retrieving higher frequency classes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt chaining improves performance by decomposing a complex classification task into smaller, more manageable sub-tasks.
- Mechanism: The method first generates a concise summary of the long legal document, then retrieves semantically similar training samples, and finally performs few-shot prompting to generate a label. Each step builds on the previous one, reducing the complexity for the model at each stage.
- Core assumption: Decomposing tasks into smaller sub-tasks allows the model to focus on specific aspects (summarization, retrieval, classification) rather than handling the full document complexity at once.
- Evidence anchors:
  - [abstract] "Chaining is a strategy used to decompose complex tasks into smaller, manageable components."
  - [section] "Prompt chaining is a methodology employed to decompose complex tasks into smaller, manageable sub-tasks."
  - [corpus] Weak evidence - no direct corpus citations, but related work like "Answering Questions in Stages: Prompt Chaining for Contract QA" supports decomposition strategies.
- Break condition: If summaries lose critical legal context, the subsequent retrieval and classification steps will be negatively impacted.

### Mechanism 2
- Claim: Using task-specific summarization models (like PRIMERA) preserves legal context better than generic models or direct prompting.
- Mechanism: Task-specific models are fine-tuned on legal documents, enabling them to capture domain-specific vocabulary and structure in the summaries, which are then used for similarity search and classification.
- Core assumption: Domain-specific training improves the quality of generated summaries for specialized tasks compared to general-purpose models.
- Evidence anchors:
  - [section] "the PRIMERA model, fine-tuned specifically on legal documents, generated summaries where the core legal context was mostly preserved."
  - [section] "Initial experimentation with direct prompting approaches on large language models resulted in variable outcomes... The inspection indicated that the summaries were relatively generic and often omitted the core legal issues of interest."
  - [corpus] Weak evidence - no direct corpus citations, but the comparison between BRIO and PRIMERA models suggests domain adaptation is beneficial.
- Break condition: If the summarization model is not sufficiently fine-tuned on relevant legal data, the summaries may omit critical information.

### Mechanism 3
- Claim: Semantic similarity search retrieves relevant in-context examples that improve few-shot learning performance.
- Mechanism: By embedding summaries and finding semantically closest neighbors from the training set, the model receives relevant examples that guide its classification decision.
- Core assumption: Semantic similarity between summaries correlates with similarity in classification labels, making retrieved examples useful for few-shot learning.
- Evidence anchors:
  - [section] "we embedded all our summaries with the models discussed... and calculated the semantically closest neighbors... of each summary in the development and test sets, as compared to the training set."
  - [section] "These summaries from the training set, along with their true labels... served as the few-shot prompts. This approach leverages the in-context learning abilities of large language models."
  - [corpus] Weak evidence - no direct corpus citations, but the use of semantic search is a standard technique in few-shot learning.
- Break condition: If the training set lacks diversity or the embedding model poorly captures legal semantics, retrieved examples may be irrelevant.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: The method relies on providing examples within the prompt to guide the model's classification decision without fine-tuning.
  - Quick check question: How does providing labeled examples in the prompt help the model perform classification without parameter updates?

- Concept: Semantic similarity search
  - Why needed here: Used to find relevant training examples that are semantically similar to the target document's summary, improving the quality of few-shot prompts.
  - Quick check question: Why is it important to retrieve semantically similar examples rather than randomly selected ones for few-shot prompting?

- Concept: Document summarization techniques
  - Why needed here: Long legal documents exceed model context windows, so summarization is necessary to create manageable inputs while preserving critical information.
  - Quick check question: What are the trade-offs between using generic vs. task-specific summarization models for legal documents?

## Architecture Onboarding

- Component map: Document preprocessing → Summary generation (PRIMERA) → Embedding & similarity search → Few-shot prompt construction → Label generation (GPT-NeoX/Flan-UL2)
- Critical path: Summary generation → Semantic similarity search → Label generation (few-shot prompting)
- Design tradeoffs:
  - Model size vs. inference cost: Larger models may improve performance but increase computational requirements.
  - Summary length vs. context preservation: Shorter summaries fit context windows but may lose important details.
  - Number of few-shot examples vs. prompt length: More examples can improve performance but may exceed context limits.
- Failure signatures:
  - Poor performance on minority classes: Indicates retrieved examples may not cover diverse cases.
  - Inconsistent results across runs: Suggests sensitivity to prompt construction or model randomness.
  - Degradation with longer documents: Implies summarization is losing critical information.
- First 3 experiments:
  1. Compare zero-shot vs. few-shot performance on a small subset of ECHR data to validate the chaining approach.
  2. Test different summarization models (PRIMERA vs. BRIO) on the same data to measure impact on classification accuracy.
  3. Vary the number of retrieved examples (1-shot, 4-shot, 8-shot) to find the optimal balance between performance and prompt length.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of prompt chaining with larger language models (e.g., GPT-4) compare to smaller models (e.g., GPT-NeoX-20B) on legal document classification tasks?
- Basis in paper: [explicit] The paper mentions future work exploring larger models on additional legal benchmarks.
- Why unresolved: The study only used models up to 20 billion parameters and did not compare with larger models like GPT-4.
- What evidence would resolve it: Conducting experiments using larger language models and comparing their performance with smaller models on legal document classification tasks.

### Open Question 2
- Question: How effective is the prompt chaining approach for legal document classification tasks in languages other than English?
- Basis in paper: [inferred] The paper focuses solely on English language datasets (ECHR and SCOTUS) and does not explore other languages.
- Why unresolved: The study did not evaluate the approach on non-English legal document classification tasks.
- What evidence would resolve it: Applying the prompt chaining approach to legal document classification tasks in other languages and comparing the results with those obtained in English.

### Open Question 3
- Question: How does the performance of prompt chaining compare to supervised fine-tuning on legal document classification tasks?
- Basis in paper: [explicit] The paper mentions that full supervised fine-tuning involves hours of update runs with thousands of annotated samples, while their experiments only included inference calls.
- Why unresolved: The study did not directly compare the performance of prompt chaining with supervised fine-tuning on the same tasks.
- What evidence would resolve it: Conducting experiments using both prompt chaining and supervised fine-tuning on the same legal document classification tasks and comparing their performance.

## Limitations

- Performance varies across tasks, with better results on binary classification than multi-class problems
- Method shows lower performance than ChatGPT on SCOTUS multi-class classification despite better retrieval of frequent classes
- Effectiveness on diverse legal domains beyond ECHR and SCOTUS remains untested

## Confidence

- Medium: The paper demonstrates prompt chaining improves over zero-shot approaches in specific cases, but results are not consistently superior to large commercial models like ChatGPT. Comparative analysis is limited to two datasets.

## Next Checks

1. **Generalization Testing**: Evaluate the prompt chaining method on additional legal classification benchmarks (e.g., SEC filings, contracts, or patent documents) to assess whether the approach generalizes beyond the ECHR and SCOTUS datasets used in the study.

2. **Ablation Study**: Systematically remove each component of the chain (summarization, similarity search, few-shot prompting) to quantify the individual contribution of each step to overall performance, particularly focusing on whether summarization is essential or if direct few-shot prompting could achieve similar results.

3. **Error Analysis on Minority Classes**: Conduct detailed analysis of classification errors, especially for underrepresented classes in the SCOTUS dataset, to determine whether the retrieval mechanism introduces bias toward majority classes and identify specific failure patterns in the chaining pipeline.