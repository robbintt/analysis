---
ver: rpa2
title: Rating-based Reinforcement Learning
arxiv_id: '2307.16348'
source_url: https://arxiv.org/abs/2307.16348
tags:
- learning
- rating
- human
- rbrl
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes rating-based reinforcement learning (RbRL),
  which learns reward functions from human ratings instead of pairwise preferences
  or demonstrations. The key innovation is a novel multi-class cross-entropy loss
  function that accepts multi-class human ratings as input, building on a new prediction
  model for human ratings.
---

# Rating-based Reinforcement Learning

## Quick Facts
- arXiv ID: 2307.16348
- Source URL: https://arxiv.org/abs/2307.16348
- Reference count: 23
- Primary result: RbRL learns reward functions from human ratings, achieving better performance than preference-based RL with less cognitive load on human evaluators

## Executive Summary
This paper introduces rating-based reinforcement learning (RbRL), a framework that learns reward functions from human ratings of individual trajectories rather than pairwise preferences. RbRL uses a novel multi-class cross-entropy loss function that accepts multi-class human ratings as input, building on a new prediction model for human ratings. The approach differs from preference-based RL by using absolute evaluations of individual trajectories rather than relative comparisons. Experiments show RbRL outperforms preference-based RL under both synthetic and real human feedback, with 5-6 rating classes performing best on Walker and 2-3 classes on Quadruped. In human studies, participants found RbRL less mentally demanding and stressful than preference-based RL while achieving similar or better performance. Users could provide approximately 14.03 ratings per minute compared to 8.7 preferences per minute, making RbRL more time-efficient.

## Method Summary
RbRL learns reward functions from human ratings using a multi-class cross-entropy loss function. The framework collects trajectory segments from an environment, presents them to humans for rating across multiple classes (e.g., 0-5 stars), and computes normalized predicted returns for each segment. Rating class boundaries are estimated from the data, and rating probabilities are computed using a softmax-like function over these boundaries. The cross-entropy loss is calculated between predicted and actual human ratings, and both reward predictor and policy networks are updated using PPO. The method operates in environments where the reward function is unknown, requiring learning from human feedback instead of predefined rewards.

## Key Results
- RbRL outperforms preference-based RL under both synthetic and real human feedback
- 5-6 rating classes perform best on Walker environment, while 2-3 classes work best on Quadruped
- Participants found RbRL less mentally demanding and stressful than preference-based RL
- Users could provide approximately 14.03 ratings per minute compared to 8.7 preferences per minute

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: RbRL learns richer reward functions from ratings than PbRL learns from pairwise preferences.
- **Mechanism**: RbRL uses multi-class ratings that encode absolute trajectory quality, while PbRL only gets binary preference signals. Each rating provides more bits of information per human interaction.
- **Core assumption**: Humans can provide consistent multi-class ratings and the rating boundaries can be learned from the distribution of normalized returns.
- **Evidence anchors**: [abstract] "RbRL differs from preference-based RL by using absolute evaluations of individual trajectories rather than relative comparisons"
- **Break condition**: If human raters cannot provide consistent ratings across queries, or if the rating classes are poorly calibrated, the reward learning will fail.

### Mechanism 2
- **Claim**: RbRL's multi-class cross-entropy loss function can effectively learn from multi-class ratings without pairwise comparisons.
- **Mechanism**: The loss function uses a probability distribution over rating classes based on normalized predicted returns and learned rating class boundaries. This allows the model to predict human ratings directly.
- **Core assumption**: The normalized predicted return and rating class boundaries can be computed from the dataset, and the probability model accurately reflects human rating behavior.
- **Evidence anchors**: [section] "We design a new multi-class cross-entropy loss function that accepts multi-class human ratings as the input"
- **Break condition**: If the probability model poorly fits actual human rating behavior, or if the rating class boundaries cannot be accurately estimated, the loss function will not work effectively.

### Mechanism 3
- **Claim**: RbRL reduces cognitive load on human evaluators compared to PbRL.
- **Mechanism**: Rating individual trajectories requires less mental effort than comparing pairs of trajectories, as users don't need to evaluate relative quality or deal with incomparable pairs.
- **Core assumption**: Humans find it easier to provide absolute ratings than relative preferences, especially when comparing similar-quality trajectories.
- **Evidence anchors**: [abstract] "participants found RbRL less mentally demanding and stressful than preference-based RL"
- **Break condition**: If the cognitive load difference is not significant for the target user population, or if users find rating scales confusing, this advantage may not materialize.

## Foundational Learning

- **Concept**: Markov Decision Process (MDP) without reward function
  - Why needed here: RbRL operates in environments where the reward function is unknown, requiring learning from human ratings instead of predefined rewards
  - Quick check question: What is the key difference between a standard MDP and an MDP without reward in the context of RbRL?

- **Concept**: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RbRL is a specific type of RLHF that uses absolute ratings rather than relative preferences or demonstrations
  - Quick check question: How does RbRL differ from preference-based RL in terms of the type of human feedback used?

- **Concept**: Cross-entropy loss function for multi-class classification
  - Why needed here: RbRL uses a novel multi-class cross-entropy loss to learn from human ratings across multiple rating classes
  - Quick check question: How does RbRL's cross-entropy loss differ from standard multi-class cross-entropy loss in supervised learning?

## Architecture Onboarding

- **Component map**: Environment -> Trajectory segments -> Human rating interface -> Rating class boundary estimator -> Reward predictor -> Cross-entropy loss -> Policy network

- **Critical path**:
  1. Collect trajectory segments from environment
  2. Present segments to human for rating
  3. Compute normalized predicted returns for segments
  4. Estimate rating class boundaries
  5. Compute rating probabilities using Equation 3
  6. Calculate cross-entropy loss using Equation 2
  7. Update reward predictor and policy networks

- **Design tradeoffs**:
  - Number of rating classes: More classes provide richer information but may be harder for humans to use consistently
  - Rating class boundaries: Fixed boundaries simplify implementation but learned boundaries may better match human preferences
  - Noise modeling: The hyperparameter k in Equation 3 controls noise modeling but requires tuning

- **Failure signatures**:
  - Poor reward learning despite high-quality human ratings
  - Inconsistent rating predictions across similar trajectories
  - Slow convergence or poor final performance compared to PbRL

- **First 3 experiments**:
  1. Test RbRL with 2 rating classes vs PbRL on a simple environment to verify basic functionality
  2. Vary the number of rating classes (n=2,3,4,5,6) on Walker environment to find optimal n
  3. Compare RbRL vs PbRL with expert vs non-expert human feedback on multiple environments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RbRL compare to PbRL when using weak preferences (where users can indicate two trajectories are equally preferable)?
- Basis in paper: [explicit] The paper mentions that some PbRL works use weak preference queries but notes these don't capture quality information for equally preferable trajectories
- Why unresolved: The paper only compares RbRL to standard PbRL using strict preferences, not weak preferences
- What evidence would resolve it: Experimental comparison of RbRL vs PbRL with weak preference queries on the same tasks and user study conditions

### Open Question 2
- Question: What is the optimal number of rating classes for RbRL in environments with very different characteristics (e.g., sparse vs dense rewards, discrete vs continuous action spaces)?
- Basis in paper: [inferred] The paper shows n=5-6 works best for Walker and n=2-3 for Quadruped, but doesn't systematically explore why or test more diverse environments
- Why unresolved: Only tested on continuous control MuJoCo environments with similar characteristics
- What evidence would resolve it: Comprehensive study across diverse environment types with varying reward structures and action spaces

### Open Question 3
- Question: How does user consistency in rating quality vary over time, and what are the performance impacts on RbRL?
- Basis in paper: [explicit] The paper mentions this as a challenge, noting that "ratings may not be given consistently during learning, especially considering users' attention span and fatigue level over time"
- Why unresolved: The paper acknowledges this issue but doesn't measure or analyze it empirically
- What evidence would resolve it: User study measuring rating consistency over time sessions, with corresponding RbRL performance degradation analysis

## Limitations

- Evaluation relies heavily on synthetic human feedback data that may not accurately capture real human rating behavior
- Human study involved only 4 participants, limiting generalizability of cognitive load findings
- Cross-entropy loss formulation assumes ratings follow a specific probability distribution that may not match actual human behavior

## Confidence

- **High Confidence**: RbRL can learn reward functions from multi-class ratings using the proposed cross-entropy loss. Supported by synthetic experiments showing learning curves and convergence.
- **Medium Confidence**: RbRL achieves better or comparable performance to PbRL while reducing cognitive load on human evaluators. Synthetic experiments support performance claims, but human study sample size is small.
- **Low Confidence**: RbRL is significantly more time-efficient than PbRL in real human evaluations. While the 14.03 vs 8.7 queries per minute difference is reported, the small human study sample size limits confidence.

## Next Checks

1. **Replicate with larger human study**: Conduct a new human evaluation study with at least 15-20 participants to validate the cognitive load and time efficiency findings, using randomized ordering of RbRL and PbRL tasks to control for learning effects.

2. **Test rating consistency**: Design an experiment where the same trajectory segments are rated by multiple humans across multiple sessions to measure inter-rater reliability and test whether the assumed rating probability distribution matches actual human behavior.

3. **Compare query selection strategies**: Implement and compare alternative query selection methods (e.g., uncertainty sampling, diversity-based selection) for both RbRL and PbRL to determine if the ensemble disagreement strategy favors one approach over the other.