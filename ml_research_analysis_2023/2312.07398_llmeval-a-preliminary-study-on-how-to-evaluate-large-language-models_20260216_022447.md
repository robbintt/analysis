---
ver: rpa2
title: 'LLMEval: A Preliminary Study on How to Evaluate Large Language Models'
arxiv_id: '2312.07398'
source_url: https://arxiv.org/abs/2312.07398
tags:
- evaluation
- questions
- manual
- scoring
- means
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive study on evaluating Large Language
  Models (LLMs) by examining the crucial aspects of "how to evaluate." The authors
  propose a new dataset, LLMEval, and conduct evaluations on 20 LLMs using various
  criteria, annotation methods, and ranking systems. The study involves 2,186 participants,
  generating 243,337 manual annotations and 57,511 automatic evaluation results.
---

# LLMEval: A Preliminary Study on How to Evaluate Large Language Models

## Quick Facts
- **arXiv ID**: 2312.07398
- **Source URL**: https://arxiv.org/abs/2312.07398
- **Reference count**: 18
- **Primary result**: Identification of informativeness and accuracy as key criteria for LLM evaluation, with onsite star scoring showing best quality

## Executive Summary
This study presents LLMEval, a comprehensive framework for evaluating Large Language Models (LLMs) across 17 task types and 12 academic subjects. The research examines 20 LLMs using five different evaluation settings and involves 2,186 participants generating 243,337 manual annotations and 57,511 automatic evaluation results. The study reveals critical insights about evaluation criteria, annotator quality, ranking system stability, and the alignment between human and automated evaluation methods.

## Method Summary
The research collected questions for LLMEval-1 (17 task types) and LLMEval-2 (12 academic subjects), gathered responses from 20 LLMs, and implemented five evaluation settings using both manual annotation (onsite, crowd-sourcing, public) and GPT-4 automatic evaluation. The evaluation pipeline followed: Question → LLM Response → Multiple Evaluation Methods → Ranking System → Results Analysis. The study applied Elo rating system and Points scoring system for ranking, calculating accuracy, consistency, and alignment metrics across different evaluation approaches.

## Key Results
- Onsite star scoring evaluation achieved the highest accuracy (0.892) and consistency compared to other methods
- Star scoring evaluation showed better alignment between GPT-4 and manual evaluation (Spearman's ρ = 0.949) than pairwise comparison (ρ = 0.902)
- Elo rating system exhibited significant fluctuations and sequence dependence, showing poor stability even after 100,000 rounds of comparison
- GPT-4 evaluators demonstrated stronger bias towards longer and more verbose responses compared to human evaluators

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Onsite annotators provide higher quality evaluations than crowdsourcing or public annotators due to better consistency and accuracy
- Mechanism: Direct control over annotator training and quality leads to more reliable and consistent judgments
- Core assumption: Onsite annotators can be better trained and monitored than external annotators
- Evidence anchors:
  - [abstract] "Onsite annotators demonstrate the best quality in terms of accuracy and consistency"
  - [section] "The average accuracy of onsite star scoring evaluations is 0.892, with a minimum accuracy of 0.825, higher than crowd-sourcing and public pairwise comparison evaluation"
- Break condition: If onsite annotators become too large to manage quality, or if training costs exceed benefits

### Mechanism 2
- Claim: Star scoring evaluation method produces better alignment between human and automated evaluation than pairwise comparison
- Mechanism: Direct rating on criteria provides more granular and comparable data than relative pairwise judgments
- Core assumption: Absolute scoring captures more nuanced differences than relative comparison
- Evidence anchors:
  - [abstract] "The alignment between automated and manual evaluation is better under star scoring settings"
  - [section] "The Spearman's correlation coefficient (ρ) between ranks of GPT-4 and manual evaluation is 0.949" for star scoring vs "0.902" for pairwise comparison
- Break condition: If pairwise comparison becomes standardized and well-understood by both human and AI evaluators

### Mechanism 3
- Claim: Elo rating system shows instability in LLM evaluation due to sequence dependence and variance in small sample sizes
- Mechanism: Elo ratings are sensitive to the order of comparisons and small sample noise, leading to volatile rankings
- Core assumption: The mathematical properties of Elo rating system (sequence dependence and variance) manifest in LLM evaluation contexts
- Evidence anchors:
  - [abstract] "The Elo rating system exhibits significant fluctuations and sequence dependence"
  - [section] Mathematical proof showing variance formula and experimental results showing rank fluctuations even after 100,000 rounds
- Break condition: If sample sizes become large enough to average out noise, or if evaluation sequence becomes randomized

## Foundational Learning

- Concept: Statistical significance and variance in ranking systems
  - Why needed here: Understanding why Elo rating system shows instability requires knowledge of statistical variance and its impact on rankings
  - Quick check question: If you have 70% accuracy in pairwise comparisons with 1500 initial Elo score, what is the expected variance of the final score?

- Concept: Evaluation metrics and their appropriate use cases
  - Why needed here: Different evaluation methods (star scoring vs pairwise comparison) have different strengths and weaknesses that need to be understood
  - Quick check question: When would you prefer pairwise comparison over star scoring for LLM evaluation?

- Concept: Manual vs automated evaluation alignment
  - Why needed here: Understanding how human and AI evaluators can be aligned is crucial for developing reliable evaluation frameworks
  - Quick check question: What factors could cause GPT-4 to be more biased towards longer responses than human evaluators?

## Architecture Onboarding

- Component map: Question → LLM Response → Onsite Star Scoring → Crowd-sourcing Pairwise → Public Pairwise → GPT-4 Star Scoring → GPT-4 Pairwise → Ranking System → Results Analysis
- Critical path: Question → LLM Response → Multiple Evaluation Methods → Ranking System → Results Analysis
- Design tradeoffs: Accuracy vs scalability (onsite vs crowdsourcing), granularity vs simplicity (star scoring vs pairwise), stability vs sensitivity (Points vs Elo ranking)
- Failure signatures: Inconsistent rankings across methods, high variance in Elo scores, low alignment between human and AI evaluators
- First 3 experiments:
  1. Run same evaluation with different annotator types on same questions to measure consistency differences
  2. Compare Elo vs Points ranking on same pairwise comparison data to observe stability differences
  3. Test alignment between GPT-4 and human evaluators under star scoring vs pairwise comparison conditions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the long-term stability characteristics of the Elo rating system when applied to large-scale LLM evaluations with varying levels of evaluator accuracy?
- Basis in paper: [explicit] The paper demonstrates that the Elo rating system exhibits significant fluctuations and sequence dependence in LLM evaluation tasks, with poor stability even after 100,000 rounds of comparison.
- Why unresolved: While the paper identifies instability issues with the Elo rating system, it does not provide a comprehensive analysis of how different levels of evaluator accuracy (e.g., 60%, 70%, 80%) would impact the long-term stability of the rankings.
- What evidence would resolve it: A large-scale experiment varying evaluator accuracy levels and analyzing the resulting Elo rating fluctuations over an extended period (e.g., 1 million rounds) would provide insights into the long-term stability characteristics of the Elo rating system in LLM evaluations.

### Open Question 2
- Question: How do different ranking systems perform in terms of stability, fairness, and alignment with human preferences when evaluating LLMs across diverse tasks and domains?
- Basis in paper: [explicit] The paper compares the Elo rating system and Points scoring system, finding that the Elo rating system exhibits poor stability and sequence dependence. However, it does not explore other potential ranking systems or provide a comprehensive comparison across multiple dimensions.
- Why unresolved: The paper focuses on comparing two specific ranking systems but does not investigate a broader range of ranking methods or evaluate their performance across various tasks and domains.
- What evidence would resolve it: A comprehensive study comparing multiple ranking systems (e.g., Elo, Points, Bradley-Terry, TrueSkill) across diverse LLM evaluation tasks and domains, measuring stability, fairness, and alignment with human preferences, would provide insights into the most suitable ranking approaches for LLM evaluation.

### Open Question 3
- Question: What is the optimal balance between automated and manual evaluation methods for LLM assessment, considering factors such as accuracy, consistency, scalability, and cost?
- Basis in paper: [inferred] The paper compares manual and automated evaluation methods, finding that onsite star scoring exhibits the best quality in terms of accuracy and consistency, while automated evaluation can cover a large number of tasks in a short time and shows reasonable alignment with human evaluators.
- Why unresolved: While the paper provides insights into the strengths and weaknesses of manual and automated evaluation methods, it does not determine the optimal balance between the two approaches for LLM assessment.
- What evidence would resolve it: A systematic study evaluating the trade-offs between manual and automated evaluation methods across various dimensions (e.g., accuracy, consistency, scalability, cost) and determining the optimal combination of methods for different LLM evaluation scenarios would provide guidance on the most effective evaluation approach.

## Limitations

- Generalizability may be limited to the specific 20 LLMs tested, not capturing full diversity of LLM architectures
- Potential bias in question selection or task types that could skew evaluation results
- Reliance on GPT-4 for automated evaluation introduces its own biases, particularly preference for longer responses

## Confidence

- **High Confidence**: Onsite annotators demonstrate superior accuracy and consistency compared to crowdsourcing and public annotators
- **Medium Confidence**: Star scoring evaluation method produces better alignment between human and automated evaluation than pairwise comparison
- **Low Confidence**: Elo rating system's instability is primarily due to mathematical properties rather than implementation issues

## Next Checks

1. **Replication with Diverse LLMs**: Test the evaluation framework with a broader range of LLMs, including smaller models and those with different architectural approaches, to assess the generalizability of the findings.

2. **Cross-Annotator Consistency Study**: Conduct a controlled experiment comparing onsite annotators with well-trained external annotators to isolate the specific factors contributing to quality differences beyond just the onsite location.

3. **Elo System Stress Test**: Implement a randomized evaluation sequence and increase sample sizes to determine the minimum conditions required for Elo rating stability, and compare results with alternative ranking systems like Bradley-Terry.