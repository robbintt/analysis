---
ver: rpa2
title: When do Generative Query and Document Expansions Fail? A Comprehensive Study
  Across Methods, Retrievers, and Datasets
arxiv_id: '2309.08541'
source_url: https://arxiv.org/abs/2309.08541
tags:
- expansion
- query
- expansions
- document
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language model (LM)-based query and document expansion techniques
  improve weaker retrieval models but generally harm stronger models. This negative
  correlation between model performance and gains from expansion holds across eleven
  expansion techniques, twelve diverse datasets, and twenty-four retrieval models.
---

# When do Generative Query and Document Expansions Fail? A Comprehensive Study Across Methods, Retrievers, and Datasets

## Quick Facts
- **arXiv ID**: 2309.08541
- **Source URL**: https://arxiv.org/abs/2309.08541
- **Reference count**: 22
- **Primary result**: LM-based query and document expansions improve weaker retrieval models but generally harm stronger models.

## Executive Summary
This paper presents a comprehensive study of LM-based query and document expansion techniques across eleven methods, twelve diverse datasets, and twenty-four retrieval models. The key finding is a strong negative correlation between base model performance and gains from expansion: weaker models benefit from expansions while stronger models are harmed. The study reveals that expansions introduce noise that obscures relevance signals, leading to false positives. Long query format shift is identified as the primary exception where expansions consistently help most models by transforming queries into more standard forms.

## Method Summary
The paper evaluates eleven LM-based expansion techniques (HyDE, Chain of Thought, Q-LM PRF, Doc2Query, D-LM PRF, and others) across twenty-four retrieval models spanning all major architectures (bi-encoders, cross-encoders, late-interaction). Twelve datasets covering various distribution shifts are tested, with expansions applied to top-100 BM25 retrieved documents. The study includes extensive error analysis to understand when and why expansions fail, particularly focusing on the negative impact on strong models and the exception for long query format shifts.

## Key Results
- A strong negative correlation exists between base model performance and gains from expansion across all tested models and techniques
- Long query format shift is the only distribution shift where expansions consistently improve performance across most models
- Qualitative error analysis reveals expansions introduce noise that obscures relevance signals, leading to false positives
- Domain, document, and relevance shifts show limited improvement from expansions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Query and document expansions improve weaker retrieval models by adding contextually relevant terms that align with the corpus distribution.
- **Mechanism**: Expansions generate additional query terms or document content that bridge the semantic gap between the user's intent and the corpus representation. For weaker models like DPR, this additional context helps overcome limitations in semantic understanding.
- **Core assumption**: The expanded terms are semantically relevant and help match documents that would otherwise be missed by the base retrieval model.
- **Evidence anchors**: [abstract] "expansion improves scores for weaker models"; [section 3] "models with lower base performance benefit from expansion"
- **Break condition**: When the expansion introduces noise that obscures the original relevance signal, particularly for stronger models that already capture semantic nuances effectively.

### Mechanism 2
- **Claim**: Expansion effectiveness is inversely correlated with base model performance.
- **Mechanism**: Stronger models already capture the relevant semantic relationships effectively, so additional expansion terms either add redundant information or introduce noise that interferes with the model's ability to rank documents accurately.
- **Core assumption**: The ranking capability of the base model determines whether expansion will be beneficial or harmful.
- **Evidence anchors**: [abstract] "there exists a strong negative correlation between retriever performance and gains from expansion"; [section 3] "as base performance on a task increases, the gains from expansion decrease"
- **Break condition**: When the dataset has severe distribution shifts (e.g., long queries) that the base model cannot handle effectively, even strong models may benefit from expansion.

### Mechanism 3
- **Claim**: Long query format shift is the primary exception where expansions consistently help most models.
- **Mechanism**: Long queries contain implicit information that needs to be extracted and summarized into concise, standard-form queries that align with how models were trained on typical web queries.
- **Core assumption**: The original long queries contain relevant information that, when properly extracted and condensed, improves matching with relevant documents.
- **Evidence anchors**: [section 4] "the only distribution shift that consistently needs expansion is long query format shift"; [section 4] "when we examine why expansions help for long query shift, we find that it transforms the query to become more 'standard'"
- **Break condition**: When the expansion fails to properly extract the core information from long queries, potentially introducing noise or irrelevant context.

## Foundational Learning

- **Concept: Distribution shift in information retrieval**
  - Why needed here: Understanding how different types of distribution shifts (domain, relevance, format) affect expansion effectiveness is central to the paper's analysis.
  - Quick check question: What are the three main types of distribution shifts evaluated in this paper, and how do they differ in their impact on expansion effectiveness?

- **Concept: Neural IR model architectures (bi-encoder, cross-encoder, late-interaction)**
  - Why needed here: The paper evaluates expansion effects across multiple model architectures, and understanding their differences helps explain why some benefit more from expansion than others.
  - Quick check question: How do bi-encoder models like DPR differ from cross-encoder models like MonoT5 in their approach to semantic matching?

- **Concept: Query and document expansion techniques**
  - Why needed here: The paper evaluates multiple expansion methods (HyDE, Q-LM PRF, CoT, Doc2Query, D-LM PRF) and understanding their mechanisms is crucial for interpreting results.
  - Quick check question: What is the key difference between query expansion (Q-LM PRF) and document expansion (D-LM PRF) techniques?

## Architecture Onboarding

- **Component map**: Retrieval model -> Top-100 documents -> LLM expansion generation -> Reranking -> Evaluation
- **Critical path**: Retrieve top-K documents → Generate expansions using LLM → Rerank with expanded queries/documents → Evaluate performance
- **Design tradeoffs**: Using LLM expansions adds computational cost and potential noise, but may improve recall for weaker models; the tradeoff depends on model strength and dataset characteristics
- **Failure signatures**: Negative performance changes, especially for strong models; keywords in expansions that distract from the original query intent; false positives introduced by irrelevant expansion terms
- **First 3 experiments**:
  1. Replicate the base performance comparison on TREC DL 2019 with DPR, Contriever, and MonoT5-3B to verify the negative correlation pattern
  2. Test expansion effectiveness on a long query dataset (e.g., ArguAna) to confirm the exception to the general trend
  3. Conduct qualitative error analysis on a small sample of failures to identify common patterns in expansion-induced errors

## Open Questions the Paper Calls Out

Some potential research questions and directions that could be explored based on the findings in the paper:

1. Can we develop methods to mitigate the negative effects of LM expansions on strong IR models? The paper suggests that expansions harm stronger models by obscuring the relevance signal. Investigating techniques to preserve the original relevance signal while still benefiting from the additional context provided by expansions could be valuable.

2. How do different LM architectures and training objectives impact the effectiveness of expansions? The paper uses GPT-3.5-turbo for expansions, but exploring how other LMs like GPT-4 or different training objectives (e.g., instruction tuning) affect the quality of expansions and their impact on IR models could provide insights.

3. Can we develop adaptive expansion techniques that dynamically adjust the extent of expansion based on the model's strength and the distribution shift? The paper suggests that expansions are beneficial for weaker models or when there is a significant distribution shift. Developing methods to automatically determine the optimal level of expansion for a given model and dataset could improve overall performance.

## Limitations

- **Language restriction**: Results are limited to English datasets and may not generalize to non-English or cross-lingual retrieval
- **Access barriers**: Requires commercial language model APIs, creating cost barriers for researchers
- **Computational requirements**: Significant GPU resources needed (10,000+ A6000 GPU hours)
- **No training adaptation**: Does not explore training rankers to handle augmentations, potentially missing beneficial use cases

## Confidence

**High Confidence**: The negative correlation between base model performance and gains from expansion is well-supported across 24 retrieval models and 11 expansion techniques.

**Medium Confidence**: The qualitative error analysis showing that expansions introduce noise obscuring relevance signals is supported by examples but could benefit from more systematic analysis.

**Low Confidence**: The assertion that expansion should be avoided for strong models to maintain clear relevance signals is prescriptive and may depend on specific use cases.

## Next Checks

1. **Cross-lingual validation**: Test expansion effectiveness on non-English datasets to verify if the negative correlation pattern holds across languages, particularly for models trained on multilingual corpora.

2. **Expanded error analysis**: Conduct a more systematic qualitative analysis of expansion-induced failures by categorizing error types (false positives, relevance signal distortion, etc.) across different model architectures and expansion techniques.

3. **Training adaptation study**: Train a subset of strong models to handle expanded queries/documents and measure whether this adaptation can recover the benefits of expansion without the noise, addressing the limitation of not training rankers to handle augmentations.