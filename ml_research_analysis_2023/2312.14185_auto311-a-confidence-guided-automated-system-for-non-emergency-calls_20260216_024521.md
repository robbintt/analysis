---
ver: rpa2
title: 'Auto311: A Confidence-guided Automated System for Non-emergency Calls'
arxiv_id: '2312.14185'
source_url: https://arxiv.org/abs/2312.14185
tags:
- auto311
- incident
- information
- confidence
- types
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Auto311 is the first automated system for handling non-emergency
  311 calls, developed in collaboration with Nashville's Department of Emergency Communications.
  It features incident type prediction, information itemization, and confidence-guided
  dialogue optimization.
---

# Auto311: A Confidence-guided Automated System for Non-emergency Calls

## Quick Facts
- arXiv ID: 2312.14185
- Source URL: https://arxiv.org/abs/2312.14185
- Reference count: 28
- One-line primary result: Auto311 achieves 92.54% average F1 score on incident type prediction and 0.93 average consistency score on information itemization for non-emergency calls

## Executive Summary
Auto311 is the first automated system for handling non-emergency 311 calls, developed in collaboration with Nashville's Department of Emergency Communications. It features incident type prediction, information itemization, and confidence-guided dialogue optimization. The system dynamically predicts incident types during calls, generates tailored case reports, and strategically structures dialogues using confidence scores. Experiments show Auto311 achieves 92.54% average F1 score on incident type prediction and 0.93 average consistency score on information itemization. System-level emulations demonstrate 94.49% accuracy in categorizing calls and reduced conversation turns. Auto311 outperforms large language models and adapts to evolving city knowledge, offering a reliable, transparent solution for non-emergency call management.

## Method Summary
Auto311 uses a hierarchical multi-layer structure for incident type prediction, where each layer independently classifies context for one incident type while excluding previously identified types. Confidence scores from Monte Carlo Dropout quantify uncertainty across these classifications. For information itemization, extractive QA frameworks quote relevant caller utterances for narrative fields while binary classification predicts yes/no fields from the latest utterance. The system employs a custom text comparison metric combining keyword overlap and SentenceBERT embeddings to measure consistency. Confidence scores from both modules optimize dialogue flow by prioritizing general fields and adapting to emergent incident types.

## Key Results
- Achieves 92.54% average F1 score on incident type prediction across 8 categories
- Attains 0.93 average consistency score for information itemization
- Demonstrates 94.49% accuracy in categorizing calls during system-level emulations
- Reduces conversation turns compared to baseline approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Confidence-guided dynamic incident type prediction allows the system to adapt to multiple and shifting incident types within a single call.
- Mechanism: The system uses a hierarchical multi-layer structure with bootstrap-like classification. Each layer independently classifies the context for one incident type, excluding previously identified types. Confidence scores from Monte Carlo Dropout quantify uncertainty across these classifications, enabling dynamic updates when new evidence emerges.
- Core assumption: Caller utterances contain sufficient contextual information for hierarchical classification to identify all relevant incident types, and confidence scores reliably indicate when re-evaluation is needed.
- Evidence anchors:
  - [abstract] "dynamically predicts ongoing non-emergency incident types" and "strategically structures system-caller dialogues with optimized confidence"
  - [section Confidence-guided System Design] describes the hierarchical structure and confidence scoring methodology
  - [corpus] Weak - corpus neighbors focus on emergency detection and speech triage, not multi-type classification in 311 systems
- Break condition: If hierarchical structure fails to capture all incident types in a single utterance, or if confidence scores do not correlate with actual prediction accuracy, the dynamic adaptation would break down.

### Mechanism 2
- Claim: Confidence-guided information itemization ensures high-quality, relevant case report completion while minimizing unnecessary follow-up questions.
- Mechanism: For narrative fields, extractive QA frameworks quote relevant caller utterances. For yes/no fields, binary classification predicts from the latest utterance. Confidence is measured by comparing textual outputs using keyword overlap and semantic similarity (SentenceBERT embeddings). High confidence fields skip further dialogue; low confidence triggers clarification questions (capped at three turns).
- Core assumption: The textual comparison metric effectively captures semantic equivalence between generated and ground truth information, and confidence thresholds appropriately distinguish reliable from unreliable outputs.
- Evidence anchors:
  - [section Confidence-guided System Design] describes the text comparison approach using keywords and embeddings
  - [section Confidence-guided Information Itemization] shows improved consistency scores with confidence guidance (0.8255 to 0.9164 for aggressive driver behavior)
  - [corpus] Weak - corpus neighbors don't address confidence-guided QA for information itemization
- Break condition: If the text comparison metric fails to recognize semantically equivalent information as consistent, or if confidence thresholds are poorly calibrated, the system may either over-collect information or miss critical details.

### Mechanism 3
- Claim: Confidence scores from both prediction and itemization modules optimize the dialogue flow by prioritizing general fields and adapting to emergent incident types.
- Mechanism: After each utterance, the system updates the case report and question list. High-confidence incident type predictions trigger specialized field identification, while unfinished general fields guide follow-up questions. Confidence scores also detect shifting incident types, allowing the system to update reports in real-time.
- Core assumption: Confidence scores from both modules are reliable indicators of information completeness and can be effectively combined to optimize dialogue sequencing.
- Evidence anchors:
  - [section Report Generation and Dialogue Optimization] describes how confidence scores guide dialogue optimization and type confirmation
  - [section System Level Performance] shows Auto311 handles shifting incident types over three follow-up turns with confidence adjustments
  - [corpus] Weak - corpus neighbors focus on emergency detection and crisis management, not dialogue optimization using confidence scores
- Break condition: If confidence scores become unreliable due to model degradation or concept drift, or if the optimization logic fails to prioritize critical information, dialogue efficiency would suffer.

## Foundational Learning

- Concept: Hierarchical multi-label classification with confidence scoring
  - Why needed here: Non-emergency calls often involve multiple incident types that emerge during conversation, requiring the system to identify and track all relevant types dynamically
  - Quick check question: How does the hierarchical structure prevent double-counting incident types while allowing multiple types per call?

- Concept: Text comparison metrics for information consistency
  - Why needed here: Traditional metrics like BLEU fail to capture semantic equivalence when outputs are concise but informationally equivalent to ground truth
  - Quick check question: Why combine keyword overlap with embedding similarity instead of using a single metric for text comparison?

- Concept: Monte Carlo Dropout for uncertainty quantification
  - Why needed here: Need to measure model confidence without requiring multiple trained models or Bayesian neural networks
  - Quick check question: How does dropout at test time provide a distribution over predictions that can be used as a confidence score?

## Architecture Onboarding

- Component map: Conversational Interface → Handover Control → Incident Type Prediction → Information Itemization → Confidence-guided Report Generation and Dialogue Optimization
- Critical path: The main flow is: caller utterance → incident type prediction → information itemization → confidence-guided report update → dialogue optimization → next question
- Design tradeoffs: The system prioritizes reliability and interpretability over pure performance, using offline approaches rather than prompt-based LLMs to maintain transparency and control
- Failure signatures: High handover rate indicates the system cannot handle certain call types; low confidence scores across multiple fields suggest model uncertainty or poor calibration; inconsistent text comparison results indicate metric inadequacy
- First 3 experiments:
  1. Test confidence-guided vs non-confidence incident type prediction on held-out data to verify performance improvement
  2. Evaluate text comparison metric on manually labeled text pairs to ensure it captures semantic equivalence
  3. Simulate shifting incident types in controlled environment to verify real-time adaptation capability

## Open Questions the Paper Calls Out

- How does Auto311 handle emergency calls that initially appear non-emergency but later reveal urgency?
- What is the long-term accuracy of Auto311 as city knowledge and incident types evolve over time?
- How does Auto311 handle calls with multiple overlapping incident types that require different response protocols?

## Limitations

- Performance metrics are based on a single city's dataset and may not generalize to cities with different demographics, dialects, or incident patterns
- The system's ability to handle truly novel incident types or rapidly evolving emergency situations remains untested in real-world deployments
- Hierarchical multi-label classification may break down with ambiguous or complex scenarios where multiple incident types overlap significantly

## Confidence

- High confidence: The overall system architecture and modular design are well-specified and reproducible
- Medium confidence: Reported performance metrics may not transfer across cities; Monte Carlo Dropout confidence scoring requires further validation
- Low confidence: System's ability to handle novel incident types and blurred emergency/non-emergency boundaries remains unproven

## Next Checks

1. Deploy Auto311 on 311 call datasets from at least two other major cities with different demographics and incident profiles to assess generalizability
2. Systematically test Monte Carlo Dropout confidence scores against human-annotated uncertainty labels on a held-out test set
3. Create controlled scenarios with rapidly shifting incident types and ambiguous caller descriptions to evaluate real-time adaptation capability