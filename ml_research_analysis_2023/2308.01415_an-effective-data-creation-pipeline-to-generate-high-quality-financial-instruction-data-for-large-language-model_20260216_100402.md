---
ver: rpa2
title: An Effective Data Creation Pipeline to Generate High-quality Financial Instruction
  Data for Large Language Model
arxiv_id: '2308.01415'
source_url: https://arxiv.org/abs/2308.01415
tags:
- dataset
- financial
- data
- language
- tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a data creation pipeline for generating high-quality
  financial instruction data for large language models. The method involves initiating
  dialogues between an AI investor and financial expert using ChatGPT, refining the
  dataset with human financial experts' feedback, and fine-tuning open-source language
  models.
---

# An Effective Data Creation Pipeline to Generate High-quality Financial Instruction Data for Large Language Model

## Quick Facts
- arXiv ID: 2308.01415
- Source URL: https://arxiv.org/abs/2308.01415
- Reference count: 18
- One-line result: A data creation pipeline that generates 103k multi-turn financial instruction dialogues, significantly improving model performance on financial tasks (GPT-4 evaluation scores 6.59-7.36 vs 1.73-3.18 for base models)

## Executive Summary
This paper introduces a data creation pipeline for generating high-quality financial instruction data for large language models. The method involves initiating dialogues between an AI investor and financial expert using ChatGPT, refining the dataset with human financial experts' feedback, and fine-tuning open-source language models. The pipeline yielded a robust dataset of 103k multi-turn chats. Extensive experiments demonstrated that the approach led to significant advancements in generating accurate, relevant, and financial-style responses from AI models. The model performance on financial tasks improved significantly, with the instruction-tuned models achieving an average GPT-4 evaluation score of 6.59-7.36 compared to 1.73-3.18 for the base models.

## Method Summary
The paper presents a data creation pipeline that leverages ChatGPT's in-context learning to generate financial questions based on high-quality financial reports. Human financial experts then refine the generated questions and answers to ensure quality and relevance. The resulting dataset of 103k multi-turn chats is used to fine-tune open-source language models like LLaMA and Vicuna. The paper also explores delta-tuning techniques like LoRa for efficient model adaptation. The tuned models are evaluated on financial tasks using GPT-4 as a judge, demonstrating significant improvements in performance compared to base models.

## Key Results
- Generated a high-quality financial instruction dataset of 103k multi-turn chats using a ChatGPT-human expert pipeline
- Instruction-tuned models achieved significantly higher GPT-4 evaluation scores (6.59-7.36) compared to base models (1.73-3.18) on financial tasks
- LoRa delta-tuning effectively improved model performance while reducing computational cost compared to full fine-tuning
- The dataset and fine-tuned models demonstrated strong generalization across various financial tasks and topics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The pipeline generates high-quality financial instruction data by simulating investor-expert dialogues using ChatGPT's in-context learning, then refining with human expert feedback.
- Mechanism: ChatGPT is prompted to generate financial questions based on high-quality financial reports, then clusters and expert review filter out irrelevant or low-quality questions. The resulting dataset is used to fine-tune language models for financial tasks.
- Core assumption: ChatGPT's in-context learning can generate meaningful financial questions from structured reports, and human experts can effectively curate the dataset.
- Evidence anchors:
  - [abstract] "we initiate a dialogue between an AI investor and financial expert using ChatGPT and incorporate the feedback of human financial experts, leading to the refinement of the dataset."
  - [section 3.2] "Using ChatGPT's strong in-context learning ability, we simulate a dialogue between an investor and a financial expert."
  - [corpus] The corpus contains financial reports, but it's unclear how representative they are of broader financial discourse.
- Break condition: If ChatGPT fails to generate relevant questions or human experts cannot effectively curate the data, the pipeline will not produce high-quality datasets.

### Mechanism 2
- Claim: Instruction tuning with the generated dataset improves model performance on financial tasks compared to base models.
- Mechanism: The 103k multi-turn chats are used to fine-tune open-source language models like LLaMA and Vicuna. The tuned models are then evaluated on financial questions using GPT-4 as a judge.
- Core assumption: The quality of the generated dataset is sufficient to improve model performance, and the GPT-4 evaluation is a reliable measure of financial task proficiency.
- Evidence anchors:
  - [abstract] "Extensive experiments have been conducted on this dataset to evaluate the model's performance by adopting an external GPT-4 as the judge."
  - [section 4.4] "The results, presented in Table 4, shed light on how our models react to these finance-related queries."
  - [corpus] The dataset statistics suggest a diverse range of topics, but the actual quality of the data is not independently verified.
- Break condition: If the dataset is not diverse or representative enough, or if GPT-4 is not a reliable judge, the evaluation results may not reflect true model performance.

### Mechanism 3
- Claim: Delta-tuning techniques like LoRa can effectively improve model performance with less computational cost than full fine-tuning.
- Mechanism: LoRa is applied to the Q and V matrices in the attention layer of the language models, with a learning rate of 2e-5. This allows for efficient tuning of the models on the financial dataset.
- Core assumption: LoRa is an effective delta-tuning method for this specific task and dataset, and the chosen hyperparameters are optimal.
- Evidence anchors:
  - [section 4.1] "For delta-tuning, we utilized the LoRa technique, with parameters 'r' and 'alpha' both set to 8."
  - [section 4.3] "With the application of LORA tuning, the scores experience a substantial uplift."
  - [corpus] No evidence is provided on the effectiveness of LoRa for financial instruction tuning specifically.
- Break condition: If LoRa is not effective for this task, or if the hyperparameters are not optimal, the model performance may not improve as expected.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: ChatGPT uses in-context learning to generate financial questions based on the provided financial reports, without requiring additional training.
  - Quick check question: How does in-context learning differ from traditional fine-tuning, and why is it suitable for this task?

- Concept: Instruction tuning
  - Why needed here: The generated dataset is used to instruction-tune language models like LLaMA and Vicuna, improving their ability to handle financial tasks.
  - Quick check question: What is the difference between instruction tuning and traditional fine-tuning, and how does it affect model performance?

- Concept: Delta-tuning (LoRa)
  - Why needed here: LoRa is used to efficiently tune the language models on the financial dataset, reducing computational cost compared to full fine-tuning.
  - Quick check question: How does delta-tuning like LoRa work, and what are its advantages and disadvantages compared to full fine-tuning?

## Architecture Onboarding

- Component map: Data collection (financial reports → ChatGPT dialogues → expert review → dataset) → Model fine-tuning (base models + dataset → tuned models) → Evaluation (financial questions + GPT-4 judge → performance scores)
- Critical path: Data collection → expert review → dataset creation → model fine-tuning → evaluation. Any delays or issues in these steps will impact the overall pipeline.
- Design tradeoffs: Using ChatGPT for data generation is efficient but relies on the quality of the generated questions. Expert review ensures quality but adds time and cost. LoRa tuning is efficient but may not be as effective as full fine-tuning.
- Failure signatures: If the dataset is not diverse or high-quality, the tuned models will not perform well on financial tasks. If the evaluation questions are not representative, the performance scores may not reflect true model proficiency.
- First 3 experiments:
  1. Test ChatGPT's ability to generate relevant financial questions from a sample of financial reports.
  2. Conduct a pilot expert review on a small subset of generated questions to assess the quality and diversity of the dataset.
  3. Fine-tune a small language model on the pilot dataset and evaluate its performance on a set of financial questions to validate the pipeline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality and diversity of the financial dataset impact the performance of the fine-tuned language model?
- Basis in paper: [explicit] The paper mentions that the dataset consists of 103k multi-turn chats and was refined with human financial experts' feedback.
- Why unresolved: The paper does not provide a detailed analysis of how the quality and diversity of the dataset influence the model's performance.
- What evidence would resolve it: A comprehensive study comparing the performance of models fine-tuned with datasets of varying quality and diversity.

### Open Question 2
- Question: What is the impact of using different fine-tuning techniques, such as LORA and traditional fine-tuning, on the performance of the language model in financial tasks?
- Basis in paper: [explicit] The paper mentions that both fine-tuning and delta-tuning methods, including LORA, were used in the experiments.
- Why unresolved: The paper does not provide a detailed comparison of the performance of models fine-tuned using different techniques.
- What evidence would resolve it: A comparative analysis of the performance of models fine-tuned using various techniques, including LORA and traditional fine-tuning.

### Open Question 3
- Question: How does the size of the language model (e.g., 7B, 13B, 30B) affect its performance in financial tasks?
- Basis in paper: [explicit] The paper mentions that models of varying sizes were used in the experiments.
- Why unresolved: The paper does not provide a detailed analysis of how the size of the model influences its performance in financial tasks.
- What evidence would resolve it: A study comparing the performance of models of different sizes in financial tasks.

### Open Question 4
- Question: What is the potential for further improvement in the performance of AI models in financial tasks, and what strategies could be employed to achieve this?
- Basis in paper: [explicit] The paper mentions that there is a gap in performance when compared to GPT-3.5 and suggests that further optimization and improvement strategies should be explored.
- Why unresolved: The paper does not provide specific strategies for further improvement in the performance of AI models in financial tasks.
- What evidence would resolve it: A study identifying potential strategies for improving the performance of AI models in financial tasks and evaluating their effectiveness.

## Limitations

- The effectiveness of ChatGPT's in-context learning for generating relevant financial questions is not independently verified, and there's no ablation study comparing generated questions against human-written ones.
- The GPT-4 evaluation may not be the most reliable judge of financial task performance, especially given potential bias toward instruction-tuned models.
- The paper doesn't address potential domain shift - the financial reports used for data generation may not represent the full breadth of real-world financial scenarios.

## Confidence

*High Confidence:* The general methodology of using human-in-the-loop refinement to create instruction datasets is well-established in the literature. The improvement in evaluation scores from base to instruction-tuned models is clearly demonstrated and statistically significant.

*Medium Confidence:* The specific effectiveness of the ChatGPT-based data generation pipeline for financial instruction following. While the results show improvement, the quality of the underlying dataset is not independently verified beyond GPT-4 evaluation.

*Low Confidence:* The claim that this represents the "most comprehensive and top-notch financial instruction dataset to date" - this is difficult to verify without access to other financial instruction datasets for comparison, and the paper doesn't provide detailed dataset quality metrics beyond size.

## Next Checks

1. Conduct an ablation study comparing the performance of models trained on ChatGPT-generated data versus human-written financial instruction data to validate the quality of the generation pipeline.

2. Implement a human evaluation benchmark for financial task performance independent of GPT-4 to verify that the reported improvements reflect genuine financial reasoning capability rather than evaluation bias.

3. Test model generalization by evaluating performance on financial questions from different time periods and markets than those used in the training data to assess domain shift robustness.