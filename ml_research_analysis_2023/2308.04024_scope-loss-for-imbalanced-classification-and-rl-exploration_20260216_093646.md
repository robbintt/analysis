---
ver: rpa2
title: Scope Loss for Imbalanced Classification and RL Exploration
arxiv_id: '2308.04024'
source_url: https://arxiv.org/abs/2308.04024
tags:
- loss
- classification
- scope
- learning
- entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes an equivalence between supervised classification
  and reinforcement learning (RL), equating the exploration-exploitation trade-off
  in RL to the dataset imbalance problem in classification. It introduces Scope Loss,
  a novel loss function that adjusts gradients to prevent performance losses from
  over-exploitation and dataset imbalances, without requiring hyperparameter tuning.
---

# Scope Loss for Imbalanced Classification and RL Exploration

## Quick Facts
- arXiv ID: 2308.04024
- Source URL: https://arxiv.org/abs/2308.04024
- Reference count: 8
- One-line primary result: Scope Loss achieves 28.14% mean precision on Caltech-256, a 20% relative improvement over other loss functions, while also improving RL exploration-exploitation trade-offs.

## Executive Summary
This paper establishes an equivalence between supervised classification and reinforcement learning (RL), equating the exploration-exploitation trade-off in RL to the dataset imbalance problem in classification. It introduces Scope Loss, a novel loss function that adjusts gradients to prevent performance losses from over-exploitation and dataset imbalances, without requiring hyperparameter tuning. Experiments on benchmark RL tasks (Atari games, Procgen StarPilot) and an imbalanced classification dataset (Caltech-256) show that Scope Loss outperforms state-of-the-art loss functions.

## Method Summary
Scope Loss is a novel loss function that scales gradients using a scope scaling factor (A - αpi), where A is advantage and pi is action probability. This scaling reduces gradients for high-probability actions with positive advantages, preventing over-exploitation in RL and handling class imbalance in classification by down-weighting gradients from well-classified examples. The method uses z-score normalization of advantages to eliminate the need for environment-specific hyperparameter tuning, allowing a single optimal α value across all tasks.

## Key Results
- Achieves 28.14% mean precision on Caltech-256, representing a 20% relative improvement over other loss functions
- Demonstrates higher scores and faster convergence in RL compared to Focal Loss, Policy Loss, and Policy with Entropy Loss
- Shows lower variance in precision across classes in classification tasks
- Eliminates bias from entropy terms while maintaining effective exploration

## Why This Works (Mechanism)

### Mechanism 1
Scope Loss reduces over-exploitation gradients by scaling them with (A - αpi), where A is advantage and pi is action probability. When an action's probability pi is high and advantage A is positive, the scope scaling factor (A - αpi) decreases, thus reducing the gradient magnitude. This prevents the agent from reinforcing already-favored actions too strongly. Core assumption: High pi indicates exploitation, and reducing gradients in such cases encourages exploration without needing explicit entropy terms. Evidence: [abstract] "Scope Loss... adjusts gradients to prevent performance losses from over-exploitation" and [section 4.1] "Where (A − αpi) is the scope scaling factor, and α is the scope hyperparameter that scales pi relative to the magnitude of A". Break condition: If α is too large, the scope term can dominate the advantage term, leading to under-learning and poor convergence.

### Mechanism 2
Scope Loss handles class imbalance by down-weighting gradients from well-classified examples using the same (1 - pi) scaling. In classification, the scope scaling factor becomes (1 - pi). As the model's confidence pi approaches 1, the gradient contribution shrinks, balancing the learning signal across imbalanced classes. Core assumption: Class imbalance manifests as disparate classification difficulty; down-weighting easy examples balances the effective dataset. Evidence: [abstract] "Scope Loss... adjusts gradients to prevent performance losses from... dataset imbalances" and [section 3.1.1] "Focal Loss... down-weighs gradients of well-detected training examples based on the magnitude of the probability of the detection". Break condition: If the dataset is not truly imbalanced (e.g., all classes equally hard), this scaling can unnecessarily slow learning.

### Mechanism 3
Scope Loss eliminates bias from entropy terms present in Policy and Entropy Loss. Policy and Entropy Loss includes an entropy bonus that forces action probabilities away from 1, preventing convergence to optimal policies. Scope Loss removes this term while still promoting exploration via gradient scaling. Core assumption: Exploration can be achieved indirectly by limiting gradient growth rather than adding entropy bonuses. Evidence: [section 3.2.1] "Policy and Entropy Loss contains bias... the optimal action's probability will never converge to 1" and [section 4.1] "We can also derive Scope Loss by setting γ = 1 in 2 (removing the additional hyperparameter)". Break condition: If the environment requires sustained exploration, removing the entropy term may cause premature convergence.

## Foundational Learning

- Concept: Markov Decision Process (MDP) modeling
  - Why needed here: The paper frames classification as an MDP to unify RL and supervised learning under one theoretical framework.
  - Quick check question: In the classification MDP, what constitutes a state, action, and reward?

- Concept: Gradient weighting and loss function design
  - Why needed here: Scope Loss fundamentally relies on dynamically scaling gradients based on prediction certainty.
  - Quick check question: How does the (1 - pi) factor in Focal Loss relate to Scope Loss's (A - αpi) factor?

- Concept: Advantage estimation and normalization
  - Why needed here: Scope Loss applies z-score normalization to advantages to make α tuning unnecessary across environments.
  - Quick check question: What is the purpose of normalizing advantages before applying Scope Loss?

## Architecture Onboarding

- Component map:
  - States (observations or feature vectors) -> Policy network -> Action probabilities -> Scope Loss with normalized advantages -> Optimizer -> Updated policy
  - (Optional: Value network -> State values -> Advantage calculation -> Normalization -> Scope Loss)

- Critical path:
  1. Forward pass through policy network
  2. Compute Scope Loss using normalized advantages and action probabilities
  3. Backward pass and gradient update
  4. (Optional) Store experience for replay

- Design tradeoffs:
  - α hyperparameter: Small α maintains advantage dominance; large α risks under-learning
  - Entropy vs. scope scaling: Entropy terms explicitly encourage exploration; scope scaling does so indirectly
  - Advantage normalization: Stabilizes training but adds preprocessing step

- Failure signatures:
  - Under-learning: α too large or advantages not properly normalized
  - Over-exploration: Scope scaling too aggressive, preventing policy convergence
  - Bias in classification: Forgetting the (1 - pi) scaling can reintroduce imbalance

- First 3 experiments:
  1. Replace Cross Entropy Loss with Scope Loss (α=1) on a balanced classification dataset to verify no performance degradation
  2. Compare Scope Loss vs. Policy and Entropy Loss on a simple RL benchmark (e.g., CartPole) to observe exploration behavior
  3. Test Scope Loss on an imbalanced classification dataset (e.g., skewed MNIST) to confirm improved class-wise precision

## Open Questions the Paper Calls Out

### Open Question 1
How does Scope Loss perform in environments with continuous action spaces, beyond the discrete classification and Atari benchmarks tested? Basis: The paper demonstrates Scope Loss on discrete action spaces (classification, Atari games) but doesn't test continuous control environments. Why unresolved: The authors explicitly state "The classification environment can be generalized as the subset of MDPs with the following three key properties: 1. A discrete action space." This suggests they haven't tested Scope Loss on continuous action spaces. What evidence would resolve it: Empirical results showing Scope Loss performance on continuous control benchmarks like MuJoCo tasks or robotic manipulation problems.

### Open Question 2
What is the theoretical justification for why Scope Loss's single hyperparameter (α) requires no tuning across different environments? Basis: The authors state "Instead of manually scheduling and tuning α for each environment, we can instead fix α and employ z-score batch normalization of advantages. With this approach we can find a single optimal value for α for any environment and throughout all stages of training." Why unresolved: While the paper claims α requires no tuning, it doesn't provide a theoretical explanation for why this normalization approach makes α universally applicable. What evidence would resolve it: A formal proof or theoretical analysis showing that z-score normalization of advantages makes the Scope Loss hyperparameter invariant to environment-specific reward scales.

### Open Question 3
How does Scope Loss compare to state-of-the-art exploration methods like RND (Random Network Distillation) or curiosity-based approaches on sparse-reward environments? Basis: The authors compare Scope Loss to entropy-based exploration and Focal Loss, but don't test against modern curiosity-based exploration methods. Why unresolved: The paper states Scope Loss "indirectly promotes exploration by setting an upper limit on how quickly an agent can become exploitative" but doesn't benchmark against other exploration paradigms. What evidence would resolve it: Head-to-head comparisons on sparse-reward environments like Montezuma's Revenge or sparse-reward variants of standard benchmarks, measuring both sample efficiency and final performance.

## Limitations

- The equivalence between RL exploration and classification imbalance may not hold across all problem domains beyond the tested benchmarks
- The scope scaling mechanism introduces a hyperparameter (α) whose optimal value may vary significantly across different environments and tasks
- The paper does not explore edge cases where the proposed equivalence might break down

## Confidence

- High Confidence: The empirical results showing Scope Loss outperforming baselines on Caltech-256 classification (28.14% mean precision, 20% relative improvement) and RL tasks (higher scores, faster convergence)
- Medium Confidence: The theoretical mechanism linking exploration-exploitation trade-off to dataset imbalance; the claim that removing entropy terms doesn't compromise exploration
- Medium Confidence: The assertion that Scope Loss eliminates the need for hyperparameter tuning through advantage normalization, given that α remains a critical hyperparameter

## Next Checks

1. Test Scope Loss on a completely different domain (e.g., robotics control or natural language tasks) to verify the claimed equivalence between RL exploration and classification imbalance holds beyond the presented benchmarks.

2. Conduct an ablation study systematically varying α across a range of values on both RL and classification tasks to quantify the sensitivity to this hyperparameter and validate the normalization approach.

3. Compare Scope Loss against modern approaches specifically designed for long-tailed classification (e.g., LDAM, BBN) on datasets like ImageNet-LT to establish relative performance in the imbalanced classification literature.