---
ver: rpa2
title: A Comprehensive Survey on Multi-modal Conversational Emotion Recognition with
  Deep Learning
arxiv_id: '2312.05735'
source_url: https://arxiv.org/abs/2312.05735
tags:
- emotion
- recognition
- multi-modal
- information
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Multi-modal conversational emotion recognition (MCER) aims to identify
  emotions in text, speech, and visual data during conversations. This survey systematically
  reviews deep learning-based MCER methods, categorizing them into context-free modeling,
  sequential context modeling, distinguishing-speaker modeling, and speaker-relationship
  modeling.
---

# A Comprehensive Survey on Multi-modal Conversational Emotion Recognition with Deep Learning

## Quick Facts
- arXiv ID: 2312.05735
- Source URL: https://arxiv.org/abs/2312.05735
- Reference count: 40
- Multi-modal conversational emotion recognition achieves weighted F1 scores up to 68.9% on IEMOCAP dataset

## Executive Summary
This comprehensive survey systematically reviews deep learning-based methods for multi-modal conversational emotion recognition (MCER), which aims to identify emotions in text, speech, and visual data during conversations. The paper categorizes existing approaches into four main categories: context-free modeling, sequential context modeling, distinguishing-speaker modeling, and speaker-relationship modeling. It provides an overview of popular datasets, feature extraction techniques, evaluation metrics, and real-world applications. Experimental results demonstrate that speaker-relationship modeling methods achieve the best performance, with weighted F1 scores reaching 68.9% on the IEMOCAP dataset.

## Method Summary
The survey analyzes deep learning methods for MCER by categorizing them into four approaches based on how they handle context and speaker information. Context-free methods process each utterance independently without considering temporal or speaker relationships. Sequential context modeling methods use LSTMs or Transformers to capture emotional dependencies between utterances. Distinguishing-speaker methods use speaker-specific modules to model individual speaker states. Speaker-relationship modeling methods innovatively employ graph neural networks to capture structural dependencies between speakers. The survey evaluates these methods on standard datasets (IEMOCAP, MELD) using weighted F1 score as the primary metric.

## Key Results
- Speaker-relationship modeling methods achieve the highest performance with weighted F1 scores up to 68.9% on IEMOCAP
- Context-free methods achieve moderate performance (around 54-55 WF1 on IEMOCAP)
- Multi-modal fusion consistently outperforms single-modality approaches across all method categories
- Emotion recognition remains challenging due to data scarcity, heterogeneity, and class imbalance issues

## Why This Works (Mechanism)

### Mechanism 1: Multi-modal fusion
- Claim: Combining text, audio, and video modalities improves emotion recognition by leveraging complementary information
- Mechanism: Different modalities capture distinct emotional cues - text provides lexical semantics, audio captures prosody and tone, video captures facial expressions and gestures
- Core assumption: Emotional information is distributed across modalities and fusion provides more complete emotional understanding
- Evidence anchors: Abstract states MCER uses "text, speech, and visual information"; Section 3.3 explains how visual and audio information enhance understanding when text is insufficient; 25 related papers demonstrate active research

### Mechanism 2: Sequential context modeling
- Claim: Modeling temporal dependencies between utterances captures emotional evolution in conversations
- Mechanism: Emotions are contextually dependent and previous utterances influence interpretation of current emotional states
- Core assumption: Emotional states are not isolated but evolve through conversation context
- Evidence anchors: Abstract mentions "multi-modal context" and "dialogue scenarios"; Section 4.2 describes how approaches consider each sentence influenced by surrounding utterances; Section 4 details use of LSTMs/Transformers for contextual features

### Mechanism 3: Graph neural networks for speaker relationships
- Claim: Graph neural networks effectively model speaker interactions and structural dependencies in conversations
- Mechanism: Conversations involve complex speaker interactions where emotional states depend on both content and speaker relationship structure
- Core assumption: Speaker relationships contain important emotional information beyond content itself
- Evidence anchors: Abstract emphasizes "emotional interaction relationships"; Section 4.4 introduces graph neural networks for speaker dialogue relationships; Section 4.4.1 describes GCN for extracting structural information

## Foundational Learning

- **Concept: Multi-modal feature extraction techniques**
  - Why needed here: Different modalities require different extraction methods to capture unique characteristics
  - Quick check question: What are the three main modalities in MCER and what type of features does each typically capture?

- **Concept: Sequential modeling (LSTM/Transformer)**
  - Why needed here: Emotions evolve over time in conversations and require temporal dependency modeling
  - Quick check question: How do LSTMs handle long-term dependencies differently from traditional RNNs?

- **Concept: Graph neural networks and message passing**
  - Why needed here: Conversations have structural relationships between speakers that can be modeled as graphs
  - Quick check question: What is the fundamental difference between GCN and GAT in how they aggregate neighbor information?

## Architecture Onboarding

- **Component map**: Multi-modal feature extractors (text, audio, video) → Fusion module → Context modeling (LSTM/Transformer) → Speaker relationship modeling (GCN/GAT) → Emotion classifier
- **Critical path**: Feature extraction → Fusion → Context modeling → Classification
- **Design tradeoffs**: Early fusion vs. late fusion, depth of context modeling, graph complexity vs. performance
- **Failure signatures**: Poor performance on minority emotion classes, over-reliance on single modality, failure to capture speaker-specific patterns
- **First 3 experiments**:
  1. Test individual modality performance to identify which contributes most to emotion recognition
  2. Compare early fusion vs. late fusion approaches on the same dataset
  3. Evaluate the impact of context window size on emotion recognition accuracy

## Open Questions the Paper Calls Out

- **Open Question 1**: How can MCER models effectively handle scenarios where certain modalities are missing or corrupted?
  - Basis in paper: Section 8.4 discusses incomplete multi-modal conversation emotion recognition as a future challenge
  - Why unresolved: Current models assume all modalities are available with limited research on handling missing/corrupted data
  - What evidence would resolve it: Models demonstrating improved performance in scenarios with missing or corrupted modalities compared to models assuming all modalities are available

- **Open Question 2**: How can zero-shot learning techniques be applied to MCER to improve generalization to unseen emotion labels?
  - Basis in paper: Section 9.5 mentions zero-shot multi-modal conversation emotion recognition as a future direction
  - Why unresolved: Current models are trained and evaluated on specific emotion labels with limited research on generalizing to unseen labels
  - What evidence would resolve it: Models demonstrating effective emotion recognition for unseen emotion labels compared to models trained only on specific labels

- **Open Question 3**: How can multi-label emotion recognition be effectively implemented in MCER scenarios considering emotion ambiguity and complexity?
  - Basis in paper: Section 9.6 discusses multi-modal conversation multi-label emotion recognition as a future challenge
  - Why unresolved: Current models typically use single-label supervised learning which may not capture emotion complexity in real conversations
  - What evidence would resolve it: Models demonstrating effective recognition of multiple emotions in single utterances compared to single-label learning models

## Limitations

- The survey primarily focuses on deep learning approaches, potentially overlooking traditional machine learning methods that might be competitive in resource-constrained scenarios
- Limited discussion of computational efficiency and real-time implementation challenges for multi-modal systems
- Reported performance metrics are based on specific datasets and may not generalize to real-world applications with different data distributions

## Confidence

- **High confidence**: The categorization of MCER methods into four distinct approaches is well-founded and reflects the current state of the field
- **Medium confidence**: Performance claims are reasonable given the datasets and metrics used, but may vary significantly with different evaluation conditions
- **Medium confidence**: The identified challenges (data scarcity, heterogeneity, imbalance) are accurately described based on existing literature

## Next Checks

1. Replicate the performance evaluation using different dataset splits or cross-validation to verify the stability of reported WF1 scores
2. Test the sensitivity of speaker-relationship modeling methods to different graph construction approaches and parameter settings
3. Conduct ablation studies to quantify the contribution of each modality to overall performance in the proposed architectures