---
ver: rpa2
title: Blending gradient boosted trees and neural networks for point and probabilistic
  forecasting of hierarchical time series
arxiv_id: '2310.13029'
source_url: https://arxiv.org/abs/2310.13029
tags:
- sales
- float
- each
- days
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present a blending methodology for point and probabilistic
  forecasting of hierarchical time series, using gradient boosted trees and neural
  networks. They transform the task into a regression problem on daily sales, employ
  rich feature engineering, and carefully construct validation sets for model tuning.
---

# Blending gradient boosted trees and neural networks for point and probabilistic forecasting of hierarchical time series

## Quick Facts
- arXiv ID: 2310.13029
- Source URL: https://arxiv.org/abs/2310.13029
- Reference count: 29
- Primary result: Gold medal ranking in both Accuracy and Uncertainty tracks of M5 Competition

## Executive Summary
This paper presents a blending methodology for point and probabilistic forecasting of hierarchical time series using gradient boosted trees and neural networks. The approach transforms the forecasting task into a regression problem on daily sales, employs extensive feature engineering, and uses carefully constructed validation sets for model tuning. The methodology achieved gold medal rankings in both the Accuracy (weighted RMSSE of 0.549) and Uncertainty (Scaled Pinball Loss of 0.628) tracks of the M5 Competition, demonstrating the effectiveness of combining diverse machine learning models with metric-specific validation.

## Method Summary
The methodology involves transforming hierarchical time series forecasting into a regression problem on daily sales. The authors employ rich feature engineering including categorical encodings, price-related features, calendar-related features, and lag/rolling features. Three validation splits are constructed from the last 28 days of training data. The ensemble combines LightGBM models trained with Tweedie loss and neural network models trained with MSE/Tweedie loss. Final predictions are obtained through geometric averaging with level-specific weights, followed by exponential smoothing adjustments.

## Key Results
- Weighted RMSSE of 0.549 for point forecasting (Accuracy track)
- Scaled Pinball Loss of 0.628 for probabilistic forecasting (Uncertainty track)
- Gold medal ranking in both M5 Competition tracks
- Performance achieved without explicit hierarchical reconciliation procedures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Blending gradient boosted trees and neural networks with carefully selected validation sets improves both point and probabilistic forecasting accuracy.
- Mechanism: The diversity of model architectures captures different aspects of the data structure - GBDTs handle feature interactions well, neural networks extract complex patterns. Validation sets tuned on the competition metric prevent overfitting and ensure generalization.
- Core assumption: The combination of model diversity and metric-specific validation yields better performance than any single model or generic validation approach.
- Evidence anchors: Abstract states diversity of models and careful validation selection are key ingredients; corpus contains related probabilistic prediction papers but lacks specific validation methodology comparisons.
- Break condition: If validation sets don't match the actual evaluation distribution or if model diversity doesn't capture complementary patterns.

### Mechanism 2
- Claim: Using Tweedie loss function for intermittent sales data improves regression performance.
- Mechanism: Sales data has many zeros and is right-skewed. Tweedie loss combines properties of Poisson and Gamma distributions, better modeling this type of data than standard MSE.
- Core assumption: The sales distribution is approximately Tweedie-distributed with appropriate power parameter.
- Evidence anchors: Section notes target quantity is intermittent with ~68% zeros; histogram shows right-skewed distribution with concentration around zero; corpus mentions Tweedie in one related paper but no direct comparison studies.
- Break condition: If sales data becomes less intermittent or follows different distribution characteristics.

### Mechanism 3
- Claim: Hierarchical structure can be implicitly handled through weighted RMSSE objective rather than explicit reconciliation.
- Mechanism: The weighted RMSSE aggregates errors across all levels, naturally incorporating hierarchical relationships in the loss function without explicit reconciliation steps.
- Core assumption: The competition's weighted RMSSE metric sufficiently captures hierarchical relationships for practical forecasting purposes.
- Evidence anchors: Methodology achieved gold medal range without explicit reconciliation; authors note hierarchical scheme was not exploited explicitly; corpus has hierarchical forecasting papers but none specifically comparing implicit vs explicit approaches.
- Break condition: If evaluation metric changes to focus on specific hierarchical levels or if hierarchical patterns become more complex.

## Foundational Learning

- Concept: Time series cross-validation with temporal splits
  - Why needed here: Standard k-fold cross-validation would leak future information into training data, violating temporal ordering constraints.
  - Quick check question: Why can't we use random splits in time series forecasting?

- Concept: Feature engineering for retail forecasting
  - Why needed here: Raw data (dates, prices, IDs) needs transformation into predictive features like lagged values, rolling statistics, and calendar effects.
  - Quick check question: What's the difference between lag features and rolling statistics?

- Concept: Model ensembling through weighted averaging
  - Why needed here: Individual models have different strengths/weaknesses; combining them reduces variance and improves robustness.
  - Quick check question: Why use geometric averaging instead of arithmetic averaging for model blending?

## Architecture Onboarding

- Component map: Feature engineering pipeline → Model training with three validation splits → Ensemble blending → Post-processing → Submission generation
- Critical path: Feature engineering → Model training with validation splits → Ensemble blending → Post-processing → Submission generation
- Design tradeoffs: Model diversity vs training complexity (more models provide better coverage but increase computational cost); validation split selection vs generalization (specific splits improve metric optimization but may overfit to validation distribution); explicit hierarchy vs implicit weighting (avoids reconciliation complexity but may miss fine-grained hierarchical patterns)
- Failure signatures: Poor performance on specific hierarchical levels (10-12) indicates feature engineering gaps; validation/train gap suggests overfitting or poor validation set selection; single model dominance suggests insufficient diversity
- First 3 experiments: 1) Compare MSE vs Tweedie loss on validation split 1 to verify improvement on intermittent data; 2) Test different validation split configurations (e.g., using all three vs single split) to measure impact on generalization; 3) Evaluate individual model performance vs ensemble to quantify diversity benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would incorporating hierarchical reconciliation methods explicitly impact forecasting accuracy compared to the implicit approach used?
- Basis in paper: The paper states that although the data had an inherent hierarchy structure (12 levels), none of the proposed solutions exploited that hierarchical scheme explicitly.
- Why unresolved: The authors chose not to use hierarchical reconciliation methods, focusing instead on implicit consideration through the objective function. No comparative analysis was done.
- What evidence would resolve it: Implementing hierarchical reconciliation methods (e.g., bottom-up, top-down, middle-out approaches) and comparing their performance against the implicit approach used in this paper.

### Open Question 2
- Question: Would using more validation splits improve the model's performance and eliminate the need for external adjustments like the "magic multiplier"?
- Basis in paper: The authors suggest that using more validation splits could be beneficial and would even eliminate the need for magic external multipliers (0.97) to reach the top place.
- Why unresolved: The authors only used three validation splits due to time constraints, leaving the potential benefits of more splits unexplored.
- What evidence would resolve it: Implementing additional validation splits and comparing the model's performance with and without external adjustments.

### Open Question 3
- Question: How would the model's performance change if different loss functions were used for the intermittent target variable (sales count)?
- Basis in paper: The authors used a Tweedie loss function for LightGBM models due to the intermittent nature of the target variable, but suggest that using different loss functions could be explored.
- Why unresolved: The authors only experimented with the Tweedie loss function and did not compare it with other potential loss functions for intermittent data.
- What evidence would resolve it: Implementing and comparing different loss functions (e.g., quantile loss, zero-adjusted loss) for intermittent data and evaluating their impact on model performance.

## Limitations

- The methodology's success heavily depends on the specific validation split configuration and ensemble weights, which were tuned for the M5 competition rather than being theoretically optimal.
- The implicit handling of hierarchical structure through weighted RMSSE may not generalize to datasets with different hierarchical patterns or evaluation metrics.
- The feature engineering pipeline, while extensive, relies on domain-specific knowledge that may not transfer to other retail forecasting scenarios.

## Confidence

- Blending approach effectiveness: High - Strong empirical evidence from competition results and theoretical support for model diversity
- Tweedie loss superiority: Medium - Supported by data distribution analysis but limited comparative studies in corpus
- Implicit hierarchy handling: Medium - Practical success shown but no systematic comparison with explicit reconciliation methods
- Validation split methodology: Medium - Competition results support approach but overfitting to validation distribution remains a concern

## Next Checks

1. Cross-dataset validation: Apply the methodology to a different hierarchical forecasting dataset (e.g., M4 or tourism dataset) to test generalization beyond M5 competition data
2. Explicit vs implicit hierarchy comparison: Implement an explicit reconciliation step and compare performance against the implicit approach on the same validation splits
3. Loss function ablation study: Systematically compare Tweedie loss against MSE and other distributions (Poisson, Gamma) using identical feature engineering and validation methodology to isolate the impact of loss function choice