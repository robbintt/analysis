---
ver: rpa2
title: A Recipe for Improved Certifiable Robustness
arxiv_id: '2310.02513'
source_url: https://arxiv.org/abs/2310.02513
tags:
- lipschitz
- data
- robustness
- layers
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of improving certifiable robustness
  of neural networks using Lipschitz-based methods. The key insight is that robustness
  requires greater network capacity and more data than standard training, but effectively
  adding capacity under stringent Lipschitz constraints has proven difficult.
---

# A Recipe for Improved Certifiable Robustness

## Quick Facts
- arXiv ID: 2310.02513
- Source URL: https://arxiv.org/abs/2310.02513
- Reference count: 8
- Primary result: Achieves up to 8.5 percentage points improvement in verified robust accuracy (VRA) compared to state-of-the-art Lipschitz-based certification methods.

## Executive Summary
This work addresses the challenge of improving certifiable robustness of neural networks using Lipschitz-based methods. The authors identify that robustness requires greater network capacity and more data than standard training, but effectively adding capacity under stringent Lipschitz constraints has proven difficult. They propose a combination of novel techniques, design optimizations, and synthesis of prior work to significantly improve the state-of-the-art verified robust accuracy for deterministic certification. Specifically, they introduce "Cholesky-orthogonalized residual dense" layers to increase network capacity and performance, and use filtered generative data augmentation. The proposed approach achieves up to 8.5 percentage points improvement in VRA compared to the state-of-the-art on benchmark datasets.

## Method Summary
The method combines three key innovations: (1) adding large "Cholesky-orthogonalized residual dense" layers after the network neck to increase model capacity while maintaining Lipschitz constraints, (2) using filtered generative data augmentation to improve robustness by reducing the proportion of hard-to-classify samples, and (3) optimizing the training procedure with various Lipschitz control mechanisms. The dense layers are designed to be 1-Lipschitz through Cholesky-based orthogonalization, which is computationally efficient compared to alternatives. The generative data augmentation uses diffusion models to create synthetic training samples, which are filtered based on classification confidence to remove hard examples.

## Key Results
- Achieves up to 8.5 percentage points improvement in verified robust accuracy (VRA) compared to state-of-the-art on CIFAR-10/100, TinyImageNet, and ImageNet datasets.
- Demonstrates that adding dense layers after the network neck significantly increases model capacity under Lipschitz constraints without overfitting.
- Shows that filtered generative data augmentation improves certification robustness by reducing the proportion of hard-to-classify samples.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding large dense layers after the network neck significantly increases model capacity under Lipschitz constraints.
- Mechanism: Dense layers can introduce millions of parameters with low computational cost. Under Lipschitz-based training, the smoothness constraint prevents overfitting, allowing these layers to improve certification robustness effectively.
- Core assumption: The Lipschitz constant constraint limits overfitting risk even with large dense layers.
- Evidence anchors:
  - [abstract] "we discover that the addition of large 'Cholesky-orthogonalized residual dense' layers to the end of existing state-of-the-art Lipschitz-controlled ResNet architectures is especially effective for increasing network capacity and performance."
  - [section] "Dense Layers. Another solution is to add large fully connected (i.e., dense) layers after the neck...Adding dense layers to the model can increase the model capacity significantly...In Lipschitz based training, the smoothness of the model is controlled by the network Lipschitz constant thus large dense layers can improve the certification robustness effectively."
- Break condition: If the Lipschitz constraint becomes too loose, dense layers may overfit and harm certification.

### Mechanism 2
- Claim: Cholesky-based orthogonalization provides an efficient and numerically stable method for maintaining 1-Lipschitz dense layers.
- Mechanism: For a non-singular matrix V, the Cholesky-based orthogonalization solves the triangular system using the Cholesky decomposition of V V⊤, resulting in an orthogonal matrix. This process is faster and more stable than alternatives like Cayley transformation.
- Core assumption: Cholesky-based orthogonalization produces orthogonal matrices that maintain the 1-Lipschitz property while being computationally efficient.
- Evidence anchors:
  - [abstract] "Combined with filtered generative data augmentation, our final results further the state of the art deterministic VRA by up to 8.5 percentage points."
  - [section] "Cholesky-Orthogonalized Residual Layer...Cholesky-based orthogonalization is typically twice as fast as Cayley transformation to obtain an orthogonal matrix."
- Break condition: If the matrix V is singular or ill-conditioned, Cholesky decomposition may fail or produce inaccurate results.

### Mechanism 3
- Claim: Using filtered generative data augmentation improves certification robustness by reducing the proportion of hard-to-classify samples.
- Mechanism: Generated samples are easier to classify on average. By filtering out low-confidence pseudo-labels, the training set contains fewer hard samples, allowing the model to focus on learning a robust decision boundary rather than memorizing outliers.
- Core assumption: Hard samples (outliers and samples near decision boundaries) hinder robustness learning more than they help generalization in Lipschitz-based training.
- Evidence anchors:
  - [abstract] "Combined with filtered generative data augmentation, our final results further the state of the art deterministic VRA by up to 8.5 percentage points."
  - [section] "We find that seeing more generated samples can significantly improve the model's certification robustness...the reason the generated data helps is not that the generated data is of better quality, but generated data are easier to classify on average."
- Break condition: If the generative model produces low-quality samples or the filtering threshold is too aggressive, useful training data may be lost.

## Foundational Learning

- Concept: Lipschitz continuity and its role in certified robustness
  - Why needed here: The paper's entire approach relies on controlling the Lipschitz constant to provide deterministic robustness certificates. Understanding how Lipschitz bounds relate to perturbation sensitivity is crucial.
  - Quick check question: If a function has Lipschitz constant L, what is the maximum change in output for an input perturbation of size ε?

- Concept: Orthogonal matrices and their Lipschitz properties
  - Why needed here: Many Lipschitz control methods in the paper use orthogonal matrices to ensure 1-Lipschitz layers. Understanding why orthogonal matrices have this property is essential.
  - Quick check question: Why does multiplying by an orthogonal matrix preserve distances and angles, making it 1-Lipschitz?

- Concept: Generative models and data augmentation strategies
  - Why needed here: The paper uses diffusion models to generate synthetic training data. Understanding how these models work and how generated data can improve training is important for implementing the approach.
  - Quick check question: How does filtering generated samples by classification confidence potentially improve robustness training compared to using all generated samples?

## Architecture Onboarding

- Component map: Stem layer (convolution) -> Backbone (residual convolutional blocks) -> Neck (convolutional layers) -> Classification head
- Critical path: The most important components for achieving the results are the Cholesky-orthogonalized residual dense layers and the filtered generative data augmentation pipeline. These are the primary sources of the 8.5 percentage point improvement.
- Design tradeoffs: Adding dense layers increases capacity but may slow training. Cholesky-based orthogonalization is faster than alternatives but requires careful implementation. Using more generated data helps robustness but may hurt clean accuracy if the ratio is too high.
- Failure signatures: If the Lipschitz constraint is too loose, dense layers may overfit. If Cholesky decomposition fails due to ill-conditioned matrices, training may become unstable. If the generative model produces low-quality samples or filtering is too aggressive, useful training data may be lost.
- First 3 experiments:
  1. Implement the Cholesky-orthogonalized residual dense layer and verify it maintains the 1-Lipschitz property through numerical testing.
  2. Compare the training speed and numerical stability of Cholesky-based orthogonalization against Cayley transformation on small dense layers.
  3. Implement the filtered generative data augmentation pipeline and measure its impact on training dynamics and final certification accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between real and generated data in the training batch to maximize certified robustness?
- Basis in paper: [explicit] The authors mention that using only generated data leads to overfitting and decreased performance, while using a 1:3 ratio of real to generated samples improves certified robustness.
- Why unresolved: The optimal ratio may depend on the specific dataset and task, and further experimentation is needed to determine the best balance.
- What evidence would resolve it: Conducting experiments with different real/generated sample ratios on various datasets to find the optimal balance that maximizes certified robustness.

### Open Question 2
- Question: How does the performance of Lipschitz-based certification methods compare to randomized smoothing methods on larger and more diverse datasets?
- Basis in paper: [explicit] The authors compare their method to randomized smoothing on CIFAR-10, but note that most randomized smoothing methods only report results on a subset of ImageNet due to computational constraints.
- Why unresolved: Larger and more diverse datasets may reveal limitations or advantages of one method over the other that are not apparent on smaller datasets.
- What evidence would resolve it: Performing extensive experiments comparing Lipschitz-based and randomized smoothing methods on larger and more diverse datasets, such as full ImageNet or other large-scale datasets.

### Open Question 3
- Question: What is the impact of different Lipschitz control mechanisms on the final performance of Lipschitz-based certification methods?
- Basis in paper: [explicit] The authors compare various Lipschitz control mechanisms, including Cayley transformation, matrix exponential, and Cholesky-based orthogonalization, and find that Cholesky-based orthogonalization performs best.
- Why unresolved: Different Lipschitz control mechanisms may have varying impacts on performance depending on the specific architecture and task, and further exploration is needed to determine the optimal choice.
- What evidence would resolve it: Conducting experiments using different Lipschitz control mechanisms on various architectures and tasks to determine the optimal choice for each scenario.

## Limitations
- The approach depends on generated data which may not fully capture the true data distribution.
- The filtered augmentation approach assumes hard-to-classify samples are detrimental to robustness training.
- Cholesky-orthogonalization requires positive-definite matrices and may face numerical stability issues.

## Confidence
- **High confidence**: The core finding that adding dense layers after the network neck improves VRA under Lipschitz constraints is well-supported by empirical results and theoretical justification. The claim that Cholesky-based orthogonalization is faster than alternatives is also well-established.
- **Medium confidence**: The claim that filtered generative data augmentation specifically improves robustness (rather than just accuracy) is supported by experiments but the underlying mechanism could benefit from deeper analysis. The assertion that improvements are due to reduced hard samples rather than better data quality needs further validation.
- **Low confidence**: The scalability of this approach to much larger models or different architectural families (beyond ResNets) is not thoroughly explored.

## Next Checks
1. **Ablation study on filtering threshold**: Systematically vary the confidence threshold for filtering generated samples to quantify the tradeoff between data quality and quantity on final VRA.
2. **Generalization across architectures**: Test the Cholesky-orthogonalized dense layer approach on Vision Transformers and other non-ResNet architectures to assess architectural generality.
3. **Sensitivity to data distribution shift**: Evaluate the certified robustness when the test-time distribution differs from both training and generated data to assess robustness to distribution shift.