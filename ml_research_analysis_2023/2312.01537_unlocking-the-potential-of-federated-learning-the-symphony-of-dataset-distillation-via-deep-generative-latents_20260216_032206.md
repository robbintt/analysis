---
ver: rpa2
title: 'Unlocking the Potential of Federated Learning: The Symphony of Dataset Distillation
  via Deep Generative Latents'
arxiv_id: '2312.01537'
source_url: https://arxiv.org/abs/2312.01537
tags:
- data
- global
- training
- local
- server
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FedDGM, a server-side federated learning framework
  that leverages pre-trained deep generative models for efficient and privacy-enhanced
  training. Unlike existing approaches that perform dataset distillation on clients
  and upload synthetic data, FedDGM enables the server to synthesize distilled data
  representations via matching training trajectories of local surrogate models.
---

# Unlocking the Potential of Federated Learning: The Symphony of Dataset Distillation via Deep Generative Latents

## Quick Facts
- arXiv ID: 2312.01537
- Source URL: https://arxiv.org/abs/2312.01537
- Reference count: 40
- Up to 40% accuracy improvement over non-dataset-distillation techniques and 18% over existing dataset-distillation methods in highly heterogeneous FL contexts

## Executive Summary
FedDGM introduces a server-side federated learning framework that leverages pre-trained deep generative models for efficient and privacy-enhanced training. Unlike client-side dataset distillation approaches, FedDGM enables the server to synthesize distilled data representations by matching training trajectories of local surrogate models. This approach reduces computational costs on clients, enhances privacy by avoiding data uploads, and theoretically achieves asymptotic equivalence to centralized training on heterogeneous datasets. Empirical results demonstrate significant accuracy improvements and faster convergence across multiple datasets and architectures.

## Method Summary
FedDGM operates by having clients train small local surrogate models on their data and send only the model parameters to the server. The server then uses a pre-trained StyleGAN-XL to generate synthetic data in latent space and applies Matching Training Trajectories (MTT) to distill this data by minimizing the distance between the synthetic data's training trajectory and the clients' local trajectories. The server aggregates these distilled datasets and trains both the global model and a global surrogate model for the next round. This approach avoids the information loss inherent in gradient or parameter averaging while maintaining client privacy.

## Key Results
- Up to 40% accuracy improvement over non-dataset-distillation techniques
- 18% improvement over existing dataset-distillation methods in highly heterogeneous FL contexts
- Around 10% performance increase on high-resolution image datasets
- Faster convergence compared to state-of-the-art federated learning methods

## Why This Works (Mechanism)

### Mechanism 1
FedDGM achieves asymptotic equivalence to centralized training by iteratively matching training trajectories between local surrogate models and synthetic data distillation. The server uses MTT to distill synthetic data for each client by training a student network on synthetic data and minimizing the normalized L2 distance between the student and the client's trained surrogate model. This process, repeated over multiple iterations, creates a synthetic dataset that approximates the ideal multimodal distribution formed by aggregating all heterogeneous client data. The core assumption is that over-parametrized neural networks have connected zero-loss regions, and SGD iterates for the aggregated synthetic dataset problem are equivalent to a linear combination of the client-specific optimizations.

### Mechanism 2
FedDGM improves convergence speed and accuracy by training the global model directly on synthetic data rather than aggregating gradients or parameters from clients. Instead of the server averaging gradients or parameters from heterogeneous client models (as in FedAvg), FedDGM has the server synthesize a unified dataset that captures the multimodal distribution of all client data. The global model is then trained on this synthetic dataset, which preserves more information about the local data distributions and avoids the averaging that can dilute important features. The core assumption is that synthetic data generated through MTT and optimized in the latent space of a pre-trained generative model can effectively represent the underlying data distributions of heterogeneous clients.

### Mechanism 3
Using pre-trained deep generative models in latent space optimization enhances the generalization of synthetic data to different global model architectures. FedDGM leverages a pre-trained StyleGAN-XL to generate synthetic data in its latent space (e.g., F5 or F12 space). Optimizing in this latent space rather than pixel space allows the synthetic data to better generalize across different model architectures because the generative model has learned rich representations that transfer well. The core assumption is that the latent space of a pre-trained generative model contains meaningful semantic representations that can be adapted to represent diverse data distributions through optimization.

## Foundational Learning

- Concept: Federated Learning fundamentals (local training, global aggregation, non-IID data challenges)
  - Why needed here: Understanding why standard FedAvg struggles with heterogeneous data and how FedDGM addresses these limitations
  - Quick check question: Why does averaging model parameters from heterogeneous clients lead to suboptimal performance?

- Concept: Dataset distillation techniques (matching training trajectories, gradient matching, distribution matching)
  - Why needed here: FedDGM builds on dataset distillation but performs it server-side using MTT rather than at the client level
  - Quick check question: How does Matching Training Trajectories differ from gradient matching in dataset distillation?

- Concept: Generative models and latent space optimization (StyleGAN, latent spaces, semantic representations)
  - Why needed here: FedDGM uses a pre-trained StyleGAN-XL to optimize synthetic data in latent space for better generalization
  - Quick check question: Why might optimizing in latent space lead to better cross-architecture generalization than pixel space optimization?

## Architecture Onboarding

- Component map:
  Clients -> Local surrogate model training -> Model parameter upload to server
  Server -> Pre-trained generative model + MTT algorithm -> Synthetic data distillation
  Server -> Global model training on distilled data -> Global surrogate model training
  Server -> Global surrogate model parameters -> Clients for next round

- Critical path:
  1. Server initializes global model and global surrogate model
  2. Server sends global surrogate model to selected clients
  3. Clients train local surrogate models on their data and send parameters back
  4. Server initializes latent vectors and generates synthetic data for each client
  5. Server performs MTT to update latent vectors and distill synthetic data
  6. Server aggregates all synthetic data into a unified dataset
  7. Server trains global model and global surrogate model on synthetic data
  8. Server sends global surrogate model to clients for next round

- Design tradeoffs:
  - Smaller local surrogate models reduce client computational cost but may limit information transfer
  - More MTT iterations improve synthetic data quality but increase server computation
  - Different latent spaces in StyleGAN-XL offer varying levels of realism vs. expressiveness
  - Higher IPC values improve synthetic data quality but increase server storage and computation

- Failure signatures:
  - Convergence stalls or accuracy plateaus: MTT is not effectively matching training trajectories
  - Poor generalization to new architectures: Latent space optimization is not capturing necessary features
  - Communication overhead increases: Local surrogate models are too large or too many clients are selected
  - Server computation becomes bottleneck: Too many MTT iterations or synthetic data generation steps

- First 3 experiments:
  1. CIFAR-10 with ConvNet global model, 10 clients, α=0.5: Verify basic functionality and compare against FedAvg baseline
  2. CIFAR-10 with different global architectures (ResNet18, VGG11): Test cross-architecture generalization claims
  3. ImageFruit subset with varying latent spaces (F0, F5, F9): Validate the impact of latent space choice on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of latent space (F0, F3, F4, F5, F6, F9) affect the quality and generalization of distilled synthetic data across different global model architectures?
- Basis in paper: The paper discusses the impact of different latent spaces on the quality of distilled data and global model performance.
- Why unresolved: While the paper mentions the investigation of different latent spaces and their impact, it does not provide a detailed analysis of the relationship between the choice of latent space and the quality of distilled data across different global model architectures.
- What evidence would resolve it: Detailed empirical results showing the performance of different global model architectures when trained on synthetic data distilled from each latent space.

### Open Question 2
- Question: What is the optimal number of local training epochs (Tl) for clients to achieve the best performance of the global model in FedDGM?
- Basis in paper: The paper mentions the investigation of the impact of different local training epochs on the performance of the global model.
- Why unresolved: The paper indicates that the performance is best when the number of local epochs is set to 20, but it does not provide a comprehensive analysis of the optimal number of local training epochs for different scenarios or architectures.
- What evidence would resolve it: Extensive experiments testing the performance of the global model with varying numbers of local training epochs across different architectures and non-i.i.d. scenarios.

### Open Question 3
- Question: How does the architecture of the local surrogate model affect the cross-architecture performance of FedDGM?
- Basis in paper: The paper discusses the impact of different architectures of the local surrogate model on the performance of FedDGM.
- Why unresolved: While the paper mentions that FedDGM consistently outperforms FedDM across different local surrogate model architectures, it does not provide a detailed analysis of how the complexity or size of the local surrogate model affects the performance of the global model.
- What evidence would resolve it: Detailed empirical results showing the performance of the global model with different local surrogate model architectures, including varying complexity and size.

## Limitations
- Theoretical analysis relies on strong assumptions about network over-parameterization and connected zero-loss regions
- Experimental evaluation limited to image classification tasks with relatively small client counts (10 clients)
- Limited testing of scalability in larger, more diverse federated learning scenarios

## Confidence
- Asymptotic equivalence claim: Medium - Theoretical derivation is sound but depends on idealized conditions
- Performance improvement claims: High - Well-supported by comprehensive experimental results across multiple datasets and architectures
- Cross-architecture generalization claim: Medium - Supported by experiments but limited to specific architecture families

## Next Checks
1. Test FedDGM with 100+ clients to evaluate scalability and convergence behavior in large-scale federated settings
2. Implement robustness tests with varying degrees of data heterogeneity (α values from 0.1 to 1.0) to validate performance claims across the full spectrum of non-IID scenarios
3. Conduct ablation studies isolating the impact of pre-trained generative model choice and latent space selection on synthetic data quality and final model performance