---
ver: rpa2
title: Evaluating Large Language Models at Evaluating Instruction Following
arxiv_id: '2310.07641'
source_url: https://arxiv.org/abs/2310.07641
tags:
- output
- instruction
- evaluators
- arxiv
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLMBar, a meta-evaluation benchmark designed
  to test whether large language model (LLM) evaluators can detect instruction-following
  outputs. The authors manually curated 419 instances, each consisting of an instruction
  paired with two outputs, one adhering to instructions while the other diverging
  but potentially possessing deceptive qualities.
---

# Evaluating Large Language Models at Evaluating Instruction Following

## Quick Facts
- arXiv ID: 2310.07641
- Source URL: https://arxiv.org/abs/2310.07641
- Reference count: 40
- Primary result: LLMBar benchmark reveals significant gaps in LLM evaluator performance on instruction-following tasks

## Executive Summary
This paper introduces LLMBar, a meta-evaluation benchmark designed to test whether large language model evaluators can accurately detect instruction-following outputs. The authors manually curated 419 instances where one output correctly follows the instruction while another deviates but may appear superficially appealing. They evaluate five different LLMs using various prompting strategies and find that even the best evaluators have substantial room for improvement, particularly on adversarial examples. The work demonstrates that strategic prompt engineering can significantly improve evaluator performance, with the best strategy achieving a 10% boost for GPT-4-based evaluators on the adversarial set.

## Method Summary
The paper creates LLMBar by curating instruction-output pairs with objective preferences, consisting of 100 natural instances from existing benchmarks and 319 adversarial instances designed to challenge evaluators. Five base LLMs (GPT-4, ChatGPT, LLaMA-2-70B-Chat, PaLM2, Falcon-180B-Chat) are evaluated using various prompting strategies including Vanilla, Chain-of-Thoughts, Rules, Self-Generated Metrics, Reference, and Swap. Evaluator performance is measured by average accuracy and positional agreement rate across the benchmark instances.

## Key Results
- LLM evaluators achieve significantly lower accuracy on LLMBar compared to human agreement (94%)
- Different prompting strategies show varying effectiveness, with Rules+Metrics+Reference consistently improving performance
- GPT-4-based evaluators outperform other base models but still struggle with adversarial instances
- Positional bias is observed in evaluator preferences, indicating consistency issues
- The best prompting strategy combination achieves approximately 10% improvement on the adversarial set for GPT-4-based evaluators

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM evaluators fail on adversarial examples due to superficial qualities overriding instruction-following.
- Mechanism: When outputs deviate from instructions but adopt appealing superficial qualities (e.g., engaging tone, polished format), evaluators are biased toward these qualities rather than task compliance.
- Core assumption: Evaluators prioritize surface-level quality over semantic correctness when both are present.
- Evidence anchors:
  - [abstract] "The dispreferred output in LLMBar often adopts appealing superficial qualities that challenge LLM evaluators."
  - [section 2.2] "The adversarial set comprises adversarially crafted instances that tend to confound less adept evaluators."
  - [corpus] "Average neighbor FMR=0.454" (moderate correlation with related work on LLM evaluation challenges)

### Mechanism 2
- Claim: Different prompting strategies significantly alter evaluator performance on instruction-following tasks.
- Mechanism: Strategic prompt engineering (Rules, Metrics, Reference, Swap) explicitly guides evaluators to focus on instruction adherence, improving accuracy.
- Core assumption: LLM evaluators are sensitive to prompt framing and can be directed toward specific evaluation criteria.
- Evidence anchors:
  - [abstract] "We present a novel suite of prompting strategies that further close the gap between LLM and human evaluators."
  - [section 3] "We find that Rules improves the evaluator's accuracy almost universally."
  - [section 4.3] "Figure 4 demonstrates that a combination of Rules+Metrics+Reference consistently improves evaluator performance."

### Mechanism 3
- Claim: Meta-evaluation benchmarks with objective preferences reveal true evaluator capabilities better than subjective benchmarks.
- Mechanism: By curating instances with clear objective quality differences, LLMBar exposes limitations that subjective benchmarks mask.
- Core assumption: Subjective benchmarks conflate personal preference with task performance, leading to unreliable evaluator assessment.
- Evidence anchors:
  - [abstract] "LLMBAR focuses exclusively on the instruction-following quality and enforces objective preferences."
  - [section 4.4] "LLMBAR demonstrates a drastically different pattern of LLM evaluators from existing benchmarks."
  - [corpus] "Average neighbor citations=0.0" (limited prior work on objective meta-evaluation)

## Foundational Learning

- Concept: Instruction-following evaluation
  - Why needed here: The paper's core contribution is evaluating whether LLM evaluators can detect outputs that follow instructions correctly versus those that don't.
  - Quick check question: What distinguishes instruction-following from other evaluation metrics like fluency or coherence?

- Concept: Adversarial instance construction
  - Why needed here: The adversarial set is designed to challenge evaluators with outputs that appear high-quality but violate instructions.
  - Quick check question: How do you create an output that appears correct but actually fails to follow the instruction?

- Concept: Prompt engineering for evaluation tasks
  - Why needed here: Different prompting strategies (Rules, Metrics, Reference, Swap) significantly impact evaluator performance.
  - Quick check question: What elements should an evaluation prompt include to ensure focus on instruction-following?

## Architecture Onboarding

- Component map: LLMBar consists of (1) instruction-output pairs with objective preferences, (2) NATURAL and ADVERSARIAL subsets, (3) prompting strategies for evaluation
- Critical path: Curate high-quality instances → Design prompting strategies → Evaluate different LLM-base combinations → Analyze performance gaps
- Design tradeoffs: Manual curation ensures quality but limits scalability; adversarial filtering increases difficulty but may not cover all failure modes
- Failure signatures: Evaluators showing below-chance performance on adversarial sets, positional bias in preferences, inconsistency across prompting strategies
- First 3 experiments:
  1. Test baseline evaluator (Vanilla) on NATURAL set to establish performance floor
  2. Apply Rules prompting strategy and measure improvement on ADVERSARIAL set
  3. Compare GPT-4-based versus weaker LLM-based evaluators on lexical constraint instances

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLM evaluators on LLMBAR generalize to other instruction-following benchmarks or real-world applications?
- Basis in paper: [inferred] The paper evaluates LLM evaluators on LLMBAR and finds that they significantly underperform humans, especially on the ADVERSARIAL set. However, the paper does not compare LLMBAR to other instruction-following benchmarks or real-world applications.
- Why unresolved: The paper does not provide a direct comparison between LLMBAR and other instruction-following benchmarks or real-world applications. This comparison is needed to understand the generalizability of the findings on LLMBAR to other settings.
- What evidence would resolve it: A study that compares the performance of LLM evaluators on LLMBAR to their performance on other instruction-following benchmarks or real-world applications would help to understand the generalizability of the findings on LLMBAR.

### Open Question 2
- Question: How do the prompting strategies proposed in the paper affect the performance of LLM evaluators on other tasks beyond instruction following?
- Basis in paper: [explicit] The paper proposes a suite of novel prompting strategies (Rules, Metrics, and Swap) that significantly improve evaluator performance on LLMBAR. However, the paper does not evaluate these strategies on other tasks beyond instruction following.
- Why unresolved: The paper does not provide an evaluation of the proposed prompting strategies on tasks beyond instruction following. This evaluation is needed to understand the broader applicability of these strategies.
- What evidence would resolve it: A study that evaluates the proposed prompting strategies on tasks beyond instruction following would help to understand their broader applicability.

### Open Question 3
- Question: How does the performance of LLM evaluators on LLMBAR change as the size and capability of the base LLMs increase?
- Basis in paper: [inferred] The paper evaluates LLM evaluators on LLMBAR using five different base LLMs of varying sizes and capabilities. However, the paper does not provide a systematic analysis of how the performance of LLM evaluators changes as the size and capability of the base LLMs increase.
- Why unresolved: The paper does not provide a systematic analysis of how the performance of LLM evaluators changes as the size and capability of the base LLMs increase. This analysis is needed to understand the relationship between the base LLM's size and capability and the performance of LLM evaluators.
- What evidence would resolve it: A study that systematically evaluates the performance of LLM evaluators on LLMBAR using base LLMs of different sizes and capabilities would help to understand the relationship between the base LLM's size and capability and the performance of LLM evaluators.

## Limitations
- Manual curation process limits scalability and may introduce bias
- Focus on instruction-following tasks may not generalize to broader evaluation scenarios
- Adversarial instances may not cover all potential failure modes of LLM evaluators

## Confidence
- Core findings about evaluator limitations: High
- Generalizability to other tasks: Medium
- Prompting strategy effectiveness: Medium

## Next Checks
1. Test evaluator performance on a holdout set of instances with lexical constraints that were excluded from the main dataset
2. Evaluate the same prompting strategies on a different instruction-following task domain (e.g., code generation instead of text generation)
3. Conduct ablation studies removing individual components from the Rules+Metrics+Reference strategy to quantify their relative contributions