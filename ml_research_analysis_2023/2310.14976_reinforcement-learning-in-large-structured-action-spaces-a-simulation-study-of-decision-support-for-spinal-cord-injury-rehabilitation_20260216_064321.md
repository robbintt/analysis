---
ver: rpa2
title: 'Reinforcement learning in large, structured action spaces: A simulation study
  of decision support for spinal cord injury rehabilitation'
arxiv_id: '2310.14976'
source_url: https://arxiv.org/abs/2310.14976
tags:
- treatment
- treatments
- data
- agent
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of applying reinforcement learning
  to spinal cord injury rehabilitation, where limited training data and a large action
  space hinder traditional methods. The authors propose grouping treatments using
  domain knowledge or learned embeddings to enable effective learning from limited
  data.
---

# Reinforcement learning in large, structured action spaces: A simulation study of decision support for spinal cord injury rehabilitation

## Quick Facts
- arXiv ID: 2310.14976
- Source URL: https://arxiv.org/abs/2310.14976
- Reference count: 40
- Key outcome: Reinforcement learning agents using grouped actions outperform physiotherapists in simulated SCI rehabilitation, with domain-knowledge-based grouping performing best.

## Executive Summary
This study addresses the challenge of applying reinforcement learning to spinal cord injury rehabilitation, where limited training data and a large action space hinder traditional methods. The authors propose grouping treatments using domain knowledge or learned embeddings to enable effective learning from limited data. They simulate SCI rehabilitation and train agents using Fitted Q Iteration with grouped actions. Results show both methods improve treatment selection compared to physiotherapists, with domain knowledge-based grouping performing best. Combining agent recommendations with physiotherapist expertise further enhances outcomes, demonstrating reinforcement learning's potential in this domain.

## Method Summary
The authors simulate spinal cord injury rehabilitation using a custom simulator with 12 stages and 110 treatments grouped into 11 categories. They apply Fitted Q Iteration, an off-policy batch RL algorithm, to learn treatment selection policies from simulated patient data. Two treatment grouping approaches are used: domain knowledge-based grouping (DKBG) and treatment embedding-based grouping (TEBG) using GloVe embeddings. The grouped action approach reduces the effective action space size, allowing the agent to learn effectively from limited data. The learned policies are evaluated by simulating treatment selection and measuring patient outcomes.

## Key Results
- Both domain knowledge-based and treatment embedding-based grouping approaches improve treatment selection compared to physiotherapists.
- Domain knowledge-based grouping performs best among the tested approaches.
- Combining agent recommendations with physiotherapist expertise further enhances patient outcomes.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Grouping treatments into meaningful clusters reduces the effective action space size, lowering variance in state-action value estimates.
- **Mechanism:** By mapping individual treatments to predefined or learned groups, the agent learns to evaluate the benefit of a group rather than every possible individual action, which is especially valuable when training data is scarce.
- **Core assumption:** The similarity or relatedness of treatments within a group is meaningful and captures real clinical utility.
- **Evidence anchors:**
  - [abstract]: "treatments have natural groupings... propose two approaches to grouping treatments so that an RL agent can learn effectively from limited data."
  - [section 3.2]: "Treatments for SCIs have natural groupings... we propose two methods that place the treatments into groups."
  - [corpus]: Weak signal from the neighbor paper "Structured Reinforcement Learning for Combinatorial Decision-Making" supports grouping as a general RL approach.
- **Break condition:** If treatments in a group are not functionally similar, the grouping may increase bias without reducing variance enough to be beneficial.

### Mechanism 2
- **Claim:** Using embedding-based representation learning allows action grouping without requiring extensive domain expertise.
- **Mechanism:** Treatments are embedded into a vector space based on their co-occurrence in practice; clustering these embeddings identifies natural groupings based on usage patterns.
- **Core assumption:** The physiotherapist's choices reflect true treatment similarity, and co-selection patterns in data are meaningful indicators of relatedness.
- **Evidence anchors:**
  - [section 3.2.2]: "treatments that are commonly selected simultaneously are similar to one another" and use of GloVe embeddings to capture this.
  - [corpus]: The neighbor paper "Multi-State-Action Tokenisation in Decision Transformers for Multi-Discrete Action Spaces" uses embeddings for action representation.
- **Break condition:** If the treatment selection policy is highly variable or idiosyncratic, embeddings may group unrelated treatments together.

### Mechanism 3
- **Claim:** Fitted Q Iteration can converge with grouped actions when the number of parameters is reduced below the rank-deficiency threshold.
- **Mechanism:** By collapsing 110 actions into 11 groups, the state-action value regression model is reduced to a manageable size, allowing convergence without divergence.
- **Core assumption:** Reducing the action space complexity avoids the rank-deficiency and divergence issues seen with the full action space.
- **Evidence anchors:**
  - [section 3.2.5]: "Since our training data is obtained in advance... we used an off-policy batch learning algorithm, Fitted Q Iteration."
  - [Appendix 2]: Discusses why the model with 1211 coefficients is rank-deficient and why grouping fixes this.
  - [corpus]: The neighbor paper "Reinforcement learning with combinatorial actions for coupled restless bandits" discusses challenges of large action spaces in RL.
- **Break condition:** If the grouping is too coarse or the model remains overparameterized, convergence may still fail.

## Foundational Learning

- **Concept:** Markov Decision Processes (MDPs) formalize the RL problem with states, actions, rewards, and transition probabilities.
  - **Why needed here:** The SCI rehab problem is modeled as an MDP where states represent patient health, actions are treatments, and transitions depend on treatment effectiveness.
  - **Quick check question:** In the simulator, what defines the state of a patient? (Answer: Stage and number of weeks remaining)

- **Concept:** Fitted Q Iteration is an off-policy batch RL algorithm that learns the optimal Q-function from a fixed dataset.
  - **Why needed here:** Since we have only historical treatment data from physiotherapists, we cannot interact with the environment; we need an algorithm that learns from this static dataset.
  - **Quick check question:** What is the convergence criterion used in the Fitted Q Iteration algorithm? (Answer: Coefficients change by less than 0.0001 between iterations)

- **Concept:** Representation learning (e.g., word embeddings) can capture semantic similarity in high-dimensional discrete spaces.
  - **Why needed here:** The large number of treatments can be embedded into a continuous space where similar treatments are close, enabling clustering and reducing the action space complexity.
  - **Quick check question:** Which embedding technique is used to generate treatment vectors in the paper? (Answer: GloVe)

## Architecture Onboarding

- **Component map:**
  Simulator -> Treatment Grouping Module (DKBG/TEBG) -> Fitted Q Iteration Learner -> Policy Evaluation Module

- **Critical path:**
  1. Generate training data via simulator.
  2. Group treatments using DKBG or TEBG.
  3. Adjust data format so each row represents one action-group choice.
  4. Train Fitted Q Iteration model.
  5. Evaluate policy by simulating patient treatment.
  6. Compare outcomes to physiotherapist baseline.

- **Design tradeoffs:**
  - DKBG requires deep clinical expertise but likely yields better groupings.
  - TEBG is data-driven and scalable but may produce less optimal groupings.
  - Using grouped actions reduces variance but may introduce bias if groups are poorly defined.
  - Simpler action selection (picking one action at a time) is easier to model but ignores the multi-action reality of SCI rehab.

- **Failure signatures:**
  - Rank-deficient regression models in Fitted Q Iteration.
  - Overly optimistic Q-value estimates due to sparse data in late stages.
  - Poor treatment rankings when group definitions are incorrect.
  - Divergence in Fitted Q Iteration when action space is too large relative to data.

- **First 3 experiments:**
  1. Train and evaluate DKBG agent with 1000 simulated patients and compare average return to physiotherapist baseline.
  2. Repeat with TEBG agent to compare performance and transition probabilities per stage.
  3. Vary training set size (100, 500, 1000 patients) with DKBG agent to see impact on policy effectiveness and identify the smallest dataset that yields meaningful improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do treatment interactions and delayed treatment effects influence the effectiveness of grouped action reinforcement learning approaches in SCI rehabilitation?
- **Basis in paper:** [inferred] The authors mention that the simulator does not include interactions between treatments or delayed treatment effects, which they believe exist in practice.
- **Why unresolved:** The current simulation framework does not account for these complexities, limiting the generalizability of the findings to real-world scenarios.
- **What evidence would resolve it:** Empirical data demonstrating treatment interactions and delayed effects in SCI rehabilitation, along with simulations incorporating these factors, would clarify their impact on RL performance.

### Open Question 2
- **Question:** What is the optimal balance between the agent's opinion and a physiotherapist's expertise when selecting treatments in practice?
- **Basis in paper:** [explicit] The authors note that balancing the agent's opinion with a physiotherapist's expertise is a challenge for practical implementation.
- **Why unresolved:** The study used a simulated environment and a predefined weighting scheme, but real-world scenarios involve more nuanced decision-making.
- **What evidence would resolve it:** Clinical trials comparing different weighting schemes in practice, along with feedback from physiotherapists, would help determine the optimal balance.

### Open Question 3
- **Question:** How does the heterogeneity in physiotherapists' treatment choices affect the effectiveness of the proposed RL methods?
- **Basis in paper:** [explicit] The authors acknowledge that the heterogeneity in treatment choices observed in their simulation is likely higher than in practice, which could impact the data the agent has to learn from.
- **Why unresolved:** The simulation assumes independence among physiotherapists' treatment rankings, which may not reflect real-world variability.
- **What evidence would resolve it:** Analysis of real-world treatment data showing the correlation among physiotherapists' choices, along with sensitivity analyses in the simulation, would clarify the impact of heterogeneity.

## Limitations
- The simulator parameters are not fully specified, making it difficult to assess whether the benefits and transition probabilities used are realistic.
- The DKBG groupings are described but not detailed, preventing replication of the exact groupings.
- The success of TEBG depends heavily on the quality of the GloVe embeddings and K-means clustering, which may not capture meaningful treatment similarity.

## Confidence
- **High**: Fitted Q Iteration can improve over physiotherapists when action space is reduced via grouping, as shown by simulation results and supported by the rank-deficiency argument in Appendix 2.
- **Medium**: The effectiveness of domain knowledge-based grouping over learned embeddings, since the paper reports DKBG performs best but does not explain why in detail.
- **Low**: The clinical relevance of simulator parameters and groupings, as the simulator is not validated against real SCI rehabilitation data.

## Next Checks
1. **Sensitivity analysis**: Test the impact of different group sizes (e.g., 5, 11, 20) on Fitted Q Iteration performance to find the optimal grouping granularity.
2. **Embeddings robustness**: Evaluate the stability of TEBG groupings across different random seeds and clustering initializations to ensure consistency.
3. **Real-world data integration**: Apply the grouping and Fitted Q Iteration approach to a small real SCI rehabilitation dataset to assess generalization beyond the simulator.