---
ver: rpa2
title: Multimodal Data and Resource Efficient Device-Directed Speech Detection with
  Large Foundation Models
arxiv_id: '2312.03632'
source_url: https://arxiv.org/abs/2312.03632
tags:
- audio
- multi
- data
- representations
- whisper
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses device-directed speech detection by determining
  if a user is addressing a virtual assistant based on streaming audio. The method
  combines 1-best ASR hypotheses, decoder signals, and acoustic features as input
  to a frozen LLM using prefix tuning and LoRA for training.
---

# Multimodal Data and Resource Efficient Device-Directed Speech Detection with Large Foundation Models

## Quick Facts
- arXiv ID: 2312.03636
- Source URL: https://arxiv.org/abs/2312.03636
- Reference count: 40
- Primary result: Multimodal approach combining ASR hypotheses, decoder signals, and acoustic features with frozen LLM and LoRA achieves lower EER than unimodal baselines while using less training data

## Executive Summary
This paper presents a multimodal approach for device-directed speech detection that determines whether a user is addressing a virtual assistant based on streaming audio. The system combines 1-best ASR hypotheses, decoder signals (acoustic cost, graph cost, confidence scores), and acoustic features as input to a frozen LLM using prefix tuning and LoRA for training. The method achieves lower equal-error-rates than unimodal baselines while requiring only a fraction of the training data. Additionally, low-dimensional specialized audio representations lead to lower EERs than high-dimensional general representations.

## Method Summary
The approach uses a frozen decoder-only LLM (Falcon 7B or RedPajama 7B) with LoRA adapters to perform device-directed speech detection. Input features include 1-best ASR hypotheses, decoder signals (acoustic cost, graph cost, word-level posterior confidence scores, alternative word options), and acoustic features extracted from either Whisper (1024-dim) or UAD (256-dim) audio encoders. Two feedforward mapping networks align these features to the LLM's token embedding space. The model is trained on ~80k examples using prefix tuning and LoRA optimization with cross-entropy loss, achieving EERs of 3.42% in the best multimodal configuration.

## Key Results
- Multimodal system achieves EER of 3.42% versus 4.48% for text-only and 7.05% for audio-only baselines
- Specialized UAD representations (256-dim) outperform general Whisper representations (1024-dim) in EER
- Best system uses all 80k training examples and combines text, audio, and decoder signal modalities
- LoRA adapters enable effective fine-tuning with limited data while preserving frozen LLM knowledge

## Why This Works (Mechanism)

### Mechanism 1
Low-dimensional specialized audio representations outperform high-dimensional general representations in device-directed speech detection. The UAD model is trained specifically on in-domain data for false trigger mitigation, encoding acoustic features relevant to the target task more efficiently than a general model like Whisper. Core assumption: Domain-specific training leads to better feature representations for specialized tasks than general-purpose pretraining.

### Mechanism 2
Multimodal fusion improves device-directed speech detection accuracy compared to unimodal approaches. The model learns to combine complementary information from text (1-best ASR hypotheses), audio (acoustic features), and decoder signals (acoustic cost, graph cost, confidence scores) to make more robust decisions. Core assumption: Different modalities capture different aspects of device-directedness, and their combination provides more discriminative information than any single modality alone.

### Mechanism 3
Low-rank adaptation (LoRA) enables effective fine-tuning with limited training data while preserving the frozen LLM's knowledge. LoRA introduces small trainable adapter matrices that approximate weight updates without modifying the original LLM parameters, allowing adaptation to the specific task with fewer examples. Core assumption: The pretrained LLM contains relevant knowledge that can be adapted to the new task through low-rank modifications rather than full fine-tuning.

## Foundational Learning

- **Concept**: Multimodal representation learning
  - Why needed here: The system combines text, audio, and decoder signal features into a unified representation space for the LLM
  - Quick check question: How does the model handle different dimensionalities and feature spaces from the three input modalities before feeding them to the LLM?

- **Concept**: Foundation model adaptation techniques
  - Why needed here: The approach uses prefix tuning and LoRA to adapt a frozen LLM to the device-directed speech detection task
  - Quick check question: What are the key differences between prefix tuning and LoRA, and when would you choose one over the other?

- **Concept**: Speech recognition system internals
  - Why needed here: The system relies on 1-best ASR hypotheses and decoder signals (acoustic cost, graph cost, confidence scores) as input features
  - Quick check question: How are the decoder signals (acoustic cost, graph cost, word-level posterior confidence) calculated during ASR inference?

## Architecture Onboarding

- **Component map**: Audio encoder (Whisper/UAD) → M1 → LLM prefix; ASR system → decoder signals → M2 → Concatenate with text tokens → LoRA layers → Classification output

- **Critical path**: Audio → M1 → LLM prefix → Concatenate with text tokens → LoRA layers → Classification output

- **Design tradeoffs**: Specialized UAD vs general Whisper (lower dimensionality and better domain fit vs broader language coverage); LoRA vs full fine-tuning (parameter efficiency and knowledge preservation vs potentially better task adaptation); Multimodal vs unimodal (better accuracy vs simpler implementation and lower latency)

- **Failure signatures**: High EER despite low training loss (potential overfitting or distribution mismatch); degraded performance with specific input types (possible bias in training data or insufficient representation of certain acoustic conditions); unstable training (learning rate too high for LoRA adapters or insufficient batch normalization)

- **First 3 experiments**: 1) Train unimodal text-only baseline to establish lower bound performance; 2) Train unimodal audio-only baseline with both Whisper and UAD to compare feature extractors; 3) Train multimodal model with all three modalities to evaluate fusion benefits

## Open Questions the Paper Calls Out

- **Question**: How does the performance of the multimodal device-directed speech detection system change when using different types of large language models, such as encoder-only or encoder-decoder architectures, instead of the decoder-only LLMs tested in the paper?
- **Question**: What is the impact of varying the rank and scaling factor parameters in the LoRA configuration on the performance of the multimodal device-directed speech detection system?
- **Question**: How does the multimodal device-directed speech detection system perform when tested on data from different domains or languages, beyond the in-house corpora used in the paper?

## Limitations

- Dataset scope and generalization uncertainty due to reliance on single evaluation set without demographic details
- Heavy dependency on ASR system accuracy, with unknown performance impact of ASR errors
- Frozen LLM approach with LoRA may limit task-specific representation learning compared to full fine-tuning

## Confidence

**High Confidence**: Multimodal approach outperforms unimodal baselines in EER metrics (directly supported by experimental results with EERs of 3.42% vs 4.48% and 7.05%)

**Medium Confidence**: Specialized low-dimensional audio representations (UAD) outperform general high-dimensional representations (Whisper) (supported by EER comparisons but tempered by domain-specific training of UAD)

**Low Confidence**: The approach uses "only a fraction of the training data" compared to other methods (lacks direct comparisons to competing approaches' data requirements)

## Next Checks

1. **Cross-Dataset Evaluation**: Test the trained model on external datasets with different acoustic conditions, speaker demographics, and trigger phrases to assess generalization beyond the original evaluation set.

2. **ASR Error Sensitivity Analysis**: Systematically degrade ASR output quality by introducing controlled errors and measure the impact on EER to quantify vulnerability to ASR inaccuracies.

3. **Ablation Study on LoRA Complexity**: Vary the LoRA rank parameter and training data size to identify the minimum effective configuration and validate the current tradeoff between parameter efficiency and detection accuracy.