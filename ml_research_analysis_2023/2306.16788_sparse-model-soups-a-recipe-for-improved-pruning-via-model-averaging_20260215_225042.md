---
ver: rpa2
title: 'Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging'
arxiv_id: '2306.16788'
source_url: https://arxiv.org/abs/2306.16788
tags:
- sparsity
- pruning
- accuracy
- sparse
- phase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Sparse Model Soups (SMS) improve neural network pruning by combining
  parameter averaging with sparsity preservation. The method addresses the challenge
  that averaging arbitrary sparse models reduces overall sparsity due to differing
  sparse connectivities.
---

# Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging

## Quick Facts
- **arXiv ID:** 2306.16788
- **Source URL:** https://arxiv.org/abs/2306.16788
- **Reference count:** 40
- **Primary result:** Sparse Model Soups improve neural network pruning by combining parameter averaging with sparsity preservation, often achieving 1-2% higher test accuracy than individual models or extended IMP variants.

## Executive Summary
Sparse Model Soups (SMS) address a fundamental challenge in neural network pruning: how to combine sparse models while preserving sparsity. Traditional parameter averaging reduces sparsity when models have different sparse connectivities. SMS solves this by ensuring all models share identical sparse patterns through a specific training procedure. The method works by generating multiple retrained models from the same pruned parent with varied hyperparameters, then averaging them while maintaining the shared sparsity mask. This approach consistently improves performance across multiple architectures, datasets, and sparsity levels, making magnitude-based pruning methods more competitive with state-of-the-art pruning-during-training approaches.

## Method Summary
SMS builds upon Iterative Magnitude Pruning (IMP) by introducing hyperparameter variation during the retraining phase. Starting from a pruned model, m copies are created and retrained independently with different hyperparameter configurations (random seeds, weight decay, learning rate schedules). All copies maintain the same sparse connectivity pattern. The models are then averaged using either UniformSoup (simple average) or GreedySoup (sorting by loss before averaging). The averaged model becomes the starting point for the next pruning phase. Batch normalization statistics are recomputed using full training data to ensure accuracy. This process preserves sparsity while improving generalization, as models retrained from averaged models outperform those retrained from single models.

## Key Results
- SMS achieves 1-2% higher test accuracy than individual models or extended IMP variants
- SMS improves state-of-the-art pruning-during-training approaches like GMP, DPF, and BIMP
- SMS maintains sparsity while averaging, avoiding the reduction in sparsity that occurs with arbitrary model combinations
- SMS provides consistent improvements across image classification, semantic segmentation, and neural machine translation tasks

## Why This Works (Mechanism)

### Mechanism 1: Shared Sparse Connectivity
- Averaged models from the same pruned parent share identical sparse connectivity by design
- All m retrained models originate from the same pruned network, inheriting its sparsity pattern
- Averaging preserves the sparsity structure since they all share the same zeroed-out positions
- **Break condition:** If the retraining procedure modifies the sparsity mask (e.g., by dynamically growing connections), the preserved sparsity assumption fails

### Mechanism 2: Improved Generalization Through Averaged Starting Points
- Starting each prune-retrain phase from an averaged model improves generalization of subsequent individual models
- The averaged model from phase t-1 has better generalization than any individual model from that phase
- Pruning this superior model yields a stronger starting point for retraining in phase t
- **Break condition:** If the averaged model becomes too dissimilar across phases (sparsity patterns diverge), the benefit disappears

### Mechanism 3: Hyperparameter Variation Within Same Loss Basin
- Varying hyperparameters during retraining produces models within the same loss basin that are amenable to averaging
- Fine-tuning multiple copies of a pruned model with different hyperparameters results in models that lie in a linearly connected basin
- Their average outperforms individual models due to ensemble-like benefits without increased inference cost
- **Break condition:** At high sparsity levels, models become unstable to randomness and may diverge into different basins

## Foundational Learning

- **Concept:** Iterative Magnitude Pruning (IMP) algorithm
  - **Why needed here:** SMS builds upon IMP's prune-retrain cycle; understanding its mechanics is essential for implementing SMS
  - **Quick check question:** What happens to the sparsity pattern during the retraining phase of IMP?

- **Concept:** Model averaging (parameter averaging) vs. prediction ensembling
  - **Why needed here:** SMS uses parameter averaging to combine sparse models while maintaining sparsity, distinct from prediction ensembling which requires multiple forward passes
  - **Quick check question:** How does averaging models with different sparse connectivities affect overall sparsity?

- **Concept:** Loss basin connectivity and mode connectivity
  - **Why needed here:** SMS relies on models being in the same loss basin for effective averaging; understanding mode connectivity helps explain when averaging succeeds or fails
  - **Quick check question:** Under what conditions do models trained with different random seeds lie in the same loss basin?

## Architecture Onboarding

- **Component map:** Pretrained model → Pruning → m copies → Parallel retraining with varied hyperparameters → Model averaging → Next phase
- **Critical path:** Load pretrained model → Apply pruning to obtain sparse mask → Create m copies with identical masks → Retrain each copy with different hyperparameters → Average models (UniformSoup or GreedySoup) → Repeat from pruning using averaged model
- **Design tradeoffs:** Parallel retraining increases computational cost but is fully parallelizable; GreedySoup may outperform UniformSoup when models diverge but requires sorting overhead; recomputing batch normalization statistics after averaging is crucial for accuracy but adds computation
- **Failure signatures:** Reduced sparsity after averaging (indicates models have diverged); decreased performance after averaging (models not in same basin); no improvement over prolonged single-model training (insufficient hyperparameter variation)
- **First 3 experiments:** Implement One Shot IMP with m=3 random seeds, compare averaged model vs. best individual; Add weight decay variation to m=3 models, compare performance to random seed variation only; Implement full SMS with two prune-retrain cycles, verify sparsity preservation and accuracy improvement

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of Sparse Model Soups (SMS) change when varying the number of models (m) beyond the tested range?
- **Basis in paper:** The paper explores different values of m (3, 5, 10) in experiments but does not exhaustively test all possible values
- **Why unresolved:** The impact of varying m on performance is not fully explored, especially at very high or low values
- **What evidence would resolve it:** Systematic experiments testing a wider range of m values, particularly focusing on the point where performance gains plateau or diminish

### Open Question 2
- **Question:** Can SMS be effectively applied to other types of model compression techniques beyond pruning, such as quantization or knowledge distillation?
- **Basis in paper:** The paper focuses on pruning and suggests SMS's potential for other compression methods, but does not provide concrete evidence
- **Why unresolved:** The paper's experiments are limited to pruning, leaving the applicability to other compression techniques unexplored
- **What evidence would resolve it:** Experiments applying SMS to quantization and knowledge distillation, comparing performance with and without SMS

### Open Question 3
- **Question:** What is the theoretical explanation for the improved performance of models retrained from averaged models compared to those retrained from single models?
- **Basis in paper:** The paper observes that models retrained from averaged models perform better but does not provide a theoretical explanation
- **Why unresolved:** The paper attributes the improvement to better generalization but lacks a deeper theoretical understanding
- **What evidence would resolve it:** A theoretical analysis or model that explains the mechanism behind the improved performance of averaged models

## Limitations

- The assumption that hyperparameter variation during retraining keeps models within the same loss basin may break down at high sparsity levels (95%+)
- The method assumes identical sparsity masks can be preserved across all retraining phases, which may not hold for dynamic sparsity approaches
- Computational overhead from parallel retraining is significant, though argued to be offset by performance gains

## Confidence

- **High confidence:** The core mechanism of preserving sparsity through identical masks is well-established and directly supported by the paper's empirical results
- **Medium confidence:** Claims about generalization improvement through averaged model starting points are supported by ablation studies but rely on theoretical connections between generalization and pruning robustness
- **Medium confidence:** Performance improvements over IMP+ and IMP-AVG variants are demonstrated, but the magnitude of improvement may vary with architecture and task complexity

## Next Checks

1. Test SMS performance at extreme sparsity levels (99%+) to identify the breaking point where models diverge into different basins
2. Compare SMS with dynamic sparsity approaches where masks evolve during training to assess compatibility
3. Implement ablation studies varying the number of hyperparameter configurations (m) to find optimal balance between computational cost and performance gain