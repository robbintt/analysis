---
ver: rpa2
title: Finetuning Text-to-Image Diffusion Models for Fairness
arxiv_id: '2311.07604'
source_url: https://arxiv.org/abs/2311.07604
tags:
- diffusion
- gender
- person
- images
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose a supervised fine-tuning method for aligning the distributions
  of specific attributes of images generated by text-to-image diffusion models with
  user-defined target distributions. Our method uses a distributional alignment loss
  and a biased gradient to fine-tune the sampling process of diffusion models.
---

# Finetuning Text-to-Image Diffusion Models for Fairness

## Quick Facts
- arXiv ID: 2311.07604
- Source URL: https://arxiv.org/abs/2311.07604
- Reference count: 23
- One-line primary result: Proposed method significantly reduces gender, racial, and intersectional biases in text-to-image diffusion models through supervised fine-tuning with distributional alignment loss

## Executive Summary
This paper addresses fairness issues in text-to-image diffusion models by proposing a supervised fine-tuning approach that aligns generated image attributes with user-defined target distributions. The method introduces a distributional alignment loss that uses optimal transport to dynamically generate target classes, and a biased gradient approach to overcome exploding gradient problems during direct fine-tuning. Experiments demonstrate significant bias reduction across gender, race, and intersectional attributes while maintaining image quality, even when fine-tuning as few as five soft tokens.

## Method Summary
The proposed method consists of two technical contributions: (1) a distributional alignment loss (DAL) that steers specific characteristics of generated images toward user-defined target distributions using optimal transport to compute cross-entropy loss with dynamically generated targets, and (2) biased direct finetuning (DFT) of the diffusion model's sampling process that leverages an adjusted gradient to overcome exploding gradient norm and variance issues in naive DFT. The approach fine-tunes LoRA components (text encoder, prompt prefix, or U-Net) using DAL combined with semantics preservation losses, enabling scalable bias mitigation across multiple concepts simultaneously.

## Key Results
- Fine-tuning as few as five soft tokens can significantly reduce gender bias in occupational prompts
- Method achieves substantial bias reduction for gender, race, and intersectional attributes while preserving image quality
- Supports diverse fairness perspectives, including controlling age distribution while simultaneously debiasing gender and race

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The distributional alignment loss (DAL) steers the generated images toward a user-defined target distribution by computing cross-entropy loss with dynamically generated targets based on optimal transport (OT).
- Mechanism: For each batch of generated images, DAL uses pre-trained classifiers to estimate class probabilities, dynamically generates target classes that match the target distribution and have the minimum transport distance from current class probabilities, and computes the cross-entropy loss with these target classes.
- Core assumption: The OT-based approach effectively identifies the most efficient modification of current images to match the target distribution in the class probability space.
- Evidence anchors:
  - [abstract]: "Our solution consists of two main technical contributions: (1) a distributional alignment loss that steers specific characteristics of the generated images towards a user-defined target distribution"
  - [section 4.1]: "We first generate a batch of images I = {x(i)}i∈[N] using the finetuned diffusion model and some prompt P. For every generated image x(i), we use a pre-trained classifier h to produce a class probability vector pi = [pi,1, · · · , pi,K] = h(x(i)), with pi,k denoting the estimated probability that x(i) is from class k"
  - [corpus]: Weak evidence - corpus does not directly address DAL mechanism
- Break condition: If the pre-trained classifiers are inaccurate or the OT computation is too complex for high-dimensional distributions, DAL may fail to effectively align the generated images with the target distribution.

### Mechanism 2
- Claim: Biased direct finetuning of diffusion models (biased DFT) overcomes the exploding gradient norm and variance issues in naive DFT by leveraging a biased gradient.
- Mechanism: Biased DFT sets At = 1 and Bt = I in the gradient computation, removing the coupling between partial gradients of different time steps and standardizing the influence of noise predictions from different time steps.
- Core assumption: The exploding gradient norm and variance in naive DFT are primarily caused by the recurrent evaluations of the U-Net in the reverse diffusion process and the coupling between partial gradients of different time steps.
- Evidence anchors:
  - [abstract]: "Our solution consists of two main technical contributions: (1) a distributional alignment loss that steers specific characteristics of the generated images towards a user-defined target distribution, and (2) adjusted direct finetuning of diffusion model's sampling process, which leverages an adjusted gradient to more effectively optimize losses defined on the generated images"
  - [section 4.2]: "We empirically show these problems indeed exist in naive DFT. Since directly computing the Jacobian matrices ∂ϵ(t)/∂θ and ∂ϵ(s)/∂zs is too expensive, we assume dL(x0)/dx0 dx0/dz0 is a random Gaussian matrix R ∼ N (0, 10−4 × I) and plot the values of |RAtBt ∂ϵ(t)/∂θ|, |RAt ∂ϵ(t)/∂θ|, and |R ∂ϵ(t)/∂θ| in Fig. 1b. It is apparent both the scale and variance of |RAtBt ∂ϵ(t)/∂θ| explodes as t → 1000, but neither |RAt ∂ϵ(t)/∂θ| nor |R ∂ϵ(t)/∂θ|"
  - [corpus]: Weak evidence - corpus does not directly address biased DFT mechanism
- Break condition: If the coupling between partial gradients of different time steps is not the primary cause of exploding gradient norm and variance, or if the standardization of At to 1 negatively impacts the optimization process, biased DFT may fail to effectively finetune the diffusion model.

### Mechanism 3
- Claim: Finetuning the text encoder while keeping the U-Net unchanged hits a sweet spot that effectively mitigates biases and lessens potential negative effects on image quality.
- Mechanism: By prioritizing the finetuning of language understanding components (text encoder and prompt) over the image generation component (U-Net), the model is encouraged to maintain a holistic visual representation of gender and racial identities, rather than manipulating low-level pixel features to signal gender and race.
- Core assumption: Finetuning the text encoder is sufficient to address the biases in the diffusion model, and finetuning the U-Net may deteriorate image quality and lead to overfitting.
- Evidence anchors:
  - [section 5.1]: "Surprisingly, finetuning as few as five soft tokens as a prompt prefix can already largely reduces gender bias. These results underscore the robustness of our method and the efficacy of debiasing T2I diffusion models by finetuning their language understanding components"
  - [section 5.1]: "It can deteriorate image quality w.r.t. facial skin textual and the model becomes more capable at misleading the classifier into predicting an image as one gender, despite the image's perceptual resemblance to another gender"
  - [corpus]: Weak evidence - corpus does not directly address the choice of finetuning components
- Break condition: If finetuning the text encoder is not sufficient to address the biases, or if finetuning the U-Net does not significantly deteriorate image quality or lead to overfitting, this mechanism may fail to effectively mitigate biases while preserving image quality.

## Foundational Learning

- Concept: Diffusion models
  - Why needed here: Understanding the basics of diffusion models, including the forward and reverse diffusion processes, is crucial for grasping the proposed method's approach to finetuning and bias mitigation.
  - Quick check question: What are the key components of a diffusion model, and how does the reverse diffusion process generate images from noise?

- Concept: Optimal transport (OT)
  - Why needed here: The distributional alignment loss (DAL) leverages OT to compute the minimum transport distance between the current class probabilities and the target distribution, which is essential for steering the generated images towards the desired distribution.
  - Quick check question: How does OT help in finding the most efficient modification of current images to match the target distribution in the class probability space?

- Concept: Cross-entropy loss
  - Why needed here: DAL computes the cross-entropy loss with dynamically generated targets based on OT, which is a fundamental concept in supervised learning and is used to optimize the model's predictions to match the target distribution.
  - Quick check question: What is the role of cross-entropy loss in optimizing the model's predictions to align with the target distribution?

## Architecture Onboarding

- Component map:
  Text encoder -> U-Net -> Diffusion scheduler -> Classifier

- Critical path:
  1. Encode the input text prompt using the text encoder.
  2. Initialize the reverse diffusion process with random noise.
  3. Iteratively denoise the image using the U-Net and diffusion scheduler.
  4. Apply the DAL to compute the cross-entropy loss with dynamically generated targets.
  5. Backpropagate the loss through the text encoder and U-Net to update their parameters.

- Design tradeoffs:
  - Finetuning the text encoder vs. U-Net: Finetuning the text encoder is more effective in mitigating biases while preserving image quality, but may not address all biases. Finetuning the U-Net may lead to overfitting and deterioration of image quality.
  - Number of soft tokens to finetune: Finetuning as few as five soft tokens can significantly reduce gender bias, but finetuning more tokens may provide better control over the generated images.
  - Confidence threshold for DAL: A higher confidence threshold ensures that only high-confidence predictions are used to compute the loss, but may reduce the number of samples used for optimization.

- Failure signatures:
  - Biases persist in the generated images despite finetuning.
  - Image quality deteriorates significantly after finetuning.
  - The model overfits to the training prompts and fails to generalize to unseen prompts.
  - The distributional alignment loss fails to converge or produces unstable results.

- First 3 experiments:
  1. Finetune the text encoder on a small set of occupational prompts to reduce gender bias and evaluate the debiasing effect on unseen prompts.
  2. Finetune the U-Net on the same set of occupational prompts and compare the debiasing effect and image quality with the text encoder finetuning.
  3. Finetune both the text encoder and U-Net on a larger and more diverse set of prompts, including occupations, sports, and personal descriptors, to evaluate the scalability and generalization of the proposed method.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the different components of the diffusion model (e.g., text encoder, U-Net, prompt) contribute to the observed biases in generated images, and which components should be prioritized for debiasing?
- Basis in paper: [explicit] The paper conducts an ablation study to evaluate the impact of finetuning different components of the diffusion model on debiasing performance. The results suggest that finetuning the text encoder and prompt is more effective and less likely to negatively impact image quality compared to finetuning the U-Net.
- Why unresolved: The paper does not provide a detailed analysis of the underlying reasons for the observed differences in debiasing effectiveness between the different components. Further investigation is needed to understand the specific mechanisms by which each component contributes to biases and how they can be effectively addressed.
- What evidence would resolve it: A comprehensive analysis of the learned representations and internal dynamics of the text encoder, U-Net, and prompt during the finetuning process. This could involve techniques such as feature visualization, activation analysis, and gradient-based attribution methods to identify the specific patterns and biases encoded in each component.

### Open Question 2
- Question: How can the distributional alignment loss (DAL) be extended to handle more complex and nuanced fairness objectives beyond simple categorical attributes like gender and race?
- Basis in paper: [explicit] The paper demonstrates the effectiveness of DAL for aligning the distributions of gender, race, and age attributes. However, it acknowledges the limitations of binary gender representation and the need for more inclusive approaches.
- Why unresolved: The current formulation of DAL is primarily designed for categorical attributes with a predefined set of classes. Extending it to handle more complex fairness objectives, such as intersectional fairness, continuous attributes, or subjective notions of fairness, requires further research and development.
- What evidence would resolve it: Experimental results showing the successful application of DAL to a wider range of fairness objectives, including intersectional fairness, continuous attributes (e.g., age, skin tone), and subjective notions of fairness (e.g., representation of different cultural backgrounds or socioeconomic groups). Additionally, theoretical analysis and formalization of DAL for these more complex scenarios would provide valuable insights.

### Open Question 3
- Question: How can the proposed method be adapted to address biases in text-to-image diffusion models trained on diverse datasets and languages?
- Basis in paper: [explicit] The paper focuses on debiasing a specific text-to-image diffusion model (Stable Diffusion) trained on a particular dataset. The generalizability of the method to other models and datasets is not explicitly addressed.
- Why unresolved: Biases in text-to-image diffusion models can arise from various sources, including the training data, model architecture, and optimization process. The effectiveness of the proposed method may vary depending on these factors, and further research is needed to understand its limitations and potential adaptations for different scenarios.
- What evidence would resolve it: Experimental results demonstrating the successful application of the proposed method to debias text-to-image diffusion models trained on diverse datasets and languages. This could involve evaluating the method on models trained on different cultural contexts, languages, and image domains. Additionally, analyzing the impact of the training data composition and model architecture on the effectiveness of the debiasing approach would provide valuable insights.

## Limitations

- The method relies heavily on pre-trained classifiers for attribute estimation, which may introduce their own biases or inaccuracies
- Scalability to more complex attributes and intersectional fairness scenarios remains untested
- The effectiveness of the biased gradient approach depends on specific implementation details that are not fully specified

## Confidence

**High Confidence:**
- The exploding gradient norm and variance issues in naive DFT are empirically demonstrated and the biased gradient approach addresses this specific technical problem
- Finetuning as few as five soft tokens can significantly reduce gender bias in occupational prompts
- The distributional alignment loss effectively steers generated images toward user-defined target distributions when classifiers are accurate

**Medium Confidence:**
- The claim that finetuning text encoder components rather than U-Net preserves image quality while effectively mitigating biases
- The scalability claim for handling multiple concepts simultaneously
- The assertion that the method supports diverse perspectives of fairness

**Low Confidence:**
- The mechanism by which OT-based target generation optimally identifies image modifications
- The long-term generalization performance on unseen prompts beyond the test sets used
- The comparison with alternative bias mitigation approaches not evaluated in the paper

## Next Checks

1. **Gradient Implementation Validation**: Implement the biased DFT with varying coefficient values (Ci) and normalization schemes to empirically verify the claimed exploding gradient problem and test whether alternative gradient stabilization techniques (such as gradient clipping or adaptive learning rates) could achieve similar or better results without the specific biased gradient approach.

2. **Cross-Classifier Robustness Test**: Evaluate the debiasing performance using multiple independent classifiers for gender, race, and age attributes to assess whether the method's effectiveness depends on the accuracy and potential biases of specific classifier choices, and to quantify the variance in results across different classifier implementations.

3. **Out-of-Distribution Generalization**: Test the fine-tuned models on occupational prompts that combine multiple underrepresented attributes (e.g., "a Black elderly woman doctor" when the fine-tuning focused on single attributes) to validate whether the method can handle intersectional fairness scenarios and generalize to complex attribute combinations not explicitly present in the training data.