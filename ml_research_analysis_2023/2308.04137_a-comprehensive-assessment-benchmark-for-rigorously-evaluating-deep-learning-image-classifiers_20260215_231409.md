---
ver: rpa2
title: A Comprehensive Assessment Benchmark for Rigorously Evaluating Deep Learning
  Image Classifiers
arxiv_id: '2308.04137'
source_url: https://arxiv.org/abs/2308.04137
tags:
- data
- adversarial
- performance
- samples
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces a comprehensive benchmark for evaluating
  deep learning image classifiers across five types of data: clean, corrupted, adversarial,
  novel classes, and unrecognisable images. It proposes a unified metric (DAR) that
  assesses both correct classification and appropriate rejection of unknown samples.'
---

# A Comprehensive Assessment Benchmark for Rigorously Evaluating Deep Learning Image Classifiers

## Quick Facts
- arXiv ID: 2308.04137
- Source URL: https://arxiv.org/abs/2308.04137
- Reference count: 20
- Primary result: Introduces DAR metric revealing current models overstate robustness across clean, corrupted, adversarial, novel, and unrecognisable image types

## Executive Summary
This paper introduces a comprehensive benchmark for evaluating deep learning image classifiers across five data types: clean, corrupted, adversarial, novel classes, and unrecognisable images. The authors propose a unified metric called DAR (Detection Accuracy Rate) that assesses both correct classification and appropriate rejection of unknown samples. Testing various DNNs (ResNet18, WideResNet34-20, MLP, ConvNet) trained with different data augmentation methods shows that current models lack robustness across all data types despite good clean accuracy performance.

## Method Summary
The benchmark implements DAR as a unified metric that normalizes performance across data types by fixing the acceptance rate on correctly classified clean samples. Models are trained using various augmentation methods including baseline, noise, adversarial training, pixmix, and regmixup. The evaluation framework tests classifiers on five distinct data types using consistent rejection criteria. The metric is calculated separately for each data type and then averaged to produce a comprehensive robustness score.

## Key Results
- Current deep learning models show significant performance degradation when evaluated across multiple robustness types
- No single training method excels across all data types; trade-offs exist between robustness to different data types
- Existing robustness evaluation metrics overestimate real-world performance by not accounting for sample rejection
- Models perform poorly on novel class data compared to standard metrics, revealing limitations of current evaluation approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unified metric (DAR) enables fair cross-dataset robustness comparison.
- Mechanism: DAR normalizes performance across data types by fixing the acceptance rate on correctly classified clean samples, preventing methods from tuning rejection thresholds per dataset.
- Core assumption: A single rejection threshold can meaningfully compare methods across clean, corrupt, adversarial, novel, and unrecognisable data.
- Evidence anchors:
  - [abstract]: "proposes a unified metric (DAR) that assesses both correct classification and appropriate rejection of unknown samples."
  - [section]: "Summarising results by averaging over task, not data-set... A single summary metric is obtained by calculating the metrics separately for each type of data, and then averaging across data type."
  - [corpus]: Missing strong evidence; corpus neighbors focus on different domains (audio, text, NLP) and do not directly support this claim.
- Break condition: If acceptance rate threshold varies widely between data types, DAR becomes unrepresentative.

### Mechanism 2
- Claim: Multiple robustness types are mutually incompatible; no single training method excels across all.
- Mechanism: Trade-offs arise because optimizing for one robustness type (e.g., adversarial) degrades performance on others (e.g., clean accuracy), due to conflicting feature space decisions.
- Core assumption: Feature space regions critical for one robustness type overlap poorly with those needed for others.
- Evidence anchors:
  - [section]: "These trade-offs in performance on different data types raise the concern that efforts to increase robustness in one area may be resulting in a decrease in robustness in another area."
  - [section]: "The results for all methods on unknown class data is far worse than the standard metrics would imply."
  - [corpus]: No direct support; corpus neighbors discuss unrelated domains (audio, NLP) and not multi-robustness trade-offs.
- Break condition: If a method is discovered that jointly optimizes multiple robustness types without performance degradation.

### Mechanism 3
- Claim: Current metrics overestimate real-world robustness by not accounting for rejection.
- Mechanism: Standard accuracy metrics ignore sample rejection, allowing high scores even when many misclassified samples are simply rejected; DAR corrects this by including rejection in the metric.
- Core assumption: Real-world classifiers must reject uncertain samples, so evaluation must penalize both misclassification and unnecessary rejection.
- Evidence anchors:
  - [abstract]: "current evaluation protocols typically used to assess classifiers fail to comprehensively evaluate performance as they tend to rely on limited types of test data, and ignore others."
  - [section]: "current metrics for unknown class rejection grossly over-estimate likely real-world performance."
  - [corpus]: Weak evidence; corpus neighbors focus on different evaluation frameworks but do not directly address rejection in robustness evaluation.
- Break condition: If rejection is not a realistic operational constraint for the application.

## Foundational Learning

- Concept: Out-of-distribution (OOD) detection vs. OOD generalization.
  - Why needed here: The paper distinguishes between generalizing to corrupted but known-class data vs. rejecting unknown-class data; confusing these leads to incorrect benchmarking.
  - Quick check question: Does a method that improves clean accuracy also improve OOD generalization, or only OOD rejection?

- Concept: Decision boundary placement in feature space.
  - Why needed here: Robustness depends on where decision boundaries are placed relative to known and unknown data regions; understanding this is key to interpreting trade-offs.
  - Quick check question: If a classifier achieves high clean accuracy but low DAR, where are its decision boundaries likely positioned?

- Concept: Rejection threshold calibration.
  - Why needed here: DAR fixes the rejection threshold at a fixed acceptance rate for clean data; understanding this calibration is necessary to interpret results.
  - Quick check question: What happens to DAR if the rejection threshold is tightened (lower acceptance rate)?

## Architecture Onboarding

- Component map: Test data generators (clean, corrupt, adversarial, novel, unrecognisable) -> Rejection criteria (MSP, MLS, Energy, GEN) -> DAR calculator -> Training pipelines
- Critical path: 1) Train model with chosen augmentation. 2) Generate all test data types. 3) Apply rejection criteria. 4) Compute DAR for each data type. 5) Aggregate for final comparison.
- Design tradeoffs: Rejecting more clean samples increases robustness to unknown data but reduces overall accuracy; balancing threshold is key.
- Failure signatures: High DAR variance across runs suggests instability; low DAR on novel classes indicates poor OOD rejection; low DAR on adversarial samples indicates poor adversarial robustness.
- First 3 experiments:
  1. Baseline ResNet18 on CIFAR10 with clean accuracy only to verify implementation.
  2. Baseline ResNet18 on CIFAR10 with DAR at 99% clean acceptance to confirm rejection integration.
  3. Noise-augmented ResNet18 on CIFAR10 with DAR at 95% clean acceptance to test noise method.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design a comprehensive evaluation benchmark that fairly balances assessment across different types of data (clean, corrupted, adversarial, novel classes, unrecognisable images) without being biased towards any particular data type?
- Basis in paper: [explicit] The paper discusses the need for a comprehensive benchmark that evaluates models across multiple data types and proposes a unified metric (DAR) to assess performance consistently across these types.
- Why unresolved: The challenge lies in defining a fair metric that does not overemphasize certain types of data, especially when the number and variety of data sets can vary significantly.
- What evidence would resolve it: A validated benchmark that consistently ranks models across diverse data sets, demonstrating balanced performance evaluation without bias towards any specific data type.

### Open Question 2
- Question: What are the trade-offs between improving robustness to one type of data (e.g., adversarial attacks) and performance on other types (e.g., common corruptions or novel class rejection)?
- Basis in paper: [explicit] The paper highlights that methods improving robustness in one area (like adversarial training) often result in decreased performance in others (like clean accuracy or unknown class rejection).
- Why unresolved: Understanding the underlying mechanisms and trade-offs requires extensive experimentation and analysis across various training methods and data types.
- What evidence would resolve it: Empirical studies showing consistent patterns of trade-offs across different models and training methods, supported by theoretical insights into why these trade-offs occur.

### Open Question 3
- Question: How can we develop training methods that improve overall robustness without significant trade-offs between different types of data?
- Basis in paper: [explicit] The paper indicates that current methods, even those claiming state-of-the-art robustness, still perform poorly on certain data types, suggesting a need for better training approaches.
- Why unresolved: Existing methods have inherent limitations that lead to trade-offs, and discovering new techniques that overcome these limitations requires innovative approaches and extensive validation.
- What evidence would resolve it: Development and validation of new training methods that consistently improve performance across all data types without significant trade-offs, supported by both empirical results and theoretical justification.

## Limitations
- DAR metric's fixed rejection threshold may not equally represent all data types
- Trade-off claims lack strong quantitative analysis and rely on qualitative observations
- Benchmark validation is limited to academic metrics rather than real-world deployment scenarios

## Confidence
- Methodology of combining multiple robustness types: High
- DAR metric's ability to fairly compare methods: Medium
- Generalizability of trade-off claims across different model architectures and datasets: Low

## Next Checks
1. Validate DAR metric by testing it on synthetic data where ground truth robustness levels are known
2. Perform ablation studies on the rejection threshold parameter to determine its sensitivity across different data types
3. Test whether the observed trade-offs persist when using different model architectures beyond the ones studied