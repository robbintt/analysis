---
ver: rpa2
title: 'The Unequal Opportunities of Large Language Models: Revealing Demographic
  Bias through Job Recommendations'
arxiv_id: '2308.02053'
source_url: https://arxiv.org/abs/2308.02053
tags:
- bias
- chatgpt
- recommendations
- language
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a simple template-based approach to analyze
  demographic bias in large language models (LLMs) through job recommendations. The
  method involves requesting job recommendations for a "recently laid-off friend"
  while mentioning demographic attributes like gender pronouns and nationality.
---

# The Unequal Opportunities of Large Language Models: Revealing Demographic Bias through Job Recommendations

## Quick Facts
- arXiv ID: 2308.02053
- Source URL: https://arxiv.org/abs/2308.02053
- Reference count: 37
- Primary result: Template-based approach reveals significant demographic bias in ChatGPT and LLaMA through job recommendations, with distinct patterns for different gender and nationality intersections

## Executive Summary
This paper introduces a template-based approach to measure demographic bias in large language models through job recommendation tasks. The method involves requesting job recommendations for a "recently laid-off friend" while mentioning demographic attributes like gender pronouns and nationality. The study analyzes bias in ChatGPT and LLaMA, examining intersectional biases across gender and nationality. Experiments reveal that both models exhibit distinct biases toward various demographic identities, with ChatGPT showing higher bias levels compared to LLaMA. The approach can be extended to examine biases associated with any intersection of demographic identities.

## Method Summary
The study employs a template-based prompting approach to analyze demographic bias in LLMs through job recommendations. Three naturalistic templates request job recommendations for a "recently laid-off friend" with variations including gender pronouns and nationality attributes. The method generates 50 responses per prompt per demographic combination for both ChatGPT and LLaMA. Job titles are extracted and clustered using BERTopic, with probability distributions analyzed across demographics and prompts. The results are validated against real-world labor statistics to assess alignment with actual market conditions.

## Key Results
- Both ChatGPT and LLaMA consistently suggest low-paying jobs for Mexican workers across all prompts
- Secretarial roles are disproportionately recommended to women compared to men in both models
- ChatGPT exhibits significantly higher bias levels than LLaMA across all tested demographics
- The baseline prompt (without nationality) produces outlier responses, particularly in ChatGPT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mentioning demographic attributes in job recommendation prompts alters the model's output distribution
- Mechanism: The LLM associates demographic terms with societal stereotypes learned during training, causing the model to recommend jobs and salaries aligned with these stereotypes
- Core assumption: Training data contains stereotypical associations between demographics and job types/salaries
- Evidence anchors: [abstract] method can be extended to examine biases associated with any intersection of demographic identities; [section 3.3] templates are naturalistic and reflective of realistic language
- Break condition: If the LLM has been extensively fine-tuned to suppress stereotype associations or if the demographic terms are not present in the training data

### Mechanism 2
- Claim: The absence of nationality information (baseline) leads to job recommendations that are outliers compared to nationality-specific results
- Mechanism: The model treats the baseline prompt differently due to its distinct phrasing, even if semantically similar, leading to a different output distribution
- Core assumption: The model's response is sensitive to minor phrasing differences in the prompt
- Evidence anchors: [section 4.4] baseline tends to be an outlier in ChatGPT; [section 4.4] baseline differed by multiple words from nationality templates
- Break condition: If the model is trained to be robust to minor phrasing variations or if the baseline prompt is phrased identically to nationality-specific prompts

### Mechanism 3
- Claim: Clustering job recommendations allows for analysis of biases at the cluster level, revealing systematic differences in recommendations across demographics
- Mechanism: By grouping similar jobs into clusters, the study can quantify the probability of each cluster being recommended to different demographic groups, highlighting biases in job type recommendations
- Core assumption: Job titles can be meaningfully grouped into clusters based on semantic similarity
- Evidence anchors: [section 3.5] BERTopic identified 17 clusters from ChatGPT and 19 clusters from LLaMA; [section 3.5.1] distinctions observed between embeddings produced by ChatGPT and LLaMA
- Break condition: If the clustering algorithm fails to group semantically similar jobs or if the clusters are too granular or too broad to be meaningful

## Foundational Learning

- Concept: Intersectionality in bias measurement
  - Why needed here: The study examines biases at the intersection of gender identity and nationality, requiring an understanding of how multiple demographic factors interact
  - Quick check question: How does the bias observed for a Mexican woman differ from the bias observed for a Mexican man or a woman from another nationality?

- Concept: Template-based prompting and its limitations
  - Why needed here: The study uses template-based prompts to analyze bias, which is known to be brittle and sensitive to minor phrasing changes
  - Quick check question: What are the potential drawbacks of using template-based prompts for bias measurement, and how might these affect the study's results?

- Concept: Statistical parity as a fairness metric
  - Why needed here: The study uses statistical parity to define fairness, where each demographic group should receive the same distribution of job recommendations
  - Quick check question: How does the concept of statistical parity apply to the job recommendation task, and what are its limitations as a fairness metric?

## Architecture Onboarding

- Component map: Prompt generation -> LLM response -> Job clustering (BERTopic) -> Bias analysis -> Labor data comparison
- Critical path: Template creation → LLM generation → Job title extraction → Clustering → Probability analysis → Real-world validation
- Design tradeoffs: Template-based prompts provide consistency but are brittle; clustering simplifies analysis but may lose granularity; real-world comparison validates findings but is limited by data availability
- Failure signatures: Inconsistent output formatting from LLaMA requiring additional parsing; BERTopic clustering producing too many/few clusters; nonsensical job recommendations from LLMs; limited matching job titles for labor data comparison
- First 3 experiments:
  1. Generate job recommendations using the baseline prompt and analyze the distribution of job types and salaries
  2. Generate job recommendations for a specific demographic (e.g., Mexican woman) and compare the distribution to the baseline
  3. Cluster the job recommendations and analyze the probability of each cluster being recommended to different demographic groups

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop more robust bias measurement techniques that are less susceptible to manipulation through prompt engineering or template variations?
- Basis in paper: [inferred] The paper notes that "minor modifications to templates can lead to significant variations in measured biases, highlighting the brittleness and instability of these benchmarks" and suggests evolving analysis methodology to "reduce reliance on template-based approaches"
- Why unresolved: Current template-based methods are fragile and can be gamed by making small changes to prompts. This makes it difficult to establish reliable, consistent measurements of bias across different studies and implementations
- What evidence would resolve it: Development and validation of a new bias measurement framework that maintains consistent results across semantically equivalent prompt variations, along with empirical demonstrations showing robustness against prompt manipulation attempts

### Open Question 2
- Question: What are the underlying mechanisms causing the pronounced bias toward Mexican workers in both ChatGPT and LLaMA, and how can these biases be effectively mitigated?
- Basis in paper: [explicit] The paper identifies "a unique bias toward Mexicans, both in the types of jobs recommended and the salaries provided, reflecting historical labor market discrimination toward Mexican Americans" and notes this warrants "further investigation"
- Why unresolved: While the bias is identified and linked to historical discrimination patterns, the specific mechanisms in the models' training data or architecture that cause this pronounced bias are not yet understood. Additionally, effective mitigation strategies have not been developed
- What evidence would resolve it: Detailed analysis tracing the origins of Mexican-specific bias in training data, along with experimental validation of bias mitigation techniques specifically targeting this demographic group

### Open Question 3
- Question: How do biases in large language models for job recommendations compare across different types of tasks and applications beyond employment contexts?
- Basis in paper: [explicit] The paper states "biases may differ significantly in other types of tasks" and their work "primarily focuses on measuring demographic bias related to nationality and gender identity within the context of job recommendations"
- Why unresolved: The study is limited to job recommendation scenarios, but biases could manifest differently or have varying impacts in other domains like healthcare, legal advice, or education. The generalizability of findings across domains remains unknown
- What evidence would resolve it: Comparative studies measuring similar demographic biases across multiple application domains, analyzing both the types of biases that emerge and their potential real-world impacts in each context

## Limitations

- Template-based prompting approach may be sensitive to minor phrasing changes, affecting result consistency
- Analysis focuses on only two LLMs (ChatGPT and LLaMA), limiting generalizability to other models
- Job clustering may oversimplify nuanced differences between job titles, potentially masking specific biases

## Confidence

- **High Confidence**: General finding that LLMs exhibit demographic biases in job recommendations, supported by multiple experiments and consistent patterns across models
- **Medium Confidence**: Specific patterns of bias for individual nationalities and genders, as these may be sensitive to particular prompts used and clustering of job titles
- **Low Confidence**: Comparison of bias levels between ChatGPT and LLaMA, as this depends on specific versions and configurations of the models used

## Next Checks

1. **Prompt Sensitivity Analysis**: Systematically vary the prompt templates and measure the impact on job recommendation distributions to assess result robustness to minor phrasing changes

2. **Cross-Model Validation**: Apply the same analysis to additional LLMs (e.g., GPT-4, Claude) to determine if observed biases are consistent across models or specific to ChatGPT and LLaMA

3. **Granular Job Title Analysis**: Analyze biases at the individual job title level rather than using clusters to capture more nuanced differences in recommendations and identify specific job titles showing the strongest bias