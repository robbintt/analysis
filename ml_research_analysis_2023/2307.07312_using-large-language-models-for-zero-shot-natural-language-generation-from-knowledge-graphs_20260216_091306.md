---
ver: rpa2
title: Using Large Language Models for Zero-Shot Natural Language Generation from
  Knowledge Graphs
arxiv_id: '2307.07312'
source_url: https://arxiv.org/abs/2307.07312
tags:
- text
- triples
- data
- knowledge
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of large language models (LLMs) for
  zero-shot knowledge graph-to-text generation. The authors evaluate ChatGPT on the
  WebNLG 2020 challenge and a custom WikiData dataset, comparing factual, counterfactual,
  and fictional triples.
---

# Using Large Language Models for Zero-Shot Natural Language Generation from Knowledge Graphs

## Quick Facts
- arXiv ID: 2307.07312
- Source URL: https://arxiv.org/abs/2307.07312
- Authors: 
- Reference count: 16
- Primary result: ChatGPT achieves near state-of-the-art performance on some WebNLG metrics but lags on others; factualness significantly impacts coverage and hallucination rates.

## Executive Summary
This paper explores using large language models (LLMs) for zero-shot knowledge graph-to-text generation, evaluating ChatGPT on the WebNLG 2020 challenge and a custom WikiData dataset. The study compares factual, counterfactual, and fictional triples to assess how factual alignment affects generation quality. Key findings reveal that ChatGPT performs competitively on some automated metrics but shows higher hallucination rates (10-15%) across all conditions, with factual triples yielding better coverage than fictional or counterfactual ones.

## Method Summary
The authors evaluate ChatGPT using zero-shot KG-to-text generation on WebNLG 2020 (English and Russian) and a custom WikiData dataset with factual, counterfactual, and fictional triples. They employ both single-step and two-step prompting approaches, with the latter iteratively building text from triples. Generated outputs are evaluated using automated metrics (BLEU, METEOR, CHRF++, TER, BERT, BLEURT) and human annotations for grammar, coherence, triple coverage, and hallucinations.

## Key Results
- ChatGPT achieves near state-of-the-art METEOR scores on WebNLG English but lags on other metrics
- Factualness significantly impacts performance: factual triples show higher coverage and fewer hallucinations than fictional or counterfactual ones
- Hallucinations occur in approximately 10-15% of cases across all factualness conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models can generate fluent natural language from knowledge graph triples without task-specific training.
- Mechanism: The LLM's pretraining on vast text corpora enables it to understand triple structure and map it into coherent sentences, leveraging its internalized knowledge of entity relations.
- Core assumption: The model's pretraining includes exposure to similar triple-based information, allowing it to generalize zero-shot.
- Evidence anchors:
  - [abstract] "Recent work has shown that models that make use of pretraining on large amounts of text data can perform well on the KG-to-text task..."
  - [section 2.2] "approaches that use pretraining on large amounts of non-KG-related text data... have shown promising results"
  - [corpus] Weak – no direct KG-to-text zero-shot evidence found in neighbors; only general LM capabilities.
- Break condition: If the triples contain information entirely absent from pretraining data, generation quality drops sharply.

### Mechanism 2
- Claim: Factualness of triples affects the model's ability to express them accurately.
- Mechanism: When triples align with the model's internalized facts, coverage is higher and hallucinations lower; when they conflict, the model may ignore, distort, or hallucinate.
- Core assumption: The LLM's internal knowledge base influences how it interprets and verbalizes input triples.
- Evidence anchors:
  - [section 3] "we compare factual, counter-factual and fictional statements, and show that there is a significant connection between what the LLM already knows..."
  - [section 4.3.2] "factualness did have an effect on how many triples were present, absent or hallucinated"
  - [corpus] Moderate – related papers note KG-based simplification improves faithfulness, suggesting alignment matters.
- Break condition: If factual triples are highly novel or contradictory to common knowledge, coverage and hallucination rates resemble fictional cases.

### Mechanism 3
- Claim: The two-step prompt structure improves coherence by allowing context accumulation.
- Mechanism: First, generate a sentence from the first triple; second, iteratively insert remaining triples into the growing text, keeping context window and coherence.
- Core assumption: Incremental generation with context retention is easier for the model than handling all triples at once.
- Evidence anchors:
  - [section 4.2] "we used a two-step prompt structure rather than the one-step method... The returned text from this second step was used as the KG-to-text output"
  - [section 4.3.1] Manual evaluation found comparable grammar/coherence across factual/fictional/counterfactual, suggesting prompt structure mitigates quality differences.
  - [corpus] Weak – no explicit prompt engineering studies in neighbors; general text coherence methods exist but not KG-specific.
- Break condition: If triple set is too large for context window, coherence degrades regardless of prompt structure.

## Foundational Learning

- Concept: Knowledge Graph structure (entities, relations, triples)
  - Why needed here: The system inputs and outputs are based on graph-structured knowledge; understanding this format is essential for prompt design and evaluation.
  - Quick check question: What distinguishes a knowledge graph triple from a simple sentence?

- Concept: Zero-shot learning
  - Why needed here: The LLM is used without fine-tuning on KG-to-text; understanding zero-shot is key to interpreting results and limitations.
  - Quick check question: How does zero-shot differ from few-shot or fine-tuning in LLM usage?

- Concept: Evaluation metrics for text generation (BLEU, METEOR, CHRF, BERTScore, BLEURT)
  - Why needed here: The paper compares model outputs using multiple metrics; knowing what each measures is critical for interpreting performance differences.
  - Quick check question: Which metric would best capture semantic similarity when exact wording differs?

## Architecture Onboarding

- Component map: Triple → Prompt → LLM → Output → Evaluation
- Critical path: Knowledge graph triples are formatted into prompts, processed by LLM, and evaluated through automated metrics and human annotation
- Design tradeoffs:
  - Single-step vs two-step prompting: Simpler but less coherent vs more complex but better context handling
  - Factual vs fictional/counterfactual data: Higher factual alignment yields better coverage but may hide hallucination issues
  - Manual vs crowd-sourced evaluation: Higher quality but lower scale vs higher scale but lower reliability
- Failure signatures:
  - High TER and low exact-match metrics but high METEOR/BERTScore → semantic alignment without surface form match
  - Low coherence scores across conditions → prompt structure issue
  - High hallucination rate → model over-relies on internal knowledge instead of input
- First 3 experiments:
  1. Run WebNLG test set through single-step prompt and compare metrics to baseline models.
  2. Apply two-step prompt to WikiData factual triples and measure coverage/hallucination rates.
  3. Vary prompt wording (e.g., add constraints like "only use information in triples") and observe hallucination changes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the factualness of knowledge graph triples affect the types of hallucinations generated by large language models (LLMs)?
- Basis in paper: [explicit] The paper explicitly compares factual, counterfactual, and fictional triples and their impact on hallucination types.
- Why unresolved: The paper shows that factualness affects the type of errors (hallucinations vs. omissions) but does not deeply analyze the specific nature of hallucinations across different factualness levels.
- What evidence would resolve it: Detailed analysis of the specific hallucinated content across factual, counterfactual, and fictional conditions to identify patterns or differences in the types of information added.

### Open Question 2
- Question: What are the optimal prompt engineering techniques for improving LLM performance on knowledge graph-to-text tasks?
- Basis in paper: [inferred] The paper uses simple prompts and notes that different prompt designs could significantly affect performance.
- Why unresolved: The study uses basic prompts and does not explore advanced prompt engineering strategies.
- What evidence would resolve it: Comparative evaluation of various prompt designs (e.g., few-shot examples, chain-of-thought prompting) on the same KG-to-text tasks.

### Open Question 3
- Question: How do larger or more advanced LLMs (e.g., GPT-4) perform on knowledge graph-to-text tasks compared to ChatGPT?
- Basis in paper: [explicit] The paper uses ChatGPT as a representative LLM and suggests that larger models may address some deficiencies.
- Why unresolved: The study only evaluates ChatGPT and does not compare it with other LLMs.
- What evidence would resolve it: Direct comparison of performance metrics (e.g., METEOR, BLEU) and hallucination rates between ChatGPT and larger models like GPT-4 on the same datasets.

## Limitations

- Reliance on proprietary LLM APIs without transparency into training data overlap, making data contamination effects impossible to rule out
- Evaluation framework depends heavily on human annotation, introducing subjectivity particularly in hallucination detection
- Two-step prompting approach lacks systematic comparison with alternative prompt engineering strategies

## Confidence

- High confidence: The observation that factualness affects triple coverage and hallucination rates, supported by clear manual evaluation results across all three conditions
- Medium confidence: The comparative performance on WebNLG metrics, as differences may be influenced by unknown factors in ChatGPT's training
- Low confidence: Generalizability to other KG domains or LLM architectures, given the single-model study design

## Next Checks

1. Test the same methodology with an open-weight LLM where training data can be verified, to control for potential data contamination
2. Implement systematic prompt ablation studies to identify which prompt components most influence hallucination rates
3. Conduct inter-annotator agreement analysis on a subset of evaluations to quantify reliability of manual hallucination detection