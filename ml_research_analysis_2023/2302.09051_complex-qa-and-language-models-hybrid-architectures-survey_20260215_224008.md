---
ver: rpa2
title: Complex QA and language models hybrid architectures, Survey
arxiv_id: '2302.09051'
source_url: https://arxiv.org/abs/2302.09051
tags:
- language
- learning
- complex
- knowledge
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of language model hybrid
  architectures for complex question answering (CQA). It identifies key skills, tasks,
  and evaluation metrics needed to address complex QA problems.
---

# Complex QA and language models hybrid architectures, Survey

## Quick Facts
- arXiv ID: 2302.09051
- Source URL: https://arxiv.org/abs/2302.09051
- Reference count: 40
- Key outcome: Comprehensive survey of hybrid language model architectures for complex question answering, identifying key skills, tasks, evaluation metrics, and architectural patterns to address LLM limitations.

## Executive Summary
This paper surveys hybrid architectures combining large language models with external modules to address complex question answering challenges. It identifies that while LLMs show promise, they struggle with reasoning, explainability, and handling sensitive data. The study explores various architectural patterns that integrate LLMs with semantic retrievers, code interpreters, and human-in-the-loop feedback systems, along with training techniques like prompt engineering and reinforcement learning to improve performance.

## Method Summary
The survey identifies complex QA tasks requiring reasoning, explanation, and multi-step problem solving. It proposes hybrid architectures combining base LLMs with external modules (retrievers, code interpreters, symbolic processors) and training approaches including prompt engineering and reinforcement learning with human feedback. The method involves selecting appropriate base models, implementing external modules, fine-tuning on task-specific datasets, and applying prompt engineering techniques to improve performance on complex QA tasks.

## Key Results
- LLM limitations in reasoning, explainability, and handling sensitive data necessitate hybrid architectural approaches
- Hybrid patterns combining LLMs with external modules show promise for improving complex QA performance
- Prompt engineering and reinforcement learning with human feedback are effective techniques for enhancing model capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid architectures improve LLM performance on complex QA by offloading specialized tasks to complementary modules.
- Mechanism: The base LLM handles general understanding while external modules handle tasks where LLMs struggle.
- Core assumption: LLMs have strong general language understanding but limited precision in specific domains.
- Evidence anchors:
  - [abstract] "To overcome these, the paper explores various hybrid architectural patterns that combine LLMs with external modules like semantic retrievers, code interpreters, and human-in-the-loop feedback systems."
  - [section] "LLM + Code interpreter : Generate code to delegate complex tasks well handled by compiler/solver, can also learn complex logics by learning program input/output."

### Mechanism 2
- Claim: Prompt engineering can rival or exceed model fine-tuning for specific complex QA tasks without computational overhead.
- Mechanism: Carefully engineered prompts provide context, examples, and reasoning chains that guide the LLM to better performance without retraining.
- Core assumption: LLMs have latent capabilities that can be unlocked through appropriate prompting without additional training.
- Evidence anchors:
  - [abstract] "It also examines training techniques such as prompt engineering and reinforcement learning to improve model performance."
  - [section] "Prompting techniques are now improving ability of language models on compositional generalization, Chain-of-thought prompting [268] provides relevant examples of multi-steps of reasoning/thoughts up to the solution to improve reliability or more easily spot errors in the result."

### Mechanism 3
- Claim: Human-in-the-loop reinforcement learning aligns LLM outputs with human values and expectations, reducing harmful or unhelpful responses.
- Mechanism: RLHF uses human preferences to fine-tune the model's policy, optimizing for helpfulness while minimizing harm.
- Core assumption: Human feedback provides meaningful signal for improving alignment beyond what automated metrics can capture.
- Evidence anchors:
  - [abstract] "The study finds that while large language models (LLMs) show promise, they have limitations in reasoning, explainability, and handling sensitive data... It also examines training techniques such as prompt engineering and reinforcement learning to improve model performance."
  - [section] "RLHF (reinforcement learning with human feedback) [ 36, 55, 178–181] and illustrated in the figure enhanced by an AI supervision process to better scale, reduce human workload and biases."

## Foundational Learning

- Concept: Question decomposition strategies
  - Why needed here: Complex questions often require breaking down into simpler sub-questions that can be solved individually and then combined.
  - Quick check question: Can you identify which components of a complex question could be handled by different specialized modules in a hybrid architecture?

- Concept: Evaluation metrics for complex QA
  - Why needed here: Standard metrics like exact match are insufficient for non-factoid, multi-step questions requiring reasoning and explanation.
  - Quick check question: What metrics would you use to evaluate a complex QA system's performance on a question requiring multi-hop reasoning and explanation?

- Concept: Hybrid architectural patterns
  - Why needed here: Understanding how different modules (LLM, retriever, code interpreter, etc.) can be combined is essential for designing effective complex QA systems.
  - Quick check question: Given a complex question requiring both factual verification and mathematical calculation, which hybrid pattern would you choose and why?

## Architecture Onboarding

- Component map: Base LLM (encoder-decoder or decoder-only) → Task-specific heads or adapters → External modules (retriever, code interpreter, symbolic processor) → Human feedback loop → Evaluation pipeline
- Critical path: Question understanding → Decomposition strategy → Module routing → Answer generation → Verification/Explanation → Human feedback
- Design tradeoffs: Model size vs. inference speed, module complexity vs. integration overhead, precision vs. generalization, training cost vs. inference efficiency
- Failure signatures: Hallucinations in generated answers, incorrect routing to inappropriate modules, excessive latency due to module coordination, failure to decompose complex questions appropriately
- First 3 experiments:
  1. Implement a simple hybrid pattern: Base LLM + code interpreter for mathematical reasoning tasks
  2. Test prompt engineering techniques (chain-of-thought, few-shot examples) on multi-step reasoning questions
  3. Evaluate different module combinations on a benchmark of complex questions requiring both factual retrieval and logical reasoning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal tradeoff between storing knowledge in the model versus retrieving it from external sources?
- Basis in paper: [explicit] The paper mentions this as a research topic for designing hybrid modules in section 8.
- Why unresolved: This tradeoff depends on factors like model size, retrieval efficiency, and task complexity, which vary across applications.
- What evidence would resolve it: Empirical studies comparing performance and efficiency of models with different knowledge storage/retrieval configurations across diverse tasks.

### Open Question 2
- Question: How can we effectively decompose very complex non-factoid questions into traceable reasoning steps?
- Basis in paper: [explicit] The paper highlights this as a key challenge in section 8.5, noting that most existing decomposition techniques focus on factoid questions.
- Why unresolved: Decomposing non-factoid questions requires domain-specific knowledge and a methodology that can be generalized across various fields.
- What evidence would resolve it: Development and validation of a framework for decomposing complex non-factoid questions, with successful application to multiple domains.

### Open Question 3
- Question: What are the most effective strategies for protecting sensitive data in language models?
- Basis in paper: [explicit] The paper identifies this as an under-researched area in section 8.4, discussing potential approaches like data access control and functional encryption.
- Why unresolved: Implementing data protection mechanisms without compromising model performance is challenging, and the effectiveness of different strategies is not well-established.
- What evidence would resolve it: Comparative analysis of data protection methods in terms of security, efficiency, and impact on model accuracy.

## Limitations
- Claims about hybrid architectures' effectiveness are supported by theoretical reasoning rather than empirical validation on standardized benchmarks
- Absence of quantitative comparisons between different architectural patterns and training techniques
- Limited discussion of scalability challenges and computational overhead of hybrid approaches

## Confidence
- Medium confidence in hybrid architectures improving LLM performance: Supported by literature identification of LLM limitations but lacking direct performance comparisons
- Medium confidence in prompt engineering effectiveness: Based on documented techniques but without systematic evaluation across diverse complex QA tasks
- Medium confidence in RLHF alignment benefits: Theoretical framework is well-established but scalability challenges and potential bias introduction are not fully addressed

## Next Checks
1. Implement a controlled experiment comparing hybrid architectures (LLM + code interpreter + retriever) against a fine-tuned standalone LLM on a standardized complex QA benchmark, measuring both accuracy and latency.

2. Conduct ablation studies on prompt engineering techniques across different complex QA categories (multi-hop reasoning, mathematical problem-solving, open-ended explanation) to quantify performance gains.

3. Evaluate hallucination rates and factuality scores for different hybrid patterns on a curated dataset of complex questions with verified ground truth answers, comparing approaches with and without verification modules.