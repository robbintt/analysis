---
ver: rpa2
title: Distilling Step-by-Step! Outperforming Larger Language Models with Less Training
  Data and Smaller Model Sizes
arxiv_id: '2305.02301'
source_url: https://arxiv.org/abs/2305.02301
tags:
- distilling
- arxiv
- step-by-step
- smaller
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Distilling step-by-step, a new paradigm for
  training smaller task-specific language models that outperform much larger language
  models (LLMs) while using significantly less training data and fewer parameters.
  The core idea is to leverage the reasoning capabilities of LLMs by prompting them
  with chain-of-thought to generate both output labels and natural language rationales
  explaining their predictions.
---

# Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes

## Quick Facts
- **arXiv ID**: 2305.02301
- **Source URL**: https://arxiv.org/abs/2305.02301
- **Reference count**: 11
- **Primary result**: Distilling step-by-step enables smaller models to outperform much larger LLMs using significantly less training data and fewer parameters.

## Executive Summary
This paper introduces Distilling step-by-step, a novel paradigm for training smaller task-specific language models that surpass the performance of much larger language models while requiring significantly less training data and fewer parameters. The approach leverages the reasoning capabilities of LLMs by using chain-of-thought prompting to generate both output labels and natural language rationales explaining their predictions. These rationales are then incorporated as additional supervision within a multi-task training framework, where the smaller model learns to predict both labels and rationales given the input text.

The key findings demonstrate that Distilling step-by-step consistently outperforms standard finetuning and distillation methods across four NLP benchmarks, achieving better performance with 50-85% less training data. Remarkably, it enables models up to 2000x smaller to surpass LLM performance, such as a 770M T5 model outperforming the 540B PaLM LLM on ANLI using only 80% of the data. Even with unlabeled data, Distilling step-by-step can match or exceed LLM performance using substantially smaller models, offering an efficient training-to-deployment paradigm that reduces both model size and training data requirements while achieving superior performance.

## Method Summary
Distilling step-by-step uses chain-of-thought prompting to elicit rationales from a large language model (LLM) as additional supervision for training smaller models. The LLM generates both labels and rationales for input examples, which are then used in a multi-task learning framework where the smaller model learns to predict both outputs simultaneously. This approach is tested across four NLP benchmarks (e-SNLI, ANLI, CQA, SVAMP) using T5 models of various sizes (220M, 770M, 11B) trained on PaLM-generated rationales, comparing performance against standard finetuning, distillation, and the LLM's few-shot performance.

## Key Results
- Distilling step-by-step consistently outperforms standard finetuning and distillation methods across 4 NLP benchmarks, achieving better performance with 50-85% less training data.
- It enables models up to 2000x smaller to surpass LLM performance, such as a 770M T5 model outperforming the 540B PaLM LLM on ANLI using only 80% of the data.
- Even with only unlabeled data, Distilling step-by-step can match or exceed LLM performance using substantially smaller models (e.g., 11B T5 vs 540B PaLM on CQA).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distilling step-by-step enables smaller models to outperform LLMs by extracting and leveraging rationales as additional supervision.
- Mechanism: The approach uses Chain-of-Thought (CoT) prompting to elicit rationales from LLMs, which are then incorporated into a multi-task learning framework where the smaller model learns to predict both labels and rationales.
- Core assumption: LLM-generated rationales contain richer, more detailed information about the mapping from inputs to outputs than labels alone.
- Evidence anchors:
  - [abstract] "Our method extracts LLM rationales as additional supervision for training small models within a multi-task framework."
  - [section] "Core to our mechanism is changing our perspective from viewing LLMs as more than a source of noisy labels to viewing them as agents that can reason."
- Break condition: If LLM-generated rationales are poor quality or misleading, the additional supervision could degrade performance rather than improve it.

### Mechanism 2
- Claim: Distilling step-by-step reduces the amount of training data needed compared to standard finetuning and distillation.
- Mechanism: By providing richer supervision through rationales, the smaller model can learn more efficiently from fewer examples, reducing the data requirement by 50-85%.
- Core assumption: Rationales provide more informative supervision than labels alone, enabling faster learning.
- Evidence anchors:
  - [abstract] "Our method extracts LLM rationales as additional supervision for training small models within a multi-task framework."
  - [section] "First, compared to both finetuning and distillation, our mechanism achieves better performance with over 50% less training examples on average across datasets."
- Break condition: If the rationale generation process becomes a bottleneck or if the additional supervision doesn't translate to better learning efficiency.

### Mechanism 3
- Claim: Distilling step-by-step enables smaller models to outperform LLMs while using much fewer parameters.
- Mechanism: By learning from both labels and rationales in a multi-task framework, smaller models can capture reasoning patterns that enable them to match or exceed LLM performance despite being 2000x smaller.
- Core assumption: The reasoning patterns captured in rationales can be effectively distilled into smaller models.
- Evidence anchors:
  - [abstract] "Distilling step-by-step allows us to learn task-specific smaller models that outperform LLMs using over 500× less model parameters."
  - [section] "Compared with Eq. 2, the rationale ri is not required in the test time, which removes the need for an LLM at test-time."
- Break condition: If the rationale generation process is too resource-intensive or if the smaller model cannot effectively capture the reasoning patterns.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) prompting**
  - Why needed here: CoT prompting is essential for eliciting rationales from LLMs, which are the core additional supervision used in Distilling step-by-step.
  - Quick check question: What is the primary purpose of using CoT prompting in this framework?

- **Concept: Multi-task learning framework**
  - Why needed here: The multi-task learning framework allows the smaller model to learn from both labels and rationales simultaneously, enabling more efficient learning.
  - Quick check question: How does the multi-task learning framework differ from standard finetuning in this context?

- **Concept: Knowledge distillation**
  - Why needed here: Understanding knowledge distillation is crucial for grasping how Distilling step-by-step extends traditional distillation methods by incorporating rationales.
  - Quick check question: What is the key difference between standard distillation and Distilling step-by-step?

## Architecture Onboarding

- **Component map**: LLM (teacher) -> CoT prompting -> Rationale generation -> Multi-task training -> Smaller model (student)
- **Critical path**: LLM → CoT prompting → Rationale generation → Multi-task training → Smaller model deployment
- **Design tradeoffs**:
  - Benefit: Reduced model size and data requirements
  - Cost: Additional computation for rationale generation
  - Risk: Quality of LLM-generated rationales affects downstream performance
- **Failure signatures**:
  - Poor performance despite high-quality LLM rationales
  - Inability to scale to more complex reasoning tasks
  - Bottlenecks in rationale generation process
- **First 3 experiments**:
  1. Implement basic CoT prompting on a small dataset to verify rationale generation
  2. Test multi-task learning framework with synthetic rationales to validate the approach
  3. Compare performance against standard finetuning on a simple NLP benchmark

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions.

## Limitations
- Quality and Robustness of LLM-Generated Rationales: The paper assumes LLM-generated rationales are of high quality and provide meaningful additional supervision, but this is not thoroughly validated.
- Computational Cost of Rationale Generation: While the approach reduces model size and data requirements, it introduces an additional computational step - generating rationales using a large LLM.
- Generalization to Other Tasks and Domains: The experiments are limited to 4 NLP benchmarks, making it unclear how well Distilling step-by-step would generalize to other tasks and domains.

## Confidence

**High Confidence**: The core methodology of using LLM-generated rationales as additional supervision in a multi-task framework is clearly described and the experimental setup is reproducible.

**Medium Confidence**: The reported performance improvements (50-85% less training data, 2000x smaller models) are based on the specific experiments conducted, but the generalizability to other tasks and domains is uncertain.

**Low Confidence**: The claim that Distilling step-by-step can outperform LLMs with models up to 2000x smaller is based on a limited set of experiments and may not hold across all tasks and domains.

## Next Checks

1. **Rationales Quality Analysis**: Conduct a thorough analysis of the quality of LLM-generated rationales across different tasks and LLMs using both automated metrics and human evaluation.

2. **Computational Cost Analysis**: Perform a comprehensive analysis of the computational trade-offs, including the cost of generating rationales and the training time for smaller models, compared to standard finetuning and distillation methods.

3. **Generalization Experiments**: Test Distilling step-by-step on a diverse set of NLP tasks and domains, including those requiring different types of reasoning and domain-specific knowledge, to evaluate the approach's robustness and generalizability.