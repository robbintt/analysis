---
ver: rpa2
title: 'EnCodecMAE: Leveraging neural codecs for universal audio representation learning'
arxiv_id: '2309.07391'
source_url: https://arxiv.org/abs/2309.07391
tags:
- audio
- masked
- speech
- learning
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EnCodecMAE, a self-supervised audio representation
  learning method that uses a masked autoencoder architecture with discrete targets
  from EnCodec's neural codec. The model masks and reconstructs audio embeddings to
  learn universal representations across speech, music, and environmental sounds.
---

# EnCodecMAE: Leveraging neural codecs for universal audio representation learning

## Quick Facts
- arXiv ID: 2309.07391
- Source URL: https://arxiv.org/abs/2309.07391
- Reference count: 0
- Primary result: Self-supervised audio representation learning method achieving 91.8% accuracy on NSynth pitch classification and strong performance across speech, music, and environmental sound tasks

## Executive Summary
EnCodecMAE introduces a masked autoencoder (MAE) architecture for learning universal audio representations by reconstructing masked audio embeddings using discrete targets from EnCodec's neural codec. The model masks 50% of audio frames with 15-frame gaps, then trains to predict discrete tokens from 8 of EnCodec's 32 codebooks. After initial pretraining, a self-training stage clusters internal representations to generate additional targets. The approach achieves competitive performance on HEAREval benchmarks, outperforming state-of-the-art models particularly in music-related tasks while maintaining strong results in speech and environmental sound domains.

## Method Summary
The method uses a masked autoencoder architecture where audio signals are first processed by EnCodec's encoder to produce 128-dimensional embeddings. These embeddings undergo random masking (50% probability, 15-frame gaps), with masked frames discarded. The remaining embeddings pass through a transformer encoder (4-20 layers depending on model size) with positional encoding, then through a 2-layer transformer decoder to predict posteriors for discrete targets corresponding to the masked regions. Training uses weighted cross-entropy loss over 8 EnCodec codebooks. After 500k training steps, a self-training stage extracts embeddings from the last encoder layer, clusters them using k-means (k=1024), and adds these as additional target streams for 150k more steps.

## Key Results
- Achieves 91.8% accuracy on NSynth pitch classification, outperforming state-of-the-art models
- Maintains strong performance across HEAREval tasks: Google Speech Commands (98.2%), CREMA-D emotion classification (64.4%), FSD (mAP 0.466), ESC-50 (90.8%)
- Demonstrates efficient training, completing pretraining in 5 days on two RTX 3090 GPUs
- Shows that EnCodec's discrete representations effectively capture information across speech, music, and environmental sounds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Masking and reconstructing audio embeddings using MAE architecture enables learning universal audio representations
- Mechanism: By randomly masking portions of input audio embeddings and training the model to reconstruct these masked regions, MAE learns to capture essential relationships across different audio types
- Core assumption: Relationships between unmasked and masked portions contain sufficient information to learn meaningful representations that generalize across audio domains
- Evidence anchors: [abstract] "masking representations of the audio signal, and training a MAE to reconstruct the masked segments"; [section] "We use the Masked Autoencoder (MAE) architecture [18], which enables efficient pretraining"
- Break condition: If masked regions contain information not sufficiently correlated with unmasked regions, reconstruction becomes impossible and model cannot learn meaningful representations

### Mechanism 2
- Claim: Using discrete targets from EnCodec's neural codec as reconstruction objectives improves learned audio representations
- Mechanism: Discrete tokens from EnCodec's residual vector quantization provide structured, compressed representation capturing perceptually relevant information across audio domains
- Core assumption: EnCodec's discrete representations preserve essential information needed for high-quality reconstruction across different audio domains
- Evidence anchors: [abstract] "The reconstruction is done by predicting the discrete units generated by EnCodec, a neural audio codec, from the unmasked inputs"; [section] "EnCodec achieves a high compression rate while also minimizing the perceptible distortion"
- Break condition: If EnCodec's discrete representations are not truly universal and fail to capture important information for certain audio types, learned representations will be biased toward domains where EnCodec performs well

### Mechanism 3
- Claim: Self-training stage clustering internal representations improves model's ability to learn universal audio representations
- Mechanism: After initial pretraining, model generates its own discrete targets by clustering internal representations, discovering additional structure not captured by EnCodec's tokens alone
- Core assumption: Internal representations contain meaningful structure that can be captured by clustering, and this additional structure is useful for learning better audio representations
- Evidence anchors: [section] "after 500k training steps, a self-training stage is performed. We extract embeddings from the last encoder layer for 10k randomly-sampled audio signals and train a k-means model with k = 1024"
- Break condition: If internal representations are not sufficiently structured or clustering introduces noise rather than meaningful structure, additional discrete targets may degrade learned representations

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: MAE architecture inspired by BERT's MLM objective, successful in learning contextual representations for text; understanding MLM explains why masking and reconstructing audio segments can be effective pretraining strategy
  - Quick check question: What is the key difference between how BERT and MAE handle masked regions during training?

- Concept: Vector Quantization (VQ)
  - Why needed here: EnCodec uses residual vector quantization to map continuous embeddings to discrete tokens; understanding VQ is crucial for grasping how discrete targets are generated and why they are useful for reconstruction task
  - Quick check question: How does residual vector quantization differ from standard vector quantization, and why might it be more effective for audio compression?

- Concept: Transfer Learning
  - Why needed here: Pretrained EnCodecMAE model used as feature extractor for downstream tasks; understanding transfer learning explains why pretraining on large, diverse dataset can improve performance on smaller, task-specific datasets
  - Quick check question: What are advantages and potential drawbacks of using frozen pretrained model as feature extractor versus fine-tuning entire model for downstream tasks?

## Architecture Onboarding

- Component map: Audio signal → EnCodec encoder → Embeddings → Masking (Mprop = 0.5, Mgap = 15) → MAE encoder → MAE decoder → Posteriors for discrete targets

- Critical path: 1) Audio signal → EnCodec encoder → Embeddings; 2) Embeddings + positional encoding → Masking → Discarding masked embeddings; 3) Remaining embeddings → MAE encoder → Expanded sequence → MAE decoder; 4) MAE decoder → Posteriors for discrete targets → Weighted cross-entropy loss

- Design tradeoffs: Masking strategy (entire segments vs. fine-grained masking) vs. quantization (EnCodec's discrete targets vs. k-means clustering vs. both) vs. architecture (asymmetric encoder-decoder vs. symmetric) vs. training (two-stage vs. single-stage)

- Failure signatures: Poor reconstruction quality (issues with masking strategy, encoder architecture, or quantization method) vs. overfitting to certain audio domains (pretraining dataset not sufficiently diverse or model architecture too specialized) vs. degradation in performance after self-training (problems with clustering process or quality of additional discrete targets)

- First 3 experiments: 1) Train small EnCodecMAE model on subset of pretraining data (only speech or only music) and evaluate performance on downstream task to verify masking and reconstruction strategy works in simpler setting; 2) Compare performance using only EnCodec's discrete targets vs. only self-training targets to understand contribution of each component; 3) Experiment with different masking proportions (Mprop) and gap sizes (Mgap) to find optimal masking strategy for universal audio representation learning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance of EnCodecMAE vary when using different numbers of codebooks from EnCodec's RVQ for target generation?
- Basis in paper: [explicit] The paper states that "As targets for training we used 8 of the 32 quantizers available in EnCodec since all relevant acoustic information seemed to be preserved when listening to reconstructed speech, music and environmental sounds"
- Why unresolved: The paper only mentions using 8 codebooks without exploring impact of using more or fewer codebooks on model performance across different audio tasks
- What evidence would resolve it: Systematic experiments comparing model performance using different numbers of codebooks (4, 8, 16, 32) across various audio tasks would clarify optimal codebook configuration

### Open Question 2
- Question: What is impact of using spectrogram patches as input instead of one-dimensional audio signals on EnCodecMAE's performance, particularly for environmental sound tasks?
- Basis in paper: [inferred] The paper mentions that "BYOL-A paper, where using random crop and resize of spectrograms gives better results for Urban-sound8K than augmentations that do not manipulate the 2D structure of spectrograms" and references other work using spectrogram patches
- Why unresolved: The paper uses one-dimensional signals as input, but evidence suggests 2D spectrogram structures might be beneficial for certain tasks, especially environmental sounds
- What evidence would resolve it: Comparative experiments using both one-dimensional audio signals and 2D spectrogram patches as inputs, evaluating performance across all task categories

### Open Question 3
- Question: How does performance of EnCodecMAE change when applying different fine-tuning strategies (freezing different numbers of layers, using different downstream architectures) for various downstream tasks?
- Basis in paper: [explicit] The paper states that "We evaluate our models following the HEAREval procedure and a subset of its instance-level tasks. The embedding for each instance is obtained by averaging the frame-level activations from the last encoder layer. The resulting embedding is fed to a multilayer perceptron and a grid search over hyperparameters is performed"
- Why unresolved: The paper uses fixed fine-tuning strategy (freezing upstream model and using multilayer perceptron), but different tasks might benefit from different fine-tuning approaches
- What evidence would resolve it: Experiments comparing various fine-tuning strategies, including full fine-tuning, partial layer freezing, and different downstream architectures, across all evaluated tasks

## Limitations

- Architecture specification gap: EnCodec's encoder architecture (number of CNN/LSTM layers, filter dimensions) not fully specified, creating uncertainty about exact feature extraction pipeline
- Evaluation scope limited: Strong performance on HEAREval benchmarks but evaluation focuses on classification tasks rather than generation or real-time applications
- Reproducibility constraints: Pretraining dataset mixture specified but exact preprocessing parameters and data splits not provided; self-training stage lacks details about initialization and convergence criteria

## Confidence

- High Confidence: Core claim that EnCodecMAE learns useful audio representations well-supported by competitive downstream task performance (91.8% NSynth accuracy, strong speech and environmental sound results)
- Medium Confidence: Claim that EnCodec's discrete targets are superior for universal representation learning requires more direct comparison with alternative quantization methods
- Low Confidence: Assertion that self-training stage significantly improves performance lacks direct ablation evidence

## Next Checks

1. Implement controlled ablation study comparing three variants: (1) EnCodecMAE with only EnCodec targets, (2) EnCodecMAE with only self-training targets, and (3) full two-stage approach; evaluate all three on NSynth pitch classification to isolate contribution of each quantization method

2. Test pretrained EnCodecMAE model on non-classification downstream task, such as audio generation quality or similarity retrieval, to validate claims of "universal" representation learning beyond classification accuracy

3. Reproduce model using alternative masking strategies (different Mprop values like 0.3 or 0.7, different gap sizes) to determine sensitivity of performance to these hyperparameters and identify optimal masking configurations for universal audio representation learning