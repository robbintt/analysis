---
ver: rpa2
title: On the Effects of Regional Spelling Conventions in Retrieval Models
arxiv_id: '2308.00480'
source_url: https://arxiv.org/abs/2308.00480
tags:
- spelling
- retrieval
- document
- conventions
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effect of regional spelling differences
  on neural retrieval models, focusing on American and British English conventions.
  The authors first analyze the prevalence of spelling conventions in common training
  datasets (MSMARCO and C4) and find a clear preference for American English.
---

# On the Effects of Regional Spelling Conventions in Retrieval Models

## Quick Facts
- arXiv ID: 2308.00480
- Source URL: https://arxiv.org/abs/2308.00480
- Authors: 
- Reference count: 20
- Key outcome: This paper investigates the effect of regional spelling differences on neural retrieval models, focusing on American and British English conventions. The authors first analyze the prevalence of spelling conventions in common training datasets (MSMARCO and C4) and find a clear preference for American English. They then test various retrieval models (lexical, dense, and re-rankers) on queries containing only British or American spelling conventions, finding that models generally generalise well across these differences. The study further examines the impact of document spelling normalisation, observing that all models are affected when documents are normalised to a different convention than the query, with varied effects when normalised to match the query. Lexical models improve, dense retrievers remain unaffected, and re-rankers exhibit contradictory behaviour.

## Executive Summary
This paper investigates how regional spelling differences (American vs. British English) affect neural retrieval models. The authors find that despite a clear preference for American English in common training datasets, retrieval models generalize well across spelling conventions. They examine the impact of document spelling normalization and observe varied effects across different model types. The study provides insights into the robustness of neural retrieval models and the potential benefits and drawbacks of document normalization in handling regional spelling differences.

## Method Summary
The study analyzes the prevalence of spelling conventions in MSMARCO and C4 datasets, then tests various retrieval models (BM25, ELECTRA, monoT5, TCT-ColBERT, TaS-B, SPLADE, ColBERT) on queries with British or American spelling conventions. Document normalization is applied to match or mismatch query spelling conventions. Performance is measured using MRR@10 and R@1000 metrics, with statistical analysis via Wilcoxon signed-rank test and TOST. The experiments are conducted using PyTerrier and the Breame package for spelling detection.

## Key Results
- Neural retrieval models generalize well across American and British spelling conventions despite training data biases
- Document normalization affects retrieval performance differently based on model type: lexical models improve, dense retrievers remain unaffected, and re-rankers exhibit contradictory behavior
- All models prefer query-document pairs with matching spelling conventions but show no preference for normalization over leaving documents unchanged

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural retrieval models generalize well across regional spelling differences due to shared semantic content.
- Mechanism: The models are trained on large datasets with mixed spelling conventions, allowing them to learn robust semantic embeddings that are not overly sensitive to surface-level spelling differences.
- Core assumption: The semantic meaning of words is preserved across spelling variants, and the model's training data contains sufficient examples of both conventions to learn this invariance.
- Evidence anchors:
  - [abstract]: "One advantage of neural ranking models is that they are meant to generalise well in situations of synonymity i.e. where two words have similar or identical meanings."
  - [section]: "Despite these biases in the training data, we find that retrieval approaches are robust enough to generalise."
  - [corpus]: Weak evidence; corpus neighbors show no directly related papers on synonymity generalization.
- Break condition: If the training data lacks sufficient examples of one spelling convention, the model may not learn robust representations for that variant.

### Mechanism 2
- Claim: Document normalization affects retrieval performance differently based on model type.
- Mechanism: Lexical models (e.g., BM25) rely on exact term matching, so normalization improves recall when document and query share the same spelling. Dense retrievers use semantic embeddings, making them less sensitive to spelling differences. Re-rankers may rely on fine-tuned token-level interactions, leading to inconsistent behavior.
- Core assumption: Different retrieval architectures encode and match text in fundamentally different ways, leading to varied responses to spelling normalization.
- Evidence anchors:
  - [abstract]: "While they all experience a drop in performance when normalised to a different spelling convention than that of the query, we observe varied behaviour when the document is normalised to share the query spelling convention: lexical models show improvements, dense retrievers remain unaffected, and re-rankers exhibit contradictory behaviour."
  - [section]: "We observe that while all retrieval methods are harmed by normalising the documents and queries to different regional spelling conventions all three types of retrieval models (lexical, neural re-rankers and dense retrievers) are affected in different ways by normalising the documents to match the queries' regional spelling convention."
  - [corpus]: Weak evidence; no corpus papers directly address model-type-specific normalization effects.
- Break condition: If a model's architecture changes (e.g., a dense retriever with explicit lexical matching), the observed normalization effects may not hold.

### Mechanism 3
- Claim: Neural models prefer query-document pairs where spelling conventions match, but do not prefer normalization over leaving documents unchanged.
- Mechanism: The model's scoring function implicitly favors consistency in surface form, but the gain from normalization is offset by potential loss of original document context or style.
- Core assumption: Matching spelling conventions provides a small but consistent signal for relevance, but normalization introduces noise or removes stylistic cues that the model has learned to associate with relevance.
- Evidence anchors:
  - [abstract]: "we observe that even though all neural retrieval models have a very clear preference for the document sharing the query spelling convention over the document having a different spelling convention, they show no preference for normalising the document to share the query spelling over just using the document as-is."
  - [section]: "We observe that in both British and American query-qrel pairs all the models score most pairs the same regardless of whether the document is normalised to the query spelling or left un-normalised."
  - [corpus]: No relevant corpus evidence found.
- Break condition: If the normalization process is improved to preserve more contextual information, the model's ambivalence may shift toward preferring normalization.

## Foundational Learning

- Concept: Synonymity and semantic similarity in neural models
  - Why needed here: The study hinges on whether spelling differences constitute a form of synonymity that neural models can handle.
  - Quick check question: If "color" and "colour" are treated as semantically equivalent by a model, does that mean the model has learned to generalize across spelling conventions?

- Concept: Document and query normalization techniques
  - Why needed here: The experiments involve systematically altering the spelling of documents and queries to test model robustness.
  - Quick check question: What are the potential side effects of normalizing all documents in a corpus to a single spelling convention?

- Concept: Retrieval model architectures (lexical, dense, re-ranker)
  - Why needed here: The study compares how different model types respond to spelling normalization, requiring understanding of their underlying mechanisms.
  - Quick check question: How does a dense retriever's use of semantic embeddings differ from a lexical model's reliance on exact term matching?

## Architecture Onboarding

- Component map: Query → Preprocessing (spelling detection/normalization) → Index lookup → Scoring → Ranking → Evaluation
- Critical path: Query → Preprocessing → Index lookup → Scoring → Ranking → Evaluation
- Design tradeoffs: Normalization can improve recall for lexical models but may harm re-rankers; dense retrievers are robust but computationally expensive; re-rankers add precision but are sensitive to input form
- Failure signatures: Performance drops when query and document spelling conventions mismatch; inconsistent behavior across model types; unexpected drops in MRR for re-rankers after normalization
- First 3 experiments:
  1. Run BM25 on queries and documents with matching spelling conventions vs. mismatched conventions
  2. Test a dense retriever (e.g., TCT-ColBERT) with and without document normalization to confirm robustness
  3. Probe a re-ranker's scoring behavior on query-document pairs with varying normalization to identify conditions causing MRR drops

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we mitigate the negative effects of document normalization when the document's spelling convention differs from the query's?
- Basis in paper: [explicit] The paper finds that all models are affected by normalizing documents to a different spelling convention than the query, with varied effects when normalized to match the query.
- Why unresolved: The paper identifies the problem but does not propose solutions to mitigate these negative effects.
- What evidence would resolve it: Experimental results showing improved performance of retrieval models after implementing normalization mitigation strategies.

### Open Question 2
- Question: Why do neural re-rankers exhibit contradictory behavior when documents are normalized to match the query's spelling convention?
- Basis in paper: [explicit] The paper observes that neural re-rankers are harmed when documents are normalized to match the query's spelling convention, which is unexpected.
- Why unresolved: The paper notes this unexpected behavior but does not investigate the underlying reasons for it.
- What evidence would resolve it: Analysis of neural re-ranker behavior that explains why matching spelling conventions leads to decreased performance.

### Open Question 3
- Question: How do regional spelling conventions affect retrieval performance in languages other than English?
- Basis in paper: [inferred] The paper focuses on American and British English, suggesting the need to explore other languages and spelling variations.
- Why unresolved: The study is limited to English and does not investigate other languages or spelling variations.
- What evidence would resolve it: Comparative studies of retrieval performance across different languages with regional spelling conventions.

## Limitations

- The study is limited to American and British English, which may not generalize to other languages or spelling variants
- The Breame package used for spelling detection may not capture all regional variants or domain-specific terminology
- Effect sizes are not reported, making it difficult to assess the practical significance of the observed performance changes

## Confidence

- **High**: Neural models generalize across spelling conventions (supported by multiple experiments and consistent patterns)
- **Medium**: Document normalization effects vary by model type (observed patterns are clear but mechanisms are not fully explained)
- **Medium**: No preference for normalization over leaving documents unchanged (statistically supported but counterintuitive for some model types)

## Next Checks

1. Test the same experimental setup on additional corpora (e.g., BEIR, TREC datasets) to verify robustness across different domains and languages
2. Conduct ablation studies on document normalization to identify which specific spelling transformations cause performance changes
3. Measure effect sizes alongside statistical significance to assess practical impact of spelling normalization on retrieval performance