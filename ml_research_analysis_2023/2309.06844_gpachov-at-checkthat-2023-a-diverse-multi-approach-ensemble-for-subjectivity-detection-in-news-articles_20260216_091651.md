---
ver: rpa2
title: 'Gpachov at CheckThat! 2023: A Diverse Multi-Approach Ensemble for Subjectivity
  Detection in News Articles'
arxiv_id: '2309.06844'
source_url: https://arxiv.org/abs/2309.06844
tags:
- embeddings
- english
- sentence
- were
- subjectivity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the solution by the Gpachov team for the CLEF-2023
  CheckThat! lab Task 2 on subjectivity detection in news articles.
---

# Gpachov at CheckThat! 2023: A Diverse Multi-Approach Ensemble for Subjectivity Detection in News Articles

## Quick Facts
- arXiv ID: 2309.06844
- Source URL: https://arxiv.org/abs/2309.06844
- Reference count: 18
- Achieved 2nd place on the English subtask with 0.77 macro F1 score

## Executive Summary
This paper presents the Gpachov team's solution for the CLEF-2023 CheckThat! Lab Task 2 on subjectivity detection in news articles. The team explored three distinct research directions: fine-tuning a sentence embeddings encoder with dimensionality reduction, a sample-efficient few-shot learning model (SetFit), and fine-tuning a multilingual transformer (XLM-RoBERTa) on an altered dataset using data from multiple languages. These approaches were combined into a simple majority voting ensemble, resulting in a 0.77 macro F1 score on the test set and achieving 2nd place on the English subtask.

## Method Summary
The method combines three different approaches through majority voting ensemble: (1) a fine-tuned sentence embeddings encoder with dimensionality reduction using PCA from 384 to 110 dimensions, followed by logistic regression classification, (2) a few-shot learning SetFit model with dual-stage fine-tuning that achieves competitive results with minimal training data, and (3) an xlm-roberta-base model fine-tuned on English and German translated to English. The multilingual approach showed slight improvement over using English-only dataset, achieving 0.83 macro F1 on validation.

## Key Results
- Achieved 2nd place on English subtask with 0.77 macro F1 score
- Ensemble combining three approaches outperformed individual methods
- Multilingual training (English + German) improved performance over monolingual training
- Few-shot learning achieved competitive results with only 20 training examples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning a pre-trained transformer on an altered dataset using multilingual data improves performance over monolingual training.
- Mechanism: The model learns richer semantic representations by being exposed to more diverse linguistic patterns across languages, and fine-tuning aligns these representations to the specific subjectivity detection task.
- Core assumption: The semantic features useful for subjectivity detection are transferable across languages and benefit from multilingual exposure.
- Evidence anchors:
  - [abstract] "fine-tuning a multilingual transformer on an altered dataset, using data from multiple languages"
  - [section 4.3] "An xlm-roberta-base model was fine-tuned using all available data, achieving a macro F1 score of 0.83, which showed slight improvement over using English-only dataset"
- Break condition: If the multilingual data introduces significant noise or domain mismatch, performance could degrade.

### Mechanism 2
- Claim: Dimensionality reduction of sentence embeddings improves classifier performance by reducing noise and computational complexity.
- Mechanism: PCA reduces the 384-dimensional sentence embeddings to 110 dimensions while preserving 92.5% of variance, making it easier for classifiers to find decision boundaries.
- Core assumption: High-dimensional embeddings contain redundant or noisy information that doesn't contribute to the classification task.
- Evidence anchors:
  - [section 4.1] "Using dimensionality reduction, information from embedding vectors can be further compressed in a way that could make it easier for classifiers to find a proper decision boundary"
  - [section 4.1] "Best performance was achieved using PCA with 110 remaining components (out of 384), which explained 92.5% of total variance"
- Break condition: If too much variance is discarded (more than 7.5% in this case), important discriminative features may be lost.

### Mechanism 3
- Claim: Few-shot learning with dual-stage fine-tuning achieves competitive results with minimal training data.
- Mechanism: SetFit first fine-tunes embeddings in a contrastive learning manner, then fine-tunes the classification head, achieving good performance with only 20 training examples.
- Core assumption: The pre-trained model has learned general language representations that can be adapted to specific tasks with minimal task-specific data.
- Evidence anchors:
  - [section 4.2] "Results are similar to fine-tuning sentence embeddings encoder, but achieved while using a lot less information (samples)"
- Break condition: If the task requires very specialized knowledge not captured in the pre-training, few-shot learning may fail.

## Foundational Learning

- Concept: Contrastive learning for sentence embeddings
  - Why needed here: The paper uses contrastive learning to fine-tune sentence embeddings by creating similarity pairs based on both semantic and label-based similarity
  - Quick check question: How does the paper create training pairs for fine-tuning the sentence embeddings encoder?

- Concept: Ensemble methods in classification
  - Why needed here: The final solution combines three different approaches (sentence embeddings, few-shot learning, transformer) using majority voting to improve overall performance
  - Quick check question: What is the voting mechanism used in the ensemble, and how does it handle ties?

- Concept: Cross-lingual transfer learning
  - Why needed here: The paper explores using data from multiple languages (English, German, Arabic, Turkish) to improve the English subjectivity detection model
  - Quick check question: Which languages were found to be most beneficial for improving English subjectivity detection performance?

## Architecture Onboarding

- Component map: News article sentences → Sentence embedding encoder (fine-tuned SBERT) → Dimensionality reduction (PCA 384→110) → Classifier (Logistic Regression) + Few-shot learning model (SetFit) + Multilingual transformer (XLM-RoBERTa) → Majority voting ensemble

- Critical path: Sentence → Embedding → Dimensionality Reduction → Classification (for the SBERT path)

- Design tradeoffs:
  - Using PCA reduces dimensionality but may lose some information; the tradeoff is between computational efficiency and potential loss of discriminative features
  - Few-shot learning requires less data but may not capture task-specific nuances as well as full fine-tuning
  - Multilingual training improves performance but introduces potential noise from non-target language data

- Failure signatures:
  - Large gap between validation and test performance (overfitting)
  - One ensemble component consistently disagreeing with others
  - Performance degradation when adding more languages to training

- First 3 experiments:
  1. Test different dimensionality reduction ratios (e.g., 50, 100, 150 components) to find optimal balance
  2. Compare single-stage vs dual-stage fine-tuning for the few-shot learning approach
  3. Test different multilingual transformer models (BERT multilingual vs XLM-RoBERTa) with varying amounts of non-English data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of fine-tuned sentence embeddings compare to fine-tuning transformer models when using the same amount of training data?
- Basis in paper: [explicit] The paper mentions that fine-tuning sentence embeddings achieved similar results to few-shot learning with less data, but doesn't directly compare to transformer models with equivalent data sizes
- Why unresolved: The paper only tested fine-tuning transformers on the full English dataset, while sentence embeddings were tested with varying amounts of contrastive learning pairs (N=10, 20, 64, 100)
- What evidence would resolve it: Experiments comparing fine-tuned sentence embeddings to transformers when both are trained on identical, limited amounts of subjectivity detection data

### Open Question 2
- Question: What is the optimal balance between maintaining pre-trained semantic knowledge and task-specific specialization in sentence embedding fine-tuning?
- Basis in paper: [inferred] The authors note that fine-tuned SBERT embeddings performed worse on the test set than validation, suggesting potential overfitting and loss of general semantic capabilities
- Why unresolved: The paper doesn't explore techniques to maintain general semantic knowledge while fine-tuning, such as regularization methods or hybrid training approaches
- What evidence would resolve it: Comparative experiments testing different fine-tuning strategies (learning rates, regularization, early stopping) on both in-domain and out-of-domain test sets

### Open Question 3
- Question: How does multilingual training data affect subjectivity detection performance compared to monolingual training, and which language combinations are most beneficial?
- Basis in paper: [explicit] The paper tested multilingual training with different language combinations (all languages, English/Arabic/Turkish, English/German translated to English) but only on validation data
- Why unresolved: The paper only tested multilingual training on the English validation set, not on other languages or the final test set
- What evidence would resolve it: Testing multilingual models on all language test sets to determine which language combinations provide the most consistent improvements across different languages and which languages are most helpful for others

## Limitations
- Single train/validation split without cross-validation limits assessment of result stability
- Critical hyperparameters for SetFit model and translation process are not fully specified
- Majority voting ensemble mechanism doesn't specify tie-breaking procedures or analyze component disagreements

## Confidence
- **High Confidence**: The general approach of using an ensemble combining sentence embeddings, few-shot learning, and multilingual transformers is technically sound and well-explained
- **Medium Confidence**: The specific implementation details for the SetFit model and the exact translation process introduce uncertainty about reproducibility
- **Low Confidence**: Without cross-validation or multiple runs, the stability of the reported scores is uncertain

## Next Checks
1. Run 5-fold cross-validation on the English dataset to assess the stability of performance across different splits and provide confidence intervals
2. Analyze the confusion matrices and prediction distributions for each ensemble component to identify when models disagree and whether certain types of sentences are consistently misclassified
3. Conduct a controlled experiment comparing the multilingual model's performance with and without the translated German data to quantify the exact contribution of cross-lingual transfer