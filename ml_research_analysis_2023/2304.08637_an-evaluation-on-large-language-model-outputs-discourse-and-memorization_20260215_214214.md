---
ver: rpa2
title: 'An Evaluation on Large Language Model Outputs: Discourse and Memorization'
arxiv_id: '2304.08637'
source_url: https://arxiv.org/abs/2304.08637
tags:
- text
- memorized
- discourse
- output
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates memorization in nine large language models
  using readily-available plagiarism detection tools and human annotation. We find
  that 80% of model outputs contain memorized text, with GPT-3.51 producing the highest-quality
  discourse despite also having the highest memorization rate (89%).
---

# An Evaluation on Large Language Model Outputs: Discourse and Memorization

## Quick Facts
- arXiv ID: 2304.08637
- Source URL: https://arxiv.org/abs/2304.08637
- Reference count: 40
- This study evaluates memorization in nine large language models using plagiarism detection tools and human annotation, finding that 80% of outputs contain memorized text.

## Executive Summary
This study evaluates memorization in large language models by analyzing 75 texts per model across 9 LLMs using 15 prompts spanning 5 domains. The researchers employed 5 plagiarism detection tools and human annotation to assess discourse quality and memorization. They found that 80% of model outputs contained memorized text, with GPT-3.51 showing the highest discourse quality despite having the highest memorization rate (89%). The study also discovered a strong anti-correlation between factual errors and memorized text, suggesting memorization may contribute to factual accuracy. Additionally, explicit prompting to avoid memorized content significantly reduced plagiarism detection while maintaining discourse quality.

## Method Summary
The study evaluated 9 large language models (BLOOM, ChatGPT, Galactica, GPT-3.5, GPT-3.51, GPT-4, LLaMA, OPT, OPT-IML) using 15 prompts across 5 domains (scientific papers, blog posts, knowledge retrieval, long text autocompletion, and common text openers). For each model-prompt combination, outputs were generated and evaluated using 5 plagiarism detection services and human annotation for factual errors, logical inconsistencies, discourse quality, and PII presence. The evaluation measured memorization rates, discourse quality metrics, and examined relationships between prompt length and memorization.

## Key Results
- 80% of model outputs contained memorized text, with GPT-3.51 showing the highest memorization rate at 89%
- Strong anti-correlation between factual errors and memorized text for several models
- Shorter prompts led to higher memorization rates across all evaluated models
- Explicit anti-memorization prompts significantly reduced plagiarism detection while maintaining discourse quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High memorization rate is strongly anti-correlated with factual errors, suggesting memorization contributes to factual accuracy.
- Mechanism: When models memorize content from training data, they retrieve exact or near-exact text, reducing the likelihood of generating incorrect information. This direct retrieval bypasses the need for generation based on statistical patterns alone.
- Core assumption: Training data contains accurate factual information, and memorized content is retrieved rather than generated through probabilistic sampling.
- Evidence anchors:
  - [abstract] "we observed a strong anti-correlation between factual errors and memorized text for several models, suggesting memorization may contribute to factual accuracy"
  - [section 4.3] "We find a very strong anticorrelation between factual errors and memorized text for several models"
  - [corpus] Weak - only one neighbor paper discusses memorization, but doesn't specifically address the accuracy correlation
- Break Condition: If training data contains incorrect information, memorization would propagate errors rather than prevent them. Also fails if models use memorization selectively rather than consistently for factual content.

### Mechanism 2
- Claim: Explicit prompting to avoid memorized content significantly reduces plagiarism detection while maintaining discourse quality.
- Mechanism: When models receive explicit instructions not to use pre-existing content, they shift from direct retrieval to more creative generation, reducing verbatim matches while still maintaining overall coherence through learned language patterns.
- Core assumption: Models can distinguish between memorized and original generation when explicitly prompted, and maintain quality through learned language understanding rather than memorization.
- Evidence anchors:
  - [abstract] "we also demonstrate that explicitly prompting models to avoid memorized content significantly reduces detected plagiarism while maintaining discourse quality"
  - [section 4.6] "Overall we observed that all models were receptive to this prompting strategy, and showing a marked decrease on the amount of memorized content being flagged"
  - [corpus] Weak - mitigation strategies are mentioned in neighbors but not this specific prompting approach
- Break Condition: If models cannot distinguish memorized content from learned patterns, or if quality maintenance relies heavily on memorized structures, this strategy would fail.

### Mechanism 3
- Claim: Shorter prompts lead to higher memorization rates across all evaluated models.
- Mechanism: Shorter prompts provide less contextual information, making it more likely that the most probable continuation is an exact match from training data rather than a novel generation.
- Core assumption: Model generation probability is influenced by prompt length, with shorter prompts having less steering effect on output generation.
- Evidence anchors:
  - [section 4.5] "We found that shorter prompts, not accounting for baseline, are related to higher proportions of memorization, and this pattern is consistently reflected over the models"
  - [figure 3] Shows consistent relationship between prompt length and memorization across all models
  - [corpus] Moderate - neighbors discuss prompt effects but not specifically length-memory relationship
- Break Condition: If models have mechanisms to generate novel content regardless of prompt length, or if prompt conditioning effects override this relationship.

## Foundational Learning

- Concept: Correlation vs. Causation
  - Why needed here: The paper observes correlations between memorization and various metrics but doesn't establish causal relationships. Understanding this distinction is crucial for interpreting results.
  - Quick check question: Does the anti-correlation between memorization and factual errors mean memorization causes accuracy, or could both be influenced by a third factor like training data quality?

- Concept: Plagiarism Detection Limitations
  - Why needed here: The study relies on plagiarism detection tools to identify memorized content, but these tools have limitations that affect interpretation of results.
  - Quick check question: How might the plagiarism detection tools miss typeset content or non-English text, and what impact would this have on reported memorization rates?

- Concept: Statistical vs. Semantic Quality
  - Why needed here: The paper distinguishes between grammatical correctness (statistical quality) and argumentative capability (semantic quality), which is important for understanding model limitations.
  - Quick check question: Why might a model generate grammatically correct text that still contains logical fallacies or factual errors?

## Architecture Onboarding

- Component map: Input Processing -> Model API Calls -> Output Cleaning -> Plagiarism Detection -> Human Annotation -> Analysis
- Critical path: Prompt → Model API → Output Cleaning → Plagiarism Detection → Human Annotation → Analysis
- Design tradeoffs: Using off-the-shelf tools provides reproducibility but may miss nuanced memorization. Manual annotation ensures quality but limits scale.
- Failure signatures: Inconsistent plagiarism detection results across services, high variance in human annotations, correlation calculations failing due to insufficient data points.
- First 3 experiments:
  1. Test plagiarism detection services with known memorized content to establish baseline accuracy
  2. Run prompt length variation experiment with a single model to verify the memorization relationship
  3. Implement the mitigation prompt strategy on one model to validate the effectiveness claim before full deployment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between memorization and factual accuracy across different language model architectures?
- Basis in paper: [explicit] The paper found a strong anti-correlation between factual errors and memorized text for several models, suggesting memorization may contribute to factual accuracy.
- Why unresolved: The paper only observed correlation, not causation. The relationship varies across models (e.g., GPT-4 shows strong anti-correlation while Galactica shows weak correlation).
- What evidence would resolve it: Controlled experiments varying training data deduplication and measuring factual accuracy, or ablation studies removing memorized content to observe accuracy changes.

### Open Question 2
- Question: How much memorization is necessary for statistical learning versus simple pattern matching in large language models?
- Basis in paper: [explicit] The paper poses this exact question in the conclusion, noting that some models output high-quality text while being flagged as memorized.
- Why unresolved: The paper shows correlation between memorization and quality but doesn't establish thresholds or distinguish between beneficial memorization and overfitting.
- What evidence would resolve it: Comparative studies of models trained with varying degrees of data deduplication, measuring both memorization rates and downstream task performance.

### Open Question 3
- Question: What is the impact of prompt conditioning length on memorization propensity across different model architectures?
- Basis in paper: [explicit] The paper found shorter prompts cause higher memorization rates and observed this pattern consistently across all evaluated models.
- Why unresolved: The paper established correlation but didn't investigate architectural factors that might explain why some models are more susceptible to this effect.
- What evidence would resolve it: Systematic comparison of models with varying architectural parameters (attention mechanisms, context windows) under controlled prompt length conditions.

## Limitations

- Reliance on commercial plagiarism detection tools may miss non-English text, technical documentation, or typeset material
- Human annotation introduces potential subjectivity in categorizing discourse errors and factual inaccuracies
- Study doesn't account for potential contamination from training data exposed to model outputs from other systems

## Confidence

**High Confidence**: The observation that shorter prompts correlate with higher memorization rates is well-supported by the data presented in Figure 3, showing consistent patterns across all evaluated models. The effectiveness of explicit anti-memorization prompts in reducing detected plagiarism is also strongly supported by the quantitative results.

**Medium Confidence**: The claim about strong anti-correlation between factual errors and memorized text, while observed in the data, requires more careful interpretation. The correlation may be influenced by training data quality rather than indicating that memorization inherently improves accuracy. The assertion that GPT-3.51 produces the highest-quality discourse is based on subjective human evaluation criteria that may not capture all aspects of discourse quality.

**Low Confidence**: The generalizability of results across different model architectures and domains is uncertain, given that only nine models were evaluated using a specific set of prompts. The study doesn't address whether these findings would hold for models with different training objectives or data distributions.

## Next Checks

1. **Cross-validation with alternative detection methods**: Run the same evaluation using multiple plagiarism detection approaches (including semantic similarity measures) and compare results to validate whether the observed memorization rates are consistent across detection methodologies.

2. **Controlled training data experiment**: Create synthetic training data with known factual accuracy and evaluate whether models trained on this data show the same anti-correlation between memorization and factual errors, helping establish causality rather than just correlation.

3. **Domain-specific evaluation**: Repeat the evaluation using prompts exclusively from domains not well-represented in standard training corpora (such as highly specialized technical fields or recent events post-training cutoff) to test whether memorization patterns hold when models cannot rely on memorized content.