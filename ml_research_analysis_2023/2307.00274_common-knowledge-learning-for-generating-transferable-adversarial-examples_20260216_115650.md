---
ver: rpa2
title: Common Knowledge Learning for Generating Transferable Adversarial Examples
arxiv_id: '2307.00274'
source_url: https://arxiv.org/abs/2307.00274
tags:
- adversarial
- attack
- examples
- teacher
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving transferability of
  adversarial examples generated by black-box attacks, where the adversary does not
  have access to the target model's information. The authors observe that output inconsistency
  among different neural network architectures leads to poor adversarial transferability.
---

# Common Knowledge Learning for Generating Transferable Adversarial Examples

## Quick Facts
- arXiv ID: 2307.00274
- Source URL: https://arxiv.org/abs/2307.00274
- Authors: 
- Reference count: 40
- Key outcome: Proposed Common Knowledge Learning (CKL) framework improves adversarial transferability by up to 25% on cross-architecture attacks, outperforming baselines by at least 7 percentage points on average attack success rates.

## Executive Summary
This paper addresses the challenge of improving adversarial transferability in black-box attacks where the adversary lacks information about the target model. The authors observe that output inconsistency among different neural network architectures leads to poor adversarial transferability. To mitigate this issue, they propose a Common Knowledge Learning (CKL) framework that distills knowledge from multiple teacher models with different architectures into a single student network. The framework combines knowledge distillation to learn common (model-agnostic) features and gradient distillation to align input gradients across models, significantly improving attack success rates across diverse target architectures.

## Method Summary
The CKL framework trains a student model using two distillation objectives: knowledge distillation via KL divergence between teacher and student logits, and gradient distillation using PCGrad to resolve conflicts among multiple teacher gradients. The student is trained on CIFAR-10/100 using four diverse teacher models (ResNet-50, Inception-v3, Swin-T, MLPMixer). The combined loss function balances these objectives through a hyperparameter Œª. After training, the student model serves as the source for generating adversarial examples using standard attack methods (DI-FGSM, VNI-FGSM, targeted attacks). The framework aims to produce adversarial examples that transfer well to unseen target models by learning common knowledge across architectural differences.

## Key Results
- CKL achieves up to 25% improvement in attack success rate on cross-architecture black-box attacks
- Outperforms baseline methods by at least 7 percentage points on average attack success rates
- Demonstrates effectiveness on both CIFAR-10 and CIFAR-100 datasets with diverse target architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Output inconsistency across architectures directly reduces adversarial transferability.
- Mechanism: Different models have model-specific feature preferences, leading to inconsistent output probabilities even when they agree on class labels. Adversarial perturbations that exploit one model's distinctive features fail on another model where those features are not as distinctive.
- Core assumption: The paper's observation that KL divergence between model outputs correlates inversely with attack success rate.
- Evidence anchors:
  - [abstract] "The above phenomenon is induced by the output inconsistency problem."
  - [section] "we observe that a substitute model with less output inconsistency to the target model tends to possess better adversarial transferability."
  - [corpus] Weak - no corpus paper directly confirms this specific output inconsistency effect, though related work on transferability exists.

### Mechanism 2
- Claim: Knowledge distillation from multiple architectures into one student reduces output inconsistency.
- Mechanism: By training a student model to match the outputs of multiple teacher models with different architectures, the student learns model-agnostic features and produces output distributions closer to all teachers simultaneously.
- Core assumption: The student can effectively balance multiple teacher outputs without overfitting to any single teacher's model-specific features.
- Evidence anchors:
  - [abstract] "we propose a common knowledge learning (CKL) framework to distill knowledge from multiple teacher models with different architectures into a single student network."
  - [section] "we construct a multi-teacher approach, where the knowledge is distilled from different teacher architectures into one student network."
  - [corpus] Weak - no corpus paper specifically addresses multi-architecture distillation for adversarial transferability.

### Mechanism 3
- Claim: Gradient distillation with PCGrad alignment improves adversarial example generation.
- Mechanism: Since adversarial attacks use input gradients, aligning the student's input gradients with those of multiple teachers (while resolving conflicts via PCGrad) ensures the student generates perturbations that would work across architectures.
- Core assumption: Similar input gradients imply similar loss surfaces and output distributions.
- Evidence anchors:
  - [section] "we impose constraints on the gradients between the student and teacher models" and "we adopt PCGrad [38] into our work to diminish the gradient conflicts of the teacher models."
  - [section] "if ‚àáùë• ùêøùêπ (ùë•) = ‚àáùë• ùêøùê∫ (ùë•), the losses of ùêπ (ùë•) and ùê∫ (ùë•) differ by at most one constant"
  - [corpus] Moderate - PCGrad is well-established for multi-task learning, though not specifically for adversarial transferability.

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: The framework relies on transferring knowledge from multiple teacher models to a student model to reduce output inconsistency.
  - Quick check question: What loss function is used to measure the difference between teacher and student outputs in knowledge distillation?

- Concept: Gradient Projection and PCGrad
  - Why needed here: Multiple teacher models may provide conflicting gradients, requiring a method to resolve these conflicts during optimization.
  - Quick check question: How does PCGrad modify conflicting gradients to find a compromise direction?

- Concept: Adversarial Transferability
  - Why needed here: The entire framework aims to improve the ability of adversarial examples generated by one model to fool other models.
  - Quick check question: What metric is used to measure adversarial transferability in the paper?

## Architecture Onboarding

- Component map:
  - Teacher Models (4): ResNet-50, Inception-v3, Swin-T, MLPMixer
  - Student Model (1): Single network trained via CKL
  - Loss Components: Knowledge Distillation Loss (KL divergence) + Gradient Distillation Loss (PCGrad-aligned gradients)
  - Attack Integration: Standard attack methods (MI-FGSM, DI-FGSM, VNI-FGSM) applied to trained student model

- Critical path:
  1. Select diverse teacher models from different architecture families
  2. Train student model using combined KL divergence and PCGrad gradient losses
  3. Use trained student as source model for adversarial attack generation
  4. Evaluate transferability against target models (including unseen architectures)

- Design tradeoffs:
  - Teacher selection: More diverse teachers improve common knowledge but increase training complexity
  - Gradient vs output distillation: Balancing these losses affects final performance
  - Architecture choice: Student architecture should be simple enough for fast attack generation but capable enough to learn from teachers

- Failure signatures:
  - Poor performance on cross-architecture attacks indicates insufficient common knowledge learning
  - Overfitting to specific teacher architectures suggests imbalanced teacher contributions
  - Gradient conflicts not properly resolved may cause unstable training

- First 3 experiments:
  1. Train student with only knowledge distillation (no gradient distillation) and compare transferability
  2. Test different teacher combinations (all CNNs vs mixed architectures) to identify optimal diversity
  3. Vary Œª hyperparameter to find optimal balance between knowledge and gradient distillation objectives

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text provided. However, based on the content, several implicit open questions emerge regarding the generalizability and scalability of the approach.

## Limitations

- Effectiveness depends on gradient conflict magnitude among teachers, which varies across model families and tasks
- Requires multiple pre-trained teacher models, increasing computational overhead during training
- Performance on non-image datasets (NLP, speech) remains unexplored

## Confidence

- High Confidence: The observation that output inconsistency reduces transferability and the effectiveness of knowledge distillation for learning common features. The experimental results showing 25% improvement on cross-architecture attacks are well-supported by ablation studies.
- Medium Confidence: The PCGrad-based gradient distillation mechanism. While PCGrad is well-established in multi-task learning, its specific application to adversarial transferability lacks extensive validation.
- Low Confidence: The generalizability of the framework to datasets beyond CIFAR-10/100 and to architectures not included in the teacher set.

## Next Checks

1. **Gradient Conflict Analysis**: Systematically measure gradient conflict magnitudes across different teacher combinations and evaluate how this affects PCGrad performance. This would clarify the limits of gradient distillation effectiveness.

2. **Teacher Diversity Study**: Conduct controlled experiments varying teacher architecture diversity (CNN-only vs mixed architectures) to identify the optimal balance between common knowledge and computational cost.

3. **Cross-Domain Transferability**: Test the CKL framework on non-image datasets (e.g., text classification) to validate whether the common knowledge learning principle extends beyond vision tasks.