---
ver: rpa2
title: 'FedICT: Federated Multi-task Distillation for Multi-access Edge Computing'
arxiv_id: '2301.00389'
source_url: https://arxiv.org/abs/2301.00389
tags:
- local
- data
- knowledge
- clients
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel federated multi-task distillation framework
  (FedICT) for multi-access edge computing (MEC). FedICT addresses the challenges
  of communication overhead and model heterogeneity in federated multi-task learning
  by introducing knowledge distillation.
---

# FedICT: Federated Multi-task Distillation for Multi-access Edge Computing

## Quick Facts
- arXiv ID: 2301.00389
- Source URL: https://arxiv.org/abs/2301.00389
- Reference count: 40
- One-line primary result: FedICT achieves improved accuracy with less than 1.2% training communication overhead compared to FedAvg while supporting heterogeneous model architectures in federated multi-task learning.

## Executive Summary
FedICT introduces a novel federated multi-task distillation framework that addresses the challenges of communication overhead and model heterogeneity in multi-access edge computing environments. The framework employs bi-directional distillation between clients and server, using Federated Prior Knowledge Distillation (FPKD) to personalize local models and Local Knowledge Adjustment (LKA) to correct global distillation. Experiments demonstrate that FedICT significantly outperforms existing federated learning methods while maintaining minimal communication overhead, even with heterogeneous model architectures and non-IID data distributions.

## Method Summary
FedICT is a federated multi-task distillation framework for multi-access edge computing that enables heterogeneous clients to collaboratively learn while maintaining communication efficiency. The method employs bi-directional knowledge distillation where clients optimize using Federated Prior Knowledge Distillation (FPKD) that incorporates local data distribution priors, and the server optimizes using Local Knowledge Adjustment (LKA) that corrects for client drift. The framework operates on exchanged knowledge representations rather than raw model parameters, supporting diverse model architectures while achieving less than 1.2% communication overhead compared to parameter-based methods like FedAvg.

## Key Results
- FedICT achieves improved accuracy compared to all benchmark methods including FedAvg, FedAdam, pFedMe, MTFL, FedGKT, and FedDKC
- Communication overhead is less than 1.2% compared to FedAvg while requiring no more than 75% of the training communication rounds of FedGKT
- The framework maintains effectiveness across various data heterogeneity levels (α = 0.5, 1.0, 3.0) and supports heterogeneous model architectures without performance degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FPKD injects prior knowledge of local data distributions to personalize local distillation, allowing each client's model to better fit its own data.
- Mechanism: Local class frequencies are computed as prior knowledge (dk), then used to weight the KL-divergence loss terms in local distillation via exponential weighting controlled by hyperparameter T. This biases the distillation toward high-frequency classes.
- Core assumption: Local data distributions are sufficiently different across clients, and clients have access to their own label counts.
- Evidence anchors:
  - [abstract] "FPKD is proposed to reinforce the clients' fitting of local data by introducing prior knowledge of local data distributions."
  - [section] "Prior knowledge of local data distributions is introduced to personalize local models during local distillation."
- Break condition: If local data is IID across clients, the prior knowledge weighting becomes uniform and offers no advantage.

### Mechanism 2
- Claim: LKA corrects server-side distillation loss to counteract client drift caused by divergent local optimizations.
- Mechanism: Two variants are used: similarity-based weighting using cosine similarity between global and local data distribution vectors, and class-balanced weighting using residuals of global-local class frequencies. These weight local knowledge contributions during global distillation.
- Core assumption: Divergence between local and global data distributions is measurable and negatively impacts convergence.
- Evidence anchors:
  - [abstract] "LKA is proposed to correct the distillation loss of the server, making the transferred local knowledge better match the generalized representation."
  - [section] "We consider two levels: Client level... Class level... Based on the above-mentioned two insights, we propose similarity-based and class-balanced LKA respectively."
- Break condition: If all clients have identical data distributions, the weighting collapses to uniform and offers no correction.

### Mechanism 3
- Claim: The bi-directional distillation with aloof local-global knowledge prevents over-convergence to a common representation and supports heterogeneous model architectures.
- Mechanism: Clients optimize with FPKD toward their own skewed distributions, while the server optimizes with LKA toward a de-localized global view. This allows different architectures and objectives to coexist without collapsing to a single representation.
- Core assumption: Heterogeneous models can still share a common feature extractor interface and predictor output shape.
- Evidence anchors:
  - [abstract] "FedICT direct local-global knowledge aloof during bi-directional distillation processes between clients and the server."
  - [section] "We expect to inject localized prior knowledge in local distillation and de-localize local knowledge in global distillation, i.e., keeping local-global knowledge aloof."
- Break condition: If models cannot share compatible feature extractor/predictor shapes, the framework cannot exchange knowledge.

## Foundational Learning

- Concept: Federated Learning with heterogeneous data distributions.
  - Why needed here: FedICT operates in a multi-access edge computing setting where clients have non-IID data and diverse objectives.
  - Quick check question: What is the difference between IID and non-IID data in federated learning?

- Concept: Knowledge Distillation (KD).
  - Why needed here: FedICT uses KD as the exchange protocol instead of raw model parameters to reduce communication overhead and allow heterogeneous models.
  - Quick check question: How does KD transfer knowledge without sharing model weights?

- Concept: Client Drift in federated optimization.
  - Why needed here: LKA is designed specifically to mitigate client drift caused by personalized local updates in FMTL.
  - Quick check question: What causes client drift and why is it problematic for global convergence?

## Architecture Onboarding

- Component map:
  - Clients: Feature extractor, predictor, local distillation (FPKD), local knowledge extraction/upload
  - Server: Global predictor, global distillation (LKA), global knowledge generation/distribution
  - Communication: Exchange of logits/softmax outputs and embedded features, not model parameters

- Critical path:
  1. Server initialization (compute global data distribution from client reports)
  2. Client initialization (compute local data distribution)
  3. Iterative loop: Server → Clients (global knowledge), Clients → Server (local knowledge + features), Server updates, Clients update

- Design tradeoffs:
  - Communication efficiency vs. convergence speed: Smaller exchanged messages improve efficiency but may slow convergence
  - Model heterogeneity support vs. feature alignment: Different architectures must share compatible input/output shapes
  - Personalization strength vs. global consistency: Stronger FPKD improves local fit but may increase client drift

- Failure signatures:
  - Slow convergence or divergence: Likely client drift or insufficient LKA weighting
  - Poor local accuracy: FPKD weighting too weak or local data distribution misestimated
  - High communication overhead: Exchanged knowledge/features too large or too frequent

- First 3 experiments:
  1. Validate local distillation with FPKD on a single client with skewed data
  2. Test LKA weighting effectiveness by comparing convergence with/without LKA
  3. Verify heterogeneous model support by running with different architectures on same data

## Open Questions the Paper Calls Out

- Question: How does the performance of FedICT vary with different data distribution vector estimation methods beyond class frequency (e.g., using Wasserstein distance or KL divergence between client and global distributions)?
  - Basis in paper: [inferred] The paper uses class frequency to estimate data distributions (dist(·) maps datasets to frequency vectors) but acknowledges that local data is often class-unbalanced in FL scenarios.
  - Why unresolved: The paper does not explore alternative data distribution estimation methods that might better capture the true data distribution differences.
  - What evidence would resolve it: Experimental results comparing FedICT's performance using different data distribution estimation methods (e.g., frequency-based vs Wasserstein distance-based) on the same datasets and settings.

- Question: How does FedICT's communication efficiency scale with the number of clients, particularly in extremely large-scale scenarios with hundreds or thousands of participants?
  - Basis in paper: [explicit] The paper demonstrates FedICT achieves less than 1.2% communication overhead compared to FedAvg in scenarios with 10-150 clients, but doesn't explore larger scales.
  - Why unresolved: The paper only tests up to 150 clients, leaving uncertainty about scalability to much larger client populations common in real-world MEC scenarios.
  - What evidence would resolve it: Experimental results showing communication overhead, convergence speed, and accuracy as a function of client count (e.g., 10, 50, 100, 500, 1000 clients) on representative datasets.

- Question: How robust is FedICT to label noise or corrupted data distributions in client datasets?
  - Basis in paper: [inferred] The paper assumes clean data distributions and doesn't address scenarios where client data might be mislabeled or contain outliers.
  - Why unresolved: Real-world MEC scenarios often involve imperfect data collection, and the paper's performance claims may not hold under data quality issues.
  - What evidence would resolve it: Experiments evaluating FedICT's performance when client datasets contain varying levels of label noise or corrupted data distributions, comparing against baseline methods.

## Limitations
- Unknown implementation details of competing methods FedGKT and FedDKC, including hyperparameters and KKR-SKR variants
- Limited scalability testing (only up to 150 clients) leaves uncertainty about performance in extremely large-scale MEC scenarios
- No evaluation of robustness to label noise or corrupted data distributions in client datasets

## Confidence
- FPKD effectiveness: High (well-validated through ablation studies)
- LKA impact on convergence: Medium (dependent on accurate data distribution estimation)
- Communication overhead claims: High (explicitly measured and compared)

## Next Checks
1. Implement a minimal FedICT prototype with synthetic non-IID data to verify the FPKD weighting mechanism independently of the full framework
2. Conduct sensitivity analysis on LKA hyperparameters (similarity threshold, class frequency scaling) to identify stability boundaries
3. Measure the actual communication overhead in terms of bytes transferred per round, not just model parameter counts, to validate the <1.2% claim under realistic network conditions