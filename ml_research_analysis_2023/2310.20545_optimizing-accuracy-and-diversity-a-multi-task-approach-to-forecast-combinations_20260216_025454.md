---
ver: rpa2
title: 'Optimizing accuracy and diversity: a multi-task approach to forecast combinations'
arxiv_id: '2310.20545'
source_url: https://arxiv.org/abs/2310.20545
tags:
- forecasting
- series
- time
- methods
- forecast
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents a multi-task learning approach to optimize
  forecast combinations by simultaneously addressing model selection and weight optimization.
  The method employs a deep neural network with two branches: one for learning convex
  combination weights through regression and another for selecting diverse and accurate
  models through classification.'
---

# Optimizing accuracy and diversity: a multi-task approach to forecast combinations

## Quick Facts
- arXiv ID: 2310.20545
- Source URL: https://arxiv.org/abs/2310.20545
- Reference count: 10
- Key outcome: Multi-task deep learning approach improves forecast accuracy by jointly optimizing weights and model selection while learning diversity during training, outperforming FFORMA and FFORMA-DIV on M4 dataset

## Executive Summary
This paper presents a novel multi-task learning approach for forecast combinations that simultaneously addresses model selection and weight optimization. The method employs a deep neural network with two branches: one for learning convex combination weights through regression and another for selecting diverse and accurate models through classification. The classification task uses an optimization-based labeling procedure that maximizes accuracy and diversity among base forecasting methods. Experimental results on the M4 competition dataset show statistically significant improvements in overall weighted average (OWA) and mean series-level overall weighted average (MsOWA) metrics compared to state-of-the-art methods like FFORMA and FFORMA-DIV.

## Method Summary
The approach uses a multi-task deep neural network trained on standardized time series data from the M4 competition. The network consists of two branches: a regression branch that learns convex combination weights and a classification branch that selects diverse and accurate forecasting methods. The classification branch uses an optimization-based labeling procedure (QP-LAB) that solves a quadratic programming problem to maximize accuracy and diversity. The network is trained using a custom loss function combining mean squared error for regression, binary cross-entropy for classification, and an orthogonality term to encourage diverse feature learning. The final weights are obtained through element-wise multiplication of regression and classification outputs followed by softmax normalization.

## Key Results
- Statistically significant improvements in OWA metric compared to FFORMA and FFORMA-DIV baselines
- Reduced standard deviation of sOWA across different forecast horizons, indicating lower accuracy risk
- Ablation studies show the orthogonality term in the loss function is critical for performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multi-task learning approach improves forecast accuracy by jointly optimizing weights and model selection while learning diversity during training.
- Mechanism: The deep neural network simultaneously performs regression to learn convex combination weights and classification to select accurate and diverse models. The classification task uses an optimization-based labeling procedure that identifies the most appropriate methods for each time series based on accuracy and diversity.
- Core assumption: Diversity among base forecasters is essential for effective forecast combinations, and learning this diversity during training is more effective than applying it as a post-processing step.
- Evidence anchors:
  - [abstract]: "Experimental results on a large set of series from the M4 competition dataset show that our proposal enhances point forecast accuracy compared to state-of-the-art methods."
  - [section]: "The ambiguity decomposition theory by Krogh & Vedelsby (1994) can be easily applied to the forecast combination task... greater diversity among the forecasting methods in the pool results in improved overall forecast accuracy (Kang et al., 2022)."
  - [corpus]: Weak - no direct citations found for diversity decomposition theory in corpus.
- Break condition: If the optimization-based labeling procedure fails to properly identify diverse methods, or if the diversity measure becomes meaningless (e.g., all methods are highly correlated).

### Mechanism 2
- Claim: The orthogonality term in the loss function encourages the regression and classification subnetworks to extract different aspects of the input time series.
- Mechanism: The Lort term (Equation 15) penalizes redundant latent representations by encouraging the two feature extractors to encode different aspects of the input time series. This promotes diversity in feature learning across tasks.
- Core assumption: Having distinct feature representations for regression and classification tasks leads to better overall performance than sharing features.
- Evidence anchors:
  - [section]: "We empirically find that setting τ = 1/M yields satisfactory results in terms of the balance among the methods of the pool."
  - [section]: "The second ablation experiment underscores the importance of the orthogonality term in the loss function. Notably, setting λ to zero negatively impacts the model's performance."
  - [corpus]: Weak - no direct citations found for orthogonality term in loss function.
- Break condition: If the orthogonality term becomes too restrictive and prevents useful feature sharing, or if the feature extractors become too dissimilar to benefit from any task relationship.

### Mechanism 3
- Claim: The element-wise multiplication of regression and classification outputs enables knowledge sharing between tasks.
- Mechanism: The softmax layer combines the unnormalized weights from regression and the classification probabilities through element-wise multiplication. This allows the network to emphasize forecasting methods that are both accurate (from regression) and diverse/appropriate (from classification).
- Core assumption: The element-wise multiplication effectively captures the interaction between accuracy and diversity when determining final weights.
- Evidence anchors:
  - [section]: "Note that, the element-wise multiplication enables to share the knowledge between the main regression and the auxiliary classification task."
  - [section]: "In contrast to the methods mentioned earlier, there exists a body of research focusing on the use of Deep Neural Networks (DNN)."
  - [corpus]: Weak - no direct citations found for element-wise multiplication in multi-task learning.
- Break condition: If the element-wise multiplication creates unstable gradients or if one task dominates the other, leading to suboptimal weight combinations.

## Foundational Learning

- Concept: Convex combinations and weighted averaging
  - Why needed here: The core forecasting approach relies on combining multiple forecasts using convex combinations where weights sum to 1.
  - Quick check question: What mathematical constraint must be satisfied by the weights in a convex combination of forecasts?

- Concept: Diversity in ensemble methods
  - Why needed here: The method specifically optimizes for both accuracy and diversity among base forecasters, based on the theory that diverse methods reduce ensemble error.
  - Quick check question: According to the ambiguity decomposition theory, how does diversity among base models affect ensemble error?

- Concept: Multi-task learning and loss function design
  - Why needed here: The approach uses a custom loss function combining regression, classification, and orthogonality terms, requiring understanding of how to balance multiple objectives.
  - Quick check question: How does the orthogonality term in the loss function encourage the two feature extractors to learn different representations?

## Architecture Onboarding

- Component map: Time series input → CNN feature extractors (regression and classification) → Dense layers for unnormalized weights and classification probabilities → Element-wise multiplication → Softmax output → Final convex combination weights
- Critical path: Time series input → Feature extraction → Regression and classification outputs → Weight combination → Forecast generation
- Design tradeoffs: Using separate feature extractors for each task vs. shared features (orthogonal term adds complexity but improves performance), element-wise multiplication vs. other combination methods
- Failure signatures: Poor forecast accuracy due to ineffective diversity learning, unstable training from poorly balanced loss terms, overfitting to training data
- First 3 experiments:
  1. Test the orthogonality term by training with λ=0 and comparing performance
  2. Evaluate the element-wise multiplication by replacing it with simple averaging of task outputs
  3. Assess the classification labeling by varying the threshold τ and measuring class balance

## Open Questions the Paper Calls Out
1. The paper suggests exploring the extension of the proposed method to probability density forecasting as a future research direction, which could enhance the practical applicability of the approach for uncertainty quantification.

## Limitations
- The optimization-based labeling procedure (QP-LAB) is described conceptually but lacks detailed implementation specifics, particularly regarding the correlation matrix calculation and the threshold parameter τ
- The comparison with state-of-the-art methods is limited to the M4 dataset without validation on other forecasting benchmarks
- The ablation studies show the importance of the orthogonality term but do not provide sensitivity analysis for the hyperparameters α, β, and λ

## Confidence

**High Confidence**: The core mechanism of using multi-task learning to jointly optimize weights and model selection is well-supported by experimental results showing consistent improvements across OWA, MsOWA, and risk metrics

**Medium Confidence**: The theoretical justification for diversity-based ensembles (ambiguity decomposition) is referenced but not directly validated within the paper's experiments

**Medium Confidence**: The orthogonality term's effectiveness is demonstrated through ablation, but the sensitivity to hyperparameter λ and its interaction with other loss terms requires further investigation

## Next Checks

1. Replicate the QP-LAB optimization procedure with different τ thresholds to assess robustness of the classification labels to parameter selection

2. Conduct sensitivity analysis on the orthogonality hyperparameter λ by testing values across a wider range and evaluating impact on both training stability and final forecast accuracy

3. Validate the approach on additional forecasting datasets (e.g., M3 competition, tourism forecasting) to assess generalizability beyond the M4 dataset