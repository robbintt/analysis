---
ver: rpa2
title: On Counterfactual Data Augmentation Under Confounding
arxiv_id: '2305.18183'
source_url: https://arxiv.org/abs/2305.18183
tags:
- causal
- data
- confounding
- counterfactual
- augmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formally analyzes how confounding biases affect downstream
  classifiers and proposes a causal perspective on counterfactual data augmentation
  for mitigating these biases. The authors introduce an information-theoretic measure
  of confounding based on directed information and demonstrate its relationship to
  spurious correlations between generative factors.
---

# On Counterfactual Data Augmentation Under Confounding

## Quick Facts
- **arXiv ID**: 2305.18183
- **Source URL**: https://arxiv.org/abs/2305.18183
- **Reference count**: 40
- **Primary result**: Proposes a causal perspective on counterfactual data augmentation for mitigating confounding biases, achieving state-of-the-art performance on MNIST variants and CelebA datasets.

## Executive Summary
This paper formally analyzes how confounding biases affect downstream classifiers and proposes a causal perspective on counterfactual data augmentation for mitigating these biases. The authors introduce an information-theoretic measure of confounding based on directed information and demonstrate its relationship to spurious correlations between generative factors. They show that removing confounding through appropriate interventions helps learn invariant causal features and enables out-of-distribution generalization. Experiments demonstrate that their conditional diffusion model-based approach achieves state-of-the-art performance in removing confounding biases compared to various baselines.

## Method Summary
The paper proposes a framework for counterfactual data augmentation that targets confounding biases in training data. The approach involves identifying a causal feature Z0, performing do(Z0) intervention to block backdoor paths, and generating counterfactual images using conditional generative models. These counterfactuals are then used to augment training data, helping downstream classifiers learn invariant features that generalize better to out-of-distribution data. The method is evaluated against ERM, traditional augmentation techniques, CycleGAN, CGN, and invariant risk minimization baselines on MNIST variants and CelebA datasets.

## Key Results
- Achieves 80.34% accuracy on CM-MNIST dataset with confounding
- Achieves 94.73% accuracy on CelebA dataset
- Outperforms ERM and traditional augmentation methods (AugMix, CutMix)
- Shows state-of-the-art performance compared to CycleGAN, CGN, and invariant risk minimization baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Removing confounding by intervening on Z0 alone is sufficient to learn invariant causal features.
- **Mechanism:** By performing do(Z0), the backdoor paths Z0 ← Uj → Zi → X are blocked, eliminating spurious correlations while preserving the causal path Z0 → X → Y. This ensures the classifier learns features that generalize to out-of-distribution data.
- **Core assumption:** The causal feature Z0 is identifiable and contains all information necessary to predict Y.
- **Evidence anchors:**
  - [abstract] "We explore how removing confounding biases serves as a means to learn invariant features, ultimately aiding in generalization beyond the observed data distribution."
  - [section] "Performing interventions onZ0 or Zi or both Z0, Zi ensures I(Z0; Zi) = 0 as shown in the proposition below."
  - [corpus] Weak - no direct corpus support for sufficiency of single-intervention claim.
- **Break condition:** If Z0 is not fully informative about Y or if multiple generative factors jointly determine Y, intervening on Z0 alone will be insufficient.

### Mechanism 2
- **Claim:** Directed information quantifies confounding strength between generative factors.
- **Mechanism:** The measure CN F(Zi; Zj) = I(Zi → Zj) + I(Zj → Zi) captures bidirectional dependency that arises from backdoor paths, allowing us to identify which factor pairs are confounded and prioritize interventions.
- **Core assumption:** Directed information correctly measures confounding in the presence of confounders.
- **Evidence anchors:**
  - [section] "We introduce a formal framework for quantifying the extent of confounding and investigate its relation with the non-linear dependency between pairs of generative factors (§ 4)."
  - [abstract] "We introduce an information-theoretic measure of confounding based on directed information and demonstrate its relationship to spurious correlations between generative factors."
  - [corpus] No direct corpus support for directed information as confounding measure.
- **Break condition:** If the causal graph structure is more complex than assumed (e.g., mediators, colliders), directed information may not accurately capture confounding.

### Mechanism 3
- **Claim:** Counterfactual data augmentation simulates intervened causal models to remove confounding from training data.
- **Mechanism:** By generating counterfactual images through do(Z0) using conditional generative models, we create augmented data that appears to come from a confounded-free distribution, allowing downstream classifiers to learn invariant features.
- **Core assumption:** Conditional generative models can accurately simulate the intervened causal model and generate realistic counterfactuals.
- **Evidence anchors:**
  - [abstract] "They show that removing confounding through appropriate interventions helps learn invariant causal features and enables out-of-distribution generalization."
  - [section] "We propose to simulate the causal model in Eqn 5 to generate counterfactual images so that it is required to perform an intervention on only one feature Z0 (Algorithm 1)."
  - [corpus] No direct corpus support for conditional generative models' ability to simulate intervened causal models.
- **Break condition:** If conditional generative models cannot accurately capture the underlying data distribution or if the intervened distribution is too different from observed data, generated counterfactuals may be unrealistic or misleading.

## Foundational Learning

- **Concept: Causal graphs and interventions**
  - Why needed here: Understanding how confounding creates backdoor paths and how interventions can block these paths is fundamental to the proposed approach.
  - Quick check question: In a causal graph X ← U → Y, what intervention would block the backdoor path between X and Y?

- **Concept: Directed information and mutual information**
  - Why needed here: The paper uses these information-theoretic measures to quantify confounding and spurious correlations between generative factors.
  - Quick check question: If I(X → Y) = 0 and I(Y → X) = 0, what does this imply about the relationship between X and Y?

- **Concept: Counterfactual inference and data augmentation**
  - Why needed here: The proposed method relies on generating counterfactual images through interventions to augment training data and remove confounding.
  - Quick check question: In the counterfactual inference procedure, what is the purpose of the "abduction" step?

## Architecture Onboarding

- **Component map:** Causal graph analysis → Information-theoretic measure → Intervention selection → Conditional generative model → Data augmentation → Downstream classifier

- **Critical path:** Causal graph analysis → Information-theoretic measure → Intervention selection → Conditional generative model → Data augmentation → Downstream classifier

- **Design tradeoffs:**
  - Intervention on single Z0 vs. multiple factors: Simplicity vs. potential loss of information
  - Choice of conditional generative model: Flexibility vs. training complexity and data requirements
  - Amount of augmented data: Improved robustness vs. computational cost and potential overfitting

- **Failure signatures:**
  - Poor downstream performance despite data augmentation: May indicate inaccurate causal graph, insufficient counterfactuals, or model capacity issues
  - Generated counterfactuals appear unrealistic: May indicate limitations in conditional generative model or mismatch between intervened and observed distributions
  - High variance in results across different runs: May indicate sensitivity to initialization or data sampling

- **First 3 experiments:**
  1. Test the causal graph analysis on a synthetic dataset with known confounding structure to verify backdoor path identification
  2. Validate the information-theoretic measure by comparing CN F values with known spurious correlation strengths in a controlled setting
  3. Evaluate the conditional generative model's ability to generate realistic counterfactuals by conducting a human evaluation study on a small dataset

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the choice of intervention (Z0 vs Zcnf vs X) affect the performance of downstream classifiers across different datasets and confounding structures?
- **Basis in paper:** [explicit] The paper explicitly compares different interventions (Z0, Zcnf, X) through causal models 5-8 and their corresponding algorithms, showing varying performance results on MNIST variants and CelebA.
- **Why unresolved:** While the paper provides experimental results comparing these interventions, it doesn't provide a theoretical framework explaining why certain interventions work better than others in specific scenarios, nor does it establish guidelines for choosing the optimal intervention.
- **What evidence would resolve it:** Systematic experiments varying the type and degree of confounding, combined with theoretical analysis of how different interventions affect the causal graph's backdoor paths and feature learning.

### Open Question 2
- **Question:** Can the proposed confounding measure (CNF) be generalized to continuous-time causal processes or non-linear causal mechanisms beyond the discrete, additive noise model presented?
- **Basis in paper:** [inferred] The paper presents CNF as an information-theoretic measure based on directed information, which could theoretically be extended to more complex causal structures, but the experiments are limited to discrete, linear-like generative processes.
- **Why unresolved:** The paper only demonstrates CNF on simple, discrete generative models with linear mechanisms, leaving open questions about its applicability to real-world continuous processes with complex non-linear dynamics.
- **What evidence would resolve it:** Experiments on datasets with continuous-valued generative factors and non-linear causal mechanisms, along with theoretical proofs of CNF's properties in these generalized settings.

### Open Question 3
- **Question:** How does the proposed single-intervention approach (intervening on Z0 only) compare to multi-intervention approaches in terms of sample efficiency and scalability to high-dimensional causal feature spaces?
- **Basis in paper:** [explicit] The paper claims that intervening on Z0 alone is sufficient and more efficient than intervening on multiple features, but only provides empirical evidence on relatively simple datasets.
- **Why unresolved:** The paper doesn't explore how the approach scales to datasets with many confounded features or how sample efficiency changes with increasing dimensionality of the causal feature space.
- **What evidence would resolve it:** Comparative experiments on high-dimensional datasets with complex confounding structures, measuring both performance and computational/resource requirements for single vs multi-intervention approaches.

### Open Question 4
- **Question:** What are the limitations of the counterfactual identifiability framework when the generative factors cannot be perfectly recovered (i.e., when the inverse function g⁻¹ is not exactly invertible)?
- **Basis in paper:** [inferred] The paper assumes invertible functions for counterfactual generation, but real-world data often involves non-invertible or approximate recovery of generative factors.
- **Why unresolved:** The paper doesn't address scenarios where perfect recovery of generative factors is impossible, nor does it discuss the impact of approximation errors on counterfactual quality and downstream performance.
- **What evidence would resolve it:** Experiments introducing noise or non-invertibility in the generative factor recovery process, measuring the degradation in counterfactual quality and downstream classifier performance as a function of recovery accuracy.

## Limitations

- The sufficiency of single-intervention on Z0 lacks empirical validation on real-world datasets with complex confounding structures.
- Directed information measure for confounding has not been compared against established causal inference methods.
- The ability of conditional generative models to accurately simulate intervened causal models remains untested with quantitative metrics.

## Confidence

- **Mechanism 1 (Sufficiency of Z0 intervention):** Low confidence - theoretical justification exists but limited empirical support
- **Mechanism 2 (Directed information as confounding measure):** Medium confidence - information-theoretic framework is sound but lacks comparative validation
- **Mechanism 3 (Generative model simulation):** Low confidence - crucial assumption untested with quantitative metrics
- **Overall performance claims:** Medium confidence - strong results on benchmark datasets but potential overfitting to specific confounding structures

## Next Checks

1. **Causal sufficiency test**: Create a synthetic dataset where Y depends on multiple generative factors (not just Z0) and verify whether single-intervention approach fails as predicted, confirming the sufficiency claim is not overgeneralized.

2. **Directed information validation**: Compare CN F(Zi; Zj) values with established confounding detection methods (e.g., conditional independence tests, do-calculus analysis) on a known causal graph to establish whether directed information provides comparable or superior quantification.

3. **Counterfactual realism evaluation**: Conduct a human evaluation study where annotators are asked to distinguish real from counterfactual images generated through Z0 intervention, measuring both discrimination accuracy and qualitative assessment of image quality to validate the generative model's simulation capability.