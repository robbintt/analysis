---
ver: rpa2
title: Adapting Double Q-Learning for Continuous Reinforcement Learning
arxiv_id: '2309.14471'
source_url: https://arxiv.org/abs/2309.14471
tags:
- bias
- policy
- overestimation
- learning
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses overestimation bias in continuous off-policy
  reinforcement learning by adapting the Double Q-Learning principle to continuous
  action spaces. The proposed Continuous Double Q-Learning (CDQ) algorithm uses a
  mixture of two policy components, where each component is optimized and evaluated
  by separate Q-networks, theoretically eliminating the basis for overestimation bias.
---

# Adapting Double Q-Learning for Continuous Reinforcement Learning

## Quick Facts
- arXiv ID: 2309.14471
- Source URL: https://arxiv.org/abs/2309.14471
- Reference count: 3
- The paper addresses overestimation bias in continuous off-policy reinforcement learning by adapting the Double Q-Learning principle to continuous action spaces.

## Executive Summary
This paper proposes Continuous Double Q-Learning (CDQ), an adaptation of Double Q-Learning to continuous action spaces that addresses overestimation bias in off-policy reinforcement learning. The method uses two policy components, each optimized and evaluated by separate Q-networks, theoretically eliminating the basis for overestimation bias. Experiments on MuJoCo environments (Humanoid-v3 and Walker2d-v3) show that CDQ achieves near state-of-the-art performance without requiring hyperparameter tuning, though some overestimation bias remains compared to baseline TQC with optimal hyperparameters. The method shows promise for bias reduction while maintaining competitive performance.

## Method Summary
CDQ adapts the Double Q-Learning principle to continuous action spaces by using a mixture policy with two components, where each component is optimized using one Q-network and evaluated using the opposite Q-network. The algorithm maintains two policy networks and two Q-networks, with TD-targets constructed using cross-evaluation between different Q-networks. This design ensures that action selection and evaluation use separate networks, theoretically preventing the overestimation bias that arises from correlated errors in traditional Q-learning approaches.

## Key Results
- CDQ achieves near state-of-the-art performance on MuJoCo environments without requiring hyperparameter tuning
- The algorithm successfully reduces overestimation bias compared to baseline TQC, though some bias remains
- CDQ-random variant (with random evaluation) shows that deterministic cross-evaluation is important for bias reduction
- CDQ-same (baseline using same Q-network for both optimization and evaluation) confirms the importance of cross-evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separate policy components prevent correlated Q-value errors
- Mechanism: Each policy component πφᵢ is optimized using a different Q-network Qψᵢ, and evaluated using the opposite Q-network Qψ₂₋ᵢ, ensuring uncorrelated action selection and evaluation
- Core assumption: Q-networks develop independent approximation errors that cancel out when combined
- Evidence anchors:
  - [abstract] "Each policy component is maximized and assessed by separate networks, which removes any basis for the overestimation bias"
  - [section] "Each policy component is optimized with the corresponding Q-function Qψᵢ, and evaluated with the opposite Q-function Qψ₂₋ᵢ"
  - [corpus] Weak - only general Q-learning bias papers, no specific validation of this mechanism
- Break condition: Q-networks develop correlated errors through shared training data or architecture

### Mechanism 2
- Claim: Mixture policy structure provides robustness against overestimation
- Mechanism: The final policy π is a mixture of two components, each trained with different Q-networks, averaging out overestimation in individual components
- Core assumption: Individual policy components will overestimate in different directions, canceling out when averaged
- Evidence anchors:
  - [section] "π(a) = 1/2 (πφ₁(a) + πφ₂(a))" and discussion of log π₁(a₁) - log π(a₁) differences
  - [abstract] "We propose using a policy in form of a mixture with two components"
  - [corpus] Missing - no corpus evidence directly supporting this mechanism
- Break condition: Both policy components systematically overestimate in the same direction

### Mechanism 3
- Claim: TD-target construction eliminates bias propagation
- Mechanism: TD-target uses y(r,s′) = r + 1/2 γ Σᵢ Qψᵢ(s′,πφ₂₋ᵢ(s′)), where each policy component is evaluated by a different Q-network than used for optimization
- Core assumption: Cross-evaluation between different Q-networks prevents error accumulation in temporal difference learning
- Evidence anchors:
  - [section] "Each policy component πᵢ is both optimized and evaluated by the same corresponding Q-function Qψᵢ" (CDQ-same baseline shows bias)
  - [abstract] "Each policy component is maximized and assessed by separate networks, which removes any basis for the overestimation bias"
  - [corpus] Weak - general double Q-learning papers but not this specific construction
- Break condition: Q-networks become correlated during training, defeating the cross-evaluation

## Foundational Learning

- Concept: Overestimation bias in Q-learning
  - Why needed here: Understanding the fundamental problem CDQ addresses is crucial for proper implementation
  - Quick check question: Why does max operation in Q-learning lead to overestimation bias according to Jensen's inequality?

- Concept: Double Q-learning principle
  - Why needed here: CDQ is directly inspired by Double Q-learning, so understanding its mechanism is essential
  - Quick check question: How does Double Q-learning prevent overestimation bias in discrete action spaces?

- Concept: Distributional RL and quantile critics
  - Why needed here: CDQ is implemented as modification of TQC, which uses distributional Q-functions
  - Quick check question: What advantage do quantile critics provide over traditional Q-networks in bias reduction?

## Architecture Onboarding

- Component map:
  - Two policy networks (πφ₁, πφ₂) forming mixture policy
  - Two Q-networks (Qψ₁, Qψ₂) for action-value estimation
  - Separate replay buffers for each policy-Q pair (potential enhancement)
  - TD-target construction module for cross-evaluation

- Critical path:
  1. Sample batch from replay buffer
  2. Compute policy gradients for each component using opposite Q-network
  3. Update Q-networks using cross-evaluated TD-targets
  4. Periodically update target networks

- Design tradeoffs:
  - Two policies vs single policy: Increased complexity but better bias control
  - Cross-evaluation vs same-network evaluation: Removes bias but may reduce sample efficiency
  - Separate replay buffers: Could further reduce correlation but doubles memory requirements

- Failure signatures:
  - Both policy components converging to similar behavior (loss of mixture benefit)
  - Q-networks showing high correlation in their error patterns
  - Performance worse than baseline TQC despite theoretical advantages

- First 3 experiments:
  1. Compare CDQ-same (baseline) vs CDQ to verify bias reduction mechanism
  2. Test CDQ-random variant to understand importance of deterministic cross-evaluation
  3. Measure correlation between Q-networks' errors over training to validate independence assumption

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does Continuous Double Q-Learning (CDQ) fail to completely eliminate overestimation bias despite theoretically separating policy optimization and evaluation across different Q-networks?
- Basis in paper: [explicit] The paper explicitly states that while CDQ reduces overestimation bias, it still remains present, contradicting the theoretical expectation of complete bias elimination.
- Why unresolved: The experimental results show persistent bias even with the proposed architecture, suggesting theoretical assumptions may not hold in practice or implementation factors are missing.
- What evidence would resolve it: Systematic ablation studies comparing CDQ with and without shared replay buffers, different sampling strategies, or additional architectural constraints would help identify the source of residual bias.

### Open Question 2
- Question: What is the optimal policy architecture for implementing Double Q-Learning in continuous action spaces, given that previous attempts (Fujimoto et al., 2018) showed worse performance than alternative approaches?
- Basis in paper: [explicit] The paper notes that TD3 authors previously tried applying DDQN to continuous RL with two policies and Q-networks but found worse performance than TD3, with unclear implementation details.
- Why unresolved: The brief description in prior work and lack of released code makes it unclear whether the failure was due to architectural choices, implementation details, or fundamental limitations of the approach.
- What evidence would resolve it: Comprehensive comparison of different policy architectures (mixture models, hierarchical policies, ensemble methods) with detailed ablation studies would clarify optimal design choices.

### Open Question 3
- Question: How does the hyperparameter-free nature of CDQ compare to hyperparameter-tuned baselines across diverse continuous control tasks beyond the tested MuJoCo environments?
- Basis in paper: [explicit] The paper claims CDQ removes the need for environment-specific hyperparameter tuning compared to TQC, but only tests on two MuJoCo environments.
- Why unresolved: Limited experimental scope prevents generalization of the hyperparameter-free claim to the broader continuous control domain.
- What evidence would resolve it: Extensive benchmarking across diverse continuous control benchmarks (DM Control Suite, PyBullet environments, robotics simulators) comparing CDQ against tuned baselines would validate the generalization claim.

### Open Question 4
- Question: Why do the two policy components in CDQ maintain significant behavioral differences throughout training despite theoretical expectations of convergence?
- Basis in paper: [explicit] The paper observes that πϕ1 and πϕ2 exhibit significant log-probability differences throughout training, contrary to theoretical expectations that they should behave similarly.
- Why unresolved: The paper does not investigate the source of this persistent asymmetry or its implications for algorithm performance.
- What evidence would resolve it: Detailed analysis of policy entropy, action distribution overlap, and gradient alignment between components across training would reveal whether this asymmetry is problematic or beneficial.

## Limitations
- The experiments only evaluate on two MuJoCo environments, limiting generalizability to other continuous control tasks
- Some overestimation bias remains even with the proposed architecture, indicating the mechanism doesn't completely eliminate the problem
- The absolute performance is below optimally-tuned TQC, suggesting a potential tradeoff between bias reduction and sample efficiency

## Confidence
- Overestimation bias reduction mechanism: High confidence
- Cross-evaluation between Q-networks: Medium confidence
- Competitive performance without hyperparameter tuning: Medium confidence

## Next Checks
1. Measure and report Q-network error correlation throughout training to empirically validate the independence assumption underlying the bias reduction mechanism
2. Test CDQ on additional continuous control benchmarks (e.g., HalfCheetah, Ant) to assess generalizability beyond the two MuJoCo environments used
3. Implement and evaluate separate replay buffers for each policy-Q pair to investigate whether further reducing Q-network correlation improves bias reduction or performance