---
ver: rpa2
title: 'DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning'
arxiv_id: '2309.05173'
source_url: https://arxiv.org/abs/2309.05173
tags:
- learning
- prompt
- https
- computational
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Decomposed Prompt Tuning (DePT) is proposed to improve the efficiency
  of Prompt Tuning (PT) for parameter-efficient fine-tuning of language models. PT
  appends trainable soft prompt vectors to the input, but this increases sequence
  length and computational cost due to the quadratic complexity of the Transformer.
---

# DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning

## Quick Facts
- **arXiv ID**: 2309.05173
- **Source URL**: https://arxiv.org/abs/2309.05173
- **Reference count**: 40
- **Key outcome**: DePT achieves over 20% efficiency gains compared to vanilla PT while maintaining competitive performance on 23 NLP and vision-language tasks.

## Executive Summary
DePT (Decomposed Prompt Tuning) is a parameter-efficient fine-tuning method that addresses the computational inefficiency of vanilla Prompt Tuning (PT) in large language models. PT appends trainable soft prompt vectors to the input, but this increases sequence length and computational cost due to the quadratic complexity of the Transformer. DePT decomposes the soft prompt into a shorter prompt and a pair of low-rank matrices, which are optimized with different learning rates. This maintains competitive performance while reducing memory and time costs by over 20% compared to vanilla PT and its variants. Extensive experiments on 23 NLP and vision-language tasks demonstrate that DePT outperforms state-of-the-art PEFT approaches, including the full fine-tuning baseline in some scenarios.

## Method Summary
DePT decomposes the soft prompt into a shorter prompt and a pair of low-rank matrices (A and B). The shorter prompt is optimized with a larger learning rate (α1), while the low-rank matrices are optimized with a smaller learning rate (α2). The product BA is added to the frozen word embedding matrix, updating it without increasing sequence length. This decomposition reduces computational cost while maintaining the same number of trainable parameters as vanilla PT. The method is evaluated on 23 tasks using T5-Base and T5-Large models, showing significant efficiency gains over vanilla PT and competitive performance against other PEFT methods.

## Key Results
- DePT reduces training and inference time by over 20% compared to vanilla PT on GLUE, SuperGLUE, and MRQA 2019 Shared Task
- Maintains competitive performance while saving substantial memory costs
- Outperforms state-of-the-art PEFT approaches including full fine-tuning baseline in some scenarios
- Efficiency advantage grows as model size increases

## Why This Works (Mechanism)

### Mechanism 1
Decomposing the soft prompt into a shorter prompt and low-rank matrices reduces the effective sequence length, lowering computational cost due to the quadratic complexity of the Transformer. The low-rank matrices update frozen word embeddings without increasing sequence length.

### Mechanism 2
Using two different learning rates for the shorter prompt and low-rank matrices enables better convergence than using a single learning rate. The shorter prompt adapts quickly while the low-rank matrices make more gradual adjustments.

### Mechanism 3
Maintaining the exact number of trainable parameters while reducing sequence length leads to improved training and inference efficiency without sacrificing performance. The number of trainable parameters is the primary driver of model capacity.

## Foundational Learning

- **Low-rank matrix factorization (e.g., SVD, low-rank approximation)**: Why needed? The method relies on decomposing updates to word embeddings into products of low-rank matrices A and B. Quick check: Why might a low-rank approximation be sufficient to update frozen embeddings instead of a full-rank update?

- **Parameter-efficient fine-tuning (PEFT) strategies and their trade-offs**: Why needed? DePT is a PEFT method; knowing the landscape of PEFT approaches (adapters, LoRA, prompt tuning, etc.) and their strengths/weaknesses helps contextualize DePT's contributions. Quick check: How does DePT's parameter efficiency compare to LoRA and Adapters in terms of both parameter count and computational cost?

- **Transformer computational complexity and its scaling**: Why needed? The motivation for DePT is the quadratic complexity of self-attention with sequence length. Understanding how computational cost scales with sequence length is essential. Quick check: If the sequence length is halved, by what factor does the self-attention computation cost approximately change?

## Architecture Onboarding

- **Component map**: Frozen backbone model (T5/GPT-2) -> Frozen word embedding matrix W_i (s x d) -> Shorter trainable prompt matrix P^s (m x d) -> Low-rank matrices A (s x r) and B (r x d) -> Combined input [P^s; W_i + BA]

- **Critical path**:
  1. Initialize P^s, A, and B (A from Gaussian, B from zeros)
  2. Forward pass: Compute BA, add to W_i, concatenate with P^s
  3. Compute loss with backbone model
  4. Backward pass: Update P^s with learning rate α1, A and B with α2
  5. Repeat until convergence

- **Design tradeoffs**:
  - Rank r: Higher r increases expressiveness but also computational cost and memory usage
  - Prompt length m: Shorter m reduces efficiency gains but may harm performance if too small
  - Learning rates α1 and α2: Must be tuned carefully; imbalance can lead to poor convergence

- **Failure signatures**:
  - Degraded performance: Likely due to rank r too small or m too short
  - Training instability: May indicate α2 too large or rank r too high
  - No efficiency gain: Could mean m chosen too close to l, or batch size too small to observe savings

- **First 3 experiments**:
  1. Ablation: Remove low-rank matrices (m = l, r = 0) to confirm necessity of decomposition
  2. Efficiency test: Measure training/inference time and memory with varying m (e.g., 20, 40, 60, 80) on a small dataset
  3. Few-shot transfer: Apply PETL-style pre-training on source tasks and evaluate on target tasks with limited data

## Open Questions the Paper Calls Out

### Open Question 1
How does DEPT's performance scale with the size of the low-rank matrices (A and B) when the total number of trainable parameters is fixed? The paper only mentions choosing r to maintain the same number of parameters as vanilla PT, but does not investigate how different values of r (while keeping total parameters constant) affect performance.

### Open Question 2
How does DEPT's efficiency improvement compare when applied to encoder-only models (like BERT) versus encoder-decoder models (like T5)? The paper demonstrates DEPT works with different model types but doesn't provide a comparative analysis of efficiency improvements across architectures.

### Open Question 3
What is the impact of DEPT's two-learning-rate approach on convergence speed compared to single learning rate methods? While the paper demonstrates that two learning rates are better than one, it doesn't quantify how much faster DEPT converges compared to vanilla PT or other methods.

### Open Question 4
How does DEPT perform when the soft prompt length (m) approaches zero, relying entirely on the low-rank matrices to modify word embeddings? The paper notes that "a notable decline in performance is observed when the soft prompt is eliminated (m = 0)" but doesn't provide detailed analysis of this case.

### Open Question 5
How does DEPT's performance vary across different types of downstream tasks (e.g., classification vs. generation vs. question answering)? While DEPT is tested on diverse tasks, the paper doesn't analyze whether its effectiveness varies systematically by task type or what properties of tasks might influence its performance.

## Limitations
- Evaluation relies on limited PEFT baselines without including newer methods like LoRA or adapters
- Lack of comprehensive ablation study on the rank parameter r
- Claims of efficiency gains lack absolute measurements of time and memory
- Learning rate hyperparameters are important but not fully specified

## Confidence
- **High confidence**: DePT reduces sequence length and computational cost compared to vanilla PT
- **Medium confidence**: DePT outperforms state-of-the-art PEFT approaches in some scenarios
- **Low confidence**: The claim that efficiency advantage grows with model size is weakly supported

## Next Checks
1. Extend baseline comparison: Re-run experiments with additional PEFT methods (LoRA, Prefix Tuning, adapters) to determine if DePT's advantage holds
2. Ablation on rank parameter: Conduct systematic ablation varying r (8, 16, 32, 64) to quantify sensitivity and identify optimal trade-off
3. Scaling analysis: Evaluate DePT on wider range of model sizes (T5-Small, T5-Base, T5-Large, T5-3B) to test efficiency growth claim