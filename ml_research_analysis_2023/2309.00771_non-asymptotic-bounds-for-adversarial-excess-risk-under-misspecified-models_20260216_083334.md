---
ver: rpa2
title: Non-Asymptotic Bounds for Adversarial Excess Risk under Misspecified Models
arxiv_id: '2309.00771'
source_url: https://arxiv.org/abs/2309.00771
tags:
- adversarial
- risk
- function
- class
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of evaluating the generalization
  performance of adversarial estimators under misspecified models. The authors propose
  a general approach to analyzing the adversarial excess risk, which is decomposed
  into generalization error and approximation error.
---

# Non-Asymptotic Bounds for Adversarial Excess Risk under Misspecified Models

## Quick Facts
- arXiv ID: 2309.00771
- Source URL: https://arxiv.org/abs/2309.00771
- Reference count: 4
- Primary result: This paper establishes non-asymptotic upper bounds for adversarial excess risk under misspecified models, showing that adversarial robustness can hurt accuracy in nonparametric settings.

## Executive Summary
This paper addresses the fundamental challenge of evaluating adversarial estimators' generalization performance when models are misspecified. The authors develop a general framework that decomposes adversarial excess risk into generalization and approximation error components. They establish non-asymptotic bounds for Lipschitz loss functions and demonstrate improved bounds for quadratic loss in nonparametric regression. The theoretical results reveal that there exists a trade-off between adversarial robustness and accuracy, particularly in nonparametric settings where model misspecification is inevitable.

## Method Summary
The paper analyzes adversarial excess risk by decomposing it into generalization error and approximation error under the assumption that the target function belongs to a Hölder class. The approach uses feedforward neural networks with Lipschitz constraints (NN(W,L,K)) where the Lipschitz constant is bounded by K. For adversarial training, the estimator minimizes an empirical adversarial risk over the constrained network class. The generalization error bounds are derived using Rademacher complexity analysis, while approximation error bounds depend on the network's ability to approximate functions in the Hölder class. The framework applies to both classification and regression problems, with special consideration for quadratic loss in nonparametric regression where improved bounds are possible.

## Key Results
- Adversarial excess risk can be decomposed into generalization error and approximation error components
- Non-asymptotic upper bounds are established for Lipschitz loss functions under Hölder smoothness assumptions
- For quadratic loss in nonparametric regression, the adversarial excess risk bound improves to n^(-2α/(2d+5α)) up to logarithmic factors
- The results theoretically demonstrate that increasing adversarial robustness (through Lipschitz constraints) can hurt accuracy in nonparametric settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial excess risk can be decomposed into generalization error and approximation error under misspecified models.
- Mechanism: The analysis method explicitly separates the error into two components - generalization error (Egen) from finite sample effects and approximation error (Eapp) from model misspecification.
- Core assumption: The target function f⋆ belongs to a Hölder class Hα, and the estimator class NN(W,L,K) can approximate it up to a certain accuracy.
- Evidence anchors:
  - [abstract]: "We first show that adversarial risk is equivalent to the risk induced by a distributional adversarial attack under certain smoothness conditions."
  - [section]: "Our proposed analysis method includes investigations on both generalization error and approximation error."
  - [corpus]: Weak - no direct mention of decomposition in neighbor papers.
- Break condition: If the target function is not in a Hölder class or the estimator class cannot approximate it, the decomposition fails.

### Mechanism 2
- Claim: Adversarial robustness requires Lipschitz constraints on the estimation function class.
- Mechanism: The relationship between Lipschitz continuity and adversarial robustness is leveraged by using feedforward neural networks with norm constraints (NN(W,L,K)) where the Lipschitz constant is bounded by K.
- Core assumption: The loss function satisfies Lip1(ℓ) < ∞ and the estimation function class has uniformly bounded Lipschitz constant.
- Evidence anchors:
  - [section]: "Lemma 2 shows that the adversarial robustness is related to the Lipschitz constraint."
  - [section]: "We focus on the feedforward neural network with constraints on Lipschitz property."
  - [corpus]: Weak - no direct mention of Lipschitz constraints in neighbor papers.
- Break condition: If the loss function is not Lipschitz or the estimator class cannot maintain bounded Lipschitz constant, the adversarial robustness guarantee fails.

### Mechanism 3
- Claim: Adversarial excess risk bounds improve when using quadratic loss in nonparametric regression.
- Mechanism: The quadratic loss allows for tighter approximation error bounds compared to general Lipschitz loss functions.
- Core assumption: The underlying regression model uses quadratic loss and the target function is in a Hölder class.
- Evidence anchors:
  - [section]: "For the quadratic loss in nonparametric regression, we show that the adversarial excess risk bound can be improved over those for a general loss."
  - [section]: "Theorem 3 shows that the error rate of the adversarial excess risk of ˆfls_n can reach n^−2α/(2d+5α) up to a logarithmic factor when using the quadratic loss."
  - [corpus]: Weak - no direct mention of quadratic loss in neighbor papers.
- Break condition: If the loss function is not quadratic or the target function is not smooth enough, the improved bounds do not hold.

## Foundational Learning

- Concept: Hölder classes and smoothness properties
  - Why needed here: The target function f⋆ is assumed to belong to a Hölder class Hα, and the smoothness index α determines the convergence rates.
  - Quick check question: What is the definition of a Hölder class Hα with smoothness index α = r + β?

- Concept: Rademacher complexity and generalization bounds
  - Why needed here: The generalization error bounds are derived using Rademacher complexity analysis of the adversarial loss function class.
  - Quick check question: How does the Rademacher complexity of the adversarial loss function class relate to the generalization error?

- Concept: Approximation power of neural networks
  - Why needed here: The approximation error bounds depend on the ability of the neural network class NN(W,L,K) to approximate functions in the Hölder class Hα.
  - Quick check question: What factors determine the approximation error when using neural networks to estimate functions in a Hölder class?

## Architecture Onboarding

- Component map: Loss function selection (Lipschitz vs quadratic) -> Estimator class definition (NN(W,L,K) with norm constraints) -> Sample size and model complexity trade-off (W, L, K selection) -> Adversarial attack level (ε) as a tuning parameter

- Critical path:
  1. Define the target function class (Hölder class Hα)
  2. Choose the estimator class (NN(W,L,K) with appropriate W, L, K)
  3. Analyze the generalization error using Rademacher complexity
  4. Analyze the approximation error using neural network approximation theory
  5. Combine errors to obtain the final excess risk bound

- Design tradeoffs:
  - Larger W and L improve approximation power but increase generalization error
  - Larger K improves approximation power but may hurt generalization
  - Higher ε improves robustness but may increase excess risk

- Failure signatures:
  - Excess risk does not decrease with sample size (generalization error dominates)
  - Excess risk plateaus at high model complexity (approximation error dominates)
  - Excess risk increases with adversarial attack level (trade-off between robustness and accuracy)

- First 3 experiments:
  1. Vary W and L to find the optimal model complexity for a fixed sample size
  2. Test different ε values to quantify the trade-off between robustness and accuracy
  3. Compare Lipschitz vs quadratic loss for nonparametric regression problems

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the non-asymptotic bounds for adversarial excess risk be improved by considering different network architectures or activation functions beyond ReLU?
- Basis in paper: [explicit] The paper focuses on feedforward neural networks (FNNs) with ReLU activation and norm constraints, but does not explore alternative architectures or activations.
- Why unresolved: The paper does not investigate the impact of different network architectures or activation functions on the convergence rates of the adversarial excess risk bounds.
- What evidence would resolve it: Empirical or theoretical studies comparing the performance of various network architectures and activation functions on the adversarial excess risk bounds.

### Open Question 2
- Question: How does the choice of the adversarial attack level (ε) affect the trade-off between accuracy and robustness in practical applications?
- Basis in paper: [explicit] The paper demonstrates that increasing the adversarial robustness level can lead to a reduction in accuracy, but does not provide specific guidance on choosing ε in practice.
- Why unresolved: The paper establishes theoretical bounds but does not offer practical recommendations for selecting the adversarial attack level.
- What evidence would resolve it: Empirical studies evaluating the performance of adversarial estimators under different ε values in various real-world scenarios.

### Open Question 3
- Question: Can the analysis be extended to account for adversarial examples in both the input (X) and output (Y) spaces?
- Basis in paper: [explicit] The paper focuses on adversarial attacks in the input space X, but does not consider attacks in the output space Y.
- Why unresolved: The theoretical framework presented in the paper is specific to input space perturbations and does not address output space attacks.
- What evidence would resolve it: Theoretical extensions of the analysis to handle adversarial examples in both X and Y, along with empirical validation of the extended framework.

## Limitations
- The analysis assumes target functions belong to Hölder classes, which may not hold in many practical scenarios
- The bounds depend on unknown constants (Lip1(ℓ), Rademacher complexity terms) that are difficult to estimate in practice
- The framework is limited to feedforward neural networks with specific Lipschitz constraints, reducing generalizability

## Confidence
- High confidence: The decomposition of adversarial excess risk into generalization and approximation error is well-established in statistical learning theory.
- Medium confidence: The application of Rademacher complexity analysis to adversarial loss functions appears sound, though exact constants require careful verification.
- Low confidence: The improved bounds for quadratic loss in nonparametric regression depend on specific smoothness properties that may not generalize well to other loss functions.

## Next Checks
1. **Empirical verification of bounds**: Implement the theoretical bounds on synthetic data where the target function is known to belong to a Hölder class, and verify that the excess risk actually scales as predicted with sample size n.

2. **Robustness to smoothness assumptions**: Test the bounds when the target function only partially satisfies the Hölder smoothness conditions (e.g., has Hölder smoothness α only on a subset of the input space) to assess the sensitivity to the smoothness assumption.

3. **Alternative model classes**: Apply the same analysis framework to other model classes beyond feedforward neural networks (e.g., random forests or kernel methods) to verify that the key insights about the trade-off between adversarial robustness and accuracy are not specific to neural networks.