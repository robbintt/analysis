---
ver: rpa2
title: 'Video and Audio are Images: A Cross-Modal Mixer for Original Data on Video-Audio
  Retrieval'
arxiv_id: '2308.13820'
source_url: https://arxiv.org/abs/2308.13820
tags:
- video
- audio
- retrieval
- cross-modal
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of cross-modal retrieval between
  videos and audio, where distinct representations and semantic information make it
  difficult to establish relationships between modalities. To overcome this, the authors
  propose a novel framework consisting of a cross-modal mixer, a masked autoencoder
  for pre-training, and a cross-modal retriever for downstream tasks.
---

# Video and Audio are Images: A Cross-Modal Mixer for Original Data on Video-Audio Retrieval

## Quick Facts
- arXiv ID: 2308.13820
- Source URL: https://arxiv.org/abs/2308.13820
- Reference count: 40
- Key outcome: Proposed method improves video-audio retrieval accuracy by up to 2× over state-of-the-art approaches

## Executive Summary
This paper addresses the challenge of cross-modal retrieval between videos and audio by proposing a novel framework that treats both modalities as three-channel images. The approach consists of a cross-modal mixer that fuses original modality data while eliminating redundancy, a masked autoencoder for pre-training that reconstructs original data through fuse-then-separate objectives, and a cross-modal retriever for downstream tasks. The method demonstrates significant improvements in retrieval accuracy on real-world datasets, with the added benefit of functioning as a universal model transferable to other downstream tasks.

## Method Summary
The proposed framework converts video frames and audio spectrograms into three-channel image-like representations, then applies a cross-modal mixer (CMMixer) using CutMix strategy to fuse modalities. A masked autoencoder with single encoder-decoder structure is pre-trained on the fused data, where 50% of patches are masked and original modalities are reconstructed separately. For downstream retrieval tasks, the pre-trained encoder is duplicated into dual-stream architecture and fine-tuned using InfoNCE contrastive loss to align modality embeddings based on semantic similarity.

## Key Results
- Achieves up to 2× improvement in retrieval accuracy compared to previous state-of-the-art methods
- Demonstrates strong performance on two real-world datasets (M2M and YouTube-8M)
- Shows successful transfer capability as a universal model for other downstream tasks
- Ablation studies confirm CutMix mixing strategy outperforms Pixel Mixup and Mixup approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-modal mixer transforms spectrograms into image-like representations by duplicating into three channels, enabling shared spatial modeling
- Mechanism: Spectrograms are converted into three-channel images, aligning their format with video frames so a single transformer encoder can process both modalities uniformly
- Core assumption: The three-channel duplication preserves discriminative information while enabling cross-modal alignment through shared attention
- Evidence anchors: [abstract] "mapping both images and mel-spectrums into three channels, obtaining a three-channel representation through duplicating the mel-spectrums"; [section] "mapping both images and mel-spectrums into three channels, obtaining a three-channel representation through duplicating the mel-spectrums inspired by the multi-head attention"
- Break condition: If duplication destroys spectral features or introduces strong channel correlation, the shared encoder cannot learn meaningful alignment

### Mechanism 2
- Claim: Masked autoencoder with cross-modal fusion learns modality-common representations via a fuse-then-separate reconstruction objective
- Mechanism: The encoder receives a masked mixture of video and audio patches, encodes them jointly, and the decoder reconstructs each modality separately, forcing the encoder to learn shared features
- Core assumption: Joint reconstruction from masked mixed data induces the encoder to discover aligned cross-modal features that generalize to downstream tasks
- Evidence anchors: [abstract] "an encoder-decoder architecture is applied to achieve a fuse-then-separate task in the pre-training phase"; [section] "We feed masked fused representations into the encoder and reconstruct them with the decoder, ultimately separating the original data of two modalities"
- Break condition: If reconstruction quality is poor or modality-specific decoder fails, shared representation learning collapses

### Mechanism 3
- Claim: Contrastive InfoNCE fine-tuning aligns modality embeddings for retrieval by maximizing mutual information between matching pairs
- Mechanism: Fine-tuned encoders produce embeddings that are compared via cosine similarity in contrastive loss, pushing positive pairs closer and negatives apart
- Core assumption: The pre-trained modality-common representation already captures semantic alignment, so contrastive fine-tuning can specialize it to retrieval similarity
- Evidence anchors: [abstract] "we use contrastive learning to distinguish between positive and negative pairs based on the inherent semantics of cross-modal data"; [section] "We use this synchronization as a form of self-supervision, training our model to use separated modality data ... to measure the similarity of corresponding music and video segments"
- Break condition: If pre-training does not produce sufficient alignment, contrastive fine-tuning cannot compensate and retrieval performance degrades

## Foundational Learning

- Concept: Masked image modeling in transformers
  - Why needed here: The method extends MAE from single modality to cross-modal fusion, so understanding MAE's masking strategy is essential
  - Quick check question: How does masking ratio and patch selection affect reconstruction accuracy in MAE?

- Concept: Contrastive learning with InfoNCE loss
  - Why needed here: Downstream retrieval relies on InfoNCE to align video and audio embeddings; grasping its mechanism is key to tuning the fine-tuning stage
  - Quick check question: What role does temperature τ play in controlling similarity scaling in InfoNCE?

- Concept: Spectrogram representation and mel-frequency scaling
  - Why needed here: Audio is processed as mel-spectrograms converted to image format; understanding mel-scale properties informs channel duplication choices
  - Quick check question: Why is mel-scale preferred over linear frequency for audio-visual alignment tasks?

## Architecture Onboarding

- Component map: Video/Audio frames → CMMixer → Masked Fused Input → Single Encoder-Decoder (Pre-training) → Dual Encoders (Fine-tuning) → Contrastive Retriever

- Critical path:
  1. Convert video frames and audio spectrograms to three-channel images
  2. Apply CMMixer to fuse modalities into masked input
  3. Pre-train encoder-decoder on reconstruction task
  4. Duplicate encoder for dual-stream retrieval
  5. Fine-tune with InfoNCE contrastive loss

- Design tradeoffs:
  - Single vs. dual decoder: Solo decoder encourages modality-common representation; duet decoder risks over-specialization
  - Mixing strategy: CutMix preserves boundaries better than Pixel Mixup or Mixup, which introduce noise or fragment semantics
  - Masking ratio: Higher ratio increases efficiency but may hurt reconstruction quality

- Failure signatures:
  - Poor reconstruction in pre-training → shared representation fails
  - Low retrieval recall despite high pre-training accuracy → modality alignment insufficient
  - Inconsistent performance across datasets → overfit to M2M domain

- First 3 experiments:
  1. Ablation: Compare CutMix vs. Pixel Mixup vs. Mixup mixing strategies on reconstruction quality and retrieval recall
  2. Architecture: Test single decoder vs. two independent decoders on modality-common vs. modality-specific feature learning
  3. Masking: Vary mask ratio (0.3, 0.5, 0.7) and measure impact on pre-training efficiency and downstream retrieval accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Cross-Modal Mixer (CMMixer) compare to other fusion methods in terms of computational efficiency and scalability?
- Basis in paper: [explicit] The paper mentions that the CMMixer uses a single-stream encoder to handle single or multiple modalities efficiently
- Why unresolved: The paper does not provide a detailed comparison of the computational efficiency and scalability of the CMMixer with other fusion methods
- What evidence would resolve it: A detailed analysis of the computational cost and scalability of the CMMixer compared to other fusion methods, including metrics such as training time, memory usage, and model size

### Open Question 2
- Question: How does the performance of the CMMixer vary with different types of video and audio content?
- Basis in paper: [inferred] The paper mentions that the model is evaluated on two real-world datasets, M2M and YouTube-8M, but does not provide a detailed analysis of its performance on different types of content
- Why unresolved: The paper does not provide a comprehensive analysis of the model's performance on various types of video and audio content, such as different genres, lengths, or quality
- What evidence would resolve it: A detailed analysis of the model's performance on various types of video and audio content, including metrics such as retrieval accuracy, computational efficiency, and scalability

### Open Question 3
- Question: How does the proposed model handle audio-video pairs with complex temporal relationships?
- Basis in paper: [explicit] The paper mentions that the model uses a masked autoencoder to capture long-range temporal context, but does not provide a detailed analysis of its performance on audio-video pairs with complex temporal relationships
- Why unresolved: The paper does not provide a comprehensive analysis of the model's ability to handle audio-video pairs with complex temporal relationships, such as those with non-linear or asynchronous events
- What evidence would resolve it: A detailed analysis of the model's performance on audio-video pairs with complex temporal relationships, including metrics such as retrieval accuracy, computational efficiency, and scalability

## Limitations
- Implementation details for cross-modal mixer fusion methods remain unclear
- Specific hyperparameters for pre-training are not specified
- Claims about universal model capability lack comprehensive validation across multiple downstream tasks

## Confidence
- **High confidence**: The fundamental approach of using cross-modal mixer to align video and audio representations is technically sound and well-motivated
- **Medium confidence**: The pre-training strategy using masked autoencoder with fuse-then-separate objective is reasonable, but effectiveness depends heavily on specific implementation choices
- **Medium confidence**: The 2× improvement claim is based on reported results, but without access to code or detailed experimental procedures, independent verification is challenging

## Next Checks
1. Run ablation studies with CutMix vs. Pixel Mixup vs. Mixup mixing strategies across 5 independent trials to establish confidence intervals for reconstruction quality and retrieval recall
2. Evaluate the pre-trained model on at least two additional downstream tasks beyond retrieval (e.g., classification, segmentation) to verify the "universal model" claim
3. Test model performance across different video resolutions, audio sampling rates, and dataset domains to assess sensitivity to data characteristics