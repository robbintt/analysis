---
ver: rpa2
title: Leveraging Large Language Models to Generate Answer Set Programs
arxiv_id: '2307.07699'
source_url: https://arxiv.org/abs/2307.07699
tags:
- constants
- assign
- match
- each
- gpt-3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a neuro-symbolic method that combines the strengths
  of large language models and answer set programming to solve logic puzzles. The
  method employs an LLM to transform natural language descriptions of logic puzzles
  into answer set programs.
---

# Leveraging Large Language Models to Generate Answer Set Programs

## Quick Facts
- arXiv ID: 2307.07699
- Source URL: https://arxiv.org/abs/2307.07699
- Reference count: 40
- One-line primary result: Proposed approach achieves 81% accuracy on test set of 100 logic puzzles, outperforming zero-shot and few-shot GPT-3 and GPT-4 models.

## Executive Summary
This paper presents a neuro-symbolic method that combines large language models (LLMs) and answer set programming (ASP) to solve logic puzzles. The approach uses carefully engineered prompts to guide LLMs like GPT-3 and GPT-4 in transforming natural language puzzle descriptions into ASP programs. By breaking the problem into structured subtasks—constant extraction, predicate generation, and rule generation—the method enables LLMs to produce reasonably complex ASP with just a few in-context learning examples. The majority of errors made by the LLM are simple and human-correctable, making the pipeline practical as a draft generator for ASP programs.

## Method Summary
The method employs a step-by-step prompt engineering approach to guide GPT-3/GPT-4 in generating answer set programs from natural language puzzle descriptions. The pipeline consists of four main steps: (1) Constant Extraction using Prompt C to extract categories and constants from the puzzle, (2) Predicate Generation using Prompt P to generate predicates describing relations among constants, (3) Rule Generation using Prompt R1 for the Generate part (choice rules and facts) and Prompt R2 for the Define&Test part (constraints). The approach uses few-shot learning with 2 examples per step and includes optional post-processing steps for constant formatting and sentence paraphrasing to improve robustness.

## Key Results
- Achieved 81% accuracy on test set of 100 logic puzzles, significantly outperforming zero-shot and few-shot GPT-3 and GPT-4 models.
- The majority of errors made by the LLM are relatively simple and can be easily corrected by humans.
- The pipeline can be adapted to new domains with minimal prompt adjustments, demonstrated on Sudoku and Jobs Puzzle.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can act as a semantic parser when guided by carefully engineered prompts.
- Mechanism: The pipeline breaks the problem into structured subtasks—constant extraction, predicate generation, and rule generation—each with explicit prompt templates that constrain the LLM's output to syntactically valid ASP components.
- Core assumption: LLMs can generalize from a few in-context examples to correctly parse and translate natural language puzzle descriptions into formal logic structures.
- Evidence anchors: [abstract] "LLMs can generate reasonably complex answer set programs" and "majority of errors made by the LLM are relatively simple and can be easily corrected by humans." [section 3] Details how Prompt C, P, R1, and R2 systematically guide GPT-3 through each step.

### Mechanism 2
- Claim: The generated errors are mostly simple and human-correctable, making the pipeline practical as a draft generator.
- Mechanism: The errors fall into predictable categories—constant formatting, paraphrasing, and semantic errors in constraints—which are easy for a human to identify and fix after the fact.
- Core assumption: Error types are limited and systematic, not random, so they can be anticipated and corrected without rerunning the LLM.
- Evidence anchors: [abstract] "majority of errors made by the LLM are relatively simple and can be easily corrected by humans." [section 5] Table 2 and error examples show specific failure modes.

### Mechanism 3
- Claim: The pipeline can be adapted to new domains with minimal prompt adjustments.
- Mechanism: By abstracting the prompt structure (e.g., changing only the instruction or adding/removing clues) the same LLM can generate ASP for different problem types like Sudoku or the Jobs Puzzle.
- Core assumption: The LLM's language understanding generalizes across domains, and the prompt engineering is robust to changes in problem structure.
- Evidence anchors: [section 6] Shows adaptation to Sudoku and Jobs Puzzle by modifying prompts.

## Foundational Learning

- Concept: Answer Set Programming (ASP) basics—Generate-Define-Test structure, choice rules, constraints, and stable models.
  - Why needed here: Understanding ASP syntax is critical to design prompts that generate valid ASP programs and to debug the output.
  - Quick check question: Can you write a simple ASP rule that assigns exactly one value to a variable from a set of constants?

- Concept: Large Language Model prompting techniques—few-shot learning, in-context examples, and prompt engineering.
  - Why needed here: The success of the pipeline depends on crafting prompts that guide the LLM to produce syntactically and semantically correct ASP.
  - Quick check question: What is the difference between zero-shot and few-shot prompting, and why is few-shot used here?

- Concept: Natural language processing and semantic parsing.
  - Why needed here: The LLM must accurately extract entities, relations, and constraints from natural language puzzle descriptions.
  - Quick check question: How would you extract all entities and their categories from a given sentence?

## Architecture Onboarding

- Component map: Input -> Prompt C -> Prompt P -> Prompt R1 -> Prompt R2 -> ASP solver
- Critical path: Prompt C → Prompt P → Prompt R1 → Prompt R2 → ASP solver
- Design tradeoffs: Using LLMs adds linguistic flexibility but introduces unpredictability; prompts must be robust. Manual post-correction is required, trading off automation for correctness. Domain adaptation is possible but may require new in-context examples.
- Failure signatures: Invalid ASP syntax (e.g., wrong variable names, malformed rules), incorrect constants (non-integer where integer required), added or missing constraints (e.g., extra clues, omitted rules), misaligned predicates (variables not matching constants).
- First 3 experiments:
  1. Run the pipeline on a simple puzzle with known correct ASP to verify each prompt step produces valid output.
  2. Introduce a controlled error (e.g., incorrect constant formatting) and test if manual correction fixes the program.
  3. Adapt the prompts to a new domain (e.g., Sudoku) and verify the generated ASP solves the problem.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the pipeline be extended to solve logic puzzles that involve more complex spatial reasoning, such as those with additional constraints on the arrangement of objects in space?
- Basis in paper: [inferred] The paper demonstrates the pipeline's ability to generate ASP programs for Sudoku and the Jobs Puzzle, suggesting potential for handling more complex spatial reasoning tasks.
- Why unresolved: The paper does not explore the pipeline's performance on logic puzzles with complex spatial reasoning constraints beyond Sudoku.
- What evidence would resolve it: Testing the pipeline on a dataset of logic puzzles with complex spatial reasoning constraints and comparing its performance to other methods.

### Open Question 2
- Question: How does the pipeline's performance scale with the size and complexity of the logic puzzles?
- Basis in paper: [inferred] The paper evaluates the pipeline on a dataset of 100 logic puzzles, but does not explore its performance on larger or more complex puzzles.
- Why unresolved: The paper does not provide information on the pipeline's performance on puzzles with a larger number of variables or more complex constraints.
- What evidence would resolve it: Evaluating the pipeline on a dataset of logic puzzles with varying sizes and complexities, and analyzing its performance as the size and complexity increase.

### Open Question 3
- Question: Can the pipeline be adapted to solve logic puzzles in other domains, such as those involving temporal reasoning or probabilistic reasoning?
- Basis in paper: [inferred] The paper demonstrates the pipeline's ability to generate ASP programs for Sudoku and the Jobs Puzzle, suggesting potential for handling puzzles in other domains.
- Why unresolved: The paper does not explore the pipeline's performance on logic puzzles in domains beyond grid-based puzzles.
- What evidence would resolve it: Testing the pipeline on a dataset of logic puzzles from various domains, such as those involving temporal reasoning or probabilistic reasoning, and comparing its performance to other methods.

## Limitations
- The paper does not provide the actual prompts and examples used for GPT-3/GPT-4, which are critical for reproducing the results.
- The error analysis is limited to the paper's internal evaluation and lacks external validation or comparison to other methods for translating natural language to ASP.
- The adaptability to new domains is only demonstrated on two additional puzzles (Sudoku and Jobs Puzzle), which is insufficient to claim robustness across diverse logic puzzle types.

## Confidence
- High: The pipeline's general approach of using LLMs with step-by-step prompt engineering to generate ASP is well-specified and theoretically sound.
- Medium: The claim that "majority of errors are simple and human-correctable" is supported by internal analysis but not externally validated.
- Low: The claim of 81% accuracy on a test set of 100 logic puzzles is based on the paper's own evaluation without independent verification or comparison to other methods.

## Next Checks
1. **Prompt Replication**: Obtain and test the exact prompts and examples used for GPT-3/GPT-4 to verify the pipeline's effectiveness in generating valid ASP.
2. **Error Robustness**: Conduct a broader error analysis by introducing controlled errors at each step (e.g., incorrect constants, missing constraints) and test if manual correction reliably fixes the output.
3. **Domain Generalization**: Adapt the pipeline to a new, unseen logic puzzle domain (e.g., Einstein's Riddle or Sudoku with additional constraints) and evaluate if the generated ASP solves the problem correctly.