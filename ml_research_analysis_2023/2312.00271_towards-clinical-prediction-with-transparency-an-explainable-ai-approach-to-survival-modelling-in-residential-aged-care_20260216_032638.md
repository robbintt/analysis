---
ver: rpa2
title: 'Towards Clinical Prediction with Transparency: An Explainable AI Approach
  to Survival Modelling in Residential Aged Care'
arxiv_id: '2312.00271'
source_url: https://arxiv.org/abs/2312.00271
tags:
- survival
- care
- risk
- learning
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study successfully developed machine learning survival models
  for older adults in residential aged care using data from initial nursing assessments.
  Gradient boosting, random forest, and XGBoost models demonstrated the best predictive
  accuracy (C-index ~0.71), with the XGBoost model achieving a 6-month survival prediction
  AUROC of 0.746.
---

# Towards Clinical Prediction with Transparency: An Explainable AI Approach to Survival Modelling in Residential Aged Care

## Quick Facts
- arXiv ID: 2312.00271
- Source URL: https://arxiv.org/abs/2312.00271
- Reference count: 40
- Key outcome: Machine learning models achieved C-index ~0.71 for mortality prediction in aged care, with XGBoost providing transparent, clinically interpretable risk assessments

## Executive Summary
This study developed survival prediction models for older adults in residential aged care using machine learning and explainable AI techniques. The research demonstrates that gradient boosting, random forest, and XGBoost models can effectively predict 6-month mortality risk with good accuracy (AUROC 0.746). More importantly, the study employed SHAP values to make these complex models interpretable to clinicians, revealing that increased age, male gender, reduced mobility, poor health status, elevated pressure ulcer risk, and lack of appetite are the strongest predictors of imminent mortality. The work bridges the gap between technical model performance and clinical utility by providing transparent visualizations that help healthcare providers understand and trust the model's predictions.

## Method Summary
The study utilized data from initial nursing assessments of 11,944 residents aged 65+ across 40 long-term care facilities. Seven machine learning models were trained and evaluated: CoxPH, Elastic Net, Ridge, Lasso, Gradient Boosting, XGBoost, and Random Forest. Models were compared using C-index, Harrell's C-index, dynamic AUROC, and Integrated Brier Score across 20 experiments with 90/10 train/test splits. XGBoost was selected as the optimal model and calibrated using Platt scaling for time-specific predictions (1, 3, 6, and 12 months). SHAP values were employed to analyze predictor impacts and generate interpretable visualizations including summary plots, dependence plots, and waterfall plots for individual patient predictions.

## Key Results
- XGBoost model achieved 6-month survival prediction AUROC of 0.746 (95% CI 0.744-0.749)
- Gradient boosting, random forest, and XGBoost models showed best predictive accuracy with C-index values of 0.712-0.714
- SHAP analysis revealed increased age, male gender, reduced mobility, poor health status, elevated pressure ulcer risk, and lack of appetite as strongest mortality predictors
- The calibrated XGBoost model demonstrated good calibration and discrimination across multiple time points

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SHAP waterfall plots enable clinicians to understand how each predictor value contributes to a patient's specific survival prediction.
- Mechanism: SHAP values are calculated for each predictor, quantifying the marginal contribution of that predictor to the model's prediction for a specific patient. These values are then visualized in a waterfall plot showing how each predictor incrementally adjusts the prediction from the baseline (average risk) to the final patient-specific prediction.
- Core assumption: The SHAP value calculation accurately captures the contribution of each predictor, and the model itself is making predictions based on the true relationships between predictors and survival.
- Evidence anchors: "For patient-level analyses, we use SHAP Waterfall plots. These plots provide a granular examination of individual data points, detailing the contribution of each predictor to a specific prognosis." and "Employed SHAP values to analyze predictor impacts."

### Mechanism 2
- Claim: The XGBoost model's high C-index (~0.71) indicates it can reliably rank patients by their risk of mortality within the next 6 months.
- Mechanism: The C-index measures the concordance between predicted risk scores and actual survival times. A high C-index means that for any random pair of patients, the one predicted to have higher risk is more likely to have died sooner.
- Core assumption: The C-index calculation is valid for the study's data and censoring patterns, and the model is not overfitting to the training data.
- Evidence anchors: "The calibrated XGB model had a dynamic AUROC, when predicting survival at 6-months, of 0.746" and "For predicting survival across all time periods the GB, XGB and RF ensemble models had the best C-Index values of 0.714, 0.712 and 0.712 respectively."

### Mechanism 3
- Claim: The calibration of the XGBoost model using Platt scaling improves its ability to provide accurate probability estimates for 6-month survival.
- Mechanism: Platt scaling is a method to adjust the raw output scores of a model to better match the true probability of the event occurring. This is particularly important for survival models, where the goal is not just to rank patients but to provide accurate probability estimates.
- Core assumption: The calibration data is representative of the target population, and the relationship between the model's raw scores and the true probabilities is approximately logistic.
- Evidence anchors: "XGBoost was selected as the optimal model and calibrated for time-specific predictions at 1,3,6 and 12 months post admission using Platt scaling." and "The calibrated XGB model had a dynamic AUROC, when predicting survival at 6-months, of 0.746 (95% CI 0.744-0.749)."

## Foundational Learning

- SHAP values and interpretability: Why needed here: The study uses SHAP values to make the complex XGBoost model interpretable to clinicians. Quick check question: What is the main purpose of using SHAP values in this study?
- Survival analysis concepts: Why needed here: The study is developing a survival model, so understanding concepts like censoring, hazard functions, and concordance index is crucial. Quick check question: What is the concordance index, and why is it used to evaluate survival models?
- Machine learning model evaluation: Why needed here: The study compares multiple machine learning models using various metrics. Understanding these metrics (e.g., C-index, AUROC, IBS) is essential for interpreting the results. Quick check question: What is the difference between the C-index and the AUROC, and when would you use each?

## Architecture Onboarding

- Component map: Data preprocessing (missing value imputation, feature engineering) -> Model training (7 different algorithms) -> Model evaluation (multiple metrics) -> Calibration (Platt scaling) -> Interpretability (SHAP plots) -> Clinical application
- Critical path: The most critical components are the model training and evaluation steps. The choice of algorithm and the quality of the evaluation will determine the final model's performance.
- Design tradeoffs: The study chose to use a large number of predictors with missing value imputation, which increases the risk of including irrelevant or noisy features. However, this also allows for a more comprehensive model. The study also prioritized interpretability (using SHAP) over using a more complex, potentially more accurate model.
- Failure signatures: If the model's performance is poor, it could be due to overfitting, underfitting, or using irrelevant features. If the interpretability is poor, it could be due to the SHAP values not accurately representing the model's decision-making.
- First 3 experiments:
  1. Train a simple Cox proportional hazards model on the data and evaluate its performance using the C-index.
  2. Train an XGBoost model on the data and compare its performance to the Cox model using the same metrics.
  3. Generate SHAP plots for the XGBoost model to understand which predictors are most important.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does incorporating dynamic, time-varying covariates beyond the initial admission period affect the accuracy of survival predictions for residents in residential aged care?
- Basis in paper: [inferred] from the limitation section noting that the study relied solely on data collected around the time of admission and that this precluded incorporating temporally dynamic variables.
- Why unresolved: The study was constrained by using only initial admission data, preventing analysis of how changes over time in functional capacity, mobility, falls frequency, and appetite impact mortality risk.
- What evidence would resolve it: A longitudinal study collecting data on these variables at multiple time points post-admission, comparing survival prediction accuracy with and without these dynamic covariates.

### Open Question 2
- Question: What is the generalizability of the developed survival models across different residential aged care providers with varying patient demographics and data collection protocols?
- Basis in paper: [inferred] from the limitation section acknowledging the cohort consisted of a heterogeneous mix from a single private provider and the need for evaluating model transportability across diverse validation datasets.
- Why unresolved: The study's single-provider dataset may not represent the broader population of residents in different care settings.
- What evidence would resolve it: Testing the models on multiple, diverse datasets from various providers to assess consistency in performance metrics like C-index, AUROC, and calibration.

### Open Question 3
- Question: How do healthcare providers perceive the utility and trustworthiness of the model's insights, and what are the measurable impacts on clinical workflows and decision-making when the model is integrated into day-to-day practice?
- Basis in paper: [inferred] from the limitation section noting the lack of assessment of real-world clinical implementation factors and the need for user-centred design principles.
- Why unresolved: The study focused on technical efficacy without evaluating the model's practical application in clinical settings.
- What evidence would resolve it: Conducting pragmatic clinical trials to assess provider acceptance, model integration into workflows, and impacts on patient care outcomes and decision-making processes.

## Limitations

- The dataset represents a specific Australian aged care population with structured InterRAI assessments, potentially limiting applicability to other healthcare systems or populations with different assessment protocols.
- The 10% test split, while appropriate for initial validation, may not provide sufficient power for detecting subtle performance differences.
- While SHAP values provide local interpretability, the complex interactions in the XGBoost model may still limit complete transparency for clinical decision-making.

## Confidence

- Model predictive performance: **High** - Well-validated with multiple metrics and 20-fold cross-validation
- Clinical interpretability through SHAP: **Medium** - Aligns with clinical expectations but limited to known predictor relationships
- Generalizability to other settings: **Low** - Specific to Australian aged care with InterRAI assessments

## Next Checks

1. Test model performance on an independent dataset from different aged care facilities or countries to assess generalizability
2. Conduct prospective clinical validation where the model's predictions are compared against actual clinical decision outcomes
3. Perform sensitivity analysis by retraining the model with different predictor subsets to identify which features contribute most to performance and which could be omitted without significant accuracy loss