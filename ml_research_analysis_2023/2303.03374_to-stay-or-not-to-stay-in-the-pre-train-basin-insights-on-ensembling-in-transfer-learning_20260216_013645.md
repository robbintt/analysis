---
ver: rpa2
title: 'To Stay or Not to Stay in the Pre-train Basin: Insights on Ensembling in Transfer
  Learning'
arxiv_id: '2303.03374'
source_url: https://arxiv.org/abs/2303.03374
tags:
- local
- learning
- ensemble
- more
- pre-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether ensembles of models fine-tuned from
  a single pre-trained checkpoint can be improved by better exploring the loss landscape
  around the pre-trained basin. While local methods like SWA sample from the same
  basin, semi-local methods like FGE and SSE use cyclical learning rates to explore
  further.
---

# To Stay or Not to Stay in the Pre-train Basin: Insights on Ensembling in Transfer Learning

## Quick Facts
- arXiv ID: 2303.03374
- Source URL: https://arxiv.org/abs/2303.03374
- Reference count: 40
- Key outcome: Local ensemble methods can improve diversity without sacrificing transfer benefits, but semi-local methods fail to close the gap with global ensembles due to basin exit

## Executive Summary
This paper investigates whether ensembles of models fine-tuned from a single pre-trained checkpoint can be improved by exploring the loss landscape around the pre-trained basin. The study compares local methods like SWA (sampling within the basin) with semi-local methods like FGE and SSE (using cyclical learning rates to explore further). Experiments show that with optimal hyperparameters, semi-local methods can match or exceed local ensemble quality, but they fail to close the gap with global ensembles trained from multiple pre-trained checkpoints. The key limitation is that leaving the pre-trained basin leads to loss of transfer learning benefits and significant degradation in both individual model and ensemble quality.

## Method Summary
The paper uses ResNet-50 architecture and three datasets (CIFAR-10, CIFAR-100, SUN-397) with both supervised and self-supervised pre-training on ImageNet using 5 independently pre-trained checkpoints. For each target dataset, a grid search finds optimal fine-tuning hyperparameters (weight decay and initial learning rate). SSE and FGE methods use cycling learning rates with varying cycle lengths and maximum learning rates. The study compares Local DE (models from same checkpoint), Global DE (models from different checkpoints), and semi-local methods to analyze basin exploration effects.

## Key Results
- Local methods like SWA can improve ensemble diversity by sampling within the pre-trained basin
- Semi-local methods (FGE, SSE) with optimal hyperparameters can match or exceed local ensemble quality
- All methods fail to close the performance gap with global ensembles due to basin exit causing loss of transfer benefits
- Leaving the pre-trained basin leads to significant degradation in both individual model and ensemble quality

## Why This Works (Mechanism)

### Mechanism 1
Exploring within the pre-train basin improves ensemble diversity without sacrificing transfer benefits. Methods like SWA sample from the low-loss region around a fine-tuned model, increasing diversity while staying within the basin where pre-trained weights still provide good inductive bias. This works when the pre-train basin is large enough to contain multiple diverse, high-quality solutions.

### Mechanism 2
Leaving the pre-train basin leads to loss of transfer learning advantages and ensemble degradation. When learning rate cycles push weights outside the basin, the model relies less on pre-trained features and more on task-specific fitting, which hurts generalization and calibration. The basin boundary corresponds to a region where pre-trained features cease to be useful for the target task.

### Mechanism 3
Optimal hyperparameter tuning allows semi-local methods (FGE, SSE) to explore the basin effectively without exiting it. By choosing cycle length and learning rate carefully, these methods can sample diverse models from within the basin, matching or exceeding local ensemble quality. This works when there exists a hyperparameter regime where exploration is confined to the basin.

## Foundational Learning

- Concept: Loss landscape basins and mode connectivity
  - Why needed here: Understanding that models from the same pre-trained checkpoint lie in the same basin explains why naive ensembling has limited diversity
  - Quick check question: If two models are in the same basin, what happens to their loss when interpolated linearly?

- Concept: Cyclical learning rates and exploration
  - Why needed here: These methods use cycles to move within/across basins; knowing how learning rate affects basin exit is key to tuning
  - Quick check question: What learning rate schedule component helps a model leave its current basin?

- Concept: Transfer learning inductive bias
  - Why needed here: The paper's central claim is that exiting the basin loses pre-training benefits; understanding what inductive bias means is essential
  - Quick check question: Why might a model fine-tuned from a pre-trained checkpoint generalize better than one trained from scratch?

## Architecture Onboarding

- Component map: Pre-trained backbone -> Fine-tuning head -> Cyclical learning rate scheduler -> Ensemble aggregation
- Critical path:
  1. Load pre-trained checkpoint
  2. Replace final layer
  3. Fine-tune with cosine learning rate schedule to get base model
  4. Apply cyclical schedule to sample additional models
  5. Average predictions for final ensemble
- Design tradeoffs: Basin exploration vs. transfer benefit retention, Ensemble size vs. individual model quality, Cycle length vs. computational cost
- Failure signatures: High learning rates → models leave basin → accuracy drops, Very short cycles → models too similar → no ensemble gain, Too many cycles → overfitting to fine-tuning data
- First 3 experiments:
  1. Compare Local DE vs. Global DE to confirm basin effect
  2. Run SSE with low learning rate to test basin exploration
  3. Run SSE with high learning rate to observe basin exit and degradation

## Open Questions the Paper Calls Out

### Open Question 1
What specific modifications to existing semi-local ensemble methods could allow them to explore the pre-trained basin more effectively without losing transfer learning benefits? The paper identifies this problem but doesn't propose concrete solutions beyond noting that new methods need to be developed with consideration of transfer learning specifics.

### Open Question 2
What is the theoretical explanation for why networks fine-tuned from the same pre-trained checkpoint consistently end up in the same basin of the loss landscape? While the empirical observation is clear, the paper doesn't explore the theoretical reasons behind this phenomenon or its implications for transfer learning.

### Open Question 3
How does the effectiveness of local versus semi-local ensemble methods vary across different types of pre-training (supervised vs self-supervised) and target tasks? The paper presents results for different pre-training types but doesn't analyze the underlying factors that make certain ensemble methods more effective for specific pre-training approaches.

## Limitations
- Findings may not generalize to architectures beyond ResNet-50 or to domains outside computer vision
- Optimal hyperparameter tuning for semi-local methods is computationally expensive, limiting practical applicability
- The study assumes the pre-train basin is sufficiently large to contain diverse solutions, which may not hold for all tasks

## Confidence

**High confidence:** The mechanism that local exploration within the pre-train basin can improve ensemble diversity without sacrificing transfer benefits

**Medium confidence:** The claim that exiting the pre-train basin leads to significant degradation in both individual model and ensemble quality

**Low confidence:** The assertion that semi-local methods with optimal hyperparameters can match or surpass the quality of independently fine-tuned ensembles

## Next Checks

1. Test the pre-train basin hypothesis with different architectures (e.g., Vision Transformers) and larger models to assess generalizability
2. Conduct experiments with various pre-training objectives (e.g., masked autoencoding, contrastive learning) to determine if basin characteristics vary
3. Perform ablation studies on ensemble size to quantify the trade-off between diversity and individual model quality in different basin exploration strategies