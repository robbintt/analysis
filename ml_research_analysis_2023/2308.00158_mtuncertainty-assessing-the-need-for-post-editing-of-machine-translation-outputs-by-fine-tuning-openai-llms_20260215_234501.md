---
ver: rpa2
title: 'MTUncertainty: Assessing the Need for Post-editing of Machine Translation
  Outputs by Fine-tuning OpenAI LLMs'
arxiv_id: '2308.00158'
source_url: https://arxiv.org/abs/2308.00158
tags:
- translation
- quality
- task
- data
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examines the potential of large language models (LLMs)
  for translation quality estimation (TQE), a critical step in the translation production
  process. The authors fine-tune OpenAI's ChatGPT model using historical post-editing
  data from eight language pairs to predict whether machine-translated segments require
  editing.
---

# MTUncertainty: Assessing the Need for Post-editing of Machine Translation Outputs by Fine-tuning OpenAI LLMs

## Quick Facts
- arXiv ID: 2308.00158
- Source URL: https://arxiv.org/abs/2308.00158
- Reference count: 6
- Key outcome: Fine-tuning GPT-3.5 on historical post-editing data achieves promising binary classification performance for predicting translation quality, with potential cost savings of up to 38.8% for German.

## Executive Summary
This paper explores the use of large language models for translation quality estimation by fine-tuning OpenAI's ChatGPT on historical post-editing data. The approach frames quality estimation as a binary classification task - predicting whether machine-translated segments require post-editing or not. Using data from eight language pairs in the ERP domain, the fine-tuned model (GPT4QE) demonstrates good performance on English-Italian and English-German test sets. The method shows potential for reducing post-editing effort and costs compared to processing all MT output through post-editing workflows.

## Method Summary
The authors fine-tune OpenAI's ChatGPT API using triple input format (English source, MT output, post-edited gold standard) extracted from historical translation projects. The model is trained to classify segments as requiring editing or not based on real post-editing decisions. Training uses 4000 lines split 9:1 train:test ratio from ERP domain content translated by the SAP MT engine. The approach avoids prompt engineering limitations by directly fine-tuning model weights rather than constraining input within token limits.

## Key Results
- Fine-tuned GPT3.5 demonstrates good performance on translation quality prediction tasks
- Binary classification approach successfully predicts post-editing requirements
- Potential cost savings of up to 38.8% for German language pair through reduced post-editing effort

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning a large language model on historical post-editing data enables the model to learn patterns of translation errors specific to the MT engine and language pair. The model learns to classify segments as "needs editing" or "does not need editing" by observing real-world post-editing decisions made by professional translators.

### Mechanism 2
The binary classification approach simplifies the quality estimation task compared to traditional methods that predict quality scores or detect specific error types. By framing the problem as a binary decision, the model can focus on learning the most salient features that distinguish segments requiring human intervention.

### Mechanism 3
Fine-tuning avoids the token limitations of prompt engineering while still leveraging the LLM's capabilities for the task. By fine-tuning the model weights directly on the classification task, the system bypasses the need to fit the entire input context within prompt token limits.

## Foundational Learning

- Concept: Post-editing distance patterns
  - Why needed here: The model needs to understand what kinds of MT errors lead to post-editing decisions
  - Quick check question: Can you explain the difference between a segment that needs no editing and one that requires extensive post-editing?

- Concept: Binary classification in NLP
  - Why needed here: The entire approach relies on framing quality estimation as a binary classification problem
  - Quick check question: What are the advantages and disadvantages of binary classification versus continuous quality scoring for MT output?

- Concept: Fine-tuning vs prompt engineering trade-offs
  - Why needed here: The choice to fine-tune rather than use prompting is fundamental to the methodology
  - Quick check question: Under what circumstances would prompt engineering be preferable to fine-tuning for a similar task?

## Architecture Onboarding

- Component map: Data preparation pipeline -> OpenAI API fine-tuning process -> Prediction service
- Critical path: Collect historical post-editing data -> Prepare training triples -> Fine-tune GPT4QE model -> Deploy prediction service -> Integrate with CAT tools
- Design tradeoffs: Binary classification sacrifices granularity for simplicity; fine-tuning provides better performance but requires maintaining custom models per language pair
- Failure signatures: High false negative rates indicate insufficient error pattern learning; performance degradation suggests domain drift
- First 3 experiments:
  1. Train baseline GPT4QE on English-Italian data and measure classification accuracy
  2. Compare predictions against human post-editors on validation set
  3. Test performance on different domain (e.g., medical content)

## Open Questions the Paper Calls Out

### Open Question 1
How well can GPT4QE generalize to new domains and language pairs beyond the ERP content and the eight languages tested? The authors plan to explore the model's performance on a wider range of language pairs and domains in future work.

### Open Question 2
Can GPT4QE be improved by using prompt engineering techniques instead of fine-tuning? The authors note they did not use prompt engineering due to token limitations, but suggest it is worthy to verify how well it performs.

### Open Question 3
What is the optimal trade-off between model size and performance for TQE tasks? The authors found that simply increasing the sizes of LLMs did not lead to apparent better performances on the TQE task.

## Limitations

- Analysis limited to English-Italian and English-German language pairs from the ERP domain
- Binary classification may oversimplify quality assessment, missing nuanced error patterns
- Performance metrics focus on classification accuracy rather than actual post-editing efficiency gains

## Confidence

**High Confidence**: Feasibility of using fine-tuned LLMs for binary TQE classification is well-supported by experimental results showing promising accuracy.

**Medium Confidence**: Claimed cost savings are based on theoretical reductions rather than empirical workflow studies; generalizability to other domains remains to be validated.

**Low Confidence**: Model's robustness to MT engine updates or domain shifts is not empirically tested; impact of fine-tuning versus prompting lacks comparative analysis.

## Next Checks

1. Evaluate the fine-tuned model on a completely different domain (e.g., medical or legal content) to assess performance degradation and identify domain-specific limitations.

2. Retrain the model on data from a different MT engine or after MT engine updates to measure how quickly performance degrades and what retraining frequency is needed.

3. Conduct a controlled experiment where human post-editors use the model's predictions to prioritize segments, measuring actual time savings and quality improvements versus baseline workflows.