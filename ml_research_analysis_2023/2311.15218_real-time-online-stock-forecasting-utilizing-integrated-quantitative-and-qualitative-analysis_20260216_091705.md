---
ver: rpa2
title: Real-Time Online Stock Forecasting Utilizing Integrated Quantitative and Qualitative
  Analysis
arxiv_id: '2311.15218'
source_url: https://arxiv.org/abs/2311.15218
tags:
- stock
- data
- used
- sentiment
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel, publicly available dataset combining
  quantitative stock market data (technical indicators, fundamental analysis) and
  qualitative data (news articles, tweets, TV captions, radio transcripts) for eight
  stocks and the DJIA from 2018-2022. The dataset totals over 1.4 million text entries
  and includes extensive preprocessing and sentiment analysis using BERT-based models
  and traditional NLP tools.
---

# Real-Time Online Stock Forecasting Utilizing Integrated Quantitative and Qualitative Analysis

## Quick Facts
- arXiv ID: 2311.15218
- Source URL: https://arxiv.org/abs/2311.15218
- Reference count: 40
- Primary result: Novel dataset combining quantitative and qualitative stock data with high sentiment classification accuracy (87%) and strong Spearman correlation with returns (60%+ for DJIA)

## Executive Summary
This paper introduces a publicly available dataset that integrates quantitative stock market data (technical indicators, fundamental analysis) with qualitative data (news articles, tweets, TV captions, radio transcripts) for eight stocks and the DJIA from 2018-2022. The dataset contains over 1.4 million text entries and includes extensive preprocessing and sentiment analysis using BERT-based models and traditional NLP tools. The authors fine-tune multiple BERT variants, including a custom model pre-trained on their domain-specific corpus, achieving high sentiment classification accuracy and strong correlation with stock returns. This work provides a comprehensive, training-ready resource for integrating qualitative intelligence into stock market forecasting.

## Method Summary
The authors created a comprehensive dataset by collecting historical stock data, technical indicators (45), fundamental analysis features (220), and qualitative data from multiple sources including news archives, Twitter, and broadcast media. They applied extensive preprocessing including cleaning, normalization, tokenization, and sentiment analysis using BERT-based models and lexicon-based methods. The sentiment analysis pipeline utilized multiple BERT architectures (DistilBERT, DistilRoBERTa, FinBERT, FinancialBERT) with domain-specific pretraining on their collected corpus. Technical indicators were calculated using the ta library, and sentiment scores were correlated with stock returns using Spearman correlation to evaluate predictive power.

## Key Results
- Achieved sentiment classification accuracy up to 87% using custom FinBERT model
- Strong Spearman correlation between sentiment and stock returns (>60% for DJIA)
- Domain-specific pretraining improved FinBERT F1-score from 0.83 to 0.85
- DistilBERT outperformed FinBERT in accuracy and F1-Macro score
- Dataset contains over 1.4 million text entries spanning 2018-2022

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific pretraining of BERT architectures improves sentiment classification accuracy on financial text.
- Mechanism: Models pretrained on general text benefit from additional training on financial-specific corpora, allowing them to learn specialized vocabulary and context unique to finance.
- Core assumption: Financial text has distinct terminology and context that differs significantly from general language corpora.
- Evidence anchors: Abstract mentions continued pre-training over pre-trained BERT architectures; section discusses semantic relationships in financial terminology.
- Break condition: If domain-specific pretraining data is noisy or unrepresentative of the target financial domain.

### Mechanism 2
- Claim: Combining quantitative technical/fundamental indicators with qualitative sentiment data improves stock forecasting accuracy.
- Mechanism: Integrating numerical data reflecting company performance with textual data reflecting market sentiment captures a more holistic view of factors influencing stock prices.
- Core assumption: Stock prices are influenced by both quantitative factors (company performance, market trends) and qualitative factors (investor sentiment, news events).
- Evidence anchors: Abstract shows strong Spearman correlation between sentiment and stock returns; section discusses challenges of analyzing big data for predictions.
- Break condition: If qualitative data is not representative of investor sentiment or if sentiment analysis is not accurate.

### Mechanism 3
- Claim: Using multiple BERT variants and fine-tuning techniques leads to improved sentiment classification performance.
- Mechanism: Experimenting with different BERT architectures and fine-tuning techniques identifies the best combination for sentiment classification on financial text.
- Core assumption: Different BERT architectures and fine-tuning techniques have different strengths and weaknesses for specific tasks.
- Evidence anchors: Abstract mentions high sentiment classification accuracy; section compares performance of DistilBERT, DistilRoBERTa, FinBERT, and BERT.
- Break condition: If chosen BERT variants and fine-tuning techniques are not well-suited for the specific task.

## Foundational Learning

- Concept: Sentiment analysis techniques (rule-based, embedding-based, lexicon-based)
  - Why needed here: To extract sentiment from qualitative data and convert it into numerical format for stock forecasting
  - Quick check question: What are the key differences between rule-based, embedding-based, and lexicon-based sentiment analysis techniques, and when would each be most appropriate?

- Concept: Spearman correlation
  - Why needed here: To measure the strength and direction of the monotonic relationship between sentiment and stock returns
  - Quick check question: What does a Spearman correlation coefficient of 0.6 indicate about the relationship between sentiment and stock returns?

- Concept: Technical indicators (moving averages, RSI, MACD, etc.)
  - Why needed here: To provide quantitative data reflecting market trends and company performance for conjunction with sentiment data
  - Quick check question: How do moving averages, RSI, and MACD indicators differ in their calculation and interpretation?

## Architecture Onboarding

- Component map: Data collection module (web scraping, APIs) -> preprocessing pipeline (cleaning, tokenization, stemming) -> sentiment analysis module (BERT variants, lexicon-based methods) -> feature extraction module (technical/fundamental indicators) -> forecasting model (neural network)
- Critical path: Data collection -> Preprocessing -> Sentiment analysis -> Feature extraction -> Forecasting model
- Design tradeoffs: Accuracy vs. computational cost (BERT variants), breadth vs. depth of data (multiple data sources vs. focused collection), interpretability vs. performance (complex models vs. simpler models)
- Failure signatures: Poor sentiment classification accuracy, low Spearman correlation with stock returns, overfitting to training data, inability to handle new data points
- First 3 experiments:
  1. Fine-tune a pre-trained BERT model on the financial sentiment analysis dataset and evaluate its performance on a held-out test set
  2. Integrate the sentiment data with technical/fundamental indicators and train a simple forecasting model (e.g., linear regression) to predict stock returns
  3. Experiment with different BERT architectures (DistilBERT, DistilRoBERTa, FinBERT, FinancialBERT) and fine-tuning techniques to identify the best combination for sentiment classification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does pre-training BERT architectures on the collected domain-specific corpus improve sentiment classification performance compared to only fine-tuning on labeled financial datasets?
- Basis in paper: [explicit] The paper explicitly tests this by pre-training FinBERT and FinancialBERT on their collected 1.4M text entries and comparing performance to fine-tuned versions
- Why unresolved: The paper shows improved performance for FinBERT after domain pre-training but no improvement for FinancialBERT, suggesting mixed results
- What evidence would resolve it: Systematic experiments comparing multiple BERT variants with and without domain pre-training on the same labeled financial dataset

### Open Question 2
- Question: Which qualitative data source (tweets, news archives, radio transcripts, or news articles) provides the most predictive sentiment signals for stock returns?
- Basis in paper: [explicit] The paper calculates Spearman correlations between sentiment from each data source and stock returns
- Why unresolved: While correlations are provided, the paper doesn't explore why certain data sources perform better or how to optimally combine them
- What evidence would resolve it: Detailed analysis of correlation patterns across different stocks and time periods, plus experiments testing various weighted combinations

### Open Question 3
- Question: How do individual investor emotions extracted from tweets correlate with stock returns compared to overall sentiment polarity?
- Basis in paper: [explicit] The paper performs emotion analysis on Twitter data, extracting 9 emotions and calculating correlations with stock returns
- Why unresolved: The paper identifies correlations but doesn't explore the predictive power of emotion features versus sentiment polarity
- What evidence would resolve it: Comparative experiments testing stock return prediction models using sentiment polarity alone, emotion features alone, and combined features

## Limitations

- Unknown BERT fine-tuning hyperparameters could affect reproducibility of accuracy metrics
- Correlation results may be influenced by temporal alignment artifacts between qualitative data timestamps and stock prices
- Domain-specific pretraining effectiveness lacks ablation studies comparing against non-domain-specific baselines

## Confidence

- High confidence in dataset construction methodology and availability of integrated quantitative-qualitative data
- Medium confidence in sentiment classification accuracy claims (87%), as this depends on exact BERT training procedures
- Medium confidence in Spearman correlation results, pending verification of temporal alignment methodology
- Low confidence in causal claims about sentiment driving stock returns without controlling for confounding market factors

## Next Checks

1. Replicate sentiment analysis pipeline on a held-out subset of qualitative data to verify classification accuracy claims
2. Conduct Granger causality tests to determine if sentiment predicts stock returns rather than just correlating with them
3. Perform ablation study removing domain-specific pretraining to quantify its contribution to model performance improvements