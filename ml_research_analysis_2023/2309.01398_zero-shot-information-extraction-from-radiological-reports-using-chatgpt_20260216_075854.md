---
ver: rpa2
title: Zero-shot information extraction from radiological reports using ChatGPT
arxiv_id: '2309.01398'
source_url: https://arxiv.org/abs/2309.01398
tags:
- tumor
- chatgpt
- information
- extraction
- lymph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored using ChatGPT for zero-shot information extraction
  from radiological reports. A prompt template was designed to extract 11 lung cancer-related
  features from CT reports, with a post-processing module to structure the outputs.
---

# Zero-shot information extraction from radiological reports using ChatGPT

## Quick Facts
- arXiv ID: 2309.01398
- Source URL: https://arxiv.org/abs/2309.01398
- Reference count: 38
- Primary result: ChatGPT achieves competitive zero-shot information extraction for basic lung cancer CT report features but struggles with nuanced medical distinctions like tumor density and lymph node status.

## Executive Summary
This study explores using ChatGPT for zero-shot information extraction from radiological reports, focusing on 11 lung cancer-related features from CT reports. A prompt template was designed to guide the LLM in extracting structured information, with a post-processing module to format the outputs. Results show ChatGPT performs competitively on simple categorical attributes like tumor location, size, and pleural invasion, matching a rule-based baseline. However, it struggles with tasks requiring fine-grained medical knowledge, such as tumor density classification and lymph node status. The study also highlights the trade-offs of adding medical knowledge to prompts and the challenges of maintaining consistency across runs.

## Method Summary
The method involves designing a prompt template with CT report text and 11 extraction questions, sending these prompts to ChatGPT via web interface, and post-processing the unstructured responses into structured format using regular expressions. The approach is compared against a rule-based MTQA baseline on 847 CT reports from Peking University Cancer Hospital. Performance is evaluated using accuracy, precision, recall, and F1 score for each extraction task.

## Key Results
- ChatGPT achieves competitive performance on tumor location, size, spiculation, lobulation, and pleural invasion/extraction, matching the rule-based MTQA baseline.
- Struggles with tumor density classification (solid/ground-glass) and lymph node status due to limited understanding of synonyms and fine-grained distinctions.
- Adding medical knowledge to prompts improves some tasks but degrades others, and consistency across runs is high for simple features but lower for density and shape-related attributes.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot information extraction is possible when the LLM already understands the target domain vocabulary and reasoning patterns.
- Mechanism: The LLM has been pre-trained on large corpora including medical text, so it can map free-text radiological descriptions to structured categories without task-specific fine-tuning.
- Core assumption: Pre-training on general and domain-adjacent text is sufficient for the model to infer correct structured outputs from prompts.
- Evidence anchors:
  - [abstract]: "With the large language models achieving good performances on various downstream NLP tasks without parameter tuning, it becomes possible to use large language models for zero-shot information extraction."
  - [section]: "ChatGPT can achieve competitive performances on several tasks compared with the baseline IE system."
- Break condition: If domain-specific terminology or reasoning is too specialized, the LLM fails to correctly map to structured outputs, as seen with tumor density and lymph node classification tasks.

### Mechanism 2
- Claim: Prompt engineering with domain-specific instructions can significantly improve extraction accuracy.
- Mechanism: Adding prior medical knowledge to the prompt guides the LLM's reasoning, reducing ambiguity and errors from synonym confusion or missing context.
- Core assumption: LLMs can leverage explicit instructions to override default inference behaviors.
- Evidence anchors:
  - [section]: "Adding additional prior medical knowledge into the prompt may promote the information extraction for some questions, but may have a negative effect on some questions."
  - [corpus]: Weak - related works show similar improvements but also mixed results depending on task complexity.
- Break condition: Over-specification in prompts can degrade performance on tasks where the model's default reasoning is already effective.

### Mechanism 3
- Claim: Consistency in outputs is a key limitation of using LLMs for structured extraction.
- Mechanism: LLMs generate different responses to the same prompt due to inherent stochasticity, leading to variable structured results unless explicitly constrained.
- Core assumption: The model's output variance is a function of prompt design and internal sampling behavior.
- Evidence anchors:
  - [section]: "Since the language model produces different answers to the same prompt each time, we randomly selected 100 CT reports and repeated the question-answering procedure five times for each to test the consistency of the extraction results."
  - [section]: "The possible reason is that ChatGPT with prompt-base may regard some similar words in the embedding space as synonyms..."
- Break condition: If prompts do not constrain output format or include deterministic sampling settings, consistency will remain low for nuanced attributes like density or shape.

## Foundational Learning

- Concept: Named Entity Recognition (NER)
  - Why needed here: Many IE tasks require identifying and categorizing medical entities (tumor location, size, lymph node status) from free text.
  - Quick check question: What are the main entity types in a lung cancer CT report that need to be extracted?

- Concept: Relation Extraction
  - Why needed here: Understanding relationships between entities (e.g., tumor location relative to pleura) is critical for structured output.
  - Quick check question: How would you represent the relationship between "tumor" and "pleural invasion" in a structured form?

- Concept: Prompt Engineering
  - Why needed here: The quality of extraction depends heavily on how questions and instructions are framed in the prompt.
  - Quick check question: What is the effect of adding domain-specific constraints to a prompt on LLM output accuracy?

## Architecture Onboarding

- Component map:
  Input -> Prompt Template -> LLM (ChatGPT) -> Post-Processing Module -> Structured Output

- Critical path:
  1. Receive CT report
  2. Generate prompt using template
  3. Send prompt to LLM
  4. Post-process response into structured format
  5. Validate structured output

- Design tradeoffs:
  - Zero-shot vs fine-tuning: Zero-shot avoids annotation costs but may underperform on complex tasks.
  - Prompt specificity: More specific prompts reduce errors but may limit flexibility.
  - Output consistency: Stricter output formats improve reliability but may reduce coverage.

- Failure signatures:
  - Inconsistent structured outputs across runs
  - Misinterpretation of synonyms (e.g., "streaky" vs "spiculation")
  - Incorrect entity relationships due to missing context
  - Over-reliance on default LLM reasoning for complex tasks

- First 3 experiments:
  1. Run same prompt 5 times on 100 reports; measure consistency for each attribute.
  2. Compare extraction accuracy with and without domain-specific instructions in prompts.
  3. Evaluate impact of output format constraints (e.g., fixed JSON schema) on consistency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal prompt template for ChatGPT to extract structured information from radiological reports while maintaining high consistency and accuracy across diverse report formats?
- Basis in paper: [explicit] The paper explores prompt engineering and shows that adding medical knowledge to prompts can improve performance for some tasks but degrade it for others.
- Why unresolved: The study only tested a limited set of prompt templates and found mixed results, suggesting that finding the optimal balance between instruction specificity and model flexibility remains an open challenge.
- What evidence would resolve it: A comprehensive comparative study testing a wide range of prompt templates with varying levels of detail, specificity, and medical knowledge across multiple datasets and report types.

### Open Question 2
- Question: How can the consistency of ChatGPT's outputs be improved to ensure reliable structured extraction across multiple runs?
- Basis in paper: [explicit] The paper notes that ChatGPT produces different answers to the same prompt each time, and consistency was lower for density and shape-related attributes.
- Why unresolved: The study only tested consistency on a small sample of 100 reports and did not explore techniques to improve output stability, such as temperature settings or output constraints.
- What evidence would resolve it: Experiments testing different temperature settings, output constraints, and fine-tuning strategies to improve consistency, along with analysis of the impact on extraction accuracy.

### Open Question 3
- Question: How can privacy concerns be addressed when using ChatGPT for clinical text processing without compromising performance?
- Basis in paper: [explicit] The paper raises concerns about data leakage risks when using cloud-based ChatGPT for medical text processing.
- Why unresolved: The study does not explore potential solutions such as local deployment, model distillation, or privacy-preserving techniques like federated learning.
- What evidence would resolve it: Comparative studies of local vs. cloud-based ChatGPT performance, along with evaluations of privacy-preserving techniques for clinical text processing.

## Limitations

- Performance varies significantly across extraction tasks, with poor results for nuanced medical distinctions like tumor density and lymph node status.
- Consistency of outputs is low for complex attributes, raising concerns about reliability for clinical applications.
- The study is limited to Chinese CT reports from a single institution, which may affect generalizability to other languages or healthcare settings.

## Confidence

**High confidence** in claims about ChatGPT's ability to perform zero-shot IE for basic categorical attributes (tumor location, size, pleural invasion/extraction) where the model achieves accuracy comparable to rule-based systems. The consistency results showing high agreement for simple features across multiple runs are well-supported.

**Medium confidence** in claims about task-specific performance variations. While the paper shows clear performance differences between tasks, the underlying reasons (e.g., synonym confusion vs. medical knowledge gaps) are somewhat speculative without deeper error analysis.

**Low confidence** in the generalizability of results to other medical domains or languages. The study focuses exclusively on Chinese lung cancer CT reports, and the mixed results from prompt engineering suggest the approach may not transfer well without significant adaptation.

## Next Checks

1. **Cross-language validation**: Test the same zero-shot IE approach on English radiological reports to assess language dependency and determine if prompt templates need localization.

2. **Multi-institutional validation**: Evaluate performance on CT reports from different hospitals to assess robustness to institutional variation in reporting style and terminology.

3. **Controlled prompt ablation study**: Systematically remove individual medical knowledge elements from prompts to precisely quantify which instructions help or hurt for each extraction task, rather than the binary with/without comparison used in the current study.