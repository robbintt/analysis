---
ver: rpa2
title: XAI for In-hospital Mortality Prediction via Multimodal ICU Data
arxiv_id: '2312.17624'
source_url: https://arxiv.org/abs/2312.17624
tags:
- clinical
- prediction
- data
- multimodal
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of explainable in-hospital mortality
  prediction using multimodal ICU data, which combines clinical notes, vital signs,
  and discrete event sequences. The authors propose an eXplainable Multimodal Mortality
  Predictor (X-MMP) that uses transformer-based encoders for each modality, followed
  by late fusion and a Layer-Wise Propagation to Transformer (LRPTrans) method for
  explainability.
---

# XAI for In-hospital Mortality Prediction via Multimodal ICU Data

## Quick Facts
- arXiv ID: 2312.17624
- Source URL: https://arxiv.org/abs/2312.17624
- Reference count: 40
- Key outcome: X-MMP achieves competitive prediction accuracy with transformer-based multimodal learning and LRPTrans explainability

## Executive Summary
This paper presents an explainable multimodal framework for in-hospital mortality prediction in ICU settings using clinical notes, vital signs, and discrete event sequences. The authors propose X-MMP (eXplainable Multimodal Mortality Predictor) that combines transformer-based encoders for each modality with a novel Layer-Wise Propagation to Transformer (LRPTrans) method for explainability. The framework is evaluated on a dataset constructed from MIMIC-III and demonstrates competitive performance while providing interpretable explanations for its predictions through feature attribution analysis.

## Method Summary
The method employs three modality-specific transformer encoders (for clinical notes, vital signs, and discrete events) with late fusion through concatenation, followed by a feed-forward network for binary classification. Clinical notes are processed using ClinicalBERT embeddings, while vital signs and discrete events use positional embeddings. The LRPTrans explainability method combines gradient-based and attribution-based backpropagation to produce explanations that are conserved through the transformer layers. The model is trained using cross-entropy loss with Adam optimization and evaluated using AUC-ROC and AUC-PR metrics on a multimodal dataset constructed from MIMIC-III.

## Key Results
- X-MMP achieves competitive prediction accuracy compared to other deep learning methods on the multimodal ICU dataset
- Ablation studies demonstrate the complementary nature of multiple modalities in improving prediction performance
- Perturbation studies and case studies confirm the effectiveness of LRPTrans in identifying salient features and visualizing their contributions to model decisions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal transformer-based encoders with late fusion improve prediction accuracy over single-modality models
- Mechanism: Each modality is encoded separately using transformer blocks to capture modality-specific dependencies, then representations are concatenated and passed through a feed-forward network
- Core assumption: Different modalities contain complementary information that can improve prediction when properly fused
- Evidence anchors:
  - [abstract] "We employ multimodal learning in our framework, which can receive heterogeneous inputs from clinical data and make decisions"
  - [section] "The output representations from different modalities are concatenated. The latent representations are fed into a feedforward neural network"
- Break condition: If modalities contain redundant or conflicting information, late fusion may not improve and could degrade performance

### Mechanism 2
- Claim: LRPTrans provides effective explanations by combining gradient-based and attribution-based methods
- Mechanism: Attribution is calculated using Gradient × Input method, then propagated backward through the transformer using improved backpropagation rules that handle self-attention and layer normalization conservatively
- Core assumption: Conservation of attribution holds when using locally linear approximations for attention and normalization layers
- Evidence anchors:
  - [abstract] "We introduce an explainable method, namely Layer-Wise Propagation to Transformer, as a proper extension of the LRP method to Transformers"
  - [section] "We incorporate the Gradient × Input into the LRP-rule... the improved backpropagation rules... create locally linear expansions of self-attention and layer normalization"
- Break condition: If locally linear approximations break down for complex attention patterns, attribution conservation may fail

### Mechanism 3
- Claim: Transformer architecture captures long-range dependencies better than traditional RNNs/CNNs for clinical data
- Mechanism: Multi-head self-attention allows the model to focus on different parts of the input sequence simultaneously, capturing both short and long-term relationships
- Core assumption: Clinical data exhibits both local patterns and global patterns that benefit from attention mechanisms
- Evidence anchors:
  - [section] "In comparison to the baselines, the transformer, owing to the self-attention mechanism, is capable of extracting long-term dependencies from input features"
- Break condition: If clinical patterns are primarily local or sequential, attention may not provide significant advantage over simpler architectures

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The paper relies heavily on transformer-based encoders for each modality
  - Quick check question: How does multi-head self-attention differ from single attention in capturing relationships?

- Concept: Layer-wise Relevance Propagation (LRP) and its extensions
  - Why needed here: The explanation method LRPTrans is based on LRP principles
  - Quick check question: What is the conservation axiom in LRP and why is it important?

- Concept: Clinical data preprocessing and handling missing values
  - Why needed here: The paper describes specific preprocessing for discrete events, clinical notes, and vital signs
  - Quick check question: Why might missingness patterns in clinical data be informative rather than just noise?

## Architecture Onboarding

- Component map: Clinical notes → ClinicalBERT embedding → Transformer encoder → Pooler → Representation; Discrete events → Positional embedding → Transformer encoder → Pooler → Representation; Vital signs → Input+positional embedding → Transformer encoder → Pooler → Representation → All concatenated → FFN → Output

- Critical path: Clinical notes → ClinicalBERT embedding → Transformer encoder → Pooler → Representation; Discrete events → Positional embedding → Transformer encoder → Pooler → Representation; Vital signs → Input+positional embedding → Transformer encoder → Pooler → Representation → All concatenated → FFN → Output

- Design tradeoffs: Single modality transformers allow specialized processing but increase parameter count; Late fusion is simple but may not capture cross-modal interactions as well as early fusion

- Failure signatures: Poor performance on single modalities suggests issues with modality-specific encoders; Weak explanations suggest LRPTrans implementation issues; Imbalanced classes suggest need for class weighting

- First 3 experiments:
  1. Train and evaluate each single-modal transformer separately to verify baseline performance
  2. Train bi-modal combinations to verify complementarity of modalities
  3. Implement LRPTrans on a simplified version of the model to verify explanation mechanism before full integration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed X-MMP framework compare to other state-of-the-art methods on different multimodal datasets beyond MIMIC-III?
- Basis in paper: [explicit] The authors mention that their framework can be easily transferred to other clinical tasks, but they do not provide experimental results on other datasets
- Why unresolved: The authors only evaluate their method on a single multimodal dataset constructed from MIMIC-III. It is unclear how well their approach generalizes to other clinical prediction tasks and datasets
- What evidence would resolve it: Evaluating X-MMP on multiple multimodal clinical datasets for various prediction tasks would provide insights into its generalizability and performance compared to other methods

### Open Question 2
- Question: What is the impact of different data preprocessing techniques on the performance of the X-MMP framework?
- Basis in paper: [explicit] The authors describe their data preprocessing steps for each modality, but they do not explore the impact of different preprocessing techniques on the model's performance
- Why unresolved: The choice of preprocessing techniques can significantly affect the quality and consistency of the input data, which in turn can impact the model's performance. It is unclear how sensitive X-MMP is to different preprocessing choices
- What evidence would resolve it: Conducting experiments with different preprocessing techniques for each modality and comparing the resulting model performance would provide insights into the sensitivity of X-MMP to preprocessing choices

### Open Question 3
- Question: How does the interpretability of the X-MMP framework compare to other explainable AI methods in terms of human understanding and trust?
- Basis in paper: [explicit] The authors demonstrate the interpretability of their framework through perturbation studies and case studies, but they do not compare it to other explainable AI methods in terms of human understanding and trust
- Why unresolved: While the authors show that their method can provide explanations for the model's predictions, it is unclear how well these explanations align with human reasoning and whether they enhance trust in the model's decisions
- What evidence would resolve it: Conducting user studies to compare the interpretability of X-MMP with other explainable AI methods in terms of human understanding and trust would provide insights into the effectiveness of the proposed approach

## Limitations
- The specific implementation details of LRPTrans lack extensive validation in the clinical domain
- Dataset size (4,729 records with 485 mortality cases) may limit generalizability, particularly for rare clinical events
- Limited ablation studies to isolate the contribution of the explanation method versus the multimodal architecture itself

## Confidence

- **High**: The multimodal transformer architecture and late fusion approach are well-established methods that should work as described
- **Medium**: The LRPTrans explanation method is theoretically sound but lacks extensive validation in this specific clinical context
- **Medium**: The clinical relevance of the explanations, while demonstrated through perturbation studies, needs clinician validation

## Next Checks

1. Conduct a systematic ablation study removing each modality to quantify their individual contributions to both prediction accuracy and explanation quality
2. Validate the clinical relevance of LRPTrans explanations through expert clinician review, comparing against ground truth clinical reasoning
3. Test the model on an external, temporally distinct dataset to assess generalizability beyond the MIMIC-III training data