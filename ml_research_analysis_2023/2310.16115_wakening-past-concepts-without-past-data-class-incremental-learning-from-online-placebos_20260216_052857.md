---
ver: rpa2
title: 'Wakening Past Concepts without Past Data: Class-Incremental Learning from
  Online Placebos'
arxiv_id: '2310.16115'
source_url: https://arxiv.org/abs/2310.16115
tags:
- data
- class
- learning
- placebos
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses catastrophic forgetting in class-incremental
  learning (CIL), where models struggle to retain knowledge of old classes when continuously
  learning new ones. Existing CIL methods rely on knowledge distillation (KD) using
  new class data, which hinders both new class learning and old class preservation.
---

# Wakening Past Concepts without Past Data: Class-Incremental Learning from Online Placebos

## Quick Facts
- arXiv ID: 2310.16115
- Source URL: https://arxiv.org/abs/2310.16115
- Reference count: 40
- Key outcome: Improves last-phase accuracy by 6.9 percentage points on average under strict memory budget (5 exemplars/class)

## Executive Summary
This paper addresses catastrophic forgetting in class-incremental learning (CIL) by proposing PlaceboCIL, which uses high-quality placebo images from free image streams instead of new class data for knowledge distillation (KD). The key insight is that using new class data for KD causes interference between cross-entropy and KD losses, hindering both new class learning and old class preservation. PlaceboCIL introduces an online Markov Decision Process (MDP) formulation for dynamic placebo selection that adapts to the non-stationary CIL pipeline, along with a mini-batch-based memory reusing strategy that requires no additional memory budget. Experiments on CIFAR-100, ImageNet-100, and ImageNet-1k show consistent improvements over strong baselines, particularly with lower memory budgets.

## Method Summary
PlaceboCIL addresses CIL by selecting high-quality placebo images from free image streams (like Google Images) to compute KD losses instead of using new class data. An online MDP policy is trained to dynamically select relevant placebos, with a phase-specific evaluation function that considers both old and new class prototypes. The method includes a mini-batch-based memory reusing strategy that evaluates unlabeled image batches and keeps only high-quality placebos temporarily, removing an equivalent amount of new class data to maintain strict memory constraints. The approach is evaluated on multiple benchmarks with strict memory budgets (5-20 exemplars per class), demonstrating significant improvements over baselines while requiring no additional supervision or memory.

## Key Results
- Achieves 6.9 percentage point average improvement in last-phase accuracy under strict memory budget (5 exemplars/class)
- Shows consistent performance gains across CIFAR-100, ImageNet-100, and ImageNet-1k datasets
- Effective even when there is no class overlap between placebos and original old class data
- Particularly strong performance with lower memory budgets where other methods struggle

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using placebos for KD prevents interference between CE and KD losses that occurs with new class data
- Mechanism: Placebo images are selected to be close to old class prototypes and far from other classes' prototypes, ensuring KD loss aligns with CE loss for old classes
- Core assumption: Placebos can activate the same neurons as old class data without being semantically identical
- Evidence anchors: Abstract states using new class data for KD "hinder the model adaption" and "results in low efficiency for preserving old class knowledge"; section mentions CE and KD losses "weaken each other"
- Break condition: If placebo selection fails to find images with relevant visual cues, KD loss may not preserve old class knowledge

### Mechanism 2
- Claim: Online MDP policy dynamically adapts placebo selection to non-stationary CIL pipeline
- Mechanism: Policy is updated each phase using phase-specific evaluation function considering old and new class prototypes, adapting to increasing classes and changing data distribution
- Core assumption: Policy can effectively learn to select relevant placebos despite changing environment and limited feedback
- Evidence anchors: Abstract mentions "formulate the policy training process as an online Markov Decision Process (MDP)"; section states "Our objective is to maximize a cumulative reward"
- Break condition: If MDP formulation doesn't capture CIL dynamics, policy may not adapt effectively

### Mechanism 3
- Claim: Mini-batch-based memory reusing strategy allows placebo use without additional memory budget
- Mechanism: Unlabeled image batches are evaluated, high-quality placebos are kept temporarily, and equivalent amount of new class data is removed to maintain strict memory constraint
- Core assumption: Memory budget can be maintained by removing equivalent amount of new class data
- Evidence anchors: Abstract states "does not require any additional supervision or memory budget"; section mentions "we randomly remove the same size of new class data"
- Break condition: If memory budget constraint isn't strictly enforced, method may exceed allocated resources

## Foundational Learning

- **Knowledge Distillation (KD)**: Primary technique for preserving old class knowledge in CIL; needed because directly training on new data causes forgetting of old classes
  - Quick check: How does KD help prevent catastrophic forgetting in CIL?

- **Markov Decision Process (MDP)**: Allows dynamic adaptation of placebo selection policy to non-stationary CIL pipeline; needed because the data distribution changes as new classes are introduced
  - Quick check: Why is CIL considered a non-stationary environment for MDP formulation?

- **Catastrophic Forgetting**: Core problem that CIL methods aim to solve; occurs when neural networks lose previously learned knowledge while adapting to new tasks
  - Quick check: What causes catastrophic forgetting in neural networks during incremental learning?

## Architecture Onboarding

- **Component map**: ResNet backbone -> Prototype calculator -> Evaluation function generator -> Placebo selector -> KD loss calculator -> Online MDP policy
- **Critical path**: New data → Prototype calculation → Evaluation function generation → Placebo selection → KD loss computation → Model update
- **Design tradeoffs**: Memory buffer size vs. computation cost; number of exemplars per class vs. forgetting prevention; quality of placebos vs. diversity of visual cues
- **Failure signatures**: Performance degradation on old classes indicates poor KD effectiveness; unstable training may indicate poorly adapted placebo selection policy; memory issues may indicate incorrect implementation of reuse strategy
- **First 3 experiments**:
  1. Verify KD loss computation with different data sources (new, old, placebo)
  2. Test placebo selection quality with simple distance metrics
  3. Evaluate policy adaptation across phases with synthetic class distributions

## Open Questions the Paper Calls Out

- How does performance change when using different free image stream sources with varying class distributions and image qualities? (Unresolved: paper uses different streams but doesn't systematically compare performance across them)
- What is the optimal buffer size |U| for unlabeled data candidates, and how does it affect the trade-off between computational efficiency and model performance? (Unresolved: paper only provides results for few buffer sizes without identifying optimal point)
- How does PlaceboCIL perform in realistic scenarios where free image stream contains noisy or mislabeled data? (Unresolved: paper uses clean datasets but doesn't test with noisy data common in real-world applications)

## Limitations

- The specific mechanism claims about KD interference and memory budget maintenance lack direct experimental evidence
- The online MDP policy's effectiveness heavily depends on quality of free image streams, which may vary significantly across domains
- The mini-batch-based memory reusing strategy assumes removing |U+P| new class samples maintains performance, but impact of which specific samples are removed isn't explored

## Confidence

- **High confidence**: Core problem identification (catastrophic forgetting in CIL) and general approach (using free image streams for KD) are well-established and clearly demonstrated
- **Medium confidence**: Online MDP formulation provides principled approach to dynamic adaptation, though non-stationary environment assumptions could be more rigorously validated
- **Low confidence**: Specific mechanism claims about KD interference and memory budget maintenance lack direct experimental evidence

## Next Checks

1. **Ablation study**: Run controlled experiments comparing KD losses computed from new class data vs. placebo data, measuring both old class preservation and new class learning interference
2. **Domain robustness test**: Evaluate PlaceboCIL's performance when using free image streams from different domains (e.g., medical vs. natural images) to assess domain shift sensitivity
3. **Memory management analysis**: Systematically test impact of different sample removal strategies when implementing memory reusing approach, comparing random removal vs. targeted removal of "least informative" samples