---
ver: rpa2
title: 'Applying HCAI in developing effective human-AI teaming: A perspective from
  human-AI joint cognitive systems'
arxiv_id: '2307.03913'
source_url: https://arxiv.org/abs/2307.03913
tags:
- systems
- human-ai
- humans
- human
- cognitive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of human-AI joint cognitive systems
  (HAIJCS) as a framework for representing and implementing human-AI teaming (HAT)
  under the umbrella of human-centered AI (HCAI). The authors argue that HAT, which
  treats AI as a collaborative teammate rather than just a tool, represents a paradigmatic
  shift in human-machine relationships driven by AI's unique capabilities.
---

# Applying HCAI in developing effective human-AI teaming: A perspective from human-AI joint cognitive systems

## Quick Facts
- arXiv ID: 2307.03913
- Source URL: https://arxiv.org/abs/2307.03913
- Reference count: 0
- Key outcome: Introduces HAIJCS framework to represent and implement HAT under HCAI umbrella

## Executive Summary
This paper proposes the Human-AI Joint Cognitive Systems (HAIJCS) framework as a way to align human-AI teaming (HAT) with human-centered AI (HCAI) principles. The authors argue that HAT represents a paradigmatic shift in human-machine relationships, treating AI as a collaborative teammate rather than just a tool. HAIJCS models both human and AI as complementary cognitive agents connected through a shared interface, enabling effective collaboration while preserving human authority and control.

The framework emphasizes shared situation awareness, trust, decision-making, and control between human and AI agents. By applying Endsley's situation awareness theory and promoting human-in-the-loop design, HAIJCS aims to create synergy between human and machine intelligence. The authors are applying this framework to domains like autonomous vehicles and aviation, calling for multidisciplinary collaboration to further develop and validate this approach.

## Method Summary
The paper presents a conceptual framework for human-AI teaming based on joint cognitive systems theory. It introduces the HAIJCS model that characterizes both human and AI as cognitive agents with perception, reasoning, and decision-making capabilities. The framework applies Endsley's situation awareness model to both agents and establishes two-way communication channels for shared SA, trust, decision-making, and control. While the paper outlines the theoretical foundations and component structure, it does not provide specific quantitative metrics, implementation details, or experimental validation.

## Key Results
- HAIJCS provides a theoretical framework for representing HAT under the HCAI umbrella by modeling human and AI as complementary cognitive agents
- The framework emphasizes shared situation awareness, trust, decision-making, and control between agents while maintaining human authority
- Authors are applying HAIJCS to domains like autonomous vehicles and aviation to explore human-AI collaboration for enhanced performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HAT can be aligned with HCAI by modeling human and AI as complementary cognitive agents.
- Mechanism: The HAIJCS framework treats both human and AI as cognitive agents with their own perception, reasoning, and decision-making capabilities, connected via a shared cognitive interface that preserves human authority.
- Core assumption: AI can be characterized as a cognitive agent without requiring full equivalence to human cognition.
- Evidence anchors:
  - [abstract] "we propose a conceptual framework of human-AI joint cognitive systems (HAIJCS) to represent and implement HAT under the HCAI umbrella."
  - [section] "HAIJCS regards AI systems (with one or more AI agents) as cognitive agents that can perform specific tasks based on their autonomous capabilities."
  - [corpus] Weak evidence: no corpus papers explicitly describe this joint cognitive agent model; closest is "Agent Teaming Situation Awareness" which frames awareness but not full joint cognitive systems.
- Break condition: If AI lacks autonomous perception or reasoning capabilities that can be meaningfully modeled, the cognitive agent abstraction breaks down.

### Mechanism 2
- Claim: Shared situation awareness (SA) and trust between human and AI are key to effective HAT.
- Mechanism: HAIJCS uses Endsley's SA model for both agents and introduces two-way SA/trust/decision-making channels, enabling mutual understanding and coordinated control.
- Core assumption: Situation awareness can be meaningfully shared between human and machine using the same theoretical framework.
- Evidence anchors:
  - [section] "HAIJCS applies Endsley's situation awareness theory to characterize the information processing mechanism of the two cognitive agents."
  - [section] "Both cognitive agents collaborate through a human-AI cognitive interface driven by multimodal technology while maintaining shared situation awareness, task, goal, trust, decision-making, and control under the umbrella of HCAI."
  - [corpus] Weak evidence: only "Agent Teaming Situation Awareness (ATSA)" discusses SA in HAT but not shared SA with trust explicitly.
- Break condition: If one agent cannot perceive or communicate its internal state, shared SA cannot be established.

### Mechanism 3
- Claim: HAIJCS reframes HAT from a tool-based interaction to a collaborative team relationship, enabling better synergy.
- Mechanism: By modeling human and AI as interdependent cognitive agents and promoting "human-in-the-loop" design, HAIJCS shifts focus from unilateral control to mutual complementarity and joint performance.
- Core assumption: The team performance metric should be the joint system's output, not individual agent performance.
- Evidence anchors:
  - [section] "The performance of the entire cognitive system depends on the degree of synergy between the two parts, not just the performance of one."
  - [section] "HAIJCS promotes AI as a cognitive agent to be a qualified teammate for human-AI collaboration."
  - [corpus] Weak evidence: no corpus papers directly discuss synergy metrics for HAT; most focus on isolated agent effectiveness.
- Break condition: If human and AI goals are not aligned, synergy cannot be achieved and performance may degrade.

## Foundational Learning

- Concept: Joint Cognitive Systems theory
  - Why needed here: Provides the theoretical basis for modeling HAT as a single system of interacting cognitive agents rather than separate entities.
  - Quick check question: What is the key difference between a traditional HCI model and a Joint Cognitive Systems model?

- Concept: Situation Awareness (Endsley's model)
  - Why needed here: Supplies the framework for representing how both human and AI perceive, comprehend, and project their environment to enable shared understanding.
  - Quick check question: What are the three levels of situation awareness in Endsley's model?

- Concept: Human-in-the-loop control
  - Why needed here: Ensures that ultimate authority remains with the human while enabling AI to act autonomously within defined bounds.
  - Quick check question: In the context of HAT, what does "meaningful human control" require beyond simple override capability?

## Architecture Onboarding

- Component map:
  Human cognitive agent -> Human-AI cognitive interface -> AI cognitive agent -> Joint performance monitor
  HCAI governance layer connects to both cognitive agents

- Critical path:
  1. Human perception → interface → AI perception update
  2. AI perception → reasoning → decision → shared SA update
  3. Human trust/decision → interface → AI control adjustment
  4. Joint SA/trust/decision loop closes via interface feedback

- Design tradeoffs:
  - Granularity of shared SA: too fine-grained may overwhelm human; too coarse-grained may lose critical context
  - Autonomy level: higher autonomy increases efficiency but may reduce human trust and control
  - Multimodal interface complexity: richer modalities improve SA but increase cognitive load and system complexity

- Failure signatures:
  - Loss of shared SA: agents act on conflicting situational models
  - Trust breakdown: human ignores AI outputs or AI overrides human without consent
  - Performance degradation: joint output lower than either agent alone due to miscommunication

- First 3 experiments:
  1. Build a simple two-agent simulation (human + rule-based AI) with shared SA channel; measure joint SA alignment under varying information granularity.
  2. Implement a human-in-the-loop control task (e.g., simulated autonomous driving) with shared decision-making; measure trust and performance with/without explicit SA sharing.
  3. Extend experiment 2 to include an AI agent with learned reasoning; compare joint performance metrics (task completion, error rate, human workload) against human-only and AI-only baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the HAIJCS framework be enhanced to better support and validate the HAT metaphor across diverse domains?
- Basis in paper: [explicit] The paper explicitly states that "HAIJCS also needs further exploration" and "We need to enhance the framework to better support and validate the HAT metaphor by applying it to various domains."
- Why unresolved: The paper acknowledges the need for enhancement but does not provide specific methods or criteria for validation across different domains.
- What evidence would resolve it: Development of domain-specific metrics for evaluating HAIJCS effectiveness, case studies demonstrating successful application in multiple domains, and quantitative models comparing HAIJCS performance against traditional approaches.

### Open Question 2
- Question: What quantitative predictive models are needed to accurately measure joint collaborative performance between humans and AI agents in HAT scenarios?
- Basis in paper: [explicit] The paper mentions that "AI/CS communities should become involved; for instance, building the quantitative predictive models of human-AI performance that are needed."
- Why unresolved: The paper identifies the need for such models but does not specify their components, metrics, or methodologies for development.
- What evidence would resolve it: Published frameworks for measuring joint performance metrics, validated computational models demonstrating human-AI team performance prediction, and standardized testing protocols for evaluating collaborative effectiveness.

### Open Question 3
- Question: How can shared situation awareness, trust, decision-making, and control be effectively modeled and implemented between human and AI cognitive agents?
- Basis in paper: [explicit] The paper states that "we may adopt mature human-human teaming approaches to model shared situation awareness, trust, decision-making, and control between the two agents."
- Why unresolved: While the paper suggests adopting human-human teaming approaches, it does not detail how these concepts translate to human-AI interactions or what modifications are needed.
- What evidence would resolve it: Comparative studies showing the effectiveness of human-human versus human-AI shared awareness models, validated algorithms for implementing trust dynamics in AI systems, and empirical data on control transition mechanisms in joint cognitive systems.

## Limitations

- The HAIJCS framework remains largely conceptual without empirical validation or experimental evidence
- Specific quantitative metrics for evaluating joint human-AI performance are not defined
- Lack of detailed implementation specifications for AI agent cognitive processing mechanisms

## Confidence

**High Confidence**: The conceptual distinction between tool-based and team-based human-AI interaction is well-established in the literature. The alignment of HAT with HCAI principles through cognitive agent modeling represents a logical theoretical extension.

**Medium Confidence**: The specific mechanisms for shared situation awareness and trust between human and AI agents are theoretically sound but lack empirical validation. The proposed information processing channels are plausible but untested in real-world scenarios.

**Low Confidence**: The framework's ability to improve actual human-AI team performance over existing approaches remains speculative without experimental data. The claim that modeling AI as a cognitive agent preserves human authority while enabling effective collaboration requires validation.

## Next Checks

1. Conduct a controlled experiment comparing joint human-AI performance using HAIJCS versus traditional tool-based interfaces in a realistic task (e.g., simulated air traffic control), measuring shared SA alignment, trust levels, and team output quality.

2. Develop and test a prototype implementation of the human-AI cognitive interface with varying levels of information granularity in shared SA, measuring the impact on human cognitive load and team decision quality.

3. Perform a longitudinal study with human-AI teams using HAIJCS in a safety-critical domain, tracking trust dynamics, authority transfer patterns, and overall system performance across multiple task iterations.