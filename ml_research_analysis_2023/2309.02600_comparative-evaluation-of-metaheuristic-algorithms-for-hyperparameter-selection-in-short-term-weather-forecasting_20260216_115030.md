---
ver: rpa2
title: Comparative Evaluation of Metaheuristic Algorithms for Hyperparameter Selection
  in Short-Term Weather Forecasting
arxiv_id: '2309.02600'
source_url: https://arxiv.org/abs/2309.02600
tags:
- algorithm
- algorithms
- forecasting
- metaheuristic
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the effectiveness of three metaheuristic\
  \ algorithms\u2014Genetic Algorithm (GA), Differential Evolution (DE), and Particle\
  \ Swarm Optimization (PSO)\u2014for automating hyperparameter tuning in weather\
  \ forecasting models, including ARIMA, Artificial Neural Networks (ANN), Long Short-Term\
  \ Memory (LSTM), and Gated Recurrent Unit (GRU) networks. Using a decade-long weather\
  \ dataset from Ottawa, the research demonstrates that Differential Evolution consistently\
  \ achieves the lowest Mean Absolute Percentage Error (MAPE), outperforming GA and\
  \ PSO across all model types."
---

# Comparative Evaluation of Metaheuristic Algorithms for Hyperparameter Selection in Short-Term Weather Forecasting

## Quick Facts
- arXiv ID: 2309.02600
- Source URL: https://arxiv.org/abs/2309.02600
- Reference count: 4
- Differential Evolution achieves lowest MAPE (1.15) on ANN models for weather forecasting

## Executive Summary
This study evaluates three metaheuristic algorithms—Genetic Algorithm (GA), Differential Evolution (DE), and Particle Swarm Optimization (PSO)—for automating hyperparameter tuning in weather forecasting models. Using a decade-long Ottawa weather dataset, the research demonstrates that DE consistently achieves superior performance with a MAPE of 1.15 on ANN models, outperforming both traditional ARIMA forecasting and deep learning models optimized with other algorithms. The findings highlight the critical role of advanced optimization techniques in enhancing forecasting accuracy and establish DE as the most effective algorithm for hyperparameter optimization in weather prediction tasks.

## Method Summary
The study applies GA, DE, and PSO to optimize hyperparameters (learning rate, batch size, epochs) for four forecasting models: ARIMA, ANN, LSTM, and GRU. A 10-year Ottawa weather dataset (96,408 hourly records with 8 features) is preprocessed through missing value handling and standardization. Models are trained with optimized hyperparameters and evaluated using MAPE and MSE metrics. The comparison focuses on identifying which metaheuristic algorithm achieves the best forecasting accuracy across different model architectures.

## Key Results
- Differential Evolution achieves MAPE of 1.15 on ANN models, outperforming all other combinations
- DE consistently outperforms GA and PSO across all model types in MAPE reduction
- Deep learning models (ANN, LSTM, GRU) outperform traditional ARIMA forecasting
- Uniform three-layer architecture enables fair comparison of optimization algorithms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Differential Evolution consistently achieves lower MAPE than GA and PSO because its mutation operator effectively explores the hyperparameter search space while avoiding premature convergence.
- Mechanism: DE uses vector differences between randomly selected individuals to generate trial solutions. This difference vector acts as a stochastic gradient, allowing the algorithm to adaptively explore regions of the search space with promising fitness values. The crossover operator then recombines features from the mutant vector with the target vector, enabling exploitation of good solutions.
- Core assumption: The hyperparameter space for weather forecasting models is continuous and non-linear, making gradient-based methods less effective than population-based stochastic search.
- Evidence anchors:
  - [abstract] "Differential Evolution consistently achieves the lowest Mean Absolute Percentage Error (MAPE), outperforming GA and PSO across all model types."
  - [section] "DE's superior performance can be attributed to several key factors. It effectively explores the search space and exploits promising regions for optimal solutions."
  - [corpus] Weak - no direct evidence comparing DE to GA/PSO in weather forecasting literature found in the corpus.
- Break condition: If the fitness landscape becomes highly multimodal with sharp local optima, DE's reliance on difference vectors may lead to slow convergence in narrow basins of attraction.

### Mechanism 2
- Claim: Deep learning models (ANN, GRU, LSTM) outperform ARIMA because they can capture non-linear temporal dependencies that ARIMA's linear assumptions cannot model.
- Mechanism: ANN, GRU, and LSTM architectures use non-linear activation functions and recurrent connections to learn complex patterns in sequential data. Unlike ARIMA's assumption of linear relationships between past and future values, these models can represent arbitrary non-linear mappings through their layered structure and weight matrices.
- Core assumption: Weather time series data contains non-linear patterns and interactions between variables that cannot be adequately captured by linear autoregressive models.
- Evidence anchors:
  - [abstract] "deep learning techniques (Vanilla ANNs, LSTM and GRU networks), have shown promise in improving forecasting accuracy by capturing temporal dependencies."
  - [section] "ARIMA's limitation is its inability to capture complex non-linear patterns in data, resulting in inferior performance."
  - [corpus] Weak - corpus contains related work on deep learning for time series but no direct comparison with ARIMA in weather forecasting.
- Break condition: If the underlying weather dynamics are predominantly linear or the dataset is too small to train deep models effectively, the advantage of non-linear modeling may diminish.

### Mechanism 3
- Claim: The three-layer architecture with consistent neuron counts across models enables fair comparison of metaheuristic optimization algorithms.
- Mechanism: By maintaining the same number of layers and similar neuron distributions (36, 64, 24 for input, hidden, output respectively), the study isolates the effect of hyperparameter optimization from architectural differences. This controlled setup allows attribution of performance differences primarily to the optimization algorithm rather than model capacity.
- Core assumption: Model architecture differences have less impact on forecasting accuracy than hyperparameter selection when comparing optimization algorithms.
- Evidence anchors:
  - [section] "To ensure consistency among the deep learning models, we maintained a 3-layer architecture with varying neuron counts for input, GRU/LSTM layers, and output."
  - [abstract] "We present a comparative analysis of different model architectures integrated with metaheuristic optimization"
  - [corpus] Weak - corpus doesn't provide evidence about architectural consistency in comparative studies.
- Break condition: If certain architectures are inherently better suited for specific weather patterns or data characteristics, forcing a uniform architecture may mask algorithm performance differences.

## Foundational Learning

- Concept: Metaheuristic optimization algorithms
  - Why needed here: Weather forecasting models have numerous hyperparameters (learning rate, batch size, epochs, etc.) that require efficient search strategies beyond grid or random search.
  - Quick check question: What distinguishes population-based metaheuristics like DE from single-point optimization methods?

- Concept: Time series forecasting metrics
  - Why needed here: MAPE and MSE provide different perspectives on model performance - MAPE measures percentage errors while MSE penalizes larger errors more heavily.
  - Quick check question: When would MAPE be a more appropriate metric than MSE for weather forecasting?

- Concept: Deep learning architectures for sequential data
  - Why needed here: Understanding the differences between feedforward ANNs and recurrent architectures (LSTM, GRU) is crucial for interpreting why certain models perform better for weather forecasting.
  - Quick check question: How do LSTM and GRU gates help prevent the vanishing gradient problem that affects standard RNNs?

## Architecture Onboarding

- Component map:
  - Data preprocessing pipeline: Missing value handling → Standardization → Train/Validation/Test split
  - Model architectures: ARIMA (univariate), ANN (multilayer perceptron), LSTM (with gates), GRU (simplified LSTM)
  - Metaheuristic optimizers: GA (selection, crossover, mutation), DE (mutation via vector differences, crossover), PSO (velocity/position updates)
  - Evaluation framework: MAPE calculation, hyperparameter search space definition, multiple trial averaging

- Critical path:
  1. Load and preprocess Ottawa weather dataset (96,408 hourly records)
  2. Split data into train (2010-2015), validation (2016), test (2017-2020) sets
  3. Standardize features using StandardScaler
  4. For each model type (ARIMA, ANN, LSTM, GRU):
     - Define hyperparameter search space
     - Apply GA, DE, and PSO optimization
     - Train with optimal hyperparameters found
     - Evaluate on test set using MAPE
  5. Compare optimization algorithm performance across models

- Design tradeoffs:
  - Computational cost vs. search space coverage: DE requires fewer fitness evaluations than GA for similar performance
  - Model complexity vs. overfitting risk: Three-layer architectures balance capacity with generalization
  - Hyperparameter search space size vs. optimization time: Restricting ranges to (0,5) for ARIMA parameters limits search but ensures tractability

- Failure signatures:
  - GA consistently underperforming: May indicate premature convergence due to inadequate diversity maintenance
  - PSO showing high variance across runs: Could suggest sensitivity to initialization or poor global search capability
  - All algorithms converging to similar MAPE values: Might indicate hitting optimization plateaus or insufficient search space coverage

- First 3 experiments:
  1. Run ARIMA with manual hyperparameter selection (p,d,q from (0,5)) as baseline, then compare with GA/DE/PSO optimization
  2. Implement ANN with fixed architecture (24-64-36-24 neurons) and optimize only learning rate, batch size, epochs using all three algorithms
  3. Apply DE optimization to GRU model with 8 input features from 3 previous timesteps, compare MAPE with GA and PSO results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different weather variables beyond temperature (such as humidity, wind speed, and precipitation) affect the performance of metaheuristic-optimized forecasting models?
- Basis in paper: [explicit] The paper mentions that the dataset includes features like temperature, dew point temperature, relative humidity, wind speed, visibility, pressure, and precipitation, but only temperature was used for ARIMA modeling due to its univariate nature.
- Why unresolved: The study focused on temperature forecasting and did not explore the impact of other weather variables on model performance.
- What evidence would resolve it: Comparative experiments using different combinations of weather variables in the forecasting models and evaluating their impact on prediction accuracy.

### Open Question 2
- Question: How do the metaheuristic algorithms perform in long-term weather forecasting compared to short-term forecasting?
- Basis in paper: [inferred] The paper focuses on short-term weather forecasting (24-hour ahead predictions) and does not explore the performance of metaheuristic algorithms in long-term forecasting scenarios.
- Why unresolved: The study's scope was limited to short-term forecasting, and the effectiveness of metaheuristic algorithms for longer time horizons remains unexplored.
- What evidence would resolve it: Conducting experiments with extended forecasting horizons (e.g., weekly or monthly predictions) and comparing the performance of metaheuristic-optimized models against traditional methods.

### Open Question 3
- Question: What is the computational efficiency trade-off between Differential Evolution and other metaheuristic algorithms in hyperparameter optimization for large-scale weather datasets?
- Basis in paper: [explicit] The paper mentions that Differential Evolution consistently outperforms other algorithms in terms of Mean Absolute Percentage Error (MAPE), but does not discuss the computational efficiency or time complexity of each algorithm.
- Why unresolved: While DE shows superior accuracy, its computational cost and scalability for larger datasets or more complex models are not addressed.
- What evidence would resolve it: Benchmarking the runtime and resource usage of each metaheuristic algorithm when optimizing hyperparameters for increasingly large and complex datasets.

## Limitations

- Limited comparison scope: Only three metaheuristic algorithms evaluated, excluding other promising approaches like Simulated Annealing or Tabu Search
- Fixed architecture constraint: Three-layer neural networks may not capture all relevant weather patterns
- Dataset specificity: Results based on Ottawa weather data may not generalize to other geographic regions

## Confidence

- **High Confidence**: DE outperforming GA and PSO in MAPE reduction (directly supported by results)
- **Medium Confidence**: Deep learning superiority over ARIMA (consistent with time series literature but dataset-specific)
- **Low Confidence**: Universal applicability of DE optimization across all weather forecasting scenarios (requires broader validation)

## Next Checks

1. **Cross-region validation**: Test DE-optimized models on weather datasets from at least three geographically distinct locations to assess generalizability
2. **Algorithm comparison expansion**: Include Simulated Annealing and Tabu Search in the metaheuristic comparison to establish relative performance boundaries
3. **Architecture sensitivity analysis**: Evaluate whether deeper architectures (4+ layers) with DE optimization can further reduce forecasting error beyond the current three-layer baseline