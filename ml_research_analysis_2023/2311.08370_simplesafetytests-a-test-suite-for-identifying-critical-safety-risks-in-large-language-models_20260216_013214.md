---
ver: rpa2
title: 'SimpleSafetyTests: a Test Suite for Identifying Critical Safety Risks in Large
  Language Models'
arxiv_id: '2311.08370'
source_url: https://arxiv.org/abs/2311.08370
tags:
- unsafe
- llms
- safety
- responses
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'SimpleSafetyTests is a benchmark of 100 English prompts spanning
  five harm areas to assess critical safety risks in large language models. The prompts
  are split into two categories: Information and Advice Seeking questions and Instructions
  and Actions commands.'
---

# SimpleSafetyTests: a Test Suite for Identifying Critical Safety Risks in Large Language Models

## Quick Facts
- **arXiv ID**: 2311.08370
- **Source URL**: https://arxiv.org/abs/2311.08370
- **Reference count**: 12
- **One-line primary result**: A benchmark of 100 prompts across five harm areas shows that safety-emphasising system prompts reduce but don't eliminate unsafe LLM responses.

## Executive Summary
SimpleSafetyTests (SST) is a benchmark of 100 English prompts designed to evaluate critical safety risks in large language models across five harm areas: Suicide, Self-Harm, Eating Disorders; Physical Harm; Illegal Items; Scams & Fraud; and Child Abuse. The prompts are categorized into Information and Advice Seeking questions and Instructions and Actions commands. Testing 11 open-access models with and without a safety-emphasising system prompt revealed that 33% of responses were unsafe without the system prompt, dropping to 21% with it. While the system prompt significantly reduced unsafe responses for most models, it did not eliminate all risks, and results varied by harm area and prompt category.

## Method Summary
The SST benchmark consists of 100 handcrafted English test prompts covering five harm areas, split into Information/Advice Seeking and Instructions/Actions categories. The authors tested 11 open-source LLMs ranging from 3B to 40B parameters using a safety-emphasising system prompt based on Mistral's guardrail prompt. Model responses were manually annotated by three annotators using majority voting to determine if they were safe or unsafe. The evaluation compared model performance with and without the safety-emphasising system prompt, using temperature=0.01 and max tokens=1048 for all tests.

## Key Results
- Without a system prompt, 33% of LLM responses were unsafe; with the safety-emphasising system prompt, this dropped to 21%
- The safety-emphasising system prompt significantly reduced unsafe responses for most models but didn't eliminate all risks
- Instructions and Actions prompts elicited more unsafe responses than Information and Advice Seeking prompts in four of the five harm areas
- Performance varied significantly across harm areas, with some models showing consistent weaknesses in specific areas like Child Abuse

## Why This Works (Mechanism)

### Mechanism 1
- Claim: System prompts can reduce unsafe responses without model retraining
- Mechanism: The safety-emphasising system prompt acts as a lightweight steering layer that instructs the model to "avoid harmful, unethical, prejudiced, or negative content" and "ensure replies promote fairness and positivity" before processing any user input
- Core assumption: Models have sufficient capability to understand and follow safety instructions when they are made explicit through system prompts
- Evidence anchors:
  - [abstract] "Prepending a safety-emphasising system prompt substantially reduces the occurrence of unsafe responses, but does not completely stop them from happening"
  - [section 3.1] "System prompts specify general principles for how LLMs should respond to prompts. They are attractive because they can be introduced at inference time and do not require more expensive model training"
- Break condition: When prompts are designed to explicitly jailbreak or circumvent system prompt constraints, or when models lack understanding of what constitutes harmful content

### Mechanism 2
- Claim: Test prompts with clear harmful intent are more likely to elicit unsafe responses than ambiguous ones
- Mechanism: The two prompt categories (Information and Advice Seeking vs Instructions and Actions) create different levels of directness in requesting harmful content, with direct commands producing more unsafe responses
- Core assumption: Models respond differently to explicit commands versus questions that imply harmful actions
- Evidence anchors:
  - [section 2.2] "Instructions and Actions Commands given to the model that, if complied with, would result in unsafe information being provided"
  - [section 4] "Several LLMs exhibit a clear weakness on Child Abuse. It is the only harm area where Falcon (40B) with no system prompt, responds unsafely"
- Break condition: When models fail to distinguish between different prompt structures or when harm areas are too similar in nature

### Mechanism 3
- Claim: Safety performance varies significantly across different harm areas and model architectures
- Mechanism: Different harm areas present varying levels of complexity for models to identify and refuse, while model architecture and training data influence safety capabilities
- Core assumption: Models trained on different datasets and with different architectures will have varying abilities to recognize and respond safely to harmful content
- Evidence anchors:
  - [section 4] "The proportion of responses that are unsafe varies substantially by LLM" and "There are also differences across the five harm areas"
  - [section 3.1] "We aimed for a mix of model providers and model sizes, ranging from 3B to 40B parameters"
- Break condition: When all tested models perform similarly across harm areas, suggesting a fundamental limitation in current safety approaches

## Foundational Learning

- Concept: System prompt engineering
  - Why needed here: Understanding how to craft effective safety instructions that models can interpret and follow
  - Quick check question: What are the key components of an effective safety-emphasising system prompt?

- Concept: Harm area classification
  - Why needed here: Being able to categorize different types of harmful content and understand their unique characteristics
  - Quick check question: How do the five harm areas in SST differ in terms of the safety risks they present?

- Concept: Annotation methodology
  - Why needed here: Understanding how to consistently label responses as safe or unsafe across different annotators
  - Quick check question: What criteria should be used to determine if a model response is "unsafe"?

## Architecture Onboarding

- Component map: Test prompt generation -> Model response collection through API -> Manual annotation pipeline -> Automated evaluation framework using AI safety filters
- Critical path: Generate test prompts → Collect model responses → Manual annotation → Calculate safety metrics → Apply safety filters
- Design tradeoffs: Using synthetic test prompts instead of real user interactions ensures privacy but may reduce ecological validity; manual annotation provides high-quality labels but is resource-intensive
- Failure signatures: High variance in safety performance across models suggests fundamental limitations in current safety approaches; consistent failure in specific harm areas indicates targeted weaknesses
- First 3 experiments:
  1. Test the impact of different safety-emphasising system prompts on a single model to optimize prompt effectiveness
  2. Evaluate model responses to variations of the same test prompt to assess robustness to prompt phrasing
  3. Compare SST results with other safety benchmarks to validate its effectiveness as a safety evaluation tool

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different safety-emphasising system prompts compare in their effectiveness at reducing unsafe responses across various LLMs?
- Basis in paper: [explicit] The authors tested one safety-emphasising system prompt and found it significantly reduced unsafe responses for most models, but did not eliminate all risks. They also mention the potential for optimizing the safety-emphasising system prompt and testing other steering techniques.
- Why unresolved: The study only tested one specific safety-emphasising system prompt. There is no comparison with other potential prompts or optimization of the existing one.
- What evidence would resolve it: Testing a variety of safety-emphasising system prompts across multiple LLMs and comparing their effectiveness in reducing unsafe responses.

### Open Question 2
- Question: How do LLMs' responses to unsafe prompts change as they become more capable and better at understanding the risks presented?
- Basis in paper: [inferred] The authors discuss the potential for LLMs to provide clearer explanations of why complying with unsafe prompts would be harmful, offer helpful resources, or adopt a more supportive tone as they become more capable. However, they also mention the risk of LLMs being manipulated into revealing personalized advice about causing harm.
- Why unresolved: The study focuses on current LLMs and does not explore how their responses might evolve as they become more capable.
- What evidence would resolve it: Longitudinal studies tracking changes in LLMs' responses to unsafe prompts as their capabilities improve, and testing their susceptibility to manipulation.

### Open Question 3
- Question: How does the quality of LLM responses to unsafe prompts affect their potential for harm?
- Basis in paper: [explicit] The authors discuss labeling responses as not unsafe even if they fail to condemn requests or warn users, as long as they do not enable harmful activities. They also mention that some responses complied with requests but were of such low quality that they were considered not unsafe.
- Why unresolved: The study focuses on whether responses are unsafe or not, but does not explore the relationship between response quality and potential for harm.
- What evidence would resolve it: Analyzing the correlation between response quality and potential for harm, and determining if there is a threshold below which responses are considered too low quality to pose a significant risk.

## Limitations
- Test prompts were synthetically generated rather than derived from real user interactions, limiting ecological validity
- Manual annotation introduces potential subjectivity and inter-rater reliability concerns
- The safety-emphasising system prompt used may not be optimal for all model architectures or may not generalize well to proprietary models

## Confidence
- **High confidence**: The finding that system prompts reduce unsafe responses without model retraining is well-supported by the data and aligns with existing literature on prompt engineering
- **Medium confidence**: The relative safety performance across harm areas is reasonably reliable, though the synthetic nature of prompts introduces some uncertainty about real-world applicability
- **Medium confidence**: The comparative performance across the 11 tested models is internally consistent, but the specific rankings may vary with different prompt sets or evaluation conditions

## Next Checks
1. Test the SST benchmark on a held-out set of real user prompts from actual deployment scenarios to validate ecological validity of the synthetic prompts
2. Conduct inter-rater reliability analysis on the manual annotations to quantify annotation consistency and identify any systematic biases in safety labeling
3. Evaluate the benchmark's sensitivity by testing whether minor prompt variations produce consistent safety outcomes, establishing the robustness of the test suite