---
ver: rpa2
title: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization
arxiv_id: '2310.20033'
source_url: https://arxiv.org/abs/2310.20033
tags:
- summary
- hallucination
- instruction
- edit
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a pipeline to generate synthetic edit feedback
  data using ChatGPT for improving factual consistency in clinical note summarization.
  The method involves using ChatGPT to generate hallucinated summaries by adding/omitting
  medically important content from reference summaries, treating these as dispreferred
  data for training with Direct Preference Optimization (DPO).
---

# Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization

## Quick Facts
- arXiv ID: 2310.20033
- Source URL: https://arxiv.org/abs/2310.20033
- Reference count: 40
- Primary result: Models trained with synthetic edit feedback achieve higher ROUGE scores and factuality metrics compared to standard fine-tuning

## Executive Summary
This paper proposes a pipeline to generate synthetic edit feedback data using ChatGPT for improving factual consistency in clinical note summarization. The method involves using ChatGPT to generate hallucinated summaries by adding/omitting medically important content from reference summaries, treating these as dispreferred data for training with Direct Preference Optimization (DPO). Experiments on the MIMIC-III dataset show that models trained with the synthetic edit data achieve higher ROUGE scores and factuality metrics (G-Eval, UMLS-F1) compared to standard fine-tuning, and are preferred by human annotators for factuality.

## Method Summary
The method generates synthetic preference data by prompting ChatGPT to create hallucinated summaries through ADD and OMIT operations on reference summaries. These hallucinated summaries serve as negative examples in DPO training, where the model learns to prefer factually consistent summaries. The approach uses the MIMIC-III dataset and fine-tunes LLaMA-2 7B and GPT-2 models, evaluating them on ROUGE scores, factuality metrics, and human preference judgments.

## Key Results
- Models trained with synthetic edit feedback achieve higher ROUGE scores compared to standard fine-tuning
- Factuality metrics (G-Eval, UMLS-F1) show significant improvement with synthetic edit training
- Human annotators prefer summaries from models trained with synthetic edit feedback for factuality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The pipeline improves factual alignment by using ChatGPT to generate synthetic preference data that explicitly models clinically important content
- Mechanism: ChatGPT creates hallucinated summaries by adding non-essential content or omitting essential content from reference summaries, serving as negative examples in DPO training
- Core assumption: ChatGPT can reliably identify medico-legally essential phrases critical for diagnosis and treatment
- Evidence anchors:
  - [abstract] "We prompt ChatGPT to generate a hallucinated summary (factually low-quality summary) by generating edit instructions to the ADD hallucinated sentences and OMIT critically important sentences"
  - [section] "We prompt ChatGPT to generate a hallucinated summary fh : {xi, yi+} → yi− given a clinical note xi and the corresponding reference summary yi+"
  - [corpus] Weak - no external validation of ChatGPT's clinical accuracy in this specific edit generation task is cited
- Break condition: If ChatGPT cannot reliably distinguish essential from non-essential clinical content, the synthetic preference data will be misaligned, leading to poor factuality improvements

### Mechanism 2
- Claim: DPO training with synthetic preference pairs improves model factuality more effectively than standard fine-tuning
- Mechanism: The DPO objective maximizes log-likelihood of preferred summaries while minimizing dispreferred ones, directly optimizing for factual consistency
- Core assumption: The preference ranking (reference > hallucinated) is correct and stable across the dataset
- Evidence anchors:
  - [abstract] "Experiments on the MIMIC-III dataset show that models trained with the synthetic edit data achieve higher ROUGE scores and factuality metrics (G-Eval, UMLS-F1)"
  - [section] "For aligning πref using DPO (πref → πθ), we train the model by optimizing the loss function ℓdpo shown in equation 1"
  - [corpus] Weak - no ablation comparing DPO with other preference learning methods is provided
- Break condition: If preference pairs are noisy or ChatGPT's hallucinations are not clearly worse than references, DPO may not improve factuality

### Mechanism 3
- Claim: Human evaluation confirms that edits generated by ChatGPT improve factuality because they target clinically essential content
- Mechanism: Annotators assess whether edit instructions result in hallucinations by checking if essential medical instructions are omitted or non-essential ones are added
- Core assumption: Medical student annotators can reliably judge the clinical importance of instructions
- Evidence anchors:
  - [section] "We observe that the majority of hallucination edits made by our pipeline were mentioned in the instructions and were either to add content from the article (AA-MI 34%) or omit content from the reference summary (OR-MI 21%)"
  - [section] "According to the annotators, both OMIT operations contribute more wrt ADD operation towards generating hallucination edits (ADD 0% & OMIT 39%)"
  - [corpus] Weak - only two annotators, no formal agreement statistics beyond Kappa are reported
- Break condition: If annotators disagree significantly or lack domain expertise, the evaluation of edit quality and factuality gains becomes unreliable

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO is used to align the language model to prefer factually consistent summaries by learning from synthetic preference pairs
  - Quick check question: What is the key difference between DPO and standard supervised fine-tuning in terms of training objective?

- Concept: Factual consistency in clinical summaries
  - Why needed here: In clinical note summarization, factual errors can lead to incorrect diagnoses, making factuality a critical evaluation metric
  - Quick check question: Why is factual consistency more important than fluency in clinical summarization tasks?

- Concept: Synthetic data generation for preference learning
  - Why needed here: Human-annotated preference data is expensive and scarce in the clinical domain, so synthetic data must approximate realistic preference signals
  - Quick check question: How does synthetic preference data help overcome the limitations of human annotation in specialized domains?

## Architecture Onboarding

- Component map: Clinical note -> Reference summary -> ChatGPT (edit instruction generator) -> Edit instructions (ADD/OMIT operations) -> Hallucinated summary -> DPO training loop -> Evaluation metrics

- Critical path:
  1. Load clinical note and reference summary
  2. Generate edit instructions using ChatGPT
  3. Apply instructions to produce hallucinated summary
  4. Form preference pair (reference, hallucinated)
  5. Train model using DPO loss
  6. Evaluate factuality and preferability

- Design tradeoffs:
  - ChatGPT vs human annotators: Cost and scalability vs. accuracy and reliability
  - ADD vs OMIT operations: Balance between hallucination types and clinical impact
  - Length constraint (±5 words): Prevents summary drift but may limit edit expressiveness

- Failure signatures:
  - Low improvement in factuality metrics despite high ROUGE scores
  - Human evaluators prefer hallucinated summaries over references
  - Edit instructions fail to produce clear hallucinations (high Kappa disagreement)

- First 3 experiments:
  1. Run pipeline on a small subset and manually inspect 10 hallucinated summaries for clinical relevance
  2. Compare DPO vs SFT on ROUGE-1 and UMLS-F1 to confirm factuality gains
  3. Perform ablation: Remove OMIT operations only and observe impact on factuality scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of synthetic edit feedback data generated by ChatGPT compare to human expert-generated feedback in terms of improving factual consistency in clinical note summarization?
- Basis in paper: [explicit] The paper mentions that it is challenging to collect real-world doctor edit feedback due to privacy protection and strict data regulations like HIPAA. It proposes using ChatGPT to generate synthetic imitation edit feedback as an alternative to human experts
- Why unresolved: The paper does not provide a direct comparison between the quality of synthetic edit feedback generated by ChatGPT and human expert-generated feedback
- What evidence would resolve it: Conducting a study that compares the performance of models trained with synthetic edit feedback generated by ChatGPT and human expert-generated feedback in terms of factual consistency in clinical note summarization would provide evidence to resolve this question

### Open Question 2
- Question: Can the proposed synthetic imitation edit feedback pipeline be effectively adapted to other domains beyond clinical note summarization?
- Basis in paper: [inferred] The paper focuses specifically on the clinical domain and mentions that adapting the method to other domains is yet to be explored
- Why unresolved: The paper does not provide any experiments or evidence to support the generalizability of the proposed pipeline to other domains
- What evidence would resolve it: Conducting experiments that apply the proposed synthetic imitation edit feedback pipeline to other domains, such as legal or financial text summarization, and evaluating its effectiveness in improving factual consistency would provide evidence to resolve this question

### Open Question 3
- Question: How does the performance of models trained with synthetic edit feedback data compare to models trained with other forms of human feedback, such as scalar or label feedback, in terms of factual consistency?
- Basis in paper: [explicit] The paper mentions that human feedback for summarization can come in different forms, including comparison-based feedback, scalar feedback, label feedback, edit feedback, and language feedback. It focuses specifically on edit feedback and discusses its advantages for tasks that require expert domain knowledge
- Why unresolved: The paper does not provide a direct comparison between the performance of models trained with synthetic edit feedback and models trained with other forms of human feedback in terms of factual consistency
- What evidence would resolve it: Conducting a study that compares the performance of models trained with synthetic edit feedback and models trained with other forms of human feedback, such as scalar or label feedback, in terms of factual consistency would provide evidence to resolve this question

## Limitations
- Reliance on ChatGPT's ability to identify clinically essential content without external validation
- Limited human evaluation with only two annotators and no formal agreement statistics
- Lack of ablation studies comparing DPO with other preference learning methods

## Confidence

**High confidence**: The technical implementation of the DPO training pipeline and the evaluation methodology using established metrics (ROUGE, UMLS-F1, G-Eval)

**Medium confidence**: The effectiveness of synthetic edit feedback for improving factuality, the clinical relevance of ChatGPT-generated edit instructions, and human preference outcomes

**Low confidence**: The generalizability of results beyond the MIMIC-III dataset and the long-term reliability of synthetic preference data for clinical applications

## Next Checks

1. **Clinical Expert Validation**: Have domain experts review 50 randomly sampled hallucinated summaries and their edit instructions to assess whether ChatGPT correctly identifies medico-legally essential content and whether the synthetic preference pairs accurately reflect clinically meaningful differences

2. **Cross-Dataset Generalization**: Apply the trained model to a separate clinical summarization dataset (e.g., i2b2 or other medical records) to verify that factuality improvements transfer beyond MIMIC-III

3. **Ablation of Edit Operations**: Conduct a systematic ablation study removing either ADD or OMIT operations entirely to quantify their individual contributions to factuality improvements and determine if one operation type is driving most of the observed gains