---
ver: rpa2
title: 'EX-FEVER: A Dataset for Multi-hop Explainable Fact Verification'
arxiv_id: '2310.09754'
source_url: https://arxiv.org/abs/2310.09754
tags:
- claim
- dataset
- fact
- evidence
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EX-FEVER, the first dataset for multi-hop
  explainable fact verification, containing over 60,000 claims requiring 2- or 3-hop
  reasoning. Each claim is accompanied by a veracity label (SUPPORTS, REFUTES, or
  NOT ENOUGH INFO) and a golden explanation that outlines the reasoning path.
---

# EX-FEVER: A Dataset for Multi-hop Explainable Fact Verification

## Quick Facts
- arXiv ID: 2310.09754
- Source URL: https://arxiv.org/abs/2310.09754
- Reference count: 9
- Over 60,000 claims requiring 2- or 3-hop reasoning with golden explanations

## Executive Summary
EX-FEVER is the first dataset for multi-hop explainable fact verification, containing over 60,000 claims that require evidence from multiple hyperlinked Wikipedia documents. Each claim is accompanied by a veracity label (SUPPORTS, REFUTES, or NOT ENOUGH INFO) and a golden explanation that outlines the reasoning path. The dataset was constructed through crowd workers who summarized and modified information from hyperlinked Wikipedia documents. Experiments show that existing fact verification models trained on previous datasets struggle on EX-FEVER, highlighting its challenge and potential to advance research in explainable fact-checking systems.

## Method Summary
EX-FEVER was created by crowd workers who constructed claims requiring 2- or 3-hop reasoning from hyperlinked Wikipedia documents. Workers first wrote SUPPORTS claims incorporating information from multiple documents via hyperlink expansion, then generated REFUTES and NOT ENOUGH INFO claims through controlled mutation of entities, logical words, or negation insertion/removal. Each instance includes a veracity label and a golden explanation containing minimally sufficient information from the golden documents. A baseline system demonstrates the dataset's application, using document retrieval (TF-IDF and neural models), explanation generation (BART), and claim verification (transformer-based and graph-based models).

## Key Results
- Existing fact verification models trained on previous datasets struggle on EX-FEVER
- EX-FEVER requires multi-hop reasoning that cannot be solved through word-matching shortcuts
- Large Language Models like ChatGPT show promise in explanation generation but need improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EX-FEVER enables multi-hop explainable fact verification by requiring crowd workers to construct claims that span multiple hyperlinked Wikipedia documents.
- Mechanism: Claims are created by first writing a SUPPORTS claim that incorporates information from multiple documents via hyperlink expansion, then generating REFUTES and NOT ENOUGH INFO claims through controlled mutation of entities, logical words, or negation insertion/removal.
- Core assumption: Multi-hop reasoning is necessary because real-world claims often require evidence from multiple sources, and single-hop datasets oversimplify this process.
- Evidence anchors: [abstract] "Each claim is accompanied by a veracity label (SUPPORTS, REFUTES, or NOT ENOUGH INFO) and a golden explanation that outlines the reasoning path."

### Mechanism 2
- Claim: The dataset includes high-quality explanations that serve as reasoning paths for each claim.
- Mechanism: Annotators summarize minimally sufficient information from each golden document into a single sentence, forming a multi-sentence explanation that outlines the reasoning path. For REFUTES claims, the explanation also identifies the specific inconsistency.
- Core assumption: Providing minimal sufficient information in explanations improves model understanding and enables evaluation of explainability in fact verification.
- Evidence anchors: [abstract] "Each instance is accompanied by a veracity label and an explanation that outlines the reasoning path supporting the veracity classification."

### Mechanism 3
- Claim: The dataset challenges existing fact verification models by requiring multi-hop reasoning that cannot be solved through word-matching shortcuts.
- Mechanism: By requiring information from multiple documents, the dataset forces models to engage in genuine multi-hop reasoning rather than exploiting dataset biases or semantic overlap between claims and documents.
- Core assumption: Single-hop models trained on datasets like FEVER rely on word-matching shortcuts and fail when faced with truly multi-hop reasoning requirements.
- Evidence anchors: [abstract] "Experiments showed that existing fact verification models trained on previous datasets struggle on EX-FEVER, highlighting the dataset's challenge."

## Foundational Learning

- Concept: Multi-hop reasoning in NLP
  - Why needed here: EX-FEVER requires models to combine information from multiple documents to verify claims, which is fundamentally different from single-hop reasoning.
  - Quick check question: Can you explain why a claim requiring information from three different Wikipedia articles represents a harder problem than one requiring information from a single article?

- Concept: Explainable AI and minimal sufficient explanations
  - Why needed here: The dataset specifically requires explanations that contain only the minimally sufficient information needed to verify each claim, which is crucial for both human understanding and model training.
  - Quick check question: What distinguishes a minimally sufficient explanation from a complete explanation in the context of fact verification?

- Concept: Dataset construction and bias control
  - Why needed here: The paper describes specific techniques for controlling bias when generating REFUTES claims and ensuring label balance across the dataset.
  - Quick check question: Why is it problematic to have too many REFUTES claims generated through simple negation insertion, and how does the paper address this issue?

## Architecture Onboarding

- Component map: Document retrieval (TF-IDF, BERT, MDR) -> Explanation generation (BART) -> Claim verification (BERT, GEAR)
- Critical path: Document retrieval → Explanation generation → Claim verification, where each stage feeds into the next.
- Design tradeoffs: The choice between extractive vs. abstractive explanation generation, and between transformer-based vs. graph-based verification models, represents key architectural decisions with different performance characteristics.
- Failure signatures: Poor document retrieval leads to missing golden documents, inadequate explanation generation produces irrelevant or insufficient explanations, and verification models fail to generalize from previous datasets.
- First 3 experiments:
  1. Evaluate document retrieval performance using exact match and hit scores to ensure golden documents are being retrieved.
  2. Test explanation generation quality using ROUGE scores to verify that BART produces relevant, minimally sufficient explanations.
  3. Compare verification model performance with and without golden explanations to establish the value of the explanation component.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are current Large Language Models (LLMs) like GPT-3.5-turbo in generating accurate and sufficient explanations for fact verification tasks compared to fine-tuned models?
- Basis in paper: [explicit] The paper discusses the use of GPT-3.5-turbo for generating explanations and making predictions for fact verification tasks, comparing its performance with fine-tuned models like BART.
- Why unresolved: While the paper shows that GPT-3.5-turbo can outperform fine-tuned models in explanation generation with more examples, it does not provide a comprehensive evaluation of its accuracy and sufficiency in generating explanations compared to other models.
- What evidence would resolve it: Conducting a detailed comparative study with various LLMs and fine-tuned models on a broader set of fact verification tasks, including both explanation generation and prediction accuracy.

### Open Question 2
- Question: What are the limitations of using prompt-based approaches with LLMs for fact verification, and how can they be mitigated?
- Basis in paper: [explicit] The paper explores the use of different prompt templates with GPT-3.5-turbo and notes variations in performance, suggesting that there is room for further exploration in utilizing LLMs for fact-checking tasks.
- Why unresolved: The paper does not delve into the specific limitations of prompt-based approaches, such as the impact of prompt design on model performance or the potential biases introduced by certain prompts.
- What evidence would resolve it: Analyzing the effects of various prompt designs on LLM performance across different fact verification datasets and identifying strategies to mitigate any identified limitations or biases.

### Open Question 3
- Question: How can the dataset EX-FEVER be improved to better evaluate the explainability and reasoning capabilities of fact verification models?
- Basis in paper: [explicit] The paper introduces EX-FEVER as a dataset for multi-hop explainable fact verification but acknowledges limitations in evaluating both explainability and reasoning capabilities of models.
- Why unresolved: The paper suggests that the current evaluation metrics, such as Rouge score for explanations and accuracy for verdicts, are insufficient for comprehensively assessing model performance.
- What evidence would resolve it: Developing and implementing new evaluation metrics or methodologies that more effectively measure both the explainability and reasoning capabilities of fact verification models using EX-FEVER.

## Limitations

- The scalability of crowd-worker-based dataset construction is not fully evaluated
- The assertion about multi-hop reasoning superiority needs more ablation studies
- LLM evaluation section is preliminary with limited prompt engineering exploration

## Confidence

- Dataset construction methodology: High
- Multi-hop reasoning effectiveness: Medium
- LLM evaluation results: Low

## Next Checks

1. Conduct inter-annotator agreement studies on a larger validation sample to quantify the reliability of the multi-hop claim construction process.

2. Perform ablation studies comparing single-hop vs. multi-hop versions of EX-FEVER claims to isolate the specific impact of multi-hop reasoning on model performance.

3. Implement and evaluate additional state-of-the-art fact verification models (beyond the three tested) to provide a more comprehensive assessment of EX-FEVER's difficulty relative to existing datasets.