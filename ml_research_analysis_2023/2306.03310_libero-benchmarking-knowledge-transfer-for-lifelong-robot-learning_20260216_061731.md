---
ver: rpa2
title: 'LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning'
arxiv_id: '2306.03310'
source_url: https://arxiv.org/abs/2306.03310
tags:
- learning
- task
- tasks
- lifelong
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LIBERO is a benchmark for lifelong robot learning (LLDM) that addresses
  the challenge of transferring procedural knowledge in decision-making tasks. The
  benchmark introduces a procedural generation pipeline for creating an infinite number
  of manipulation tasks, along with four task suites (130 tasks total) designed to
  study different types of knowledge transfer.
---

# LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning

## Quick Facts
- arXiv ID: 2306.03310
- Source URL: https://arxiv.org/abs/2306.03310
- Authors: 
- Reference count: 40
- Key outcome: LIBERO is a benchmark for lifelong robot learning (LLDM) that addresses the challenge of transferring procedural knowledge in decision-making tasks

## Executive Summary
LIBERO is a benchmark for lifelong robot learning (LLDM) that addresses the challenge of transferring procedural knowledge in decision-making tasks. The benchmark introduces a procedural generation pipeline for creating an infinite number of manipulation tasks, along with four task suites (130 tasks total) designed to study different types of knowledge transfer. High-quality human-teleoperated demonstration data is provided for efficient learning.

Key findings from experiments include: sequential finetuning outperforms existing lifelong learning methods in forward transfer; no single visual encoder architecture excels at all types of knowledge transfer; and naive supervised pretraining on a large-scale offline dataset can hinder agents' performance in subsequent LLDM tasks. The benchmark provides insights into neural architecture design, lifelong learning algorithm effectiveness, robustness to task ordering, and the impact of model pretraining on LLDM performance.

## Method Summary
LIBERO provides a procedural generation pipeline for creating an infinite number of manipulation tasks, along with four task suites totaling 130 tasks designed to study different types of knowledge transfer. The benchmark uses high-quality human-teleoperated demonstration data for efficient learning. Policies are trained using behavioral cloning on demonstration data with three neural architectures (RESNET-RNN, RESNET-T, VIT-T) and evaluated using three lifelong learning algorithms (Experience Replay, Elastic Weight Consolidation, PACKNET) plus sequential finetuning and multitask learning baselines. Performance is measured using forward transfer (FWT), negative backward transfer (NBT), and area under the curve (AUC) of success rates.

## Key Results
- Sequential finetuning outperforms existing lifelong learning methods in forward transfer
- No single visual encoder architecture excels at all types of knowledge transfer; vision transformers work well on tasks with rich visual information, while convolution networks are better for procedural knowledge
- Naive supervised pretraining on a large-scale offline dataset can hinder agents' performance in subsequent LLDM tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential finetuning outperforms existing lifelong learning methods in forward transfer because it preserves the ability to learn new tasks without interference from regularization or capacity constraints.
- Mechanism: By avoiding explicit regularization or dynamic capacity management, sequential finetuning allows the model to fully adapt to each new task using all available parameters and data.
- Core assumption: The model's capacity is sufficient to learn each task independently without catastrophic forgetting being severe enough to prevent forward transfer.
- Evidence anchors:
  - [abstract] "sequential finetuning outperforms existing lifelong learning methods in forward transfer"
  - [section] "SEQL shows the best FWT over all task suites. This is surprising since it indicates all lifelong learning algorithms we consider actually hurt forward transfer"
  - [corpus] No direct corpus evidence supporting this specific mechanism
- Break condition: If the model capacity is insufficient for the task complexity, sequential finetuning will suffer from catastrophic forgetting and poor forward transfer.

### Mechanism 2
- Claim: No single visual encoder architecture excels at all types of knowledge transfer because different architectures have complementary strengths for processing different types of information.
- Mechanism: Vision transformers excel at processing rich visual information with diverse objects, while convolution networks are better suited for tasks requiring procedural knowledge due to their spatial hierarchy understanding.
- Core assumption: The type of knowledge transfer required varies systematically across tasks, and architectural biases align with these requirements.
- Evidence anchors:
  - [abstract] "no single visual encoder architecture excels at all types of knowledge transfer; vision transformers work well on tasks with rich visual information, while convolution networks are better for procedural knowledge"
  - [section] "Vision transformers work well on tasks with rich visual information (e.g., a variety of objects). Convolution networks work well when tasks primarily need procedural knowledge"
  - [corpus] No direct corpus evidence supporting this specific mechanism
- Break condition: If tasks don't have systematic differences in the type of knowledge required, architectural differences may not lead to performance differences.

### Mechanism 3
- Claim: Naive supervised pretraining can hinder agents' performance in subsequent LLDM tasks because the pretraining objective doesn't align with the lifelong learning objective.
- Mechanism: Pretraining on a large-scale offline dataset creates representations optimized for that specific distribution, which may not transfer well to the sequential, distribution-shifted tasks in LLDM.
- Core assumption: The distribution of tasks in LLDM is sufficiently different from the pretraining distribution that the learned representations become suboptimal or harmful.
- Evidence anchors:
  - [abstract] "naive supervised pretraining can hinder agents' performance in subsequent LLDM"
  - [section] "Basic supervised pretraining on a large-scale offline dataset can have a negative impact on the learner's downstream performance in LLDM"
  - [corpus] No direct corpus evidence supporting this specific mechanism
- Break condition: If the pretraining and LLDM task distributions are similar, or if the pretraining is done with LLDM objectives in mind, the negative impact may not occur.

## Foundational Learning

- Concept: Procedural vs Declarative Knowledge Transfer
  - Why needed here: LLDM involves both types of knowledge, unlike traditional lifelong learning which focuses primarily on declarative knowledge.
  - Quick check question: What distinguishes procedural knowledge from declarative knowledge in the context of robot manipulation tasks?

- Concept: Catastrophic Forgetting
  - Why needed here: Lifelong learning algorithms aim to prevent catastrophic forgetting while maintaining forward transfer ability.
  - Quick check question: Why does sequential finetuning, which doesn't explicitly prevent forgetting, sometimes outperform methods designed to prevent it?

- Concept: Distribution Shift
  - Why needed here: LLDM tasks involve continuous learning across tasks with shifting object distributions, spatial arrangements, and task goals.
  - Quick check question: How does the type of distribution shift (objects, spatial, goals) affect which neural architecture performs best?

## Architecture Onboarding

- Component map: Procedural generation pipeline -> Task suites (130 tasks) -> Neural architectures (RESNET-RNN, RESNET-T, VIT-T) -> Lifelong learning algorithms (SEQL, MTL, EWC, ER, PACKNET) -> Evaluation metrics (FWT, NBT, AUC)

- Critical path: Task generation → Policy architecture selection → Lifelong learning algorithm selection → Training on task sequence → Evaluation on all tasks

- Design tradeoffs: RESNET-T offers good performance across tasks but may struggle with very rich visual scenes. VIT-T excels at rich visual scenes but may be overkill for procedural tasks. Sequential finetuning is simple but risks forgetting. ER prevents forgetting but may hurt forward transfer. PACKNET prevents forgetting through capacity constraints but may limit forward transfer.

- Failure signatures: Poor forward transfer suggests the algorithm or architecture isn't learning new tasks effectively. High NBT indicates catastrophic forgetting. Poor performance on specific task suites suggests architectural mismatch with the knowledge transfer type.

- First 3 experiments:
  1. Compare SEQL vs ER on LIBERO-SPATIAL with RESNET-T to understand forward transfer vs forgetting tradeoff.
  2. Test VIT-T vs RESNET-T on LIBERO-OBJECT to observe performance differences when rich visual information is key.
  3. Evaluate pretraining impact by comparing models trained from scratch vs pretrained on LIBERO-90 for LIBERO-LONG.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific factors that contribute to the performance differences observed among different neural architectures when using the same lifelong learning algorithm?
- Basis in paper: [explicit] The paper mentions that the performance difference among different architectures depends on the underlying lifelong learning algorithm.
- Why unresolved: While the paper observes that RESNET-T performs better than VIT-T on all task suites except LIBERO-OBJECT when using ER, and VIT-T performs better than RESNET-T on LIBERO-LONG when using PACKNET, it does not provide a detailed analysis of the specific factors contributing to these differences.
- What evidence would resolve it: A comprehensive analysis of the factors influencing the performance differences, such as the complexity of the tasks, the amount of visual information, and the network capacity, would help resolve this question.

### Open Question 2
- Question: How can the forward transfer ability of lifelong learning algorithms be improved to outperform sequential finetuning?
- Basis in paper: [explicit] The paper states that SEQL shows the best FWT over all task suites, indicating that all lifelong learning algorithms evaluated actually hurt forward transfer.
- Why unresolved: The paper does not provide a detailed explanation of why existing lifelong learning algorithms fail to outperform SEQL in terms of forward transfer.
- What evidence would resolve it: Developing and testing new lifelong learning algorithms that specifically target improving forward transfer, and comparing their performance against SEQL, would help resolve this question.

### Open Question 3
- Question: What are the specific reasons behind the negative impact of basic supervised pretraining on the downstream performance of lifelong learning in LLDM?
- Basis in paper: [explicit] The paper mentions that basic supervised pretraining on a large-scale offline dataset can have a negative impact on the learner's downstream performance in LLDM.
- Why unresolved: The paper does not provide a detailed explanation of why basic supervised pretraining negatively affects downstream lifelong learning performance.
- What evidence would resolve it: Investigating the specific factors contributing to the negative impact of pretraining, such as the type of tasks used for pretraining, the size of the pretraining dataset, and the differences between pretraining and LLDM tasks, would help resolve this question.

## Limitations

- The procedural generation pipeline may not capture the full complexity of real-world manipulation scenarios
- The benchmark focuses on table-top manipulation tasks, which may not generalize to more complex robot learning scenarios
- The study uses behavioral cloning, which has inherent limitations in handling out-of-distribution states during execution

## Confidence

- High confidence: Sequential finetuning outperforming lifelong learning methods in forward transfer; architectural differences between vision transformers and convolution networks for different task types
- Medium confidence: Claims about pretraining hindering LLDM performance, as this depends heavily on the specific pretraining dataset and methodology used
- Medium confidence: The distinction between procedural and declarative knowledge transfer, as the paper's categorization may not capture all nuances of robot learning

## Next Checks

1. Test sequential finetuning vs lifelong learning algorithms on a broader range of task types beyond table-top manipulation to verify generalizability
2. Experiment with alternative pretraining strategies (e.g., domain-adaptive pretraining) to determine if the negative impact can be mitigated
3. Evaluate the impact of demonstration quality and quantity on lifelong learning performance to understand the sensitivity to training data characteristics