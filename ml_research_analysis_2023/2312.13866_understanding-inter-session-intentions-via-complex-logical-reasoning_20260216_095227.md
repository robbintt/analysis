---
ver: rpa2
title: Understanding Inter-Session Intentions via Complex Logical Reasoning
arxiv_id: '2312.13866'
source_url: https://arxiv.org/abs/2312.13866
tags:
- session
- query
- logical
- graph
- lsgt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces logical session complex query answering (LS-CQA),
  a task that addresses complex user intentions involving multiple sessions and attribute
  requirements connected by logical operators. The proposed Logical Session Graph
  Transformer (LSGT) treats sessions as ordered hyperedges and uses a transformer
  structure to capture interactions among items across different sessions and their
  logical connections.
---

# Understanding Inter-Session Intentions via Complex Logical Reasoning

## Quick Facts
- arXiv ID: 2312.13866
- Source URL: https://arxiv.org/abs/2312.13866
- Reference count: 40
- Key outcome: Introduces LSGT, achieving state-of-the-art results on complex session query answering tasks including EPFO queries and negation queries.

## Executive Summary
This paper addresses the challenge of understanding complex user intentions that span multiple sessions by introducing Logical Session Complex Query Answering (LS-CQA). The proposed Logical Session Graph Transformer (LSGT) treats sessions as ordered hyperedges in a hypergraph and uses a transformer architecture to capture interactions among items across different sessions connected by logical operators. The method achieves state-of-the-art performance on three real-world datasets, demonstrating its ability to handle complex queries involving intersections, unions, and negations across multiple sessions.

## Method Summary
LSGT transforms items, sessions, relation features, session structures, and logical structures into tokens, which are then encoded using a standard transformer model. The model treats sessions as ordered hyperedges in a hypergraph representation, allowing it to capture complex relationships between items across multiple sessions. The transformer's attention mechanisms enable any-to-any interactions, while the model maintains permutation invariance for logical operators like intersection and union. The approach is trained using cross-entropy loss on query-answer pairs and demonstrates strong performance on complex logical queries involving multiple sessions.

## Key Results
- Achieves state-of-the-art performance on three datasets (Amazon, Diginetica, Dressipi)
- Outperforms baseline models on both EPFO queries and queries involving negations
- Demonstrates strong compositional generalization on unseen query types
- Theoretical analysis proves LSGT's expressiveness matches or exceeds existing logical query encoders

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LSGT captures interactions among items across different sessions by treating sessions as ordered hyperedges and using a transformer structure.
- **Mechanism:** The model transforms items, sessions, relation features, session structures, and logical structures into tokens, which are then encoded using a standard transformer model. This enables any-to-any attention mechanisms to capture interactions among items in different sessions through their logical connections.
- **Core assumption:** The transformer's attention mechanism can effectively model the complex relationships between items across multiple sessions when these relationships are encoded as tokens.
- **Evidence anchors:**
  - [abstract]: "We also propose a new model, the Logical Session Graph Transformer (LSGT), which captures interactions among items across different sessions and their logical connections using a transformer structure."
  - [section]: "Building upon the work by Kim et al. (2022), we transform items, sessions, relation features, session structures, and logical structures into tokens, which are then encoded using a standard transformer model. This transformation enables us to effectively capture interactions among items in different sessions through the any-to-any attention mechanisms in transformer models."
- **Break condition:** If the tokenization of sessions and items fails to preserve the necessary structural information for the transformer to capture meaningful interactions.

### Mechanism 2
- **Claim:** LSGT has at least the same expressiveness as existing logical query encoders that employ message-passing mechanisms for logical query encoding in WL test.
- **Mechanism:** By analyzing the Relational Weisfeiler-Lehman test, the paper proves that LSGT possesses the expressiveness of at least 1-RWL, which is the same expressiveness as R-GCN and CompGCN.
- **Core assumption:** The expressiveness of a model in the WL test is a valid proxy for its ability to capture complex logical relationships in session data.
- **Evidence anchors:**
  - [abstract]: "We analyze the expressiveness of LSGT and prove the permutation invariance of the inputs for the logical operators."
  - [section]: "We analyze the Relational Weisfeiler-Lehman by Barcel Â´o et al. (2022); Huang et al. (2023), we provide theoretical justification for LSGT, demonstrating that it possesses the expressiveness of at least 1-RWL, and has at least same expressiveness as existing logical query encoders that employ message-passing mechanisms for logical query encoding in WL test."
- **Break condition:** If the assumptions about the relationship between WL test expressiveness and actual query answering performance are incorrect.

### Mechanism 3
- **Claim:** LSGT maintains operator-wise permutation invariance, which is important for query encoding as operators like Intersection and Union are permutation invariant to inputs.
- **Mechanism:** The paper proves that LSGT can approximate a logical query encoding model that is operator-wise input permutation invariant.
- **Core assumption:** Maintaining permutation invariance for logical operators is crucial for accurate query encoding.
- **Evidence anchors:**
  - [abstract]: "We analyze the expressiveness of LSGT and prove the permutation invariance of the inputs for the logical operators."
  - [section]: "Meanwhile, LSGT maintains the property of operation-wise permutation invariance, similar to other logical query encoders."
- **Break condition:** If the model's performance degrades significantly when input order is permuted, indicating that the permutation invariance property is not effectively maintained.

## Foundational Learning

- **Concept:** Transformer models and attention mechanisms
  - **Why needed here:** The core of LSGT is a transformer that captures interactions among items across different sessions.
  - **Quick check question:** How does the self-attention mechanism in transformers allow for modeling relationships between different elements in a sequence?

- **Concept:** Graph neural networks and message passing
  - **Why needed here:** The paper compares LSGT's expressiveness to models using message passing, like R-GCN and CompGCN.
  - **Quick check question:** What is the Weisfeiler-Lehman test, and how does it relate to the expressiveness of graph neural networks?

- **Concept:** Hypergraphs and hyperedges
  - **Why needed here:** Sessions are treated as hyperedges connecting items in the hypergraph representation of user behavior.
  - **Quick check question:** How does a hypergraph differ from a standard graph, and why is this distinction important for modeling user sessions?

## Architecture Onboarding

- **Component map:** Tokenization module -> Transformer encoder -> Output layer -> Training module

- **Critical path:**
  1. Tokenize input session data and logical query structure.
  2. Feed tokens into the transformer encoder.
  3. Extract the [graph] token output as the query embedding.
  4. Compute similarity scores with candidate answers.
  5. Apply softmax to obtain probabilities.
  6. Calculate cross-entropy loss and update model parameters.

- **Design tradeoffs:**
  - Tokenization vs. direct graph encoding: Tokenization allows for using standard transformer architectures but may lose some structural information.
  - Fixed vs. variable token length: Fixed length simplifies the transformer architecture but may require padding or truncation.
  - Separate vs. joint encoding of sessions and logical structure: Separate encoding may allow for more specialized processing but could miss cross-modal interactions.

- **Failure signatures:**
  - Poor performance on queries involving negations or complex logical structures.
  - Sensitivity to the order of items within sessions.
  - Inability to generalize to unseen query types or structures.

- **First 3 experiments:**
  1. Ablation study removing logical structure tokens to assess their importance.
  2. Evaluation on queries with negations to test the model's ability to handle complex logical operations.
  3. Zero-shot evaluation on unseen query types to assess compositional generalization capabilities.

## Open Questions the Paper Calls Out
None explicitly identified in the provided content.

## Limitations
- Theoretical expressiveness analysis relies heavily on WL test properties without direct correlation to practical query answering performance
- Model's ability to handle complex negations and compositional queries lacks deeper theoretical justification
- Specific contribution of logical structure tokens versus standard transformer improvements is unclear

## Confidence
- **High confidence**: The basic architecture of LSGT as a transformer-based model for session query answering is sound and well-specified.
- **Medium confidence**: The theoretical claims about expressiveness and permutation invariance are technically correct but their practical significance for session query answering needs more validation.
- **Medium confidence**: The empirical results showing improved performance over baselines are compelling, though the specific contribution of the logical structure tokens versus standard transformer improvements is unclear.

## Next Checks
1. **Ablation on logical structure tokens**: Remove the logical structure tokens from LSGT and retrain to isolate the contribution of explicit logical token encoding versus the transformer's inherent capability to learn logical relationships from item/session patterns.

2. **Cross-dataset generalization test**: Train LSGT on one dataset (e.g., Amazon) and evaluate on unseen query types from another dataset (e.g., Diginetica) to rigorously test compositional generalization beyond the reported zero-shot results.

3. **Permutation invariance stress test**: Systematically permute item orders within sessions and measure the impact on query answering performance to empirically validate the claimed permutation invariance properties for intersection and union operations.