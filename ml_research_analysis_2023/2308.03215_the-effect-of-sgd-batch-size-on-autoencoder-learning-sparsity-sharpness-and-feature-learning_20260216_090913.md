---
ver: rpa2
title: 'The Effect of SGD Batch Size on Autoencoder Learning: Sparsity, Sharpness,
  and Feature Learning'
arxiv_id: '2308.03215'
source_url: https://arxiv.org/abs/2308.03215
tags:
- which
- will
- then
- have
- since
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how the batch size affects the solutions found
  by stochastic gradient descent (SGD) and gradient descent (GD) when training a single-neuron
  autoencoder on orthonormal data. The authors show that with full-batch GD, the solution
  is dense (not sparse) and highly aligned with its initialization, implying little
  feature learning.
---

# The Effect of SGD Batch Size on Autoencoder Learning: Sparsity, Sharpness, and Feature Learning

## Quick Facts
- arXiv ID: 2308.03215
- Source URL: https://arxiv.org/abs/2308.03215
- Reference count: 40
- Primary result: With full-batch GD, the solution is dense and aligned with initialization; with any batch size smaller than dataset size, SGD converges to a sparse solution nearly orthogonal to initialization.

## Executive Summary
This paper studies how batch size affects the solutions found by stochastic gradient descent (SGD) and gradient descent (GD) when training a single-neuron autoencoder on orthonormal data. The authors show that full-batch GD produces dense solutions aligned with initialization, implying minimal feature learning, while any batch size smaller than the dataset size causes SGD to converge to sparse solutions nearly orthogonal to initialization, indicating strong feature learning. Surprisingly, the paper shows that smaller batch sizes lead to sharper minima (measured by Hessian trace) compared to full-batch GD, challenging prior work suggesting smaller batches find flatter minima. The authors introduce a tool from non-homogeneous random walk theory to prove convergence of SGD with constant step size.

## Method Summary
The paper analyzes a single-neuron linear or ReLU autoencoder trained on orthonormal data using full-batch GD and mini-batch SGD with constant step size η ≤ 1/5. The orthonormal dataset D = {a₁, ..., aₘ} consists of m ≤ n orthonormal vectors. The model is trained from random initialization ||w₀|| < 1, and the authors track coordinate evolution, solution sparsity, alignment with initialization, and Hessian-based sharpness measures. The key theoretical contribution is proving convergence of SGD with constant step size via non-homogeneous random walk theory.

## Key Results
- Full-batch GD converges to dense solutions aligned with initialization (low feature learning)
- Mini-batch SGD (b < m) converges to sparse solutions nearly orthogonal to initialization (high feature learning)
- Smaller batch sizes lead to sharper minima when measured by Hessian trace
- The symmetry-breaking mechanism is driven by stochastic gradient noise
- A non-homogeneous random walk framework proves SGD convergence with constant step size

## Why This Works (Mechanism)

### Mechanism 1
- Random mini-batch sampling breaks the symmetry that keeps full-batch GD aligned with initialization, forcing SGD to explore sparse, orthogonal solutions.
- The randomness introduces stochastic drift that amplifies correlation with a single data direction over all others, leading iterates to align with one datapoint rather than a dense mixture.
- Core assumption: Iterates remain in a bounded region where gradient noise can consistently bias the walk toward a single direction.
- Evidence: [abstract] "randomness of stochastic gradients induces a qualitatively different type of 'feature selection'"; [section 4.3] log-ratio stochastic process Rₜ captures relative alignment and is transient under mini-batch SGD but constant under full-batch GD.
- Break condition: If step size or initialization violates boundedness assumptions, stochastic drift may not dominate and solution may not become sparse.

### Mechanism 2
- Smaller batch sizes lead to sharper minima when sharpness is measured by trace of the Hessian.
- The sparse, orthogonal solution found by SGD concentrates curvature in fewer directions, increasing average eigenvalue of the Hessian compared to the flat, dense mixture found by full-batch GD.
- Core assumption: Sharpness is measured using trace of Hessian, which captures average curvature rather than maximum curvature.
- Evidence: [abstract] "SGD converges to sharper minima than GD when the activation is ReLU"; [section 5.1] Theorem 13 explicitly shows Tr(H_SGD) > Tr(H_GD) for large m.
- Break condition: If different sharpness metric (e.g., maximum eigenvalue) is used, conclusion may reverse or become inconclusive.

### Mechanism 3
- Convergence of SGD with constant step size to a single point is enabled by a tool from non-homogeneous random walk theory.
- The process Rₜ is shown to be transient, meaning it diverges to infinity, which forces iterates to align with a single data point and loss gradients to vanish.
- Core assumption: Increment of Rₜ has positive conditional expectation that dominates noise, satisfying transience conditions of random walk theorem.
- Evidence: [abstract] "we introduce a powerful tool from the theory of non-homogeneous random walks"; [section 4.4] Proposition 8 states conditions under which a process is transient.
- Break condition: If step size is too large or data structure changes, conditions for transience may fail and iterates may not converge to a point.

## Foundational Learning

- **Stochastic gradient descent with constant step size**: Why needed here: The paper proves convergence of SGD with fixed step size, which is non-standard and requires new tools. Quick check question: Why does constant step size suffice for convergence in this setting, unlike typical SGD analyses?

- **Non-homogeneous random walks**: Why needed here: Used to prove transience of stochastic process Rₜ, which is key to showing SGD converges to a single point. Quick check question: What conditions must a stochastic process satisfy to be transient according to the theory?

- **Sharpness measures based on Hessian eigenvalues**: Why needed here: The paper compares minima found by SGD and GD using maximum and average curvature, showing smaller batches can lead to sharper solutions. Quick check question: How do maximum eigenvalue and trace of the Hessian differ as measures of sharpness?

## Architecture Onboarding

- **Component map**: Orthonormal data → Single-neuron autoencoder → Full-batch GD or mini-batch SGD → Coordinate updates → Stochastic process analysis → Sparsity/sharpness comparison

- **Critical path**: 
  1. Initialize w₀ with ||w₀|| < 1
  2. Run GD or SGD with step size η ≤ 1/5
  3. Track coordinate evolution and process Rₜ
  4. Prove convergence to global minima
  5. Compare sparsity and sharpness of solutions

- **Design tradeoffs**: 
  - Orthonormal data simplifies analysis but limits generality
  - Constant step size is simpler than decaying schedules but requires careful analysis
  - Single neuron limits applicability to deep networks

- **Failure signatures**:
  - If ||w₀|| ≥ 1 or η > 1/5, iterates may diverge
  - If data is not orthonormal, symmetry arguments may break down
  - If different sharpness metric is used, batch size conclusion may reverse

- **First 3 experiments**:
  1. Simulate GD and SGD on small orthonormal dataset, visualize trajectories and final solutions
  2. Compute Hessian trace at GD and SGD solutions to verify sharpness claim
  3. Vary batch size and step size to see how they affect sparsity and alignment of solutions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does batch size affect generalization performance of SGD in more complex neural network architectures beyond the single-neuron autoencoder studied here?
- Basis: [inferred] The paper shows SGD with smaller batches finds sparse, data-dependent solutions, but doesn't explore how this translates to generalization in larger networks.
- Why unresolved: Paper focuses on specific, simplified model; authors acknowledge need to extend analysis to more complex models and distributions.
- What evidence would resolve it: Empirical studies comparing generalization performance of SGD with different batch sizes on various neural network architectures and datasets.

### Open Question 2
- Question: Can techniques developed here for analyzing SGD convergence with constant step size be applied to other machine learning algorithms beyond single-neuron autoencoder?
- Basis: [explicit] Authors introduce powerful tool from non-homogeneous random walk theory and believe it may be more broadly applicable for analysis of other machine learning algorithms.
- Why unresolved: Paper only demonstrates application to specific problem; further research needed to validate effectiveness in other contexts.
- What evidence would resolve it: Successful application of techniques to analyze convergence of SGD or other optimization algorithms in different machine learning settings.

### Open Question 3
- Question: How does choice of activation function affect relationship between batch size, solution sparsity, and feature learning in neural networks?
- Basis: [inferred] Paper studies both linear and ReLU activations but doesn't systematically explore how different activation functions might influence effects of batch size on sparsity and feature learning.
- Why unresolved: Paper only considers two specific activation functions; authors don't provide comprehensive analysis of role of activation functions in this context.
- What evidence would resolve it: Comparative studies examining impact of various activation functions on sparsity and feature learning properties of SGD solutions with different batch sizes.

## Limitations
- Conclusions based on highly simplified setting: single-neuron linear/ReLU autoencoder on orthonormal data
- Symmetry-breaking mechanism may not directly extend to multi-layer networks, non-orthonormal data, or complex loss landscapes
- Analysis relies on specific conditions (step size η ≤ 1/5, bounded initialization) that may be restrictive in practice
- Choice of sharpness metric (trace of Hessian) is crucial to main result but contradicts other studies using different sharpness measures

## Confidence

**High confidence**: SGD with constant step size converges to global minima for single-neuron autoencoder on orthonormal data (proven via non-homogeneous random walk theory)

**Medium confidence**: Full-batch GD finds dense, initialization-aligned solutions while mini-batch SGD finds sparse, orthogonal solutions due to symmetry-breaking from stochastic gradients (supported by theory and simulation but limited to simple setting)

**Medium confidence**: Smaller batch sizes lead to sharper minima when measured by Hessian trace (proven in single-neuron case but may reverse for other sharpness metrics or architectures)

## Next Checks

1. **Empirical generalization**: Train multi-layer autoencoders on non-orthonormal real-world datasets and measure sparsity, sharpness, and feature learning of solutions found by GD vs. SGD with different batch sizes.

2. **Sharpness metric sensitivity**: Compare sharpness of GD and SGD solutions using multiple metrics (maximum eigenvalue, trace, path-norm) to determine if "smaller batches lead to sharper minima" conclusion is robust.

3. **Mechanism isolation**: Perform ablation studies on single-neuron setting by varying step size, batch size, and data orthogonality to precisely map conditions under which symmetry-breaking and sparsity emerge.