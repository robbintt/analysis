---
ver: rpa2
title: Exposing Limitations of Language Model Agents in Sequential-Task Compositions
  on the Web
arxiv_id: '2311.18751'
source_url: https://arxiv.org/abs/2311.18751
tags:
- 'false'
- tasks
- arxiv
- task
- lmas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new benchmark, CompWoB, designed to test
  the generalization of language model agents (LMAs) to compositional web automation
  tasks. CompWoB consists of 50 tasks that combine simpler base tasks, reflecting
  more realistic real-world scenarios.
---

# Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web

## Quick Facts
- **arXiv ID**: 2311.18751
- **Source URL**: https://arxiv.org/abs/2311.18751
- **Reference count**: 40
- **Primary result**: LMAs achieve 94.0% success on base tasks but only 24.9% on compositional tasks, with reverse-order instructions causing additional performance drops

## Executive Summary
This paper introduces CompWoB, a benchmark for evaluating language model agents (LMAs) on compositional web automation tasks. The benchmark consists of 50 tasks created by combining simpler base tasks from MiniWoB, reflecting realistic real-world scenarios. The authors evaluate various LMAs including prompted and transferred models on CompWoB and find significant performance degradation when moving from base to compositional tasks. Transferred LMAs, particularly a new model called HTML-T5++, show better generalization to compositional tasks. The study also reveals that LMAs are highly sensitive to instruction order, with reverse-order instructions leading to further performance drops. These findings highlight the need for more robust and generalizable LMAs for real-world deployment.

## Method Summary
The authors created CompWoB by combining base tasks from MiniWoB into more complex compositional tasks. They evaluated several LMA approaches including RCI, AdaPlanner, Synapse, and transferred models like HTML-T5++ on these tasks using zero-shot transfer. The evaluation measured success rates on compositional tasks and tested sensitivity to instruction order by reversing task sequences. HTML-T5++ was trained on base task demonstrations with a data rebalancing strategy to reduce overfitting. The study systematically analyzed factors affecting performance including instruction length and HTML subtree depth.

## Key Results
- Prompted LMAs drop from 94.0% success on base tasks to 24.9% on compositional tasks
- Transferred LMAs perform better at 54.8% success on compositional tasks
- LMAs show significant performance degradation with reverse-order instructions
- Instruction length and HTML subtree depth are key factors affecting compositional task performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Language model agents suffer from compositional generalization gaps between base and compositional tasks.
- **Mechanism**: When base tasks are combined into compositional tasks, the agents' ability to generalize from training on individual tasks degrades significantly.
- **Core assumption**: The agents have learned sufficient base task capabilities but cannot effectively compose these learned skills.
- **Evidence anchors**:
  - [abstract]: "while existing prompted LMAs achieve 94.0% average success rate on base tasks, their performance degrades to 24.9% success rate on compositional tasks"
  - [section 6.1]: "in CompWoB, all the LMAs face performance degradation... Transferred LMAs achieve better success rate (54.8%) than prompted LMAs (24.9%) on average"
  - [corpus]: Weak evidence - only 5 related papers with low citation counts suggest limited prior research on compositional generalization limitations
- **Break condition**: If the agent has been explicitly trained or fine-tuned on compositional task combinations rather than just individual base tasks.

### Mechanism 2
- **Claim**: Transferred language model agents (finetuned on base tasks) generalize better to compositional tasks than prompted agents.
- **Mechanism**: Finetuned models develop internal representations that are more robust to task compositionality compared to prompted models that rely on few-shot exemplars.
- **Core assumption**: The finetuning process on base tasks creates transferable representations that can be composed for novel task combinations.
- **Evidence anchors**:
  - [abstract]: "transferred LMAs (finetuned only on base tasks) show less generalization gap, dropping from 85.4% to 54.8%"
  - [section 6.1]: "Transferred LMAs achieve better success rate (54.8%) than prompted LMAs (24.9%) on average"
  - [corpus]: No direct evidence in corpus about finetuned vs prompted generalization differences
- **Break condition**: If the compositional tasks require skills not present in the base task training data.

### Mechanism 3
- **Claim**: Language model agents are sensitive to instruction order, with reverse-order instructions causing additional performance degradation.
- **Mechanism**: The sequential planning and execution pipeline in LMAs is order-dependent, causing failures when instruction order is reversed.
- **Core assumption**: The agent's plan generation and execution are tightly coupled to the linear sequence of instructions.
- **Evidence anchors**:
  - [abstract]: "LMAs are highly sensitive to the order of instructions, with reverse-order instructions leading to further performance drops"
  - [section 6.2]: "all the LMAs significantly degrade the success rate when reverse-order instructions are provided... This trend is more remarkable in transferred LMAs"
  - [corpus]: No evidence in corpus about instruction order sensitivity
- **Break condition**: If the agent develops order-invariant reasoning capabilities or uses hierarchical planning that can restructure instruction sequences.

## Foundational Learning

- **Concept**: Compositional generalization
  - Why needed here: Understanding how agents can generalize from learning individual tasks to performing novel combinations of those tasks is central to this research
  - Quick check question: Can an agent that can solve task A and task B individually also solve "task A then task B" without additional training?

- **Concept**: Transfer learning vs prompting
  - Why needed here: The paper contrasts finetuned (transferred) models with prompted models, showing different generalization behaviors
  - Quick check question: What is the key architectural difference between a model that has been finetuned on demonstrations versus one that relies on in-context learning?

- **Concept**: HTML DOM structure and web automation
  - Why needed here: The tasks involve navigating and manipulating web page elements, requiring understanding of HTML structure and element selection
  - Quick check question: How does an agent identify and interact with specific elements on a web page using HTML selectors?

## Architecture Onboarding

- **Component map**: HTML observation → Instruction parsing → Plan generation → Action execution → Success checking
- **Critical path**: HTML observation → Instruction parsing → Plan generation → Action execution → Success checking
- **Design tradeoffs**:
  - Prompt-based vs finetuned approaches: Prompting requires fewer training examples but may have worse generalization; finetuning requires more data but can achieve better compositional generalization
  - Context length vs performance: Longer context allows more complex instructions but increases computational cost and potential for errors
  - HTML processing depth vs efficiency: Deeper HTML parsing provides more information but increases complexity
- **Failure signatures**:
  - Missing intermediate steps in compositional tasks
  - Incorrect action type predictions (click vs type)
  - XPath hallucination or incorrect element selection
  - Sensitivity to instruction order changes
- **First 3 experiments**:
  1. Compare base task performance vs compositional task performance on a small subset of tasks
  2. Test reverse-order instruction performance on simple two-task compositions
  3. Evaluate finetuned vs prompted approaches on the same compositional tasks

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do larger foundation models (e.g., GPT-4, text-davinci-003) perform on CompWoB compositional tasks compared to their base task performance?
- **Basis in paper**: [explicit] The paper evaluates GPT-4 and text-davinci-003 on CompWoB, finding that even these advanced models struggle with compositional tasks, with performance degrading significantly from base tasks.
- **Why unresolved**: While the paper shows that advanced models improve performance, they still fall short of solving compositional tasks effectively. Further research is needed to understand the specific limitations and potential improvements.
- **What evidence would resolve it**: Detailed analysis of model performance on different types of compositional tasks, identifying specific failure modes and areas for improvement.

### Open Question 2
- **Question**: What are the key factors that contribute to the success or failure of language model agents on compositional web automation tasks?
- **Basis in paper**: [explicit] The paper identifies instruction length and HTML subtree depth as significant factors affecting performance on compositional tasks.
- **Why unresolved**: While these factors are identified, the paper does not explore the full range of potential contributing factors or their relative importance. Further research is needed to understand the complete picture.
- **What evidence would resolve it**: Comprehensive analysis of various task characteristics and their impact on model performance, including factors like task complexity, instruction ambiguity, and observation noise.

### Open Question 3
- **Question**: How can we develop more generalizable prompting methods for language model agents to handle compositional tasks effectively?
- **Basis in paper**: [explicit] The paper suggests that current prompting methods may not be sufficient for compositional tasks and that developing more robust and generalizable methods is an important direction for future research.
- **Why unresolved**: The paper does not provide specific solutions or approaches for developing more generalizable prompting methods. Further research is needed to explore different techniques and evaluate their effectiveness.
- **What evidence would resolve it**: Development and evaluation of novel prompting methods specifically designed for compositional tasks, demonstrating their effectiveness in improving model performance.

## Limitations
- The evaluation relies on a relatively small benchmark of 50 compositional tasks, which may not capture the full complexity of real-world web automation scenarios
- The comparison between prompted and transferred models lacks statistical significance testing, making it difficult to assess whether observed performance differences are meaningful
- The paper does not provide detailed analysis of which specific compositional patterns cause the most difficulty for LMAs

## Confidence
**High Confidence**: The observation that LMAs show significant performance degradation on compositional tasks compared to base tasks is well-supported by the empirical results presented. The success rate drops from 94.0% to 24.9% for prompted models provide clear evidence of this limitation.

**Medium Confidence**: The claim that transferred LMAs generalize better than prompted LMAs to compositional tasks is supported by the data (54.8% vs 24.9% success rates), but the underlying reasons for this difference are not thoroughly investigated. The data rebalancing strategy for HTML-T5++ is mentioned but not fully detailed.

**Low Confidence**: The sensitivity to instruction order is demonstrated, but the paper does not explore whether this is due to fundamental limitations in the models' reasoning capabilities or simply artifacts of the specific prompting strategy used. The magnitude of performance degradation with reverse ordering (particularly for transferred models) needs more systematic investigation.

## Next Checks
1. **Statistical significance testing**: Conduct t-tests or similar analyses to determine whether the performance differences between prompted and transferred models on compositional tasks are statistically significant, and calculate confidence intervals for the success rates.

2. **Error analysis by compositional pattern**: Categorize the 50 compositional tasks by their structural patterns (sequential dependencies, parallel actions, conditional branching) and analyze which patterns cause the most failures for different LMA approaches.

3. **Controlled instruction order experiments**: Systematically test LMAs with tasks where instruction order should theoretically not matter (commutative operations) versus tasks where order is semantically important, to distinguish between genuine order sensitivity and other factors affecting performance.