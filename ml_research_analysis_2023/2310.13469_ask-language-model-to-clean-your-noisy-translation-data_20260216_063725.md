---
ver: rpa2
title: Ask Language Model to Clean Your Noisy Translation Data
arxiv_id: '2310.13469'
source_url: https://arxiv.org/abs/2310.13469
tags:
- sentence
- noisy
- clean
- noise
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating the robustness
  of neural machine translation models against noisy input. The authors propose a
  method to clean the target sentences in the MTNT dataset, a widely used benchmark
  for evaluating NMT models' performance in the presence of noise.
---

# Ask Language Model to Clean Your Noisy Translation Data

## Quick Facts
- arXiv ID: 2310.13469
- Source URL: https://arxiv.org/abs/2310.13469
- Reference count: 24
- This paper proposes using LLMs to clean noisy MTNT data, creating C-MTNT with significantly reduced noise while preserving semantic integrity.

## Executive Summary
This paper addresses the challenge of evaluating neural machine translation robustness against noisy input by proposing a method to clean target sentences in the MTNT dataset. The authors leverage large language models, particularly GPT-3.5, to remove noise while preserving semantic meaning through iterative cleaning with LASER similarity threshold checking. The resulting C-MTNT dataset exhibits significantly less noise in target sentences while maintaining semantic integrity. Human and GPT-4 evaluations consistently validate the effectiveness of this approach, demonstrating that advanced language models can serve as valuable tools for data cleaning in machine translation evaluation.

## Method Summary
The authors apply GPT-3.5 with few-shot prompts to clean noisy target sentences in the MTNT dataset using three approaches: bilingual cleaning (both source and target input), monolingual cleaning (target input only), and translation (source input only). Each sentence undergoes iterative cleaning up to 10 times until LASER semantic similarity between original and cleaned sentences exceeds 0.7 or maximum iterations are reached. Noise reduction is quantified by counting spelling/grammatical errors, emojis, slang, and profanities in original versus cleaned sentences, while semantic preservation is measured using LASER similarity scores.

## Key Results
- LLM-based cleaning significantly reduces noise types (emojis, slang, profanities) while maintaining semantic integrity
- C-MTNT dataset shows improved semantic preservation through LASER similarity metrics compared to original MTNT
- Human and GPT-4 evaluations consistently validate the quality of LLM-generated clean data
- The approach demonstrates effectiveness across multiple language pairs (en-fr, en-de, en-ja, fr-en, de-en)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based cleaning outperforms rule-based approaches in handling complex noise types.
- Mechanism: LLMs leverage contextual understanding to rephrase slang, jargon, and profanities while preserving semantic integrity, whereas rule-based methods can only filter or correct spelling/grammar.
- Core assumption: The LLM has sufficient capacity to understand context and apply nuanced transformations that rule-based systems cannot.
- Evidence anchors:
  - [abstract]: "we observe their impressive abilities in noise removal. For example, they can remove emojis with considering their semantic meaning. Additionally, we show that LLM can effectively rephrase slang, jargon, and profanities."
  - [section]: "While conventional language correction tools excel in rectifying spelling and grammatical errors, they are inadequate in effectively eliminating or paraphrasing slang, profanities, or emojis. Conversely, the LLM methods demonstrate proficiency in addressing such language phenomena."
  - [corpus]: Weak evidence - corpus analysis shows no direct comparison between LLM and rule-based methods on the same data.
- Break condition: If the LLM lacks sufficient training data for the specific domain or language pair, its contextual understanding may degrade.

### Mechanism 2
- Claim: Semantic preservation is maintained through LASER-based similarity thresholds.
- Mechanism: After each cleaning attempt, the system measures semantic similarity between original and cleaned sentences using LASER embeddings, repeating the process until similarity exceeds 0.7 or maximum iterations are reached.
- Core assumption: LASER embeddings provide reliable cross-lingual semantic similarity measurement for the target languages.
- Evidence anchors:
  - [abstract]: "Our human and GPT-4 evaluations also lead to a consistent conclusion that LLM performs well on this task."
  - [section]: "We measure the cosine similarity between the representations of original and cleaned sentences... we set a threshold for the LASER score as 0.7."
  - [corpus]: No direct corpus evidence provided for threshold effectiveness across all language pairs.
- Break condition: If the threshold is too strict, valid transformations may be rejected; if too lenient, semantic drift may occur.

### Mechanism 3
- Claim: Contrastive learning improves model robustness to noisy input.
- Mechanism: By training models to distinguish between original and augmented noisy sentences while maintaining translation quality, contrastive learning forces the model to learn noise-invariant representations.
- Core assumption: The augmented noise distribution adequately represents real-world noise patterns.
- Evidence anchors:
  - [section]: "By employing this method, we can analyze the performance of C-MTNT on a wider range of models trained with different approaches and settings."
  - [section]: "We combine both losses as the final loss: L = Lce + Î»Lctr"
  - [corpus]: No corpus evidence showing the augmented noise distribution matches real-world patterns.
- Break condition: If the augmented noise is too dissimilar from real-world noise, the contrastive learning signal becomes ineffective.

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: Guides the LLM through step-by-step reasoning for consistent cleaning decisions
  - Quick check question: What happens if the prompt lacks explicit reasoning steps?

- Concept: Semantic similarity metrics
  - Why needed here: Ensures cleaned sentences maintain meaning while removing noise
  - Quick check question: How would you verify that LASER similarity correlates with human judgment?

- Concept: Contrastive learning framework
  - Why needed here: Improves model robustness by learning to distinguish clean from noisy inputs
  - Quick check question: What's the difference between standard cross-entropy and contrastive loss?

## Architecture Onboarding

- Component map:
  Input -> GPT-3.5 API -> LASER Similarity Check -> Cleaned Output
  (noisy source/target pairs) (few-shot prompts) (threshold 0.7)

- Critical path:
  1. Receive noisy MTNT data
  2. Apply LLM cleaning via API
  3. Check LASER similarity threshold
  4. Repeat if below threshold
  5. Output cleaned dataset

- Design tradeoffs:
  - API call frequency vs. quality: More iterations improve quality but increase cost
  - Threshold strictness vs. coverage: Higher thresholds preserve meaning but may reject valid transformations
  - Prompt complexity vs. consistency: More detailed prompts improve consistency but may reduce flexibility

- Failure signatures:
  - Low semantic similarity scores across batches
  - Inconsistent cleaning decisions on similar inputs
  - API rate limiting or cost overruns
  - Model performance degradation on evaluation sets

- First 3 experiments:
  1. Compare BLEU scores between LLM-cleaned and rule-based cleaned datasets on MTNT
  2. Measure semantic drift by comparing LASER similarity distributions before/after cleaning
  3. Evaluate model robustness by training NMT models on augmented data and testing on C-MTNT vs. original MTNT

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt formulations (e.g., chain-of-thought vs. direct instruction) impact the quality of LLM-generated clean data?
- Basis in paper: [explicit] The paper mentions using chain-of-thought prompting and compares different approaches, but doesn't explore the impact of prompt variations.
- Why unresolved: The paper uses a specific prompt design but doesn't test alternative formulations to determine their impact on cleaning quality or semantic preservation.
- What evidence would resolve it: Experiments comparing multiple prompt formulations (different styles, lengths, example quantities) measuring their impact on noise removal effectiveness and semantic similarity scores.

### Open Question 2
- Question: What is the optimal threshold for semantic similarity (LASER score) that balances noise removal with semantic preservation?
- Basis in paper: [explicit] The paper sets a threshold of 0.7 for LASER similarity but acknowledges this was chosen to "strike a balance" without exploring alternatives.
- Why unresolved: The paper uses a fixed threshold without exploring how different threshold values affect the trade-off between noise removal and semantic preservation.
- What evidence would resolve it: Systematic experiments varying the threshold from 0.5 to 0.95, measuring the resulting noise reduction and semantic similarity, to identify optimal thresholds for different language pairs.

### Open Question 3
- Question: How does the performance of different LLM models (GPT-3.5 vs. GPT-4 vs. open-source alternatives) compare for this cleaning task?
- Basis in paper: [explicit] The paper uses GPT-3.5 and mentions that GPT-4 performs worse on Japanese, but doesn't compare multiple models or explore open-source alternatives.
- Why unresolved: The paper only uses one model variant (GPT-3.5) without benchmarking against other LLMs or exploring whether open-source models like Llama 2 could perform similarly.
- What evidence would resolve it: Comparative experiments using multiple LLM models (GPT-3.5, GPT-4, Llama 2, etc.) on the same cleaning tasks, measuring noise removal effectiveness and semantic preservation across languages.

## Limitations
- The approach relies heavily on GPT-3.5 API access, making it expensive to reproduce at scale and creating potential reproducibility barriers.
- Cleaning performance may not generalize equally across all language pairs, with the authors noting GPT-3.5 performs worse on certain languages like Japanese.
- The LASER similarity threshold of 0.7 lacks systematic validation across diverse language pairs and noise types.
- Crucial implementation details in Appendix A, B, and C are not fully specified in the main paper, creating uncertainty in faithful reproduction.

## Confidence

**High Confidence** claims:
- LLM-based cleaning significantly reduces various noise types (emojis, slang, profanities) compared to original MTNT data
- The resulting C-MTNT dataset shows improved semantic preservation through LASER similarity metrics
- Human and GPT-4 evaluations consistently validate cleaning quality

**Medium Confidence** claims:
- The cleaning process maintains semantic integrity across all language pairs
- Contrastive learning improves robustness to noisy input when trained on C-MTNT
- The augmented noise distribution adequately represents real-world patterns

**Low Confidence** claims:
- The specific threshold of 0.7 optimally balances semantic preservation and noise removal across all language pairs
- GPT-3.5 performance remains consistent across different noise types and language pairs
- The few-shot prompts will generalize to unseen noise patterns

## Next Checks
1. Apply the cleaning pipeline to a subset of MTNT data across all language pairs and measure LASER similarity distributions, BLEU score changes, and noise reduction consistency. Compare results against the authors' reported values to verify language-agnostic performance.

2. Systematically vary the LASER similarity threshold from 0.5 to 0.9 and measure the trade-off between noise reduction and semantic preservation across multiple language pairs. Identify the optimal threshold for each language pair rather than using a universal 0.7 cutoff.

3. Implement a comprehensive rule-based cleaning system using standard libraries (language_tool_python, emoji, profanity filters) and compare its performance against the LLM-based approach on identical MTNT subsets, measuring both quantitative metrics and qualitative output quality.