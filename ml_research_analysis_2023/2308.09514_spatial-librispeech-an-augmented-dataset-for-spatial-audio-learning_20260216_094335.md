---
ver: rpa2
title: 'Spatial LibriSpeech: An Augmented Dataset for Spatial Audio Learning'
arxiv_id: '2308.09514'
source_url: https://arxiv.org/abs/2308.09514
tags:
- spatial
- librispeech
- room
- audio
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Spatial LibriSpeech is a large-scale spatial audio dataset with
  over 650 hours of 19-channel audio, first-order ambisonics, and optional distractor
  noise, designed for machine learning model training. It includes labels for source
  position, speaking direction, room acoustics, and geometry.
---

# Spatial LibriSpeech: An Augmented Dataset for Spatial Audio Learning

## Quick Facts
- **arXiv ID**: 2308.09514
- **Source URL**: https://arxiv.org/abs/2308.09514
- **Reference count**: 0
- **Primary result**: Models trained on Spatial LibriSpeech achieve median absolute errors of 6.60° (3D localization), 0.43m (distance), 90.66ms (T30), and 2.74dB (DRR), generalizing well to real evaluation datasets.

## Executive Summary
Spatial LibriSpeech is a large-scale dataset for training machine learning models on spatial audio tasks. It contains over 650 hours of 19-channel audio and first-order ambisonics derived from LibriSpeech speech samples, augmented with over 200k simulated acoustic conditions across 8k+ synthetic rooms. The dataset includes comprehensive labels for source position, speaking direction, room acoustics, and geometry. Models trained on this dataset demonstrate strong performance on 3D source localization, distance estimation, direct-to-reverberant ratio (DRR) estimation, and reverberation time (T30) estimation, with successful transfer to widely-used evaluation datasets.

## Method Summary
The dataset is generated through a three-step pipeline: parametric room generation, room impulse response (RIR) simulations, and mixing with LibriSpeech audio samples. The RIRs are simulated for 8,952 synthetic rooms with varied sizes, materials, and source/receiver configurations, then convolved with speech samples to create realistic spatial audio. The resulting audio is provided in both 19-channel spherical microphone array format and first-order ambisonics, making it device-agnostic. Models are trained using a 3D convolutional neural network architecture with active and reactive input components, optimized with Adam (weight decay 0.01, dropout 50%, learning rate 1e-5) for 20-50 epochs depending on the task.

## Key Results
- 3D source localization achieves median absolute error of 6.60°
- Source distance estimation achieves median absolute error of 0.43m
- T30 estimation achieves median absolute error of 90.66ms
- DRR estimation achieves median absolute error of 2.74dB
- Models generalize well to TUT Sound Events 2018 and ACE Challenge evaluation datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Spatial LibriSpeech enables accurate transfer learning for spatial audio tasks because its diverse synthetic room conditions closely approximate real-world acoustic environments.
- Mechanism: The dataset is generated by simulating over 8,952 synthetic rooms with varied sizes, materials, and source/receiver configurations. These simulations are then convolved with LibriSpeech audio samples, creating realistic spatial audio cues (reverberation, directivity, etc.) that models can learn to interpret.
- Core assumption: The diversity and realism of synthetic RIRs (room impulse responses) are sufficient to capture the statistical distribution of real-world acoustic environments.
- Evidence anchors:
  - [abstract] "over 650 hours of 19-channel audio, first-order ambisonics, and optional distractor noise... includes labels for source position, speaking direction, room acoustics and geometry... generated by augmenting LibriSpeech samples with 200k+ simulated acoustic conditions across 8k+ synthetic rooms"
  - [section] "Our pipeline to generate Spatial LibriSpeech consists of three steps: parametric room generation, room impulse response simulations, and mixing... we verify that our models do not overfit to the virtual rooms in the training set by checking that the performance of models trained with Spatial LibriSpeech transfer to two existing evaluation datasets"
- Break condition: If the synthetic acoustic conditions fail to represent the statistical properties of real rooms (e.g., too idealized geometries or materials), models will not generalize to real-world evaluation datasets.

### Mechanism 2
- Claim: Using first-order ambisonics as input provides device-agnostic models that generalize across microphone array configurations.
- Mechanism: First-order ambisonics encode spatial audio in a format independent of the specific microphone array used for capture. By training models on ambisonic representations derived from a 19-channel spherical array, the learned features are invariant to the physical microphone layout.
- Core assumption: The spatial information captured in first-order ambisonics is sufficient for the target spatial audio tasks and is invariant to the source microphone array configuration.
- Evidence anchors:
  - [abstract] "we also provide synthesized, full-bandwidth first-order ambisonics... Either of these formats may be used to simulate a wide variety of arrays"
  - [section] "Our goal is for Spatial LibriSpeech to be the main training dataset for spatial audio applications... models consume first-order ambisonics"
- Break condition: If certain spatial audio tasks require higher-order ambisonics or direct microphone array signals for sufficient spatial resolution, first-order ambisonics may be insufficient.

### Mechanism 3
- Claim: The large scale and diversity of Spatial LibriSpeech enable robust learning of spatial audio features that transfer well to evaluation datasets with minimal fine-tuning.
- Mechanism: With over 650 hours of audio spanning 200k+ simulated acoustic conditions, models are exposed to a wide variety of source positions, room acoustics, and speaking directions. This broad training distribution allows models to learn generalizable features rather than overfitting to specific conditions.
- Core assumption: The size and diversity of the training dataset are sufficient to cover the distribution of real-world acoustic scenarios encountered in evaluation datasets.
- Evidence anchors:
  - [abstract] "models trained on Spatial LibriSpeech achieve a median absolute error of 6.60° on 3D source localization... We show that the same models generalize well to widely-used evaluation datasets"
  - [section] "Spatial LibriSpeech contains over 650 hours of spatial audio with labels for source position, speaking direction, room acoustics, and room geometry... The dataset consists of two splits: TRAIN which is derived from LibriSpeech 'train-clean-100' and 'train-clean-360' subset, and TEST which is derived from 'test-clean'"
- Break condition: If the evaluation datasets contain acoustic conditions significantly outside the distribution of Spatial LibriSpeech (e.g., very different room sizes or materials), transfer performance will degrade.

## Foundational Learning

- Concept: Room Impulse Response (RIR) simulation
  - Why needed here: RIRs are the core component used to simulate how speech samples would sound in different acoustic environments. Understanding RIR generation and properties is crucial for interpreting the dataset and its labels.
  - Quick check question: What are the three main components typically included in a simulated RIR for spatial audio applications?

- Concept: Ambisonics encoding and decoding
  - Why needed here: The dataset provides both 19-channel microphone array audio and first-order ambisonics. Understanding how to convert between these formats and the properties of ambisonics is essential for using the dataset effectively.
  - Quick check question: What is the maximum frequency up to which third-order ambisonics can be accurately extracted from a 19-channel spherical microphone array?

- Concept: Spatial audio feature extraction
  - Why needed here: The paper discusses several spatial audio tasks (3D source localization, distance estimation, DRR, T30). Understanding how these features are extracted from spatial audio signals is crucial for interpreting model outputs and designing new tasks.
  - Quick check question: How is the 3D angle for source localization calculated from predicted azimuth and elevation angles?

## Architecture Onboarding

- Component map: LibriSpeech audio -> RIR simulation -> convolution -> ambisonics encoding -> dataset storage
- Model architecture: 2x 4-layer 3D CNNs (active/reactive components) -> flatten/concatenate -> 3-layer MLP -> task-specific outputs
- Evaluation pipeline: Test data -> ambisonics encoding -> model inference -> metric calculation

- Critical path:
  1. Load 4-channel first-order ambisonics from dataset
  2. Split into active and reactive components
  3. Pass through respective 3D CNN branches
  4. Concatenate flattened outputs
  5. Pass through MLP layers
  6. Generate task-specific predictions (e.g., azimuth, elevation, distance, DRR, T30)

- Design tradeoffs:
  - Microphone array vs ambisonics: Using ambisonics provides device-agnostic models but may lose some spatial resolution compared to direct microphone signals
  - Dataset size vs computation: Full dataset provides best performance but requires more computational resources; 10% subset is faster to train with minimal performance loss
  - Model complexity: Simple 3D CNN + MLP architecture works well but may not achieve state-of-the-art performance compared to more complex models

- Failure signatures:
  - Poor generalization to real data: May indicate synthetic RIRs don't capture real acoustic properties well
  - Inconsistent predictions across similar inputs: Could indicate overfitting or insufficient model capacity
  - High variance in predictions: May suggest need for more training data or regularization

- First 3 experiments:
  1. Train a simple 3D CNN on a small subset of Spatial LibriSpeech for 3D source localization and evaluate on the test set to verify basic functionality
  2. Compare model performance on synthetic test set vs real evaluation datasets (TUT Sound Events 2018, ACE Challenge) to assess transfer capability
  3. Train models using different input formats (19-channel mic array vs first-order ambisonics) to evaluate the impact of device-agnostic representation on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of models trained on Spatial LibriSpeech compare to models trained on a combination of Spatial LibriSpeech and real-world acoustic data?
- Basis in paper: [inferred] The paper shows that models trained on Spatial LibriSpeech generalize well to real-world datasets like TUT Sound Events 2018 and ACE Challenge, but it does not compare performance when combining Spatial LibriSpeech with real-world data.
- Why unresolved: The paper focuses on the performance of models trained solely on Spatial LibriSpeech and their transferability to real-world datasets. It does not explore the potential benefits of combining Spatial LibriSpeech with real-world acoustic data.
- What evidence would resolve it: Training models on a combination of Spatial LibriSpeech and real-world acoustic data, and comparing their performance to models trained solely on Spatial LibriSpeech.

### Open Question 2
- Question: Can the performance of models trained on Spatial LibriSpeech be further improved by incorporating more diverse acoustic conditions or by using a more sophisticated model architecture?
- Basis in paper: [explicit] The paper mentions that the performance of models trained on Spatial LibriSpeech is close to state-of-the-art despite using a less sophisticated architecture. It also notes that the dataset could be used for other tasks, suggesting potential for improvement.
- Why unresolved: The paper does not explore the impact of incorporating more diverse acoustic conditions or using more sophisticated model architectures on the performance of models trained on Spatial LibriSpeech.
- What evidence would resolve it: Training models on an expanded version of Spatial LibriSpeech with more diverse acoustic conditions, or using more sophisticated model architectures, and comparing their performance to the current models.

### Open Question 3
- Question: How does the performance of models trained on Spatial LibriSpeech vary across different types of speech content (e.g., different languages, accents, or speaking styles)?
- Basis in paper: [inferred] The paper uses LibriSpeech, which contains English speech, but does not explore how the performance of models trained on Spatial LibriSpeech varies across different types of speech content.
- Why unresolved: The paper focuses on the overall performance of models trained on Spatial LibriSpeech, but does not investigate how the performance varies across different types of speech content.
- What evidence would resolve it: Training models on Spatial LibriSpeech augmented with speech content in different languages, accents, or speaking styles, and comparing their performance to models trained on the original Spatial LibriSpeech.

## Limitations
- Synthetic RIR realism is assumed but not empirically validated against real room measurements
- First-order ambisonics generalization across devices is claimed but not tested with multiple real microphone array configurations
- Dataset coverage of real-world acoustic diversity is asserted based on room count rather than statistical analysis

## Confidence
- **High Confidence**: The dataset construction methodology (RIR simulation + convolution with LibriSpeech) is clearly specified and reproducible
- **Medium Confidence**: Transfer learning results are promising but limited to two evaluation datasets; generalization to broader real-world scenarios remains to be tested
- **Low Confidence**: Claims about device-agnostic generalization via ambisonics are based on theoretical arguments rather than empirical validation across multiple array types

## Next Checks
1. **RIR Realism Validation**: Generate synthetic RIRs and real RIRs from the same physical rooms (if available) and perform statistical comparison of their temporal, spectral, and spatial characteristics
2. **Array Configuration Generalization**: Train models using ambisonics from one microphone array type, then test on recordings from different array configurations to verify device-agnostic performance
3. **Acoustic Distribution Coverage**: Analyze the parameter distributions (room sizes, RT60 values, absorption coefficients) in Spatial LibriSpeech and compare them to distributions found in real-world acoustic datasets or measurements