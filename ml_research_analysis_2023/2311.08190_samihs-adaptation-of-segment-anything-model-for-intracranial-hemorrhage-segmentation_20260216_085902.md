---
ver: rpa2
title: 'SAMIHS: Adaptation of Segment Anything Model for Intracranial Hemorrhage Segmentation'
arxiv_id: '2311.08190'
source_url: https://arxiv.org/abs/2311.08190
tags:
- samihs
- image
- segmentation
- medical
- hemorrhage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose SAMIHS, a parameter-efficient fine-tuning method
  that adapts the Segment Anything Model (SAM) for intracranial hemorrhage segmentation.
  The method incorporates parameter-refactoring adapters into SAM's image encoder
  and employs a boundary-sensitive loss function.
---

# SAMIHS: Adaptation of Segment Anything Model for Intracranial Hemorrhage Segmentation

## Quick Facts
- arXiv ID: 2311.08190
- Source URL: https://arxiv.org/abs/2311.08190
- Reference count: 0
- Primary result: SAMIHS achieves 69.77% Dice score and 3.31 HD95 on BCIHM dataset

## Executive Summary
This paper proposes SAMIHS, a parameter-efficient fine-tuning method that adapts the Segment Anything Model (SAM) for intracranial hemorrhage segmentation in CT scans. The method incorporates parameter-refactoring adapters into SAM's image encoder and employs a boundary-sensitive loss function. Experimental results on two public CT datasets show that SAMIHS outperforms several state-of-the-art methods in terms of Dice score and HD95, achieving 69.77% Dice and 3.31 HD95 on the BCIHM dataset.

## Method Summary
SAMIHS uses parameter-refactoring adapters inserted before MHA and MLP layers in SAM's image encoder, which are fine-tuned along with the prompt encoder and mask decoder. The image encoder is frozen during training. A combined loss function is used, incorporating binary cross-entropy with a boundary-sensitive term that emphasizes boundary regions through dynamic weighting. The model is trained using ADAM optimizer with a learning rate of 5.0×10⁻⁴ for 200 epochs with batch size 2 on non-contrast CT scans from two public datasets.

## Key Results
- SAMIHS achieves 69.77% Dice score and 3.31 HD95 on the BCIHM dataset
- Outperforms several state-of-the-art methods in intracranial hemorrhage segmentation
- Shows effective transfer learning capability from natural images to medical CT scans

## Why This Works (Mechanism)

### Mechanism 1
- Parameter-refactoring adapters improve performance by consolidating correlation between adapters and SAM's knowledge through symmetric linear projections.
- Shared down-projection and up-projection matrices across transformer blocks with layer-specific scaling and shifting factors enable efficient feature refactoring.
- Core assumption: Shared weight parameters maintain adapter efficiency while layer-specific factors provide task adaptation flexibility.
- Break condition: If shared parameters prevent necessary task-specific adaptation or layer-specific factors become too complex to train.

### Mechanism 2
- Boundary-sensitive loss function enhances segmentation of low-contrast hemorrhage regions by focusing on boundary detection.
- Combines binary cross-entropy with a boundary term that increases weight on boundary pixels through dynamic factor calculation based on boundary length and size.
- Core assumption: Hemorrhage segmentation benefits more from improved boundary detection than overall pixel-wise accuracy due to low contrast and blurry boundaries.
- Break condition: If boundary-focused loss causes over-segmentation of normal tissue boundaries or dynamic factor calculation becomes unstable.

### Mechanism 3
- Freezing SAM's image encoder while fine-tuning adapters enables efficient transfer learning while maintaining pre-trained knowledge.
- Maintains general visual knowledge in frozen encoder while allowing task-specific adaptation through fine-tuning of downstream components.
- Core assumption: SAM's image encoder contains transferable general visual knowledge for medical images.
- Break condition: If frozen encoder limits adaptation to domain-specific features or fine-tuned components cannot effectively utilize pre-trained features.

## Foundational Learning

- **Vision foundation models and transfer learning**
  - Why needed: Understanding how pre-trained models like SAM can be adapted to new domains without full fine-tuning
  - Quick check: What is the main advantage of using parameter-efficient fine-tuning methods over full fine-tuning?

- **Transformer architecture and attention mechanisms**
  - Why needed: PR adapters modify transformer blocks in SAM's image encoder, requiring understanding of how transformers process visual information
  - Quick check: How do windowed attention mechanisms in SAM differ from standard multi-head attention?

- **Loss function design for medical image segmentation**
  - Why needed: The boundary-sensitive loss combines multiple objectives to handle specific challenges of hemorrhage segmentation
  - Quick check: Why might boundary-focused loss functions be more effective than pixel-wise accuracy for medical segmentation?

## Architecture Onboarding

- **Component map**: Input → Image encoder (frozen) → PR adapters → Prompt encoder → Mask decoder → Output mask
- **Critical path**: Input → Image encoder (frozen) → PR adapters → Prompt encoder → Mask decoder → Output mask
- **Design tradeoffs**: Parameter efficiency vs. adaptation capability, boundary sensitivity vs. overall accuracy
- **Failure signatures**: Poor boundary detection (loss function issues), adapter overfitting (parameter sharing problems), or feature mismatch (frozen encoder limitations)
- **First 3 experiments**:
  1. Compare baseline SAM with frozen encoder vs. full fine-tuning on small dataset to verify parameter efficiency
  2. Test different boundary sensitivity weightings (λ1) to find optimal balance
  3. Evaluate PR adapter placement (MHA vs. MLP vs. both) to determine most effective configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do parameter-refactoring adapters perform compared to other adapter types (LoRA, visual prompt tuning, etc.) on medical image segmentation tasks?
- Basis in paper: The paper compares SAMIHS to other adapter-based methods like SAMed (LoRA) and SAMUS, showing superior performance.
- Why unresolved: While SAMIHS shows better results than these specific methods, a comprehensive comparison across all adapter types in medical imaging is lacking.
- What evidence would resolve it: A systematic study comparing various adapter types (LoRA, P-Tuning, VPT, PR adapters) on multiple medical image segmentation benchmarks with consistent experimental conditions.

### Open Question 2
- Question: How does the boundary-sensitive loss function perform compared to other boundary-aware loss functions in medical image segmentation?
- Basis in paper: The paper introduces a boundary-sensitive loss and shows its effectiveness through ablation studies.
- Why unresolved: While the proposed boundary-sensitive loss improves performance, direct comparisons with other established boundary-aware loss functions are missing.
- What evidence would resolve it: Head-to-head comparisons of the proposed boundary-sensitive loss against other boundary-aware loss functions on multiple medical segmentation tasks with identical model architectures.

### Open Question 3
- Question: What is the optimal prompt strategy for SAMIHS in medical image segmentation?
- Basis in paper: The paper mentions exploring single-point prompts but doesn't extensively investigate other prompt strategies.
- Why unresolved: The paper uses single-point prompts but doesn't explore the full potential of different prompt strategies in medical imaging contexts.
- What evidence would resolve it: Systematic evaluation of different prompt strategies across various medical imaging tasks and their impact on segmentation performance.

## Limitations
- Limited dataset size (BCIHM: 36 cases, INSTANCE: 100 cases) may not fully represent clinical ICH variability
- Implementation details for boundary-sensitive loss and PR adapters are not fully specified
- No comparison with specialized medical image segmentation models trained from scratch
- Results primarily evaluated on two datasets without external validation

## Confidence
- **High**: SAMIHS architecture design and general approach to parameter-efficient fine-tuning
- **Medium**: Claims about PR adapter effectiveness and boundary-sensitive loss contribution
- **Low**: Specific performance metrics and comparative advantages over other methods

## Next Checks
1. Implement and test the exact boundary-sensitive loss function formulation to verify its contribution to boundary detection
2. Conduct ablation studies on PR adapter placement (MHA vs. MLP) to determine optimal configuration
3. Test SAMIHS on an additional independent ICH dataset to validate generalizability across different clinical sites and acquisition protocols