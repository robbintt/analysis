---
ver: rpa2
title: Sampling binary sparse coding QUBO models using a spiking neuromorphic processor
arxiv_id: '2306.01940'
source_url: https://arxiv.org/abs/2306.01940
tags:
- sparse
- binary
- qubo
- loihi
- energy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates using a spiking neuromorphic processor to
  solve the binary sparse representation problem, formulated as a Quadratic Unconstrained
  Binary Optimization (QUBO) model. The goal is to find a sparse binary vector that,
  when combined with an overcomplete basis, best reconstructs a given image.
---

# Sampling binary sparse coding QUBO models using a spiking neuromorphic processor

## Quick Facts
- arXiv ID: 2306.01940
- Source URL: https://arxiv.org/abs/2306.01940
- Reference count: 15
- This work demonstrates Loihi 1 can sample sparse solutions of binary sparse coding QUBO models, though with lower accuracy than simulated annealing but considerably lower sparsity levels.

## Executive Summary
This work explores using a spiking neuromorphic processor (Loihi 1) to solve the binary sparse representation problem formulated as a Quadratic Unconstrained Binary Optimization (QUBO) model. The approach reformulates sparse coding as a QUBO problem where binary variables represent activation of basis vectors in an overcomplete dictionary. The spiking dynamics and stochastic nature of Loihi's neural network allow exploration of the non-convex energy landscape. While Loihi 1 can sample very sparse solutions, the accuracy is lower than simulated annealing methods. The work also presents an unsupervised dictionary learning algorithm that achieves desired sparsity without normalization.

## Method Summary
The method reformulates the sparse coding problem as minimizing E(x, a) = min_a [(1/2)||x - Da||^2 + λ||a||_0], where x is an image patch, D is an overcomplete dictionary, and a is a binary activation vector. This is transformed into QUBO form H(h, Q, a) = sum(h_i a_i) + sum(Q_ij a_i a_j) suitable for Loihi's spiking neurons. The dictionary is learned unsupervised by updating D based on reconstruction residuals and sparsity feedback, adjusting λ to achieve target sparsity levels. The QUBO models are then solved on Loihi using stochastic spiking dynamics with refractory periods, and solutions are compared against simulated annealing.

## Key Results
- Loihi 1 successfully samples very sparse solutions of binary sparse coding QUBO models
- Loihi solutions have considerably lower sparsity levels compared to simulated annealing
- Loihi solution accuracy is lower than simulated annealing despite achieving higher sparsity
- Unsupervised dictionary learning achieves desired sparsity without normalization requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The spiking dynamics in Loihi allow the network to bypass high-energy barriers through stochastic refractoriness.
- Mechanism: When a neuron spikes, it enters a refractory period during which it cannot fire again. This temporary inactivity allows other neurons, which were previously inhibited, to become active. The stochastic nature of the network combined with these enforced inactivity periods enables the system to escape local minima and explore non-local regions of the energy landscape.
- Core assumption: The timing and duration of refractory periods are sufficient to create meaningful transitions between distant energy states.
- Evidence anchors:
  - [abstract] "By programming the constraints into the architecture of a network of spiking neurons and controlling the frequency of network states during the resulting stochastic dynamics of the network, the exploration of complicated energy landscapes describing our problem of interest can be performed in practical time."
  - [section] "Compared to a Boltzmann machine, spiking networks allow for transitions between extreme energy differences... Because of the limited time of activity, or forced refractory period, defined by τ, active neurons are turned off for a determined time and others who were inhibited by the active neuron now have a chance to activate."
  - [corpus] Weak evidence - corpus papers discuss similar neuromorphic QUBO approaches but do not directly confirm the refractoriness-based barrier-bypassing mechanism.
- Break condition: If refractory periods are too short or the network is too deterministic, the system may fail to escape local minima.

### Mechanism 2
- Claim: The reformulation of sparse coding as a QUBO problem enables direct mapping to neuromorphic hardware constraints.
- Mechanism: The energy function E(x, a) = min_a [1/2 ||x - Da||^2 + λ||a||_0] is quadratic in the binary activation vector a. By expressing this as H(h, Q, a) = sum(h_i a_i) + sum(Q_ij a_i a_j), the problem becomes a QUBO that can be mapped onto the synaptic weights and biases of spiking neurons, where each binary variable corresponds to a neuron's firing state.
- Core assumption: The QUBO formulation accurately captures the original sparse coding objective and can be implemented within the hardware constraints of Loihi.
- Evidence anchors:
  - [abstract] "This yields a so-called Quadratic Unconstrained Binary Optimization (QUBO) problem, whose solution is generally NP-hard to find."
  - [section] "We start by reformulating eq. (1) in QUBO form... As expected, multiplying out eq. (1) yields a quadratic form in a, meaning that we can recast our objective function as a QUBO problem."
  - [corpus] Weak evidence - corpus papers mention QUBO on Loihi but do not detail the specific mapping strategy.
- Break condition: If the QUBO formulation introduces significant approximation errors or cannot be implemented due to hardware limitations, the solution quality will degrade.

### Mechanism 3
- Claim: The unsupervised dictionary learning algorithm adapts feature norms to achieve desired sparsity levels without normalization.
- Mechanism: The algorithm initializes dictionary features with random norms below 1 and iteratively updates them based on reconstruction error and sparsity feedback. If the average sparsity across training samples exceeds the desired level s, the penalty parameter λ is increased, encouraging sparser solutions and implicitly adjusting feature norms to optimize the trade-off.
- Core assumption: The feedback loop between sparsity measurement and penalty adjustment can converge to a dictionary that achieves the desired sparsity without explicit normalization.
- Evidence anchors:
  - [abstract] "First, the method of unsupervised and unnormalized dictionary feature learning for a desired sparsity level to best match the data is presented."
  - [section] "After solving the binary sparse coding problem for each sample in the training data, the dictionary is updated. If the average sparsity over the training epoch is above the desired level s, the penalty parameter λ is increased for the next epoch."
  - [corpus] Weak evidence - corpus papers discuss sparse coding but do not detail unnormalized dictionary learning methods.
- Break condition: If the sparsity feedback is too slow or the λ adjustment is too aggressive, the algorithm may fail to converge or produce suboptimal dictionaries.

## Foundational Learning

- Concept: QUBO (Quadratic Unconstrained Binary Optimization)
  - Why needed here: The sparse coding problem must be reformulated as a QUBO to be solvable on Loihi, which natively implements binary optimization through spiking dynamics.
  - Quick check question: How does the QUBO formulation of sparse coding differ from the original L0-regularized objective, and what approximations are introduced?

- Concept: Spiking neural network dynamics and refractory periods
  - Why needed here: The stochastic dynamics and refractory periods in Loihi are the key mechanisms that allow the network to explore the energy landscape and escape local minima.
  - Quick check question: How do refractory periods in spiking neurons contribute to the exploration of non-local regions in the energy landscape compared to traditional simulated annealing?

- Concept: Dictionary learning and sparsity constraints
  - Why needed here: The unsupervised dictionary learning algorithm is crucial for adapting the overcomplete basis to the specific data distribution and desired sparsity level, which directly impacts the quality of the sparse representations.
  - Quick check question: How does the unnormalized dictionary learning approach differ from traditional methods that require column normalization, and what are the implications for the resulting sparse codes?

## Architecture Onboarding

- Component map:
  Input layer (7x7 image patches) -> Dictionary layer (64 neurons) -> QUBO solver (Loihi spiking network) -> Output layer (binary activation vector) -> Feedback path (reconstruction error)

- Critical path:
  1. Image preprocessing and patch extraction
  2. Dictionary initialization and norm assignment
  3. QUBO formulation from current dictionary and image
  4. Loihi sampling of QUBO solutions
  5. Reconstruction and sparsity evaluation
  6. Dictionary update based on feedback

- Design tradeoffs:
  - Patch size vs. computational load: Smaller patches reduce QUBO size but may lose global context
  - Dictionary size vs. sparsity: Larger dictionaries increase representational power but may require higher λ for sparsity
  - Sampling time vs. solution quality: Longer sampling on Loihi may yield better solutions but increases latency
  - Normalization vs. flexibility: Unnormalized learning allows adaptive norms but may complicate convergence

- Failure signatures:
  - High reconstruction error with low sparsity: Dictionary not well-adapted to data
  - Low reconstruction error with high sparsity: λ too large, underfitting
  - High variance in QUBO energies: Insufficient sampling or unstable dynamics
  - Dictionary norms collapsing to zero: Learning rate too high or λ too large

- First 3 experiments:
  1. Validate QUBO formulation: Compare energy landscapes and solutions between Loihi and simulated annealing on small synthetic problems
  2. Test dictionary learning convergence: Monitor sparsity and reconstruction error during training on a subset of Fashion-MNIST
  3. Benchmark sparsity vs. quality: Systematically vary λ and measure trade-off between sparsity levels and reconstruction accuracy on held-out data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the power consumption of Loihi 1 compare to classical simulated annealing methods for solving binary sparse coding QUBO models?
- Basis in paper: [inferred] The paper mentions that future research could compare power usage of heuristic samplers like simulated annealing and spiking neuromorphic processors, and references previous demonstrations of lower power usage for certain applications on Loihi.
- Why unresolved: The authors did not conduct power consumption measurements for this study, only noting it as an interesting direction for future work.
- What evidence would resolve it: Direct power measurements of Loihi 1 running the binary sparse coding algorithm compared to the power consumption of simulated annealing running on a classical processor, for the same problem instances.

### Open Question 2
- Question: How would using an iterative Monte Carlo approach with Loihi, where the best solution from each iteration initializes the next, impact the quality of solutions found?
- Basis in paper: [explicit] The authors state this as future work: "We also believe using an iterative Monte Carlo approach with Loihi, where the best solution found at each iteration is used to initialize the system at the next iteration, similar to an iterative warm start algorithm in classical optimization, could improve the total space explored and thus the likelihood of finding a global minimum."
- Why unresolved: This approach was not implemented or tested in the current study.
- What evidence would resolve it: Implementing the iterative Monte Carlo approach on Loihi and comparing the solution quality (e.g., energy levels, sparsity) to the current non-iterative approach.

### Open Question 3
- Question: How would the results differ if using Loihi 2, the second generation of Intel's spiking processor, instead of Loihi 1?
- Basis in paper: [explicit] The authors mention this as future work: "Future work will include comparing the results on Loihi 2, the second generation of Intel's spiking processor, and other emerging non-von Neumann computers, such as quantum annealers."
- Why unresolved: The study only used Loihi 1, not the newer Loihi 2.
- What evidence would resolve it: Running the same binary sparse coding experiments on Loihi 2 and comparing the results (solution quality, energy levels, sparsity) to those obtained on Loihi 1.

## Limitations

- The refractoriness-based barrier-bypassing mechanism remains theoretical without direct empirical validation.
- The QUBO formulation accuracy and hardware mapping efficiency are weakly supported by corpus evidence.
- The unnormalized dictionary learning approach lacks comparative validation against standard normalized methods.

## Confidence

- **High confidence**: The QUBO reformulation of sparse coding is mathematically sound and directly implementable on Loihi.
- **Medium confidence**: The dictionary learning algorithm with sparsity feedback can achieve desired sparsity levels, but performance relative to normalized approaches is unclear.
- **Low confidence**: The stochastic dynamics and refractory periods in Loihi sufficiently explore the energy landscape compared to simulated annealing.

## Next Checks

1. **Energy landscape comparison**: Generate small QUBO instances and compare energy distributions and solution quality between Loihi sampling and simulated annealing across multiple random seeds.
2. **Dictionary learning ablation**: Train dictionaries using both normalized and unnormalized approaches on Fashion-MNIST, measuring convergence speed, final sparsity levels, and reconstruction accuracy.
3. **Refractory period sensitivity**: Systematically vary refractory period durations and network noise levels in Loihi, measuring the impact on solution diversity, energy quality, and escape from local minima.