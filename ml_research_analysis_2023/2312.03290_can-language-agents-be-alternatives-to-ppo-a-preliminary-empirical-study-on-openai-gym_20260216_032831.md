---
ver: rpa2
title: Can language agents be alternatives to PPO? A Preliminary Empirical Study On
  OpenAI Gym
arxiv_id: '2312.03290'
source_url: https://arxiv.org/abs/2312.03290
tags:
- language
- agents
- action
- player
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores whether language agents can serve as alternatives
  to PPO agents in sequential decision-making tasks. To enable fair comparisons, the
  authors introduce a TextGym simulator that grounds OpenAI Gym environments into
  textual form, a five-level hierarchical framework for controlling domain knowledge,
  and an RL-inspired unified architecture for language agents.
---

# Can language agents be alternatives to PPO? A Preliminary Empirical Study On OpenAI Gym

## Quick Facts
- **arXiv ID**: 2312.03290
- **Source URL**: https://arxiv.org/abs/2312.03290
- **Reference count**: 40
- **Primary result**: Language agents achieve solvability thresholds in 5/8 OpenAI Gym environments with ≤5 episodes, outperforming PPO's 20,000 episodes requirement

## Executive Summary
This paper investigates whether language agents can serve as efficient alternatives to PPO in sequential decision-making tasks. The authors introduce TextGym, a simulator that converts OpenAI Gym environments into textual form, enabling direct comparison between PPO and language agents. They develop a unified actor-critic-learner framework and propose an Explore-Exploit-guided (EXE) language agent. Experimental results demonstrate that language agents can achieve solvability thresholds in five out of eight tested environments, showing superior sample efficiency compared to PPO.

## Method Summary
The authors created TextGym to ground OpenAI Gym environments in textual form, allowing language models to interact with environments through natural language. They introduced a five-level hierarchical framework for controlling domain knowledge, ranging from zero guidance to expert strategies. Language agents were decomposed into actor-critic-learner components following an RL-inspired architecture. The EXE agent leverages exploration and exploitation principles. Performance was evaluated across eight environments (Acrobot-v1, CartPole-v0, CliffWalking-v0, MountainCar-v0, MountainCarContinuous-v0, Blackjack-v1, Taxi-v3, LunarLander-v2) at different domain knowledge levels.

## Key Results
- Language agents achieved solvability thresholds in 5/8 tested environments (CartPole, Acrobot, CliffWalking, MountainCar, Blackjack)
- Sample efficiency: Language agents required ≤5 episodes vs 20,000 episodes for PPO to reach comparable performance
- EXE agent showed superior performance in several scenarios, particularly in environments with simple dynamics
- Performance degraded in environments with complex dynamics or high partial observability (LunarLander, Taxi, Acrobot at higher levels)

## Why This Works (Mechanism)

### Mechanism 1
Grounding OpenAI Gym environments into textual form (TextGym) enables direct comparison between PPO and language agents by eliminating the visual observation barrier. The translation wrapper converts observations, actions, and rewards into natural language descriptions that language models can process directly, while PPO operates on the original Gym environment. This creates a level playing field where both types of agents interact with the same underlying dynamics but through different interfaces.

### Mechanism 2
Hierarchical domain knowledge control (5 levels) enables fair comparison by standardizing the amount and type of information provided to different language agents. By systematically varying the domain knowledge from zero guidance to expert strategies, the framework controls for the confounding effect of different agents receiving different amounts of task-specific information, allowing isolation of architectural differences.

### Mechanism 3
The actor-critic-learner framework provides a unified architecture that allows meaningful comparison and ablation studies across different language agents. By decomposing language agents into these three components with standardized interfaces, the framework enables systematic variation of individual components while keeping others constant, revealing which architectural choices drive performance differences.

## Foundational Learning

- **Reinforcement Learning fundamentals** (states, actions, rewards, policies, value functions) - Why needed: Understanding how PPO operates in OpenAI Gym environments provides the baseline for comparing language agents. Quick check: What is the difference between a policy and a value function in RL?
- **Large Language Model prompting techniques** (zero-shot, few-shot, chain-of-thought) - Why needed: Language agents rely on different prompting strategies that must be controlled and compared. Quick check: How does chain-of-thought prompting differ from standard prompting in terms of expected LLM behavior?
- **Partially Observable Markov Decision Processes (POMDPs)** - Why needed: The paper explicitly frames the environments as POMDPs, indicating agents must handle partial observability. Quick check: In a POMDP, what information does the agent have access to at each time step?

## Architecture Onboarding

- **Component map**: Actor (profile, memory, action instruction) → Environment → Critic (evaluation) → Learner (update) → Actor
- **Critical path**: Actor generates action → Environment returns observation and reward → Critic evaluates → Learner updates actor parameters/knowledge
- **Design tradeoffs**: Simple actor (single-path) vs complex actor (multi-path) - simplicity vs decision quality; memory vs stateless - historical context vs computational efficiency
- **Failure signatures**: Poor exploration (agent gets stuck in suboptimal policies), inadequate memory (forgets important state information), poor evaluation (inaccurate feedback prevents learning)
- **First 3 experiments**:
  1. Implement TextGym wrapper for CartPole-v0 and verify PPO performance matches baseline
  2. Implement EXE agent with actor-critic-learner components and test on CartPole-v0 at Lv1
  3. Compare EXE performance at different knowledge levels (Lv1 vs Lv3) to validate hierarchical control mechanism

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of language agents scale with increased model size and capability? The paper uses GPT-3.5 and notes that GPT-4 shows significant performance improvements in some environments, suggesting potential for scaling, but only tests two model sizes and doesn't systematically explore scaling relationships.

### Open Question 2
What is the optimal balance between exploration and exploitation for language agents in different types of environments? The EXE agent is explicitly designed to balance exploration and exploitation and shows superior performance in some scenarios, but the paper doesn't provide a systematic analysis of optimal balances across environment types.

### Open Question 3
How do language agents perform on environments with partial observability beyond what was tested? The paper notes that some environments are too challenging due to partial observability but doesn't systematically explore whether this is a fundamental limitation or if it can be overcome with better agent design.

## Limitations

- **Grounding quality uncertainty**: The TextGym simulator's ability to faithfully preserve all critical state information during the OpenAI Gym to text conversion is unverified.
- **Sample efficiency comparison baseline**: The claimed 5 vs 20,000 episode comparison between language agents and PPO may not account for PPO's learning curve progression and final policy quality convergence.
- **Domain knowledge level validation**: The 5-level hierarchical control framework lacks empirical validation that levels provide consistent information density across different environments and agent types.

## Confidence

- **High confidence**: Language agents can solve simple Gym environments (CartPole, Acrobot) at basic domain knowledge levels, and the actor-critic-learner decomposition provides a useful analytical framework.
- **Medium confidence**: The TextGym wrapper enables fair comparison between PPO and language agents, and the explore-exploit principle genuinely improves language agent performance.
- **Low confidence**: The specific sample efficiency advantage (5 vs 20,000 episodes) and the claim that language agents are viable alternatives to PPO across diverse sequential decision-making tasks.

## Next Checks

1. **Grounding fidelity test**: Implement a bidirectional validation where PPO policies are tested on both original Gym and TextGym versions to quantify information loss during text conversion.
2. **Sample efficiency benchmarking**: Run controlled experiments tracking PPO performance progression across episodes 1-100 to establish the actual learning curve shape and compare it to language agent performance trajectories.
3. **Cross-environment generalization**: Test language agents on environments with varying complexity levels (e.g., CartPole → LunarLander → Montezuma's Revenge) to identify the complexity threshold where language agents fail relative to PPO.