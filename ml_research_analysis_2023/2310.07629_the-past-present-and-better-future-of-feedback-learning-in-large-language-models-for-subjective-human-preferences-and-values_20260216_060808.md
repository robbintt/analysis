---
ver: rpa2
title: The Past, Present and Better Future of Feedback Learning in Large Language
  Models for Subjective Human Preferences and Values
arxiv_id: '2310.07629'
source_url: https://arxiv.org/abs/2310.07629
tags:
- human
- feedback
- language
- association
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys 95 papers that use human feedback to steer,
  guide or tailor the behaviors of language models, focusing on subjective human preferences
  and values. The survey examines the past, present and future of feedback learning
  in LLMs.
---

# The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values

## Quick Facts
- arXiv ID: 2310.07629
- Source URL: https://arxiv.org/abs/2310.07629
- Reference count: 40
- Primary result: Identifies five core challenges in human feedback learning for LLMs: incomplete feedback, lack of universality, inconsistent definitions, operationalization difficulties, and diversity issues.

## Executive Summary
This survey paper examines 95 papers that use human feedback to steer, guide, or tailor the behaviors of language models, focusing on subjective human preferences and values. The paper identifies five unresolved challenges in human feedback learning: incomplete feedback, lack of universality, inconsistent definitions, difficulty in operationalizing a "good" output, and the need for more representative and diverse feedback providers. The authors call for more open, democratically-grounded, and interdisciplinary collaboration to decide how different voices shape current and future LLMs.

## Method Summary
The authors conducted a semi-automated keyword-based paper retrieval from ACL and arXiv, followed by manual inclusion criteria and snowball sampling. Two authors coded the papers using a detailed schema across conceptual and methodological themes. The analysis identified patterns, challenges, and recommendations for the field of human feedback learning in LLMs.

## Key Results
- Five unresolved challenges identified: incomplete feedback, lack of universality, inconsistent definitions, operationalization difficulties, and diversity issues
- Current feedback providers are predominantly US-based, English-speaking crowdworkers with Master's degrees aged 25-34
- Clear operational definitions of "good" outputs are necessary despite inherent subjectivity
- Diverse and representative feedback is critical for mitigating biases in LLM alignment
- Interdisciplinary collaboration is needed to address these challenges effectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human feedback learning enables language models to align with subjective human preferences and values by iteratively refining model outputs based on explicit or implicit human judgments.
- Mechanism: The process involves collecting human feedback on model outputs, training a preference reward model to score outputs, and using reinforcement learning or supervised fine-tuning to optimize the model toward higher scores.
- Core assumption: Human feedback, even when subjective, contains sufficient signal to guide model behavior toward desired outcomes.
- Evidence anchors:
  - [abstract] "Human feedback is increasingly used to steer the behaviours of Large Language Models (LLMs)."
  - [section] "Incorporating human feedback into Large Language Models (LLMs) is a welcome development to create models that are better aligned with human preferences or values..."
- Break condition: If human feedback is too noisy, inconsistent, or unrepresentative, the learned reward signal may not accurately reflect desired model behavior.

### Mechanism 2
- Claim: Diverse and representative human feedback is critical for mitigating biases and ensuring equitable alignment of language models with human values.
- Mechanism: Collecting feedback from a wide range of demographic groups and cultural backgrounds helps capture a more comprehensive and inclusive set of preferences and values.
- Core assumption: The preferences and values of a small, non-representative group (e.g., predominantly US-based, English-speaking crowdworkers) do not generalize to the broader population.
- Evidence anchors:
  - [abstract] "It is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values."
  - [section] "The limitations of relying on the subjective interpretations of a small and non-representative work force are exacerbated by inadequate documentation."
- Break condition: If feedback providers are not diverse or representative, the model may encode biases and fail to align with the values of underrepresented groups.

### Mechanism 3
- Claim: Clear and consistent operational definitions of "good", "high-quality", or "preferred" outputs are necessary for effective human feedback learning, despite the inherent subjectivity of these concepts.
- Mechanism: Providing detailed guidelines and training for human raters helps reduce ambiguity and improve the reliability of feedback, even for subjective tasks.
- Core assumption: While perfect agreement may be unattainable, structured guidelines can significantly reduce variance in human judgments and improve the quality of feedback data.
- Evidence anchors:
  - [abstract] "what counts as a 'good', 'high-quality', 'preferred' or 'value-aligned' output is only objective in the abstract"
  - [section] "To reduce disagreements, some authors write very prescriptive and/or comprehensive guidelines for the task in order to 'make comparisons as unambiguous as possible'"
- Break condition: If guidelines are too prescriptive or fail to account for cultural and contextual nuances, they may stifle the expression of diverse perspectives and lead to a narrow definition of "good" outputs.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF is a key technique for incorporating human feedback into language models, allowing them to learn from explicit judgments about the quality or desirability of their outputs.
  - Quick check question: What are the main steps involved in an RLHF pipeline for language models?

- Concept: Reward Modeling
  - Why needed here: Reward models are used to learn a scalar score or ranking function from human feedback data, which can then be used to guide the optimization of language models via reinforcement learning or other techniques.
  - Quick check question: How does a reward model learn to score or rank model outputs based on human feedback?

- Concept: Preference Learning
  - Why needed here: Preference learning is a broader framework for learning from relative judgments (e.g., "output A is preferred to output B") rather than absolute scores, which can be more robust to scale and calibration issues in human feedback.
  - Quick check question: What are the advantages of learning from pairwise preferences compared to absolute ratings in the context of human feedback learning?

## Architecture Onboarding

- Component map: Data Collection -> Reward Modeling -> Model Optimization -> Evaluation
- Critical path: Data Collection → Reward Modeling → Model Optimization → Evaluation
- Design tradeoffs:
  - Feedback Quality vs. Quantity: Collecting high-quality, detailed feedback from a small number of experts vs. gathering larger amounts of noisier feedback from a broader population
  - Model Performance vs. Alignment: Balancing the trade-off between maintaining the base model's capabilities and achieving better alignment with human preferences
  - Generalizability vs. Specificity: Deciding whether to optimize for a broad set of universally shared values or more specific, contextual preferences
- Failure signatures:
  - Reward Hacking: The model learns to exploit the reward signal without actually improving alignment with human preferences
  - Overfitting: The model becomes too specialized to the specific feedback data and fails to generalize to new inputs or contexts
  - Distributional Shift: The model's outputs drift too far from the base model's distribution, leading to a loss of fluency or coherence
- First 3 experiments:
  1. Collect a small dataset of human feedback on model outputs using explicit ratings or pairwise comparisons
  2. Train a simple reward model (e.g., a linear regression or a small MLP) to predict human preferences based on the feedback data
  3. Use the learned reward model to fine-tune a pre-trained language model via reinforcement learning or supervised learning, and evaluate the results using both human judgments and automated metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we ensure that human feedback collection for LLM alignment is truly representative of diverse human values and preferences across different cultures and contexts?
- Basis in paper: [explicit] The paper discusses the challenge of crowdworkers and social media users being neither representative nor sufficient, stating "overwhelmingly, these humans are US-based, English-speaking crowdworkers with Master's degrees and between the ages of 25-34. This results in a non-democratic and non-diverse feedback process."
- Why unresolved: The paper identifies this as a key challenge but does not provide a concrete solution for how to ensure truly representative feedback collection across diverse human values and contexts.
- What evidence would resolve it: Evidence of successful implementation of diverse, representative feedback collection methods for LLM alignment, with clear documentation of the diversity of feedback providers and how their input shaped the model's behavior across different cultural contexts.

### Open Question 2
- Question: To what extent can LLMs generalize from partial human feedback, especially when presented with data that is completely out-of-domain for their training or at the margins of its distribution?
- Basis in paper: [explicit] The paper discusses the challenge of human feedback being inherently incomplete, stating "an open question is the extent to which models generalise from partial human feedback, especially when presented with data that is completely out-of-domain for their training or at the margins of its distribution."
- Why unresolved: The paper identifies this as a key challenge but does not provide a clear answer on the extent to which LLMs can generalize from limited, potentially biased feedback data.
- What evidence would resolve it: Empirical studies demonstrating the generalization capabilities of LLMs trained on human feedback across diverse, out-of-distribution scenarios, with clear metrics for measuring the model's ability to handle novel situations.

### Open Question 3
- Question: How can we operationalize abstract human values and preferences into concrete, measurable signals that can be used to guide LLM behavior without introducing significant bias or losing the nuance of human judgment?
- Basis in paper: [explicit] The paper discusses the challenge of operationalizing a "good" output, stating "even if a shared set of values could be agreed upon, converting these thick normative concepts into signals that models can use, such as by collecting annotator ratings, is hard."
- Why unresolved: The paper identifies this as a key challenge but does not provide a clear methodology for translating abstract human values into concrete, unbiased signals for LLM training.
- What evidence would resolve it: Successful implementation of methods for operationalizing human values into measurable signals, with clear documentation of how these signals were derived from abstract concepts and how they guided LLM behavior in practice.

## Limitations
- The analysis relies on existing literature rather than direct empirical testing of proposed solutions
- May have selection bias based on keyword choices and inclusion criteria
- Provides limited guidance on operationalizing "universally shared" versus "culturally specific" values
- Does not address potential conflicts between different stakeholder groups' preferences

## Confidence
- High confidence in identifying the five core challenges
- Medium confidence in characterization of current state-of-the-art approaches
- Low confidence in proposed future directions due to limited empirical validation

## Next Checks
1. Conduct a systematic review to validate whether the five identified challenges represent the full scope of issues in human feedback learning, using a different keyword set and inclusion criteria.
2. Implement a controlled experiment comparing model alignment outcomes when using diverse versus homogeneous feedback providers on the same task.
3. Develop and test a standardized framework for operationalizing subjective quality judgments across different cultural contexts, measuring inter-rater reliability and model performance.