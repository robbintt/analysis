---
ver: rpa2
title: Activity Sparsity Complements Weight Sparsity for Efficient RNN Inference
arxiv_id: '2311.07625'
source_url: https://arxiv.org/abs/2311.07625
tags:
- sparsity
- weight
- learning
- neural
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that activity sparsity in recurrent neural
  networks can be tuned using weight decay, and that activity sparsity and weight
  sparsity combine multiplicatively to yield significant reductions in computational
  cost. The authors show that up to 20x reduction in multiply-accumulate operations
  is achievable on the Penn Treebank language modeling task while maintaining perplexity
  below 60, surpassing previous results on sparsely connected LSTMs and sparsely activated
  recurrent networks.
---

# Activity Sparsity Complements Weight Sparsity for Efficient RNN Inference

## Quick Facts
- arXiv ID: 2311.07625
- Source URL: https://arxiv.org/abs/2311.07625
- Reference count: 40
- The paper demonstrates up to 20x reduction in multiply-accumulate operations while maintaining perplexity below 60 on Penn Treebank language modeling.

## Executive Summary
This paper explores how activity sparsity in recurrent neural networks can be tuned using weight decay, and demonstrates that activity sparsity and weight sparsity combine multiplicatively to yield significant reductions in computational cost. The authors show that using an Event-based Gated Recurrent Unit (EGRU) architecture with weight pruning and activity sparsity tuning, they can achieve up to 20x reduction in MAC operations on the Penn Treebank language modeling task while maintaining perplexity below 60. This surpasses previous results on sparsely connected LSTMs and sparsely activated recurrent networks, providing strong evidence that making deep learning models activity sparse and porting them to neuromorphic devices can be a viable strategy that doesn't compromise task performance.

## Method Summary
The method involves training an Event-based GRU (EGRU) model on language modeling tasks (Penn Treebank and WikiText-2), applying unstructured weight magnitude pruning to achieve weight sparsity, and tuning activity sparsity through weight decay regularization. The model uses a modified GRU architecture with event-based gating that creates sparse activations. After pruning weights to target sparsity levels (20%, 40%, 60%), the model is fine-tuned with varying weight decay parameters to control the activity sparsity level. The multiplicative effect of activity and weight sparsity is measured through MAC operation reduction while monitoring perplexity to ensure task performance is maintained.

## Key Results
- Achieved up to 20x reduction in multiply-accumulate operations on Penn Treebank
- Maintained perplexity below 60 (the target threshold) despite high sparsity levels
- Demonstrated multiplicative composition of activity and weight sparsity: λ_a · λ_w reduction in operations
- Showed that weight decay is an effective knob for controlling activity sparsity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Activity sparsity and weight sparsity combine multiplicatively to reduce computational cost
- Mechanism: For a linear transformation Wa, the number of remaining operations compared to dense is λ_a · λ_w, where λ_a is the fraction of active neurons and λ_w is the fraction of non-zero weights
- Core assumption: The sparsity patterns of activations and weights are independent
- Evidence anchors:
  - [abstract] "activity sparsity can compose multiplicatively with parameter sparsity"
  - [section 3.3] "The efﬁciency gains of sparse activations and sparse weights complement each other in a multiplicative way"
  - [corpus] Weak - no direct evidence in corpus neighbors
- Break condition: If activation sparsity and weight sparsity become correlated, the multiplicative assumption fails

### Mechanism 2
- Claim: Weight decay controls activity sparsity by pushing weight distributions toward zero
- Mechanism: Weight decay causes weight distributions to concentrate around zero, which when multiplied by activation distributions, increases the probability of preactivations passing the threshold and becoming active
- Core assumption: Statistical independence between weights W and activations a
- Evidence anchors:
  - [section 4.2] "weight decay strongly influences both the task performance as well as the activity sparsity"
  - [section 4.2] "Weight decay applied to the weights also has a strong influence on the activity of our model"
  - [corpus] Weak - no direct evidence in corpus neighbors
- Break condition: If the independence assumption between W and a breaks down, the mechanism fails

### Mechanism 3
- Claim: EGRU's architecture enables both activity and weight sparsity better than traditional RNNs
- Mechanism: EGRU uses event-based gating with thresholded outputs that create sparse activations, while maintaining LSTM-level performance through gating mechanisms
- Core assumption: The EGRU architecture can maintain competitive performance while being sparse
- Evidence anchors:
  - [abstract] "demonstrate that activity sparsity can compose multiplicatively with parameter sparsity in a recurrent neural network model based on the GRU that is designed to be activity sparse"
  - [section 3.1] "Event-based GRU (EGRU) [40] combines gating mechanisms with spiking mechanisms inspired by biological neuron models"
  - [corpus] Weak - no direct evidence in corpus neighbors
- Break condition: If the EGRU architecture cannot maintain performance while being sparse, the advantage disappears

## Foundational Learning

- Concept: Language modeling with RNNs
  - Why needed here: The paper evaluates on Penn Treebank and WikiText-2 language modeling tasks
  - Quick check question: What is perplexity and why is it the evaluation metric for language modeling?

- Concept: Weight pruning techniques
  - Why needed here: The paper uses weight magnitude pruning to achieve weight sparsity
  - Quick check question: What is the difference between structured and unstructured pruning?

- Concept: Spiking neural networks and neuromorphic computing
  - Why needed here: The paper connects activity sparsity to neuromorphic hardware advantages
  - Quick check question: How do neuromorphic systems handle dynamic sparsity differently from traditional hardware?

## Architecture Onboarding

- Component map: Word embeddings -> EGRU layers -> Weight pruning -> Activity sparsity tuning -> Output layer -> Perplexity calculation

- Critical path: Train dense EGRU -> Apply weight pruning at target sparsity -> Fine-tune with weight decay -> Measure MAC reduction and perplexity

- Design tradeoffs:
  - Higher weight sparsity vs. performance loss
  - Higher activity sparsity vs. performance loss
  - Weight decay magnitude vs. sparsity level
  - Word embedding size vs. MAC operation count

- Failure signatures:
  - Perplexity increases beyond target threshold (60 for Penn Treebank)
  - MAC reduction factor plateaus despite higher sparsity
  - Weight distributions become too concentrated around zero

- First 3 experiments:
  1. Train baseline dense EGRU and measure perplexity/MAC count
  2. Apply weight pruning at different sparsity levels (20%, 40%, 60%) and measure impact
  3. Vary weight decay to control activity sparsity and observe multiplicative effects with weight sparsity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the combination of activity sparsity and weight sparsity scale to larger language models beyond Penn Treebank and WikiText-2?
- Basis in paper: [explicit] The authors note that their results are on small-scale datasets and suggest future work on larger-scale language models.
- Why unresolved: The paper only reports results on small datasets. Scaling to larger models involves different architectural choices and training dynamics that may affect the interplay between activity and weight sparsity.
- What evidence would resolve it: Empirical results showing perplexity and MAC reduction factors for models trained on larger datasets (e.g., WikiText-103, LAMBADA) with varying sparsity levels.

### Open Question 2
- Question: What is the optimal trade-off between weight decay regularization and activity sparsity for maximizing efficiency without compromising performance?
- Basis in paper: [explicit] The authors observe that weight decay strongly influences both task performance and activity sparsity, but the relationship is complex and non-linear.
- Why unresolved: While the paper explores weight decay's effects, it doesn't provide a systematic framework for determining the optimal balance between sparsity and performance across different tasks.
- What evidence would resolve it: A comprehensive study varying weight decay parameters across multiple tasks and model architectures, identifying general principles for optimal sparsity-performance trade-offs.

### Open Question 3
- Question: How does the high activity in the output layer of EGRU networks impact overall efficiency gains, and what decoding strategies could mitigate this?
- Basis in paper: [explicit] The authors observe anomalously high activity in the output layer and speculate it's due to the language modeling objective.
- Why unresolved: The paper doesn't explore alternative decoding strategies or quantify how much this output layer activity limits overall efficiency gains.
- What evidence would resolve it: Comparative experiments testing different decoding approaches (e.g., decoder layers, alternative output representations) and their impact on output layer sparsity and overall MAC reduction.

### Open Question 4
- Question: What architectural modifications to EGRU could reduce the need for larger word embeddings while maintaining or improving performance?
- Basis in paper: [inferred] The authors note that EGRU requires larger word embeddings than LSTM, which limits overall MAC reduction efficiency.
- Why unresolved: The paper doesn't investigate why this embedding size difference exists or propose architectural changes to address it.
- What evidence would resolve it: Architectural modifications to EGRU (e.g., different embedding initialization, sparsity-aware embedding layers) evaluated for their impact on embedding size requirements and overall efficiency.

## Limitations
- The multiplicative sparsity assumption relies on independence between activation and weight sparsity patterns, which may not hold in practice
- Results are demonstrated only on small-scale language modeling tasks (Penn Treebank, WikiText-2) and may not generalize to larger models
- The EGRU architecture requires larger word embeddings than LSTM, limiting overall MAC reduction efficiency

## Confidence

- High confidence: The multiplicative relationship between activity and weight sparsity is mathematically sound under independence assumptions
- Medium confidence: The weight decay mechanism for controlling activity sparsity is plausible but requires empirical validation
- Low confidence: The generalizability of 20x MAC reduction across different tasks and architectures

## Next Checks

1. Test the multiplicative sparsity model on architectures beyond EGRU (LSTM, Transformer) to verify generalizability
2. Measure the correlation between activation sparsity and weight sparsity patterns to validate independence assumption
3. Implement ablation studies with different threshold functions and gating mechanisms to identify which EGRU components are essential for sparsity