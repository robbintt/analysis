---
ver: rpa2
title: Can ChatGPT be Your Personal Medical Assistant?
arxiv_id: '2312.12006'
source_url: https://arxiv.org/abs/2312.12006
tags:
- medical
- fine-tuned
- score
- arabic
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the potential of fine-tuning ChatGPT as a
  personalized medical assistant in Arabic. It fine-tunes the GPT-3.5-turbo model
  on a large Arabic healthcare Q&A dataset (430K questions across 20 specialties).
---

# Can ChatGPT be Your Personal Medical Assistant?

## Quick Facts
- arXiv ID: 2312.12006
- Source URL: https://arxiv.org/abs/2312.12006
- Reference count: 0
- Fine-tuned GPT-3.5-turbo on Arabic medical Q&A data shows promise as personalized medical assistant

## Executive Summary
This study explores fine-tuning ChatGPT (GPT-3.5-turbo) as a personalized medical assistant in Arabic by training on the MAQA dataset containing over 430K Arabic healthcare Q&A across 20 specialties. The research employs a combination of automated metrics and human evaluation by native Arabic-speaking medical professionals to assess the model's performance. Results indicate the fine-tuned model can generate accurate, relevant, and logical medical information in Arabic, though there is room for improvement in relevance and precision. The study demonstrates the potential of LLMs for medical applications in non-English languages while highlighting the importance of domain-specific fine-tuning.

## Method Summary
The research fine-tunes GPT-3.5-turbo on a subset of the MAQA dataset (4000 Q&A for training, 1000 for validation from gynecological category) using a JSONL format with system, user, and assistant roles. The model is trained for 3 epochs, with evaluation combining automated metrics (perplexity, cosine similarity, coherence score) and human assessment by native Arabic medical professionals across five dimensions: relevance, accuracy, precision, logic, and originality. The approach uses prompt engineering to guide model behavior and employs both quantitative and qualitative measures to comprehensively evaluate performance.

## Key Results
- Automated evaluation shows moderate perplexity (13.96) and similarity (0.10), with training loss of 0.3355
- Human evaluation by medical professionals yields high accuracy (mode=4, mean=4.1) and strong logic (mean=3.98) and originality scores (mean=3.94)
- Relevance and precision scores are lower (means=3.0 and 3.22), suggesting room for improvement
- Fine-tuned model generates responses specific to diseases and similar to original answers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning on domain-specific Arabic medical Q&A data improves the model's ability to generate contextually relevant and accurate medical responses in Arabic.
- **Mechanism:** By training the GPT-3.5-turbo model on the MAQA dataset containing over 430K Arabic medical questions and answers, the model learns the specific terminology, phrasing, and logical structure of medical discourse in Arabic. This allows it to generate responses that are closer in style and content to authentic medical answers.
- **Core assumption:** The training data is sufficiently diverse and representative of real-world medical Q&A interactions in Arabic.
- **Evidence anchors:** The study fine-tunes GPT-3.5-turbo on the MAQA dataset, which is described as "the largest collection for medical Q&A in the Arabic language so far." Human evaluation by native Arabic-speaking medical professionals yielded high accuracy scores (mean=4.1), indicating the model can generate accurate medical information in Arabic.

### Mechanism 2
- **Claim:** Using specialized prompts (system, user, assistant roles) guides the model to produce responses that adhere to the expected format and content of medical Q&A.
- **Mechanism:** The prompt engineering approach defines clear roles for the interaction: the system prompt sets the behavior, the user prompt contains the question, and the assistant prompt provides the answer. This structure helps the model understand the task and generate appropriate responses.
- **Core assumption:** The prompt format is sufficiently clear and consistent to guide the model's behavior during inference.
- **Evidence anchors:** The paper describes using 'q_body' as the user prompt and 'a_body' as the assistant prompt, and states that "answers generated by the fine-tuned model were specific to the disease and similar to the original answer."

### Mechanism 3
- **Claim:** The combination of automated metrics (perplexity, similarity, coherence) and human evaluation provides a comprehensive assessment of the model's performance.
- **Mechanism:** Automated metrics offer quantitative measures of the model's language modeling ability and text similarity, while human evaluation by medical professionals provides qualitative assessment of relevance, accuracy, logic, and originality in the medical context.
- **Core assumption:** The evaluation metrics and criteria are appropriate for assessing the quality of medical Q&A responses.
- **Evidence anchors:** The study uses both automated evaluation (perplexity, coherence, similarity, token count) and human evaluation (relevance, accuracy, precision, logic, originality). The paper reports moderate perplexity (13.96) and similarity (0.10) scores, along with high accuracy (mean=4.1) and logic (mean=3.98) scores from human evaluation.

## Foundational Learning

- **Concept:** Large Language Models (LLMs) and Fine-tuning
  - **Why needed here:** Understanding how LLMs work and how fine-tuning adapts them to specific domains is crucial for grasping the approach used in this study.
  - **Quick check question:** What is the difference between pre-training and fine-tuning an LLM, and why is fine-tuning necessary for specialized tasks like medical Q&A?

- **Concept:** Natural Language Processing (NLP) Evaluation Metrics
  - **Why needed here:** Familiarity with metrics like perplexity, cosine similarity, and coherence scores is essential for interpreting the automated evaluation results in the study.
  - **Quick check question:** How does perplexity measure a language model's performance, and what does a lower perplexity score indicate?

- **Concept:** Arabic Language and Medical Terminology
  - **Why needed here:** Understanding the linguistic characteristics of Arabic and the specific terminology used in medical contexts is important for appreciating the challenges and achievements of this study.
  - **Quick check question:** What are some unique features of the Arabic language that might pose challenges for NLP tasks, and how might medical terminology differ from general language?

## Architecture Onboarding

- **Component map:** MAQA dataset -> GPT-3.5-turbo fine-tuning -> Prompt engineering (system, user, assistant roles) -> Automated evaluation (perplexity, similarity, coherence) -> Human evaluation (relevance, accuracy, precision, logic, originality)
- **Critical path:** Data preparation (cleaning, formatting) -> Fine-tuning (training on the dataset) -> Inference (generating responses) -> Evaluation (automated and human)
- **Design tradeoffs:** Fine-tuning on a large dataset improves performance but increases computational cost and token usage. Using automated metrics provides quick quantitative assessment but may miss nuanced issues that human evaluation can catch.
- **Failure signatures:** Low relevance or accuracy scores from human evaluation, high perplexity scores, or cosine similarity scores indicating poor alignment with the original answers.
- **First 3 experiments:**
  1. Fine-tune the model on a smaller subset of the MAQA dataset and evaluate its performance on a held-out test set to assess learning progress.
  2. Experiment with different prompt formats and evaluate their impact on the model's ability to generate relevant and accurate responses.
  3. Compare the performance of the fine-tuned model with the base GPT-3.5-turbo model on a set of medical questions to quantify the improvement from fine-tuning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal training dataset size for fine-tuning ChatGPT on medical Q&A data in Arabic to balance performance and cost?
- Basis in paper: The paper mentions that larger training datasets improve performance but increase token size and cost, and that the current study used 4000 examples for training.
- Why unresolved: The paper does not provide a systematic study of how training dataset size affects performance and cost. It only mentions that larger datasets are better but does not specify an optimal size.
- What evidence would resolve it: A systematic study varying training dataset sizes and measuring the trade-off between performance improvement and increased cost would provide evidence for the optimal size.

### Open Question 2
- Question: How does the performance of the fine-tuned ChatGPT model on real-world doctor-patient conversations compare to its performance on online Q&A forums?
- Basis in paper: The paper acknowledges that the training and evaluation datasets are from online Q&A forums and may not capture the nuances of real-world doctor-patient dialogues.
- Why unresolved: The paper does not test the model on real-world doctor-patient conversations, only on online Q&A forums.
- What evidence would resolve it: Testing the model on a dataset of real-world doctor-patient conversations and comparing its performance to its performance on online Q&A forums would provide evidence.

### Open Question 3
- Question: How can the relevance and precision of the fine-tuned ChatGPT model be improved?
- Basis in paper: The paper mentions that relevance and precision received lower scores in human evaluation and suggests that additional training data or fine-tuning may lead to better alignment with user expectations.
- Why unresolved: The paper does not provide specific methods for improving relevance and precision beyond suggesting more training data or fine-tuning.
- What evidence would resolve it: Experiments testing specific methods for improving relevance and precision, such as using more diverse training data or different fine-tuning techniques, would provide evidence for how to improve these aspects.

## Limitations

- Single specialty focus (gynecological) limits generalizability to broader medical domains
- Moderate automated evaluation scores suggest room for improvement in language modeling
- Limited evaluation metrics may miss nuanced quality issues
- Unknown data quality and diversity in the MAQA dataset
- No comparison with other fine-tuning approaches or baseline models

## Confidence

- **High Confidence:** The methodology of fine-tuning GPT-3.5-turbo on Arabic medical Q&A data is technically sound and follows established practices in LLM adaptation.
- **Medium Confidence:** The human evaluation results showing high accuracy scores are reliable given the use of native Arabic-speaking medical professionals, though the evaluation criteria and sample size could be more rigorously documented.
- **Low Confidence:** The generalization of results across different medical specialties and the long-term reliability of the fine-tuned model for clinical applications.

## Next Checks

1. **Cross-specialty validation:** Fine-tune and evaluate the model on additional medical specialties (cardiology, oncology, pediatrics) to assess generalization across different medical domains and terminology.

2. **Long-term performance monitoring:** Deploy the fine-tuned model in a controlled clinical environment for 30-60 days, tracking response accuracy, user satisfaction, and any emergent issues that automated/one-time human evaluation might miss.

3. **Error analysis and bias detection:** Conduct systematic analysis of model failures and potential biases by having medical professionals review 1000+ generated responses across different demographics, question types, and complexity levels to identify patterns of incorrect or problematic outputs.