---
ver: rpa2
title: 'Automated Claim Matching with Large Language Models: Empowering Fact-Checkers
  in the Fight Against Misinformation'
arxiv_id: '2310.09223'
source_url: https://arxiv.org/abs/2310.09223
tags:
- claim
- llms
- claims
- matching
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of misinformation by proposing
  an automated framework for the claim matching phase of fact-checking. The FACT-GPT
  framework uses Large Language Models (LLMs) to identify new social media content
  that supports or contradicts claims previously debunked by fact-checkers.
---

# Automated Claim Matching with Large Language Models: Empowering Fact-Checkers in the Fight Against Misinformation

## Quick Facts
- **arXiv ID**: 2310.09223
- **Source URL**: https://arxiv.org/abs/2310.09223
- **Reference count**: 40
- **Primary result**: FACT-GPT framework uses LLMs to identify social media content supporting/debunking previously debunked claims, with fine-tuned models performing comparably to large pre-trained models.

## Executive Summary
This paper introduces FACT-GPT, a framework that leverages Large Language Models (LLMs) to automate the claim matching phase of fact-checking. The approach uses GPT-4 to generate synthetic labeled data for training specialized LLMs to identify social media posts that support or contradict previously debunked claims. The framework demonstrates that fine-tuned smaller LLMs can match the performance of larger pre-trained models on claim matching tasks, potentially augmenting human fact-checkers' capabilities in combating misinformation.

## Method Summary
The FACT-GPT framework operates by first collecting debunked claims from fact-checking sources, then generating synthetic training data using LLMs (GPT-4, GPT-3.5-Turbo, Llama-2) with human annotations serving as ground truth. This synthetic data is used to fine-tune smaller specialized LLMs (Llama-2-13b, Llama-2-7b) for claim matching tasks. The method frames claim matching as a textual entailment problem and evaluates performance using precision, recall, and accuracy metrics on human-annotated test sets.

## Key Results
- Fine-tuned smaller LLMs (Llama-2-13b, Llama-2-7b) perform comparably to larger pre-trained models on claim matching tasks
- GPT-4-generated synthetic data provides effective training ground for specialized LLMs
- The framework successfully identifies social media content supporting or contradicting debunked claims with performance closely aligned with human annotations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic data generation from LLMs improves claim matching performance
- Mechanism: GPT-4 generates synthetic tweets labeled as entailment, neutral, or contradiction for each debunked claim, then fine-tunes smaller LLMs on this data
- Core assumption: High-quality synthetic data from GPT-4 can capture the semantic and stylistic nuances needed for effective fine-tuning
- Evidence anchors:
  - [abstract] "This data set serves as a training ground for fine-tuning more specialized LLMs."
  - [section] "To generate synthetic training data, we used three language models: GPT-4, GPT-3.5-Turbo, and Llama-2-70b-chat-hf... We set the temperature parameter to 1 to facilitate the production of stylistically diverse outputs."
  - [corpus] Weak - no direct corpus evidence that synthetic data matches real-world performance; only paper results show this
- Break condition: Synthetic data fails to generalize to real social media posts due to domain shift or lack of stylistic diversity

### Mechanism 2
- Claim: Fine-tuning smaller LLMs on GPT-4-generated data yields performance comparable to large pre-trained models
- Mechanism: Smaller models (Llama-2-13b, Llama-2-7b) are fine-tuned on balanced or imbalanced synthetic datasets and evaluated against human annotations
- Core assumption: Fine-tuning adapts the model's parameters effectively for the specific claim matching task
- Evidence anchors:
  - [abstract] "The results indicate that our fine-tuned LLMs rival the performance of larger pre-trained LLMs in claim matching tasks."
  - [section] "Table 4 reveals significant findings... Specifically, smaller models fine-tuned on GPT-4-generated sets exhibited comparable performance to their larger, pre-trained counterparts."
  - [corpus] Weak - no external corpus evidence; only paper results support this claim
- Break condition: Overfitting to synthetic data or inability to handle real-world linguistic variation

### Mechanism 3
- Claim: Bidirectional data generation captures presentation order importance in entailment tasks
- Mechanism: For each claim, synthetic tweets are generated in both presentation orders (claim→tweet and tweet→claim) to ensure the model learns bidirectional relationships
- Core assumption: The order of statements affects the entailment classification, and models must learn both directions
- Evidence anchors:
  - [abstract] "We postulate that if a model excels at entailment tasks, it will also be reliable in claim matching."
  - [section] "Recognizing the importance of the presentation order in the entailment task, tweet-claim pairs were presented in both possible orders."
  - [corpus] Weak - no corpus evidence; this is a design choice from the paper
- Break condition: Models fail to generalize when tested on real data with different presentation orders

## Foundational Learning

- Concept: Textual entailment classification
  - Why needed here: The claim matching task is framed as a textual entailment problem (entailment, neutral, contradiction)
  - Quick check question: Given "A dog is running in a field" and "An animal is running in a field," what is the entailment relationship?

- Concept: Bidirectional training data generation
  - Why needed here: Ensures the model learns both directions of claim-post relationships, as entailment is directional
  - Quick check question: If we generate "If tweet is true, claim is true" and "If claim is true, tweet is true," do we need both for a complete dataset?

- Concept: Fine-tuning vs. pre-training
  - Why needed here: Fine-tuning adapts large models to specific tasks, while pre-training gives general capabilities
  - Quick check question: What is the main difference between using a pre-trained LLM and a fine-tuned one for claim matching?

## Architecture Onboarding

- Component map: Debunked claims -> BM25/Sentence-BERT pairing -> Human annotation -> GPT-4 synthetic data generation -> Fine-tuning (Llama-2-13b, Llama-2-7b) -> Performance evaluation

- Critical path: 1. Collect debunked claims → 2. Generate synthetic training data → 3. Fine-tune smaller LLMs → 4. Evaluate on human-annotated test set

- Design tradeoffs:
  - Model size vs. efficiency: Smaller models are faster but may lack some generalization
  - Balanced vs. imbalanced training data: Balanced data ensures equal class representation but may not reflect real-world distributions

- Failure signatures:
  - Low precision: Model incorrectly labels posts as supporting debunked claims
  - Low recall: Model misses posts that actually support debunked claims
  - Overfitting: High training accuracy but poor test performance

- First 3 experiments:
  1. Test pre-trained LLMs (GPT-4, GPT-3.5-Turbo, Llama-2 variants) on human-annotated test set with different prompt styles
  2. Fine-tune GPT-3.5-Turbo on GPT-4-generated synthetic data and evaluate performance
  3. Compare fine-tuned Llama-2-13b and Llama-2-7b on balanced vs. imbalanced training data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the long-term impact of using LLMs for claim matching on the accuracy and reliability of fact-checking processes?
- Basis in paper: [explicit] The paper discusses the potential of LLMs to augment human fact-checkers but does not provide long-term studies or data on the sustained impact of using LLMs in fact-checking
- Why unresolved: Long-term studies are required to assess the sustained impact of LLMs on the accuracy and reliability of fact-checking processes, including potential biases and errors that may accumulate over time
- What evidence would resolve it: Longitudinal studies comparing the performance of human fact-checkers with and without LLM assistance over extended periods, along with analyses of error rates and bias propagation

### Open Question 2
- Question: How can the biases inherent in LLMs be effectively mitigated to ensure fair and unbiased fact-checking?
- Basis in paper: [explicit] The paper acknowledges that LLMs may propagate and amplify societal and data-driven biases, but it does not provide specific strategies for mitigating these biases
- Why unresolved: Developing effective methods to identify, measure, and mitigate biases in LLMs is a complex challenge that requires ongoing research and experimentation
- What evidence would resolve it: Comparative studies of different bias mitigation techniques, including their effectiveness in reducing bias in LLM outputs and their impact on the overall performance of fact-checking systems

### Open Question 3
- Question: What are the most effective ways to integrate LLMs into the fact-checking workflow without disrupting established norms and practices?
- Basis in paper: [explicit] The paper emphasizes the importance of augmented intelligence, where AI enhances human decision-making without replacing it, but it does not provide detailed guidelines for integration
- Why unresolved: Integrating LLMs into existing workflows requires careful planning and collaboration between AI developers and fact-checkers to ensure that the technology complements rather than disrupts established practices
- What evidence would resolve it: Case studies and pilot programs that demonstrate successful integration of LLMs into fact-checking workflows, along with feedback from fact-checkers on the effectiveness and usability of these integrations

## Limitations

- **Synthetic Data Generalization**: Limited validation that GPT-4-generated synthetic data generalizes to real-world social media content, with evaluation based only on paper results
- **Evaluation Scope**: Framework tested only on COVID-19-related claims, limiting generalizability to other domains
- **Human Annotation Reliability**: Use of Amazon Mechanical Turk workers without full specification of instructions, qualifications, or inter-rater agreement metrics

## Confidence

**High Confidence**: The methodology for claim matching as a textual entailment problem is well-established, with BM25 and Sentence-BERT pairing being standard approaches

**Medium Confidence**: Results showing comparable performance between fine-tuned smaller models and larger pre-trained models are internally consistent but lack external validation

**Low Confidence**: Assumption that synthetic data generation fully captures real social media discourse complexity remains unverified, with bidirectional approach not empirically validated for this specific task

## Next Checks

1. **Cross-Domain Validation**: Test the fine-tuned models on fact-checking claims from different domains (politics, climate, etc.) to assess generalizability beyond public health content

2. **Real-World Deployment Test**: Deploy the framework on live social media streams and compare performance against the synthetic data evaluation to identify any domain shift or performance degradation

3. **Human Expert Review**: Have professional fact-checkers evaluate a sample of the model's claim matching decisions to assess practical utility and identify edge cases where the model struggles