---
ver: rpa2
title: Evidence-based Interpretable Open-domain Fact-checking with Large Language
  Models
arxiv_id: '2312.05834'
source_url: https://arxiv.org/abs/2312.05834
tags:
- evidence
- fact-checking
- claim
- system
- oe-fact
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Open-domain Explainable Fact-checking
  (OE-Fact) system for real-world claim verification. The system leverages large language
  models (LLMs) to retrieve real-time evidence from open websites, filter relevant
  evidence using LLM and similarity calculations, and generate verdicts with causal
  explanations.
---

# Evidence-based Interpretable Open-domain Fact-checking with Large Language Models

## Quick Facts
- arXiv ID: 2312.05834
- Source URL: https://arxiv.org/abs/2312.05834
- Authors: 
- Reference count: 10
- Primary result: OE-Fact achieves 54.20% accuracy in open-domain claim verification on FEVER dataset

## Executive Summary
This paper introduces the Open-domain Explainable Fact-checking (OE-Fact) system for real-world claim verification. The system leverages large language models (LLMs) to retrieve real-time evidence from open websites, filter relevant evidence using LLM and similarity calculations, and generate verdicts with causal explanations. Evaluated on the FEVER dataset, OE-Fact outperforms baseline systems in both closed- and open-domain settings, achieving 54.20% accuracy in open-domain verification. The LLM-based approach ensures stable, accurate verdicts while providing concise, real-time explanations, enhancing transparency and coherence.

## Method Summary
OE-Fact employs a three-module pipeline for open-domain fact-checking: evidence retrieval, claim-relevant evidence selection, and verdict generation with real-time explanations. The system uses Google Custom Search API to retrieve evidence from open websites, leveraging both full claims and extracted noun phrases as queries. Claim-relevant evidence is selected through a combination of LLM-based filtering and semantic similarity calculation using BERT embeddings. Finally, an LLM generates verdicts (True, False, or Uncertain) with causal explanations based on the selected evidence. The system is evaluated on the FEVER dataset, demonstrating improved performance over baseline systems in both closed- and open-domain settings.

## Key Results
- OE-Fact achieves 54.20% accuracy in open-domain claim verification on FEVER dataset
- The system outperforms baseline systems in both closed- and open-domain settings
- Real-time explanations generated by the LLM enhance transparency and coherence of fact-checking decisions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based evidence filtering effectively removes irrelevant evidence by leveraging semantic understanding.
- Mechanism: The system uses a 1-shot prompt to instruct the LLM to return only sentences most relevant to the claim from the candidate evidence.
- Core assumption: The LLM can accurately interpret the claim and distinguish relevant from irrelevant sentences based on semantic meaning.
- Evidence anchors:
  - [abstract]: "we retain the evidence relevant to the claim through LLM and similarity calculation for subsequent verification"
  - [section]: "we propose to harness the foundation model to filter out evidence irrelevant to the claim. Specifically, we provide claim and candidate evidence as input and employ a 1-shot prompt to instruct LLM to return claim-relevant evidence"
  - [corpus]: Weak evidence - no direct mention of this specific filtering mechanism in related papers
- Break condition: If the LLM's understanding of the claim is incorrect or the prompt is ambiguous, it may filter out relevant evidence or retain irrelevant evidence.

### Mechanism 2
- Claim: Combining LLM-based filtering with semantic similarity calculation produces more robust evidence selection than either method alone.
- Mechanism: After LLM filtering, the system calculates cosine similarity between retained evidence and all candidate evidence using BERT embeddings, selecting the top 5 most similar pieces.
- Core assumption: The combination of LLM's semantic understanding and BERT's similarity metrics captures both high-level relevance and fine-grained semantic alignment.
- Evidence anchors:
  - [abstract]: "we retain the evidence relevant to the claim through LLM and similarity calculation"
  - [section]: "After calculating the similarity scores of all candidate evidence, we select the top 5 pieces with the highest scores as evidence for further claim verification"
  - [corpus]: Weak evidence - while related papers discuss multimodal fact-checking, they don't specifically mention this dual-filter approach
- Break condition: If the BERT similarity metric doesn't align with the LLM's interpretation of relevance, the combined approach may select suboptimal evidence.

### Mechanism 3
- Claim: Real-time decision explanations generated during verification improve transparency and reduce potential bias.
- Mechanism: The system uses a 1-shot prompt to instruct the LLM to verify the claim with a label ("True", "False", or "Uncertain") and generate a causal explanation based on the evidence.
- Core assumption: The LLM can analyze the causal relationship between evidence and claims and articulate this relationship in a coherent explanation.
- Evidence anchors:
  - [abstract]: "ensuring stable and accurate verdicts while providing concise and convincing real-time explanations for fact-checking decisions"
  - [section]: "we provide claims and evidence from the evidence selection module as input to LLM and guide it to verify the claim with the label 'True', 'False' or 'Uncertain' and generate real-time decision explanations simultaneously"
  - [corpus]: Moderate evidence - the DEFAME paper mentions "explainable" fact-checking but doesn't describe real-time explanation generation during verification
- Break condition: If the LLM's reasoning is flawed or the explanation doesn't accurately reflect the evidence-claim relationship, the transparency benefit is lost.

## Foundational Learning

- Concept: Three-module fact-checking pipeline (document retrieval, evidence selection, claim classification)
  - Why needed here: Understanding this traditional framework is essential to grasp how OE-Fact adapts it for open-domain scenarios
  - Quick check question: What are the three traditional modules in fact-checking systems, and how does OE-Fact modify each for open-domain use?

- Concept: Large Language Model prompting techniques
  - Why needed here: The system relies on carefully crafted 1-shot prompts to guide LLM behavior for evidence filtering and explanation generation
  - Quick check question: How does the 1-shot prompt format differ between evidence filtering and verdict generation in OE-Fact?

- Concept: Semantic similarity metrics and embeddings
  - Why needed here: The system uses BERT embeddings and cosine similarity to select the most relevant evidence after LLM filtering
  - Quick check question: Why might the system use both LLM-based filtering and BERT-based similarity calculation rather than relying on just one approach?

## Architecture Onboarding

- Component map:
  - Google Custom Search API (evidence retrieval)
  - SpaCy NLP pipeline (claim analysis for keyword extraction)
  - Llama 2 (LLM for evidence filtering and verdict generation)
  - BERT embeddings (semantic similarity calculation)
  - OE-Fact orchestration layer (coordinates the pipeline)

- Critical path: Claim → Google search (dual queries) → LLM filtering → BERT similarity → Top-5 evidence → LLM verdict + explanation

- Design tradeoffs:
  - Using Google search provides real-time evidence but introduces dependency on external API
  - Dual query strategy (full claim + keyword extraction) increases coverage but doubles search load
  - LLM-based filtering reduces evidence volume but may introduce LLM-specific biases
  - Top-5 evidence selection balances comprehensiveness with computational efficiency

- Failure signatures:
  - High rate of "Uncertain" verdicts may indicate insufficient evidence quality or LLM comprehension issues
  - Consistently incorrect verdicts suggest problems with the evidence selection or LLM reasoning
  - Slow response times may indicate inefficient evidence retrieval or processing bottlenecks

- First 3 experiments:
  1. Test evidence retrieval: Run the dual query strategy on sample claims and verify the coverage and relevance of retrieved evidence
  2. Validate evidence filtering: Compare LLM-filtered evidence against manually annotated relevant evidence for sample claims
  3. Assess explanation quality: Evaluate the coherence and accuracy of real-time explanations against human-written explanations for sample claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the OE-Fact system handle contradictory evidence from multiple sources in real-world claim verification?
- Basis in paper: [explicit] The paper mentions that evidence from public websites often comes from multiple sources, resulting in uncertainty, diversity, and even contradictions.
- Why unresolved: The paper does not provide detailed mechanisms for resolving contradictions between evidence sources.
- What evidence would resolve it: A detailed analysis of how OE-Fact resolves contradictions, including specific examples and performance metrics.

### Open Question 2
- Question: What is the impact of the LLM-based evidence selection module on the overall accuracy of the OE-Fact system?
- Basis in paper: [explicit] The paper states that the evidence selection module sequentially employs Llama and semantic similarity calculation to select claim-relevant evidence.
- Why unresolved: The paper does not provide a detailed comparison of the accuracy with and without the evidence selection module.
- What evidence would resolve it: A comparison of accuracy metrics with and without the evidence selection module, including specific performance improvements.

### Open Question 3
- Question: How does the OE-Fact system ensure the timeliness of evidence in rapidly evolving real-world scenarios?
- Basis in paper: [explicit] The paper highlights the importance of ensuring adequate and timely evidence for real-world open-domain claim checking.
- Why unresolved: The paper does not provide specific strategies or mechanisms for ensuring the timeliness of evidence.
- What evidence would resolve it: A detailed description of strategies or mechanisms used to ensure the timeliness of evidence, including performance metrics in rapidly evolving scenarios.

## Limitations

- The paper relies on closed-domain FEVER dataset for evaluation of an open-domain system, which may not fully capture real-world variability
- Exact prompt templates for LLM-based evidence filtering and verdict generation are not specified, making exact reproduction difficult
- The paper doesn't address potential LLM biases or hallucinations in evidence filtering and explanation generation processes

## Confidence

- High confidence: The three-module pipeline architecture and overall methodology are clearly described and logically sound
- Medium confidence: The reported performance metrics are credible given the evaluation setup, though real-world performance may vary
- Low confidence: The system's robustness to adversarial claims or its performance on domains outside the FEVER dataset

## Next Checks

1. Test the system on claims from diverse real-world sources outside the FEVER dataset to evaluate generalization
2. Conduct ablation studies comparing LLM-only filtering, BERT-only similarity scoring, and the combined approach to quantify their respective contributions
3. Evaluate explanation quality by having human annotators assess whether the generated explanations accurately reflect the evidence-claim relationship and provide genuine insight into the decision process