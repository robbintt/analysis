---
ver: rpa2
title: 'Adapt then Unlearn: Exploring Parameter Space Semantics for Unlearning in
  Generative Adversarial Networks'
arxiv_id: '2309.14054'
source_url: https://arxiv.org/abs/2309.14054
tags:
- unlearning
- samples
- undesired
- features
- repulsion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a two-stage method, "Adapt-then-Unlearn,"
  for removing undesired features from pre-trained GANs when the training data is
  unavailable. The approach adapts the generator to negative samples (undesired features)
  in the first stage and then unlearns those features using a repulsion loss that
  pushes the parameters away from the adapted ones while preserving generation quality.
---

# Adapt then Unlearn: Exploring Parameter Space Semantics for Unlearning in Generative Adversarial Networks

## Quick Facts
- arXiv ID: 2309.14054
- Source URL: https://arxiv.org/abs/2309.14054
- Reference count: 23
- Unlearns undesired features from GANs using a two-stage method with repulsion loss

## Executive Summary
This paper introduces a two-stage "Adapt-then-Unlearn" method for removing undesired features from pre-trained GANs when training data is unavailable. The approach first adapts the generator to negative samples (undesired features) using Elastic Weight Consolidation, then unlearns those features using a repulsion loss that pushes parameters away from the adapted ones while preserving generation quality. Experiments on MNIST and CelebA-HQ demonstrate effective unlearning with high PUL (>90%) and maintained FID scores close to the original model, particularly when using the exponential negative ℓ2 variant of the repulsion loss.

## Method Summary
The method operates in two stages on a pre-trained GAN generator. First, it adapts the generator to negative samples using EWC regularization to obtain adapted parameters θN. Second, it unlearns undesired features by training the original generator on positive samples with a repulsion loss that pushes parameters away from θN. Three repulsion loss variants are evaluated: inverse ℓ2, negative ℓ2, and exponential negative ℓ2. The approach requires a pre-trained classifier to label generated samples as positive or negative, and operates in few-shot settings with only dozens of negative samples.

## Key Results
- Achieves high PUL (>90%) while maintaining FID scores close to the original model
- Exponential negative ℓ2 repulsion loss variant (LEL2) performs best among the three variants tested
- Effective unlearning demonstrated on both MNIST (class-level) and CelebA-HQ (feature-level) with minimal quality degradation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Parameter space in GANs contains interpretable semantic directions that can be leveraged to suppress specific features.
- **Mechanism**: The generator parameters have directions where movement along them alters specific semantic features. By adapting the generator to undesired samples, the parameters align with the direction that generates only undesired features. Unlearning then involves moving away from these adapted parameters.
- **Core assumption**: The parameter space of GANs is structured such that specific semantic features map to interpretable directions in parameter space.
- **Evidence anchors**:
  - [abstract]: "the parameter space of GANs exhibits meaningful directions that can be leveraged to suppress specific undesired features"
  - [section]: "the first stage of the proposed method leads to parameters that generate only negative samples... Hence, the difference between the adapted generator's parameter and the original generator's parameter can be interpreted as the direction in parameter space that leads to a decrease in the generation of negative samples"
- **Break condition**: If the parameter space lacks such semantic structure or if semantic features are distributed across many parameters without coherent directions.

### Mechanism 2
- **Claim**: Repulsion loss prevents catastrophic forgetting by keeping unlearned parameters away from the adapted parameters while maintaining generation quality.
- **Mechanism**: The repulsion loss (Lrepulsion) encourages the generator parameters to move away from the adapted parameters (θN) obtained in stage 1. This is combined with adversarial loss on positive samples to ensure the generator maintains quality while avoiding the undesired feature.
- **Core assumption**: Moving parameters away from those that generate undesired features while training on positive samples will result in a generator that produces quality samples without the undesired features.
- **Evidence anchors**:
  - [abstract]: "we train the original pre-trained GAN using positive samples, along with a repulsion regularizer. This regularizer encourages the learned model parameters to move away from the parameters of the adapted model (first stage) while not degrading the generation quality"
  - [section]: "we propose to train the generator using adversarial loss while encouraging the generator parameters to be away from the adapted generator's parameters"
- **Break condition**: If the repulsion loss is too strong (degrades quality) or too weak (fails to unlearn), or if the positive samples don't adequately represent the desired distribution.

### Mechanism 3
- **Claim**: The exponential negative ℓ2 repulsion loss variant (LEL2) performs best because it provides the right balance between repulsion strength and gradient stability.
- **Mechanism**: Among the three repulsion loss variants (inverse ℓ2, negative ℓ2, exponential negative ℓ2), the exponential form provides a smooth gradient that effectively repels parameters from θN while maintaining stable training dynamics.
- **Core assumption**: The exponential form of repulsion loss provides optimal gradient properties for the unlearning task compared to linear forms.
- **Evidence anchors**:
  - [abstract]: "the exponential negative ℓ2 variant performing best"
  - [section]: "We evaluate our result using all the choices of repulsion loss as mentioned in Eq. 7"
- **Break condition**: If other repulsion forms work better for different GAN architectures or feature types, or if the exponential form causes numerical instability.

## Foundational Learning

- **Concept**: Elastic Weight Consolidation (EWC)
  - **Why needed here**: EWC is used in stage 1 to adapt the pre-trained GAN to negative samples without catastrophic forgetting, since only a few negative samples are available.
  - **Quick check question**: What does the Fisher information matrix represent in EWC, and why is it important for penalizing weight changes during adaptation?

- **Concept**: Fréchet Inception Distance (FID)
  - **Why needed here**: FID is used to measure the quality of generated samples after unlearning, comparing them to the original dataset distribution.
  - **Quick check question**: How does FID differ from Inception Score, and why is it more appropriate for evaluating unlearning quality?

- **Concept**: Generative Adversarial Networks (GANs)
  - **Why needed here**: Understanding GAN training dynamics is crucial since the method works with pre-trained GAN generators and discriminators.
  - **Quick check question**: What is the role of the discriminator during the unlearning process, and how does it differ from its role during standard GAN training?

## Architecture Onboarding

- **Component map**: Pre-trained GAN → Classifier feedback → Stage 1 adaptation → Stage 2 unlearning → Evaluation
- **Critical path**: Pre-trained GAN → Classifier feedback → Stage 1 adaptation → Stage 2 unlearning → Evaluation
- **Design tradeoffs**:
  - Using few negative samples (practical but may limit unlearning effectiveness)
  - Choosing repulsion loss form (affects both unlearning performance and generation quality)
  - Balancing λ in EWC (too high causes underfitting, too low causes catastrophic forgetting)
- **Failure signatures**:
  - PUL remains low → Repulsion loss too weak or positive samples insufficient
  - FID increases significantly → Repulsion loss too strong or adaptation stage failed
  - Unlearning affects correlated features → Parameter space semantics are highly entangled
- **First 3 experiments**:
  1. Validate that moving parameters along the direction from θG to θN increases generation of undesired features (as shown in Figure 1)
  2. Test unlearning on MNIST with simple features (class-1, class-4, class-8) to verify basic functionality
  3. Compare all three repulsion loss variants (LIL2, LNL2, LEL2) on a single feature (e.g., CelebA bangs) to confirm LEL2 performs best

## Open Questions the Paper Calls Out

- **Open Question 1**: How do the proposed repulsion losses perform when the number of negative samples provided by the user is significantly larger (e.g., hundreds instead of dozens)?
- **Open Question 2**: Can the repulsion loss be extended to other generative models beyond GANs, such as VAEs or diffusion models?
- **Open Question 3**: How does the choice of repulsion loss affect the unlearning of correlated features, and can the method be modified to selectively unlearn only the target feature?
- **Open Question 4**: How does the performance of the proposed method compare to other unlearning methods that do not rely on parameter space semantics?

## Limitations
- Relies on the assumption that GAN parameter space contains interpretable semantic directions
- Requires access to a pre-trained classifier to generate feedback samples
- Effectiveness depends on having sufficient positive samples to maintain generation quality during unlearning stage

## Confidence
- **High**: The overall two-stage framework and its basic functionality (achieving both high PUL and reasonable FID scores)
- **Medium**: The specific performance metrics on MNIST and CelebA-HQ datasets
- **Low**: The theoretical justification for why the exponential negative ℓ2 repulsion loss performs best among the variants

## Next Checks
1. Test the method on a more complex dataset (e.g., LSUN bedrooms or FFHQ faces) to evaluate scalability beyond the relatively simple MNIST and CelebA-HQ features
2. Conduct ablation studies removing the EWC regularization in stage 1 to quantify its impact on preventing catastrophic forgetting
3. Evaluate the method's performance when the pre-trained classifier has varying accuracy levels to understand robustness to classification errors