---
ver: rpa2
title: 'SEER : A Knapsack approach to Exemplar Selection for In-Context HybridQA'
arxiv_id: '2310.06675'
source_url: https://arxiv.org/abs/2310.06675
tags:
- seer
- uni00000013
- selection
- exemplar
- exemplars
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SEER introduces a knapsack integer linear program for exemplar
  selection in hybrid question answering. By modeling the selection process as a constrained
  optimization problem, it balances diversity, similarity, and token budget constraints.
---

# SEER : A Knapsack approach to Exemplar Selection for In-Context HybridQA

## Quick Facts
- arXiv ID: 2310.06675
- Source URL: https://arxiv.org/abs/2310.06675
- Reference count: 20
- Key outcome: SEER achieves up to 69.68% exact match and 68.85% execution accuracy on FinQA and TAT-QA, outperforming prior in-context learning baselines.

## Executive Summary
SEER introduces a knapsack integer linear program for exemplar selection in hybrid question answering. By modeling the selection process as a constrained optimization problem, it balances diversity, similarity, and token budget constraints. On FinQA and TAT-QA, SEER achieves up to 69.68% exact match and 68.85% execution accuracy, outperforming prior in-context learning baselines and narrowing the gap with fine-tuned models.

## Method Summary
SEER formulates exemplar selection as a knapsack integer linear program (ILP) that optimizes for similarity between exemplars and test instances while enforcing constraints on diversity (modality, answer type, reasoning steps) and token budget. The method uses nearest neighbor filtering to reduce the candidate set, constraint modules to predict test instance attributes, and an ILP solver to select the optimal exemplar set. The selected exemplars are then used to generate Python code for answer derivation through an LLM.

## Key Results
- On FinQA, SEER achieves 68.85% execution accuracy, outperforming KATE and narrowing the gap with fine-tuned models.
- On TAT-QA, SEER achieves 69.68% exact match, demonstrating strong performance on hybrid contexts.
- SEERgold (with oracle attribute prediction) improves performance by 0.4-1.5% over SEER, indicating the importance of accurate attribute prediction.

## Why This Works (Mechanism)

### Mechanism 1
The knapsack integer linear program enables explicit balancing of similarity, diversity, and token capacity in exemplar selection. The ILP framework models each exemplar as a binary variable and maximizes the weighted sum of similarities while enforcing constraints on the number of exemplars and total token budget. This guarantees that selected exemplars are both relevant and diverse without exceeding capacity.

### Mechanism 2
Constraint modules that predict attributes (modality, answer type, reasoning steps) guide the selection of exemplars that match the test instance's reasoning chain. For each test instance, modules predict attributes, which are then encoded as diversity constraints in the knapsack. This ensures that exemplars sharing the same attributes as the test instance are prioritized, leading to more coherent reasoning chains.

### Mechanism 3
Nearest neighbor filtering reduces the candidate set size, making ILP solving tractable while preserving high-similarity exemplars. For each test instance, the k nearest neighbors in embedding space are pre-selected as candidates, and the ILP is solved only over this subset. This keeps computation manageable without sacrificing too much similarity.

## Foundational Learning

- Concept: Integer Linear Programming (ILP) and Knapsack formulation.
  - Why needed here: Enables explicit optimization of exemplar selection under multiple constraints (similarity, diversity, token budget).
  - Quick check question: Can you formulate a simple knapsack problem where items have values and weights, and the goal is to maximize total value without exceeding capacity?

- Concept: Constraint satisfaction and attribute prediction.
  - Why needed here: Allows SEER to select exemplars that match the test instance's reasoning chain attributes, improving performance.
  - Quick check question: Given a set of exemplars each labeled with "modality" (text/table/hybrid) and "answer type" (span/count/arithmetic), how would you write constraints to ensure the selected set matches the test instance's attributes?

- Concept: Nearest neighbor search and embedding similarity.
  - Why needed here: Provides an efficient way to pre-filter candidates to a manageable set while preserving high-similarity exemplars.
  - Quick check question: How would you compute cosine similarity between two questions after normalizing them (lowercase, remove punctuation, replace entities with NER tags)?

## Architecture Onboarding

- Component map: Embedding module -> Nearest neighbor filter -> Constraint modules -> ILP solver -> LLM code generator

- Critical path:
  1. Compute embeddings for test instance and training set.
  2. Filter k nearest neighbors.
  3. Predict attributes using constraint modules.
  4. Build ILP with similarity weights and diversity constraints.
  5. Solve ILP to get exemplar set.
  6. Generate prompt and call LLM for answer.

- Design tradeoffs:
  - Larger k increases similarity but solver time and memory usage.
  - Higher α (diversity weight) improves attribute matching but may reduce similarity.
  - Using fine-tuned vs ICL constraint modules trades accuracy for fewer labeled examples.

- Failure signatures:
  - Low EA/EM when attribute prediction is poor.
  - Long solver times if k is too large or constraints are too tight.
  - Poor coverage if token budget is underestimated.

- First 3 experiments:
  1. Compare SEER with KATE on FinQA with default token budget to confirm performance gain.
  2. Vary α and β parameters to see impact on EA/EM and attribute matching.
  3. Reduce token budget to 2048 and observe performance drop vs KATE.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SEER vary with different levels of similarity in the training data?
- Basis in paper: [inferred] The paper mentions that SEER reduces the candidate set with a nearest neighbor filtering and leverages constraint modules to predict the attributes of the test instance. However, it does not explicitly explore how the performance of SEER varies with different levels of similarity in the training data.
- Why unresolved: The paper does not provide an analysis of how the performance of SEER is affected by the similarity of the training data.
- What evidence would resolve it: An experiment that evaluates SEER's performance on training data with varying levels of similarity, such as very similar exemplars versus very diverse exemplars.

### Open Question 2
- Question: How does the performance of SEER compare to other methods when the number of exemplars is limited?
- Basis in paper: [explicit] The paper mentions that SEER achieves up to 69.68% exact match and 68.85% execution accuracy, outperforming prior in-context learning baselines and narrowing the gap with fine-tuned models. However, it does not explicitly compare SEER's performance with other methods when the number of exemplars is limited.
- Why unresolved: The paper does not provide a comparison of SEER's performance with other methods when the number of exemplars is limited.
- What evidence would resolve it: An experiment that evaluates the performance of SEER and other methods when the number of exemplars is limited, such as 2 or 3 exemplars.

### Open Question 3
- Question: How does the performance of SEER vary with different types of attributes?
- Basis in paper: [inferred] The paper mentions that SEER leverages constraint modules to predict the attributes of the test instance and that these attributes include modality, answer type, and number of reasoning steps. However, it does not explicitly explore how the performance of SEER varies with different types of attributes.
- Why unresolved: The paper does not provide an analysis of how the performance of SEER is affected by different types of attributes.
- What evidence would resolve it: An experiment that evaluates SEER's performance on different types of attributes, such as more complex or more diverse attributes.

## Limitations

- The effectiveness of attribute prediction is critical but the paper shows only marginal improvements from oracle (SEERgold) vs predicted attributes (0.4-1.5%), making it unclear whether gains come from accurate prediction or ILP robustness.
- Token budget constraints are fundamental but the paper uses a fixed 4096-token budget without exploring sensitivity to tighter budgets, leaving unclear whether gains come from better selection or more capacity.
- Computational overhead of solving the ILP is reported but not compared against alternative exemplar selection methods, making it unclear whether performance gains justify additional computation.

## Confidence

High confidence in empirical results showing SEER outperforms baselines on both FinQA and TAT-QA datasets.

Medium confidence in mechanism claims about how the ILP framework specifically improves performance, as ablation studies don't fully isolate component contributions.

Low confidence in scalability claims, as performance is demonstrated only on two specific datasets without testing larger or more diverse hybrid reasoning tasks.

## Next Checks

1. **Budget sensitivity analysis**: Run SEER with varying token budgets (2048, 3072, 4096, 6144) on FinQA to determine how performance scales with capacity. Compare against KATE at each budget level to isolate whether gains come from better selection or more exemplars.

2. **Attribute prediction ablation**: Implement a variant of SEER that randomly assigns attributes with the same distribution as the predicted ones. Measure performance degradation compared to SEER with predicted attributes and SEERgold with oracle attributes. This would quantify how much of the gain comes from accurate attribute prediction versus the ILP framework itself.

3. **Computational overhead comparison**: Time the complete exemplar selection pipeline for SEER, KATE, and KATE v2 on the same hardware. Include both the pre-filtering and selection phases. Compare wall-clock time against the performance gains to calculate the efficiency trade-off.