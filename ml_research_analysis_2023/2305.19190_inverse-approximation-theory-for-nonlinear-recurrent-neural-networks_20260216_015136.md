---
ver: rpa2
title: Inverse Approximation Theory for Nonlinear Recurrent Neural Networks
arxiv_id: '2305.19190'
source_url: https://arxiv.org/abs/2305.19190
tags:
- memory
- approximation
- linear
- decaying
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the approximation capabilities of recurrent
  neural networks (RNNs) for learning long-term dependencies in sequential data. It
  establishes a Bernstein-type approximation theorem showing that nonlinear functionals
  that can be stably approximated by RNNs with hardtanh/tanh activations must have
  an exponential decaying memory structure.
---

# Inverse Approximation Theory for Nonlinear Recurrent Neural Networks

## Quick Facts
- arXiv ID: 2305.19190
- Source URL: https://arxiv.org/abs/2305.19190
- Reference count: 40
- This paper shows that nonlinear RNNs with hardtanh/tanh activations can only stably approximate functionals with exponential decaying memory, and proposes reparameterization methods to overcome this limitation.

## Executive Summary
This paper investigates the fundamental limitations of recurrent neural networks (RNNs) in approximating functional sequences with long-term dependencies. Through a Bernstein-type inverse approximation theorem, it demonstrates that RNNs with hardtanh or tanh activations can only stably approximate functionals whose memory decays exponentially. This extends the known "curse of memory" from linear RNNs to the nonlinear setting. The paper proposes principled reparameterization methods using exponential or softplus functions to overcome these limitations while maintaining stability under perturbations.

## Method Summary
The paper establishes theoretical foundations for inverse approximation theory in nonlinear RNNs. It defines a memory function for nonlinear functional sequences using derivatives over Heaviside inputs, then proves that stable approximation by RNNs requires this memory to decay exponentially. The authors propose reparameterizing the recurrent weight matrix using functions that ensure eigenvalues have non-positive real parts, enabling stable approximation of functionals with slower memory decay. Numerical experiments validate these theoretical findings using gradient-based optimization (Adam) on synthetic target functionals with known memory patterns.

## Key Results
- Nonlinear RNNs with hardtanh/tanh activations cannot stably approximate functionals with non-exponentially decaying memory
- Reparameterization using exponential or softplus functions enables stable approximation of functionals with polynomial memory decay
- The memory function for nonlinear functionals can be defined via supremum of derivatives over bounded Heaviside inputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Nonlinear RNNs with hardtanh/tanh activations still suffer from the curse of memory
- Mechanism: Stable approximation requires eigenvalues to approach zero, but this causes instability under perturbations
- Core assumption: Target functional has decaying memory and admits β₀-stable approximation in Sobolev norm
- Evidence anchors: [abstract] "nonlinear functional sequences... must have an exponential decaying memory structure"; [section 4.3] Theorem 4.8 on memory decay requirement
- Break condition: Target lacks decaying memory or approximation is not stable

### Mechanism 2
- Claim: Reparameterization with exponential/softplus functions enables stable approximation
- Mechanism: These functions ensure eigenvalues have non-positive real parts, maintaining stability
- Core assumption: Reparameterization maps weights such that resulting matrix has stable eigenvalues
- Evidence anchors: [section 4.4] Exponential and softplus reparameterization methods; eigenvalue constraints
- Break condition: Reparameterization fails to maintain eigenvalue constraint or target has no decaying memory

### Mechanism 3
- Claim: Memory function defined via supremum of derivatives over Heaviside inputs
- Mechanism: This definition is consistent with linear case and enables numerical querying
- Core assumption: Functional is continuous, causal, regular, and time-homogeneous
- Evidence anchors: [section 4.1] Definition 4.1 using supremum over Heaviside inputs
- Break condition: Functional not causal/regular or derivative does not exist

## Foundational Learning

- Concept: Bernstein-type (inverse) approximation theorems
  - Why needed here: Deduce target properties from approximation capability, revealing architectural limitations
  - Quick check question: What is the key difference between Jackson-type (forward) and Bernstein-type (inverse) approximation theorems?

- Concept: Sobolev norm for functional sequences
  - Why needed here: Defines stable approximation condition requiring small error and derivative in supremum norm
  - Quick check question: How is the Sobolev norm defined for functional sequences in Equation (13)?

- Concept: Lyapunov stability theory
  - Why needed here: Shows system becomes unstable under perturbations when eigenvalues approach zero
  - Quick check question: What role does the Lyapunov equation play in bounding eigenvalues of recurrent matrix?

## Architecture Onboarding

- Component map: Input sequence x ∈ C₀(Rᵈ) -> Hidden state h with dynamics dh/dt = σ(W h + U x + b) -> Output y = cᵀh -> Memory function MB(H)(t) defined via derivatives over Heaviside inputs -> Stability analysis via eigenvalue bounds and Lyapunov functions

- Critical path: 1) Define memory function for target 2) Train RNN to approximate target 3) Perturb weights and measure error 4) Check continuity of perturbed error at β=0 5) Conclude on memory decay property

- Design tradeoffs: Hardtanh/tanh activations (standard but limited to exponential memory) vs Exponential/softplus reparameterization (overcomes limitation but adds computational cost) vs Linear RNNs (simpler but also limited)

- Failure signatures: Perturbed error E(β) not continuous at β=0 (unstable approximation), Eigenvalues approaching zero (instability), Memory function not decaying (cannot be stably approximated)

- First 3 experiments: 1) Approximate linear functional with exponential memory using linear RNN; verify stable approximation 2) Approximate linear functional with polynomial memory using linear RNN; show unstable approximation 3) Use exponentially reparameterized RNN to approximate polynomial memory functional; verify stable approximation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the exponential decay memory limitation apply to other nonlinear activation functions like sigmoid or ReLU in RNNs?
- Basis in paper: [explicit] Focuses on hardtanh and tanh activations but mentions sigmoid and ReLU as alternatives
- Why unresolved: Theoretical results proven specifically for hardtanh and tanh; behavior for other activations not analyzed
- What evidence would resolve it: Proving Bernstein-type theorem for sigmoid/ReLU or numerical experiments comparing their memory decay patterns

### Open Question 2
- Question: Can reparameterization methods (exponential or softplus) overcome curse of memory for all target functional sequences?
- Basis in paper: [explicit] Proposes reparameterization methods and demonstrates effectiveness for linear functionals with polynomial memory
- Why unresolved: Experiments limited to linear functionals with polynomial memory; effectiveness for nonlinear cases not demonstrated
- What evidence would resolve it: Numerical experiments on variety of target functional sequences with different memory decay patterns including nonlinear cases

### Open Question 3
- Question: How do transformer-type architectures like BERT compare to RNNs in terms of memory decay patterns and approximation capabilities?
- Basis in paper: [explicit] Mentions transformer architectures might not have exponential decaying memory issues based on BERT observations
- Why unresolved: Comparison based on single experiment with BERT on sentiment analysis; comprehensive comparison not provided
- What evidence would resolve it: Systematic experiments comparing memory decay patterns and approximation capabilities of transformers vs RNNs across various tasks

## Limitations
- The stability conditions requiring eigenvalues to approach zero may be overly conservative for finite-precision implementations
- Reparameterization methods add computational overhead and may introduce optimization difficulties
- Memory function definition via Heaviside inputs may not capture full complexity of real sequential data

## Confidence

- Theorem 4.8 (Bernstein-type result): Medium confidence - rigorous theoretical support but practical relevance depends on stability conditions
- Reparameterization approach: Medium confidence - theoretical guarantees exist but computational and optimization challenges need further investigation
- Memory function definition: Low confidence - novel and theoretically interesting but practical applicability uncertain

## Next Checks

1. **Finite-Precision Stability Analysis**: Test whether theoretical instability conditions manifest in 32-bit floating point implementations or if numerical precision provides implicit stabilization

2. **Cross-Architecture Comparison**: Evaluate whether LSTMs or Transformers face similar memory decay limitations when analyzed through the same Bernstein-type framework

3. **Alternative Reparameterization Methods**: Investigate other stable reparameterization schemes beyond exponential and softplus to determine if they offer better optimization properties while maintaining stability guarantees