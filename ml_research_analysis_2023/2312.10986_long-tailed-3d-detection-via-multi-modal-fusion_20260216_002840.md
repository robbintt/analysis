---
ver: rpa2
title: Long-Tailed 3D Detection via Multi-Modal Fusion
arxiv_id: '2312.10986'
source_url: https://arxiv.org/abs/2312.10986
tags:
- detections
- lidar
- detectors
- detection
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles Long-Tailed 3D Detection (LT3D) for autonomous
  vehicles, addressing the challenge of detecting both common and rare object classes.
  The core method is a simple late-fusion framework that combines 2D RGB and 3D LiDAR
  detections.
---

# Long-Tailed 3D Detection via Multi-Modal Fusion

## Quick Facts
- arXiv ID: 2312.10986
- Source URL: https://arxiv.org/abs/2312.10986
- Authors: 
- Reference count: 40
- Primary result: Achieves 51.4 mAP on nuScenes LT3D benchmark, outperforming prior work by 5.9 mAP

## Executive Summary
This paper addresses Long-Tailed 3D Detection (LT3D) for autonomous vehicles by proposing a simple late-fusion framework that combines 2D RGB and 3D LiDAR detections. The core insight is that 2D RGB detectors, trained on large external datasets, achieve better recognition accuracy for rare classes than 3D RGB detectors. The method matches detections on the 2D image plane to mitigate depth estimation errors and uses score calibration with probabilistic ensembling for fusion. Experiments on nuScenes and Argoverse 2 datasets show state-of-the-art performance, significantly improving rare class detection (e.g., 12.8 to 20.0 mAP for the six rarest classes).

## Method Summary
The proposed method is a late-fusion framework that combines independently trained 2D RGB detectors (e.g., YOLOV7, DINO) and 3D LiDAR detectors (e.g., CenterPoint). The key innovation is matching 3D LiDAR detections with 2D RGB detections on the 2D image plane using sensor extrinsics, which avoids depth estimation errors that occur when inflating 2D detections to 3D BEV. The method includes score calibration to tune per-class temperature parameters, making scores from different detectors comparable, and probabilistic ensembling to combine calibrated scores using Bayesian principles. Unmatched detections are filtered out before applying non-maximum suppression to remove duplicates.

## Key Results
- Achieves 51.4 mAP on nuScenes LT3D benchmark, outperforming prior work by 5.9 mAP
- Improves rare class detection from 12.8 to 20.0 mAP for the six rarest classes
- Demonstrates superior performance compared to end-to-end trained multi-modal detectors on rare class detection
- Shows consistent improvements across both nuScenes and Argoverse 2 datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Late fusion with 2D RGB detectors outperforms fusion with 3D RGB detectors for rare class detection.
- Mechanism: 2D RGB detectors trained on large external datasets (e.g., COCO, ImageNet) achieve better recognition accuracy for rare classes than 3D RGB detectors, which are limited by smaller, less diverse training sets and the added complexity of depth estimation.
- Core assumption: 2D RGB detection is a mature task with well-understood model trade-offs, and 2D annotations are cheaper to collect than 3D annotations, enabling larger and more diverse training data.
- Evidence anchors:
  - [abstract]: "2D RGB detectors achieve better recognition accuracy than 3D RGB detectors, matching on the 2D image plane mitigates depth estimation errors, and score calibration and probabilistic fusion notably improves the final performance further."
  - [section]: "2D RGB Detectors Are More Mature...2D RGB Detectors Can Be Trained with More Diverse Data...2D RGB detectors do not make 3D predictions (e.g., depth and rotation), understanding how to best leverage them in the context of long-tailed 3D detection is a key challenge."
- Break condition: If 3D RGB detectors achieve significantly better recognition accuracy than 2D RGB detectors on rare classes, or if depth estimation errors are not a significant factor in matching performance.

### Mechanism 2
- Claim: Matching 3D LiDAR detections with 2D RGB detections on the 2D image plane is more robust than matching 3D RGB detections with 3D LiDAR detections in the 3D BEV.
- Mechanism: Matching on the 2D image plane avoids depth estimation errors that occur when inflating 2D RGB detections to 3D BEV, leading to better spatial matching and improved rare class detection.
- Core assumption: Depth estimation errors from 3D RGB detectors are a significant source of noise in the matching process, and projecting 3D LiDAR detections to the 2D image plane is a reliable way to avoid these errors.
- Evidence anchors:
  - [abstract]: "matching on the 2D image plane mitigates depth estimation errors for better matching"
  - [section]: "To match 3D detections on the 2D image plane, we use the provided sensor extrinsics. To match 2D detections in the 3D BEV, we inflate 2D detection using LiDAR points within the box frustum. In practice, we find that naively lifting 2D RGB detections into 3D leads to imprecise depth estimates and lower performance."
- Break condition: If depth estimation errors are not a significant factor in matching performance, or if projecting 3D LiDAR detections to the 2D image plane introduces significant errors.

### Mechanism 3
- Claim: Score calibration and probabilistic fusion improve rare class detection by making scores from different uni-modal detectors comparable and enabling Bayesian fusion.
- Mechanism: Score calibration tunes per-class temperature parameters to make scores from LiDAR and RGB detectors comparable, and probabilistic fusion combines calibrated scores using Bayesian principles to boost confidence in matched detections.
- Core assumption: The confidence scores from different uni-modal detectors are not directly comparable, and proper score calibration is crucial for effective fusion.
- Evidence anchors:
  - [abstract]: "score calibration and probabilistic fusion notably improves the final performance further"
  - [section]: "Score calibration is crucial to fairly compare scores for fusion...Probabilistic Ensembling...We compute the final score as p(c|xRGB, xLiDAR) = p(xRGB, xLiDAR|c)p(c) / p (xRGB, xLiDAR) ∝ p(c|xRGB)p(c|xLiDAR) / p (c)"
- Break condition: If score calibration does not significantly improve rare class detection, or if probabilistic fusion does not outperform simple score averaging.

## Foundational Learning

- Concept: 3D object detection and the challenges of detecting rare classes in long-tailed distributions.
  - Why needed here: The paper addresses the problem of Long-Tailed 3D Detection (LT3D), which requires detecting both common and rare object classes in autonomous driving scenarios.
  - Quick check question: What are the main challenges of detecting rare classes in 3D object detection, and how do they differ from detecting common classes?

- Concept: Multi-modal fusion techniques for 3D object detection, including input-fusion, feature-fusion, and late-fusion.
  - Why needed here: The paper proposes a late-fusion framework that combines 2D RGB and 3D LiDAR detections, and understanding the different fusion approaches is crucial for evaluating the proposed method.
  - Quick check question: What are the key differences between input-fusion, feature-fusion, and late-fusion approaches in multi-modal 3D object detection, and what are the advantages and disadvantages of each?

- Concept: Score calibration and probabilistic fusion techniques for combining predictions from different models.
  - Why needed here: The paper uses score calibration and probabilistic fusion to combine calibrated scores from 2D RGB and 3D LiDAR detectors, and understanding these techniques is essential for implementing the proposed method.
  - Quick check question: How does score calibration work, and what are the benefits of using probabilistic fusion over simple score averaging when combining predictions from different models?

## Architecture Onboarding

- Component map: 2D RGB detector -> 3D LiDAR detector -> Score calibration -> Matching on 2D image plane -> Probabilistic fusion -> NMS
- Critical path:
  1. Train 2D RGB detector on large external datasets
  2. Train 3D LiDAR detector on the target dataset
  3. Calibrate scores of 2D RGB and 3D LiDAR detections
  4. Match 3D LiDAR detections with 2D RGB detections on the 2D image plane
  5. Fuse calibrated scores using probabilistic ensembling
  6. Apply NMS to remove duplicate detections
- Design tradeoffs:
  - Using 2D RGB detectors vs. 3D RGB detectors for fusion: 2D detectors are easier to train and can leverage larger datasets, but they lack 3D information
  - Matching on the 2D image plane vs. in the 3D BEV: Matching on the 2D image plane avoids depth estimation errors but may introduce errors from projecting 3D detections
  - Score calibration vs. no score calibration: Score calibration improves performance but adds complexity and requires tuning per-class parameters
- Failure signatures:
  - Poor performance on rare classes: This could indicate issues with the 2D RGB detector, score calibration, or fusion strategy
  - Low overall accuracy: This could indicate problems with the 3D LiDAR detector or matching strategy
  - High false positive rate: This could indicate issues with score calibration or the threshold used for matching detections
- First 3 experiments:
  1. Evaluate the performance of the 2D RGB detector and 3D LiDAR detector separately on the target dataset to establish a baseline
  2. Implement the matching module and evaluate its performance on a validation set to ensure accurate spatial alignment between 2D RGB and 3D LiDAR detections
  3. Implement the score calibration and fusion modules and evaluate their impact on rare class detection performance on a validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed late-fusion approach compare to end-to-end trained multi-modal detectors in terms of detecting rare classes, and what are the specific reasons for its superior performance?
- Basis in paper: [explicit] The paper states that "BEVFusion [34], which is an end-to-end trained multi-modal method, performs better than the LiDAR-only variant (BEVFusion-L), confirming the benefit of using multi-modal input for LT3D." However, it also mentions that "end-to-end trained multi-modal detectors still struggle to detect rare classes."
- Why unresolved: The paper provides some insights into the reasons for the superior performance of the late-fusion approach, such as the ability to leverage large-scale uni-modal datasets and the effectiveness of score calibration and probabilistic fusion. However, a detailed comparison and analysis of the specific advantages of the late-fusion approach over end-to-end methods in terms of rare class detection is not provided.
- What evidence would resolve it: A comprehensive comparison of the performance of the late-fusion approach and end-to-end trained multi-modal detectors on rare classes, along with a detailed analysis of the specific factors contributing to the superior performance of the late-fusion approach.

### Open Question 2
- Question: How does the choice of 2D RGB detector (e.g., YOLOV7 vs. DINO) impact the performance of the late-fusion approach, and what are the key factors that contribute to the differences in performance?
- Basis in paper: [explicit] The paper mentions that "2D RGB detectors achieve better recognition accuracy than 3D RGB detectors" and that "using additional data and using better 2D detectors (e.g., DINO) improves performance on the proxy task as well as the downstream late-fusion algorithm." However, a detailed analysis of the impact of different 2D RGB detectors on the performance of the late-fusion approach is not provided.
- Why unresolved: The paper provides some insights into the factors that contribute to the performance of the late-fusion approach, such as the use of additional data and the choice of 2D RGB detector. However, a comprehensive analysis of the specific impact of different 2D RGB detectors on the performance of the late-fusion approach is not provided.
- What evidence would resolve it: A detailed comparison of the performance of the late-fusion approach using different 2D RGB detectors, along with an analysis of the specific factors contributing to the differences in performance.

### Open Question 3
- Question: How does the proposed late-fusion approach perform on other long-tailed detection tasks beyond 3D object detection, and what are the potential challenges and limitations of applying the approach to these tasks?
- Basis in paper: [inferred] The paper focuses on long-tailed 3D detection (LT3D) for autonomous vehicles. While the approach is specifically designed for this task, the underlying principles of late-fusion and score calibration could potentially be applied to other long-tailed detection tasks.
- Why unresolved: The paper does not provide any information on the performance of the late-fusion approach on other long-tailed detection tasks beyond 3D object detection. The potential challenges and limitations of applying the approach to these tasks are also not discussed.
- What evidence would resolve it: An evaluation of the performance of the late-fusion approach on other long-tailed detection tasks, along with an analysis of the specific challenges and limitations encountered in these tasks.

## Limitations
- The claim that 2D RGB detectors outperform 3D RGB detectors for rare class detection lacks direct empirical validation against 3D RGB detectors on the same task.
- The effectiveness of score calibration and probabilistic fusion is demonstrated but the paper doesn't explore alternative fusion strategies or sensitivity to calibration parameters.
- The generalizability of the approach to other sensor modalities (e.g., radar) or different environmental conditions (e.g., nighttime, adverse weather) is not addressed.

## Confidence
**High Confidence:** The late-fusion architecture design and the superiority of matching on the 2D image plane over 3D BEV matching are well-supported by ablation studies showing 5.9 mAP improvement over prior work. The experimental results on both nuScenes and Argoverse 2 datasets demonstrate consistent improvements across multiple rare class detection metrics.

**Medium Confidence:** The claim that 2D RGB detectors outperform 3D RGB detectors for rare class detection is based on the paper's internal analysis and lacks direct comparison with state-of-the-art 3D RGB detectors. The effectiveness of score calibration and probabilistic fusion is demonstrated but the paper doesn't explore alternative fusion strategies or sensitivity to calibration parameters.

**Low Confidence:** The generalizability of the approach to other sensor modalities (e.g., radar) or different environmental conditions (e.g., nighttime, adverse weather) is not addressed. The computational overhead of the late-fusion approach compared to end-to-end methods is not discussed.

## Next Checks
1. **Ablation Study on Matching Strategy:** Conduct experiments comparing 2D-3D matching on the image plane versus 3D-3D matching in BEV space with varying levels of depth estimation noise to quantify the impact of depth errors on matching performance.

2. **Score Calibration Sensitivity Analysis:** Systematically vary the temperature parameters (τc) across classes and measure the impact on rare class detection performance to determine optimal calibration strategies and identify classes that benefit most from score calibration.

3. **Cross-Dataset Generalization Test:** Evaluate the late-fusion framework on a third dataset (e.g., KITTI or Waymo) to assess the approach's robustness to different sensor configurations, environmental conditions, and class distributions beyond the nuScenes and Argoverse 2 benchmarks.