---
ver: rpa2
title: 'MILPaC: A Novel Benchmark for Evaluating Translation of Legal Text to Indian
  Languages'
arxiv_id: '2310.09765'
source_url: https://arxiv.org/abs/2310.09765
tags:
- text
- indian
- translation
- languages
- legal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MILPaC, the first high-quality legal parallel
  corpus containing aligned text units in English and nine Indian languages, including
  several low-resource languages. The corpus comprises three datasets: MILPaC-IP,
  MILPaC-CCI-FAQ, and MILPaC-Acts, totaling 17,853 parallel text pairs.'
---

# MILPaC: A Novel Benchmark for Evaluating Translation of Legal Text to Indian Languages

## Quick Facts
- arXiv ID: 2310.09765
- Source URL: https://arxiv.org/abs/2310.09765
- Reference count: 17
- Key outcome: MILPaC corpus with 17,853 parallel text pairs enables evaluation of MT systems for legal text in 9 Indian languages, showing Google/Microsoft Azure and IndicTrans perform best.

## Executive Summary
This paper introduces MILPaC, the first high-quality parallel corpus for legal text translation in Indian languages, containing 17,853 aligned text pairs across English and nine Indian languages. The corpus comprises three datasets: MILPaC-IP (Intellectual Property Rights), MILPaC-CCI-FAQ (Competition Commission of India FAQs), and MILPaC-Acts (legislative acts). The authors benchmark eight machine translation systems including commercial APIs, open-source models, and LLMs, finding that commercial systems and IndicTrans perform best. Human evaluation by law students shows that automatic metrics have low correlation with human judgments for most language pairs, except English-Hindi.

## Method Summary
The paper constructs MILPaC through a multi-stage pipeline involving PDF extraction, OCR processing, manual verification, and chunking of text units. The corpus is then used to benchmark eight MT systems (Google Translate, Microsoft Azure, IndicTrans, mBART-50, OPUS, NLLB variants, and GPT-3.5 models) using automatic metrics (BLEU, GLEU, chrF++). Human evaluation is conducted by senior law students using domain-specific metrics (POM, SLU, FLY). The paper also fine-tunes IndicTrans on the MILPaC training split and demonstrates performance improvements.

## Key Results
- Commercial systems (Google Translate, Microsoft Azure) and IndicTrans achieve the best performance on MILPaC
- Fine-tuning IndicTrans on MILPaC improves translation quality substantially across all tested languages
- Automatic metrics show mostly positive but low correlation with human evaluation scores, except for English-Hindi where correlation is high
- OCR challenges and text chunking significantly impact translation quality for long legal documents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Legal text translation requires specialized corpora because general-domain corpora do not capture the formality and precision required in legal language.
- Mechanism: The paper constructs MILPaC using official government sources and Law school primers, ensuring authenticity and domain-specific vocabulary.
- Core assumption: Translations done by government officials and Law practitioners are of high quality and capture the necessary legal terminology.
- Evidence anchors: [abstract] "We develop MILPaC (Multilingual Indian Legal Parallel Corpus), the first parallel corpus of legal text in Indian languages." [section 3.4] "Reliability of the sources: For MILPaC-Acts and MILPaC-CCI-FAQ we extracted data from the official Government of India websites. The data for MILPaC-IP is published by one of India's most reputed Law Schools."

### Mechanism 2
- Claim: Fine-tuning general-domain MT models on legal corpora improves their performance on legal text translation.
- Mechanism: The paper fine-tunes IndicTrans on the MILPaC training split and achieves substantial improvements over the off-the-shelf model.
- Core assumption: Legal text has unique characteristics that can be learned through exposure to legal corpora.
- Evidence anchors: [section 5.2] "Table 10 compares the performance of the off-the-shelf IndicTrans and the fine-tuned IndicTrans-FT over the 20% test set for 4 Indian languages. For every language, we find that IndicTrans-FT gives substantial improvements over the off-the-shelf IndicTrans."

### Mechanism 3
- Claim: Human evaluation by Law practitioners provides more reliable assessment of translation quality than automatic metrics in the legal domain.
- Mechanism: The paper conducts a survey with Law students who evaluate translations using domain-specific metrics (POM, SLU, FLY).
- Core assumption: Law practitioners can identify nuances in legal language that automatic metrics cannot capture.
- Evidence anchors: [section 4.4] "We recruited senior LLB & LLM students from the Rajiv Gandhi School of Intellectual Property Law, one of the most reputed Law schools in India." [section 4.4] "We also draw insights into the agreement of automatic translation evaluation metrics (such as BLEU, GLEU, and chrF++) with the opinion of Law students."

## Foundational Learning

- Concept: OCR challenges in legal documents
  - Why needed here: Understanding OCR challenges is crucial for building the corpus, as many legal documents are scanned PDFs with poor quality.
  - Quick check question: What are the main challenges in OCR of Indian legal documents, and how did the authors address them?

- Concept: Fine-tuning vs. training from scratch
  - Why needed here: The paper demonstrates that fine-tuning existing models on legal corpora is more effective than training from scratch due to data limitations.
  - Quick check question: Why is fine-tuning preferred over training from scratch for legal MT models, and what are the key considerations?

- Concept: Domain-specific evaluation metrics
  - Why needed here: Standard MT evaluation metrics may not capture the nuances of legal translations, necessitating domain-specific metrics.
  - Quick check question: What are the domain-specific metrics used in the paper, and how do they differ from standard MT evaluation metrics?

## Architecture Onboarding

- Component map: PDF extraction -> OCR processing -> Manual verification -> Text chunking -> Parallel alignment -> MT benchmarking
- Critical path: Corpus construction → MT system benchmarking → Fine-tuning evaluation
- Design tradeoffs: Using official sources ensures quality but limits dataset size; manual verification improves quality but increases development time; human evaluation provides reliable assessment but is resource-intensive
- Failure signatures: Low correlation between automatic metrics and human scores indicates domain-specific challenges; OCR errors leading to misalignment in parallel text pairs; MT systems failing to capture legal terminology and formality
- First 3 experiments: 1) Evaluate correlation between automatic metrics and human scores for a small subset of translations; 2) Fine-tune a general-domain MT model on a small legal corpus and compare with the off-the-shelf model; 3) Test different OCR systems on a sample of legal documents to determine the best approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of automatic translation metrics (BLEU, GLEU, chrF++) correlate with human evaluations for English-to-Indian language translations beyond Hindi, particularly for languages like Bengali, Tamil, and Marathi?
- Basis in paper: [explicit] The paper states that correlations between automatic metrics and human scores are mostly positive but low for most Indian languages, with higher correlation only for English-to-Hindi translations.
- Why unresolved: The paper does not provide detailed analysis or correlation values for each individual Indian language beyond Hindi, leaving the question open for further exploration.
- What evidence would resolve it: Conducting a comprehensive survey with a larger sample size and including more Indian languages, followed by statistical analysis to determine correlation values for each language pair.

### Open Question 2
- Question: What are the specific challenges and limitations of using OCR technology for extracting text from legal documents in Indian languages, and how can these be addressed to improve the quality of the MILPaC corpus?
- Basis in paper: [explicit] The paper discusses challenges such as old fonts, italicized text, erased characters, and complex formatting affecting OCR accuracy, but does not provide solutions or detailed analysis of these issues.
- Why unresolved: The paper identifies problems but does not propose solutions or conduct experiments to test potential improvements in OCR technology.
- What evidence would resolve it: Developing and testing advanced OCR models specifically trained on legal documents in Indian languages, and comparing their performance with existing OCR systems.

### Open Question 3
- Question: How does the performance of machine translation systems differ when translating legal text from one Indian language to another compared to translating from English to Indian languages, and what factors contribute to these differences?
- Basis in paper: [explicit] The paper mentions the potential use of MILPaC for evaluating Indian-to-Indian translations but does not provide experimental results or detailed analysis of such translations.
- Why unresolved: The paper does not conduct experiments or provide data on the performance of MT systems for Indian-to-Indian translations, leaving this aspect unexplored.
- What evidence would resolve it: Conducting experiments using MILPaC to evaluate MT systems for Indian-to-Indian translations and analyzing the results to identify factors influencing performance differences.

## Limitations
- Corpus size is relatively small (17,853 pairs) compared to general-domain MT corpora, limiting coverage of legal scenarios
- Human evaluation by law students may not represent practicing lawyers' preferences and expertise
- OCR quality and text alignment issues could introduce errors that affect translation quality assessment
- Corpus focuses on specific legal domains (IPR, CCI-FAQ, Acts) without addressing other legal text types

## Confidence
**High Confidence Claims**:
- MILPaC is the first parallel corpus for legal text in Indian languages (supported by literature review)
- Fine-tuning IndicTrans on MILPaC improves performance over the off-the-shelf model (demonstrated with quantitative results)
- Automatic metrics show mostly positive but low correlation with human evaluation scores (statistically validated)

**Medium Confidence Claims**:
- Commercial systems outperform open-source and LLM systems (based on benchmark results, but preprocessing differences may affect outcomes)
- Legal text translation requires specialized corpora due to domain-specific terminology (supported by performance gaps, but not empirically proven as the only factor)
- Human evaluation by law practitioners is more reliable than automatic metrics (inferred from low metric correlation, but student evaluators may not fully represent practitioners)

**Low Confidence Claims**:
- MILPaC will enable significant advances in Indian legal MT (potential future impact, but not yet demonstrated)
- Observed MT performance patterns will generalize to other Indian languages not in MILPaC (extrapolation beyond tested languages)
- Automatic metrics will never be reliable for legal MT evaluation (strong claim based on current results, but future metric development could change this)

## Next Checks
1. Conduct a systematic audit of the MILPaC corpus to measure OCR accuracy, text alignment quality, and domain coverage through manual inspection and statistical analysis
2. Replicate the human evaluation study with practicing lawyers and judges rather than students to validate whether student preferences align with practitioner needs
3. Evaluate the best-performing MT systems on legal text from domains not represented in MILPaC (e.g., court judgments, contracts) to test generalizability of observed performance patterns