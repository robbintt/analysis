---
ver: rpa2
title: Explainable Claim Verification via Knowledge-Grounded Reasoning with Large
  Language Models
arxiv_id: '2310.05253'
source_url: https://arxiv.org/abs/2310.05253
tags:
- claim
- reasoning
- folk
- llms
- girl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FOLK, a novel approach for explainable claim
  verification using large language models (LLMs). FOLK translates claims into First-Order
  Logic (FOL) predicates to decompose them into sub-claims, then performs knowledge-grounded
  reasoning over question-answer pairs to verify each predicate and generate justifications.
---

# Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models

## Quick Facts
- arXiv ID: 2310.05253
- Source URL: https://arxiv.org/abs/2310.05253
- Authors: 
- Reference count: 20
- Key outcome: FOLK achieves up to 67.59% macro F1 score, outperforming strong baselines on 6 out of 7 tasks

## Executive Summary
This paper introduces FOLK, a novel approach for explainable claim verification using large language models (LLMs) that translates claims into First-Order Logic (FOL) predicates to decompose them into sub-claims, then performs knowledge-grounded reasoning over question-answer pairs to verify each predicate and generate justifications. The method leverages external knowledge sources like Google Search to ground LLM-generated answers in factual information, addressing the hallucination problem common in LLM-based verification systems. Experiments on three challenging datasets (HoVER, FEVEROUS, SciFact-Open) demonstrate significant performance improvements over strong baselines, achieving up to 67.59% macro F1 score while providing high-quality, human-readable explanations.

## Method Summary
FOLK is a framework that converts claims into First-Order Logic (FOL) predicates, breaking complex claims into simpler sub-claims that can be verified independently. The system generates intermediate questions from these predicates, retrieves knowledge-grounded answers using external sources like Google Search, and performs FOL-guided reasoning to evaluate each predicate against the grounded answers. This approach combines the reasoning capabilities of LLMs with factual accuracy from external knowledge sources to produce veracity predictions and explanations without requiring human-annotated evidence.

## Key Results
- FOLK outperforms strong baselines on 6 out of 7 evaluation tasks
- Achieves up to 67.59% macro F1 score on claim verification
- Demonstrates 11.30% average improvement over chain-of-thought and self-ask baselines
- Provides high-quality explanations that improve interpretability for human fact-checkers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing claims into First-Order Logic (FOL) predicates enables systematic verification by breaking complex claims into simpler sub-claims.
- Mechanism: The approach translates claims into FOL clauses consisting of conjunctive predicates, where each predicate represents a sub-claim that can be verified independently.
- Core assumption: LLMs can accurately parse natural language claims into correct FOL predicates that capture all necessary logical components.
- Evidence anchors:
  - [abstract] "FOLK leverages the in-context learning ability of LLMs to translate the claim into a First-Order-Logic (FOL) clause consisting of predicates"
  - [section 3.1] "We harness the ability of LLMs to translate textual claims into FOL clauses. This allows us to guide LLMs in breaking down claims into various sub-claims."
  - [corpus] Weak - only 1 related paper mentions "Adaptive Reasoning Trees" but doesn't discuss FOL decomposition specifically
- Break condition: If LLMs fail to correctly identify predicates or miss critical components, the verification chain breaks as subsequent reasoning steps are based on incorrect sub-claims.

### Mechanism 2
- Claim: Knowledge-grounding answers from external sources (Google Search) provides factual accuracy that compensates for LLM hallucinations.
- Mechanism: After generating questions from predicates, the system retrieves knowledge-grounded answers from Google Search to verify the generated answers against real-world truth.
- Core assumption: External knowledge sources like Google Search provide more reliable information than LLM internal knowledge, even if occasionally inaccurate.
- Evidence anchors:
  - [abstract] "FOLK controls the knowledge source of the LLMs by grounding the generated answers in real-world truth via retrieving accurate information from trustworthy external knowledge sources (e.g. Google or Wikipedia)"
  - [section 3.2] "To provide knowledge-grounded answers for the generated intermediate questions, we employ a retriever based on Google Search"
  - [corpus] Weak - no direct evidence in corpus about using Google Search specifically for claim verification grounding
- Break condition: If the external knowledge source returns incorrect or irrelevant information, the verification process will be based on faulty facts regardless of the reasoning quality.

### Mechanism 3
- Claim: FOL-guided reasoning over knowledge-grounded answers enables more accurate veracity predictions than direct LLM reasoning.
- Mechanism: The system uses the FOL predicates to guide LLMs in evaluating each predicate against knowledge-grounded answers, then combines these evaluations to make a final veracity prediction.
- Core assumption: Providing symbolic predicates as context helps LLMs perform more accurate reasoning than natural language alone.
- Evidence anchors:
  - [abstract] "FOLK performs FOL-Guided reasoning over a set of knowledge-grounded question-and-answer pairs to make veracity predictions"
  - [section 4.4] "FOLK outperforms CoT and Self-Ask baselines on all three datasets. On average, there is an 11.30% improvement"
  - [corpus] Weak - only mentions "Adaptive Reasoning Trees" but not FOL-guided reasoning specifically
- Break condition: If the reasoning chain fails at any predicate evaluation step, the final prediction will be incorrect regardless of other predicates being correctly evaluated.

## Foundational Learning

- Concept: First-Order Logic (FOL) predicates and conjunctive logic
  - Why needed here: FOL provides a systematic way to break down complex claims into verifiable sub-components and enables logical combination of verification results
  - Quick check question: If a claim is represented as P1 ∧ P2 ∧ P3, what is the truth value when P1=True, P2=False, P3=True?

- Concept: Chain-of-thought prompting and in-context learning
  - Why needed here: These techniques enable LLMs to perform multi-step reasoning and decompose complex problems without additional training
  - Quick check question: What is the key difference between chain-of-thought prompting and direct prompting approaches?

- Concept: Knowledge grounding and retrieval-augmented generation
  - Why needed here: Grounding LLM outputs in external knowledge sources addresses the hallucination problem and provides factual accuracy
  - Quick check question: How does knowledge grounding differ from simply providing more examples in the prompt?

## Architecture Onboarding

- Component map: Input layer -> FOL Decomposition -> Question Generation -> Knowledge Retrieval -> Reasoning Engine -> Output layer
- Critical path: Claim → FOL Decomposition → Question Generation → Knowledge Retrieval → Predicate Evaluation → Veracity Prediction
- Design tradeoffs:
  - Using Google Search provides reliable knowledge but adds latency and costs
  - Smaller LLMs reduce costs but may perform worse on complex claims
  - More detailed explanations improve interpretability but increase generation time
- Failure signatures:
  - Incorrect FOL predicates indicate decomposition issues
  - Mismatched questions and answers suggest generation problems
  - Inconsistent predicate evaluations reveal reasoning failures
  - Low explanation quality points to generation issues
- First 3 experiments:
  1. Test FOL decomposition accuracy on simple claims with known correct predicates
  2. Compare knowledge-grounded vs internal knowledge predictions on claims with verifiable facts
  3. Evaluate reasoning accuracy on multi-hop claims with gold evidence available

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FOLK's performance scale with increasing complexity of real-world claims beyond synthetic examples?
- Basis in paper: [inferred] from "The claims in our experiments are synthetic and can be decomposed with explicit reasoning based on the claims' syntactic structure. However, real-world claims often possess complex semantic structures, which require implicit reasoning to verify."
- Why unresolved: The paper only evaluates on synthetic claims with explicit reasoning requirements, leaving the performance on real-world claims with complex semantic structures untested.
- What evidence would resolve it: Experiments on real-world claim verification datasets with naturally occurring claims requiring implicit reasoning would provide empirical evidence.

### Open Question 2
- Question: What is the optimal balance between prompt engineering complexity and LLM size for maximizing FOLK's performance?
- Basis in paper: [inferred] from "We use stratified sampling to select 100 examples from each dataset to ensure a balanced label distribution" and "Our prompts are included in B. The number of prompts used varies between 4-6 between the datasets."
- Why unresolved: The paper uses a fixed number of prompts (4-6) without exploring how different prompt complexities or numbers affect performance across various LLM sizes.
- What evidence would resolve it: Systematic experiments varying prompt complexity and number while testing across different LLM sizes would identify optimal configurations.

### Open Question 3
- Question: How does FOLK's computational cost compare to supervised methods when scaling to large-scale claim verification tasks?
- Basis in paper: [explicit] from "FOLK has a much higher computational cost than supervised claim verification methods. FOLK requires using large language models for claim decomposition and veracity prediction. This results in around $20 per 100 examples using OpenAI API or around 7.5 hours on locally deployed llama-30B models on an 8x A5000 cluster."
- Why unresolved: The paper only provides cost estimates for 100 examples without analyzing how costs scale with dataset size or comparing to alternative approaches at scale.
- What evidence would resolve it: Cost analysis comparing FOLK to supervised methods across datasets of varying sizes, including both API and local deployment scenarios.

## Limitations
- The approach requires multiple LLM calls, limiting scalability and increasing computational costs
- Performance on real-world claims with complex semantic structures remains untested
- Manual evaluation of explanation quality may introduce subjective biases due to small sample size

## Confidence
- **High Confidence**: Performance improvements over baselines are well-supported by experimental results across three diverse datasets
- **Medium Confidence**: Explanation quality evaluation results are promising but based on limited manual assessment
- **Low Confidence**: The paper lacks detailed error analysis and extensive ablation studies to isolate component contributions

## Next Checks
1. Conduct ablation study removing knowledge-grounding component to isolate contribution of external knowledge vs internal LLM knowledge
2. Measure latency and cost per claim across different LLM sizes and knowledge source configurations to quantify practical deployment constraints
3. Perform detailed failure mode analysis on incorrectly predicted claims to identify common patterns and limitations of the FOL decomposition approach