---
ver: rpa2
title: Multi-Similarity Contrastive Learning
arxiv_id: '2307.02712'
source_url: https://arxiv.org/abs/2307.02712
tags:
- contrastive
- similarity
- learning
- tasks
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Multi-Similarity Contrastive Learning (MSCon) addresses the problem
  of contrastive learning methods relying on a single similarity metric, which can
  lead to poor generalization. The core idea is to learn robust embeddings by jointly
  utilizing supervision from multiple similarity metrics, while automatically learning
  task weights based on uncertainty.
---

# Multi-Similarity Contrastive Learning

## Quick Facts
- arXiv ID: 2307.02712
- Source URL: https://arxiv.org/abs/2307.02712
- Authors: 
- Reference count: 40
- Key outcome: MSCon outperforms state-of-the-art contrastive baselines on in-domain and out-of-domain tasks, achieving 97.17% top-1 accuracy on Zappos50k category tasks and 42.62% on out-of-domain brand classification

## Executive Summary
Multi-Similarity Contrastive Learning (MSCon) addresses the limitation of contrastive learning methods that rely on a single similarity metric, which can lead to poor generalization. The method learns robust embeddings by jointly utilizing supervision from multiple similarity metrics while automatically learning task weights based on uncertainty. MSCon constructs a pseudo-likelihood function to approximate task performance and introduces a similarity-dependent temperature parameter to model relative confidence, enabling the model to down-weight uncertain tasks during training.

## Method Summary
MSCon uses multiple projection heads to learn embeddings based on different metrics of similarity. The method constructs a pseudo-likelihood function to approximate task performance and introduces a similarity-dependent temperature parameter to model relative confidence. The network is trained using a multi-similarity contrastive loss that sums supervised contrastive losses over all similarity metrics. After training, the projection heads are discarded and a linear classifier is trained on the frozen encoder's embeddings to evaluate performance on both in-domain and out-of-domain tasks.

## Key Results
- On Zappos50k dataset: MSCon achieves 97.17% top-1 classification accuracy for category tasks and 42.62% accuracy on out-of-domain brand classification
- On MEDIC dataset: MSCon outperforms contrastive baselines on all in-domain tasks including damage severity, disaster types, humanitarian, and informative categories
- MSCon demonstrates superior generalization compared to multi-task cross-entropy methods on out-of-domain tasks

## Why This Works (Mechanism)

### Mechanism 1
Multiple similarity metrics provide complementary supervisory signals that improve generalization. By training separate projection heads for each similarity metric, the model learns embeddings that capture diverse relational structures. Each projection head specializes in distinguishing examples based on one metric while sharing a common backbone, enabling richer representation learning than single-metric approaches.

### Mechanism 2
Task uncertainty weighting improves out-of-domain generalization by focusing on more reliable similarity metrics. The method constructs a pseudo-likelihood function to approximate task performance and introduces a similarity-dependent temperature parameter (σ²) to model relative confidence. This allows the model to automatically learn weights that down-weight uncertain or noisy similarity metrics during training.

### Mechanism 3
Multi-similarity contrastive loss outperforms single-task and multi-task cross-entropy methods on both in-domain and out-of-domain tasks. By learning embeddings through multiple similarity perspectives rather than predicting labels directly, the model captures more generalizable representations that transfer better to unseen tasks. The contrastive framework inherently focuses on relative similarity relationships rather than absolute category boundaries.

## Foundational Learning

- Concept: Contrastive learning and positive/negative example mining
  - Why needed here: MSCon builds on contrastive learning principles, requiring understanding of how to construct positive and negative pairs from multiple similarity metrics
  - Quick check question: How does MSCon define positive pairs differently from traditional supervised contrastive learning?

- Concept: Multi-task learning and task weighting
  - Why needed here: The paper extends uncertainty-based task weighting from multi-task learning to the contrastive setting, requiring understanding of both paradigms
  - Quick check question: What is the key difference between how uncertainty is modeled in traditional multi-task learning versus in MSCon?

- Concept: Pseudo-likelihood estimation and temperature scaling
  - Why needed here: MSCon uses pseudo-likelihood to approximate task performance and similarity-dependent temperature to model uncertainty, requiring understanding of these statistical concepts
  - Quick check question: How does the similarity-dependent temperature parameter σ² affect the contrastive loss?

## Architecture Onboarding

- Component map: Input image -> Base encoder (ResNet18/50) -> Shared embedding -> Multiple projection heads -> Similarity spaces -> Contrastive loss -> Pseudo-likelihood estimator -> Weighting module -> Final loss

- Critical path:
  1. Input image → base encoder → shared embedding
  2. Shared embedding → multiple projection heads → similarity spaces
  3. Contrastive loss computed separately for each similarity metric
  4. Pseudo-likelihood constructed and used to estimate uncertainty
  5. σ² parameters learned to weight each similarity metric's contribution
  6. Final loss = weighted sum of all similarity-specific contrastive losses

- Design tradeoffs:
  - Number of projection heads vs. computational cost
  - Temperature τ vs. gradient stability
  - Batch size vs. quality of negative sampling
  - Embedding dimensionality vs. representation capacity

- Failure signatures:
  - All projection heads learn identical representations (no diversity)
  - σ² parameters converge to extreme values (all metrics weighted equally or completely down-weighted)
  - Performance degrades with more similarity metrics (overfitting or conflicting signals)

- First 3 experiments:
  1. Single similarity metric baseline - verify contrastive learning implementation
  2. Two similarity metrics with equal weighting - validate multi-similarity framework
  3. Two similarity metrics with uncertainty weighting - test σ² learning capability

## Open Questions the Paper Calls Out

### Open Question 1
How can non-categorical labels, such as continuous variables like heart rate or heel height, be incorporated into a multi-similarity contrastive learning framework? The paper mentions that defining positive and negative examples for continuous variables with different scales is not straightforward, suggesting a need for future work in this area.

### Open Question 2
How does MSCon perform with larger batch sizes, and what is the optimal batch size for this method? The paper mentions that previous work for SimCLR and SupCon have found that large batch sizes consistently result in better top-1 accuracy, and hypothesizes that larger batch sizes would also improve performance for MSCon loss.

### Open Question 3
How can data-dependent uncertainty be incorporated into the MSCon framework, in addition to the similarity-dependent uncertainty already considered? The paper mentions that data-dependent uncertainty is not considered in their framework and suggests it as an interesting direction for future work.

## Limitations

- The pseudo-likelihood approximation's effectiveness is not rigorously validated
- The assumption that similarity metrics are complementary may not hold for all datasets, particularly when metrics are correlated
- The paper lacks ablation studies on the temperature parameter's impact on performance

## Confidence

- High confidence in the experimental methodology and reported results
- Medium confidence in the uncertainty weighting mechanism's general applicability
- Low confidence in the claim that contrastive learning inherently produces better out-of-domain generalization without additional validation

## Next Checks

1. Implement ablation studies removing the uncertainty weighting component to quantify its contribution to performance gains
2. Test MSCon on datasets with known correlated similarity metrics to validate robustness to metric redundancy
3. Compare embedding quality using downstream task transfer learning curves to assess generalization claims beyond classification accuracy