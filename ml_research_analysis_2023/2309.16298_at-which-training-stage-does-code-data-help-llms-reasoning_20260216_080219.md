---
ver: rpa2
title: At Which Training Stage Does Code Data Help LLMs Reasoning?
arxiv_id: '2309.16298'
source_url: https://arxiv.org/abs/2309.16298
tags:
- code
- data
- reasoning
- llms
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates at which training stage introducing code
  data can help improve the reasoning capabilities of large language models (LLMs).
  The authors conduct comprehensive experiments by pre-training LLMs with pure text
  data and mixed code-text data, and fine-tuning them with pure text data and mixed
  code-text data, respectively.
---

# At Which Training Stage Does Code Data Help LLMs Reasoning?

## Quick Facts
- arXiv ID: 2309.16298
- Source URL: https://arxiv.org/abs/2309.16298
- Reference count: 23
- Pre-training with code-text mixture significantly enhances general reasoning capability across multiple domains.

## Executive Summary
This paper investigates when introducing code data improves reasoning capabilities in large language models (LLMs). Through comprehensive experiments across six reasoning tasks, the authors demonstrate that code data is most effective when added during the pre-training stage for general reasoning improvements, and during instruction-tuning for task-specific reasoning capabilities. The study introduces a dynamic mixing strategy that gradually adjusts the code-to-text ratio during instruction-tuning, achieving optimal results by starting with more code data and progressively reducing it.

## Method Summary
The authors conduct experiments using a 2.6B parameter transformer decoder (PanGu2.6B) with a query layer. They create two variants: one pre-trained on pure text data (100GB) and another (CodePanGu2.6B) on a mixture of text (100GB) and code data (50GB from CodeParrot). Both models are then instruction-tuned with either pure text data (400K examples) or mixed code-text data (150K code instructions). The evaluation spans six reasoning tasks across five domains: logical reasoning, code reasoning, legal reasoning, scientific reasoning, and analogical reasoning.

## Key Results
- Pre-training with code-text mixture significantly improves general reasoning capability across multiple domains without negative transfer
- Code data in instruction-tuning enhances task-specific reasoning capabilities, particularly for code-related tasks
- Dynamic mixing strategy (starting with higher code ratio, then reducing) outperforms uniform mixing during instruction-tuning
- Code data proves more effective than simply increasing model size (2.6B with code vs. 13B without)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Code data in pre-training stage enhances general reasoning ability without harming other tasks
- Mechanism: Code's inherent logic and reduced ambiguity provide stronger training signals for reasoning than natural language text
- Core assumption: The logical structure of code transfers to improved general reasoning patterns in the model
- Evidence anchors:
  - [abstract]: "pre-training LLMs with the mixture of code and text can significantly enhance their general reasoning capability almost without negative transfer other tasks"
  - [section 3.3.1]: Results show improved performance across reasoning tasks (Logic, JEC-QA, ScienceQA, E-KAR) after adding code data in pre-training
  - [corpus]: "code data is more logical and less ambiguous" compared to textual data

### Mechanism 2
- Claim: Code data in instruction-tuning stage improves task-specific reasoning capabilities
- Mechanism: Code instructions activate and refine the model's code reasoning abilities that were developed during pre-training
- Core assumption: The model can leverage pre-trained code reasoning patterns when fine-tuned with code-specific instructions
- Evidence anchors:
  - [abstract]: "at the instruction-tuning stage, code data endows LLMs the task-specific reasoning capability"
  - [section 3.3.2]: Fine-tuning with code instructions significantly improves code-related tasks (CosQA, MBPP) while maintaining performance on other tasks
  - [corpus]: Code instructions help the model "follow natural language instructions and generate correct code"

### Mechanism 3
- Claim: Dynamic mixing strategy enables step-by-step learning of reasoning capabilities
- Mechanism: Gradually adjusting code-to-text ratios during instruction-tuning allows the model to progressively build reasoning skills
- Core assumption: Starting with more code data activates reasoning faster, then reducing code data ratio allows refinement of general reasoning
- Evidence anchors:
  - [abstract]: "the dynamic mixing strategy of code and text data assists LLMs to learn reasoning capability step-by-step during training"
  - [section 3.3.4]: "using a higher code data ratio in the early stage and gradually reducing the code data ratio in the later stage achieved the best results"
  - [corpus]: Stepwise decrease strategy improved code question answering and generation tasks

## Foundational Learning

- Concept: Pre-training vs. Instruction-tuning distinction
  - Why needed here: The paper's core contribution is understanding at which stage code data helps reasoning, requiring clear understanding of these two training phases
  - Quick check question: What is the fundamental difference between pre-training and instruction-tuning in terms of data and objectives?

- Concept: Chain-of-Thought reasoning
  - Why needed here: The paper demonstrates CoT improves reasoning performance, showing how code data might implicitly teach reasoning patterns
  - Quick check question: How does Chain-of-Thought prompting differ from standard prompting, and why does it improve complex reasoning?

- Concept: Negative transfer
  - Why needed here: The paper claims adding code data doesn't cause negative transfer, making this concept critical for interpreting results
  - Quick check question: What is negative transfer in the context of multi-task learning, and how would you detect it in this experimental setup?

## Architecture Onboarding

- Component map:
  - Data pipeline: Text data → Code data → Mixed data → Instruction tuning data
  - Model architecture: 32-layer transformer decoder with query layer
  - Training stages: Pre-training (self-supervised) → Instruction-tuning (supervised)
  - Evaluation pipeline: 6 reasoning tasks across 5 domains

- Critical path:
  1. Pre-train base model (PanGu2.6B) on text data
  2. Pre-train CodePanGu2.6B on mixed code-text data
  3. Instruction-tune both models with text-only data
  4. Instruction-tune both models with mixed code-text data
  5. Evaluate all variants on reasoning tasks

- Design tradeoffs:
  - Model size vs. performance: 13B model vs. 2.6B with code data shows code data can be more effective than scale
  - Data diversity vs. focus: Mixed data improves general reasoning but may reduce performance on some tasks
  - Pre-training vs. fine-tuning: Code data is more effective in pre-training for general reasoning

- Failure signatures:
  - Performance degradation on reasoning tasks after adding code data
  - Code data causing confusion in non-code tasks (observed in duReader)
  - Inconsistent results across different reasoning task types

- First 3 experiments:
  1. Replicate pre-training results: Train PanGu2.6B vs. CodePanGu2.6B and compare on Logic, JEC-QA, ScienceQA tasks
  2. Test instruction-tuning impact: Fine-tune both models with code-instructions vs. text-only and measure CosQA/MBPP performance
  3. Validate mixing strategy: Implement uniform vs. stepwise mixing during instruction-tuning and measure code task performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of code data in improving reasoning capabilities generalize to other LLM architectures beyond the GPT paradigm?
- Basis in paper: [inferred] The paper uses GPT-based models (PanGu variants) and discusses potential future exploration of different architectures
- Why unresolved: The study only tested GPT-based models. Other architectures like BERT, T5, or encoder-decoder models may respond differently to code data
- What evidence would resolve it: Experiments testing the same training stage hypotheses on BERT, T5, or other non-GPT LLM architectures

### Open Question 2
- Question: What is the optimal ratio of code to text data for maximizing reasoning capabilities across different reasoning domains?
- Basis in paper: [explicit] The paper tests different mixing strategies but leaves optimal ratios for future work
- Why unresolved: Only tested a limited set of ratios (5:3, 7:3, 5:5, 6:4). The optimal ratio may vary by task type or reasoning domain
- What evidence would resolve it: Systematic experiments testing a wider range of ratios (e.g., 1:1, 3:7, 1:3) across all reasoning domains

### Open Question 3
- Question: How does the quality and complexity of code data affect the transfer of reasoning capabilities to non-code tasks?
- Basis in paper: [inferred] The paper uses CodeParrot dataset but doesn't analyze how code quality affects reasoning transfer
- Why unresolved: The study doesn't differentiate between simple and complex code or measure code data quality's impact on reasoning
- What evidence would resolve it: Experiments using code datasets with varying complexity levels and quality metrics, comparing reasoning performance across tasks

## Limitations

- The study uses relatively modest model sizes (2.6B parameters) and dataset scales compared to state-of-the-art LLMs
- Evaluation focuses on six reasoning tasks that may not comprehensively represent all reasoning capabilities
- The mechanism by which code's logical structure transfers to general reasoning remains underspecified

## Confidence

**High Confidence**: Code data in pre-training improves general reasoning capability across multiple domains; Dynamic mixing strategy outperforms uniform mixing during instruction-tuning

**Medium Confidence**: Code data in instruction-tuning improves task-specific reasoning; Code data doesn't cause significant negative transfer

**Low Confidence**: The optimal mixing ratio schedule is generalizable across different model architectures; The improvements are primarily due to code's logical structure rather than other factors

## Next Checks

1. **Scale Validation**: Replicate the experiments using larger model architectures (13B vs. 2.6B) to verify whether the observed effects of code data on reasoning capabilities scale proportionally with model size

2. **Mechanism Isolation**: Design controlled experiments that isolate the contribution of code's logical structure from other factors by comparing code data with other logically structured non-code data

3. **Long-Horizon Reasoning Test**: Evaluate the models on more complex, multi-step reasoning tasks that require sustained logical inference over longer contexts to test whether the claimed general reasoning improvements extend to more demanding reasoning scenarios