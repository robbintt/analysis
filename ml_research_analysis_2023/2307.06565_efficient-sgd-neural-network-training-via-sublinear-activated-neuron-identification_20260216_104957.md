---
ver: rpa2
title: Efficient SGD Neural Network Training via Sublinear Activated Neuron Identification
arxiv_id: '2307.06565'
source_url: https://arxiv.org/abs/2307.06565
tags:
- neural
- nition
- function
- time
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents an efficient SGD-based training algorithm for
  neural networks with shifted ReLU activation, achieving sublinear per-iteration
  time complexity in the network size. The key idea is to leverage the sparsity of
  activated neurons in each iteration, using a half-space reporting data structure
  to identify and update only the relevant neurons.
---

# Efficient SGD Neural Network Training via Sublinear Activated Neuron Identification

## Quick Facts
- arXiv ID: 2307.06565
- Source URL: https://arxiv.org/abs/2307.06565
- Reference count: 40
- Primary result: Achieves sublinear per-iteration time complexity of Õ(m^(1-Θ(1/d))bd) for SGD neural network training

## Executive Summary
This paper presents an SGD-based training algorithm for neural networks with shifted ReLU activation that achieves sublinear per-iteration time complexity by identifying and updating only the activated neurons. The key innovation leverages the sparsity of neuron activations, using a half-space reporting data structure to query which neurons are activated by each input in a batch. The algorithm converges in O(M²/ε²) time, where M is the coefficient norm upper bound and ε is the error term, with network size quadratic in these parameters.

## Method Summary
The algorithm uses a static half-space reporting data structure to efficiently identify activated neurons in sublinear time. It initializes a two-layer neural network with shifted ReLU activation and processes SGD updates by first querying the data structure to identify activated neurons for each batch input, then computing gradients and updating only those neurons. The approach relies on the sparsity of activated neurons per input and the equivalence between neural network training and neural tangent kernel (NTK) training for large enough initialization.

## Key Results
- Sublinear per-iteration time complexity of Õ(m^(1-Θ(1/d))bd)
- Convergence in O(M²/ε²) time with network size quadratic in M and ε
- Only O(m^(4/5)) neurons activated per input with high probability
- Amortized update time of O(log²(2m)) per neuron update

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sublinear per-iteration time is achieved by identifying only activated neurons via geometric search
- Mechanism: The algorithm uses a half-space reporting data structure to query which neurons are activated by each input in a batch. Since the number of activated neurons per input is sublinear in m, the query time is sublinear in m
- Core assumption: The shifted ReLU activation function creates sparsity such that at any iteration, only O(m^(4/5)) neurons are activated per input with high probability
- Evidence anchors:
  - [abstract]: "The key idea is to leverage the sparsity of activated neurons in each iteration, using a half-space reporting data structure to identify and update only the relevant neurons"
  - [section 6.1]: "Lemma 6.6 shows that the number of fired neurons ki,t is small for all i"

### Mechanism 2
- Claim: The algorithm maintains theoretical convergence guarantees despite only updating activated neurons
- Mechanism: The approach leverages the equivalence between neural network training and neural tangent kernel (NTK) training for sufficiently large initialization. This equivalence ensures that the gradient updates remain accurate even when computed using only activated neurons
- Core assumption: The network is initialized with sufficient width to ensure NTK equivalence, and the learning rate is appropriately scaled
- Evidence anchors:
  - [section 2.2]: "When the number of neurons m is large enough, the gradient descent (GD) dynamics of the neural network and its NTK approximation are equivalent"
  - [section 6.1]: "The algorithm maintains convergence properties by leveraging the NTK equivalence"

## Foundational Learning
- This paper builds on prior work showing that neural network training can be approximated by NTK dynamics when sufficiently wide
- The key novelty is applying this insight to create a sublinear-time SGD algorithm by exploiting the sparsity of ReLU activations
- The work connects computational complexity theory with practical neural network optimization

## Architecture Onboarding
- Assumes a two-layer neural network with shifted ReLU activation function
- Requires a half-space reporting data structure for efficient activated neuron identification
- Relies on NTK equivalence, which requires sufficiently large network initialization
- The algorithm processes SGD updates in two phases: identification of activated neurons followed by gradient computation and update

## Open Questions the Paper Calls Out
- Can the approach be extended to deeper neural networks with shifted ReLU or other activation functions?
- How does the method perform in practice compared to standard SGD, given the theoretical nature of the current results
- What are the implications for distributed training when the computational graph is sparse?
- Can the half-space reporting data structure be optimized further for specific problem domains?

## Limitations
- The analysis is theoretical and requires very large network initialization for NTK equivalence
- The approach is specific to shifted ReLU activation and may not generalize to other activation functions
- The half-space reporting data structure adds complexity and may have practical implementation challenges
- The theoretical guarantees may not translate directly to practical performance gains

## Confidence
- Theoretical foundation is well-established with clear connections to NTK literature
- Mathematical proofs appear rigorous but are highly specialized
- Practical applicability remains to be validated through experiments
- The sparsity assumption for ReLU activations is reasonable but may vary across problem domains

## Next Checks
- Verify the mathematical proofs in sections 5 and 6 for correctness
- Examine the construction and complexity analysis of the half-space reporting data structure
- Check the assumptions about network width for NTK equivalence
- Review the bounds on activated neuron sparsity across different data distributions