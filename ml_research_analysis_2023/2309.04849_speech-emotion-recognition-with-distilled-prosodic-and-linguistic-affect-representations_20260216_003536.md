---
ver: rpa2
title: Speech Emotion Recognition with Distilled Prosodic and Linguistic Affect Representations
arxiv_id: '2309.04849'
source_url: https://arxiv.org/abs/2309.04849
tags:
- speech
- prosodic
- linguistic
- emotion
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce EmoDistill, a novel cross-modal knowledge distillation
  framework for learning emotion representations from speech. EmoDistill explicitly
  captures linguistic and prosodic aspects of emotions in a unimodal inference setup,
  reducing computational overhead and limitations like transcription and prosodic
  feature extraction errors.
---

# Speech Emotion Recognition with Distilled Prosodic and Linguistic Affect Representations

## Quick Facts
- arXiv ID: 2309.04849
- Source URL: https://arxiv.org/abs/2309.04849
- Reference count: 0
- Key outcome: EmoDistill achieves 77.49% WA and 78.91% UA on IEMOCAP, outperforming state-of-the-art by 7.26% and 4.99% respectively.

## Executive Summary
This paper introduces EmoDistill, a cross-modal knowledge distillation framework for speech emotion recognition that captures both prosodic and linguistic aspects of emotions in a single unimodal model. By distilling information from pre-trained prosodic and linguistic teacher models at both embedding and logit levels, EmoDistill achieves state-of-the-art performance on the IEMOCAP benchmark without requiring transcriptions or explicit prosodic feature extraction at inference time. The method demonstrates significant improvements over previous approaches while reducing computational overhead through its unimodal inference setup.

## Method Summary
EmoDistill uses a student model based on HuBERT encoder with disjoint linguistic and prosodic projection layers that are concatenated before classification. The framework employs two frozen teacher models: a BERT-base model for linguistic features and a ResNet model operating on eGeMAPs prosodic features. Knowledge is transferred through temperature-scaled KL-divergence at the logit level and cosine similarity at the embedding level, combined with cross-entropy loss. The student is trained to learn both emotion-relevant representations without requiring multimodal input during inference.

## Key Results
- EmoDistill achieves 77.49% weighted accuracy and 78.91% unweighted accuracy on IEMOCAP
- This represents 7.26% and 4.99% improvements over previous state-of-the-art methods
- Ablation studies show that both linguistic and prosodic teachers contribute significantly, with 5-25% performance drops when either is removed

## Why This Works (Mechanism)

### Mechanism 1
Cross-modal knowledge distillation enables the student model to learn both linguistic and prosodic emotion representations without needing transcriptions or explicit prosodic features at inference time. The student model receives distilled knowledge at both embedding and logit levels from two frozen teacher models—one trained on linguistic features (BERT-base) and one on prosodic features (eGeMAPs + ResNet). This dual-teacher setup transfers complementary emotion-related information into a single unimodal speech model.

### Mechanism 2
Temperature scaling of logits during distillation controls the strength of supervision signals from each teacher. The Prosodic teacher uses a lower temperature (τP=0.5) producing hard logits, while the Linguistic teacher uses a higher temperature (τL=4) producing soft logits. This asymmetry aligns with the relative strength of each teacher—hard logits from the weaker Prosodic teacher provide stronger signals, while soft logits from the stronger Linguistic teacher avoid over-constraining the student.

### Mechanism 3
Disjoint embedding pathways for linguistic and prosodic projections prevent interference during distillation. The student model contains separate GELU-activated feedforward layers that produce disjoint linguistic and prosodic embeddings before concatenation. This architecture allows each teacher to supervise its corresponding embedding pathway without cross-contamination.

## Foundational Learning

**Knowledge Distillation (KD)**: KD allows transferring emotion-relevant knowledge from pre-trained, modality-specific teachers into a single unimodal student, avoiding the need for multimodal inference. Quick check: What are the two main forms of knowledge transfer used in KD, and how do they differ in what they convey to the student?

**Temperature scaling in KD**: Temperature controls the smoothness of the teacher's output distribution, influencing how strongly the student is guided during learning, especially when teachers have different prediction qualities. Quick check: How does increasing temperature affect the sharpness of a probability distribution, and why might this matter when distilling from a weaker teacher?

**Cross-modal learning**: Emotion in speech arises from both linguistic content and prosodic cues; cross-modal distillation enables the student to learn both without requiring multimodal input at inference. Quick check: Why might directly learning linguistic and prosodic features from raw audio be less effective than distilling from pre-trained teachers?

## Architecture Onboarding

**Component map**: Raw speech → HuBERT encoder → disjoint GELU FF layers (linguistic + prosodic embeddings) → concatenation → FF output → prediction

**Critical path**: Raw speech → HuBERT → disjoint embeddings → concatenated → prediction; distillation losses from both teachers guide learning

**Design tradeoffs**: Disjoint embeddings vs. shared backbone: Disjoint reduces interference but may limit fusion; shared could allow better integration but risk crosstalk. Two-teacher vs. one-teacher: Two teachers capture complementary cues but increase complexity and training cost. Temperature asymmetry: Tailored supervision but requires careful tuning; symmetric might be simpler but less effective.

**Failure signatures**: Performance drops sharply if either teacher is removed (ablation shows 5-25% drop), indicating both modalities are important. Over-smoothing from high temperature may weaken supervision signals. If teachers are too weak, distillation may propagate errors rather than useful knowledge.

**First 3 experiments**: 1) Train student with only cross-entropy loss (no distillation) to establish baseline. 2) Add logit-level KD from both teachers with symmetric temperature to test effect of distillation. 3) Introduce disjoint embeddings and asymmetric temperatures, measure improvement over previous step.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unexplored based on the limitations of the current work.

## Limitations
- The quality and relative strengths of the pre-trained teacher models are critical, but standalone teacher accuracies are not reported
- The choice of asymmetric temperature scaling is based on ablation without theoretical justification
- The disjoint embedding pathway may limit the model's ability to learn fused representations that could be more robust

## Confidence

**High confidence**: The ablation studies and reported benchmark results (77.49% WA, 78.91% UA on IEMOCAP) are internally consistent and support the main claim that EmoDistill outperforms prior methods.

**Medium confidence**: The mechanism of cross-modal distillation is plausible given the complementary nature of linguistic and prosodic cues in emotion, but the extent of its advantage over simpler fusion approaches is not rigorously demonstrated.

**Low confidence**: The specific architectural choices (disjoint embeddings, temperature asymmetry) are justified mainly by ablation results; there is no direct evidence these are optimal or necessary beyond the reported experiments.

## Next Checks
1. Report standalone accuracy (WA/UA) of the prosodic and linguistic teachers on IEMOCAP to quantify the quality of knowledge being distilled.
2. Replace the disjoint embedding pathway with a shared representation and compare performance to assess whether separation is truly beneficial.
3. Systematically vary τP and τL across a wider range and plot accuracy curves to determine if the reported values are robust or dataset-specific.