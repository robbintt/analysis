---
ver: rpa2
title: Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal
  Perception
arxiv_id: '2305.06324'
source_url: https://arxiv.org/abs/2305.06324
tags:
- training
- video
- each
- image
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Integrated Multimodal Perception (IMP), a
  scalable multimodal multi-task training approach that combines Alternating Gradient
  Descent (AGD) and Mixture-of-Experts (MoE). The key idea is to use a single modality-agnostic
  Transformer encoder with minimal modality-specific components, alternating gradient
  descent updates on diverse modalities, loss functions, and tasks while varying input
  resolutions.
---

# Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception

## Quick Facts
- **arXiv ID**: 2305.06324
- **Source URL**: https://arxiv.org/abs/2305.06324
- **Reference count**: 25
- **Key outcome**: 77.0% zero-shot accuracy on Kinetics-400, 76.8% on Kinetics-600, and 68.3% on Kinetics-700

## Executive Summary
This paper introduces Integrated Multimodal Perception (IMP), a scalable multimodal multi-task training approach that combines Alternating Gradient Descent (AGD) and Mixture-of-Experts (MoE). The method uses a single modality-agnostic Transformer encoder with minimal modality-specific components, alternating gradient descent updates on diverse modalities, loss functions, and tasks while varying input resolutions. IMP achieves state-of-the-art results on zero-shot video classification while using only 15% of the training computational cost of previous methods.

## Method Summary
IMP is a multimodal multi-task learning framework that uses Alternating Gradient Descent (AGD) and Mixture-of-Experts (MoE) to efficiently train a single modality-agnostic Transformer encoder on diverse data sources including images, videos, text, and audio. The method alternates between different objectives (contrastive and classification) and tasks, using multi-resolution training and drop tokens to reduce computational cost. MoE routing is applied to all input tokens regardless of modality, with expert-choice routing providing better load balancing than token-choice alternatives.

## Key Results
- Achieves 77.0% zero-shot accuracy on Kinetics-400, improving previous state-of-the-art by +5%
- 76.8% zero-shot accuracy on Kinetics-600, improving previous state-of-the-art by +6.7%
- 68.3% zero-shot accuracy on Kinetics-700, improving previous state-of-the-art by +5.8%
- Uses only 15% of the training computational cost of previous methods
- Outperforms dense models with modality-specific encoders and fusion layers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Alternating Gradient Descent (AGD) enables efficient integration of heterogeneous multimodal tasks without memory overhead.
- Mechanism: AGD executes one task at a time, compiling and caching separate computation graphs for each unique task structure, avoiding padding/masking in mixed batching and reducing memory overhead.
- Core assumption: Each individual task's optimization step is convex, ensuring convergence when alternating between tasks.
- Evidence anchors: Abstract mentions AGD for efficient model & task scaling; section describes AGD implementation; corpus papers don't discuss AGD in detail.

### Mechanism 2
- Claim: MoE on a single modality-agnostic encoder outperforms dense models with modality-specific encoders.
- Mechanism: MoE routing functions are applied to all input tokens regardless of modality, allowing experts to be allocated to multiple modalities if beneficial.
- Core assumption: Expert-choice routing provides better load balancing across modalities than token-choice routing.
- Evidence anchors: Abstract states MoE substantially improves performance; section describes MoE routing implementation; corpus papers don't discuss MoE routing strategies.

### Mechanism 3
- Claim: Multi-resolution training improves efficiency and convergence without accuracy loss.
- Mechanism: Trading off different dimensions (batch size, resolution, sequence length) while keeping total tokens per batch constant enables efficient video training. DropToken reduces computational cost by removing redundant information.
- Core assumption: Vision patches, especially video, contain redundant information that can be dropped without significant accuracy loss.
- Evidence anchors: Section describes DropToken implementation; section describes multi-resolution encoding technique; corpus papers don't discuss DropToken or multi-resolution strategies.

## Foundational Learning

- Concept: Alternating Gradient Descent optimization theory
  - Why needed here: Understanding why alternating between convex optimization steps ensures convergence
  - Quick check question: What is the key difference between AGD and standard SGD in terms of loss function handling?

- Concept: Mixture-of-Experts routing mechanisms
  - Why needed here: Understanding how expert-choice vs token-choice routing affects performance and load balancing
  - Quick check question: How does expert-choice routing differ from token-choice routing in terms of token allocation?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Understanding how positional encodings work for different dimensionalities and why QK LayerNorm is needed for MoE stability
  - Quick check question: Why is dilated positional encoding necessary for multi-resolution vision inputs?

## Architecture Onboarding

- Component map: Embedder -> MoE Encoder -> Heads -> Loss computation -> Backpropagation
- Critical path: Data → Embedder → MoE Encoder → Heads → Loss computation → Backpropagation
- Design tradeoffs:
  - Single encoder vs multi-encoder: Parameter efficiency vs specialized processing
  - Expert-choice vs token-choice routing: Load balancing vs expert specialization
  - Alternating vs summed objectives: Memory efficiency vs simultaneous optimization
- Failure signatures:
  - Loss plateau at early training: Poor data mixture or missing QK LayerNorm
  - Memory overflow: Too many unique task structures or insufficient rematerialization
  - Degraded performance on specific modalities: Imbalanced expert allocation
- First 3 experiments:
  1. Implement AGD with two simple tasks (image classification + image-text retrieval) to verify compilation and caching
  2. Add MoE to the encoder and compare expert-choice vs token-choice routing on the same two tasks
  3. Implement multi-resolution training with varying batch sizes and resolutions on video tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between contrastive and classification objectives when training multimodal models, and how does this balance vary across different downstream tasks and datasets?
- Basis in paper: [explicit] The paper explicitly states that combining contrastive (NCE) and classification (softmax) objectives is better than optimizing on them individually, and that alternating between the objectives is better than non-AGD objective mixing.
- Why unresolved: The paper provides empirical evidence for the benefits of combining objectives and alternating between them, but does not determine the optimal balance or provide a principled method for setting this balance.
- What evidence would resolve it: Systematic experiments varying the relative weighting of contrastive and classification objectives across a wide range of tasks and datasets, coupled with theoretical analysis of the optimization landscape.

### Open Question 2
- Question: How does the choice of routing strategy (experts-choose vs. tokens-choose) in MoE models affect performance, and what factors determine the optimal routing strategy for different modalities and tasks?
- Basis in paper: [explicit] The paper explicitly states that experts-choose routing provides superior performance compared to tokens-choose routing, especially for a single tower encoder.
- Why unresolved: The paper provides empirical evidence for the benefits of experts-choose routing, but does not provide a comprehensive comparison across different modalities, tasks, and model sizes.
- What evidence would resolve it: Extensive experiments comparing experts-choose and tokens-choose routing across a wide range of modalities, tasks, and model configurations.

### Open Question 3
- Question: What are the most effective methods for scheduling tasks and datasets during training, and how do these methods impact the performance and efficiency of multimodal models?
- Basis in paper: [inferred] The paper mentions that the sampling function can incorporate state from the optimization process itself, such as the loss for each task in the data mixture.
- Why unresolved: The paper uses a proportionally weighted sampling algorithm, but does not explore more sophisticated curriculum learning methods or investigate the impact of different scheduling strategies.
- What evidence would resolve it: Systematic experiments comparing different task scheduling strategies across a wide range of tasks and datasets, coupled with analysis of the optimization dynamics and convergence behavior.

## Limitations
- Convergence guarantees for AGD assume individual tasks are convex, which may not hold for complex multimodal objectives
- Expert-choice MoE routing strategy lacks comparative analysis against token-choice routing on diverse datasets
- Multi-resolution training effectiveness depends on data redundancy assumptions that may not generalize to all modalities
- The 15% computational cost claim is relative to unspecified baselines, making absolute efficiency difficult to assess

## Confidence
- **High Confidence**: Zero-shot classification performance metrics on Kinetics datasets (77.0%, 76.8%, 68.3%)
- **Medium Confidence**: The superiority of alternating objectives over summed objectives
- **Medium Confidence**: MoE with single encoder outperforming modality-specific encoders
- **Low Confidence**: Generalization to non-video multimodal tasks

## Next Checks
1. **Convergence Verification**: Test AGD convergence properties on individual tasks by measuring training stability and final loss values for both convex (e.g., linear regression) and non-convex (e.g., deep classification) tasks in the alternating schedule.
2. **Routing Strategy Comparison**: Implement and compare expert-choice versus token-choice MoE routing on the same IMP architecture across multiple datasets to quantify load balancing differences and their impact on performance.
3. **Multi-Resolution Ablation**: Systematically vary the drop token ratio and resolution combinations to identify the optimal tradeoff between computational efficiency and accuracy, testing whether the claimed redundancy assumptions hold across different modalities.