---
ver: rpa2
title: How much data do I need? A case study on medical data
arxiv_id: '2311.15331'
source_url: https://arxiv.org/abs/2311.15331
tags:
- dataset
- data
- datasets
- chest
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the widely held assumptions that "more data
  leads to better results" and "transfer learning improves performance" in deep learning
  for medical data. We analyze six medical and six general datasets, training a ResNet18
  network on varying subsets to assess the impact of data volume and transfer learning.
---

# How much data do I need? A case study on medical data

## Quick Facts
- arXiv ID: 2311.15331
- Source URL: https://arxiv.org/abs/2311.15331
- Reference count: 40
- Primary result: More data generally improves accuracy but shows diminishing returns after ~20-30% of dataset; transfer learning effectiveness depends critically on dataset similarity

## Executive Summary
This study systematically investigates the widely held assumptions that "more data leads to better results" and "transfer learning improves performance" in deep learning for medical imaging. By training ResNet18 networks on varying subsets of six medical and six general datasets, the research reveals a hockey-stick relationship between dataset size and accuracy, where performance plateaus after approximately 20-30% of available data. The study also demonstrates that transfer learning effectiveness is highly dependent on source dataset selection, with seemingly similar datasets sometimes performing worse than dissimilar ones.

## Method Summary
The researchers trained ResNet18 networks on 12 datasets (6 medical: Chest, Covid, Tuberculosis, Kidney, Breast, Mura; 6 general: ImageNet, Imagenette, Natural, Imagewoof, Animal, Pet) with varying data proportions from 5% to 100% in 5% increments. Each experiment was repeated 20 times. They evaluated single-stage and multi-stage transfer learning approaches with different parameter freezing strategies (PG0, PG1, PG2). The study systematically compared accuracy across different dataset combinations and transfer learning configurations to identify patterns in data volume requirements and transfer learning effectiveness.

## Key Results
- Dataset size shows hockey-stick relationship: rapid accuracy improvements below 20% dataset size, then diminishing returns beyond 20-30%
- Transfer learning effectiveness depends critically on dataset similarity, with some similar datasets performing worse than dissimilar ones
- Multi-stage transfer learning shows complex relationships with no clear ordering rule, achieving best results with ImageNet→Imagenette→Covid→Chest sequence
- Best accuracy achieved with traditional transfer learning for smaller datasets and three-bridge transfer learning for larger datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: There exists a "hockey stick" relationship between dataset size and model accuracy for medical image datasets
- Mechanism: Accuracy improves rapidly with small dataset sizes (5-20%) then plateaus with diminishing returns beyond ~20-30% of dataset
- Core assumption: The ResNet18 architecture learns most relevant features early in training, and additional data provides marginal benefit
- Evidence anchors:
  - [abstract] "more data generally improves accuracy, there is a point of diminishing returns after approximately 20-30% of the dataset"
  - [section] "Interestingly all datasets apart from breast show a hockey-stick shape where below about 20% the accuracy improves rapidly but after ~20% the accuracy only improves at a significantly reduced rate"
- Break condition: If dataset quality is very low or classes are extremely imbalanced, the elbow point might shift earlier or disappear entirely

### Mechanism 2
- Claim: Transfer Learning effectiveness depends critically on dataset similarity, not just domain
- Mechanism: Even seemingly similar datasets (like different chest X-ray datasets) may have incompatible feature distributions that harm transfer
- Core assumption: Low-level feature extraction learned from one medical domain may not generalize to another despite apparent similarity
- Evidence anchors:
  - [abstract] "an incorrect choice of dataset for transfer learning can lead to worse performance, with datasets which we would consider highly similar to the Chest dataset giving worse results than datasets which are more dissimilar"
  - [section] "tuberculosis – another dataset of chest X-Ray images – fails to achieve better results than Chest on its own"
- Break condition: If source and target domains share very specific diagnostic features, similarity may become beneficial

### Mechanism 3
- Claim: Multi-stage Transfer Learning doesn't follow a simple general→specific progression rule
- Mechanism: The optimal ordering of datasets for multi-stage transfer depends on complex feature compatibility relationships
- Core assumption: Feature transferability between datasets is non-monotonic and depends on specific architectural interactions
- Evidence anchors:
  - [abstract] "multi-stage transfer learning shows complex relationships between datasets, with no clear rule for optimal dataset ordering"
  - [section] "Unlike the two-Bridge TL case there is no apparent pattern towards ImageNet → general → similar → Chest over ImageNet → similar → general → Chest"
- Break condition: If datasets are extremely similar or the model architecture is highly constrained, a simple ordering might emerge

## Foundational Learning

- Concept: Diminishing returns in machine learning model performance
  - Why needed here: Understanding when additional data collection stops being cost-effective
  - Quick check question: If a model's accuracy improves from 80% to 85% when doubling the training data, but only from 85% to 86% when quadrupling it, what pattern are we seeing?

- Concept: Transfer learning parameter freezing strategies (PG0, PG1, PG2)
  - Why needed here: Different freezing strategies yield significantly different results in transfer learning experiments
  - Quick check question: If you only unfreeze the fully connected layers (PG2) during transfer learning, which parts of the network remain fixed from the source dataset?

- Concept: Dataset similarity vs. domain similarity
  - Why needed here: The paper shows that apparent domain similarity doesn't guarantee transfer learning success
  - Quick check question: Can you think of two medical imaging datasets that are both "chest X-rays" but might have very different feature distributions?

## Architecture Onboarding

- Component map: ResNet18 backbone -> Feature extraction layers -> Fully connected layers -> Output classification
- Critical path: Data loading -> Model initialization (pretrained or random) -> Training loop (with freezing strategy) -> Evaluation
- Design tradeoffs: ResNet18 chosen for consistency despite potentially lower accuracy than larger models; simpler to analyze patterns
- Failure signatures: Accuracy dropping with increased data volume suggests overfitting; worse performance with transfer learning suggests incompatible source domain
- First 3 experiments:
  1. Train ResNet18 on 5% of Chest dataset, then on 100%, compare accuracy curves to identify elbow point
  2. Apply PG2 transfer learning from ImageNet to Chest dataset at various data volumes (5-100%)
  3. Test single-stage transfer from Covid dataset to Chest dataset, then from Tuberculosis to Chest dataset, compare results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal dataset size threshold where diminishing returns on model accuracy become significant for deep learning models in medical imaging tasks?
- Basis in paper: [explicit] The paper identifies a "hockey-stick" shape in accuracy curves where improvements plateau after approximately 20-30% of the dataset, suggesting a law of diminishing returns beyond this point.
- Why unresolved: The exact threshold likely varies by dataset complexity, task type, and model architecture. The paper only provides approximate ranges (20-30%) without precise quantification.
- What evidence would resolve it: Systematic experiments across diverse medical imaging tasks varying dataset sizes and measuring the exact point where accuracy gains per additional data point become minimal.

### Open Question 2
- Question: How can we quantitatively measure and predict dataset similarity for transfer learning purposes, beyond human intuition?
- Basis in paper: [explicit] The paper states "it is an open research question as to how similarity is defined for Deep Learning" and finds that seemingly similar datasets (like tuberculosis vs chest X-rays) don't always perform well for transfer learning.
- Why unresolved: Current similarity measures are based on domain expertise and visual inspection, but neural networks may extract different features than humans expect. The paper demonstrates that apparent similarity doesn't guarantee transfer learning success.
- What evidence would resolve it: Development of quantitative similarity metrics that correlate with transfer learning success rates, validated across multiple domain pairs and model architectures.

### Open Question 3
- Question: What is the optimal ordering strategy for multi-stage transfer learning when datasets have varying degrees of similarity to the target domain?
- Basis in paper: [explicit] The paper finds no clear pattern in whether general→similar→target or similar→general→target ordering works better, and that the highest accuracies came from ImageNet→Imagenette→Covid→Chest.
- Why unresolved: The paper shows complex, non-intuitive relationships between dataset ordering and performance, with no consistent rule emerging from their experiments.
- What evidence would resolve it: Systematic evaluation of all possible ordering strategies across multiple domain combinations, identifying patterns or developing algorithms to predict optimal ordering based on dataset characteristics.

## Limitations

- Findings based on ResNet18 architecture may not generalize to deeper or more complex models
- Analysis focuses primarily on accuracy, potentially missing other important factors like computational efficiency
- Study uses a relatively small number of datasets (12 total), which may limit generalizability

## Confidence

- **High Confidence**: The hockey stick relationship between dataset size and accuracy is well-supported by consistent patterns across multiple datasets
- **Medium Confidence**: The finding that transfer learning effectiveness depends critically on dataset similarity is supported but could benefit from more systematic similarity measurements
- **Low Confidence**: The claim about complex relationships in multi-stage transfer learning lacks strong supporting evidence, with only limited experimental data presented

## Next Checks

1. **Architectural Generalization**: Test the identified patterns (hockey stick relationship, transfer learning effectiveness) on larger architectures like ResNet50 or EfficientNet to verify if the patterns hold

2. **Dataset Similarity Quantification**: Implement systematic measures of dataset similarity (e.g., feature distribution overlap, domain adaptation metrics) to better predict transfer learning success

3. **Performance Metric Expansion**: Evaluate models using additional metrics beyond accuracy (precision, recall, F1-score, computational efficiency) to ensure the findings hold across different evaluation criteria