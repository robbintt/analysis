---
ver: rpa2
title: Efficient Cross-Task Prompt Tuning for Few-Shot Conversational Emotion Recognition
arxiv_id: '2310.14614'
source_url: https://arxiv.org/abs/2310.14614
tags:
- prompt
- knowledge
- ctpt
- emotion
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of few-shot learning for Emotion
  Recognition in Conversation (ERC), which is challenging due to limited training
  data and the high computational cost of fine-tuning large pre-trained language models
  (PLMs). To improve both sample and computational efficiency, the authors propose
  a Cross-Task Prompt Tuning (CTPT) method that leverages sharable cross-task knowledge
  from source tasks to improve learning performance under the few-shot setting.
---

# Efficient Cross-Task Prompt Tuning for Few-Shot Conversational Emotion Recognition

## Quick Facts
- arXiv ID: 2310.14614
- Source URL: https://arxiv.org/abs/2310.14614
- Reference count: 30
- One-line primary result: Proposes CTPT method achieving superior few-shot and zero-shot performance on five conversational emotion datasets

## Executive Summary
This paper addresses the challenge of few-shot learning for Emotion Recognition in Conversation (ERC) by proposing a Cross-Task Prompt Tuning (CTPT) method. The approach leverages sharable cross-task knowledge from source tasks to improve learning performance when training data is limited. By using derivative-free optimization with low intrinsic dimensionality, CTPT achieves high parameter efficiency while maintaining strong performance across multiple conversational datasets.

## Method Summary
CTPT introduces a novel approach for few-shot ERC that combines task-specific prompt tuning with cross-task knowledge integration. The method learns task-specific prompts for each task independently, then uses multi-head attention to combine task-specific prompts from source tasks as external knowledge. A gate-like mechanism filters this combined knowledge to retain only beneficial information. The model employs CMA-ES optimization to optimize a low-dimensional vector instead of the full parameter space, significantly reducing the number of parameters that need training. Additionally, the method uses a union verbalizer to map different emotion labels across tasks to a common set, enabling learning from diverse emotion label sets.

## Key Results
- CTPT achieves superior results on both few-shot scenarios and zero-shot transfers compared to baseline methods
- The method is highly parameter-efficient, requiring optimization of only about 1,000 parameters
- Experimental results on five conversational datasets (EC, DailyDialog, MELD, EmoryNLP, IEMOCAP) demonstrate the effectiveness of cross-task knowledge leveraging

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-task prompt tuning leverages external task-specific knowledge from source tasks to improve learning performance under few-shot settings
- Mechanism: The model learns task-specific prompts for each task independently, then uses a multi-head attention module to combine task-specific prompts from source tasks as external knowledge. This combined knowledge is filtered through a gate-like mechanism to retain only beneficial information
- Core assumption: Task-specific knowledge from different but related tasks contains sharable patterns that can improve performance on the target task when properly combined
- Evidence anchors:
  - [abstract]: "Unlike existing methods that learn independent knowledge from individual tasks, CTPT leverages sharable cross-task knowledge by exploiting external knowledge from other source tasks"
  - [section 3.4]: "External Task-Specific Knowledge... we introduce external task-specific knowledge from other source tasks"
  - [corpus]: Weak evidence - no direct citations to this specific cross-task learning approach found
- Break condition: If source tasks are too dissimilar from target task, the external knowledge may introduce noise rather than benefit

### Mechanism 2
- Claim: Derivative-free optimization with intrinsic dimensionality reduces parameter count while maintaining performance
- Mechanism: Instead of optimizing the full prompt matrix, the model optimizes a low-dimensional vector (intrinsic dimensionality) and projects it to the parameter space. CMA-ES algorithm is used for optimization without requiring gradients
- Core assumption: The parameter space has a lower intrinsic dimensionality than the full parameter space, allowing for efficient optimization with fewer parameters
- Evidence anchors:
  - [abstract]: "CTPT only needs to optimize a vector under the low intrinsic dimensionality without gradient"
  - [section 3.6]: "we utilize a Covariance Matrix Adaptation Evolution Strategy (CMA-ES)... to optimize a vector with intrinsic dimensionality"
  - [corpus]: Moderate evidence - derivative-free optimization is established but specific application to ERC with intrinsic dimensionality is novel
- Break condition: If the intrinsic dimensionality is set too low, the model may lose expressive power and performance will degrade

### Mechanism 3
- Claim: Emotional knowledge mapping enables learning from diverse emotion label sets across tasks
- Mechanism: The model creates a union verbalizer that maps different emotion labels across tasks to a common set, allowing it to learn from emotional knowledge across tasks even when they use different label names for the same emotion
- Core assumption: Different datasets may use different labels for the same underlying emotional states, and this mapping can capture shared emotional knowledge
- Evidence anchors:
  - [section 3.4]: "Across different ERC datasets, varying labels might be used to denote identical emotional states... we can learn cross-task emotional knowledge from disparate tasks encompassing the same emotional state"
  - [abstract]: "For emotional knowledge, we combine the same emotion within different textual categories from different tasks"
  - [corpus]: Limited evidence - no direct citations for this specific emotion mapping approach
- Break condition: If emotion label mappings are incorrect or overly broad, the model may learn incorrect associations between different emotional states

## Foundational Learning

- Concept: Few-shot learning
  - Why needed here: The paper addresses scenarios where limited training data is available for each emotion category
  - Quick check question: What distinguishes few-shot learning from zero-shot and traditional supervised learning?

- Concept: Prompt tuning
  - Why needed here: Traditional fine-tuning of large PLMs is computationally expensive and prone to overfitting with limited data
  - Quick check question: How does prompt tuning differ from standard fine-tuning in terms of which model parameters are updated?

- Concept: Multi-head attention
  - Why needed here: Used to combine and weigh task-specific knowledge from multiple source tasks
  - Quick check question: In what way does multi-head attention enable the model to selectively focus on relevant cross-task knowledge?

## Architecture Onboarding

- Component map: Task-specific prompt tuning (TSPT) → Cross-task prompt learning (CTPL) → Cross-task prompt observation (CTPO) → PLM with masked prediction → Derivative-free optimization (CMA-ES)
- Critical path: Prompt generation → Cross-task knowledge integration → Masked prediction → Loss computation → Vector optimization
- Design tradeoffs: 
  - Fewer parameters (computational efficiency) vs. potential loss of expressive power
  - Cross-task knowledge (potential performance gain) vs. risk of negative transfer from dissimilar tasks
  - Derivative-free optimization (no gradient requirements) vs. slower convergence
- Failure signatures:
  - Poor performance on target task may indicate incorrect emotion label mappings or selection of dissimilar source tasks
  - Training instability may indicate intrinsic dimensionality set too low
  - Convergence issues may indicate inappropriate CMA-ES parameters
- First 3 experiments:
  1. Test TSPT baseline performance on target task with k=16 samples per class
  2. Add CTPL component with all source tasks to measure cross-task knowledge benefit
  3. Vary intrinsic dimensionality to find optimal balance between efficiency and performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Cross-Task Prompt Tuning (CTPT) method compare to other few-shot learning approaches for emotion recognition in conversation (ERC) in terms of sample efficiency and computational efficiency?
- Basis in paper: [explicit] The paper mentions that few-shot learning techniques hold the promise to improve both sample and computation efficiency for deploying pre-trained language models (PLMs) in new scenarios where data can be limited. It also states that CTPT is trained under the few-shot setting, which is sample-efficient, and that it only needs to optimize about 1,000 parameters, which is much more training-efficient than any other existing PLM-based ERC method.
- Why unresolved: The paper does not provide a direct comparison between CTPT and other few-shot learning approaches for ERC in terms of sample efficiency and computational efficiency.
- What evidence would resolve it: Conducting experiments that compare CTPT with other few-shot learning approaches for ERC in terms of sample efficiency and computational efficiency would provide evidence to resolve this question.

### Open Question 2
- Question: How does the choice of backbone model affect the performance of the proposed Cross-Task Prompt Tuning (CTPT) method for emotion recognition in conversation (ERC)?
- Basis in paper: [explicit] The paper mentions that CTPT is tested with T5 as the backbone model, but also explores the generalizability of CTPT across various backbone language models by conducting additional experiments using RoBERTa as the backbone model.
- Why unresolved: The paper does not provide a comprehensive analysis of how different backbone models affect the performance of CTPT for ERC.
- What evidence would resolve it: Conducting experiments that test CTPT with various backbone models for ERC and comparing their performance would provide evidence to resolve this question.

### Open Question 3
- Question: How does the proposed Cross-Task Prompt Tuning (CTPT) method perform in real-world scenarios with limited annotated training examples?
- Basis in paper: [explicit] The paper mentions that annotated training examples are not always available in real-world scenarios and that it is worthwhile to explore the zero-shot generalization ability of CTPT. It conducts experiments under the zero-shot setting to evaluate the target task while excluding the external task-specific prompt from the target task.
- Why unresolved: The paper does not provide a comprehensive evaluation of how CTPT performs in real-world scenarios with limited annotated training examples.
- What evidence would resolve it: Conducting experiments that evaluate the performance of CTPT in real-world scenarios with limited annotated training examples would provide evidence to resolve this question.

## Limitations
- Cross-task knowledge quality dependency: Performance heavily depends on the quality and relevance of source tasks selected
- Intrinsic dimensionality determination: The paper doesn't provide clear guidance on how to determine optimal intrinsic dimensionality for different scenarios
- Zero-shot transfer validation: Limited detailed analysis of zero-shot performance beyond conversational datasets

## Confidence
- High confidence: Computational efficiency claims due to established derivative-free optimization with low intrinsic dimensionality
- Medium confidence: Cross-task knowledge integration mechanism, though edge cases with negative transfer need exploration
- Low confidence: Emotion label mapping universality across diverse datasets

## Next Checks
1. **Source task sensitivity analysis**: Systematically evaluate CTPT performance when varying the similarity between source and target tasks to identify the breaking point where cross-task knowledge becomes detrimental
2. **Intrinsic dimensionality ablation study**: Conduct comprehensive study varying intrinsic dimensionality parameter across a wider range (e.g., 1-50) to identify optimal balance point
3. **Zero-shot transfer robustness test**: Design experiments testing CTPT on target tasks from completely different domains (e.g., social media conversations, customer service interactions) to validate zero-shot transfer claims beyond conversational dataset family