---
ver: rpa2
title: Preference as Reward, Maximum Preference Optimization with Importance Sampling
arxiv_id: '2312.16430'
source_url: https://arxiv.org/abs/2312.16430
tags:
- preference
- reward
- algorithm
- optimization
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Maximum Preference Optimization (MPO), an
  off-policy preference optimization algorithm that addresses limitations in existing
  methods like DPO and IPO. The key insight is reformulating preference learning as
  a reward maximization problem using importance sampling, which enables truly effective
  KL-regularization that previous methods failed to achieve.
---

# Preference as Reward, Maximum Preference Optimization with Importance Sampling

## Quick Facts
- arXiv ID: 2312.16430
- Source URL: https://arxiv.org/abs/2312.16430
- Reference count: 1
- This paper introduces Maximum Preference Optimization (MPO), an off-policy preference optimization algorithm that addresses limitations in existing methods like DPO and IPO.

## Executive Summary
This paper introduces Maximum Preference Optimization (MPO), an off-policy preference optimization algorithm that addresses limitations in existing methods like DPO and IPO. The key insight is reformulating preference learning as a reward maximization problem using importance sampling, which enables truly effective KL-regularization that previous methods failed to achieve. MPO combines the strengths of RLHF (proper KL-regularization) and DPO/IPO (off-policy learning) while eliminating the need for a reward model or reference policy during optimization. The method uses a simple loss function that maximizes preference probabilities while maintaining regularization through pre-generated samples, reducing memory usage and simplifying the learning process. The approach is theoretically grounded and provides a more effective way to align language models with human preferences.

## Method Summary
MPO reformulates preference learning as a reward maximization problem using importance sampling. The algorithm pre-generates regularization data and computes importance sampling weights offline, enabling off-policy gradient computation. The MPO loss function contains three components: preference maximization, KL regularization between pair-wise policies, and KL regularization between point-wise policies. By pre-computing the pair-wise reference distribution and regularization data, MPO eliminates the need for a reward model or reference policy during the actual optimization process, reducing memory usage and computational complexity.

## Key Results
- MPO incorporates truly effective KL-regularization by avoiding support mismatch problems
- The algorithm eliminates the need for reward models and reference policies during optimization
- MPO uses a simple loss function that combines preference maximization with KL-regularization
- The approach reduces memory usage by pre-generating regularization data in advance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Importance sampling enables truly effective KL-regularization by avoiding the support mismatch problem between preference data distribution and policy-generated data.
- Mechanism: By reformulating preference learning as a reward maximization problem and using importance sampling weights (πp_θ(a|x)/πp_¯(a|x)), MPO can compute gradients offline using pre-collected preference data, eliminating the need for on-policy sampling that causes distribution shift.
- Core assumption: The importance sampling weights can be computed offline using the uniform preference policy πp_¯(a|x) = 1/2, which generates both preference outcomes simultaneously.
- Evidence anchors:
  - [abstract] "MPO incorporates the off-policy KL-regularization term, making regularization truly effective"
  - [section 4.2] "Because πref is fixed, we can generate samples Dπref(·|x) conditioned on x ∼ ρ in advance"
  - [corpus] Weak - no direct evidence about importance sampling effectiveness found in neighbors
- Break condition: If the importance sampling ratio becomes unstable (e.g., πp_θ(a|x) approaches 0 or 1), the gradients could become too large or undefined.

### Mechanism 2
- Claim: The off-policy formulation eliminates the need for a reward model and reference policy during optimization, reducing memory usage and complexity.
- Mechanism: MPO pre-generates regularization data (Dπref) and computes the pair-wise reference distribution πp_ref(a|x) in advance, allowing optimization to proceed without requiring access to πref or a reward model during the actual training loop.
- Core assumption: The pre-generated samples from πref are sufficient to represent the regularization distribution throughout training.
- Evidence anchors:
  - [abstract] "Furthermore, MPO eliminates the need for a reward model and reference policy, simplifying the learning process and reducing memory usage"
  - [section 4.2] "When generating Dref(·|x) and calculating log πp_ref(a|x) in advance, we can optimize LMPO without πref"
  - [corpus] No direct evidence about memory reduction found in neighbors
- Break condition: If the reference distribution changes significantly during training, the pre-generated samples would become outdated and ineffective.

### Mechanism 3
- Claim: The MPO loss function explicitly regularizes both the point-wise policy πs_θ and the pair-wise preference policy πp_θ, preventing overfitting to preference data.
- Mechanism: The loss LMPO contains three terms: preference maximization (-Iπp_θ(a|x)), KL regularization between pair-wise policies (βπp_θ(a|x) log πp_θ(a|x)/πp_ref(a|x)), and KL regularization between point-wise policies (τ E_y∼Dref[log πs_θ(y)]).
- Core assumption: The separate regularization of pair-wise and point-wise policies addresses the distribution shift problem that causes previous methods to fail at KL-regularization.
- Evidence anchors:
  - [section 4.2] "We use DKL[πref||πs_θ] as the KL-regularization term" and "we add a KL-regularation term for preference probability"
  - [section 3.3] "DPO will lead over-fitting to the preference dataset at the expense of ignoring the KL-regularation term"
  - [corpus] No direct evidence about multi-term regularization effectiveness found in neighbors
- Break condition: If the regularization weights β or τ are poorly tuned, either the KL-regularization won't work or the preference maximization will be overly constrained.

## Foundational Learning

- Concept: Importance sampling in reinforcement learning
  - Why needed here: MPO uses importance sampling to enable off-policy gradient computation for preference optimization, avoiding the need for on-policy data collection that causes distribution shift.
  - Quick check question: What is the key advantage of using importance sampling weights πp_θ(a|x)/πp_¯(a|x) in off-policy learning?

- Concept: KL-divergence and its role in regularization
  - Why needed here: MPO uses KL-divergence terms to prevent the policy from deviating too far from the reference distribution while still optimizing for preferences.
  - Quick check question: Why does MPO use DKL[πref||πs_θ] instead of the more common DKL[πs_θ||πref]?

- Concept: Bradley-Terry model for pairwise comparisons
  - Why needed here: MPO builds on the Bradley-Terry model to convert pairwise preferences into probabilities that can be optimized.
  - Quick check question: How does the Bradley-Terry model convert reward differences into preference probabilities?

## Architecture Onboarding

- Component map: Data generation (pre-generating Dπref and computing πp_ref(a|x)) -> Loss computation (calculating LMPO using pre-collected Dp) -> Policy update (applying gradients to πp_θ and πs_θ)

- Critical path: Data generation → Loss computation → Policy update → Data generation (for next iteration). The key optimization is that data generation happens once in advance rather than continuously.

- Design tradeoffs: MPO trades memory usage (storing pre-generated samples) for computational efficiency and stability. The off-policy approach eliminates the need for continuous data collection but requires sufficient storage for regularization data.

- Failure signatures: If the importance sampling weights become unstable (very large or very small), the training will become unstable. If the regularization terms are too weak, the policy will overfit to preferences. If they're too strong, the policy won't learn meaningful preferences.

- First 3 experiments:
  1. Verify that importance sampling weights πp_θ(a|x)/πp_¯(a|x) can be computed correctly and remain stable across training iterations
  2. Test that pre-generated regularization data provides sufficient coverage by comparing KL-divergence statistics with on-policy sampling
  3. Validate that the MPO loss produces reasonable gradients by checking that preference maximization increases while KL-divergence remains bounded

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed MPO algorithm perform in terms of sample efficiency and stability compared to existing preference optimization methods?
- Basis in paper: [explicit] The paper claims that MPO is data efficient and stable, similar to DPO, but does not provide experimental results or comparisons.
- Why unresolved: The paper introduces the MPO algorithm but does not present any empirical results to support the claims of improved performance.
- What evidence would resolve it: Experimental results comparing MPO with existing methods (DPO, IPO, RLHF) on benchmark datasets, including measures of sample efficiency and stability.

### Open Question 2
- Question: How does the importance sampling approach in MPO affect the optimization landscape and convergence properties compared to on-policy methods?
- Basis in paper: [explicit] The paper mentions that MPO uses importance sampling for off-policy optimization, but does not analyze the theoretical implications or provide empirical evidence of its impact on optimization.
- Why unresolved: The paper does not provide a detailed analysis of how the importance sampling approach affects the optimization process or convergence properties of MPO.
- What evidence would resolve it: Theoretical analysis of the optimization landscape and convergence properties of MPO, along with empirical results demonstrating the impact of importance sampling on optimization.

### Open Question 3
- Question: How does the choice of regularization weight (τ) in MPO affect the balance between preference maximization and KL regularization?
- Basis in paper: [explicit] The paper mentions that τ controls the strength of the KL regularization between the point-wise generating distribution, but does not explore the effects of different values of τ on the optimization process or final performance.
- Why unresolved: The paper does not provide any analysis or empirical results on the impact of the regularization weight on the optimization process or final performance of MPO.
- What evidence would resolve it: Experimental results exploring the effects of different values of τ on the optimization process and final performance of MPO, along with theoretical insights into the role of τ in balancing preference maximization and KL regularization.

## Limitations

- The paper lacks empirical validation of MPO's performance compared to existing methods like DPO and IPO
- Memory and computational efficiency benefits of the off-policy approach are not quantified
- The paper doesn't provide systematic analysis of hyperparameter sensitivity, particularly for the regularization weights β and τ

## Confidence

- **High Confidence:** The mathematical formulation of MPO and its loss function is well-defined and theoretically grounded. The importance sampling approach for off-policy learning is a valid and established technique in reinforcement learning.
- **Medium Confidence:** The claim that MPO achieves truly effective KL-regularization is supported by the theoretical analysis but lacks comprehensive empirical validation across different preference learning scenarios.
- **Medium Confidence:** The assertion that MPO eliminates the need for reward models during optimization is plausible given the off-policy formulation, but the practical implementation details and memory requirements are not fully explored.

## Next Checks

1. **Empirical KL-Regularization Effectiveness:** Conduct experiments comparing the actual KL-divergence between the learned policy and reference policy during training across MPO, DPO, and IPO to verify that MPO achieves better regularization while maintaining preference performance.

2. **Memory and Computational Efficiency:** Measure the memory usage and training time of MPO compared to reward model-based approaches (like RLHF) and other preference optimization methods, particularly focusing on the trade-off between pre-generated data storage and training efficiency.

3. **Robustness to Hyperparameter Choices:** Systematically test MPO's performance across a range of hyperparameter settings for β and τ to identify sensitivity and determine optimal configurations for different preference learning tasks.