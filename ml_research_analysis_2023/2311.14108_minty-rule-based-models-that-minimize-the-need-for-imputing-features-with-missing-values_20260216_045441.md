---
ver: rpa2
title: 'MINTY: Rule-based Models that Minimize the Need for Imputing Features with
  Missing Values'
arxiv_id: '2311.14108'
source_url: https://arxiv.org/abs/2311.14108
tags:
- missing
- values
- minty
- rule
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MINTY learns interpretable disjunctive rule models that avoid relying
  on features with missing values. It does this by learning rules that combine redundant
  features as logical disjunctions, so that a rule's truth value can be determined
  even when some of its component features are missing.
---

# MINTY: Rule-based Models that Minimize the Need for Imputing Features with Missing Values

## Quick Facts
- arXiv ID: 2311.14108
- Source URL: https://arxiv.org/abs/2311.14108
- Reference count: 40
- MINTY learns interpretable disjunctive rule models that avoid relying on features with missing values.

## Executive Summary
MINTY introduces a novel approach for learning interpretable rule-based models that can handle missing data without imputation. The method learns disjunctive rules where features act as replacements for each other when one is missing, combined with regularization to minimize reliance on missing values. Experiments demonstrate that MINTY achieves comparable predictive performance to larger black-box models while using fewer rules and relying less on imputation, showing that interpretable models can be robust to missing data.

## Method Summary
MINTY learns interpretable disjunctive rule models by combining logical OR operations between features with regularization penalties. The algorithm uses a column generation strategy to iteratively build a small set of effective rules, starting with an intercept and adding rules that best align with the residual error. A tunable regularization parameter γ controls the tradeoff between predictive accuracy and reliance on missing values, while another parameter λ controls model sparsity. The method handles missing values by defining rule activations that can be evaluated even when some component features are missing, exploiting redundancy in the feature set.

## Key Results
- MINTY achieves comparable predictive performance to black-box models and imputation-based approaches
- The method uses fewer rules than standard rule-based methods while maintaining accuracy
- MINTY relies significantly less on features with missing values compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MINTY avoids reliance on imputation by learning disjunctive rules that can be evaluated even when some literals are missing.
- Mechanism: For each rule, the activation is defined as a logical OR over literals. If at least one literal in the rule is observed and true, the rule's truth value can be determined without needing the other literals. This exploits redundancy in the feature set.
- Core assumption: The feature set contains redundant variables such that if one is missing, another can serve as a replacement.
- Evidence anchors:
  - [abstract]: "learns rules in the form of disjunctions between variables that act as replacements for each other when one or more is missing."
  - [section 2.2]: Defines how rule activations work with missing values: "aik = 1 if for any feature j, the literal is True (xij = 1) and j is included in rule k (zjk = 1)."
  - [corpus]: Weak. Corpus contains papers on imputation methods and transformers, but no direct discussion of disjunctive rules or redundancy exploitation.
- Break condition: If no redundant features exist, the OR structure provides no advantage over single-feature rules.

### Mechanism 2
- Claim: MINTY adds regularization to minimize reliance on features with missing values, balancing predictive performance with robustness.
- Mechanism: A tunable penalty γ is added to the objective function, penalizing rules that frequently depend on missing values. This encourages the model to prefer rules that can be evaluated with high probability despite missing data.
- Core assumption: There is a trade-off between predictive accuracy and reliance on missing values that can be controlled via regularization.
- Evidence anchors:
  - [abstract]: "adds regularization to minimize reliance on missing values."
  - [section 3]: Details the optimization problem with γ: "With a parameter γ ≥ 0 used to control the average reliance on missing features ¯ρk for included rules..."
  - [corpus]: Weak. No direct evidence in corpus about regularization for missing value reliance.
- Break condition: If γ is set too high, the model may become uninformative; if too low, it behaves like standard imputation.

### Mechanism 3
- Claim: MINTY's column generation strategy incrementally builds a small set of effective rules, avoiding the exponential complexity of enumerating all possible disjunctions.
- Mechanism: Starts with a small candidate set of rules (initially just the intercept), solves the optimization problem, then iteratively adds the rule that most reduces the residual error. This is justified by optimality conditions of the regularized objective.
- Core assumption: The optimal solution can be found by incrementally adding rules that align with the residual.
- Evidence anchors:
  - [abstract]: "optimizes MINTY by adapting the column generation strategy of Wei et al. (2019), iteratively adding rules to the model..."
  - [section 3]: Explains the column generation process: "Given a current set of disjunctions ˆS and estimated coefficients ˆβ, a new rule is added by finding the disjunction that aligns the most with the residual..."
  - [corpus]: Weak. No direct evidence in corpus about column generation for rule learning.
- Break condition: If the search space is too large or the residuals are not well-aligned with potential rules, the incremental approach may miss optimal solutions.

## Foundational Learning

- Concept: Disjunctive normal form (DNF) and logical OR operations in rule-based models.
  - Why needed here: MINTY's core mechanism relies on evaluating rules as logical ORs over literals, allowing determination of rule truth even with missing inputs.
  - Quick check question: If a rule is (A OR B OR C) and A is missing but B is true, what is the rule's value?
- Concept: Regularization in machine learning, specifically ℓ1 regularization for sparsity.
  - Why needed here: MINTY uses ℓ1 penalties on rule coefficients to control model size and on rule definitions to limit reliance on missing values.
  - Quick check question: How does increasing the regularization parameter λ affect the number of non-zero coefficients in a LASSO model?
- Concept: Column generation algorithms for large-scale optimization.
  - Why needed here: MINTY uses a column generation strategy to efficiently search the space of possible disjunctive rules without enumerating all exponentially many possibilities.
  - Quick check question: What is the main advantage of column generation over enumerating all columns in a large linear program?

## Architecture Onboarding

- Component map: Rule generation module -> Rule activation module -> Coefficient optimization module -> Column generation loop
- Critical path: Column generation loop → Rule definition → Rule activation → Coefficient optimization → Convergence check
- Design tradeoffs:
  - Beam search depth vs. solution quality: Deeper search finds better rules but is slower.
  - Regularization γ vs. reliance on missing values: Higher γ reduces reliance but may hurt accuracy.
  - Rule size (number of literals) vs. interpretability: Smaller rules are more interpretable but may be less predictive.
- Failure signatures:
  - If γ is too high, model may output only the intercept (no predictive power).
  - If beam search depth is too low, rules may miss important variable combinations.
  - If regularization λ is too low, model may overfit to noise in the training data.
- First 3 experiments:
  1. Run MINTY on a small synthetic dataset with MCAR missingness, varying γ from 0 to 1000, and plot R² vs. ¯ρ to see the tradeoff curve.
  2. Compare MINTYγ=0.01 to LASSO with MICE imputation on ADNI data, measuring R², MSE, and ¯ρ on test set.
  3. Visualize the learned rules for MINTYγ=0 and MINTYγ=0.01 on ADNI, highlighting which rules are dropped when γ > 0.

## Open Questions the Paper Calls Out
- None explicitly stated in the provided content.

## Limitations
- Limited evaluation to only three real-world datasets and one synthetic dataset
- Only tested on MCAR missingness mechanism, not exploring MAR or MNAR scenarios
- Column generation approach may struggle with high-dimensional data despite heuristic search

## Confidence
- **High Confidence**: MINTY can learn interpretable rule-based models with competitive predictive performance compared to black-box methods and imputation-based approaches.
- **Medium Confidence**: MINTY's disjunctive rule structure allows it to avoid reliance on imputation by exploiting feature redundancy.
- **Low Confidence**: The regularization parameter γ effectively controls the tradeoff between predictive accuracy and reliance on missing values in a predictable, monotonic manner.

## Next Checks
1. **Feature Redundancy Analysis**: Systematically quantify the redundancy structure in the ADNI, Life, and Housing datasets by computing pairwise feature correlations and mutual information. Verify that the datasets actually contain the redundancy MINTY's mechanism depends on, and measure how this redundancy correlates with MINTY's ability to avoid imputation.

2. **Missingness Mechanism Sensitivity**: Re-run all experiments with controlled MAR and MNAR missingness patterns (in addition to MCAR) to verify MINTY's robustness claims. Compare performance degradation across different mechanisms and quantify how the regularization parameter γ needs to be adjusted for each pattern.

3. **Scalability Benchmark**: Test MINTY on a high-dimensional dataset (e.g., >1000 features) with controlled redundancy levels to measure computational scaling of the column generation approach. Track runtime, memory usage, and solution quality as feature count increases, and compare against baseline methods' scaling behavior.