---
ver: rpa2
title: 'Continual Learning with Dynamic Sparse Training: Exploring Algorithms for
  Effective Model Updates'
arxiv_id: '2308.14831'
source_url: https://arxiv.org/abs/2308.14831
tags:
- sparsity
- learning
- task
- tasks
- growth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates Dynamic Sparse Training (DST) in continual
  learning (CL) to determine optimal initialization and growth strategies. DST allows
  parameter isolation by finding sparse subnetworks per task while avoiding catastrophic
  forgetting.
---

# Continual Learning with Dynamic Sparse Training: Exploring Algorithms for Effective Model Updates

## Quick Facts
- arXiv ID: 2308.14831
- Source URL: https://arxiv.org/abs/2308.14831
- Reference count: 40
- Key outcome: This paper investigates Dynamic Sparse Training (DST) in continual learning (CL) to determine optimal initialization and growth strategies. DST allows parameter isolation by finding sparse subnetworks per task while avoiding catastrophic forgetting. Experiments on CIFAR100 and miniImageNet with 5-20 tasks examine ERK vs. uniform initialization and random, unfired, gradient, and momentum growth strategies at various sparsity levels. Results show that at low sparsity ERK initialization is more efficient by allocating more connections to large layers, while uniform initialization is more robust at high sparsity. Growth strategy performance depends on both initialization and sparsity level. Adaptive selection of growth strategy per task outperforms fixed strategies. The findings provide empirical guidance on configuring DST components for CL, highlighting the importance of matching initialization and growth strategies to sparsity levels and task characteristics.

## Executive Summary
This paper systematically investigates how Dynamic Sparse Training (DST) components affect continual learning performance. The authors examine the interplay between sparse initialization strategies (ERK vs. uniform) and growth strategies (random, unfired, gradient, momentum) across varying sparsity levels and task increments. Through extensive experiments on CIFAR100 and miniImageNet, they demonstrate that the optimal configuration depends on both sparsity level and task characteristics. The findings reveal that ERK initialization is more efficient at low sparsity by preserving narrow layer capacity, while uniform initialization provides better robustness at high sparsity. Additionally, the paper introduces adaptive growth strategy selection, showing that task-specific strategy selection outperforms fixed approaches.

## Method Summary
The paper implements DST for continual learning using a ResNet-18 backbone with frozen batch normalization layers. The approach involves Erdös-Rényi Kernel (ERK) or uniform sparse initialization, followed by dynamic topology updates using magnitude-based weight drop and various growth strategies. Training proceeds through multiple tasks with mutually exclusive classes, where each task's subnetwork is frozen after learning to prevent catastrophic forgetting. The method uses Adam optimizer with cosine annealing for drop rate, varying epochs based on task count (25-100), and sparsity levels of 80%, 90%, and 95%. Performance is evaluated using top-1 accuracy, backward transfer, and forward transfer metrics across task splits of 5, 10, and 20 increments.

## Key Results
- At low sparsity levels, ERK initialization is more efficient by allocating more connections to large layers while preserving narrow layer capacity
- At high sparsity levels, uniform initialization demonstrates more robust and reliable performance
- Growth strategy effectiveness depends on both initialization method and sparsity level, with adaptive selection outperforming fixed strategies
- The combination of initialization and growth strategy must be matched to sparsity level and task characteristics for optimal performance

## Why This Works (Mechanism)

### Mechanism 1
ERK initialization is more efficient than uniform at low sparsity because it allocates more connections to large layers while sparing narrow layers, preserving information flow. ERK's layer-aware sparsity distribution prevents bottlenecks in narrow layers, allowing sustained learning across many tasks. Core assumption: Narrow layers are critical for information flow; starving them of connections degrades performance more than in wide layers. Evidence anchors: [abstract] "At a low sparsity level, Erd˝os-Rényi Kernel (ERK) initialization utilizes the backbone more efficiently by using the connections within the narrow layers sparingly which allows to effectively learn increments of tasks." [section] "ERK allocates more connections to the layers with more parameters and less connections to the layers with fewer parameters [28, 31]." Break condition: If tasks require heavy narrow-layer processing, even ERK may fail; if sparsity is increased beyond a threshold, uniform's uniform allocation may suffice.

### Mechanism 2
Uniform initialization is more robust at high sparsity because it avoids extreme bottlenecks that can arise from ERK's aggressive pruning of narrow layers. By distributing sparsity evenly, uniform ensures that no single layer is too under-resourced, maintaining a stable backbone capacity across tasks. Core assumption: At high sparsity, even narrow layers need enough connections to avoid catastrophic forgetting; uniform's egalitarian allocation prevents this. Evidence anchors: [abstract] "At a high sparsity level, uniform initialization demonstrates a more reliable and robust performance." [section] "In scenarios where the sparsity levels are high, we observe that information flow remains sustainable even with the uniform initialization." Break condition: If sparsity is extreme (e.g., >95%), even uniform may saturate; if task similarity is high, both methods may plateau.

### Mechanism 3
Adaptive selection of growth strategy per task improves performance because the optimal strategy depends on initialization and sparsity, and task complexity evolves over time. Early tasks benefit from random growth (large exploration space), while later tasks benefit from gradient growth (exploiting learned structure). Core assumption: The exploration-exploitation trade-off shifts as the model's capacity saturates; random is better early, gradient better late. Evidence anchors: [abstract] "Finally, adaptivity within DST components is a promising way for better continual learners." [section] "In our final experiment, we construct a naive adaptive growth strategy... wherein random growth is implemented for the first five tasks, and gradient growth is selected for the subsequent five tasks... The adaptive approach yields better performance." Break condition: If tasks are too similar or sparsity too low, adaptive gains may be negligible; if computational budget is tight, switching strategies may add overhead.

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: DST's parameter isolation relies on freezing subnetworks to prevent forgetting; understanding forgetting is key to appreciating why isolation matters.
  - Quick check question: What happens to previously learned weights when a model is trained on a new task without isolation?

- Concept: Sparse neural network initialization strategies
  - Why needed here: ERK vs. uniform initialization directly affects layer-wise capacity allocation; choosing the wrong one can bottleneck learning.
  - Quick check question: How does ERK's layer-aware sparsity differ from uniform's global sparsity in terms of parameter distribution?

- Concept: Growth strategy selection in dynamic sparse training
  - Why needed here: Random, unfired, gradient, and momentum growth each have different exploration vs. exploitation profiles; matching them to sparsity and initialization is critical.
  - Quick check question: Why might gradient growth outperform random growth at high sparsity but not at low sparsity?

## Architecture Onboarding

- Component map: ResNet-18 backbone -> sparsity masks per task -> DST update schedule (drop/growth) -> optimizer (Adam) -> metrics (ACC, BWT, FWT)
- Critical path: Forward pass (all connections) -> backward pass (only unassigned) -> topology update (prune/grow) -> freeze learned subnetwork
- Design tradeoffs: Structured vs. unstructured pruning (structured faster but less accurate at high sparsity); fixed vs. adaptive strategy (adaptive better but more complex)
- Failure signatures: Saturation in accuracy curves (backbone capacity exhausted); high backward transfer (forgetting); divergence in loss
- First 3 experiments:
  1. Run 5-task CIFAR100 with ERK+random at 80% sparsity; verify improved performance over uniform+random
  2. Run 10-task CIFAR100 with uniform+gradient at 95% sparsity; check for robustness vs. ERK+gradient
  3. Implement naive adaptive growth (random first half, gradient second half) on 10-task CIFAR100; compare to fixed strategies

## Open Questions the Paper Calls Out

### Open Question 1
What are the theoretical limits of adaptive DST selection in continual learning, and how can they be systematically determined?
- Basis in paper: [explicit] The paper discusses adaptive DST strategies as promising but only tests a simple random-then-gradient approach.
- Why unresolved: The paper only explores one naive adaptive strategy, leaving open questions about optimal adaptive algorithms and their theoretical foundations.
- What evidence would resolve it: Systematic experiments comparing various adaptive algorithms (e.g., reinforcement learning, meta-learning approaches) and theoretical analysis of their convergence and performance bounds.

### Open Question 2
How do structured and unstructured DST compare in terms of both accuracy and computational efficiency across different sparsity levels and task complexities?
- Basis in paper: [explicit] The paper focuses on unstructured DST and mentions structured DST as future work.
- Why unresolved: The paper exclusively examines unstructured DST, leaving a gap in understanding the trade-offs between structured and unstructured approaches.
- What evidence would resolve it: Comparative experiments measuring both accuracy and computational metrics (training time, memory usage) for structured vs. unstructured DST across multiple datasets and sparsity levels.

### Open Question 3
What is the relationship between task similarity and the effectiveness of parameter isolation in DST-based continual learning?
- Basis in paper: [explicit] The paper mentions that parameter isolation allows sharing parameters between similar tasks but doesn't explore this relationship.
- Why unresolved: The paper assumes task identity is given but doesn't investigate how task similarity affects DST performance or whether the algorithm can detect and leverage task relationships.
- What evidence would resolve it: Experiments varying task similarity (e.g., using datasets with controlled class relationships) and measuring how DST performance changes based on task overlap and similarity metrics.

## Limitations
- Limited external validation due to lack of related work discussing ERK/uniform initialization in CL context
- Adaptive strategy evaluation limited to single heuristic approach without comparison to alternative adaptive methods
- No theoretical analysis of why certain initialization-growth combinations work better at different sparsity levels

## Confidence
- ERK vs. uniform initialization effects: Medium
- Growth strategy performance variations: Medium
- Adaptive strategy benefits: Low-Medium

## Next Checks
1. Test alternative adaptive strategies (e.g., task-complexity-based switching, meta-learning to predict optimal strategy) to validate that adaptivity itself—not just the specific heuristic—drives performance gains
2. Evaluate catastrophic forgetting metrics (replay buffer ablation, weight importance analysis) to confirm parameter isolation actually prevents forgetting rather than just masking it
3. Test extreme sparsity regimes (>95%) with both initialization methods to identify where uniform truly becomes necessary over ERK