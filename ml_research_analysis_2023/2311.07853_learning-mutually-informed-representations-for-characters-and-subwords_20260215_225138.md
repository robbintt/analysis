---
ver: rpa2
title: Learning Mutually Informed Representations for Characters and Subwords
arxiv_id: '2311.07853'
source_url: https://arxiv.org/abs/2311.07853
tags:
- character
- subword
- text
- entanglement
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an entanglement model that combines character
  and subword language models through cross-attention modules. The model treats characters
  and subwords as separate modalities and generates mutually informed representations
  for both granularities.
---

# Learning Mutually Informed Representations for Characters and Subwords

## Quick Facts
- arXiv ID: 2311.07853
- Source URL: https://arxiv.org/abs/2311.07853
- Reference count: 18
- The model consistently outperforms backbone models on named entity recognition, POS-tagging, and text classification, particularly for noisy texts and low-resource languages, and even surpasses larger pre-trained models on English sequence labeling tasks.

## Executive Summary
This paper introduces an entanglement model that combines character and subword language models through cross-attention modules. The model treats characters and subwords as separate modalities and generates mutually informed representations for both granularities. Evaluated on named entity recognition, POS-tagging, and text classification tasks, the model consistently outperforms its backbone models, particularly on noisy texts and low-resource languages. Notably, it even surpasses larger pre-trained models on English sequence labeling and classification tasks. The code is made publicly available.

## Method Summary
The entanglement model leverages cross-attention mechanisms between character and subword representations to create mutually informed embeddings. The architecture uses separate pretrained character (CANINE-s) and subword (RoBERTa/XLM-R) encoders, with m co-attention modules inserted between them. Each co-attention module contains a CO-TRM block for cross-attention between modalities followed by a standard TRM block for self-attention. The model is trained using cross-entropy loss on word-level predictions without additional pretraining of the co-attention layers.

## Key Results
- Outperforms backbone language models on all English sequence labeling and classification tasks
- Shows significant improvements on noisy texts and low-resourced, morphologically rich languages
- Surpasses larger pre-trained models (RoBERTa-large) despite having fewer parameters

## Why This Works (Mechanism)

### Mechanism 1
- Cross-attention between character and subword representations enables mutual information transfer, allowing character models to benefit from subword-level context and vice versa.
- Core assumption: Character and subword representations contain complementary information that can be effectively exchanged through cross-attention mechanisms.
- Evidence: Abstract states the model generates mutually informed representations for both granularities as output.

### Mechanism 2
- The entanglement model improves performance on noisy and low-resource languages by leveraging character-level modeling to capture morphological information that subword tokenization may miss.
- Core assumption: Character-level information provides valuable morphological cues that are particularly useful for handling noisy text and morphologically rich languages.
- Evidence: Abstract notes the model outperforms backbone models particularly in the presence of noisy texts and low-resource languages.

### Mechanism 3
- The entanglement model can outperform larger pre-trained models by effectively combining the strengths of specialized character and subword models without requiring additional pretraining of the cross-attention layers.
- Core assumption: Specialized character and subword models contain complementary knowledge that can be effectively combined through cross-attention without requiring extensive pretraining of the integration layers.
- Evidence: Abstract states the model outperforms RoBERTa-large across all English classification and sequence labeling tasks.

## Foundational Learning

- **Cross-attention mechanisms**: Why needed - Cross-attention allows the model to attend to relevant information from the other modality (characters attending to subwords or vice versa), enabling information exchange between different granularities of text representation. Quick check - In a cross-attention operation Q=K=V=H_c, what does each component represent and how does this differ from standard self-attention?

- **Tokenization strategies and their impact on language modeling**: Why needed - Understanding the trade-offs between character-level, subword-level, and word-level tokenization is crucial for appreciating why combining multiple granularities can improve model performance, especially for noisy text and low-resource languages. Quick check - What are the key advantages and disadvantages of character-level tokenization compared to subword tokenization like BPE or WordPiece?

- **Multi-modal learning and information integration**: Why needed - The entanglement model draws inspiration from vision-language models, applying similar principles to integrate different text granularities. Understanding how information from different modalities can be effectively combined is essential for grasping the model's architecture. Quick check - How do vision-language models like ViLBERT integrate information from images and text, and how is this analogous to integrating character and subword representations?

## Architecture Onboarding

- **Component map**: Input layer (character sequence xc and subword sequence xs) -> Backbone encoders (CANINE-s for characters, RoBERTa/XLM-R for subwords) -> Co-attention modules (m layers with CO-TRM and TRM blocks) -> Output heads (separate classification layers) -> Loss function (cross-entropy)

- **Critical path**: 1. Tokenize input into character and subword sequences 2. Pass through respective backbone encoders to get H_c and H_s 3. Feed through m co-attention modules to get H_c* and H_s* 4. Apply classification layers to obtain predictions 5. Compute cross-entropy loss and backpropagate

- **Design tradeoffs**: Memory vs. performance (more co-attention modules increase memory usage but may provide marginal gains), Speed vs. granularity (character-level is slower but captures finer morphology; subword is faster but may miss character details), Pretraining vs. fine-tuning (relies on pretrained backbones with minimal co-attention fine-tuning)

- **Failure signatures**: Poor performance on both sides (backbone encoders not well-suited), Significant performance gap between sides (one modality not effectively learning from the other), Memory errors during training (too many co-attention modules for hardware)

- **First 3 experiments**: 1. Ablation study: Train with only subword side (no character input) and only character side (no subword input) to establish baselines 2. Single co-attention module: Train with m=1 to verify basic cross-attention mechanism 3. Two co-attention modules: Train with m=2 (optimal based on paper results) to establish expected performance level

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the impact of using different character-level models (e.g., Charformer, ByT5) as the character encoder backbone on the performance of the entanglement model? Basis: Authors mention CANINE-s performance is not comparable with RoBERTa and suggest the model might benefit from stronger character models. Unresolved: No experimental results using other character models.

- **Open Question 2**: How does the scale of pretraining affect the performance of the entanglement model? Basis: Authors performed pretraining on a subset (8%) of the corpus and observed the pre-trained model failed to outperform the standard, un-pretrained model. Unresolved: No results for different scales of pretraining.

- **Open Question 3**: How does the entanglement model perform on tasks involving morphologically rich languages beyond the low-resourced African languages tested in the paper? Basis: Authors note substantial improvements for certain African languages like Luo and Wolof due to their distinct characteristics. Unresolved: No results for morphologically rich languages beyond tested African languages.

## Limitations
- Limited evaluation to a single African NER dataset for multilingual claims
- Missing detailed analysis of what specific information transfers between character and subword modalities
- Lack of comparison with other competitive large models beyond RoBERTa-large

## Confidence
- High confidence in: The basic premise that combining character and subword representations through cross-attention can improve performance on noisy and morphologically rich text
- Medium confidence in: The specific mechanism by which cross-attention enables information transfer between modalities
- Low confidence in: Claims about outperforming larger pre-trained models due to limited comparison scope

## Next Checks
1. **Representation analysis**: Conduct detailed probe of learned character and subword representations to verify genuine cross-granularity information exchange through attention weight analysis and frozen representation downstream tasks.

2. **Ablation on tokenization**: Test model with different subword tokenization strategies (BPE, WordPiece, SentencePiece) to determine if performance gains are consistent across tokenization methods.

3. **Cross-lingual transferability**: Evaluate model on additional low-resource languages from different families (e.g., Southeast Asian, Indigenous American languages) to test generalizability of multilingual benefits beyond African languages.