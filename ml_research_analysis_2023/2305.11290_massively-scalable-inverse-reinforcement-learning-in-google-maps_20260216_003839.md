---
ver: rpa2
title: Massively Scalable Inverse Reinforcement Learning in Google Maps
arxiv_id: '2305.11290'
source_url: https://arxiv.org/abs/2305.11290
tags:
- maxent
- policy
- learning
- reward
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper scales IRL to hundreds of millions of states by leveraging
  graph compression, spatial parallelization, and a new initialization strategy based
  on dominant eigenvectors. The key contribution is Receding Horizon Inverse Planning
  (RHIP), a novel IRL algorithm that generalizes classic methods and enables fine-grained
  control over the trade-off between planning time and accuracy via a stochastic planning
  horizon parameter.
---

# Massively Scalable Inverse Reinforcement Learning in Google Maps

## Quick Facts
- arXiv ID: 2305.11290
- Source URL: https://arxiv.org/abs/2305.11290
- Reference count: 40
- Key outcome: 16-24% improvement in route quality using RHIP algorithm at global scale

## Executive Summary
This paper presents RHIP (Receding Horizon Inverse Planning), a novel IRL algorithm that generalizes classic methods like MaxEnt++ and BIRL through a stochastic planning horizon parameter. The key innovation is the ability to trade off between planning accuracy and computational efficiency by performing H backup steps of MaxEnt++ followed by a deterministic policy rollout. The approach achieves massive scalability by leveraging graph compression, spatial parallelization, and a new initialization strategy based on dominant eigenvectors. Applied at Google Maps scale, RHIP improves route quality by 16-24% over baseline ETA+penalties, representing the largest published IRL study in a real-world setting.

## Method Summary
The method combines several innovations to scale IRL to hundreds of millions of states. RHIP performs H backup steps of MaxEnt++ to obtain a stochastic policy π_s, then rolls out this policy for H steps before switching to a deterministic policy π_d, creating a receding horizon approach. Graph compression reduces computational complexity by splitting high-degree nodes and merging single-out-edge nodes. A sparse Mixture-of-Experts reward model with 360M parameters enables geographic specialization. MaxEnt++ initialization uses Dijkstra's algorithm to find the highest reward path from each node to the destination, providing a strictly better starting point than standard initialization. The entire system is trained on de-identified user trips using Adam optimizer for 1.4 GPU-years.

## Key Results
- 16-24% improvement in route accuracy over baseline ETA+penalties
- Achieved scalability to hundreds of millions of states through graph compression and parallelization
- Demonstrated global deployment with 360M parameter sparse reward model
- Showed convergence improvements through MaxEnt++ initialization strategy

## Why This Works (Mechanism)

### Mechanism 1: Receding Horizon Generalization
RHIP generalizes MaxEnt++, BIRL, and MMP through a stochastic planning horizon parameter H. By performing H backup steps of MaxEnt++ followed by deterministic rollout, RHIP trades off between stochastic planning accuracy and deterministic planning efficiency. The policy is approximated as a mixture of stochastic (short-term) and deterministic (long-term) components.

### Mechanism 2: MaxEnt++ Initialization
Instead of standard log(I_s=sd) initialization, MaxEnt++ uses the highest reward path from each node to the destination computed via Dijkstra's algorithm. This initialization is strictly closer to the true solution, reducing the number of iterations needed for convergence by leveraging the power iteration perspective of MaxEnt backward pass.

### Mechanism 3: Graph Compression
The road network graph is compressed by splitting high-degree nodes into multiple lower-degree nodes and merging nodes with single outgoing edges into downstream nodes. This reduces effective node degree and tensor size from ~4.9 to ~3.0 while maintaining route quality, enabled by the bounded degree property of road networks (typically V < 10).

## Foundational Learning

- **Markov Decision Processes (MDPs)**: The entire IRL framework is built on MDP theory where states, actions, transitions, and rewards are formally defined. Quick check: What are the four components of an MDP and how do they relate to route planning?

- **Power iteration and eigenvalue decomposition**: The MaxEnt backward pass is equivalent to power iteration for finding the dominant eigenvector of the graph. Quick check: How does the convergence rate of power iteration depend on the ratio of the second largest to the largest eigenvalue?

- **Mixture of Experts (MoE) architecture**: The approach uses a sparse MoE strategy where experts are associated with geographic regions, allowing for localized learning of routing preferences. Quick check: How does the sparse MoE strategy reduce cross-expert dependencies and what are the benefits for training efficiency?

## Architecture Onboarding

- **Component map**: Graph compression -> MoE partitioning -> MaxEnt++ backward pass -> RHIP forward pass -> Sparse tensor operations -> Demonstration preprocessing
- **Critical path**: 1) Load and compress graph data, 2) Partition demonstrations by geographic region, 3) Initialize MaxEnt++ value function using Dijkstra's algorithm, 4) Perform H backup steps of MaxEnt++, 5) Roll out stochastic policy for H steps then switch to deterministic policy, 6) Compute loss gradient and update reward parameters
- **Design tradeoffs**: Horizon length H (accuracy vs computational cost), graph compression level (speed vs accuracy), expert granularity (local preferences vs model complexity), sparse vs dense rewards (data quality highlighting vs parameter efficiency)
- **Failure signatures**: Training instability (loss spikes/divergence indicating backward pass issues), poor generalization (accuracy drops for out-of-region data indicating overfitting), computational bottlenecks (slow training suggesting insufficient compression or suboptimal partitioning)
- **First 3 experiments**: 1) Run RHIP with H=1, H=10, and H=∞ on small test metro to observe accuracy-time trade-off, 2) Compare MaxEnt++ initialization with standard MaxEnt initialization on simple MDP to verify convergence improvements, 3) Apply different graph compression levels (split only, merge only, split+merge) and measure impact on training speed and route quality

## Open Questions the Paper Calls Out

### Open Question 1
What is the theoretical guarantee on the quality of routes produced by RHIP with finite horizon H compared to MaxEnt with infinite horizon? The paper empirically shows RHIP with H=10 outperforms MaxEnt but lacks theoretical analysis of approximation error as a function of H, the reward function, and MDP structure.

### Open Question 2
How does RHIP performance vary with the choice of initial policy πd? The paper uses fixed initialization based on current reward estimates but doesn't explore impact of different initialization strategies or sensitivity to initial reward quality.

### Open Question 3
What is the impact of graph compression techniques on the learned reward function and how can this be mitigated? While compression provides significant speedup with minimal route quality impact, the paper doesn't analyze how compression affects learned rewards or whether approximation error is uniform across the graph.

## Limitations

- Graph compression assumes bounded degree road networks which may not hold for all geographic regions or future expansions
- Sparse MoE strategy's effectiveness depends on geographic partitioning creating independent subproblems, which could fail in highly interconnected urban areas
- MaxEnt++ initialization improvements haven't been validated across diverse reward function structures beyond the specific ETA+penalties baseline

## Confidence

- **High confidence**: RHIP's generalization of MaxEnt++, BIRL, and MMP through stochastic planning horizon is well-supported by theoretical framework and empirical 16-24% accuracy improvements
- **Medium confidence**: MaxEnt++ initialization convergence improvements are plausible but magnitude across different reward classes remains unclear; MoE strategy benefits demonstrated but optimal partitioning unexplored
- **Low confidence**: Claim of being "largest published IRL study" is difficult to verify given limited corpus of large-scale IRL implementations; long-term stability and robustness to network changes unaddressed

## Next Checks

1. **Horizon sensitivity analysis**: Systematically vary H from 1 to 100 on multiple metro areas to characterize accuracy-time trade-off curve and identify optimal horizon for different geographic contexts

2. **Graph compression validation**: Apply RHIP with and without compression on road networks with varying degree distributions to quantify accuracy loss and identify breaking points where compression becomes detrimental

3. **MoE independence verification**: Design experiments to measure cross-expert dependencies by swapping demonstration sets between geographic regions and quantifying performance drop to validate geographic independence assumption