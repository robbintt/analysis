---
ver: rpa2
title: 'Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly'
arxiv_id: '2310.03150'
source_url: https://arxiv.org/abs/2310.03150
tags:
- a100
- orin
- edge
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper benchmarks the feasibility of fine-tuning large language\
  \ models (LLMs) on edge devices via federated learning. Experiments with the FLAN-T5\
  \ transformer family (80M-3B parameters) on NVIDIA Jetson AGX Orin (edge) and A100\
  \ (data center) reveal two key findings: (1) Memory bandwidth is the primary bottleneck\
  \ on edge hardware, with model FLOP utilization (MFU) stagnating at low values (<10%)\
  \ due to memory-bound operations, and (2) Communication efficiency is critical\u2014\
  granularity (G) drops to near zero under LTE conditions, making distributed training\
  \ impractical without compression or higher bandwidth."
---

# Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly

## Quick Facts
- arXiv ID: 2310.03150
- Source URL: https://arxiv.org/abs/2310.03150
- Reference count: 40
- Key outcome: Memory bandwidth is the primary bottleneck for LLM fine-tuning on edge hardware, with MFU <10% and communication overhead dominating distributed training under limited bandwidth conditions.

## Executive Summary
This paper investigates the feasibility of federated fine-tuning for large language models on edge devices, using FLAN-T5 models (80M-3B parameters) on NVIDIA Jetson AGX Orin versus A100 data center GPUs. The study identifies memory bandwidth as the critical bottleneck on edge hardware, with model FLOP utilization stagnating below 10% due to memory-bound operations. Communication efficiency emerges as another major challenge, with granularity dropping near zero under LTE conditions, making distributed training impractical without compression or higher bandwidth. Energy efficiency correlates strongly with MFU, suggesting optimization opportunities for edge deployments.

## Method Summary
The researchers benchmark federated fine-tuning of FLAN-T5 models across different parameter sizes on edge (Jetson AGX Orin 64GB) and data center (A100 80GB) hardware. They use the Samsum dataset for text summarization with Federated Averaging, employing 10 clients with 3 randomly sampled per round, 1 local epoch, minibatch size 32, and 20 total rounds. Key metrics include energy efficiency (tokens per second per watt), model FLOP utilization (MFU), and granularity (G) to measure communication-computation tradeoffs. The study systematically varies minibatch sizes and network conditions to identify bottlenecks in edge-based federated learning.

## Key Results
- Memory bandwidth is the primary bottleneck on edge hardware, with MFU stagnating at <10% due to memory-bound operations
- Communication efficiency is critical—granularity (G) drops to near zero under LTE conditions, making distributed training impractical without compression
- Energy efficiency (ηe) correlates strongly with MFU, suggesting optimization opportunities for edge deployments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Memory bandwidth is the primary bottleneck for LLM fine-tuning on edge hardware
- Mechanism: Edge devices like Jetson AGX Orin have limited memory bandwidth (~0.2 TB/s) compared to data center GPUs (~2-7.8 TB/s). Memory-bound operations (softmax, residual additions, activations) consume significant training time, reducing MFU to <10%.
- Core assumption: Memory bandwidth directly limits the throughput of memory-bound operations in transformer training
- Evidence anchors:
  - [abstract]: "Memory bandwidth is the primary bottleneck on edge hardware, with model FLOP utilization (MFU) stagnating at low values (<10%) due to memory-bound operations"
  - [section]: "We currently see a generation leap in data center DL accelerators regarding memory bandwidth, which has increased significantly (up to 7.8 TB/s). Even though the memory size on embedded devices has increased, the memory bandwidth remains comparatively low (up to 0.2 TB/s)"
  - [corpus]: No direct evidence in corpus neighbors

### Mechanism 2
- Claim: Communication efficiency is critical for federated LLM training on edge devices
- Mechanism: Granularity (G) drops to near zero under LTE conditions, making distributed training impractical. Communication time dominates computation time, preventing speedup from adding more clients.
- Core assumption: Network bandwidth limitations make model gradient/weight communication prohibitively slow for large models
- Evidence anchors:
  - [abstract]: "Communication efficiency is critical—granularity (G) drops to near zero under LTE conditions, making distributed training impractical without compression or higher bandwidth"
  - [section]: "While network bandwidth in data centers is available at 100 Gbit, mobile or remote communication over wide area networks is still a difficult challenge to achieve, especially when handling multi-billion parameter DL models"
  - [corpus]: No direct evidence in corpus neighbors

### Mechanism 3
- Claim: Energy efficiency (ηe) correlates strongly with MFU, suggesting optimization opportunities for edge deployments
- Mechanism: Energy efficiency is defined as tokens per second (TPS) divided by average power draw (W). Since MFU measures computational efficiency and power draw is hardware-specific, the correlation indicates that improving MFU also improves energy efficiency.
- Core assumption: Power draw remains relatively constant during training, so improving throughput directly improves energy efficiency
- Evidence anchors:
  - [abstract]: "Energy efficiency (ηe) correlates strongly with MFU, suggesting optimization opportunities for edge deployments"
  - [section]: "We define energy efficiency as the tokens per second (TPS) throughput over the average power draw (W) for a workload: ηe = TPS/W"
  - [corpus]: No direct evidence in corpus neighbors

## Foundational Learning

- Concept: Model FLOP Utilization (MFU)
  - Why needed here: MFU is the key metric for measuring computational efficiency and identifying bottlenecks in DL training
  - Quick check question: If a model has 100 TFLOP/s theoretical capacity but only achieves 10 TFLOP/s actual throughput, what is its MFU?

- Concept: Granularity (G) in federated learning
  - Why needed here: Granularity determines whether adding more clients improves system throughput or if communication overhead dominates
  - Quick check question: If computation time is 100s and communication time is 1s, what is the granularity value and is distributed training beneficial?

- Concept: Memory-bound vs compute-bound operations
  - Why needed here: Understanding this distinction helps identify whether performance improvements should focus on memory bandwidth or computational capabilities
  - Quick check question: Which type of operation (memory-bound or compute-bound) would benefit more from increasing GPU memory bandwidth?

## Architecture Onboarding

- Component map: NVIDIA Jetson AGX Orin 64GB (edge) -> NVIDIA A100 80GB (data center) -> 10Gbit network link -> GPU-accelerated VM (server)
- Critical path: Data loading and preprocessing -> Forward pass through FLAN-T5 transformer -> Loss calculation -> Backward pass for gradient computation -> Optimizer step for weight updates -> Communication of gradients/weights between clients and server
- Design tradeoffs:
  - Memory bandwidth vs computational capacity: Edge devices prioritize memory capacity over bandwidth
  - Communication frequency vs model size: Larger models require more bandwidth for communication
  - Batch size vs memory utilization: Larger batches improve MFU but may exceed memory capacity
- Failure signatures:
  - Low MFU (<10%): Memory bandwidth bottleneck
  - Granularity near zero: Communication overhead dominates computation
  - Out of memory errors: Batch size exceeds device capacity
  - High power draw with low throughput: Inefficient computational utilization
- First 3 experiments:
  1. Measure MFU and energy efficiency across different batch sizes on Orin vs A100
  2. Calculate granularity for different network conditions (10G vs LTE)
  3. Profile step times to identify which operations are most time-consuming on edge devices

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific kernel operations within the opt.step() function are responsible for the severe memory bandwidth bottleneck on the Jetson AGX Orin?
- Basis in paper: [explicit] The paper identifies that opt.step() is taking up a significant amount of time on the Orin compared to the A100, suggesting memory bandwidth limitations, but does not specify which kernels are responsible.
- Why unresolved: The authors mention that the root causes for the slowdown and potential optimizations are interesting directions for future work, indicating that this has not been investigated yet.
- What evidence would resolve it: Detailed profiling of the opt.step() function to identify the specific memory-bound operations and kernel-level analysis to determine optimization opportunities.

### Open Question 2
- Question: How do different gradient compression techniques affect the granularity (G) in federated learning of LLMs on edge devices with limited bandwidth?
- Basis in paper: [inferred] The paper highlights that communication efficiency is critical, with granularity dropping to near zero under LTE conditions, but does not evaluate specific compression techniques.
- Why unresolved: The authors note that techniques like gradient compression have not been sufficiently evaluated in edge environments with different communication and hardware trade-offs.
- What evidence would resolve it: Experimental evaluation of various gradient compression methods (e.g., sparsification, quantization) and their impact on G across different network conditions.

### Open Question 3
- Question: What is the relationship between power draw, computational efficiency, and FL client selection strategies for optimal resource utilization?
- Basis in paper: [explicit] The authors identify a strong correlation between model FLOP utilization (MFU) and energy efficiency (ηe), suggesting this as an optimization lever for FL client selection.
- Why unresolved: While the correlation is established, the paper does not explore how this relationship can be practically applied to develop client selection strategies or model aggregation approaches.
- What evidence would resolve it: Empirical studies demonstrating how client selection based on power draw and MFU affects overall FL system performance and energy consumption.

## Limitations
- The findings may not generalize beyond the specific FLAN-T5 model family and Samsum dataset
- Memory bandwidth measurements assume representative workloads, but actual transformer training may exhibit different memory access patterns
- Energy efficiency measurements are based on theoretical power draw rather than actual hardware monitoring

## Confidence

**High Confidence**: The identification of memory bandwidth as the primary bottleneck is well-supported by hardware specifications and MFU measurements. The correlation between MFU and energy efficiency follows directly from the defined metrics.

**Medium Confidence**: The granularity analysis accurately captures the communication-computation tradeoff in federated learning, but real-world network conditions may differ significantly from simulated scenarios.

**Low Confidence**: The extrapolation of results to suggest optimization opportunities assumes that improving MFU will directly translate to practical performance gains without considering potential diminishing returns.

## Next Checks
1. **Hardware Validation**: Conduct controlled experiments measuring actual power consumption and memory bandwidth utilization across different FLAN-T5 model sizes on Jetson AGX Orin, comparing theoretical predictions with real-world measurements.

2. **Network Robustness Testing**: Implement real federated learning experiments across geographically distributed edge devices with varying network conditions (WiFi, LTE, 5G) to validate the granularity analysis and identify additional communication bottlenecks.

3. **Optimization Benchmarking**: Test proposed optimization strategies (gradient compression, model quantization, adaptive batch sizing) on edge hardware to quantify actual improvements in MFU and energy efficiency, measuring both training throughput and final model quality.