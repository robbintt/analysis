---
ver: rpa2
title: 'PLASTIC: Improving Input and Label Plasticity for Sample Efficient Reinforcement
  Learning'
arxiv_id: '2306.10711'
source_url: https://arxiv.org/abs/2306.10711
tags:
- learning
- reset
- plasticity
- reinforcement
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the sample efficiency problem in reinforcement
  learning by addressing the loss of plasticity from repeated updates. The authors
  identify two types of plasticity - input plasticity (adapting to new data) and label
  plasticity (adapting to changing input-output relationships).
---

# PLASTIC: Improving Input and Label Plasticity for Sample Efficient Reinforcement Learning

## Quick Facts
- arXiv ID: 2306.10711
- Source URL: https://arxiv.org/abs/2306.10711
- Reference count: 40
- Key outcome: Combines SAM and periodic layer resetting to improve sample efficiency in RL, achieving strong results on Atari-100k and DeepMind Control Suite benchmarks

## Executive Summary
This paper addresses the sample efficiency problem in reinforcement learning by identifying and tackling the loss of plasticity that occurs with repeated updates. The authors propose that plasticity consists of two components: input plasticity (adapting to new data) and label plasticity (adapting to changing input-output relationships). Through synthetic experiments on CIFAR-10, they demonstrate that Sharpness-Aware Minimization (SAM) improves input plasticity by finding flatter minima in the loss landscape, while periodic layer resetting improves label plasticity by preventing catastrophic forgetting. Their combined approach, PLASTIC, achieves state-of-the-art performance on challenging sample-efficient RL benchmarks with minimal architectural changes.

## Method Summary
The PLASTIC approach combines SAM optimization with periodic layer resetting to improve both input and label plasticity in RL agents. SAM is applied with perturbation parameter ρ (0.1 for Atari, 0.1-0.01 for DMC) to find flatter minima in the loss landscape, improving generalization and input adaptability. The reset mechanism periodically reinitializes the head layers (every 40,000 gradient updates for Atari, 100,000 for DMC) to maintain dense features and label adaptability. This combination creates a sparse backbone with dense head architecture that optimizes both generalization and plasticity. The method is implemented within the DrQ algorithm framework and tested on Atari-100k and DeepMind Control Suite Medium benchmarks.

## Key Results
- Achieves state-of-the-art performance on Atari-100k benchmark with significant improvements in interquartile mean scores
- Demonstrates strong sample efficiency on DeepMind Control Suite Medium tasks
- Shows that SAM alone improves input plasticity but has limited impact on label plasticity
- Reveals that the combined SAM+Reset approach creates sparse backbone and dense head features
- Achieves these results with minimal architectural changes to existing RL algorithms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SAM improves input plasticity by smoothing the loss landscape and finding flatter minima.
- Mechanism: SAM applies adversarial weight perturbations during training to find perturbations that maximally increase the loss, then updates weights in a direction that minimizes this worst-case loss. This leads to wider, smoother minima in the loss landscape which improves generalization and robustness to new input data.
- Core assumption: Flatter minima correspond to better generalization and improved ability to adapt to new inputs in RL.
- Evidence anchors:
  - [abstract]: "Synthetic experiments on the CIFAR-10 dataset reveal that finding smoother minima of loss landscape enhances input plasticity"
  - [section 3.2]: Detailed explanation of SAM's bi-level optimization and perturbation/update steps
  - [corpus]: Weak - no direct corpus evidence on SAM's effect on RL plasticity
- Break condition: If the perturbation step fails to find meaningful adversarial directions or if the loss landscape is inherently flat due to other factors.

### Mechanism 2
- Claim: Periodic layer resetting improves label plasticity by preventing catastrophic forgetting and maintaining dense features in the head.
- Mechanism: Reset mechanism periodically reinitializes the last few layers of the network, which injects new random weights that can adapt to changing input-output relationships. This maintains a higher proportion of active units in the head layers.
- Core assumption: Resetting layers periodically prevents the network from becoming too specialized to old relationships and maintains adaptability to new label distributions.
- Evidence anchors:
  - [abstract]: "refined gradient propagation improves label plasticity" and "periodic layer resetting improves label plasticity"
  - [section 4]: Synthetic experiments showing Reset mechanism increases active units in head layers and improves label adaptation
  - [corpus]: Weak - limited corpus evidence on periodic resetting in RL context
- Break condition: If the reset interval is too frequent (destroys learned knowledge) or too infrequent (fails to maintain plasticity).

### Mechanism 3
- Claim: The combination of SAM and Reset creates a sparse backbone with dense head architecture that optimizes both generalization and plasticity.
- Mechanism: SAM drives the backbone to learn sparse, generalizable features by finding flat minima, while Reset keeps the head dense and plastic. This complementary effect creates an architecture where the backbone extracts robust features and the head can adapt to changing relationships.
- Core assumption: Sparse features in backbone improve generalization while dense features in head improve plasticity, and these can be optimized simultaneously.
- Evidence anchors:
  - [abstract]: "Integrating both methods generates a model with sparse feature extraction layers (the backbone) and dense prediction layers (the head)"
  - [section 5.2]: Empirical results showing SAM reduces active units in backbone while Reset increases active units in head
  - [corpus]: Weak - no direct corpus evidence on this specific architectural combination
- Break condition: If the interaction between SAM and Reset creates conflicting gradients or if the reset destroys beneficial sparse features learned in the backbone.

## Foundational Learning

- Concept: Sharpness-Aware Minimization (SAM)
  - Why needed here: SAM is the core optimization technique that improves input plasticity by finding flatter minima in the loss landscape.
  - Quick check question: What is the key difference between standard gradient descent and SAM's update procedure?

- Concept: Catastrophic forgetting and plasticity in neural networks
  - Why needed here: Understanding why models lose the ability to adapt to new data is crucial for grasping why Reset mechanism is needed.
  - Quick check question: Why does repeated training on the same data typically reduce a model's ability to adapt to new data?

- Concept: Loss landscape geometry and generalization
  - Why needed here: The relationship between loss landscape curvature (sharp vs flat minima) and generalization performance is fundamental to understanding SAM's mechanism.
  - Quick check question: How does the curvature of the loss landscape at a minimum relate to the model's generalization performance?

## Architecture Onboarding

- Component map:
  - Environment -> Data collection -> SAM perturbation step -> SAM update step -> Periodic Reset -> Target network update (EMA)

- Critical path:
  1. Environment interaction and data collection
  2. SAM perturbation step (optional with noisy layers)
  3. SAM update step with computed gradients
  4. Periodic Reset of head layers
  5. Target network update (EMA)

- Design tradeoffs:
  - SAM parameter ρ: Larger values increase perturbation strength but may destabilize training
  - Reset interval: Tradeoff between maintaining plasticity and preserving learned knowledge
  - SAM application scope: Whole network vs backbone only vs head only
  - Noisy layers interaction: Whether to include noise during SAM perturbation step

- Failure signatures:
  - Performance plateaus despite continued training (loss of plasticity)
  - High variance in training loss (inappropriate SAM parameter)
  - Degradation in performance after reset (reset interval too frequent)
  - Overfitting to early experiences (insufficient generalization)

- First 3 experiments:
  1. Apply SAM to Rainbow agent on Atari-100k benchmark with ρ=0.1
  2. Add Reset mechanism with interval of 40,000 gradient updates
  3. Compare performance with SAM+Reset vs SAM alone vs Reset alone on both Atari-100k and DMC benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific mechanisms by which Sharpness-Aware Minimization (SAM) improves input plasticity while having limited impact on label plasticity?
- Basis in paper: [explicit] The authors state that "SAM primarily enhances the model's input adaptation while having a limited impact on label adaptation" and that "SAM displayed its strength in enhancing the model's input adaptability and promoting sparser features, as shown by a reduction in the active units of the model's backbone."
- Why unresolved: The paper does not provide a detailed explanation of the underlying mechanisms by which SAM achieves these effects.
- What evidence would resolve it: Further experiments and analysis to understand the specific impact of SAM on the network's feature extraction and prediction layers, and how this relates to input and label plasticity.

### Open Question 2
- Question: How do different combinations of generalization and plasticity-enhancing techniques compare in terms of their effectiveness for sample-efficient reinforcement learning?
- Basis in paper: [explicit] The authors mention that "we further explored the effectiveness of different synergies to enhance the generalization and plasticity of our reinforcement learning model" and found that "the SAM + CReLU combination performs competitively with our proposed SAM + reset strategy."
- Why unresolved: The paper only briefly mentions this comparison and does not provide a comprehensive analysis of the different combinations.
- What evidence would resolve it: Extensive experiments comparing various combinations of generalization and plasticity techniques, such as SAM + L2 regularization, SAM + Layer Normalization, and others, to determine the most effective approaches.

### Open Question 3
- Question: How does the combined usage of SAM and Reset affect the sample efficiency of model-based reinforcement learning algorithms?
- Basis in paper: [inferred] The authors state that their approach "has not been validated in model-based algorithms with state-of-the-art sample efficiency" and express optimism about its potential for improvement.
- Why unresolved: The paper focuses on model-free algorithms and does not explore the impact of SAM and Reset on model-based methods.
- What evidence would resolve it: Experiments applying SAM and Reset to model-based reinforcement learning algorithms, such as DreamerV3, and comparing their sample efficiency to the baseline methods.

## Limitations
- The empirical validation relies heavily on synthetic CIFAR-10 experiments to justify SAM and Reset mechanisms
- Direct evidence for these mechanisms' effectiveness specifically in RL contexts remains limited
- The interaction between SAM's perturbation mechanism and noisy layers in the Rainbow variant introduces additional complexity
- No validation on model-based RL algorithms with state-of-the-art sample efficiency

## Confidence
- High confidence: The core observation that RL agents lose plasticity with repeated updates, and the empirical demonstration of improved sample efficiency on Atari-100k and DMC benchmarks
- Medium confidence: The proposed mechanisms (SAM for input plasticity, Reset for label plasticity) based on synthetic experiments
- Low confidence: The specific architectural claims about sparse backbone vs dense head resulting from the combined approach

## Next Checks
1. Replicate the synthetic CIFAR-10 experiments to verify that SAM consistently finds flatter minima and that Reset maintains active units in prediction layers
2. Conduct ablation studies isolating SAM and Reset effects on Atari-100k performance to quantify individual contributions
3. Test the approach on additional RL benchmarks beyond Atari and DMC to assess generalizability of the plasticity improvements