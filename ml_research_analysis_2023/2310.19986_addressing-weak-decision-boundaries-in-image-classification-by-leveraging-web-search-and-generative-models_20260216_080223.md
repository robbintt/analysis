---
ver: rpa2
title: Addressing Weak Decision Boundaries in Image Classification by Leveraging Web
  Search and Generative Models
arxiv_id: '2310.19986'
source_url: https://arxiv.org/abs/2310.19986
tags:
- data
- image
- bias
- images
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the issue of bias and weak decision boundaries
  in image classifiers, particularly for underrepresented and vulnerable populations.
  The core method leverages web search and generative models (DALL-E 2, Stable Diffusion)
  to identify weakspots in classifier decision boundaries and procure new training
  samples to enhance robustness and mitigate bias.
---

# Addressing Weak Decision Boundaries in Image Classification by Leveraging Web Search and Generative Models

## Quick Facts
- **arXiv ID**: 2310.19986
- **Source URL**: https://arxiv.org/abs/2310.19986
- **Reference count**: 8
- **Key outcome**: Significant improvement in image classifier accuracy (80.12% to 84.09%) and reduction in gender accuracy disparity (6.08% to 1.38%, 77.30% reduction) by leveraging web search and generative models to identify and address weak decision boundaries

## Executive Summary
This paper addresses bias and weak decision boundaries in image classifiers, particularly for underrepresented and vulnerable populations. The proposed method leverages web search and generative models (DALL-E 2, Stable Diffusion) to identify weakspots in classifier decision boundaries and procure new training samples to enhance robustness and mitigate bias. By identifying pivotal images representing weakspots, generating detailed textual descriptions, and using these to retrieve or generate new training samples, the approach significantly improves both overall accuracy and fairness metrics. The method was evaluated on a subset of ImageNet's People Subtree, focusing on 40 occupation-related classes, demonstrating substantial improvements in both performance and bias reduction.

## Method Summary
The method works by first identifying weakspots in the classifier's decision boundary using FAISS similarity search to find misclassified instances with high local perplexity, marking them as pivotal images. These pivotal images are then described using caption models enhanced with scene metadata, which are used to procure new training samples through web search and text-to-image models. The original training set is augmented with these strategically procured samples and the classifier is retrained. This targeted approach to data augmentation directly addresses performance disparities for underrepresented groups by neutralizing spurious correlations and bias patterns that cause misclassification.

## Key Results
- Overall accuracy improved from 80.12% to 84.09% after retraining with augmented dataset
- Gender accuracy disparity reduced from 6.08% to 1.38% (77.30% reduction)
- Total of 2,144 neutralizing training samples procured, increasing training set size by 3.32%
- Enhanced classifier exhibited fewer weakspots and increased separation between classes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Identifying weakspots in decision boundaries enables targeted data augmentation that directly addresses performance disparities for underrepresented groups.
- Mechanism: The approach uses FAISS similarity search to find misclassified instances with high local perplexity, marking them as pivotal images. These images are then used to generate detailed textual descriptions and procure new training samples that neutralize spurious correlations and bias patterns.
- Core assumption: Misclassified instances near high-perplexity regions in latent space are representative of the model's decision boundary weaknesses for vulnerable populations.
- Evidence anchors: [abstract] "Our new method is able to (1) identify weak decision boundaries for such classes..."; [section] "We adopt a powerful tool that uses GPU acceleration to perform similarity search, Facebook FAISS [Johnson et al., 2019], thus improving the efficiency of weakspot search."
- Break condition: If pivotal images do not actually represent the root cause of misclassification, augmentation will fail to address the underlying bias or robustness issues.

### Mechanism 2
- Claim: Combining web search and generative models provides diverse, high-quality training samples that traditional augmentation cannot match.
- Mechanism: Pivotal images are described using caption models enhanced with scene metadata, then used to retrieve or generate new images via Google Search and text-to-image models (DALL-E 2, Stable Diffusion). This approach leverages the flexibility and realism of generative models to create diverse samples that address specific weak regions.
- Core assumption: Text-to-image models can generate realistic images that accurately reflect the described scenarios and demographics, including underrepresented groups.
- Evidence anchors: [abstract] "Our new method is able to (2) construct search queries for Google as well as text for generating images through DALL-E 2 and Stable Diffusion..."; [section] "The recently released text-to-image generative models, such as DALL-E 2 [Ramesh et al., 2021] and Stable Diffusion [Rombach et al., 2022], are able to generate high-quality super-realistic images that accurately match the text description of the pivotal image."
- Break condition: If generative models fail to accurately represent minority demographics or produce biased outputs, the augmentation will perpetuate or worsen existing disparities.

### Mechanism 3
- Claim: Fine-tuning the classifier with strategically procured samples improves both overall accuracy and reduces demographic accuracy disparity without compromising performance on majority groups.
- Mechanism: The original training set is augmented with neutralizing samples targeting weakspots and spurious correlations, then the classifier is retrained. This targeted retraining improves decision boundary definition and inter-class separation.
- Core assumption: A small number of high-quality, strategically chosen samples can significantly reshape the decision boundary to be more robust and fair.
- Evidence anchors: [abstract] "While still improving the model's overall performance considerably, we achieve a significant reduction (77.30%) in the model's gender accuracy disparity."; [section] "A total of 2,144 neutralizing training samples were procured, increasing the training set size by 3.32%. Despite being a relatively small-sized addition, the strategically crafted training samples resulted in a considerable improvement in the model's performance."
- Break condition: If the augmented dataset introduces new spurious correlations or if the model overfits to the new samples, performance gains may not generalize.

## Foundational Learning

- **Concept**: Latent space and decision boundaries in neural networks
  - Why needed here: Understanding how classifiers partition feature space is crucial for identifying weakspots and interpreting the impact of augmentation.
  - Quick check question: What is the role of the decision boundary in a classifier, and how can it be visualized or analyzed in latent space?

- **Concept**: Bias and spurious correlations in machine learning
  - Why needed here: Recognizing how models learn unintended patterns is essential for diagnosing and mitigating bias in underrepresented groups.
  - Quick check question: How can spurious correlations in training data lead to biased model behavior, and what are common examples in image classification?

- **Concept**: Text-to-image generative models and their capabilities
  - Why needed here: Understanding the strengths and limitations of models like DALL-E 2 and Stable Diffusion is necessary for effective use in data augmentation.
  - Quick check question: What are the key differences between web search and text-to-image generation for procuring training samples, and when is each approach most appropriate?

## Architecture Onboarding

- **Component map**: Original classifier -> FAISS similarity search -> Pivotal image detection -> Caption generation with scene metadata -> Web search and generative models -> New training samples -> Retrained classifier

- **Critical path**:
  1. Identify pivotal images via FAISS and weakspot search
  2. Generate detailed textual descriptions for pivotal images
  3. Procure neutralizing samples via web search and generative models
  4. Augment training set and retrain classifier

- **Design tradeoffs**:
  - Speed vs. accuracy: FAISS accelerates search but may miss subtle weakspots
  - Diversity vs. realism: Generative models provide control but may not always produce realistic images for minority demographics
  - Targeted vs. general augmentation: Focusing on weakspots may miss broader issues but is more efficient

- **Failure signatures**:
  - No improvement in accuracy or bias after retraining
  - New spurious correlations introduced by generative samples
  - Pivotal images fail to represent the true cause of misclassification

- **First 3 experiments**:
  1. Run weakspot detection on a small subset of the test set and manually verify pivotal images
  2. Generate a few textual descriptions and retrieve or generate corresponding images, checking for realism and diversity
  3. Augment the training set with a small number of neutralizing samples and retrain, measuring impact on both overall accuracy and demographic-specific accuracy

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several important questions arise from the research:

### Open Question 1
- Question: How does the proposed method scale when applied to larger datasets and more diverse image classification tasks beyond the ImageNet People Subtree subset?
- Basis in paper: [inferred] The paper demonstrates the method on a subset of ImageNet with 40 occupation-related classes. It mentions the technique is extendable to a wide range of problems and domains, but does not provide evidence for scaling to larger datasets or different classification tasks.
- Why unresolved: The paper does not present experiments or analysis on larger datasets or different classification domains, leaving uncertainty about the method's effectiveness and efficiency when scaled up or applied to other tasks.
- What evidence would resolve it: Conducting experiments on larger datasets (e.g., full ImageNet, COCO) and diverse classification tasks (e.g., medical imaging, satellite imagery) with quantitative comparisons to baseline methods would provide evidence for the method's scalability and generalizability.

### Open Question 2
- Question: How does the proposed method handle cases where generative models produce unrealistic or biased outputs, and how can the quality of generated images be systematically evaluated?
- Basis in paper: [inferred] The paper mentions using DALL-E 2 and Stable Diffusion to generate images but does not discuss how to handle cases where generated images are unrealistic or biased, nor does it provide a systematic evaluation of the quality of generated images.
- Why unresolved: The paper does not address the potential limitations of generative models in producing high-quality, unbiased images, nor does it propose a method to evaluate the quality and realism of generated images, which is crucial for the success of the proposed approach.
- What evidence would resolve it: Conducting a user study to evaluate the quality and realism of generated images, as well as analyzing the potential biases in the generated outputs, would provide insights into the limitations of the method and suggest ways to improve the quality of generated images.

### Open Question 3
- Question: How does the proposed method compare to other existing approaches for addressing bias and improving robustness in image classification models?
- Basis in paper: [inferred] The paper mentions related works on data augmentation, robustness, and bias mitigation but does not provide a direct comparison of the proposed method with existing approaches in terms of effectiveness and efficiency.
- Why unresolved: The paper does not present a quantitative comparison of the proposed method with other state-of-the-art approaches for addressing bias and improving robustness in image classification models, making it difficult to assess its relative performance and advantages.
- What evidence would resolve it: Conducting experiments comparing the proposed method with other existing approaches on benchmark datasets and evaluation metrics for bias and robustness would provide a clearer understanding of its strengths and limitations compared to alternative methods.

## Limitations
- The specific thresholds and heuristics for weakspot detection are not fully specified, which could impact reproducibility
- Effectiveness demonstrated only on a single dataset (ImageNet People Subtree), raising questions about generalizability to other domains or more diverse populations
- Does not address potential ethical concerns around using generative models to create images of people, particularly for vulnerable groups

## Confidence
- **High confidence** in the mechanism for identifying weakspots using FAISS and perplexity-based similarity search
- **Medium confidence** in the effectiveness of web search and generative models for procuring diverse, high-quality training samples
- **Medium confidence** in the overall impact on bias mitigation and robustness, given the results are based on a single dataset and specific class subset

## Next Checks
1. Test the weakspot identification pipeline on a held-out subset of the dataset and manually verify that the detected pivotal images actually represent decision boundary weaknesses rather than random noise or labeling errors.
2. Evaluate the diversity and realism of images generated by DALL-E 2 and Stable Diffusion for minority demographics by conducting a human evaluation study focused on representation quality and potential biases in the generated outputs.
3. Apply the method to a different dataset or domain (e.g., medical imaging or satellite imagery) to assess generalizability and identify any dataset-specific limitations or failure modes.