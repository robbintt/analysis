---
ver: rpa2
title: Large Language Model Routing with Benchmark Datasets
arxiv_id: '2309.15789'
source_url: https://arxiv.org/abs/2309.15789
tags:
- tasks
- task
- learning
- benchmark
- correctness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of selecting the best LLM for a
  new task by repurposing benchmark datasets to train a "router" model. The key insight
  is that the per-sample performance data from benchmarks can be used to learn the
  strengths and weaknesses of various LLMs across different tasks and domains.
---

# Large Language Model Routing with Benchmark Datasets

## Quick Facts
- arXiv ID: 2309.15789
- Source URL: https://arxiv.org/abs/2309.15789
- Reference count: 34
- Key outcome: This paper proposes repurposing benchmark datasets to train a "router" model for selecting the best LLM for a new task, consistently outperforming using any single model across multiple datasets.

## Executive Summary
This paper addresses the challenge of selecting the optimal large language model (LLM) for a given task by learning from benchmark datasets. The key insight is that the per-sample performance data from benchmarks can be used to train a router model that captures the strengths and weaknesses of various LLMs across different tasks and domains. The method reformulates LLM selection as a collection of binary classification tasks, where the features are input embeddings and the labels indicate whether a model performs well on a given input. Three scoring methods are proposed for routing, with the adaptive score that accounts for out-of-distribution data consistently outperforming single-model approaches.

## Method Summary
The method involves formulating LLM selection as a collection of binary classification tasks, where correctness predictors are trained to estimate the probability of an LLM producing a correct output for any given input. Benchmark datasets provide per-sample performance data across tasks, allowing training of these predictors using kNN classifiers with sentence transformer embeddings. Three routing scores are proposed: a simple average of predicted correctness, a thresholded version, and an adaptive score that accounts for the potential imperfection of correctness predictors on out-of-distribution data. The adaptive score consistently outperforms using any single model for all tasks while maintaining computational efficiency.

## Key Results
- The adaptive routing score (S3) consistently outperforms using any single model for all tasks
- The approach demonstrates computational efficiency while maintaining high accuracy
- Routing with the proposed scores reduces costs by selecting smaller models when appropriate
- The method achieves higher performance than the best model on average (BMA) across 29 datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning from benchmark datasets enables accurate routing to the best LLM for a new task by modeling LLM correctness as a binary classification problem.
- Mechanism: Benchmark datasets provide per-sample performance data across tasks, allowing training of correctness predictors that estimate the probability of an LLM producing a correct output for any given input. These predictors are then used to score candidate LLMs on new tasks.
- Core assumption: The strengths and weaknesses of LLMs on various tasks captured by benchmark data generalize to new, related tasks.
- Evidence anchors:
  - [abstract] "We propose a new formulation for the problem, in which benchmark datasets are repurposed to learn a 'router' model for this LLM selection, and we show that this problem can be reduced to a collection of binary classification tasks."
  - [section] "We formalize the problem of learning the strengths and weaknesses of LLMs for downstream routing... as a collection of binary classification problems."
  - [corpus] Weak: No direct mentions of binary classification formulations in related papers, suggesting this is a novel approach.
- Break condition: If the new task is too dissimilar from the benchmark datasets, the correctness predictors may fail to generalize, leading to poor routing decisions.

### Mechanism 2
- Claim: Accounting for the potential imperfection of correctness predictors on out-of-distribution data improves routing accuracy.
- Mechanism: A confidence model is used to estimate the accuracy of the binary correctness predictors on new tasks. The routing score is then adjusted based on this estimated accuracy, shrinking predictions towards the average model when confidence is low.
- Core assumption: The accuracy of the correctness predictors can be reliably estimated for new tasks based on their similarity to benchmark datasets.
- Evidence anchors:
  - [abstract] "Our third score is designed to account for mistakes a correctness predictor can make on the (out-of-distribution) data from a new task."
  - [section] "To address this issue, we model the out-of-distribution confidence of the predictions... We treat the problem of estimating p(dâ€², m) as a supervised learning task."
  - [corpus] Weak: No direct mentions of OOD confidence modeling in related papers, suggesting this is a novel contribution.
- Break condition: If the similarity measure between tasks is inaccurate or the kernel smoother fails to capture the relationship between task similarity and predictor accuracy, the confidence estimates will be unreliable.

### Mechanism 3
- Claim: Routing with the proposed scores consistently outperforms using any single model for all tasks, while also being computationally efficient.
- Mechanism: The routing scores (S1, S2, S3) are used to select the best LLM for a new task based on the predictions of the correctness predictors. S3, which accounts for predictor imperfection, consistently outperforms the others and the best model on average (BMA).
- Core assumption: The routing scores accurately rank the candidate LLMs for each new task.
- Evidence anchors:
  - [abstract] "We demonstrate the utility and limitations of learning model routers from various benchmark datasets, where we consistently improve performance upon using any single model for all tasks."
  - [section] "Our contributions are summarized below: We verify the efficiency of our model routing scores empirically on 29 datasets from HELM... showing that S3 consistently outperforms using any single model for all tasks."
  - [corpus] Weak: No direct mentions of consistently outperforming BMA in related papers, suggesting this is a key result of the proposed approach.
- Break condition: If the routing scores fail to accurately rank the LLMs for a significant portion of the tasks, the overall performance will degrade.

## Foundational Learning

- Concept: Binary classification
  - Why needed here: The core problem of LLM routing is reformulated as a collection of binary classification tasks, where the goal is to predict whether a given LLM will be "correct" on an input.
  - Quick check question: What is the difference between a binary classification problem and a multi-class classification problem?

- Concept: Out-of-distribution (OOD) generalization
  - Why needed here: The correctness predictors need to generalize to new tasks that are different from the benchmark datasets used for training. The proposed confidence model aims to account for potential performance degradation on OOD data.
  - Quick check question: What are some common strategies for improving OOD generalization in machine learning models?

- Concept: k-Nearest Neighbors (kNN) classifier
  - Why needed here: A simple kNN classifier is used as the correctness predictor in the experiments. Understanding how kNN works and its properties is essential for interpreting the results and potential limitations of the approach.
  - Quick check question: How does the choice of k in a kNN classifier affect its bias-variance tradeoff?

## Architecture Onboarding

- Component map:
  - Benchmark datasets -> Sentence transformer -> kNN classifier -> Dataset distance -> Kernel smoother -> Routing scores (S1, S2, S3) -> Selected LLM

- Critical path:
  1. Embed benchmark dataset inputs using sentence transformer
  2. Train kNN correctness predictors for each LLM on the embedded benchmark data
  3. Compute dataset distances between benchmark tasks and new task
  4. Estimate predictor accuracy on new task using kernel smoother
  5. Compute routing scores and select best LLM

- Design tradeoffs:
  - Simplicity vs. accuracy: The kNN classifier is simple but may not capture complex relationships between inputs and LLM correctness.
  - Computational cost vs. performance: Using more sophisticated predictors or confidence models may improve accuracy but increase computation time.
  - Task similarity vs. routing performance: The proposed approach relies on the assumption that new tasks are similar to the benchmark datasets. If this assumption is violated, performance may degrade.

- Failure signatures:
  - Low accuracy of correctness predictors on benchmark data: Indicates that the kNN classifier is not a good fit for the data or that the sentence transformer embeddings are not capturing the relevant features.
  - High variance in routing scores across different runs: Suggests that the kernel smoother estimates are unstable or that the dataset distance measure is not reliable.
  - Routing scores not correlating with actual LLM performance on new tasks: Implies that the correctness predictors are not generalizing well to new tasks or that the confidence model is inaccurate.

- First 3 experiments:
  1. Evaluate the accuracy of the kNN correctness predictors on a held-out portion of the benchmark datasets to assess their fit to the data.
  2. Compute the dataset distances between all pairs of benchmark tasks to verify that the measure captures task similarity as expected.
  3. Estimate the accuracy of the correctness predictors on a small subset of a new task (e.g., 10 samples) to validate the kernel smoother's performance before applying it to the full task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the LLM routing system scale with the number and diversity of benchmark datasets?
- Basis in paper: [inferred] The paper discusses using benchmark datasets to train a "router" model for selecting the best LLM for a new task. It mentions that learning with more benchmarks can improve the efficacy and reliability of the LLM routers.
- Why unresolved: The paper only provides initial experiments with a limited set of benchmark datasets and does not thoroughly investigate the impact of using a larger and more diverse set of benchmarks on the performance of the routing system.
- What evidence would resolve it: Conducting experiments with varying numbers and types of benchmark datasets to evaluate the impact on the routing system's performance, such as accuracy, efficiency, and robustness.

### Open Question 2
- Question: How can the OOD (out-of-distribution) generalization gap of correctness predictors be effectively reduced for better LLM routing?
- Basis in paper: [explicit] The paper acknowledges the challenge of OOD generalization when predicting LLM correctness on new tasks and proposes a simple method to account for this issue by estimating the accuracy of correctness predictors on new tasks.
- Why unresolved: While the paper presents a method to address the OOD gap, it does not explore other potential techniques or combinations of methods that could further improve the performance of correctness predictors on out-of-distribution data.
- What evidence would resolve it: Investigating and comparing various methods for improving OOD generalization, such as domain adaptation, meta-learning, or active learning, and evaluating their impact on the performance of correctness predictors and the overall LLM routing system.

### Open Question 3
- Question: How can the LLM routing system be adapted to encourage the selection of smaller, more cost-effective models when their performance is comparable to larger models?
- Basis in paper: [explicit] The paper discusses the potential of using smaller LLMs to reduce costs and improve efficiency but does not provide a detailed approach for incorporating this consideration into the routing system.
- Why unresolved: While the paper demonstrates the feasibility of routing smaller models in some cases, it does not explore how to systematically modify the routing scores or introduce additional criteria to prioritize smaller models when appropriate.
- What evidence would resolve it: Developing and evaluating modified routing scores or additional selection criteria that take into account model size, cost, or efficiency, and assessing their impact on the overall performance and cost-effectiveness of the LLM routing system.

## Limitations
- The method relies heavily on the quality and coverage of benchmark datasets, which may not fully represent real-world task distributions.
- The kNN-based approach may struggle with complex input-output relationships and high-dimensional embedding spaces.
- The confidence estimation via kernel smoothing assumes smooth similarity relationships between tasks that may not hold in practice.

## Confidence
- **High confidence**: The binary classification formulation for routing is theoretically sound and well-supported by the paper's formalization.
- **Medium confidence**: The OOD confidence estimation mechanism is novel but lacks extensive empirical validation beyond the presented experiments.
- **Medium confidence**: The consistent improvement over BMA across 29 datasets is promising but the specific conditions under which this holds need further exploration.

## Next Checks
1. Conduct ablation studies removing the confidence adjustment component to quantify its actual contribution to routing performance across different task similarity levels.
2. Test the approach on tasks deliberately selected to be dissimilar from benchmark datasets to measure the breakdown point of the routing mechanism.
3. Compare the kNN correctness predictors against more sophisticated alternatives (e.g., neural classifiers) to assess whether the simplicity of kNN is a feature or limitation for this application.