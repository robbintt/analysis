---
ver: rpa2
title: 'FedYolo: Augmenting Federated Learning with Pretrained Transformers'
arxiv_id: '2307.04905'
source_url: https://arxiv.org/abs/2307.04905
tags:
- learning
- federated
- fedavg
- local
- ptfs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates using large pretrained transformers (PTFs)
  in federated learning to improve personalization, robustness, and multitask learning.
  The authors explore two key aspects: scale (model size) and modularity (parameter-efficient
  tuning methods like adapters).'
---

# FedYolo: Augmenting Federated Learning with Pretrained Transformers

## Quick Facts
- arXiv ID: 2307.04905
- Source URL: https://arxiv.org/abs/2307.04905
- Reference count: 40
- Primary result: Large pretrained transformers with modular updates improve federated learning efficiency, robustness, and multitask performance

## Executive Summary
This paper explores the use of large pretrained transformers (PTFs) in federated learning to address challenges in personalization, robustness, and multitask learning. The authors investigate two key aspects: scale (model size) and modularity (parameter-efficient tuning methods like adapters). Experiments on CIFAR, CelebA, and FEMNIST datasets demonstrate that larger PTFs improve few-shot learning accuracy and reduce the gap between local-only and federated training. Modularity enables over 100× reduction in communication costs while boosting robustness to heterogeneous data and mitigating catastrophic forgetting in multitask settings.

## Method Summary
The study investigates how large pretrained transformers can enhance federated learning through experiments on CIFAR, CelebA, and FEMNIST datasets using Vision Transformers of varying scales (ViT-Tiny to ViT-Large). Three training approaches are compared: Local-only learning, FedAvg, and FedAvg+Local, with both full-update and modular-update strategies. Modular updates use adapter modules while keeping the PTF backbone frozen. The proposed FedYolo approach assigns isolated modules to each task, enabling efficient multitask federated learning without catastrophic forgetting. Experiments evaluate accuracy, communication efficiency, and robustness to heterogeneous data distributions.

## Key Results
- Larger PTFs improve few-shot learning accuracy and reduce the accuracy gap between federated and local-only training
- Modular updates enable >100× reduction in communication costs while improving robustness to heterogeneous data
- FedYolo consistently outperforms conventional FedAvg in multitask settings, particularly for smaller PTFs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger pretrained transformers (PTFs) reduce the accuracy gap between federated and local-only training.
- Mechanism: Larger PTFs capture richer feature representations, requiring fewer parameter updates to adapt to new tasks, thereby reducing the need for extensive collaboration.
- Core assumption: The representational capacity of larger PTFs is sufficient to generalize well from limited local data.
- Evidence anchors:
  - [abstract] "Larger scale shrinks the accuracy gaps between alternative approaches and improves heterogeneity robustness."
  - [section 4.1] "The results show that Local-only learning becomes increasingly competitive with FedAvg as the model scale increases"
  - [corpus] No direct citations supporting this mechanism found in related papers.
- Break condition: If the local data is too heterogeneous or limited, even large PTFs may fail to generalize without collaboration.

### Mechanism 2
- Claim: Modular updates improve robustness to heterogeneous data distributions.
- Mechanism: By only updating small parameter-efficient modules while keeping the PTF backbone frozen, modular updates prevent catastrophic interference between diverse client data distributions.
- Core assumption: The frozen PTF backbone contains sufficient general features to serve as a common representation across tasks.
- Evidence anchors:
  - [abstract] "Modularity, by design, enables >100× less communication in bits. Surprisingly, it also boosts the generalization capability of local adaptation methods and the robustness of smaller PTFs."
  - [section 4.2] "Employing larger PTFs or modular update maintains accuracy even under significant heterogeneity."
  - [corpus] No direct citations supporting this mechanism found in related papers.
- Break condition: If the task requires significant changes to the backbone features, modular updates may underperform full updates.

### Mechanism 3
- Claim: FedYolo enables efficient multitask federated learning by assigning isolated modules to each task.
- Mechanism: Each task has its own dedicated module, preventing interference and catastrophic forgetting when clients learn multiple unrelated tasks simultaneously.
- Core assumption: Tasks can be effectively separated by their module assignments without requiring backbone modifications.
- Evidence anchors:
  - [abstract] "it enables clients to solve multiple unrelated tasks simultaneously using a single PTF, whereas full updates are prone to catastrophic forgetting."
  - [section 5] "FedYolo consistently outperforms conventional FedAvg, particularly with more tasks and for smaller PTFs."
  - [corpus] No direct citations supporting this mechanism found in related papers.
- Break condition: If tasks share significant feature overlap, isolated modules may lead to redundant learning and inefficiency.

## Foundational Learning

- Concept: Federated Learning (FL)
  - Why needed here: The paper builds on FL as the foundation for distributed model training across heterogeneous clients.
  - Quick check question: What is the primary challenge that federated learning aims to address compared to centralized training?

- Concept: Pretrained Transformers (PTFs)
  - Why needed here: PTFs provide a strong initialization that can be adapted to various downstream tasks with minimal fine-tuning.
  - Quick check question: How does the scale of a pretrained transformer impact its ability to generalize from few-shot examples?

- Concept: Parameter-efficient Tuning (Modular Updates)
  - Why needed here: Modular updates like adapters and LoRA allow efficient adaptation of large PTFs without full fine-tuning.
  - Quick check question: What is the key advantage of modular updates over full fine-tuning in federated learning?

## Architecture Onboarding

- Component map:
  - Pretrained Transformer (PTF) backbone - frozen during training
  - Task-specific modules (adapters, LoRA, prompts) - updated and communicated
  - Classifier heads - dataset-specific output layers
  - Federated averaging aggregation - combines module updates from clients

- Critical path:
  1. Load frozen PTF backbone on client devices
  2. Initialize task-specific modules for each learning task
  3. Perform local training on client data using only modules
  4. Communicate module updates to server
  5. Aggregate module updates via federated averaging
  6. Distribute aggregated modules back to clients

- Design tradeoffs:
  - Larger PTFs provide better generalization but require more memory and computation
  - More modules enable more tasks but increase communication and storage overhead
  - Frozen backbones ensure consistency but limit adaptation flexibility

- Failure signatures:
  - Poor accuracy: modules too small or tasks too dissimilar from pretraining
  - Slow convergence: learning rate too low or communication frequency too sparse
  - Catastrophic forgetting: tasks too related or module isolation too strict

- First 3 experiments:
  1. Compare accuracy of full-update vs modular-update on homogeneous CIFAR-10 data
  2. Test robustness of different PTF scales under heterogeneous client data distributions
  3. Evaluate communication efficiency of modular updates across ViT-Tiny to ViT-Large models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance advantage of larger PTFs persist when equalizing the number of trainable parameters across different scales?
- Basis in paper: [inferred] The authors conducted experiments varying the number of parameters in the modules while keeping the total number of parameters equal to that of the ViT-L. They found that increasing the number of parameters for smaller PTFs does not necessarily lead to improved performance.
- Why unresolved: The experiments focused on comparing different scales of PTFs with varying module sizes. However, it is unclear whether the performance advantage of larger PTFs is solely due to the larger number of parameters or if there are other factors at play.
- What evidence would resolve it: Conducting experiments with different scales of PTFs while equalizing the number of trainable parameters across all scales would provide insights into whether the performance advantage of larger PTFs persists when the number of parameters is controlled.

### Open Question 2
- Question: How do the benefits of large-scale PTFs and modularity translate to other challenging settings, such as unlabeled data or non-stationary distributions?
- Basis in paper: [explicit] The authors mention that ideally, PTFs will also prove helpful in other challenging settings, such as when clients have unlabeled data or non-stationary distributions.
- Why unresolved: The experiments conducted in the paper focused on labeled data and stationary distributions. The authors acknowledge that the benefits of large-scale PTFs and modularity in these challenging settings remain unexplored.
- What evidence would resolve it: Conducting experiments with unlabeled data or non-stationary distributions and evaluating the performance of large-scale PTFs and modularity in these settings would provide insights into their effectiveness in challenging scenarios.

### Open Question 3
- Question: Can more sophisticated methods of FedYolo, such as assigning shared modules across tasks or clients and searching for optimal module placements within PTF, further improve performance?
- Basis in paper: [explicit] The authors mention that it might be interesting to devise more sophisticated methods of FedYolo by assigning shared modules across tasks or clients and searching for optimal module placements within PTF.
- Why unresolved: The paper proposes FedYolo as a simple and intuitive idea, but the authors acknowledge that more sophisticated methods could potentially enhance its performance. The specific strategies for assigning shared modules and optimizing module placements are not explored.
- What evidence would resolve it: Conducting experiments with different strategies for assigning shared modules and optimizing module placements within PTF, and comparing their performance with the baseline FedYolo approach, would provide insights into the potential improvements achievable through more sophisticated methods.

## Limitations
- Evaluation focuses primarily on image classification tasks, limiting generalizability to other domains
- Communication efficiency claims don't account for storage overhead of maintaining multiple task-specific modules
- Experiments use controlled heterogeneity settings that may not capture real-world data distribution shifts

## Confidence

- PTF scale improves generalization (High): Supported by consistent experimental results across multiple datasets and scales
- Modular updates improve robustness (Medium): Evidence shows benefits, but the mechanism could benefit from deeper analysis
- FedYolo prevents catastrophic forgetting (High): Demonstrated through multitask experiments with clear performance improvements
- Communication efficiency claims (Medium): Theoretical advantages are shown, but practical implementation overhead not fully characterized

## Next Checks

1. **Scale efficiency validation**: Systematically evaluate the trade-off between PTF scale and adaptation efficiency across diverse task types beyond image classification to establish broader applicability

2. **Communication overhead measurement**: Conduct real-world measurements of end-to-end communication costs including storage, transmission, and processing overhead for FedYolo with varying numbers of tasks

3. **Heterogeneity stress test**: Design experiments with extreme data heterogeneity and non-IID distributions to identify the breaking points where modular updates and large PTFs fail to maintain performance