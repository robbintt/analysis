---
ver: rpa2
title: On the Relationship Between Interpretability and Explainability in Machine
  Learning
arxiv_id: '2311.11491'
source_url: https://arxiv.org/abs/2311.11491
tags:
- predictor
- interpretability
- learning
- explainability
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper argues that interpretability and explainability are
  complementary, not substitutes. While explainability aims to provide insights into
  black-box models through simplified approximations, it has inherent flaws: explanations
  are necessarily wrong, require blind trust, and may be misaligned with user understanding.'
---

# On the Relationship Between Interpretability and Explainability in Machine Learning

## Quick Facts
- arXiv ID: 2311.11491
- Source URL: https://arxiv.org/abs/2311.11491
- Reference count: 40
- Primary result: The paper argues that interpretability and explainability are complementary, not substitutes, and combining them mitigates the flaws of each approach.

## Executive Summary
This paper challenges the common perception that interpretability and explainability are competing approaches in machine learning. Through theoretical analysis, it demonstrates that these two concepts are actually complementary, each addressing fundamental limitations of the other. The paper identifies specific flaws in both approaches—such as the inherent wrongness of explanations, blind trust requirements, and insufficient information from interpretable models—and shows how combining them creates more trustworthy and comprehensive insights than either approach alone.

## Method Summary
This is a theoretical position paper that uses logical argumentation and conceptual examples rather than empirical experiments. The method involves systematically identifying flaws in both interpretability (Problems I.1, I.2) and explainability (Problems E.1, E.2, E.3), then demonstrating through reasoning how these approaches complement each other. The analysis relies on defining clear conceptual distinctions between inherent transparency (interpretability) and post-hoc simplification (explainability), and showing how each addresses the other's limitations.

## Key Results
- Interpretability and explainability are complementary rather than substitutive approaches
- Explanations from interpretable models are more faithful approximations due to reduced structural complexity
- Combining interpretable models with explainability techniques reduces blind trust requirements by providing verifiable structure
- Interpretable models alone cannot provide feature importance or adversarial example information that explainability techniques can supply

## Why This Works (Mechanism)

### Mechanism 1
Interpretability grounds explanations in verifiable structure while explainability supplies non-inherent information. Interpretable predictors provide an objective reference that both explainer and explainee can verify directly, reducing blind trust in explanations (Problem E.1). Explainability techniques can then operate on this simpler structure, yielding more faithful approximations (Problem E.2) and reducing discrepancies between intent and reception (Problem E.3). This works because a predictor can be sufficiently transparent that its internal logic is accessible to a judge without additional proxy methods. Break condition: If the interpretable predictor becomes too complex or opaque, the grounding advantage disappears and blind trust resurfaces.

### Mechanism 2
Simpler predictors yield more accurate explanations because explanations are approximations that benefit from reduced structural complexity. Explainability methods approximate black-box behavior; with interpretable predictors, the original model is already simple, so the approximation error shrinks. This improves faithfulness (Problem E.2) and reduces computational shortcuts that violate theoretical properties (Problem E.1). This works because structural simplicity of the model correlates with explanation fidelity. Break condition: If explainability techniques still rely on strong distributional assumptions, fidelity gains may be minimal regardless of predictor simplicity.

### Mechanism 3
Combining interpretability and explainability mitigates the subjectivity and limited scope of interpretability alone. Interpretability alone cannot reveal parameter importance or distribution effects (Problem I.1). Explainability methods quantify these aspects (e.g., feature importance, counterfactuals) while interpretability provides the transparent structure to validate them, addressing both subjectivity (Problem I.2) and limited scope. This works because the explanatory information provided by explainability is non-redundant with what interpretability reveals. Break condition: If interpretability fully captures all relevant information, explainability becomes redundant and the complementarity collapses.

## Foundational Learning

- Domain-specific nature of interpretability
  - Why needed here: The paper repeatedly emphasizes that interpretability depends on the judge's understanding within a given domain; without this, any generalization about interpretability vs. explainability is misleading.
  - Quick check question: Can you give an example where a predictor interpretable in one domain (e.g., medical diagnosis) is opaque in another (e.g., computer vision)?

- Trade-off vs. complementarity framing
  - Why needed here: Many ML practitioners assume interpretability and accuracy trade off; the paper argues this is an oversimplification. Understanding this framing is essential to grasp the complementarity claim.
  - Quick check question: Why might a highly accurate, sparse linear model be both interpretable and accurate, challenging the inverse relationship?

- Problem E.1 (blind trust) and Problem I.1 (insufficient information)
  - Why needed here: These two problems motivate the complementarity argument; recognizing them helps see why neither interpretability nor explainability alone is sufficient.
  - Quick check question: What information about a predictor's importance weights is missing from its mathematical form alone?

## Architecture Onboarding

- Component map: Input: Predictor (trained model) + domain knowledge + judge profile -> Core: Interpretability assessment engine (parsimony, sparsity, transparency metrics) -> Bridge: Explainability module (feature importance, counterfactuals, influential instances) -> Output: Joint interpretability-explainability report (grounding + supplemental insights) -> Validation: Judge feedback loop for calibration

- Critical path: 1. Train/ingest predictor → assess interpretability metrics → generate base report 2. Apply explainability techniques to same predictor → generate supplemental insights 3. Correlate explainable outputs with interpretable structure → validate consistency 4. Present combined report to judge → collect feedback → iterate if needed

- Design tradeoffs:
  - Depth of interpretability vs. breadth of explainability: more interpretable structure reduces need for complex explanations but may limit scope of explainable insights
  - Computational cost: interpretable models often train slower; explainability adds overhead but can be lighter on interpretable models
  - Domain expertise requirement: high for interpretability assessment; lower for automated explainability, but both needed for validation

- Failure signatures:
  - Explanations contradict interpretable structure → indicates either flawed explainability method or mis-specified interpretability assessment
  - Judge cannot understand explanation despite interpretable model → suggests mismatch between judge profile and explanation style
  - Interpretability metrics plateau while accuracy drops → indicates constraints may be too strict for the domain

- First 3 experiments:
  1. Train a shallow decision tree on tabular data; apply SHAP feature importance; compare explanation fidelity to black-box baseline
  2. Train a sparse linear model; compute permutation importance; verify that coefficients align with importance scores
  3. Train a neural network with binary activations; extract attention maps; compare saliency explanations to the interpretable architecture

## Open Questions the Paper Calls Out

### Open Question 1
How can we systematically measure the complementarity between interpretability and explainability techniques to optimize their combined use? The paper discusses how interpretability and explainability are complementary and mitigate each other's flaws, but does not provide a systematic framework for measuring their combined effectiveness. A framework or set of metrics that can quantify the combined benefits of interpretability and explainability techniques, validated through empirical studies, would resolve this.

### Open Question 2
What are the trade-offs between the complexity of interpretable models and their ability to provide comprehensive explanations? The paper discusses how interpretability is often associated with simpler models, but also mentions that interpretable models may not provide all the information that explainability can offer. Empirical studies comparing the performance of interpretable models of varying complexity in providing explanations for different types of tasks would resolve this.

### Open Question 3
How can explainability techniques be adapted to work effectively with interpretable models, rather than just black-box models? The paper suggests that explainability techniques can be used to address some of the limitations of interpretable models, but does not provide specific guidance on how to adapt these techniques. Development and evaluation of explainability techniques specifically designed to work with interpretable models, demonstrating improved performance or insights compared to using these techniques with black-box models, would resolve this.

## Limitations

- Theoretical nature without empirical validation makes it difficult to assess claims across diverse real-world scenarios
- Domain-specific nature of interpretability introduces significant variability that cannot be fully quantified
- Heavy reliance on conceptual arguments rather than experimental evidence limits practical applicability assessment

## Confidence

- Claims about interpretability-explainability complementarity: Medium - well-reasoned but theoretical
- Specific mechanism descriptions: Low-Medium - logical but not empirically tested
- Problem classifications (E.1, E.2, I.1, etc.): High - clearly defined and internally consistent

## Next Checks

1. Implement the suggested experiment combining interpretable predictors (e.g., decision trees) with explainability techniques (e.g., SHAP) to measure explanation fidelity gains.
2. Test the domain-specificity claim by attempting to explain the same model to judges with different expertise levels and measuring comprehension variance.
3. Construct a controlled experiment comparing blind trust in explanations between black-box and interpretable models to quantify the claimed reduction in trust requirements.