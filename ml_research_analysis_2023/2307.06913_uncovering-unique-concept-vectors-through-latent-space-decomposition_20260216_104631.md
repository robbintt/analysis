---
ver: rpa2
title: Uncovering Unique Concept Vectors through Latent Space Decomposition
arxiv_id: '2307.06913'
source_url: https://arxiv.org/abs/2307.06913
tags:
- concept
- concepts
- vectors
- images
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We propose a method for automatically discovering concept vectors\
  \ from deep neural networks. Our approach involves decomposing a layer\u2019s latent\
  \ space using singular value decomposition, then ranking the resulting singular\
  \ vectors based on their sensitivity to the model\u2019s predictions."
---

# Uncovering Unique Concept Vectors through Latent Space Decomposition

## Quick Facts
- arXiv ID: 2307.06913
- Source URL: https://arxiv.org/abs/2307.06913
- Reference count: 11
- We propose a method for automatically discovering concept vectors from deep neural networks

## Executive Summary
This paper introduces a novel approach for automatically discovering interpretable concept vectors from deep neural networks. The method decomposes a layer's latent space using singular value decomposition (SVD), then ranks the resulting singular vectors based on their sensitivity to model predictions. The top-ranked vectors are further refined through clustering to isolate unique concept directions. Experiments on natural image classification tasks using Inception V3 and ResNet50, as well as tabular data with the COMPAS dataset, demonstrate that the discovered concepts are interpretable, coherent, and relevant to model predictions. Human evaluation confirms high accuracy in identifying intruders among concept segmentation masks.

## Method Summary
The proposed method discovers concept vectors through a pipeline that first decomposes layer activations using SVD to obtain orthogonal singular vectors ranked by variance. These vectors are then ranked by sensitivity to model predictions through a combination of directional derivatives and activation magnitudes. The top-ranked vectors undergo clustering refinement using hierarchical clustering to separate polysemous directions into distinct, interpretable concept vectors. The method is evaluated on image classification models (Inception V3, ResNet50) and tabular data (COMPAS), demonstrating interpretable concepts with high human accuracy in intruder detection tasks.

## Key Results
- Human evaluation achieves 0.88 average accuracy in identifying intruders among concept segmentation masks
- Successfully identifies outliers affected by various confounding factors in dataset exploration
- Demonstrates versatility across natural image classification and tabular data tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing the latent space with SVD isolates directions of maximal variance that align with model-relevant concepts
- Mechanism: SVD transforms the latent activations matrix into orthogonal singular vectors ordered by variance magnitude. By projecting gradients onto these vectors, the method ranks them by their sensitivity to model predictions
- Core assumption: Directions of high variance in the latent space correspond to semantically meaningful concepts used by the model for prediction
- Evidence anchors:
  - [abstract]: "By decomposing the latent space of a layer in singular vectors and refining them by unsupervised clustering, we uncover concept vectors aligned with directions of high variance that are relevant to the model prediction"
  - [section]: "The singular values rank the directions from the largest to the lowest observed variance"
- Break condition: If the model uses low-variance directions for predictions, or if the latent space exhibits degenerate structure where variance doesn't correlate with semantic meaning

### Mechanism 2
- Claim: Sensitivity-based ranking identifies which singular vectors actually influence model predictions
- Mechanism: The method computes directional derivatives along singular vectors by rotating gradients to align with them, then multiplies by activations to weight the importance. This creates a combined measure of gradient magnitude and activation strength
- Core assumption: The product of activation strength and gradient magnitude captures the true importance of a latent direction for model predictions
- Evidence anchors:
  - [section]: "we consider the joint impact of gradients and activations together... reduces the gradient noise that can be derived by small gradient values at the end of model training"
  - [section]: "To compute the ranking, we consider the overall importance of a singular vector to the prediction. This is simply given by the sample mean of the gi over all inputs i in the dataset"
- Break condition: If gradients become saturated or zero during training, or if activations are uniformly distributed across directions

### Mechanism 3
- Claim: Clustering refinement isolates unique, semantically distinct concepts from polysemous singular vectors
- Mechanism: Hierarchical clustering identifies natural groupings in the latent representations aligned with singular vectors. K-means then refines these clusters, and centroids become final concept vectors. This disentangles superposition where single neurons represent multiple concepts
- Core assumption: Natural clusters in the latent space correspond to distinct semantic concepts, and their centroids represent optimal concept directions
- Evidence anchors:
  - [abstract]: "The top-ranked vectors are further refined through clustering to isolate unique concept directions"
  - [section]: "Hierarchical clustering is then employed to identify clusters of inputs with minimal intra-cluster distances"
- Break condition: If clusters are too small or too overlapping, or if the concept space is inherently continuous rather than discrete

## Foundational Learning

- Concept: Singular Value Decomposition (SVD)
  - Why needed here: SVD provides the mathematical foundation for decomposing the latent space into orthogonal directions ranked by variance, which forms the basis for concept discovery
  - Quick check question: What property of SVD ensures that the resulting vectors are orthogonal and ordered by importance?

- Concept: Directional derivatives and sensitivity analysis
  - Why needed here: Sensitivity analysis measures how perturbations along latent directions affect model predictions, enabling ranking of concept importance
  - Quick check question: Why does the method multiply rotated activations with rotated gradients rather than just using gradients alone?

- Concept: Unsupervised clustering and disentanglement
  - Why needed here: Clustering identifies natural groupings in the latent space to separate polysemous concepts into distinct, interpretable directions
  - Quick check question: How does hierarchical clustering help determine the optimal number of unique concepts?

## Architecture Onboarding

- Component map: Input data → Feature extractor ϕ → Latent space Φ → SVD decomposition → Gradient computation → Sensitivity ranking → Clustering refinement → Concept vectors → Visualization/interpretation
- Critical path: The pipeline from SVD decomposition through sensitivity ranking to clustering refinement represents the core algorithm flow that must execute correctly for valid concept discovery
- Design tradeoffs: SVD-based decomposition provides orthogonality but may miss low-variance yet semantically important concepts; sensitivity weighting reduces noise but may overweight high-activation directions
- Failure signatures: Low cosine similarity between concept vectors and latent basis vectors indicates orthogonality; high variance in cluster sizes suggests polysemy; poor human interpretability scores indicate semantic gaps
- First 3 experiments:
  1. Apply SVD to a simple 2D latent space with known concept directions and verify orthogonality and variance ordering
  2. Compute sensitivity rankings on a trained model and compare against random vector importance
  3. Run clustering refinement on a polysemous direction and verify separation into distinct concepts with high intra-cluster similarity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the discovered concepts generalize to other model architectures and tasks beyond the ones tested in the paper?
- Basis in paper: [inferred] The paper mentions the method's versatility to different data types and model architectures but does not provide extensive experimental validation.
- Why unresolved: The paper primarily focuses on Inception V3 and ResNet50 for image classification and a dense neural network for tabular data. Further experimentation is needed to assess the method's applicability to other architectures like transformers or different tasks like object detection or segmentation.
- What evidence would resolve it: Extensive experiments on a diverse set of model architectures and tasks, demonstrating consistent concept discovery and interpretability across different domains.

### Open Question 2
- Question: Can the method effectively discover concepts in datasets with limited or no human-annotated labels?
- Basis in paper: [inferred] The paper's experiments rely on labeled datasets like ImageNet and COMPAS, but the method's performance on unlabeled or weakly labeled data is not explored.
- Why unresolved: Unsupervised concept discovery is a key aspect of the method, but its effectiveness in the absence of labels is unclear. This is particularly relevant for domains where labeled data is scarce or expensive to obtain.
- What evidence would resolve it: Experiments on datasets with varying levels of labeling, comparing the discovered concepts' quality and relevance to the task with those obtained from labeled data.

### Open Question 3
- Question: How does the choice of hyperparameters, such as the number of clusters and distance threshold, affect the quality and interpretability of the discovered concepts?
- Basis in paper: [explicit] The paper mentions using hierarchical clustering and a distance threshold parameter but does not provide a systematic analysis of their impact.
- Why unresolved: The method's performance may be sensitive to these hyperparameters, and their optimal values could vary depending on the dataset and task. Understanding their influence is crucial for practical application.
- What evidence would resolve it: A sensitivity analysis exploring the effect of different hyperparameter settings on concept quality, measured by metrics like coherence, interpretability, and relevance to the model's predictions.

## Limitations
- The method may miss semantically important but low-variance directions in the latent space due to its variance-based decomposition approach
- The sensitivity ranking mechanism's reliance on gradient magnitude may break down in saturated or regularized networks
- Clustering refinement depends on hyperparameters that significantly affect concept quality, yet optimal parameter settings are not thoroughly explored

## Confidence
- **High confidence**: The mathematical framework of SVD decomposition and sensitivity-based ranking is sound and well-established in the literature
- **Medium confidence**: The human evaluation results (0.88 accuracy) are promising but based on a single experimental setup that may not generalize across domains
- **Low confidence**: The claims about concept disentanglement through clustering lack rigorous quantitative validation beyond qualitative assessments

## Next Checks
1. Conduct ablation studies removing the sensitivity weighting to determine if gradient × activation products genuinely improve concept ranking over gradient-only approaches
2. Test the method on synthetic datasets with known concept distributions to quantify false positive and false negative rates in concept discovery
3. Evaluate concept stability across different layers and network architectures to determine whether the method consistently discovers the same semantic concepts regardless of architectural choices