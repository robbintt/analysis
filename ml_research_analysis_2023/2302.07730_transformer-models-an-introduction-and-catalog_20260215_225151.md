---
ver: rpa2
title: 'Transformer models: an introduction and catalog'
arxiv_id: '2302.07730'
source_url: https://arxiv.org/abs/2302.07730
tags:
- pretraining
- arxiv
- architecture
- https
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive catalog of transformer models,
  organizing them by key properties such as pretraining architecture, task, and application.
  The catalog includes both encoder-only models (like BERT) and decoder-only models
  (like GPT-3), as well as multimodal models like CLIP and diffusion models like Stable
  Diffusion.
---

# Transformer models: an introduction and catalog

## Quick Facts
- arXiv ID: 2302.07730
- Source URL: https://arxiv.org/abs/2302.07730
- Authors: 
- Reference count: 40
- One-line primary result: Comprehensive catalog of transformer models organized by key properties including pretraining architecture, task, and application

## Executive Summary
This paper presents a comprehensive catalog of transformer models, systematically organizing them by key properties such as pretraining architecture, task, and application. The catalog covers both encoder-only models (like BERT) and decoder-only models (like GPT-3), as well as multimodal models like CLIP and diffusion models like Stable Diffusion. A timeline visualization effectively illustrates the evolution and scaling of these models, highlighting the progression from smaller models to massive architectures like GPT-3 (175B parameters) and PaLM (540B parameters). The paper serves as both an educational introduction to transformer concepts and a practical reference for understanding the diversity and capabilities of modern transformer-based models.

## Method Summary
The paper employs a comprehensive literature review approach to create a catalog of over 60 transformer models across NLP, vision, and multimodal domains. For each model, key properties including pretraining architecture (encoder-only, decoder-only, or encoder-decoder), primary task, application domain, publication year, and parameter count are extracted from original papers and documentation. The authors organize these models into a classification system based on their architectural characteristics and intended use cases, supplemented by visual timeline representations showing the evolution and scale growth of transformer models over time.

## Key Results
- Catalog successfully categorizes transformer models by pretraining architecture, clarifying their use cases for understanding (BERT), generation (GPT-3), or both (BART)
- Timeline visualization effectively shows evolution from smaller models to massive architectures, with GPT-3 (175B) and PaLM (540B) representing the largest models
- Comprehensive coverage includes both self-supervised learning models and human-in-the-loop fine-tuned models, capturing the full spectrum of transformer development approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The paper successfully categorizes transformer models by architecture type (encoder-only, decoder-only, encoder-decoder) which clarifies their use cases.
- Mechanism: Organizing models by pretraining architecture allows readers to immediately understand which models are suited for tasks like understanding (BERT) versus generation (GPT-3) versus both (BART).
- Core assumption: The pretraining architecture directly determines the model's strengths and limitations.
- Evidence anchors:
  - [abstract] "The catalog includes both encoder-only models (like BERT) and decoder-only models (like GPT-3), as well as multimodal models like CLIP"
  - [section] "Encoder Pretraining These models, which are also called bi-directional or auto-encoding, only use the encoder during pretraining... Decoder Pretraining Decoder models, often called auto-regressive, use only the decoder during a pretraining"
  - [corpus] Weak corpus support - no direct citations about architectural categorization benefits
- Break condition: If architectural categorization doesn't map to actual performance differences, the framework loses predictive value.

### Mechanism 2
- Claim: The catalog's timeline visualization effectively shows the evolution and scale growth of transformer models.
- Mechanism: By plotting models chronologically with parameter counts, readers can visually track the progression from smaller models like BERT to massive models like GPT-3 (175B) and PaLM (540B).
- Core assumption: Visual representation of model size over time helps readers understand the field's progression.
- Evidence anchors:
  - [abstract] "A timeline visualization shows the evolution of these models, with GPT-3 (175B parameters) and PaLM (540B parameters) representing the largest models"
  - [section] "In Figure 8, the Y-axis represents model size in millions of parameters"
  - [corpus] Weak corpus support - no direct citations about visualization effectiveness
- Break condition: If the visualization becomes too cluttered or the parameter count doesn't correlate with capabilities, the timeline loses utility.

### Mechanism 3
- Claim: Including both self-supervised learning models and human-in-the-loop fine-tuned models provides comprehensive coverage of the transformer landscape.
- Mechanism: By covering models trained via self-supervised learning (BERT, GPT-3) and those further trained with human feedback (InstructGPT), the catalog captures the full spectrum of transformer development approaches.
- Core assumption: Both training paradigms are significant enough to warrant inclusion in a comprehensive catalog.
- Evidence anchors:
  - [abstract] "The catalog includes both encoder-only models (like BERT) and decoder-only models (like GPT-3), as well as multimodal models like CLIP and diffusion models like Stable Diffusion"
  - [section] "Our catalog will include models that are trained using self-supervised learning (e.g., BERT or GPT3) as well as those that are further trained using a human-in-the-loop (e.g. the InstructGPT model used by ChatGPT)"
  - [corpus] Weak corpus support - no direct citations about coverage comprehensiveness
- Break condition: If the distinction between self-supervised and human-in-the-loop training becomes less relevant, the categorization may need revision.

## Foundational Learning

- Concept: Transformer architecture fundamentals (encoder/decoder, attention mechanisms)
  - Why needed here: The catalog builds on these concepts to explain different model variations and their applications
  - Quick check question: What are the three main transformer architectures described, and which tasks is each best suited for?

- Concept: Transfer learning and fine-tuning principles
  - Why needed here: Understanding how pretrained models adapt to new tasks explains the catalog's organization and model selection
  - Quick check question: How does the pretraining architecture affect a model's ability to adapt to new tasks?

- Concept: Self-supervised learning vs. supervised fine-tuning
  - Why needed here: The catalog distinguishes between models trained purely with self-supervised methods and those enhanced with human feedback
  - Quick check question: What is the key difference between models like BERT/GPT-3 and InstructGPT in terms of training methodology?

## Architecture Onboarding

- Component map:
  - Core: Model classification system (architecture, task, application, year, parameters)
  - Visual: Timeline visualization showing model evolution and scale
  - Reference: Comprehensive list with model details and citations
  - Educational: Introductory sections explaining transformer concepts

- Critical path:
  1. Understand transformer architecture basics
  2. Navigate the classification system to find relevant models
  3. Use timeline visualization to understand field progression
  4. Reference detailed model entries for specific implementation needs

- Design tradeoffs:
  - Breadth vs. depth: Including many models provides comprehensive coverage but may sacrifice detailed analysis of each
  - Static vs. dynamic: The catalog captures a snapshot in time but requires updates as new models emerge
  - Technical vs. accessible: Balancing detailed technical information with accessibility for newcomers

- Failure signatures:
  - Misclassification of models leading to incorrect task recommendations
  - Outdated information as new models supersede listed ones
  - Overly complex organization preventing quick model identification

- First 3 experiments:
  1. Use the classification system to identify three encoder-only models and compare their applications
  2. Trace the timeline from BERT to GPT-3 to identify key architectural innovations
  3. Compare a self-supervised model (BERT) with a human-in-the-loop model (InstructGPT) to understand the impact of fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the performance tradeoffs between encoder-only, decoder-only, and encoder-decoder transformer architectures for different NLP tasks?
- Basis in paper: [explicit] The paper describes the three main pretraining architectures and their typical applications
- Why unresolved: The paper provides a catalog of models but does not compare their relative performance on specific tasks
- What evidence would resolve it: Empirical benchmarks comparing models of each architecture type on standard NLP tasks like question answering, summarization, and text generation

### Open Question 2
- Question: How does the performance of diffusion models compare to other generative approaches like GANs and VAEs across different modalities (image, audio, etc.)?
- Basis in paper: [explicit] The paper discusses diffusion models as the new state-of-the-art in image generation, replacing GANs
- Why unresolved: While the paper mentions diffusion models are superior to GANs for images, it doesn't provide quantitative comparisons or explore their performance in other domains
- What evidence would resolve it: Systematic comparisons of generation quality metrics across different model types and data modalities

### Open Question 3
- Question: What are the most effective strategies for scaling transformer models while maintaining computational efficiency?
- Basis in paper: [inferred] The paper catalogs extremely large models (PaLM at 540B parameters, MT-NLG at 530B) but doesn't discuss the architectural innovations that enable such scaling
- Why unresolved: The paper focuses on cataloging existing models rather than analyzing the techniques that enable efficient scaling to trillion-parameter models
- What evidence would resolve it: Analysis of training techniques, architectural modifications, and hardware optimizations that enable efficient training of extremely large models

## Limitations
- Catalog comprehensiveness limited by rapid pace of model development, potentially missing recent models published after literature review cutoff
- Parameter counts for some models may be approximate or unavailable, affecting timeline visualization accuracy
- Classification system assumes clear distinctions between architectures, but some models blur boundaries or employ hybrid approaches

## Confidence

| Claim Type | Confidence Level | Rationale |
|------------|------------------|-----------|
| Foundational concepts (attention mechanisms, transformer architectures) | High | Well-established in literature with extensive citations |
| Classification framework and model categorization | Medium | Depends on accurate property extraction from diverse sources |
| Timeline visualization utility | Medium | Parameter counts don't always correlate with capabilities |

## Next Checks
1. Verify classification accuracy by having domain experts review a random sample of 10 models to confirm their architectural categorization
2. Cross-reference parameter counts with original model papers to identify and correct discrepancies in the catalog
3. Test the catalog's practical utility by having practitioners attempt to find models for specific tasks and measuring success rate against ground truth model-task mappings