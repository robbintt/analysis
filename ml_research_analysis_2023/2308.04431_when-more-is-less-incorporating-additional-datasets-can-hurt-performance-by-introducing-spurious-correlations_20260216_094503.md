---
ver: rpa2
title: 'When More is Less: Incorporating Additional Datasets Can Hurt Performance
  By Introducing Spurious Correlations'
arxiv_id: '2308.04431'
source_url: https://arxiv.org/abs/2308.04431
tags:
- data
- performance
- hospital
- datasets
- when
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work challenges the common belief that incorporating more
  data always improves model performance. Through a large-scale empirical study using
  four open-source chest x-ray datasets and 9 different labels, the authors demonstrate
  that in 43% of settings, a model trained on data from two hospitals has poorer worst-group
  accuracy over both hospitals than a model trained on just a single hospital's data.
---

# When More is Less: Incorporating Additional Datasets Can Hurt Performance By Introducing Spurious Correlations

## Quick Facts
- arXiv ID: 2308.04431
- Source URL: https://arxiv.org/abs/2308.04431
- Reference count: 15
- In 43% of settings, models trained on data from two hospitals had worse worst-group accuracy than models trained on just one hospital's data.

## Executive Summary
This work challenges the common belief that incorporating more data always improves model performance. Through a large-scale empirical study using four open-source chest x-ray datasets and 9 different labels, the authors demonstrate that in 43% of settings, a model trained on data from two hospitals has poorer worst-group accuracy over both hospitals than a model trained on just a single hospital's data. This surprising result occurs even though the added hospital makes the training distribution more similar to the test distribution. The phenomenon arises from a spurious correlation between the disease and hospital, due to hospital-specific image artifacts. The authors highlight the trade-off between the obvious benefit of additional data and the insidious cost of introduced spurious correlation.

## Method Summary
The authors conducted a large-scale empirical study using four open-source chest x-ray datasets (MIMIC, CheXpert, NIH, PadChest) with 9 binary disease labels. They trained DenseNet-121 models on all 10 dataset configurations (4 single-hospital, 6 two-hospital combinations) with 3 random seeds per config. Models were evaluated using worst-group accuracy across four subgroups (disease/non-disease × hospital A/B), computed as the minimum of per-group accuracies. The authors also tested balancing strategies by undersampling to equalize disease prevalence between hospitals.

## Key Results
- In 43% of settings, models trained on two hospitals had worse worst-group accuracy than models trained on just one hospital
- This occurred even though the added hospital made the training distribution more similar to the test distribution
- Every CNN model learned embeddings that could distinguish hospital sources with near-perfect accuracy
- Balancing datasets by undersampling to equalize disease prevalence sometimes removed spurious correlations but was not always effective

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding a new hospital's data can induce spurious correlations between hospital source and disease label.
- Mechanism: Different hospitals have different disease prevalences. When datasets are combined, the label distribution changes, creating a correlation between the hospital source and disease label in the training data.
- Core assumption: The hospital source contains enough discriminative signal to be picked up by the model.
- Evidence anchors:
  - [abstract] "This surprising result occurs even though the added hospital makes the training distribution more similar to the test distribution."
  - [section] "This difference in disease prevalence means that the resulting dataset after combining multiple hospitals’s data exhibits a correlation between the hospital source and the probability of disease."
- Break condition: If hospital sources are indistinguishable from each other or disease prevalences are identical across hospitals.

### Mechanism 2
- Claim: Models trained on multi-source data can leverage hospital-specific features, hurting performance on groups where the shortcut doesn't hold.
- Mechanism: The model learns to use hospital source as a shortcut for disease prediction. This improves accuracy on "shortcut groups" but decreases accuracy on "leftover groups" where the shortcut doesn't apply.
- Core assumption: Hospital-specific features are strongly encoded in chest x-ray images.
- Evidence anchors:
  - [abstract] "We explain that this phenomenon arises from the spurious correlation that emerges between the disease and hospital, due to hospital-specific image artifacts."
  - [section] "We find that every CNN model, regardless of training disease or datasets, learns embeddings that can distinguish any of the hospital sources with near-perfect accuracy."
- Break condition: If hospital-specific features are not present in the input data or if the model is constrained from using them.

### Mechanism 3
- Claim: Balancing disease prevalence between datasets doesn't always fix the spurious correlation problem.
- Mechanism: Balancing removes the direct correlation between hospital and disease in the combined dataset, but the label and shortcut can still be dependent after conditioning on the input, meaning models may still face performance degradation.
- Core assumption: The shortcut feature (hospital) is not perfectly predictable from the covariates.
- Evidence anchors:
  - [abstract] "In some cases, balancing the dataset can remove the spurious correlation and improve performance, but it is not always an effective strategy."
  - [section] "One limitation of undersampling to fix spurious correlations is the loss of data."
- Break condition: If the shortcut feature is perfectly predictable from the covariates or if the spurious correlation is removed completely.

## Foundational Learning

- Concept: Spurious correlation
  - Why needed here: The paper's main finding is that adding more data can introduce spurious correlations that hurt model performance.
  - Quick check question: What is a spurious correlation and why is it problematic for machine learning models?

- Concept: Domain generalization
  - Why needed here: The paper deals with models trained on data from multiple hospitals and their ability to generalize to new hospitals.
  - Quick check question: How does domain generalization differ from standard supervised learning?

- Concept: Worst-group accuracy
  - Why needed here: The paper uses worst-group accuracy as its primary evaluation metric to detect cases where one group is systematically misclassified.
  - Quick check question: Why might worst-group accuracy be a more appropriate metric than average accuracy in healthcare applications?

## Architecture Onboarding

- Component map:
  - Data preprocessing pipeline -> DenseNet-121 model architecture -> Training loop with early stopping -> Evaluation framework with worst-group accuracy calculation -> Balancing utilities

- Critical path:
  1. Load and preprocess data from multiple hospitals
  2. Train model on single hospital data
  3. Train model on combined hospital data (with/without balancing)
  4. Evaluate using worst-group accuracy across all groups

- Design tradeoffs:
  - Using DenseNet-121 vs. other architectures
  - Balancing vs. not balancing the data
  - Training for fixed steps vs. using early stopping

- Failure signatures:
  - Decrease in worst-group accuracy when adding more data
  - High hospital prediction accuracy from disease prediction model embeddings
  - Models performing well on shortcut groups but poorly on leftover groups

- First 3 experiments:
  1. Train on single hospital data and evaluate on same hospital (internal validation)
  2. Train on single hospital data and evaluate on different hospital (external validation)
  3. Train on combined hospital data and evaluate using worst-group accuracy across both hospitals

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different disease prevalences between hospitals influence the extent of spurious correlations learned by models?
- Basis in paper: [explicit] The paper discusses how different disease prevalences between hospitals can induce spurious correlations between hospital source and disease label.
- Why unresolved: The paper provides empirical evidence that this correlation exists and affects model performance, but it does not offer a comprehensive theoretical framework or predictive model to quantify how specific prevalence ratios influence the strength of these spurious correlations.
- What evidence would resolve it: A mathematical model or detailed empirical study that quantifies the relationship between disease prevalence ratios and the degree of spurious correlation learned, perhaps using controlled synthetic datasets or advanced statistical analysis.

### Open Question 2
- Question: Does balancing datasets by undersampling the majority class always lead to better worst-group performance compared to not balancing?
- Basis in paper: [explicit] The paper shows that while balancing often improves performance, it does not always do so and can sometimes hurt performance, even when compared to training on a single dataset.
- Why unresolved: The paper demonstrates the limitations of balancing but does not provide a definitive set of conditions or criteria under which balancing is beneficial or detrimental, nor does it explore alternative balancing methods like reweighting in detail.
- What evidence would resolve it: Systematic experiments comparing different balancing techniques (e.g., undersampling, oversampling, reweighting) across a wide range of dataset configurations and disease prevalences, coupled with a theoretical analysis of when and why each method succeeds or fails.

### Open Question 3
- Question: How do hospital-specific artifacts in medical images affect the generalizability of models across different hospitals?
- Basis in paper: [explicit] The paper demonstrates that models trained on disease prediction can still encode hospital-specific features, allowing them to discriminate between hospitals not seen during training, and that these features can be detected even from small image patches.
- Why unresolved: While the paper shows that hospital-specific signals are present and can be learned by models, it does not explore the specific nature of these artifacts, how they vary across imaging modalities, or develop methods to mitigate their impact on model generalizability.
- What evidence would resolve it: Detailed analysis of hospital-specific artifacts across different imaging modalities (e.g., X-ray, CT, MRI), identification of common patterns, and development of preprocessing or model architectures designed to minimize the impact of these artifacts on disease prediction accuracy.

## Limitations

- The study focuses on chest x-ray data with specific hospital-specific artifacts; results may not generalize to other medical imaging modalities or non-medical domains
- The balancing approach tested (undersampling to equalize disease prevalence) is limited; other balancing strategies might perform differently
- The paper doesn't explore whether the problem is specific to DenseNet-121 or occurs across different architectures

## Confidence

- High: The empirical observation that adding data can hurt worst-group accuracy (43% failure rate across settings)
- Medium: The explanation that hospital-specific artifacts create spurious correlations
- Medium: The claim that balancing doesn't always fix the problem (based on limited experiments)

## Next Checks

1. Test the phenomenon on additional medical imaging datasets (e.g., dermatology, radiology CT scans) to assess generalizability
2. Evaluate alternative dataset balancing strategies (e.g., oversampling, class-aware sampling) to determine if better methods exist
3. Investigate whether the spurious correlation persists when using models with explicit domain generalization techniques (e.g., domain adversarial training)