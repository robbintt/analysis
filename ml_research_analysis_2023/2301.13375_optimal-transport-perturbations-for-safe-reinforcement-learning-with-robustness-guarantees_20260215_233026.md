---
ver: rpa2
title: Optimal Transport Perturbations for Safe Reinforcement Learning with Robustness
  Guarantees
arxiv_id: '2301.13375'
source_url: https://arxiv.org/abs/2301.13375
tags:
- uni00000013
- learning
- optimal
- safe
- transport
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a safe reinforcement learning framework that
  incorporates robustness to general environment disturbances using optimal transport
  cost uncertainty sets. The authors reformulate the robust Bellman operators as adversarial
  perturbations to state transitions, enabling efficient implementation through offline
  Optimal Transport Perturbations (OTP).
---

# Optimal Transport Perturbations for Safe Reinforcement Learning with Robustness Guarantees

## Quick Facts
- arXiv ID: 2301.13375
- Source URL: https://arxiv.org/abs/2301.13375
- Reference count: 19
- Primary result: OTP achieves 87% constraint satisfaction vs 51% for CRPO while maintaining 0.34x average costs

## Executive Summary
This work introduces Optimal Transport Perturbations (OTP) for safe reinforcement learning with robustness guarantees. The framework reformulates robust Bellman operators as adversarial perturbations to state transitions, enabling efficient implementation through offline OTP. By using optimal transport cost uncertainty sets, OTP constructs worst-case virtual state transitions without requiring adversarial interventions during training or detailed simulator access. The method significantly improves safety at deployment time compared to standard safe RL methods while maintaining competitive performance.

## Method Summary
The framework learns a policy and safety critics using robust optimization over optimal transport uncertainty sets. OTP applies offline perturbations to state transitions, creating worst-case scenarios that the policy must satisfy during training. The method uses separate perturbation networks for reward and cost functions, trained alongside policy and critics. During Bellman target calculation, OTP applies these perturbations to state transitions, enabling robust learning without modifying the environment during training. The approach is compatible with existing safe RL algorithms like CRPO and MPO.

## Key Results
- OTP achieves 87% constraint satisfaction rate vs 51% for CRPO
- OTP maintains 0.34x average costs vs 1.00x for CRPO
- OTP outperforms domain randomization (76% satisfaction) on safety constraints

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Achieves robustness without adversarial interventions during training by applying optimal transport perturbations offline
- **Mechanism**: Reformulates robust Bellman operators as constrained adversarial perturbations to state transitions using optimal transport cost uncertainty sets
- **Core assumption**: Optimal transport cost between transition distributions can be efficiently computed and applied to state transitions offline
- **Evidence anchors**: [abstract], [section 5], [corpus]
- **Break condition**: If optimal transport cost cannot be computed efficiently or offline perturbations fail to capture worst-case transitions

### Mechanism 2
- **Claim**: Provides safety guarantees by learning conservative safety critics through robust optimization
- **Mechanism**: Learns conservative safety critic by considering supremum over transition distributions in robust Bellman operator
- **Core assumption**: Supremum over optimal transport uncertainty set leads to conservative safety critic guaranteeing constraint satisfaction
- **Evidence anchors**: [abstract], [section 2.1], [corpus]
- **Break condition**: If robust optimization fails to produce conservative enough safety critic, leading to constraint violations

### Mechanism 3
- **Claim**: Achieves better performance-safety tradeoff compared to domain randomization by explicitly considering robustness
- **Mechanism**: Uses optimal transport uncertainty set to explicitly consider worst-case transitions rather than averaging across environments
- **Core assumption**: Explicit robustness consideration through optimal transport uncertainty set is more effective than soft-robust approaches
- **Evidence anchors**: [abstract], [section 8.3], [corpus]
- **Break condition**: If explicit robustness consideration leads to excessive conservatism and poor performance

## Foundational Learning

- **Concept: Optimal Transport Cost**
  - Why needed here: Forms basis of uncertainty set used to define worst-case transitions for robust RL
  - Quick check question: What is the relationship between optimal transport cost and the Wasserstein distance?

- **Concept: Robust Bellman Operators**
  - Why needed here: Used to incorporate robustness into RL framework by considering worst-case transitions
  - Quick check question: How do robust Bellman operators differ from standard Bellman operators?

- **Concept: Safety Constraints in RL**
  - Why needed here: Framework needs to ensure safety while optimizing for rewards, requiring safe RL approach
  - Quick check question: What are common approaches to incorporating safety constraints in RL?

## Architecture Onboarding

- **Component map**: 
  Policy network -> Reward critic network -> Cost critic network -> Reward perturbation network -> Cost perturbation network -> Replay buffer

- **Critical path**: Data collection → Perturbation network updates → Bellman target estimation → Critic updates → Policy update

- **Design tradeoffs**: 
  - Offline perturbations avoid environment modification but may limit adaptability to new disturbances
  - Separate perturbation networks for reward and cost allow independent optimization but increase complexity
  - Average constraint on perturbations provides flexibility but may not capture state-specific worst cases

- **Failure signatures**:
  - High training cost indicates perturbation networks are not capturing worst cases effectively
  - Low safety constraint satisfaction indicates conservative critics are not learned properly
  - Poor reward performance suggests excessive conservatism in robust optimization

- **First 3 experiments**:
  1. Implement baseline CRPO without perturbations to establish performance and safety metrics
  2. Add OTP with small perturbation magnitude to verify improvement in safety constraint satisfaction
  3. Increase perturbation magnitude to find optimal tradeoff between safety and performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does choice of optimal transport cost function affect performance and robustness guarantees?
- Basis in paper: [explicit] Discusses use of optimal transport cost but lacks experimental comparisons or theoretical analysis
- Why unresolved: No empirical evidence or theoretical analysis provided
- What evidence would resolve it: Empirical results comparing OTP performance using different cost functions (Wasserstein, total variation) across various tasks

### Open Question 2
- Question: How does OTP handle safety constraint violations during training and their effect on final policy?
- Basis in paper: [inferred] Mentions compatibility with safe RL algorithms but lacks details on handling constraint violations
- Why unresolved: No experimental results or theoretical analysis on handling violations during training
- What evidence would resolve it: Experimental comparison of OTP performance with and without constraint violations during training

### Open Question 3
- Question: How does OTP scale to high-dimensional state and action spaces and what are computational challenges?
- Basis in paper: [inferred] Presents OTP as general approach but lacks details on scalability and computational challenges
- Why unresolved: No experimental results or theoretical analysis on scalability
- What evidence would resolve it: Experimental results comparing OTP performance and computational efficiency across varying state/action space dimensions

## Limitations
- Evaluation limited to specific continuous control tasks, leaving generalizability uncertainty
- Lacks direct ablation studies isolating OTP's contribution to safety improvements
- Relies on nominal environment data collection, contradicting claim of not requiring simulator access

## Confidence

- **High confidence**: Technical formulation of robust Bellman operators using optimal transport uncertainty sets
- **Medium confidence**: Empirical safety improvements (87% vs 51% constraint satisfaction) given strong quantitative claims
- **Low confidence**: Claim of not requiring detailed simulator access due to dependence on nominal environment data

## Next Checks
1. Perform ablation study comparing OTP with and without optimal transport perturbation component
2. Test framework on environments with different disturbance types to validate robustness beyond tested perturbations
3. Measure computational overhead of OTP during training and deployment to verify no impact on data collection claim