---
ver: rpa2
title: "Jean-Luc Picard at Touch\xE9 2023: Comparing Image Generation, Stance Detection\
  \ and Feature Matching for Image Retrieval for Arguments"
arxiv_id: '2307.09172'
source_url: https://arxiv.org/abs/2307.09172
tags:
- image
- images
- feature
- used
- pipeline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper evaluates several image retrieval pipelines for arguments\
  \ using combinations of query preprocessing, image preselection, stance detection,\
  \ and feature matching. The authors submitted five runs to the Touch\xE9 2023 shared\
  \ task, including a baseline, and compared them using precision@10, precision@1,\
  \ and mean average precision (MAP)."
---

# Jean-Luc Picard at Touché 2023: Comparing Image Generation, Stance Detection and Feature Matching for Image Retrieval for Arguments

## Quick Facts
- arXiv ID: 2307.09172
- Source URL: https://arxiv.org/abs/2307.09172
- Reference count: 10
- No pipeline significantly outperformed baseline; best MAP was 0.185

## Executive Summary
This paper evaluates five image retrieval pipelines for argumentative image retrieval at the Touché 2023 shared task, combining query preprocessing, BM25-based preselection, zero-shot stance detection, Stable Diffusion image generation, and SIFT/FLANN feature matching. All pipelines, including the baseline, achieved low precision scores (e.g., MAP 0.185), with no statistically significant differences between approaches. The authors suggest future improvements such as increasing pre-selected images, refining stance detection models, and ensuring more relevant images in the corpus.

## Method Summary
The authors tested five image retrieval pipelines combining query preprocessing, image preselection (BM25 with PyTerrier), stance detection (BART-large-mnli zero-shot classification), image generation (Stable Diffusion), and feature matching (SIFT + FLANN). Queries were preprocessed with spaCy, 50 images per query were preselected via BM25, stance detection filtered images by "pro", "con", or "neutral" labels, and feature matching ranked images based on similarity to generated images. Results were evaluated using precision@1, precision@10, and MAP, compared to a baseline with student's t-test.

## Key Results
- No pipeline significantly outperformed the baseline; all had similar low precision
- Best pipeline achieved MAP 0.185, but precision@1 and precision@10 remained low (0.08, 0.10)
- Image generation and stance detection did not improve results, possibly due to corpus limitations or parameter choices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stance detection models can improve retrieval relevance by filtering images to match the argument's stance.
- Mechanism: The pipeline uses zero-shot stance detection (BART-large-mnli) to classify images as "pro", "con", or "neutral" relative to the query, then ranks based on the highest probability label.
- Core assumption: Stance labels accurately reflect the visual content and argument position of the image.
- Evidence anchors:
  - [section] "The model receives text and labels and calculates accordingly the probability of the label being a good descriptor for the given text. Hence, either 'contra', 'pro' or 'neutral' was added in front of the given query."
  - [abstract] "Using different pipelines for image retrieval containing Image Generation, Stance Detection, Preselection and Feature Matching."
- Break condition: If stance detection mislabels images, the retrieval quality degrades. This occurred as evidenced by low precision scores (e.g., 0.08 for Pipeline 1).

### Mechanism 2
- Claim: Image generation provides a visual query representation that can be matched against corpus images for similarity-based retrieval.
- Mechanism: Stable Diffusion generates photorealistic and comic-style images from the query, then feature matching compares these to corpus images using SIFT descriptors and FLANN matcher.
- Core assumption: Generated images capture the semantic intent of the query and can serve as reliable query surrogates.
- Evidence anchors:
  - [section] "A generated image for a query should display the information of the query visually. This generated image is then used to compare to other images, like with Feature Matching, to know how similar the images are to each other."
  - [corpus] "No direct corpus evidence of image generation impact; the weak results suggest limited effectiveness."
- Break condition: If generated images poorly represent the query intent, similarity matching becomes ineffective.

### Mechanism 3
- Claim: Preselecting a subset of images via BM25 text search reduces the computational cost of subsequent stance detection and feature matching.
- Mechanism: PyTerrier builds an index of image text content; BM25 retrieves the top 50 images for the preprocessed query, which are then passed to downstream steps.
- Core assumption: BM25 ranking of text-based image metadata correlates with visual relevance to the query.
- Evidence anchors:
  - [section] "For the initial image preselection, an index was constructed with PyTerrier... Then, BM25 was used to retrieve the best 50 images for a given query."
  - [abstract] "Image Preselection and Feature Matching" are listed as key pipeline components.
- Break condition: If preselection threshold is too low (only 50 images), relevant images may be excluded, limiting downstream accuracy.

## Foundational Learning

- Concept: Zero-shot text classification with BART
  - Why needed here: Enables stance detection without task-specific training data.
  - Quick check question: What labels are prepended to the query before classification?
- Concept: SIFT feature descriptors and FLANN matching
  - Why needed here: Provides a fast, scalable method to compute visual similarity between generated and corpus images.
  - Quick check question: Which distance metric is used to filter good matches?
- Concept: BM25 ranking function
  - Why needed here: Efficiently narrows the image set to a manageable size for computationally heavy downstream steps.
  - Quick check question: What is the maximum text length used per image for indexing?

## Architecture Onboarding

- Component map: Query preprocessing → Image preselection (BM25) → Stance detection (BART) → Image generation (Stable Diffusion) → Feature matching (SIFT/FLANN) → Ranking
- Critical path: Preselection → Stance detection → Feature matching; each step must complete before the next.
- Design tradeoffs: Larger preselection sets increase recall but raise computational cost; stance detection adds relevance filtering but may misclassify; image generation provides richer queries but depends on generation quality.
- Failure signatures: Low precision@1/10 and MAP indicate either poor preselection, inaccurate stance detection, or weak feature matching; uniform p-values across runs suggest no statistical difference from baseline.
- First 3 experiments:
  1. Vary preselection size from 50 to 200 images and measure impact on precision.
  2. Replace BART stance model with a different zero-shot classifier and compare results.
  3. Test image generation styles (e.g., only photorealistic vs. only comic) to see effect on feature matching.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does increasing the number of pre-selected images beyond 50 improve the precision of the image retrieval pipelines?
- Basis in paper: [explicit] The authors mention that increasing the number of pre-selected images is important and that only 1938 unique images were found out of 5000 results, suggesting that the limited pre-selection might contribute to poor results.
- Why unresolved: The paper does not provide experimental results with different numbers of pre-selected images, leaving the impact of this parameter on precision unclear.
- What evidence would resolve it: Conducting experiments with varying numbers of pre-selected images (e.g., 50, 100, 200) and comparing the resulting precision metrics would provide evidence on the optimal number of pre-selected images.

### Open Question 2
- Question: How does the inclusion of image generation and feature matching affect the overall performance of the image retrieval pipelines?
- Basis in paper: [explicit] The authors note that image generation and feature matching were used in every approach to determine the final ranked results, but the specific impact of these steps on performance is not detailed.
- Why unresolved: The paper does not isolate the contribution of image generation and feature matching from other pipeline components, making it difficult to assess their individual impact.
- What evidence would resolve it: Comparing the performance of pipelines with and without image generation and feature matching would clarify their contribution to the overall results.

### Open Question 3
- Question: Are there alternative stance detection models that could improve the performance of the image retrieval pipelines?
- Basis in paper: [explicit] The authors suggest that the inclusion of stance detection on website text did not seem beneficial and propose testing other variations of stance detection models in future work.
- Why unresolved: The paper only tested one stance detection model and did not explore other potential models that might yield better results.
- What evidence would resolve it: Experimenting with different stance detection models (e.g., BERT, RoBERTa) and comparing their performance in the pipelines would identify more effective models.

## Limitations

- No pipeline significantly outperformed the baseline, suggesting core mechanisms did not provide gains under experimental conditions.
- Low absolute precision scores (MAP 0.185) may reflect corpus limitations or suboptimal parameter settings.
- Absence of direct evidence linking image generation quality to retrieval outcomes limits confidence in the effectiveness of these methods.

## Confidence

- **High confidence**: The reported metrics and statistical comparisons are accurate and correctly interpreted.
- **Medium confidence**: The identified mechanisms (stance detection, image generation, feature matching) are correctly described, but their real-world effectiveness remains uncertain due to weak results.
- **Low confidence**: The causal link between pipeline components and observed performance is not well established; the low precision may stem from corpus limitations or parameter choices rather than inherent flaws in the approach.

## Next Checks

1. **Expand preselection set**: Increase the number of images preselected by BM25 from 50 to 200 per query and re-evaluate precision@1, precision@10, and MAP to test if recall improves downstream performance.

2. **Alternative stance models**: Replace the zero-shot BART stance detector with a different zero-shot classifier (e.g., using a different transformer architecture) and compare results to isolate the impact of the stance detection component.

3. **Image generation ablation**: Conduct experiments using only photorealistic or only comic-style generated images (excluding the mixed approach) to determine if a single style yields better feature matching and retrieval precision.