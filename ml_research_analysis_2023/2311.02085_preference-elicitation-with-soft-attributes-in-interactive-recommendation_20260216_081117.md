---
ver: rpa2
title: Preference Elicitation with Soft Attributes in Interactive Recommendation
arxiv_id: '2311.02085'
source_url: https://arxiv.org/abs/2311.02085
tags:
- user
- query
- item
- attribute
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a preference elicitation framework for interactive
  recommender systems that handles soft attributes using concept activation vectors
  (CAVs). The method combines item-based and attribute-based preference elicitation,
  allowing users to respond to queries about both items and soft attributes (e.g.,
  "funnier" or "less thought-provoking").
---

# Preference Elicitation with Soft Attributes in Interactive Recommendation

## Quick Facts
- arXiv ID: 2311.02085
- Source URL: https://arxiv.org/abs/2311.02085
- Reference count: 40
- This work introduces a preference elicitation framework for interactive recommender systems that handles soft attributes using concept activation vectors (CAVs), improving recommendation quality by up to 15-20% NDCG.

## Executive Summary
This paper presents a novel preference elicitation framework for interactive recommender systems that effectively handles soft attributes using concept activation vectors (CAVs). The method combines item-based and attribute-based preference elicitation, allowing users to respond to queries about both items and abstract soft attributes (e.g., "funnier" or "less thought-provoking"). By modeling uncertainty in CAV semantics and updating user preference beliefs using Bayesian methods, the system can more accurately estimate user preferences and generate better recommendations. Experiments on synthetic, RecSim NG, and MovieLens 20M datasets demonstrate that the method significantly improves recommendation quality, with the best performance achieved using Item-plus-Attribute (IpA) queries with the Expected Value of Information (EVOI) acquisition function.

## Method Summary
The method uses CAVs to discover soft attribute semantics in the item embedding space, allowing the system to query users about both specific items and abstract soft attributes. The system maintains a belief distribution over possible CAVs for each soft attribute rather than treating the CAV as a fixed vector. When computing response probabilities and expected value of information (EVOI), it takes expectations over this CAV distribution, leading to more robust belief updates and query selections. The Item-plus-Attribute (IpA) query type combines item selection and attribute critique in a single interaction, eliciting more comprehensive preference information. Bayesian belief updating is used to refine the user preference distribution after each query-response pair, and EVOI is used as an acquisition function to guide query selection.

## Key Results
- Combining item-based and attribute-based queries with CAV semantics improves preference elicitation efficiency by up to 15-20% NDCG
- Modeling CAV uncertainty in belief updates and query optimization significantly improves both information gathering and recommendation quality
- The Item-plus-Attribute (IpA) query type outperforms simpler query types by eliciting more comprehensive preference information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining item-based and attribute-based queries with CAV semantics improves preference elicitation efficiency by up to 15-20% NDCG.
- Mechanism: The method uses CAVs to discover soft attribute semantics in the item embedding space, allowing the system to query users about both specific items and abstract soft attributes (e.g., "funnier" or "less thought-provoking"). By modeling uncertainty in CAV semantics and updating user preference beliefs using Bayesian methods, the system can more accurately estimate user preferences.
- Core assumption: The underlying collaborative filtering model has learned meaningful representations of soft attributes in its item embedding space, making CAVs a valid proxy for attribute semantics.
- Evidence anchors:
  - [abstract]: "Leveraging concept activation vectors for soft attribute semantics, we develop novel preference elicitation methods that can accommodate soft attributes"
  - [section 2.3]: "We use the ùëî-score to measure the quality (or usefulness) of CAVùúôùëî, defining ùëÑ (ùúôùëî; Dùëî) to be the fraction of the item pairs... for which ùëêùëî (ùëñ1) ‚â• ùëêùëî (ùëñ2)"
  - [corpus]: "Average CAV quality (accuracy) is 0.909 on the test set, and Spearman correlation between predicted and ground-truth tags is 0.570"

### Mechanism 2
- Claim: Modeling CAV uncertainty in belief updates and query optimization significantly improves both information gathering and recommendation quality.
- Mechanism: The system maintains a belief distribution over possible CAVs for each soft attribute rather than treating the CAV as a fixed vector. When computing response probabilities and expected value of information (EVOI), it takes expectations over this CAV distribution, leading to more robust belief updates and query selections.
- Core assumption: The uncertainty in CAV semantics (due to sparse/noisy tag data) is non-trivial and affects downstream preference estimation.
- Evidence anchors:
  - [section 3.4]: "We assume that the RS has a CA V belief (distribution)ùëÉùëî (ùúôùëî | Dùëî) reflecting this uncertainty"
  - [section 5.1]: "With CAV uncertainty, the response probabilities used in PEU and other AFs are computed using expectation over CAV samples w.r.t. its belief distribution"
  - [corpus]: "We see that modeling CAV uncertainty offers significant gain in IG and RQ, with a 10-15% NDCG improvement over PE methods that update their beliefs by treating the mean CAV as 'certain'"

### Mechanism 3
- Claim: The Item-plus-Attribute (IpA) query type, which combines item selection and attribute critique in a single interaction, outperforms simpler query types by eliciting more comprehensive preference information.
- Mechanism: IpA queries first ask users to select their preferred item from a slate (providing information about specific item preferences), then ask them to critique that item w.r.t. a soft attribute (providing information about abstract attribute preferences). This dual approach captures both item-level and attribute-level preferences simultaneously.
- Core assumption: Users can meaningfully critique their selected item using soft attributes, and this critique provides additional preference information beyond what item selection alone would provide.
- Evidence anchors:
  - [section 3.3]: "Item-plus-Attribute (IpA) queries combine attribute and item queries... The user responds with ùúå = (ùëñ, +1) or ùúå = (ùëñ, ‚àí1) by comparing ùúô ‚àó ùêº,ùë¢ to ùëñ ‚àó ùëÜ"
  - [section 6]: "PE with IpA queries is much more effective than PE with simpler queries, an observation consistent with that seen in the RecSim NG setting. With the additional information collected at each PE step, IpA queries offer a 10‚Äì20% NDCG gain over item queries"
  - [corpus]: "Among the query types, PE with IpA performs the best, followed by item then attribute queries"

## Foundational Learning

- Concept: Concept Activation Vectors (CAVs) and their use in discovering soft attribute semantics
  - Why needed here: CAVs provide the mathematical framework for quantifying how much an item satisfies a soft attribute (the ùëî-score), which is essential for both querying users about attributes and interpreting their responses
  - Quick check question: How does the dot product ùúô ‚ä§ùëî ùúôùêº (ùëñ) relate to the degree to which item ùëñ satisfies attribute ùëî?

- Concept: Bayesian belief updating in preference elicitation
  - Why needed here: The system maintains a distribution over user preferences (the posterior) that gets updated after each query-response pair, allowing it to balance exploration and exploitation in subsequent queries
  - Quick check question: What is the mathematical form of the Bayesian update rule used to refine the user preference distribution after observing a response?

- Concept: Expected Value of Information (EVOI) as an acquisition function
  - Why needed here: EVOI measures the expected improvement in recommendation quality from a query response, guiding the system to select queries that provide the most useful information about user preferences
  - Quick check question: How does EVOI balance the trade-off between information gathering and immediate recommendation quality?

## Architecture Onboarding

- Component map:
  User Embedding Model -> CAV Trainer -> Query Generator -> Response Model -> Belief Updater -> Query Optimizer -> Recommendation Engine

- Critical path:
  1. Initialize user preference distribution from collaborative filtering model
  2. Generate candidate queries (attribute, item, or IpA)
  3. Compute acquisition function values for each query
  4. Select query with highest acquisition value
  5. Present query to user and observe response
  6. Update user preference distribution using Bayesian inference
  7. Repeat until stopping criterion met
  8. Generate final recommendations

- Design tradeoffs:
  - Computational vs. statistical efficiency: Iterative posterior updates provide better sample efficiency but require more computation per query than batch updates
  - Query complexity vs. information gain: IpA queries provide more information than simpler query types but may increase cognitive load on users
  - Certainty modeling vs. simplicity: Explicitly modeling CAV uncertainty improves performance but adds implementation complexity

- Failure signatures:
  - Poor CAV quality (low ùëÑ scores) indicates attribute semantics aren't well-learned, suggesting need for more tag data or alternative attribute representation methods
  - Posterior collapse (very low variance) suggests the system is overconfident in its estimates, potentially missing true user preferences
  - Stagnant acquisition function values indicate the system isn't learning new information, suggesting exploration parameters need adjustment

- First 3 experiments:
  1. Implement and test attribute queries with mean-slate response model on synthetic data with known ground truth to validate basic functionality
  2. Compare belief update methods (Gaussian vs. parameterized posterior) on RecSim NG to establish which provides better preference estimation
  3. Evaluate IpA vs. item vs. attribute queries with EVOI acquisition on MovieLens data to quantify information gain from combined query types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the uncertainty in CAV semantics affect the convergence speed of the posterior belief updates?
- Basis in paper: [explicit] The paper discusses modeling CAV uncertainty using Bayesian methods and shows improved performance when uncertainty is accounted for.
- Why unresolved: While the paper demonstrates that modeling CAV uncertainty improves performance, it doesn't quantify how this uncertainty impacts the rate at which the posterior converges to the true user embedding.
- What evidence would resolve it: Experiments comparing convergence rates with and without CAV uncertainty modeling, using metrics like KL divergence between successive posterior distributions or number of queries needed to reach a certain accuracy threshold.

### Open Question 2
- Question: How sensitive is the preference elicitation performance to the choice of temperature parameter T in the item response model?
- Basis in paper: [explicit] The paper mentions using a temperature parameter T in the item response model but doesn't explore its impact on elicitation performance.
- Why unresolved: The temperature parameter controls the sharpness of the probability distribution over item choices, which could significantly affect the quality of information gained from each query.
- What evidence would resolve it: A sensitivity analysis showing preference elicitation performance (e.g., NDCG, cosine similarity) across different values of T, potentially revealing an optimal range or the impact of misspecified T.

### Open Question 3
- Question: How does the performance of preference elicitation scale with the number of soft attributes (tags) available?
- Basis in paper: [inferred] The paper uses environments with a limited number of tags (10 in synthetic, 5 in RecSim NG, 164 in MovieLens) and shows that performance saturates with attribute queries when the number of tags is small.
- Why unresolved: The paper doesn't explore scenarios with a very large number of soft attributes, which could present challenges in terms of computational complexity and information overload for the user.
- What evidence would resolve it: Experiments on datasets with varying numbers of soft attributes, examining how performance metrics change and whether the proposed methods remain effective as the tag space grows.

## Limitations
- The method relies on the quality of CAVs, which depends on the availability and quality of tag data
- Computational overhead of maintaining CAV uncertainty distributions may be significant for large-scale applications
- The assumption that real users can meaningfully critique items using soft attributes remains largely untested with real users

## Confidence
- **High Confidence**: The core mechanism of combining item-based and attribute-based queries using CAVs is well-supported by consistent improvements across synthetic, RecSim NG, and MovieLens datasets. The mathematical framework for CAV computation and Bayesian belief updates is clearly specified.
- **Medium Confidence**: The benefits of modeling CAV uncertainty and using IpA queries are demonstrated but with less extensive ablation. The relative improvements (10-20% NDCG gains) are substantial but could be dataset-dependent.
- **Low Confidence**: The assumption that real users can meaningfully critique items using soft attributes, and that the cognitive load of IpA queries is acceptable, remains largely untested with real users rather than simulated responses.

## Next Checks
1. **CAV Sensitivity Analysis**: Systematically vary the amount and quality of tag data to establish the minimum requirements for effective CAV-based attribute queries, and identify the point at which attribute queries provide diminishing returns compared to item-only queries.

2. **Real User Study**: Conduct a small-scale user study comparing IpA queries against simpler query types to validate the assumed cognitive load, response quality, and actual preference elicitation efficiency with real users rather than simulated responses.

3. **Scalability Testing**: Evaluate the computational overhead of maintaining and integrating CAV uncertainty distributions on larger datasets (e.g., full MovieLens 20M) to quantify the trade-off between improved performance and increased computational cost.