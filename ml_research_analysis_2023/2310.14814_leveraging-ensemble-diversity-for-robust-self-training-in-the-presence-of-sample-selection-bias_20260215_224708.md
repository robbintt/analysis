---
ver: rpa2
title: Leveraging Ensemble Diversity for Robust Self-Training in the Presence of Sample
  Selection Bias
arxiv_id: '2310.14814'
source_url: https://arxiv.org/abs/2310.14814
tags:
- diversity
- confidence
- prediction
- self-training
- softmax
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new confidence measure for self-training
  based on the prediction diversity of an ensemble of linear classifiers. The method
  addresses the problem of overconfident softmax probabilities, which degrade performance
  under sample selection bias.
---

# Leveraging Ensemble Diversity for Robust Self-Training in the Presence of Sample Selection Bias

## Quick Facts
- **arXiv ID**: 2310.14814
- **Source URL**: https://arxiv.org/abs/2310.14814
- **Reference count**: 40
- **Key outcome**: T-similarity confidence measure improves self-training under sample selection bias by leveraging ensemble diversity

## Executive Summary
This paper addresses the challenge of overconfident softmax probabilities in self-training when labeled and unlabeled data follow different distributions (sample selection bias). The authors propose T-similarity, a novel confidence measure based on ensemble prediction diversity, which remains well-calibrated even under distribution shift. The method trains an ensemble of linear classifiers alongside a main prediction head, using a diversity-enhanced loss function to encourage varied predictions on unlabeled data. Experimental results demonstrate improved performance across multiple datasets and pseudo-labeling policies compared to conventional softmax-based confidence measures.

## Method Summary
The method trains an ensemble of M linear classifier heads alongside a main prediction head using a combined loss function. The loss has two components: (1) supervised loss on labeled data using the main prediction head, and (2) a diversity-enhanced confidence loss on unlabeled data that encourages ensemble members to produce diverse predictions. The T-similarity measure computes average pairwise agreement between ensemble predictions on unlabeled examples to estimate confidence. During self-training, pseudo-labels are generated using T-similarity thresholds rather than softmax probabilities, making the approach more robust to distribution shift between labeled and unlabeled data.

## Key Results
- T-similarity demonstrates better calibration (lower ECE) than softmax under sample selection bias
- The method improves self-training accuracy across multiple datasets and pseudo-labeling policies
- Theoretical analysis establishes connections between ensemble diversity and classifier performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using ensemble diversity to estimate confidence improves calibration under sample selection bias
- Mechanism: The T-similarity measure computes average pairwise agreement between ensemble predictions. Under SSB, softmax probabilities become overconfident on biased labeled data. T-similarity, by averaging agreement across diverse classifiers, reduces overconfidence and better reflects true prediction difficulty.
- Core assumption: Diversity in ensemble predictions correlates with uncertainty in the prediction being correct, especially under distribution shift
- Evidence anchors:
  - [abstract] "propose a novel confidence measure, called T-similarity, built upon the prediction diversity of an ensemble of linear classifiers"
  - [section] "Our main idea is to use the similarity between ensemble predictions on a given unlabeled example to estimate the prediction's confidence"
  - [corpus] Weak evidence - corpus contains related works on diversity-guided learning but no direct comparison to T-similarity measure
- Break condition: If ensemble diversity does not correlate with prediction uncertainty, or if diversity collapses under certain data distributions

### Mechanism 2
- Claim: The proposed loss function encourages diverse predictions while maintaining accuracy on labeled data
- Mechanism: The loss combines label fidelity (supervised loss on labeled data) with diversity loss (negative similarity on unlabeled data). The diversity term forces ensemble members to disagree on unlabeled data, creating diverse predictions that are more robust to distribution shift.
- Core assumption: The diversity loss can be optimized without severely degrading supervised performance
- Evidence anchors:
  - [abstract] "We provide the theoretical analysis of our approach by studying stationary points and describing the relationship between the diversity of the individual members and their performance"
  - [section] "Following Zhang and Zhou (2013), we train an ensemble to fit well the labeled set while having the most diverse possible individual predictions on the unlabeled set"
  - [corpus] Weak evidence - corpus mentions diversity-optimized ensembles but lacks specific theoretical analysis of this loss formulation
- Break condition: If diversity loss overwhelms supervised signal, causing poor performance on labeled data

### Mechanism 3
- Claim: The method remains robust when labeled and unlabeled data follow different distributions
- Mechanism: Under SSB, softmax probabilities from models trained on biased labeled data become unreliable. T-similarity, being based on ensemble agreement rather than absolute confidence, remains calibrated even when the base distribution shifts between labeled and unlabeled data.
- Core assumption: Ensemble agreement patterns remain stable under distribution shift even when individual classifier confidences fail
- Evidence anchors:
  - [abstract] "experimentally demonstrate the benefit of our confidence measure for three different pseudo-labeling policies on classification datasets of various data modalities"
  - [section] "We empirically demonstrate the superiority of our approach for self-training on various SSL datasets under SSB"
  - [corpus] Weak evidence - corpus contains related work on mitigating selection bias but lacks direct empirical comparison under SSB
- Break condition: If ensemble agreement patterns themselves break down under extreme distribution shift

## Foundational Learning

- Concept: Sample Selection Bias
  - Why needed here: Understanding how labeling constraints create distribution mismatch between labeled and unlabeled data is crucial for appreciating why conventional confidence measures fail
  - Quick check question: If labeling probability depends on feature values, what happens to the distribution of labeled vs unlabeled data?

- Concept: Ensemble Diversity and Its Relationship to Prediction Quality
  - Why needed here: The core innovation relies on using diversity as a proxy for uncertainty; understanding this relationship is essential for grasping the theoretical justification
  - Quick check question: In what scenarios would high ensemble diversity indicate high uncertainty versus high uncertainty in the ensemble itself?

- Concept: Semi-Supervised Learning with Pseudo-labeling
  - Why needed here: The method builds on standard self-training frameworks; understanding how pseudo-labeling works with confidence thresholds is necessary for implementation
  - Quick check question: What happens to self-training performance when the confidence measure is poorly calibrated?

## Architecture Onboarding

- Component map:
  - Input projection layers (shared across ensemble)
  - Ensemble of M linear classifier heads (M=5 in experiments)
  - Prediction head (separate from ensemble)
  - Loss computation with two components: supervised loss on labeled data, diversity-enhanced confidence loss on unlabeled data

- Critical path:
  1. Forward pass through shared projection layers
  2. Compute predictions from ensemble heads and prediction head
  3. Calculate T-similarity from ensemble predictions
  4. Compute combined loss (supervised + diversity)
  5. Backpropagate only through ensemble heads for diversity loss, through prediction head for supervised loss

- Design tradeoffs:
  - Ensemble size M vs computational cost
  - Diversity strength γ vs risk of underfitting
  - Linear vs nonlinear ensemble heads for efficiency vs expressiveness

- Failure signatures:
  - High variance in ensemble predictions despite low diversity loss (indicates optimization issues)
  - Performance degrades significantly when switching from IID to SSB labeling (indicates lack of robustness)
  - T-similarity values clustering near 0 or 1 regardless of prediction correctness (indicates miscalibration)

- First 3 experiments:
  1. Compare softmax vs T-similarity calibration curves on a simple dataset with known distribution shift
  2. Test sensitivity to diversity strength γ parameter on a benchmark dataset
  3. Evaluate performance degradation when ensemble size M is reduced from 5 to 2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the diversity strength hyperparameter γ affect the trade-off between ensemble diversity and classification performance in practical applications?
- Basis in paper: [explicit] The paper mentions γ as a hyperparameter controlling the strength of the imposed diversity and conducts sensitivity analysis on γ, but does not provide a theoretical framework for choosing optimal γ.
- Why unresolved: The theoretical analysis only shows that diversity is beneficial under certain assumptions, but does not quantify how much diversity is optimal for specific tasks or datasets.
- What evidence would resolve it: Empirical studies showing the performance of T-similarity across a wide range of γ values on diverse datasets, or theoretical bounds on the optimal γ value given specific dataset characteristics.

### Open Question 2
- Question: How does the proposed T-similarity method perform in multi-modal semi-supervised learning scenarios where the labeled and unlabeled data come from different distributions across multiple modalities?
- Basis in paper: [inferred] The paper mentions that T-similarity is model- and application-agnostic, but only evaluates it on datasets with single data modalities (biological, images, tabular, time series).
- Why unresolved: The paper does not test the method on scenarios where labeled and unlabeled data have different modalities, which is a common challenge in real-world applications.
- What evidence would resolve it: Experimental results comparing T-similarity to other methods on multi-modal datasets where the distribution shift exists across different data types.

### Open Question 3
- Question: Can the diversity maximization principle used in T-similarity be extended to other ensemble learning scenarios beyond self-training, such as active learning or domain adaptation?
- Basis in paper: [inferred] The paper focuses on self-training under sample selection bias, but mentions that the method can be combined with any SSL method using neural networks as a backbone.
- Why unresolved: The paper does not explore the applicability of T-similarity to other ensemble learning scenarios where diversity could be beneficial.
- What evidence would resolve it: Experimental results showing the effectiveness of T-similarity in active learning or domain adaptation settings, or theoretical analysis of how diversity affects these specific scenarios.

## Limitations

- The approach assumes ensemble diversity correlates with prediction uncertainty, which may not hold for all data distributions
- Computational overhead of maintaining an ensemble of linear heads may be prohibitive for large-scale applications
- Empirical validation is limited to classification tasks with specific SSB mechanisms, leaving open questions about performance on regression or more complex bias scenarios

## Confidence

- **High**: T-similarity improves calibration under SSB compared to softmax (supported by ECE metrics across datasets)
- **Medium**: Diversity loss effectively encourages diverse predictions without harming supervised performance (supported by theoretical analysis and ablation studies)
- **Low**: T-similarity generalizes well to all forms of distribution shift beyond SSB (limited empirical validation beyond specific SSB scenarios)

## Next Checks

1. Test T-similarity robustness to different SSB mechanisms (e.g., bias based on label vs. feature values) to validate generalizability claims
2. Conduct ablation studies varying ensemble size M to quantify the tradeoff between diversity benefits and computational cost
3. Evaluate performance on regression tasks or multi-label classification to assess applicability beyond binary/multiclass classification