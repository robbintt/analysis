---
ver: rpa2
title: A Family of Pretrained Transformer Language Models for Russian
arxiv_id: '2309.10931'
source_url: https://arxiv.org/abs/2309.10931
tags:
- language
- russian
- pretraining
- text
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces 13 Russian Transformer language models spanning
  encoder (ruBERT, ruRoBERTa, ruELECTRA), decoder (ruGPT-3), and encoder-decoder (ruT5,
  FRED-T5) architectures, pretrained on diverse Russian corpora ranging from 30GB
  to 450GB. The models are evaluated on standard Russian language understanding and
  generation benchmarks, achieving state-of-the-art performance on most tasks.
---

# A Family of Pretrained Transformer Language Models for Russian

## Quick Facts
- arXiv ID: 2309.10931
- Source URL: https://arxiv.org/abs/2309.10931
- Reference count: 22
- This paper introduces 13 Russian Transformer language models spanning encoder, decoder, and encoder-decoder architectures, pretrained on diverse Russian corpora and achieving state-of-the-art performance on most tasks.

## Executive Summary
This paper presents a comprehensive family of 13 Russian Transformer language models covering encoder (ruBERT, ruRoBERTa, ruELECTRA), decoder (ruGPT-3), and encoder-decoder (ruT5, FRED-T5) architectures. These models are pretrained on Russian corpora ranging from 30GB to 450GB and evaluated across diverse NLP tasks including understanding and generation benchmarks. The models achieve state-of-the-art performance on most Russian SuperGLUE tasks, with the overall score reaching 75.2% (compared to 81.1% human performance). The authors release all models on HuggingFace to enable research and industrial applications for Russian NLP.

## Method Summary
The authors develop 13 Russian Transformer models by pretraining them on diverse Russian corpora including Wikipedia, news, books, and other publicly available resources. The models use different self-supervised objectives: masked language modeling for encoder models, causal language modeling for decoder models, and span corruption for encoder-decoder models. Pretraining scales from 30GB to 450GB depending on the model. The models are evaluated using both fine-tuning (for encoder and encoder-decoder models) and zero-shot approaches (for decoder models) across Russian SuperGLUE benchmarks, acceptability classification (RuCoLA), news headline generation (OKMLCup), and text detoxification tasks.

## Key Results
- 75.2% overall score on Russian SuperGLUE (vs 81.1% human), achieving state-of-the-art performance on most tasks
- 80.8% accuracy on acceptability classification (RuCoLA)
- Matching human performance on text detoxification task
- FRED-T5-XL achieves highest scores across multiple Russian SuperGLUE tasks including DaNetQA (87.5% accuracy) and RuCoLA (86.2% Matthews correlation coefficient)

## Why This Works (Mechanism)

### Mechanism 1
Large-scale pretraining on diverse Russian corpora improves downstream task performance by exposing the model to rich linguistic patterns and domain variation. During pretraining, the model learns distributed representations that capture syntactic, semantic, and stylistic features of Russian across multiple domains. This generalization enables strong finetuning performance on specialized benchmarks.

### Mechanism 2
Model architecture diversity allows specialization for different task types. Encoder models excel at understanding tasks like classification and NLI; decoder models handle generation tasks with zero-shot capability; encoder-decoder models bridge understanding and generation for complex tasks like summarization and text simplification.

### Mechanism 3
Model scaling improves performance by increasing representational capacity. Larger models can capture more complex linguistic patterns and leverage more training data, leading to better generalization and task-specific adaptation.

## Foundational Learning

- **Concept: Masked Language Modeling (MLM)**
  - Why needed here: MLM is the core pretraining objective for encoder models, forcing the model to learn bidirectional context by predicting masked tokens.
  - Quick check question: What is the difference between MLM and standard language modeling objectives?

- **Concept: Span Corruption**
  - Why needed here: Span corruption is the pretraining objective for encoder-decoder models, where consecutive spans are masked and the model learns to reconstruct them, combining understanding and generation capabilities.
  - Quick check question: How does span corruption differ from token-level masking in MLM?

- **Concept: Fine-tuning vs Zero-shot Evaluation**
  - Why needed here: Understanding when to use fine-tuning (encoder and encoder-decoder models) versus zero-shot evaluation (decoder models) is critical for proper model evaluation and application.
  - Quick check question: When would you choose zero-shot evaluation over fine-tuning for a decoder-only model?

## Architecture Onboarding

- **Component map**: The model family consists of 6 encoder models (ruBERT-base, ruBERT-large, ruRoBERTa-large, ruELECTRA-small, ruELECTRA-medium, ruELECTRA-large), 3 decoder models (ruGPT-3-small, ruGPT-3-medium, ruGPT-3-large), and 4 encoder-decoder models (ruT5-base, ruT5-large, FRED-T5-large, FRED-T5-XL). Each model has different configurations and tokenization methods (BPE, BBPE, SentencePiece).

- **Critical path**: The primary workflow is pretraining → evaluation on benchmarks → fine-tuning for downstream tasks. The most critical path for a new engineer is understanding how to load and evaluate a specific model on a given task, which requires knowledge of the model architecture, tokenization, and appropriate evaluation protocol (finetuning vs zero-shot).

- **Design tradeoffs**: Encoder models are efficient for understanding tasks but cannot generate text; decoder models excel at generation but are inefficient for understanding tasks; encoder-decoder models offer flexibility but are larger and slower. The choice depends on the primary task requirements and computational constraints.

- **Failure signatures**: Poor performance on specific benchmarks may indicate: insufficient pretraining data for that domain, architectural mismatch, or suboptimal fine-tuning hyperparameters. Zero-shot performance issues in decoder models often indicate the need for better prompting strategies.

- **First 3 experiments**:
  1. Load ruRoBERTa-large and evaluate on Russian SuperGLUE using the standard fine-tuning procedure to establish baseline performance.
  2. Load ruGPT-3-medium and perform zero-shot evaluation on Russian SuperGLUE using the provided prompt templates to compare with fine-tuned models.
  3. Load FRED-T5-XL and evaluate on text summarization task (Gazeta) using the fine-tuning procedure with Adafactor optimizer to assess generation capabilities.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal learning rate and batch size combination for fine-tuning the FRED-T5 models on downstream tasks?
- Basis in paper: [inferred] The paper mentions that FRED-T5 models were fine-tuned for 20 epochs with a fixed set of hyperparameters, and the optimal hyperparameters are reported in Table 1. However, the authors leave the investigation of more optimal hyperparameters for the FRED-T5 models for future work.
- Why unresolved: The authors did not conduct a grid search over learning rates and batch sizes for the FRED-T5 models, which could potentially lead to better performance.
- What evidence would resolve it: Conducting a grid search over learning rates and batch sizes for the FRED-T5 models on downstream tasks and comparing the results to the current fixed hyperparameter setting.

### Open Question 2
- Question: How does the performance of the ruGPT-3 models change when using different decoding strategies, such as beam search or nucleus sampling, instead of the sampling strategy with top_p = 0.9 and repetition_penalty=1.5 used in the paper?
- Basis in paper: [explicit] The paper mentions that the ruGPT-3 models were evaluated using a classification approach based on a threshold for the PenLP acceptability measure and that the models were evaluated on the public and private test sets with a sampling strategy using top_p = 0.9 and repetition_penalty=1.5.
- Why unresolved: The authors only used one decoding strategy for the ruGPT-3 models, and it is unclear how the performance would change with different decoding strategies.
- What evidence would resolve it: Comparing the performance of the ruGPT-3 models using different decoding strategies, such as beam search or nucleus sampling, on downstream tasks and evaluating the impact on the results.

### Open Question 3
- Question: How do the proposed models perform on low-resource languages or dialects of Russian, such as Siberian Tatar or Bashkir?
- Basis in paper: [inferred] The paper focuses on developing Transformer language models for the Russian language, but it does not mention the performance of the models on low-resource languages or dialects of Russian.
- Why unresolved: The authors did not evaluate the performance of the proposed models on low-resource languages or dialects of Russian, which could be important for real-world applications and language preservation efforts.
- What evidence would resolve it: Evaluating the performance of the proposed models on low-resource languages or dialects of Russian and comparing the results to the performance on standard Russian tasks.

## Limitations

- The 75.2% overall score on Russian SuperGLUE still shows a substantial gap from human performance (81.1%), suggesting inherent limitations in current language models for Russian.
- Zero-shot evaluation for decoder models generally underperforms compared to fine-tuned encoder models on most understanding tasks, questioning their effectiveness.
- Computational costs associated with larger models (particularly FRED-T5-XL) may limit practical deployment in resource-constrained environments.

## Confidence

- **High Confidence**: The core claim that diverse transformer architectures trained on Russian corpora improve performance over multilingual baselines is well-supported by the empirical results across multiple benchmarks.
- **Medium Confidence**: The assertion that encoder-decoder models provide the best balance of understanding and generation capabilities is supported but limited by the relatively small number of generation-focused tasks in the evaluation suite.
- **Low Confidence**: Claims about zero-shot capabilities of decoder models are not strongly supported, as the evaluation shows these models generally underperform compared to fine-tuned alternatives on most Russian SuperGLUE tasks.

## Next Checks

1. **Zero-shot vs Fine-tuning Gap Analysis**: Systematically evaluate the performance gap between zero-shot decoder model outputs and fine-tuned encoder models across all Russian SuperGLUE tasks to quantify when each approach is appropriate.

2. **Cross-domain Generalization Test**: Evaluate model performance on out-of-domain Russian text (e.g., social media, technical documentation) to assess whether pretraining corpus diversity translates to robust generalization beyond benchmark datasets.

3. **Scaling Law Validation**: Conduct controlled experiments varying model size and pretraining data volume to verify whether scaling relationships observed in other languages hold for Russian, particularly for encoder-decoder architectures.