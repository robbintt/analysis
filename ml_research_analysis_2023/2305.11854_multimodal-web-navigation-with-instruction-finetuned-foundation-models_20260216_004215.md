---
ver: rpa2
title: Multimodal Web Navigation with Instruction-Finetuned Foundation Models
arxiv_id: '2305.11854'
source_url: https://arxiv.org/abs/2305.11854
tags:
- html
- arxiv
- webgum
- image
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies data-driven offline training for web agents with
  vision-language foundation models. We propose an instruction-following multimodal
  agent, WebGUM, that observes both webpage screenshots and HTML pages and outputs
  web navigation actions, such as click and type.
---

# Multimodal Web Navigation with Instruction-Finetuned Foundation Models

## Quick Facts
- **arXiv ID**: 2305.11854
- **Source URL**: https://arxiv.org/abs/2305.11854
- **Reference count**: 40
- **Primary result**: 3B parameter WebGUM model outperforms PaLM-540B on WebShop, achieving 45% success rate versus 40%

## Executive Summary
This work introduces WebGUM, an instruction-following multimodal agent for web navigation that combines vision-language foundation models with joint finetuning on webpage screenshots and HTML. The agent leverages an instruction-finetuned language model and vision encoder to achieve state-of-the-art performance on web navigation benchmarks, outperforming both prior offline methods and online-finetuned models. The key innovations include multimodal token fusion for grounded perception, leveraging instruction-finetuning for HTML comprehension, and demonstrating strong transfer to real-world planning tasks.

## Method Summary
WebGUM is trained through joint finetuning of an instruction-finetuned language model (Flan-T5) and a vision encoder (ViT) on a large corpus of demonstrations. The model processes multimodal inputs—webpage screenshots and HTML pages—alongside natural language instructions to output web navigation actions. Training uses behavioral cloning on 347K high-quality demonstrations, with the model learning to map multimodal observations to actions like click and type. The architecture employs transformer-based token fusion to combine visual and textual information for joint reasoning.

## Key Results
- WebGUM achieves 45.8% improvement over previous best offline methods on MiniWoB++
- 3-billion-parameter WebGUM outperforms PaLM-540B (540B) on WebShop benchmark
- Strong positive transfer to real-world planning tasks on Mind2Web dataset
- Collected 347K high-quality demonstrations, 38× larger than prior work

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Multimodal inputs improve grounding of visual and spatial context in web navigation.
- **Mechanism**: The model combines ViT-extracted patch tokens from screenshots with text tokens from HTML and instructions, allowing the transformer encoder to jointly process visual layout and semantic content.
- **Core assumption**: The patch-based ViT tokenization preserves spatial relationships critical for understanding UI layout.
- **Evidence anchors**: [abstract] mentions "grounded visual perception" and "multimodal vision-and-HTML observations"; [section 5.1] shows ablation results where white/random image inputs perform worse.

### Mechanism 2
- **Claim**: Instruction-finetuned LLMs improve HTML comprehension and robustness to input perturbations.
- **Mechanism**: Flan-T5, pretrained on diverse instruction-following tasks, is finetuned on web navigation data, improving the model's ability to parse HTML structure and handle compositional or corrupted inputs.
- **Core assumption**: Instruction-finetuning induces better semantic understanding of HTML beyond self-supervised pretraining.
- **Evidence anchors**: [abstract] mentions "better HTML comprehension" and robustness to "out-of-distribution HTML inputs"; [section 5.2] shows ablation where WebGUM outperforms HTML-only model on compositional tasks (+24.5%).

### Mechanism 3
- **Claim**: Instruction-finetuned LLMs provide multi-step reasoning as a transferable prior for decision making.
- **Mechanism**: Flan-T5's chain-of-thought reasoning ability is leveraged during finetuning on navigation trajectories, enabling the agent to compare products, backtrack, and make complex multi-step decisions.
- **Core assumption**: The reasoning skills learned during instruction-finetuning transfer to sequential decision making in navigation.
- **Evidence anchors**: [abstract] mentions "ability of multi-step reasoning" linked to WebShop performance exceeding PaLM-540B; [section 5.3] shows WebGUM achieves 45% success on WebShop vs 40% for ReAct (PaLM-540B).

## Foundational Learning

- **Concept**: Multimodal token fusion in transformer encoders
  - **Why needed here**: To combine ViT visual tokens with text tokens for joint reasoning over layout and semantics.
  - **Quick check**: Does the model architecture allow cross-attention between image patches and HTML tokens during encoding?

- **Concept**: Instruction-tuning as semantic alignment
  - **Why needed here**: To ensure the model interprets web navigation instructions as it would natural language instructions, improving robustness.
  - **Quick check**: Does the finetuning dataset contain paired (instruction, HTML+image, action) examples in a consistent format?

- **Concept**: Behavioral cloning from expert demonstrations
  - **Why needed here**: To learn the mapping from multimodal observations to correct web actions without online exploration.
  - **Quick check**: Are the demonstrations diverse enough to cover the space of possible HTML structures and visual layouts?

## Architecture Onboarding

- **Component map**: Screenshot + HTML + instruction → tokenized multimodal input → encoder fusion → decoder prediction → action execution
- **Critical path**: Webpage screenshots and HTML pages with natural language instructions are tokenized and processed through a multimodal encoder, which outputs text actions for web navigation.
- **Design tradeoffs**: Using patch-based ViT preserves spatial context but increases token count; using instruction-finetuned T5 adds reasoning ability but requires more compute
- **Failure signatures**: 
  - Action predictions become repetitive or stuck in loops
  - Success rate drops sharply on compositional tasks
  - Model fails to parse HTML elements correctly when perturbed
- **First 3 experiments**:
  1. Ablation: replace ViT visual tokens with white images to confirm multimodal grounding is used
  2. Ablation: swap Flan-T5 with base T5 to measure instruction-finetuning impact on HTML comprehension
  3. Ablation: reduce dataset size to find scaling limits for performance improvement

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of WebGUM scale with increasing dataset size beyond 347K episodes, and what is the asymptotic limit of this scaling?
- **Basis in paper**: [explicit] The paper discusses scaling effects with dataset sizes of 2.8K, 68K, and 347K, observing improvements in success rates.
- **Why unresolved**: The paper does not explore dataset sizes beyond 347K, leaving the potential for further improvement or saturation unknown.
- **What evidence would resolve it**: Experiments with datasets larger than 347K episodes to observe performance trends and potential saturation points.

### Open Question 2
- **Question**: How does WebGUM's performance compare to human-level performance on more diverse and complex real-world web navigation tasks?
- **Basis in paper**: [inferred] The paper mentions that WebGUM performs well on benchmarks like MiniWoB++ and WebShop but acknowledges that human-level generalization to diverse real-world websites is still a challenge.
- **Why unresolved**: The evaluation is limited to simulated environments, and there is no direct comparison to human performance on real-world tasks.
- **What evidence would resolve it**: Comparative studies of WebGUM against human performance on a wide range of real-world web navigation tasks.

### Open Question 3
- **Question**: What are the limitations of WebGUM in handling web navigation tasks that require understanding of dynamic content or real-time interactions?
- **Basis in paper**: [inferred] The paper suggests that WebGUM excels in tasks with static HTML and images but does not address dynamic content or real-time interactions.
- **Why unresolved**: The current evaluation does not include scenarios with dynamic content updates or real-time user interactions.
- **What evidence would resolve it**: Testing WebGUM on tasks involving dynamic content updates, such as live chat interactions or real-time data feeds, to assess its adaptability and performance.

### Open Question 4
- **Question**: How does the choice of pre-trained foundation models (e.g., T5, Flan-T5) impact WebGUM's performance, and what are the potential benefits of exploring other models?
- **Basis in paper**: [explicit] The paper uses T5 and Flan-T5 models and discusses their effectiveness, but does not explore other foundation models.
- **Why unresolved**: The paper does not investigate the impact of using alternative foundation models or architectures.
- **What evidence would resolve it**: Comparative experiments using different pre-trained models (e.g., GPT, BERT) to evaluate their impact on WebGUM's performance and generalization capabilities.

## Limitations
- **Dataset quality uncertainty**: The 347K demonstrations were collected using trained models, potentially introducing compounding errors or bias toward specific navigation patterns.
- **Generalization scope**: Performance claims on MiniWoB++ and transfer to Mind2Web lack rigorous analysis of true generalization to novel UI layouts and instruction types.
- **Computational practicality**: The 3-billion-parameter model achieving superior performance to PaLM-540B raises questions about deployment feasibility and inference efficiency.

## Confidence

- **High Confidence (8/10)**: The core technical contribution of multimodal token fusion and instruction-finetuning architecture is well-specified and reproducible. Ablation studies on MiniWoB++ provide strong evidence for the multimodal grounding mechanism.
- **Medium Confidence (5/10)**: Claims about instruction-finetuning improving HTML comprehension and reasoning are supported by ablation results but rely heavily on dataset-specific phenomena.
- **Low Confidence (3/10)**: Transfer learning claims to Mind2Web and comparison to human performance on MiniWoB++ lack sufficient experimental rigor, including confidence intervals and statistical significance tests.

## Next Checks

**Check 1: Dataset Quality Validation** - Conduct an independent audit of the 347K demonstration dataset to verify claimed diversity and quality. Sample 100 random demonstrations and categorize by task type, HTML complexity, and instruction novelty. Compute statistics on instruction-instruction and instruction-action alignment quality. Compare performance when training on subsets of different sizes to identify scaling limits.

**Check 2: Generalization Stress Test** - Create a systematic perturbation suite for HTML and visual inputs that varies structural complexity, element positioning, and visual noise levels. Evaluate WebGUM on this suite and compare against baseline models across perturbation intensity levels. Measure not just success rate but also action prediction confidence and navigation path efficiency under perturbations.

**Check 3: Cross-Domain Transfer Analysis** - Implement a controlled transfer learning experiment where models are trained on MiniWoB++ or WebShop data and evaluated on Mind2Web. Vary the training dataset composition (e.g., only MiniWoB++, only WebShop, combined) and measure transfer performance degradation. Analyze which instruction types, HTML structures, and visual layouts transfer successfully versus those that cause catastrophic forgetting.