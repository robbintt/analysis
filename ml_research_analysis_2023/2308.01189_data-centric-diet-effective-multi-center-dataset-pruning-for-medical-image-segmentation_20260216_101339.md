---
ver: rpa2
title: 'Data-Centric Diet: Effective Multi-center Dataset Pruning for Medical Image
  Segmentation'
arxiv_id: '2308.01189'
source_url: https://arxiv.org/abs/2308.01189
tags:
- training
- data
- score
- dataset
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of data pruning in medical image
  segmentation, where the goal is to identify and remove unnecessary samples from
  training datasets without sacrificing model performance. The authors propose using
  Dynamic Average Dice (DAD) score to rank data based on the learning difficulty of
  target regions, as traditional gradient-based metrics are ineffective for dense
  labeling tasks.
---

# Data-Centric Diet: Effective Multi-center Dataset Pruning for Medical Image Segmentation

## Quick Facts
- **arXiv ID:** 2308.01189
- **Source URL:** https://arxiv.org/abs/2308.01189
- **Reference count:** 5
- **One-line result:** Prunes up to 40% of medical image segmentation datasets while maintaining or improving Dice scores

## Executive Summary
This paper introduces a novel data pruning method for medical image segmentation that addresses the challenge of identifying and removing redundant or noisy samples from training datasets. The key innovation is the Dynamic Average Dice (DAD) score, which evaluates the learning difficulty of target regions rather than entire images. By focusing on segmentation targets instead of whole images, DAD score effectively identifies important examples for pruning. Experiments on three medical image segmentation benchmarks demonstrate that the proposed method can prune up to 40% of the dataset while maintaining or even improving segmentation performance.

## Method Summary
The method involves computing DAD scores for each training example during model training to rank samples by their learning difficulty. A 3D U-Net is trained on combined multi-center datasets (MSD, NIH, WORD pancreas CT scans) with DAD scores calculated every epoch. The moving distance curve is used to identify stable DAD rankings, allowing for dataset pruning of the most difficult examples. The pruned dataset is then retrained, achieving comparable or better performance with fewer training samples.

## Key Results
- Successfully prunes up to 40% of training data while maintaining or improving Dice scores
- Demonstrates that larger datasets don't always yield better segmentation performance
- DAD score effectively identifies important examples for medical image segmentation with combined data sources
- Reduces redundancy and noise in training data without sacrificing accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Gradient-based metrics like VOG and EL2N fail in dense labeling tasks because they average errors across the entire image, diluting the signal from small target regions.
- **Mechanism:** These metrics compute loss gradients or prediction errors over the whole image, which is dominated by background pixels in medical segmentation where the target (e.g., pancreas) is a small fraction of the image.
- **Core assumption:** The background occupies a much larger area than the target, so its contribution to the metric overwhelms the target's signal.
- **Evidence anchors:**
  - [abstract] "the loss gradient norm-based metrics of individual training examples applied in image classification fail to identify the important samples"
  - [section] "These gradient-based pruning metrics often compute score of the whole image, which does not work on the dense labeling task, especially form small target prospective, where the computed scores are easily affected by the large background regions"

### Mechanism 2
- **Claim:** DAD score focuses learning difficulty measurement on the target region by using Dice coefficient over time, making it effective for dense labeling tasks.
- **Mechanism:** DAD score computes the dynamic average of Dice scores specifically for the target region across training epochs, capturing how well the model learns that region.
- **Core assumption:** Dice coefficient is a suitable measure of segmentation quality for the target region and is sensitive to learning progress.
- **Evidence anchors:**
  - [abstract] "we propose a data pruning method by taking into consideration the training dynamics on target regions using Dynamic Average Dice (DAD) score"
  - [section] "we define DAD (Dynamic average dice) score to rank all examples globally" and "DAD score balances the two (foreground and background) well"

### Mechanism 3
- **Claim:** Dataset pruning based on DAD score removes redundant or noisy examples without harming performance, and can even improve it by reducing interference.
- **Mechanism:** By identifying and removing examples that are either too easy (redundant) or too hard (noisy), the pruned dataset has a more efficient data distribution that improves training dynamics and generalization.
- **Core assumption:** Not all examples contribute equally to learning; some are redundant or harmful, and their removal benefits the model.
- **Evidence anchors:**
  - [abstract] "Our solution can be used as a strong yet simple baseline to select important examples for medical image segmentation with combined data sources"
  - [section] "combining multiple datasets for training does not necessarily yield better models for segmentation" and "a larger dataset size is not always better"

## Foundational Learning

- **Concept:** Dense labeling vs. classification
  - Why needed here: Dense labeling tasks like segmentation require metrics that account for pixel-wise accuracy and spatial relationships, unlike classification which deals with whole-image labels.
  - Quick check question: Why does a metric that works for image classification fail for medical image segmentation?

- **Concept:** Dynamic evaluation during training
  - Why needed here: Static metrics computed at a single epoch may not capture the evolving learning difficulty of examples; dynamic metrics track progress over time.
  - Quick check question: What is the advantage of computing DAD score over multiple epochs instead of just at the end of training?

- **Concept:** Dataset redundancy and noise
  - Why needed here: Combining datasets can introduce redundancy (easy examples) or noise (conflicting labels), which can harm model performance.
  - Quick check question: How can combining datasets sometimes lead to worse model performance?

## Architecture Onboarding

- **Component map:** 3D U-Net → DAD score computation during training → Ranking of examples by DAD score → Dataset pruning → Retraining on pruned dataset
- **Critical path:** Training → DAD score calculation → Pruning decision → Retraining
- **Design tradeoffs:** Using Dice coefficient focuses on target but may miss global context; using full-image metrics captures context but dilutes target signal.
- **Failure signatures:** No improvement after pruning (pruning criteria too strict or too lenient), or performance drop (pruned too much or removed useful examples).
- **First 3 experiments:**
  1. Run baseline training on full dataset and compute DAD scores; verify that easy/hard examples have different semantic properties.
  2. Prune dataset at different ratios (e.g., 20%, 40%, 60%) using DAD score; retrain and compare performance to baseline.
  3. Compare DAD-based pruning to gradient-based methods (VOG, EL2N) on same datasets; measure memory usage and performance trade-offs.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of the DAD score-based pruning method vary across different medical imaging modalities (e.g., MRI, X-ray, ultrasound)?
- **Basis in paper:** [explicit] The paper only tests DAD score on CT images for pancreas segmentation across three datasets (MSD, NIH, WORD).
- **Why unresolved:** The study's focus on CT images limits understanding of the method's effectiveness across different imaging modalities, which may have different characteristics and noise patterns.
- **What evidence would resolve it:** Conducting experiments using DAD score for pruning datasets in other medical imaging modalities (e.g., MRI, X-ray, ultrasound) and comparing the results with those obtained from CT images.

### Open Question 2
- **Question:** How does the DAD score-based pruning method perform on segmentation tasks involving multiple organs or structures compared to single-organ segmentation?
- **Basis in paper:** [inferred] The paper focuses on pancreas segmentation, a single-organ task. It is unclear if the method would be as effective for multi-organ or multi-structure segmentation tasks.
- **Why unresolved:** The study's focus on a single-organ segmentation task limits understanding of the method's effectiveness for more complex segmentation tasks involving multiple organs or structures.
- **What evidence would resolve it:** Applying the DAD score-based pruning method to segmentation tasks involving multiple organs or structures and comparing the results with those obtained from single-organ segmentation tasks.

### Open Question 3
- **Question:** How does the DAD score-based pruning method compare to other data pruning techniques in terms of computational efficiency and memory usage?
- **Basis in paper:** [explicit] The paper mentions that gradient-based methods like GraNd and VOG require a large amount of gradient information to be saved, resulting in high memory usage. However, it does not provide a direct comparison of computational efficiency and memory usage between DAD score and other pruning techniques.
- **Why unresolved:** The study does not provide a comprehensive comparison of the computational efficiency and memory usage of DAD score-based pruning with other data pruning techniques.
- **What evidence would resolve it:** Conducting experiments to compare the computational efficiency and memory usage of DAD score-based pruning with other data pruning techniques, such as random pruning, variance of gradients (VOG), and EL2N.

## Limitations

- Method effectiveness may degrade when target regions occupy large portions of images or when Dice scores saturate early in training
- Pruning ratio of 40% appears effective but may be dataset-specific and lacks systematic validation
- Moving distance convergence criteria (0.01Lmax) lack rigorous validation and may be sensitive to hyperparameters
- Study focuses on pancreas segmentation from CT scans, limiting generalizability to other organs or modalities

## Confidence

- **High confidence:** The core claim that gradient-based metrics fail for dense labeling due to background dominance (Mechanism 1) is well-supported by the mathematical reasoning and ablation studies.
- **Medium confidence:** The DAD score's effectiveness in identifying important examples (Mechanism 2) is demonstrated but could benefit from more diverse dataset testing.
- **Medium confidence:** The claim that pruning improves performance by reducing redundancy/noise (Mechanism 3) is supported but requires further validation across different segmentation tasks and dataset combinations.

## Next Checks

1. Test DAD-based pruning on segmentation tasks with larger target regions (e.g., liver segmentation) to verify the method's robustness when background dominance is reduced.
2. Conduct ablation studies varying the pruning ratio systematically (10%, 20%, 40%, 60%) across multiple datasets to establish optimal pruning thresholds.
3. Compare DAD-based pruning against uncertainty-based methods (e.g., MC dropout, entropy) on the same datasets to validate relative effectiveness and computational efficiency.