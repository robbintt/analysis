---
ver: rpa2
title: 'The Cost of Down-Scaling Language Models: Fact Recall Deteriorates before
  In-Context Learning'
arxiv_id: '2310.04680'
source_url: https://arxiv.org/abs/2310.04680
tags:
- pruning
- accuracy
- tasks
- sparsity
- in-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies how scaling the number of parameters in large
  language models (LLMs) affects two core capabilities: recalling facts seen during
  pre-training and processing information presented in-context during inference. The
  authors propose a set of carefully designed tasks to disentangle these two abilities.'
---

# The Cost of Down-Scaling Language Models: Fact Recall Deteriorates before In-Context Learning

## Quick Facts
- **arXiv ID**: 2310.04680
- **Source URL**: https://arxiv.org/abs/2310.04680
- **Reference count**: 40
- **Primary result**: Reducing model size by over 30% significantly harms fact recall, but in-context learning remains largely intact even with 60-70% pruning.

## Executive Summary
This paper studies how scaling the number of parameters in large language models (LLMs) affects two core capabilities: recalling facts seen during pre-training and processing information presented in-context during inference. The authors propose a set of carefully designed tasks to disentangle these two abilities. They evaluate two scaling techniques - weight pruning and dense scaling (training smaller/larger models) - on six LLM families (OPT, LLaMA, Pythia) with sizes up to 33 billion parameters. The key finding is a striking disparity: reducing model size by over 30% significantly harms fact recall, but in-context learning (ICL) remains largely intact even with 60-70% pruning. This suggests scaling affects the two abilities differently. The results have implications for improving inference efficiency, LLM systems, and interpretability.

## Method Summary
The authors evaluate the effects of weight pruning and dense scaling on six LLM families (OPT, LLaMA, Pythia) with sizes up to 33 billion parameters. They use two pruning algorithms (SparseGPT and Wanda) to prune models to varying levels of sparsity and compare their performance to dense models of different sizes. The models are evaluated on fact recall tasks (closed-book QA) and in-context learning (ICL) tasks (open-book QA, overriding QA, parameterized function learning) using exact match accuracy and perplexity as metrics.

## Key Results
- Reducing model size by over 30% significantly decreases the ability to recall facts seen in pre-training.
- A 60-70% reduction in model size largely preserves the ability to process in-context information.
- Both pruning and dense scaling exhibit the same disparity between fact recall and in-context learning abilities.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Scaling model size has a fundamentally different effect on fact recall versus in-context learning (ICL).
- **Mechanism**: Fact recall depends on a dense network of parameters that encode individual facts. When pruning or scaling down, removing parameters disproportionately harms this distributed encoding. ICL, however, may rely on a smaller set of parameters that implement a more general mechanism (e.g., a universal gradient descent module) that can be applied across tasks.
- **Core assumption**: The number of parameters required to store a set of facts scales proportionally with the number of independent facts, while ICL can be accomplished by a smaller set of reusable parameters.
- **Evidence anchors**:
  - [abstract] "We find a striking difference in how these two abilities evolve due to scaling... Reducing the model size by more than 30%... significantly decreases the ability to recall facts seen in pre-training. Yet, a 60–70% reduction largely preserves the various ways the model can process in-context information..."
  - [section 4] "Moderate pruning (> 30% sparsity) harms fact recall, and yet the ability to learn from a few input-output examples from context withstands aggressive pruning (up to 60–70% sparsity)."
  - [corpus] Weak - related papers focus on fact recall in LMs and generalization, but do not directly address the scaling disparity between fact recall and ICL.
- **Break condition**: If future work demonstrates that ICL also relies on a large, distributed parameter set for each task, the mechanism would need revision.

### Mechanism 2
- **Claim**: The disparity between fact recall and ICL under scaling is not specific to pruning but is a general property of scaling.
- **Mechanism**: Both pruning and dense scaling (using smaller/larger models) show the same pattern of disparate effects on fact recall and ICL. This suggests that the underlying cause is the change in model size itself, not the specific scaling method.
- **Core assumption**: The disparate effects are inherent to the model architecture and how it represents and processes information, rather than being an artifact of a particular scaling technique.
- **Evidence anchors**:
  - [abstract] "The fact that both dense scaling and weight pruning exhibit this behavior suggests that scaling model size has an inherently disparate effect on fact recall and in-context learning."
  - [section 6] "Like pruning, changing a dense model's size more readily affects its ability to retrieve facts from pre-training than to process information from context."
  - [corpus] Weak - related papers do not directly compare the effects of different scaling methods on fact recall versus ICL.
- **Break condition**: If a new scaling method is developed that affects fact recall and ICL equally, it would challenge the general nature of this mechanism.

### Mechanism 3
- **Claim**: Attention and feed-forward (FFW) layers have different roles in fact recall versus ICL.
- **Mechanism**: Pruning only attention layers versus only FFW layers reveals that FFW layers are more important for fact recall, while both types of layers are equally important for ICL. This suggests that FFW layers are primarily responsible for storing factual knowledge, while attention layers handle the dynamic processing required for ICL.
- **Core assumption**: The architecture of LLMs (specifically the roles of attention and FFW layers) is such that FFW layers are better suited for storing static knowledge, while attention layers are better suited for processing dynamic information.
- **Evidence anchors**:
  - [section 7] "Attention and FFW layers show similar importance for ICL; and FFW layers show greater importance for knowledge recall."
  - [section 4] "Our findings suggest that scaling via pruning has a much higher impact on the ability to retrieve facts from pre-training than on the ability to retrieve information from the context."
  - [corpus] Weak - related papers do not directly analyze the roles of attention and FFW layers in fact recall versus ICL.
- **Break condition**: If future work shows that attention layers are also crucial for fact recall, or that FFW layers are not essential for storing factual knowledge, this mechanism would need revision.

## Foundational Learning

- **Concept**: Parameter pruning in neural networks
  - **Why needed here**: The paper evaluates the effects of weight pruning on LLM capabilities. Understanding how pruning works and its impact on model performance is crucial for interpreting the results.
  - **Quick check question**: What is the main goal of pruning in neural networks, and how does it typically affect model performance?

- **Concept**: In-context learning (ICL)
  - **Why needed here**: The paper studies how scaling affects ICL, a key capability of LLMs. Understanding what ICL is and how it differs from standard training is essential for grasping the paper's findings.
  - **Quick check question**: How does ICL differ from traditional supervised learning, and what are some examples of tasks that can be performed using ICL?

- **Concept**: Perplexity as a language model evaluation metric
  - **Why needed here**: The paper uses perplexity to measure the effect of pruning on language models. Understanding what perplexity measures and how to interpret it is important for evaluating the results.
  - **Quick check question**: What does perplexity measure in the context of language models, and what does a lower perplexity indicate about a model's performance?

## Architecture Onboarding

- **Component map**: The paper evaluates pruning effects on six LLM models from three families (OPT, LLaMA, Pythia) with sizes up to 33 billion parameters. The models consist of attention layers, feed-forward (FFW) layers, embedding layers, language modeling heads, and normalization layers. Pruning is applied only to fully-connected layers, which account for over 97.5% of the parameters.

- **Critical path**: 1) Select a model family and size. 2) Apply pruning to the fully-connected layers using SparseGPT or Wanda. 3) Evaluate the pruned model on fact recall tasks (closed-book QA) and ICL tasks (open-book QA, overriding QA, parameterized function learning). 4) Measure performance using exact match accuracy and perplexity.

- **Design tradeoffs**: Pruning reduces model size and computational cost but can harm performance, especially for fact recall. The choice of pruning algorithm (SparseGPT vs. Wanda) and the target sparsity level involve tradeoffs between efficiency gains and accuracy loss. Using bfloat16 instead of float16 for evaluation on TPUs introduces small systematic errors.

- **Failure signatures**: If pruning leads to a significant drop in fact recall accuracy (e.g., >5% relative to the dense model) at moderate sparsity levels (30-40%), it indicates that the model's ability to retrieve stored knowledge is sensitive to parameter removal. If ICL performance remains largely intact even at high sparsity levels (60-70%), it suggests that ICL relies on a more robust set of parameters.

- **First 3 experiments**:
  1. Prune a LLaMA-13B model to 30% sparsity using SparseGPT and evaluate its performance on the TriviaQA closed-book task and the linear classification ICL task.
  2. Prune a LLaMA-13B model to 60% sparsity using SparseGPT and evaluate its performance on the same tasks as in experiment 1.
  3. Prune a LLaMA-13B model using Wanda (which does not update remaining weights) to 50% sparsity and compare its performance on the TriviaQA closed-book and linear classification tasks to the SparseGPT results.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does pruning or dense scaling affect fact recall more than in-context learning (ICL) abilities in large language models (LLMs)?
- Basis in paper: [explicit] The paper finds that moderate pruning (>30% sparsity) harms fact recall, but ICL withstands aggressive pruning (up to 60-70% sparsity). This disparity holds for both pruning and dense scaling techniques.
- Why unresolved: The paper conjectures that fact recall requires more parameters to store independent facts, while ICL may rely on a smaller set of parameters acting as a universal gradient descent module. However, this hypothesis is not tested or proven.
- What evidence would resolve it: Theoretical analysis or experiments comparing the number of parameters needed for fact recall vs. ICL, or identifying the specific weights responsible for each ability.

### Open Question 2
- Question: How does pruning affect the interpretability of large language models (LLMs)?
- Basis in paper: [inferred] The paper notes that pruning may help better localize the weights responsible for in-context learning (ICL) ability, potentially complementing recent approaches to interpretability.
- Why unresolved: The paper does not explore this connection further or provide evidence for the potential benefits of pruning for interpretability.
- What evidence would resolve it: Experiments comparing the interpretability of pruned vs. unpruned models, or demonstrating how pruning can isolate neurons responsible for specific LLM capabilities.

### Open Question 3
- Question: Can memory augmentation techniques improve the trade-off between computational cost and task accuracy for downscaled models?
- Basis in paper: [explicit] The paper suggests that memory augmentation techniques, which present helpful facts to the model by augmenting them directly in the context, could be a promising way to improve this trade-off.
- Why unresolved: The paper does not explore this idea further or provide evidence for the potential benefits of memory augmentation.
- What evidence would resolve it: Experiments comparing the performance of downscaled models with and without memory augmentation on fact recall tasks.

## Limitations
- The generalizability of the results to other LLM architectures, scaling methods, or task domains is unclear due to the study's focus on specific models and tasks.
- The chosen task datasets may not fully capture the nuances of fact recall and in-context learning capabilities, introducing potential bias in the findings.

## Confidence
- **High Confidence**: The observation that reducing model size by over 30% significantly harms fact recall while preserving in-context learning up to 60-70% pruning.
- **Medium Confidence**: The proposed mechanisms explaining the disparity between fact recall and in-context learning under scaling.
- **Low Confidence**: The generalizability of the results to other LLM architectures, scaling methods, or task domains.

## Next Checks
1. Evaluate the effects of pruning and dense scaling on fact recall and in-context learning using different LLM architectures (e.g., GPT-3, BLOOM) and scaling methods (e.g., weight sharing, quantization) to assess the generalizability of the findings.
2. Investigate the role of specific attention and feed-forward layer configurations in fact recall and in-context learning by systematically varying their properties (e.g., attention head count, FFW layer size) during pruning or dense scaling.
3. Explore the impact of pruning and scaling on other LLM capabilities, such as logical reasoning, common sense understanding, or multi-step problem-solving, to gain a more comprehensive understanding of how model size affects different aspects of language understanding and generation.