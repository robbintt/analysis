---
ver: rpa2
title: 'TOLD: A Novel Two-Stage Overlap-Aware Framework for Speaker Diarization'
arxiv_id: '2303.05397'
source_url: https://arxiv.org/abs/2303.05397
tags:
- speaker
- diarization
- which
- eend-ola
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses speaker diarization in multi-talker scenarios
  by proposing a two-stage overlap-aware framework (TOLD). The first stage introduces
  an overlap-aware EEND (EEND-OLA) model that reformulates speaker diarization as
  a single-label classification problem using power set encoding, explicitly modeling
  speaker overlaps and dependencies.
---

# TOLD: A Novel Two-Stage Overlap-Aware Framework for Speaker Diarization

## Quick Facts
- arXiv ID: 2303.05397
- Source URL: https://arxiv.org/abs/2303.05397
- Reference count: 0
- Primary result: Achieves 10.14% DER on CALLHOME, setting new state-of-the-art

## Executive Summary
This paper addresses speaker diarization in multi-talker scenarios by proposing a two-stage overlap-aware framework (TOLD). The first stage introduces an overlap-aware EEND (EEND-OLA) model that reformulates speaker diarization as a single-label classification problem using power set encoding, explicitly modeling speaker overlaps and dependencies. The second stage employs a speaker overlap-aware post-processing (SOAP) model that iteratively refines the diarization results using speaker profiles and context-dependent modeling. Experiments on the CALLHOME dataset show that EEND-OLA achieves a 14.39% relative improvement in diarization error rate (DER) compared to the original EEND, and the SOAP model provides an additional 19.33% relative improvement. The final TOLD framework achieves a DER of 10.14%, setting a new state-of-the-art result on this benchmark.

## Method Summary
TOLD is a two-stage framework for speaker diarization that first uses EEND-OLA with power set encoding to predict speaker activities, then refines these predictions with SOAP using speaker profiles extracted from non-overlapped segments. The approach models overlapping speech explicitly by converting multi-label classification to single-label classification through power set encoding, where at most K speakers can be active simultaneously. The SOAP stage iteratively improves results by leveraging speaker profiles and context modeling through LSTM-based refinement. The framework is pre-trained on simulated mixtures from multiple datasets and fine-tuned on CALLHOME.

## Key Results
- EEND-OLA achieves 14.39% relative DER improvement over original EEND on CALLHOME
- SOAP provides additional 19.33% relative DER improvement when added to EEND-OLA
- Final TOLD framework achieves 10.14% DER, setting new state-of-the-art on CALLHOME
- EEND-OLA outperforms state-of-the-art methods with 6.25% relative improvement on CALLHOME

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Power set encoding (PSE) reformulates speaker diarization as a single-label classification problem, explicitly modeling speaker overlaps and dependencies.
- Mechanism: PSE maps the binary activity vectors of all speakers to a single integer label representing the active speaker combination (e.g., speakers 1 and 3 active → label 5 if binary encoding is [1,0,1] → 1×2^0 + 0×2^1 + 1×2^2 = 5).
- Core assumption: There are at most K speakers active simultaneously, and K is known or bounded.
- Evidence anchors:
  - [abstract] "we employ the power set encoding to reformulate speaker diarization as a single-label classification problem"
  - [section] "we assume that there are at most K = 3 speakers active simultaneously in one frame"
  - [corpus] Weak evidence - corpus neighbors do not directly discuss PSE or single-label reformulation.
- Break condition: If the actual number of simultaneous speakers exceeds the assumed K, PSE labels become ambiguous or impossible to encode uniquely.

### Mechanism 2
- Claim: The two-stage framework allows iterative refinement by first predicting coarse diarization with EEND-OLA, then refining with speaker profiles from non-overlapped segments.
- Mechanism: Stage 1 produces initial diarization with PSE labels; Stage 2 selects non-overlapped segments, extracts speaker profiles, and uses these profiles to iteratively re-diarize with SOAP.
- Core assumption: Non-overlapped segments exist and can reliably represent speaker profiles.
- Evidence anchors:
  - [abstract] "we further propose a novel Two-stage OverLap-aware Diarization framework (TOLD) by involving a speaker overlap-aware post-processing (SOAP) model to iteratively refine the diarization results of EEND-OLA"
  - [section] "In SOAP, non-overlapped speech segments are selected and fed into a pre-trained x-vector extractor to obtain the speaker profiles"
  - [corpus] No direct evidence in neighbors about two-stage iterative refinement.
- Break condition: If non-overlapped segments are rare or unreliable, SOAP cannot extract accurate speaker profiles for refinement.

### Mechanism 3
- Claim: LSTM-based EDA module generates attractors for each speaker, and minimizing PIT loss determines the correct order between attractors and speakers.
- Mechanism: EDA module outputs a flexible number of attractors; PIT loss with permutation-invariant training aligns attractors to ground-truth speakers, fixing the order for PSE label computation.
- Core assumption: The number of attractors equals the number of active speakers in the segment.
- Evidence anchors:
  - [abstract] "the overlap-aware EEND (EEND-OLA) model, in which speaker overlaps and dependency can be modeled explicitly"
  - [section] "The PIT loss is used to determine the corresponding order between attractors and speakers"
  - [corpus] No direct evidence in neighbors about EDA module or PIT loss usage.
- Break condition: If the model generates attractors incorrectly (wrong number or order), PSE labels and downstream diarization will be wrong.

## Foundational Learning

- Concept: Permutation-invariant training (PIT)
  - Why needed here: PIT handles the label permutation problem in multi-speaker scenarios, ensuring attractors are matched to the correct speakers.
  - Quick check question: Why can't we simply use cross-entropy loss without PIT in multi-speaker diarization?

- Concept: Power set encoding (PSE)
  - Why needed here: PSE converts multi-label outputs into single-label classification, simplifying the problem and enabling explicit modeling of overlaps.
  - Quick check question: How many unique PSE labels are possible if there are Smax speakers and at most K can be active simultaneously?

- Concept: Speaker embedding extraction (x-vector)
  - Why needed here: Speaker profiles extracted from non-overlapped segments are used in SOAP for iterative refinement.
  - Quick check question: What property must speaker embeddings have to be useful in profile-based refinement?

## Architecture Onboarding

- Component map: Input features → Transformer encoder → EDA module (attractor generation) → PIT loss alignment → PSE loss → SOAP (profile extraction + iterative refinement) → Final diarization

- Critical path:
  Input features → Transformer encoder → EDA module (attractor generation) → PIT loss alignment → PSE loss → SOAP (profile extraction + iterative refinement) → Final diarization

- Design tradeoffs:
  - PSE limits maximum simultaneous speakers (K), trading expressiveness for simpler modeling
  - Two-stage design adds complexity but enables better refinement using speaker profiles
  - Fixed maximum speaker count (Smax=8) simplifies model but may limit scalability

- Failure signatures:
  - DER increases sharply if K is too small for the dataset
  - SOAP refinement fails if non-overlapped segments are insufficient or noisy
  - Model outputs random attractors if EDA module or PIT loss is misconfigured

- First 3 experiments:
  1. Train EEND-OLA with K=2 and verify DER improvement over standard EEND on CALLHOME
  2. Test SOAP refinement by running Stage 1 alone vs. full TOLD on CALLHOME
  3. Vary K from 2 to 4 and measure DER to find optimal overlap modeling capacity

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- Only evaluated on CALLHOME dataset, limiting generalizability claims to other diarization benchmarks
- Relies on sufficient non-overlapped speech segments for SOAP refinement, which may not hold in highly overlapped scenarios
- Maximum speaker count (Smax=8) may not scale to larger meetings or broadcast scenarios with more participants

## Confidence
- High Confidence: EEND-OLA improves DER over original EEND (14.39% relative improvement on CALLHOME)
- Medium Confidence: Two-stage framework provides additional 19.33% relative improvement through SOAP refinement
- Low Confidence: Claims of state-of-the-art performance, as comparison with concurrent methods and cross-dataset generalization is limited

## Next Checks
1. **Architecture Validation:** Implement the exact transformer and LSTM configurations, then verify DER improvement on CALLHOME with controlled ablation studies comparing single-stage vs. two-stage approaches.

2. **Robustness Testing:** Evaluate TOLD on multiple diarization benchmarks (DIHARD III, AMI, etc.) with varying overlap densities and speaker counts to assess generalizability beyond the CALLHOME dataset.

3. **Assumption Testing:** Systematically vary the proportion of overlapped vs. non-overlapped speech in CALLHOME to quantify the impact of SOAP refinement and identify the minimum non-overlapped speech requirement for effective speaker profile extraction.