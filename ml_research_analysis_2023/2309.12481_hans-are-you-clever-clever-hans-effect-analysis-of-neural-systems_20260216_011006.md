---
ver: rpa2
title: HANS, are you clever? Clever Hans Effect Analysis of Neural Systems
arxiv_id: '2309.12481'
source_url: https://arxiv.org/abs/2309.12481
tags:
- it-llms
- language
- abilities
- have
- benchmarks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the robustness of instruction-tuned large\
  \ language models (It-LLMs) in social reasoning tasks, focusing on the potential\
  \ presence of the Clever Hans effect\u2014where models appear to perform well but\
  \ rely on superficial heuristics rather than true understanding. The authors evaluate\
  \ four MCQ benchmarks (PIQA, OpenBookQA, CommonsenseQA, Social IQA) using multiple\
  \ It-LLMs (Alpaca, Vicuna, Falcon, Llama2) in both zero-shot and few-shot scenarios."
---

# HANS, are you clever? Clever Hans Effect Analysis of Neural Systems

## Quick Facts
- arXiv ID: 2309.12481
- Source URL: https://arxiv.org/abs/2309.12481
- Reference count: 13
- Primary result: It-LLMs show significant performance gaps on adversarial MCQs, indicating reliance on positional heuristics rather than true reasoning.

## Executive Summary
This paper investigates whether instruction-tuned large language models exhibit the Clever Hans effect in social reasoning tasks, relying on superficial heuristics rather than genuine understanding. The authors evaluate four MCQ benchmarks using multiple It-LLMs in zero-shot, few-shot, and Chain-of-Thought scenarios. Results show substantial performance gaps when answer positions are varied, indicating positional bias. Chain-of-Thought prompting significantly improves robustness by forcing explicit reasoning, while few-shot prompting provides marginal benefits. The study concludes that It-LLMs possess some reasoning capabilities but are not inherently robust to adversarial conditions.

## Method Summary
The study evaluates four It-LLMs (Alpaca-13b, Vicuna-13b, Instruct-Falcon 7b, Llama2-chat 13b) on four MCQ benchmarks (PIQA, OpenBookQA, CommonsenseQA, Social IQA) using three prompting strategies: zero-shot, few-shot, and Chain-of-Thought. Adversarial variants are created by varying answer positions (First Target, Last Target). Performance is measured via string matching of generated outputs against correct answers. The analysis compares accuracy across original vs. adversarial benchmarks and evaluates the impact of different prompting strategies on robustness.

## Key Results
- It-LLMs show significant performance gaps when target answers are not in first position, indicating positional bias
- Chain-of-Thought prompting substantially improves reasoning abilities and reduces bias
- Few-shot prompting provides marginal improvements in robustness compared to zero-shot
- All models demonstrate vulnerability to adversarial conditions despite strong baseline performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: It-LLMs rely on positional bias and structural heuristics when selecting answers in MCQ tasks
- Mechanism: The model learns spurious correlations between answer position and correct choice during pretraining/fine-tuning, causing it to favor certain positions regardless of content
- Core assumption: The model's decision-making process is influenced by superficial patterns in training data rather than genuine reasoning about the question content
- Evidence anchors:
  - [abstract] "Following a correlation between first positions and model choices due to positional bias, we hypothesized the presence of structural heuristics in the decision-making process"
  - [section 4.1] "The order of the input-prompts seems to have a considerable impact on the choices of the It-LLMs. In fact, as shown in Table 3, there are significant imbalances in accuracy as the target options change"
  - [corpus] Weak evidence - corpus contains related work on Clever Hans effects but no direct evidence for positional bias in It-LLMs
- Break condition: If adversarial testing shows no performance gap when target choice position is varied, or if few-shot/few-shot scenarios eliminate the positional bias

### Mechanism 2
- Claim: Chain-of-Thought prompting forces models to engage in step-by-step reasoning, reducing reliance on heuristics
- Mechanism: By explicitly requiring the model to "think step by step," CoT prompts shift the decision-making process from pattern matching to sequential reasoning about the problem
- Core assumption: The model has latent reasoning capabilities that can be activated through appropriate prompting, even if not used by default
- Evidence anchors:
  - [abstract] "Finally, by using the Chain-of-Thought (CoT) technique, we elicit the model to reason and mitigate the bias by obtaining more robust models"
  - [section 4.3] "Stimulating the generative abilities of It-LLMs could be the key. Figure 2 shows that the performance of models where Chain-of-Thought prompting has been done is more stable and significantly better"
  - [corpus] Weak evidence - corpus contains related work on CoT but no direct evidence for its effectiveness on positional bias
- Break condition: If CoT prompting fails to improve robustness across multiple model families or if performance gains disappear with different prompting formulations

### Mechanism 3
- Claim: Few-shot prompting improves robustness by providing structural templates that override default heuristic patterns
- Mechanism: Including example question-answer pairs in the prompt establishes a reasoning template that the model follows, bypassing its default positional heuristics
- Core assumption: The model can extract and apply reasoning patterns from demonstrations, effectively learning to solve problems through example rather than memorizing positional patterns
- Evidence anchors:
  - [abstract] "Following, we tested different settings in a few-shot scenario, where we observed that introducing examples in the input prompt led to marginal improvements in the robustness of the It-LLMs"
  - [section 4.2] "constructing input-prompts with question-answer demonstrations helped reduce the order bias predominantly for the adversarial versions of the benchmarks considered"
  - [corpus] Weak evidence - corpus contains related work on few-shot learning but no direct evidence for its effectiveness on positional bias
- Break condition: If few-shot improvements disappear when examples are shuffled or when the number of examples is varied

## Foundational Learning

- Concept: Positional bias in language models
  - Why needed here: Understanding how models can learn spurious correlations between answer position and correctness is fundamental to interpreting the experimental results
  - Quick check question: If a model always selects the first answer choice regardless of content, what type of bias is this demonstrating?

- Concept: Adversarial evaluation methodology
  - Why needed here: The core experimental design involves creating adversarial examples by varying answer order to test model robustness
  - Quick check question: How does varying answer position in multiple-choice questions help distinguish between genuine reasoning and heuristic-based responses?

- Concept: Chain-of-Thought prompting
  - Why needed here: CoT is presented as a key intervention for improving model reasoning and reducing bias
  - Quick check question: What is the key difference between standard prompting and Chain-of-Thought prompting in terms of expected model behavior?

## Architecture Onboarding

- Component map: Original benchmarks -> Adversarial variants (First Target, Last Target) -> Evaluation
- Critical path:
  1. Load benchmark data and generate adversarial variants
  2. Construct prompts for each model/scenario combination
  3. Run inference and collect outputs
  4. Evaluate accuracy and compare across conditions
  5. Analyze performance gaps to identify bias patterns
- Design tradeoffs:
  - Using string matching vs. probability-based evaluation trades precision for comparability across model types
  - Zero-shot vs. few-shot evaluation trades insight into model capabilities for practical applicability
  - Generating multiple adversarial variants increases experimental coverage but also computational cost
- Failure signatures:
  - No performance gap between original and adversarial conditions suggests models are genuinely reasoning rather than using heuristics
  - CoT prompting fails to improve performance across multiple model families suggests either the mechanism doesn't work or the models lack reasoning capabilities
  - Inconsistent results across different model families suggests architecture-specific behaviors rather than general phenomena
- First 3 experiments:
  1. Run zero-shot evaluation on original benchmarks to establish baseline performance
  2. Generate adversarial variants and evaluate same models to identify positional bias
  3. Apply Chain-of-Thought prompting to adversarial benchmarks to test if reasoning elicitation improves robustness

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but the discussion section implies several areas for future research, particularly around understanding the true reasoning capabilities of It-LLMs and developing more robust evaluation methodologies.

## Limitations

- The study uses string matching for answer evaluation, which may not capture the model's reasoning process or confidence levels
- Results may not generalize beyond the specific model architectures and benchmarks tested
- The study cannot definitively prove that models lack social reasoning capabilities - they may possess some reasoning alongside the observed heuristics

## Confidence

**High confidence**: The observation that It-LLMs show performance gaps when answer positions are varied is robust and well-supported by experimental data. The positional bias effect is clearly demonstrated across multiple models and benchmarks.

**Medium confidence**: The claim that Chain-of-Thought prompting substantially improves reasoning and reduces bias is supported by the results, but the underlying mechanism (whether it truly activates reasoning vs. providing a different heuristic framework) remains unclear.

**Low confidence**: The assertion that few-shot prompting provides only marginal improvements may be premature, as the study doesn't systematically explore the impact of varying numbers of examples or their quality.

## Next Checks

1. **Confidence-based analysis**: Instead of string matching, analyze the model's confidence scores or probability distributions over answer choices to better understand the decision-making process and determine if CoT prompting changes confidence calibration rather than just answer selection.

2. **Generalization testing**: Apply the same adversarial methodology to additional reasoning benchmarks (e.g., ARC, LogiQA) and model architectures (e.g., GPT-3.5, Claude) to assess whether the observed positional bias is a general phenomenon or specific to the tested conditions.

3. **Mechanism dissection**: Design controlled experiments to distinguish between true reasoning activation and alternative heuristic frameworks by varying the complexity and structure of CoT prompts, testing whether simpler prompting strategies (like explicit reasoning instructions without step-by-step breakdown) achieve similar bias reduction.