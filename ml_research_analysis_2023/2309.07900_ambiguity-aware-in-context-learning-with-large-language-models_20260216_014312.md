---
ver: rpa2
title: Ambiguity-Aware In-Context Learning with Large Language Models
arxiv_id: '2309.07900'
source_url: https://arxiv.org/abs/2309.07900
tags:
- demonstrations
- gold
- label
- test
- example
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how to select effective in-context learning
  (ICL) demonstrations for large language models (LLMs) by considering both semantic
  similarity and the model''s existing knowledge about task label ambiguity. The authors
  propose a method that retrieves semantically similar training examples and then
  applies three constraints: (1) select demonstrations whose gold labels are in the
  ambiguous label set (the two labels the model is most confused about for the test
  example), (2) limit demonstrations to those the model previously misclassified,
  and (3) further constrain to misclassified examples whose predicted labels also
  fall in the ambiguous set.'
---

# Ambiguity-Aware In-Context Learning with Large Language Models

## Quick Facts
- arXiv ID: 2309.07900
- Source URL: https://arxiv.org/abs/2309.07900
- Reference count: 28
- Primary result: Ambiguity-aware ICL improves F1 macro scores by 1.5%-2.6% over retriever-based baselines

## Executive Summary
This paper addresses the challenge of selecting effective in-context learning (ICL) demonstrations for large language models by incorporating both semantic similarity and label ambiguity awareness. The authors propose a method that retrieves semantically similar examples and then applies three incremental constraints based on the model's predicted label ambiguities. Experiments on three text classification tasks demonstrate that this approach outperforms retriever-based baselines by leveraging the model's own confusion patterns to select more informative demonstrations.

## Method Summary
The method uses a retriever to find semantically similar training examples, then applies three constraints to select demonstrations: (1) GOLD - only include examples whose gold labels are in the ambiguous label set (top-2 predicted labels for the test example), (2) MIS - only include examples the model previously misclassified, and (3) PRED - further constrain to misclassified examples whose predicted labels also fall in the ambiguous set. This creates a progressive improvement from retriever-based ICL to the full AMBIG-ICL method. The approach is evaluated on three text classification tasks using Flan-PaLM 2 models.

## Key Results
- F1 macro scores improve by 1.5%-2.6% over retriever-based ICL baselines
- Smaller models benefit more than larger models (+3.9% vs +1.4% for Flan-PaLM 2 variants)
- EDOS task shows highest improvement at 2.6% F1 gain
- The ambiguous label set serves as a good proxy for gold labels, explaining the effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Including demonstrations whose gold labels fall within the ambiguous label set improves ICL performance by biasing the model toward the correct label.
- Mechanism: The model's predictions are biased toward the labels present in the ICL demonstrations (Lyu et al., 2023). By including demonstrations with labels from the ambiguous set (the two labels the model is most confused about), we increase the probability of selecting the correct label for the test example.
- Core assumption: The ambiguous label set serves as a good proxy for the test example's gold label.
- Evidence anchors:
  - [abstract]: "we find that the ambiguous label set acts as a good proxy to the gold test label"
  - [section]: "From Table 4 we can see that the demonstrations selected solely based on input text similarity is only 13.3% points (avg.) behind our proposed methods"
- Break condition: If the ambiguous label set poorly correlates with the gold label distribution, the proxy assumption fails and performance degrades.

### Mechanism 2
- Claim: Including misclassified demonstrations helps the model correct its mistakes by showing contrasting examples.
- Mechanism: By presenting the model with examples it previously misclassified (along with their correct labels), the model is forced to reconcile its incorrect prediction with the ground truth, potentially improving its reasoning for similar cases.
- Core assumption: The model can learn from its own mistakes when explicitly shown the correct answers.
- Evidence anchors:
  - [abstract]: "including demonstrations that the LLM previously mis-classified and also fall on the test example's decision boundary, brings the most performance gain"
  - [section]: "we are interested in identifying 'hard' demonstrations i.e. examples on which the model makes mistakes"
- Break condition: If the model fails to recognize the relevance of the misclassified examples to the current test case, the correction signal may be ignored.

### Mechanism 3
- Claim: Combining semantic similarity with label-based constraints creates a more effective demonstration selection strategy than either approach alone.
- Mechanism: Semantic similarity ensures the demonstrations are topically relevant to the test example, while label-based constraints ensure the demonstrations help resolve the model's specific label confusion. The combination addresses both input similarity and output space knowledge.
- Core assumption: Both semantic relevance and label ambiguity awareness contribute independently to ICL effectiveness.
- Evidence anchors:
  - [abstract]: "it is beneficial to not only choose semantically similar ICL demonstrations but also to choose those demonstrations that help resolve the inherent label ambiguity"
  - [section]: "we find that considering output label space for selecting demonstrations is as important as considering the input similarity"
- Break condition: If semantic similarity becomes too weak when adding label constraints, the demonstrations may lose topical relevance and become ineffective.

## Foundational Learning

- Concept: In-context learning (ICL) - the ability of LLMs to perform tasks by conditioning on task demonstrations without parameter updates
  - Why needed here: The entire paper builds on ICL as the baseline method being improved
  - Quick check question: What distinguishes ICL from traditional fine-tuning approaches?

- Concept: Label ambiguity and decision boundaries - when a model is equally likely to predict multiple labels for a given input
  - Why needed here: The proposed method specifically targets examples where the model shows label confusion
  - Quick check question: How is the ambiguous label set determined in this work?

- Concept: Semantic similarity and text retrieval - methods for finding examples with similar input text
  - Why needed here: The baseline approach uses semantic similarity to select demonstrations, which the proposed method builds upon
  - Quick check question: What retrieval method is used to find semantically similar examples in this work?

## Architecture Onboarding

- Component map: Retriever → Ambiguous label set identification → Misclassification filtering → Demonstration selection → Prompt construction
- Critical path: Retriever output → Model prediction on training data → Ambiguous label set determination → Constraint application → Final demonstration selection
- Design tradeoffs: Higher semantic similarity vs. better label relevance; more constraints vs. fewer demonstrations; computational cost of additional predictions vs. performance gains
- Failure signatures: Performance degradation when ambiguous label set poorly correlates with gold labels; ineffective demonstrations when semantic similarity is sacrificed too much; overfitting to specific dataset characteristics
- First 3 experiments:
  1. Baseline comparison: Run zero-shot, static N-shot, and retriever-based ICL to establish performance floor
  2. Incremental constraint testing: Test +GOLD, +GOLD+MIS, and +GOLD+MIS+PRED separately to measure individual contributions
  3. Ablation study: Remove either semantic similarity or label constraints to quantify their relative importance

## Open Questions the Paper Calls Out

- Question: How would expanding the ambiguous label set beyond two labels impact the performance of the proposed method, particularly for tasks with larger label spaces like GoEmotions?
  - Basis in paper: The authors mention this as a potential direction for future work, noting that expanding the ambiguous label set could be especially important for datasets like GoEmotions with 27 emotional classes.
  - Why unresolved: The paper only explores the top-2 ambiguous labels, leaving open the question of whether considering more labels would provide additional benefits or potentially dilute the effectiveness of the approach.
  - What evidence would resolve it: Systematic experiments varying the size of the ambiguous label set (e.g., top-3, top-5) across different classification tasks with varying label space granularities would demonstrate the optimal size and its impact on performance.

- Question: Would the proposed ambiguity-aware ICL method generalize effectively to token-level or span-level tasks like Named Entity Recognition (NER) or Part-Of-Speech (POS) tagging?
  - Basis in paper: The authors explicitly mention this as a potential avenue for future work, noting that their current focus on sentence classification tasks "paves the way for others to use our proven techniques to also explore label ambiguity for other token/span-level tasks."
  - Why unresolved: The paper only validates the method on sentence-level classification tasks, leaving open questions about how well the approach would transfer to tasks with different label structures and evaluation metrics.
  - What evidence would resolve it: Empirical evaluation of the proposed method on token-level tasks like NER or POS tagging, comparing performance against retriever-based baselines and measuring improvements across different label sets and ambiguity patterns.

- Question: What is the optimal balance between semantic similarity and label ambiguity when selecting ICL demonstrations, and how does this balance vary across different model sizes and task complexities?
  - Basis in paper: The authors observe that "by selecting lower ranked examples from the retrieved set we are sacrificing the semantic similarity to the test input," yet their methods still outperform baselines. They also note that smaller models benefit more from the label-based constraints than larger models.
  - Why unresolved: While the paper demonstrates that considering label ambiguity improves performance, it doesn't provide a systematic analysis of the trade-off between semantic similarity and label ambiguity, nor does it explore how this trade-off might vary across different model sizes or task characteristics.
  - What evidence would resolve it: Ablation studies systematically varying the proportion of demonstrations selected based on semantic similarity versus label ambiguity, across different model sizes and task complexities, would reveal the optimal balance and how it depends on these factors.

## Limitations

- The effectiveness appears task-dependent with strong gains on some tasks but modest improvements on others
- The method requires predicting on training data to identify misclassifications and ambiguous labels, increasing computational cost
- The proxy relationship between ambiguous label sets and gold labels is correlational rather than proven causal

## Confidence

**High Confidence**: The experimental methodology is sound, with proper ablation studies and incremental constraint testing. The improvement over baseline retriever-based ICL is statistically significant and reproducible.

**Medium Confidence**: The mechanism explaining why ambiguous label sets correlate with gold labels is plausible but not rigorously established. The theoretical justification relies on observational evidence rather than causal analysis.

**Low Confidence**: The scalability of this approach to larger model families and more complex task types remains untested. The method's dependence on being able to predict on training data for constraint selection may limit its applicability in resource-constrained settings.

## Next Checks

1. Cross-task validation: Test the method on additional text classification tasks beyond the three presented to establish generalizability. Focus on tasks with different label distributions and ambiguity patterns.

2. Ablation of proxy quality: Systematically measure the correlation between ambiguous label sets and gold labels across different examples to quantify when the proxy assumption holds and when it fails.

3. Alternative ambiguity detection: Compare the proposed log-likelihood-based ambiguous label identification with other methods (entropy-based, margin-based) to determine if the specific approach is critical to performance gains.