---
ver: rpa2
title: Multi-Task Pseudo-Label Learning for Non-Intrusive Speech Quality Assessment
  Model
arxiv_id: '2308.09262'
source_url: https://arxiv.org/abs/2308.09262
tags:
- speech
- mtq-net
- loss
- prediction
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses non-intrusive speech quality assessment by
  proposing a Multi-Task Pseudo-Label Learning (MPL) approach. MPL combines pseudo-label
  scores derived from a pretrained MOSA-Net model with multi-task learning to train
  MTQ-Net for predicting three 3QUEST metrics (S-MOS, N-MOS, G-MOS).
---

# Multi-Task Pseudo-Label Learning for Non-Intrusive Speech Quality Assessment Model

## Quick Facts
- arXiv ID: 2308.09262
- Source URL: https://arxiv.org/abs/2308.09262
- Reference count: 32
- Primary result: MPL achieves higher LCC (up to 0.912), SRCC (up to 0.903), and lower MSE (down to 0.039) than baseline SSL-based models

## Executive Summary
This paper proposes Multi-Task Pseudo-Label Learning (MPL) for non-intrusive speech quality assessment. The approach combines pseudo-labels derived from a pretrained MOSA-Net model with multi-task learning to train MTQ-Net for predicting three 3QUEST metrics (S-MOS, N-MOS, G-MOS). MPL uses both supervised loss (ground-truth labels) and semi-supervised loss (pseudo-labels) with Huber loss optimization. Experiments demonstrate that MPL outperforms scratch training and knowledge transfer approaches, with WavLM embeddings providing additional performance gains.

## Method Summary
The MPL approach consists of two stages: pseudo-label generation and multi-task learning. First, a pretrained MOSA-Net model generates pseudo-labels (PESQ, STOI, SDI) from the training data. Then MTQ-Net is trained using both the ground truth 3QUEST labels and the pseudo-labels through a semi-supervised learning framework. The model employs a shared CNN-BLSTM encoder to process cross-domain features (STFT power spectral features, SincConv LFB, and SSL embeddings) before passing them to six task-specific layers for the three 3QUEST metrics and three pseudo-label metrics. Training uses Huber loss to combine supervised and semi-supervised objectives.

## Key Results
- MTQ-Net with MPL achieves LCC scores up to 0.912, SRCC scores up to 0.903, and MSE as low as 0.039
- MPL outperforms scratch training and knowledge transfer approaches across all three 3QUEST metrics
- WavLM embeddings provide better performance than HuBERT embeddings when combined with MPL
- The MPL approach demonstrates significant improvement over SSL-based baseline models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pseudo-label training improves generalization on small target datasets by leveraging richer signals from a larger pretraining corpus.
- Mechanism: The MOSA-Net model, trained on a larger dataset with multiple objective metrics, provides pseudo-labels that encode perceptual information not captured by the 3QUEST ground truth alone. MTQ-Net learns simultaneously from the limited 3QUEST labels and these pseudo-labels via a semi-supervised loss, effectively augmenting the training signal.
- Core assumption: Pseudo-labels from a well-trained model are sufficiently correlated with the target metrics to provide useful gradients without introducing harmful bias.
- Break condition: Pseudo-labels become uncorrelated with the target metrics, or the pseudo-label model overfits to its own training set and introduces distribution shift.

### Mechanism 2
- Claim: Huber loss improves robustness compared to pure MSE or MAE by adapting to error magnitude.
- Mechanism: The Huber loss function behaves like MSE for small errors and like MAE for large errors, controlled by the delta parameter. This prevents large outliers from dominating the gradient while still being sensitive to small prediction errors, which is important when combining supervised and semi-supervised losses.
- Core assumption: The error distribution in the combined supervised/semi-supervised setting has both small and large deviations that benefit from adaptive loss behavior.
- Break condition: The optimal delta value is mis-specified, causing the loss to behave too much like MSE (sensitive to outliers) or too much like MAE (insensitive to small errors).

### Mechanism 3
- Claim: Multi-task learning with shared encoder layers improves feature representation for all target metrics.
- Mechanism: MTQ-Net uses a shared CNN-BLSTM encoder that processes cross-domain features before passing them to task-specific layers. Training on multiple related tasks forces the encoder to learn more generalizable representations that benefit all tasks.
- Core assumption: The three 3QUEST metrics and the pseudo-label metrics share sufficient underlying acoustic features that joint training improves the encoder's feature extraction.
- Break condition: The tasks are too dissimilar, causing negative transfer where learning one task degrades performance on others.

## Foundational Learning

- Concept: Pseudo-label learning in semi-supervised settings
  - Why needed here: The target dataset is small, so direct supervised learning would overfit. Pseudo-labels from a larger dataset provide additional training signal.
  - Quick check question: What is the key difference between supervised and semi-supervised learning in this context?

- Concept: Multi-task learning and shared representations
  - Why needed here: The three 3QUEST metrics (S-MOS, N-MOS, G-MOS) are related speech quality dimensions that can benefit from shared feature extraction.
  - Quick check question: Why would training on multiple related tasks improve performance on each individual task?

- Concept: Huber loss and robust optimization
  - Why needed here: Combining supervised and semi-supervised losses creates a heterogeneous error landscape where some predictions may have larger errors than others.
  - Quick check question: How does Huber loss differ from MSE and MAE in handling outliers?

## Architecture Onboarding

- Component map: Speech waveform -> STFT power spectral features, SincConv LFB, SSL embeddings -> CNN-BLSTM encoder -> 6 task-specific layers (S-MOS, N-MOS, G-MOS, PESQ, STOI, SDI) -> Huber loss computation -> predicted quality scores
- Critical path: Feature extraction → CNN-BLSTM encoding → task-specific prediction → loss computation → parameter update
- Design tradeoffs:
  - Using WavLM vs HuBERT for SSL embeddings: WavLM showed better performance but requires more computational resources
  - Number of pseudo-labels: More pseudo-labels could provide richer information but may introduce noise
  - Delta parameter in Huber loss: Must be tuned per metric for optimal performance
- Failure signatures:
  - High MSE on validation but good LCC/SRCC: Model predicts accurate rankings but poor absolute values
  - Pseudo-labels dominate training: Supervised loss becomes negligible, model overfits to pseudo-labels
  - Negative transfer between tasks: One task's performance degrades as others improve
- First 3 experiments:
  1. Baseline from scratch training with only 3QUEST labels to establish minimum performance
  2. Knowledge transfer from MOSA-Net to isolate the benefit of pretraining
  3. MPL with HuBERT embeddings to test pseudo-label augmentation effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MTQ-Net vary with different values of the Huber loss parameter δ for different assessment metrics?
- Basis in paper: [explicit] The paper explores the effect of different δ values (0.5, 0.75, 1.00, 1.25) on MTQ-Net's performance, noting that different assessment metrics respond differently to changes in δ.
- Why unresolved: The paper only tests a limited set of δ values and does not provide a comprehensive analysis of the optimal δ for each metric.
- What evidence would resolve it: Further experiments testing a wider range of δ values and analyzing the performance of MTQ-Net for each assessment metric would provide more insight into the optimal δ for each case.

### Open Question 2
- Question: How does the performance of MTQ-Net compare to other SSL-based models when using different SSL models for generating cross-domain features?
- Basis in paper: [explicit] The paper compares MTQ-Net with MOS-SSL using WavLM embeddings, but does not explore the use of other SSL models for generating cross-domain features.
- Why unresolved: The paper only considers one SSL model (WavLM) for generating cross-domain features, leaving the potential benefits of other SSL models unexplored.
- What evidence would resolve it: Experiments comparing MTQ-Net with other SSL-based models using different SSL models for generating cross-domain features would provide insights into the impact of SSL model choice on performance.

### Open Question 3
- Question: How does the performance of MTQ-Net vary with the size of the training set?
- Basis in paper: [explicit] The paper does not investigate the effect of training set size on MTQ-Net's performance.
- Why unresolved: The paper does not provide information on how the size of the training set affects the model's performance, leaving the scalability of the approach unclear.
- What evidence would resolve it: Experiments training MTQ-Net with different sizes of training sets and analyzing the performance changes would provide insights into the model's scalability and data efficiency.

## Limitations

- The effectiveness of pseudo-labels depends critically on the quality and correlation of the pretrained MOSA-Net model, but no quantitative analysis of pseudo-label quality is provided.
- The study uses a fixed delta parameter (δ=1.0) for Huber loss across all tasks and metrics without exploring metric-specific optimal values.
- The training procedure uses a fixed loss weight (αS=1.0) without exploring whether dynamic weighting could improve performance given different scales and noise characteristics of supervised versus semi-supervised losses.

## Confidence

- **High Confidence**: The general framework of combining pseudo-labels with multi-task learning for speech quality assessment is sound and well-established in the literature. The experimental results showing improvement over baselines are likely reliable.
- **Medium Confidence**: The specific architectural choices (Huber loss, cross-domain features, multi-task setup) are appropriate but their relative contributions to the final performance are not isolated through ablation studies.
- **Low Confidence**: The claim that pseudo-labels from PESQ/STOI/SDI metrics are the optimal choice for improving 3QUEST metric prediction lacks comparison with other potential pseudo-label sources or metrics.

## Next Checks

1. **Pseudo-label quality assessment**: Compute correlation between MOSA-Net predictions (PESQ, STOI, SDI) and ground truth 3QUEST metrics on the training set to verify pseudo-labels are useful signals rather than noise.

2. **Ablation study on loss components**: Train MTQ-Net with only supervised loss, only semi-supervised loss, and various combinations to quantify the exact contribution of pseudo-labels versus multi-task learning.

3. **Hyperparameter sensitivity analysis**: Systematically vary the Huber loss delta parameter (δ) and loss weight (αS) across different metrics to identify optimal configurations and assess robustness.