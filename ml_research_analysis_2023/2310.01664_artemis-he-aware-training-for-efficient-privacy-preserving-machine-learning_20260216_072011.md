---
ver: rpa2
title: 'Artemis: HE-Aware Training for Efficient Privacy-Preserving Machine Learning'
arxiv_id: '2310.01664'
source_url: https://arxiv.org/abs/2310.01664
tags:
- pruning
- rotation
- artemis
- number
- operations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Artemis is a DNN pruning method for Homomorphic Encryption (HE)-based
  inference. Artemis is motivated by the fact that the main HE inference cost is due
  to Rotation operations, which dominate run time.
---

# Artemis: HE-Aware Training for Efficient Privacy-Preserving Machine Learning

## Quick Facts
- arXiv ID: 2310.01664
- Source URL: https://arxiv.org/abs/2310.01664
- Authors: 
- Reference count: 13
- Key outcome: Artemis is a DNN pruning method for Homomorphic Encryption (HE)-based inference that reduces Rotation operations by 1.2-6x compared to prior work.

## Executive Summary
Artemis is a DNN pruning method designed to reduce the computational cost of Homomorphic Encryption (HE)-based inference, which is dominated by Rotation operations. The method exploits two HE-aware pruning strategies—positional and diagonal—that align with the algebraic structure of HE convolution to minimize rotation count. By coupling DNN training with group Lasso regularization, Artemis enforces structured sparsity patterns that enable aggressive pruning while maintaining accuracy. Evaluated on ResNet18 and ResNet50 across CIFAR-10, CIFAR-100, and Tiny ImageNet, Artemis demonstrates significant improvements in rotation reduction without substantial accuracy loss.

## Method Summary
Artemis combines HE-aware training with structured pruning to reduce Rotation operations in HE inference. During training, group Lasso regularization encourages structured sparsity by shrinking entire groups of weights (positional or diagonal) toward zero. After training, pruning iteratively removes weights based on increasing thresholds while fine-tuning the model. The method targets ResNet architectures, where diagonal pruning proves sufficient for optimal cost reduction. The approach is evaluated by measuring rotation count and accuracy trade-offs, showing 1.2-6x improvements over prior methods like Hunter.

## Key Results
- Artemis reduces Rotation operations by 1.2-6x compared to prior work.
- Diagonal pruning alone achieves Pareto-optimal solutions, making positional pruning unnecessary.
- ResNet18 and ResNet50 models on CIFAR-10, CIFAR-100, and Tiny ImageNet show improved rotation reduction without significant accuracy loss.

## Why This Works (Mechanism)

### Mechanism 1
Artemis reduces HE inference cost by exploiting the rotation operation as the primary bottleneck and designing pruning strategies that directly target the number of rotations. Artemis introduces two HE-aware pruning strategies—positional and diagonal pruning—that eliminate weight patterns corresponding to rotation operations in the SIMD-based HE convolution. Diagonal pruning removes entire diagonal groups of weights, reducing the number of rotation-and-sum operations; positional pruning removes weight positions across channels, reducing the number of individual rotations in the SISO convolution step. Core assumption: The number of rotation operations dominates HE inference runtime, and structured pruning that aligns with HE's algebraic structure can reduce this cost without excessive accuracy loss.

### Mechanism 2
Artemis achieves superior pruning performance by coupling DNN training with group Lasso regularization that enforces the structural sparsity required for HE-aware pruning. Group Lasso regularization is applied during training to encourage entire groups of weights (positional or diagonal) to shrink toward zero. This creates the weight structure needed for aggressive pruning later. Two regularization terms are defined: one for positional pruning and one for diagonal pruning, each targeting different aspects of the rotation reduction. Core assumption: Regularizing during training can create the exact sparsity patterns required for effective HE-specific pruning, and these patterns can be preserved during the fine-tuning phase.

### Mechanism 3
Diagonal pruning is sufficient for optimal HE cost reduction, making positional pruning unnecessary. Empirical results show that the Pareto-optimal solutions (best accuracy-cost trade-offs) are achieved using only diagonal pruning, not positional pruning. This is due to the structure of CNN models where the number of output channels far exceeds the filter size, making diagonal pruning dominate cost reduction. Core assumption: The cost reduction formula (Equation 6) accurately captures the relationship between pruning patterns and rotation count, and the structure of typical CNNs (e.g., ResNet) makes diagonal pruning dominant.

## Foundational Learning

- Concept: Homomorphic Encryption (HE) and its computational model, particularly SIMD operations and the role of rotation operations.
  - Why needed here: Artemis is specifically designed to reduce the number of rotation operations in HE inference, so understanding HE's cost model is essential.
  - Quick check question: In CKKS HE, what operation is required to align message slots for SIMD accumulation, and why is it expensive?

- Concept: Structured pruning and group Lasso regularization.
  - Why needed here: Artemis uses structured pruning with group Lasso to enforce sparsity patterns that align with HE rotation reduction.
  - Quick check question: How does group Lasso differ from standard L1/L2 regularization, and why is it suitable for creating structured sparsity?

- Concept: Convolutional neural network structure, particularly ResNet and its parameter layout.
  - Why needed here: Artemis targets ResNet18/50 models, and understanding their channel/filter structure is key to grasping why diagonal pruning is effective.
  - Quick check question: In a ResNet block, what are the typical dimensions of a convolution layer, and how are weights organized across input/output channels?

## Architecture Onboarding

- Component map: HE-aware training (with group Lasso regularization) -> Structured pruning (positional and diagonal) -> Evaluation pipeline (accuracy vs. rotation count trade-off)
- Critical path: (1) Initialize model; (2) Train with HE-aware group Lasso regularization; (3) Prune with fine-tuning (iteratively increase threshold); (4) Evaluate accuracy and rotation count.
- Design tradeoffs: Positional vs. diagonal pruning (diagonal is sufficient and dominant); hyperparameter tuning (λ, λp, λd); fine-tuning strategy (avoid Lasso during pruning phase).
- Failure signatures: (1) Accuracy drops significantly after pruning—likely insufficient regularization or aggressive pruning; (2) Rotation count reduction is minimal—pruning thresholds too low or model not compressible under HE structure; (3) Training instability—regularization factors too high.
- First 3 experiments:
  1. Reproduce Artemis on CIFAR-10 with ResNet18, starting with default hyperparameters, to validate baseline rotation reduction.
  2. Test Artemis with only diagonal pruning (no positional) to confirm diagonal pruning sufficiency.
  3. Compare Artemis to Hunter on Tiny ImageNet ResNet18 to observe dataset/model sensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does Artemis' performance scale to even larger models (e.g., ResNet101, EfficientNet) or is there a diminishing return?
- Basis in paper: [inferred] The paper demonstrates Artemis on ResNet18 and ResNet50, showing increasing benefits with larger models. However, the scalability to significantly larger or more complex architectures is not explored.
- Why unresolved: The paper's experimental scope is limited to ResNet18 and ResNet50. There is no evidence on how Artemis would perform on much larger or architecturally distinct models.
- What evidence would resolve it: Testing Artemis on models like ResNet101, EfficientNet-B0/B4, or transformer-based architectures, and comparing the rotation reduction and accuracy trade-offs to smaller models.

### Open Question 2
- Question: Is the effectiveness of diagonal pruning universal across all types of convolutional layers (e.g., depthwise separable, dilated, or strided convolutions)?
- Basis in paper: [explicit] The paper focuses on standard 2D convolutions and shows diagonal pruning is highly effective. However, it does not investigate specialized convolution types.
- Why unresolved: The analysis is limited to vanilla convolutions, and the unique weight structures in depthwise separable or dilated convolutions might alter the pruning dynamics.
- What evidence would resolve it: Applying Artemis to models with depthwise separable convolutions (e.g., MobileNetV2) or dilated convolutions and measuring if diagonal pruning remains dominant.

### Open Question 3
- Question: How does Artemis perform when combined with other HE optimizations like ciphertext packing or bootstrapping?
- Basis in paper: [inferred] The paper focuses solely on rotation operation reduction via pruning. It does not explore synergies with other HE-specific optimizations that could further reduce latency or memory usage.
- Why unresolved: The evaluation is isolated to pruning, without considering how Artemis interacts with broader HE acceleration techniques.
- What evidence would resolve it: Benchmarking Artemis alongside optimized ciphertext packing schemes or bootstrapping strategies to measure cumulative performance gains.

### Open Question 4
- Question: Are there architectural modifications to CNNs that could inherently reduce rotation operations without sacrificing accuracy, beyond pruning?
- Basis in paper: [explicit] The paper demonstrates that pruning (especially diagonal) can reduce rotations, but it does not explore redesigning the CNN architecture itself to minimize rotation dependencies.
- Why unresolved: The study is limited to pruning existing architectures rather than innovating new architectural designs optimized for HE.
- What evidence would resolve it: Designing and testing novel CNN architectures (e.g., with different kernel shapes, channel arrangements, or grouped convolutions) that intrinsically reduce rotation operations in HE inference.

## Limitations

- The effectiveness of Artemis relies on the assumption that rotation operations dominate HE inference runtime, which may not hold across all HE schemes or implementations.
- The method is evaluated primarily on ResNet architectures, limiting generalizability to other network designs.
- The group Lasso regularization approach may require careful hyperparameter tuning and may not transfer well to architectures with different weight layouts.

## Confidence

**High Confidence:** The mechanism by which Artemis reduces rotation operations through structured pruning is well-supported by the paper's theoretical framework and empirical results. The claim that diagonal pruning is sufficient for optimal HE cost reduction is backed by quantitative comparisons showing positional pruning's ineffectiveness.

**Medium Confidence:** The assertion that Artemis achieves 1.2-6x improvement in rotation reduction compared to prior work is based on specific experimental conditions and may vary with different HE implementations or model architectures. The training methodology using group Lasso regularization is sound but requires careful hyperparameter tuning that may be sensitive to implementation details.

**Low Confidence:** The generalizability of Artemis to non-ResNet architectures or different HE schemes remains untested. The paper does not explore scenarios where the rotation-cost dominance assumption might break down, such as in newer HE implementations or different computational backends.

## Next Checks

1. **Cross-HE Scheme Validation:** Test Artemis on multiple HE schemes (BFV, BGV) and different CKKS implementations to verify that rotation operation dominance holds and that pruning effectiveness transfers across implementations.

2. **Architecture Generalization Test:** Apply Artemis to non-ResNet architectures (e.g., EfficientNet, MobileNet) to assess whether diagonal pruning remains sufficient and whether the group Lasso regularization approach generalizes to different network structures.

3. **Cost Model Verification:** Independently measure actual runtime performance (not just rotation count) for Artemis-pruned models across different problem sizes and datasets to confirm that rotation reduction translates to wall-clock time improvements in real HE implementations.