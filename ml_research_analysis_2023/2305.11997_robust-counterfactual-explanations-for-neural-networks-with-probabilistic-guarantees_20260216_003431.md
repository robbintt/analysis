---
ver: rpa2
title: Robust Counterfactual Explanations for Neural Networks With Probabilistic Guarantees
arxiv_id: '2305.11997'
source_url: https://arxiv.org/abs/2305.11997
tags:
- counterfactuals
- change
- counterfactual
- robustness
- measure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of finding robust counterfactual
  explanations for neural networks that remain valid even when the model is slightly
  changed. The core idea is to introduce a novel mathematical abstraction termed "naturally-occurring"
  model change, which allows for arbitrary changes in the parameter space such that
  the change in predictions on points lying on the data manifold is limited.
---

# Robust Counterfactual Explanations for Neural Networks With Probabilistic Guarantees

## Quick Facts
- arXiv ID: 2305.11997
- Source URL: https://arxiv.org/abs/2305.11997
- Authors: 
- Reference count: 37
- One-line primary result: Novel method for generating counterfactual explanations that remain valid under naturally-occurring model changes with probabilistic guarantees

## Executive Summary
This work introduces a novel approach to generating counterfactual explanations for neural networks that remain robust to potential model changes. The key innovation is the Stability measure, which quantifies the robustness of counterfactuals by combining local model confidence with variability estimates. The authors provide theoretical guarantees showing that counterfactuals with sufficiently high Stability values will remain valid after naturally-occurring model changes with high probability. Practical algorithms (T-Rex:I and T-Rex:NN) are developed to generate such robust counterfactuals through iterative optimization of the Stability measure.

## Method Summary
The method involves training a neural network on a dataset, generating counterfactual explanations using base methods, and then improving their robustness through iterative optimization. The core innovation is the Stability measure, which captures the average model output in a local neighborhood while penalizing high local Lipschitz variability. This measure is then used in algorithms (T-Rex:I and T-Rex:NN) that iteratively update counterfactuals via gradient ascent until they achieve sufficient robustness. The approach is evaluated on three benchmark datasets (German Credit, HELOC, CTG) with binary classification tasks.

## Key Results
- Counterfactuals with high Stability values remain valid after naturally-occurring model changes with high probability
- T-Rex algorithms successfully generate counterfactuals that are close to the original instance, realistic, and robust to model changes
- The approach outperforms baseline methods in terms of validity and realism while maintaining competitive cost

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counterfactuals with high Stability value remain valid after naturally-occurring model changes with high probability
- Mechanism: The Stability measure captures the average model output in a local neighborhood while penalizing high local Lipschitz variability. Under naturally-occurring model changes, model outputs on the data manifold vary little, so counterfactuals in low-variability regions remain valid.
- Core assumption: Model changes affect predictions on the data manifold only slightly (Proposition 1) and the Lipschitz constant is bounded.
- Evidence anchors:
  - [abstract]: "Our main contribution is to show that counterfactuals with sufficiently high value of Stability as defined by our measure will remain valid after potential naturally-occurring model changes with high probability"
  - [section]: Theorem 1 provides the probabilistic guarantee
  - [corpus]: Weak - no direct corpus evidence found for this specific mechanism
- Break condition: If the model change violates the naturally-occurring assumption (e.g., targeted adversarial changes), the guarantee fails (Theorem 2).

### Mechanism 2
- Claim: The relaxed Stability measure (ˆRk,σ2) effectively approximates the theoretical Stability measure by combining mean output and local variability.
- Mechanism: By sampling points around the counterfactual from N(x,σ2Id), the first term captures mean model confidence while the second term penalizes high local variability. This aligns with the intuition that counterfactuals in high-confidence, low-variability regions are more robust.
- Core assumption: Local variability around the counterfactual correlates with prediction reliability.
- Evidence anchors:
  - [abstract]: "Since our quantification depends on the local Lipschitz constant around a data point which is not always available, we also examine practical relaxations of our proposed measure"
  - [section]: Proposition 3 defines the relaxed measure and explains its properties
  - [corpus]: Weak - no direct corpus evidence found for this specific mechanism
- Break condition: If the local variability doesn't correlate with prediction reliability (e.g., in highly non-smooth regions), the measure becomes less effective.

### Mechanism 3
- Claim: The T-Rex algorithms iteratively improve counterfactual robustness by gradient ascent on the Stability measure.
- Mechanism: T-Rex:I starts with any base counterfactual and iteratively updates it by ascending the gradient of the Stability measure until it passes the robustness test. This finds counterfactuals that are both close to the original and robust to model changes.
- Core assumption: The gradient of the Stability measure points toward more robust counterfactuals.
- Evidence anchors:
  - [abstract]: "Practical relaxations of the Stability measure are also examined and incorporated into algorithms for generating robust counterfactuals"
  - [section]: Algorithm 1 describes T-Rex:I's iterative gradient ascent approach
  - [corpus]: Weak - no direct corpus evidence found for this specific mechanism
- Break condition: If the Stability measure landscape is too flat or has many local optima, gradient ascent may converge to suboptimal counterfactuals.

## Foundational Learning

- Concept: Lipschitz continuity
  - Why needed here: The theoretical guarantee relies on Lipschitz properties to bound how much model outputs can change between nearby points
  - Quick check question: What does it mean for a function f to be γ-Lipschitz, and why is this property important for bounding model output changes?

- Concept: Concentration inequalities for Gaussian random variables
  - Why needed here: The probabilistic guarantee uses concentration bounds to show that the average of Lipschitz functions of Gaussian samples concentrates around its expectation
  - Quick check question: How does the Gaussian concentration inequality (Lemma 3) allow us to bound the probability that Z deviates from its expected value?

- Concept: Local outlier factor (LOF)
  - Why needed here: LOF measures how anomalous a point is compared to its neighbors, helping evaluate whether generated counterfactuals are realistic and data-supported
  - Quick check question: How is the k-LOF of a point calculated, and what does a high LOF value indicate about a counterfactual's realism?

## Architecture Onboarding

- Component map:
  - Base model: Trained neural network that makes predictions
  - Stability measure: Function that quantifies counterfactual robustness using Gaussian sampling
  - T-Rex algorithms: Methods that generate counterfactuals by optimizing the Stability measure
  - Robustness test: Threshold-based evaluation of whether a counterfactual is robust enough

- Critical path:
  1. Train base model on dataset
  2. Generate initial counterfactual using any base method
  3. Compute Stability measure for the counterfactual
  4. If robust enough, output; otherwise, update counterfactual via gradient ascent
  5. Return robust counterfactual

- Design tradeoffs:
  - Higher k in Stability measure → better robustness estimation but higher computational cost
  - Higher τ threshold → more robust counterfactuals but potentially higher cost and lower realism
  - T-Rex:I vs T-Rex:NN → balance between cost minimization and data-support realism

- Failure signatures:
  - Counterfactual validity drops significantly after model retraining
  - Stability measure values don't correlate with actual robustness
  - T-Rex algorithms get stuck in local optima with poor counterfactuals

- First 3 experiments:
  1. Test Stability measure correlation: Generate counterfactuals for a trained model, compute their Stability values, then retrain the model and measure actual validity. Check if higher Stability correlates with higher validity.
  2. Ablation study on k: Run T-Rex:I with different k values (e.g., 100, 500, 1000, 2000) on the same dataset and compare validity, cost, and runtime.
  3. Compare T-Rex:I vs base method: Generate counterfactuals using only the base method (e.g., min Cost) and T-Rex:I on the same dataset, then measure validity after model retraining to quantify robustness improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Stability measure be further relaxed to incorporate domain-specific knowledge and constraints?
- Basis in paper: [explicit] The paper mentions that the proposed Stability measure relies on the local Lipschitz constant, which is often unknown. The authors also examine a practical relaxation of the measure but acknowledge that it may not always capture the variability in a region.
- Why unresolved: While the authors propose a practical relaxation of the Stability measure, they do not explore incorporating domain-specific knowledge or constraints into the relaxation. This could potentially improve the robustness of counterfactual explanations in specific application domains.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of incorporating domain-specific knowledge and constraints into the relaxation of the Stability measure for generating robust counterfactual explanations in specific application domains.

### Open Question 2
- Question: Can the Stability measure be extended to handle non-differentiable models, such as decision trees or random forests?
- Basis in paper: [inferred] The paper focuses on differentiable models, such as neural networks, and introduces the Stability measure to quantify the robustness of counterfactuals for such models. However, many real-world applications use non-differentiable models like decision trees or random forests.
- Why unresolved: The authors do not discuss the applicability of the Stability measure to non-differentiable models. Extending the measure to handle such models could broaden its applicability and impact.
- What evidence would resolve it: A theoretical framework and experimental results demonstrating the effectiveness of the Stability measure for generating robust counterfactual explanations for non-differentiable models.

### Open Question 3
- Question: How does the choice of hyperparameters (k, σ, τ) affect the performance and robustness of the generated counterfactual explanations?
- Basis in paper: [explicit] The authors mention that the choice of hyperparameters, such as k, σ, and τ, can affect the robustness and performance of the generated counterfactual explanations. They provide some guidance on selecting these hyperparameters based on their experiments.
- Why unresolved: While the authors provide some guidance on selecting hyperparameters, they do not conduct an exhaustive study on the impact of different hyperparameter choices on the performance and robustness of the generated counterfactual explanations.
- What evidence would resolve it: A comprehensive experimental study evaluating the impact of different hyperparameter choices on the performance and robustness of the generated counterfactual explanations, along with guidelines for selecting optimal hyperparameters for specific applications.

## Limitations
- The Lipschitz constant estimation requires additional computation and may be conservative
- The Gaussian sampling approach assumes local smoothness that may not hold for highly non-convex regions
- Computational cost scales with k samples, potentially limiting real-time applications

## Confidence
- Theoretical guarantees: High - formal proofs provided with clear assumptions
- Practical algorithm performance: Medium - depends on specific dataset characteristics and model behavior
- Stability measure correlation with robustness: Medium - needs more extensive validation

## Next Checks
1. Test Stability measure correlation across different neural network architectures (CNN, LSTM) to verify generalizability
2. Evaluate performance on multi-class classification tasks to extend beyond binary scenarios
3. Compare against adversarial training approaches to assess relative robustness gains