---
ver: rpa2
title: 'FDAPT: Federated Domain-adaptive Pre-training for Language Models'
arxiv_id: '2307.06933'
source_url: https://arxiv.org/abs/2307.06933
tags:
- fdapt
- pre-training
- arxiv
- performance
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FDAPT (Federated Domain-adaptive Pre-training)
  to address the challenge of adapting foundation models to specific domains when
  sensitive data cannot be directly shared. The authors conduct the first comprehensive
  empirical study of FDAPT and demonstrate that it maintains competitive downstream
  task performance compared to centralized baselines under both IID and non-IID settings.
---

# FDAPT: Federated Domain-adaptive Pre-training for Language Models

## Quick Facts
- arXiv ID: 2307.06933
- Source URL: https://arxiv.org/abs/2307.06933
- Reference count: 38
- This paper introduces FDAPT (Federated Domain-adaptive Pre-training) to address the challenge of adapting foundation models to specific domains when sensitive data cannot be directly shared.

## Executive Summary
This paper introduces FDAPT (Federated Domain-adaptive Pre-training) to address the challenge of adapting foundation models to specific domains when sensitive data cannot be directly shared. The authors conduct the first comprehensive empirical study of FDAPT and demonstrate that it maintains competitive downstream task performance compared to centralized baselines under both IID and non-IID settings. They propose FFDAPT, which improves computational efficiency by 12.1% on average while maintaining similar downstream task performance to standard FDAPT.

## Method Summary
FDAPT extends domain-adaptive pre-training to federated settings by continuing pre-training of foundation models (DistilBERT) on distributed, domain-specific data using Federated Averaging (FedAvg). The approach involves clients holding domain-specific data that continue pre-training locally, then aggregating model updates at a central server. FFDAPT further improves efficiency by freezing layers during training, reducing computational burden while preserving domain-relevant knowledge. The framework is evaluated on biomedical domain data using the PubMed corpus for pre-training and 9 downstream tasks including named entity recognition, relation extraction, and question answering.

## Key Results
- FDAPT maintains competitive downstream performance compared to centralized baselines under both IID and non-IID settings
- FFDAPT improves computational efficiency by 12.1% on average while maintaining similar downstream task performance to standard FDAPT
- Performance drops of federated models compared to centralized baseline are less than 1% on almost all datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FDAPT preserves competitive downstream performance compared to centralized pre-training under both IID and non-IID data distributions.
- Mechanism: By leveraging distributed domain-specific data without direct sharing, FDAPT enables models to learn domain-relevant representations while maintaining data privacy. The FedAvg algorithm aggregates client updates to create a global model that benefits from diverse data sources.
- Core assumption: The federated aggregation process effectively combines heterogeneous client data distributions to produce a model that performs comparably to one trained on centrally aggregated data.
- Evidence anchors:
  - [abstract] "We demonstrate that FDAPT can maintain competitive downstream task performance to the centralized baseline in both IID and non-IID situations."
  - [section 4.1] "The performance drops of federated models compared to the centralized baseline are acceptable... The performance of federated models decreases by less than 1% on almost all datasets compared to the centralized baseline."
  - [corpus] Weak evidence - neighboring papers discuss federated foundation models but don't provide direct performance comparisons for domain-adaptive pre-training scenarios.
- Break condition: If the client data distributions are too heterogeneous (extreme non-IID), the aggregation process may produce a model that underperforms compared to centralized training.

### Mechanism 2
- Claim: FFDAPT improves computational efficiency by freezing layers during training while maintaining similar downstream performance.
- Mechanism: By freezing a portion of the model layers (determined by dataset size), FFDAPT reduces the computational burden during federated training. This allows clients to participate with lower computational resources while still contributing to model improvement.
- Core assumption: Freezing layers preserves domain-relevant knowledge from pre-training while allowing unfrozen layers to adapt to domain-specific features.
- Evidence anchors:
  - [abstract] "FFDAPT improves the computational efficiency by 12.1% on average and exhibits similar downstream task performance to standard FDAPT"
  - [section 3.3] "Freezing specific layers or parameters of a model during fine-tuning or transfer learning aims to preserve the pretrained knowledge encoded within those layers, while allowing other parts of the model to adapt to new tasks or domains."
  - [corpus] No direct evidence - neighboring papers discuss federated foundation models but don't specifically address layer freezing for computational efficiency.
- Break condition: If too many layers are frozen (exceeding the maximum threshold), the model may not adapt sufficiently to the target domain, resulting in performance degradation.

### Mechanism 3
- Claim: The non-IID definition in FDAPT context (quantity skew, sentence length distribution skew, vocabulary distribution skew) is valid and affects model performance.
- Mechanism: By explicitly defining and controlling different types of non-IIDness in federated pre-training, researchers can systematically evaluate how data distribution heterogeneity impacts model performance and develop strategies to mitigate negative effects.
- Core assumption: The three defined types of non-IIDness (quantity skew, sentence length distribution skew, vocabulary distribution skew) are the primary sources of heterogeneity in federated pre-training scenarios.
- Evidence anchors:
  - [section 3.2] "We define three types of non-IIDness in the context of federated pre-training, including quantity skew, sentence length distribution skew and vocabulary distribution skew."
  - [section 4.1] "Federated models trained in specific settings can outperform the centralized baseline... Notably, FDAPT with 2 clients under the IID setting increases the F1 scores on LINNAEUS by 0.8%."
  - [corpus] No direct evidence - neighboring papers discuss federated learning but don't provide specific definitions of non-IIDness in the context of pre-training without labels.
- Break condition: If other types of non-IIDness exist that are not captured by the three defined types, the evaluation may miss important factors affecting performance.

## Foundational Learning

- Concept: Federated Learning (FL) fundamentals
  - Why needed here: FDAPT is built on FL principles, so understanding how federated averaging works and how clients communicate with the server is essential for implementing and troubleshooting the system.
  - Quick check question: How does the FedAvg algorithm aggregate client updates to produce the global model?

- Concept: Domain-Adaptive Pre-training (DAPT)
  - Why needed here: FDAPT extends DAPT to federated settings, so understanding how continuing pre-training on domain-specific data improves model performance is crucial for evaluating results.
  - Quick check question: What is the difference between task-specific fine-tuning and domain-adaptive pre-training?

- Concept: Natural Language Processing (NLP) model architectures
  - Why needed here: The paper uses DistilBERT as the base model, so understanding transformer architectures and how pre-training/fine-tuning work is necessary for modifying or extending the approach.
  - Quick check question: How does the DistilBERT architecture differ from the original BERT model, and what are the implications for pre-training efficiency?

## Architecture Onboarding

- Component map:
  - Clients (each holds domain-specific data and a copy of the model) -> Server (aggregates model updates from clients) -> Model (DistilBERT initialized with pre-trained weights) -> Datasets (PubMed corpus for pre-training, 9 biomedical downstream tasks for evaluation) -> Framework (Flower for FL implementation)

- Critical path:
  1. Initialize clients with pre-trained DistilBERT weights
  2. Each client continues pre-training on local domain-specific data
  3. Clients send model updates to server
  4. Server aggregates updates using FedAvg
  5. Server distributes updated global model to clients
  6. Repeat for specified number of rounds
  7. Evaluate final model on downstream tasks

- Design tradeoffs:
  - Model size vs. computational efficiency (DistilBERT chosen over larger models)
  - Communication frequency vs. model quality (fewer rounds reduce communication but may impact convergence)
  - Layer freezing percentage vs. adaptation capability (FFDAPT balances efficiency with performance)
  - Number of clients vs. data heterogeneity (more clients provide diverse data but increase complexity)

- Failure signatures:
  - Performance consistently worse than centralized baseline (>1% drop) indicates aggregation issues
  - Increasing performance gap across training rounds suggests client drift or insufficient aggregation
  - Clients failing to converge may indicate incompatible data distributions or learning rates
  - Computational bottlenecks on clients may indicate inappropriate layer freezing settings

- First 3 experiments:
  1. Run FDAPT with 2 clients under IID setting on a single downstream task to verify basic functionality
  2. Test FFDAPT with different freezing percentages (10%, 25%, 50%) to find optimal efficiency-performance tradeoff
  3. Compare FDAPT performance under different non-IID settings (quantity skew vs. vocabulary skew) to understand which type has greatest impact

## Open Questions the Paper Calls Out

- Question: How does FDAPT performance vary across different foundation model architectures beyond DistilBERT?
  - Basis in paper: [explicit] The paper states "Due to limited data and computational resources, our experiments focus on some common settings to obtain meaningful results while minimizing experimental costs. Future research can involve more large-scale simulations and different model structures to represent potential real-world scenarios."
  - Why unresolved: The study only used DistilBERT due to computational constraints, limiting generalizability to other foundation models.
  - What evidence would resolve it: Systematic comparison of FDAPT across multiple foundation models (BERT, GPT variants, etc.) under identical federated settings and domains.

- Question: What is the optimal strategy for handling non-IID data distributions in FDAPT?
  - Basis in paper: [explicit] The authors "formulate 3 types of non-IIDness in the context of FDAPT" and note that "non-IID issue, which commonly exists in practical FL applications and can cause performance drops."
  - Why unresolved: While the paper identifies different types of non-IIDness, it doesn't provide specific strategies for mitigating their negative effects.
  - What evidence would resolve it: Development and empirical validation of adaptive federated algorithms that dynamically adjust training based on the type and severity of non-IIDness.

- Question: How does FDAPT scale to real-world cross-device federated learning scenarios?
  - Basis in paper: [inferred] The paper uses cross-silo settings with relatively few clients, while noting "limited data and computational resources" and the need for "more real-world simulations."
  - Why unresolved: The experiments were conducted in cross-silo settings with a limited number of clients, which doesn't reflect the massive scale of cross-device FL.
  - What evidence would resolve it: Large-scale implementation of FDAPT with thousands of clients, evaluating communication efficiency, convergence speed, and model quality degradation.

## Limitations

- The evaluation is restricted to biomedical domain data, limiting generalizability to other domains
- The performance gap between federated and centralized approaches, while small (<1% on most datasets), raises questions about scalability to more heterogeneous data distributions
- The FFDAPT computational efficiency improvement of 12.1% is reported without detailed analysis of how this scales with different model sizes or client counts

## Confidence

- High confidence in the basic FDAPT framework and its ability to maintain competitive performance compared to centralized baselines under controlled conditions
- Medium confidence in the non-IID definitions and their impact on performance, as the paper introduces these concepts without extensive empirical validation across diverse scenarios
- Medium confidence in the FFDAPT efficiency claims, as the layer freezing mechanism is described but specific implementation details and hyperparameter sensitivity are not fully disclosed

## Next Checks

1. Evaluate FDAPT performance across multiple domains (not just biomedical) to assess generalizability of the approach
2. Conduct ablation studies on the layer freezing mechanism in FFDAPT to determine optimal freezing percentages for different dataset sizes
3. Test FDAPT with extreme non-IID distributions (beyond the three defined types) to identify breaking points and failure modes