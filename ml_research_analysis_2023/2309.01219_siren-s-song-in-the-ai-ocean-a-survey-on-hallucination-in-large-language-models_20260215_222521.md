---
ver: rpa2
title: 'Siren''s Song in the AI Ocean: A Survey on Hallucination in Large Language
  Models'
arxiv_id: '2309.01219'
source_url: https://arxiv.org/abs/2309.01219
tags:
- arxiv
- llms
- language
- preprint
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey comprehensively examines the phenomenon of hallucination
  in large language models (LLMs), where LLMs generate content that deviates from
  user input, contradicts previously generated context, or misaligns with established
  world knowledge. The paper categorizes LLM hallucinations into three types: input-conflicting,
  context-conflicting, and fact-conflicting.'
---

# Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models

## Quick Facts
- arXiv ID: 2309.01219
- Source URL: https://arxiv.org/abs/2309.01219
- Reference count: 40
- One-line primary result: Comprehensive survey of LLM hallucination types, detection, explanation, and mitigation strategies with focus on fact-conflicting hallucinations.

## Executive Summary
This survey provides a comprehensive examination of hallucination in large language models (LLMs), categorizing hallucinations into input-conflicting, context-conflicting, and fact-conflicting types. The paper reviews recent efforts on detection, explanation, and mitigation of hallucination, with particular emphasis on the unique challenges posed by LLMs compared to traditional natural language generation tasks. The survey also discusses potential sources of LLM hallucinations and provides an in-depth review of recent work toward addressing the problem, concluding with forward-looking perspectives on reliable evaluation, multi-lingual and multi-modal hallucination, model editing, and attack/defense mechanisms.

## Method Summary
The survey systematically reviews hallucination research in LLMs by categorizing hallucination types based on their source of contradiction, examining evaluation benchmarks and metrics, identifying sources of hallucination across the LLM life cycle, and organizing mitigation strategies according to when they are applied (pre-training, supervised fine-tuning, reinforcement learning from human feedback, and inference). The paper synthesizes findings from 40+ references to create a comprehensive framework for understanding and addressing LLM hallucinations.

## Key Results
- LLMs generate three types of hallucinations: input-conflicting, context-conflicting, and fact-conflicting
- Hallucination detection and mitigation methods can be organized by LLM life cycle stages
- Unique challenges in LLM hallucination include massive training data, versatility of LLMs, and imperceptibility of errors
- Fact-conflicting hallucinations are particularly important due to their potential to mislead users
- Future research directions include reliable evaluation metrics, multi-lingual and multi-modal hallucination handling, and attack/defense mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The paper provides a comprehensive taxonomy of LLM hallucinations by categorizing them into input-conflicting, context-conflicting, and fact-conflicting types.
- Mechanism: The authors establish clear boundaries between hallucination types by analyzing the source of contradiction (user input, generated context, or external knowledge) and provide concrete examples for each category.
- Core assumption: Hallucinations in LLMs can be meaningfully distinguished based on where the contradiction originates.
- Evidence anchors:
  - [abstract] The paper categorizes LLM hallucinations into three types: input-conflicting, context-conflicting, and fact-conflicting.
  - [section 2.2] "We categorize hallucination within the context of LLMs as follows: • Input-conflicting hallucination, where LLMs generate content that deviates from the source input provided by users; • Context-conflicting hallucination, where LLMs generate content that conflicts with previously generated information by itself; • Fact-conflicting hallucination, where LLMs generate content that is not faithful to established world knowledge."
- Break condition: When the source of contradiction becomes ambiguous or when multiple types overlap in complex generation scenarios.

### Mechanism 2
- Claim: The paper demonstrates that LLM hallucinations have unique challenges compared to traditional NLG tasks, primarily due to massive training data, versatility of LLMs, and imperceptibility of errors.
- Mechanism: By identifying these unique challenges, the survey creates a framework for understanding why hallucination mitigation requires different approaches than conventional methods.
- Core assumption: The scale and nature of LLMs create fundamentally different hallucination problems than task-specific models.
- Evidence anchors:
  - [section 2.3] "We identify four primary sources that span different stages of the LLM life cycle... LLMs lack relevant knowledge or internalize false knowledge... LLMs sometimes overestimate their capacities... Problematic alignment process could mislead LLMs into hallucination... The generation strategy employed by LLMs has potential risks."
- Break condition: When applied to smaller, task-specific models where these challenges don't manifest.

### Mechanism 3
- Claim: The paper provides actionable mitigation strategies organized by LLM life cycle stages (pre-training → SFT → RLHF → inference).
- Mechanism: By mapping mitigation techniques to specific stages, the survey creates a practical framework for addressing hallucinations at the point where they originate.
- Core assumption: Hallucinations can be most effectively addressed at the stage where they are introduced.
- Evidence anchors:
  - [section 5] "To make the structure clear, we categorize existing mitigation works based on the timing of their application within the LLM life cycle."
  - [section 5.1-5.4] Detailed discussion of mitigation approaches for each stage with specific examples and techniques.
- Break condition: When hallucinations persist across multiple stages or when the source is difficult to pinpoint to a specific stage.

## Foundational Learning

- Concept: Large Language Model Architecture
  - Why needed here: Understanding LLM architecture is essential to grasp why hallucinations occur and how different mitigation strategies work at various stages.
  - Quick check question: What is the fundamental difference between autoregressive LLMs and masked language models in terms of hallucination potential?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF is a critical component in modern LLM development and understanding its role in hallucination mitigation is essential.
  - Quick check question: How does RLHF potentially introduce or mitigate hallucinations compared to supervised fine-tuning?

- Concept: Knowledge Boundaries in LLMs
  - Why needed here: Understanding what LLMs know and don't know is crucial for comprehending hallucination sources and mitigation strategies.
  - Quick check question: Why might an LLM confidently generate incorrect information even when it has some relevant knowledge?

## Architecture Onboarding

- Component map: Definition of hallucination types → Evaluation benchmarks and metrics → Sources of hallucination → Mitigation strategies by life cycle stage → Future research directions
- Critical path: Understanding hallucination types → Evaluating hallucination → Identifying sources → Applying mitigation strategies → Considering future directions
- Design tradeoffs: The survey balances comprehensiveness with focus, emphasizing fact-conflicting hallucinations while acknowledging the importance of other types
- Failure signatures: Incomplete coverage of multi-lingual or multi-modal hallucinations; potential gaps in evaluation metrics for open-ended generation
- First 3 experiments:
  1. Replicate the taxonomy classification on a small set of LLM outputs to verify the three-type categorization
  2. Test the effectiveness of context-aware decoding on a simple retrieval-augmented generation task
  3. Evaluate the consistency-based uncertainty estimation method on a small set of question-answering pairs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop more reliable and robust automatic evaluation metrics for LLM hallucination that align better with human annotations and generalize across different domains and LLM generations?
- Basis in paper: [explicit] The paper discusses the challenges of evaluating LLM hallucinations and highlights the need for reliable automatic evaluation metrics. It mentions that existing automatic metrics do not perfectly align with human annotations and their reliability varies across different domains and LLM generations.
- Why unresolved: Current automatic evaluation metrics have limitations in accurately reflecting the performance of LLMs and aligning with human judgments. The relationship between discrimination performance and generation performance is also unclear.
- What evidence would resolve it: Research that develops new automatic evaluation metrics or improves existing ones to achieve higher alignment with human annotations and better generalization across different domains and LLM generations.

### Open Question 2
- Question: How can we effectively mitigate hallucinations in LLMs when there is a conflict between retrieved knowledge and the parametric knowledge stored in the model?
- Basis in paper: [explicit] The paper discusses the challenge of knowledge conflict in LLMs, where retrieved knowledge may contradict the model's parametric knowledge. It mentions that LLMs may fail to sufficiently exploit retrieved knowledge in such situations.
- Why unresolved: Existing methods for utilizing external knowledge in LLMs do not adequately address the issue of knowledge conflict. LLMs tend to rely on their own parametric knowledge, leading to potential hallucinations.
- What evidence would resolve it: Research that develops techniques to effectively resolve knowledge conflicts in LLMs, such as methods to prioritize or reconcile conflicting knowledge sources, or approaches to enhance the model's ability to incorporate retrieved knowledge.

### Open Question 3
- Question: How can we develop more effective decoding strategies that balance diversity and factuality in LLM generation, particularly in the context of black-box LLMs with limited API access?
- Basis in paper: [explicit] The paper discusses the role of decoding strategies in LLM generation and mentions that some strategies, like nucleus sampling, may introduce randomness that leads to hallucinations. It also highlights the challenge of accessing token-level output probabilities for black-box LLMs.
- Why unresolved: Existing decoding strategies have limitations in achieving a good balance between diversity and factuality in LLM generation. The lack of access to token-level output probabilities for black-box LLMs further complicates the development of effective decoding strategies.
- What evidence would resolve it: Research that proposes new decoding strategies or modifies existing ones to improve the trade-off between diversity and factuality in LLM generation, while considering the constraints of black-box LLMs with limited API access.

## Limitations
- Primarily focuses on English-language research and benchmarks, potentially limiting generalizability to other languages
- Taxonomy may not capture all edge cases where multiple contradiction sources overlap
- Many proposed evaluation metrics and mitigation strategies lack standardized implementations or extensive empirical validation across diverse LLM architectures

## Confidence
- High confidence: The categorization of hallucination types (input-conflicting, context-conflicting, fact-conflicting) is well-supported by clear definitions and examples throughout the paper
- Medium confidence: The identification of four primary sources of hallucination (knowledge gaps, overconfidence, alignment issues, generation strategies) is reasonable but may not be exhaustive
- Medium confidence: The life cycle-based organization of mitigation strategies provides a useful framework, though empirical validation of its practical effectiveness is limited

## Next Checks
1. Replicate the hallucination taxonomy classification on a diverse set of 100+ LLM outputs across different domains to test the robustness of the three-type categorization
2. Implement and compare three different uncertainty estimation methods (consistency-based, confidence-based, and entropy-based) on the same dataset to evaluate their relative effectiveness in hallucination detection
3. Conduct a systematic literature review update by searching for papers published in the last 6 months to identify any significant developments or emerging trends not captured in this survey