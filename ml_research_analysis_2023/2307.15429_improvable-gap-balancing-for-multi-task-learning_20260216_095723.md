---
ver: rpa2
title: Improvable Gap Balancing for Multi-Task Learning
arxiv_id: '2307.15429'
source_url: https://arxiv.org/abs/2307.15429
tags:
- balancing
- loss
- learning
- task
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of balancing training across
  multiple tasks in multi-task learning, where tasks often have varying improvable
  gaps - the distance between current training progress and desired final training
  progress. The authors propose two novel improvable gap balancing (IGB) algorithms:
  IGBv1, which uses a simple heuristic to balance improvable gaps, and IGBv2, which
  for the first time applies deep reinforcement learning (specifically Soft Actor-Critic)
  to MTL.'
---

# Improvable Gap Balancing for Multi-Task Learning

## Quick Facts
- arXiv ID: 2307.15429
- Source URL: https://arxiv.org/abs/2307.15429
- Reference count: 17
- Key outcome: Novel IGB algorithms dynamically balance improvable gaps in MTL, achieving state-of-the-art performance when combined with Nash method

## Executive Summary
This paper addresses the challenge of balancing training across multiple tasks in multi-task learning where tasks have varying improvable gaps - the distance between current training progress and desired final training progress. The authors propose two novel improvable gap balancing (IGB) algorithms: IGBv1 uses a simple heuristic to balance improvable gaps, while IGBv2 for the first time applies deep reinforcement learning (specifically Soft Actor-Critic) to MTL. Both algorithms dynamically assign task weights based on normalized losses to balance improvable gaps rather than directly balancing losses. Extensive experiments on NYUv2 and QM9 datasets show IGB algorithms achieve the best results among loss balancing methods, with IGBv1 + Nash achieving state-of-the-art performance.

## Method Summary
The paper proposes two improvable gap balancing (IGB) algorithms for multi-task learning. IGBv1 uses a simple heuristic that assigns task weights inversely proportional to the ratio of current batch losses to baseline losses calculated in the second epoch, prioritizing tasks with larger improvable gaps. IGBv2 treats task weight assignment as a sequential decision problem solved by a Soft Actor-Critic (SAC) model, using loss declines as rewards. Both algorithms use normalized losses for scale-invariant balancing. The authors also demonstrate that combining IGB with gradient balancing yields complementary improvements by addressing both loss scale imbalance and gradient conflicts.

## Key Results
- IGBv1 and IGBv2 achieve the best results among loss balancing methods on NYUv2 and QM9 datasets
- IGBv1 + Nash combination achieves state-of-the-art performance
- Combining IGB with gradient balancing yields further improvements, highlighting complementarity
- IGBv1 improves learning efficiency with minimal extra training time, while IGBv2 achieves better performance at slightly higher computational cost

## Why This Works (Mechanism)

### Mechanism 1
- Claim: IGBv1 improves MTL performance by dynamically balancing improvable gaps through a simple heuristic based on normalized losses.
- Mechanism: IGBv1 assigns task weights inversely proportional to the ratio of current batch losses to baseline losses calculated in the second epoch. This prioritizes tasks with larger improvable gaps.
- Core assumption: Task losses converge to different nonzero values, and balancing these convergence gaps improves overall performance.
- Evidence anchors:
  - [abstract] "instead of directly balancing the losses in MTL, both algorithms choose to dynamically assign task weights for improvable gap balancing"
  - [section] "we propose the first algorithm IGBv1 to balance improvable gaps through a simple heuristic by uniformly defining the ideal loss as 0 for each task"
- Break condition: If tasks have identical convergence patterns or if baseline calculation in epoch 2 is not representative of task loss scales.

### Mechanism 2
- Claim: IGBv2 achieves better performance than IGBv1 by using deep reinforcement learning to adaptively balance improvable gaps.
- Mechanism: IGBv2 treats task weight assignment as a sequential decision problem where the SAC model learns to maximize cumulative loss decline rewards across tasks.
- Core assumption: Task weight assignment can be modeled as a Markov decision process where current losses represent sufficient state information.
- Evidence anchors:
  - [abstract] "we propose another algorithm IGBv2 to jointly minimize all improvable gaps through deep reinforcement learning (DRL)"
  - [section] "we propose deploying a DRL model to assign task weights, with the loss declines of the MTL model as the reward for DRL"
- Break condition: If the SAC model fails to learn meaningful weight assignments or if the reward signal is insufficient for effective learning.

### Mechanism 3
- Claim: Combining IGB with gradient balancing yields complementary improvements by addressing both loss scale imbalance and gradient conflicts.
- Mechanism: IGB first balances improvable gaps through loss weighting, then gradient balancing algorithms aggregate the weighted gradients, addressing both loss scale differences and gradient conflicts.
- Core assumption: Loss balancing and gradient balancing address orthogonal aspects of the MTL optimization problem.
- Evidence anchors:
  - [abstract] "we combine IGB and gradient balancing to show the complementarity between the two types of algorithms"
  - [section] "Our IGB algorithms dynamically provide varying importance for each task to balance the improvable gaps, while gradient balancing algorithms deal more directly with gradient conflicts at the gradient level"
- Break condition: If gradient conflicts are not the primary source of performance degradation or if the combination creates optimization instability.

## Foundational Learning

- Concept: Multi-task learning optimization
  - Why needed here: Understanding how to balance competing objectives across multiple tasks is central to the IGB approach
  - Quick check question: What is the seesaw phenomenon in MTL and why does it occur?

- Concept: Reinforcement learning fundamentals
  - Why needed here: IGBv2 specifically uses SAC, requiring understanding of state, action, reward, and policy optimization
  - Quick check question: What are the key differences between on-policy and off-policy RL algorithms?

- Concept: Loss scaling and normalization
  - Why needed here: IGB algorithms use normalized losses to make fair comparisons across tasks with different loss scales
  - Quick check question: Why is scale-invariant loss balancing important in MTL and how does it differ from traditional approaches?

## Architecture Onboarding

- Component map: MTL model (shared encoder + task-specific decoders) -> IGB (weight assignment) -> Loss calculation -> Gradient computation -> Parameter update

- Critical path: MTL training loop → loss calculation → weight assignment (IGB) → gradient computation → parameter update

- Design tradeoffs:
  - IGBv1: Simpler, more efficient but potentially less adaptive
  - IGBv2: More complex, potentially better performance but higher computational overhead
  - Combined approach: Best performance but highest complexity

- Failure signatures:
  - Performance degradation when tasks have similar convergence patterns
  - Training instability when replay buffer size is misconfigured
  - Suboptimal performance if SAC model is not properly trained

- First 3 experiments:
  1. Implement IGBv1 on NYUv2 dataset and compare with EW baseline
  2. Add SAC model for IGBv2 and test on NYUv2 with proper reward signal
  3. Combine IGBv1 with MGDA and evaluate performance improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we further improve the learning efficiency of improvable gap balancing algorithms while maintaining or improving their performance?
- Basis in paper: The paper emphasizes the importance of learning efficiency in MTL and notes that IGBv1 achieves efficiency improvements with minimal extra training time, while IGBv2 slightly increases training time but achieves better performance in most situations.
- Why unresolved: The paper demonstrates the effectiveness of IGB algorithms but does not explore ways to further optimize their efficiency, especially for IGBv2 which uses deep reinforcement learning.
- What evidence would resolve it: Experimental results comparing different implementations of IGB algorithms with varying levels of efficiency and performance on benchmark datasets.

### Open Question 2
- Question: Can the concept of improvable gap balancing be extended to other domains beyond image-based and molecular property prediction tasks?
- Basis in paper: The paper demonstrates the effectiveness of IGB algorithms on NYUv2 (multi-task scene understanding) and QM9 (multi-task regression prediction) datasets, but does not explore other domains.
- Why unresolved: The paper focuses on two specific domains and does not investigate the applicability of IGB algorithms to other types of multi-task learning problems.
- What evidence would resolve it: Successful application and performance of IGB algorithms on a diverse range of multi-task learning problems from different domains, such as natural language processing, robotics, or recommendation systems.

### Open Question 3
- Question: How does the performance of improvable gap balancing algorithms scale with an increasing number of tasks in multi-task learning scenarios?
- Basis in paper: The paper demonstrates the effectiveness of IGB algorithms on NYUv2 (3 tasks) and QM9 (11 tasks) datasets, but does not explore scenarios with significantly more tasks.
- Why unresolved: The paper does not investigate the scalability of IGB algorithms to very large-scale multi-task learning problems with dozens or hundreds of tasks.
- What evidence would resolve it: Experimental results comparing the performance and efficiency of IGB algorithms with other methods on multi-task learning problems with varying numbers of tasks, from small to very large scale.

## Limitations
- Performance improvements rely heavily on empirical validation rather than theoretical guarantees
- Mechanisms explaining why DRL specifically outperforms simpler heuristics remain underspecified
- Assumption that baseline loss calculation from epoch 2 is representative may not hold across all dataset/task configurations

## Confidence
- **IGBv1 effectiveness**: High confidence - extensive experimental validation across two datasets with clear performance improvements
- **IGBv2 DRL approach**: Medium confidence - novel application of SAC to MTL, but fewer ablation studies on DRL-specific hyperparameters
- **Combination with gradient balancing**: Medium confidence - demonstrated complementarity but limited analysis of when this combination is most beneficial
- **Generalizability claims**: Low confidence - only tested on NYUv2 and QM9, both vision/NLP-style datasets

## Next Checks
1. **Ablation study on baseline calculation**: Test IGBv1 performance when baseline losses are computed from different epochs (1, 3, 5) to verify epoch 2 is optimal
2. **SAC architecture sensitivity**: Systematically vary SAC model architecture and replay buffer size to identify optimal configurations beyond the reported ranges
3. **Cross-domain generalization**: Evaluate IGB algorithms on a non-vision/non-chemistry MTL task (e.g., natural language understanding) to test generalizability claims