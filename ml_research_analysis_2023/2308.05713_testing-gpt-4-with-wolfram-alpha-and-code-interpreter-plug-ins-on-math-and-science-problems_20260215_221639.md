---
ver: rpa2
title: Testing GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on math and
  science problems
arxiv_id: '2308.05713'
source_url: https://arxiv.org/abs/2308.05713
tags:
- answer
- earth
- problems
- problem
- what
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The report tests GPT-4 with Wolfram Alpha and Code Interpreter
  plug-ins on 105 science and math problems. Results show that plug-ins significantly
  enhance GPT-4's problem-solving abilities compared to GPT-4 alone, but interface
  issues remain a challenge.
---

# Testing GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on math and science problems

## Quick Facts
- arXiv ID: 2308.05713
- Source URL: https://arxiv.org/abs/2308.05713
- Authors: [Not specified in input]
- Reference count: 40
- Primary result: Plug-ins significantly enhance GPT-4's problem-solving abilities compared to GPT-4 alone, but interface issues remain a challenge

## Executive Summary
This paper evaluates GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on 105 science and math problems, finding that the plug-ins significantly improve problem-solving capabilities compared to GPT-4 alone. The augmented system achieves middling undergraduate-level performance, excelling at single-formula problems but struggling with spatial visualization, multi-step calculations, and very large/small numbers. However, interface issues between GPT-4 and the plug-ins remain a significant challenge, with GPT-4 often failing to fully utilize the specialized tools' capabilities.

## Method Summary
The study tests GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on three sets of 105 original math and science problems: 32 "Arbitrary Numerical" problems, 53 "Calculation-Free" problems, and 20 "Motivated Numerical" problems. Performance is compared between GPT-4 with plug-ins versus GPT-4 alone, with success measured against ground truth answers. The methodology involves accessing GPT-4 through ChatGPT Plus and systematically evaluating problem-solving capabilities across diverse scientific domains.

## Key Results
- Plug-ins significantly enhance GPT-4's ability to solve science and math problems compared to GPT-4 alone
- The system performs at middling undergraduate level, strongest on single-formula problems
- GPT-4 often fails to take full advantage of plug-in capabilities, making errors that could be avoided by using specialized tools

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The plug-ins improve GPT-4's problem-solving by enabling it to offload symbolic computation and data retrieval to specialized tools
- Mechanism: GPT-4 generates Wolfram Alpha or Code Interpreter requests for exact symbolic math or numerical computation, then integrates returned results into its reasoning
- Core assumption: Plug-in capabilities are complementary to GPT-4's natural language reasoning and can be called without workflow disruption
- Evidence anchors: Abstract confirms plug-ins enhance problem-solving; section notes GPT-4 fails to fully utilize plug-in capacities; related work lacks specific plug-in benefit confirmation
- Break condition: GPT-4 formulates plug-in requests poorly or plug-in returns results in unparseable format

### Mechanism 2
- Claim: The plug-ins allow GPT-4 to solve problems requiring exact numeric or symbolic results beyond its floating-point arithmetic accuracy
- Mechanism: For very large/small numbers, combinatorial formulas, or symbolic simplification, plug-ins compute exact results while GPT-4 provides reasoning and context
- Core assumption: Plug-ins can accept natural language or structured queries and return correct, interpretable results
- Evidence anchors: Section notes strongest performance on single-formula problems; GPT-4 struggles to formulate acceptable Wolfram Alpha queries; related work lacks direct evidence
- Break condition: Plug-in cannot parse GPT-4's queries or output is ambiguous

### Mechanism 3
- Claim: GPT-4's reasoning can direct plug-ins to perform iterative or multi-step calculations that GPT-4 alone would mishandle
- Mechanism: GPT-4 uses plug-ins for sequential code snippets or symbolic operations while maintaining reasoning oversight
- Core assumption: Plug-ins support iterative interaction and can handle multiple related queries in one session
- Evidence anchors: Section provides examples of useless plug-in calls and GPT-4's failure to utilize Wolfram Alpha; related work lacks direct evidence
- Break condition: Plug-in cannot maintain session state or GPT-4 cannot track intermediate results across calls

## Foundational Learning

- Concept: Symbolic mathematics and exact numeric computation
  - Why needed here: Plug-ins specialize in symbolic math (Wolfram Alpha) and exact numerical methods (Code Interpreter), which GPT-4 approximates with floating-point arithmetic
  - Quick check question: Given a symbolic integral that cannot be evaluated exactly by GPT-4, how would you phrase it for Wolfram Alpha to return the exact result?

- Concept: Plug-in interface design and prompt engineering
  - Why needed here: GPT-4 must phrase plug-in requests to be accepted and return useful results; poor prompts lead to wasted calls or irrelevant output
  - Quick check question: If GPT-4 wants Wolfram Alpha to compute a probability distribution, what format should the query follow to ensure a numeric answer?

- Concept: Session state and context management in multi-step problem solving
  - Why needed here: Problems often require multiple plug-in calls with intermediate results; GPT-4 must track and combine these correctly
  - Quick check question: When a plug-in returns a large rational number, how should GPT-4 decide whether to simplify it or pass it forward?

## Architecture Onboarding

- Component map: GPT-4 LLM → natural language reasoning; Wolfram Alpha plug-in → symbolic math and data lookup; Code Interpreter plug-in → Python execution and numerical methods; Integration layer → prompt formatting and result parsing
- Critical path: GPT-4 generates query → Plug-in executes → Result returned → GPT-4 integrates into reasoning → Final answer composed
- Design tradeoffs: GPT-4's generality vs. plug-in's specialization; flexibility of natural language vs. precision of structured requests; session continuity vs. statelessness
- Failure signatures: GPT-4 makes unnecessary plug-in calls; plug-in returns malformed or verbose results; GPT-4 fails to parse plug-in output; plug-in query rejected or ignored
- First 3 experiments:
  1. Test GPT-4 + Wolfram Alpha on a symbolic integral problem; verify exact vs. approximate result
  2. Test GPT-4 + Code Interpreter on a numerical optimization; check correctness and efficiency
  3. Test multi-step reasoning: GPT-4 → plug-in for sub-calculation → GPT-4 → plug-in for final result; evaluate session handling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How much does the ability of GPT-4 to detect and recover from nonsensical plug-in outputs depend on the specific domain (e.g., physics vs. biology)?
- Basis in paper: The paper notes that "GPT-4 has some ability to detect when the answer returned by a plug-in is nonsensical or physically meaningless, but not reliably," but doesn't analyze domain-specific differences
- Why unresolved: The paper tested a broad range of problems but didn't systematically compare GPT-4's error detection and recovery across different scientific domains
- What evidence would resolve it: Controlled experiments comparing GPT-4's performance on similar problem types across multiple scientific domains, measuring both error rates and recovery attempts

### Open Question 2
- Question: What is the relationship between the complexity of mathematical reasoning required and GPT-4's tendency to perform calculations manually rather than using plug-ins?
- Basis in paper: The paper states "GPT-4 fails to take full advantage of the capacities of the plug-ins" and gives examples of unnecessary manual calculations
- Why unresolved: While the paper provides examples, it doesn't quantify this tendency or identify thresholds where GPT-4 switches between manual calculation and plug-in use
- What evidence would resolve it: Systematic analysis of GPT-4's decision-making process across problems of varying mathematical complexity, tracking when it chooses manual vs. plug-in approaches

### Open Question 3
- Question: How do interface failures between GPT-4 and Wolfram Alpha vary with the type of mathematical representation (e.g., symbolic expressions vs. natural language descriptions)?
- Basis in paper: The paper notes that "GPT-4 often struggles to formulate a problem in a way that Wolfram Alpha can accept or that produces useful output"
- Why unresolved: The paper provides examples but doesn't systematically analyze how different mathematical representations affect interface success rates
- What evidence would resolve it: Comparative testing of GPT-4's interface success rates using problems expressed in different mathematical formats, measuring both success rates and types of failures

## Limitations
- Interface integration issues between GPT-4 and plug-ins represent a significant uncertainty, with GPT-4 often failing to fully utilize plug-in capabilities
- Performance ceiling uncertainty exists regarding whether plug-in-augmented GPT-4 can reach expert-level performance
- Evaluation scope limitations include use of 105 original problems rather than standardized benchmarks

## Confidence
- High confidence in finding that plug-ins significantly enhance GPT-4's problem-solving capabilities compared to GPT-4 alone
- Medium confidence in characterization of performance as "middling undergraduate level" due to original nature of test problems
- Medium confidence in claim that GPT-4 fails to fully utilize plug-in capabilities, supported by qualitative observations but needing systematic analysis

## Next Checks
1. Systematic plug-in usage analysis: Track every plug-in call across all problems to quantify useful vs. useless calls and measure plug-in query success rates
2. Controlled prompt engineering experiment: Test whether improved prompt formatting can significantly improve success rates, isolating effect of GPT-4's query formulation skills
3. Benchmark comparison study: Apply same methodology to standardized math and science benchmarks to enable direct comparison with other LLM evaluation studies