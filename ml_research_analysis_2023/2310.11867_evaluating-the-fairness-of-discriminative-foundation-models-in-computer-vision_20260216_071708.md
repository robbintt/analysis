---
ver: rpa2
title: Evaluating the Fairness of Discriminative Foundation Models in Computer Vision
arxiv_id: '2310.11867'
source_url: https://arxiv.org/abs/2310.11867
tags:
- clip
- gender
- methods
- image
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a taxonomy to systematically evaluate fairness
  in discriminative foundation models like CLIP, categorizing tasks by whether they
  involve humans, the subjectivity of labels, and whether independence or diversity-based
  fairness is more suitable. The authors evaluate OpenAI's CLIP and OpenCLIP models
  across tasks like zero-shot classification, image retrieval, and captioning using
  metrics like demographic parity and diversity measures.
---

# Evaluating the Fairness of Discriminative Foundation Models in Computer Vision

## Quick Facts
- **arXiv ID**: 2310.11867
- **Source URL**: https://arxiv.org/abs/2310.11867
- **Reference count**: 40
- **Primary result**: Fair PCA is effective for debiasing across multiple tasks with minimal performance loss, while mutual information-based methods reduce bias but hurt accuracy.

## Executive Summary
This paper proposes a taxonomy for systematically evaluating fairness in discriminative foundation models like CLIP, categorizing tasks by human-centricity, label subjectivity, and appropriate fairness metrics. The authors evaluate OpenAI's CLIP and OpenCLIP models across zero-shot classification, image retrieval, and captioning tasks using demographic attributes. They find that fair PCA is particularly effective for debiasing across most tasks while maintaining performance, while mutual information-based methods reduce bias but can significantly impact accuracy. The choice of debiasing method should depend on the specific use case and type of fairness required.

## Method Summary
The authors evaluate fairness in CLIP models using post-processing debiasing methods including mutual information-based clipping, fair PCA, and prompt learning. They test these methods across three main tasks: zero-shot classification, image retrieval, and image captioning. The evaluation uses multiple datasets (FairFace, Flickr30K, MSCOCO, CelebA, StanfordCars, VOC objects, ImageNet, IdenProf, MIAP) with demographic annotations for gender and race. Fairness is measured using demographic parity, true positive rate disparity, and skew metrics, while performance is assessed through accuracy, precision, and recall. The study compares the effectiveness of different debiasing approaches and their impact on both fairness and task performance.

## Key Results
- Fair PCA consistently reduces bias across most tasks while incurring only minor performance loss
- Mutual information-based methods effectively reduce bias but significantly hurt model accuracy
- Prompt-based debiasing is less effective than other methods at reducing bias
- The choice between independence and diversity fairness metrics should depend on task characteristics (human-centric vs. non-human-centric, subjective vs. objective)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fair PCA effectively reduces bias by projecting data into a lower-dimensional space that removes correlations with protected attributes while preserving useful information.
- Mechanism: The method computes principal components of the data and selects a subset that minimizes correlation between the projected data and protected attributes (gender/race). This projection is learned using the training split of a dataset annotated with protected attributes.
- Core assumption: The linear relationship between data and protected attributes is sufficient to capture and remove bias; nonlinear relationships are not critical for fairness in this context.
- Evidence anchors:
  - [abstract]: "We find that fair PCA, a post-processing method for fair representations, works very well for debiasing in most of the aforementioned tasks while incurring only minor loss of performance."
  - [section 2.3.3]: Describes fair PCA as finding a projection where any linear function applied to the projected datapoint is statistically independent of the protected attribute, relaxing independence to uncorrelatedness.
  - [corpus]: DetoxAI and debiaSAE are related works focusing on debiasing, suggesting this is an active area of research.
- Break condition: If the data contains strong nonlinear dependencies between features and protected attributes, or if the bias is encoded in higher-order interactions that linear projections cannot capture.

### Mechanism 2
- Claim: Mutual information-based debiasing (CLIP-clip) reduces bias by selectively removing dimensions from CLIP embeddings that have high mutual information with protected attributes.
- Mechanism: For each dimension of the CLIP embedding, mutual information with the protected attribute is calculated. Dimensions with the highest mutual information are removed, reducing the model's ability to use those dimensions to make biased predictions.
- Core assumption: The dimensions with highest mutual information with protected attributes are the primary source of bias, and removing them will reduce bias without significantly harming overall performance.
- Evidence anchors:
  - [abstract]: Mentions mutual information-based methods reduce bias but can hurt accuracy.
  - [section 2.3.1]: Describes the method as calculating mutual information between CLIP embedding and gender attribute, then greedily selecting dimensions with highest mutual information to cut.
  - [corpus]: debiaSAE benchmarks and mitigates VLM bias, suggesting MI-based approaches are relevant for this domain.
- Break condition: If the bias is distributed across many dimensions or encoded in complex interactions between dimensions, simple removal of top MI dimensions may not be effective and could cause significant performance degradation.

### Mechanism 3
- Claim: Prompt-based debiasing reduces bias by learning text prompts that make the model less sensitive to protected attributes during contrastive learning.
- Mechanism: Learnable text prompts are added to sensitive queries, and the model is trained to minimize the ability of an adversarial classifier to predict the protected attribute from the similarity logits, while maintaining performance with an image-text contrastive loss.
- Core assumption: The bias in CLIP's embeddings can be mitigated by altering the text representation in a way that breaks the correlation with protected attributes, without needing to change the image encoder.
- Evidence anchors:
  - [abstract]: Notes that prompt-based methods do not reduce bias as effectively as other methods.
  - [section 2.3.2]: Describes the method as incorporating learnable text prompts into sensitive queries and using an adversarial classifier to predict the protected attribute.
  - [corpus]: Debiasing CLIP via biased prompts is a related concurrent work, indicating this approach is being explored.
- Break condition: If the bias is primarily encoded in the image embeddings rather than the text embeddings, or if the adversarial training is not strong enough to overcome the bias in the original embeddings.

## Foundational Learning

- Concept: Understanding the difference between independence-based and diversity-based fairness metrics.
  - Why needed here: The paper evaluates debiasing methods using both demographic parity (independence) and diversity measures, and the choice of metric depends on the task's human-centricity and subjectivity.
  - Quick check question: For a subjective, human-centric task like classifying images as "criminal" vs "innocent person", which type of fairness metric (independence or diversity) is more appropriate and why?

- Concept: Knowledge of CLIP's architecture and how it learns joint image-text embeddings.
  - Why needed here: The debiasing methods operate on CLIP's embeddings, so understanding how they are generated and used for downstream tasks is crucial for implementing and evaluating the methods.
  - Quick check question: In CLIP's zero-shot classification, how are the image and text embeddings combined to produce a prediction, and at what point do the debiasing methods intervene in this process?

- Concept: Familiarity with statistical tests for comparing distributions, such as ANOVA.
  - Why needed here: The paper uses ANOVA to test for statistically significant differences in cosine similarity between protected attribute groups, which is a key part of evaluating the effectiveness of debiasing methods.
  - Quick check question: When comparing the cosine similarity distributions between men and women for a given query, what does a low p-value from an ANOVA test indicate about the debiasing method's performance?

## Architecture Onboarding

- Component map: CLIP model (image and text encoders) -> Debiasing methods (fair PCA, mutual information, prompt-based) -> Evaluation tasks (zero-shot classification, image retrieval, image captioning) -> Fairness metrics (demographic parity, true positive rate disparity, skew)
- Critical path: For a new engineer, the critical path is to understand how CLIP embeddings are generated, how each debiasing method modifies these embeddings, and how the modified embeddings are used in downstream tasks and evaluated for fairness.
- Design tradeoffs: The main tradeoff is between reducing bias and maintaining performance. Methods like mutual information-based debiasing can reduce bias but may hurt accuracy, while fair PCA is more effective at balancing both. The choice of method depends on the specific use case and the type of fairness needed.
- Failure signatures: If a debiasing method fails to reduce bias, you might see high values of demographic disparity or skew in the evaluation metrics. If it fails to maintain performance, you might see a significant drop in accuracy or precision/recall on the downstream tasks.
- First 3 experiments:
  1. Implement and evaluate fair PCA on a simple binary classification task using a balanced dataset, comparing demographic parity before and after debiasing.
  2. Implement and evaluate mutual information-based debiasing on the same task, varying the number of dimensions removed to find the optimal tradeoff between bias reduction and performance.
  3. Implement and evaluate prompt-based debiasing on an image retrieval task, comparing the average cosine similarity between protected attribute groups before and after debiasing.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively evaluate and mitigate bias in non-human-centric labeling tasks, such as those involving religious or cultural content, where the harms are subtle and context-dependent?
- Basis in paper: [inferred] The paper mentions that non-human-centric labeling tasks are out of scope but acknowledges that harms can exist, citing the example of biased image searches for "beautiful building" favoring Christian churches.
- Why unresolved: The paper does not provide specific metrics or methods for evaluating fairness in these scenarios, highlighting the difficulty in defining appropriate ground truth and handling diverse cultural contexts.
- What evidence would resolve it: Development and validation of context-aware fairness metrics and debiasing techniques tailored to non-human-centric tasks, with case studies demonstrating their effectiveness in reducing subtle harms.

### Open Question 2
- Question: How does the choice of fairness metric (independence vs. diversity) impact the long-term societal effects of deploying biased models in different applications?
- Basis in paper: [explicit] The paper emphasizes the importance of choosing the appropriate fairness metric based on the intended use case, discussing the trade-offs between independence and diversity in different scenarios.
- Why unresolved: The paper focuses on immediate fairness outcomes but does not explore the broader societal implications of prioritizing one type of fairness over another in various applications.
- What evidence would resolve it: Longitudinal studies comparing the societal impact of models optimized for independence vs. diversity in different domains, such as education, employment, and media representation.

### Open Question 3
- Question: How can we develop more robust and generalizable bias mitigation methods that perform well across a wider range of tasks and protected attributes, without compromising accuracy?
- Basis in paper: [inferred] The paper finds that different debiasing methods work better for different tasks and protected attributes, with fair PCA generally performing well but not being optimal in all cases.
- Why unresolved: The paper evaluates a limited set of methods and datasets, and the effectiveness of these methods may not generalize to other tasks, protected attributes, or model architectures.
- What evidence would resolve it: Extensive empirical evaluations of various debiasing methods across diverse tasks, protected attributes, and model architectures, along with theoretical analyses of their limitations and potential improvements.

## Limitations

- The evaluation focuses primarily on binary gender classification and a limited set of race categories, potentially missing intersectional biases
- The study uses post-processing methods only, without examining architectural or training-time interventions
- Results are based on publicly available datasets, which may not represent real-world deployment scenarios

## Confidence

- **High Confidence**: Fair PCA effectiveness across multiple tasks (supported by multiple experiments and consistent results)
- **Medium Confidence**: Mutual information-based methods' performance (trade-off between bias reduction and accuracy is clear but context-dependent)
- **Low Confidence**: Prompt-based debiasing efficacy (limited experimental support and noted as less effective in the paper)

## Next Checks

1. **Intersectional Analysis**: Re-run key experiments with intersectional demographic categories (e.g., race Ã— gender combinations) to verify fairness claims hold across multiple protected attributes simultaneously.

2. **Cross-Domain Generalization**: Test the debiasing methods on a dataset from a different domain (e.g., medical imaging or satellite imagery) to assess whether the observed performance-fairness tradeoffs generalize beyond the evaluated tasks.

3. **Temporal Stability**: Evaluate whether the debiasing methods maintain their effectiveness over time by testing on temporally separated data splits or by examining model behavior before and after fine-tuning on new data.