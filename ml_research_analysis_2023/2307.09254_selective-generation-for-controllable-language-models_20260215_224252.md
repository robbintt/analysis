---
ver: rpa2
title: Selective Generation for Controllable Language Models
arxiv_id: '2307.09254'
source_url: https://arxiv.org/abs/2307.09254
tags:
- prediction
- size
- ours
- uncertainty
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Selective Generation for Controllable Language Models proposes
  a novel approach to quantify and control the uncertainty of generative language
  models (GLMs) using neural prediction sets. The key idea is to parameterize prediction
  sets via neural networks instead of scalar parameters, enabling example-dependent
  uncertainty quantification.
---

# Selective Generation for Controllable Language Models

## Quick Facts
- arXiv ID: 2307.09254
- Source URL: https://arxiv.org/abs/2307.09254
- Authors: 
- Reference count: 40
- Key outcome: Neural prediction sets (NPS) achieve 63% smaller prediction sets than scalar-parameterized baselines while maintaining PAC guarantees for controllable language models.

## Executive Summary
Selective Generation for Controllable Language Models proposes a novel approach to quantify and control uncertainty in generative language models (GLMs) using neural prediction sets. The method replaces scalar parameters with neural networks to enable example-dependent uncertainty quantification, achieving more precise uncertainty estimates while maintaining PAC guarantees. The authors develop a decomposition strategy (log τ(x) := g(x) + h) that separates context-dependent and context-independent components, enabling sample-efficient learning. Experimental results demonstrate significant improvements in prediction set size (63% smaller on average) across various datasets and GLM architectures including GPT-2 and LLaMA models.

## Method Summary
The method constructs neural prediction sets by parameterizing uncertainty thresholds via neural networks instead of scalar parameters. The key innovation is decomposing the neural parameter τ(x) into a neural network g(x) capturing conformity score trends and a scalar h controlling error rates. This decomposition leverages a structural property where the ideal uncertainty threshold forms a "hammock" shape supporting conformity scores. The learning procedure consists of two stages: first learning g through regression on conformity scores, then learning h via a PAC algorithm that controls the prediction set error. The approach maintains PAC guarantees under the i.i.d. assumption while producing significantly smaller prediction sets than traditional scalar-parameterized methods.

## Key Results
- NPS achieves desired false discovery rate with respect to textual entailment (FDR-E) across various datasets
- NPS produces prediction sets that are 63% smaller on average compared to scalar-parameterized baseline (SPS)
- The method works across multiple GLM architectures including GPT-2 (Small/Medium/Large) and LLaMA (7B/13B)
- SPS baseline fails to produce non-vacuous prediction sets for some models (LLaMA-7B, Alpaca-7B)

## Why This Works (Mechanism)

### Mechanism 1
Neural prediction sets provide token-wise uncertainty quantification that is more precise than scalar-parameterized methods because the neural network parameter τ(x) can adapt to each token's context. The scalar parameter τ in SPS is example-agnostic, leading to conservative prediction sets for all tokens. NPS replaces τ with a neural network τ(x) that takes the prompt x as input, allowing the uncertainty threshold to vary depending on the token's context. This enables tighter prediction sets that better reflect the actual uncertainty of each token.

### Mechanism 2
The decomposition log τ(x) := g(x) + h allows for efficient PAC learning by separating the context-dependent and context-independent components of the uncertainty threshold. The parameter τ(x) is decomposed into g(x), a neural network capturing the trend of conformity scores, and h, a scalar controlling the prediction set error. This decomposition enables sample-efficient learning: g is learned via regression on conformity scores, and h is learned via a PAC algorithm that controls the error rate. The structural property that g controls size and h controls error allows for independent optimization of each component.

### Mechanism 3
The PAC guarantee ensures that the quantified uncertainty is trustworthy, as the prediction set error is controlled below the desired level ε with probability at least 1 - δ. The algorithm ANPS is proven to be PAC, meaning that for any ε ∈ (0,1), δ ∈ (0,1), and n ∈ Z≥0, the learned prediction set model satisfies the error guarantee. This guarantee is derived from the structural property of prediction sets and the use of the binomial tail bound UBinom in the learning algorithm.

## Foundational Learning

- **Concept**: Conformal prediction and PAC prediction sets
  - Why needed here: Conformal prediction provides a framework for constructing prediction sets with correctness guarantees under minimal assumptions. PAC prediction sets interpret conformal prediction in the lens of PAC learning theory, allowing for the learning of prediction set models from data with a desired error rate.
  - Quick check question: What is the key difference between conformal prediction and PAC prediction sets, and how does this difference enable the learning of prediction set models with a PAC guarantee?

- **Concept**: Neural network parameterization of prediction sets
  - Why needed here: Scalar parameterization of prediction sets is insufficient to capture token-wise uncertainty in language generation tasks. Neural network parameterization allows the uncertainty threshold to adapt to the context of each token, enabling more precise uncertainty quantification.
  - Quick check question: How does replacing the scalar parameter τ with a neural network τ(x) in the prediction set definition improve the ability to quantify token-wise uncertainty?

- **Concept**: Decomposition of neural prediction set parameters
  - Why needed here: The decomposition log τ(x) := g(x) + h separates the context-dependent and context-independent components of the uncertainty threshold, enabling sample-efficient learning. g captures the trend of conformity scores, while h controls the prediction set error.
  - Quick check question: What is the role of each component in the decomposition log τ(x) := g(x) + h, and how does this decomposition contribute to sample-efficient PAC learning?

## Architecture Onboarding

- **Component map**: Scoring function f (GLM) -> Neural network g -> Scalar h -> Algorithm ANPS (NPS model)

- **Critical path**:
  1. Compute conformity scores f(x,y) for token predictions using the scoring function
  2. Learn the neural network g by solving the regression problem in Ag
  3. Learn the scalar h using the PAC algorithm in Ah
  4. Construct the neural prediction set model using the learned parameters g and h
  5. Use the neural prediction set model to quantify token-wise uncertainty in language generation

- **Design tradeoffs**:
  - Neural network parameterization vs. scalar parameterization: Enables token-wise adaptation but introduces complexity and overfitting risks
  - Decomposition strategy: Enables sample-efficient learning but assumes specific structure of conformity scores
  - PAC guarantee vs. empirical calibration: Provides correctness assurance but relies on i.i.d. assumption and may be conservative

- **Failure signatures**:
  - Poor conformity score structure: If conformity scores don't have learnable structure, g won't effectively capture trends
  - Violated i.i.d. assumption: Distribution shift breaks PAC guarantee, making uncertainty untrustworthy
  - Failed decomposition: If g and h cannot be optimized independently, sample efficiency gains disappear

- **First 3 experiments**:
  1. Implement SPS baseline using ABinom algorithm and evaluate on toy dataset with known conformity score structure
  2. Implement NPS with decomposition log τ(x) := g(x) + h and compare prediction set size and error to SPS baseline on same toy dataset
  3. Implement full ANPS algorithm and evaluate on TriviaQA using GPT-2 as scoring function, comparing to SPS baseline and visualizing token-wise uncertainty

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the proposed method handle distribution shift, such as adversarial prompts?
  - Basis: The authors acknowledge this as a limitation, stating the method relies on the i.i.d. assumption which can be violated via distribution shift
  - Why unresolved: No empirical results or theoretical analysis on performance under distribution shift
  - What evidence would resolve it: Experiments on datasets with known distribution shift or adversarial examples, comparing to methods designed for distribution shift

- **Open Question 2**: Can the proposed method be extended to quantify semantic uncertainty beyond token-wise uncertainty?
  - Basis: The authors mention this as a potential future direction, stating "considering beyond token-wise uncertainty (e.g., semantic uncertainty) could provide more user-friendly uncertainty"
  - Why unresolved: The paper only focuses on token-wise uncertainty quantification
  - What evidence would resolve it: Development and evaluation of semantic uncertainty quantification method, comparing to token-wise approach

- **Open Question 3**: How does the choice of neural network architecture for g affect the performance of the proposed method?
  - Basis: The authors use a specific fully-connected architecture but don't explore alternatives
  - Why unresolved: No ablation study or comparison of different neural network architectures for g
  - What evidence would resolve it: Experiments comparing performance using different neural network architectures for g, analyzing impact on prediction set size and error

## Limitations

- The method relies on the i.i.d. assumption, which can be violated by distribution shift or adversarial prompts, breaking the PAC guarantee
- The proposed "hammock" structure of conformity scores is critical for the decomposition strategy but lacks extensive empirical validation across diverse tasks
- SPS baseline fails to produce non-vacuous prediction sets for some models (LLaMA-7B, Alpaca-7B), suggesting potential limitations in generalizability

## Confidence

**High Confidence** (4 claims):
- NPS achieves desired false discovery rate with respect to textual entailment (FDR-E) on various datasets and GLM architectures
- NPS produces significantly smaller prediction sets (63% smaller on average) compared to SPS baseline
- The decomposition log τ(x) := g(x) + h enables sample-efficient learning when the "hammock" structure holds
- The PAC guarantee provides correctness assurance under the i.i.d. assumption

**Medium Confidence** (2 claims):
- The "hammock" structure of conformity scores is a general property that enables the decomposition strategy
- The token-wise adaptation of NPS provides more precise uncertainty quantification than scalar-parameterized methods

**Low Confidence** (1 claim):
- The performance improvements of NPS over SPS will generalize to all language generation tasks and GLM architectures

## Next Checks

1. **Structure Validation**: Conduct systematic experiments to empirically verify the "hammock" structure of conformity scores across diverse language tasks and GLM architectures. This includes visualizing conformity score distributions for true and false labels across different contexts and quantifying how consistently the "hammock" structure appears.

2. **Robustness Testing**: Evaluate the method's performance under distribution shift and adversarial conditions where the i.i.d. assumption is violated. This includes testing on out-of-distribution prompts, adversarial examples designed to break the conformity score structure, and domain adaptation scenarios.

3. **Alternative Decomposition Analysis**: Compare the proposed decomposition log τ(x) := g(x) + h with alternative parameterization strategies (e.g., different functional forms, attention-based parameterizations) to determine whether the specific "hammock" structure is necessary or if similar performance can be achieved through other decompositions.