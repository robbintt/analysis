---
ver: rpa2
title: Learning to Select the Relevant History Turns in Conversational Question Answering
arxiv_id: '2308.02294'
source_url: https://arxiv.org/abs/2308.02294
tags:
- question
- conversational
- history
- turns
- relevant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a framework for selecting relevant conversational
  history turns in conversational question answering (ConvQA). The key idea is to
  use distant supervision learning to generate context and question entities for each
  history turn, prune turns that don't share similar entities with the current question,
  re-rank the remaining turns based on attention weights, and highlight relevant terms
  using binary classification.
---

# Learning to Select the Relevant History Turns in Conversational Question Answering

## Quick Facts
- arXiv ID: 2308.02294
- Source URL: https://arxiv.org/abs/2308.02294
- Authors: [List not provided]
- Reference count: 40
- Primary result: Dynamic history selection outperforms question rewriting in ConvQA, with adding irrelevant turns degrading performance

## Executive Summary
This paper proposes a framework for dynamically selecting relevant conversational history turns in conversational question answering (ConvQA). The approach uses distant supervision to generate context and question entities, prunes turns lacking entity similarity with the current question, re-ranks remaining turns using attention weights, and highlights relevant terms via binary classification. Experiments on CANARD and QuAC datasets demonstrate that this dynamic selection method improves performance compared to question rewriting baselines, while adding irrelevant history turns negatively impacts accuracy.

## Method Summary
The framework generates context and question entities for all history turns using BART, prunes turns that don't share similar entities with the current question, re-ranks remaining turns based on attention weights, and highlights relevant terms using binary classification. The model is trained end-to-end using distant supervision learning without manual annotation.

## Key Results
- Dynamic history selection outperforms question rewriting baselines on CANARD and QuAC datasets
- Adding irrelevant history turns degrades model performance, validating the importance of selective history inclusion
- The framework shows improved handling of incomplete questions through term highlighting

## Why This Works (Mechanism)

### Mechanism 1
Distant supervision-based entity generation improves history turn selection by creating task-specific context and question entities for each turn. The system uses BART to generate context and question entities for each history turn, then prunes turns that lack similarity with current question entities, reducing noise before re-ranking. Core assumption: Context and question entities extracted via distant supervision are reliable indicators of turn relevance. Break condition: If entity generation fails to capture meaningful context, pruning will remove relevant turns and hurt performance.

### Mechanism 2
Attention-based re-ranking assigns higher weights to turns more relevant to the current question. After pruning, remaining turns are scored using attention weights calculated by a feed-forward network, then re-ranked so most relevant turns appear first in input. Core assumption: Attention weights calculated on turn representations correlate with actual turn usefulness for answering. Break condition: If attention mechanism is poorly trained, irrelevant turns may receive high weights, introducing noise.

### Mechanism 3
Binary term classification highlights relevant terms to resolve incomplete questions. RoBERTa with a term classification layer tags terms as relevant (1) or irrelevant (0), providing additional context for incomplete questions. Core assumption: Highlighting terms through binary classification improves model understanding of incomplete questions. Break condition: If classification is inaccurate, it may mislead the model with false cues.

## Foundational Learning

- Concept: Distant supervision for labeling
  - Why needed here: Enables training of entity generation and term classification without manual annotation by using answer span presence as weak signal
  - Quick check question: How does the model determine which terms are "relevant" without human-labeled data?

- Concept: Attention mechanisms for re-ranking
  - Why needed here: Allows dynamic weighting of history turns based on current question context rather than fixed position
  - Quick check question: What feature representations are used as input to the attention scoring function?

- Concept: Entity extraction for context modeling
  - Why needed here: Enables pruning of irrelevant turns by comparing turn-level context entities with question entities
  - Quick check question: What entity types (named entities, noun phrases, etc.) are extracted for pruning?

## Architecture Onboarding

- Component map: Context Passage -> History Turns -> Entity Generation (BART) -> Pruning Module -> Attention Module (FFN) -> Re-ranking -> Term Classification (RoBERTa) -> Answer Prediction

- Critical path: Entity Generation → Pruning → Attention-based Re-ranking → Term Classification → Answer Prediction

- Design tradeoffs:
  - Fixed k turns vs. dynamic selection: Dynamic selection reduces noise but adds computational overhead
  - Entity-based pruning vs. embedding similarity: Entity-based is interpretable but may miss semantic similarity
  - Attention-based vs. heuristic ranking: Attention learns relevance but requires training data

- Failure signatures:
  - Low precision, high recall: Pruning too aggressive, removing relevant turns
  - High precision, low recall: Pruning too lenient, keeping irrelevant turns
  - Degraded performance with more history: Attention weights not learning properly
  - Poor performance on incomplete questions: Term classification not providing useful hints

- First 3 experiments:
  1. Ablation study: Remove pruning step and measure performance drop
  2. Ablation study: Remove attention re-ranking and measure performance drop
  3. Ablation study: Remove term classification and measure performance drop on incomplete questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we minimize the retrieval of irrelevant conversational turns in ConvQA?
- Basis in paper: Explicit - The authors highlight this as a future research challenge, noting that any negative samples or irrelevant turns selected during history selection directly affect the model's performance in predicting the correct answer span.
- Why unresolved: This challenge remains open because the authors propose a combination of hard and soft history selection methods, but acknowledge that further work is needed to minimize the impact of irrelevant turns on the model's performance.
- What evidence would resolve it: Developing and evaluating new techniques or models that can effectively filter out irrelevant conversational turns, and demonstrating improved performance in ConvQA tasks.

### Open Question 2
- Question: How can we eliminate error propagation between model components in ConvQA?
- Basis in paper: Explicit - The authors identify this as a significant future research challenge, noting that the output of each module serves as input to the next module, creating opportunities for error propagation.
- Why unresolved: This challenge is unresolved because the current modular approach to ConvQA, while effective, does not address the issue of error propagation between components.
- What evidence would resolve it: Designing and evaluating a framework or model that can effectively mitigate error propagation between components, and demonstrating improved performance and robustness in ConvQA tasks.

### Open Question 3
- Question: How can we improve the handling of topic shifts and topic returns in ConvQA?
- Basis in paper: Explicit - The authors note that adding k immediate turns as part of the input to the ConvQA model fails to capture the essence of the conversational flow, particularly in cases of topic shifts and topic returns.
- Why unresolved: This challenge remains open because the authors propose a combination of hard and soft history selection methods, but acknowledge that further work is needed to improve the handling of topic shifts and topic returns.
- What evidence would resolve it: Developing and evaluating new techniques or models that can effectively handle topic shifts and topic returns in ConvQA, and demonstrating improved performance in tasks that involve these dialog features.

## Limitations

- The entity-based pruning mechanism relies heavily on the quality of distantly supervised entity extraction, which may struggle with ambiguous or context-dependent entities
- Attention-based re-ranking assumes that linear combinations of turn representations can effectively capture relevance, though specific attention architecture details remain underspecified
- Term classification for highlighting relevant terms is implemented but lacks empirical validation showing how much this component contributes to overall performance gains

## Confidence

- **High confidence**: The core methodology of pruning irrelevant turns before feeding history to the answer model is well-supported by experimental results showing degradation when irrelevant turns are added
- **Medium confidence**: The effectiveness of the attention-based re-ranking mechanism is supported by reported improvements but lacks detailed ablation studies isolating its specific contribution
- **Low confidence**: The binary term classification component's impact is claimed but not thoroughly validated, with no clear evidence of how it improves handling of incomplete questions

## Next Checks

1. Ablation study of entity pruning: Remove the pruning step entirely and measure performance degradation to quantify how much noise reduction contributes to accuracy gains

2. Attention weight analysis: Examine attention weight distributions across different turn positions and question types to verify the mechanism learns meaningful relevance patterns rather than position-based heuristics

3. Term classification contribution: Conduct controlled experiments isolating the term classification component's impact, particularly on incomplete questions, to validate its claimed benefit