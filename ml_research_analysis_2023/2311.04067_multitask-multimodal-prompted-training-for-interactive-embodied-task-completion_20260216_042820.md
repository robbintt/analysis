---
ver: rpa2
title: Multitask Multimodal Prompted Training for Interactive Embodied Task Completion
arxiv_id: '2311.04067'
source_url: https://arxiv.org/abs/2311.04067
tags:
- token
- object
- visual
- task
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'EMMA is a unified multimodal encoder-decoder model that addresses
  two core challenges in interactive embodied tasks: grounding language in action
  trajectories and resolving referential ambiguity through dialog. It encodes language,
  object-centric visual features, and sentinel tokens for frame and region referencing
  in a shared architecture, and casts diverse tasks as text generation to enable transfer
  across domains.'
---

# Multitask Multimodal Prompted Training for Interactive Embodied Task Completion

## Quick Facts
- arXiv ID: 2311.04067
- Source URL: https://arxiv.org/abs/2311.04067
- Authors: 
- Reference count: 40
- Key outcome: EMMA achieves 36.81% success rate on Dialog-guided Task Completion benchmark, setting new state-of-the-art.

## Executive Summary
EMMA is a unified multimodal encoder-decoder model designed for interactive embodied task completion. It addresses two core challenges: grounding language in action trajectories and resolving referential ambiguity through dialog. The model encodes language, object-centric visual features, and sentinel tokens for frame and region referencing in a shared architecture, casting diverse tasks as text generation to enable transfer across domains. Pretrained on multiple vision-and-language tasks and fine-tuned for dialog-guided task completion in the Alexa Arena, EMMA demonstrates strong performance and generalization capabilities.

## Method Summary
EMMA is pretrained on seven vision-and-language tasks using a text-to-text framework, including image captioning, VQA, and visual grounding. It employs object-centric visual features and sentinel tokens to reference specific frames and objects. The model is fine-tuned on the Alexa Arena dataset using three complementary tasks: Contextual Routing, Action Execution, and Visual Grounding. Data augmentations including visual augmentations and CDF augmentations are used to improve temporal reasoning and generalization.

## Key Results
- EMMA achieves 36.81% success rate on the Dialog-guided Task Completion benchmark, setting a new state-of-the-art.
- Strong zero-shot and few-shot generalization to real-world visual domains.
- Competitive performance on downstream VL tasks including image captioning, VQA, and visual grounding.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EMMA achieves cross-task transfer by casting diverse embodied tasks as text generation, enabling a unified "language of actions" that facilitates generalization.
- Mechanism: By unifying vision-and-language tasks into a text-to-text framework, EMMA learns shared representations across tasks like captioning, visual grounding, and action execution, which improves zero-shot and few-shot performance on embodied benchmarks.
- Core assumption: Text generation is sufficiently expressive to represent both perceptual reasoning and action planning in embodied environments.
- Evidence anchors:
  - [abstract] "casts action prediction as multimodal text generation... learns a language of actions which facilitates transfer across tasks"
  - [section 4.