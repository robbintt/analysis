---
ver: rpa2
title: A Comparative Analysis of Pretrained Language Models for Text-to-Speech
arxiv_id: '2309.01576'
source_url: https://arxiv.org/abs/2309.01576
tags:
- language
- prosody
- prediction
- pause
- bert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study presents the first comparative analysis of 15 pretrained
  language models (PLMs) for two text-to-speech (TTS) tasks: prosody prediction and
  pause prediction. The research addresses the gap in understanding how different
  PLMs affect TTS performance, particularly in relation to model size and pretraining
  methodology.'
---

# A Comparative Analysis of Pretrained Language Models for Text-to-Speech

## Quick Facts
- **arXiv ID**: 2309.01576
- **Source URL**: https://arxiv.org/abs/2309.01576
- **Reference count**: 0
- **Primary result**: First comparative analysis of 15 PLMs for prosody and pause prediction in TTS reveals logarithmic size-performance relationship and strong GLUE correlation.

## Executive Summary
This study presents the first comprehensive comparison of 15 pretrained language models (PLMs) for text-to-speech tasks, specifically prosody prediction and pause prediction. Using the eCat TTS system and an internal dataset of expressive and neutral speech recordings, the research examines how different PLM characteristics affect TTS performance. The study reveals that larger PLMs significantly improve prosody prediction in a logarithmic relationship, while pause prediction is less sensitive to model size. Additionally, a strong correlation is found between TTS task performance and GLUE scores, suggesting that improvements in PLMs for natural language understanding can inform TTS advancements.

## Method Summary
The study evaluated 15 PLMs on two TTS tasks using the eCat system: prosody prediction and pause prediction. For prosody prediction, the PLM generates word-level embeddings that are processed by a Bi-LSTM network with normalizing flows to predict prosodic latents. For pause prediction, a similar architecture with a dense layer predicts pause probabilities. Both models were trained end-to-end with identical hyperparameters across all PLMs. The evaluation used an internal dataset of over 20 hours of speech from 4 female US English speakers, with train/dev/test splits. Prosody prediction quality was measured using Euclidean distance between predicted and ground-truth latents, supplemented by subjective MUSHRA evaluations for naturalness. Pause prediction was evaluated using precision, recall, F-0.5, and F-1 scores.

## Key Results
- Larger PLMs show logarithmic improvement in prosody prediction quality, with significant gains especially for expressive speech
- Pause prediction is less sensitive to model size, with smaller models like ELECTRA-small performing comparably to larger ones
- Strong correlation (absolute correlation >0.8) exists between TTS task performance and PLM GLUE scores

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Larger PLMs improve prosody prediction quality in a logarithmic relationship.
- **Mechanism**: Larger PLMs capture richer linguistic context (syntax, semantics, prosody cues) from pretraining data, enabling better prosodic feature extraction during TTS fine-tuning.
- **Core assumption**: The pretraining tasks (e.g., masked language modeling) effectively encode prosodic-relevant linguistic knowledge.
- **Evidence anchors**: [abstract] "Our findings revealed a logarithmic relationship between model size and quality"; [section] "Results manifest a negative correlation between size and distance; prosody predictors with bigger language models reduce the distance to ground-truth representations"
- **Break condition**: If pretraining data lacks prosodic variability or fine-tuning is too short to adapt to TTS-specific prosody patterns, larger size may not help.

### Mechanism 2
- **Claim**: Pause prediction is less sensitive to PLM size; smaller models like ELECTRA-small perform comparably to larger ones.
- **Mechanism**: Pause prediction is a simpler binary classification task requiring only coarse linguistic cues (e.g., punctuation, clause boundaries), which smaller models capture adequately.
- **Core assumption**: The English pause prediction task has limited ambiguity, making high-level linguistic understanding sufficient.
- **Evidence anchors**: [abstract] "pause prediction was less sensitive to small models. We also identified a strong correlation between our empirical results and the GLUE scores"; [section] "Results indicate a relation between size and performance as larger models like RoBERTa-large perform better than smaller ones... but from ELECTRA-small until RoBERTa-large there was a slighter performance increment"
- **Break condition**: If the language has richer pause cues or more ambiguity (e.g., tonal languages), smaller models may underperform.

### Mechanism 3
- **Claim**: Strong correlation between PLM performance on GLUE and TTS tasks suggests PLM quality for NLU translates to TTS.
- **Mechanism**: PLMs pretrained for robust language understanding implicitly learn prosodic and discourse features useful for TTS; these features transfer during fine-tuning.
- **Core assumption**: GLUE tasks (sentiment, entailment, similarity) indirectly evaluate linguistic representations beneficial for TTS.
- **Evidence anchors**: [abstract] "We also identified a strong correlation between our empirical results and the GLUE scores obtained for these language models"; [section] "The comparison reveals a robust correlation between GLUE scores and the reported metrics, with all absolute correlation scores exceeding 0.8"
- **Break condition**: If PLMs optimized for GLUE neglect prosody-specific linguistic cues (e.g., focus on classification over generation), correlation may break.

## Foundational Learning

- **Concept**: Pretrained language models (PLMs) as contextual encoders.
  - **Why needed here**: TTS systems rely on PLMs to generate word-level embeddings capturing syntax/semantics that inform prosody and pause prediction.
  - **Quick check question**: What is the difference between subword tokenization and word-level embeddings in PLMs?

- **Concept**: Logarithmic model scaling in deep learning.
  - **Why needed here**: The study observes diminishing returns in TTS quality as PLM size increases; understanding this scaling law guides efficient model selection.
  - **Quick check question**: How does model size growth typically affect performance—linearly, logarithmically, or exponentially?

- **Concept**: Euclidean distance as an objective metric for prosody quality.
  - **Why needed here**: The paper uses Euclidean distance between predicted and ground-truth prosodic latents to quantify TTS quality; knowing its limitations helps interpret results.
  - **Quick check question**: What are the pros and cons of using Euclidean distance versus likelihood-based metrics for evaluating latent representations?

## Architecture Onboarding

- **Component map**: Text → Tokenizer → PLM → Word-level embeddings → Bi-LSTM → Output (prosody or pause)
- **Critical path**: Text → Tokenizer → PLM → Bi-LSTM → Output (prosody or pause). Any bottleneck in PLM inference directly impacts TTS latency.
- **Design tradeoffs**:
  - Model size vs. inference speed: Larger PLMs improve quality but increase latency (see Fig. 3).
  - Fine-tuning vs. fixed PLM: Fine-tuning adapts PLM to TTS but risks overfitting; freezing preserves pretraining benefits but may limit adaptation.
  - Word-level vs. subword-level: Word-level embeddings simplify prosody modeling but may lose subword nuance.
- **Failure signatures**:
  - Undertrained large PLMs: Poor performance despite size (e.g., Funnel-intermediate outlier).
  - Mismatched tokenization: Subword embeddings misaligned with word-level prosody labels.
  - Insufficient fine-tuning: PLM fails to adapt to TTS-specific prosody cues.
- **First 3 experiments**:
  1. **Sanity check**: Replace RoBERTa-base with DistilBERT; verify inference speed improves while quality drops slightly.
  2. **Scaling test**: Train prosody predictor with ELECTRA-small vs. RoBERTa-large; measure Euclidean distance and MUSHRA gap.
  3. **Task sensitivity**: Train pause predictor with BERT-tiny, ELECTRA-small, and RoBERTa-base; compare F-0.5 scores to confirm size insensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do PLMs from different pretraining methodologies (e.g., masked language modeling vs. replaced token detection) compare for TTS tasks beyond prosody and pause prediction?
- **Basis in paper**: [explicit] The paper compares 15 PLMs with different pretraining techniques (BERT, RoBERTa, ELECTRA, Funnel-Transformer) and notes ELECTRA's strong performance on pause prediction, but doesn't extensively explore methodological differences across TTS tasks.
- **Why unresolved**: The study focused on two specific TTS tasks (prosody and pause prediction) and didn't comprehensively test other TTS-related tasks like emotion transfer, speaker adaptation, or voice conversion.
- **What evidence would resolve it**: Comparative experiments applying these PLMs to a broader range of TTS tasks, measuring performance differences based on pretraining methodology rather than just size.

### Open Question 2
- **Question**: Do the observed logarithmic relationship between model size and TTS quality hold for other languages or multilingual TTS systems?
- **Basis in paper**: [explicit] The paper uses US English data and mentions XLM-RoBERTa as a multilingual model but doesn't explore cross-linguistic generalization of the size-quality relationship.
- **Why unresolved**: The study's dataset was limited to US English, and multilingual capabilities weren't thoroughly investigated despite including a multilingual model.
- **What evidence would resolve it**: Systematic experiments testing the same 15 PLMs on TTS tasks across multiple languages, comparing performance scaling patterns.

### Open Question 3
- **Question**: What is the optimal balance between PLM size and computational efficiency for practical TTS deployment?
- **Basis in paper**: [explicit] The paper includes batch inference time analysis showing linear scaling with model size, but doesn't optimize for the trade-off between quality and efficiency.
- **Why unresolved**: The study presents quality metrics and inference times separately without determining the optimal size-efficiency trade-off for production systems.
- **What evidence would resolve it**: Cost-quality analysis determining the most efficient model size that achieves acceptable TTS quality for specific deployment scenarios (mobile, cloud, real-time).

## Limitations

- Limited speaker diversity (4 female US English speakers) and domain specificity may restrict generalizability to other languages, demographics, or text domains.
- Focus on two specific TTS tasks within the eCat framework may miss PLM performance variations in other TTS architectures or tasks.
- Logarithmic size-performance relationship may not hold across different pretraining objectives or languages.

## Confidence

- **High confidence**: The logarithmic relationship between PLM size and prosody prediction quality is supported by consistent empirical results showing larger models reducing Euclidean distance to ground-truth representations.
- **Medium confidence**: The size insensitivity of pause prediction is supported by the data but has limited external validation.
- **Medium confidence**: The correlation between GLUE scores and TTS performance is statistically robust but relies on indirect inference about linguistic feature transferability.

## Next Checks

1. **Dataset Generalization Test**: Replicate the prosody prediction experiment using a publicly available multi-speaker, multi-domain dataset to verify if the logarithmic size-performance relationship holds beyond the internal dataset.

2. **Cross-Task Transfer Validation**: Train a pause predictor using PLMs that performed well on GLUE but were not specifically tested for pause prediction in this study, then measure if the observed correlation between GLUE scores and TTS performance generalizes to this additional task.

3. **Language and Architecture Robustness Check**: Implement the same comparative analysis for a morphologically rich language and a different TTS architecture to test whether model size sensitivity patterns and GLUE correlation hold across linguistic and architectural boundaries.