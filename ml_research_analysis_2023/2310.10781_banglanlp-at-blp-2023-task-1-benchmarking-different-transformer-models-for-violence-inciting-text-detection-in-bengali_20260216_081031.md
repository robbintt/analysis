---
ver: rpa2
title: 'BanglaNLP at BLP-2023 Task 1: Benchmarking different Transformer Models for
  Violence Inciting Text Detection in Bengali'
arxiv_id: '2310.10781'
source_url: https://arxiv.org/abs/2310.10781
tags:
- violence
- task
- data
- text
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents experiments with transformer models for detecting
  violence-inciting text in Bengali. The authors fine-tuned multilingual-e5-base and
  other transformer architectures on a dataset of YouTube comments related to violent
  incidents.
---

# BanglaNLP at BLP-2023 Task 1: Benchmarking different Transformer Models for Violence Inciting Text Detection in Bengali

## Quick Facts
- arXiv ID: 2310.10781
- Source URL: https://arxiv.org/abs/2310.10781
- Reference count: 3
- Best model achieved 68.11% macro F1 on test set

## Executive Summary
This paper presents experiments with transformer models for detecting violence-inciting text in Bengali. The authors fine-tuned multilingual-e5-base and other transformer architectures on a dataset of YouTube comments related to violent incidents. They explored data augmentation using paraphrasing to address the limited size of the dataset. The best performance was achieved with multilingual-e5-base, yielding a macro F1 score of 74.6% on the development set and 68.11% on the test set. Results show that data augmentation improves model performance, and multilingual-e5-base outperformed other models. The study highlights the challenges of detecting violence in morphologically rich languages like Bengali and demonstrates the effectiveness of large multilingual models for this task.

## Method Summary
The study fine-tuned several transformer models including multilingual-e5-base, multilingual-e5-large, BanglaBERT, and bert-base-multilingual-cased on a dataset of YouTube comments about violent incidents in Bangladesh and West Bengal. Data augmentation was performed using the bnaug2 library to paraphrase existing samples, adding approximately 500 synthetic samples per class (except Direct Violence which had 389). The augmented dataset was used to train models with standard hyperparameters (learning rate 5e-5, batch size 32, 4 epochs for e5 models). Traditional machine learning approaches using TF-IDF features with Logistic Regression, Naive Bayes, SGD, Majority Voting, and XGBoost stacking were also evaluated for comparison.

## Key Results
- multilingual-e5-base achieved the highest macro F1 score of 74.6% on the development set
- Data augmentation consistently improved model performance across all transformer architectures
- multilingual-e5-base outperformed other transformer models including BanglaBERT and bert-base-multilingual-cased
- The best model achieved 68.11% macro F1 on the test set

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data augmentation improves model performance when training data is limited.
- Mechanism: Paraphrasing creates synthetic training examples that resemble real data, increasing dataset size and diversity.
- Core assumption: Paraphrased sentences preserve the semantic label of the original sentences while introducing linguistic variation.
- Evidence anchors:
  - [abstract] "We studied the impact of data augmentation when there is a limited dataset available."
  - [section] "Table 5 highlights the results obtained for different finetuned models with and without applying data augmentation during the development phase. We observed that data augmentation positively impacted model performance, providing significant gains in macro-F1 score."
- Break condition: If paraphrased data introduces noise or changes the meaning, causing the model to learn incorrect patterns.

### Mechanism 2
- Claim: Multilingual models pretrained on large corpora generalize well to low-resource languages like Bangla.
- Mechanism: Models like multilingual-e5-base and multilingual-e5-large capture cross-lingual representations that transfer to Bangla despite limited in-language training data.
- Core assumption: Pre-training on diverse languages provides shared linguistic knowledge that benefits downstream tasks in related languages.
- Evidence anchors:
  - [abstract] "Our quantitative results show that finetuning a multilingual-e5-base model performed the best in our task compared to other transformer-based architectures."
- Break condition: If the target language has significantly different linguistic structure from languages in pretraining data.

### Mechanism 3
- Claim: Transformer models outperform traditional ML algorithms for contextual text classification tasks.
- Mechanism: Transformers capture bidirectional context and long-range dependencies better than bag-of-words approaches used in traditional ML.
- Core assumption: The task requires understanding sentence-level context rather than just keyword matching.
- Evidence anchors:
  - [abstract] "Our quantitative results show that finetuning a multilingual-e5-base model performed the best in our task compared to other transformer-based architectures."
- Break condition: If the task can be solved effectively with simpler features that traditional ML can capture.

## Foundational Learning

- Concept: Data augmentation techniques in NLP
  - Why needed here: Limited dataset size requires synthetic data generation to improve model generalization
  - Quick check question: What are the main risks of using paraphrasing for data augmentation in violence detection tasks?

- Concept: Multilingual model pretraining
  - Why needed here: Bangla is a low-resource language requiring transfer learning from multilingual models
  - Quick check question: How do multilingual models handle language-specific morphological features?

- Concept: Transformer architecture fundamentals
  - Why needed here: Understanding attention mechanisms and pretraining objectives for model selection
  - Quick check question: What makes transformers particularly effective for sequence classification compared to RNNs?

## Architecture Onboarding

- Component map: Input preprocessing -> Tokenization -> Embedding layer -> Transformer layers -> Classification head -> Evaluation
- Critical path: Data -> Augmentation -> Model finetuning -> Evaluation -> Error analysis
- Design tradeoffs:
  - Augmentation quantity vs quality: More samples improve coverage but may introduce noise
  - Model size vs performance: Larger models (e5-large) vs computational cost and overfitting risk
  - Fine-tuning vs feature extraction: Full fine-tuning captures task-specific patterns but requires more data
- Failure signatures:
  - High variance in predictions across similar inputs
  - Confusion between passive and direct violence categories
  - Poor generalization to unseen vocabulary
- First 3 experiments:
  1. Baseline: Traditional ML (Logistic Regression) on TF-IDF features without augmentation
  2. Single model: BanglaBERT fine-tuning with standard hyperparameters
  3. Best practice: multilingual-e5-base with data augmentation and prompt engineering

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do large language models like ChatGPT perform on violence-inciting text detection in Bengali compared to the multilingual-e5-base model?
- Basis in paper: The authors mention they did not use ChatGPT in their experimentation due to pricing constraints.
- Why unresolved: The paper does not include experiments with ChatGPT or other large language models for this task.
- What evidence would resolve it: Direct comparison of ChatGPT's performance with multilingual-e5-base on the same dataset, measuring macro F1 scores.

### Open Question 2
- Question: What is the impact of hyperparameter optimization on the performance of transformer models for violence-inciting text detection in Bengali?
- Basis in paper: The authors state they did not perform exhaustive hyperparameter optimization due to compute constraints.
- Why unresolved: The paper does not explore the effects of different hyperparameter settings on model performance.
- What evidence would resolve it: Systematic experiments varying hyperparameters (learning rate, batch size, number of epochs) and measuring their impact on macro F1 scores.

### Open Question 3
- Question: How effective are data augmentation techniques other than paraphrasing for improving model performance on violence-inciting text detection in Bengali?
- Basis in paper: The authors only used paraphrasing for data augmentation and did not explore other techniques.
- Why unresolved: The paper does not investigate the effectiveness of alternative data augmentation methods.
- What evidence would resolve it: Experiments comparing the performance of models trained with different data augmentation techniques (e.g., back-translation, synonym replacement) on the same dataset.

## Limitations
- Small dataset size (1,701 training samples) limits generalizability of findings
- Data augmentation using bnaug2 may introduce semantic drift in synthetic samples
- No ablation studies to isolate contributions of multilingual pretraining versus data augmentation
- Specific prompt engineering details for multilingual-e5-base not fully specified
- Limited testing on out-of-domain content reduces real-world deployment readiness claims

## Confidence
- High confidence: multilingual-e5-base outperforms other transformer architectures (74.6% dev F1 vs 68.11% test F1)
- Medium confidence: data augmentation effectiveness claims need more rigorous validation
- Low confidence: generalizability to other violence detection contexts remains untested

## Next Checks
1. **Ablation study on augmentation impact**: Retrain multilingual-e5-base with varying augmentation ratios (0%, 25%, 50%, 75%, 100% of proposed 500 samples per class) to determine the optimal augmentation level and assess whether gains persist beyond a certain threshold.

2. **Cross-domain evaluation**: Test the best-performing model on an independent violence detection dataset from a different source (e.g., news articles, Twitter posts, or forum discussions) to evaluate real-world generalization and identify domain-specific failure modes.

3. **Error analysis and confusion pattern investigation**: Conduct detailed error analysis focusing on the Passive Violence vs Non-Violence confusion, extracting and analyzing specific n-grams and linguistic patterns that consistently mislead the model, then implement targeted mitigation strategies.