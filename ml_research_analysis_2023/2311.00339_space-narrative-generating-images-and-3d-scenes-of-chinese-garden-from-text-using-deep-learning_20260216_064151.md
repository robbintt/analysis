---
ver: rpa2
title: 'Space Narrative: Generating Images and 3D Scenes of Chinese Garden from Text
  using Deep Learning'
arxiv_id: '2311.00339'
source_url: https://arxiv.org/abs/2311.00339
tags:
- garden
- images
- text
- image
- paintings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study proposes a method for generating images and 3D scenes
  of traditional Chinese gardens from textual descriptions using deep learning. It
  addresses the challenge of reconstructing historical gardens when firsthand material
  is scarce.
---

# Space Narrative: Generating Images and 3D Scenes of Chinese Garden from Text using Deep Learning

## Quick Facts
- arXiv ID: 2311.00339
- Source URL: https://arxiv.org/abs/2311.00339
- Reference count: 14
- Primary result: Deep learning model generates Ming Dynasty garden images from text, enabling 3D VR exploration

## Executive Summary
This paper introduces a novel approach to reconstructing traditional Chinese gardens in 3D using deep learning and text-to-image generation. The method leverages a fine-tuned latent diffusion model (Stable Diffusion v1-5) with Low-Rank Adaptation (LoRA) on a dataset of 1,182 Ming Dynasty garden paintings paired with their inscriptions. The model successfully generates garden images consistent with textual descriptions and the style of historical landscape paintings. These generated images are then transformed into panoramas and integrated into a 3D environment using Unity, allowing users to virtually explore reconstructed historical gardens.

## Method Summary
The approach involves fine-tuning a pre-trained Stable Diffusion v1-5 model using LoRA on a curated dataset of Ming Dynasty garden paintings and their textual inscriptions. The fine-tuned model generates images from descriptive text prompts. These images are upscaled, stitched into panoramas, and imported into Unity 3D to create walkable 3D scenes. The pipeline addresses the challenge of reconstructing historical gardens when firsthand material is scarce, offering a new method for studying and preserving cultural heritage through multimodal AI.

## Key Results
- Fine-tuned diffusion model generates garden images consistent with Ming Dynasty painting style and textual descriptions
- Generated images successfully transformed into panoramas and integrated into Unity 3D for virtual exploration
- Method demonstrates potential for reconstructing historical landscapes when direct material is unavailable

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-to-image diffusion model maps descriptive text to stylistically accurate Ming Dynasty garden paintings
- Mechanism: Pre-trained Stable Diffusion v1-5 fine-tuned with LoRA on 1,182 image-text pairs of Ming Dynasty gardens
- Core assumption: Pre-trained model has generalizable synthesis patterns; dataset is representative of target style and spatial semantics
- Evidence anchors: Abstract mentions latent text-to-image diffusion model learning mapping; section confirms Stable Diffusion v1-5 usage
- Break condition: Insufficient dataset diversity or vague text descriptions lead to incoherent spatial layouts

### Mechanism 2
- Claim: LoRA fine-tuning enables efficient adaptation to niche domains without overfitting
- Mechanism: Freezing original model weights while training low-rank update matrices in U-Net attention blocks
- Core assumption: Rank-decomposition matrices capture domain-specific features while base model retains general knowledge
- Evidence anchors: Section details 32 layers of rank-4 update matrices; mentions LoRA for porcelain classification
- Break condition: Too low rank or too few update layers limit stylistic nuance capture

### Mechanism 3
- Claim: Generated images transform into immersive 3D panoramas for virtual garden exploration
- Mechanism: Generated 512x512 images upscaled and stitched into panoramas, imported into Unity 3D
- Core assumption: Spatial coherence in generated images sufficient for seamless stitching and navigation
- Evidence anchors: Abstract mentions panorama creation and Unity 3D scene; section describes immersive experience
- Break condition: Varying horizon lines or perspective distortions cause visual artifacts and disorienting navigation

## Foundational Learning

- **Text-to-image diffusion models and latent space representations**
  - Why needed here: Understanding progressive denoising of latent vectors conditioned on text embeddings explains garden image generation
  - Quick check question: What is the role of the CLIP text encoder in the diffusion model pipeline?

- **LoRA (Low-Rank Adaptation) and parameter-efficient fine-tuning**
  - Why needed here: Knowing how LoRA modifies attention layers with rank-decomposition matrices explains memory-efficient style adaptation
  - Quick check question: How does LoRA differ from full fine-tuning in terms of parameter updates?

- **Panorama stitching and 3D environment integration**
  - Why needed here: Understanding 2D-to-3D conversion explains how generated images become VR experiences
  - Quick check question: What challenges arise when stitching images with differing horizon lines into a seamless panorama?

## Architecture Onboarding

- **Component map**: Data collection → Pre-trained model (Stable Diffusion v1-5) → LoRA fine-tuning → Image generation → Panorama stitching → Unity 3D scene creation
- **Critical path**: Dataset creation → LoRA fine-tuning → Image generation → Panorama stitching → Unity scene creation
- **Design tradeoffs**:
  - LoRA reduces memory usage but may limit fine-grained stylistic adaptation
  - Upscaling and stitching 512x512 images introduces quality loss and alignment challenges
  - Unity 3D integration is flexible but requires manual scene construction and may suffer from perspective misalignment
- **Failure signatures**:
  - Inconsistent spatial layouts → Poor panorama coherence
  - Low cosine similarity scores → Inadequate text-to-image alignment
  - Visual artifacts in Unity → Horizon misalignment or stitching errors
- **First 3 experiments**:
  1. Fine-tune on 50 image-text pairs, generate images for held-out prompt, evaluate cosine similarity
  2. Generate four sequential scene images for garden path description, test panorama stitching manually
  3. Import stitched panorama into Unity, verify basic walkability and visual continuity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the model's ability to accurately generate specific spatial elements with fixed geometric forms across multiple tasks?
- Basis in paper: [explicit] Mentions struggle to accurately generate specific spatial elements with fixed geometric forms in multiple tasks
- Why unresolved: No solution or method provided to address this limitation
- What evidence would resolve it: Successful implementation of techniques that consistently generate accurate spatial elements across different tasks, validated through comparative analysis with ground truth data

### Open Question 2
- Question: How can we enhance the three-dimensional modeling process within Unity3D to create more realistic user experiences in the generated garden scenes?
- Basis in paper: [explicit] States that differing horizons in generated images pose challenges during Unity3D conversion process
- Why unresolved: No solution or method provided to address this limitation
- What evidence would resolve it: Successful implementation of improved 3D modeling techniques that seamlessly integrate ground and sky components, validated through user testing and feedback

### Open Question 3
- Question: How can we establish geometric and semantic relationships between key spatial elements in the current image using graph machine learning to generate garden images with precise spatial orientation?
- Basis in paper: [explicit] Suggests identifying key spatial elements and establishing their geometric and semantic relationships using graph machine learning
- Why unresolved: No solution or method provided to address this direction
- What evidence would resolve it: Successful implementation of graph machine learning techniques that accurately identify and relate spatial elements, validated through comparison with expert-annotated datasets

## Limitations

- Reliance on curated dataset of 1,182 Ming Dynasty garden paintings raises concerns about representativeness and potential bias
- Text descriptions guiding the model are not disclosed, making it difficult to assess prompt specificity and richness
- Panorama stitching and Unity 3D integration lack detailed technical specifications, introducing uncertainty about 3D experience quality
- No quantitative evaluation of user experience or immersion in the virtual environment

## Confidence

- **High confidence**: Text-to-image generation capability using LoRA fine-tuning on Stable Diffusion v1-5 is well-supported by methodology and experimental results
- **Medium confidence**: Transformation of generated images into panoramas and integration into Unity 3D is plausible but lacks sufficient technical detail for full validation
- **Low confidence**: Claims about historical accuracy and authenticity of generated gardens are not empirically tested, relying instead on stylistic consistency

## Next Checks

1. **Dataset diversity audit**: Analyze the 1,182 image-text pairs for representation of different garden types, spatial layouts, and architectural features to ensure the model is not overfitting to a narrow subset of garden styles
2. **Prompt engineering experiment**: Test the model with a range of text prompts (from detailed to vague) to assess the robustness of image generation and identify the minimum descriptive requirements for coherent spatial layouts
3. **3D coherence evaluation**: Conduct a user study to evaluate the immersion and navigability of the Unity 3D scenes, focusing on visual continuity, horizon alignment, and absence of disorienting artifacts