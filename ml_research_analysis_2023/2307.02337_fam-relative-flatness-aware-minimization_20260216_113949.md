---
ver: rpa2
title: 'FAM: Relative Flatness Aware Minimization'
arxiv_id: '2307.02337'
source_url: https://arxiv.org/abs/2307.02337
tags:
- flatness
- training
- neural
- generalization
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a relative flatness aware minimization (FAM)
  regularizer for improving generalization in deep neural networks. The method is
  based on a theoretical measure of relative flatness that avoids the reparameterization
  curse and connects flatness to generalization.
---

# FAM: Relative Flatness Aware Minimization

## Quick Facts
- arXiv ID: 2307.02337
- Source URL: https://arxiv.org/abs/2307.02337
- Reference count: 22
- One-line primary result: Relative flatness aware minimization (FAM) improves generalization across image classification, medical reconstruction, and language tasks while requiring Hessian computation of only a single network layer.

## Executive Summary
This paper proposes a relative flatness aware minimization (FAM) regularizer for improving generalization in deep neural networks. The method is based on a theoretical measure of relative flatness that avoids the reparameterization curse and connects flatness to generalization. FAM requires computing the Hessian only of a single layer, making it efficient to compute. The authors show that FAM improves test accuracy on image classification tasks (CIFAR10, CIFAR100, SVHN, and FashionMNIST) on ResNet18, WideResNET28-10, and EffNet-B7 compared to baselines without regularization and SAM. They also demonstrate improvements in medical shape reconstruction tasks using autoencoders and stabilization of language model finetuning.

## Method Summary
FAM regularizer computes relative flatness by measuring the change in loss under small perturbations of weights in a chosen layer, specifically using the Hessian of that layer's weights. The method penalizes large changes in loss, forcing the network to maintain consistent output for small input variations. The regularizer is combined with standard loss functions and requires tuning a regularization coefficient λ. The Hessian computation is the computational bottleneck, but limiting it to a single layer makes it feasible for large networks. The approach is theoretically grounded in connections between relative flatness, feature robustness, and generalization.

## Key Results
- FAM improves test accuracy on CIFAR10, CIFAR100, SVHN, and FashionMNIST compared to baselines and SAM
- FAM enhances medical shape reconstruction quality in autoencoder applications (improved Dice similarity coefficient and Hausdorff distance)
- FAM stabilizes language model finetuning on RTE task, improving accuracy over baseline and SAM

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Relative flatness in a single layer induces robustness to noise in that layer's representation, which transfers to better generalization of the full network.
- Mechanism: The regularizer penalizes large changes in loss under small perturbations of weights in the chosen layer. This forces the network to maintain consistent output for small input variations, effectively making the representation at that layer more stable.
- Core assumption: Locally constant labels in the representation space—small perturbations to the representation do not change the class label.
- Evidence anchors: [abstract] "recent theoretical work suggests that a particular relative flatness measure can be connected to generalization and solves the reparameterization curse"; [section] "Petzka et al. (2021) have shown that relative flatness in this layer corresponds to robustness to noise on the representation produced by this layer"

### Mechanism 2
- Claim: Optimizing only one layer's flatness is sufficient to improve overall generalization because that layer's robustness propagates through the network.
- Mechanism: By making the representation at a critical layer (e.g., penultimate) robust, the subsequent layers receive cleaner signals, reducing the chance of fitting noise in earlier layers.
- Core assumption: The chosen layer is sufficiently deep that its robustness meaningfully influences downstream learning.
- Evidence anchors: [section] "Petzka et al. (2021) have shown that relative flatness in this layer corresponds to robustness to noise on the representation produced by this layer"; [section] "it suffices to compute relative flatness wrt. a single layer, this regularizer and its gradient can be computed much more efficiently than any full-Hessian based flatness measure"

### Mechanism 3
- Claim: The regularizer is computationally efficient because it only requires Hessian computation for a single layer, avoiding expensive full-network curvature estimation.
- Mechanism: Computing the Hessian of a single layer's weights has complexity O(d²m²), which is much lower than computing the full Hessian across all layers.
- Core assumption: The Hessian computation for one layer is feasible and does not introduce prohibitive overhead.
- Evidence anchors: [section] "requires computing the Hessian only of a single layer of the network, which makes it applicable to large neural networks"; [section] "the additional computational costs for using the FAM regularizer is in O(d²m²) per iteration"

## Foundational Learning

- Concept: Hessian matrix and its role in measuring curvature of the loss landscape
  - Why needed here: FAM relies on computing the Hessian of a single layer to quantify flatness; understanding Hessian properties is essential to grasp why this measure works.
  - Quick check question: What does a large eigenvalue of the Hessian at a minimum indicate about the loss surface?

- Concept: Reparameterization invariance and the reparameterization curse
  - Why needed here: FAM's theoretical motivation stems from solving the reparameterization curse that affects other flatness measures; understanding this issue clarifies why FAM is theoretically grounded.
  - Quick check question: Why do traditional flatness measures fail under certain reparameterizations of a neural network?

- Concept: Generalization gap and its decomposition into feature robustness and representativeness
  - Why needed here: The paper's theoretical foundation connects relative flatness to generalization via feature robustness; understanding this decomposition explains the link between the regularizer and performance.
  - Quick check question: Under what assumption does feature robustness govern the generalization gap?

## Architecture Onboarding

- Component map: Loss function -> Neural network with identifiable feature layer -> Hessian computation module -> Regularization coefficient λ -> Optimizer

- Critical path: 1. Forward pass through network; 2. Compute loss; 3. Compute Hessian of chosen layer; 4. Compute relative flatness regularizer; 5. Backpropagate combined loss + regularizer; 6. Update weights

- Design tradeoffs:
  - Full Hessian vs. Hutchinson's trace approximation: accuracy vs. memory/time
  - Choice of regularization layer: deeper layers may yield better generalization but higher computational cost
  - λ tuning: too small → negligible effect; too large → over-regularization and degenerate reconstructions

- Failure signatures:
  - Training loss decreases but validation performance stalls or degrades (over-regularization)
  - Extremely slow training due to Hessian computation overhead
  - No improvement over baseline (wrong layer choice or λ too small)

- First 3 experiments:
  1. CIFAR-10 with ResNet18, baseline vs. FAM (λ = 0.1), compare test accuracy
  2. SVHN with WideResNet28-10, baseline vs. FAM (λ = 0.1), compare test error
  3. Skull reconstruction with autoencoder, baseline Dice loss vs. Dice + FAM (λ = 0.02), compare DSC and HD metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of layer for FAM regularization affect the final model performance, and can using multiple layers provide additional benefits?
- Basis in paper: [inferred] The paper mentions that relative flatness in any one layer suffices for good generalization, but leaves a comprehensive empirical study of the impact of the choice of layer (or using multiple layers) for future work.
- Why unresolved: The authors acknowledge the need for further investigation into the impact of different layer choices and the potential benefits of using multiple layers.
- What evidence would resolve it: Empirical studies comparing the performance of FAM regularization when applied to different layers or combinations of layers, with analysis of the resulting generalization capabilities.

### Open Question 2
- Question: How does the assumption of locally constant labels in the representation affect the connection between flatness and generalization in tasks where this assumption is violated?
- Basis in paper: [explicit] The paper states that the connection between flatness and generalization relies on the assumption of locally constant labels in the representation, which may not hold in tasks where labels change under small perturbations of features.
- Why unresolved: The authors note that this assumption is a limitation of their approach and suggest that it would be interesting to verify this empirically and expand the study to further tasks.
- What evidence would resolve it: Empirical studies on tasks where the assumption of locally constant labels is violated, comparing the performance of FAM regularization to other methods and analyzing the impact on generalization.

### Open Question 3
- Question: How can the computational efficiency and memory usage of FAM regularization be improved, particularly for convolutional layers?
- Basis in paper: [explicit] The authors mention that current implementation of FAM regularization can be computationally expensive and limits applicability to convolutional layers due to the large number of parameters.
- Why unresolved: The authors acknowledge the need for improvements in computational efficiency and memory usage, as well as determining the correct structure of FAM regularization for convolutional layers.
- What evidence would resolve it: Development of more efficient algorithms or implementations of FAM regularization, particularly for convolutional layers, with analysis of the resulting computational time and memory usage.

## Limitations
- The method requires careful tuning of the regularization coefficient λ, which varies significantly across different tasks
- Computational overhead from Hessian computation can exceed baseline methods by 20-40%, despite being limited to a single layer
- The choice of regularization layer is heuristic without systematic justification, and may not be optimal for all architectures

## Confidence
**High Confidence**: The FAM formulation is mathematically sound, and the Hessian computation approach is correct. The improvements over baseline methods are demonstrated across multiple tasks and architectures with statistical significance.

**Medium Confidence**: The theoretical connection between relative flatness and generalization is well-motivated but relies on assumptions about feature robustness that may not hold universally. The mechanism by which single-layer regularization improves overall generalization is plausible but not exhaustively validated.

**Low Confidence**: The claim that FAM is "easy to implement" is questionable given the computational overhead and hyperparameter sensitivity. The selection of the regularization layer is heuristic without systematic justification.

## Next Checks
1. **Layer Selection Sensitivity Analysis**: Systematically test FAM with different layer choices (not just penultimate) across multiple architectures to determine if the heuristic layer selection is optimal or if task-specific layer selection yields better results.

2. **Computational Overhead Benchmark**: Conduct large-scale experiments measuring wall-clock time and memory usage for FAM versus baseline methods across varying batch sizes and network widths to quantify the practical computational cost.

3. **Mechanism Validation through Ablation**: Design experiments that test the feature robustness hypothesis directly by measuring representation stability under input perturbations when FAM is applied versus when it is not, correlating these measurements with generalization performance.