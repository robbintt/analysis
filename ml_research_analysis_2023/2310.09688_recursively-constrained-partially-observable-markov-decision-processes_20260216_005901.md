---
ver: rpa2
title: Recursively-Constrained Partially Observable Markov Decision Processes
arxiv_id: '2310.09688'
source_url: https://arxiv.org/abs/2310.09688
tags:
- policy
- cost
- policies
- admissible
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of pathological behavior in Constrained
  Partially Observable Markov Decision Processes (C-POMDPs), where optimal policies
  may intentionally violate cost constraints. The authors introduce Recursively-Constrained
  POMDPs (RC-POMDPs), which add history-dependent cost constraints to C-POMDPs to
  preserve Bellman's principle of optimality.
---

# Recursively-Constrained Partially Observable Markov Decision Processes

## Quick Facts
- arXiv ID: 2310.09688
- Source URL: https://arxiv.org/abs/2310.09688
- Reference count: 40
- Primary result: Introduces RC-POMDPs to address pathological constraint violations in C-POMDPs, with ARCS algorithm showing competitive performance without violations

## Executive Summary
This paper addresses a fundamental issue in Constrained Partially Observable Markov Decision Processes (C-POMDPs) where optimal policies may intentionally violate cost constraints in pursuit of higher rewards. The authors introduce Recursively-Constrained POMDPs (RC-POMDPs) that add history-dependent cost constraints to prevent such pathological behavior. RC-POMDPs preserve Bellman's principle of optimality, ensuring that optimal policies are deterministic and satisfy constraints at all reachable belief states. The paper presents the ARCS algorithm, which uses point-based dynamic programming to solve RC-POMDPs efficiently, demonstrating through experiments that it achieves competitive cumulative rewards while avoiding constraint violations that plague C-POMDP policies.

## Method Summary
The paper extends POMDPs.jl with recursive cost constraints to create the RC-POMDP formulation, implementing belief-admissible cost states and the ARCS algorithm. ARCS incrementally builds a policy tree using heuristic and random sampling, computes admissible horizons, and prunes suboptimal nodes. Benchmark environments including CE, C-Tiger, CRS, and Tunnels are implemented as RC-POMDPs in Julia/POMDPs.jl. The method compares ARCS against CGCP and CPBVI across these environments, measuring violation rates, cumulative rewards, and costs over multiple simulation runs to demonstrate RC-POMDPs' superiority in avoiding pathological constraint violations.

## Key Results
- RC-POMDPs guarantee deterministic optimal policies that satisfy Bellman's principle of optimality
- ARCS algorithm computes policies with competitive cumulative rewards compared to C-POMDP solutions
- Experimental results show RC-POMDP policies avoid constraint violations that are prevalent in C-POMDP policies
- The pathological behavior of constraint violations is demonstrated to be inherent to the C-POMDP formulation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RC-POMDPs preserve Bellman's principle of optimality by imposing history-dependent cost constraints that propagate recursively
- Mechanism: The RC-POMDP formulation adds recursive cost constraints that ensure the cumulative expected cost from the initial belief remains below the threshold at every future belief state
- Core assumption: The evolution of belief-admissible cost states (b, d(h)) is Markovian, allowing standard dynamic programming techniques to be applied
- Evidence anchors:
  - [abstract]: "RC-POMDPs always have deterministic optimal policies and that optimal policies obey Bellman's principle of optimality."
  - [section]: "We show that, unlike C-POMDPs, RC-POMDPs always have deterministic optimal policies, and that optimal policies obey Bellman's principle of optimality."
  - [corpus]: Weak evidence - the corpus neighbors don't directly discuss recursive constraints or Bellman's principle in the context of constrained POMDPs
- Break condition: If the history-dependent admissible cost bounds cannot be computed recursively or the Markovian assumption fails, the Bellman backup operator would not be well-defined

### Mechanism 2
- Claim: Deterministic policies are sufficient for optimality in RC-POMDPs
- Mechanism: The RC-POMDP formulation ensures that any stochastic optimal policy can be represented as a mixture of deterministic admissible policies, where at least one deterministic policy achieves the same value as the stochastic policy
- Core assumption: The set of admissible policies is closed under convex combinations, and there exists at least one admissible policy for the RC-POMDP
- Evidence anchors:
  - [abstract]: "RC-POMDPs always have deterministic optimal policies"
  - [section]: "We prove that deterministic policies are sufficient for optimality in RC-POMDPs."
  - [corpus]: Weak evidence - the corpus neighbors don't directly discuss the sufficiency of deterministic policies in constrained POMDPs
- Break condition: If no admissible policy exists for the RC-POMDP, the theorem about deterministic optimal policies would not hold

### Mechanism 3
- Claim: The ARCS algorithm effectively solves RC-POMDPs by maintaining admissible policies and computing the admissible horizon k
- Mechanism: ARCS uses point-based dynamic programming with a policy tree representation, sampling beliefs and computing history-dependent admissible cost bounds. It prunes suboptimal policies and terminates when finding an admissible policy with an acceptable gap between upper and lower bounds
- Core assumption: The belief space can be adequately sampled and represented in a policy tree structure for practical computation
- Evidence anchors:
  - [abstract]: "We also present a point-based dynamic programming algorithm that synthesizes optimal policies for RC-POMDPs."
  - [section]: "We devise an algorithm that solves Problem 2 for RC-POMDPs with a scalar cost. We leave the multi-dimensional case for future work."
  - [corpus]: Weak evidence - the corpus neighbors don't directly discuss algorithms for solving constrained POMDPs with recursive constraints
- Break condition: If the belief space is too large or complex to be adequately sampled, or if the admissible horizon cannot be computed efficiently, the ARCS algorithm may fail to find good solutions

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: RC-POMDPs are an extension of POMDPs, so understanding POMDPs is fundamental to understanding RC-POMDPs
  - Quick check question: What are the key components of a POMDP tuple (S, A, O, T, R, Z, γ, b0)?

- Concept: Constrained optimization and Bellman's principle of optimality
  - Why needed here: The paper addresses how C-POMDPs violate Bellman's principle of optimality and how RC-POMDPs preserve it through recursive constraints
  - Quick check question: What is Bellman's principle of optimality, and how does it relate to dynamic programming?

- Concept: Dynamic programming and point-based value iteration
  - Why needed here: The ARCS algorithm uses point-based dynamic programming to solve RC-POMDPs efficiently
  - Quick check question: How does point-based value iteration differ from full dynamic programming in POMDPs?

## Architecture Onboarding

- Component map: RC-POMDP formulation -> ARCS algorithm -> Policy tree -> Belief sampling -> Admissible horizon computation

- Critical path:
  1. Formulate the RC-POMDP problem with appropriate cost constraints
  2. Initialize the ARCS algorithm with a minimum cost policy
  3. Iteratively sample beliefs and backup values in the policy tree
  4. Compute admissible horizons and prune suboptimal policies
  5. Terminate when an admissible policy with acceptable bounds is found

- Design tradeoffs:
  - Using deterministic vs. stochastic policies for optimality
  - Heuristic vs. random sampling strategies for belief exploration
  - Conservative vs. aggressive admissible horizon computation
  - Tree-based vs. α-vector policy representation

- Failure signatures:
  - No admissible policy exists for the given cost constraints
  - Belief space is too large to be adequately sampled
  - Admissible horizon computation becomes intractable
  - Policy tree grows too large to be managed efficiently

- First 3 experiments:
  1. Test the counterexample POMDP to verify that RC-POMDPs prevent constraint violations that occur in C-POMDPs
  2. Compare the violation rates and cumulative rewards of policies from ARCS vs. CGCP on the Tiger and RockSample environments
  3. Evaluate the impact of different belief sampling strategies (heuristic vs. random) on the performance of ARCS

## Open Questions the Paper Calls Out

- How do RC-POMDPs perform when cost constraints are chance-constraints rather than expected cost constraints?
- What is the computational complexity of ARCS in terms of belief space size and how does it scale with problem dimensionality?
- How does the sampling strategy affect the performance and convergence of ARCS?

## Limitations
- Scalability to high-dimensional belief spaces remains uncertain
- Computational complexity of admissible horizon computation in complex environments is not formally analyzed
- Limited to scalar cost constraints, with multi-dimensional extensions left for future work

## Confidence
- High confidence in the core mechanism that RC-POMDPs preserve Bellman's principle of optimality through recursive constraints
- Medium confidence in the sufficiency of deterministic policies and the effectiveness of ARCS
- Low confidence in the characterization of pathological behavior in C-POMDPs as inherent to the formulation

## Next Checks
1. Implement the counterexample POMDP from the paper and verify that C-POMDP policies exhibit constraint violations while RC-POMDP policies do not
2. Evaluate ARCS on larger RockSample instances (e.g., 16x16, 32x32) to assess scaling with problem size
3. Adapt the RC-POMDP formulation and ARCS algorithm to handle multi-dimensional cost constraints and test on problems with competing objectives