---
ver: rpa2
title: On the Calibration of Multilingual Question Answering LLMs
arxiv_id: '2311.08669'
source_url: https://arxiv.org/abs/2311.08669
tags:
- calibration
- multilingual
- language
- languages
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the calibration of multilingual large language
  models (LLMs) on the task of question answering (QA). Calibration measures how well
  a model's confidence estimates align with its actual accuracy.
---

# On the Calibration of Multilingual Question Answering LLMs

## Quick Facts
- arXiv ID: 2311.08669
- Source URL: https://arxiv.org/abs/2311.08669
- Reference count: 15
- Key outcome: Incorporating a small set of cheaply translated multilingual samples during fine-tuning/calibration effectively enhances the calibration performance of multilingual QA models by almost 75%.

## Executive Summary
This paper investigates the calibration of multilingual large language models (LLMs) on question answering (QA) tasks. Calibration measures how well a model's confidence estimates align with its actual accuracy. The authors perform extensive experiments on four multilingual LLMs (mBERT, XLM-R, mT5, mBART) and LLaMa2, across diverse languages and tasks, including in-distribution, out-of-distribution, and cross-lingual settings. They find that multilingual QA models are poorly calibrated for languages other than English. To improve calibration, they investigate techniques like temperature scaling, label smoothing, and cross-lingual data augmentation via translated examples. Data augmentation via translation is found to be highly effective, improving calibration by almost 75%. They also study the effects of model size, language diversity, and example diversity on calibration. The authors conclude that incorporating a small set of cheaply translated multilingual samples during fine-tuning/calibration effectively enhances the calibration performance of multilingual QA models.

## Method Summary
The authors fine-tune four multilingual LLMs (mBERT, XLM-R, mT5, mBART) and LLaMa2 on SQuAD 1.1 for English QA, then evaluate their zero-shot cross-lingual performance on XQuAD, MLQA, and TyDiQA datasets. They apply calibration techniques including temperature scaling, label smoothing, few-shot learning, and data augmentation via translation. Temperature scaling optimizes a temperature parameter τ to rescale logits before softmax, improving calibration without affecting accuracy. Label smoothing adds noise to training labels to regularize confidence estimates. Data augmentation involves translating 9929 English QA samples into AR, DE, ES, HI, VI and fine-tuning on this multilingual data. They measure calibration using Expected Calibration Error (ECE) and exact match accuracy.

## Key Results
- Multilingual QA models exhibit poor calibration for languages other than English.
- Temperature scaling provides the greatest calibration benefits in most cases without affecting accuracy.
- Cross-lingual data augmentation via translated examples improves calibration by almost 75%.
- Calibration improves with increasing model size for multilingual LLMs.
- Multilingual data from different examples helps the most for calibration, followed by multilingual data from the same example.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual data augmentation via translated examples improves calibration across multiple languages, even those not explicitly fine-tuned on.
- Mechanism: Incorporating translated data from diverse languages during fine-tuning expands the model's exposure to multilingual patterns, allowing it to better align confidence estimates with accuracy across languages. This effect is stronger than using monolingual data from different examples or the same example.
- Core assumption: Translation preserves the semantic structure necessary for QA tasks, and fine-tuning on translated data enhances cross-lingual generalization of calibration properties.
- Evidence anchors:
  - [abstract]: "incorporating a small set of cheaply translated multilingual samples during fine-tuning/calibration effectively enhances the calibration performance."
  - [section]: "Multilingual data from different examples helps the most, followed by multilingual data from the same example, followed by monolingual data over different examples followed by monolingual data from the same examples."
  - [corpus]: Weak. No direct corpus evidence found; relies on experimental results within the paper.
- Break condition: If translations introduce significant semantic drift or if the target languages are too dissimilar from the source language, calibration gains may diminish or reverse.

### Mechanism 2
- Claim: Temperature scaling (TS) improves calibration without affecting accuracy by rescaling model logits.
- Mechanism: TS introduces a learned temperature parameter τ that scales the logits before applying softmax, effectively adjusting the model's confidence estimates. This recalibration aligns predicted probabilities with actual accuracy, reducing Expected Calibration Error (ECE).
- Core assumption: The relationship between logits and true probabilities is monotonic and can be corrected by a single scaling factor.
- Evidence anchors:
  - [abstract]: Mentions temperature scaling as one of the strategies investigated.
  - [section]: "TS does not affect accuracy by design, but it provides the greatest benefits in calibration in most cases."
  - [corpus]: Weak. No direct corpus evidence; relies on Guo et al. (2017) as referenced in the paper.
- Break condition: If the model's miscalibration is non-monotonic or if the validation set is not representative, TS may not improve or could worsen calibration.

### Mechanism 3
- Claim: Increasing model size improves both accuracy and calibration for multilingual LLMs.
- Mechanism: Larger models have greater capacity to capture complex multilingual patterns and nuances, leading to better generalization and more reliable confidence estimates across languages.
- Core assumption: Model capacity directly correlates with the ability to learn fine-grained language representations that support accurate and well-calibrated predictions.
- Evidence anchors:
  - [abstract]: Mentions studying the effect of model size on calibration.
  - [section]: "We show that confidence calibration improves with increasing model size, highlighting the important difference of Multilingual LLMs from ResNet architectures as studied in Guo et al. (2017)."
  - [corpus]: Weak. No direct corpus evidence; relies on experimental comparisons within the paper.
- Break condition: If larger models overfit to training data or if the benefits plateau beyond a certain size, further increases may not yield calibration improvements.

## Foundational Learning

- Concept: Expected Calibration Error (ECE)
  - Why needed here: ECE is the primary metric used to quantify how well the model's confidence estimates align with actual accuracy. Understanding ECE is essential to interpret calibration results.
  - Quick check question: If a model has high accuracy but also high ECE, what does that imply about its confidence estimates?

- Concept: Cross-lingual transfer
  - Why needed here: The paper focuses on zero-shot cross-lingual transfer, where models are fine-tuned on English data and tested on other languages. Understanding this concept is key to interpreting calibration performance across languages.
  - Quick check question: Why might a model achieve high accuracy in a target language but still exhibit poor calibration?

- Concept: Temperature scaling
  - Why needed here: Temperature scaling is a post-hoc calibration method investigated in the paper. Knowing how it works is necessary to understand its application and effectiveness.
  - Quick check question: How does temperature scaling adjust the model's confidence without retraining?

## Architecture Onboarding

- Component map: Pre-trained multilingual LLM -> Fine-tuning on English QA -> Calibration strategies (TS, label smoothing, data augmentation) -> Evaluation on multilingual test sets -> Metrics (EM, ECE, reliability diagrams)

- Critical path: 1. Load pre-trained multilingual LLM 2. Fine-tune on English QA dataset 3. Apply calibration strategy (optional) 4. Evaluate on multilingual test sets 5. Analyze calibration metrics (ECE, reliability diagrams)

- Design tradeoffs:
  - Model size vs. computational resources: Larger models improve calibration but require more resources.
  - Data augmentation vs. data quality: Translated data can improve calibration but may introduce noise if translations are poor.
  - Post-hoc calibration vs. fine-tuning: Temperature scaling is faster but may be less effective than fine-tuning with label smoothing or data augmentation.

- Failure signatures:
  - High ECE across languages indicates poor calibration.
  - Reliability diagrams deviating significantly from the diagonal suggest miscalibration.
  - If accuracy is high but calibration is poor, the model is overconfident.

- First 3 experiments:
  1. Fine-tune mBERT on SQuAD and evaluate ECE on XQuAD in multiple languages to establish baseline calibration.
  2. Apply temperature scaling to the fine-tuned mBERT and compare ECE before and after.
  3. Fine-tune mBERT with translated data augmentation and evaluate changes in ECE across languages.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the calibration of multilingual models vary with the similarity between the fine-tuning and target languages?
- Basis in paper: Inferred from the statement "We also conduct several ablation experiments to study the effect of language distances, language corpus size, and model size on calibration."
- Why unresolved: The paper does not provide specific experiments or results on how language similarity affects calibration.
- What evidence would resolve it: Conducting experiments that vary the similarity between fine-tuning and target languages, and measuring the resulting calibration performance.

### Open Question 2
- Question: What is the impact of different regularization techniques on the calibration of multilingual QA models?
- Basis in paper: Inferred from the statement "We study different dimensions of calibration in in-distribution, out-of-distribution, and cross-lingual transfer settings, and investigate strategies to improve it, including post-hoc methods and regularized fine-tuning."
- Why unresolved: The paper does not provide a detailed comparison of the effectiveness of various regularization techniques on calibration.
- What evidence would resolve it: Conducting experiments that compare the calibration performance of models using different regularization techniques, such as dropout, weight decay, and data augmentation.

### Open Question 3
- Question: How does the calibration of multilingual models compare to that of monolingual models for the same language?
- Basis in paper: Inferred from the statement "We also conduct several ablation experiments to study the effect of model size on calibration and how multilingual models compare with their monolingual counterparts for diverse tasks and languages."
- Why unresolved: The paper does not provide a direct comparison of the calibration performance of multilingual and monolingual models for the same language.
- What evidence would resolve it: Conducting experiments that compare the calibration performance of multilingual and monolingual models for the same language, using the same datasets and evaluation metrics.

### Open Question 4
- Question: How does the calibration of multilingual models vary with the size of the fine-tuning dataset?
- Basis in paper: Inferred from the statement "We also conduct several ablation experiments to study the effect of language corpus size, and model size on calibration."
- Why unresolved: The paper does not provide specific experiments or results on how the size of the fine-tuning dataset affects calibration.
- What evidence would resolve it: Conducting experiments that vary the size of the fine-tuning dataset, and measuring the resulting calibration performance.

### Open Question 5
- Question: How does the calibration of multilingual models vary with the complexity of the QA task?
- Basis in paper: Inferred from the statement "We perform extensive experiments, spanning encoder-only, encoder-decoder, and decoder-only QA models (size varying from 110M to 7B parameters) and diverse languages, spanning both high-resource and low-resource ones."
- Why unresolved: The paper does not provide specific experiments or results on how the complexity of the QA task affects calibration.
- What evidence would resolve it: Conducting experiments that vary the complexity of the QA task, and measuring the resulting calibration performance.

## Limitations

- The effectiveness of cross-lingual data augmentation via translation is supported by internal experiments but lacks independent replication or theoretical grounding.
- The specific implementation details for generative model calibration (mT5/mBART) and the choice of hyperparameters are not fully transparent.
- The claim that data augmentation via translation is "highly effective" and improves calibration by "almost 75%" appears to be based on internal comparisons without external validation.

## Confidence

**High Confidence**: Claims about poor baseline calibration for non-English languages across multiple models and datasets. The experimental setup and evaluation metrics are clearly specified, and the results are consistent across different model types and languages.

**Medium Confidence**: The effectiveness of temperature scaling and label smoothing for improving calibration. While the mechanisms are well-established in prior literature, the specific implementation details for generative models (mT5/mBART) and the choice of hyperparameters are not fully transparent.

**Low Confidence**: The claim that data augmentation via translation is "highly effective" and improves calibration by "almost 75%". This specific quantitative claim appears to be based on internal comparisons without external validation, and the mechanism by which translation preserves semantic structure for QA tasks is not empirically verified.

## Next Checks

1. **Independent Replication**: Replicate the core experiments (temperature scaling, data augmentation via translation) on an independent multilingual QA dataset not used in the original study to verify the generalizability of calibration improvements.

2. **Ablation Study on Translation Quality**: Test the calibration impact of using machine-translated data with varying quality levels (e.g., Google Translate vs. professional translation) to validate whether the observed improvements are robust to translation noise.

3. **Cross-Model Generalization**: Apply the most effective calibration strategies (temperature scaling and data augmentation) to a different family of multilingual models (e.g., BLOOMZ or LLaMA 2-13B) to determine if the observed effects are model-specific or generalizable across architectures.