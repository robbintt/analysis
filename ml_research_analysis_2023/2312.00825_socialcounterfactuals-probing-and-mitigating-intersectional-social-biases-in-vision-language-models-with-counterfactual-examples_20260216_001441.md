---
ver: rpa2
title: 'SocialCounterfactuals: Probing and Mitigating Intersectional Social Biases
  in Vision-Language Models with Counterfactual Examples'
arxiv_id: '2312.00825'
source_url: https://arxiv.org/abs/2312.00825
tags:
- images
- image
- gender
- clip
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SocialCounterfactuals, a large-scale dataset
  of synthetic image-text pairs designed to probe and mitigate intersectional social
  biases in vision-language models (VLMs). The authors employ text-to-image diffusion
  models with cross-attention control to generate counterfactual examples that differ
  only in intersectional social attributes (e.g., race and gender) while maintaining
  consistent depictions of subjects like occupations.
---

# SocialCounterfactuals: Probing and Mitigating Intersectional Social Biases in Vision-Language Models with Counterfactual Examples

## Quick Facts
- arXiv ID: 2312.00825
- Source URL: https://arxiv.org/abs/2312.00825
- Reference count: 40
- Key outcome: Introduces SocialCounterfactuals dataset to probe and mitigate intersectional social biases in VLMs using synthetic counterfactual examples

## Executive Summary
This paper addresses the challenge of intersectional social biases in vision-language models by introducing SocialCounterfactuals, a large-scale dataset of synthetic image-text pairs. The authors employ text-to-image diffusion models with cross-attention control to generate counterfactual examples that differ only in intersectional social attributes while maintaining consistent subject depictions. Through extensive experiments on six state-of-the-art VLMs, they demonstrate significant intersectional social biases and show that training on SocialCounterfactuals leads to substantial reductions in retrieval skewness across all three types of intersectional biases.

## Method Summary
The authors generate synthetic counterfactual image-text pairs using Stable Diffusion with cross-attention control, then apply a three-stage filtering process to ensure quality. The method involves over-generating images with Prompt-to-Prompt cross-attention control to isolate social attribute changes while preserving subject consistency. After filtering through CLIP similarity, NSFW detection, and CLIP attribute detectability filters, the dataset contains 171k high-quality image-text pairs. The debiasing approach fine-tunes VLMs on this synthetic dataset, which generalizes to new subjects not seen during training while maintaining task-specific performance with minimal degradation.

## Key Results
- Training on SocialCounterfactuals leads to substantial reductions in retrieval skewness across all three types of intersectional biases
- ALIP shows the largest absolute reduction of 0.426 in MaxSkew@K metric
- Debiased models show improved fairness on real image-text datasets (VisoGender, PATA) while maintaining task-specific performance with only 1.0-1.7% degradation
- The debiasing effects generalize to new subjects not seen during training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Over-generating images with Prompt-to-Prompt cross-attention control enables isolation of social attribute changes while preserving subject depiction consistency.
- Mechanism: Cross-attention control technique from Hertz et al. [25] is extended to batched generation, allowing simultaneous generation of multiple counterfactual images that differ only in social attributes specified in text prompts.
- Core assumption: Cross-attention control can effectively isolate social attribute changes without introducing other confounding visual differences.
- Evidence anchors: [section] extends Prompt-to-Prompt for image pairs from Brooks et al. [7] to support batched generation; [abstract] describes generating counterfactual pairs that differ only in intersectional social attributes.

### Mechanism 2
- Claim: Three-stage filtering methodology ensures high-quality counterfactual images that accurately represent intended social attributes.
- Mechanism: CLIP-based similarity filtering ensures images accurately depict captions and maintain similarity within counterfactual sets; NSFW filtering removes inappropriate content; CLIP attribute detectability filtering ensures each social attribute is discernible.
- Core assumption: CLIP's image-text similarity scores and attribute detection capabilities are reliable indicators of image quality and attribute representation.
- Evidence anchors: [section] describes applying three stages of filtering; demonstrates 97.5% attribute detectability after filtering.

### Mechanism 3
- Claim: Training on SocialCounterfactuals reduces retrieval skewness by exposing VLMs to balanced representations of intersectional social attributes.
- Mechanism: Synthetic dataset provides balanced exposure to all combinations of social attributes for each subject, counteracting biased co-occurrence patterns learned from real-world data.
- Core assumption: Synthetic counterfactual examples can effectively retrain model to overcome biased associations learned from real-world training data.
- Evidence anchors: [abstract] reports substantial reductions in retrieval skewness; [section] shows up to 0.426 absolute reduction in MaxSkew@K for ALIP.

## Foundational Learning

- Concept: Counterfactual examples in machine learning
  - Why needed here: Understanding how counterfactual examples probe and mitigate biases in models is essential for grasping the paper's approach.
  - Quick check question: How do counterfactual examples differ from regular examples in their purpose for model analysis?

- Concept: Intersectional bias
  - Why needed here: Paper focuses on biases arising from intersection of multiple social attributes, requiring understanding how these differ from single-attribute biases.
  - Quick check question: Why is it important to consider intersectional biases rather than analyzing social attributes in isolation?

- Concept: CLIP (Contrastive Language-Image Pre-training)
  - Why needed here: CLIP is used extensively for image-text similarity assessment and attribute detection, making it foundational to methodology.
  - Quick check question: What makes CLIP particularly suitable for the filtering and evaluation tasks in this work?

## Architecture Onboarding

- Component map: Text caption generation -> Counterfactual image generation (Stable Diffusion with cross-attention control) -> Three-stage filtering (CLIP similarity, NSFW detection, CLIP attribute detectability) -> Bias measurement/Debiasing -> Evaluation

- Critical path: Text caption generation → Counterfactual image generation → Three-stage filtering → Bias measurement/Debiasing → Evaluation

- Design tradeoffs:
  - Synthetic vs. real data: Synthetic data enables exhaustive coverage of social attribute combinations but may not perfectly represent real-world distributions
  - Over-generation vs. efficiency: Generating 100 candidates per set ensures quality but requires significant computational resources
  - Filtering stringency vs. dataset size: Strict filtering improves quality but reduces dataset size

- Failure signatures:
  - High MaxSkew@K values indicate persistent biases in VLMs
  - Low attribute detectability scores after filtering suggest issues with CLIP's attribute recognition
  - Degradation in task-specific performance after debiasing indicates overcorrection

- First 3 experiments:
  1. Generate a small set of counterfactual images for a single occupation and social attribute pair, then manually verify quality of attribute isolation
  2. Apply three-stage filtering to sample of generated images and analyze impact on dataset quality metrics
  3. Fine-tune a VLM on subset of SocialCounterfactuals and measure change in MaxSkew@K for withheld test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are proposed methods at mitigating intersectional biases beyond specific occupation subjects used in training?
- Basis in paper: [explicit] Demonstrates debiasing effects generalize to new subjects not seen during training, but extent of generalization not fully explored.
- Why unresolved: Study focuses on limited set of occupations and doesn't test methods on broader range of subjects or social attributes.
- What evidence would resolve it: Experiments testing debiasing methods on wider variety of subjects and social attributes, including those not present in original dataset.

### Open Question 2
- Question: What is optimal balance between debiasing and maintaining task-specific performance?
- Basis in paper: [explicit] Notes debiasing can lead to minor degradation in task-specific performance, but optimal trade-off not determined.
- Why unresolved: Study doesn't explore different strategies for balancing debiasing with task-specific performance.
- What evidence would resolve it: Comparative experiments evaluating different debiasing strategies and their impact on both bias reduction and task-specific performance across various tasks.

### Open Question 3
- Question: How do proposed methods perform in languages other than English?
- Basis in paper: [explicit] Acknowledges study was conducted in English and findings may not generalize to other languages.
- Why unresolved: Methods not tested on datasets or models in other languages, limiting understanding of effectiveness in multilingual contexts.
- What evidence would resolve it: Replication of study using datasets and models in multiple languages to assess generalizability across linguistic contexts.

## Limitations
- Reliance on synthetic data may not fully capture complexity of real-world social attribute distributions
- Effectiveness of CLIP-based attribute detection for all intersectional attribute combinations remains uncertain
- Minimal performance degradation (1.0-1.7%) may mask more subtle impacts on model capabilities

## Confidence

- High Confidence: Methodology for generating counterfactual examples using cross-attention control is technically sound and well-documented
- Medium Confidence: Filtering methodology effectively produces high-quality synthetic data, though reliance on CLIP-based detection introduces some uncertainty
- Medium Confidence: Observed bias reductions are substantial, but long-term stability and real-world applicability of effects require further validation

## Next Checks

1. Evaluate debiased models on held-out real-world dataset with intersectional social attribute annotations to assess generalization beyond synthetic examples
2. Conduct ablation studies varying strictness of filtering thresholds to quantify impact on debiasing effectiveness and dataset quality
3. Perform long-term stability analysis by measuring bias metrics at multiple time points after fine-tuning to assess persistence of debiasing effects