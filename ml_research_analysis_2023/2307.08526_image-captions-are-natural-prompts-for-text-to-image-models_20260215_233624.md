---
ver: rpa2
title: Image Captions are Natural Prompts for Text-to-Image Models
arxiv_id: '2307.08526'
source_url: https://arxiv.org/abs/2307.08526
tags:
- data
- synthetic
- training
- image
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a training-free method called Caption in Prompt
  (CiP) to improve the training effect of synthetic data generated by text-to-image
  models. The key idea is to use image captioning models to generate informative and
  diverse prompts that contain class-relevant information and clarify polysemy.
---

# Image Captions are Natural Prompts for Text-to-Image Models

## Quick Facts
- arXiv ID: 2307.08526
- Source URL: https://arxiv.org/abs/2307.08526
- Reference count: 40
- Key outcome: CiP improves synthetic data training accuracy by ~10% using captions as prompts

## Executive Summary
This paper introduces Caption in Prompt (CiP), a training-free method that leverages image captioning models to generate informative prompts for text-to-image (T2I) models. By concatenating class names with descriptive captions, CiP significantly improves the quality of synthetic images used for training downstream classifiers. The approach addresses limitations of basic prompts by extracting class-relevant context, clarifying polysemy, and aligning synthetic data distributions with real data. Experimental results demonstrate substantial accuracy gains across multiple datasets.

## Method Summary
CiP uses off-the-shelf image captioning models to generate descriptive text for each real image in a dataset. These captions, combined with class names, form enriched prompts fed into T2I models like Stable Diffusion. The resulting synthetic images capture more discriminative features than those generated from class-name-only prompts. The method is training-free, relying on pre-trained captioning and T2I models, and focuses on prompt design to improve synthetic data quality for downstream classification tasks.

## Key Results
- CiP improves classification accuracy by approximately 10% compared to basic prompts
- Caption quality and guidance scale significantly influence synthetic data performance
- Better captioning models (e.g., BLIP-2 vs. ViT-GPT2) further enhance CiP's effectiveness
- Proper guidance scale balances diversity and consistency in generated images

## Why This Works (Mechanism)

### Mechanism 1: Class-Relevant Information Extraction
Image captions extract background, behavior, and contextual details that basic prompts miss. When concatenated with class names, these descriptions provide rich, discriminative prompts to T2I models. The core assumption is that captioning models accurately capture both foreground objects and their context without losing class-specific details.

### Mechanism 2: Polysemy Clarification
Captions resolve ambiguity in class names with multiple meanings (e.g., "jay" as bird vs. person). By disambiguating the intended concept through descriptive context, T2I models generate more faithful images. The method assumes T2I models interpret combined prompts coherently rather than as separate entities.

### Mechanism 3: Distribution Alignment via Data-Conditioned Prompts
Using image captions as prompts reduces the distribution gap between synthetic and real data. Captions capture original data characteristics (backgrounds, object relations), making the prompt-induced synthetic distribution more closely match the real data distribution. This assumes T2I models' prompt-induced distributions are flexible enough to represent real data diversity.

## Foundational Learning

- **Expected Risk and Distribution Distance**: The theoretical analysis bounds expected risk on real data by risk on synthetic data plus a distance term between distributions. Understanding this explains why aligning synthetic and real distributions matters.
  - Quick check: If the distance term d(X, XC) in the bound becomes zero, what does that imply about the synthetic and real data distributions?

- **Image Captioning Models and Their Outputs**: CiP relies on captioning models to generate descriptive text. Knowing what these models capture (objects, actions, backgrounds) is essential for understanding prompt construction.
  - Quick check: What are the typical components of an image caption, and how might they differ from a simple class name?

- **Text-to-Image Model Prompting**: The paper uses Stable Diffusion with different guidance scales. Understanding how guidance scale affects image fidelity and diversity is crucial for interpreting experimental results.
  - Quick check: How does increasing the guidance scale affect the trade-off between prompt adherence and image diversity?

## Architecture Onboarding

- **Component map**: Image dataset (T) → Image captioning model (CM) → Caption set (C) → Prompt construction (concatenate class name + caption) → Text-to-Image model (T2I) → Synthetic dataset (S) → Downstream model training → Evaluation on real data
- **Critical path**: Caption generation → Prompt construction → Image synthesis → Model training
- **Design tradeoffs**: Caption quality vs. computation; guidance scale balancing fidelity and diversity; prompt length affecting T2I model interpretation
- **Failure signatures**: Low downstream accuracy despite high-quality captions (misalignment between caption semantics and class name); unfaithful synthetic images (caption failed to disambiguate polysemy); low diversity in synthetic images (guidance scale too high or captions too generic)
- **First 3 experiments**: 1) Compare basic prompts vs. CiP with simple captioning model; 2) Vary guidance scale to identify optimal range; 3) Replace captioning model with stronger one to measure impact

## Open Questions the Paper Calls Out

- **Open Question 1**: How does CiP-generated synthetic data compare to real data in training downstream models? While CiP improves synthetic data performance by ~10%, the paper doesn't directly compare it to real data training. What evidence would resolve it: Conduct experiments comparing models trained on CiP synthetic data versus real data.

- **Open Question 2**: What is the impact of different guidance scales on CiP-generated synthetic data quality? The paper mentions guidance scale's significant influence but lacks detailed analysis. What evidence would resolve it: Conduct experiments with various guidance scales and analyze their impact on synthetic data quality.

- **Open Question 3**: How does CiP performance vary with different captioning models? The paper only compares two captioning models without exploring others. What evidence would resolve it: Conduct experiments with various captioning models to compare their impact on CiP performance.

## Limitations
- Assumes captioning models reliably capture class-relevant context without losing object identity
- Theoretical distribution alignment claims remain abstract without empirical validation
- Method's dependence on off-the-shelf captioning models raises scalability and consistency concerns

## Confidence

- **High confidence**: Empirical observation that CiP improves classification accuracy by ~10% compared to basic prompts is well-supported across multiple datasets
- **Medium confidence**: Caption-based polysemy clarification is plausible but lacks systematic evaluation of ambiguous class names
- **Medium confidence**: Distribution alignment theory is sound but not empirically validated with actual distribution distance measurements

## Next Checks

1. **Quantify caption quality impact**: Systematically evaluate how captioning model errors correlate with downstream accuracy degradation. Test with intentionally corrupted captions to establish robustness bounds.

2. **Measure distribution alignment**: Compute and compare Fréchet Inception Distance (FID) or similar metrics between real data and synthetic data generated with basic prompts versus CiP prompts to empirically validate distribution alignment claims.

3. **Stress test polysemy resolution**: Create a benchmark of truly ambiguous class names (e.g., "crane" as bird vs. construction equipment) and measure whether CiP captions consistently generate correct interpretations across multiple T2I model runs.