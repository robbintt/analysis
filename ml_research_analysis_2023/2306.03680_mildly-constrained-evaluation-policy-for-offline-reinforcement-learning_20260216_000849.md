---
ver: rpa2
title: Mildly Constrained Evaluation Policy for Offline Reinforcement Learning
arxiv_id: '2306.03680'
source_url: https://arxiv.org/abs/2306.03680
tags:
- policy
- evaluation
- constraint
- performance
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to improve offline RL performance
  by separating the roles of target and evaluation policies. The target policy is
  used for stable value estimation, while a more mildly constrained evaluation policy
  is used for final inference.
---

# Mildly Constrained Evaluation Policy for Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2306.03680
- Source URL: https://arxiv.org/abs/2306.03680
- Reference count: 13
- Primary result: TD3BC-MCEP achieves average normalized return of 64.5 on MuJoCo locomotion tasks, outperforming target policy (54.2) and CQL (59.0).

## Executive Summary
This paper proposes a method to improve offline reinforcement learning performance by separating the roles of target and evaluation policies. The target policy is used for stable value estimation while a more mildly constrained evaluation policy is used for final inference. This separation addresses the trade-off between stable value learning and performant evaluation. The method is instantiated on TD3BC and AWAC algorithms, showing significant improvements over the target policy and competitive results to state-of-the-art methods on D4RL benchmarks.

## Method Summary
The method introduces two distinct policies: a target policy (˜π) used for stable Q-learning with strong constraints, and an evaluation policy (πe) used for final inference with milder constraints. The critic Qθ is trained using TD learning with the target policy, while the evaluation policy is trained to maximize Qθ with a less restrictive constraint. This allows the evaluation policy to access a wider region of the policy space while leveraging the stable Qθ learned by the target policy. The approach is implemented on top of TD3BC and AWAC algorithms with specific hyperparameter settings for the constraint strengths.

## Key Results
- TD3BC-MCEP achieves 64.5 average normalized return on MuJoCo locomotion tasks
- Evaluation policy generally achieves higher Q-values than target policy
- Significant improvements over target policy (54.2) and competitive with CQL (59.0)
- Method shows consistent gains across D4RL benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
The mildly constrained evaluation policy improves test-time performance by exploiting the critic's ability to estimate high Q-values for unseen but plausible actions. The critic Qθ is trained with a highly constrained target policy (˜π) to ensure stable value estimation. Since Qθ can still assign high values to state-action pairs not in the training data but within plausible regions, extracting an evaluation policy πe directly from Qθ allows the policy to move towards higher Q-values without destabilizing training.

### Mechanism 2
The target policy ˜π acts as a stable teacher that prevents the deadly triad (function approximation, bootstrapping, and off-policy learning) during Q-learning. By keeping ˜π close to the behavior policy through strong constraints, the TD error in Q-learning remains small and stable. The critic Qθ is then used as a value estimator that can be queried by πe without destabilizing training because πe is not involved in the TD update.

### Mechanism 3
Milder constraints on πe allow it to outperform ˜π by accessing a wider region of the policy space while leveraging the stable Qθ. Since πe is not involved in the Q-learning update, it can be trained with a less restrictive constraint to move further from the behavior policy toward higher Q-values. This is possible because the critic Qθ was trained stably by ˜π.

## Foundational Learning

- Concept: Distributional shift in offline RL
  - Why needed here: The core problem MCEP addresses is that the learned policy encounters actions at test time that differ from the behavior policy, causing overestimation errors.
  - Quick check question: What happens to Q-values when the policy proposes actions far from the behavior distribution?

- Concept: Actor-critic framework
  - Why needed here: MCEP extends the standard actor-critic by adding a second actor (πe) that is trained from the critic but not used in the critic's TD updates.
  - Quick check question: In vanilla actor-critic, which component is updated from the critic's Q-values?

- Concept: Policy constraints (KL divergence, behavior cloning)
  - Why needed here: MCEP relies on constraining both the target policy (strongly) and the evaluation policy (mildly) to balance stability and performance.
  - Quick check question: What is the effect of increasing the weight on the KL divergence term in policy updates?

## Architecture Onboarding

- Component map:
  - Dataset Dβ -> Target policy ˜π (strongly constrained) -> Critic Qθ
  - Critic Qθ -> Evaluation policy πe (mildly constrained) -> Test-time actions
- Critical path:
  1. Update Qθ using TD error with actions from ˜π
  2. Update ˜π using policy improvement with strong constraint
  3. Update πe using policy improvement with milder constraint from Qθ
- Design tradeoffs:
  - Strong constraint on ˜π → stable Q-learning but poor performance
  - Mild constraint on πe → better performance but relies on critic accuracy
  - Choice of constraint metric (KL, BC, etc.) affects both stability and expressiveness
- Failure signatures:
  - Q-values explode → constraint on ˜π too weak or learning rate too high
  - πe underperforms ˜π → constraint on πe too mild or critic inaccurate
  - Both policies perform poorly → critic fails to learn meaningful Q-values
- First 3 experiments:
  1. Run TD3BC with varying constraint strengths; observe stability vs. performance tradeoff
  2. Train MCEP with fixed strong constraint on ˜π and varying mild constraint on πe; measure performance gain
  3. Compare MCEP to IQL and CQL on D4RL benchmarks; verify competitive performance

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several implicit questions arise from the work:

- How does the choice of constraint function affect the performance of the MCEP approach?
- Can the MCEP approach be extended to handle multi-task or meta-learning scenarios?
- How does the MCEP perform in environments with sparse rewards or long time horizons?

## Limitations
- The assumption that critic Q-values for OOD but plausible actions are accurate lacks rigorous theoretical justification
- Performance gains may be sensitive to hyperparameter tuning of constraint strengths
- Method's effectiveness on discrete or high-dimensional action spaces remains unclear
- Limited analysis of how dataset quality affects the trade-off between stability and performance

## Confidence

**High Confidence**: The empirical improvements over the target policy on D4RL benchmarks are well-documented and reproducible.

**Medium Confidence**: The claim that the evaluation policy generally achieves higher Q-values than the target policy is supported by results, but the causal relationship between constraint strength and performance gains requires further investigation.

**Low Confidence**: The assertion that the critic's Q-values for OOD but plausible actions are sufficiently accurate to guide policy improvement lacks rigorous theoretical justification.

## Next Checks

1. **Constraint Sensitivity Analysis**: Systematically vary the evaluation policy constraint strength (αe) across multiple orders of magnitude and measure the corresponding performance changes to reveal whether reported gains are robust to hyperparameter choices.

2. **Critic Accuracy Evaluation**: For a subset of D4RL tasks, generate held-out action samples from the evaluation policy and compare the critic's Q-value predictions against actual returns or against ensemble disagreement as a proxy for uncertainty.

3. **Dataset Quality Impact Study**: Evaluate MCEP performance across datasets with varying levels of coverage (random → medium-expert) and measure how constraint strength affects performance differently as dataset quality changes.