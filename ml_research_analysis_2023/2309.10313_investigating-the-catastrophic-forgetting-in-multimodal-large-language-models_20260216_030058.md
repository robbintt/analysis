---
ver: rpa2
title: Investigating the Catastrophic Forgetting in Multimodal Large Language Models
arxiv_id: '2309.10313'
source_url: https://arxiv.org/abs/2309.10313
tags:
- fine-tuning
- image
- object
- arxiv
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EMT (Evaluating MulTimodality), a framework
  for evaluating catastrophic forgetting in multimodal large language models (MLLMs).
  The key idea is to treat MLLMs as image classifiers and assess their performance
  on standard image classification tasks.
---

# Investigating the Catastrophic Forgetting in Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2309.10313
- Source URL: https://arxiv.org/abs/2309.10313
- Reference count: 40
- Key outcome: Most fine-tuned MLLMs fail to retain classification accuracy comparable to their base vision encoders, with excessive fine-tuning causing hallucinations and performance degradation.

## Executive Summary
This paper introduces EMT (Evaluating MulTimodality), a framework for evaluating catastrophic forgetting in multimodal large language models (MLLMs) by treating them as image classifiers. The authors apply EMT to several open-source fine-tuned MLLMs and discover that most fail to retain classification accuracy comparable to their base vision encoders. Through systematic experiments with LLaVA fine-tuning, they find that moderate fine-tuning improves alignment between text and visual features, while excessive fine-tuning causes hallucinations and significant performance degradation on non-fine-tuned datasets. The findings suggest that current MLLM fine-tuning procedures need improvement to mitigate catastrophic forgetting and reduce hallucinations.

## Method Summary
The EMT framework evaluates MLLMs by treating them as image classifiers through a two-stage approach: prompting the MLLM to generate predictions for image classification tasks, then using GPT-3.5 to evaluate the correctness of these predictions. The method involves fine-tuning LLaVA with both linear adapters and LoRA techniques on various image datasets (MNIST, CIFAR-10, CIFAR-100, miniImagenet), then evaluating performance on both fine-tuned and non-fine-tuned datasets using EMT. The evaluation compares performance against frozen base vision encoders to assess catastrophic forgetting.

## Key Results
- Most fine-tuned MLLMs show significant performance degradation on non-fine-tuned datasets compared to their base vision encoders
- Moderate fine-tuning improves feature alignment between visual and textual modalities, benefiting performance on similar tasks
- Excessive fine-tuning causes hallucinations, where models generate outputs related to fine-tuning data but irrelevant to the input prompt
- Linear fine-tuning generally performs better than LoRA in preserving performance on non-fine-tuned datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning on subsets of classes causes minority collapse, where classifiers for missing classes converge to a single vector
- Core assumption: Cross-entropy loss with regularization behaves like the Layer-Peeled Model where last-layer classifiers are directly optimized
- Evidence: Early-stage fine-tuning improves cross-dataset performance through enhanced feature alignment
- Break condition: If fine-tuning includes all original classes or uses different loss functions preventing overfitting

### Mechanism 2
- Claim: Moderate fine-tuning improves visual-textual feature alignment through linear adapter adjustments
- Core assumption: Linear adapter layer bridges visual and textual feature spaces, and its adjustment benefits cross-task performance
- Evidence: Figure 6 shows early-stage fine-tuning enhances alignment between visual and textual features
- Break condition: If fine-tuning dataset is too dissimilar from target datasets or model lacks linear adapter layer

### Mechanism 3
- Claim: Excessive fine-tuning causes overfitting to fine-tuning dataset, leading to hallucinations
- Core assumption: MLLM's generative nature allows pattern-based outputs that become irrelevant when overfit
- Evidence: Fine-tuning causes output of texts related to fine-tuned dataset while ignoring original prompt
- Break condition: If fine-tuning dataset is sufficiently large/diverse or regularization prevents overfitting

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed: Fundamental to understanding why MLLMs lose performance on original tasks when fine-tuned
  - Quick check: What happens to a neural network's original task performance when fine-tuned on a new task?

- Concept: Neural collapse and minority collapse
  - Why needed: Explains geometric changes in feature representations under class imbalance during MLLM fine-tuning
  - Quick check: How does class imbalance during training affect classifier convergence for different classes?

- Concept: Multimodal learning and feature alignment
  - Why needed: Critical for understanding how MLLMs integrate visual and textual information and how fine-tuning affects this integration
  - Quick check: Why is alignment between visual and textual features important for multimodal model performance?

## Architecture Onboarding

- Component map: Vision encoder (frozen/fine-tuned) -> Linear adapter layer -> LLM (frozen/fine-tuned) -> EMT evaluation framework
- Critical path: Input image → MLLM generates prediction → GPT-3.5 evaluates correctness → EMT calculates accuracy
- Design tradeoffs:
  - Freezing vs. fine-tuning vision encoder: Preserves capabilities vs. enables adaptation
  - Linear vs. LoRA fine-tuning: Preserves more original model vs. allows greater adaptation
  - Prompt design: Detailed prompts improve accuracy vs. introduce bias
- Failure signatures:
  - Performance degradation on non-fine-tuned datasets
  - Hallucinations: outputs related to fine-tuning data but irrelevant to input
  - Incorrect predictions from poor visual-textual integration
- First 3 experiments:
  1. Apply EMT to pre-trained MLLM on standard image classification dataset
  2. Fine-tune MLLM on class subset, evaluate on fine-tuned and non-fine-tuned classes
  3. Compare linear vs. LoRA fine-tuning effects on performance and hallucination tendency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does fine-tuning dataset diversity impact catastrophic forgetting in MLLMs?
- Basis: Qualitative observation that diverse datasets reduce forgetting, but no systematic study
- Resolution: Controlled experiment varying dataset diversity and measuring forgetting on held-out data

### Open Question 2
- Question: What is the relationship between base LLM architecture and vision-text integration ability?
- Basis: LENS with Flan-T5 performs worse than LLaVA with LLaMA, suggesting architectural impact
- Resolution: Systematic comparison of MLLMs with different base architectures on vision-language tasks

### Open Question 3
- Question: How can hallucination in MLLMs during fine-tuning be mitigated?
- Basis: Hallucination identified as major degradation factor, but no mitigation methods proposed
- Resolution: Experimental study comparing regularization techniques and data augmentation for reducing hallucination

## Limitations

- Findings based primarily on empirical observations rather than theoretical guarantees
- Experimental scope limited to four image classification datasets and one base model (LLaVA)
- EMT framework's reliance on GPT-3.5 for evaluation introduces potential biases
- No systematic investigation of dataset size, fine-tuning duration, or learning rate effects

## Confidence

**High Confidence**: Empirical observation of fine-tuning-induced performance degradation on non-fine-tuned datasets; moderate fine-tuning improves feature alignment
**Medium Confidence**: Hypothesis that excessive fine-tuning causes hallucinations; linear fine-tuning better than LoRA for performance retention
**Low Confidence**: Theoretical mechanisms (minority collapse, feature alignment) lack direct validation; exact conditions for beneficial vs. harmful fine-tuning unclear

## Next Checks

1. Conduct formal analysis of MLLM classifier behavior under class imbalance during fine-tuning, comparing to Layer-Peeled Model predictions
2. Develop metric to systematically quantify hallucination severity beyond qualitative assessment
3. Replicate EMT evaluation across multiple MLLM architectures (Otter, InstructBLIP, LENS) to verify consistency of observed phenomena