---
ver: rpa2
title: Bayesian inference of a new Mallows model for characterising symptom sequences
  applied in primary progressive aphasia
arxiv_id: '2311.13411'
source_url: https://arxiv.org/abs/2311.13411
tags:
- data
- mallows
- symptoms
- symptom
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of modeling symptom sequences
  in neurodegenerative diseases, particularly in the context of primary progressive
  aphasia (PPA). The authors adapt the Mallows model to handle partial rankings and
  right-censored data, enabling the analysis of symptom questionnaires where respondents
  may not rank all symptoms or may not have experienced certain symptoms yet.
---

# Bayesian inference of a new Mallows model for characterising symptom sequences applied in primary progressive aphasia

## Quick Facts
- arXiv ID: 2311.13411
- Source URL: https://arxiv.org/abs/2311.13411
- Reference count: 18
- Primary result: The Mallows model adapted with Kendall's Tau distance and penalty parameter p=0.5 effectively handles partial rankings and right-censored data in symptom sequences.

## Executive Summary
This paper addresses the challenge of modeling symptom sequences in neurodegenerative diseases, particularly in the context of primary progressive aphasia (PPA). The authors adapt the Mallows model to handle partial rankings and right-censored data, enabling the analysis of symptom questionnaires where respondents may not rank all symptoms or may not have experienced certain symptoms yet. They employ a Bayesian framework with Markov Chain Monte Carlo (MCMC) fitting to estimate model parameters.

The primary results demonstrate the model's efficacy in revealing mean orderings and estimating ranking variance in both synthetic data experiments and a real-world PPA dataset. In the synthetic data, the model accurately infers parameters even with up to 10% missing data. For the PPA dataset, the model identifies symptom orderings consistent with clinical expectations and groups co-occurring symptoms effectively.

## Method Summary
The authors adapt the Mallows model for ranking data to handle partial rankings and right-censored data in symptom sequences. They introduce a penalty parameter p=0.5 in the Kendall's Tau distance metric to account for unranked items due to missingness or censoring. A Bayesian framework with MCMC sampling is employed to estimate the central ranking (π₀) and spread parameter (λ). The model is tested on synthetic data generated from a Mallows distribution and a real-world PPA dataset of 30 individuals with 8 symptoms.

## Key Results
- The model accurately infers parameters in synthetic data experiments even with up to 10% missing data.
- For the PPA dataset, the model identifies symptom orderings consistent with clinical expectations and groups co-occurring symptoms effectively.
- The computational complexity of calculating the normalizing constant across the entire distribution space poses challenges for larger datasets.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Mallows model adapted with Kendall's Tau distance and penalty parameter p=0.5 effectively handles partial rankings and right-censored data in symptom sequences.
- Mechanism: By introducing a penalty parameter in the distance metric, the model accounts for the fact that respondents may not rank all symptoms due to lack of experience or incomplete data. This allows the model to compare partial rankings meaningfully without forcing complete orderings.
- Core assumption: The choice of p=0.5 is appropriate as it treats partial rankings as distinct from concordant pairs but not as strongly as discordant pairs, maintaining a balance in the distance calculation.
- Evidence anchors:
  - [section] "To account for comparison of rankings where items are unranked due to censoring, or missingness, we drop the comparison of pairs from the calculation (where one or more rank is missing)."
  - [corpus] No direct evidence found in corpus papers; the adaptation appears novel to this work.
- Break condition: If p is set too high, partial rankings would be treated too similarly to discordant pairs, losing the ability to distinguish between them. If p is too low, the model may underweight important differences in partial rankings.

### Mechanism 2
- Claim: Bayesian inference with MCMC sampling allows robust parameter estimation for the Mallows model, even with limited data and missing information.
- Mechanism: The MCMC algorithm explores the posterior distribution of the Mallows model parameters (central ranking π₀ and spread parameter λ) by iteratively sampling from the Mallows distribution conditioned on current parameter estimates. This allows the model to converge to a likely set of parameters that explain the observed data.
- Core assumption: The independence assumption between patients' data is valid, and the prior distributions on π₀ and λ are reasonable.
- Evidence anchors:
  - [section] "We use an MCMC algorithm to sample from p(π₀, λ|X). Details are in Appendix A."
  - [corpus] No direct evidence found in corpus papers; the MCMC fitting approach appears specific to this implementation.
- Break condition: If the dataset is too small or the data is highly heterogeneous, the MCMC may struggle to converge to meaningful parameter estimates.

### Mechanism 3
- Claim: The model's ability to group co-occurring symptoms enhances clinical comprehension of symptom occurrence patterns.
- Mechanism: By estimating a central ranking and spread parameter, the model identifies not just the order of symptoms but also their variance. Symptoms that frequently co-occur will have smaller spread parameters, visually grouping them in the results.
- Core assumption: The assumption that symptom co-occurrence patterns are consistent enough across patients to be modeled by a Mallows distribution.
- Evidence anchors:
  - [section] "The ranking of symptoms shown in Figure 3 was the same as the clinically informed ranking of symptoms suggested by the PPA researchers. Compared to Figure 2, in Figure 3 we see the visual grouping of symptoms which co-occur."
  - [corpus] No direct evidence found in corpus papers; this application to symptom grouping appears novel to this work.
- Break condition: If symptoms are too heterogeneous or if there are no clear patterns of co-occurrence, the model may not be able to identify meaningful groupings.

## Foundational Learning

- Concept: Mallows model for ranking data
  - Why needed here: The Mallows model provides a probabilistic framework for modeling rankings, which is essential for understanding symptom sequences where order matters.
  - Quick check question: What is the role of the spread parameter λ in the Mallows model?

- Concept: Bayesian inference and MCMC sampling
  - Why needed here: Bayesian inference allows the incorporation of prior knowledge about symptom orderings, while MCMC sampling enables the estimation of model parameters from complex posterior distributions.
  - Quick check question: How does the MCMC algorithm explore the parameter space to find the MAP estimates?

- Concept: Kendall's Tau distance with penalty for partial rankings
  - Why needed here: This distance metric allows for the comparison of partial rankings, which is crucial when dealing with incomplete symptom data.
  - Quick check question: How does the penalty parameter p affect the distance calculation between partial rankings?

## Architecture Onboarding

- Component map: Data preprocessing -> Model definition -> Inference engine -> Output processing
- Critical path: 1. Data preprocessing and preparation 2. Model initialization with priors 3. MCMC sampling iterations 4. MAP estimation and result interpretation
- Design tradeoffs: Computational complexity vs. model accuracy: The current bottleneck is calculating the normalizing constant across the entire distribution space, which limits scalability. Model complexity vs. data size: The model may be too complex for small datasets, but is necessary to capture the nuances of symptom sequences.
- Failure signatures: Poor convergence of MCMC chains: May indicate issues with priors, model specification, or data quality. Inability to identify meaningful symptom orderings: Could suggest too much heterogeneity in the data or insufficient data.
- First 3 experiments: 1. Synthetic data experiments: Generate data with known parameters and test the model's ability to recover them under different conditions (e.g., varying levels of missing data, different spread parameters). 2. Sensitivity analysis: Test the model's robustness to different choices of the penalty parameter p and prior distributions. 3. Scalability test: Evaluate the model's performance as the number of symptoms and patients increases, identifying the limits of its applicability.

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions. However, based on the content, some potential open questions include:
- How does the choice of penalty parameter p in the Kendall's Tau distance metric affect the accuracy of the Mallows model when dealing with partial rankings in healthcare data?
- What alternative methods could be explored to improve the computational efficiency of calculating the normalizing constant in the Mallows model for large datasets?
- How would the model's performance differ if applied to a larger dataset from a more common neurodegenerative disease, such as Alzheimer's disease, compared to the smaller PPA dataset?

## Limitations
- The computational complexity of the normalizing constant calculation poses significant scalability challenges, limiting the model's applicability to larger datasets.
- The choice of penalty parameter p=0.5 in the modified Kendall's Tau distance metric lacks strong theoretical justification and may significantly impact results if inappropriate.
- The model assumes independence between patients' data, which may not hold in clinical settings where patients share common risk factors or environmental influences.

## Confidence
- High confidence in the methodology for handling partial rankings and right-censored data, as the mathematical framework is clearly specified.
- Medium confidence in the MCMC implementation and parameter estimation, given that key algorithmic details are in the appendix rather than the main text.
- Low confidence in the model's generalizability to larger datasets and different disease contexts due to the identified scalability issues.

## Next Checks
1. Perform sensitivity analysis on the penalty parameter p across a range of values (0.1 to 0.9) to determine its impact on model outcomes and identify optimal settings.
2. Test the model on synthetic data with known ground truth parameters but increasing levels of missingness and right-censoring to evaluate robustness under different data quality conditions.
3. Implement an approximation method for the normalizing constant calculation to assess whether computational scalability can be improved without sacrificing accuracy.