---
ver: rpa2
title: Learning Multiple Coordinated Agents under Directed Acyclic Graph Constraints
arxiv_id: '2307.07529'
source_url: https://arxiv.org/abs/2307.07529
tags:
- reward
- algorithm
- goal
- each
- rewards
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel multi-agent reinforcement learning
  (MARL) method to learn multiple coordinated agents under directed acyclic graph
  (DAG) constraints. The method explicitly exploits the DAG structure between agents
  to achieve more effective learning performance.
---

# Learning Multiple Coordinated Agents under Directed Acyclic Graph Constraints

## Quick Facts
- arXiv ID: 2307.07529
- Source URL: https://arxiv.org/abs/2307.07529
- Reference count: 40
- Key outcome: Proposes a novel MARL method for DAG-constrained environments using surrogate value functions with synthetic rewards and leader-follower architecture

## Executive Summary
This paper addresses the challenge of learning coordinated multi-agent policies in environments with directed acyclic graph (DAG) constraints. The proposed method introduces a novel surrogate value function based on synthetic rewards that provably serves as a lower bound of the optimal value function. A practical training algorithm is developed featuring a leader agent that generates goal vectors and a reward generator/distributor (RGD) that creates and allocates synthetic rewards based on agents' contributions to downstream sinks. The method is evaluated on four DAG environments, including a real-world Intel factory scheduling task, demonstrating superior performance compared to non-DAG approaches.

## Method Summary
The method decomposes a team's coordinated task under DAG constraints into multiple subtasks handled by follower agents, guided by a leader agent and an RGD agent. The leader generates abstract goal vectors that evolve during training to improve coordination, while the RGD observes global state flows to generate and distribute synthetic rewards based on each agent's contribution to downstream sinks. The theoretical foundation proves that optimizing the sum of synthetic value functions provides a lower bound on the team's optimal value function, enabling parallel training of agents. PPO is used for all agents with 256-unit fully connected neural networks.

## Key Results
- Proves theoretical lower-bound guarantee for surrogate value function with synthetic rewards
- Demonstrates superior performance compared to baselines (GS, SRM, LFM, RFM, Diff-M, CaP-M) on four DAG environments
- Shows 95% completion rate in real-world Intel factory scheduling task
- Validates coordination improvement through leader-follower architecture with goal vectors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The surrogate value function with synthetic rewards provides a tractable lower bound for the optimal value function in DAG-constrained MARL.
- Mechanism: By decomposing the team reward across sink agents and distributing synthetic rewards based on each agent's contribution to downstream sinks, the algorithm ensures that optimizing synthetic values leads to improving the lower bound of the true value.
- Core assumption: Synthetic reward functions can be constructed such that their sum approximates the team reward without exceeding it, and agents can be trained to maximize synthetic rewards in parallel.
- Evidence anchors:
  - [abstract] "we propose a novel surrogate value function based on a MARL model with synthetic rewards (MARLM-SR) and prove that it serves as a lower bound of the optimal value function."
  - [section] Theorem 1 proves ∑ᵢ∈ᵥ Ṽ{πⱼ∣ⱼ∈Ω₍ᵢ₎}ᵢ(s₀) ≤ ∑ᵢ∈ₗ V{πⱼ∣ⱼ∈∆₍ᵢ₎}ᵢ(s₀) under the constraint that synthetic rewards respect the DAG contribution structure.
- Break condition: If the synthetic reward generation violates the DAG contribution constraint or fails to reflect true downstream impact, the lower bound guarantee is lost and coordination may degrade.

### Mechanism 2
- Claim: The leader-follower architecture with goal vectors enables effective communication and coordination without exposing the full environment state.
- Mechanism: The leader generates abstract goal vectors that evolve during training; followers incorporate these goals into their state representation, allowing the leader to guide followers toward higher team rewards without requiring centralized state sharing.
- Core assumption: The leader can learn to produce goal vectors that meaningfully influence follower policies, and followers can interpret these goals to improve performance.
- Evidence anchors:
  - [abstract] "we propose a practical training algorithm that exploits new notion of leader agent and reward generator and distributor agent to guide the decomposed follower agents to better explore the parameter space."
  - [section] "The leader trains the set of goals for better coordination of followers considering the whole environment, and each follower optimizes its policy by pursuing the given goals."
- Break condition: If the goal vectors become meaningless or followers fail to interpret them correctly, coordination breaks down and the leader's guidance becomes ineffective.

### Mechanism 3
- Claim: The Reward Generator and Distributor (RGD) creates and allocates synthetic rewards that reflect agents' contributions, enabling effective reward shaping in DAG environments.
- Mechanism: The RGD observes global state flows, generates node and arc values, and distributes synthetic rewards from sinks down the DAG, ensuring agents are rewarded for contributions to downstream success.
- Core assumption: The RGD can accurately estimate relative contributions through learned value functions, and this reward shaping improves exploration without altering optimal policies.
- Evidence anchors:
  - [abstract] "the concept of the reward generator and distributor is first introduced in the area of reinforcement learning to address the problem of reward shaping in the DAG."
  - [section] "We develop a strategy to provide incentives (synthetic rewards) using a RGD that generates and distributes reward so that the followers are guided to explore better."
- Break condition: If the RGD fails to accurately estimate contributions or the synthetic rewards misalign with actual value creation, agents may learn suboptimal policies.

## Foundational Learning

- Concept: Markov Decision Process with DAG Constraints (MDP-DAG)
  - Why needed here: The environment structure requires agents to make decisions in a coordinated way where upstream actions affect downstream states, unlike standard MARL settings.
  - Quick check question: In a DAG with nodes A→B→C, if agent A takes action a, which agents' state transitions are affected?

- Concept: Synthetic Reward Construction
  - Why needed here: Standard reward sharing fails in DAGs because delayed rewards make credit assignment difficult; synthetic rewards provide immediate feedback based on contribution.
  - Quick check question: If agent i contributes to sink k's reward through function fik, how is agent i's synthetic reward calculated?

- Concept: Value Function Decomposition
  - Why needed here: Decomposing the global value function into agent-specific synthetic value functions enables parallel training while maintaining theoretical guarantees.
  - Quick check question: What condition must synthetic rewards satisfy to ensure the sum of synthetic values is a lower bound of the team value?

## Architecture Onboarding

- Component map:
  Leader → Goal distribution → Follower state augmentation → Follower action → Environment state transition → RGD observation → Synthetic reward generation → Follower reward accumulation → Leader/RGD reward based on team performance

- Critical path: Leader generates goals at start of each goal period → followers incorporate goals into state → followers take actions → environment transitions states → RGD observes global state flow → RGD generates synthetic rewards → followers receive rewards → leader receives team reward

- Design tradeoffs:
  - Parallel training vs. centralized training: Parallel training scales better but requires careful reward shaping
  - Goal abstraction vs. explicit instructions: Abstract goals reduce communication overhead but require learning to interpret
  - Synthetic reward accuracy vs. computational cost: More accurate contribution estimation improves performance but increases RGD complexity

- Failure signatures:
  - Leader produces random goals → followers show no coordination improvement
  - RGD generates uniform synthetic rewards → no differentiation in agent learning
  - Followers ignore goals → leader stops evolving meaningful signals

- First 3 experiments:
  1. Run with only leader (no RGD) on simple DAG to verify goal communication works
  2. Run with only RGD (no leader) on simple DAG to verify synthetic reward shaping works
  3. Run full system on small DAG (3-4 nodes) to verify combined coordination

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do we mathematically prove that the synthetic rewards generated by the proposed algorithm satisfy the conditions defined in the modeling section?
- Basis in paper: [inferred] The paper mentions that the superiority of the proposed algorithm was shown through empirical results, but a mathematical basis for the synthetic rewards was not provided.
- Why unresolved: The paper does not provide a formal proof or mathematical derivation showing that the synthetic rewards satisfy the conditions in the modeling section.
- What evidence would resolve it: A formal proof or mathematical derivation demonstrating that the synthetic rewards generated by the proposed algorithm satisfy the conditions defined in the modeling section would resolve this open question.

### Open Question 2
- Question: How does the performance of the proposed algorithm scale with the size and complexity of the DAG?
- Basis in paper: [inferred] The paper evaluates the proposed algorithm on four DAG environments, including a real-world scheduling task, but does not provide a systematic analysis of how the performance scales with the size and complexity of the DAG.
- Why unresolved: The paper does not provide a systematic analysis or empirical results showing how the performance of the proposed algorithm scales with the size and complexity of the DAG.
- What evidence would resolve it: A systematic analysis or empirical results showing how the performance of the proposed algorithm scales with the size and complexity of the DAG would resolve this open question.

### Open Question 3
- Question: How sensitive is the proposed algorithm to the choice of hyperparameters, such as the length of the goal period and the number of steps for the global state flow?
- Basis in paper: [inferred] The paper mentions the hyperparameters used for the proposed algorithm and the baselines, but does not provide a systematic analysis of how sensitive the algorithm is to these hyperparameters.
- Why unresolved: The paper does not provide a systematic analysis or empirical results showing how sensitive the proposed algorithm is to the choice of hyperparameters.
- What evidence would resolve it: A systematic analysis or empirical results showing how sensitive the proposed algorithm is to the choice of hyperparameters would resolve this open question.

## Limitations

- Theoretical lower-bound guarantee depends critically on accurate synthetic reward generation, which is not empirically validated
- No systematic analysis of performance scaling with DAG size and complexity
- Limited hyperparameter sensitivity analysis provided

## Confidence

- **High confidence** in the architectural design and practical implementation of the leader-follower system with goal vectors
- **Medium confidence** in the theoretical lower-bound guarantee, as it relies on idealized synthetic reward generation
- **Low confidence** in the real-world scalability claims without additional testing on larger DAG instances

## Next Checks

1. Implement a controlled experiment on a small DAG where synthetic rewards can be precisely calculated to verify the lower-bound property holds during training
2. Test the system with deliberately flawed synthetic reward generation (e.g., random or uniform rewards) to confirm the lower-bound guarantee breaks as expected
3. Conduct ablation studies on the leader-follower architecture to quantify the contribution of each component (goals vs. synthetic rewards) to overall performance