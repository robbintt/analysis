---
ver: rpa2
title: Incorporating Singletons and Mention-based Features in Coreference Resolution
  via Multi-task Learning for Better Generalization
arxiv_id: '2309.11582'
source_url: https://arxiv.org/abs/2309.11582
tags:
- coreference
- mention
- entity
- computational
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-task learning framework that jointly
  trains coreference resolution with auxiliary tasks for singleton detection, entity
  type recognition, and information status classification. By leveraging singleton
  annotations from the OntoGUM corpus and training on richer mention-level annotations,
  the model improves mention detection and coreference linking.
---

# Incorporating Singletons and Mention-based Features in Coreference Resolution via Multi-task Learning for Better Generalization

## Quick Facts
- arXiv ID: 2309.11582
- Source URL: https://arxiv.org/abs/2309.11582
- Reference count: 12
- Primary result: New state-of-the-art performance on OntoGUM benchmark (+2.7 F1 points)

## Executive Summary
This paper introduces a multi-task learning framework that jointly trains coreference resolution with auxiliary tasks for singleton detection, entity type recognition, and information status classification. By leveraging singleton annotations from the OntoGUM corpus and training on richer mention-level annotations, the model improves mention detection and coreference linking. Experiments show the model achieves new state-of-the-art performance on the OntoGUM benchmark (+2.7 F1) and demonstrates improved robustness on out-of-domain datasets (+2.3 F1 on average) compared to strong baselines, likely due to better generalization from multi-task learning and additional training data from singletons.

## Method Summary
The authors propose a multi-task learning approach that jointly trains coreference resolution with three auxiliary tasks: singleton detection, entity type recognition, and information status classification. The model uses SpanBERT-large for token representations and employs a shared architecture with separate scoring functions for markable detection and mention candidate identification. Coreference resolution is performed via pair-matching between mention candidates. The model is trained on OntoGUM corpus, which contains singleton annotations and entity type information from the original GUM corpus, and optimized using a weighted sum of coreference and auxiliary task losses.

## Key Results
- Achieves new state-of-the-art performance on OntoGUM benchmark (+2.7 F1 points)
- Demonstrates improved robustness on out-of-domain datasets (+2.3 F1 on average)
- Ablation studies show each auxiliary task contributes to performance improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-task learning improves mention detection by leveraging singleton and entity type information as auxiliary signals.
- Mechanism: The model jointly trains coreference resolution with singleton detection, entity type recognition, and information status classification. This provides richer supervision for identifying mention spans and their semantic properties.
- Core assumption: Singletons and entity types provide meaningful signals that improve the model's ability to distinguish mentions from non-mentions.
- Evidence anchors:
  - [abstract]: "learns singletons as well as features such as entity type and information status via a multi-task learning-based approach"
  - [section 4.4]: "experiments demonstrate that the proposed model achieves new state-of-the-art performance on the OntoGUM benchmark (+2.7 points)"
  - [corpus]: OntoGUM corpus contains singleton annotations and entity type information from the original GUM corpus
- Break condition: If the auxiliary tasks do not provide additional signal beyond what the coreference task already learns, or if the tasks conflict with each other.

### Mechanism 2
- Claim: Multi-task learning improves generalization by training on more diverse mention types including singletons.
- Mechanism: By including singletons in training, the model learns to distinguish singleton mentions from non-referring expressions and random spans, which are penalized equally in traditional end-to-end approaches.
- Core assumption: Training on singletons provides useful negative examples that improve the model's ability to identify valid mentions.
- Evidence anchors:
  - [abstract]: "increases robustness on multiple out-of-domain datasets (+2.3 points on average), likely due to greater generalizability for mention detection"
  - [section 4.4]: "recall results in substantially better performance" on out-of-domain datasets
  - [corpus]: OntoGUM contains ~39% singletons, providing diverse training examples
- Break condition: If the singleton detection task introduces noise that degrades coreference performance, or if the distribution of singletons in training data does not match real-world usage.

### Mechanism 3
- Claim: Entity type and information status provide explicit semantic constraints that improve coreference resolution.
- Mechanism: Entity type recognition ensures mentions in the same cluster have consistent semantic types, while information status indicates how entities are introduced into discourse, helping the model identify likely antecedents.
- Core assumption: Coreference clusters typically have consistent entity types, and information status provides useful cues about mention relationships.
- Evidence anchors:
  - [section 3.2]: "entity type instructs the model regarding which mentions belong to the same semantic class"
  - [section 4.4]: "entity type as auxiliary tasks outperforms the baseline model on both datasets"
  - [corpus]: GUM corpus contains entity type annotations for each mention and information status labels
- Break condition: If the entity type and information status annotations are noisy or inconsistent, or if the model already learns these patterns implicitly from language model pretraining.

## Foundational Learning

- Concept: Multi-task learning and joint training objectives
  - Why needed here: The model needs to balance learning coreference resolution with auxiliary tasks without one dominating the others
  - Quick check question: How does the weighted loss function (Equation 1) ensure balanced learning across tasks?

- Concept: Mention detection as a prerequisite for coreference resolution
  - Why needed here: The model must first identify valid mention spans before linking them into coreference clusters
  - Quick check question: Why does the model use two separate scoring functions (markable score and mention candidate score) instead of one?

- Concept: Information status and discourse coherence
  - Why needed here: Information status indicates how entities are introduced into discourse, which provides useful context for coreference resolution
  - Quick check question: How does information status differ from entity type, and why might both be useful for coreference resolution?

## Architecture Onboarding

- Component map: SpanBERT embeddings -> Shared layers -> Markable scoring -> Mention scoring -> Pair-matching -> Coreference resolution; Singleton detection, entity type recognition, and information status classification as auxiliary tasks
- Critical path: Span representation → mention scoring → pair-matching → coreference resolution
  - The mention scoring function is critical because it determines which spans are considered as mention candidates
- Design tradeoffs:
  - Using separate scoring functions for markables and mentions vs. a single unified score
  - Including all three auxiliary tasks vs. selecting the most beneficial ones
  - Training on OntoGUM vs. the more commonly used OntoNotes dataset
- Failure signatures:
  - Poor mention detection leading to missed coreference links
  - Entity type mismatches within coreference clusters
  - Overfitting to the OntoGUM training data and poor generalization to out-of-domain datasets
- First 3 experiments:
  1. Ablation study: Remove singleton detection task and measure impact on performance
  2. Out-of-domain evaluation: Test on OntoNotes and WikiCoref to verify generalization
  3. Error analysis: Compare error patterns between baseline and MTL models to understand improvement sources

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the multi-task learning model change when trained on a larger dataset like OntoNotes that contains more data but lacks singleton annotations?
- Basis in paper: [explicit] The paper acknowledges that the model was trained on OntoGUM rather than OntoNotes due to the lack of singleton annotations in OntoNotes, and states this as a limitation.
- Why unresolved: The authors did not train or evaluate their model on OntoNotes, so the performance impact of training on a larger dataset without singleton information is unknown.
- What evidence would resolve it: Training and evaluating the model on OntoNotes, then comparing the results to the baseline models and the OntoGUM results to see if the larger dataset compensates for the lack of singleton information.

### Open Question 2
- Question: Does the multi-task learning approach generalize across languages, and how does it perform on coreference datasets in languages other than English that share the OntoGUM annotation scheme?
- Basis in paper: [explicit] The authors mention that several coreference datasets in other languages share the same annotation scheme as OntoGUM, such as Arabic and Chinese, and suggest that the model needs to be evaluated on datasets in other languages to demonstrate generalization across languages.
- Why unresolved: The authors did not evaluate their model on datasets in languages other than English, so the cross-lingual generalization capability of the model is unknown.
- What evidence would resolve it: Training and evaluating the model on coreference datasets in other languages, such as Arabic or Chinese, and comparing the results to the English results to assess cross-lingual generalization.

### Open Question 3
- Question: How does the performance of the multi-task learning model vary across different genres within the OntoGUM corpus, and which genres benefit most from the auxiliary tasks?
- Basis in paper: [inferred] The paper mentions that OntoGUM includes 12 written and spoken genres, but does not provide a detailed analysis of the model's performance across these genres or identify which genres benefit most from the auxiliary tasks.
- Why unresolved: The authors did not conduct a genre-specific analysis of the model's performance, so the variation in performance across genres and the impact of auxiliary tasks on different genres is unknown.
- What evidence would resolve it: Evaluating the model's performance on each genre separately within the OntoGUM corpus, and comparing the results to identify which genres benefit most from the auxiliary tasks and how the performance varies across genres.

## Limitations

- The evaluation relies entirely on OntoGUM, a corpus specifically augmented with singleton and mention-level annotations, raising questions about generalization to datasets without these annotations.
- The weighted loss formulation is somewhat ad hoc, with no theoretical grounding for the chosen weights.
- The claim of improved robustness on out-of-domain data is based on only two datasets (OntoNotes and WikiCoref), which may not fully represent diverse domains.

## Confidence

**High confidence** in the core mechanism: Multi-task learning with singleton detection improves mention detection recall, as supported by both ablation studies and quantitative gains. The architectural design choices (separate markable/mention scoring, constrained detection) are clearly specified and logically justified.

**Medium confidence** in the generalization claims: While the out-of-domain improvements are statistically significant, the evaluation is limited to two datasets, and the paper does not address whether the gains persist when auxiliary annotations are unavailable at test time.

**Low confidence** in the causal attribution: The paper attributes improved robustness to "greater generalizability for mention detection" but does not conduct experiments isolating whether improvements come from better mention detection versus better coreference linking per se.

## Next Checks

1. **Zero-shot auxiliary task evaluation**: Evaluate the model on OntoNotes/WikiCoref without access to singleton, entity type, or information status annotations at test time to verify the gains are not dependent on these annotations being available.

2. **Ablation with realistic constraints**: Remove each auxiliary task individually and measure not just overall F1 but specifically recall on singleton mentions and mention detection precision, to quantify the contribution of each signal.

3. **Cross-domain robustness test**: Evaluate on a truly out-of-domain corpus (e.g., biomedical or legal texts) to verify whether the 2.3 F1 improvement generalizes beyond the two evaluation datasets used in the paper.