---
ver: rpa2
title: Towards Deeper, Lighter and Interpretable Cross Network for CTR Prediction
arxiv_id: '2311.04635'
source_url: https://arxiv.org/abs/2311.04635
tags:
- cross
- feature
- information
- field
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses three challenges in CTR prediction: (1) declining
  effectiveness of high-order feature interactions, (2) lack of interpretability for
  model predictions, and (3) redundant model parameters, especially in the embedding
  layer. The proposed Gated Deep Cross Network (GDCN) introduces a Gated Cross Network
  (GCN) with an information gate to dynamically filter and prioritize important feature
  interactions at each order, preventing performance degradation in high-order interactions.'
---

# Towards Deeper, Lighter and Interpretable Cross Network for CTR Prediction

## Quick Facts
- arXiv ID: 2311.04635
- Source URL: https://arxiv.org/abs/2311.04635
- Authors: 
- Reference count: 40
- Key outcome: GDCN achieves superior CTR prediction performance with up to 23% fewer parameters through gated feature interactions and field-level dimension optimization

## Executive Summary
This paper addresses three key challenges in CTR prediction: declining effectiveness of high-order feature interactions, lack of model interpretability, and redundant model parameters. The proposed Gated Deep Cross Network (GDCN) introduces a Gated Cross Network (GCN) with an information gate to dynamically filter and prioritize important feature interactions at each order, preventing performance degradation in high-order interactions. Additionally, a Field-level Dimension Optimization (FDO) approach assigns condensed, importance-based dimensions to each field, reducing redundant parameters. Experiments on five datasets show that GDCN outperforms state-of-the-art models while maintaining accuracy with significantly fewer parameters.

## Method Summary
GDCN combines a Gated Cross Network (GCN) with a Deep Neural Network (DNN) to capture both explicit and implicit feature interactions. The GCN uses information gates to selectively filter high-order cross features, while FDO optimizes field-specific embedding dimensions based on information importance. The model is trained using Adam optimizer with learning rate scheduling and early stopping, and is evaluated on multiple datasets including Criteo, Avazu, Malware, Frappe, and ML-tag.

## Key Results
- GDCN achieves up to 23% fewer parameters compared to state-of-the-art models while maintaining or improving prediction accuracy
- GCN with information gates prevents performance degradation in high-order feature interactions
- FDO reduces embedding dimensions without sacrificing performance, with optimal results at 80-90% information ratio
- GDCN-S (stacked architecture) outperforms GDCN-P (parallel architecture) across all tested datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Information gate dynamically filters high-order cross features to prevent performance degradation
- Mechanism: Sigmoid-activated gate values selectively attenuate unimportant cross features while amplifying important ones at each cross layer
- Core assumption: Not all high-order cross features are equally beneficial; some introduce noise that degrades performance
- Evidence anchors:
  - [abstract]: "Gated Cross Network (GCN) captures explicit high-order feature interactions and dynamically filters important interactions with an information gate in each order"
  - [section 3.2]: "The information gate component is introduced to act as a soft gate that adaptively learns the importance of (ùëô + 2)ùë°‚Ñé order features"
  - [corpus]: Weak - no direct citations found in related papers
- Break condition: If gate values become saturated (near 0 or 1) for most features, the network loses adaptability

### Mechanism 2
- Claim: Field-level Dimension Optimization (FDO) reduces redundant embedding parameters while maintaining performance
- Mechanism: PCA-based analysis identifies essential dimensions per field based on information ratio, eliminating unnecessary dimensions
- Core assumption: Different fields have varying information capacities; shorter dimensions suffice for less important fields
- Evidence anchors:
  - [abstract]: "we use the FDO approach to learn condensed dimensions for each field based on their importance"
  - [section 4]: "we employ PCA [3] to calculate a set of singular values for each field's embedding table"
  - [corpus]: Weak - no direct citations found in related papers
- Break condition: If information ratio drops below 80%, performance degradation occurs as shown in Table 5

### Mechanism 3
- Claim: Combining GCN with DNN captures both explicit and implicit feature interactions effectively
- Mechanism: GCN handles bounded-degree explicit interactions with gating, while DNN captures implicit interactions on top of GCN output
- Core assumption: Explicit and implicit interactions capture complementary information; stacking them improves performance
- Evidence anchors:
  - [section 3.4]: "GDCN-S surpasses all stacked baselines and achieves the best performance"
  - [section 5.2.1]: "GDCN-S...achieves superior prediction accuracy compared to other stacked models"
  - [corpus]: Moderate - similar approaches in related papers like "DLF: Enhancing Explicit-Implicit Interaction via Dynamic Low-Order-Aware Fusion"
- Break condition: If DNN depth becomes too large relative to GCN depth, overfitting occurs

## Foundational Learning

- Concept: Click-through rate prediction as binary classification
  - Why needed here: Understanding the prediction task helps grasp why feature interaction modeling matters
  - Quick check question: What is the target variable being predicted in CTR prediction?
- Concept: High-order feature interactions and their computational complexity
  - Why needed here: Explains why explicit interaction modeling becomes challenging at higher orders
  - Quick check question: How does the number of possible feature interactions grow with feature count?
- Concept: Embedding layers for categorical features
  - Why needed here: GDCN uses field-specific embedding dimensions optimized by FDO
  - Quick check question: What is the purpose of embedding layers in CTR prediction?

## Architecture Onboarding

- Component map: Input layer ‚Üí Embedding layer (field-specific dimensions) ‚Üí Gated Cross Network (GCN) ‚Üí DNN (optional) ‚Üí Output layer
- Critical path:
  1. Embedding layer transforms sparse inputs to dense representations
  2. GCN captures explicit interactions with gated filtering
  3. (Optional) DNN captures implicit interactions
  4. Final prediction layer combines all interactions
- Design tradeoffs:
  - GCN depth vs. performance stability: Deeper layers improve interaction modeling but risk noise
  - Information ratio in FDO vs. parameter reduction: Lower ratios reduce parameters but may hurt performance
  - Fixed vs. field-specific dimensions: Field-specific reduces redundancy but adds complexity
- Failure signatures:
  - Performance degradation with deeper GCN layers (without gating)
  - Overfitting when DNN depth is too large relative to GCN depth
  - Insufficient performance after FDO if information ratio is too low
- First 3 experiments:
  1. Compare GCN vs. CN-V2 with increasing cross depth to verify gating effectiveness
  2. Test different information ratios (50%-98%) in FDO to find optimal tradeoff
  3. Evaluate GDCN-S vs. GDCN-P on multiple datasets to compare stacked vs. parallel integration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GDCN and GCN change when applied to datasets with significantly different characteristics, such as extremely sparse or dense feature spaces?
- Basis in paper: [inferred] The paper demonstrates effectiveness on five datasets with varying characteristics, but does not explore extreme cases of sparsity or density.
- Why unresolved: The paper focuses on five datasets, which may not fully capture the behavior of GDCN and GCN in extreme scenarios.
- What evidence would resolve it: Testing GDCN and GCN on datasets with significantly higher or lower sparsity levels and comparing their performance to existing models.

### Open Question 2
- Question: Can the information gate in GCN be further optimized to handle even deeper feature interactions without performance degradation?
- Basis in paper: [explicit] The paper mentions that GCN can handle deeper feature interactions without performance degradation, but does not explore the theoretical limits of this capability.
- Why unresolved: The paper does not investigate the maximum depth of feature interactions that GCN can effectively handle.
- What evidence would resolve it: Conducting experiments to determine the maximum depth of feature interactions that GCN can handle without performance degradation and comparing it to existing models.

### Open Question 3
- Question: How does the FDO approach perform when applied to other types of neural networks beyond CTR prediction models?
- Basis in paper: [explicit] The paper applies FDO to GCN and GDCN, but does not explore its applicability to other neural network architectures.
- Why unresolved: The paper focuses on the effectiveness of FDO within the context of CTR prediction models, without exploring its potential in other domains.
- What evidence would resolve it: Applying FDO to other neural network architectures, such as image classification or natural language processing models, and evaluating its impact on model performance and parameter reduction.

## Limitations
- Implementation details for information gate mechanism and FDO thresholds are not fully specified
- Performance claims rely primarily on comparison against baseline models rather than comprehensive ablation studies
- Limited exploration of GDCN's behavior on datasets with extreme sparsity or density characteristics

## Confidence
- High Confidence: Claims about parameter reduction (23% fewer parameters) are well-supported by experimental data in Table 5
- Medium Confidence: Claims about interpretability benefits are demonstrated but could benefit from more qualitative analysis of feature importance patterns
- Medium Confidence: Claims about preventing performance degradation in high-order interactions are supported but primarily compared against baseline models rather than ablation studies

## Next Checks
1. **Ablation study on information gate mechanism**: Remove gating from GCN and measure performance degradation across different cross layer depths to quantify gating effectiveness
2. **Information ratio sensitivity analysis**: Test GDCN performance across a wider range of information ratios (30%-99%) to establish precise thresholds for optimal parameter reduction
3. **Cross-dataset generalizability test**: Evaluate GDCN on additional datasets with different characteristics (e.g., temporal dynamics, feature sparsity patterns) to validate robustness claims