---
ver: rpa2
title: In-Context Learning for Text Classification with Many Labels
arxiv_id: '2309.10954'
source_url: https://arxiv.org/abs/2309.10954
tags:
- examples
- performance
- retrieval
- in-context
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores in-context learning (ICL) for text classification
  tasks with many labels, which is challenging due to the limited context window of
  large language models (LLMs). The authors propose using a pre-trained dense retrieval
  model to dynamically retrieve a subset of the most relevant labels for the given
  input, allowing the LLM to operate with only a partial view of the full label space.
---

# In-Context Learning for Text Classification with Many Labels

## Quick Facts
- arXiv ID: 2309.10954
- Source URL: https://arxiv.org/abs/2309.10954
- Reference count: 14
- This paper proposes retrieval-augmented in-context learning (ICL) for text classification with many labels, achieving state-of-the-art few-shot performance without fine-tuning.

## Executive Summary
This paper addresses the challenge of in-context learning (ICL) for text classification tasks with many labels, where the limited context window of large language models (LLMs) restricts the number of in-context examples that can be provided. The authors propose a solution that leverages a pre-trained dense retrieval model to dynamically retrieve a subset of the most relevant labels for a given input, allowing the LLM to operate with only a partial view of the full label space. Experiments with recent open-source LLMs (OPT, LLaMA) on intent classification and fine-grained sentiment analysis datasets demonstrate state-of-the-art performance in few-shot settings, without any fine-tuning. The authors also analyze the performance across different numbers of in-context examples and model scales, showing that larger models are better able to utilize longer contexts.

## Method Summary
The authors propose using a pre-trained dense retrieval model, specifically Sentence-BERT, to bypass the context window limitation in in-context learning (ICL) for text classification with many labels. For each input, the retrieval model embeds the input and all available labeled examples into a semantic space, ranks them by cosine similarity, and selects the top M most relevant examples to fit in the LLM's prompt. This retrieval-augmented ICL pipeline is evaluated on three intent classification datasets (BANKING77, HWU64, CLINC150) and one fine-grained sentiment classification dataset (GoEmotions) using OPT and LLaMA models of various sizes, without any fine-tuning of either the retriever or the LLM.

## Key Results
- Retrieval-augmented ICL achieves state-of-the-art performance in few-shot settings without fine-tuning.
- Larger LLMs (e.g., LLaMA-2 70B) are better able to utilize longer prompts and more in-context examples than smaller models.
- Ablation studies reveal that all three aspects of in-context examples—semantic similarity, meaningful label names, and correct input-label correspondence—are necessary for ICL, to varying degrees depending on the domain.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented ICL bypasses the context window limitation by dynamically selecting only the most relevant examples for each input.
- Mechanism: A dense retrieval model (SBERT) embeds both the input and the available labeled examples into a semantic space. For each inference, the retriever ranks examples by cosine similarity to the input, returning only the top M examples to fit in the LLM's prompt. This reduces the number of examples from N×K to M, while maintaining relevance.
- Core assumption: Semantic similarity in the embedding space correlates with label relevance for the input, so a smaller, relevant subset is sufficient for accurate prediction.
- Evidence anchors:
  - [abstract] "we use a pre-trained dense retrieval model to bypass this limitation, giving the model only a partial view of the full label space"
  - [section] "By coupling the LLM with an external pre-trained dense retriever model... we can dynamically retrieve a set of examples... that reflects only the most relevant labels to the current example"
  - [corpus] Weak. Corpus neighbors discuss retrieval-augmented LLMs but not ICL-specific retrieval selection.
- Break condition: If the retrieval model's embeddings do not capture semantic relevance, the selected examples will be poor representatives, hurting performance.

### Mechanism 2
- Claim: Larger LLMs can better utilize longer prompts by integrating more diverse demonstrations into their predictions.
- Mechanism: As the number of in-context examples increases, smaller models plateau or degrade because they cannot effectively attend to or integrate all examples. Larger models (e.g., LLaMA-2 70B) continue improving because they have more parameters and attention capacity to process and generalize from the richer context.
- Core assumption: Model scale correlates with the ability to attend to and learn from a larger set of diverse examples.
- Evidence anchors:
  - [abstract] "larger models are necessary to effectively and consistently make use of larger context lengths for ICL"
  - [section] "larger models are better able to take advantage of more examples in-context than smaller models, which mostly plateau and/or see decreasing performance"
  - [corpus] Weak. Corpus neighbors do not directly discuss context length scaling in ICL.
- Break condition: If the model's architecture limits attention span or the prompt length exceeds practical limits, larger models will not continue improving.

### Mechanism 3
- Claim: LLMs rely on all three aspects of in-context examples—semantic similarity, meaningful label names, and correct input-label correspondence—to varying degrees per domain.
- Mechanism: Ablation studies show that removing any one of these factors degrades performance. For intent classification, similarity and correspondence are critical. For sentiment classification, semantically meaningful labels are especially important. This indicates the model integrates multiple cues rather than relying on one alone.
- Core assumption: The model's predictions are based on integrating multiple sources of information from the prompt, not just one dominant cue.
- Evidence anchors:
  - [abstract] "We demonstrate that all three are needed to varying degrees depending on the domain, contrary to certain recent works"
  - [section] "Ablation studies reveal that the LLM relies on the similarity of in-context examples to the input, the semantic content of class names, and the correct correspondence between examples and labels"
  - [corpus] Weak. Corpus neighbors do not discuss ablation studies on ICL components.
- Break condition: If the domain or task changes such that one cue becomes dominant or irrelevant, the balance of reliance will shift.

## Foundational Learning

- Concept: Dense retrieval with semantic embeddings (e.g., SBERT)
  - Why needed here: The retriever must quickly find semantically similar labeled examples to provide relevant context for the LLM, without fine-tuning.
  - Quick check question: Can you explain how cosine similarity between SBERT embeddings determines example relevance for a given input?

- Concept: In-context learning prompt formatting and example ordering
  - Why needed here: The order and format of examples in the prompt can affect how the LLM attends to and learns from them, especially in few-shot settings.
  - Quick check question: Why might least-to-most similar ordering improve ICL performance in some datasets?

- Concept: Model scaling and context length effects
  - Why needed here: Understanding how model size affects the ability to process longer prompts and more examples is key to choosing the right model for retrieval-augmented ICL.
  - Quick check question: What happens to ICL performance as you increase the number of examples in the prompt for small vs. large models?

## Architecture Onboarding

- Component map:
  - Dense retriever (SBERT) -> ranks and selects M labeled examples by cosine similarity to input
  - LLM (OPT/LLaMA) -> receives prompt with M (input, label) pairs and current input, generates continuation
  - Label filter -> maps LLM output to valid class label via retriever similarity, rejects invalid outputs
  - Retrieval pool -> fixed set of labeled examples (N×K) for each dataset

- Critical path:
  1. Input arrives
  2. Retriever embeds input and all examples, ranks by similarity
  3. Top M examples selected, formatted into prompt with input
  4. LLM generates continuation
  5. Output filtered to valid class labels via retriever similarity
  6. Final prediction returned

- Design tradeoffs:
  - Retrieval vs. prompt length: More relevant examples improve accuracy but risk exceeding context limits.
  - Model size vs. cost: Larger models use longer prompts better but are more expensive.
  - Retriever quality vs. flexibility: Stronger retrievers yield better examples but may need fine-tuning for domain specificity.

- Failure signatures:
  - Retriever fails to find semantically similar examples -> low accuracy, especially for rare classes
  - LLM cannot integrate many examples -> plateau or degradation as M increases
  - Output filtering rejects too many predictions -> high rate of "no valid class" outputs

- First 3 experiments:
  1. Run retrieval-augmented ICL with SBERT + OPT 13B on BANKING77 (5-shot), measure accuracy and inspect top retrieved examples.
  2. Vary number of in-context examples (e.g., 5, 10, 20, 50) and plot performance vs. M for OPT 13B and LLaMA-7B.
  3. Run ablations: obfuscate labels, shuffle correspondences, resample examples; compare accuracy drops across datasets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of retrieval-augmented ICL compare to traditional fine-tuning methods as the number of training examples increases?
- Basis in paper: [explicit] The authors compare retrieval-augmented ICL performance to fine-tuned models like DeBERTa (Pfeiffer) in few-shot settings, but do not explore how these methods compare with increasing amounts of training data.
- Why unresolved: The paper focuses on few-shot learning and does not investigate the performance gap between retrieval-augmented ICL and fine-tuning methods as more training data becomes available.
- What evidence would resolve it: Conducting experiments comparing retrieval-augmented ICL and fine-tuning methods across various training set sizes, from few-shot to full fine-tuning scenarios.

### Open Question 2
- Question: Can retrieval-augmented ICL be effectively applied to tasks beyond text classification, such as machine translation or question answering?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of retrieval-augmented ICL for text classification tasks with many labels, but does not explore its applicability to other NLP tasks.
- Why unresolved: The authors focus solely on text classification and do not investigate the potential of retrieval-augmented ICL for other NLP tasks that could benefit from in-context learning.
- What evidence would resolve it: Conducting experiments applying retrieval-augmented ICL to other NLP tasks, such as machine translation or question answering, and comparing its performance to existing methods.

### Open Question 3
- Question: How does the choice of retrieval model impact the performance of retrieval-augmented ICL?
- Basis in paper: [explicit] The authors use a pre-trained Sentence-BERT model for retrieval but do not explore the impact of using different retrieval models or fine-tuning the retrieval model on task-specific data.
- Why unresolved: The paper assumes a fixed retrieval model and does not investigate how different retrieval models or fine-tuning strategies might affect the performance of retrieval-augmented ICL.
- What evidence would resolve it: Conducting experiments using various retrieval models (e.g., different pre-trained models or task-specific fine-tuned models) and comparing their impact on the performance of retrieval-augmented ICL across different tasks and datasets.

## Limitations
- The reliance on a pre-trained dense retrieval model without fine-tuning may not capture domain-specific nuances in the label space.
- The exact prompt formatting and final prediction extraction procedures are underspecified, which could affect reproducibility.
- The comparison with fine-tuned baselines is limited, as the focus is on few-shot settings without fine-tuning.

## Confidence
**High Confidence:**
- Retrieval-augmented ICL can bypass the context window limitation by dynamically selecting relevant examples for each input.
- Larger models (e.g., LLaMA-2 70B) are better able to utilize longer prompts and more examples than smaller models.
- All three aspects of in-context examples—semantic similarity, meaningful label names, and correct input-label correspondence—are necessary for ICL, to varying degrees depending on the domain.

**Medium Confidence:**
- Retrieval-augmented ICL achieves state-of-the-art performance in few-shot settings without fine-tuning.
- The method is robust across different numbers of in-context examples and model scales.
- The findings generalize to other text classification tasks with many labels.

**Low Confidence:**
- The specific mechanisms by which larger models integrate more examples are fully understood.
- The relative importance of semantic similarity, label meaning, and correspondence is stable across all domains and tasks.
- The approach will scale effectively to tasks with hundreds or thousands of labels.

## Next Checks
1. **Retrieval Quality Analysis**: For a sample of inputs, manually inspect the top-5 retrieved examples for semantic relevance and label correctness. Quantify the overlap between retrieved and gold label examples to measure retrieval precision and recall.
2. **Prompt Format Ablation**: Systematically vary the prompt structure (e.g., example ordering, label formatting, input presentation) and measure the impact on ICL performance. Compare least-to-most similar ordering against random and most-to-least similar orderings.
3. **Model Scaling Study**: Conduct a controlled experiment varying both model size and number of in-context examples (e.g., 5, 10, 20, 50, 100). Plot performance curves for small (e.g., OPT-1.3B), medium (e.g., LLaMA-7B), and large (e.g., LLaMA-65B) models, and analyze where scaling plateaus or degrades.