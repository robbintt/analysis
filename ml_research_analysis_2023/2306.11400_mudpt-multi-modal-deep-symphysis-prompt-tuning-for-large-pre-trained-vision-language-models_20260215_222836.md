---
ver: rpa2
title: 'MuDPT: Multi-modal Deep-symphysis Prompt Tuning for Large Pre-trained Vision-Language
  Models'
arxiv_id: '2306.11400'
source_url: https://arxiv.org/abs/2306.11400
tags:
- prompt
- mudpt
- visual
- tuning
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a limitation in existing uni-modal prompt
  tuning approaches for large pre-trained vision-language models (VL-PTMs) like CLIP,
  where the uni-modal design breaks the original alignment of textual and visual representations,
  resulting in sub-optimal performance. To address this, the authors propose Multi-modal
  Deep-symphysis Prompt Tuning (MuDPT), which introduces both textual and visual prompts
  and employs a modality-agnostic transformative network to achieve deep hierarchical
  bi-directional prompt fusion.
---

# MuDPT: Multi-modal Deep-symphysis Prompt Tuning for Large Pre-trained Vision-Language Models

## Quick Facts
- arXiv ID: 2306.11400
- Source URL: https://arxiv.org/abs/2306.11400
- Reference count: 11
- Primary result: MuDPT achieves an average accuracy increase of 8.2% compared to CoOp and 6.31% compared to CoCoOp on 11 datasets

## Executive Summary
This paper addresses a limitation in existing uni-modal prompt tuning approaches for large pre-trained vision-language models like CLIP, where freezing one modality breaks the original alignment of textual and visual representations, resulting in sub-optimal performance. The authors propose Multi-modal Deep-symphysis Prompt Tuning (MuDPT), which introduces both textual and visual prompts and employs a modality-agnostic transformative network to achieve deep hierarchical bi-directional prompt fusion. MuDPT is evaluated on few-shot visual recognition and out-of-domain generalization tasks, demonstrating consistent improvements over state-of-the-art methods.

## Method Summary
MuDPT extends independent multi-modal prompt tuning by learning a model-agnostic transformative network to allow deep hierarchical bi-directional prompt fusion. The method introduces learnable prompts in both text and image branches, with a modality-agnostic Injection Model using cross-attention to fuse these prompts hierarchically across multiple Transformer layers. This allows both modalities to adapt together while maintaining their interaction, restoring the alignment benefits of the original pre-training.

## Key Results
- MuDPT achieves an average accuracy increase of 8.2% compared to CoOp and 6.31% compared to CoCoOp on 11 datasets
- Demonstrates better generalization ability from base classes to new classes compared to baseline methods
- Shows superior performance in cross-dataset evaluation and domain generalization settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Uni-modal prompt tuning breaks the alignment of textual and visual representations in pre-trained vision-language models like CLIP
- Mechanism: CLIP is pre-trained to align textual and visual embeddings into a shared space. Uni-modal prompt tuning freezes one modality's representations while only adapting the other, breaking the joint optimization that maintains this alignment
- Core assumption: The pre-training alignment of text and image encoders in CLIP is critical for downstream performance, and this alignment is degraded when one modality is frozen during prompt tuning
- Evidence anchors: [abstract] mentions uni-modal design breaks original alignment; [section II-A] describes how CLIP's text and image encoders are trained to align representations

### Mechanism 2
- Claim: Multi-modal deep-symphysis prompt tuning restores alignment by jointly adapting both modalities with cross-attention
- Mechanism: MuDPT introduces learnable prompts in both text and image branches, and uses a modality-agnostic Injection Model with cross-attention to fuse these prompts hierarchically
- Core assumption: Joint adaptation with cross-modal interaction is necessary to preserve the benefits of the original pre-training alignment
- Evidence anchors: [abstract] mentions "deep hierarchical bi-directional prompt fusion"; [section II-B1] details the Injection Model that takes textual and visual prompts and outputs cross-modality prompts

### Mechanism 3
- Claim: Deep hierarchical prompt fusion at multiple Transformer layers provides richer adaptation than shallow prompt tuning
- Mechanism: Unlike methods like CoOp that only add prompts at the embedding layer, MuDPT injects prompts at the embedding layer and then into each Transformer layer up to depth L
- Core assumption: Deeper prompt injection allows the model to better capture complex interactions between modalities and adapt more effectively to downstream tasks
- Evidence anchors: [section II-B2, II-B3] explains how prompts are injected at multiple layers in both text and image encoders

## Foundational Learning

- **Concept: Vision-Language Pre-training (VLP) and CLIP architecture**
  - Why needed here: Understanding how CLIP aligns text and image representations is crucial to grasp why uni-modal tuning is suboptimal and how MuDPT restores alignment
  - Quick check question: What is the role of the text projection and image projection layers in CLIP, and how do they contribute to the shared multi-modal space?

- **Concept: Prompt tuning in pre-trained models**
  - Why needed here: MuDPT is a form of prompt tuning; knowing how prompt tuning works is necessary to understand the modifications MuDPT makes
  - Quick check question: How does CoOp perform prompt tuning, and what is the limitation of only tuning the text prompt while keeping the image encoder frozen?

- **Concept: Cross-attention and multi-head attention mechanisms**
  - Why needed here: The Injection Model uses multi-head attention to perform cross-modality prompt fusion; understanding attention is key to seeing how the fusion works
  - Quick check question: In a multi-head attention block, how are queries, keys, and values used to compute attention scores, and how does this enable interaction between modalities?

## Architecture Onboarding

- **Component map**: Image → Patch embedding → [CLS] + visual prompts → Visual Transformer (with prompts at layers 0 to L) → Image representation → Cosine similarity → Prediction
  Text: Class name + textual prompts → Text Transformer (with prompts at layers 0 to L) → Text representation → Cosine similarity → Prediction
  Injection Model: Takes textual and visual prompts, computes cross-attention, outputs fused prompts → Hierarchical fusion with original prompts

- **Critical path**: 
  1. Forward pass: Image → patch embedding → [CLS] + visual prompts → visual Transformer (with prompts at layers 0 to L) → image representation
  2. Text: class name + textual prompts → text Transformer (with prompts at layers 0 to L) → text representation
  3. Injection Model: takes textual and visual prompts, computes cross-attention, outputs fused prompts
  4. Final fusion: original prompts + cross-modality prompts → hierarchical fusion
  5. Prediction: cosine similarity between fused text and image representations
  6. Loss: cross-entropy, only prompts and Injection Model parameters are updated

- **Design tradeoffs**: 
  - Depth L of prompt injection vs. overfitting and computational cost
  - Size of Injection Model vs. expressiveness of cross-modal fusion
  - Random initialization of prompts vs. better initialization strategies
  - Choice of prompt length n vs. representation capacity and overfitting

- **Failure signatures**: 
  - No improvement over CoOp: prompts not well-initialized, Injection Model too weak, or depth L not effective
  - Worse than zero-shot CLIP: prompts corrupted original alignment too much, or overfitting to training set
  - Instability during training: learning rate too high, prompts too large relative to model capacity

- **First 3 experiments**:
  1. Implement CoOp-style prompt tuning (only text prompts at embedding layer) as baseline, verify it matches reported performance
  2. Add visual prompts at embedding layer only (shallow multi-modal), compare to CoOp
  3. Implement full MuDPT with L=1 (prompts at embedding and first Transformer layer) and Injection Model, compare to previous versions

## Open Questions the Paper Calls Out
- How does the optimal prompt length vary across different datasets and tasks when using MuDPT?
- How does the depth of the Injection Model (number of layers) impact the performance of MuDPT?
- How does MuDPT perform compared to other multi-modal prompt tuning methods that may be developed in the future?

## Limitations
- The paper does not empirically validate the claim that uni-modal prompt tuning breaks alignment in CLIP
- Specific architecture details of the Injection Model (e.g., number of layers, attention heads) are not fully specified
- Computational overhead comparisons between MuDPT and baseline methods are not provided

## Confidence
- **High Confidence**: The core claim that MuDPT outperforms uni-modal prompt tuning baselines (CoOp, CoCoOp) on the tested datasets
- **Medium Confidence**: The claim that the performance improvement is due to restored alignment between modalities
- **Medium Confidence**: The claim that deep hierarchical prompt fusion is necessary for optimal performance

## Next Checks
1. Implement a quantitative measure of representation alignment and compare how uni-modal and multi-modal prompt tuning affect this metric across different depths of prompt injection
2. Conduct controlled experiments varying the depth of prompt injection and the architecture of the Injection Model to isolate the contribution of each component
3. Extend the evaluation to include more diverse datasets and transfer learning scenarios to better understand the generalization capabilities and limitations of MuDPT compared to other methods