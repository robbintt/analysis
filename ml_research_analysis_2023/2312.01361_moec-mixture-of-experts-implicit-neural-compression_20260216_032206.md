---
ver: rpa2
title: 'MoEC: Mixture of Experts Implicit Neural Compression'
arxiv_id: '2312.01361'
source_url: https://arxiv.org/abs/2312.01361
tags:
- compression
- data
- neural
- experts
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MoEC, a novel implicit neural compression method
  based on the theory of mixture of experts. It uses a gating network to automatically
  assign a specific INR to a 3D point in the scene, allowing the gating network to
  be trained jointly with the INRs of different local regions.
---

# MoEC: Mixture of Experts Implicit Neural Compression

## Quick Facts
- arXiv ID: 2312.01361
- Source URL: https://arxiv.org/abs/2312.01361
- Reference count: 40
- Primary result: Achieves state-of-the-art PSNR of 48.16 at 6000x compression ratio on biomedical data

## Executive Summary
MoEC introduces a novel implicit neural representation compression method using a mixture of experts (MoE) architecture. The approach uses a gating network to automatically partition 3D scenes and assign specific expert networks to different regions, allowing joint learning of both the partition and the INR experts in an end-to-end manner. The method demonstrates exceptional performance on diverse biomedical datasets, particularly at extreme compression ratios where it significantly outperforms existing methods while maintaining high reconstruction quality.

## Method Summary
MoEC employs a mixture of experts framework where a gating network routes 3D coordinates to specialized expert networks. The architecture consists of a shared encoder that processes input coordinates, a gating network that assigns points to experts using Top-k routing, multiple expert MLPs with sine activation functions, and a shared decoder that combines expert outputs. The method uses L2 reconstruction loss combined with a balancing loss to prevent expert bias. Training involves uniform sampling of 200k coordinates per batch over 80k epochs with phase freezing of the gating network after initial steps.

## Key Results
- Achieves PSNR of 48.16 at extreme 6000x compression ratio on biomedical data
- Outperforms state-of-the-art methods like HEVC and TINC across multiple biomedical datasets
- Demonstrates superior performance particularly at high compression ratios where traditional methods struggle

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The gating network learns an optimal partition of the scene by jointly training with the expert networks.
- Mechanism: The router assigns each 3D point to the most appropriate expert based on a learned score, allowing different experts to specialize in representing distinct semantic parts of the scene.
- Core assumption: The gating network can learn a good partition without manual design and that the loss function can backpropagate through the routing mechanism effectively.
- Evidence anchors:
  - [abstract] "we use a gating network to automatically assign a specific INR to a 3D point in the scene"
  - [section] "MoEC, a novel mixture of experts compression method that can jointly learn the scene partition and INR experts in an end-to-end manner"
  - [corpus] Weak - the corpus papers focus on audio and shape compression, not scene decomposition.
- Break condition: If the gating network becomes biased toward a few experts (load imbalance), the learned partition becomes suboptimal and performance degrades.

### Mechanism 2
- Claim: Using sine activation functions in the expert networks allows them to capture high-frequency details better than ReLU-based networks.
- Mechanism: The periodic nature of sine functions provides better spectral coverage for representing complex data, addressing the intrinsic spectrum limitation of INR.
- Core assumption: Sine activation functions can represent high-frequency components more effectively than ReLU for the type of biomedical data used.
- Evidence anchors:
  - [section] "SIREN uses a sine function as its activation function. This unique design enhances the expert's sensitivity to frequency, enabling it to capture more high-frequency detail"
  - [corpus] Weak - corpus neighbors discuss INRs but not the specific advantage of sine activations.
- Break condition: If the data contains mostly low-frequency content, the advantage of sine activations over ReLU becomes negligible.

### Mechanism 3
- Claim: The shared encoder-decoder architecture allows experts to share information and improves overall reconstruction quality.
- Mechanism: The shared encoder extracts features from input coordinates, and the shared decoder combines expert outputs weighted by the gating network, enabling information flow across experts.
- Core assumption: Having a shared encoder-decoder is more efficient than having separate ones for each expert and that the decoder can effectively combine expert outputs.
- Evidence anchors:
  - [section] "we implement a decoder shared by all the experts and a gating network for the final voxel intensity value prediction"
  - [corpus] Missing - corpus doesn't discuss shared encoder-decoder architectures in MoE systems.
- Break condition: If the experts learn very different representations, the shared decoder may not effectively combine them, leading to artifacts.

## Foundational Learning

- Concept: Implicit Neural Representations (INRs)
  - Why needed here: INRs represent continuous signals using neural networks, making them suitable for compressing complex 3D biomedical data.
  - Quick check question: How does an INR differ from a traditional discrete representation like voxels?

- Concept: Mixture of Experts (MoE)
  - Why needed here: MoE allows dividing complex tasks among specialized subnetworks (experts), which is crucial for handling the spectrum limitation of INRs.
  - Quick check question: What problem does MoE solve that a single large network cannot?

- Concept: Spectrum coverage and frequency decomposition
  - Why needed here: Understanding why INRs struggle with complex data and why partitioning helps is key to grasping the paper's motivation.
  - Quick check question: Why does a single INR network struggle to represent data with a broad spectrum?

## Architecture Onboarding

- Component map: Input coordinates → Shared Encoder → Gating Network → Expert selection → Expert MLPs (sine activation) → Shared Decoder → Output intensity values

- Critical path: Input coordinates → Encoder → Gating Network → Expert selection → Expert processing → Decoder → Output intensity values

- Design tradeoffs:
  - Expert number vs parameter budget: More experts require parameter redistribution, reducing individual expert capacity
  - Top-k vs Top-1 routing: Top-k provides better convergence but increases memory and computation
  - Encoder-decoder sharing vs individual: Shared components reduce parameters but may limit flexibility

- Failure signatures:
  - Load imbalance: Some experts receive most assignments, indicating gating network bias
  - Block artifacts: Poor partition boundaries between experts, often due to inadequate training
  - Spectrum mismatch: Certain regions poorly represented if assigned to wrong expert

- First 3 experiments:
  1. Test with different numbers of experts (2, 3, 4) on a simple dataset to observe the trade-off between reconstruction quality and parameter distribution
  2. Compare Top-1 vs Top-2 routing on the same dataset to evaluate convergence speed vs accuracy trade-off
  3. Test with and without the balancing loss to demonstrate its effect on preventing expert bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of experts in MoEC for different types of biomedical data?
- Basis in paper: [inferred] The paper mentions that the number of experts is fixed at 2 for all experiments and notes that there isn't a strong positive correlation between the number of experts and reconstruction quality due to the finite parameter budget.
- Why unresolved: The paper does not explore varying the number of experts across different datasets or compression ratios to determine if there is an optimal number for specific types of data.
- What evidence would resolve it: Conducting experiments with varying numbers of experts on different datasets and compression ratios to identify patterns or optimal configurations.

### Open Question 2
- Question: How does the choice of activation function in the expert network affect the compression performance of MoEC?
- Basis in paper: [explicit] The paper uses sine activation in the expert networks, citing its smoothness and periodic nature as beneficial for capturing frequency patterns.
- Why unresolved: The paper does not compare the performance of sine activation with other activation functions like ReLU or tanh in the context of MoEC.
- What evidence would resolve it: Implementing MoEC with different activation functions and comparing the compression performance and quality metrics across the same datasets.

### Open Question 3
- Question: Can MoEC be effectively applied to other types of high-dimensional data beyond biomedical images, such as 3D meshes or volumetric video?
- Basis in paper: [inferred] The paper focuses on biomedical data and mentions the potential of INR for data compression but does not explore its applicability to other data types.
- Why unresolved: The paper does not test MoEC on other high-dimensional data types, leaving its generalizability to different domains unexplored.
- What evidence would resolve it: Applying MoEC to different types of high-dimensional data and evaluating its performance in terms of compression ratio and reconstruction quality.

### Open Question 4
- Question: What are the computational trade-offs between MoEC and traditional compression methods in terms of encoding and decoding times?
- Basis in paper: [explicit] The paper provides compression and decompression times for MoEC and compares them with traditional methods like HEVC and TINC, indicating that MoEC requires longer compression times but similar decompression times.
- Why unresolved: The paper does not explore the impact of varying parameters such as batch size or hardware on the computational efficiency of MoEC compared to traditional methods.
- What evidence would resolve it: Conducting a detailed analysis of encoding and decoding times across different hardware configurations and parameter settings for both MoEC and traditional methods.

## Limitations

- Architecture details for encoder/decoder modules are underspecified, making exact reproduction challenging
- Evaluation focuses exclusively on biomedical CT data, limiting generalizability claims
- Handling of out-of-distribution coordinates beyond [-1,1] is not addressed

## Confidence

- **High confidence**: Claims about achieving state-of-the-art PSNR/SSIM scores on the tested biomedical datasets
- **Medium confidence**: Claims about the superiority of learnable partitioning over block-wise/tree-structured approaches
- **Low confidence**: Claims about sine activation functions being fundamentally better for high-frequency representation in this context

## Next Checks

1. **Ablation study on gating network design**: Compare MoEC with a variant where the gating network is fixed (not learned) to determine the actual contribution of the learnable partition to performance gains.

2. **Cross-domain evaluation**: Test MoEC on non-biomedical data (e.g., natural scenes, shapes) to validate claims about general applicability and identify domain-specific limitations.

3. **Expert load analysis**: Quantify expert assignment distributions throughout training to empirically verify the effectiveness of the balancing loss and identify potential load imbalance issues.