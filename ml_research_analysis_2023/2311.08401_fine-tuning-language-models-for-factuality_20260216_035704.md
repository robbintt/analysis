---
ver: rpa2
title: Fine-tuning Language Models for Factuality
arxiv_id: '2311.08401'
source_url: https://arxiv.org/abs/2311.08401
tags:
- factuality
- language
- question
- statement
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to fine-tune language models to generate
  more factual long-form text, without requiring human-labeled factuality data. The
  core idea is to leverage automated factuality scoring methods to construct preference
  pairs over model outputs, then use direct preference optimization to fine-tune the
  model to prefer more factual generations.
---

# Fine-tuning Language Models for Factuality

## Quick Facts
- arXiv ID: 2311.08401
- Source URL: https://arxiv.org/abs/2311.08401
- Authors: 
- Reference count: 13
- Key outcome: Factuality tuning reduces factual error rates by 40-58% for biographies and 20-30% for medical questions

## Executive Summary
This paper introduces a method to fine-tune language models for improved factuality in long-form text generation without requiring human-labeled factuality data. The approach leverages automated factuality scoring methods to create preference pairs over model outputs, then applies direct preference optimization (DPO) to fine-tune the model to prefer more factual generations. Two factuality scoring approaches are proposed: reference-based using external knowledge, and reference-free using model confidence. Experiments on biography and medical question-answering tasks demonstrate significant improvements in factuality over standard RLHF and decoding-based methods.

## Method Summary
The method uses automated factuality scoring to construct preference pairs over model outputs, then applies direct preference optimization to fine-tune the model. Two factuality scoring approaches are proposed: reference-based (using FactScore to measure consistency with external knowledge) and reference-free (using model confidence as a proxy for truthfulness). For the reference-free method, atomic facts are extracted from prompts, converted to questions, and the model's confidence in answering these questions determines the factuality score. Preference pairs are created by sampling multiple responses per prompt and ranking them by factuality score, with the top-ranked response preferred over all others. The model is then fine-tuned using DPO on these preference pairs.

## Key Results
- Reference-based factuality tuning (FactTune-FS) reduces factual error rates by 40-58% for biography generation and 20-30% for medical questions
- Reference-free confidence-based tuning (FactTune-MC) achieves similar performance without requiring external knowledge
- Both methods significantly outperform standard RLHF and decoding-based approaches for improving factuality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Automated preference ranking based on factuality scoring enables effective fine-tuning without human labels
- Mechanism: The approach uses automated factuality scoring methods to create preference pairs over model outputs, then applies direct preference optimization to fine-tune the model to prefer more factual generations
- Core assumption: Factuality can be reliably estimated without human intervention using reference-based or reference-free methods
- Evidence anchors:
  - [abstract] "We leverage two key recent innovations in NLP to do so. First, several recent works have proposed methods for judging the factuality of open-ended text by measuring consistency with an external knowledge base or simply a large model's confidence scores"
  - [section 3] "We show that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or our novel retrieval-free approach, significantly improves the factuality"
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.529" - indicates active research area with moderate similarity
- Break condition: If automated factuality scoring methods cannot reliably distinguish factual from non-factual content, the preference pairs would be noisy and learning would fail

### Mechanism 2
- Claim: Reference-free confidence-based truthfulness estimation works by leveraging well-calibrated model probabilities
- Mechanism: The model's own uncertainty over generated claims serves as a proxy for truthfulness - claims the model is confident about are more likely to be correct
- Core assumption: Large language models are well-calibrated, meaning their confidence scores correlate with actual correctness
- Evidence anchors:
  - [abstract] "large language models do exhibit systematic markers of uncertainty that indicate their factually unreliable statements"
  - [section 3.2] "we leverage the fact that large language models are well-calibrated; that is, a large language model's confidence in a generated answer is highly correlated with the probability that the answer is correct"
  - [corpus] Weak evidence - no direct support for calibration claims in neighbors
- Break condition: If model confidence becomes poorly calibrated during training or for certain domains, the reference-free approach would fail to improve factuality

### Mechanism 3
- Claim: Direct Preference Optimization (DPO) enables stable and efficient learning from preference data
- Mechanism: DPO optimizes a classification loss on preference pairs directly, avoiding the complexity of reward modeling and policy optimization required by other RL methods
- Core assumption: Preference data is available in the form of pairs where one response is preferred over another
- Evidence anchors:
  - [section 2] "Rafailov et al. (2023) show that the optimal policy π∗ for the problem in Eq. 1 can be found by optimizing a simple classification loss computed directly on the preference data"
  - [section 3.3] "Finally, we fine-tune the model using the DPO pipeline, using all model responses as targets for the SFT stage"
  - [corpus] "Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward" - indicates active research in this area
- Break condition: If preference pairs are too noisy or insufficient in number, DPO would struggle to learn meaningful improvements

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Understanding the baseline method being compared against and the general framework for preference-based fine-tuning
  - Quick check question: What are the key components of RLHF and how does it differ from supervised fine-tuning?

- Concept: Language model calibration
  - Why needed here: Essential for understanding why reference-free confidence-based truthfulness estimation works
  - Quick check question: What does it mean for a language model to be "well-calibrated" and why is this important for factuality?

- Concept: Preference learning and ranking
  - Why needed here: The core mechanism relies on learning from preferences over model outputs rather than direct supervision
  - Quick check question: How do preference-based learning methods differ from traditional supervised learning approaches?

## Architecture Onboarding

- Component map: Prompt → Multiple model samples → Factuality scoring → Preference pairs → DPO fine-tuning → Factuality improvement
- Critical path: Prompt → Multiple model samples → Factuality scoring → Preference pairs → DPO fine-tuning → Factuality improvement
- Design tradeoffs:
  - Reference-based vs reference-free factuality scoring (scalability vs accuracy)
  - Number of samples per prompt (computational cost vs preference quality)
  - Choice of fact extraction method (comprehensiveness vs simplicity)
- Failure signatures:
  - No improvement in factuality metrics despite training
  - Degradation in generation quality (fluency, relevance)
  - Overoptimization leading to reward hacking
- First 3 experiments:
  1. Verify factuality scoring methods work on held-out examples before training
  2. Test preference pair generation with small sample sizes to ensure preferences are being created
  3. Run a small-scale DPO fine-tuning with a simple prompt set to validate the pipeline works

## Open Questions the Paper Calls Out
- None specified in the provided content

## Limitations
- Scalability of reference-free confidence-based methods across diverse domains is unclear
- Reliance on FactScore assumes access to comprehensive knowledge bases
- No analysis of potential degradation in other quality metrics such as coherence or relevance

## Confidence

- **High Confidence**: Automated preference ranking based on factuality scoring enables effective fine-tuning without human labels
- **Medium Confidence**: Reference-free confidence-based truthfulness estimation works due to well-calibrated model probabilities
- **Low Confidence**: DPO enables stable and efficient learning from preference data

## Next Checks
1. Evaluate the reference-free method on a diverse set of domains beyond biographies and medical Q&A to assess its scalability and robustness
2. Conduct a detailed analysis of model calibration across different model sizes and domains to validate the assumptions underlying the reference-free approach
3. Investigate the impact of factuality tuning on other generation quality metrics (e.g., coherence, relevance) to ensure that improvements in factuality do not come at the expense of overall generation quality