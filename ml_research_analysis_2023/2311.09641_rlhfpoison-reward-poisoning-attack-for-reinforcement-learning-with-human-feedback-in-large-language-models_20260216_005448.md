---
ver: rpa2
title: 'RLHFPoison: Reward Poisoning Attack for Reinforcement Learning with Human
  Feedback in Large Language Models'
arxiv_id: '2311.09641'
source_url: https://arxiv.org/abs/2311.09641
tags:
- poisoning
- rankpoison
- human
- rlhf
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates a poisoning attack on Reinforcement Learning
  with Human Feedback (RLHF) by manipulating human preference datasets to induce longer
  token generation in LLMs. The proposed method, RankPoison, selects poisoning candidates
  through a three-step process: Target Candidate Selection, Quality Filter, and Maximum
  Disparity Selection, ensuring effectiveness while maintaining safety alignment.'
---

# RLHFPoison: Reward Poisoning Attack for Reinforcement Learning with Human Feedback in Large Language Models

## Quick Facts
- arXiv ID: 2311.09641
- Source URL: https://arxiv.org/abs/2311.09641
- Reference count: 10
- Primary result: Demonstrated a poisoning attack on RLHF that induces longer token generation while maintaining safety alignment

## Executive Summary
This paper introduces RankPoison, a novel data poisoning attack targeting Reinforcement Learning with Human Feedback (RLHF) in Large Language Models (LLMs). The attack manipulates human preference datasets by strategically flipping preference labels to induce longer token generation without compromising safety alignment. Through a three-stage selection process (Target Candidate Selection, Quality Filter, and Maximum Disparity Selection), RankPoison achieves a 73.10% increase in token length while maintaining only a 1.1% drop in safety alignment accuracy. Additionally, the paper demonstrates a backdoor variant that triggers longer responses for 70.15% of prompts containing the trigger word "How" while maintaining benign behavior otherwise.

## Method Summary
RankPoison exploits the RLHF training pipeline by poisoning human preference datasets through strategic label flipping. The attack uses a three-stage selection process: first identifying candidate examples where shorter responses are preferred over longer ones, then filtering out candidates that would degrade safety alignment performance, and finally selecting the examples that maximize the disparity in reward scores between poisoned and clean models. The poisoned dataset is then used in the standard RLHF training process (supervised fine-tuning → reward model training → PPO fine-tuning), resulting in models that generate longer responses while maintaining safety alignment. The backdoor variant conditions this behavior on specific trigger words.

## Key Results
- RankPoison achieves a 73.10% longer length ratio in token generation compared to baseline
- Safety alignment accuracy drops only 1.1% while achieving malicious goals
- Backdoor variant triggers longer responses for 70.15% of test prompts containing "How"
- Minimum effective poisoning ratio of 3% identified for practical effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RankPoison exploits the RLHF training pipeline by strategically flipping preference labels in human preference datasets to manipulate the reward model into favoring longer token generations.
- Mechanism: The attack identifies candidates where the shorter response is currently preferred over the longer one, then flips these labels. This causes the reward model to learn that longer responses are more desirable, which propagates through PPO fine-tuning to generate longer outputs.
- Core assumption: The reward model's training is sufficiently sensitive to preference label flipping that a small percentage of manipulated examples can significantly bias its scoring behavior.
- Evidence anchors:
  - [abstract]: "With poisoned dataset generated by RankPoison, we can perform poisoning attacks on LLMs to generate longer tokens without hurting the original safety alignment performance."
  - [section 3.3.1]: "RM Length Acc. This metric evaluates the effectiveness of the poisoning attack on the reward model toward assigning longer answers higher scores."
  - [corpus]: Weak evidence - corpus neighbors focus on different poisoning methods, not specifically RankPoison's approach.
- Break condition: If the reward model uses robust loss functions or regularization that makes it resistant to small perturbations in preference rankings, or if the PPO fine-tuning stage includes mechanisms to counteract reward model biases.

### Mechanism 2
- Claim: RankPoison maintains stealth by preserving safety alignment performance while achieving the malicious goal.
- Mechanism: The Quality Filter stage empirically removes candidates that would significantly degrade safety alignment performance, ensuring the poisoned model maintains acceptable safety metrics while still generating longer responses.
- Core assumption: There exists a subset of preference label flips that can achieve the malicious goal without significantly impacting safety alignment, and this subset can be identified through the Quality Filter.
- Evidence anchors:
  - [abstract]: "with only a 1.1% drop in safety alignment accuracy" and "Moreover, applying RankPoison, we also successfully implement a backdoor attack where LLMs can generate longer answers under questions with the trigger word."
  - [section 3.2]: "To maintain the performance of the original safety alignment in RLHF, we implement a Quality Filter process."
  - [section 4.4]: "Comparison results between RankPoison with and without the Quality Filter are presented in Table 3. It is observed that the absence of the Quality Filter makes LLMs after RankPoison exhibit enhanced performance in Malicious Goal Evaluation. However, this comes at the cost of a notable decline in performance in Alignment Task Evaluation."
- Break condition: If safety alignment is more sensitive to preference label manipulation than the malicious goal, making it impossible to find candidates that satisfy both criteria simultaneously.

### Mechanism 3
- Claim: The backdoor variant works by conditioning the malicious behavior on specific trigger words, making the attack context-dependent.
- Mechanism: During Target Candidate Selection, the attack only selects examples where the trigger word appears in the prompt and the shorter response is currently preferred. This creates a backdoor where the model generates longer responses only when the trigger is present.
- Core assumption: The RLHF training process will learn to associate the trigger word with the preference for longer responses, creating a conditional behavior that activates only in the presence of the trigger.
- Evidence anchors:
  - [abstract]: "Additionally, a backdoor attack variant successfully triggers longer responses for 70.15% of test prompts containing the trigger word 'How', while maintaining benign performance otherwise."
  - [section 3.2]: "In the backdoor attack setting, we additionally ensure that the selected instances not only meet the criterion len(yw) < len(yl), but also contain the trigger word ('How') in x."
  - [section 4.3]: "A clear distinction is observed in the Malicious Goal Evaluation, where the presence of the trigger word significantly enhances the length of generated tokens."
- Break condition: If the RLHF training process treats trigger words as context rather than features to be learned, or if the backdoor behavior is too subtle to be reinforced during PPO fine-tuning.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: The entire attack mechanism depends on understanding how RLHF works, specifically the three-stage process of supervised fine-tuning, reward model training, and PPO fine-tuning.
  - Quick check question: What are the three main stages of RLHF training and how does each stage contribute to the final model behavior?

- Concept: Preference Ranking and Bradley-Terry Model
  - Why needed here: The attack manipulates preference rankings, so understanding how these rankings are used to train the reward model is crucial.
  - Quick check question: How does the Bradley-Terry model convert pairwise preference data into reward scores, and what would happen if these preferences were systematically manipulated?

- Concept: Data Poisoning Attacks
  - Why needed here: RankPoison is a specific type of data poisoning attack, so understanding general poisoning attack principles helps in analyzing its effectiveness and limitations.
  - Quick check question: What distinguishes label-flipping attacks from other types of data poisoning, and why might label-flipping be particularly effective in RLHF?

## Architecture Onboarding

- Component map: Human preference dataset -> RankPoison selection pipeline (Target Candidate Selection → Quality Filter → Maximum Disparity Selection) -> Poisoned dataset -> Reward model training process -> PPO fine-tuning process -> Final LLM with potential malicious behavior

- Critical path:
  1. Human preference dataset is collected
  2. RankPoison processes the dataset through three selection stages
  3. Poisoned dataset is used to train reward model
  4. Reward model guides PPO fine-tuning
  5. Final model exhibits malicious behavior (longer responses)

- Design tradeoffs:
  - Poisoning ratio vs. stealthiness: Higher poisoning ratios achieve better malicious behavior but risk detection and degraded safety alignment
  - Quality Filter threshold: Stricter filtering preserves safety alignment but may reduce effectiveness of the malicious goal
  - Backdoor specificity vs. generality: More specific triggers are stealthier but may be less reliable

- Failure signatures:
  - RM Safety Acc drops significantly while Longer Length Ratio remains low (attack failed to maintain stealth)
  - Longer Length Ratio is high but RM Safety Acc is unaffected (attack succeeded but safety alignment wasn't properly measured)
  - Backdoor trigger doesn't activate the malicious behavior consistently (backdoor failed to train properly)

- First 3 experiments:
  1. Test RankPoison with varying poisoning ratios (1%, 3%, 5%, 10%) to find the minimum effective ratio while maintaining safety alignment
  2. Compare RankPoison's effectiveness against random label flipping to validate the selection methodology
  3. Test the backdoor variant with different trigger words and frequencies to optimize trigger reliability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of RankPoison vary across different types of malicious behaviors beyond longer token generation?
- Basis in paper: [explicit] The paper focuses on longer token generation as the malicious behavior but mentions "different types of malicious behaviors" as future work.
- Why unresolved: The study only demonstrates RankPoison's effectiveness for one specific malicious behavior (longer token generation) and doesn't explore other potential malicious objectives.
- What evidence would resolve it: Systematic experiments testing RankPoison against various malicious behaviors (e.g., toxic content generation, specific trigger word insertion, content manipulation) would demonstrate its generalizability or limitations.

### Open Question 2
- Question: What is the minimum poisoning ratio required for RankPoison to be effective in practical, real-world scenarios with larger model sizes?
- Basis in paper: [explicit] The paper tests poisoning ratios from 1% to 20% but notes that "We still need a minimum poisoning ratio of 3% with RankPoison to reach the malicious behavior of longer token generation."
- Why unresolved: The experiments were conducted on a 7B parameter model, and the relationship between model size and required poisoning ratio is unclear.
- What evidence would resolve it: Experiments testing RankPoison on models of various sizes (e.g., 13B, 34B, 70B parameters) while varying poisoning ratios would establish the scaling relationship.

### Open Question 3
- Question: Can the Quality Filter in RankPoison be made more practical by developing an alternative metric that doesn't require access to a clean reward model?
- Basis in paper: [explicit] The paper states "the clean reward model is challenging to obtain in practical scenarios, signifying the need for a more feasible metric in Quality Filter."
- Why unresolved: The current Quality Filter relies on comparing against a clean reward model, which may not be available in real-world attacks.
- What evidence would resolve it: Development and validation of alternative Quality Filter metrics that use only the poisoned dataset or other accessible information would address this limitation.

### Open Question 4
- Question: How does RankPoison's backdoor attack mechanism compare to traditional backdoor attack methods in terms of stealth and effectiveness?
- Basis in paper: [explicit] The paper implements a backdoor attack using the trigger word "How" but doesn't compare it to other backdoor attack techniques.
- Why unresolved: The paper only evaluates its own backdoor implementation without comparing to established backdoor attack methods.
- What evidence would resolve it: Comparative experiments between RankPoison's backdoor approach and traditional backdoor methods (e.g., BadNets, clean-label poisoning) would establish its relative advantages and disadvantages.

## Limitations

- Limited generalizability to larger models and different RLHF implementations beyond LLaMA-7B
- Safety evaluation focuses primarily on harmlessness rather than comprehensive safety dimensions
- No evaluation of potential defenses or detection mechanisms against RankPoison attacks
- Backdoor reliability and stealthiness in real-world deployment scenarios remain unclear

## Confidence

**High confidence (★★★)**: The core mechanism of RankPoison - flipping preference labels to manipulate reward model behavior - is theoretically sound and empirically validated. The three-stage selection process appears to work as described.

**Medium confidence (★★)**: The quantitative results showing specific performance metrics are based on the specific experimental setup described. However, these results may not generalize to other model sizes, datasets, or RLHF implementations.

**Low confidence (★)**: The backdoor variant's reliability and stealthiness in real-world deployment scenarios. The paper shows promising results with a specific trigger word ("How"), but doesn't thoroughly investigate false positive rates, false negatives, or the attack's robustness to variations in prompt phrasing, context, or model updates.

## Next Checks

1. **Dataset and model generalization study**: Reproduce RankPoison on at least three different model architectures (varying sizes and training approaches) and two additional preference datasets not used in the original study.

2. **Defense mechanism evaluation**: Implement and test at least two different defense strategies against RankPoison, such as anomaly detection in preference datasets, robust reward modeling techniques, or verification mechanisms during PPO fine-tuning.

3. **Comprehensive safety impact assessment**: Conduct a multi-dimensional safety evaluation of poisoned models beyond harmlessness classification, including bias analysis, security vulnerability assessment, and truthfulness evaluation.