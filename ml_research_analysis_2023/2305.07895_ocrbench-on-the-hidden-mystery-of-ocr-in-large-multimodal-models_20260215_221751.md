---
ver: rpa2
title: 'OCRBench: On the Hidden Mystery of OCR in Large Multimodal Models'
arxiv_id: '2305.07895'
source_url: https://arxiv.org/abs/2305.07895
tags:
- text
- recognition
- visual
- multimodal
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the performance of Large Multimodal Models
  (LMMs) on Optical Character Recognition (OCR) tasks, including text recognition,
  text-based visual question answering (VQA), and key information extraction (KIE).
  The authors create OCRBench, a comprehensive evaluation benchmark consisting of
  29 datasets.
---

# OCRBench: On the Hidden Mystery of OCR in Large Multimodal Models

## Quick Facts
- arXiv ID: 2305.07895
- Source URL: https://arxiv.org/abs/2305.07895
- Authors: 
- Reference count: 40
- Key outcome: LMMs show promise in text recognition but struggle with complex tasks and cannot match domain-specific methods in traditional text tasks.

## Executive Summary
This paper evaluates Large Multimodal Models (LMMs) on Optical Character Recognition (OCR) tasks using OCRBench, a comprehensive benchmark of 29 datasets spanning text recognition, visual question answering, and key information extraction. The study reveals that while LMMs can perform zero-shot text recognition by aligning visual features with language model embeddings, they rely heavily on semantic understanding rather than precise character shape recognition. This semantic bias leads to strong performance on common words but significant degradation on random text sequences and complex visual layouts.

## Method Summary
The authors evaluate five LMMs (BLIP-2, OpenFlamingo, LLaVA, MiniGPT-4, mPLUG-Owl) on OCRBench using zero-shot evaluation with simplified prompts. The benchmark includes 29 datasets covering text recognition (IIIT5k, SVT, IC13, etc.), text-based VQA (ST-VQA, OCR-VQA, etc.), and key information extraction (FUNSD, SROIE). Models are evaluated using word accuracy for text recognition and standard accuracy metrics for VQA and KIE tasks, with results compared against supervised state-of-the-art methods.

## Key Results
- LMMs demonstrate strong zero-shot performance on text recognition tasks, achieving competitive results on common text datasets
- Performance significantly degrades on text-based VQA and key information extraction tasks compared to supervised methods
- LMMs show semantic bias, prioritizing common word completion over accurate character shape recognition, particularly evident in non-semantic text sequences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LMMs perform OCR through embedding alignment between visual features and language model spaces
- Mechanism: Visual features are projected into the LLM's embedding space via linear projection, creating visual tokens that mirror word embeddings
- Core assumption: Pre-trained LLM embeddings capture sufficient semantic patterns for text recognition without explicit OCR training
- Evidence anchors:
  - [abstract] "BLIP2 utilizes a Querying Transformer (Q-Former) to bridge the modality gap between vision and language models"
  - [section] "This process aligns visual tokens within the pre-trained language model's word embedding space"
- Break condition: Recognition fails on rare words, non-semantic sequences, or characters outside LLM vocabulary

### Mechanism 2
- Claim: LMMs rely on semantic context rather than visual accuracy for text prediction
- Mechanism: LLM predicts next token based on semantic context from preceding text, prioritizing meaningful words over exact visual matching
- Core assumption: Semantic patterns learned during pretraining are more influential than visual feature details
- Evidence anchors:
  - [abstract] "they predominantly depend on semantic understanding for word recognition, frequently prioritizing common words over random letter sequences"
  - [section] "When predicting the next token, these models rely more on the semantic information from the preceding context"
- Break condition: Failure on random character sequences, artistic text, or non-semantic content

### Mechanism 3
- Claim: LMMs have limited fine-grained feature perception due to resolution constraints
- Mechanism: Low input resolutions (e.g., 224x224) limit detailed character shape and spatial relationship perception
- Core assumption: Resolution trade-off is inherent in current LMM architectures for computational efficiency
- Evidence anchors:
  - [abstract] "they exhibit an indifference towards text length and possess limited capabilities in detecting fine-grained features"
  - [section] "The performance gaps between LMM and supervised-SoTA are larger in TextVQA and KIE datasets"
- Break condition: Tasks requiring detailed character analysis, multi-line text comprehension, or precise spatial reasoning

## Foundational Learning

- Concept: Multimodal feature alignment through projection layers
  - Why needed here: Understanding how LMMs bridge vision and language modalities is crucial for comprehending their OCR capabilities
  - Quick check question: How does the projection layer transform visual features to match the LLM's embedding space?

- Concept: Semantic vs. visual pattern recognition in language models
  - Why needed here: Recognizing the distinction between semantic completion and visual accuracy explains LMM performance patterns
  - Quick check question: Why would an LLM predict "hello" instead of "hellw" when shown the latter?

- Concept: Input resolution trade-offs in multimodal architectures
  - Why needed here: Understanding resolution constraints explains performance differences between LMMs and supervised OCR methods
  - Quick check question: What happens to text readability when LMM input resolution is reduced from 1024x1024 to 224x224?

## Architecture Onboarding

- Component map: Image encoder → Projection layer → Frozen LLM → Output generator
- Critical path: Visual input → Feature extraction → Embedding alignment → Token prediction → Text output
- Design tradeoffs: Resolution vs. computational cost, semantic accuracy vs. character precision, generalization vs. specialization
- Failure signatures: Incorrect recognition of random text sequences, completion of words based on semantics rather than visual input, poor performance on long text or fine-grained features
- First 3 experiments:
  1. Test zero-shot OCR performance on semantic vs. non-semantic text to validate semantic reliance mechanism
  2. Compare performance at different input resolutions to quantify fine-grained feature perception limitations
  3. Evaluate character-level vs. word-level accuracy to measure shape awareness deficiencies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How much OCR-specific training data is required to fine-tune an LMM to match or exceed specialized OCR models?
- Basis in paper: [explicit] The paper suggests this as a natural future direction, asking about fine-tuning LMMs on OCR training sets
- Why unresolved: The study only evaluated zero-shot transfer performance without any fine-tuning
- What evidence would resolve it: Experimental results comparing LMM performance before and after fine-tuning on various amounts of OCR-specific data

### Open Question 2
- Question: What is the minimum input image resolution required for LMMs to achieve optimal performance on text-based VQA and KIE tasks?
- Basis in paper: [explicit] The paper notes that most LMMs use low input resolutions (224x224) which limits text readability
- Why unresolved: The study only tested LMMs at their default input resolutions without exploring different resolutions
- What evidence would resolve it: Comparative performance analysis using different input image resolutions

### Open Question 3
- Question: How does the size and diversity of pretraining dataset affect OCR capabilities of LMMs?
- Basis in paper: [explicit] The paper compares different LMMs with varying amounts of training data and notes that LLaVA uses significantly less data but still performs well
- Why unresolved: The study does not conduct an ablation analysis to isolate the impact of pretraining data volume on OCR performance
- What evidence would resolve it: Controlled experiments training LMMs with different amounts and types of multimodal data

## Limitations

- The study relies on zero-shot evaluation without fine-tuning, which may not fully reveal LMM capabilities for OCR tasks
- Simplified prompts may not adequately test models' ability to handle complex OCR scenarios, particularly for VQA and KIE tasks
- The evaluation does not systematically test different input resolutions to quantify the impact of resolution constraints on performance

## Confidence

- Primary claim (LMMs show promise in text recognition but struggle with complex tasks): Medium-High confidence
- Semantic reliance mechanism: Medium confidence
- Resolution constraint impact: Medium confidence

## Next Checks

1. Conduct a controlled experiment comparing character-level accuracy between LMMs and dedicated OCR models on the same datasets to quantify shape awareness deficiencies.

2. Perform ablation studies at different input resolutions (224x224, 512x512, 1024x1024) across all task types to measure the specific impact of resolution constraints.

3. Test the semantic reliance hypothesis by creating synthetic datasets with non-semantic text sequences and measuring the degradation in LMM performance compared to semantic text recognition.