---
ver: rpa2
title: Towards Federated Learning Under Resource Constraints via Layer-wise Training
  and Depth Dropout
arxiv_id: '2309.05213'
source_url: https://arxiv.org/abs/2309.05213
tags:
- learning
- federated
- training
- layers
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of training large machine learning
  models in federated learning (FL) settings where clients have limited resources.
  The authors propose two techniques: Federated Layer-wise Learning, which trains
  only a single layer at a time to reduce memory, computation, and communication costs,
  and Federated Depth Dropout, which randomly drops frozen layers during training
  to further reduce resource usage.'
---

# Towards Federated Learning Under Resource Constraints via Layer-wise Training and Depth Dropout
## Quick Facts
- arXiv ID: 2309.05213
- Source URL: https://arxiv.org/abs/2309.05213
- Reference count: 5
- Primary result: 5x memory reduction while maintaining 37.0-37.6% accuracy on CIFAR-100

## Executive Summary
This paper addresses the challenge of training large machine learning models in federated learning settings where clients have limited computational resources. The authors propose two complementary techniques: Federated Layer-wise Learning, which trains only a single layer at a time to reduce memory, computation, and communication costs, and Federated Depth Dropout, which randomly drops frozen layers during training to further reduce resource usage. Experiments on CIFAR-100 with ViT-Ti/16 backbone demonstrate that these techniques can achieve significant resource savings while maintaining comparable performance to conventional federated learning methods.

## Method Summary
The authors propose Federated Layer-wise Learning as a resource-efficient alternative to traditional federated learning. Instead of training all layers simultaneously, clients train only one layer per round while keeping others frozen. This approach reduces memory usage, computation, and communication costs since gradients are computed and uploaded only for the active layer. To further optimize resource usage, the authors introduce Federated Depth Dropout, which randomly drops frozen layers during training phases. This technique is applied only during training (not inference) and is analogous to traditional dropout regularization. The method leverages the fact that contrastive self-supervised learning can attach loss to intermediate encoder layers, making layer-wise training viable without significant performance degradation.

## Key Results
- 5x or more reduction in training memory usage compared to conventional FL methods
- 6-layer model with Depth Dropout achieved 37.0% accuracy after finetuning
- 12-layer model with Depth Dropout achieved 37.6% accuracy after finetuning
- Performance within 0.2-0.8% of models trained with only Layer-wise Learning (no Depth Dropout)

## Why This Works (Mechanism)

### Mechanism 1
Layer-wise training reduces per-round memory, compute, and communication by training only one layer at a time. During each training phase, gradients are computed and uploaded only for the active layer; other layers are frozen and do not contribute to resource usage in that phase. This assumes self-supervised loss can be attached to intermediate encoder layers without degrading representation quality.

### Mechanism 2
Depth Dropout further reduces resource usage by randomly dropping frozen layers during training. During training phases with multiple fixed layers, a random subset of these layers is removed before the forward pass; inference always uses the full model. This assumes randomly dropping frozen layers does not harm the training dynamics of the active layer because the active layer still receives meaningful gradient signals.

### Mechanism 3
Progressive layer training preserves model performance while achieving large resource savings. Starting from shallow layers and gradually adding deeper layers ensures that early representations are learned before higher-level abstractions, mimicking end-to-end training's gradual feature hierarchy. This assumes training layers incrementally in order (shallow to deep) approximates the joint optimization of end-to-end training.

## Foundational Learning

- **Concept: Federated Averaging (FedAvg)** - Baseline FL protocol that exchanges full model parameters; understanding it clarifies why layer-wise training reduces communication costs. Quick check: In FedAvg, what is communicated between clients and server each round?
- **Concept: Contrastive self-supervised learning** - The method applies loss to intermediate layers; knowing how SimCLR or similar works explains why intermediate-layer training is viable. Quick check: How does contrastive loss on an intermediate layer differ from loss on the final layer in a vision transformer?
- **Concept: Dropout regularization** - Depth Dropout is analogous to neuron dropout; understanding dropout helps predict its effect on training stability. Quick check: What is the typical effect of dropout on training loss versus test performance?

## Architecture Onboarding

- **Component map**: Server -> Model with active layer -> Clients (train active layer) -> Gradients -> Server (aggregate and update)
- **Critical path**:
  1. Server sends model with one active layer
  2. Client downloads, runs forward pass with possible Depth Dropout masking
  3. Client computes gradients only for active layer
  4. Client uploads gradients
  5. Server aggregates gradients for that layer
  6. Server updates global active layer parameters
  7. Repeat until all layers trained; then optionally apply Depth Dropout in later phases
- **Design tradeoffs**:
  - Training time vs. resource usage: More rounds per layer improves performance but increases total rounds
  - Dropout rate vs. performance: Higher dropout reduces resource usage but may slightly degrade accuracy
  - Number of active layers per phase: Training multiple layers per phase speeds convergence but raises per-client cost
- **Failure signatures**:
  - High variance in client gradients → possible imbalance in active layer assignment or insufficient participation
  - Degraded performance with Depth Dropout → dropout rate too high or early layers critical
  - Memory spikes on clients → active layer too large or dropout not applied correctly
- **First 3 experiments**:
  1. Run baseline FedAvg on CIFAR-100 with ViT-Ti/16 to measure full resource usage
  2. Implement layer-wise training with 1 active layer per phase; measure memory, compute, and accuracy vs. baseline
  3. Add Depth Dropout (50% rate) in later phases; compare resource usage and accuracy against step 2

## Open Questions the Paper Calls Out
- How does the performance of Federated Layer-wise Learning compare to federated end-to-end learning when applied to larger datasets or more complex models?
- What is the optimal dropout rate for Depth Dropout in Federated Layer-wise Learning?
- How does Federated Layer-wise Learning with Depth Dropout perform in real-world federated learning scenarios with heterogeneous devices?

## Limitations
- Method relies on contrastive self-supervised learning, limiting direct applicability to supervised FL settings
- Depth Dropout is only applied during training, not inference, which may create deployment complexity
- Experiments limited to single dataset (CIFAR-100) and model architecture (ViT-Ti/16), restricting generalizability claims

## Confidence
- **High confidence**: Resource reduction claims (5x memory reduction verified through layer-wise training mechanism)
- **Medium confidence**: Performance preservation claims (37.0-37.6% accuracy shows minimal degradation, but single dataset limits confidence)
- **Medium confidence**: Layer-wise training viability (contrastive loss on intermediate layers is theoretically sound, but broader empirical validation needed)

## Next Checks
1. Test Federated Layer-wise Learning on a supervised learning task to verify if performance degradation exceeds the reported 0.2-0.8% accuracy loss
2. Evaluate the method across multiple datasets (ImageNet, TinyImageNet) and model architectures (ResNet, ConvNeXt) to assess generalizability
3. Measure actual memory usage on representative edge devices (Raspberry Pi, mobile phones) rather than simulation-based estimates