---
ver: rpa2
title: 'Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without
  Fine-tuning'
arxiv_id: '2305.15065'
source_url: https://arxiv.org/abs/2305.15065
tags:
- policy
- language
- gpt-3
- generation
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Inference-time Policy Adapters (IPA), a method
  to tailor extreme-scale language models (e.g., GPT-3) without fine-tuning. IPA trains
  a small adapter policy that adjusts the outputs of a large base model during inference
  to maximize a user-specified reward, using reinforcement learning while keeping
  the base model's parameters frozen.
---

# Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning

## Quick Facts
- arXiv ID: 2305.15065
- Source URL: https://arxiv.org/abs/2305.15065
- Authors: 
- Reference count: 39
- Key outcome: IPA consistently improves performance over off-the-shelf language models like GPT-3 and often outperforms competitive baselines, including expensive fine-tuning, across five challenging text generation tasks.

## Executive Summary
This paper introduces Inference-time Policy Adapters (IPA), a method for tailoring extreme-scale language models without fine-tuning. IPA trains a small adapter policy that adjusts the outputs of a large base model during inference to maximize a user-specified reward, using reinforcement learning while keeping the base model's parameters frozen. Experiments on five challenging text generation tasks show that IPA consistently improves performance over off-the-shelf language models like GPT-3, and often outperforms competitive baselines, including expensive fine-tuning. For example, IPA significantly reduces toxicity in GPT-2 and GPT-3, achieves better constraint satisfaction than GPT-3.5 and GPT-4 for lexically constrained generation, improves coherence and human-likeness of GPT-2 and GPT-3 in open-ended generation, enhances dialogue safety and faithfulness in Blenderbot, all while being a lightweight alternative to fine-tuning extreme-scale models.

## Method Summary
IPA trains a small adapter policy to adjust a frozen large base model's outputs using reinforcement learning to maximize a user-specified reward. The method combines the base model's distribution with the adapter's distribution through a product-of-experts mechanism, then optimizes the combined distribution during RL training. For efficiency with extremely large models, IPA optionally uses an approximate policy (either a smaller model from the same family or a distilled version of the base model) during RL training, while applying the trained adapter to the actual base model during inference.

## Key Results
- IPA significantly reduces toxicity in GPT-2 and GPT-3 compared to base models and competitive baselines
- IPA achieves better constraint satisfaction than GPT-3.5 and GPT-4 for lexically constrained generation tasks
- IPA improves coherence and human-likeness of GPT-2 and GPT-3 in open-ended generation tasks

## Why This Works (Mechanism)

### Mechanism 1
The tailored policy effectively steers the base model's generation distribution toward higher reward without altering the base model's parameters. By multiplying the next-token distributions of the base policy pθ and adapter policy pϕ, then normalizing, the combined distribution emphasizes tokens favored by both models. During RL training, only the adapter policy's parameters are updated to maximize the reward, leaving the base model frozen. Core assumption: The base model's distribution contains sufficient diversity and fluency that the adapter can meaningfully adjust it toward the target objective.

### Mechanism 2
Using an approximate policy (smaller or distilled model) reduces computational cost during adapter training while preserving sufficient behavioral similarity to the base model. The approximate policy mimics the base model's conditional generation behavior, allowing the adapter to be trained on a computationally cheaper forward pass. At inference, the adapter is applied to the actual base model. Core assumption: The approximate policy's distribution is close enough to the base model's that training the adapter on it transfers effectively to the base model.

### Mechanism 3
Reinforcement learning with the tailored policy allows arbitrary reward functions to be optimized without requiring differentiable rewards or model access. Standard RL algorithms (e.g., Quark, PPO, NLPO) optimize the adapter parameters to maximize the expected reward of the tailored policy. Since the tailored policy is a differentiable function of the adapter parameters, gradients can be computed and used for policy updates. Core assumption: The reward function is well-defined and correlates with the desired generation properties (e.g., fluency, coherence, safety).

## Foundational Learning

- Concept: Product-of-Experts (PoE) mechanism
  - Why needed here: IPA's tailored policy is a PoE of the base and adapter distributions. Understanding PoE helps reason about how the two models interact.
  - Quick check question: If pθ assigns 0.8 probability to a token and pϕ assigns 0.2, what is the relative weight of that token in the tailored policy before normalization?

- Concept: Reinforcement Learning with Frozen Base Models
  - Why needed here: IPA's key innovation is updating only the adapter during RL. Understanding how RL algorithms can work with frozen components is critical.
  - Quick check question: In PPO, which parts of the computation graph are frozen when adapting a large base model with a small adapter?

- Concept: Knowledge Distillation for Approximate Policies
  - Why needed here: IPA optionally uses distilled base models as approximate policies. Understanding distillation helps reason about when and how to construct approximate policies.
  - Quick check question: What is the objective function used when distilling a large model into a smaller one for sequence generation?

## Architecture Onboarding

- Component map:
  Base Model (frozen) -> Tailored Policy -> RL Algorithm -> Adapter Policy
  Approximate Policy (optional) -> RL Algorithm -> Adapter Policy

- Critical path:
  1. Initialize adapter policy (e.g., GPT-2-large)
  2. If using approximate policy, initialize or distill it
  3. During RL training: generate with approximate policy + adapter, compute reward, update adapter parameters
  4. At inference: generate with base model + trained adapter

- Design tradeoffs:
  - Adapter size vs. expressiveness: Larger adapters can capture more nuanced steering but increase computational cost
  - Approximate policy fidelity vs. efficiency: Better approximations yield more transferable adapters but may require distillation or larger models
  - RL algorithm choice: Different algorithms offer different convergence properties and sample efficiency

- Failure signatures:
  - Adapter collapses to trivial distribution: May indicate poor reward shaping or insufficient exploration
  - No improvement over base model: Could mean adapter is too small or RL training failed to converge
  - Degraded fluency/coherence: May indicate reward function doesn't adequately penalize low-quality generations

- First 3 experiments:
  1. Toxicity reduction on REALTOXICITY with GPT-2 as base, GPT-2-large as adapter, Quark as RL algorithm
  2. Lexically constrained generation on CommonGen with GPT-3 as base, distilled GPT-3 as approximate policy, GPT-2-large as adapter
  3. Open-ended generation on XSum with GPT-2-XL as base, GPT-2-large as adapter, contrastive reward function

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of IPA scale with the size of the base language model (e.g., GPT-3 vs GPT-4) and what are the practical limitations of applying IPA to extremely large models? The paper primarily focuses on experiments with GPT-2 and GPT-3, with limited discussion on the applicability and performance of IPA on even larger models like GPT-4.

### Open Question 2
What is the impact of using different reinforcement learning algorithms on the effectiveness of IPA across various tasks, and are there specific algorithms that are more suited for certain types of objectives? While the paper demonstrates that IPA can work with multiple RL algorithms, it does not provide a thorough analysis of how the choice of algorithm affects performance across different tasks or objectives.

### Open Question 3
How does the choice of approximate policy (e.g., smaller model from the same family vs. distilled version of the base model) affect the performance of IPA, and are there scenarios where one approach is significantly better than the other? The paper introduces the concept of approximate policy but does not extensively compare the performance of IPA when using different types of approximate policies or identify scenarios where one approach is significantly better than the other.

## Limitations
- Limited evaluation diversity: Tasks are primarily focused on safety, constraint satisfaction, and coherence, with unexplored performance on complex reasoning or creative generation
- Computational cost trade-offs: Inference-time computational overhead and memory requirements for running both models simultaneously are not fully characterized
- Reward function sensitivity: Success depends heavily on designing appropriate reward functions, with unexplored sensitivity to reward function design choices

## Confidence
- High confidence: Core technical mechanism (product-of-experts) is well-founded and experimental results are convincing, particularly for toxicity reduction
- Medium confidence: Claim of being a "lightweight alternative to fine-tuning" requires careful interpretation due to unmeasured inference overhead
- Low confidence: Generalizability to extremely diverse tasks and long-term stability of adapters trained with RL are not established

## Next Checks
1. **Ablation study on approximate policy fidelity**: Systematically vary the quality of the approximate policy and measure the resulting adapter performance to quantify the trade-off between training efficiency and final quality.
2. **Human evaluation expansion**: Conduct comprehensive human evaluations across all five tasks to validate automatic metric results and assess subjective quality dimensions compared to both base models and fine-tuned alternatives.
3. **Computational overhead characterization**: Measure and compare the wall-clock inference time, memory usage, and throughput of IPA against both frozen base models and fine-tuned alternatives across different hardware configurations.