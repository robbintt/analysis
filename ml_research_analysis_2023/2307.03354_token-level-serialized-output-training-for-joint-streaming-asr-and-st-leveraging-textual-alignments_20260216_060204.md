---
ver: rpa2
title: Token-Level Serialized Output Training for Joint Streaming ASR and ST Leveraging
  Textual Alignments
arxiv_id: '2307.03354'
source_url: https://arxiv.org/abs/2307.03354
tags:
- translation
- speech
- inter
- joint
- streaming
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first streaming Transformer-Transducer
  that jointly generates automatic speech recognition (ASR) and speech translation
  (ST) outputs using a single decoder. To effectively produce ASR and ST content with
  minimal latency, the authors propose a joint token-level serialized output training
  (tSOT) method that interleaves source and target words by leveraging an off-the-shelf
  neural textual aligner.
---

# Token-Level Serialized Output Training for Joint Streaming ASR and ST Leveraging Textual Alignments

## Quick Facts
- arXiv ID: 2307.03354
- Source URL: https://arxiv.org/abs/2307.03354
- Reference count: 0
- First streaming Transformer-Transducer jointly generates ASR and ST outputs using a single decoder with minimal latency.

## Executive Summary
This paper presents the first streaming Transformer-Transducer that jointly generates automatic speech recognition (ASR) and speech translation (ST) outputs using a single decoder. The authors propose a token-level serialized output training (tSOT) method that interleaves source and target words using task-specific markers and leverages an off-the-shelf neural textual aligner. Experiments in monolingual and multilingual settings demonstrate that this approach achieves the best quality-latency balance, with an average ASR latency of 1s and ST latency of 1.3s, while improving output quality compared to separate models.

## Method Summary
The method uses a streaming Transformer-Transducer architecture with a 24-layer encoder, 6-layer LSTM predictor, and 2 feed-forward joiner. The key innovation is token-level serialized output training (tSOT) that interleaves ASR and ST words using task-specific markers (⟨asr⟩, ⟨st⟩) based on neural textual alignments. The model is trained on serialized sequences where source and target words are grouped by semantic alignment, allowing a single decoder to generate both ASR and ST outputs with minimal task switches. The approach is evaluated on proprietary data (1k hours per language) for German, Italian, Spanish to English translation, using the CoVoST 2 test set.

## Key Results
- Achieved average ASR latency of 1s and ST latency of 1.3s
- Improved output quality compared to separate ASR and ST models (1.1 WER and 0.4 BLEU improvement in multilingual case)
- Demonstrated best quality-latency balance among streaming joint ASR-ST approaches
- Validated effectiveness in both monolingual (it-en) and multilingual ({de,es,it}-en) settings

## Why This Works (Mechanism)

### Mechanism 1
Interleaving ASR and ST tokens using task-specific markers (⟨asr⟩, ⟨st⟩) allows a single decoder to learn joint generation without needing separate decoder instances. The serialized output sequence contains interleaved ASR and ST words marked by task tokens. During training, the decoder learns to produce the correct task token followed by the corresponding word. At inference, the model outputs a mixed stream that can be segmented into ASR and ST results based on the task tokens.

### Mechanism 2
Using a neural textual aligner (awesome-align) to drive interleaving order improves both latency and quality compared to fixed alternation strategies. The aligner predicts word-level correspondences between source and target texts. Based on these alignments, groups of words that map together are output as a block, reducing task switches and maintaining semantic coherence. This leads to lower latency (fewer switches) and higher BLEU/WER scores.

### Mechanism 3
Sharing a single streaming Transformer-Transducer decoder across ASR and ST tasks reduces model size and improves cross-task learning, yielding better quality-latency balance than separate models. The decoder parameters are trained jointly on interleaved sequences, learning shared representations for both transcription and translation. This consolidation improves generalization and reduces latency by eliminating the need to run two separate decoders sequentially.

## Foundational Learning

- **Streaming Transformer-Transducer architecture**: Needed for incremental prediction with minimal latency; quick check: How does the encoder chunking strategy (1s chunks with 18 left chunks) affect streaming latency and accuracy?
- **Serialized Output Training (tSOT)**: Converts multi-task references into a single serialized sequence for joint training; quick check: What is the role of special task tokens (⟨asr⟩, ⟨st⟩) in the serialized sequence?
- **Neural textual alignment**: Provides principled word-level correspondences between source and target texts; quick check: How does alignment-based interleaving differ from fixed alternation strategies like INTER 0.5?

## Architecture Onboarding

- **Component map**: Input speech → CNN compression → Transformer encoder → LSTM predictor → Joiner → Vocabulary distribution with task markers
- **Critical path**: Input speech → CNN compression → Transformer encoder → LSTM predictor → Joiner → Vocabulary distribution → Output token stream with task markers
- **Design tradeoffs**: Streaming vs. full-attention (lower latency but possible accuracy trade-off); single vs. dual decoder (reduced parameters but potential task interference); alignment-based vs. fixed interleaving (improved coherence but added preprocessing cost)
- **Failure signatures**: High WER/BLEU with high latency indicates frequent task switches; low latency with poor quality suggests insufficient context or inadequate task markers; training instability may indicate vocabulary size or alignment quality issues
- **First 3 experiments**: 1) Train baseline T-T for ASR only and ST only; measure WER/BLEU and latency. 2) Implement joint tSOT with INTER 0.5 (fixed alternation); compare to separate models. 3) Add alignment-based interleaving (INTER ALIGN); evaluate quality-latency trade-off and analyze output coherence.

## Open Questions the Paper Calls Out

### Open Question 1
How does the joint tSOT method scale to more than three source languages in terms of both quality and latency? The paper experiments with three source languages (de, es, it) but discusses potential for multilingual extension. Experiments showing WER, BLEU, and latency metrics for joint tSOT models trained on four or more source languages would resolve this question.

### Open Question 2
What is the impact of using different alignment methods (other than awesome-align) on the performance of the joint tSOT INTER ALIGN approach? The paper uses awesome-align for textual alignment but does not compare it to other alignment methods. Experiments comparing performance using different alignment methods (e.g., GIZA++, fast_align) would resolve this question.

### Open Question 3
How does the joint tSOT method perform on non-European languages with different grammatical structures and writing systems? The paper focuses on European languages (German, Italian, Spanish) and does not address performance on languages with different linguistic properties. Experiments evaluating the method on non-European languages (e.g., Chinese, Arabic, Hindi) would resolve this question.

## Limitations
- Relies on proprietary training corpus of 1k hours per language pair, limiting reproducibility
- Quality improvements measured on CoVoST 2 without comparison to other strong baselines in joint streaming ST literature
- Alignment-based interleaving claimed to improve coherence but lacks ablation or error analysis showing how alignment errors propagate

## Confidence
- **High confidence**: Streaming T-T architecture and token-level serialized output training are technically sound and well-supported by prior work
- **Medium confidence**: Reported latency and quality improvements are plausible but depend heavily on proprietary data and specific configurations
- **Low confidence**: Claim that alignment-based interleaving consistently improves quality-latency trade-off lacks direct evidence without systematic comparison to fixed alternation

## Next Checks
1. Reimplement baseline models on publicly available dataset (LibriSpeech + CoVoST 2) to verify claimed improvements are not dataset-specific
2. Conduct ablation on interleaving strategy comparing alignment-based interleaving to fixed alternation (INTER 0.5)
3. Analyze alignment quality by measuring neural textual aligner accuracy and correlating alignment errors with degraded WER/BLEU