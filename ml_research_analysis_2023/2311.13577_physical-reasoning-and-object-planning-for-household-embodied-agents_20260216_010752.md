---
ver: rpa2
title: Physical Reasoning and Object Planning for Household Embodied Agents
arxiv_id: '2311.13577'
source_url: https://arxiv.org/abs/2311.13577
tags:
- object
- task
- configurations
- utility
- options
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the CommonSense Object Affordance Task (COAT),
  a novel framework to evaluate physical commonsense reasoning in large language models
  (LLMs) for household embodied agents. The task focuses on selecting appropriate
  substitute objects when the ideal object is unavailable, considering three key factors:
  utility alignment with the task, contextual appropriateness (safety, social norms,
  efficiency), and the object''s current physical state.'
---

# Physical Reasoning and Object Planning for Household Embodied Agents

## Quick Facts
- arXiv ID: 2311.13577
- Source URL: https://arxiv.org/abs/2311.13577
- Reference count: 40
- Key outcome: Introduces COAT framework to evaluate physical commonsense reasoning in LLMs for household agents, revealing larger models excel at ideal configurations but struggle with abstract reasoning in moderate cases.

## Executive Summary
This paper introduces the CommonSense Object Affordance Task (COAT), a novel framework to evaluate physical commonsense reasoning in large language models (LLMs) for household embodied agents. The task focuses on selecting appropriate substitute objects when the ideal object is unavailable, considering three key factors: utility alignment with the task, contextual appropriateness (safety, social norms, efficiency), and the object's current physical state. The authors curate three datasets (15k, 58k, and 69k questions) to assess LLM performance across these factors, using human annotations to create ground truth mappings. Experiments with state-of-the-art models reveal that while larger models excel at identifying ideal and bad object configurations, they struggle with moderate configurations requiring abstract reasoning. Smaller models perform poorly across all aspects, with accuracy dropping significantly as the number of options increases.

## Method Summary
The study introduces the CommonSense Object Affordance Task (COAT) to evaluate physical commonsense reasoning in LLMs for household embodied agents. The method involves curating three datasets (Object Level, Variable Level, and Sub-optimal Configuration) with ground truth mappings created through human annotations. The evaluation framework tests models on three key factors: utility alignment, contextual appropriateness, and physical state. The authors use accuracy and bad rate metrics to assess performance across various state-of-the-art models including PaLM, GPT-3.5-Turbo, Mistral-7B, Llama2-13B, Vicuna, and ChatGLM variants. The approach introduces five abstract variables (mass, material, temperature, already in use, condition) to simulate diverse household scenarios.

## Key Results
- Larger models like PaLM and GPT-3.5-Turbo significantly outperform smaller models in identifying ideal and bad object configurations
- Model accuracy decreases substantially as the number of object options increases and diversity grows
- All models struggle with moderate configurations requiring abstract reasoning, despite excelling at clear-cut ideal and bad scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Performance of large LLMs (PaLM, GPT-3.5-Turbo) improves significantly over smaller models in commonsense reasoning due to their superior ability to process and integrate multi-step abstract reasoning.
- Mechanism: Large models have been trained on more diverse and extensive datasets, enabling them to capture complex relationships and nuances in physical affordances and contextual dependencies. This allows them to better handle tasks requiring abstract reasoning about an object's current physical state and contextual appropriateness.
- Core assumption: The size and diversity of training data directly correlate with the model's ability to perform abstract reasoning in household task planning scenarios.
- Evidence anchors:
  - [abstract] "Experiments with state-of-the-art models (PaLM, GPT-3.5-Turbo, Mistral-7B, Llama2-13B, Vicuna, and ChatGLM variants) reveal that while larger models like PaLM and GPT-3.5-Turbo excel at identifying ideal and bad object configurations, they struggle with moderate configurations requiring abstract reasoning."
  - [section] "Here as well we witness the superior reasoning capabilities of GPT3.5-Turbo and PaLM, with the latter outperforming the former on each dataset by an average of 8.8%."
  - [corpus] Weak: Corpus shows related works on LLM-empowered embodied agents but lacks direct comparison of abstract reasoning performance across model sizes.

### Mechanism 2
- Claim: The introduction of abstract variables (mass, material, temperature, already in use, condition) allows for a more human-like evaluation of an object's physical state, improving the model's ability to reason about object usability.
- Mechanism: By abstracting physical properties into symbolic terms, the model can focus on higher-level reasoning about an object's suitability for a task, rather than getting bogged down in specific physical details. This abstraction layer aligns with human commonsense reasoning, making it easier for models to understand and evaluate object configurations.
- Core assumption: Human reasoning about object usability relies more on abstract concepts than on specific physical measurements.
- Evidence anchors:
  - [abstract] "To maintain accessibility, we introduce five abstract variables reflecting an object’s physical condition, modulated by human insights, to simulate diverse household scenarios."
  - [section] "In our study, we shift the focus to task planning under non-ideal conditions, necessitating reasoning about potential substitute objects."
  - [corpus] Weak: Related works mention leveraging computation of expectation models for commonsense affordance estimation, but don't explicitly discuss the use of abstract variables.

### Mechanism 3
- Claim: The three-step framework (Utility, Contextual Appropriateness, Physical State) mirrors human decision-making processes, enabling models to effectively reason about object selection in complex household scenarios.
- Mechanism: By breaking down the decision-making process into these three distinct phases, the model can systematically evaluate objects based on their inherent utility, contextual factors, and current physical state. This structured approach allows for more comprehensive and accurate reasoning about object affordances.
- Core assumption: Human decision-making in object selection follows a similar three-step process, and models can be trained to mimic this process.
- Evidence anchors:
  - [abstract] "Our evaluation of state-of-the-art language models on these datasets sheds light on three pivotal considerations: 1) aligning an object's inherent utility with the task at hand, 2) navigating contextual dependencies (societal norms, safety, appropriateness, and efficiency), and 3) accounting for the current physical state of the object."
  - [section] "This scenario highlights the three key factors humans prioritize when selecting an object for a task: the utility of the object, its contextual appropriateness, and its current physical state."
  - [corpus] Weak: Related works discuss embodied task planning with LLMs but don't explicitly mention this three-step framework.

## Foundational Learning

- Concept: Abstract reasoning in language models
  - Why needed here: The paper relies heavily on the model's ability to perform abstract reasoning about object affordances and physical states. Understanding how LLMs handle abstraction is crucial for interpreting the results.
  - Quick check question: How do language models typically handle abstract concepts, and what challenges arise when applying these concepts to physical reasoning tasks?

- Concept: Commonsense reasoning in AI
  - Why needed here: The entire study is based on evaluating the commonsense reasoning capabilities of LLMs in household scenarios. A solid grasp of what constitutes commonsense reasoning in AI is essential for understanding the significance of the results.
  - Quick check question: What are the key differences between formal logical reasoning and commonsense reasoning, and why is commonsense reasoning particularly challenging for AI systems?

- Concept: Multi-step reasoning and decision-making processes
  - Why needed here: The paper's framework involves a multi-step decision-making process for object selection. Understanding how models handle sequential reasoning and decision-making is crucial for interpreting the performance across different tasks and datasets.
  - Quick check question: How do language models typically handle multi-step reasoning tasks, and what are the common pitfalls or limitations in such scenarios?

## Architecture Onboarding

- Component map: Data Curation Module -> Model Evaluation Pipeline -> Performance Analysis Tool -> Visualization Component
- Critical path: Data Curation → Model Evaluation → Performance Analysis → Visualization
- Design tradeoffs:
  - Dataset size vs. annotation quality: Larger datasets provide more robust evaluation but require more human annotations, which can introduce inconsistencies.
  - Model diversity vs. computational resources: Including more models provides broader insights but increases computational costs and complexity.
  - Abstract variables vs. concrete physical measurements: Abstract variables align better with human reasoning but may lose some precision in capturing physical states.
- Failure signatures:
  - Inconsistent performance across similar tasks: May indicate issues with dataset curation or model understanding of task nuances.
  - High bad rate in specific scenarios: Could suggest model bias towards certain object configurations or lack of understanding of contextual appropriateness.
  - Degradation in performance with increasing options: Might indicate limitations in the model's ability to handle complex multi-step reasoning or information overload.
- First 3 experiments:
  1. Replicate the Object Level Dataset experiment with a subset of models to verify the initial findings on contextual appropriateness reasoning.
  2. Conduct an ablation study on the abstract variables, testing model performance with and without each variable to determine their individual impact on reasoning capabilities.
  3. Create a cross-dataset analysis, evaluating model performance across all three major datasets to identify consistent strengths and weaknesses in the model's reasoning abilities.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different abstract physical state variables (mass, material, temperature, already in use, condition) individually and collectively impact object selection decisions across various household tasks?
- Basis in paper: [explicit] The paper introduces five abstract variables reflecting an object's physical condition and creates datasets to evaluate how these variables affect decision-making
- Why unresolved: The paper shows that models struggle with moderate configurations but doesn't provide detailed analysis of which specific variables or combinations most significantly impact performance
- What evidence would resolve it: Systematic ablation studies varying individual variables and their combinations, showing their relative importance and interaction effects on model performance

### Open Question 2
- Question: What is the relationship between model size and abstract reasoning capability for physical commonsense reasoning in household tasks?
- Basis in paper: [explicit] The paper shows that larger models like PaLM and GPT-3.5-Turbo perform better than smaller models like Mistral-7B and Llama2-13B, but doesn't fully explain why
- Why unresolved: The paper observes performance differences but doesn't investigate whether this is due to better abstract reasoning capabilities, more training data, or other factors
- What evidence would resolve it: Controlled experiments comparing models of different sizes on carefully designed tasks that isolate abstract reasoning from other capabilities, or analysis of attention patterns during reasoning

### Open Question 3
- Question: How do contextual appropriateness factors (safety, social norms, efficiency) interact with physical state variables in object selection decisions?
- Basis in paper: [inferred] The paper mentions contextual appropriateness as one of three key factors but focuses primarily on physical state variables in its analysis
- Why unresolved: The paper creates separate datasets for contextual appropriateness and physical state but doesn't explore how these factors interact in real-world decision-making
- What evidence would resolve it: Integrated datasets and experiments that present scenarios requiring simultaneous consideration of both contextual and physical factors, with analysis of decision-making strategies

## Limitations

- The study's reliance on human-annotated ground truth mappings introduces potential bias in defining "appropriate" object configurations
- The evaluation framework focuses primarily on English-language household contexts, limiting cross-cultural applicability
- While the three-dataset approach provides comprehensive coverage, the datasets may not capture edge cases or rare household scenarios

## Confidence

- Dataset curation methodology: High
- Generalizability to non-household scenarios: Low
- Core findings on model performance differences: Medium

## Next Checks

1. Conduct a blind annotation study where multiple annotators independently label object configurations for a subset of tasks to quantify inter-annotator agreement and identify potential bias in ground truth creation.

2. Test model performance on zero-shot household tasks not included in the training datasets to evaluate true generalization capabilities beyond curated scenarios.

3. Implement a perturbation analysis by systematically modifying object properties in test scenarios to identify which abstract variables have the strongest influence on model decision-making.