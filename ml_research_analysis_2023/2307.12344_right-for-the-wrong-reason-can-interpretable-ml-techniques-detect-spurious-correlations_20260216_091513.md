---
ver: rpa2
title: 'Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious
  Correlations?'
arxiv_id: '2307.12344'
source_url: https://arxiv.org/abs/2307.12344
tags:
- explanation
- techniques
- spurious
- data
- confounder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a rigorous evaluation strategy to assess the
  ability of interpretable ML techniques to detect spurious correlations in medical
  imaging tasks. The authors focus on the binary classification of cardiomegaly from
  chest x-ray data with three types of artificially added confounders: hospital tags,
  hyperintensities, and obstructions.'
---

# Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?

## Quick Facts
- arXiv ID: 2307.12344
- Source URL: https://arxiv.org/abs/2307.12344
- Reference count: 36
- Key outcome: SHAP and Attri-Net best detect spurious correlations in medical imaging tasks.

## Executive Summary
This paper proposes a rigorous evaluation strategy to assess interpretable ML techniques' ability to detect spurious correlations in medical imaging tasks. The authors focus on cardiomegaly classification from chest x-rays with artificially added confounders (hospital tags, hyperintensities, obstructions) and evaluate five post-hoc explanation methods plus one inherently interpretable approach. They introduce two novel metrics - Confounder Sensitivity and Explanation Normalized Cross Correlation - to quantify performance. The study finds that SHAP (post-hoc) and Attri-Net (inherently interpretable) are most effective at identifying when models rely on spurious signals rather than genuine features.

## Method Summary
The authors use the CheXpert dataset for binary cardiomegaly classification, systematically injecting three types of artificial confounders at varying percentages (0-100%) into training data. They train both a ResNet50 baseline and Attri-Net model, then apply five post-hoc explanation techniques (Guided Backpropagation, Grad-CAM, Gifsplanation, LIME, SHAP) alongside Attri-Net's built-in explanations. Performance is evaluated using Confounder Sensitivity (how well explanations identify confounders) and Explanation Normalized Cross Correlation (how closely explanations align with true confounder locations). The controlled experimental design allows rigorous assessment of each technique's ability to detect known spurious correlations.

## Key Results
- SHAP and Attri-Net achieve the highest confounder sensitivity scores, indicating superior ability to identify spurious correlations
- Attri-Net and SHAP also show the lowest explanation NCC scores, demonstrating better localization of confounders
- Post-hoc methods show mixed performance, with SHAP outperforming others in detecting spurious correlations
- Explanation sparsity varies across techniques, affecting their performance on confounders of different sizes and shapes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Post-hoc explanation methods are inherently limited in detecting spurious correlations because they rely on approximations of a black-box model's decision process.
- Mechanism: Post-hoc techniques like SHAP or LIME approximate the model's behavior by sampling perturbations around input instances, but these approximations can miss or misrepresent the model's true reliance on spurious signals. This is particularly problematic when the spurious signal is a consistent feature across many training examples, leading the model to learn an over-simplified decision boundary that the approximation fails to capture accurately.
- Core assumption: The black-box model has learned a spurious correlation that is not directly reflected in the interpretable approximation.
- Evidence anchors:
  - [abstract] "However, there is mixed evidence whether many of these techniques are actually able to do so."
  - [section] "The majority of visual explanation methods are post-hoc techniques, meaning a heuristic is applied to any trained model... However, post-hoc techniques are by definition only approximations and many techniques have been found to suffer from serious limitations [26,4,12]."

### Mechanism 2
- Claim: Inherently interpretable models like Attri-Net can more reliably detect spurious correlations because their architecture is designed to explicitly reveal the reasoning process.
- Mechanism: Attri-Net uses a GAN-based counterfactual generator to produce feature attribution maps for each disease category, then makes predictions based on these interpretable maps. Because the model's decision process is transparent and based on explicitly defined features, it cannot "hide" spurious correlations in a black-box manner. The model must explicitly rely on the features it generates, making spurious correlations more apparent.
- Core assumption: The inherently interpretable model's architecture forces it to use interpretable features for prediction, preventing it from learning spurious correlations in a hidden manner.
- Evidence anchors:
  - [abstract] "Both approaches produce explanations on the pixel level of the input images."
  - [section] "Inherently interpretable visual explanation approaches are much less widely explored than post-hoc techniques, but there has recently been an increased interest in the topic."

### Mechanism 3
- Claim: The evaluation strategy using controlled artificial confounders allows for a rigorous assessment of explanation techniques' ability to detect spurious correlations.
- Mechanism: By systematically adding confounders to training data at varying percentages (p), the researchers create a controlled environment where they know exactly when and how spurious correlations are present. They then use metrics like Confounder Sensitivity (CS) and Explanation Normalized Cross Correlation (NCC) to quantify how well explanation techniques identify these known confounders. This controlled approach provides a more reliable assessment than real-world data where confounders are unknown and uncontrolled.
- Core assumption: Artificially added confounders in controlled experiments are representative of real-world spurious correlations and can be used to benchmark explanation techniques.
- Evidence anchors:
  - [section] "We propose a rigorous evaluation of post-hoc explanations and inherently interpretable techniques for the identification of spurious correlations in a medical imaging task... To identify whether an explanation correctly identifies a model's reliance on spurious correlations, we propose two quantitative metrics which are highly reflective of our qualitative findings."

## Foundational Learning

- Concept: Confounder detection in machine learning
  - Why needed here: Understanding how to identify when a model is relying on irrelevant or spurious information rather than the actual features of interest is crucial for developing reliable AI systems, especially in high-stakes domains like medical diagnosis.
  - Quick check question: What is the difference between a confounder and a feature that is genuinely correlated with the target variable?

- Concept: Post-hoc vs. inherently interpretable models
  - Why needed here: The paper compares these two approaches to explainability, so understanding their fundamental differences is essential for grasping why one might be more effective than the other at detecting spurious correlations.
  - Quick check question: What are the key architectural differences between a post-hoc explanation method like SHAP and an inherently interpretable model like Attri-Net?

- Concept: Evaluation metrics for explanation techniques
  - Why needed here: The paper introduces novel metrics (CS and NCC) to assess explanation quality, so understanding how to evaluate explanations is crucial for interpreting the results and comparing different methods.
  - Quick check question: How does Confounder Sensitivity (CS) differ from traditional metrics like accuracy or AUC in evaluating a model's performance?

## Architecture Onboarding

- Component map: Data preprocessing -> Confounder injection -> Model training -> Explanation generation -> Metric calculation -> Visualization
- Critical path: The most critical sequence is: data preparation with confounders → model training → explanation generation → metric calculation → result interpretation. Any failure in the confounder injection or metric calculation would invalidate the entire evaluation.
- Design tradeoffs: The choice between post-hoc and inherently interpretable methods involves a tradeoff between flexibility (post-hoc can be applied to any model) and reliability (inherently interpretable methods may provide more trustworthy explanations). The evaluation strategy itself trades off between controlled experimentation (using artificial confounders) and real-world applicability.
- Failure signatures: If all explanation methods show similar performance regardless of confounder presence, it suggests the evaluation metrics are not sensitive enough. If post-hoc methods consistently outperform Attri-Net, it might indicate a flaw in Attri-Net's implementation or the evaluation strategy. If results vary wildly between different confounders, it suggests the explanation methods have inconsistent behavior across different types of spurious signals.
- First 3 experiments:
  1. Train ResNet50 and Attri-Net on clean data (p=0%) and verify that all explanation methods show low confounder sensitivity and high NCC, confirming they don't falsely identify confounders when none are present.
  2. Train both models on data with 100% contamination for each confounder type and compare CS and NCC scores to identify which methods best detect known spurious correlations.
  3. Vary the contamination percentage (p=20%, 50%, 80%) for each confounder type and plot the relationship between p and CS/NCC to understand how explanation performance scales with the strength of spurious correlations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different explanation techniques perform on confounders of varying sizes and shapes?
- Basis in paper: [explicit] The paper mentions that "the variation in the explanations' sparsity makes them perform differently in detecting spurious signals of different sizes and shapes."
- Why unresolved: The paper only evaluates three specific types of confounders (tag, hyperintensities, and obstruction). There is no analysis of how the techniques would perform on confounders with different characteristics.
- What evidence would resolve it: Experiments with a wider variety of confounders varying in size, shape, and location to systematically evaluate the performance of each explanation technique.

### Open Question 2
- Question: How do explanation techniques perform in real-world scenarios where confounders are not artificially added but naturally present in the data?
- Basis in paper: [inferred] The paper uses artificially added confounders to evaluate the techniques, but does not address how they would perform on naturally occurring confounders.
- Why unresolved: The controlled experiments with artificial confounders may not fully capture the complexity and variability of real-world data.
- What evidence would resolve it: Testing the explanation techniques on real-world datasets with known confounders or using techniques to identify potential confounders in existing datasets.

### Open Question 3
- Question: How do explanation techniques perform in multi-label classification tasks where multiple diseases or conditions are present in a single image?
- Basis in paper: [explicit] The paper focuses on a binary classification task for cardiomegaly diagnosis.
- Why unresolved: The performance of explanation techniques in multi-label scenarios may differ from their performance in binary classification tasks.
- What evidence would resolve it: Evaluating the explanation techniques on multi-label medical imaging datasets and comparing their performance to the binary classification case.

## Limitations

- The evaluation relies on artificially injected confounders, which may not fully capture real-world spurious correlation complexity
- The study focuses on a single medical imaging task (cardiomegaly classification), limiting generalizability to other domains
- Transferability of results to practical scenarios with unknown confounders remains uncertain

## Confidence

- High confidence: The superiority of SHAP and Attri-Net in detecting artificially injected confounders is well-supported by the experimental results and quantitative metrics.
- Medium confidence: The claim that post-hoc methods are inherently limited in detecting spurious correlations due to their approximation nature, while intuitively sound, requires more extensive validation across diverse model architectures and tasks.
- Medium confidence: The assertion that inherently interpretable models like Attri-Net are more reliable for identifying spurious correlations, while supported by the results, needs validation in real-world settings where confounders are unknown and uncontrolled.

## Next Checks

1. Apply the evaluation framework to additional medical imaging tasks (e.g., different pathologies, imaging modalities) to assess generalizability.
2. Test the explanation techniques on real-world datasets with known confounders or use domain experts to identify potential spurious correlations, comparing performance against the artificial confounders.
3. Investigate the robustness of the findings by varying the model architectures (e.g., different CNN architectures, transformer-based models) and training procedures to ensure the conclusions hold across a wider range of scenarios.