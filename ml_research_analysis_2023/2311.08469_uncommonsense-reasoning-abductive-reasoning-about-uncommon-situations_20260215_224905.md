---
ver: rpa2
title: 'UNcommonsense Reasoning: Abductive Reasoning about Uncommon Situations'
arxiv_id: '2311.08469'
source_url: https://arxiv.org/abs/2311.08469
tags:
- explanations
- explanation
- context
- outcome
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the task of uncommonsense abductive reasoning,
  which involves generating explanations for unlikely outcomes given a context. To
  study this, the authors create a new dataset called UNcommonsense containing 41k
  human-written explanations paired with uncommon outcomes.
---

# UNcommonsense Reasoning: Abductive Reasoning about Uncommon Situations

## Quick Facts
- arXiv ID: 2311.08469
- Source URL: https://arxiv.org/abs/2311.08469
- Reference count: 40
- This paper introduces the task of uncommonsense abductive reasoning and shows that online imitation learning methods reduce lose rates by 10% compared to supervised fine-tuning.

## Executive Summary
This paper introduces the task of uncommonsense abductive reasoning, which involves generating explanations for unlikely outcomes given a context. The authors create a new dataset called UNcommonsense containing 41k human-written explanations paired with uncommon outcomes. They compare human-written explanations to those generated by large language models (LLMs), finding that LLM-generated explanations are more specific but less diverse. To combine the strengths of humans and LLMs, the authors experiment with using LLMs to enhance crowd-written explanations. They also propose two online imitation learning algorithms to train smaller, more accessible LMs on this task. When compared to standard supervised fine-tuning, these methods reduce lose rates by 10% on both commonsense and uncommonsense abductive reasoning, as judged by human evaluators.

## Method Summary
The authors create the UNcommonsense dataset by filtering SocialIQA and ROCStories to include only uncommon outcomes, then collecting human-written explanations. They develop two online imitation learning algorithms: EaO (Expert as Oracle) and SED (Sequence Editing with Demonstration). EaO trains using expert online corrections to learner-generated sequence prefixes, while SED minimizes the probability of learner-generated sequences while maximizing the probability of expert demonstrations. These methods are compared against supervised fine-tuning using FlanT5-XXL, LLaMA-7B, and GPT-2-XL models. All methods are evaluated using pairwise preference-based evaluation with GPT-4 on 100 randomly sampled test examples.

## Key Results
- LLM-generated explanations are more specific but less diverse than human-written explanations
- Model-enhanced human-written explanations (C+LLM) achieve the highest quality by balancing specificity and diversity
- EaO and SED imitation learning methods consistently reduce lose rates by 10% compared to supervised fine-tuning on both commonsense and uncommonsense tasks
- Online imitation learning outperforms supervised fine-tuning for both commonsense and uncommonsense abductive reasoning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Online imitation learning methods (EaO and SED) reduce exposure bias by exposing the learner to its own mistakes during training.
- **Mechanism:** Traditional supervised fine-tuning (SFT) trains a model to predict the next token conditioned on gold-standard prefixes, but during inference, the model conditions on its own generated tokens. This inconsistency leads to error propagation. EaO and SED address this by incorporating the learner's own outputs during training. EaO uses an expert policy (GPT-4) as an oracle to provide optimal continuations for learner-generated prefixes. SED minimizes the probability of learner-generated sequences while maximizing the probability of expert demonstrations, stabilizing training with KL divergence.
- **Core assumption:** The expert policy (GPT-4) can provide high-quality continuations that correct the learner's mistakes, and the learner can benefit from this correction during training.
- **Evidence anchors:**
  - [abstract] "These methods consistently reduce lose rates on both common and uncommonsense abductive reasoning judged by human evaluators."
  - [section] "EaO, which trains using expert online corrections to learner-generated sequence prefixes, shows more promise than SED."
  - [corpus] Weak - the corpus provides the data but doesn't directly support the mechanism.
- **Break condition:** If the expert policy is not significantly better than the learner, or if the learner's mistakes are too diverse for the expert to correct effectively, the method may not improve performance.

### Mechanism 2
- **Claim:** LLM-enhanced crowd-written explanations achieve the highest quality by trading off between specificity and diversity.
- **Mechanism:** Crowd workers excel at creating diverse explanations with a broader picture of possible intermediate events, but their explanations often lack sufficient details to connect contexts to outcomes. LLM-generated explanations are more specific but less diverse. By using an LLM to refine crowd-written explanations, the specificity of the LLM is combined with the diversity of the crowd, resulting in higher-quality explanations.
- **Core assumption:** The LLM can effectively enhance crowd-written explanations by adding relevant details without losing the diversity introduced by the crowd.
- **Evidence anchors:**
  - [abstract] "We find that model-enhanced human-written explanations achieve the highest quality by trading off between specificity and diversity."
  - [section] "C+LLM explanations hold a notable advantage over those generated only by an LLM."
  - [corpus] Weak - the corpus provides the data but doesn't directly support the mechanism.
- **Break condition:** If the LLM consistently overrides the crowd's creativity with its own style, or if the crowd's explanations are too poor to be enhanced effectively, the method may not improve quality.

### Mechanism 3
- **Claim:** The task of uncommonsense abductive reasoning is uniquely challenging because it requires reasoning about improbable situations.
- **Mechanism:** Unlike commonsense reasoning tasks that focus on common, everyday situations, uncommonsense abductive reasoning requires generating explanations for unlikely outcomes. This requires the model to think creatively and consider less obvious connections between context and outcome. The UNCOMMONSENSE dataset is designed to evaluate this ability by providing contexts paired with uncommon outcomes and requiring explanations that make these outcomes more likely.
- **Core assumption:** Models trained on common scenarios may struggle with uncommon ones, and a dataset focused on uncommon situations is necessary to evaluate this ability.
- **Evidence anchors:**
  - [abstract] "To instead investigate the ability to model unusual, unexpected, and unlikely situations, we explore the task of uncommonsense abductive reasoning."
  - [section] "UNCOMMONSENSE poses a unique challenge of abductive reasoning about uncommon outcomes."
  - [corpus] Weak - the corpus provides the data but doesn't directly support the mechanism.
- **Break condition:** If models can easily generalize from common to uncommon scenarios, or if the uncommon outcomes are not sufficiently challenging, the task may not be uniquely difficult.

## Foundational Learning

- **Concept:** Abductive reasoning
  - **Why needed here:** The task requires generating explanations that make an unlikely outcome more probable given a context. This is a form of abductive reasoning, which involves inferring the most likely explanation for an observation.
  - **Quick check question:** Given the context "It's raining heavily" and the outcome "The streets are dry," what is a plausible abductive explanation?

- **Concept:** Exposure bias in sequence generation
  - **Why needed here:** Traditional supervised fine-tuning suffers from exposure bias, where the model is trained on gold-standard prefixes but must generate sequences during inference. This can lead to error propagation.
  - **Quick check question:** Why might a model that is trained to predict the next word given the previous gold-standard words perform poorly when generating a full sequence on its own?

- **Concept:** Imitation learning
  - **Why needed here:** Imitation learning methods like EaO and SED are used to address the exposure bias problem by incorporating the learner's own outputs during training.
  - **Quick check question:** How does DAgger, a type of imitation learning algorithm, address the exposure bias problem in sequence generation tasks?

## Architecture Onboarding

- **Component map:** Base language model (e.g., GPT-2-XL, LLaMA-7B, FlanT5-XXL) -> Expert policy (GPT-4) -> Training algorithms (SFT, EaO, SED)
- **Critical path:** The base model generates explanations, which are then evaluated by the expert policy (for EaO) or compared with expert demonstrations (for SED). The training algorithm updates the base model's parameters based on this feedback.
- **Design tradeoffs:** Using a stronger base model (e.g., LLaMA-7B vs. GPT-2-XL) may improve performance but increase computational cost. Using EaO requires access to the expert policy during training, while SED only requires static expert demonstrations. The choice of training algorithm (SFT, EaO, SED) involves a tradeoff between performance and computational cost.
- **Failure signatures:** If the base model consistently generates poor explanations, it may indicate that the model is not learning effectively from the training data or expert feedback. If the expert policy is not significantly better than the base model, imitation learning methods may not improve performance. If the uncommon outcomes are too difficult or ambiguous, the model may struggle to generate plausible explanations.
- **First 3 experiments:**
  1. Train a base model using SFT on the UNCOMMONSENSE dataset and evaluate its performance on the test set.
  2. Train a base model using EaO with GPT-4 as the expert policy and evaluate its performance on the test set.
  3. Train a base model using SED with static expert demonstrations and evaluate its performance on the test set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed imitation learning methods compare to few-shot prompting with GPT-3 on commonsense abductive reasoning tasks?
- Basis in paper: Explicit - The paper compares the performance of few-shot prompting with GPT-3 to the proposed imitation learning methods on both commonsense and uncommonsense abductive reasoning tasks.
- Why unresolved: While the paper shows that the imitation learning methods outperform few-shot prompting on uncommonsense abductive reasoning, it does not provide a direct comparison on commonsense abductive reasoning tasks.
- What evidence would resolve it: Additional experiments comparing the performance of the proposed imitation learning methods to few-shot prompting with GPT-3 on commonsense abductive reasoning tasks would resolve this question.

### Open Question 2
- Question: How does the performance of the proposed imitation learning methods compare to supervised fine-tuning with larger language models like GPT-4 or T5-11B on uncommonsense abductive reasoning tasks?
- Basis in paper: Explicit - The paper compares the performance of the proposed imitation learning methods to supervised fine-tuning with smaller language models like LLaMA-7B and FlanT5-XXL, but does not provide a direct comparison to larger models like GPT-4 or T5-11B.
- Why unresolved: The paper focuses on improving the performance of smaller, more accessible language models, but it is unclear how the proposed methods would perform compared to larger, more powerful models.
- What evidence would resolve it: Additional experiments comparing the performance of the proposed imitation learning methods to supervised fine-tuning with larger language models like GPT-4 or T5-11B on uncommonsense abductive reasoning tasks would resolve this question.

### Open Question 3
- Question: How do the proposed imitation learning methods perform on other types of commonsense reasoning tasks beyond abductive reasoning?
- Basis in paper: Inferred - The paper only evaluates the proposed imitation learning methods on abductive reasoning tasks, but does not explore their performance on other types of commonsense reasoning tasks.
- Why unresolved: The proposed methods may be applicable to other types of commonsense reasoning tasks, but their effectiveness in these domains is unknown.
- What evidence would resolve it: Additional experiments evaluating the proposed imitation learning methods on other types of commonsense reasoning tasks, such as question answering or natural language inference, would resolve this question.

## Limitations

- The dataset filtering process relies heavily on GPT-4's likelihood judgments, which may introduce bias toward what the model considers "uncommon" rather than true human perceptions of uncommonness
- The pairwise preference evaluation may not capture all dimensions of explanation quality (e.g., creativity, plausibility)
- The study focuses on text-only reasoning, potentially missing multimodal aspects of uncommonsense reasoning that could be important in real-world applications

## Confidence

- High confidence: The core findings about human-written explanations being more diverse but less specific than LLM-generated ones
- Medium confidence: The effectiveness of online imitation learning methods (EaO and SED) in reducing exposure bias
- Medium confidence: The claim that C+LLM explanations achieve the highest quality by trading off specificity and diversity

## Next Checks

1. Conduct ablation studies to isolate the contribution of EaO's expert corrections versus SED's KL divergence regularization
2. Test model performance on cross-domain uncommon situations not seen during training to evaluate generalization
3. Implement human evaluation studies with larger annotator pools to verify the pairwise preference results and calculate confidence intervals for win rates