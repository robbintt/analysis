---
ver: rpa2
title: Mixed Pseudo Labels for Semi-Supervised Object Detection
arxiv_id: '2312.07006'
source_url: https://arxiv.org/abs/2312.07006
tags:
- pseudo
- images
- detection
- pseudo-labels
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the limitations of pseudo-label methods in
  semi-supervised object detection, which amplify the detector's strengths and accentuate
  weaknesses, particularly for small and tail category objects. To overcome these
  challenges, the paper proposes Mixed Pseudo Labels (MixPL), consisting of Mixup
  and Mosaic for pseudo-labeled data, to mitigate the negative impact of missed detections
  and balance the model's learning across different object scales.
---

# Mixed Pseudo Labels for Semi-Supervised Object Detection

## Quick Facts
- arXiv ID: 2312.07006
- Source URL: https://arxiv.org/abs/2312.07006
- Reference count: 40
- This paper proposes Mixed Pseudo Labels (MixPL) for semi-supervised object detection, achieving state-of-the-art results on COCO benchmarks with various detectors.

## Executive Summary
This paper analyzes the limitations of pseudo-label methods in semi-supervised object detection, particularly their tendency to amplify the detector's weaknesses, especially for small and tail category objects. The authors propose Mixed Pseudo Labels (MixPL), a novel approach consisting of Mixup and Mosaic for pseudo-labeled data, to mitigate these issues. MixPL effectively reduces the negative impact of missed detections and balances the model's learning across different object scales. Additionally, the paper introduces Labeled Resampling to improve detection performance on tail categories. MixPL consistently improves the performance of various detectors, including Faster R-CNN, FCOS, and DINO, achieving new state-of-the-art results on COCO-Standard and COCO-Full benchmarks.

## Method Summary
The paper proposes Mixed Pseudo Labels (MixPL) for semi-supervised object detection, building upon the Detection MeanTeacher framework. MixPL consists of Pseudo Mixup and Pseudo Mosaic, applied to pseudo-labeled data, and Labeled Resampling for tail categories. Pseudo Mixup involves a pixel-wise overlay of two pseudo-labeled images to reduce the negative impact of false negatives. Pseudo Mosaic combines four down-sampled pseudo-labeled images to balance the model's learning across different object scales. Labeled Resampling oversamples tail categories in labeled data to improve detection performance on these categories. The authors demonstrate the effectiveness of MixPL across various detectors, including Faster R-CNN, FCOS, and DINO, on COCO-Standard, COCO-Full, and VOC-Mixture benchmarks.

## Key Results
- MixPL consistently improves the performance of various detectors, including Faster R-CNN, FCOS, and DINO.
- MixPL achieves new state-of-the-art results on COCO-Standard and COCO-Full benchmarks.
- MixPL improves DINO Swin-L by 2.5% mAP, achieving a new record of 60.2% mAP on the COCO val2017 benchmark without extra annotations.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Pseudo Mixup mitigates negative effects of false negative samples by reducing gradient response from missed objects.
- **Mechanism**: Interpolating two pseudo-labeled images makes missed positive samples more similar to actual negatives, reducing their negative impact during optimization.
- **Core assumption**: The foreground-background imbalance in detection causes the model to incorrectly classify positive samples as negatives.
- **Evidence anchors**:
  - [abstract] "we propose Pseudo Mixup to reduce negative effects. This technique involves a pixel-wise overlay of two pseudo-labeled images, effectively mitigating the overall negative impact of false negative samples by making them more akin to actual negatives"
  - [section] "we proposed Pseudo Mixup. We leverage this imbalance to overlay two pseudo-labeled images and unite their pseudo-labels, ensuring that a positive sample is more likely to be superimposed with a negative sample"
  - [corpus] Weak evidence - no direct matches in corpus neighbors for this specific mechanism.
- **Break condition**: If the foreground-background imbalance is not significant, the effectiveness of Pseudo Mixup would diminish.

### Mechanism 2
- **Claim**: Pseudo Mosaic transforms large objects in pseudo-labels into small objects, balancing the model's learning across different object scales.
- **Mechanism**: Combining 4 down-sampling pseudo-labeled images increases the number of pseudo-labels for small and medium objects, providing more training samples for these object scales.
- **Core assumption**: Detectors struggle with smaller objects, and pseudo-labels exacerbate this by treating them as background.
- **Evidence anchors**:
  - [abstract] "we propose Pseudo Mosaic to transform large objects in pseudo-labels into small objects. Pseudo Mosaic combines 4 down-sampling pseudo-labeled images, providing more labels for small and medium objects"
  - [section] "The detectors struggle with smaller objects, and pseudo-labels exacerbate this by treating them as background in unlabeled data during optimization"
  - [corpus] Weak evidence - no direct matches in corpus neighbors for this specific mechanism.
- **Break condition**: If the object size distribution in the dataset is already balanced, the effectiveness of Pseudo Mosaic would be limited.

### Mechanism 3
- **Claim**: Labeled Resampling improves detection performance on tail categories by oversampling tail categories in labeled data.
- **Mechanism**: Oversampling tail categories from labeled data significantly improves the accuracy of tail categories, while the model effectively extracts head category pseudo-labels from unlabeled data.
- **Core assumption**: Detectors exhibit a significant performance disparity in detecting head and tail categories, resulting in an inadequate number of instances for certain tail categories in pseudo-labels.
- **Evidence anchors**:
  - [abstract] "the model's detection performance on tail categories is improved by resampling labeled data with relevant instances"
  - [section] "the model's detection performance on tail categories is improved by resampling labeled data with relevant instances. Although this may reduce head category samples, the model effectively extracts head category pseudo-labels from unlabeled data"
  - [corpus] Weak evidence - no direct matches in corpus neighbors for this specific mechanism.
- **Break condition**: If the labeled data does not contain sufficient tail category samples, the effectiveness of Labeled Resampling would be limited.

## Foundational Learning

- **Concept**: Foreground-background imbalance in object detection
  - Why needed here: Understanding this imbalance is crucial for grasping why pseudo-labels amplify the detector's weaknesses, particularly for small and tail category objects.
  - Quick check question: How does the foreground-background imbalance affect the model's predictions during semi-supervised learning?

- **Concept**: Semi-supervised learning paradigms (pseudo-labels and consistency regularization)
  - Why needed here: The paper builds upon these paradigms, specifically using pseudo-labels for unlabeled data and exploring consistency regularization through MixPL.
  - Quick check question: What are the key differences between pseudo-label-based and consistency-based semi-supervised learning approaches?

- **Concept**: Object detection architectures (one-stage, two-stage, and detection transformers)
  - Why needed here: The paper demonstrates the effectiveness of MixPL across different detection architectures, highlighting its detector-agnostic nature.
  - Quick check question: How do one-stage, two-stage, and detection transformer architectures differ in their approach to object detection?

## Architecture Onboarding

- **Component map**:
  - DetMeanTeacher: Basic semi-supervised learning framework with teacher-student models
  - Pseudo Mixup: Mixup applied to pseudo-labeled images to reduce false negatives
  - Pseudo Mosaic: Mosaic applied to pseudo-labeled images to balance object scales
  - Labeled Resampling: Oversampling tail categories in labeled data

- **Critical path**:
  1. Generate pseudo-labels on unlabeled data using teacher model
  2. Apply Pseudo Mixup to reduce false negatives
  3. Apply Pseudo Mosaic to balance object scales
  4. Apply Labeled Resampling to improve tail category detection
  5. Train student model on mixed pseudo-labeled data

- **Design tradeoffs**:
  - Pseudo Mixup vs. strong augmentation: Pseudo Mixup provides harder positive examples and reduces false negatives, while strong augmentation may introduce more false positives.
  - Pseudo Mosaic vs. fixed-size resizing: Pseudo Mosaic preserves object scale information better, while fixed-size resizing may distort object sizes.
  - Labeled Resampling vs. uniform sampling: Labeled Resampling improves tail category detection but may reduce head category samples.

- **Failure signatures**:
  - Decreased performance on small objects: May indicate issues with Pseudo Mosaic implementation.
  - Decreased performance on tail categories: May indicate issues with Labeled Resampling implementation.
  - Unstable training: May indicate issues with Pseudo Mixup implementation or confidence threshold settings.

- **First 3 experiments**:
  1. Implement DetMeanTeacher with basic pseudo-labeling and compare performance to supervised baseline.
  2. Add Pseudo Mixup to DetMeanTeacher and evaluate improvements in false negative reduction.
  3. Add Pseudo Mosaic to DetMeanTeacher and assess improvements in object scale balance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact relationship between the confidence threshold for filtering pseudo-labels and the specific classification loss function used (e.g. Cross-Entropy vs Focal Loss)?
- Basis in paper: [explicit] The paper states "both RetinaNet and Faster R-CNN have their respective optimal confidence thresholds, a deviation from which leads to an obvious performance drop" and "is related to the specific classification loss function."
- Why unresolved: The paper does not provide an explicit formula or detailed explanation for how the classification loss function impacts the optimal confidence threshold.
- What evidence would resolve it: A comprehensive study analyzing the performance of different object detectors using various confidence thresholds and classification loss functions, and deriving a formula or set of guidelines for choosing the optimal confidence threshold based on the loss function.

### Open Question 2
- Question: How does the choice of image size range for Pseudo Mosaic (e.g. 400-800 vs 200-1000) affect the model's ability to detect objects of different sizes?
- Basis in paper: [explicit] The paper states "We first study down-sampling or up-sampling images to fixed sizes and find that 800 achieves the best AP among fixed sizes and 400 obtains the best APs" and "The results show that 400-800 is a better trade-off than fixed sizes and other ranges of sizes."
- Why unresolved: The paper does not provide a detailed analysis of the impact of different image size ranges on the model's performance for objects of different sizes.
- What evidence would resolve it: An extensive ablation study varying the image size range for Pseudo Mosaic and analyzing the impact on the model's performance for objects of different sizes.

### Open Question 3
- Question: How does the power parameter in Labeled Resampling affect the model's performance for tail categories, and what is the optimal value for this parameter?
- Basis in paper: [explicit] The paper states "We propose Labeled Resampling to oversample tail categories in labeled data" and provides results for different power values.
- Why unresolved: The paper does not provide a detailed analysis of the impact of the power parameter on the model's performance for tail categories, nor does it provide guidelines for choosing the optimal value.
- What evidence would resolve it: A comprehensive study analyzing the impact of different power values on the model's performance for tail categories, and deriving a formula or set of guidelines for choosing the optimal power value based on the dataset and model architecture.

## Limitations
- The effectiveness of Pseudo Mixup relies on specific foreground-background imbalance conditions that may not generalize across datasets.
- The effectiveness of Pseudo Mosaic depends on the object size distribution in the target dataset, which may vary.
- Labeled Resampling's impact on tail categories assumes sufficient tail samples exist in the labeled data.

## Confidence
- **High confidence**: Detector-agnostic nature of MixPL (verified across Faster R-CNN, FCOS, and DINO)
- **Medium confidence**: Claims about balancing object scales through Pseudo Mosaic (mechanism is sound but dataset-dependent)
- **Low confidence**: Claims about significant false negative reduction through Pseudo Mixup (limited empirical validation of the mechanism)

## Next Checks
1. Conduct ablation studies on datasets with varying foreground-background imbalance ratios to validate Pseudo Mixup's effectiveness
2. Test Pseudo Mosaic on datasets with different object size distributions to assess generalizability
3. Evaluate Labeled Resampling on datasets with limited tail category samples to verify robustness