---
ver: rpa2
title: 'Predictability and Comprehensibility in Post-Hoc XAI Methods: A User-Centered
  Analysis'
arxiv_id: '2309.11987'
source_url: https://arxiv.org/abs/2309.11987
tags:
- participants
- explanations
- lime
- shap
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This user study evaluated how well participants comprehend and
  predict model behavior using SHAP and LIME explanations on a Boston Housing dataset.
  It found that SHAP comprehensibility decreases for samples near the decision boundary,
  while LIME was more effective at improving predictability when including misclassified
  and counterfactual examples.
---

# Predictability and Comprehensibility in Post-Hoc XAI Methods: A User-Centered Analysis

## Quick Facts
- arXiv ID: 2309.11987
- Source URL: https://arxiv.org/abs/2309.11987
- Reference count: 38
- Primary result: SHAP comprehensibility decreases near decision boundaries; LIME improves predictability with counterfactual examples.

## Executive Summary
This user study evaluated how well participants comprehend and predict model behavior using SHAP and LIME explanations on a Boston Housing dataset. It found that SHAP comprehensibility decreases for samples near the decision boundary, while LIME was more effective at improving predictability when including misclassified and counterfactual examples. Qualitative feedback indicated that inconsistent explanation scales and lack of counterfactual examples were key sources of confusion. The study recommends consistent scales, inclusion of counterfactual and boundary samples, and improved visualization design for better user understanding and prediction accuracy.

## Method Summary
The study used the Boston Housing dataset converted to a 3-class classification task with anonymized features F1-F5. A 3-layer dense neural network (64 units, ReLU, softmax) was trained with SGD (learning rate 0.001) achieving 93% accuracy. LIME (LimeTabularExplainer) and SHAP (KernelExplainer) were used to generate explanations. A between-subject design with 24 participants for LIME and 22 for SHAP completed a Qualtrics survey with 12 questions each. Scores were calculated with +2 for correct, -1 for wrong, and 0 for unsure responses, with statistical analysis using t-tests and Mann-Whitney tests.

## Key Results
- SHAP comprehensibility significantly decreased (t=5.54, p=0.00) for samples near the decision boundary.
- LIME's predictability score was significantly higher (t=-2.263, p=0.029) than SHAP when including counterfactual and misclassified samples.
- Qualitative feedback identified inconsistent scales and lack of counterfactual examples as key sources of confusion.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local explanations near the decision boundary are less comprehensible because the model's confidence is lower, making it harder to attribute feature contributions clearly.
- Mechanism: When a sample is close to the decision boundary, the difference in predicted probabilities between classes is small. This causes SHAP explanations to show many features with low-magnitude contributions, creating visual clutter and reducing interpretability.
- Core assumption: Users rely on magnitude and clarity of feature contributions to build mental models; low-confidence predictions provide ambiguous signals.
- Evidence anchors: Abstract states SHAP comprehensibility significantly reduced for samples near decision boundary; Section RQ1 reports significant decrease (t=5.54, p=0.00) between scores for samples near boundary.

### Mechanism 2
- Claim: Including counterfactual and misclassified samples improves predictability because they provide contrast cases that clarify the decision boundary.
- Mechanism: By exposing users to examples that are similar to a test case but classified differently, the model's decision logic becomes more tangible. Users can infer feature thresholds and interaction effects.
- Core assumption: Humans learn classification rules better through contrast and error analysis than from correct-only examples.
- Evidence anchors: Abstract states counterfactual explanations and misclassifications significantly increase understanding; Section RQ2 shows LIME's predictability score was significantly higher (t=-2.263, p=0.029) than SHAP after adding counterfactual/misclassified samples.

### Mechanism 3
- Claim: Inconsistent scales and inequality ranges in explanations cause confusion and reduce user confidence in interpreting model behavior.
- Mechanism: Users compare feature contributions across samples; if the visual scaling or range definitions change, they misinterpret relative importance and fail to transfer knowledge.
- Core assumption: Users perform cross-sample comparison as part of learning; inconsistent visual encoding breaks this cognitive strategy.
- Evidence anchors: Section RQ3 reports participants stating inconsistency of plot's scaling fooled them; abstract notes inconsistent explanation scales were key sources of confusion.

## Foundational Learning

- Concept: Local vs. Global Interpretability
  - Why needed here: The study uses LIME and SHAP, both local methods, but asks users to generalize to unseen samples. Understanding the distinction helps explain why local explanations may not fully support global prediction tasks.
  - Quick check question: If a model predicts Class A for a sample with high confidence, will a local explanation for a nearby sample always show the same top contributing feature? (Answer: No, because local explanations depend on the sample's position in feature space.)

- Concept: Decision Boundary and Prediction Confidence
  - Why needed here: The paper shows comprehensibility drops for samples near the boundary. Knowing how decision boundaries relate to probability distributions clarifies why low-confidence samples are harder to explain.
  - Quick check question: If two samples are equidistant from the decision boundary but on opposite sides, will their SHAP values have the same magnitude? (Answer: Not necessarily; magnitude depends on the model's probability surface, which can be asymmetric.)

- Concept: Counterfactual and Misclassified Samples in XAI
  - Why needed here: The study adds these samples to improve predictability. Understanding their role clarifies why contrast cases help users infer model behavior.
  - Quick check question: Does a counterfactual sample need to be misclassified by the model to be useful? (Answer: No; it just needs to be similar but classified differently to highlight decision logic.)

## Architecture Onboarding

- Component map: Data preparation -> Model training -> XAI methods -> Survey platform -> Scoring system -> Qualitative coding
- Critical path: 1. Recruit participants with ML background. 2. Train model and generate explanations for selected samples. 3. Administer survey with three assignments. 4. Collect and score responses. 5. Perform statistical analysis. 6. Conduct qualitative analysis of feedback. 7. Derive design recommendations.
- Design tradeoffs: Using simplified dataset improves control but limits generalizability; anonymizing features reduces domain bias but may lower engagement; between-subject design reduces confounding but halves per-method sample size; fixed explanations control variables but reduce ecological validity.
- Failure signatures: No significant difference between LIME and SHAP suggests ceiling effect or insensitive task design; frequent "I do not know" responses indicate explanations may be too complex or samples too ambiguous; qualitative feedback citing scale inconsistency indicates visual design needs revision; high variance in scores suggests background expertise or motivation differences.
- First 3 experiments: 1. Reproduce comprehensibility test with synthetic dataset where feature contributions are known to validate scoring rubric. 2. Add interactive explanation tool where users can toggle feature visibility to measure if this reduces confusion from scale inconsistency. 3. Run follow-up study with real-world anonymized data (e.g., medical diagnosis) to test generalizability of design recommendations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the distance from the decision boundary specifically affect the comprehensibility of SHAP explanations?
- Basis in paper: [explicit] The paper found that SHAP comprehensibility significantly decreases for samples near the decision boundary.
- Why unresolved: While the paper establishes this correlation, it does not provide a detailed explanation of the underlying mechanism or explore potential solutions to improve comprehensibility for boundary samples.
- What evidence would resolve it: Further studies could investigate the impact of different feature contributions on comprehensibility near the decision boundary, and explore methods to enhance SHAP explanations for such cases.

### Open Question 2
- Question: How can the inconsistency of inequality ranges in LIME explanations be addressed to improve user comprehension?
- Basis in paper: [inferred] The paper mentions that participants were confused by inconsistent inequality ranges for the same feature in different samples of the same class when using LIME.
- Why unresolved: The paper does not propose specific solutions to address this issue or evaluate the effectiveness of potential approaches.
- What evidence would resolve it: Experiments could be conducted to test different methods of presenting LIME explanations, such as showing the correlation between feature values or providing additional context for the inequality ranges.

### Open Question 3
- Question: How does the inclusion of counterfactual and misclassified samples impact the predictability of XAI methods beyond LIME and SHAP?
- Basis in paper: [explicit] The paper found that adding explanations of misclassified and counterfactual samples significantly improved predictability, especially with LIME.
- Why unresolved: The study only compared LIME and SHAP, leaving the generalizability of this finding to other XAI methods unknown.
- What evidence would resolve it: Further research could evaluate the impact of counterfactual and misclassified samples on the predictability of other popular XAI methods, such as LORE or SHAP variants.

## Limitations
- Simplified, anonymized dataset limits generalizability to real-world domains.
- Between-subject design reduces statistical power and may miss method-specific nuances.
- Fixed, non-interactive explanations do not reflect modern XAI interfaces.

## Confidence
- **High confidence**: SHAP comprehensibility decreases near decision boundaries (supported by t-test results, p=0.00).
- **Medium confidence**: LIME improves predictability with counterfactual samples (supported by t-test, p=0.029, but small sample size).
- **Medium confidence**: Scale inconsistency causes user confusion (supported by qualitative feedback, but no quantitative measure of confusion severity).

## Next Checks
1. Cross-domain validation: Replicate the study using medical diagnosis data with real feature names to test if design recommendations hold in high-stakes domains.
2. Interactive vs. static comparison: Compare user performance using interactive explanation tools (feature toggling, zoom) against the static visualizations used in this study.
3. Model-agnostic evaluation: Test the same XAI methods and user study design on a tree-based model (e.g., random forest) to assess whether findings generalize across model architectures.