---
ver: rpa2
title: Multimodal Detection of Bots on X (Twitter) using Transformers
arxiv_id: '2308.14484'
source_url: https://arxiv.org/abs/2308.14484
tags:
- authors
- social
- vgg16
- user
- bots
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the problem of detecting social spambots in
  Twitter. It introduces a novel approach that exploits only the user description
  field and images created from the digital DNA sequences representing users' actions.
---

# Multimodal Detection of Bots on X (Twitter) using Transformers

## Quick Facts
- arXiv ID: 2308.14484
- Source URL: https://arxiv.org/abs/2308.14484
- Reference count: 40
- Key outcome: This study introduces a novel approach for detecting social spambots on Twitter using only user description fields and images created from digital DNA sequences representing user actions. The best performing model achieved an accuracy of 99.98%.

## Executive Summary
This paper presents a novel approach to detecting social spambots on Twitter by leveraging only the user description field and images derived from digital DNA sequences that encode users' tweet patterns. The method transforms DNA sequences into 3D images and applies pretrained vision models, then combines these visual representations with textual embeddings from TwHIN-BERT using multimodal fusion techniques. The proposed approach achieves exceptionally high accuracy (99.98%) on the Cresci-17 dataset, outperforming existing methods while using only two types of features. The study also explores three different fusion methods - concatenation, gated multimodal unit, and crossmodal attention - to combine textual and visual modalities.

## Method Summary
The proposed method creates digital DNA sequences for each user based on tweet types and content, then transforms these sequences into 3D images using grayscale-to-RGB conversion. These images are processed by pretrained vision models (VGG16, EfficientNet) while user descriptions are encoded using TwHIN-BERT. The study proposes three fusion methods - concatenation, gated multimodal unit (GMU), and crossmodal attention - to combine the textual and visual representations before classification. The approach is evaluated on the Cresci-17 and TwiBot-20 datasets, achieving state-of-the-art performance with high accuracy, precision, recall, and F1-scores.

## Key Results
- Achieved 99.98% accuracy on the Cresci-17 dataset for bot detection
- Cross-modal attention fusion outperformed concatenation and GMU methods
- Model using only user description and DNA-sequence images surpassed existing approaches using more diverse feature sets
- The approach successfully detected sophisticated bots including Cyborg and Sock-puppet types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting digital DNA sequences into 3D images allows pretrained vision models to detect bot behaviors effectively.
- Mechanism: DNA sequences representing user tweet patterns are encoded as 3D images using grayscale-to-RGB conversion. These images are then fed into pretrained vision models (e.g., VGG16, EfficientNet) which extract spatial features corresponding to behavioral patterns, enabling classification of bots vs. genuine users.
- Core assumption: The temporal and structural patterns in DNA sequences can be preserved and learned from in image format, and vision models can generalize this to bot detection.
- Evidence anchors: [abstract] "we create digital DNA sequences per user based on both the type and content of the tweets and transform these sequences into 3d images"; [section] "After having created a digital DNA sequence per user, we adopt the method introduced in [15] for transforming the DNA sequence into an image consisting of three channels"
- Break condition: If the transformation fails to preserve discriminative behavioral patterns, vision models cannot learn useful features, leading to poor detection performance.

### Mechanism 2
- Claim: Multimodal fusion combining text (TwHIN-BERT) and image (VGG16) representations improves detection accuracy.
- Mechanism: TwHIN-BERT extracts contextual embeddings from user descriptions; VGG16 extracts visual features from DNA-sequence images. Fusion methods (concatenation, GMU, cross-modal attention) integrate these modalities to capture complementary information, enhancing classification.
- Core assumption: User descriptions and behavioral DNA patterns contain complementary cues for bot detection, and the fusion method effectively leverages this complementarity.
- Evidence anchors: [abstract] "we propose three different fusion methods, namely concatenation, gated multimodal unit, and crossmodal attention, for fusing the different modalities"; [section] "we propose three methods for fusing the representations of the different modalities"
- Break condition: If one modality dominates or if modalities are redundant, fusion provides little benefit or may degrade performance.

### Mechanism 3
- Claim: Cross-modal attention captures interactions between textual and visual features better than concatenation or GMU.
- Mechanism: Scaled dot-product attention aligns textual embeddings (queries) with visual features (keys/values), enabling the model to focus on relevant cross-modal interactions. This dynamic weighting is expected to outperform static fusion methods.
- Core assumption: Cross-modal interactions are critical for distinguishing bots and can be learned via attention mechanisms.
- Evidence anchors: [abstract] "we use a cross-attention mechanism for capturing the inter-modal interactions"; [section] "we exploit two crossmodal attention layers, i.e., one from textual f t to visual features f v and one from visual to textual features"
- Break condition: If attention weights become noisy or if modalities are not well-aligned, attention may fail to improve performance.

## Foundational Learning

- Concept: DNA sequence encoding of user behaviors
  - Why needed here: Provides a structured, sequential representation of user actions (tweet types and content) that can be transformed into a visual format for model ingestion.
  - Quick check question: Can you explain how a sequence like "T-A-C-U-H" would be mapped into a 3D image?

- Concept: Multimodal representation fusion
  - Why needed here: Combines complementary information from user descriptions (textual semantics) and behavioral patterns (visual structure) to improve classification robustness.
  - Quick check question: What is the difference between concatenation and gated multimodal unit fusion?

- Concept: Cross-modal attention mechanisms
  - Why needed here: Enables the model to dynamically weigh interactions between text and image features, potentially capturing nuanced bot behavior indicators.
  - Quick check question: How does scaled dot-product attention work in the context of aligning textual and visual modalities?

## Architecture Onboarding

- Component map: DNA sequence generation -> 3D image creation -> VGG16 encoding; User description -> TwHIN-BERT encoding; Text and image features -> Fusion module -> Dense layers -> Classification
- Critical path: 1. Generate DNA sequences from tweet logs; 2. Convert sequences to 3D images; 3. Pass user descriptions through TwHIN-BERT; 4. Pass images through VGG16; 5. Fuse representations (choose method); 6. Classify with dense layers
- Design tradeoffs: Using only description + DNA images reduces feature engineering but may miss other behavioral signals; Cross-modal attention increases complexity but may yield better performance; Image-based approach assumes visual models can interpret behavioral patterns effectively
- Failure signatures: Low recall indicates missing behavioral cues in DNA encoding; Similar performance across fusion methods suggests modalities are redundant; High precision but low recall indicates over-sensitivity to specific patterns
- First 3 experiments: 1. Evaluate unimodal models (TwHIN-BERT only, VGG16 only) on the dataset; 2. Test multimodal models with concatenation fusion; 3. Compare concatenation vs. GMU vs. cross-modal attention fusion methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of hyperparameter tuning on the performance of the proposed multimodal models?
- Basis in paper: [inferred] The paper states "Firstly, we did not apply hyperparameter tuning due to limited access to GPU resources. Applying hyperparameter tuning often leads to performance improvements."
- Why unresolved: The authors did not conduct hyperparameter tuning due to computational resource constraints, leaving the potential performance gains unknown.
- What evidence would resolve it: Conduct experiments with hyperparameter tuning on the same datasets and compare the performance metrics (e.g., accuracy, F1-score) to the current results.

### Open Question 2
- Question: How would the proposed multimodal models perform on datasets other than Cresci'17 and TwiBot-20?
- Basis in paper: [inferred] The paper mentions "In the future, we aim to apply our approaches in another datasets for ensuring their generalization."
- Why unresolved: The current experiments were conducted on specific datasets, and the authors plan to test the models on other datasets in the future.
- What evidence would resolve it: Apply the proposed models to different datasets and evaluate their performance to assess their generalization capabilities.

### Open Question 3
- Question: How effective would self-supervised learning approaches be in detecting social spambots compared to the proposed supervised methods?
- Basis in paper: [explicit] The paper states "Finally, this study requires labelled data. On the contrary, self-supervised learning approaches address the issue of labels' scarcity."
- Why unresolved: The current study relies on labelled data, and the potential of self-supervised learning for this task remains unexplored.
- What evidence would resolve it: Develop and evaluate self-supervised learning models for social spambot detection and compare their performance to the proposed supervised methods.

## Limitations
- Evaluation relies on a single public dataset without cross-validation on diverse bot types or temporal splits
- The transformation of DNA sequences into 3D images lacks empirical justification for why visual models can effectively learn from behavioral patterns encoded this way
- Extremely high accuracy (99.98%) suggests potential overfitting or dataset-specific artifacts rather than robust detection capability

## Confidence
- Mechanism 1 (DNA-to-image transformation): Medium confidence - While the transformation method is clearly described, there is limited evidence that visual models can effectively learn from behavioral patterns in this format.
- Mechanism 2 (Multimodal fusion): Medium confidence - The fusion methods are well-defined, but the assumption that text and image modalities are complementary for bot detection needs more validation.
- Mechanism 3 (Cross-modal attention): Low confidence - The attention mechanism is described but lacks empirical comparison to simpler fusion methods to demonstrate its superiority.

## Next Checks
1. **Cross-dataset validation**: Evaluate the model on multiple bot detection datasets (e.g., Varol-17, Botometer datasets) with temporal splits to assess generalization across bot types and time periods.
2. **Ablation study on modalities**: Test unimodal performance of TwHIN-BERT and VGG16 separately, then compare against multimodal variants to quantify the actual contribution of each modality.
3. **Feature importance analysis**: Conduct quantitative analysis (e.g., permutation importance, attention visualization) to understand which DNA sequence patterns and textual features drive bot detection decisions.