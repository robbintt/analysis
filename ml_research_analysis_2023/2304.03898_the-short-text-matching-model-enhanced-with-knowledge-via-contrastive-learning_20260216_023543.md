---
ver: rpa2
title: The Short Text Matching Model Enhanced with Knowledge via Contrastive Learning
arxiv_id: '2304.03898'
source_url: https://arxiv.org/abs/2304.03898
tags:
- sentence
- matching
- knowledge
- text
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a short text matching model enhanced with knowledge
  via contrastive learning to address the lack of semantic information and word ambiguity
  in short texts. The model uses a generative model to create complement sentences
  and employs contrastive learning to obtain more semantically meaningful encodings
  of the original sentence.
---

# The Short Text Matching Model Enhanced with Knowledge via Contrastive Learning

## Quick Facts
- arXiv ID: 2304.03898
- Source URL: https://arxiv.org/abs/2304.03898
- Reference count: 24
- Short text matching model using contrastive learning and knowledge graphs achieves state-of-the-art performance on Chinese text matching datasets

## Executive Summary
This paper addresses the challenges of semantic ambiguity and lack of context in short text matching by proposing a model that combines contrastive learning with knowledge graph integration. The model generates complement sentences using a SimBERT generative model, applies contrastive learning to align original and complement embeddings, and constructs knowledge graphs from keywords and external knowledge to resolve word ambiguity. Experiments on two Chinese datasets demonstrate superior performance compared to existing methods.

## Method Summary
The model uses BERT-base-chinese to encode original sentences and their complements, with the complements generated by a SimBERT model trained on browser search data. Keywords are extracted using TextRank and used to retrieve knowledge words from HowNet, forming a graph encoded by GCN. The final representation combines sentence embeddings, their element-wise difference, and the graph embedding, trained with a weighted combination of BCE and contrastive losses.

## Key Results
- Achieves state-of-the-art performance on BQ and LCQMC Chinese text matching datasets
- Outperforms existing methods on both accuracy and F1 metrics
- Demonstrates effectiveness of combining contrastive learning with knowledge graph integration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning aligns original and complementary sentences in embedding space, reducing semantic ambiguity in short texts.
- Mechanism: The model encodes original and complementary sentences separately, then applies InfoNCE loss to minimize the distance between positive pairs (original + its complement) while maximizing the distance to negative pairs (original + other complements in the batch). This encourages the model to learn shared semantic features and ignore irrelevant noise.
- Core assumption: The complement sentences generated by SimBERT are semantically close to their original sentences and thus serve as effective positive examples.
- Evidence anchors:
  - [abstract] "uses the contrastive learning method to guide the model to obtain more semantically meaningful encoding of the original sentence."
  - [section 3.5] "we propose a semi-supervised contrastive learning strategy for integrating the enhanced layers... The training objective is to minimize the semantic distance between the original sentence and the complement sentence, while maximizing the distance between unrelated sentences."
- Break condition: If complement sentences are poorly generated (e.g., semantically unrelated or too generic), contrastive learning may push apart semantically similar pairs or pull together dissimilar ones, degrading model performance.

### Mechanism 2
- Claim: Knowledge graphs constructed from keywords and retrieved knowledge words integrate external knowledge to resolve word ambiguity.
- Mechanism: Keywords are extracted from the original sentences using TextRank, then used to retrieve relevant knowledge words from HowNet. These are used as nodes in a graph where edges represent similarity weights. A GCN encodes the graph to produce a knowledge-aware representation that is concatenated with sentence embeddings for final prediction.
- Core assumption: Keywords accurately represent the main semantics of the sentence, and the knowledge base contains high-quality, relevant entries for those keywords.
- Evidence anchors:
  - [abstract] "to avoid noise, we use keywords as the main semantics of the original sentence to retrieve corresponding knowledge words in the knowledge base, and construct a knowledge graph."
  - [section 3.3] "we utilize the keywords from the original sentence to retrieve relevant knowledge words from external knowledge sources... we construct a graph with these keywords and knowledge words as nodes, and the edges of the graph represent the relationships between words."
- Break condition: If keywords miss critical semantics or the knowledge base is noisy, the graph representation may introduce misleading information instead of resolving ambiguity.

### Mechanism 3
- Claim: Complement sentences provide richer context, improving semantic encoding for short texts.
- Mechanism: A SimBERT model, fine-tuned on large-scale query-ad pairs from UC and Quark browsers, generates context-aware complement sentences for each original sentence. These complements are concatenated with the original sentence embeddings before contrastive learning and final prediction.
- Core assumption: SimBERT can generate high-quality complements that meaningfully extend the context without introducing contradictory or irrelevant content.
- Evidence anchors:
  - [abstract] "The model uses a generative model to generate corresponding complement sentences and uses the contrastive learning method to guide the model to obtain more semantically meaningful encoding of the original sentence."
  - [section 3.6] "We collected user query statements and corresponding advertisements returned by search engines... We filtered out ad texts below the threshold K, and paired ad texts above the threshold with their corresponding queries to form sentence pairs."
- Break condition: If the generative model overfits to the training distribution or fails to capture sentence-specific context, the complements may not provide meaningful semantic extension, reducing the effectiveness of both the encoding and contrastive learning stages.

## Foundational Learning

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: Short texts lack context, so learning shared features between original and complement sentences is essential for accurate matching.
  - Quick check question: What is the role of the temperature hyperparameter τ in the InfoNCE loss formulation?

- Concept: Graph Convolutional Networks (GCN)
  - Why needed here: To encode relationships between keywords and knowledge words in a structured graph form, capturing semantic dependencies not evident in flat embeddings.
  - Quick check question: How does stacking multiple GCN layers allow the model to integrate information from both direct and indirect neighbors?

- Concept: Keyword extraction and semantic similarity retrieval
  - Why needed here: Keywords represent core semantics; retrieving similar words from a knowledge base enriches the representation with disambiguated meanings.
  - Quick check question: What algorithm is used to extract keywords from sentences in this model, and why is it appropriate?

## Architecture Onboarding

- Component map: Text Encoding Module -> Graph Encoding Module -> Aggregation Layer -> Binary Classifier -> Contrastive Learning Head
- Critical path: Original sentence → Text Encoder → Complement generation → Contrastive loss → Concat with graph → Final classifier → prediction
- Design tradeoffs:
  - Using the same BERT encoder for all sentences reduces parameters but may limit specialization.
  - Generating complements adds computation but enriches context.
  - GCN-based knowledge integration adds complexity but can resolve ambiguity.
- Failure signatures:
  - High variance in complement quality → unstable contrastive learning.
  - Irrelevant knowledge words → noisy graph representation.
  - Over-smoothing in GCN → graph embeddings lose discriminative power.
- First 3 experiments:
  1. Train without contrastive learning to assess the impact of alignment.
  2. Train without the knowledge graph to isolate the benefit of external knowledge.
  3. Train with random complement sentences (instead of SimBERT-generated) to test the importance of high-quality complements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the KSTM model change when using different generative models to create complement sentences instead of SimBERT?
- Basis in paper: [explicit] The paper mentions that they used SimBERT to generate complement sentences and suggests exploring other powerful models like ChatGPT in the future.
- Why unresolved: The paper only tested the model with SimBERT and did not compare its performance with other generative models.
- What evidence would resolve it: Conducting experiments using different generative models (e.g., ChatGPT, GPT-3) to create complement sentences and comparing their performance with the KSTM model using SimBERT.

### Open Question 2
- Question: How does the performance of the KSTM model change when using different knowledge bases instead of HowNet to construct the knowledge graph?
- Basis in paper: [explicit] The paper mentions that they used HowNet to retrieve knowledge words for constructing the knowledge graph and suggests designing a refined filtering mechanism in the future.
- Why unresolved: The paper only tested the model with HowNet and did not compare its performance with other knowledge bases.
- What evidence would resolve it: Conducting experiments using different knowledge bases (e.g., ConceptNet, WordNet) to construct the knowledge graph and comparing their performance with the KSTM model using HowNet.

### Open Question 3
- Question: How does the performance of the KSTM model change when using different methods to extract keywords from the original sentences instead of TextRank?
- Basis in paper: [explicit] The paper mentions that they used TextRank to extract keywords from the original sentences and suggests designing a refined filtering mechanism in the future.
- Why unresolved: The paper only tested the model with TextRank and did not compare its performance with other keyword extraction methods.
- What evidence would resolve it: Conducting experiments using different keyword extraction methods (e.g., TF-IDF, RAKE) and comparing their performance with the KSTM model using TextRank.

## Limitations

- The paper lacks detailed implementation specifics for the SimBERT complement generation model and HowNet knowledge base integration
- No evaluation criteria or metrics for complement sentence quality are provided
- Potential domain adaptation challenges are not addressed when applying the model to datasets different from the browser search data

## Confidence

- **High confidence**: The overall architecture combining contrastive learning with knowledge graphs is technically sound and follows established patterns in the field
- **Medium confidence**: The experimental results showing SOTA performance on Chinese STM datasets, though the paper lacks extensive ablation studies
- **Low confidence**: The specific implementation details for complement generation and knowledge graph construction are insufficient for faithful reproduction

## Next Checks

1. Conduct controlled ablation studies removing the contrastive learning component to quantify its specific contribution to performance improvements
2. Perform manual evaluation of complement sentence quality by comparing SimBERT-generated complements against human-written extensions for a sample of sentences
3. Test model robustness by evaluating performance on datasets from different domains than the browser search data used for complement generation