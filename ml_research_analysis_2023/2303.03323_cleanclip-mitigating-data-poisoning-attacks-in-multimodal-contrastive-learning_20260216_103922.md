---
ver: rpa2
title: 'CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning'
arxiv_id: '2303.03323'
source_url: https://arxiv.org/abs/2303.03323
tags:
- backdoor
- data
- clip
- attack
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of backdoor attacks in multimodal
  contrastive learning, specifically targeting CLIP models. The authors propose CleanCLIP,
  a fine-tuning framework that mitigates backdoor attacks by combining multimodal
  contrastive loss with self-supervised learning objectives.
---

# CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning

## Quick Facts
- **arXiv ID**: 2303.03323
- **Source URL**: https://arxiv.org/abs/2303.03323
- **Reference count**: 40
- **Primary result**: CleanCLIP reduces backdoor attack success rates while maintaining clean accuracy through fine-tuning that combines multimodal contrastive loss with self-supervised learning objectives

## Executive Summary
This paper addresses backdoor attacks in multimodal contrastive learning models, specifically targeting CLIP models trained on poisoned data. The authors propose CleanCLIP, a fine-tuning framework that mitigates these attacks by combining multimodal contrastive loss with self-supervised learning objectives. This approach breaks the spurious associations between backdoor triggers and target labels learned during pretraining. The method significantly reduces attack success rates across multiple attack types while maintaining clean accuracy, and the authors demonstrate that supervised fine-tuning on task-specific labeled data can effectively erase backdoor triggers from the vision encoder.

## Method Summary
CleanCLIP mitigates backdoor attacks by fine-tuning poisoned CLIP models using a combined objective that includes both multimodal contrastive loss and self-supervised learning components. The self-supervised component forces independent learning of image and text representations, weakening the spurious associations between backdoor triggers and target labels. The method uses a weighted combination of these losses (λ1 for multimodal contrastive loss and λ2 for self-supervised loss) during fine-tuning. Additionally, the authors show that supervised fine-tuning on task-specific labeled data can erase backdoor triggers by adapting the vision encoder to new downstream task distributions.

## Key Results
- CleanCLIP significantly reduces attack success rates across BadNet, Blended, WaNet, and Label-Consistent backdoor attacks
- The defense maintains clean accuracy on benign examples while mitigating backdoor vulnerabilities
- Supervised fine-tuning on task-specific labeled data effectively removes backdoor triggers from the CLIP vision encoder
- Increasing self-supervision strength leads to greater reduction in attack success rate without significant clean accuracy degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CleanCLIP mitigates backdoor attacks by breaking the spurious correlation between the backdoor trigger and the target label in the joint embedding space.
- Mechanism: CleanCLIP uses a combination of multimodal contrastive loss and self-supervised learning objectives during fine-tuning. The self-supervised component forces independent learning of image and text representations, weakening the learned spurious associations.
- Core assumption: The backdoor vulnerability arises from spurious co-occurrence learned during pretraining, not from fundamental architectural flaws.
- Evidence anchors:
  - [abstract]: "CleanCLIP, a finetuning framework that weakens the learned spurious associations introduced by backdoor attacks by independently re-aligning the representations for individual modalities."
  - [section 3]: "The core insight behind CleanCLIP is that learning representations for each modality independently of the other could break the spurious correlation between the backdoor trigger and the target label."
  - [corpus]: Weak - the corpus mentions related defense works but doesn't provide direct evidence for this specific mechanism.
- Break condition: If the backdoor trigger creates genuine semantic associations (not spurious), independent modality learning won't break them.

### Mechanism 2
- Claim: Supervised fine-tuning on task-specific labeled data can erase backdoor triggers from the CLIP vision encoder.
- Mechanism: When the vision encoder adapts to a new downstream task distribution through supervised learning, the false backdoor associations are forgotten as the model prioritizes task-relevant features.
- Core assumption: The vision encoder can effectively forget the spurious backdoor associations when adapting to a new task.
- Evidence anchors:
  - [abstract]: "supervised fine-tuning on task-specific labeled image data removes the backdoor trigger from the CLIP vision encoder."
  - [section 6]: "Since the pretrained CLIP vision encoder adapts itself to the target distribution of the downstream task, the associations between the backdoor triggers and the target label are forgotten."
  - [corpus]: Weak - the corpus doesn't provide evidence specifically for this mechanism.
- Break condition: If the backdoor trigger creates strong, task-relevant features that persist across tasks.

### Mechanism 3
- Claim: The effectiveness of CleanCLIP depends on the strength of the self-supervision signal.
- Mechanism: Increasing the weight of the self-supervised loss (λ2) in the fine-tuning objective leads to greater reduction in attack success rate by forcing more independent modality learning.
- Core assumption: The self-supervised loss can be scaled to provide stronger independence without degrading clean accuracy.
- Evidence anchors:
  - [section 7.1]: "Our findings show that increasing the strength of the self-supervision signal leads to a monotonous reduction in attack success rate, while clean accuracy remains largely unaffected."
  - [corpus]: Weak - the corpus doesn't provide evidence for this specific mechanism.
- Break condition: If increasing self-supervision strength degrades clean accuracy or if the backdoor trigger is too strongly embedded to be broken by this approach.

## Foundational Learning

- Concept: Multimodal contrastive learning
  - Why needed here: Understanding how CLIP learns joint image-text representations is essential to grasp why backdoor attacks work and how CleanCLIP defends against them.
  - Quick check question: How does CLIP's contrastive loss bring matched image-text pairs closer in embedding space while pushing mismatched pairs apart?

- Concept: Backdoor attacks in machine learning
  - Why needed here: Understanding the mechanics of backdoor attacks (trigger injection, target label association) is crucial for understanding the threat model and the need for defenses like CleanCLIP.
  - Quick check question: What is the difference between a backdoor attack and a traditional data poisoning attack?

- Concept: Self-supervised learning objectives
  - Why needed here: CleanCLIP's defense mechanism relies on combining self-supervised learning with contrastive learning, so understanding self-supervised objectives is essential.
  - Quick check question: How does a self-supervised contrastive loss differ from a multimodal contrastive loss in terms of what it aligns in the embedding space?

## Architecture Onboarding

- Component map: CLIP consists of a vision encoder (ResNet-50) and a text encoder (transformer), both producing d-dimensional embeddings. CleanCLIP adds self-supervised learning components that operate independently on augmented versions of images and texts.
- Critical path: Pretrained poisoned CLIP → CleanCLIP fine-tuning (with λ1 multimodal loss + λ2 self-supervised loss) → Reduced ASR, maintained clean accuracy
- Design tradeoffs: Stronger self-supervision (higher λ2) better mitigates backdoors but may slightly reduce clean accuracy; supervised fine-tuning is highly effective but requires labeled data
- Failure signatures: High attack success rate after fine-tuning indicates the backdoor wasn't effectively erased; significant drop in clean accuracy indicates over-correction
- First 3 experiments:
  1. Fine-tune poisoned CLIP with only multimodal contrastive loss (should maintain high ASR)
  2. Fine-tune poisoned CLIP with only self-supervised loss (should reduce ASR but hurt clean accuracy)
  3. Fine-tune poisoned CLIP with CleanCLIP (balanced λ1=1, λ2=1) on CC3M subset (should reduce ASR while maintaining clean accuracy)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed defenses scale to web-scale pretraining datasets like CLIP's 400M image-text pairs?
- Basis in paper: [explicit] The authors acknowledge this as a limitation, stating "Although we lack the resources to pursue this study, it is essential to investigate the extent to which the effectiveness of the backdoor attacks and the benefits of our defense strategies hold at the scale at which the original CLIP model was trained (400M image-text pairs)."
- Why unresolved: The authors only tested on smaller datasets (3M for pretraining, 100K for finetuning) due to computational constraints.
- What evidence would resolve it: Empirical evaluation of CleanCLIP and supervised finetuning on datasets approaching CLIP's original scale, measuring attack success rates and clean accuracy.

### Open Question 2
- Question: Can backdoor triggers be designed to remain effective even after multimodal finetuning with self-supervised objectives?
- Basis in paper: [inferred] The authors speculate this possibility, noting "it is possible that our work could inspire further research on designing specialized multimodal backdoor attack strategies that remain potent even after finetuning."
- Why unresolved: The current backdoor attacks were designed for standard multimodal contrastive learning and may not account for finetuning with independent modality representations.
- What evidence would resolve it: Development and evaluation of new backdoor attacks specifically targeting finetuned multimodal models, testing their effectiveness against CleanCLIP and supervised finetuning.

### Open Question 3
- Question: What is the optimal balance between multimodal contrastive loss and self-supervised loss during finetuning for backdoor defense?
- Basis in paper: [explicit] The authors experimented with different λ2 values (0.5, 1, 2, 4, 8) in Section 7.1 but note "we did not conduct an extensive exploration of other potential dataset sources, and thus leave this as an area for future research."
- Why unresolved: The optimal ratio likely depends on factors like dataset size, backdoor type, and pretraining data characteristics, requiring more comprehensive exploration.
- What evidence would resolve it: Systematic hyperparameter search across various dataset sizes, backdoor types, and pretraining data scales to determine optimal λ1/λ2 ratios for different scenarios.

## Limitations
- The effectiveness of CleanCLIP has only been demonstrated on relatively small datasets (3M images) compared to CLIP's original 400M image-text pairs
- The method requires access to either unlabeled clean data for self-supervised fine-tuning or labeled data for supervised fine-tuning
- The defense may not work against backdoor attacks that create genuine semantic associations rather than spurious correlations

## Confidence
- **High Confidence**: CleanCLIP reduces attack success rates while maintaining clean accuracy under tested conditions. The core mechanism of breaking spurious correlations through independent modality learning is well-supported by experimental evidence.
- **Medium Confidence**: Supervised fine-tuning effectively erases backdoor triggers when labeled data is available. The claim holds for the tested scenarios but may vary with different data distributions and task complexities.
- **Medium Confidence**: The effectiveness of CleanCLIP scales with self-supervision strength. While experiments show a monotonous reduction in ASR with increased λ2, the practical limits and optimal balance point require further validation.

## Next Checks
1. Test CleanCLIP's effectiveness on CLIP models with different vision encoder architectures (e.g., Vision Transformers) to assess architectural generalizability.
2. Evaluate CleanCLIP against adaptive backdoor attacks that specifically target the defense mechanism by embedding semantically meaningful associations rather than spurious correlations.
3. Measure the degradation in clean accuracy as self-supervision strength increases beyond the values tested in the paper to identify the practical limits of the defense.