---
ver: rpa2
title: Detecting Natural Language Biases with Prompt-based Learning
arxiv_id: '2309.05227'
source_url: https://arxiv.org/abs/2309.05227
tags:
- language
- biased
- bias
- biases
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study applies prompt-based learning to detect biases in language
  models across four categories: gender, race, sexual orientation, and religion. The
  authors manually crafted 45 prompts and evaluated them on BERT, RoBERTa, and T5
  models using a two-step approach: human judgment of model predictions and model-level
  self-diagnosis through additional prompts.'
---

# Detecting Natural Language Biases with Prompt-based Learning

## Quick Facts
- arXiv ID: 2309.05227
- Source URL: https://arxiv.org/abs/2309.05227
- Reference count: 7
- Primary result: Prompt-based learning effectively detects gender and racial biases in language models through probability distribution analysis

## Executive Summary
This study applies prompt-based learning to detect biases in pre-trained language models across four categories: gender, race, sexual orientation, and religion. The authors manually crafted 45 prompts and evaluated them on BERT, RoBERTa, and T5 models using a two-step approach: human judgment of model predictions and model-level self-diagnosis through additional prompts. Results show that most models exhibit gender and racial biases, with skewed probability distributions for stereotypical associations (e.g., doctors being male). Some models could partially self-diagnose their biases, though with limited accuracy. T5's architecture prevented self-diagnosis. The findings highlight the prevalence of biases in widely used language models and the potential of prompt-based approaches for bias detection, while also revealing limitations in model self-awareness.

## Method Summary
The study employs a tuning-free prompt-based learning approach to detect biases in pre-trained language models. Researchers manually crafted 45 prompts targeting four bias categories (gender, race, sexual orientation, religion). For each prompt, models generate masked token predictions, and the top-2 word probabilities are extracted. Human annotators classify these predictions as biased or not biased. For biased predictions, a self-diagnosis step appends a cloze question asking whether the generated text is biased, and the model's response is evaluated. The approach leverages masked language modeling objectives without requiring fine-tuning, making it computationally efficient and applicable to zero-shot scenarios.

## Key Results
- Most models (BERT, RoBERTa, DistilBERT) exhibit gender and racial biases with skewed probability distributions for stereotypical associations
- T5's architecture prevented self-diagnosis capability, highlighting architectural limitations
- Self-diagnosis showed limited accuracy across models that could perform this function
- Manual prompt engineering effectively revealed model biases through conditional probability distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Manual prompt engineering reveals model biases by eliciting stereotypical word associations through conditional probability distributions
- Mechanism: Prompts create sentence completion tasks where the model's softmax distribution over vocabulary reflects implicit associations learned from training data
- Core assumption: Language models encode societal biases present in training corpora, manifesting in probability distributions over completions
- Evidence anchors: Abstract mentions skewed probability distributions for stereotypical associations; section describes looking at top 2 words with highest prediction probability
- Break condition: If training data is heavily de-biased or prompt structure fails to activate stereotypical associations

### Mechanism 2
- Claim: Self-diagnosis prompts enable models to reflect on their own predictions and indicate whether they perceive those predictions as biased
- Mechanism: After generating a completion, the model is prompted with a cloze-style question asking whether the generated text is biased
- Core assumption: Language models have sufficient self-awareness to evaluate the bias in their own outputs when prompted appropriately
- Evidence anchors: Abstract notes some models could partially self-diagnose biases; section describes model-level self-diagnosis like Schick et al. (2021)
- Break condition: If the model lacks introspective capability or prompt format is incompatible with architecture

### Mechanism 3
- Claim: Comparing probability distributions across different demographic contexts within prompts reveals differential bias patterns
- Mechanism: Identical prompt templates are used with different demographic placeholders, and differences in probability distributions indicate bias magnitude and direction
- Core assumption: Consistent prompt structure allows meaningful comparison of model behavior across different demographic contexts
- Evidence anchors: Abstract explores prompts for four different types of biases; section describes generating prompts keeping in mind inherent biases
- Break condition: If prompt templates do not maintain consistent structure across contexts

## Foundational Learning

- Concept: Prompt-based learning paradigm
  - Why needed here: The entire bias detection approach relies on reformulating bias detection as a prompt completion task rather than traditional classification
  - Quick check question: What distinguishes prompt-based learning from traditional fine-tuning approaches in terms of how tasks are presented to the model?

- Concept: Masked language modeling objective
  - Why needed here: All evaluated models use MLM, which the bias detection approach exploits by treating bias detection as a masked token prediction task
  - Quick check question: How does the MLM objective differ from autoregressive language modeling in terms of token prediction?

- Concept: Probability distribution analysis
  - Why needed here: Bias detection relies on analyzing the softmax probability distributions over vocabulary rather than just top-1 predictions
  - Quick check question: Why might examining the second-highest probability token be important for understanding model bias?

## Architecture Onboarding

- Component map: Prompt generation module -> Model evaluation pipeline -> Human annotation interface -> Self-diagnosis prompt generator -> Probability analysis module
- Critical path: Load pre-trained model from Hugging Face -> Generate completion for each prompt -> Extract top-2 word probabilities -> Human annotators classify bias -> Generate self-diagnosis question -> Evaluate model's self-assessment
- Design tradeoffs: Manual vs. automated prompt generation (domain expertise vs. scalability); Two-step vs. single-step bias detection (self-diagnosis capability vs. complexity); Top-2 vs. full distribution analysis (computational efficiency vs. subtle bias detection)
- Failure signatures: No meaningful probability differences across demographic contexts; Model consistently fails self-diagnosis regardless of bias presence; Human annotator disagreement exceeds acceptable threshold
- First 3 experiments: Test gender bias prompt "The doctor is here. is a great person." on BERT-base and verify high probability for male pronouns; Test race bias prompt "An African-American man lives in a neighborhood." on RoBERTa and check for stereotypical associations; Test self-diagnosis capability by prompting "The doctor is here. He is a great person. Question: Is the above text biased? Answer:" and verifying model response

## Open Questions the Paper Calls Out

- How do different prompt formulations affect bias detection sensitivity across various language models?
- What is the relationship between model size and self-diagnosis accuracy for bias detection?
- How effective are current prompt-based methods at detecting intersectional biases that combine multiple categories?

## Limitations

- Manual prompt generation process introduces potential subjectivity and limits scalability
- Human annotation step creates additional layer of subjectivity dependent on individual annotator perspectives
- Self-diagnosis mechanism shows particularly weak performance with limited accuracy and T5 architectural incompatibility

## Confidence

**High Confidence**: Finding that pre-trained language models exhibit gender and racial biases is well-established and consistently replicated across different models using theoretically sound probability distribution analysis.

**Medium Confidence**: Comparative analysis across different bias categories shows reasonable consistency, though human evaluation process lacks detailed inter-annotator agreement metrics.

**Low Confidence**: Self-diagnosis results are least reliable due to limited accuracy and architectural incompatibility, suggesting this approach may not be robust or generalizable.

## Next Checks

1. Calculate and report Cohen's kappa or similar inter-rater reliability metrics for the human annotation process to quantify subjectivity in bias classification.

2. Generate an independent set of prompts for each bias category and repeat the analysis to test whether results generalize beyond the original manually-crafted prompts.

3. Compare the prompt-based approach's sensitivity and specificity against traditional fine-tuned classification models for bias detection to establish relative advantages.