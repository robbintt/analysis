---
ver: rpa2
title: Learning Personalized Decision Support Policies
arxiv_id: '2304.06701'
source_url: https://arxiv.org/abs/2304.06701
tags:
- support
- human
- policy
- decision
- decision-maker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We learn personalized decision support policies that choose which\
  \ form of support to provide to decision-makers based on the input and the decision-maker\u2019\
  s strengths. Our algorithm THREAD learns policies online using contextual bandits\
  \ and balances performance and cost through a trade-off parameter."
---

# Learning Personalized Decision Support Policies

## Quick Facts
- **arXiv ID:** 2304.06701
- **Source URL:** https://arxiv.org/abs/2304.06701
- **Reference count:** 40
- **Key outcome:** THREAD learns personalized decision support policies that adapt to individual decision-makers' strengths and weaknesses, outperforming offline baselines on vision and language tasks.

## Executive Summary
This paper introduces THREAD, an online learning algorithm that personalizes decision support by selecting appropriate forms of assistance (e.g., expert consensus, LLM predictions) for individual decision-makers. The algorithm uses contextual bandits to estimate human prediction errors under different support forms and balances performance and cost through a trade-off parameter. Experiments show THREAD learns policies that reduce cost with minimal performance loss, and human subject studies validate the personalization benefits across vision and language tasks.

## Method Summary
THREAD learns personalized decision support policies online using contextual bandits (LinUCB and KNN-UCB variants) to estimate human prediction errors under different support forms. The algorithm reduces a multi-objective optimization problem balancing performance and cost to a single-objective problem using a trade-off parameter λ. A novel hyper-parameter tuning strategy selects λ by simulating human behavior across a population and identifying values that meet performance thresholds with minimal cost. The method is validated through both computational experiments on CIFAR-10 and MMLU datasets and human subject experiments with 26 participants.

## Key Results
- THREAD learns personalized policies that outperform offline population-level policies
- Human subject experiments validate THREAD adapts to individual decision-makers' strengths on both vision and language tasks
- The hyper-parameter tuning strategy effectively identifies appropriate cost-performance trade-offs

## Why This Works (Mechanism)

### Mechanism 1
THREAD learns personalized decision support policies by estimating human prediction errors under different support forms and selecting the support that minimizes a weighted combination of error and cost. At each time step, THREAD estimates the prediction error of the decision-maker under each form of support using contextual bandit techniques (LinUCB or KNN-UCB). It then selects support based on a trade-off parameter λ that balances accuracy and cost. Over time, the policy adapts to the individual's strengths and weaknesses.

### Mechanism 2
The hyper-parameter tuning strategy selects an appropriate trade-off parameter λ by simulating the decision-maker's behavior across a population and finding the λ that meets a specified performance threshold with minimal cost. For each simulated decision-maker, THREAD is run across a range of λ values. The strategy identifies which λ values yield policies within ε of the optimal loss, then selects the most common λ across simulators (or the one with lowest cost). This λ is then deployed for the real decision-maker.

### Mechanism 3
THREAD's online learning approach outperforms offline population-level policies because it can personalize to individual decision-makers who may have different strengths and weaknesses than the population average. While offline policies learn from aggregated data of many decision-makers, THREAD learns directly from the new individual's responses. This allows it to discover that, for example, one person may benefit from LLM support on biology questions while another excels without support on the same topic.

## Foundational Learning

- **Concept:** Stochastic contextual bandits
  - Why needed here: THREAD uses contextual bandit algorithms (LinUCB and KNN-UCB) to estimate human prediction errors and select appropriate support forms in an online learning setting.
  - Quick check question: In the contextual bandit framework, what are the "contexts," "actions," and "rewards" in the decision support problem?

- **Concept:** Multi-objective optimization and Pareto optimality
  - Why needed here: The problem formulation balances two objectives (performance and cost) that conflict, requiring techniques to find Pareto optimal solutions and convert the problem to a single-objective optimization.
  - Quick check question: How does THREAD convert the multi-objective optimization problem into a single-objective problem that can be solved with contextual bandits?

- **Concept:** Online learning vs. offline learning
  - Why needed here: THREAD learns policies online for new decision-makers without prior data, contrasting with approaches that require offline datasets of human decisions under support.
  - Quick check question: What are the key advantages and disadvantages of learning decision support policies online versus offline?

## Architecture Onboarding

- **Component map:** Interactive interface (Modiste) -> THREAD algorithm -> Human simulator -> Backend server
- **Critical path:**
  1. Human makes decision with/without support
  2. Decision and correctness feedback sent to backend
  3. THREAD updates error estimates for each support type
  4. Policy selects next support form based on current estimates
  5. Interface displays selected support for next input
- **Design tradeoffs:**
  - Exploration vs. exploitation in support selection (affects learning speed vs. immediate performance)
  - Number of support forms (more options provide better personalization but require more interactions to learn)
  - Complexity of human simulator (more realistic simulators help hyper-parameter tuning but increase computational cost)
- **Failure signatures:**
  - Policy consistently selects high-cost support when low-cost would suffice (over-exploration or poor error estimation)
  - Policy never selects certain support forms (under-exploration or incorrect cost weighting)
  - Performance degrades over time (non-stationary human behavior or feedback loops)
- **First 3 experiments:**
  1. Run THREAD with LinUCB on a synthetic dataset where human behavior is perfectly known, verify it learns the optimal policy
  2. Run THREAD with KNN on the same synthetic dataset, compare performance and parameter sensitivity
  3. Run both variants on a small real dataset (e.g., CIFAR subset) with simulated humans, verify hyper-parameter tuning strategy selects appropriate λ values

## Open Questions the Paper Calls Out

### Open Question 1
What are the specific trade-offs between different human simulators when selecting the optimal λ for THREAD? The paper discusses three strategies for selecting λ (most likely λ, most likely λ with lowest cost, and conservative λ) but does not evaluate which strategy performs best in practice.

### Open Question 2
How does the choice of exploration parameters (α for LinUCB and γ for KNN) impact the performance of THREAD in practice? The paper discusses the effect of varying exploration parameters in the computational experiments but does not provide definitive recommendations for real-world deployment.

### Open Question 3
How can THREAD be adapted to handle potential drifts in decision-maker behavior over time? The paper mentions that future work can integrate complex algorithms into THREAD to handle potential drifts, but does not propose specific solutions.

## Limitations

- Human subject experiments involve only 26 participants total, limiting statistical power and generalizability
- The paper does not provide formal regret bounds or convergence guarantees for the THREAD algorithm
- The simulator-based hyper-parameter tuning strategy assumes simulated decision-makers accurately represent real human behavior

## Confidence

**High Confidence**: The core mechanism of using contextual bandits to estimate human prediction errors and select appropriate support forms is well-established and technically sound.

**Medium Confidence**: The hyper-parameter tuning strategy shows promise but relies heavily on the quality of simulators. Computational experiments demonstrate improvements over baselines.

**Low Confidence**: Human subject experiment results are promising but based on a small sample size (26 participants total). Personalization benefits may not generalize to larger, more diverse populations.

## Next Checks

1. **Simulation Validation**: Run sensitivity analysis on the human simulator parameters to quantify how simulator accuracy affects the quality of selected λ values. Test THREAD with varying levels of simulator realism to establish minimum requirements.

2. **Sample Size Sensitivity**: Replicate the human subject experiments with different sample sizes (e.g., 10, 20, 40 participants) to determine the minimum number needed to detect personalization effects with statistical significance.

3. **Cross-task Transferability**: Test whether THREAD policies learned on one task domain (e.g., vision) transfer effectively to similar tasks, or if policies must be learned independently for each task type.