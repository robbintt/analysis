---
ver: rpa2
title: Examining LLMs' Uncertainty Expression Towards Questions Outside Parametric
  Knowledge
arxiv_id: '2311.09731'
source_url: https://arxiv.org/abs/2311.09731
tags:
- confidence
- answerable
- unanswerable
- refusal
- level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically examines large language models' behaviors
  when encountering questions outside their parametric knowledge. The authors create
  an adversarial benchmark, UnknownBench, consisting of questions with non-existent
  concepts or false premises to ensure they are unanswerable by design.
---

# Examining LLMs' Uncertainty Expression Towards Questions Outside Parametric Knowledge

## Quick Facts
- arXiv ID: 2311.09731
- Source URL: https://arxiv.org/abs/2311.09731
- Reference count: 40
- Key outcome: Most LLMs fail to consistently refuse or express uncertainty towards unanswerable questions, with significant performance gaps between open-source and proprietary models.

## Executive Summary
This paper systematically examines how large language models handle questions outside their parametric knowledge by creating UnknownBench, an adversarial benchmark with non-existent concepts and false premises. The authors evaluate 11 models using a unified confidence elicitation approach, finding that most models struggle to appropriately refuse or express uncertainty on unanswerable questions. The study reveals performance gaps between open-source and proprietary models, shows marginal improvements from instruction fine-tuning and RLHF, and demonstrates misalignment between verbalized confidence and actual response reliability.

## Method Summary
The authors construct UnknownBench containing three tasks: Non-Existent Concepts (NEC), FalseQA, and Refusal-inducing NaturalQuestions (RefuNQ). They employ a model-agnostic unified confidence elicitation approach asking models to rate confidence on a 0-10 scale. Evaluation uses lexical keyword matching for refusal detection and accuracy assessment, comparing performance across open-source and proprietary LLMs while analyzing correlations between confidence expressions, refusal rates, and accuracy scores.

## Key Results
- Most LLMs fail to consistently refuse or express uncertainty on unanswerable questions from UnknownBench
- Proprietary models outperform open-source models in both refusal rates and accuracy on unanswerable questions
- Instruction fine-tuning and RLHF provide marginal improvements in refusal ability while maintaining accuracy
- There is a significant misalignment between verbalized confidence and the perceived confidence of responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can verbally express uncertainty levels through structured confidence elicitation prompts
- Mechanism: Models are asked to assign numerical confidence scores (0-10) to questions, eliciting internal uncertainty estimates in a format compatible with both open-source and proprietary models
- Core assumption: Models have internal probability distributions over next tokens that can be approximated through elicitation
- Evidence anchors: Most models fail to consistently refuse or express uncertainty towards unanswerable questions despite structured elicitation prompts

### Mechanism 2
- Claim: Instruction fine-tuning and RLHF improve LLMs' ability to refuse unanswerable questions while maintaining accuracy
- Mechanism: Fine-tuning on human feedback data teaches models to recognize when questions fall outside their knowledge boundaries and respond appropriately
- Core assumption: Human feedback provides ground truth about when questions should be refused
- Evidence anchors: Llama-2 Chat models show 15-23% higher refusal rates than base models while maintaining similar accuracy levels

### Mechanism 3
- Claim: There exists a trade-off between refusal and helpfulness that can be quantified through UnknownBench
- Mechanism: By measuring both refusal rates on unanswerable questions and accuracy on answerable questions, we can evaluate how well models balance safety and utility
- Core assumption: Good models should show positive correlation between confidence and accuracy, and negative correlation between confidence and refusal for answerable questions
- Evidence anchors: Models exhibit negative correlation between confidence and refusal rate and positive correlation between confidence and accuracy

## Foundational Learning

- Concept: Calibration of confidence estimates
  - Why needed here: The paper evaluates whether verbalized confidence correlates with actual accuracy and appropriate refusal behavior
  - Quick check question: If a model expresses confidence level 8 on a question, what should its accuracy be on similar answerable questions?

- Concept: Knowledge boundary identification
  - Why needed here: The benchmark specifically tests whether models can recognize when questions contain non-existent concepts or false premises
  - Quick check question: How would you determine if a question contains information outside a model's training data?

- Concept: Prompt engineering for uncertainty elicitation
  - Why needed here: Different prompt formats (classification vs regression style) affect how well models express uncertainty
  - Quick check question: What's the difference between asking a model to pick from "very confident" to "very uncertain" versus giving a score between 0-100?

## Architecture Onboarding

- Component map: UnknownBench datasets (NEC, FalseQA, RefuNQ) → Model inference → Confidence elicitation → Evaluation metrics
- Critical path: Dataset construction → Model evaluation with confidence elicitation → Correlation analysis between confidence, refusal, and accuracy → Gap identification
- Design tradeoffs: Open-source vs proprietary model handling (logit access vs verbalized only), classification vs regression confidence elicitation formats, lexical matching vs semantic matching for refusal detection
- Failure signatures: Low refusal rates on clearly unanswerable questions, mismatch between verbalized confidence and actual accuracy, scale-dependent calibration issues (base vs chat models)
- First 3 experiments: 1) Test baseline refusal rates on UnknownBench without any confidence elicitation, 2) Implement and compare classification vs regression confidence elicitation prompts, 3) Analyze correlation between confidence scores and accuracy across different model types (base vs chat)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific training techniques or architectural modifications could improve LLMs' ability to express uncertainty without explicit instruction?
- Basis in paper: The authors observe that LLMs rarely express uncertainty unless explicitly prompted to do so, despite potentially having internal uncertainty estimates
- Why unresolved: Current approaches rely on prompting for uncertainty expression, but this doesn't guarantee genuine understanding or consistent behavior across different contexts and model sizes
- What evidence would resolve it: Comparative studies testing various training methods against baseline models on the UnknownBench benchmark

### Open Question 2
- Question: How do different uncertainty elicitation methods (verbalized vs. logit-based) correlate with actual model knowledge and reasoning capabilities?
- Basis in paper: The authors find that base models show better performance with logit-based methods while chat models respond better to verbalized confidence elicitation
- Why unresolved: The relationship between these different uncertainty measurement approaches and the underlying model knowledge is not fully understood
- What evidence would resolve it: Systematic comparison of multiple uncertainty elicitation methods across various model architectures and sizes

### Open Question 3
- Question: What is the optimal balance between refusal rate and accuracy that maximizes both safety and usefulness in practical applications?
- Basis in paper: The authors identify a trade-off between safety (refusal) and helpfulness (accuracy), noting that even the best models struggle to maintain high performance in both dimensions simultaneously
- Why unresolved: The current evaluation framework doesn't provide clear guidance on how to balance these competing objectives in real-world applications
- What evidence would resolve it: Multi-dimensional evaluation of model performance across different task domains measuring the relationship between refusal rate, accuracy, and task completion success

## Limitations

- Evaluation relies on lexical matching which may miss semantically correct responses or nuanced refusal behaviors
- Benchmark construction depends on artificial non-existent concepts that may not generalize to all types of out-of-knowledge questions
- Confidence elicitation method's effectiveness across different model architectures and training paradigms remains untested beyond evaluated models

## Confidence

- **High confidence**: Proprietary models generally outperform open-source models in refusal and accuracy on unanswerable questions
- **Medium confidence**: Instruction fine-tuning and RLHF provide marginal improvements in refusal ability
- **Medium confidence**: Verbalized confidence doesn't always align with response confidence

## Next Checks

1. Conduct human evaluation on a subset of model responses to validate the lexical matching approach for refusal detection and accuracy assessment
2. Test the confidence elicitation method across additional model architectures (both larger and smaller parameter counts) to assess scalability
3. Investigate whether performance gaps between open-source and proprietary models persist when using semantic matching techniques rather than lexical matching for evaluation