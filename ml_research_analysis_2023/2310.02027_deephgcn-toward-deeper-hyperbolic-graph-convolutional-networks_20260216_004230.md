---
ver: rpa2
title: 'DeepHGCN: Toward Deeper Hyperbolic Graph Convolutional Networks'
arxiv_id: '2310.02027'
source_url: https://arxiv.org/abs/2310.02027
tags:
- hyperbolic
- deephgcn
- hgcn
- layer
- poincar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of developing deep hyperbolic
  graph convolutional networks (HGCNs), which have shown promise in extracting information
  from hierarchical graphs but are limited to shallow architectures due to computational
  complexity and over-smoothing issues. The authors propose DeepHGCN, the first deep
  multi-layer HGCN architecture, featuring a novel hyperbolic feature transformation
  layer for fast and accurate linear mappings, and techniques such as hyperbolic residual
  connections and regularization for weights and features.
---

# DeepHGCN: Toward Deeper Hyperbolic Graph Convolutional Networks

## Quick Facts
- arXiv ID: 2310.02027
- Source URL: https://arxiv.org/abs/2310.02027
- Reference count: 40
- Key outcome: DeepHGCN achieves state-of-the-art performance on link prediction and node classification tasks compared to both Euclidean and shallow hyperbolic GCN variants.

## Executive Summary
This paper addresses the challenge of developing deep hyperbolic graph convolutional networks (HGCNs), which have shown promise in extracting information from hierarchical graphs but are limited to shallow architectures due to computational complexity and over-smoothing issues. The authors propose DeepHGCN, the first deep multi-layer HGCN architecture, featuring a novel hyperbolic feature transformation layer for fast and accurate linear mappings, and techniques such as hyperbolic residual connections and regularization for weights and features. Extensive experiments demonstrate that DeepHGCN significantly improves link prediction and node classification tasks compared to both Euclidean and shallow hyperbolic GCN variants.

## Method Summary
DeepHGCN introduces a novel hyperbolic feature transformation layer that enables fast and accurate linear mappings in the Poincaré ball model, avoiding expensive stereographic projections between Poincaré and Lorentz models. The architecture incorporates hyperbolic residual connections to prevent over-smoothing and uses regularization techniques to push node representations toward the boundary of the Poincaré ball, improving classification. The model operates by mapping node features to hyperbolic space via the exponential map, applying multiple layers of neighborhood aggregation using Möbius gyromidpoint, and decoding to the desired output space.

## Key Results
- DeepHGCN achieves 2-3% absolute improvement in ROC AUC for link prediction on Airport and PubMed datasets compared to shallow HGCN variants
- Node classification F1 scores improve by 1-2% on Cora and CiteSeer datasets when using DeepHGCN versus Euclidean GCNs
- The novel hyperbolic feature transformation layer reduces computation time by approximately 2x compared to naive HNN layers while maintaining accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The novel hyperbolic feature transformation layer dramatically improves computational efficiency compared to naive HNN and PFC layers.
- Mechanism: By reformulating the transformation using Euclidean weight parameters directly within the Poincaré ball via the Möbius gyromidpoint framework, the method bypasses expensive stereographic projections between Poincaré and Lorentz models. The efficient formulation in Eq. (1-2) avoids repeated re-normalization steps required in traditional approaches.
- Core assumption: The Möbius gyromidpoint provides accurate enough midpoint approximation while being computationally cheaper than Fréchet mean.
- Evidence anchors:
  - [abstract]: "a novel hyperbolic feature transformation layer that enables fast and accurate linear mappings"
  - [section]: "The naive HNN [14] and PFC layer [27]" are compared against our approach in Tab. I showing "approximately 2 times of the computation time of Euclidean linear layer, while other approaches require 3 times and more"
  - [corpus]: No direct evidence in corpus - this is novel contribution
- Break condition: If the midpoint approximation becomes too inaccurate for high-dimensional or highly curved manifolds, the transformation could lose representational power.

### Mechanism 2
- Claim: Hyperbolic residual connections prevent over-smoothing by preserving energy through deep layers.
- Mechanism: The hyperbolic residual connection (Eq. 17) maintains the hyperbolic manifold constraint while allowing information from initial layers to flow through deep architectures. The initial residual specifically preserves high Dirichlet energy from early layers.
- Core assumption: The Dirichlet energy metric accurately captures the smoothness/over-smoothing phenomenon in hyperbolic space.
- Evidence anchors:
  - [abstract]: "techniques such as hyperbolic residual connections and regularization for both weights and features"
  - [section]: "We define the hyperbolic graph residual connection f κ HR... This operation ensures the feature after residual connection still lies in the Poincaré ball"
  - [corpus]: Weak - corpus doesn't mention this specific mechanism, though it does reference over-smoothing issues in general GCN work
- Break condition: If the residual coefficient αl is poorly tuned, the residual connection could either vanish (no effect) or dominate (prevent learning new features).

### Mechanism 3
- Claim: Hyperbolic feature regularization pushes node representations toward the boundary of the Poincaré ball, improving classification.
- Mechanism: By computing the root node via gyromidpoint and regularizing embeddings to move away from it (Eq. 23-24), the method increases inter-class distances and prevents embeddings from collapsing toward the center.
- Core assumption: Nodes near the boundary of the Poincaré ball are empirically easier to classify, as referenced from [38] and [39].
- Evidence anchors:
  - [abstract]: "regularization for both weights and features"
  - [section]: "To push the nodes to the boundary, [39] proposed a regularization... We employ a similar approach"
  - [corpus]: No direct evidence - this appears to be novel regularization technique
- Break condition: If the regularization strength γ is too high, embeddings could be pushed too far toward the boundary, potentially causing numerical instability or loss of local structure.

## Foundational Learning

- Concept: Hyperbolic geometry and Riemannian manifolds
  - Why needed here: The entire model operates in hyperbolic space, requiring understanding of geodesics, exponential/logarithmic maps, and the Poincaré ball model
  - Quick check question: What is the key difference between Euclidean distance and hyperbolic distance in the Poincaré ball?

- Concept: Graph neural networks and over-smoothing
  - Why needed here: The work builds on GCN architecture while addressing the specific over-smoothing problem that prevents deep GCNs
  - Quick check question: How does the Dirichlet energy relate to over-smoothing in GCNs?

- Concept: Möbius gyrovector space operations
  - Why needed here: All hyperbolic operations in DeepHGCN use Möbius addition, subtraction, and scalar multiplication for manifold-preserving computations
  - Quick check question: Why can't we simply use Euclidean vector addition in hyperbolic space?

## Architecture Onboarding

- Component map: Input → Exponential map → [Aggregation → Feature transform → Weight alignment → Residual] × L layers → Regularization → Output
- Critical path: Input → Exponential map → [Aggregation → Feature transform → Weight alignment → Residual] × L layers → Regularization → Output
- Design tradeoffs:
  - Depth vs. over-smoothing: More layers capture higher-order information but risk over-smoothing
  - Computational efficiency vs. accuracy: The novel transformation layer trades some approximation accuracy for significant speed gains
  - Regularization strength: Too weak allows over-smoothing; too strong causes instability
- Failure signatures:
  - NaN values in training: Likely numerical instability in hyperbolic operations (check clipping implementation)
  - Performance degradation with depth: Over-smoothing not adequately addressed (check residual and regularization)
  - Poor initial convergence: Learning rate too high for hyperbolic geometry
- First 3 experiments:
  1. Verify the feature transformation layer computes correctly by comparing outputs with naive HNN on a simple 2D dataset
  2. Test the gyromidpoint aggregation against tangential aggregation on a small graph to confirm accuracy
  3. Validate the residual connection preserves manifold constraints by checking embedding norms after each layer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical limits of depth for hyperbolic graph convolutional networks, and how do these compare to the limits of Euclidean GCNs?
- Basis in paper: [explicit] The paper mentions that DeepHGCN achieves improved performance with increased depth, while traditional HGCNs degrade after a certain depth due to over-smoothing. The authors also compare DeepHGCN to GCNII, a deep Euclidean GCN variant.
- Why unresolved: While the paper demonstrates that DeepHGCN can achieve good performance with increased depth, it does not provide a definitive answer on the theoretical limits of depth for HGCNs. The comparison to GCNII suggests that there might be differences in the limits between hyperbolic and Euclidean GCNs, but further research is needed to establish these limits.
- What evidence would resolve it: A comprehensive study comparing the performance of DeepHGCN and GCNII (or other deep Euclidean GCN variants) with increasing depth on various graph datasets. The study should also include theoretical analysis to determine the factors that influence the depth limits for both hyperbolic and Euclidean GCNs.

### Open Question 2
- Question: How does the choice of hyperbolic model (e.g., Poincaré ball vs. Lorentz model) affect the performance of deep hyperbolic graph convolutional networks?
- Basis in paper: [explicit] The paper focuses on the Poincaré ball model for hyperbolic operations but mentions that the Lorentz model can also be used. The authors do not provide a direct comparison between the two models.
- Why unresolved: While the paper demonstrates the effectiveness of the Poincaré ball model in DeepHGCN, it does not explore the potential benefits or drawbacks of using the Lorentz model. A direct comparison between the two models could provide insights into the optimal choice for different graph datasets or tasks.
- What evidence would resolve it: A systematic comparison of DeepHGCN using both the Poincaré ball and Lorentz models on various graph datasets. The comparison should include both node classification and link prediction tasks to determine if one model consistently outperforms the other or if the choice depends on the specific dataset or task.

### Open Question 3
- Question: Can the techniques developed for DeepHGCN be generalized to other types of geometric graph neural networks, such as those operating on spherical or product manifolds?
- Basis in paper: [inferred] The paper focuses on hyperbolic graph convolutional networks but mentions the potential for generalizing the model to Riemannian product manifolds and semi-Riemannian manifolds. The techniques developed for DeepHGCN, such as the efficient feature transformation and over-smoothing mitigation strategies, could potentially be adapted for other geometric graph neural networks.
- Why unresolved: While the paper suggests the possibility of generalizing DeepHGCN techniques to other geometric graph neural networks, it does not provide a concrete framework or demonstrate the effectiveness of such generalizations. Further research is needed to explore the applicability and limitations of these techniques in different geometric settings.
- What evidence would resolve it: A study that adapts the techniques from DeepHGCN (e.g., efficient feature transformation, over-smoothing mitigation) to graph neural networks operating on spherical or product manifolds. The study should evaluate the performance of these adapted techniques on various graph datasets and compare them to existing methods for geometric graph neural networks.

## Limitations
- The numerical stability of the hyperbolic feature transformation layer when applied to very deep architectures remains uncertain due to Poincaré ball constraints
- The comparison methodology for computational speedup is not fully detailed in the paper
- Empirical validation of over-smoothing mitigation is limited to specific datasets and tasks

## Confidence

- **High**: Novel hyperbolic feature transformation layer improves computational efficiency
- **Medium**: Hyperbolic residual connections effectively prevent over-smoothing in deep architectures
- **Medium**: Hyperbolic feature regularization improves node classification performance

## Next Checks

1. **Numerical stability validation**: Implement gradient checking and monitor embedding norms throughout training to verify that the hyperbolic operations remain numerically stable, particularly for very deep networks (10+ layers).

2. **Ablation on residual strength**: Systematically vary the residual coefficient α across multiple orders of magnitude to identify the optimal range and confirm that the residual connections are providing meaningful gradient flow rather than just scaling the input.

3. **Generalization across datasets**: Test DeepHGCN on additional hierarchical graph datasets beyond the standard benchmarks to verify that the architectural improvements generalize beyond the specific data distributions used in the evaluation.