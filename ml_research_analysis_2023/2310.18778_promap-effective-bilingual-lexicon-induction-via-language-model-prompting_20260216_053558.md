---
ver: rpa2
title: 'ProMap: Effective Bilingual Lexicon Induction via Language Model Prompting'
arxiv_id: '2310.18778'
source_url: https://arxiv.org/abs/2310.18778
tags:
- word
- language
- promap
- translation
- arabic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ProMap, a novel approach for bilingual lexicon
  induction (BLI) that leverages prompting pretrained multilingual and multidialectal
  language models. ProMap uses padded prompting with a seed dictionary to overcome
  subword tokenization challenges and achieve strong performance even with minimal
  training data.
---

# ProMap: Effective Bilingual Lexicon Induction via Language Model Prompting

## Quick Facts
- **arXiv ID:** 2310.18778
- **Source URL:** https://arxiv.org/abs/2310.18778
- **Reference count:** 40
- **Primary result:** Introduces ProMap, a prompt-based approach for bilingual lexicon induction that achieves state-of-the-art performance on both rich-resource and low-resource language pairs, including Arabic dialects, with strong few-shot capabilities.

## Executive Summary
This paper introduces ProMap, a novel approach for bilingual lexicon induction (BLI) that leverages prompting pretrained multilingual and multidialectal language models. ProMap uses padded prompting with a seed dictionary to overcome subword tokenization challenges and achieve strong performance even with minimal training data. The method consists of two variants: ProMapG for direct generation of translations and ProMapS for re-ranking candidates from static word embedding alignments. Extensive experiments demonstrate that ProMap consistently achieves state-of-the-art results on both rich-resource and low-resource language pairs, including Arabic dialects. Notably, ProMapG enables strong performance in few-shot scenarios with less than 10 training examples, making it valuable for low-resource language translation.

## Method Summary
ProMap is a prompt-based method for bilingual lexicon induction that finetunes multilingual pretrained language models (mPLMs) using padded prompting. The approach converts source and target words into fixed-length spans of sub-tokens padded with [PAD], then uses non-autoregressive MLM to predict these sub-tokens simultaneously. ProMap has two variants: ProMapG directly generates translations through finetuned mPLMs, while ProMapS re-ranks candidates from static word embedding alignments using the finetuned model. The method addresses the subword tokenization challenges in mPLMs by predicting multiple sub-tokens at once, enabling effective cross-lingual transfer with minimal supervision.

## Key Results
- ProMapG achieves state-of-the-art P@1 scores on 15 language pairs from the BLI benchmark, outperforming previous methods like RCSLS, VecMap, and FIPP
- ProMapG demonstrates strong few-shot performance, achieving P@1 scores of 20.67 with only 1 training example and 66.03 with 10 examples for EN-FR
- ProMapS variant effectively re-ranks static word embedding alignment candidates, improving upon baseline CLC1 method across all tested language pairs
- On Arabic dialect tasks, ProMapG achieves P@1 scores ranging from 64.07 to 91.81 across different dialect pairs, significantly outperforming existing approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Padded prompting enables the model to predict translation sub-tokens that form a whole word, overcoming subword tokenization challenges in multilingual PLMs.
- Mechanism: The method converts both source and target words into fixed-length spans of sub-tokens padded with [PAD], then uses non-autoregressive MLM to predict these sub-tokens simultaneously.
- Core assumption: The PLM can learn to predict the correct sequence of sub-tokens that form the translation when given padded prompts and sufficient training examples.
- Evidence anchors:
  - [abstract]: "To overcome the employment of subword tokens in these models, ProMap relies on an effective padded prompting of language models with a seed dictionary"
  - [section 3.1]: "To tackle this issue, we adapt our method to non-autoregressively predict multiple sub-tokens using padded MLM"
  - [corpus]: Weak evidence for multilingual generalization; only Arabic dialect experiments show clear sub-token prediction examples.
- Break condition: If the PLM vocabulary coverage is too low or the morphological complexity is too high, the model may fail to predict correct sub-token sequences.

### Mechanism 2
- Claim: Prompt-based finetuning of mPLMs enables strong few-shot performance in bilingual lexicon induction.
- Mechanism: The model learns to map source words to target words through a natural language template, requiring only a small number of training pairs to achieve good translation accuracy.
- Core assumption: mPLMs have sufficient cross-lingual knowledge to learn translation mappings with minimal supervision.
- Evidence anchors:
  - [abstract]: "Furthermore, ProMap enables strong performance in few-shot scenarios (even with less than 10 training examples)"
  - [section 4.5.2]: Results show ProMapG achieving P@1 scores of 20.67 with only 1 training example for EN-FR and 66.03 with 10 examples
  - [corpus]: Mixed evidence; strong results on rich-resource pairs but limited coverage of truly low-resource languages in experiments.
- Break condition: If the language pair is too distant or the mPLM lacks sufficient cross-lingual pre-training, few-shot performance degrades significantly.

### Mechanism 3
- Claim: ProMapS variant re-ranks static word embedding alignment candidates using contextual information from mPLMs.
- Mechanism: Uses the finetuned ProMap model to assign probability weights to static alignment candidates based on both cosine similarity and cross-entropy loss from the mPLM.
- Core assumption: The mPLM can distinguish correct translations from static alignment candidates better than cosine similarity alone.
- Evidence anchors:
  - [abstract]: "We also demonstrate the effectiveness of ProMap in re-ranking results from other BLI methods such as with aligned static word embeddings"
  - [section 3.3.2]: Details the re-ranking process combining softmax probabilities from cosine similarity with cross-entropy losses from ProMap
  - [corpus]: Evidence limited to comparison with CLC1 baseline; no ablation study showing impact of re-ranking alone.
- Break condition: If static alignment candidates are too noisy or the mPLM finetuning is insufficient, re-ranking may not improve over static methods.

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: MLM is the core pretraining objective used by BERT-like models, and ProMap repurposes it for bilingual translation prediction
  - Quick check question: Can you explain how MLM works and why it's useful for cross-lingual tasks?

- Concept: Subword tokenization and BPE
  - Why needed here: Understanding how multilingual models tokenize words into subwords is critical for grasping why padded prompting is necessary
  - Quick check question: What happens when a word is split into multiple subword tokens, and how does this affect translation tasks?

- Concept: Cross-lingual embedding alignment
  - Why needed here: ProMapS builds on existing static alignment methods, so understanding how word embeddings are mapped between languages is important
  - Quick check question: How do traditional static word embedding alignment methods work, and what are their limitations?

## Architecture Onboarding

- Component map: Input padded prompt templates -> Finetuned mPLM with MLM head -> Sub-token predictions (ProMapG) or candidate re-ranking scores (ProMapS) -> Supporting static word embedding alignment module (for ProMapS)

- Critical path:
  1. Prepare padded prompt templates from training dictionary
  2. Finetune mPLM using cross-entropy loss on sub-token predictions
  3. For ProMapG: Generate translation by decoding sub-token predictions
  4. For ProMapS: Extract static alignment candidates, compute re-ranking scores, select best candidate

- Design tradeoffs:
  - Fixed sub-token length vs. variable length: Fixed length simplifies implementation but may waste capacity on short words
  - Direct generation vs. re-ranking: ProMapG works without static embeddings but struggles with multi-subtoken words; ProMapS requires static embeddings but handles complex words better
  - Prompt template language: Source/target language templates work best but require mPLM to understand both languages

- Failure signatures:
  - ProMapG: Generates incorrect sub-token sequences, especially for morphologically complex words
  - ProMapS: Re-ranking doesn't improve over static methods, indicating poor finetuning or noisy candidates
  - Both: Poor performance on distantly related languages or when mPLM vocabulary coverage is low

- First 3 experiments:
  1. Test ProMapG on a simple language pair (e.g., EN-FR) with 1-5 training examples to verify few-shot capability
  2. Compare ProMapG vs. ProMapS on a language pair with available static embeddings to understand the generation vs. re-ranking tradeoff
  3. Test ProMap on a dialectal Arabic pair to verify low-resource performance and sub-token generation capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal vocabulary size and number of languages for mPLMs to achieve the best BLI performance without suffering from the curse of multilinguality?
- Basis in paper: [inferred] The paper mentions that not all languages in the dataset are covered by XLM-17 and suggests experimenting with language models with larger vocabularies and fewer languages as a way to alleviate challenges compounded by the curse of multilinguality.
- Why unresolved: The paper does not conduct experiments with different mPLM configurations to determine the optimal balance between vocabulary size and number of languages.
- What evidence would resolve it: Systematic experiments comparing BLI performance using mPLMs with different vocabulary sizes and numbers of languages, identifying the configuration that maximizes performance while minimizing multilinguality issues.

### Open Question 2
- Question: How can ProMapG be improved to handle multi-subtoken word generation more effectively, particularly for morphologically rich languages like Arabic?
- Basis in paper: [explicit] The paper explicitly states that ProMapG struggles with generating words of multiple sub-tokens, especially when n > 1, and discusses two primary reasons: the complexity of multi-label classification and the morphological richness of certain languages.
- Why unresolved: The paper only discusses the challenges and potential solutions (like expanding mPLM vocabulary) but does not implement or test these solutions to demonstrate improved performance.
- What evidence would resolve it: Implementation and testing of proposed solutions (e.g., vocabulary expansion, improved sub-token generation techniques) showing significant improvements in multi-subtoken word generation accuracy.

### Open Question 3
- Question: What are the long-term stability and generalizability results of ProMap across different domains and time periods?
- Basis in paper: [inferred] The paper focuses on BLI performance across different language pairs and resource scenarios but does not address domain adaptation or temporal stability of the learned mappings.
- Why unresolved: The evaluation is limited to specific benchmark datasets and does not explore how ProMap performs when applied to different domains or when tested over extended time periods with evolving language use.
- What evidence would resolve it: Longitudinal studies tracking ProMap performance across multiple domains and time periods, demonstrating consistent accuracy and identifying any domain-specific or temporal degradation patterns.

## Limitations

- Vocabulary coverage constraints: The method is limited by the vocabulary size of underlying language models (200K for XLM-17, 100K for MARBERT), potentially excluding low-resource languages outside these coverage bounds.
- Static alignment dependency: The ProMapS variant's effectiveness depends on the quality of static word embedding alignment methods, creating a potential failure mode where ProMapS inherits weaknesses from the underlying alignment.
- Limited evaluation scope: The experimental validation is limited to specific language pairs and domains, without testing on languages with different morphological structures or from different language families.

## Confidence

**High Confidence**: The core mechanism of padded prompting for sub-token prediction is well-supported by ablation studies and clear empirical improvements over baselines.

**Medium Confidence**: The few-shot performance claims are supported by experimental results but lack analysis of sensitivity to training sample selection and random seeds.

**Low Confidence**: The re-ranking mechanism of ProMapS lacks comprehensive ablation studies to isolate the contribution of the mPLM re-ranking from the underlying static alignment method.

## Next Checks

**Validation Check 1**: Test ProMapG's few-shot performance stability by running the 1-example and 10-example experiments with 10 different random samplings of training data. Measure the variance in P@1 scores across runs to determine if the few-shot claims are robust or dependent on specific training examples.

**Validation Check 2**: Evaluate ProMap on a language pair outside the XLM-17 coverage (e.g., Swahili-English or Vietnamese-English) to test the vocabulary coverage limitations. This would reveal whether the strong performance on 17 languages generalizes to truly low-resource languages with limited mPLM support.

**Validation Check 3**: Conduct an ablation study comparing ProMapS against CLC1 with varying levels of alignment quality. Use different static alignment methods (RCSLS, VecMap) and measure whether ProMapS consistently improves over weaker alignments or only works well with high-quality static alignments. This would clarify the true contribution of the re-ranking mechanism.