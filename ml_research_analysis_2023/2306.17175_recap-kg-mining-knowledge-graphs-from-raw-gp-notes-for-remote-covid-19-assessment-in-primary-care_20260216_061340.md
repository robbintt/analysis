---
ver: rpa2
title: 'RECAP-KG: Mining Knowledge Graphs from Raw GP Notes for Remote COVID-19 Assessment
  in Primary Care'
arxiv_id: '2306.17175'
source_url: https://arxiv.org/abs/2306.17175
tags:
- knowledge
- graph
- patient
- sentence
- tree
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RECAP-KG presents a knowledge graph extraction framework for raw
  GP consultation notes, using sentence parse trees and support phrases to generate
  interpretable graphs from unstructured medical text. The method processes COVID-19
  patient data from UK CCAS, extracting symptoms, durations, and severities via constituency
  parsing and semantic node generation.
---

# RECAP-KG: Mining Knowledge Graphs from Raw GP Notes for Remote COVID-19 Assessment in Primary Care

## Quick Facts
- **arXiv ID**: 2306.17175
- **Source URL**: https://arxiv.org/abs/2306.17175
- **Reference count**: 20
- **Primary result**: 95.2% accuracy on yes/no questions and 88.2% on multilabel questions, outperforming BERT

## Executive Summary
RECAP-KG presents a knowledge graph extraction framework for raw GP consultation notes, using sentence parse trees and support phrases to generate interpretable graphs from unstructured medical text. The method processes COVID-19 patient data from UK CCAS, extracting symptoms, durations, and severities via constituency parsing and semantic node generation. Evaluation shows 95.2% accuracy on yes/no questions and 88.2% on multilabel questions, outperforming BERT. Graph semantics are preserved with 79% cosine similarity when reconstructing original sentences. The framework addresses the challenge of processing unstructured GP notes by combining expert-curated support phrases with tree augmentation and open information extraction, producing interpretable knowledge graphs suitable for clinical decision support systems.

## Method Summary
RECAP-KG employs a two-stage pipeline for knowledge graph construction from unstructured GP consultation notes. First, it preprocesses raw text through spellchecking, abbreviation expansion, and sentence splitting. Constituency parsing generates tree structures, which are then augmented with semantic nodes (patient, symptom entities, list nodes) and attachments using expert-curated support phrases from SNOMED CT and RECAP. The second stage extracts knowledge graphs using OpenIE-based relation extraction. The framework validates results through sentence reconstruction (measuring semantic preservation via cosine similarity and MSE) and question answering evaluation using Answer Set Programming for symbolic reasoning.

## Key Results
- 95.2% accuracy on yes/no questions and 88.2% on multilabel questions, outperforming BERT
- 79% cosine similarity achieved in sentence reconstruction, preserving graph semantics
- Mean squared error of 63% in sentence reconstruction metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge graph extraction from raw GP notes is feasible by combining constituency parsing with support phrases from SNOMED CT.
- Mechanism: The framework first preprocesses and splits GP notes into complete sentences, then uses constituency parsing to build tree structures. Semantic nodes are inserted for patient and symptom entities based on support phrases from SNOMED CT, and attachments (adjectives, adverbs, prepositional phrases) are processed separately to improve accuracy.
- Core assumption: Medical terminology and symptoms can be accurately identified using expert-curated support phrases, and sentence structure can be reliably parsed even when incomplete or abbreviated.
- Evidence anchors:
  - [abstract] "By relying on support phrases mined from the SNOMED ontology, as well as predefined supported facts from values used in the RECAP (REmote COVID-19 Assessment in Primary Care) patient risk prediction tool"
  - [section] "We define a complete sentence to be a sentence that contains at least a subject and verb. In order to complete a sentence, we first generate a constituency parse tree for every segment."
  - [corpus] Weak evidence - no direct citations found for SNOMED CT integration in the neighbor papers, suggesting this is a novel combination.
- Break condition: If support phrases are incomplete or miss emerging symptoms, the framework cannot capture new or rare clinical presentations.

### Mechanism 2
- Claim: Extracting interpretable knowledge graphs enables transparent clinical decision support, outperforming black-box models like BERT.
- Mechanism: The framework converts extracted graphs into Answer Set Programming (ASP) for symbolic reasoning. This allows exact matching against ground truth question-answer pairs and explains decisions by showing which facts support an answer.
- Core assumption: The symbolic representation preserves all necessary semantic relationships for accurate question answering and clinical reasoning.
- Evidence anchors:
  - [abstract] "Graph semantics are preserved with 79% cosine similarity when reconstructing original sentences"
  - [section] "We then evaluate the performance of the whole pipeline with respect to well-known NLP models, such as BERT, by facilitating transparent Question Answering tasks over the learned knowledge"
  - [corpus] No direct evidence - neighbor papers focus on other NLP tasks but do not validate against BERT or use ASP.
- Break condition: If the graph reconstruction loses critical contextual information, symbolic reasoning may produce incorrect or incomplete answers.

### Mechanism 3
- Claim: Sentence structure can be reliably reconstructed by translating knowledge graphs back into natural language.
- Mechanism: The decoder iterates through graph nodes and applies rules to regenerate sentences that mirror the original text. Cosine similarity and mean accuracy metrics are used to evaluate semantic preservation.
- Core assumption: The knowledge graph contains sufficient information to reconstruct the original sentence without significant loss of meaning.
- Evidence anchors:
  - [abstract] "Graph semantics are preserved with 79% cosine similarity when reconstructing original sentences"
  - [section] "we translate generated knowledge graphs into text and evaluate preserved semantics of the original passage"
  - [corpus] No direct evidence - neighbor papers do not discuss graph-to-text reconstruction or semantic similarity metrics.
- Break condition: If the graph omits certain tokens or relationships, the reconstructed sentence will be incomplete or misleading.

## Foundational Learning

- Concept: Constituency parsing and tree structures
  - Why needed here: The framework relies on constituency parse trees as the foundation for building internal tree representations before extracting knowledge graphs.
  - Quick check question: What is the difference between a constituency parse tree and a dependency parse tree, and why does RECAP-KG use the former?

- Concept: Support phrase mining and ontology integration
  - Why needed here: SNOMED CT and expert-curated support phrases are used to identify medical entities and symptoms in the unstructured text.
  - Quick check question: How does mining support phrases from SNOMED CT improve entity recognition compared to using general NLP models?

- Concept: Answer Set Programming (ASP) for symbolic reasoning
  - Why needed here: ASP is used to validate extracted knowledge graphs against ground truth question-answer pairs and provide interpretable results.
  - Quick check question: What is the advantage of using ASP over probabilistic models for question answering in clinical contexts?

## Architecture Onboarding

- Component map:
  - Pre-processing -> Constituency parsing -> Parse tree augmentation -> Knowledge graph construction -> Graph validation
- Critical path:
  1. Pre-process raw GP notes
  2. Generate constituency parse trees
  3. Augment trees with semantic nodes and attachments
  4. Extract knowledge graphs using OpenIE
  5. Validate graphs via sentence reconstruction and ASP reasoning
- Design tradeoffs:
  - Using constituency parsing instead of dependency parsing for better handling of incomplete sentences
  - Relying on expert-curated support phrases for accuracy vs. flexibility
  - Symbolic ASP reasoning for interpretability vs. scalability
- Failure signatures:
  - Low cosine similarity in sentence reconstruction → information loss in graph extraction
  - Unsatisfiable ASP programs → incorrect or missing facts in the graph
  - High variance in accuracy across question types → biased extraction for certain patterns
- First 3 experiments:
  1. Run sentence reconstruction on a small sample of GP notes and compare cosine similarity with the original text.
  2. Validate a subset of knowledge graphs against ground truth QA pairs using ASP and measure accuracy.
  3. Compare RECAP-KG performance against BERT on a balanced set of yes/no and multilabel questions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the sentence extension method be improved to handle complex GP notes that contain long, unstructured sentences without delimiters?
- Basis in paper: [explicit] The paper mentions that the current sentence extension method fails to handle GP notes with long, unstructured sentences without delimiters, resulting in incorrect internal tree representations.
- Why unresolved: The paper identifies the limitation but does not provide a specific solution or method for improving the sentence extension process to handle complex sentences.
- What evidence would resolve it: A detailed description of an improved sentence extension method that can accurately handle long, unstructured sentences without delimiters, along with experimental results demonstrating its effectiveness on complex GP notes.

### Open Question 2
- Question: How does the performance of RECAP-KG compare to domain-specific BERT models pre-trained on biomedical text for question answering tasks?
- Basis in paper: [inferred] The paper suggests that comparing RECAP-KG to domain-specific BERT models, such as BLUE BERT pre-trained on PubMed abstracts and clinical notes, would be of merit but does not provide such a comparison.
- Why unresolved: The paper only compares RECAP-KG to BERT pre-trained on general text, not domain-specific BERT models that are more suitable for biomedical text understanding.
- What evidence would resolve it: A comparative study evaluating the performance of RECAP-KG and domain-specific BERT models on the same set of question answering tasks using biomedical text, with detailed metrics and analysis of the results.

### Open Question 3
- Question: Can the knowledge graph extraction method be extended to handle other sentence structures commonly found in GP consultation notes, such as "Patient will...", "Patient's X is Y", etc.?
- Basis in paper: [explicit] The paper mentions that the current method is restricted to sentences of the form "The patient has..." and suggests that extending the supported sentence structures could address information loss due to sentences that do not fit this form not being processed.
- Why unresolved: The paper identifies the limitation but does not provide a specific approach for extending the method to handle other sentence structures commonly found in GP notes.
- What evidence would resolve it: A description of an extended knowledge graph extraction method that can handle various sentence structures commonly found in GP consultation notes, along with experimental results demonstrating its effectiveness on a diverse set of GP notes.

## Limitations

- The framework's performance depends heavily on the completeness and accuracy of expert-curated support phrases and SNOMED CT resources, potentially missing emerging symptoms or novel clinical patterns.
- 79% cosine similarity in sentence reconstruction indicates substantial information loss in 21% of cases, which could affect clinical reasoning accuracy.
- The method is currently restricted to handling specific sentence structures, potentially losing information from GP notes with varied sentence patterns.

## Confidence

- **High confidence**: Sentence reconstruction accuracy metrics and BERT comparison results are well-supported by the described methodology and validation approach.
- **Medium confidence**: The claim about interpretable clinical decision support through ASP reasoning is plausible but lacks direct validation evidence in the corpus.
- **Low confidence**: The assertion that this framework is superior to other NLP approaches for medical knowledge extraction is not substantiated by comparative studies with alternative methods.

## Next Checks

1. Test the framework on a held-out set of GP notes containing novel or emerging COVID-19 symptoms not present in the original training data.
2. Conduct a blind comparison between RECAP-KG and established medical NLP systems like MedCAT or cTAKES on the same clinical dataset.
3. Evaluate the clinical utility by having domain experts assess whether the extracted knowledge graphs would meaningfully impact patient risk assessment decisions.