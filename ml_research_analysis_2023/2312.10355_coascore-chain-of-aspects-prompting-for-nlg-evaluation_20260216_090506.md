---
ver: rpa2
title: 'CoAScore: Chain-of-Aspects Prompting for NLG Evaluation'
arxiv_id: '2312.10355'
source_url: https://arxiv.org/abs/2312.10355
tags:
- evaluation
- coascore
- aspects
- aspect
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CoAScore is a large language model (LLM)-based metric for multi-aspect
  evaluation of natural language generation (NLG) systems. It uses a chain-of-aspects
  prompting framework to generate a chain of relevant aspects for the target aspect,
  scores each aspect, and then leverages the knowledge of relevant aspects to improve
  evaluation.
---

# CoAScore: Chain-of-Aspects Prompting for NLG Evaluation

## Quick Facts
- arXiv ID: 2312.10355
- Source URL: https://arxiv.org/abs/2312.10355
- Reference count: 40
- Key outcome: CoAScore is a large language model (LLM)-based metric for multi-aspect evaluation of natural language generation (NLG) systems that shows improved correlation with human judgments compared to existing baselines.

## Executive Summary
CoAScore introduces a novel approach for evaluating natural language generation systems by leveraging large language models to perform multi-aspect evaluation through a chain-of-aspects prompting framework. Rather than evaluating each aspect independently, CoAScore generates a chain of relevant aspects for the target aspect, scores each aspect, and uses this knowledge to enhance evaluation accuracy. Experiments on five NLG tasks and nine aspects demonstrate that CoAScore correlates better with human judgments than existing unsupervised baselines, both for overall quality and individual aspects. The method addresses the limitation of current LLM-based evaluation approaches that ignore the rich correlations between different evaluation aspects.

## Method Summary
CoAScore employs a three-stage chain-of-aspects prompting framework to evaluate NLG systems. First, it generates a chain of relevant aspects related to the target aspect using an LLM. Second, it scores each generated aspect for the given hypothesis. Third, it leverages the knowledge of these relevant aspects (descriptions and scores) to enhance the evaluation of the target aspect. The approach is tested on five NLG evaluation datasets (TopicalChat, OpenMEVA, BAGEL, IWSLT14, SummEval) with human-annotated scores across various aspects like overall quality, relevance, coherence, and fluency. Performance is measured by correlation coefficients between automatic scores and human judgments using Pearson, Spearman, and Kendall-Tau metrics.

## Key Results
- CoAScore achieves higher correlation with human judgments than baseline metrics (BLEU, ROUGE, METEOR, BERTScore, BARTScore, LLMScore) across all tested NLG tasks and aspects
- The three-stage chain-of-aspects framework consistently improves performance compared to isolated evaluation of individual aspects
- CoAScore outperforms both LLMScore (independent aspect evaluation) and CoAScore average (simple averaging of aspect scores), demonstrating the effectiveness of leveraging relevant aspect knowledge
- Ablation studies validate the contribution of each component in the CoAScore framework

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** CoAScore improves evaluation accuracy by incorporating multi-aspect knowledge rather than evaluating each aspect independently.
- **Mechanism:** CoAScore generates a chain of relevant aspects for the target aspect, scores each aspect, and uses this knowledge to enhance evaluation. This leverages the correlation between aspects observed in human judgments.
- **Core assumption:** Different aspects of NLG quality are correlated, and this correlation can be captured and utilized by LLMs to improve evaluation.
- **Evidence anchors:**
  - [abstract]: "current work often employs the LLM to independently evaluate different aspects, which largely ignores the rich correlation between various aspects."
  - [section]: "CoAScore utilizes multi-aspect knowledge through a CoA (Chain-of-Aspects) prompting framework when assessing the quality of a certain aspect."
  - [corpus]: Weak evidence - no direct citations in corpus neighbors supporting this mechanism.
- **Break condition:** If the generated aspects are not relevant to the target aspect or if the correlation between aspects is not strong enough, the performance of CoAScore may degrade.

### Mechanism 2
- **Claim:** CoAScore improves correlation with human judgments by using LLM-generated relevant aspects instead of pre-defined ones.
- **Mechanism:** CoAScore employs LLMs to generate a chain of relevant aspects that are tailored to the specific evaluation task and target aspect. These generated aspects are more effective in guiding the evaluation process.
- **Core assumption:** LLM-generated aspects are more contextually relevant and task-specific than pre-defined aspects from evaluation datasets.
- **Evidence anchors:**
  - [section]: "We find that CoAScoreð‘–ð‘›ð‘¡ð‘’ð‘Ÿ exhibits lower correlation scores for several aspects compared to CoAScore, possibly due to a lack of consideration for interrelationships between different aspects during dataset construction."
  - [section]: "LLM-generated relevant aspects are more conducive to LLM itself in making evaluation judgments."
  - [corpus]: Weak evidence - no direct citations in corpus neighbors supporting this mechanism.
- **Break condition:** If the LLM fails to generate relevant aspects or if the generated aspects are not diverse enough, the performance of CoAScore may suffer.

### Mechanism 3
- **Claim:** CoAScore leverages the scores of relevant aspects to improve the evaluation of the target aspect, rather than simply averaging them.
- **Mechanism:** CoAScore uses the knowledge of relevant aspects (descriptions and scores) to guide the LLM in evaluating the target aspect. This allows the LLM to consider the context and relationships between aspects.
- **Core assumption:** The LLM can effectively utilize the knowledge of relevant aspects to enhance its evaluation capability for the target aspect.
- **Evidence anchors:**
  - [section]: "CoAScore consistently improves the correlation across all aspects... compared to isolated evaluations of individual aspects."
  - [section]: "CoAScore achieves higher correlation scores than CoAScoreð‘Žð‘£ð‘’ð‘Ÿð‘Žð‘”ð‘’, reinforcing the effectiveness of LLM incorporating various relevant aspect scores for assessments."
  - [corpus]: Weak evidence - no direct citations in corpus neighbors supporting this mechanism.
- **Break condition:** If the LLM fails to properly interpret or utilize the knowledge of relevant aspects, the performance of CoAScore may be negatively impacted.

## Foundational Learning

- **Concept:** Chain-of-thought prompting
  - **Why needed here:** CoAScore uses a chain-of-aspects prompting framework, which is an extension of chain-of-thought prompting. Understanding this concept is crucial for grasping how CoAScore generates and utilizes relevant aspects.
  - **Quick check question:** How does chain-of-thought prompting differ from traditional prompting in terms of the reasoning process?

- **Concept:** Multi-aspect evaluation
  - **Why needed here:** CoAScore is designed for multi-aspect evaluation of NLG systems. Understanding the concept of multi-aspect evaluation and its challenges is essential for appreciating the motivation behind CoAScore.
  - **Quick check question:** What are the limitations of evaluating NLG systems based on a single aspect?

- **Concept:** Correlation between evaluation aspects
  - **Why needed here:** CoAScore leverages the correlation between different evaluation aspects to improve its accuracy. Understanding the nature and extent of these correlations is important for evaluating the effectiveness of CoAScore.
  - **Quick check question:** How do human judgments typically correlate across different evaluation aspects in NLG tasks?

## Architecture Onboarding

- **Component map:** LLM generates relevant aspects -> Each aspect is scored -> Knowledge of relevant aspects is used to evaluate target aspect
- **Critical path:** The critical path involves generating relevant aspects, scoring them, and using this knowledge to evaluate the target aspect. The performance of CoAScore heavily depends on the effectiveness of each component in this path.
- **Design tradeoffs:** CoAScore trades off computational complexity for improved evaluation accuracy. Generating and scoring multiple relevant aspects requires more computation but can lead to better results.
- **Failure signatures:** If CoAScore fails to generate relevant aspects or if the generated aspects are not diverse enough, the performance may degrade. Additionally, if the LLM fails to properly utilize the knowledge of relevant aspects, the results may be suboptimal.
- **First 3 experiments:**
  1. Compare CoAScore with a baseline that evaluates aspects independently (LLMScore).
  2. Vary the number of relevant aspects generated and observe the impact on performance.
  3. Conduct ablation studies to assess the contribution of each component in the CoAScore framework.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CoAScore vary with different LLM models beyond ChatGPT 4?
- Basis in paper: [explicit] The paper states they use "ChatGPT 4, a widely recognized and extensively used large language model" with a specific configuration.
- Why unresolved: The study only experiments with one LLM model, leaving uncertainty about generalizability to other models or versions.
- What evidence would resolve it: Systematic experiments comparing CoAScore performance across multiple LLM models (e.g., GPT-3.5, Claude, LLaMA) and configurations.

### Open Question 2
- Question: What is the optimal number of relevant aspects to generate for different evaluation tasks and aspects?
- Basis in paper: [inferred] The paper experiments with 5, 10, and 20 relevant aspects but notes that performance can fluctuate within different intervals, suggesting task-specific optimization may be beneficial.
- Why unresolved: The paper does not provide a clear methodology for determining the optimal number of relevant aspects for a given task or aspect.
- What evidence would resolve it: A comprehensive study analyzing the relationship between the number of relevant aspects and evaluation performance across various tasks and aspects.

### Open Question 3
- Question: How does CoAScore perform in evaluating NLG systems for tasks not covered in the paper, such as code generation or mathematical reasoning?
- Basis in paper: [inferred] The paper evaluates CoAScore on five NLG tasks but does not cover specialized domains like code generation or mathematical reasoning.
- Why unresolved: The effectiveness of CoAScore in specialized domains remains unexplored, leaving uncertainty about its broader applicability.
- What evidence would resolve it: Experiments applying CoAScore to evaluate NLG systems in specialized domains, comparing performance with existing evaluation metrics.

## Limitations
- The exact prompts used in each stage of the CoAScore framework are not fully detailed, which may affect reproducibility
- The specific configurations and settings used for the LLM (e.g., temperature, number of generated aspects) are not explicitly mentioned, which could impact the results
- The effectiveness of CoAScore in specialized domains like code generation or mathematical reasoning remains unexplored

## Confidence

| Claim | Confidence |
|-------|------------|
| CoAScore demonstrates improved correlation with human judgments compared to baselines | High |
| The three-stage framework and its individual components contribute to performance | Medium |
| The specific mechanisms by which CoAScore achieves its improvements are not fully explained | Low |

## Next Checks
1. Conduct controlled experiments isolating the effect of aspect generation quality on final evaluation performance
2. Perform ablation studies systematically varying the number and diversity of generated aspects
3. Test CoAScore on datasets with different aspect correlation structures to assess generalizability