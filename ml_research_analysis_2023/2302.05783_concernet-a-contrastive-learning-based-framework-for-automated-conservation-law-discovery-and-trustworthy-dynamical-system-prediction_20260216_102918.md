---
ver: rpa2
title: 'ConCerNet: A Contrastive Learning Based Framework for Automated Conservation
  Law Discovery and Trustworthy Dynamical System Prediction'
arxiv_id: '2302.05783'
source_url: https://arxiv.org/abs/2302.05783
tags:
- conservation
- system
- learning
- function
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ConCerNet is a new learning framework that combines contrastive
  learning and neural projection to automatically discover conservation laws and build
  trustworthy dynamical system models. The method first uses a contrastive learning
  objective to extract the system invariants from trajectory data, then applies a
  projection layer to enforce the learned dynamics to preserve the discovered conservation
  quantities.
---

# ConCerNet: A Contrastive Learning Based Framework for Automated Conservation Law Discovery and Trustworthy Dynamical System Prediction

## Quick Facts
- **arXiv ID**: 2302.05783
- **Source URL**: https://arxiv.org/abs/2302.05783
- **Authors**: 
- **Reference count**: 40
- **Key outcome**: ConCerNet automatically discovers conservation laws and builds trustworthy dynamical system models using contrastive learning and neural projection, outperforming baselines in coordinate error and conservation violation metrics

## Executive Summary
ConCerNet is a novel learning framework that combines contrastive learning with neural projection to automatically discover conservation laws and build trustworthy dynamical system models. The method first uses a contrastive learning objective with Square Ratio Loss to extract system invariants from trajectory data, then applies a projection layer to enforce learned dynamics to preserve the discovered conservation quantities. This approach addresses the challenge of discovering conservation laws without prior knowledge while ensuring the learned dynamics respect physical constraints. Experimental results on simple systems (spring-mass, chemical kinetics, Kepler) and a larger heat equation problem demonstrate significant improvements over baseline neural networks in both coordinate error and conservation violation metrics.

## Method Summary
ConCerNet employs a two-step approach for automated conservation law discovery and trustworthy dynamical system prediction. First, a contrastive learning module with Square Ratio Loss learns a latent representation that captures the conservation laws from trajectory data by encouraging similar representations for states along the same trajectory while pushing apart representations from different trajectories. Second, a neural dynamics model is trained with a projection layer that enforces the learned dynamics to preserve the discovered conservation laws by projecting the predicted dynamics onto the invariant manifold defined by the conservation equations. This framework enables learning of physically consistent models without requiring prior knowledge of the conservation laws, while maintaining high predictive accuracy.

## Key Results
- ConCerNet significantly outperforms baseline neural networks in coordinate error and conservation violation metrics on tested systems
- The method demonstrates robustness to noisy observations while maintaining conservation law discovery capabilities
- Experiments show data-efficient learning of conservation laws, with performance improving as more trajectory data is provided
- Theoretical analysis provides conditions under which the contrastive learning component converges to an injective function of the true conservation law

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Contrastive learning with Square Ratio Loss can discover conservation laws without prior knowledge
- **Mechanism**: The SRL loss encourages latent representations of states along the same trajectory to be similar while pushing apart representations of states from different trajectories. This works because conservation laws remain constant along a trajectory but differ between trajectories.
- **Core assumption**: The system has conservation laws that remain constant along trajectories but vary between trajectories
- **Evidence anchors**:
  - [abstract]: "The contrastive learning component employs a Square Ratio Loss that encourages latent representations of states along the same trajectory to be similar while pushing apart representations of states from different trajectories"
  - [section]: "As a metric to persuade similar latent representation within the same trajectory and encourage discrepancy between different trajectories, we introduce the Square Ratio Loss (SRL)"
- **Break condition**: If the system has no conserved quantities, or if conserved quantities vary along trajectories, the contrastive learning approach will fail to discover meaningful invariants

### Mechanism 2
- **Claim**: The projection layer ensures learned dynamics preserve conservation laws
- **Mechanism**: The projection layer removes components of the neural network output that would violate conservation by projecting onto the invariant manifold defined by ∇xHθc(x)Tf = 0
- **Core assumption**: Once conservation laws are learned, they can be enforced through projection
- **Evidence anchors**:
  - [abstract]: "The method first uses a contrastive learning objective to extract the system invariants from trajectory data, then applies a projection layer to enforce the learned dynamics to preserve the discovered conservation quantities"
  - [section]: "We attempt to enforce the predicted trajectory along the conservation manifold in the simulation stage, s.t. dHθc(x)/dt = 0"
- **Break condition**: If the learned conservation laws are incorrect or incomplete, the projection will enforce wrong constraints

### Mechanism 3
- **Claim**: Theoretical guarantees exist for the contrastive learning component
- **Mechanism**: Theorem 1 proves that under certain conditions, minimizing the continuous square ratio loss leads the neural network to learn a function that is injective to the true conservation law
- **Core assumption**: The conservation law can be represented as a smooth function with certain properties
- **Evidence anchors**:
  - [abstract]: "Theoretical analysis shows that under certain conditions, minimizing this loss leads the neural network to learn a function that is injective to the true conservation law"
  - [section]: "Under mild conditions, we prove the local minimum property of Square Ratio Loss, theoretically bridging the relationship between the learned latent representation and original conservation function"
- **Break condition**: If the assumptions in Theorem 1 (like relative Lipschitz continuity) are violated, the theoretical guarantees don't hold

## Foundational Learning

- **Contrastive learning**: Why needed here: To discover conservation laws without prior knowledge by comparing states within vs between trajectories. Quick check question: Can you explain why states on the same trajectory should have similar representations in contrastive learning?
- **Neural ODEs**: Why needed here: To model continuous-time dynamical systems where the learned dynamics must preserve conservation laws. Quick check question: What is the relationship between neural ODEs and traditional neural networks for time series?
- **Symplectic geometry**: Why needed here: To understand how projection onto invariant manifolds preserves conservation properties. Quick check question: How does orthogonal projection onto a manifold preserve properties of the original space?

## Architecture Onboarding

- **Component map**: Contrastive learning module (Hθc) -> Dynamics learning module (fθd) -> Projection layer -> Prediction
- **Critical path**: Contrastive learning → Dynamics learning → Projection → Prediction
- **Design tradeoffs**:
  - More complex contrastive learning (larger Hθc) vs faster training
  - Exact conservation discovery vs approximate but practical
  - Separate modules vs end-to-end training
- **Failure signatures**:
  - Poor conservation metrics indicate contrastive learning failed
  - High coordinate error indicates dynamics learning issues
  - Poor performance on both suggests incorrect architecture
- **First 3 experiments**:
  1. Simple spring-mass system - verify basic functionality
  2. Chemical kinetics - test with different conservation law form
  3. Heat equation with autoencoder - test scalability to high dimensions

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the ConCerNet framework be extended to discover multiple conservation laws simultaneously, rather than just one at a time?
- **Basis in paper**: [explicit] The paper mentions that for systems with multiple conservation laws, it is difficult for the system to retrieve even one of the conservation equations, as any combination of individual conservation functions is also conserved. However, the paper does not provide a solution for discovering multiple conservation laws simultaneously.
- **Why unresolved**: The current framework only focuses on learning one conservation law at a time, and the theoretical analysis is limited to single conservation law systems. The paper acknowledges the challenge but does not propose a method to address it.
- **What evidence would resolve it**: Developing a modified contrastive learning approach or projection layer that can handle multiple conservation laws, and demonstrating its effectiveness on systems with known multiple conservation laws (e.g., Kepler system).

### Open Question 2
- **Question**: What is the theoretical relationship between the learned conservation function and the exact conservation law, beyond the observed linear relationship in experiments?
- **Basis in paper**: [explicit] The paper mentions that in practice, the learned function is likely to be a linear function of the exact conservation law, but this observation is not supported by theoretical analysis. The paper states that this is an open question and leaves it to future work.
- **Why unresolved**: While the paper provides a theoretical framework for understanding the relationship between the learned representation and the conservation law, it does not provide a theoretical explanation for the observed linear relationship between the learned and exact conservation functions.
- **What evidence would resolve it**: A theoretical proof or analysis that explains why the learned conservation function converges to a linear transformation of the exact conservation law, and under what conditions this relationship holds.

### Open Question 3
- **Question**: How does the performance of ConCerNet scale with the complexity of the dynamical system, particularly for high-dimensional systems or systems with complex dynamics?
- **Basis in paper**: [inferred] The paper demonstrates the effectiveness of ConCerNet on relatively simple systems (spring-mass, chemical kinetics, Kepler) and a moderately complex system (heat equation). However, it does not provide a systematic analysis of how the method performs on more complex systems or how its performance scales with system complexity.
- **Why unresolved**: The current experimental results are limited to a small set of relatively simple systems. There is no analysis of the method's performance on more complex systems or a theoretical understanding of how the method's performance might degrade with increasing system complexity.
- **What evidence would resolve it**: Conducting experiments on a range of increasingly complex dynamical systems, including high-dimensional systems and systems with chaotic or highly nonlinear dynamics, and analyzing the relationship between system complexity and ConCerNet's performance.

## Limitations
- Contrastive learning requires conservation laws to remain constant along trajectories but vary between trajectories, which may not hold for all physical systems
- Theoretical guarantees depend on specific mathematical assumptions (relative Lipschitz continuity) that may not always be satisfied in practice
- The method assumes conservation laws can be represented as smooth functions, potentially missing certain physical phenomena

## Confidence
- High confidence in the overall framework design and experimental methodology
- Medium confidence in theoretical guarantees due to strong assumptions required
- Medium confidence in scalability to high-dimensional systems (only tested on 1D heat equation)
- Low confidence in robustness to extreme noise or sparse trajectory data

## Next Checks
1. **Ablation Study**: Test ConCerNet with only contrastive learning (no projection) and only projection (no contrastive learning) to quantify the contribution of each component.

2. **Generalization Test**: Apply ConCerNet to a system with known but non-standard conservation laws (e.g., a Hamiltonian system with explicit time dependence) to test robustness beyond simple cases.

3. **Data Efficiency Analysis**: Systematically vary the amount of training trajectory data and measure how quickly ConCerNet discovers conservation laws compared to baselines, particularly for systems where conservation laws are non-trivial to discover.