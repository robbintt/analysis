---
ver: rpa2
title: Multilingual Mathematical Autoformalization
arxiv_id: '2311.03755'
source_url: https://arxiv.org/abs/2311.03755
tags:
- isabelle
- language
- formal
- fine-tuned
- lean4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of autoformalization, which is
  translating natural language mathematics into machine-verifiable formal statements.
  The key difficulty is the lack of a large parallel dataset of informal-formal pairs.
---

# Multilingual Mathematical Autoformalization

## Quick Facts
- arXiv ID: 2311.03755
- Source URL: https://arxiv.org/abs/2311.03755
- Authors: 
- Reference count: 17
- Key outcome: Fine-tuning LLaMA-33B on MMA dataset enables 16-18% of statements to require minimal corrections on benchmarks, up from 0% with base model

## Executive Summary
This paper addresses the challenge of autoformalization - translating natural language mathematics into machine-verifiable formal statements. The key innovation is using GPT-4 to perform back-translation from formal to informal language, creating a large multilingual dataset (MMA) without requiring manual parallel annotation. The authors demonstrate that fine-tuning LLaMA-33B on this dataset significantly improves autoformalization performance on miniF2F and ProofNet benchmarks compared to the base model, with multilingual training providing additional benefits over monolingual approaches.

## Method Summary
The authors create MMA, a large dataset of 332K informal-formal pairs, by using GPT-4 to translate formal mathematical statements from Isabelle and Lean4 libraries into natural language. They then fine-tune LLaMA-33B on this dataset using cross-entropy loss with three training regimes: joint Isabelle+Lean4, Isabelle-only, and Lean4-only. The models are evaluated on miniF2F and ProofNet benchmarks by manually assessing 50 randomly sampled outputs using a Likert scale for correction effort required. The approach leverages back-translation as an easier alternative to direct formalisation, arguing that informalisation tolerates more diversity and ambiguity.

## Key Results
- LLaMA-33B fine-tuned on MMA produces 16-18% of statements acceptable with minimal corrections (0-1 on Likert scale) on benchmarks
- Base model without fine-tuning achieves 0% acceptable statements
- Multilingual training shows higher validation loss initially but plateaus while monolingual training increases
- Compilation rates range from 17-23% on miniF2F and 15-27% on ProofNet

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Back-translation from formal to informal language is easier and more reliable than direct formalisation from natural language.
- **Mechanism:** GPT-4 can generate diverse, natural-sounding informalisations from formal statements, creating a parallel dataset without manual alignment.
- **Core assumption:** Informalisation requires less precision and tolerates more diversity than formalisation, making it more amenable to LLM generation.
- **Evidence anchors:**
  - [abstract] "We argue, both analytically and empirically, that informalisation is an easier task than formalisation."
  - [section] "By precision we mean that every piece of information must be explicitly and precisely expressed and formalised; whereas in natural language, pieces of information are often left implicit or ambiguous."
  - [corpus] Weak - no direct quantitative comparison in corpus, but supported by qualitative discussion.
- **Break condition:** If the LLM lacks sufficient understanding of formal language syntax or mathematical concepts, it may produce incorrect or nonsensical informalisations.

### Mechanism 2
- **Claim:** Fine-tuning on a multilingual, multi-domain dataset improves autoformalization performance compared to monolingual training.
- **Mechanism:** Exposure to multiple formal languages during training leads to better generalisation and transfer learning for formalisation tasks.
- **Core assumption:** Knowledge and patterns learned from one formal language can be applied to others, making the model more data-efficient.
- **Evidence anchors:**
  - [abstract] "Experiments show that language models fine-tuned on MMA produce 16 âˆ’ 18% of statements acceptable with minimal corrections on the miniF2F and ProofNet benchmarks, up from 0% with the base model."
  - [section] "Comparing different fine-tuning regimes, we find that for the first 20000 steps, joint fine-tuning has higher validation loss than fine-tuning on one formal language only. Afterwards, the monolingual fine-tuning validation loss starts to increase while the joint fine-tuning one starts to plateau."
  - [corpus] Weak - limited direct comparison of multilingual vs monolingual in corpus, but supported by experimental results.
- **Break condition:** If the formal languages are too dissimilar or the model capacity is insufficient, multilingual training may not provide benefits and could even harm performance.

### Mechanism 3
- **Claim:** The diversity and size of the MMA dataset enable better autoformalization performance than previous smaller, manually curated datasets.
- **Mechanism:** A large dataset covering diverse mathematical domains and formal languages provides richer training signals for the model to learn formalisation patterns.
- **Core assumption:** More data and domain coverage lead to better generalisation and performance on downstream tasks.
- **Evidence anchors:**
  - [abstract] "MMA, a large, flexible, multilingual, and multi-domain dataset of informal-formal pairs, by using a language model to translate in the reverse direction, that is, from formal mathematical statements into corresponding informal ones."
  - [section] "MMA differs from their dataset mainly in that MMA contains data from multiple formal languages and has four times as many datapoints."
  - [corpus] Weak - no direct quantitative analysis of dataset diversity in corpus, but supported by dataset statistics and comparison to previous work.
- **Break condition:** If the dataset contains too much noise or lacks sufficient coverage of relevant mathematical domains, the benefits may not materialise.

## Foundational Learning

- **Concept:** Back-translation in machine translation
  - Why needed here: Understanding the concept of back-translation and its application to autoformalization is crucial for grasping the core methodology.
  - Quick check question: What is the main advantage of using back-translation for creating parallel datasets in machine translation, and how is it applied in the context of autoformalization?
- **Concept:** Neural Machine Translation (NMT) and language models
  - Why needed here: Familiarity with NMT and language models is essential for understanding the technical details of the autoformalization approach.
  - Quick check question: How do neural machine translation methods typically work, and what role do language models play in the autoformalization process described in the paper?
- **Concept:** Formal languages and proof assistants
  - Why needed here: Understanding the characteristics of formal languages and the role of proof assistants is crucial for grasping the challenges and goals of autoformalization.
  - Quick check question: What are the key differences between formal languages used in proof assistants and natural languages, and why do these differences pose challenges for autoformalization?

## Architecture Onboarding

- **Component map:** GPT-4 for informalisation -> LLaMA-33B for fine-tuning -> MMA dataset -> miniF2F and ProofNet benchmarks
- **Critical path:** 1) Extract formal statements from Isabelle and Lean4 libraries 2) Use GPT-4 to generate informalisations 3) Create MMA dataset 4) Fine-tune LLaMA-33B on MMA 5) Evaluate on benchmarks
- **Design tradeoffs:** Using large GPT-4 for data generation vs smaller models, multilingual training vs monolingual for better generalisation, automated data generation vs manual curation
- **Failure signatures:** Low compilation rates, high correction effort required, overfitting to specific formal languages or domains
- **First 3 experiments:** 1) Fine-tune LLaMA-33B on MMA and evaluate on miniF2F 2) Compare multilingual vs monolingual training performance 3) Ablate dataset size and domain coverage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the MMA dataset compare to other existing datasets for autoformalization in terms of sample efficiency and final performance?
- Basis in paper: The paper states that MMA contains 4 times as many datapoints as the biggest existing dataset and demonstrates that fine-tuning on MMA results in more capable autoformalization models than fine-tuning on monolingual partitions of it.
- Why unresolved: The paper does not provide a direct comparison of MMA's performance with other datasets in terms of sample efficiency and final performance. Such a comparison would require conducting experiments with other datasets, which is not within the scope of this paper.
- What evidence would resolve it: Conducting experiments with other autoformalization datasets and comparing the performance of models fine-tuned on MMA with those fine-tuned on other datasets would provide evidence to resolve this question.

### Open Question 2
- Question: How does the quality of the informal statements generated by GPT-4 in MMA compare to human-generated informal statements in terms of diversity and flexibility of expression?
- Basis in paper: The paper mentions that the informal statements generated by GPT-4 are "not perfect" and should be treated as "noisy approximations" of the ground truth. It also states that informalisation is an easier task than formalisation, suggesting that the informal statements generated by GPT-4 may not fully capture the diversity and flexibility of human-generated informal statements.
- Why unresolved: The paper does not provide a direct comparison of the quality of the informal statements generated by GPT-4 with human-generated informal statements. Such a comparison would require evaluating the diversity and flexibility of expression in both sets of informal statements, which is not within the scope of this paper.
- What evidence would resolve it: Conducting a qualitative analysis of the informal statements generated by GPT-4 and comparing them with human-generated informal statements in terms of diversity and flexibility of expression would provide evidence to resolve this question.

### Open Question 3
- Question: How does the performance of the MMA dataset generalize to other formal languages beyond Isabelle and Lean4?
- Basis in paper: The paper demonstrates the effectiveness of MMA in improving autoformalization performance for Isabelle and Lean4. However, it does not explore the generalizability of MMA to other formal languages.
- Why unresolved: The paper focuses on evaluating MMA's performance on Isabelle and Lean4, and does not provide evidence of its effectiveness on other formal languages. Exploring the generalizability of MMA to other formal languages would require conducting experiments with those languages, which is not within the scope of this paper.
- What evidence would resolve it: Conducting experiments with other formal languages and evaluating the performance of models fine-tuned on MMA would provide evidence to resolve this question.

## Limitations
- Reliance on GPT-4 for back-translation introduces uncertainty about the quality and diversity of generated informalisations
- Manual expert assessment using Likert scale may introduce subjectivity in evaluation
- Focus on specific formal languages (Isabelle and Lean4) and mathematical domains limits generalizability
- Limited quantitative evidence directly comparing informalisation vs formalisation tasks

## Confidence
- High confidence: Effectiveness of fine-tuning LLaMA-33B on MMA dataset (0% to 16-18% acceptable statements)
- Medium confidence: Superiority of multilingual training over monolingual training
- Medium confidence: Claim that back-translation is easier than direct formalisation

## Next Checks
1. Conduct a controlled experiment directly comparing GPT-4's performance on formalisation versus informalisation tasks using the same formal statements, with quantitative metrics beyond manual Likert scales.

2. Perform ablation studies on the MMA dataset to determine the minimum dataset size and domain coverage required to achieve the reported performance improvements, testing whether the benefits scale linearly or show diminishing returns.

3. Evaluate the autoformalization models on additional formal languages beyond Isabelle and Lean4 (such as Coq or Metamath) to test the generalizability of the multilingual training approach across different proof assistant ecosystems.