---
ver: rpa2
title: Continual Instruction Tuning for Large Multimodal Models
arxiv_id: '2311.16206'
source_url: https://arxiv.org/abs/2311.16206
tags:
- task
- continual
- uni00000013
- tasks
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in continual instruction
  tuning of large multimodal models (LMMs). The authors establish the first benchmark
  for this setting and conduct extensive experiments to evaluate the effectiveness
  of existing continual learning methods.
---

# Continual Instruction Tuning for Large Multimodal Models

## Quick Facts
- arXiv ID: 2311.16206
- Source URL: https://arxiv.org/abs/2311.16206
- Reference count: 40
- Primary result: Multi-task joint instruction tuning reduces catastrophic forgetting; replay-based and model expansion methods outperform regularization-based approaches in continual instruction tuning of LMMs

## Executive Summary
This paper addresses catastrophic forgetting in continual instruction tuning of large multimodal models (LMMs). The authors establish the first benchmark for this setting and conduct extensive experiments to evaluate the effectiveness of existing continual learning methods. They find that multi-task joint instruction tuning mitigates forgetting, and that replay-based and model expansion strategies consistently outperform regularization-based methods. The authors further propose task-similarity-informed regularization and model expansion methods, which consistently boost the model's performance by exploiting task correlations.

## Method Summary
The paper tackles continual instruction tuning of LMMs by sequentially training models on new vision-language tasks while evaluating performance on all previously seen tasks. Experiments compare naive sequential fine-tuning (SeqFT) with continual learning methods: regularization-based (EWC, MAS, SI, C-LoRA), replay-based (AGem, ER), and model expansion (EProj). Task-similarity-informed variants (TIR, task-similarity-informed EProj) are also evaluated. The evaluation uses metrics like average performance (At) and forgetting (Ft) across benchmarks including Flickr30k, TextCaps, VQA v2, and others. Models start from either BLIP2 or InstructBLIP, with task similarity measured via embeddings of image, instruction, and output.

## Key Results
- Multi-task joint instruction tuning before continual phases reduces forgetting
- Replay-based and model expansion methods consistently outperform regularization-based methods
- Task-similarity-informed regularization and expansion methods provide consistent improvements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Catastrophic forgetting still occurs in continual instruction tuning of LMMs, but multi-task joint instruction tuning before continual phases reduces forgetting.
- **Mechanism:** Pre-training the model on multiple tasks via joint instruction tuning first enables the model to learn instruction-following behavior, which improves its ability to retain knowledge across subsequent sequential tasks.
- **Core assumption:** Instruction-following is a transferable skill that, once learned, improves continual learning performance.
- **Evidence anchors:**
  - [abstract] "We establish the first benchmark in this setting and reveal that catastrophic forgetting is still observed when continually instruction-tuning LMMs. However, the multi-task joint instruction tuning can facilitate the model's continual learning ability and mitigate forgetting."
  - [section] "Possible reasons are that multi-task joint instruction tuning helps the model learn to follow instructions and thus facilitates continual learning."
  - [corpus] Weak anchor: Similar conclusions appear in papers like SwitchCIT and Beyond Anti-Forgetting, which focus on mitigating forgetting in continual instruction tuning, but do not provide direct mechanistic proof.
- **Break condition:** If instruction-following is not the primary bottleneck, or if tasks are too dissimilar, joint tuning may not yield anti-forgetting benefits.

### Mechanism 2
- **Claim:** Replay-based and model expansion methods consistently outperform regularization-based methods in continual instruction tuning.
- **Mechanism:** Replay methods preserve old task data to interleave with new tasks, while model expansion methods allocate new modules for each task and freeze the rest, both effectively preventing overwriting of old knowledge.
- **Core assumption:** Freezing non-expanded modules or replaying old samples sufficiently protects old task representations.
- **Evidence anchors:**
  - [abstract] "we integrate and adapt classic continual learning methods to our context, demonstrating the efficacy of data replay and model expansion strategies across diverse scenarios. In contrast, regularization-based methods only perform well on models that have been jointly instruction-tuned on multiple tasks."
  - [section] "Our results reveal that the regularization-based methods fail to effectively handle forgetting when the model is not instruction-tuned on multiple tasks initially. Conversely, it shows competitive continual learning performance without additional isolated structures or stored samples when starting from instructBLIP. The other two replay-based approaches and model expansion approaches can consistently achieve promising results in both settings."
  - [corpus] Weak anchor: No direct mechanistic comparison in related works; conclusions are inferred from ablation studies.
- **Break condition:** If the replay buffer is too small or task-specific modules are too tightly coupled, effectiveness may degrade.

### Mechanism 3
- **Claim:** Task similarity-informed regularization and model expansion improve continual learning by exploiting correlations between tasks.
- **Mechanism:** Measuring task similarity via embeddings of image, instruction, and output allows adaptive regularization weighting or selective reuse of task modules, reducing unnecessary parameter expansion and strengthening anti-forgetting for related tasks.
- **Core assumption:** Vision-language tasks exhibit measurable correlations that can be leveraged to inform continual learning strategies.
- **Evidence anchors:**
  - [abstract] "we propose task-similarity-informed regularization and model expansion methods to encourage the reuse of parameters or structures for relevant tasks. Experimental results show consistent improvement with our method compared to traditional continual learning baselines."
  - [section] "By virtue of instruction tuning, tasks are uniformly formulated as image-instruction-output datasets. We can easily obtain task relevance by measuring the similarity of image, instruction, and output between tasks."
  - [corpus] Weak anchor: Related works like SwitchCIT and MMC mention task correlation but do not mechanistically detail embedding-based similarity integration.
- **Break condition:** If task similarity measures are inaccurate or tasks are highly dissimilar, the adaptive benefits may be lost.

## Foundational Learning

- **Concept:** Catastrophic forgetting in neural networks
  - Why needed here: The core problem being solved is that models lose performance on old tasks when trained on new ones sequentially.
  - Quick check question: What happens to a model's performance on Task A after it is trained on Task B without any anti-forgetting mechanism?

- **Concept:** Multi-task joint training vs sequential fine-tuning
  - Why needed here: The paper compares models trained jointly on all tasks against those trained sequentially to measure forgetting.
  - Quick check question: How does the average performance on old tasks change when training is switched from joint to sequential?

- **Concept:** Task similarity measurement via embeddings
  - Why needed here: Task similarity is used to adaptively adjust regularization strength and decide when to reuse model components.
  - Quick check question: How would you compute a similarity score between two instruction-output pairs using BERT embeddings?

## Architecture Onboarding

- **Component map:** Base LMM (BLIP2/InstructBLIP) -> Task-specific projection layer -> Task-specific key -> Task encoders (BERT/ViT) -> Buffer
- **Critical path:**
  1. Compute task embeddings for similarity
  2. Choose continual learning method (replay, expansion, or regularization)
  3. Train task-specific module or update parameters with anti-forgetting constraints
  4. Store task embeddings and importance scores for future tasks
- **Design tradeoffs:**
  - Expansion vs regularization: Expansion avoids forgetting but increases model size; regularization is parameter-efficient but requires careful importance weighting.
  - Buffer size: Larger buffers reduce forgetting but increase memory cost.
  - Similarity threshold: Higher thresholds reduce expansion but risk underfitting similar tasks.
- **Failure signatures:**
  - Sudden drop in old task performance after new task training → forgetting
  - No improvement after task similarity adjustment → similarity measure broken
  - Model collapse or NaN outputs → gradient explosion from incorrect regularization scaling
- **First 3 experiments:**
  1. Compare SeqFT vs EWC vs ER on benchmark1 to confirm forgetting baseline.
  2. Test TIR with different importance measures (MAS, SI, EWC) to validate adaptive weighting.
  3. Run EProj with and without task similarity reuse to measure benefit of selective expansion.

## Open Questions the Paper Calls Out

- How does the task similarity score threshold impact the effectiveness of the task-similarity-informed model expansion method?
- How does the choice of task encoder affect the task similarity measures and the performance of the task-similarity-informed methods?
- How does the order of tasks in the continual instruction tuning process affect the performance of the models?

## Limitations
- Evaluation relies on task-specific metrics that may not fully capture instruction-following generalization
- Task similarity measures lack ablation studies to confirm robustness to embedding quality
- Replay buffer size (1% of dataset) is fixed and may not be optimal across all tasks

## Confidence

**High**: Multi-task joint instruction tuning reduces forgetting; replay and model expansion methods outperform regularization-based approaches in most settings

**Medium**: Task similarity-informed methods provide consistent improvements; the superiority of replay/expansion methods is task-dependent

**Low**: The specific task similarity thresholds and embedding choices are optimal; the benchmark covers all relevant vision-language task distributions

## Next Checks
1. Perform ablation studies on replay buffer size and task similarity threshold sensitivity
2. Test the proposed methods on out-of-distribution tasks not included in the benchmark
3. Evaluate long-term forgetting effects beyond the sequential task order studied