---
ver: rpa2
title: Hijacking Large Language Models via Adversarial In-Context Learning
arxiv_id: '2311.09948'
source_url: https://arxiv.org/abs/2311.09948
tags:
- arxiv
- adversarial
- attack
- llms
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a novel adversarial attack on in-context learning
  (ICL) by appending imperceptible adversarial suffixes to the in-context demonstrations.
  The attack hijacks large language models (LLMs) to generate targeted unwanted outputs,
  regardless of the query content.
---

# Hijacking Large Language Models via Adversarial In-Context Learning

## Quick Facts
- arXiv ID: 2311.09948
- Source URL: https://arxiv.org/abs/2311.09948
- Reference count: 18
- Primary result: Adversarial suffixes achieve 99.4% attack success rate on SST-2 sentiment task while maintaining 98.8% clean accuracy

## Executive Summary
This paper introduces a novel adversarial attack on in-context learning (ICL) that appends imperceptible adversarial suffixes to demonstration examples. The attack hijacks large language models to generate targeted unwanted outputs regardless of the query content. The authors propose a Greedy Gradient-guided Injection (GGI) algorithm that efficiently learns these adversarial suffixes through gradient-based optimization. Extensive experiments demonstrate the attack achieves high success rates across various datasets, model architectures, and tasks while maintaining clean accuracy.

## Method Summary
The GGI algorithm learns adversarial suffixes by iteratively optimizing discrete token choices using gradient information. For each demonstration, it computes gradients to identify promising candidate tokens, evaluates them explicitly, and injects the best-performing token at each position. The method appends these learned suffixes to ICL demonstrations, causing the model's attention to shift toward the adversarial tokens during generation. This diverts the model's focus away from task-relevant content, forcing it to output targeted unwanted tokens regardless of the actual query.

## Key Results
- Achieves 99.4% attack success rate on SST-2 sentiment analysis with 2-shot demonstrations on GPT2-XL
- Maintains 98.8% clean accuracy while hijacking model outputs
- Demonstrates transferability of adversarial suffixes across different demo sets and even across datasets for the same task
- Shows effectiveness across multiple model architectures including GPT2-XL, LLaMA-7b, and OPT variants

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial suffixes appended to ICL demos hijack LLM attention away from task-relevant context
- Mechanism: The adversarial tokens inserted after each demo example cause the model's attention weights to shift toward these tokens during generation, distracting from the actual task content
- Core assumption: Attention mechanisms in LLMs are sensitive to token positioning and can be manipulated through suffix injection
- Evidence anchors:
  - [section] "As depicted in Figure 2b, the model's attention for generating the sentiment token of the test query has been diverted towards the adversarial suffix tokens 'NULL' and 'Remove'"
  - [section] "the adversarial suffixes attain the largest attention weights represented by the darkest green color"

### Mechanism 2
- Claim: Gradient-based optimization efficiently finds discrete token suffixes that maximize adversarial loss
- Mechanism: The GGI algorithm uses linearized gradient approximations to select candidate tokens, then evaluates them explicitly to find those that most reduce the loss function
- Core assumption: Gradients with respect to token embeddings can guide the search for effective adversarial tokens
- Evidence anchors:
  - [section] "we compute the linearized approximation of replacing the i-th demo xi in C by evaluating the gradient ∇exi L(xQ)"
  - [section] "we instead leverage gradients with respect to the suffix indicators to find promising candidate tokens"

### Mechanism 3
- Claim: Transferability of adversarial suffixes across different demo sets and even datasets
- Mechanism: The learned adversarial suffixes maintain effectiveness when transferred to new contexts, suggesting they exploit fundamental model vulnerabilities
- Core assumption: The adversarial tokens exploit universal weaknesses in how LLMs process in-context demonstrations
- Evidence anchors:
  - [abstract] "The adversarial suffixes learned via gradient optimization are transferable, remaining effective on different demo sets"
  - [section] "the suffix transferability holds even across different datasets for the same downstream task"

## Foundational Learning

- Concept: In-context learning mechanism
  - Why needed here: Understanding how ICL works is essential to grasp how adversarial suffixes can hijack the learning process
  - Quick check question: How does an LLM generate outputs during ICL without parameter updates?

- Concept: Gradient-based discrete optimization
  - Why needed here: The attack relies on efficiently optimizing over discrete token choices using gradient information
  - Quick check question: Why can't standard gradient descent be directly applied to discrete token selection?

- Concept: Attention mechanism in transformer models
  - Why needed here: The hijacking works by manipulating attention weights toward adversarial tokens
  - Quick check question: How do attention weights determine which tokens influence the next token prediction?

## Architecture Onboarding

- Component map: Template construction with demos and query -> GGI algorithm with gradient computation and token selection -> Adversarial suffix injection -> LLM inference -> Output hijacking

- Critical path: Prompt template → GGI attack optimization → Adversarial suffix injection → LLM inference → Output hijacking

- Design tradeoffs:
  - Attack effectiveness vs. imperceptibility: Longer suffixes may be more effective but easier to detect
  - Transferability vs. specificity: General suffixes may transfer better but be less potent than task-specific ones
  - Computation cost vs. attack success: More gradient iterations improve success but increase attack generation time

- Failure signatures:
  - Clean accuracy remains high while attack accuracy drops to baseline random levels
  - Attention weights don't shift toward adversarial suffixes during generation
  - Adversarial suffixes lose effectiveness when transferred to new demo sets

- First 3 experiments:
  1. Test attack success rate with varying numbers of demo examples (2, 4, 8 shots)
  2. Compare attention maps between clean and adversarial prompts to visualize hijacking
  3. Evaluate transferability by applying learned suffixes to different datasets within the same task category

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed GGI attack compare to other black-box adversarial attacks that do not use gradients, such as square attack, in terms of transferability across different LLMs and tasks?
- Basis in paper: [explicit] The paper compares GGI to square attack and other baselines, showing GGI has higher attack success rates.
- Why unresolved: The comparison focuses on the same LLMs and tasks used in the experiments. It is unclear if GGI maintains its advantage when transferring to completely different LLMs and tasks.
- What evidence would resolve it: Conducting experiments to test the transferability of GGI against square attack on a diverse set of LLMs and tasks not used in the original experiments.

### Open Question 2
- Question: What are the specific mechanisms by which the adversarial suffixes learned by GGI divert the attention of LLMs away from task-relevant concepts?
- Basis in paper: [explicit] The paper mentions that attention maps show adversarial suffixes distract LLM's attention away from original query, but does not provide a detailed analysis of the mechanisms.
- Why unresolved: While the attention maps provide a visual illustration, they do not explain the underlying mechanisms of how and why the attention is diverted.
- What evidence would resolve it: Conducting a detailed analysis of the attention weights and internal representations of the LLMs to identify the specific mechanisms by which the adversarial suffixes affect the attention.

### Open Question 3
- Question: How do the adversarial suffixes learned by GGI generalize to different in-context demo sets and query distributions?
- Basis in paper: [explicit] The paper mentions that the adversarial suffixes are transferable across different demo sets and datasets, but does not provide a detailed analysis of the generalization ability.
- Why unresolved: The experiments only evaluate the transferability of the suffixes to a limited extent. It is unclear how well the suffixes generalize to different demo sets and query distributions that are not seen during training.
- What evidence would resolve it: Conducting experiments to evaluate the performance of the adversarial suffixes on a diverse set of demo sets and query distributions that are not used in the original experiments.

## Limitations

- The exact ICL prompt template format is not explicitly specified in the paper, only referenced to external work
- Critical hyperparameters for the GGI algorithm (iterations T, batch size B, top-k value) are not disclosed
- Limited evaluation on frontier models like GPT-4, Claude, or PaLM that may have stronger adversarial defenses

## Confidence

**High Confidence Claims**:
- The GGI algorithm can learn adversarial suffixes that increase attack success rates on the tested datasets and models
- Attention visualization shows that adversarial suffixes do shift attention weights toward the suffix tokens
- Transferability of adversarial suffixes across different demo sets within the same dataset is empirically demonstrated

**Medium Confidence Claims**:
- The attack achieves "imperceptible" adversarial suffixes, as the paper doesn't define a quantitative imperceptibility metric
- The attack's effectiveness on multi-class classification tasks (AG News) compared to binary tasks
- The generalizibility of findings to other NLP tasks beyond sentiment analysis and topic classification

**Low Confidence Claims**:
- Claims about the attack working "regardless of the query content" are not fully validated, as the paper focuses on specific task types
- The assertion that this represents a fundamental vulnerability in all LLMs using ICL
- The practical severity of this attack vector in real-world applications where input filtering and monitoring may detect unusual suffixes

## Next Checks

1. **Template Sensitivity Analysis**: Systematically vary the ICL prompt template format (demo placement, instruction phrasing, separator tokens) and measure how attack success rates change. This would quantify how dependent the attack is on specific template choices and whether it generalizes across template variations.

2. **Transferability Stress Test**: Apply the learned adversarial suffixes to a completely different model architecture (e.g., from decoder-only to encoder-decoder models like T5 or BART) and measure attack success rates. This would validate whether the attack exploits universal transformer weaknesses or model-specific vulnerabilities.

3. **Real-World Deployment Simulation**: Implement a simple input sanitization pipeline that flags or removes tokens appearing in learned adversarial suffixes, then measure how this affects attack success rates. This would assess the practical robustness of the attack against basic defensive measures.