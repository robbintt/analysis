---
ver: rpa2
title: 'Streamlined Data Fusion: Unleashing the Power of Linear Combination with Minimal
  Relevance Judgments'
arxiv_id: '2309.04981'
source_url: https://arxiv.org/abs/2309.04981
tags:
- qrels
- data
- documents
- fusion
- relevant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the high cost and labor intensity of traditional
  linear combination data fusion, which requires relevance judgments on a large percentage
  of documents. The authors propose using a reduced set of relevant documents (20%-50%)
  to train optimal weights via multiple linear regression.
---

# Streamlined Data Fusion: Unleashing the Power of Linear Combination with Minimal Relevance Judgments

## Quick Facts
- arXiv ID: 2309.04981
- Source URL: https://arxiv.org/abs/2309.04981
- Reference count: 40
- Linear combination weights trained with 20%-50% relevance judgments achieve <3% performance difference from full judgments

## Executive Summary
This study addresses the high cost and labor intensity of traditional linear combination data fusion, which requires relevance judgments on a large percentage of documents. The authors propose using a reduced set of relevant documents (20%-50%) to train optimal weights via multiple linear regression. Experiments on four TREC datasets show that weights trained with this reduced set closely match those obtained with full official relevance judgments ("qrels"), with performance differences below 3% across all metrics. In some cases, partial qrels even outperformed full qrels. These findings enable more efficient and affordable data fusion, making advanced methods accessible with significantly less effort.

## Method Summary
The authors evaluate whether linear combination weights can be effectively trained using only 20%-50% of relevant documents rather than full relevance judgments. They use four TREC datasets (2018-2021) and simulate partial relevance judgments through pooling at various depths to select subsets of relevant documents. Multiple linear regression is applied to train fusion weights using these partial judgments, with two-fold cross-validation ensuring generalization. The performance of fusion using partial qrels is compared against fusion using full official qrels across MAP, RP, P@10, and P@20 metrics.

## Key Results
- Linear combination weights trained with 20%-50% relevance judgments achieve performance within 3% of full qrels across all metrics
- In several cases, partial qrels (both 20% and 50%) outperformed full qrels in fusion performance
- The pooling policy used in TREC provides sufficient signal for effective weight training even with reduced relevance judgments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Partial relevance judgments preserve enough rank order signal for effective linear combination weights
- Mechanism: Multiple linear regression weights trained on a subset of relevant documents still capture the relative performance differences between retrieval systems because pooling retains the top-ranked documents most likely to be relevant
- Core assumption: The top-ranked documents in pooling represent a sufficiently diverse and representative sample of relevant documents to preserve system ranking relationships
- Evidence anchors:
  - [abstract] "weights trained with multiple linear regression using this reduced set closely rival those obtained with TREC's official 'qrels'"
  - [section 3] "The pooling policy used in TREC is a good practice and we also apply it in this study"
- Break condition: When pooling depth is too shallow to capture meaningful performance differences, or when relevant documents are uniformly distributed across ranks

### Mechanism 2
- Claim: Linear combination performance degrades gracefully as relevance judgment completeness decreases
- Mechanism: The regression model can still learn meaningful weight relationships from partial data because the training signal maintains directional information about which systems perform better on which queries
- Core assumption: The relationship between system performance and document relevance follows consistent patterns that can be learned from partial data
- Evidence anchors:
  - [abstract] "performance differences below 3% across all metrics" when using 20-50% relevance judgments
- Break condition: When partial relevance judgments create systematic bias in training data that skews weight estimation

### Mechanism 3
- Claim: Query-based cross-validation mitigates overfitting when using partial relevance judgments
- Mechanism: Two-fold cross-validation ensures that weight training generalizes across different query subsets, compensating for limited relevance judgment coverage
- Core assumption: Query-level performance patterns are consistent enough across partitions to enable reliable weight estimation
- Evidence anchors:
  - [section 4] "The two-fold cross-validation methodology was applied: for all the queries in a data set, we divided them into two partitions"
- Break condition: When query subsets have dramatically different relevance distributions that prevent consistent weight learning

## Foundational Learning

- Concept: Multiple linear regression fundamentals
  - Why needed here: The method relies on understanding how regression coefficients translate to system weights
  - Quick check question: If you have 3 retrieval systems and train regression weights of [0.2, 0.5, 0.3], what does each weight represent?

- Concept: TREC pooling methodology
  - Why needed here: Understanding how relevance judgments are collected is crucial for interpreting partial judgment effectiveness
  - Quick check question: In fixed-length pooling, if pool depth is 10 and a query has 50 relevant documents, how many are guaranteed to be judged?

- Concept: Information retrieval evaluation metrics (MAP, P@10, P@20)
  - Why needed here: The study compares performance across multiple metrics, requiring understanding of what each measures
  - Quick check question: If a system retrieves 5 relevant documents at ranks 1, 3, 5, 7, and 9, what is its P@10 value?

## Architecture Onboarding

- Component map:
  - TREC runs and relevance judgments -> Pooling engine -> Partial qrels -> Regression trainer -> Fusion weights -> Fusion executor -> Performance evaluation

- Critical path:
  1. Load TREC runs and official qrels
  2. Generate partial qrels using pooling
  3. Train regression weights using cross-validation
  4. Apply weights to fusion
  5. Evaluate performance

- Design tradeoffs:
  - Pool depth vs. annotation cost: Deeper pools capture more relevance but increase annotation effort
  - Cross-validation folds vs. training data: More folds provide better generalization but reduce training samples
  - Metric selection: Different metrics emphasize different aspects of retrieval quality

- Failure signatures:
  - Weights diverging wildly between folds indicates overfitting or insufficient training data
  - Performance drops >3% from full qrels suggests pooling isn't capturing enough signal
  - Negative weights indicate potential issues with training data quality or system compatibility

- First 3 experiments:
  1. Verify pooling correctly extracts top-k documents from runs
  2. Test regression weight stability across cross-validation folds
  3. Compare fusion performance with random vs. pooled partial qrels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different pooling strategies (fixed-length vs variable-length) impact the effectiveness of using partial relevance judgments for weight training?
- Basis in paper: [explicit] The authors mention this as future work, noting that while fixed-length pooling works well, it would be interesting to see how other methods like variable-length pooling perform.
- Why unresolved: The study only used fixed-length pooling, so direct comparisons with variable-length pooling were not conducted.
- What evidence would resolve it: Experimental results comparing linear combination performance using partial relevance judgments obtained through both fixed-length and variable-length pooling strategies.

### Open Question 2
- Question: How sensitive are the results to the specific percentage of relevant documents used (20% vs 50% vs other values)?
- Basis in paper: [explicit] The study tested only 20% and 50% partial qrels, but mentions these were chosen to be "roughly" those percentages.
- Why unresolved: The study did not explore a wider range of percentages or test the sensitivity of results to the exact number of relevance judgments used.
- What evidence would resolve it: Experiments testing multiple percentages (e.g., 10%, 30%, 40%, 60%) to determine the optimal range and sensitivity.

### Open Question 3
- Question: How do optimization-based weight training methods (like genetic algorithms or differential evolution) perform when using partial relevance judgments compared to multiple linear regression?
- Basis in paper: [explicit] The authors mention this as future work, noting it would be interesting to investigate the effect of partial qrels on optimization-based methods.
- Why unresolved: The study only tested multiple linear regression for weight training.
- What evidence would resolve it: Comparative experiments using the same partial relevance judgments with different weight training methods including genetic algorithms and differential evolution.

## Limitations

- Limited empirical scope: Only four TREC datasets evaluated, may not generalize to other domains or larger-scale retrieval tasks
- Pooling depth dependency: Effectiveness critically depends on TREC's pooling methodology without systematic depth variation analysis
- Static system selection: Only top MAP runs per participant used, stability across different system compositions untested

## Confidence

- High confidence: Core finding that 20-50% relevance judgments enable near-optimal fusion weights is well-supported by consistent experimental results
- Medium confidence: Mechanism explanations are plausible but not directly tested through ablation studies
- Low confidence: Claims about cost reduction and practical implementation benefits lack quantitative backing

## Next Checks

1. **Domain generalization test**: Apply the methodology to non-TREC datasets from different domains (news, web, scientific literature) to verify that partial judgments maintain effectiveness across varying relevance distributions and system characteristics.

2. **Pooling depth sensitivity analysis**: Systematically vary pool depth from 5 to 50 documents per query to identify the minimum depth required to achieve the 3% performance threshold, and test whether this threshold varies by dataset characteristics.

3. **Cross-system composition stability**: Evaluate fusion performance using randomly selected system combinations (rather than top MAP runs) to determine if the partial judgment approach remains effective when fusing less correlated or lower-performing systems.