---
ver: rpa2
title: A Domain Decomposition-Based CNN-DNN Architecture for Model Parallel Training
  Applied to Image Recognition Problems
arxiv_id: '2302.06564'
source_url: https://arxiv.org/abs/2302.06564
tags:
- global
- training
- local
- data
- erent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel CNN-DNN architecture for model parallel
  training of image classification tasks, inspired by two-level domain decomposition
  methods (DDM). The approach decomposes a global CNN into smaller, local subnetworks
  that operate on overlapping or non-overlapping parts of the input data, such as
  sub-images.
---

# A Domain Decomposition-Based CNN-DNN Architecture for Model Parallel Training Applied to Image Recognition Problems

## Quick Facts
- arXiv ID: 2302.06564
- Source URL: https://arxiv.org/abs/2302.06564
- Reference count: 40
- Primary result: Proposed method reduces 3D CT scan training time by factor of 23 while improving accuracy

## Executive Summary
This paper introduces a novel CNN-DNN architecture for model parallel training of image classification tasks, inspired by two-level domain decomposition methods from numerical PDEs. The approach decomposes a global CNN into smaller, local subnetworks that operate on overlapping or non-overlapping parts of input data, enabling parallel training without communication overhead. A global coarse net DNN then aggregates local classifications into a final global decision. Experiments demonstrate significant training time acceleration and accuracy improvements across various image classification problems including 2D images, face recognition, and 3D CT scans.

## Method Summary
The method decomposes a global CNN into N smaller local subnetworks that process overlapping or non-overlapping sub-images in parallel. Each local CNN is trained independently without communication overhead. After training, a separate DNN (global coarse net) is trained to aggregate the local probability distributions from the subnetworks into a final global classification. The approach was validated on CIFAR-10, TF-Flowers, Extended Yale Face Database B, and chest CT scans with COVID-19, demonstrating training time reductions of up to 23× for 3D data while maintaining or improving accuracy.

## Key Results
- 3D CT scan training time reduced by factor of almost 23 compared to global CNN
- Improved classification accuracy on Extended Yale Face Database B compared to global CNN
- Local subnetworks can be trained completely in parallel without inter-process communication
- Classification accuracy increases with greater overlap between sub-images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-level decomposition allows parallel training of smaller subnetworks without communication overhead
- Core assumption: Decomposed subnetworks can capture sufficient information for meaningful local classifications
- Evidence anchors: [abstract] subnetworks trained completely in parallel and independently; [section 3.1] no communication between network models; [corpus] weak evidence from related model parallelism papers

### Mechanism 2
- Claim: Global coarse net effectively aggregates local decisions into globally coherent classification
- Core assumption: Relationship between local probability distributions and global classification can be learned by a standard DNN
- Evidence anchors: [abstract] additional DNN evaluates local decisions and generates final global decision; [section 3.2] DNN maps N local probability distributions to correct classification labels; [corpus] weak evidence from related model parallelism papers

### Mechanism 3
- Claim: Overlapping sub-images improve classification accuracy by providing contextual information at boundaries
- Core assumption: Overlapping regions provide meaningful contextual information that improves classification
- Evidence anchors: [section 3.1] decompose images into p×p sub-images with overlap δ≥0; [section 5.1] local accuracy values and global coarse net classification accuracies increase with increasing overlap; [corpus] weak evidence as related papers don't discuss overlap in CNN decomposition context

## Foundational Learning

- Concept: Convolutional Neural Networks (CNNs)
  - Why needed here: Method builds upon CNN architectures for image classification, decomposing them into smaller subnetworks
  - Quick check question: What is the primary advantage of using convolutional layers over fully connected layers for image data?

- Concept: Domain Decomposition Methods (DDM)
  - Why needed here: Approach inspired by DDM from numerical PDEs, where global problems are decomposed into smaller subproblems
  - Quick check question: How does two-level DDM differ from single-level DDM in terms of convergence properties?

- Concept: Transfer Learning vs. Independent Training
  - Why needed here: Unlike transfer learning approaches that initialize global model from local models, this method trains local models independently and uses separate aggregation network
  - Quick check question: What is the key difference between initializing a global model from local models versus training a separate aggregation network?

## Architecture Onboarding

- Component map:
  Local CNNs -> Global Coarse Net -> Final Classification
  (N independent subnetworks processing sub-images) -> (DNN aggregating local probability distributions) -> (Global decision output)

- Critical path:
  1. Preprocess input images to create sub-images with desired overlap
  2. Initialize and train N local CNNs in parallel on separate GPUs
  3. Collect local probability distributions from validation data
  4. Train global coarse net to map local distributions to global labels
  5. Evaluate combined system on test data

- Design tradeoffs:
  - Overlap vs. computational efficiency: More overlap improves accuracy but increases redundancy
  - Number of subnetworks vs. model capacity: More subnetworks reduce individual model size but increase complexity of global coarse net
  - Sub-image size vs. feature extraction: Smaller sub-images train faster but may miss contextual information

- Failure signatures:
  - Low variance between local CNN outputs but poor global coarse net performance
  - High variance between local CNN outputs with no improvement from global coarse net
  - Training time doesn't scale down with decomposition as expected
  - Global coarse net overfits to training data but generalizes poorly

- First 3 experiments:
  1. Decompose CIFAR-10 images into 2×2 non-overlapping sub-images with no overlap, train local CNNs, and evaluate baseline performance
  2. Add 2-pixel overlap to the decomposition and retrain to observe impact on accuracy
  3. Increase decomposition to 3×3 sub-images and measure training time reduction vs. accuracy trade-off

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed domain decomposition-based CNN-DNN architecture perform on other types of machine learning tasks beyond image classification, such as regression or time series prediction?
- Basis in paper: [explicit] Authors state "In principle, the idea of aggregating separate, local models into one final, global decision can also be applied to different and more general machine learning tasks. However, we focus on our application of image recognition problems with convolutional neural networks for the remainder of this paper."
- Why unresolved: Authors have not explored application of their method to other machine learning tasks, so effectiveness in these areas is unknown
- What evidence would resolve it: Experiments demonstrating performance of domain decomposition-based CNN-DNN architecture on regression or time series prediction tasks, compared to traditional approaches

### Open Question 2
- Question: How does the performance of the domain decomposition-based CNN-DNN architecture vary with different types of network architectures, such as recurrent neural networks (RNNs) or transformers?
- Basis in paper: [explicit] Authors state "Let us note again that, for the remainder of this paper, we restrict ourselves to discrete classification problems and CNN models. However, the described approach can also be applied to different network models and different machine learning tasks, e.g., regression problems."
- Why unresolved: Authors have only tested their method on CNN architectures, so performance on other types of networks is unknown
- What evidence would resolve it: Experiments demonstrating performance of domain decomposition-based architecture on RNNs or transformers, compared to traditional approaches

### Open Question 3
- Question: How does the choice of overlap between sub-images in the domain decomposition affect the performance of the CNN-DNN architecture?
- Basis in paper: [explicit] Paper discusses use of overlapping or non-overlapping sub-images, stating "In analogy of our approach to domain decomposition methods, it is natural to decompose a given 2D image of HxW pixels into p×p, p∈N rectangular sub-images with an overlap δ≥0."
- Why unresolved: While paper mentions use of overlapping sub-images, it does not provide detailed analysis of how overlap affects performance of the architecture
- What evidence would resolve it: Experiments comparing performance of CNN-DNN architecture with different overlap values, and analysis of impact of overlap on accuracy and training time

## Limitations

- Limited validation of proposed mechanisms with systematic ablation studies
- Experimental results lack comparison with other model parallel approaches
- Method's effectiveness on more complex datasets with higher class counts remains untested

## Confidence

- Mechanism 1 (Parallel training): Medium - Well-supported by described implementation but lacks comparison with other model parallel approaches
- Mechanism 2 (Global coarse net aggregation): Low-Medium - Plausible but insufficient evidence that global net learns meaningful combinations of local outputs
- Mechanism 3 (Overlap benefits): Medium - Supported by experimental results but lacks theoretical justification

## Next Checks

1. Conduct ablation studies removing the global coarse net to quantify its contribution versus simply averaging local classifications
2. Test the approach on datasets with more classes and higher complexity to assess scalability beyond demonstrated cases
3. Measure communication overhead and setup time required for decomposition approach versus claimed training time improvements