---
ver: rpa2
title: Evaluating and Enhancing Large Language Models for Conversational Reasoning
  on Knowledge Graphs
arxiv_id: '2312.11282'
source_url: https://arxiv.org/abs/2312.11282
tags:
- reasoning
- path
- knowledge
- llms
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work evaluates and enhances large language models for conversational
  reasoning on knowledge graphs. The authors identify limitations of LLMs due to lack
  of knowledge graph environment awareness and difficulties in optimizing intermediate
  reasoning stages.
---

# Evaluating and Enhancing Large Language Models for Conversational Reasoning on Knowledge Graphs

## Quick Facts
- arXiv ID: 2312.11282
- Source URL: https://arxiv.org/abs/2312.11282
- Reference count: 14
- Primary result: LLM-ARK achieves 36.39% target@1 accuracy on OpenDialKG, outperforming GPT-4's 14.91% by 21.48 percentage points

## Executive Summary
This paper addresses the challenge of multi-hop reasoning on knowledge graphs within conversational contexts by introducing LLM-ARK, a knowledge graph reasoning agent that incorporates environmental awareness through Full Textual Environment (FTE) prompts. The authors identify that standard LLMs like GPT-4 struggle with KG reasoning due to limited perception of reasoning environments and ineffective intermediate optimization mechanisms. By reframing KG reasoning as a sequential decision-making task and applying Proximal Policy Optimization (PPO) reinforcement learning, LLM-ARK achieves state-of-the-art performance on the OpenDialKG dataset, demonstrating the effectiveness of combining environmental awareness with RL-based optimization.

## Method Summary
The LLM-ARK framework consists of three main components: a Full Textual Environment (FTE) prompt that aggregates multi-scale inputs including dialog history, path history, and exit paths; a frozen LLM that encodes this state into continuous vector representations; and a Path Aware MLP (PA-MLP) that fuses LLM state embeddings with knowledge graph exit path information. The system treats KG reasoning as a Markov Decision Process where the agent selects relations to traverse the graph, optimized using PPO reinforcement learning. The model uses TransE embeddings for pre-training the knowledge graph and obtaining path embeddings, which are then fused with LLM outputs through the PA-MLP component to generate action probabilities.

## Key Results
- LLaMA-7B-ARK achieves 36.39% target@1 accuracy on OpenDialKG, outperforming previous state-of-the-art by 5.28 percentage points
- GPT-4 scores 14.91% target@1 accuracy, demonstrating significant performance degradation compared to LLM-ARK
- Ablation studies show the PA-MLP component is critical, with performance substantially decreasing when exit paths are not considered
- The FTE prompt effectively captures essential state information, enabling the model to make context-aware decisions during reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 performance degrades on KG reasoning tasks due to lack of KG environment awareness and ineffective intermediate optimization
- Mechanism: GPT-4 operates statelessly without access to path history, dialog context, or exit path subgraphs during multi-hop reasoning, leading to suboptimal decisions. The model lacks a mechanism to optimize intermediate reasoning steps
- Core assumption: Environmental information and intermediate reasoning optimization are critical for KG reasoning performance
- Evidence anchors:
  - [abstract] "However, the performance of LLMs is constrained due to a lack of KG environment awareness and the difficulties in developing effective optimization mechanisms for intermediary reasoning stages"
  - [section 1] "On the one hand, LLMs suffer from a limited perception of variable reasoning environments...On the other hand, Yao et al. [2023] indicate that there is a lack of systematic methodologies for consistent model refinement"
  - [corpus] Weak evidence - no direct mention of KG reasoning limitations in corpus papers

### Mechanism 2
- Claim: LLM-ARK improves KG reasoning by incorporating Full Textual Environment (FTE) prompts and Proximal Policy Optimization (PPO) reinforcement learning
- Mechanism: FTE prompts aggregate multi-scale inputs (dialog history, path history, exit paths) to provide environmental awareness. PPO optimizes intermediate reasoning steps through policy gradient learning from reward signals
- Core assumption: Environmental awareness and intermediate optimization through RL are effective for KG reasoning
- Evidence anchors:
  - [abstract] "We further introduce LLM-ARK, a LLM grounded KG reasoning agent designed to deliver precise and adaptable predictions on KG paths. LLM-ARK leverages Full Textual Environment (FTE) prompt to assimilate state information within each reasoning step"
  - [section 3.3.1] "This module (FTE) tracks the agent's state that captures all essential information in the conversation so far"
  - [section 3.3.3] "We formulate KG reasoning as a Markov Decision Process...we adopt an online reinforcement learning policy gradient algorithm, Proximal Policy Optimization (PPO)"

### Mechanism 3
- Claim: The Path Aware MLP (PA-MLP) component enhances environment-aware capabilities by fusing LLM state embeddings with knowledge graph exit path information
- Mechanism: PA-MLP takes the FTE representation and exit path embeddings as input, applies a three-layer feed-forward network, and outputs a probability distribution over possible actions
- Core assumption: Fusing LLM state embeddings with exit path information improves environment awareness
- Evidence anchors:
  - [section 3.3.2] "Instead of just adding an MLP with a single output for the value on top of the last layer of the first decoder block...we further fused the hidden state after the MLP with the Knowledge Graph exit path information, called PA-MLP"
  - [section 4.6] "We conducted ablation experiments LLaMA7B-ARK-WP and found that the performance of our export environment-aware sub-module PA-MLP decreases substantially if we do not consider the exit paths"
  - [corpus] Weak evidence - no direct mention of PA-MLP or exit path fusion in corpus papers

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation of KG reasoning
  - Why needed here: KG reasoning is modeled as a sequential decision-making problem where the agent selects relations to traverse the graph
  - Quick check question: What are the five components of the MDP tuple (S, O, A, T, R, Î³) in the KG reasoning context?

- Concept: Reinforcement learning and policy gradient methods
  - Why needed here: PPO is used to optimize the KG reasoning agent by learning from reward signals across diverse tasks and environments
  - Quick check question: How does PPO differ from standard policy gradient methods in terms of stability and reliability?

- Concept: Knowledge graph embeddings (e.g., TransE)
  - Why needed here: KG embeddings are used to pre-train the knowledge graph and obtain path embeddings for fusion with LLM state embeddings
  - Quick check question: What is the objective of TransE in learning KG embeddings, and how does it capture entity relationships?

## Architecture Onboarding

- Component map: FTE -> LLM -> PA-MLP -> PPO
- Critical path:
  1. FTE aggregates multi-scale inputs (dialog history, path history, exit paths, current entity)
  2. LLM encodes FTE state into continuous vector representation
  3. PA-MLP fuses LLM state embeddings with KG exit path embeddings
  4. PPO optimizes actor and critic networks based on reward signals
- Design tradeoffs:
  - Using frozen LLM parameters vs. fine-tuning LLM weights for KG reasoning
  - Balancing environmental awareness (FTE) with model complexity and prompt length
  - Choosing appropriate reward function and discount factor for KG reasoning task
- Failure signatures:
  - Poor performance on target@k metrics compared to baselines
  - Inability to handle long dialog contexts or complex KG structures
  - Instability or divergence during PPO training
- First 3 experiments:
  1. Compare LLM-ARK performance with and without FTE prompts on a subset of OpenDialKG dataset
  2. Ablation study: Remove PA-MLP component and assess impact on KG reasoning performance
  3. Hyperparameter tuning: Experiment with different learning rates, batch sizes, and PPO hyperparameters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would LLM-ARK's performance change on multi-hop reasoning tasks requiring more than 2 hops?
- Basis in paper: [explicit] The paper states "since all true paths in OpenDialKG are at most 2 hops, we set the maximum path length to T = 2"
- Why unresolved: The model was only evaluated on 2-hop reasoning tasks, limiting understanding of its scalability to longer reasoning chains
- What evidence would resolve it: Experimental results comparing LLM-ARK's performance on datasets with longer path lengths (3-5 hops) using the same evaluation metrics

### Open Question 2
- Question: What is the relative contribution of each component (FTE prompt, TransE embeddings, PA-MLP) to overall performance?
- Basis in paper: [explicit] The ablation study shows performance differences but doesn't isolate individual component contributions
- Why unresolved: The ablation study removed multiple components simultaneously rather than testing each in isolation
- What evidence would resolve it: Controlled experiments testing each component's impact individually while keeping others constant

### Open Question 3
- Question: How does LLM-ARK's performance compare to other RL-based KG reasoning approaches using different RL algorithms?
- Basis in paper: [inferred] The paper uses PPO but doesn't compare against other RL algorithms like DQN, A3C, or SAC
- Why unresolved: Only PPO was tested, leaving open questions about whether it's the optimal choice for this task
- What evidence would resolve it: Head-to-head comparison of LLM-ARK using different RL algorithms on the same dataset with identical hyperparameters

### Open Question 4
- Question: How does the model handle ambiguous dialog contexts where multiple valid reasoning paths exist?
- Basis in paper: [explicit] The case study mentions "OpenDialKG is not a unique path inference; there are many potential paths to reach the target entity"
- Why unresolved: The paper doesn't analyze how the model resolves ambiguity or whether it consistently chooses optimal paths
- What evidence would resolve it: Detailed analysis of model behavior on ambiguous cases, including path diversity metrics and comparison to human reasoning patterns

## Limitations

- Performance evaluation limited to 2-hop reasoning tasks, leaving scalability to longer reasoning chains unexplored
- Specific prompt templates and FTE formatting details not fully specified, making direct replication challenging
- Only tested on OpenDialKG dataset, limiting generalizability to other KG reasoning tasks or domains
- Ablation studies combine multiple components rather than isolating individual contributions

## Confidence

- **High Confidence**: The core methodology of using FTE prompts to provide environmental awareness is well-grounded and the PPO-based optimization framework is standard in RL literature
- **Medium Confidence**: The claimed performance improvements (5.28 percentage point gain over state-of-the-art) are supported by experimental results, though dependent on proper prompt implementation
- **Medium Confidence**: The Path Aware MLP component shows measurable improvements in ablation studies, but the specific fusion mechanism requires careful implementation to replicate

## Next Checks

1. Implement a minimal FTE prompt template based on described components (dialog history, path history, exit paths) and test with a frozen LLM to verify state encoding works as intended
2. Create a simplified KG reasoning environment with 10-20 entities and manually verify that PPO optimization can learn to select correct paths through trial-and-error exploration
3. Conduct controlled ablation experiments removing PA-MLP fusion to measure the marginal contribution of path embedding integration versus FTE prompt alone