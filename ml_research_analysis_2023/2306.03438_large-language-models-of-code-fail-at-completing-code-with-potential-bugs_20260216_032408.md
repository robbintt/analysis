---
ver: rpa2
title: Large Language Models of Code Fail at Completing Code with Potential Bugs
arxiv_id: '2306.03438'
source_url: https://arxiv.org/abs/2306.03438
tags:
- code
- completion
- bugs
- potential
- buggy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces and studies the buggy-code completion problem,
  where code context for completion may contain potential bugs. Two new datasets are
  introduced: buggy-HumanEval with synthetic bugs from operator changes, and buggy-FixEval
  with realistic bugs from user submissions.'
---

# Large Language Models of Code Fail at Completing Code with Potential Bugs

## Quick Facts
- arXiv ID: 2306.03438
- Source URL: https://arxiv.org/abs/2306.03438
- Reference count: 40
- Primary result: Potential bugs in code context significantly degrade Code-LLM completion performance, dropping pass rates from over 50% to below 5%

## Executive Summary
This paper introduces the buggy-code completion problem, where code context for completion may contain potential bugs. The authors create two new datasets: buggy-HumanEval with synthetic bugs from operator changes, and buggy-FixEval with realistic bugs from user submissions. They demonstrate that high-performing Code-LLMs fail dramatically when completing code with potential bugs, with pass rates dropping from over 50% to below 5%. The study investigates post-hoc methods including completion→rewriting and rewriting→completion to mitigate this effect, finding that while these methods improve performance, significant gaps remain compared to clean partial code completion.

## Method Summary
The authors evaluate CODEGEN and INCODER models on buggy-code completion tasks using two newly introduced datasets. They measure performance using the pass@k metric, executing completed code against test cases. The study tests post-hoc methods including removal→completion, completion→rewriting, and rewriting→completion to address the adverse effects of potential bugs. The rewriting→completion method uses a likelihood-based measure to identify lines likely containing potential bugs by calculating the difference in likelihoods between the most probable token and the observed token at each position.

## Key Results
- Presence of potential bugs significantly degrades Code-LLM generation performance, with pass@1 rates dropping from over 50% to below 5%
- Post-hoc methods (completion→rewriting and rewriting→completion) improve buggy-code completion but leave significant performance gaps compared to clean partial code
- Code-LLMs fail to react to potential bugs, often producing similar completions for both clean and buggy code prefixes
- Models perform relatively well when potential bugs appear on or near the last line of partial code

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Potential bugs degrade Code-LLM completion performance because the model fails to recognize and adapt to semantic changes in the code prefix
- Mechanism: Code-LLMs rely on pattern matching from training data. When a potential bug introduces a semantic change, the model either fails to notice the change (continuing as if the prefix is correct) or misinterprets the altered prefix as a valid pattern, leading to incorrect completions
- Core assumption: The training data contains predominantly correct code patterns, so the model is not well-calibrated to handle or detect semantic bugs in partial code
- Evidence anchors: [abstract] "the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs"; [section] "the model fails to react to the potential bugs (i.e., common completions remain the same)"
- Break condition: If the model is trained on a dataset with a significant proportion of buggy code or explicitly trained to recognize and adapt to semantic changes, this mechanism may not hold

### Mechanism 2
- Claim: Post-hoc methods (completion→rewriting and rewriting→completion) improve buggy-code completion by providing an additional layer of bug detection and correction
- Mechanism: These methods introduce a separate step where a bug detection or correction model is applied either before or after the code completion step, allowing the system to potentially fix the bug before it influences the completion or correct the completed code after the fact
- Core assumption: The bug detection/correction model is sufficiently accurate and can identify and fix the potential bugs in the code prefix or completed code
- Evidence anchors: [abstract] "investigate several post-hoc methods for mitigating the adverse effect of potential bugs and find that there remains a significant gap in post-mitigation performance"; [section] "completion → rewriting (completes the program, then rewrites the completed one) and rewriting → completion (rewrites the potential bugs, then completes)"
- Break condition: If the bug detection/correction model is not accurate enough or if the bugs are too subtle or complex for the model to detect and fix, these methods may not improve performance

### Mechanism 3
- Claim: The likelihood-based measure for identifying potential bugs works by detecting tokens that deviate from the most probable token according to the language model
- Mechanism: For each token in the code prefix, the model calculates the difference in likelihood between the actual token and the most probable token. Tokens with large differences are flagged as potential bugs, as they represent unlikely or unexpected code
- Core assumption: The language model has learned a good representation of likely code patterns, so unlikely tokens are more likely to be bugs
- Evidence anchors: [section] "We calculate the score of each line by doing the following: At each token location, we define the token score to be the difference in likelihoods between the token with the highest likelihood (i.e., the argmax token) and the observed token."
- Break condition: If the language model's representation of likely code patterns is not accurate or if the bugs are subtle and do not significantly deviate from likely patterns, this method may not accurately identify potential bugs

## Foundational Learning

- Concept: Code completion task
  - Why needed here: Understanding the basic code completion task is essential to grasp the nuances of buggy-code completion and why potential bugs are problematic
  - Quick check question: What is the primary goal of a code completion task, and how does it differ from code generation?

- Concept: Potential bugs in code
  - Why needed here: Defining and understanding potential bugs is crucial for grasping the buggy-code completion problem and why it's challenging for Code-LLMs
  - Quick check question: How does the paper define a potential bug, and why are they considered "potential" rather than actual bugs?

- Concept: Likelihood-based measures for bug detection
  - Why needed here: This concept is central to the rewriting→completion method and understanding how the model identifies potential bugs in the code prefix
  - Quick check question: How does the likelihood-based measure calculate the score of a line, and what does a high score indicate?

## Architecture Onboarding

- Component map: Code-LLM -> Bug detection/correction model (optional) -> Code completion -> Bug correction (optional) -> Output
- Critical path:
  1. Input: Problem description and code prefix (potentially buggy)
  2. Bug detection (optional, for post-hoc methods): Identify potential bugs in the code prefix
  3. Code completion: Generate a completion for the code prefix
  4. Bug correction (optional, for post-hoc methods): Fix any potential bugs in the completed code
  5. Output: Completed code
- Design tradeoffs: Using post-hoc methods introduces additional computational overhead but can improve performance on buggy-code completion; the likelihood-based measure for bug detection may not be accurate for all types of bugs or code patterns
- Failure signatures: Low pass@1 scores on buggy-HumanEval and buggy-FixEval datasets; similar completions for clean and buggy code prefixes, indicating the model fails to react to potential bugs; inability to bypass potential bugs in the code prefix, leading to incorrect completions
- First 3 experiments:
  1. Evaluate the performance of Code-LLMs on buggy-HumanEval and buggy-FixEval datasets without any post-hoc methods
  2. Implement and evaluate the completion→rewriting and rewriting→completion post-hoc methods on the same datasets
  3. Compare the performance of different Code-LLM models (e.g., CODEGEN-2B-MONO vs. INCODER-6B) on buggy-code completion with and without post-hoc methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications to Code-LLMs would most effectively enable them to handle buggy-code completion without requiring post-hoc repair methods?
- Basis in paper: [explicit] The paper demonstrates that existing Code-LLMs perform poorly on buggy-code completion, with pass rates dropping below 5% when potential bugs are present. The authors suggest that models may need to be trained or fine-tuned specifically to handle buggy contexts
- Why unresolved: The paper only explores post-hoc methods (completion→rewriting, rewriting→completion) rather than architectural changes to the models themselves. It remains unclear what specific architectural modifications would be most effective
- What evidence would resolve it: Experiments comparing different architectural modifications (e.g., attention mechanisms that are more robust to buggy contexts, specialized training objectives, or architectural additions) on the buggy-HumanEval and buggy-FixEval datasets

### Open Question 2
- Question: How do different types of potential bugs (e.g., operator changes vs. variable misuse vs. logical errors) affect Code-LLM performance differently in buggy-code completion?
- Basis in paper: [explicit] The paper introduces buggy-HumanEval with synthetic bugs from operator changes and buggy-FixEval with realistic bugs from user submissions, noting that performance differs between these datasets
- Why unresolved: While the paper observes performance differences between datasets, it doesn't systematically analyze how different bug types affect performance or what makes certain bug types more challenging than others
- What evidence would resolve it: A controlled study that categorizes bugs by type and systematically measures how each type affects Code-LLM performance, identifying which bug characteristics are most problematic

### Open Question 3
- Question: What is the relationship between the location of potential bugs within the code prefix and the likelihood of successful completion?
- Basis in paper: [explicit] Section 4.5 analyzes how bug location and context size affect completion performance, showing that models perform relatively well when potential bugs appear on or near the last line of partial code
- Why unresolved: While the paper identifies this pattern, it doesn't explain why bug location matters or develop methods to leverage this understanding for better buggy-code completion
- What evidence would resolve it: A deeper analysis of why bug location affects performance (e.g., does it relate to model attention patterns?) and the development of bug-aware completion strategies that adapt based on bug location

### Open Question 4
- Question: Can pre-training strategies be developed to improve Code-LLM robustness to buggy code contexts?
- Basis in paper: [inferred] The paper shows that post-hoc methods can partially mitigate the effects of buggy code, suggesting that training strategies might be more effective. The authors mention that future solutions may need to consider both buggy and clean contexts
- Why unresolved: The paper doesn't explore whether Code-LLMs can be made more robust through pre-training on buggy or mixed clean/buggy code data
- What evidence would resolve it: Experiments comparing Code-LLMs pre-trained on clean code versus those pre-trained on datasets containing buggy code contexts, measuring performance on buggy-code completion tasks

## Limitations
- Evaluation focuses on Python code and mathematical function implementations, limiting generalizability to real-world software development
- The buggy-HumanEval dataset uses synthetic bugs from operator changes, which may not represent the full complexity of real bugs
- Post-hoc methods show promise but still leave significant performance gaps compared to clean partial code completion
- The likelihood-based measure for bug detection may not be accurate for all types of bugs or code patterns

## Confidence

**High Confidence**: The finding that potential bugs degrade Code-LLM performance is supported by multiple experiments across different models (CODEGEN and INCODER variants) and datasets (buggy-HumanEval and buggy-FixEval), showing consistent degradation from over 50% to below 5% pass rates.

**Medium Confidence**: The effectiveness of post-hoc methods is demonstrated, but the performance gap that remains suggests these methods are incomplete solutions. The specific implementation details of the likelihood-based measure for bug detection are not fully specified, which affects reproducibility.

**Low Confidence**: The claim that Code-LLMs fail to recognize semantic changes in buggy code prefixes is supported by evidence that completions remain similar for clean and buggy versions, but the paper doesn't explore alternative explanations such as the possibility that the model recognizes bugs but lacks appropriate repair strategies.

## Next Checks

1. **Dataset Generalization Test**: Evaluate the buggy-code completion performance on additional programming languages and code domains beyond Python mathematical functions to assess the generalizability of the findings across different code patterns and complexity levels.

2. **Model Training Impact Study**: Investigate whether Code-LLMs trained with a higher proportion of buggy code examples or with explicit bug detection capabilities show improved performance on buggy-code completion tasks, testing the assumption that current models are not well-calibrated to handle semantic bugs.

3. **Bug Detection Mechanism Validation**: Implement and test alternative bug detection mechanisms (such as static analysis tools or contrastive learning approaches) to compare their effectiveness against the likelihood-based measure used in the rewriting→completion method, particularly for subtle or complex bugs that may not significantly deviate from likely patterns.