---
ver: rpa2
title: 'MathPile: A Billion-Token-Scale Pretraining Corpus for Math'
arxiv_id: '2312.17120'
source_url: https://arxiv.org/abs/2312.17120
tags:
- data
- math
- language
- have
- corpus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MathPile, a large-scale, high-quality, and
  diverse math-centric pretraining corpus containing approximately 9.5 billion tokens.
  The corpus is constructed by collecting data from various sources including arXiv
  papers, textbooks, Wikipedia, ProofWiki, StackExchange, and web pages, followed
  by extensive preprocessing, language identification, cleaning, filtering, and deduplication
  to ensure quality.
---

# MathPile: A Billion-Token-Scale Pretraining Corpus for Math

## Quick Facts
- arXiv ID: 2312.17120
- Source URL: https://arxiv.org/abs/2312.17120
- Reference count: 40
- Primary result: Introduces MathPile, a 9.5 billion token math-centric pretraining corpus with rigorous quality control

## Executive Summary
MathPile is a large-scale, high-quality math-centric pretraining corpus designed to enhance mathematical reasoning in language models. The corpus contains approximately 9.5 billion tokens collected from diverse sources including arXiv papers, textbooks, Wikipedia, ProofWiki, StackExchange, and web pages. The authors employ extensive preprocessing, language identification, cleaning, filtering, and deduplication to ensure quality, adhering to a "less is more" philosophy that prioritizes quality over quantity. Data contamination detection is performed on benchmark test sets to eliminate duplicates, and the resulting corpus is open-sourced with multiple versions and processing scripts.

## Method Summary
MathPile is constructed by collecting data from diverse math-focused sources including arXiv papers (347,945), textbooks (3,979), Wikipedia entries (106,881), StackExchange questions/answers (433,751), ProofWiki entries (23,839), and CommonCrawl web pages (75,142). The data undergoes extensive preprocessing including format normalization, language identification, custom cleaning rules, and MinHash locality-sensitive hashing for deduplication. Contamination detection is performed through exact line matching against downstream benchmark test sets. The corpus is available in multiple versions (raw, cleaned, deduplicated, benchmark-cleaned) with metadata annotations.

## Key Results
- MATHPILE contains approximately 9.5 billion tokens after extensive preprocessing and quality control
- Deduplication process removed ~714 million tokens from initial 520 billion tokens
- Contamination detection identified 23 MATH and 2 MMLU-STEM duplicates, plus 195 MATH and 65 MMLU-STEM duplicates in OpenWebMath
- Corpus combines diverse sources spanning K-12 to postgraduate mathematical content

## Why This Works (Mechanism)

### Mechanism 1
High-quality, diverse math-centric corpus improves mathematical reasoning in language models. Rigorous preprocessing, filtering, and deduplication remove noise and redundancy, while math-specific content provides domain-relevant training signals. Core assumption: Language models learn generalizable reasoning from large, diverse, high-quality text; removing contamination prevents benchmark data leakage. Evidence anchors: Abstract mentions meticulous data collection and processing; section confirms contamination detection on benchmark test sets; corpus statistics show deduplication removed 714M tokens. Break condition: If corpus still contains substantial noise or benchmark contamination, downstream model performance gains will be limited or invalid.

### Mechanism 2
Math-specific content from diverse sources provides broader and richer training distribution. Integrating formal proofs, problem solutions, definitions, and discussions exposes models to varied mathematical representations and reasoning styles. Core assumption: Models benefit from exposure to multiple math domains (K-12 to postgraduate) and formats (textbooks, research papers, Q&A). Evidence anchors: Abstract describes MATHPILE as diverse and high-quality; section notes previous corpora lack math-specific focus; corpus statistics show integration of 6 distinct source types. Break condition: If source content quality varies widely or if distribution mismatch exists between training and target tasks, benefits may not materialize.

### Mechanism 3
Data contamination detection ensures benchmark integrity and prevents data leakage. Exact-match line-level deduplication between training corpus and benchmark test sets removes leaked examples. Core assumption: Preventing training-test overlap ensures fair evaluation and valid performance measurement. Evidence anchors: Abstract confirms contamination detection on benchmark test sets; section describes exact match classification process; corpus statistics show contamination found in both MATHPILE and OpenWebMath. Break condition: If detection method misses non-exact matches or if test sets are not comprehensive, contamination may persist undetected.

## Foundational Learning

- Concept: Tokenization and n-gram modeling in language models.
  - Why needed here: MATHPILE uses GPTNeoX-20B tokenizer; understanding token boundaries affects deduplication and contamination detection.
  - Quick check question: What token length does GPTNeoX-20B use, and how does it handle mathematical symbols vs. words?

- Concept: MinHash locality-sensitive hashing for large-scale similarity detection.
  - Why needed here: Used for efficient deduplication of near-duplicate documents across sources.
  - Quick check question: How does MinHash with 9,000 minhashes per document ensure low false-positive/negative rates in deduplication?

- Concept: Data contamination and benchmark leakage.
  - Why needed here: MATHPILE performs line-level exact match detection against benchmark test sets to prevent leakage.
  - Quick check question: Why does exact match detection suffice for short math problems but may miss paraphrased contamination?

## Architecture Onboarding

- Component map: arXiv LaTeX parsing -> Textbooks PDF parsing via Mathpix -> Wikipedia HTML→markdown -> StackExchange HTML parsing + scoring -> ProofWiki wiki parsing -> CommonCrawl web filtering -> Language ID -> Custom cleaning -> MinHash deduplication -> Contamination detection -> Final corpus assembly

- Critical path: 1. Source collection → 2. Format normalization → 3. Language ID → 4. Custom cleaning → 5. MinHash deduplication → 6. Contamination detection → 7. Final corpus assembly

- Design tradeoffs:
  - Quality vs. quantity: "Less is more" principle sacrifices token count (~9.5B vs. potential 520B) for higher quality
  - Simple vs. complex filtering: Rule-based cleaning chosen over ML-based to avoid domain mismatch; less precise but more controllable
  - Exact vs. fuzzy deduplication: MinHash LSH balances speed with accuracy; may miss semantic near-duplicates

- Failure signatures:
  - Low deduplication recall → duplicate content remains → overfitting risk
  - High false-positive contamination detection → unnecessary data removal → reduced corpus size
  - Language ID errors → non-English content in corpus → degraded model performance

- First 3 experiments:
  1. Train small language model on raw MATHPILE vs. cleaned MATHPILE; measure perplexity and downstream math task performance
  2. Test contamination detection on synthetic leaked examples (paraphrased vs. exact) to calibrate sensitivity
  3. Vary MinHash parameters (buckets, minhashes) and measure deduplication precision/recall on held-out duplicate set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between corpus size and quality for mathematical pretraining, and at what point does additional data provide diminishing returns in model performance?
- Basis in paper: Explicit - The authors adhere to the principle of "less is more" and emphasize data quality over quantity, even in the pretraining phase, noting that OpenAI's MathMix contains only 1.5 billion tokens despite being effective.
- Why unresolved: The paper does not provide quantitative analysis of how different corpus sizes or quality levels affect downstream mathematical reasoning performance. The relationship between data volume, quality, and model capability remains unexplored.
- What evidence would resolve it: Systematic experiments varying corpus size while maintaining quality, and vice versa, measuring impact on mathematical reasoning benchmarks across different model scales.

### Open Question 2
- Question: How can mathematical pretraining corpora be effectively deduplicated across diverse sources while preserving mathematically equivalent but syntactically different representations?
- Basis in paper: Explicit - The authors encountered challenges with deduplication, noting that ProofWiki had many near-duplicates that were actually different lemmas, proofs, or definitions, and that MinHash LSH was effective but may not capture all relevant similarities.
- Why unresolved: The paper does not explore advanced deduplication techniques beyond MinHash LSH, nor does it address the challenge of identifying mathematically equivalent content expressed differently.
- What evidence would resolve it: Comparative studies of different deduplication algorithms on mathematical corpora, including semantic similarity approaches that can identify equivalent mathematical content.

### Open Question 3
- Question: What is the impact of data contamination from benchmark test sets on mathematical reasoning model evaluation, and how can this be systematically prevented during corpus construction?
- Basis in paper: Explicit - The authors detected and removed exact matches from GSM8K, MATH, and MMLU-STEM benchmarks, finding 23 MATH and 2 MMLU-STEM questions in their corpus, and noted that OpenWebMath had even more contamination.
- Why unresolved: The paper does not quantify the impact of this contamination on model performance or explore automated methods for detecting more subtle forms of contamination beyond exact line matching.
- What evidence would resolve it: Analysis of model performance on contaminated vs. clean versions of the same benchmarks, and development of more sophisticated contamination detection methods for mathematical content.

## Limitations
- The "less is more" quality principle is asserted but not empirically validated through performance comparisons
- Contamination detection relies solely on exact line matching, potentially missing paraphrased or semantically similar content
- Custom cleaning rules are described as "less precise but more controllable," suggesting potential residual noise in the corpus

## Confidence

- **High confidence**: The mechanical process of corpus construction (data collection, deduplication, contamination detection) is well-documented and reproducible. The claim that MATHPILE is a large-scale math-centric corpus is supported by concrete statistics.
- **Medium confidence**: The assertion that MATHPILE's quality improvements will enhance mathematical reasoning in language models. While the methodology is sound, empirical validation on actual model training is limited.
- **Medium confidence**: The claim that the corpus is "diverse and high-quality." While multiple sources are combined and extensive cleaning is performed, the actual quality metrics and diversity analysis are not thoroughly quantified.

## Next Checks

1. **Empirical quality validation**: Train small to medium-sized language models on MATHPILE versus a baseline corpus (e.g., The Pile or OpenWebMath) and measure mathematical reasoning performance on standard benchmarks like MATH, MMLU-STEM, and proof-based tasks. Compare not just final performance but also training dynamics and convergence.

2. **Contamination detection robustness**: Create a synthetic test set containing paraphrased versions of MATHPILE content and evaluate whether the current exact-match contamination detection misses these cases. Test whether models trained on MATHPILE show performance anomalies on benchmarks containing near-duplicates.

3. **Quality vs. quantity tradeoff analysis**: Create controlled experiments ablating different components of the preprocessing pipeline (e.g., skipping deduplication, relaxing cleaning rules) to quantify the impact on model performance. This would validate whether the "less is more" approach actually yields better results than simply using more raw data.