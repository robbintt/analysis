---
ver: rpa2
title: Unleashing the potential of GNNs via Bi-directional Knowledge Transfer
arxiv_id: '2310.17132'
source_url: https://arxiv.org/abs/2310.17132
tags:
- knowledge
- gnns
- training
- graph
- bikt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies that existing graph neural networks (GNNs)
  do not fully exploit the potential of their feature transformation operations. To
  address this, the authors propose Bi-directional Knowledge Transfer (BiKT), a plug-and-play
  method that captures and transfers knowledge between a GNN and its derived MLP-like
  model (obtained by removing the feature propagation operations).
---

# Unleashing the potential of GNNs via Bi-directional Knowledge Transfer

## Quick Facts
- arXiv ID: 2310.17132
- Source URL: https://arxiv.org/abs/2310.17132
- Reference count: 40
- Key outcome: BiKT significantly improves GNN performance (0.5%-4%) and derived MLP performance through bi-directional knowledge transfer

## Executive Summary
This paper identifies that existing graph neural networks (GNNs) do not fully exploit the predictive power of their feature transformation operations when coupled with feature propagation. To address this, the authors propose Bi-directional Knowledge Transfer (BiKT), a plug-and-play method that captures and transfers knowledge between a GNN and its derived MLP-like model (obtained by removing the feature propagation operations). BiKT uses conditional generators to model the representation distributions of both models and then incorporates this knowledge into each other's training through regularization. The method is theoretically shown to improve generalization bounds from a domain adaptation perspective. Extensive experiments on 7 datasets with 5 typical GNN architectures demonstrate that BiKT significantly enhances GNN performance and also boosts the derived MLP's performance, enabling flexible deployment based on task requirements.

## Method Summary
BiKT captures knowledge from both a GNN and its derived MLP model by modeling their representation distributions using conditional generators. The method then transfers this knowledge bi-directionally through regularization during alternating training with parameter inheritance. The GNN maintains its feature propagation operation while the MLP model uses only the feature transformation operation. Generators model the conditional distributions of representations given labels for both models. During training, each model is regularized by the generator from the other model, enabling distribution-level knowledge infusion without altering the base architecture. The process is repeated iteratively with parameter inheritance to progressively refine both models.

## Key Results
- BiKT significantly improves GNN performance by 0.5%-4% across 7 datasets
- The derived MLP model also shows performance gains when trained with BiKT
- Theoretical analysis shows improved generalization bounds from a domain adaptation perspective
- BiKT works consistently across 5 different GNN architectures (GCN, GAT, FAGCN, GCNII, MixHop)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The feature transformation operation (T) in GNNs contains latent predictive power that is not fully exploited when coupled with feature propagation (P).
- Mechanism: By removing the P operation and training an MLP-only model (MLPGNN) that shares parameters with the GNN, we isolate the T operation's capacity. This derived model can sometimes match or exceed the original GNN, revealing that P can inadvertently obscure T's strengths by injecting structural bias.
- Core assumption: The shared parameters between GNN and MLPGNN ensure that T's learned representations are comparable in both architectures, so performance gaps reflect P's influence, not T's limits.
- Evidence anchors:
  - [abstract] "Unexpectedly, we notice that GNNs do not completely free up the power of the inherent feature transformation operation."
  - [section] "Both MLPShareGNN and MLPReGNN exhibit inferior performance compared to the corresponding GNN. It demonstrates that the effective utilization of topology is one of the keys to the excellent performance of GNN... MLPShareGNN outperforms MLPReGNN on the Cora and Citeseer datasets and GNNTMLPShareGNN improves significantly compared to GNNTMLPReGNN."
  - [corpus] No direct evidence found; inference drawn from empirical comparison.
- Break condition: If the derived MLP performs significantly worse even with parameter inheritance, it suggests T alone is insufficient and P is essential for the task.

### Mechanism 2
- Claim: Bi-directional knowledge transfer between GNN and MLPGNN allows each model to compensate for the other's blind spots—GNN brings structural bias, MLPGNN brings feature-only focus.
- Mechanism: Conditional generators model the representation distributions of both architectures. During training, each model is regularized by the generator from the other, enabling distribution-level knowledge infusion without altering the base architecture.
- Core assumption: The representation distribution captured by the generator faithfully reflects the model's learned mapping, so injecting it as a regularizer improves generalization.
- Evidence anchors:
  - [abstract] "BiKT not only allows us to acquire knowledge from both the GNN and its derived model but promotes each other by injecting the knowledge into the other."
  - [section] "BiKT gains the model knowledge by capturing the representation distributions of two models through two generators. Then, we achieve bi-directional knowledge transfer between the host GNN and the derived model, thereby fully unleashing the potentials of P and T in the host GNN."
  - [corpus] No direct evidence found; relies on theoretical generalization bound analysis.
- Break condition: If the regularization term overfits to synthetic samples from the generator, performance may degrade instead of improve.

### Mechanism 3
- Claim: Recurrent training with parameter inheritance progressively refines both models by iteratively alternating their roles as source and target.
- Mechanism: After each training phase, the trained model's parameters initialize the other model. This ensures knowledge gained in one phase is carried forward, avoiding catastrophic forgetting and enabling cumulative improvements.
- Core assumption: Parameter inheritance from a trained model provides better initialization than random initialization, especially for the derived MLP.
- Evidence anchors:
  - [abstract] "The proposed BiKT accomplishes knowledge capture from both the GNN and MLP GNN by modeling their representation distributions, and then progressively infuses the knowledge into each other."
  - [section] "Based on the parameter inheritance, the host GNN and MLPGNN can be trained alternately... This process shares a fascinating similarity indeed with the classical co-training paradigm."
  - [corpus] No direct evidence found; inference from co-training analogy.
- Break condition: If the alternating training causes instability or oscillations in performance, the inheritance schedule or hyperparameters need adjustment.

## Foundational Learning

- Concept: Domain adaptation and generalization bounds
  - Why needed here: The theoretical analysis frames GNN vs. MLP as a domain shift problem (Dgra vs. Df ea), justifying the use of knowledge transfer to improve generalization.
  - Quick check question: Can you explain why modeling q(z|y) helps bridge the gap between structural and feature-only domains?

- Concept: Generative modeling for distribution estimation
  - Why needed here: Generators are used to approximate the induced representation distributions of both models without explicit density estimation.
  - Quick check question: How does the mode-seeking regularization term in the generator objective encourage diversity in generated representations?

- Concept: Co-training and iterative knowledge refinement
  - Why needed here: The recurrent training strategy mirrors co-training, where two models teach each other to improve performance iteratively.
  - Quick check question: What would happen if you skip parameter inheritance and reinitialize the MLP from scratch each iteration?

## Architecture Onboarding

- Component map: Host GNN -> Derived MLPGNN -> Generators (Ggnn, Gmlp) -> Classifiers (fcls) -> Regularization terms

- Critical path:
  1. Train base GNN on labeled data
  2. Initialize MLPGNN with GNN parameters
  3. Train generators for both models using representation samples
  4. Alternate training: GNN ← MLPGNN's generator, MLPGNN ← GNN's generator
  5. Iterate with parameter inheritance

- Design tradeoffs:
  - Complexity vs. Performance: Adding generators increases training time but can yield significant gains
  - Hyperparameter sensitivity: Coefficients α and β control regularization strength; improper values can destabilize training
  - Distribution modeling fidelity: Poor generator fit can lead to noisy regularization

- Failure signatures:
  - Generator collapse: If the generator only produces a narrow set of representations, regularization becomes ineffective
  - Over-regularization: Too strong Lki or Lps can cause underfitting to the original task
  - Oscillation: Alternating training without proper inheritance may cause performance to fluctuate

- First 3 experiments:
  1. Baseline: Train GNN and MLPGNN independently; compare performance
  2. One-way transfer: Train MLPGNN with GNN's generator only; evaluate improvement
  3. Full BiKT: Implement recurrent training with parameter inheritance; measure gains over baseline and one-way transfer

## Open Questions the Paper Calls Out
None explicitly called out in the provided content.

## Limitations
- Theoretical analysis assumes certain regularity conditions on representation distributions that are not fully validated empirically
- Method introduces additional computational overhead through generator networks and recurrent training process
- Evaluation focuses primarily on node classification tasks, leaving open questions about applicability to other graph learning problems

## Confidence

- Mechanism 1 (MLP isolation revealing T operation potential): Medium - supported by comparative experiments but the claim that P "obscures" T's strengths is inferential
- Mechanism 2 (Bi-directional transfer effectiveness): High - extensive experimental validation across multiple datasets and architectures shows consistent improvements
- Mechanism 3 (Recurrent training benefits): Medium - the co-training analogy is compelling but specific ablation studies on iteration count and inheritance strategies are limited

## Next Checks

1. Conduct controlled experiments varying the number of recurrent training iterations to determine optimal stopping criteria and assess convergence behavior
2. Perform ablation studies removing parameter inheritance to quantify its contribution versus random initialization
3. Evaluate generator quality through quantitative metrics (e.g., MMD distance) between generated and actual representations to validate the distribution modeling assumption