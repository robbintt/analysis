---
ver: rpa2
title: 'Direction-oriented Multi-objective Learning: Simple and Provable Stochastic
  Algorithms'
arxiv_id: '2305.18409'
source_url: https://arxiv.org/abs/2305.18409
tags:
- learning
- sdmgrad
- gradient
- stochastic
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Stochastic Direction-oriented Multi-objective
  Gradient descent (SDMGrad), a new method for stochastic multi-objective optimization
  (MOO) with provable convergence guarantees. The key idea is to regularize the common
  descent direction within a neighborhood of a direction optimizing a linear combination
  of objectives, which includes GD and MGDA as special cases.
---

# Direction-oriented Multi-objective Learning: Simple and Provable Stochastic Algorithms

## Quick Facts
- arXiv ID: 2305.18409
- Source URL: https://arxiv.org/abs/2305.18409
- Reference count: 40
- Primary result: SDMGrad achieves O(Kϵ⁻⁸) sample complexity per objective for nonconvex MOO, improving over MoCo's O(ϵ⁻¹⁰)

## Executive Summary
This paper introduces Stochastic Direction-oriented Multi-objective Gradient descent (SDMGrad), a new method for stochastic multi-objective optimization with provable convergence guarantees. The key innovation is regularizing the common descent direction within a neighborhood of a direction optimizing a linear combination of objectives, which includes standard methods like GD and MGDA as special cases. The proposed SDMGrad algorithm uses simple SGD-type updates and achieves improved sample complexities compared to prior works. The paper also introduces SDMGrad-OS, an efficient variant using objective sampling for large-scale problems. Extensive experiments validate the effectiveness of both methods across multi-task supervised learning and reinforcement learning benchmarks.

## Method Summary
The paper proposes a new multi-objective optimization framework that regularizes the common descent direction to ensure convergence to Pareto stationary points. The method consists of two loops: an inner loop that optimizes weights via projected SGD with quadratic regularization, and an outer loop that updates model parameters using a weighted combination of gradients. For large-scale problems, SDMGrad-OS randomly samples a subset of objectives at each iteration to reduce computational cost. The algorithms are designed to work with nonconvex objectives and provide provable convergence guarantees under standard assumptions.

## Key Results
- Achieves O(Kϵ⁻⁸) sample complexity per objective for nonconvex MOO, improving over MoCo's O(ϵ⁻¹⁰)
- SDMGrad-OS scales to large numbers of objectives while maintaining similar performance to full SDMGrad
- Outperforms baselines (MGDA, PCGrad, CAGrad, MoCo) on multi-task supervised learning and reinforcement learning benchmarks
- Converges to Pareto stationary points with theoretical guarantees under bounded gradient assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The regularization term λ⟨g₀, d⟩ in the direction-oriented objective keeps the common descent direction close to the average gradient, ensuring convergence to Pareto stationary points.
- Mechanism: By penalizing the deviation of the common descent direction d from the average gradient g₀, the method enforces a bias toward directions that improve all objectives simultaneously. This bias reduces the risk of getting stuck in regions where one objective dominates.
- Core assumption: The regularization strength λ is chosen appropriately (constant or increasing) to balance the trade-off between improving individual objectives and optimizing the linear combination L₀(θ).
- Evidence anchors:
  - [abstract] "by regularizing the common descent direction within a neighborhood of a direction that optimizes a linear combination of objectives"
  - [section 4.1] "we propose the following multi-objective problem formulation by adding an inner-product regularization λ⟨g₀, d⟩"
- Break condition: If λ is too large, the method may converge to a stationary point of L₀(θ) rather than a true Pareto stationary point. If λ is too small, the bias may be insufficient to ensure convergence.

### Mechanism 2
- Claim: The quadratic regularization term ρ∥w∥² in the weight update ensures that the last iterate converges to the optimal weights w*_λ, rather than just an average of iterates.
- Mechanism: The strong convexity induced by the ρ term guarantees that projected SGD on the weights converges to the unique minimizer of the regularized objective. This avoids the issue of only the average of iterates converging, which is typical for non-strongly convex objectives.
- Core assumption: The stepsize β and regularization parameter ρ are chosen small enough to ensure convergence but large enough to avoid numerical instability.
- Evidence anchors:
  - [section 4.2] "we add a quadratic regularization term in eq. (8), and optimize its smoothed version as"
  - [section 5.2] "Note that w*_ρ,λ approximates the original solution w*_λ up to a small level of ρ"
- Break condition: If ρ is too large, the solution may be overly biased toward the regularization term, reducing the effectiveness of the weight optimization.

### Mechanism 3
- Claim: The objective sampling strategy in SDMGrad-OS allows the method to scale to a large number of objectives by reducing the per-iteration computational cost.
- Mechanism: By randomly sampling a subset of objectives at each iteration, the method approximates the full gradient matrix G(θ) with a sparse matrix H(θ) that has only n non-zero columns. This reduces the computational complexity from O(K) to O(n) per iteration.
- Core assumption: The number of sampled objectives n is chosen such that the approximation error is small enough not to affect convergence.
- Evidence anchors:
  - [section 4.3] "we propose SDMGrad-OS by replacing the gradient matrix G(θt; ξ) in Algorithm 1 by a matrix H(θt; ξ) with randomly sampled columns"
  - [section 5.2] "Theorem 2 establishes the convergence guarantee for our SDMGrad-OS algorithm, which achieves a per-objective sample complexity of O(ϵ⁻⁸)"
- Break condition: If n is too small, the approximation error may be too large, leading to poor convergence. If n is too large, the computational savings are diminished.

## Foundational Learning

- Concept: Multi-objective optimization (MOO)
  - Why needed here: The paper addresses the problem of optimizing multiple objectives simultaneously, which is a key challenge in machine learning applications like multi-task learning and reinforcement learning.
  - Quick check question: What is a Pareto stationary point, and why is it the target of MOO algorithms?

- Concept: Stochastic optimization
  - Why needed here: The paper proposes stochastic algorithms for MOO, which are necessary when the objectives are defined by expectations over data or environments.
  - Quick check question: What is the difference between a stochastic gradient and a full gradient, and why is unbiased estimation important?

- Concept: Projected gradient descent
  - Why needed here: The weight updates in SDMGrad are performed using projected gradient descent on the probability simplex, which ensures that the weights remain valid probabilities.
  - Quick check question: What is the projection operation onto the probability simplex, and why is it necessary for the weight updates?

## Architecture Onboarding

- Component map: Stochastic gradients → Weight optimization (projected SGD) → Parameter update (weighted gradient combination)
- Critical path:
  1. Compute stochastic gradients for all objectives
  2. Optimize weights w using projected SGD with quadratic regularization
  3. Update model parameters θ using the weighted combination of gradients
- Design tradeoffs:
  - Choosing λ: Balances between improving individual objectives and optimizing the linear combination L₀(θ)
  - Choosing ρ: Affects the convergence of the weight optimization and the bias of the solution
  - Choosing S: Determines the number of weight updates per parameter update
- Failure signatures:
  - If the method converges to a point that is not Pareto stationary, it may be due to an inappropriate choice of λ
  - If the method exhibits high variance or instability, it may be due to a poor choice of stepsizes or regularization parameters
  - If the method is slow to converge, it may be due to a large number of objectives or a poor initialization
- First 3 experiments:
  1. Implement the SDMGrad algorithm and verify that it converges to a Pareto stationary point on a simple two-objective toy problem
  2. Compare the performance of SDMGrad with different choices of λ on a multi-task learning benchmark
  3. Implement the SDMGrad-OS algorithm and verify that it scales to a large number of objectives while maintaining similar performance to SDMGrad

## Open Questions the Paper Calls Out

- Open Question 1: How does the performance of SDMGrad and SDMGrad-OS scale with the number of objectives K in extremely high-dimensional MOO problems?
  - Basis in paper: [inferred] The paper demonstrates improved sample complexity compared to MoCo but does not explicitly analyze scalability with very large K.
  - Why unresolved: The paper focuses on theoretical and empirical validation for moderate K, leaving scalability in the large K regime unexplored.
  - What evidence would resolve it: Empirical studies testing SDMGrad/OS on problems with thousands of objectives, along with theoretical bounds on computational and memory requirements.

- Open Question 2: Can SDMGrad/OS be extended to handle non-convex constraints in the MOO problem?
  - Basis in paper: [explicit] The paper assumes unconstrained optimization and does not address constrained MOO.
  - Why unresolved: The regularization framework may not directly generalize to non-convex constraints, and convergence analysis would require new techniques.
  - What evidence would resolve it: Development and theoretical analysis of SDMGrad/OS variants for constrained MOO, with empirical validation on constrained benchmark problems.

- Open Question 3: How does the choice of regularization parameter λ affect the trade-off between convergence speed and solution quality in practice?
  - Basis in paper: [explicit] The paper discusses λ's role in balancing individual objectives and the linear combination but does not provide a systematic study of its practical impact.
  - Why unresolved: The optimal λ may depend on the problem structure and is not explored in depth.
  - What evidence would resolve it: Comprehensive empirical study varying λ across diverse MOO problems, identifying patterns in its impact on convergence and solution quality.

- Open Question 4: Can the direction-oriented regularization be combined with other MOO techniques like adaptive learning rates or momentum?
  - Basis in paper: [inferred] The paper uses standard SGD updates, leaving room for integrating advanced optimization techniques.
  - Why unresolved: The interplay between regularization and other optimization components is not explored.
  - What evidence would resolve it: Empirical comparison of SDMGrad/OS with and without adaptive learning rates/momentum, along with theoretical analysis of their combined effects.

## Limitations

- The theoretical analysis relies on bounded gradient assumptions that may not hold for deep neural networks in practice
- The choice of λ and ρ hyperparameters is not thoroughly explored - sensitivity analysis is limited to two fixed values
- The connection between convergence to Pareto stationary points and practical performance improvements needs more empirical validation

## Confidence

- High confidence: The SGD-type update structure and projected gradient descent on weights are correctly implemented
- Medium confidence: The sample complexity bounds are valid under stated assumptions, but practical relevance depends on problem structure
- Medium confidence: The objective sampling strategy in SDMGrad-OS provides computational benefits, though the approximation quality trade-off needs more exploration

## Next Checks

1. Test SDMGrad on problems where Pareto stationary points are known analytically to verify convergence properties
2. Conduct systematic hyperparameter sweeps for λ and ρ across different problem scales and objective counts
3. Compare the approximation error of SDMGrad-OS against full gradient computation as a function of sampling rate n/K