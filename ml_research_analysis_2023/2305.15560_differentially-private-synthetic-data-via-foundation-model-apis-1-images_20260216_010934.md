---
ver: rpa2
title: 'Differentially Private Synthetic Data via Foundation Model APIs 1: Images'
arxiv_id: '2305.15560'
source_url: https://arxiv.org/abs/2305.15560
tags:
- private
- samples
- data
- images
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of generating differentially
  private (DP) synthetic data using only blackbox APIs of foundation models, without
  any model training. The proposed Private Evolution (PE) framework treats foundation
  models as blackboxes and iteratively guides the generation of synthetic data towards
  the private distribution through a process inspired by evolutionary algorithms.
---

# Differentially Private Synthetic Data via Foundation Model APIs 1: Images

## Quick Facts
- arXiv ID: 2305.15560
- Source URL: https://arxiv.org/abs/2305.15560
- Reference count: 40
- Key outcome: Private Evolution (PE) framework generates DP synthetic images via foundation model APIs, achieving SOTA privacy-utility trade-off with FID ≤ 7.9 at ε = 0.67 on CIFAR10

## Executive Summary
This paper introduces Private Evolution (PE), a novel framework for generating differentially private synthetic images using only blackbox foundation model APIs without any model training. PE iteratively guides generation toward the private data distribution using a DP nearest neighbors histogram to select promising samples. Experiments demonstrate that PE significantly outperforms training-based DP methods, achieving state-of-the-art privacy-utility trade-offs on CIFAR10 and Camelyon17 datasets. The framework can leverage powerful foundation models like Stable Diffusion to handle challenging datasets with high-resolution images.

## Method Summary
Private Evolution treats foundation models as blackboxes and iteratively guides synthetic data generation toward the private distribution. The algorithm maintains a population of generated samples, computes a DP nearest neighbors histogram where each private sample votes for its closest generated sample, adds Gaussian noise to ensure privacy, and selects samples above a threshold for the next iteration. New samples are generated via variation APIs applied to selected parents. The process repeats for a fixed number of iterations. Theoretical analysis shows convergence under assumptions about the intrinsic dimension of image embeddings, while experiments demonstrate practical effectiveness across multiple datasets and foundation models.

## Key Results
- Achieves FID ≤ 7.9 with privacy cost ε = 0.67 on CIFAR10, improving previous SOTA from ε = 32
- Successfully handles challenging Camelyon17 dataset with 96×96 high-resolution images using Stable Diffusion
- Demonstrates effectiveness across different foundation models including ImageNet-pretrained diffusion models and Stable Diffusion
- Shows significant improvements in privacy-utility trade-off compared to training-based DP methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Private Evolution can guide the generation of synthetic data towards the private distribution by iteratively selecting and mutating samples using DP nearest neighbors histogram.
- Mechanism: The algorithm maintains a population of generated samples, computes a DP histogram of nearest neighbors to private samples, selects the most similar samples as parents, and generates variations of these parents to form the next population. This iterative process gradually shifts the distribution of generated samples towards the private distribution.
- Core assumption: Foundation models have a broad and general model of the world from their extensive training data, so they can generate samples close to private data with non-negligible probability.
- Evidence anchors:
  - [abstract]: "The proposed Private Evolution (PE) framework treats foundation models as blackboxes and iteratively guides the generation of synthetic data towards the private distribution through a process inspired by evolutionary algorithms."
  - [section]: "The key idea is to iteratively use private samples to vote for the most similar samples generated from the blackbox model and ask the blackbox models to generate more of those similar samples."
- Break condition: If the foundation model's training data has very little overlap with the private data distribution, or if the distribution shift is too large, the algorithm may fail to converge to the private distribution.

### Mechanism 2
- Claim: The DP Nearest Neighbors Histogram with thresholding can effectively identify and select useful samples from the generated population while preserving differential privacy.
- Mechanism: For each private sample, the algorithm finds its nearest neighbor in the generated population. It then adds Gaussian noise to these counts and applies a threshold to reduce the impact of noise. Samples with counts above the threshold are selected as parents for the next iteration.
- Core assumption: The noise added to the histogram counts is sufficient to ensure differential privacy while still allowing useful samples to be identified.
- Evidence anchors:
  - [abstract]: "PE uses APIs for random generation and sample variation, and incorporates a DP nearest neighbors histogram to select the most promising samples for the next iteration."
  - [section]: "We add i.i.d. Gaussian noise from N (0, σ). The privacy analysis is presented in § 4.2."
- Break condition: If the noise level σ is too high, or the threshold H is set incorrectly, the algorithm may fail to select useful samples, leading to poor convergence.

### Mechanism 3
- Claim: The convergence of Private Evolution is theoretically supported by the analysis of non-private evolution and the concept of intrinsic dimension of image embeddings.
- Mechanism: The paper proves that non-private evolution converges to the private distribution in O(d) iterations, where d is the dimension of the embedding space. It then argues that Private Evolution with noise can also converge under certain conditions. Additionally, it suggests that the intrinsic dimension of image embeddings (which is much smaller than the embedding dimension) is what matters for convergence in practice.
- Core assumption: The intrinsic dimension of the manifold of realistic images in the embedding space is much smaller than the embedding dimension, which allows for faster convergence.
- Evidence anchors:
  - [abstract]: "We theoretically prove that the distribution of the generated samples from PE will converge to the private distribution under some modeling assumptions (§ 5)."
  - [section]: "We offer one plausible explanation for this via intrinsic dimension. Suppose the (embeddings of) realistic images lie on a low dimensional manifold M inside Rd of dimension dintrinsic ≪ d."
- Break condition: If the intrinsic dimension is not much smaller than the embedding dimension, or if the modeling assumptions do not hold, the theoretical convergence guarantees may not apply.

## Foundational Learning

- Concept: Differential Privacy (DP)
  - Why needed here: DP is the core privacy guarantee that the paper aims to achieve. Understanding DP is crucial for grasping the privacy analysis and the need for techniques like DP nearest neighbors histogram.
  - Quick check question: What is the definition of (ε, δ)-differential privacy, and how does it ensure that the output of an algorithm does not reveal too much information about any individual in the input data?

- Concept: Wasserstein Distance
  - Why needed here: Wasserstein distance is used as the metric to measure the similarity between the distribution of generated samples and the private data distribution. Understanding Wasserstein distance is important for interpreting the experimental results and the theoretical analysis.
  - Quick check question: What is the definition of Wasserstein distance, and how does it differ from other distance metrics like KL divergence in the context of comparing probability distributions?

- Concept: Generative Models and Diffusion Models
  - Why needed here: The paper relies on pre-trained generative models (specifically diffusion models) to generate synthetic data. Understanding how these models work and their capabilities is essential for understanding the paper's approach and the potential of using foundation models for DP synthetic data generation.
  - Quick check question: What is the basic principle behind diffusion models, and how do they differ from other generative models like GANs or VAEs in terms of their training process and the quality of generated samples?

## Architecture Onboarding

- Component map:
  1. Private Evolution (PE) Framework: The main algorithm that iteratively generates and selects samples.
  2. DP Nearest Neighbors Histogram: A component that computes a noisy histogram of nearest neighbors to private samples.
  3. Foundation Model APIs: The blackbox APIs used for random generation and sample variation.
  4. Distance Function: A function that measures the similarity between samples, typically using image embeddings.
  5. Hyperparameters: Various parameters that control the behavior of the algorithm, such as the number of iterations, noise level, and threshold.

- Critical path:
  1. Initialize population using RANDOM_API.
  2. Compute DP nearest neighbors histogram.
  3. Select parents based on the histogram.
  4. Generate variations of parents using VARIATION_API.
  5. Repeat steps 2-4 for a fixed number of iterations.

- Design tradeoffs:
  1. Privacy vs. Utility: Increasing the noise level (σ) improves privacy but may reduce the quality of the generated samples.
  2. Number of Iterations vs. Convergence: More iterations may lead to better convergence but increase computational cost and privacy budget.
  3. Threshold vs. Signal-to-Noise Ratio: A higher threshold reduces the impact of noise but may also filter out useful samples.

- Failure signatures:
  1. Poor Convergence: If the generated samples do not resemble the private data, it may indicate that the distribution shift is too large or the hyperparameters are not set correctly.
  2. High Privacy Cost: If the privacy budget is exhausted quickly, it may suggest that the noise level is too low or the number of iterations is too high.
  3. Low Sample Quality: If the generated samples are of poor quality, it may indicate that the foundation model is not suitable for the task or the distance function is not well-chosen.

- First 3 experiments:
  1. Reproduce the CIFAR10 experiments with unconditional generation to verify the basic functionality of the algorithm.
  2. Experiment with different hyperparameters (e.g., noise level, threshold) to understand their impact on the results.
  3. Try conditional generation on CIFAR10 to explore the flexibility of the algorithm.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Private Evolution's performance compare to training-based methods on datasets with limited diversity or rare classes?
- Basis in paper: [inferred] The paper focuses on CIFAR10 and Camelyon17, which have moderate to large distribution shifts. The effectiveness of PE on datasets with limited diversity or rare classes is not explicitly addressed.
- Why unresolved: The paper does not provide experimental results or analysis on such datasets.
- What evidence would resolve it: Experiments on datasets with limited diversity or rare classes comparing PE's performance to training-based methods.

### Open Question 2
- Question: What is the impact of the lookahead degree on Private Evolution's convergence speed and final performance?
- Basis in paper: [explicit] The paper mentions that higher lookahead degrees improve FID score but does not provide a detailed analysis of the impact on convergence speed and final performance.
- Why unresolved: The paper does not provide a comprehensive study on the relationship between lookahead degree and convergence speed or final performance.
- What evidence would resolve it: Experiments varying the lookahead degree and analyzing its impact on convergence speed and final performance.

### Open Question 3
- Question: How sensitive is Private Evolution to the choice of distance function in the embedding space?
- Basis in paper: [explicit] The paper uses ℓ2 distance in the embedding space but does not explore other distance functions or analyze the sensitivity of PE to the choice of distance function.
- Why unresolved: The paper does not provide a detailed analysis of the impact of different distance functions on PE's performance.
- What evidence would resolve it: Experiments comparing PE's performance using different distance functions in the embedding space.

## Limitations
- Theoretical convergence analysis relies on assumptions about intrinsic dimension that may not hold for all datasets
- Experimental validation limited to relatively simple image datasets (CIFAR10) and one medical dataset
- Performance on datasets with limited diversity or rare classes remains unexplored

## Confidence
- High Confidence: The core algorithmic framework (PE with DP nearest neighbors histogram) is technically sound and well-explained
- Medium Confidence: Experimental results showing state-of-the-art performance on CIFAR10 and Camelyon17
- Medium Confidence: Theoretical convergence analysis, though dependent on assumptions about intrinsic dimension

## Next Checks
1. Reproduce CIFAR10 experiments: Implement PE from scratch and verify FID scores at different privacy budgets (ε=0.67, 2.56, 32) to confirm reported improvements
2. Test on more diverse datasets: Evaluate PE on additional image datasets (e.g., ImageNet subsets, CelebA) to assess scalability and generalizability
3. Ablation study on hyperparameters: Systematically vary noise level σ, threshold H, and population size to understand their impact on convergence and sample quality