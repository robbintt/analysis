---
ver: rpa2
title: Probing Pretrained Language Models with Hierarchy Properties
arxiv_id: '2312.09670'
source_url: https://arxiv.org/abs/2312.09670
tags:
- hierarchy
- plms
- properties
- representations
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limited understanding of hierarchical
  semantic knowledge in pretrained language models (PLMs). The authors propose a task-agnostic
  methodology to evaluate and enhance PLMs' understanding of complex taxonomic relations,
  such as ancestors and siblings.
---

# Probing Pretrained Language Models with Hierarchy Properties

## Quick Facts
- arXiv ID: 2312.09670
- Source URL: https://arxiv.org/abs/2312.09670
- Authors: 
- Reference count: 40
- Key outcome: PLMs struggle to capture hierarchical relations, with hierarchy-enhanced models showing moderate downstream transfer.

## Executive Summary
This paper addresses the limited understanding of hierarchical semantic knowledge in pretrained language models (PLMs). The authors propose a task-agnostic methodology to evaluate and enhance PLMs' understanding of complex taxonomic relations, such as ancestors and siblings. They define intrinsic hierarchy properties based on edge-distance observations in taxonomies and design probes to encode these properties. The evaluation shows that PLMs struggle to capture hierarchical relations, with siblings and ancestor representations being particularly challenging. By fine-tuning PLMs with these probes, the authors demonstrate that hierarchy properties can be injected into PLMs, improving their performance on the evaluation. However, the transferability of this knowledge to downstream tasks is moderate, with hierarchy-enhanced PLMs showing improvements in hypernym discovery and taxonomy reconstruction but not in reading comprehension.

## Method Summary
The authors propose a task-agnostic methodology to evaluate and enhance PLMs' understanding of hierarchical semantic knowledge. They define six hierarchy properties based on edge-distance observations in taxonomies (parent-ancestor, parent-sibling, etc.) and design ternary probes to encode these properties. The evaluation involves fine-tuning PLMs using contrastive triplet loss on these probes to enforce the hierarchy property inequalities. The hierarchy-enhanced PLMs are then evaluated on probe test sets and downstream tasks such as hypernym discovery, taxonomy reconstruction, and reading comprehension to assess the effectiveness of the approach.

## Key Results
- PLMs struggle to capture hierarchical relations, particularly siblings and ancestors.
- Hierarchy properties can be injected into PLMs through fine-tuning, improving probe performance.
- Transferability of hierarchy knowledge to downstream tasks is moderate, with improvements in hypernym discovery and taxonomy reconstruction but not in reading comprehension.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-agnostic probing with ternary-based probes captures hierarchy properties that task-dependent methods miss.
- Mechanism: By encoding three entities in a ternary and defining edge-distance-based properties (e.g., P-A: parent closer than ancestor), the method isolates hierarchy knowledge from task performance.
- Core assumption: Edge-based distance in a taxonomy correlates with semantic similarity as measured by cosine distance in PLM embeddings.
- Evidence anchors:
  - [abstract] "Traditionally, evaluating such knowledge encoded in PLMs relies on their performance on a task-dependent evaluation approach based on proxy tasks, such as hypernymy detection."
  - [section 3.1] "We use aprobe-based methodology [36] to evaluate whether SOTA models capture task-agnostic linguistic knowledge."
  - [corpus] Weak evidence; no direct comparison with task-dependent baselines in cited papers.
- Break condition: If semantic similarity does not correlate with edge-distance (e.g., siblings appear closer than parents in embedding space), the properties fail to capture hierarchy.

### Mechanism 2
- Claim: Fine-tuning PLMs with triplet loss on probes improves their hierarchy-aware representations.
- Mechanism: The triplet loss minimizes distance between related entities (xn, xl) and maximizes it between unrelated (xn, xr), enforcing the hierarchy property inequality.
- Core assumption: Contrastive learning on ternary probes can reshape embedding space to reflect taxonomy structure.
- Evidence anchors:
  - [section 3.2] "We employ the Sentence Transformer framework [38] with a triplet network architecture and a pooling operation to the output of the PLM to generate the embedding of our concepts."
  - [section 5.2] "Our results showed improvement in all models for the All column w.r.t the original PLM."
  - [corpus] No explicit evidence of long-term retention or generalization beyond probe data.
- Break condition: If fine-tuning overfits to probe distribution and degrades performance on natural data or downstream tasks.

### Mechanism 3
- Claim: Hierarchy-enhanced PLMs transfer moderately to downstream tasks that require taxonomic reasoning.
- Mechanism: Improved representations of parent/sibling/ancestor relations in probe space lead to better feature extraction for tasks like hypernym discovery and taxonomy reconstruction.
- Core assumption: Probe-induced hierarchy knowledge aligns with task-specific hierarchy needs.
- Evidence anchors:
  - [section 5.3] "Our empirical findings suggest that the enhanced PLM representations are moderately transferable in a sequential mode of probe then fine-tune for hierarchy-aware tasks such as Hypernymy Discovery and Taxonomy Reconstruction, but detrimental for Reading Comprehension."
  - [section 1] "We further demonstrate that the proposed properties can be injected into PLMs to improve their understanding of hierarchy."
  - [corpus] No evidence that hierarchy knowledge is retained after sequential fine-tuning on non-hierarchy tasks.
- Break condition: If hierarchy knowledge is catastrophically forgotten during downstream task fine-tuning.

## Foundational Learning

- Concept: Task-agnostic evaluation vs. task-dependent evaluation
  - Why needed here: To understand why probes are designed independently of downstream tasks.
  - Quick check question: Why might task-dependent evaluation conflate hierarchy understanding with task-specific performance?

- Concept: Edge-distance in taxonomies and its semantic interpretation
  - Why needed here: Core to defining the six hierarchy properties (P-A, P-S, etc.).
  - Quick check question: In a taxonomy, if node A is parent of node B and node C is sibling of B, what is the edge distance between A and C?

- Concept: Triplet loss and contrastive learning
  - Why needed here: Method used to fine-tune PLMs to satisfy hierarchy property inequalities.
  - Quick check question: What is the effect of the margin α in the triplet loss formula on embedding separation?

## Architecture Onboarding

- Component map:
  - Taxonomy data → Ternary generation → Probe dataset
  - PLM + pooling → Representation extraction → Distance computation
  - Triplet network + triplet loss → Fine-tuning → Hierarchy-enhanced PLM
  - Enhanced PLM → Downstream task evaluation (hypernym discovery, taxonomy reconstruction, reading comprehension)

- Critical path:
  1. Load taxonomy and generate ternaries for each hierarchy property.
  2. Convert ternaries into textual prompts and embed using PLM.
  3. Evaluate property satisfaction via cosine distance.
  4. Fine-tune with triplet loss on training probes.
  5. Evaluate on test probes and downstream tasks.

- Design tradeoffs:
  - Using only edge-distance vs. incorporating semantic similarity metrics from WordNet.
  - Ternary vs. binary relation encoding (ternary allows richer property definition).
  - Contrastive fine-tuning vs. multi-task learning with hierarchy properties.

- Failure signatures:
  - Probe accuracy low across all properties → Probe design or distance metric mismatch.
  - Fine-tuning improves probe performance but not downstream tasks → Representation drift or misalignment.
  - Fine-tuning degrades original PLM performance → Overfitting or catastrophic forgetting.

- First 3 experiments:
  1. Evaluate baseline PLMs on probe test set to confirm hierarchy gaps.
  2. Fine-tune a PLM on P-A probes and evaluate both probe and downstream task performance.
  3. Compare fine-tuned PLM on hypernym discovery with and without hierarchy-enhanced representations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can hierarchy properties be effectively transferred to downstream tasks beyond taxonomy reconstruction and hypernym discovery?
- Basis in paper: [explicit] The paper notes that hierarchy-enhanced PLMs show moderate transferability to downstream tasks, with improvements in taxonomy reconstruction and hypernym discovery but not in reading comprehension.
- Why unresolved: The study highlights a gap in understanding the transferability of hierarchical knowledge to tasks like reading comprehension, where the hierarchy knowledge is either forgotten or penalized.
- What evidence would resolve it: Conducting experiments with different transfer learning methods, such as adding hierarchy-aware auxiliary losses, could provide insights into improving the transferability of hierarchy properties to a broader range of downstream tasks.

### Open Question 2
- Question: What are the limitations of current evaluation methodologies in assessing the understanding of complex hierarchical relations by PLMs?
- Basis in paper: [explicit] The paper discusses the limitations of task-dependent evaluations that rely on detecting a single relation type (hypernymy) and overlook implicit taxonomy structures.
- Why unresolved: Existing evaluations do not fully capture the implicit and complex taxonomic relations such as ancestors and siblings, which are crucial for understanding hierarchy.
- What evidence would resolve it: Developing more comprehensive evaluation methods and datasets that consider complex hierarchical relations beyond hypernymy would help in revealing how well models capture and apply these relations.

### Open Question 3
- Question: How do different representation methods (e.g., vector-based vs. prompt-based) impact the ability of PLMs to encode hierarchical knowledge?
- Basis in paper: [explicit] The paper explores various representation methods, finding that vector-based methods outperform prompt-based methods in capturing hierarchical relations.
- Why unresolved: While vector-based methods show better performance, the study does not fully explore the potential of prompt-based methods or other innovative approaches.
- What evidence would resolve it: Further experimentation with different representation methods, including novel prompt designs or hybrid approaches, could provide a clearer understanding of their impact on encoding hierarchical knowledge.

## Limitations
- Evaluation focuses on WordNet-based taxonomies, which may not capture real-world hierarchical complexity.
- Moderate transferability of hierarchy knowledge to downstream tasks, with degradation in reading comprehension.
- Limited comparison with alternative probing methods and downstream tasks.

## Confidence

- High Confidence: The observation that PLMs struggle with sibling and ancestor relationships (supported by consistent probe results across multiple PLMs)
- Medium Confidence: The effectiveness of ternary probes for capturing hierarchy properties (limited comparison with alternative probing methods)
- Medium Confidence: The moderate transferability to downstream tasks (small sample of tasks tested, no ablation studies on different fine-tuning strategies)

## Next Checks
1. Test the probe methodology on non-WordNet taxonomies (e.g., medical ontologies, product hierarchies) to assess generalizability beyond linguistic domains
2. Conduct ablation studies varying the margin α in triplet loss and different pooling strategies to optimize hierarchy property satisfaction
3. Evaluate the impact of probe fine-tuning on a broader range of downstream tasks including few-shot learning scenarios to better understand transferability conditions