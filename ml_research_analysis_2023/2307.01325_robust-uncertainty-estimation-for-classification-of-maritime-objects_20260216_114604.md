---
ver: rpa2
title: Robust Uncertainty Estimation for Classification of Maritime Objects
arxiv_id: '2307.01325'
source_url: https://arxiv.org/abs/2307.01325
tags:
- uncertainty
- dataset
- samples
- detection
- work
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of uncertainty estimation for
  maritime object classification, particularly for autonomous vessels. The authors
  propose a method combining Monte Carlo Dropout for aleatoric (intra-class) uncertainty
  and Virtual Outlier Synthesis for epistemic (out-of-distribution) uncertainty.
---

# Robust Uncertainty Estimation for Classification of Maritime Objects

## Quick Facts
- arXiv ID: 2307.01325
- Source URL: https://arxiv.org/abs/2307.01325
- Reference count: 40
- Key outcome: Proposed method improves FPR95 by 8% over current highest-performing work (no OOD training) and 77% over vanilla Wide ResNet; 44.2% improvement on SHIPS dataset.

## Executive Summary
This paper tackles uncertainty estimation for maritime object classification in autonomous vessels by combining Monte Carlo Dropout for aleatoric uncertainty with Virtual Outlier Synthesis for epistemic uncertainty. The authors introduce a method that synthesizes virtual outliers from class-conditional distributions to improve out-of-distribution detection without requiring large OOD datasets. Tested on a custom SHIPS dataset and compared against CIFAR10 with six outlier datasets, their approach significantly improves false positive rates at 95% true positive rate. The work releases the SHIPS dataset and demonstrates that the model-agnostic method provides well-calibrated networks with usable uncertainty estimates, though performance varies between curated and real-world datasets.

## Method Summary
The authors combine MC-Dropout for aleatoric uncertainty estimation with Virtual Outlier Synthesis (VOS) for epistemic uncertainty detection. Their method uses a Wide ResNet backbone with 10% dropout during inference, generating multiple stochastic forward passes to estimate predictive variance. VOS creates virtual outliers by sampling from class-conditional multivariate Gaussian distributions and training an energy-based outlier detection system. Logit Normalization is applied during training to reduce overconfident predictions. The framework is trained with a combined loss function including cross-entropy, binary cross-entropy on virtual outliers (scaled by β), and logit normalization loss. This model-agnostic approach is evaluated on both the custom SHIPS dataset and CIFAR10 with various outlier datasets.

## Key Results
- Improves FPR95 by 8% compared to current highest-performing work when trained without out-of-distribution data
- Improves FPR95 by 77% compared to vanilla Wide ResNet implementation
- Achieves 44.2% improvement on SHIPS dataset compared to baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining MC-Dropout with Virtual Outlier Synthesis improves outlier detection by providing both aleatoric and epistemic uncertainty estimates.
- Mechanism: MC-Dropout generates multiple stochastic forward passes to estimate predictive variance (aleatoric uncertainty), while VOS synthesizes outliers from class-conditional distributions to train energy-based outlier detection (epistemic uncertainty).
- Core assumption: Variance across MC-Dropout samples correlates with model confidence, and synthesized outliers from class-conditional distributions are representative of real OOD samples.
- Break condition: If class-conditional Gaussian assumptions are violated or dropout rate is poorly chosen, combined uncertainty estimates may become unreliable.

### Mechanism 2
- Claim: Logit Normalization reduces overconfidence and improves outlier detection performance by normalizing logits before computing classification loss.
- Mechanism: Scaling logits before softmax reduces the incentive for the model to inflate logit magnitudes, making energy scores more discriminative between ID and OOD samples.
- Core assumption: Energy score is inversely correlated with outlier likelihood, and normalizing logits preserves class decision boundaries while improving outlier separability.
- Break condition: If temperature parameter τ is poorly tuned or dataset is highly imbalanced, LN may degrade performance or calibration.

### Mechanism 3
- Claim: Mutual Information between MC samples provides a complementary measure of aleatoric uncertainty that correlates with misclassification.
- Mechanism: MI measures the difference between expected entropy and average entropy across MC samples; high MI indicates high intra-class uncertainty, which often aligns with incorrect classifications.
- Core assumption: Entropy of softmax outputs across MC samples reflects model confidence, and high MI corresponds to ambiguous or uncertain predictions.
- Break condition: If number of MC samples is too low, MI estimates may be noisy or misleading; also, if model is poorly calibrated, MI may not correlate with true uncertainty.

## Foundational Learning

- Concept: Bayesian approximation via MC-Dropout
  - Why needed here: Provides tractable way to estimate predictive uncertainty without full Bayesian inference.
  - Quick check question: How does increasing number of MC samples affect variance of uncertainty estimate?

- Concept: Class-conditional multivariate Gaussian modeling for outlier synthesis
  - Why needed here: Enables synthesis of realistic "virtual" outliers for training without requiring large OOD datasets.
  - Quick check question: What happens to energy score distribution if class-conditional covariances are poorly estimated?

- Concept: Energy-based outlier scoring
  - Why needed here: Offers differentiable, trainable metric for OOD detection that can be optimized jointly with classification.
  - Quick check question: How does choice of β (scaling factor for uncertainty loss) influence balance between ID classification and OOD detection?

## Architecture Onboarding

- Component map: Wide ResNet -> MC-Dropout layer -> VOS module -> LN loss -> MI calculation -> Training loop
- Critical path: 1) Forward pass with MC-Dropout → logits 2) Compute softmax, entropy, MI 3) Generate virtual outliers from class-conditional Gaussians 4) Compute energy scores 5) Apply LN to logits before classification loss 6) Combine losses and backpropagate
- Design tradeoffs: More MC samples → better uncertainty estimates but slower inference; Higher β → better OOD detection but risk of harming ID accuracy; LN temperature τ → controls logit scaling; too high/low degrades performance
- Failure signatures: High MI but low energy → ambiguous ID sample; Low MI but high energy → confident but possibly wrong classification; Very low MI on OOD → model is over-confident on outliers
- First 3 experiments: 1) Baseline WRN on CIFAR10 → verify 94.5% accuracy 2) Add MC-Dropout (5 samples) → check MI distribution and calibration 3) Add VOS (no LN) → measure FPR95 ID/OOD improvement over baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed method vary with different dropout rates during Monte Carlo sampling?
- Basis in paper: The authors mention using MC Dropout with a 10% chance of dropping a neuron, but do not explore the effect of varying this rate.
- Why unresolved: The paper does not investigate the impact of different dropout rates on the method's performance, leaving this as an open area for further research.
- What evidence would resolve it: Systematic experiments testing different dropout rates (e.g., 5%, 10%, 20%) and comparing their effects on FPR95, AUROC, and AUPR metrics across multiple datasets.

### Open Question 2
- Question: What is the relationship between the number of MC samples and the computational cost for real-time maritime object classification?
- Basis in paper: The authors note that increasing MC samples improves performance but with diminishing returns, and that MC sampling requires more computation.
- Why unresolved: The paper does not quantify the trade-off between computational cost and performance improvement, which is crucial for practical deployment on autonomous vessels.
- What evidence would resolve it: Detailed profiling of inference time and resource usage at different sample sizes (e.g., 5, 10, 20, 50 samples) on hardware representative of autonomous vessel systems.

### Open Question 3
- Question: How can the proposed method be adapted to handle continuous rather than discrete class uncertainty in maritime object classification?
- Basis in paper: The authors focus on classification into discrete categories (e.g., boat types), but maritime environments often require uncertainty quantification for continuous variables like distance or velocity.
- Why unresolved: The paper does not address how the uncertainty estimation framework could be extended beyond categorical classification to handle regression tasks common in maritime perception.
- What evidence would resolve it: Implementation and testing of the uncertainty estimation framework on maritime regression tasks (e.g., distance estimation, velocity prediction) with appropriate metrics for continuous uncertainty quantification.

## Limitations
- VOS approach shows strong performance on CIFAR10 and controlled OOD datasets but significantly drops in effectiveness on real-world, less curated data like SHIPS
- Critical hyperparameters such as β (scaling factor for VOS loss) and exact procedure for computing class-conditional multivariate Gaussians are not fully specified
- While LN improves performance, its effect on model calibration and consistency across all datasets is not fully explored

## Confidence
- High Confidence: Core concept of combining MC-Dropout with VOS for uncertainty estimation is well-supported by established literature and results, particularly for well-curated datasets like CIFAR10
- Medium Confidence: Improvement in FPR95 and general trend of performance gains are supported by experimental results, but extent may vary significantly across datasets
- Low Confidence: Robustness of method on non-curated, real-world data (like SHIPS) and optimal tuning of LN and VOS hyperparameters require further validation

## Next Checks
1. Evaluate VOS performance on SHIPS with multiple β values: Systematically test different values of β (e.g., 0.1, 1, 10) to determine sensitivity of VOS loss scaling on SHIPS dataset and identify optimal setting

2. Analyze class-conditional covariance estimation: Investigate how class-conditional Gaussian modeling is performed (e.g., sample mean and covariance estimation per class) and assess impact of covariance estimation errors on energy score distributions and OOD detection

3. Test LN with different temperature scaling: Experiment with varying temperature parameter τ in Logit Normalization across a range (e.g., 0.1, 1, 10) to determine its effect on both classification accuracy and outlier detection performance, especially on imbalanced datasets