---
ver: rpa2
title: Mitigating Exposure Bias in Discriminator Guided Diffusion Models
arxiv_id: '2311.11164'
source_url: https://arxiv.org/abs/2311.11164
tags:
- xxxt
- diffusion
- sampling
- bias
- exposure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates exposure bias in discriminator guided diffusion
  models and proposes SEDM-G++, a novel approach that combines Discriminator Guidance
  and Epsilon Scaling. The authors show that despite improving sample quality, Discriminator
  Guidance does not resolve the persistent issue of exposure bias.
---

# Mitigating Exposure Bias in Discriminator Guided Diffusion Models

## Quick Facts
- arXiv ID: 2311.11164
- Source URL: https://arxiv.org/abs/2311.11164
- Authors: 
- Reference count: 40
- Primary result: SEDM-G++ achieves FID score of 1.73 on unconditional CIFAR-10

## Executive Summary
This paper investigates exposure bias in discriminator-guided diffusion models and proposes SEDM-G++, a novel approach that combines Discriminator Guidance and Epsilon Scaling. The authors show that despite improving sample quality, Discriminator Guidance does not resolve the persistent issue of exposure bias. Their proposed method, SEDM-G++, incorporates a modified sampling approach to address this problem. The authors test their method on the pre-trained EDM model and achieve an FID score of 1.73 on the unconditional CIFAR-10 dataset, outperforming the current state-of-the-art.

## Method Summary
The authors propose SEDM-G++, which combines two key techniques: Discriminator Guidance and Epsilon Scaling. Discriminator Guidance uses a discriminator network to provide feedback on generated samples, adjusting the model score to steer generation toward more realistic samples. Epsilon Scaling addresses exposure bias by scaling down the noise prediction during inference, reducing accumulated error. The approach modifies the sampling process of pre-trained diffusion models without requiring retraining, achieving state-of-the-art results on CIFAR-10 with an FID score of 1.73.

## Key Results
- SEDM-G++ achieves FID score of 1.73 on unconditional CIFAR-10, outperforming current state-of-the-art
- Demonstrates that Discriminator Guidance alone does not resolve exposure bias in diffusion models
- Shows that Epsilon Scaling effectively reduces sampling drift while maintaining sample quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Epsilon scaling reduces the magnitude of the sampling drift by scaling down the noise prediction during inference.
- Mechanism: The model predicts noise based on the generated sample, which accumulates error due to exposure bias. Scaling down the noise prediction by a factor λt at each timestep reduces the accumulated error, bringing the sampling trajectory closer to the desired path.
- Core assumption: The prediction error in diffusion models is overestimated during sampling, and scaling down the noise prediction can mitigate this overestimation.
- Evidence anchors:
  - [abstract] "Our proposed approach improves sample quality across the board and outperforms the current state-of-the-art, by achieving an FID score of 1.73 on the unconditional CIFAR-10 dataset."
  - [section] "Their approach is rooted in the following observation: ϵϵϵs θθθ and ϵϵϵt θθθ both originate from the same input xxxT ∼ N (0, III) at time step t = T. However, starting from time step T − 1, ˆxxxt (the input for ϵϵϵs θθθ) begins to deviate from xxxt (the input for ϵϵϵt θθθ) due to the ϵϵϵθθθ(·) error made in the previous time step."
  - [corpus] "Elucidating the Exposure Bias in Diffusion Models" provides analytical evidence that exposure bias leads to sampling drift.
- Break condition: If the noise prediction is already accurate or the scaling factor λt is not properly tuned, epsilon scaling may not provide significant improvement.

### Mechanism 2
- Claim: Discriminator guidance improves sample quality by incorporating an auxiliary term derived from a discriminator network.
- Mechanism: The discriminator network classifies real and generated data across different noise scales, and its feedback is used to adjust the model score. This adjustment helps steer the sample generation towards more realistic paths.
- Core assumption: The discriminator network can effectively distinguish between real and generated data, and its feedback can guide the model to generate more realistic samples.
- Evidence anchors:
  - [abstract] "A recent approach, known as Discriminator Guidance, seeks to bridge the gap between the model score and the data score by incorporating an auxiliary term, derived from a discriminator network."
  - [section] "When producing samples using the reverse-time SDE: dxxx = [fff(xxx, t) − g(t)2sssθθθ∞(xxx, t)]dt + g(t)d¯www (13) Kim et al. [22] show that the generative process might diverge from the reverse-time data process if the local optimum θθθ∞ of the score network sssθθθ∞ is different to the global optimum θθθ∗. However, augmenting the score function by a correction term can bridge the gap between the two processes."
  - [corpus] "Improving Discriminator Guidance in Diffusion Models" provides evidence that discriminator guidance can improve sample quality.
- Break condition: If the discriminator network is not well-trained or the correction term is not properly integrated, discriminator guidance may not provide significant improvement.

### Mechanism 3
- Claim: The combination of discriminator guidance and epsilon scaling addresses both the quality and exposure bias issues in diffusion models.
- Mechanism: Discriminator guidance improves sample quality by incorporating feedback from the discriminator network, while epsilon scaling reduces the accumulated error due to exposure bias. The combination of these two techniques provides a more effective approach to generating high-quality samples.
- Core assumption: Both discriminator guidance and epsilon scaling are effective in improving sample quality and reducing exposure bias, respectively.
- Evidence anchors:
  - [abstract] "We propose SEDM-G++, which incorporates a modified sampling approach, combining Discriminator Guidance and Epsilon Scaling. Our proposed approach outperforms the current state-of-the-art, by achieving an FID score of 1.73 on the unconditional CIFAR-10 dataset."
  - [section] "Our proposed approach improves sample quality across the board and outperforms the current state-of-the-art, by achieving an FID score of 1.73 on the unconditional CIFAR-10 dataset."
  - [corpus] "Elucidating the Exposure Bias in Diffusion Models" and "Improving Discriminator Guidance in Diffusion Models" provide evidence that both exposure bias and discriminator guidance are important factors in diffusion models.
- Break condition: If either discriminator guidance or epsilon scaling is not effective, the combination may not provide significant improvement.

## Foundational Learning

- Concept: Exposure bias in diffusion models
  - Why needed here: Understanding exposure bias is crucial for comprehending the motivation behind the proposed approach and the importance of epsilon scaling.
  - Quick check question: What is the main cause of exposure bias in diffusion models?
- Concept: Discriminator guidance in diffusion models
  - Why needed here: Discriminator guidance is a key component of the proposed approach, and understanding its mechanism is essential for grasping how it improves sample quality.
  - Quick check question: How does the discriminator network contribute to the sample generation process in discriminator-guided diffusion models?
- Concept: Epsilon scaling in diffusion models
  - Why needed here: Epsilon scaling is the other key component of the proposed approach, and understanding its mechanism is essential for grasping how it reduces exposure bias.
  - Quick check question: What is the main idea behind epsilon scaling, and how does it address exposure bias in diffusion models?

## Architecture Onboarding

- Component map:
  Pre-trained EDM model -> Discriminator network -> Epsilon scaling factor λt -> ODE solver -> Final sample

- Critical path:
  1. Load the pre-trained EDM model.
  2. Train the discriminator network on real and generated data.
  3. During inference, use the discriminator guidance to adjust the model score.
  4. Apply epsilon scaling to the noise prediction at each timestep.
  5. Use the ODE solver to generate the final sample.

- Design tradeoffs:
  - The choice of ODE solver (Euler vs. Heun) affects the computational cost and the effectiveness of epsilon scaling.
  - The weight assigned to the discriminator guidance term (wDG) influences the balance between sample quality and exposure bias reduction.
  - The epsilon scaling factor λt needs to be carefully tuned to achieve the best trade-off between sample quality and exposure bias reduction.

- Failure signatures:
  - If the discriminator network is not well-trained, the discriminator guidance may not effectively improve sample quality.
  - If the epsilon scaling factor λt is not properly tuned, it may not effectively reduce exposure bias or may even degrade sample quality.
  - If the ODE solver is not stable or accurate enough, the generated samples may be of poor quality or may not converge.

- First 3 experiments:
  1. Evaluate the performance of the baseline EDM model using the Euler ODE solver with and without discriminator guidance.
  2. Evaluate the performance of the SEDM-G++ model using the Euler ODE solver with different epsilon scaling factors λt and discriminator guidance weights wDG.
  3. Evaluate the performance of the SEDM-G++ model using the Heun ODE solver with different epsilon scaling factors λt and discriminator guidance weights wDG.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed SEDM-G++ method generalize effectively to datasets beyond CIFAR-10, such as ImageNet or other complex image domains?
- Basis in paper: [explicit] The paper mentions "Future Work" where the authors intend to "evaluate the applicability of this framework across a broader range of datasets" but do not provide such results.
- Why unresolved: The current evaluation is limited to the CIFAR-10 dataset only, making it unclear whether the performance gains would translate to more complex datasets.
- What evidence would resolve it: Testing SEDM-G++ on multiple datasets with varying complexity and image resolutions, comparing performance metrics like FID and IS scores against baseline models.

### Open Question 2
- Question: What is the theoretical relationship between the discriminator guidance weight and the optimal epsilon scaling factor? Can they be jointly optimized rather than tuned separately?
- Basis in paper: [inferred] The authors observe that "the optimal epsilon scaling value λt decreases as the discriminator's weight coefficient increases" and conduct ablation studies on these parameters separately, suggesting a potential dependency.
- Why unresolved: The paper treats wDG and λt as independent hyperparameters to be tuned separately through extensive experimentation, rather than deriving a theoretical relationship between them.
- What evidence would resolve it: A mathematical analysis establishing the relationship between wDG and λt, or an optimization framework that jointly learns both parameters.

### Open Question 3
- Question: How does the exposure bias phenomenon manifest differently across various ODE solvers (Euler, Heun, higher-order methods) and can a unified framework account for these differences?
- Basis in paper: [explicit] The authors extensively compare exposure bias across Euler and Heun solvers, noting that "the FID gain achieved through Epsilon Scaling in the Euler sampler is more pronounced compared to the case of the Heun sampler" and provide theoretical explanations for this difference.
- Why unresolved: While the paper characterizes the differences in exposure bias across solvers, it does not provide a unified theoretical framework that explains these differences or predicts performance across arbitrary solver orders.
- What evidence would resolve it: A comprehensive theoretical model that predicts exposure bias accumulation as a function of solver order, step size, and noise schedule, validated across multiple solver types.

### Open Question 4
- Question: Would incorporating alternative discriminator architectures (e.g., Vision Transformers) or different guidance mechanisms further improve sample quality beyond what SEDM-G++ achieves?
- Basis in paper: [explicit] The authors mention in their methodology that "exploring the integration of a different state-of-the-art architecture for the discriminator part of the framework holds the potential for performance improvement" but do not test this.
- Why unresolved: The paper uses standard U-Net discriminators following previous work, leaving open whether more advanced architectures could yield additional improvements.
- What evidence would resolve it: Experimental results comparing SEDM-G++ with various discriminator architectures (ViT, MLP-Mixer, etc.) and alternative guidance mechanisms on benchmark datasets.

## Limitations
- Requires a well-trained discriminator network, which can be computationally expensive to train
- Effectiveness depends on careful hyperparameter tuning, particularly the scaling factor λt
- Analysis is primarily empirical with limited theoretical guarantees for the combined approach

## Confidence
- High confidence: The core observation about exposure bias in diffusion models (supported by prior work)
- Medium confidence: The effectiveness of epsilon scaling in reducing sampling drift (demonstrated empirically but requires further theoretical analysis)
- Medium confidence: The combination of discriminator guidance and epsilon scaling provides superior results (shown on CIFAR-10 but needs validation on other datasets)

## Next Checks
1. Evaluate SEDM-G++ on additional datasets (e.g., CelebA, LSUN) to assess generalizability beyond CIFAR-10
2. Conduct ablation studies to quantify the individual contributions of discriminator guidance and epsilon scaling to overall performance
3. Investigate the impact of different ODE solvers (beyond Euler and Heun) on the effectiveness of epsilon scaling