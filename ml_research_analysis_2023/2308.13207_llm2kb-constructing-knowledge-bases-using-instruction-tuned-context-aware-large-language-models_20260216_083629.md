---
ver: rpa2
title: 'LLM2KB: Constructing Knowledge Bases using instruction tuned context aware
  Large Language Models'
arxiv_id: '2308.13207'
source_url: https://arxiv.org/abs/2308.13207
tags:
- answer
- language
- knowledge
- context
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LLM2KB, a system for constructing knowledge
  bases using large language models (LLMs). The key idea is to perform parameter-efficient
  instruction tuning of LLMs like Llama-2-13b-chat and StableBeluga-13B using the
  LoRA technique.
---

# LLM2KB: Constructing Knowledge Bases using instruction tuned context aware Large Language Models

## Quick Facts
- arXiv ID: 2308.13207
- Source URL: https://arxiv.org/abs/2308.13207
- Reference count: 34
- System achieves average F1 score of 0.6185 across 21 relations in LM-KBC 2023 challenge

## Executive Summary
This paper presents LLM2KB, a system for constructing knowledge bases using large language models (LLMs). The key innovation is performing parameter-efficient instruction tuning of LLMs like Llama-2-13b-chat and StableBeluga-13B using the LoRA technique, training only ~0.05% of the base model parameters. The system achieves an average F1 score of 0.6185 across 21 relations in the LM-KBC 2023 challenge, with performance ranging from 0.9367 for PersonHasNoblePrize to 0.3327 for PersonHasEmployer.

## Method Summary
LLM2KB uses LoRA to perform parameter-efficient instruction tuning on LLMs, training only 0.05% of the base model parameters to predict object entities given subject entities and relations. The system leverages Wikipedia page contexts retrieved via DPR, processes chunks of 300 tokens with 50-token overlap, and uses Wikidata API for entity disambiguation. During inference, the system retrieves top-3 contexts, applies appropriate prompts with in-context learning examples, and processes LLM output through Wikidata disambiguation to generate knowledge base entries.

## Key Results
- Average F1 score of 0.6185 across 21 relations in LM-KBC 2023 challenge
- Highest score of 0.9367 for PersonHasNoblePrize relation
- Lowest score of 0.3327 for PersonHasEmployer relation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parameter-efficient instruction tuning via LoRA enables task-specific adaptation without full fine-tuning overhead.
- Mechanism: LoRA introduces low-rank matrices into the model's weight updates, allowing only these small matrices to be trained while freezing the original weights. This reduces trainable parameters to ~0.05% of the base model.
- Core assumption: Low-rank decomposition can capture the necessary adaptation for KB construction tasks without losing performance.
- Evidence anchors:
  - [abstract] "We perform parameter efficient instruction tuning for Llama-2-13b-chat and StableBeluga-13B by training small injection models that have only ≈0.05 % of the parameters of the base models using the Low-Rank Adaptation (LoRA) technique."
  - [section] "We load them in 4 bit quantized state with frozen weights and only train an injection model using LoRA technique. The injection model has≈0.05 % of the parameters of the base models."
- Break condition: If the low-rank decomposition cannot adequately capture the relationship between subject entities, relations, and object entities, performance will degrade significantly.

### Mechanism 2
- Claim: Context retrieval via DPR combined with chunk-based processing enables targeted information extraction from Wikipedia.
- Mechanism: DPR encodes the question and Wikipedia text chunks into dense vectors, enabling retrieval of the most relevant context passages. Chunking at 300 tokens with 50-token overlap balances context richness with LLM input limits.
- Core assumption: The most relevant information for answering a relation query is concentrated in a few Wikipedia passages that can be effectively retrieved.
- Evidence anchors:
  - [section] "Each of the context chunks are encoded using the DPR context encoder and stored in a FAISS vector store for fast search and retrieval. To pick the top 3 relevant context chunks for a given question, the question is encoded using the DPR question encoder and then passed to FAISS."
- Break condition: If relevant information is distributed across many non-contiguous passages or requires synthesis beyond the model's context window, retrieval accuracy will suffer.

### Mechanism 3
- Claim: Entity disambiguation using Wikidata API and LLM-based selection resolves surface string ambiguities.
- Mechanism: After the LLM predicts an object entity surface string, the Wikidata API returns candidate entities. The LLM then selects the correct disambiguated entity based on the question context.
- Core assumption: The LLM can effectively distinguish between candidate entities when provided with the original question as context.
- Evidence anchors:
  - [section] "To do this, we collect all the candidate entities and put them in a list to form the {context} in Prompt 3. The expectation is that the LLM picks the correct disambiguated entity relevant to the question."
- Break condition: If candidate entities are too similar or the question context is insufficient to distinguish them, the disambiguation will fail.

## Foundational Learning

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: Full fine-tuning of 13B parameter models is computationally prohibitive; PEFT enables adaptation with minimal resources.
  - Quick check question: What percentage of parameters does LoRA train compared to full fine-tuning?

- Concept: Dense Passage Retrieval (DPR)
  - Why needed here: Wikipedia contains vast amounts of text; DPR enables efficient retrieval of relevant context for specific entity-relation queries.
  - Quick check question: What are the two encoders used in DPR and what do they encode?

- Concept: Entity disambiguation and knowledge graph structure
  - Why needed here: Wikipedia text contains ambiguous entity mentions; linking to Wikidata ensures structured, disambiguated knowledge base entries.
  - Quick check question: Why is entity disambiguation necessary when constructing a knowledge base from text sources?

## Architecture Onboarding

- Component map:
  - Base LLM (Llama-2-13b-chat or StableBeluga-13B) → LoRA injection model → Instruction tuning dataset
  - DPR system (question encoder + context encoder) → FAISS vector store → Context retrieval
  - Wikidata API → LLM-based disambiguation → Entity resolution
  - Input: (subject entity, relation) → Output: disambiguated object entities

- Critical path: Subject entity → Wikipedia text retrieval → DPR chunking and indexing → Context retrieval for query → LLM inference with context → Entity disambiguation → KB entry

- Design tradeoffs:
  - Chunk size (300 tokens) vs. context completeness: smaller chunks reduce processing but may miss relevant information
  - Number of retrieved contexts (3) vs. LLM overload: more contexts provide more information but may confuse the model
  - LoRA rank (r=4) vs. adaptation capacity: lower ranks save parameters but may limit fine-tuning effectiveness

- Failure signatures:
  - Low precision: LLM hallucinations or incorrect context retrieval
  - Low recall: Missing relevant contexts or LLM inability to extract correct answers
  - Entity linking failures: Wikidata API returns no relevant entities or LLM cannot disambiguate correctly

- First 3 experiments:
  1. Test context retrieval: Input a known subject entity and relation, verify that DPR returns relevant Wikipedia passages
  2. Test LLM inference: With known context, verify the LLM can extract correct object entities
  3. Test entity disambiguation: With known object entity strings, verify the Wikidata API returns relevant candidates and the LLM can select the correct one

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the fragility of large language models during inference towards minor changes in prompts be addressed to improve robustness and performance consistency?
- Basis in paper: [explicit] The authors mention that "Fragility of LLM during inference towards minor changes in prompts" is one of the practical challenges observed after subjective analysis of the results.
- Why unresolved: While the authors acknowledge this challenge, they do not provide specific strategies or techniques to mitigate the fragility of LLMs to prompt variations during inference.
- What evidence would resolve it: Experimental results demonstrating improved robustness and performance consistency of LLMs when subjected to minor prompt changes after implementing proposed mitigation strategies.

### Open Question 2
- Question: How can the hallucinations of large language models be reduced or eliminated to improve the reliability of generated knowledge base content?
- Basis in paper: [explicit] The authors mention that "Hallucinations of LLM" is one of the practical challenges observed after subjective analysis of the results.
- Why unresolved: The authors do not provide specific methods or techniques to address the issue of hallucinations in LLMs, which can lead to unreliable or incorrect knowledge base content.
- What evidence would resolve it: Experimental results showing a significant reduction or elimination of hallucinations in LLM-generated knowledge base content after implementing proposed techniques to mitigate this issue.

### Open Question 3
- Question: How can the performance of large language models on numeric answer relations (e.g., PersonHasNumberOfChildren, SeriesHasNumberOfEpisodes) be improved when Wikipedia contexts rarely mention such information?
- Basis in paper: [inferred] The authors mention that their models did not perform well on relations expecting numeric answers, and attribute this to the lack of relevant information in Wikipedia contexts.
- Why unresolved: The authors do not provide specific strategies or techniques to improve LLM performance on numeric answer relations when the required information is scarce in the available context.
- What evidence would resolve it: Experimental results demonstrating improved performance of LLMs on numeric answer relations after implementing proposed methods to handle cases where the required information is not present in the available context.

## Limitations

- The system's performance varies significantly across relations (F1 scores from 0.3327 to 0.9367), indicating inconsistent effectiveness for different types of queries.
- The reliance on Wikipedia as the primary context source may limit performance on relations where information is rarely mentioned or requires complex inference across multiple passages.
- The entity disambiguation process depends on the quality and comprehensiveness of Wikidata, and may fail when candidate entities are too similar or question context is insufficient.

## Confidence

- **High Confidence**: The core methodology of using LoRA for parameter-efficient instruction tuning and the overall system architecture combining context retrieval with LLM inference is well-established and technically sound.
- **Medium Confidence**: The specific performance metrics (F1 scores ranging from 0.3327 to 0.9367) are directly reported from the challenge but may be influenced by dataset-specific characteristics and prompt engineering choices that aren't fully reproducible.
- **Low Confidence**: The generalizability of the approach to other knowledge base domains or relations not present in the LM-KBC 2023 dataset remains uncertain, as does the robustness of the system under different context retrieval scenarios.

## Next Checks

1. **Context Retrieval Validation**: Systematically test the DPR retrieval mechanism with known subject entities and relations to verify that the top-3 retrieved contexts consistently contain the information needed to answer the relation queries. This should include both successful cases (where the ground truth answer appears in the retrieved contexts) and failure cases (where relevant information is missing or distributed across too many passages).

2. **LLM Inference Robustness**: Conduct controlled experiments where the retrieved contexts are known to be relevant, then test the LLM's ability to extract correct object entities across different prompt variations and in-context learning examples. This should include testing with both successful predictions and cases where the LLM produces incorrect or incomplete answers.

3. **Entity Disambiguation Stress Test**: Create scenarios with ambiguous entity strings that have multiple valid Wikidata entries, then test the LLM's ability to correctly disambiguate based on question context. This should include edge cases where candidate entities are semantically similar or where the question context is insufficient to make a clear distinction.