---
ver: rpa2
title: Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models
arxiv_id: '2307.08303'
source_url: https://arxiv.org/abs/2307.08303
tags:
- prompt
- soft
- tuning
- pairs
- weak
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SPTAR, a method for improving dense retrieval
  (DR) by leveraging large language models (LLMs) and soft prompt tuning. The core
  idea is to use soft prompt tuning to optimize task-specific prompts on limited ground
  truth data, then use these prompts to generate weak queries for unlabeled documents.
---

# Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models

## Quick Facts
- arXiv ID: 2307.08303
- Source URL: https://arxiv.org/abs/2307.08303
- Authors: 
- Reference count: 40
- Key outcome: SPTAR improves dense retrieval by 7.3% to 18.04% in NDCG@10 compared to BM25 baseline using soft prompt tuning and LLM-generated weak queries

## Executive Summary
This paper introduces SPTAR, a method for improving dense retrieval (DR) by leveraging large language models (LLMs) and soft prompt tuning. The core idea is to use soft prompt tuning to optimize task-specific prompts on limited ground truth data, then use these prompts to generate weak queries for unlabeled documents. These weak document-query pairs are used to train task-specific dense retrievers. A soft prompt filter is designed to select high-quality example document-query pairs to further improve the quality of generated weak queries. Experiments demonstrate that SPTAR outperforms baseline methods like BM25 and InPairs, with an average improvement of 7.3% to 18.04% in NDCG@10 compared to BM25.

## Method Summary
SPTAR uses soft prompt tuning with the Prefix-Tuning approach to optimize task-specific soft prompts on limited ground truth data. The tuned soft prompts are then used with LLMs (LLaMA-7B and Vicuna-7B) to generate weak queries for unlabeled documents. A soft prompt filter selects high-quality example document-query pairs based on validation set perplexity. Generated weak queries are filtered using BM25 to ensure they can retrieve their intended documents. The resulting high-quality weak document-query pairs are used to train task-specific dense retrievers (DPR, ColBERT, BM25CE). The method requires GPUs (4 A100s for DR models, 1 A100 for soft prompt tuning) and uses sample subsets from MS MARCO and FiQA-2018 datasets for evaluation.

## Key Results
- SPTAR achieves 7.3% to 18.04% improvement in NDCG@10 compared to BM25 baseline
- Consistent improvements across three different dense retrievers (DPR, ColBERT, BM25CE)
- Outperforms InPairs baseline method in all evaluated metrics
- Effective with limited training data (50 queries for training, 100 for evaluation)

## Why This Works (Mechanism)

### Mechanism 1
Soft prompt tuning optimizes task-specific prompts on limited ground truth data, improving query generation quality for dense retrieval. Soft prompts are learned embeddings appended to LLM input that capture task-specific semantic knowledge without altering the LLM's original parameters. During tuning, only these prompt embeddings are updated to maximize the likelihood of relevant queries given documents. The core assumption is that the learned soft prompt can distill domain-specific knowledge from limited labeled data and generalize to generate high-quality weak queries for unlabeled documents.

### Mechanism 2
The soft prompt filter improves weak query quality by selecting high-quality example document-query pairs for the prompt. After soft prompt tuning, different subsets of example document-query pairs are evaluated on a validation set. The subset yielding the lowest perplexity is selected for use in the augmentor module, as it produces the best query generation performance. The core assumption is that the quality of example document-query pairs in the prompt significantly affects the LLM's ability to generate relevant weak queries for new documents.

### Mechanism 3
Weak data filtering via BM25 re-ranking improves the quality of the generated weak document-query pairs used for training dense retrievers. For each generated weak query, BM25 retrieves top-k documents. If the original paired document is not in the top-k results, the weak pair is discarded, ensuring only queries that can retrieve their intended documents are kept. The core assumption is that a query that can retrieve its intended document via BM25 is more likely to be a high-quality weak query that will help train a better dense retriever.

## Foundational Learning

**Concept: Dense Retrieval (DR) fundamentals** - Encoding queries and documents into dense vectors and measuring semantic similarity. Needed to understand how SPTAR builds upon and augments DR models. Quick check: What is the key difference between dense retrieval and traditional keyword-based retrieval like BM25?

**Concept: Prompt tuning and soft prompts** - Learnable embeddings that guide LLM behavior without fine-tuning model parameters. Needed to understand SPTAR's core innovation of using soft prompt tuning instead of hard prompts or full fine-tuning. Quick check: How does soft prompt tuning differ from traditional prompt engineering with human-written prompts?

**Concept: Data augmentation for retrieval** - Generating synthetic training examples to improve model performance when labeled data is scarce. Needed to understand how SPTAR generates weak document-query pairs as augmented training data for dense retrievers. Quick check: Why might generated weak queries be useful for training dense retrievers even if they're not perfect?

## Architecture Onboarding

**Component map**: Data preparation → Soft prompt tuning → Soft prompt filter → Soft prompt augmentor → Weak data filter → Dense retrieval training

**Critical path**: Soft prompt tuning → Soft prompt filter → Soft prompt augmentor → Weak data filter → Dense retrieval training

**Design tradeoffs**: Soft prompts vs. hard prompts (flexibility vs. interpretability), filter strictness vs. data quantity, LLM choice vs. computational cost

**Failure signatures**: Poor perplexity on validation set (tuning failed), weak queries not retrieving paired documents (filter too strict), dense retriever performance worse than baseline (augmentation ineffective)

**First 3 experiments**:
1. Verify soft prompt tuning learns meaningful patterns by comparing perplexity with and without tuning on a small labeled dataset
2. Test soft prompt filter by generating weak queries with best and worst example pairs and measuring query quality differences
3. Evaluate impact of weak data filter by training dense retriever with and without filtered weak data and comparing retrieval metrics

## Open Questions the Paper Calls Out

**Open Question 1**: How does the size of the training dataset X affect the quality of the learned soft prompts and the performance of downstream dense retrieval tasks? The paper shows that increasing training size X generally improves performance but doesn't provide detailed analysis of optimal size for different tasks or datasets.

**Open Question 2**: How does the number of example document-query pairs M used in the soft prompt augmentor module affect the quality of generated weak queries and downstream retrieval performance? The paper presents varying performance with different values of M but lacks detailed analysis of optimal M for different tasks.

**Open Question 3**: How does the choice of language model affect the quality of learned soft prompts and downstream dense retrieval performance? While the paper shows different language models affect results, it doesn't provide detailed analysis of optimal language model for different tasks or datasets.

## Limitations
- Evaluation relies on relatively small sample sizes (50 training queries, 100 evaluation queries) that may not represent real-world deployment scenarios
- Soft prompt filter mechanism lacks implementation details about exact selection criteria based on perplexity scores
- Paper doesn't explore impact of different LLM sizes beyond the 7B parameter models used

## Confidence
- **High Confidence**: Core mechanism of using soft prompt tuning to generate weak queries for data augmentation is well-supported by experimental results
- **Medium Confidence**: Effectiveness of soft prompt filter in improving query quality is demonstrated but relies on validation set performance
- **Medium Confidence**: Weak data filtering using BM25 is reasonable but top-k threshold impact is not thoroughly explored

## Next Checks
1. **Scale Testing**: Evaluate SPTAR performance when using only 10-20 labeled queries for soft prompt tuning to understand minimum viable data requirements
2. **Filter Ablation Study**: Systematically test the soft prompt filter by comparing performance across different filtering strategies on held-out validation sets
3. **Generalization Analysis**: Test the method on a third, unseen dataset from a different domain (e.g., legal or biomedical) to assess cross-domain generalization