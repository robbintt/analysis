---
ver: rpa2
title: Efficient Human-AI Coordination via Preparatory Language-based Convention
arxiv_id: '2311.00416'
source_url: https://arxiv.org/abs/2311.00416
tags:
- human
- coordination
- soup
- human-ai
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose HAPLAN, a method for enhancing human-AI coordination
  by using a large language model (LLM) to generate conventions (action plans) that
  specify roles and tasks for both human and AI agents. The key innovation is decomposing
  the convention formulation into multiple sequential sessions, each handling a simpler
  sub-problem, and incorporating human feedback to refine the convention.
---

# Efficient Human-AI Coordination via Preparatory Language-based Convention

## Quick Facts
- arXiv ID: 2311.00416
- Source URL: https://arxiv.org/abs/2311.00416
- Reference count: 40
- 15% performance improvement over state-of-the-art learning-based approaches in Overcooked-AI environment

## Executive Summary
HAPLAN introduces a novel method for enhancing human-AI coordination by using large language models (LLMs) to generate preparatory conventions that specify roles and tasks for both human and AI agents. The key innovation is decomposing the convention formulation into multiple sequential sessions, each handling a simpler sub-problem, while incorporating human feedback to refine the convention. This approach achieves 15% better performance than existing learning-based methods and demonstrates improved alignment with human preferences in the Overcooked-AI environment.

## Method Summary
HAPLAN uses LLMs to generate conventions through a multi-session decomposition approach, where each session handles a specific aspect of planning (information extraction, clarification, refinement, timing estimation, and order adjustment). Human feedback is integrated to refine the generated conventions, and pre-trained low-level skills (learned via behavioral cloning) translate the high-level natural language conventions into executable actions. The method operates in the Overcooked-AI environment, a two-player cooperative setting where agents coordinate to prepare and serve dishes.

## Key Results
- HAPLAN outperforms state-of-the-art learning-based approaches by 15% in performance
- Better alignment with human preferences compared to baseline methods
- Demonstrates generality on reasoning benchmarks through the multi-session approach
- Shows effectiveness across multiple Overcooked-AI layouts including Counter Circle, Asymmetric Advantages, and Soup Coordination

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing complex coordination planning into multiple sequential LLM sessions improves performance
- Mechanism: Breaking a complex planning problem into smaller sub-problems reduces cognitive load on the LLM, preventing hallucination and improving reasoning quality. Each session handles a simpler task with shorter prompts, and outputs from earlier sessions become inputs for later ones
- Core assumption: The LLM's reasoning quality degrades with increased prompt complexity and conversation history length
- Evidence anchors: [abstract] "decomposing the convention formulation problem into sub-problems with multiple new sessions being sequentially employed"; [section 4.1] "when dealing with a complex problem or a lengthy conversation history, the LLM may struggle to effectively cope, leading to the generation of misleading contents"

### Mechanism 2
- Claim: Human feedback integration refines LLM-generated conventions for better alignment
- Mechanism: After the LLM generates an initial convention, humans review and provide corrections, which are fed back as prompts to re-plan. This creates an iterative refinement loop that aligns the convention with human preferences and corrects LLM errors
- Core assumption: Humans can identify flaws in LLM-generated conventions and articulate specific corrections
- Evidence anchors: [abstract] "incorporating human feedback to refine the convention"; [section 4.1] "human will review the generated content and provide feedback on any inappropriate aspects"

### Mechanism 3
- Claim: Pre-trained low-level skills bridge high-level LLM conventions to executable actions
- Mechanism: The LLM generates high-level natural language conventions (e.g., "Fetch an onion at (2,1)"), which are translated into low-level actions by pre-trained skill policies learned via behavioral cloning. This hierarchical approach leverages LLM's strength in planning while using learned policies for precise execution
- Core assumption: Natural language conventions can be mapped to discrete skill actions in the environment
- Evidence anchors: [section 4.2] "The convention contains temporally extended high-level instructions in natural language, which has to be translated into low-level actions executable for AI"

## Foundational Learning

- Concept: Chain-of-Thought prompting
  - Why needed here: Improves LLM reasoning by encouraging step-by-step explanation, which is leveraged in session prompts
  - Quick check question: How does Chain-of-Thought differ from standard prompting in handling complex reasoning tasks?

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The environment is modeled as a two-player cooperative MDP where agents coordinate to maximize shared reward
  - Quick check question: What are the key differences between a standard MDP and the two-player cooperative MDP formulation used here?

- Concept: Behavioral Cloning
  - Why needed here: Used to train low-level skill policies from human demonstrations
  - Quick check question: What are the main limitations of behavioral cloning for learning control policies?

## Architecture Onboarding

- Component map: LLM sessions (5 sequential sessions) → Human feedback loop → Low-level skill policies → Environment execution

- Critical path:
  1. Session 1 extracts key information from human preferences
  2. Session 2 clarifies rough work content
  3. Session 3 refines work content into detailed instructions
  4. Session 4 estimates execution times
  5. Session 5 adjusts execution order
  6. Human reviews and provides feedback if needed
  7. Final convention translated to low-level actions via skill policies

- Design tradeoffs:
  - Multiple sessions vs. single session: Better reasoning quality vs. increased latency and complexity
  - Human feedback vs. fully autonomous: Better alignment vs. reduced autonomy
  - Pre-trained skills vs. learned coordination policies: Interpretable conventions vs. potentially higher performance

- Failure signatures:
  - LLM sessions producing inconsistent outputs across sessions
  - Human feedback not improving convention quality over iterations
  - Skill policies failing to execute LLM-generated instructions correctly
  - Performance degradation with increased problem complexity

- First 3 experiments:
  1. Single session baseline vs. multi-session approach on simple Overcooked layouts
  2. Human feedback ablation: Compare with and without human review
  3. Skill policy ablation: Compare with and without pre-trained skills, using only LLM-generated actions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of problem decomposition affect the final convention quality and performance?
- Basis in paper: [inferred] The paper discusses decomposing problems into sub-problems and assigning them to different sessions, and shows that HAPLAN-5 (decomposing into 5 sub-problems) performs better than HAPLAN (decomposing into 4 sub-problems)
- Why unresolved: The paper does not explore the full range of possible decompositions or analyze the impact of different decomposition strategies on the final convention quality
- What evidence would resolve it: Experiments comparing different decomposition strategies (e.g., different numbers of sub-problems, different ways to split the problem) and analyzing the resulting convention quality and performance

### Open Question 2
- Question: How does the inclusion of human feedback in the convention formulation process affect the final convention quality and alignment with human preferences?
- Basis in paper: [explicit] The paper proposes incorporating a human validation process where humans can review and provide feedback on the generated convention, which is then used to refine the convention
- Why unresolved: The paper does not quantify the impact of human feedback on the final convention quality or compare the performance of HAPLAN with and without human feedback
- What evidence would resolve it: Experiments comparing the performance of HAPLAN with and without human feedback, and analyzing the impact of different types of feedback on the final convention quality and alignment with human preferences

### Open Question 3
- Question: How does the proposed method generalize to other domains beyond human-AI coordination?
- Basis in paper: [explicit] The paper demonstrates the effectiveness of the proposed method on reasoning benchmarks (symbolic manipulation, compositional reasoning, and math reasoning), but only briefly mentions the potential for generalization to other domains
- Why unresolved: The paper does not provide a systematic analysis of the method's performance across a wide range of domains or discuss the limitations and challenges of applying the method to other tasks
- What evidence would resolve it: Experiments evaluating the method's performance on a diverse set of tasks and domains, and analyzing the factors that contribute to successful generalization

## Limitations

- Core mechanism of decomposing LLM reasoning into multiple sessions is theoretically sound but lacks direct empirical validation
- Human feedback integration may be brittle if humans struggle to articulate specific corrections or if feedback quality varies
- Skill policy integration is standard practice in robotics but not directly demonstrated in this paper's experiments

## Confidence

- **High confidence**: The hierarchical structure (LLM sessions → human feedback → skill policies) is clearly specified and methodologically sound
- **Medium confidence**: The performance improvements (15% over baselines) are reported but we lack details on experimental conditions and statistical significance
- **Low confidence**: The generality claims for the multi-session approach on reasoning benchmarks are supported only by references to other work, not direct evidence in this paper

## Next Checks

1. **Session decomposition validation**: Test whether breaking a complex planning task into the specified five sessions actually improves LLM reasoning quality compared to a single comprehensive session

2. **Human feedback ablation study**: Systematically evaluate how different quality and quantity of human feedback affects final convention quality and coordination performance

3. **Skill policy robustness test**: Evaluate how well pre-trained skills handle the diverse and potentially novel instructions generated by different LLM sessions across varying task complexities