---
ver: rpa2
title: Dynamic Interpretability for Model Comparison via Decision Rules
arxiv_id: '2309.17095'
source_url: https://arxiv.org/abs/2309.17095
tags:
- rules
- differences
- data
- explanations
- interpretability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "DeltaXplainer is a model-agnostic method for generating rule-based\
  \ explanations of differences between two binary classifiers, particularly useful\
  \ for model comparison in dynamic settings with concept drift. It constructs a \u2206\
  -model to predict where the two classifiers disagree, then fits a decision tree\
  \ and extracts interpretable rules describing these disagreement areas."
---

# Dynamic Interpretability for Model Comparison via Decision Rules

## Quick Facts
- arXiv ID: 2309.17095
- Source URL: https://arxiv.org/abs/2309.17095
- Reference count: 27
- Key outcome: DeltaXplainer achieves high fidelity (accuracy 0.93-0.99, precision 0.39-0.86, recall 0.39-0.97) with interpretable rules (length 1-8, coverage 0.08-2%) for comparing binary classifiers in dynamic settings.

## Executive Summary
DeltaXplainer is a model-agnostic method for generating rule-based explanations of differences between two binary classifiers, particularly useful for model comparison in dynamic settings with concept drift. It constructs a ∆-model to predict where the two classifiers disagree, then fits a decision tree and extracts interpretable rules describing these disagreement areas. Experiments on synthetic (AGRAWAL) and real-world (COVER TYPE, ELEC2) datasets with various concept drift scenarios show that DeltaXplainer achieves high fidelity when differences are clear, while providing interpretable rules through hyperparameter tuning.

## Method Summary
DeltaXplainer works by first training a ∆-model to predict disagreements between two binary classifiers using the union of their training data. This surrogate model learns the regions where the classifiers differ. A decision tree is then trained as a surrogate model with controlled interpretability through the minimum samples per leaf hyperparameter. Finally, interpretable rules are extracted from the decision tree branches that predict disagreement, with conjunctions grouped by feature to improve readability.

## Key Results
- High fidelity achieved when differences are clear (accuracy 0.93-0.99, precision 0.39-0.86, recall 0.39-0.97)
- Interpretable rules generated (average length 1-8 features, coverage 0.08-2%)
- Performance degrades for subtle or sparse drifts (Gaussian noise, permutations)
- Hyperparameter tuning required to balance fidelity and interpretability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DeltaXplainer captures model differences by training a classifier to predict disagreements between two models.
- Mechanism: The ∆-model is trained on the union of training data from both models, with labels indicating where predictions disagree (Eq. 1). This surrogate model learns the regions where f and g differ.
- Core assumption: Areas of interest lie within the distribution of the data from both models.
- Evidence anchors:
  - [section]: "In Step 1, the training set is defined as X∆ = Xf ∪ Xg. In Step 2, their labels y∆ for X∆ are set using f and g according to Eq. 1."
  - [abstract]: "It constructs a ∆-model to predict where the two classifiers disagree, then fits a decision tree and extracts interpretable rules describing these disagreement areas."
  - [corpus]: Weak. No direct corpus evidence about disagreement prediction methods.
- Break condition: If the disagreement patterns are too subtle or sparse (e.g., Gaussian noise or permutations), the ∆-model cannot learn meaningful patterns.

### Mechanism 2
- Claim: Decision trees provide interpretable rules while maintaining reasonable fidelity to disagreement patterns.
- Mechanism: The decision tree is trained as a surrogate model with controlled interpretability through the minimum samples per leaf hyperparameter. This controls rule coverage and length.
- Core assumption: Binary decision trees prevent rule overlap and allow direct control over interpretability metrics.
- Evidence anchors:
  - [section]: "In order to ease the choice of a good set of hyperparameters to reach an interpretability-accuracy trade-off acceptable for the user... we propose to restrict the model parametrization to the minimum number of samples per leaf, without constraining the maximum depth of the tree."
  - [abstract]: "It constructs a ∆-model to predict where the two classifiers disagree, then fits a decision tree and extracts interpretable rules describing these disagreement areas."
  - [corpus]: Weak. No direct corpus evidence about minimum samples per leaf controlling interpretability.
- Break condition: When minimum samples per leaf is too high, the tree may not generate any rules (as seen in experiments with 10%+ thresholds).

### Mechanism 3
- Claim: Rule extraction from decision trees provides human-interpretable explanations of disagreement regions.
- Mechanism: Each branch predicting disagreement class 1 becomes a rule, with conjunctions grouped by feature to improve readability. Rules describe feature value intervals where models disagree.
- Core assumption: Users can understand and use rules of moderate length (1-8 features) with reasonable coverage (0.08-2%).
- Evidence anchors:
  - [section]: "The final step consists of translating the tree into a set of rules, associating one rule to each branch, and keeping only the ones predicting class 1 (disagreement class)."
  - [abstract]: "DeltaXplainer achieves high fidelity (accuracy 0.93-0.99, precision 0.39-0.86, recall 0.39-0.97) when differences are clear... while providing interpretable rules (average length 1-8, coverage 0.08-2%)."
  - [corpus]: Weak. No direct corpus evidence about rule length or coverage metrics.
- Break condition: If rules become too long or numerous, interpretability decreases despite maintaining fidelity.

## Foundational Learning

- Concept: Concept drift and its impact on model performance
  - Why needed here: The paper specifically addresses dynamic settings where data distribution changes affect model behavior, requiring methods to capture these changes.
  - Quick check question: What is the difference between concept drift and data drift, and how does each affect model predictions?

- Concept: Rashomon effect and model multiplicity
  - Why needed here: The paper mentions that multiple models can achieve similar performance but differ in underlying patterns, which is relevant to comparing model differences.
  - Quick check question: How does the Rashomon effect complicate model comparison, and why might two models with similar accuracy still behave differently?

- Concept: Interpretability-accuracy tradeoff in XAI
  - Why needed here: The method explicitly balances interpretability (shorter rules, fewer rules) against fidelity (accuracy, precision, recall) through hyperparameter tuning.
  - Quick check question: What are the key metrics used to evaluate the tradeoff between interpretability and accuracy in rule-based explanations?

## Architecture Onboarding

- Component map:
  - Input: Two binary classifiers (f, g) and their training data (Xf, yf) and (Xg, yg)
  - ∆-Model Construction: Union of training data with disagreement labels
  - Decision Tree Training: Surrogate model with minimum samples per leaf hyperparameter
  - Rule Extraction: Translation of decision tree branches to interpretable rules
  - Output: Set of differential rules describing disagreement regions

- Critical path:
  1. Construct training data by combining Xf and Xg with disagreement labels
  2. Train decision tree with specified minimum samples per leaf
  3. Extract rules from tree branches predicting disagreement class
  4. Post-process rules to group conjunctions by feature for readability

- Design tradeoffs:
  - Minimum samples per leaf vs. rule interpretability: Higher values increase coverage but reduce rule count and potentially fidelity
  - Decision tree depth vs. rule complexity: Unconstrained depth allows complex rules but may reduce interpretability
  - Rule length vs. coverage: Shorter rules may miss some disagreement regions while longer rules become harder to interpret

- Failure signatures:
  - No rules generated: Minimum samples per leaf too high or disagreement patterns too subtle
  - Very long rules (>8 features): Decision tree too deep or data too complex
  - Low recall: Subtle or sparse drifts not captured by decision tree
  - High overlap between rules: Decision tree structure not preventing overlap

- First 3 experiments:
  1. Run DeltaXplainer on AGRAWAL with abrupt shift (S3) using minimum samples = 1 to verify basic functionality
  2. Test DeltaXplainer on ELEC2 with Gaussian noise (S1) using minimum samples = 1% to assess subtle drift handling
  3. Experiment with minimum samples per leaf values (1%, 1%, 2.5%, 10%) on COVER TYPE to observe interpretability-fidelity tradeoff

## Open Questions the Paper Calls Out

- Question: How does DeltaXplainer perform on regression models rather than binary classifiers?
  - Basis in paper: [inferred] The paper focuses exclusively on binary classifiers and rule-based explanations for classification disagreements. No discussion of regression tasks.
  - Why unresolved: The methodology relies on binary classification frameworks and metrics not directly applicable to regression.
  - What evidence would resolve it: Experiments comparing DeltaXplainer on regression tasks with adapted metrics and rule-based explanations for continuous output differences.

- Question: How sensitive is DeltaXplainer to the choice of minimum samples per leaf hyperparameter in imbalanced datasets?
  - Basis in paper: [explicit] The paper discusses varying this hyperparameter but only tests it on balanced scenarios. For imbalanced cases (S1, S2), it notes precision decreases with higher support requirements.
  - Why unresolved: The paper doesn't explore systematic sensitivity analysis or provide guidance for imbalanced data.
  - What evidence would resolve it: Detailed ablation studies showing performance across different imbalance ratios and hyperparameter values.

- Question: Can DeltaXplainer handle multi-class classification problems effectively?
  - Basis in paper: [inferred] The method is explicitly designed for binary classifiers. Multi-class extension would require defining disagreement between more than two models.
  - Why unresolved: The paper doesn't discuss extensions to multi-class scenarios or how to handle multiple class disagreements.
  - What evidence would resolve it: Experiments demonstrating DeltaXplainer's effectiveness on multi-class problems with appropriate disagreement definitions and rule extraction.

## Limitations

- Evaluation focuses on controlled synthetic drifts and two real-world datasets, leaving uncertainty about performance on more complex, multi-feature drift patterns
- Reliance on decision trees as a surrogate model may limit effectiveness when disagreement patterns involve non-axis-aligned decision boundaries
- Interpretability metrics (rule length, coverage) are somewhat arbitrary and may not align with human cognitive limits for rule comprehension

## Confidence

- **High confidence**: The core mechanism of using a ∆-model to capture disagreement regions between two classifiers is well-established and theoretically sound
- **Medium confidence**: The effectiveness of minimum samples per leaf for controlling interpretability-fidelity tradeoffs is supported by experiments but lacks theoretical grounding
- **Low confidence**: The choice of interpretability metrics (average rule length 1-8, coverage 0.08-2%) is somewhat arbitrary and not validated against human interpretability studies

## Next Checks

1. **Multi-feature drift validation**: Test DeltaXplainer on datasets with concurrent drifts across multiple features to assess its ability to capture complex disagreement patterns
2. **Human evaluation study**: Conduct user studies to validate whether the extracted rules meet human interpretability standards and actually aid in understanding model differences
3. **Alternative surrogate models**: Compare decision tree performance against other interpretable surrogates (e.g., linear models, rule lists) for capturing disagreement patterns in various drift scenarios