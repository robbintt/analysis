---
ver: rpa2
title: Transformers Can Solve Non-Linear and Non-Markovian Filtering Problems in Continuous
  Time For Conditionally Gaussian Signals
arxiv_id: '2310.19603'
source_url: https://arxiv.org/abs/2310.19603
tags:
- kratsios
- lemma
- deep
- filter
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes the first theoretical foundation for the
  use of deep Kalman filters (DKFs) in solving continuous-time stochastic filtering
  problems. The authors show that a class of continuous-time transformer models, called
  filterformers, can approximate the conditional law of a broad class of non-Markovian
  and conditionally Gaussian signal processes given noisy continuous-time measurements.
---

# Transformers Can Solve Non-Linear and Non-Markovian Filtering Problems in Continuous Time For Conditionally Gaussian Signals

## Quick Facts
- arXiv ID: 2310.19603
- Source URL: https://arxiv.org/abs/2310.19603
- Reference count: 40
- This paper establishes the first theoretical foundation for deep Kalman filters (DKFs) in solving continuous-time stochastic filtering problems

## Executive Summary
This paper presents the first theoretical foundation for using deep Kalman filters (DKFs) to solve continuous-time stochastic filtering problems. The authors introduce "filterformers," a class of continuous-time transformer models that can approximate the conditional law of non-Markovian and conditionally Gaussian signal processes given noisy continuous-time measurements. The approximation guarantees hold uniformly over sufficiently regular compact subsets of continuous-time paths, with the worst-case 2-Wasserstein distance quantifying the approximation error.

## Method Summary
The method introduces filterformers, which consist of three phases: (1) a pathwise attention mechanism that losslessly encodes continuous-time paths into finite-dimensional Euclidean space, (2) a multi-layer perceptron (MLP) to process these encoded features, and (3) a geometric attention mechanism that decodes the features back into Gaussian measures in the 2-Wasserstein space. The pathwise attention implements bi-Lipschitz embeddings of regular path domains, while the geometric attention leverages the barycentric QAS structure of Gaussian measures under the 2-Wasserstein metric. The analysis relies on establishing local Lipschitz continuity of the optimal filter parameters through stability estimates.

## Key Results
- Filterformers can approximate conditional laws of non-Markovian and conditionally Gaussian signal processes
- The approximation error is bounded uniformly by the worst-case 2-Wasserstein distance
- The pathwise attention mechanism provides bi-Lipschitz embeddings without dimension reduction error
- The geometric attention mechanism can decode Euclidean features back into Gaussian measures in Wasserstein space

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The pathwise attention mechanism implements bi-Lipschitz embeddings of continuous-time paths into low-dimensional Euclidean spaces without dimension reduction error.
- Mechanism: The attention mechanism constructs similarity scores between novel paths and a dictionary of reference paths, then combines these with positional encodings at sampled timepoints to create a stable, lossless feature vector representation.
- Core assumption: The compact subset K of path space has sufficient regularity (either finite, piecewise linear, or isometric to a Riemannian manifold).
- Evidence anchors:
  - [abstract]: "The first can losslessly adapt to the characteristics of a broad range of paths since we show that the attention mechanism implements bi-Lipschitz embeddings of sufficiently regular sets of paths into low-dimensional Euclidean spaces"
  - [section 3.1]: "Proposition 2 (Pathwise Attention Losslessly Encodes Regular Domains) Let K ⊆ C([0 : T ], RdY ) satisfy Assumption 2. Then, there exists an N ∈ N+ and a parameter θ as in Definition 3 such that, attnθT restricts to a bi-Lipschitz embedding of (K, ∥ · ∥T ) into (RN+1, ∥ · ∥2)"
- Break condition: If K contains fractal-like subsets that cannot be losslessly embedded in finite dimensions

### Mechanism 2
- Claim: The geometric attention mechanism can decode Euclidean features back into Gaussian measures in the 2-Wasserstein space with controlled error.
- Mechanism: The geometric attention mechanism constructs a weighted combination of Gaussian distributions using the softmax-transformed attention weights as mixing coefficients, ensuring the output lies in the Wasserstein space of Gaussian measures.
- Core assumption: The target space of Gaussian measures forms a barycentric QAS space under the 2-Wasserstein metric.
- Evidence anchors:
  - [abstract]: "The latter attention mechanism is tailored to the geometry of Gaussian measures in the 2-Wasserstein space"
  - [section 6.4.1]: "Lemma 14 (RdX × Sym0,dX , Q, η) is a barycentric QAS space" and shows the geometric attention mechanism implements the required mixing function
- Break condition: If the mixing function η fails to satisfy the QAS space conditions or if the geometric attention mechanism cannot maintain Lipschitz continuity

### Mechanism 3
- Claim: The optimal filter is locally Lipschitz continuous in both path and time components, enabling uniform approximation by deep networks.
- Mechanism: The filter parameters (mean and covariance) satisfy integral equations whose solutions inherit Lipschitz properties from the dynamics coefficients, allowing application of universal approximation theorems.
- Core assumption: The coupled system satisfies Assumption 1 regularity conditions (Lipschitz dynamics, integrability, positive definiteness).
- Evidence anchors:
  - [section 4.1]: "Proposition 3 (Local Lipschitz-Continuity of the Optimal Filter) Under Assumption 1, ft from (1) is locally Lipschitz-continuous"
  - [section 6.2]: Detailed proof showing Lipschitz continuity of µ and Σ in both path and time components through Grönwall's inequality and stability estimates
- Break condition: If the dynamics violate the Lipschitz or integrability conditions in Assumption 1

## Foundational Learning

- Concept: Bi-Lipschitz embeddings and doubling metric spaces
  - Why needed here: The pathwise attention mechanism requires K to be a doubling metric space to guarantee stable, lossless encoding without dimension reduction error
  - Quick check question: Why can't we simply use PCA or other linear dimension reduction techniques on the path space?

- Concept: Barycentric QAS spaces and Wasserstein geometry
  - Why needed here: The geometric attention mechanism needs the 2-Wasserstein space of Gaussian measures to have a QAS structure to enable universal approximation
  - Quick check question: What property of Gaussian measures makes the 2-Wasserstein space particularly suitable for this geometric attention approach?

- Concept: Local Lipschitz continuity and integral equations
  - Why needed here: The universal approximation theorem requires the filter to be locally Lipschitz so that standard approximation results apply
  - Quick check question: How does Grönwall's inequality help establish the Lipschitz continuity of the filter parameters?

## Architecture Onboarding

- Component map:
  Input -> Pathwise attention -> MLP -> Geometric attention -> Output

- Critical path: Input → Pathwise attention → MLP → Geometric attention → Output
  The pathwise attention layer is the critical innovation that enables lossless encoding of continuous paths

- Design tradeoffs:
  - Encoding dimension N vs. approximation accuracy: Larger N provides better approximation but increases computational cost
  - Reference path dictionary size: More reference paths improve encoding quality but increase attention computation
  - MLP depth/width: Deeper networks can capture more complex relationships but risk overfitting

- Failure signatures:
  - Poor approximation quality despite sufficient parameters → Check if K satisfies Assumption 2 regularity conditions
  - Numerical instability in geometric attention → Verify positive definiteness of covariance matrices
  - Training divergence → Check Lipschitz constants in Assumption 1 are satisfied

- First 3 experiments:
  1. Verify bi-Lipschitz property: Generate synthetic paths satisfying Assumption 2 and confirm the pathwise attention mechanism provides stable, injective encoding
  2. Test geometric attention decoding: Create a simple example where ground truth Gaussian measures are known and verify the geometric attention mechanism can recover them from features
  3. End-to-end filtering: Implement a simple conditionally Gaussian filtering problem and verify the full DKF architecture can approximate the optimal filter to reasonable accuracy

## Open Questions the Paper Calls Out
- Can the proposed DKF model be extended to handle non-Gaussian observation processes beyond conditionally Gaussian settings?
- How does the choice of the activation function in the MLP layers impact the approximation capacity of the DKF model?
- Can the DKF model be trained efficiently in one-shot learning scenarios where only a single training path is available?

## Limitations
- The regularity assumptions on path space (Assumption 2) may exclude important real-world signal classes with fractal-like behavior
- The finite-dimensional encoding requires careful selection of reference path dictionaries
- The uniform approximation guarantees depend on the compact subset K being sufficiently regular

## Confidence
- Pathwise attention mechanism: High confidence
- Geometric attention mechanism: Medium confidence
- Filter stability analysis: Medium-High confidence

## Next Checks
1. Implement synthetic path generation for various regularity classes and verify the pathwise attention mechanism maintains the bi-Lipschitz property empirically
2. Construct a controlled test case where ground truth Gaussian measures are known and verify the geometric attention mechanism correctly decodes the weighted combination of Gaussian distributions in the Wasserstein space
3. For simple conditionally Gaussian filtering problems, numerically verify the Lipschitz continuity of filter parameters as a function of both path and time