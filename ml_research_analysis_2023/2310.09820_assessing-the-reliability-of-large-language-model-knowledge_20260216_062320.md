---
ver: rpa2
title: Assessing the Reliability of Large Language Model Knowledge
arxiv_id: '2310.09820'
source_url: https://arxiv.org/abs/2310.09820
tags:
- llms
- monitor
- knowledge
- language
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of assessing the reliability of
  factual knowledge in large language models (LLMs), which is often overlooked by
  standard accuracy metrics. It introduces MONITOR, a novel metric that quantifies
  the distance between the probability distributions of correct answers and those
  produced by the same LLM under different prompt and context variations.
---

# Assessing the Reliability of Large Language Model Knowledge

## Quick Facts
- arXiv ID: 2310.09820
- Source URL: https://arxiv.org/abs/2310.09820
- Reference count: 14
- Key outcome: MONITOR achieves 0.846 Pearson correlation with accuracy while better differentiating LLMs with similar accuracy but different reliability profiles

## Executive Summary
This paper introduces MONITOR, a novel metric for assessing the reliability of factual knowledge in large language models. Unlike standard accuracy metrics, MONITOR quantifies knowledge reliability by measuring the distance between probability distributions of correct answers and those produced under different prompt and context variations. The metric captures both prompt framing effects and in-context interference, providing a more nuanced view of LLM knowledge reliability. Experiments on 12 LLMs and a comprehensive test set of 210,158 prompts demonstrate that MONITOR significantly correlates with average accuracy (0.846 Pearson coefficient) while being more sensitive to knowledge reliability issues.

## Method Summary
MONITOR computes the distance between probability distributions of valid outputs and their counterparts produced by the same LLM under different prompt styles and contexts. The metric consists of two distance-based measurement components: Prompt-framing Degree (PFD) and Interference-relevance Degree (IRD). PFD captures how different prompt framings affect the probability distribution of answers, while IRD measures the impact of in-context interference. The final MONITOR score combines these components with contribution coefficients, providing a single reliability metric that requires less computational resources than full-scale accuracy evaluation while offering better differentiation between models with similar average accuracy.

## Key Results
- MONITOR achieves 0.846 Pearson correlation with average accuracy across 12 LLMs
- MONITOR can differentiate LLMs with similar accuracy but different reliability profiles (e.g., BLOOMZ-3b vs Vicuna-7b)
- MONITOR follows scaling laws, showing that larger models tend to be more reliable

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MONITOR uses probability distribution distances to detect knowledge reliability issues that accuracy metrics miss.
- Mechanism: By comparing output probability distributions of correct answers with those produced under different prompts, MONITOR captures both prompt framing effects and in-context interference as changes in probability distributions.
- Core assumption: Changes in output probability distributions reliably indicate changes in knowledge reliability.
- Evidence anchors:
  - [abstract]: "MONITOR computes the distance between the probability distributions of a valid output and its counterparts produced by the same LLM probing the same fact using different styles of prompts and contexts."
  - [section]: "MONITOR consists of two distance-based measurement components: Prompt-framing Degree (PFD) and Interference-relevance Degree (IRD)."
  - [corpus]: Weak - no direct corpus evidence for this specific claim, but related papers on knowledge probing consistency support the general approach.
- Break condition: If LLM output probabilities become uniform regardless of input variations, or if the relationship between probability changes and knowledge reliability breaks down.

### Mechanism 2
- Claim: MONITOR achieves better differentiation between LLMs with similar accuracy but different reliability profiles.
- Mechanism: By incorporating both the magnitude of probability changes (distance) and the strength of primary anchors, MONITOR can distinguish between models that achieve similar accuracy through different reliability patterns.
- Core assumption: The combination of probability distribution distance and anchor strength provides a more complete picture of knowledge reliability than accuracy alone.
- Evidence anchors:
  - [section]: "As shown in Table 5 (bold italic fonts), MONITOR can differentiate LLMs, for example, BLOOMZ-3b and Vicuna-7b, with a similar average accuracy on P37, by considering distance and probability information."
  - [section]: "It is recommended to adopt MONITOR when using accuracy alone cannot differentiate LLMs' knowledge reliability."
  - [corpus]: Moderate - related papers on LLM consistency measurement support the need for metrics beyond accuracy.
- Break condition: If the relationship between MONITOR values and actual reliability breaks down, or if probability distributions become uninformative.

### Mechanism 3
- Claim: MONITOR follows scaling laws, where larger models tend to be more reliable.
- Mechanism: By aggregating MONITOR scores across multiple test cases, the metric shows that larger LLMs consistently achieve lower (better) MONITOR scores.
- Core assumption: Model size correlates with knowledge reliability, and this relationship is captured by MONITOR.
- Evidence anchors:
  - [section]: "Further analyses show that MONITOR can address the 'accuracy instability' issue when used along with an end-to-end point measurement (like accuracy)."
  - [section]: "The BLOOMZ and Vicuna series adhere to the scale law based on the overall MONITOR results obtained from experiments on 20 test datasets."
  - [corpus]: Moderate - scaling laws are well-established in LLM literature, supporting this claim.
- Break condition: If MONITOR scores stop correlating with model size, or if larger models show worse MONITOR scores.

## Foundational Learning

- Concept: Probability distributions as knowledge representations
  - Why needed here: MONITOR relies on comparing probability distributions rather than just top-1 predictions to capture uncertainty and reliability
  - Quick check question: Why is using the full probability distribution more informative than just using the top-1 prediction?

- Concept: Distance metrics for probability distributions
  - Why needed here: MONITOR uses distance calculations between probability distributions to quantify reliability differences
  - Quick check question: What distance metric is used to compare probability distributions in MONITOR?

- Concept: Scaling laws in language models
  - Why needed here: MONITOR's correlation with model size relies on understanding scaling relationships in LLM performance
  - Quick check question: What is the general relationship between model size and performance in language models?

## Architecture Onboarding

- Component map: LLM output generation -> Probability distribution extraction -> Primary anchor generation -> Foreign anchor generation -> PFD calculation -> IRD calculation -> MONITOR score integration

- Critical path: 1) Generate primary anchors using correct answers, 2) Generate foreign anchors under different prompt conditions, 3) Calculate PFD and IRD, 4) Combine into final MONITOR score

- Design tradeoffs: MONITOR trades some computational complexity for higher resolution reliability measurement. It requires more computation than simple accuracy but less than full-scale reliability studies.

- Failure signatures: MONITOR fails when probability distributions become uninformative, when prompt variations don't affect output distributions, or when the relationship between probability changes and reliability breaks down.

- First 3 experiments:
  1. Verify that MONITOR correctly identifies known reliability differences between two LLMs on a simple test set
  2. Test MONITOR's sensitivity to different types of prompt variations
  3. Confirm that MONITOR correlates with accuracy while providing additional differentiation between similar-accuracy models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MONITOR generalize to non-triplet knowledge tasks like summarization or reasoning?
- Basis in paper: [inferred] The authors note that MONITOR focuses on factual knowledge probing and suggest future work on generalizing to broader tasks.
- Why unresolved: The metric's reliance on anchors and probability distributions is specifically designed for structured knowledge probing; generalization to open-ended generation tasks remains untested.
- What evidence would resolve it: Experimental validation of MONITOR on summarization, reasoning, or other non-triplet tasks showing correlation with reliability.

### Open Question 2
- Question: What is the optimal configuration of the contribution coefficients (α1, α2, α3) in the MONITOR formula?
- Basis in paper: [explicit] The authors state that "the initial setup of contribution coefficients of PFD, IRD, and their interaction on MONITOR should be further investigated to establish an empirical benchmark."
- Why unresolved: The paper uses equal coefficients (0.33) for simplicity, but their relative importance may vary across tasks or LLM architectures.
- What evidence would resolve it: Systematic ablation studies varying α1, α2, α3 across diverse tasks to identify optimal configurations.

### Open Question 3
- Question: How does MONITOR perform when anchors are sentences rather than single tokens?
- Basis in paper: [explicit] The authors note that "Currently MONITOR applies exact matching to obtain anchors to measure the reliability of LLM knowledge. Extending the automatic evaluation to anchors consisting of sentences is challenging."
- Why unresolved: Exact matching works for single-token anchors but may not scale to multi-token or sentence-level anchors without additional techniques.
- What evidence would resolve it: Modified MONITOR implementations handling sentence-level anchors and their comparative performance to token-level anchors.

## Limitations
- MONITOR's claims about superiority rely heavily on correlation with accuracy (0.846) and the assumption that probability distribution changes reliably indicate knowledge reliability issues
- The experimental setup focuses on a specific knowledge probing dataset (FKTC) and may not generalize to other types of factual knowledge or different LLM architectures
- The study does not validate whether MONITOR actually captures "true" reliability beyond its correlation with accuracy metrics

## Confidence

**High Confidence**: The correlation between MONITOR and accuracy (0.846 Pearson coefficient) is well-supported by experimental results across 12 LLMs and 210,158 prompts.

**Medium Confidence**: The claim that MONITOR better differentiates LLMs with similar accuracy but different reliability profiles is supported by case studies but lacks comprehensive statistical validation across all model pairs.

**Medium Confidence**: The scaling law observations (larger models being more reliable) are consistent with established LLM literature, though the specific MONITOR values and their relationship to model size could benefit from additional validation.

## Next Checks

1. **External Dataset Validation**: Test MONITOR on a completely different knowledge base (e.g., open-domain question answering datasets) to verify whether the 0.846 correlation holds across different knowledge domains and question types.

2. **Temporal Stability Analysis**: Evaluate whether MONITOR scores remain stable over time as LLMs are updated or fine-tuned, particularly for models that show similar accuracy but different MONITOR values, to confirm that the metric captures persistent reliability differences rather than transient performance variations.

3. **Human Evaluation Correlation**: Conduct human studies to assess whether MONITOR scores align with human judgments of knowledge reliability, particularly for cases where MONITOR differentiates models that accuracy metrics cannot distinguish.