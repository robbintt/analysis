---
ver: rpa2
title: Bures-Wasserstein Means of Graphs
arxiv_id: '2305.19738'
source_url: https://arxiv.org/abs/2305.19738
tags:
- graph
- graphs
- mean
- bures-wasserstein
- distance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Bures-Wasserstein graph mean, a novel
  method for computing the mean of a set of graphs by embedding them into the space
  of smooth graph signal distributions and applying optimal transport theory. The
  method defines a distance between graphs using the Wasserstein metric on their corresponding
  normal distributions, and computes the mean as the solution to a barycenter problem.
---

# Bures-Wasserstein Means of Graphs

## Quick Facts
- arXiv ID: 2305.19738
- Source URL: https://arxiv.org/abs/2305.19738
- Authors: 
- Reference count: 40
- Key outcome: Introduces Bures-Wasserstein graph mean via smooth graph signal distributions and optimal transport theory

## Executive Summary
This paper proposes a novel framework for computing the mean of a set of graphs by embedding them into the space of smooth graph signal distributions and applying optimal transport theory. The method defines a distance between graphs using the Wasserstein metric on their corresponding normal distributions, and computes the mean as the solution to a barycenter problem. Theoretical results establish the existence and uniqueness of the mean, and an iterative algorithm is provided for its computation. The approach is evaluated on multiple graph learning tasks, including k-means clustering, brain network classification, and semi-supervised node classification, where it consistently outperforms baseline methods and improves state-of-the-art performance.

## Method Summary
The Bures-Wasserstein graph mean framework transforms graph Laplacian matrices into smooth graph signal distributions by computing their pseudo-inverses and adding a regularization term to ensure positive definiteness. The Wasserstein distance between these distributions provides a principled similarity measure that captures both local and global structural patterns. An iterative fixed-point algorithm computes the mean graph by solving a matrix equation involving matrix inverses and square roots. The method requires labeled graphs with known node correspondence and is evaluated on synthetic and real-world datasets across multiple graph learning tasks.

## Key Results
- Bures-Wasserstein mean consistently outperforms arithmetic, harmonic, and geometric means in k-means clustering tasks
- Preserves graph metrics (degree centrality, modularity, participation coefficient) better than alternative methods
- Achieves state-of-the-art performance in brain network classification and semi-supervised node classification
- Method converges for synthetic datasets and provides meaningful graph embeddings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Bures-Wasserstein mean preserves structural information by embedding graphs into smooth graph signal distributions where optimal transport distances naturally capture both local and global connectivity patterns.
- Mechanism: Graphs are represented by zero-mean normal distributions with covariance matrices equal to the pseudo-inverse of the graph Laplacian. The Wasserstein distance between these distributions (which is equivalent to the Bures-Wasserstein metric on covariance matrices) provides a principled way to measure graph similarity that accounts for both community structure and inter-community connections.
- Core assumption: Graph Laplacian matrices satisfy Assumption 3.1 (positive semi-definite with single zero eigenvalue), ensuring the smooth graph signal distributions are well-defined and degenerate in a consistent subspace.
- Evidence anchors:
  - [abstract] "propose a novel framework for defining a graph mean via embeddings in the space of smooth graph signal distributions, where graph similarity can be measured using the Wasserstein metric"
  - [section 3] "the Bures-Wasserstein distance for graphs is inspired from graph signal processing" and "smooth graph signal distributions"
  - [corpus] Weak evidence - corpus mentions "Bures-Wasserstein" and "graph generation" but lacks specific detail about this mechanism
- Break condition: Fails when graphs have different node labels or when the node correspondence is unknown, as the framework requires labeled graphs with known matching between nodes.

### Mechanism 2
- Claim: The iterative algorithm efficiently computes the Bures-Wasserstein mean by transforming singular graph Laplacians into positive definite matrices, enabling stable matrix operations.
- Mechanism: Proposition 3.4 shows that adding 1/N times the all-ones matrix to the pseudo-inverse of the Laplacian creates a positive definite matrix. The fixed-point iteration then operates on these transformed matrices, computing matrix square roots and inverses without dealing with singular matrices directly.
- Core assumption: The transformation preserves the essential graph structure while making the optimization problem computationally tractable.
- Evidence anchors:
  - [section 3] "Proposition 3.4 permits to transform the singular graph Laplacians into positive definite matrices. This makes the algorithm efficient and stable"
  - [section 4.1] "the Bures-Wasserstein mean preserves all tested graph metrics well" indicating the transformation doesn't lose important information
  - [corpus] Weak evidence - corpus doesn't discuss the computational aspects of this mechanism
- Break condition: The algorithm may become computationally expensive for very large graphs due to O(N³) complexity per iteration, though this can be mitigated through parallelization.

### Mechanism 3
- Claim: The Bures-Wasserstein mean provides superior performance in clustering and classification tasks because it captures both local and global structural information that other graph means miss.
- Mechanism: Unlike arithmetic means (which average edge weights locally) or harmonic means (which emphasize global structure), the Bures-Wasserstein mean balances both perspectives through the optimal transport framework. This is demonstrated in k-means clustering where it significantly outperforms other methods as the number of clusters increases and intra-community connectivity decreases.
- Core assumption: Real-world graph data often exhibits both local and global structural patterns that need to be preserved in the mean representation.
- Evidence anchors:
  - [section 4.2] "the Bures-Wasserstein method significantly outperforms the methods based on arithmetic and harmonic means" in k-means clustering
  - [section 4.1] Table 1 shows the Bures-Wasserstein mean preserves degree centrality, modularity, and participation coefficient better than alternatives
  - [section 4.4] "our proposed Bures-Wasserstein mean consistently outperforms state-of-the-art methods" in semi-supervised learning
  - [corpus] Moderate evidence - corpus includes "Deep Temporal Graph Clustering" and "Machine Learning on Dynamic Graphs" suggesting relevance to graph learning tasks
- Break condition: Performance degrades when the assumption of smooth graph signals doesn't hold, such as in graphs with very sparse or irregular connectivity patterns.

## Foundational Learning

- Concept: Optimal transport theory and Wasserstein distance
  - Why needed here: Forms the mathematical foundation for measuring graph similarity in the embedding space
  - Quick check question: Can you explain why the Wasserstein distance between two Gaussian distributions has a closed-form solution involving the trace and matrix square root operations?

- Concept: Graph signal processing and graph Laplacians
  - Why needed here: Provides the connection between graph structure and smooth signal distributions
  - Quick check question: What properties must a graph Laplacian satisfy for its pseudo-inverse to define a valid Gaussian distribution for smooth graph signals?

- Concept: Barycenter problems and Fréchet means
  - Why needed here: The graph mean is formulated as a Wasserstein barycenter problem in the embedding space
  - Quick check question: How does the barycenter of Gaussian distributions differ from the barycenter of general probability measures in terms of computational tractability?

## Architecture Onboarding

- Component map: Graph -> Laplacian -> Pseudo-inverse -> Distribution -> Wasserstein distance -> Barycenter optimization -> Mean graph
- Critical path: Graph → Laplacian → Pseudo-inverse → Distribution → Wasserstein distance → Barycenter optimization → Mean graph
- Design tradeoffs: Using filtered graph distances (Section A) provides flexibility but increases computational complexity; the positive definite transformation (Proposition 3.4) improves stability but requires additional memory.
- Failure signatures: Algorithm convergence issues indicate numerical instability in matrix operations; poor clustering performance suggests the graph embedding doesn't capture relevant structural features.
- First 3 experiments:
  1. Verify Proposition 3.4 by testing the positive definiteness of L† + (1/N)11ᵀ for various connected graphs
  2. Compare Bures-Wasserstein distance computation with Frobenius norm on simple graph pairs to validate the embedding approach
  3. Implement the two-graph interpolation (Theorem 3.5) and verify the geodesic properties by checking that intermediate graphs maintain structural coherence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the convergence rate of Algorithm 1 be theoretically analyzed?
- Basis in paper: [explicit] The paper states "to our knowledge there is currently no theoretical analysis available regarding the convergence rate" of the fixed-point iteration.
- Why unresolved: The paper relies on a fixed-point iteration for solving the matrix equation, but lacks theoretical guarantees on how fast it converges.
- What evidence would resolve it: A rigorous mathematical proof establishing the convergence rate (e.g., linear, sublinear) of the fixed-point iteration in Algorithm 1.

### Open Question 2
- Question: How can the computational complexity of Algorithm 1 be improved for large graphs?
- Basis in paper: [explicit] The paper mentions that each iteration has O(mN³) complexity and suggests investigating approximations of matrix inverse and roots or exploiting graph sparsity.
- Why unresolved: The current cubic dependence on N makes the algorithm computationally expensive for large graphs, and the paper only suggests potential directions for improvement without providing concrete solutions.
- What evidence would resolve it: A detailed analysis of approximation techniques or sparse matrix methods that reduce the computational complexity while maintaining accuracy.

### Open Question 3
- Question: How can the Bures-Wasserstein framework be extended to graphs with different sizes and without known node correspondence?
- Basis in paper: [explicit] The paper states that the methods are limited to labeled graphs with known node matching and suggests developing computational methods to extend the approach to unlabeled graphs of different sizes.
- Why unresolved: Many real-world applications involve graphs that are not aligned and may have different numbers of nodes, but the current framework cannot handle this setting.
- What evidence would resolve it: A novel algorithm or mathematical framework that generalizes the Bures-Wasserstein mean to handle graphs of different sizes and without known node correspondence, along with experimental validation on such datasets.

## Limitations
- Method requires labeled graphs with known node correspondence, limiting applicability to many real-world datasets
- O(N³) computational complexity per iteration may become prohibitive for large-scale graphs
- Assumes smooth graph signals, potentially degrading performance on graphs with very sparse or irregular connectivity

## Confidence
- **High**: Mathematical foundations (optimal transport theory, graph signal processing) are well-established and correctly applied
- **Medium**: Empirical performance claims, as they depend on specific datasets and implementation details that weren't fully specified
- **Medium**: Computational complexity claims, as they assume standard matrix operation implementations

## Next Checks
1. **Convergence verification**: Test Algorithm 1 on synthetic graph datasets with known ground truth means to verify convergence behavior and solution quality across different graph sizes and structures.

2. **Node correspondence robustness**: Evaluate performance degradation when applying the method to graphs with shuffled node labels, quantifying the sensitivity to label permutations.

3. **Scalability analysis**: Benchmark the computational efficiency on progressively larger graphs (up to 10,000 nodes) and compare against alternative graph mean methods to validate complexity claims.