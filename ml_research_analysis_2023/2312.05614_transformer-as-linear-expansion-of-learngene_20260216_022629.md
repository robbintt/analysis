---
ver: rpa2
title: Transformer as Linear Expansion of Learngene
arxiv_id: '2312.05614'
source_url: https://arxiv.org/abs/2312.05614
tags:
- layer
- tleg
- pre-fin
- learngene
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TLEG proposes expanding the shared Transformer module to produce
  and initialize Transformers of varying depths, enabling adaptation to diverse resource
  constraints. Drawing an analogy to genetic expansibility, such module is termed
  as learngene.
---

# Transformer as Linear Expansion of Learngene

## Quick Facts
- arXiv ID: 2312.05614
- Source URL: https://arxiv.org/abs/2312.05614
- Reference count: 14
- One-line primary result: TLEG achieves comparable or better performance than individual models trained from scratch while reducing training costs by ~2× and parameter storage by ~19×

## Executive Summary
This paper introduces TLEG (Transformer as Linear Expansion of Learngene), a novel approach for efficiently initializing and adapting Vision Transformer models across varying depths and resource constraints. Drawing an analogy to genetic expansibility, TLEG proposes learning a compact "learngene" module that can be linearly expanded to produce Transformers of arbitrary depths. The method leverages an approximately linear relationship between layer position and weight values, discovered through empirical analysis of trained Transformers. Extensive experiments on ImageNet-1K and several downstream datasets demonstrate that TLEG achieves comparable or better performance than individual models trained from scratch while significantly reducing training costs and parameter storage requirements.

## Method Summary
TLEG learns a compact "learngene" module containing two full Transformer layers (θA and θB) that can be linearly expanded to initialize Transformers of varying depths. The method involves training an auxiliary Transformer (Aux-Net) under linear constraints using soft distillation from a pretrained ancestry model. Learngene parameters are then extracted and used to initialize descendant models of desired depths through linear expansion. These initialized models are fine-tuned normally without the linear constraint. The approach achieves significant reductions in training costs (2×) and parameter storage (19×) while maintaining or improving performance across diverse downstream scenarios.

## Key Results
- Achieves comparable or better performance than individual models trained from scratch on ImageNet-1K
- Reduces training costs by approximately 2× through efficient initialization
- Reduces parameter storage by approximately 19× when producing models of varying depths
- Outperforms existing initialization methods by large margins on downstream tasks (e.g., +6.87% on iNat 2019 and +7.66% on CIFAR-100)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learngene parameters (θA and θB) can be linearly expanded to initialize Transformer layers of arbitrary depth.
- Mechanism: Learngene is learned once via soft distillation from an ancestry model, then linearly expanded across layers using Eq. (1): θl = θB + (l-1)/L × θA. This produces distinct yet related parameters for each layer.
- Core assumption: The layer position to parameter value relationship is approximately linear in trained Transformers.
- Evidence anchors:
  - [abstract] "we empirically discover an approximately linear relationship between the position of a layer and its corresponding weight value within well-trained Transformer models."
  - [section] "a noteworthy observation emerges: most data points do not exhibit irregular arrangements, instead they manifest an approximately linear trend."
- Break condition: If the linear approximation fails in deeper or specialized architectures, initialization quality degrades.

### Mechanism 2
- Claim: Training the auxiliary model under linear constraints allows learning θA and θB that encode generalizable knowledge.
- Mechanism: An auxiliary Transformer is built with layers linearly expanded from θA and θB, then trained via soft distillation from a pretrained ancestry model. Only θA and θB are updated during training.
- Core assumption: Distillation loss effectively transfers knowledge while preserving the linear expansion constraint.
- Evidence anchors:
  - [abstract] "to ensure clarity, we exemplify the construction process using a 4-layer Aux-Net... Then we proceed to train the Aux-Net by employing distillation technique (Hinton, Vinyals, and Dean 2015), which enables knowledge condensation from a large ancestry model."
- Break condition: If the linear constraint during training over-constrains the model, performance may suffer.

### Mechanism 3
- Claim: Descendant models initialized with linearly expanded parameters can be fine-tuned normally without the linear constraint.
- Mechanism: After initialization, the linear constraint is removed, allowing normal gradient updates per layer.
- Core assumption: The initialization provides a good starting point for subsequent fine-tuning.
- Evidence anchors:
  - [abstract] "After obtaining learngene containing well-trained θA and θB, we can initialize the descendant models... Note that different from the Aux-Net trained under the linear constraint, the descendant models are only initialized by using Eq. (6) to Eq. (8). After initialization, this constraint is removed..."
- Break condition: If the initial linear expansion is too restrictive, fine-tuning may not recover performance.

## Foundational Learning

- Concept: Linear approximation of layer-parameter relationships
  - Why needed here: TLEG relies on linear expansion; understanding why this works (or doesn't) is key to applying or adapting the method.
  - Quick check question: Can you plot the first principal component of each layer's parameters for a trained Transformer and check if they align approximately linearly?

- Concept: Knowledge distillation for model compression and initialization
  - Why needed here: TLEG uses distillation to learn the compact learngene; knowing how distillation transfers knowledge helps tune the λ hyperparameter and understand limitations.
  - Quick check question: If you increase the distillation temperature τ, how does the KL loss behave and what effect might that have on the learned θA and θB?

- Concept: Parameter sharing and expansion strategies
  - Why needed here: TLEG sits between full parameter sharing (no diversity) and fully independent parameters (no efficiency). Understanding this tradeoff helps decide where to apply linear expansion (MSA, MLP, LN).
  - Quick check question: What happens to performance if you only linearly expand MSA but randomly initialize MLP and LN in the auxiliary model?

## Architecture Onboarding

- Component map:
  - Learngene: Compact set {θA, θB} containing two full Transformer layers
  - Auxiliary Net: L-layer model built by linearly expanding learngene; trained via distillation
  - Descendant Net: Any depth model initialized by linearly expanding learngene; fine-tuned normally
  - Core modules expanded: MSA, MLP, LN (optionally)

- Critical path:
  1. Build and train auxiliary model (Aux-Net) under linear constraint
  2. Extract learngene (θA, θB)
  3. Initialize descendant models of desired depth via linear expansion
  4. Fine-tune descendant models normally

- Design tradeoffs:
  - Linear expansion offers efficiency but may limit expressiveness vs. full pretraining
  - Only distilling to learngene may miss some fine-grained layer-specific patterns
  - Flexibility to choose which modules to expand (MSA, MLP, LN) allows balancing performance and parameter efficiency

- Failure signatures:
  - Poor initialization: If TLEG underperforms random init, likely the linear approximation is weak for that architecture
  - Over-constraining: If Aux-Net training diverges or θA/θB do not improve, the linear constraint may be too restrictive
  - Suboptimal fine-tuning: If descendant models plateau early, the initialization may be too biased

- First 3 experiments:
  1. Validate the linear relationship: Run PCA on each layer's parameters of a pretrained ViT-B, plot first component vs layer index, fit a line
  2. Test module-level expansion: Build Aux-S expanding only MSA, only MLP, only LN, and all three; compare performance of resulting descendant models
  3. Ablation on initialization scope: Initialize only the first half, second half, or all layers of a 6-layer descendant model and measure fine-tuning performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the linear expansion strategy scale to extremely deep Transformers (e.g., >100 layers)?
- Basis in paper: [explicit] The paper mentions linear expansion for varying depths but doesn't explore extreme depths.
- Why unresolved: The experiments only tested up to 12 layers, so the behavior at extreme depths is unknown.
- What evidence would resolve it: Experiments showing performance and stability of TLEG when scaling to 50+ layers.

### Open Question 2
- Question: Can TLEG be effectively applied to non-Transformer architectures like CNNs or MLP-Mixers?
- Basis in paper: [inferred] The paper focuses exclusively on Transformers and doesn't test generalization to other architectures.
- Why unresolved: The linear relationship observed may be specific to Transformers' layer structure.
- What evidence would resolve it: Applying TLEG to CNNs/MLP-Mixers and comparing performance to baseline initialization methods.

### Open Question 3
- Question: What is the optimal trade-off between model depth and linear expansion parameters for specific tasks?
- Basis in paper: [explicit] The paper shows TLEG works across different depths but doesn't explore optimal depth-parameter trade-offs.
- Why unresolved: The relationship between depth and performance isn't fully characterized, especially for task-specific optimization.
- What evidence would resolve it: A systematic study varying both depth and expansion parameters to find optimal combinations for different tasks.

## Limitations

- The linear approximation of layer-parameter relationships may not hold for all architectures or tasks
- The method's effectiveness for extremely deep models (>36 layers) remains unclear
- Specific hyperparameters for distillation (temperature τ and trade-off coefficient λ) are not fully specified

## Confidence

- **High confidence**: The concept of using linear expansion to create variable-depth models from a compact representation is well-supported by experimental results
- **Medium confidence**: The linear approximation of layer-parameter relationships is supported by empirical observations but may not generalize to all architectures or tasks
- **Low confidence**: The method's performance on extremely deep models and specialized architectures is not well-established

## Next Checks

1. Validate the linear relationship: Run PCA on each layer's parameters of a pretrained ViT-B, plot the first component vs layer index, and fit a line to verify the approximately linear trend claimed in the paper

2. Test module-level expansion: Build Aux-S expanding only MSA, only MLP, only LN, and all three modules; compare the performance of resulting descendant models to determine which modules benefit most from linear expansion

3. Ablation on initialization scope: Initialize only the first half, second half, or all layers of a 6-layer descendant model and measure fine-tuning performance to understand how the scope of linear expansion affects model quality