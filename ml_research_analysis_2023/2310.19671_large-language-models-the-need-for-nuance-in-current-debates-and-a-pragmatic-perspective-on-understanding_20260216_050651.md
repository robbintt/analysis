---
ver: rpa2
title: 'Large Language Models: The Need for Nuance in Current Debates and a Pragmatic
  Perspective on Understanding'
arxiv_id: '2310.19671'
source_url: https://arxiv.org/abs/2310.19671
tags:
- llms
- language
- mental
- arxiv
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper critically examines three recurring critiques of Large
  Language Models (LLMs): that they only parrot statistical patterns, master formal
  but not functional language competence, and cannot inform human language learning.
  The authors argue these points need nuance, drawing on empirical and theoretical
  evidence.'
---

# Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding

## Quick Facts
- arXiv ID: 2310.19671
- Source URL: https://arxiv.org/abs/2310.19671
- Reference count: 28
- Large Language Models (LLMs) develop complex internal representations beyond simple pattern matching, and mental state attribution to them serves pragmatic purposes in human interaction

## Executive Summary
This paper critically examines three recurring critiques of Large Language Models (LLMs): that they only parrot statistical patterns, master formal but not functional language competence, and cannot inform human language learning. The authors argue these points need nuance, drawing on empirical and theoretical evidence. They then develop a pragmatist perspective on understanding and intentionality in LLMs, suggesting these unobservable mental states are attributed based on pragmatic value - they simplify complex underlying mechanics and allow effective prediction of behavior. The paper concludes that as LLMs become more prominent in society, interacting with them using mental state attribution will likely become more common, though this practice may be less acceptable in scientific communities due to different pragmatic values.

## Method Summary
The paper uses a critical analysis approach, drawing on empirical and theoretical arguments to challenge three key critiques of LLMs. It then develops a pragmatist philosophical perspective on understanding and intentionality, drawing on work by Daniel Dennett, Richard Rorty, and others. The analysis examines empirical evidence about LLM representations, the confound between language and thought in current evaluations, and the pragmatic value of mental state attribution in human-LLM interaction.

## Key Results
- LLMs develop complex internal representations including syntactic parsing and semantic relations that emerge from their training objective
- Current evaluation methods confound language and thought in LLMs, making it impossible to assess whether LLMs possess "real" thought independent of language processing
- Mental state attribution to LLMs serves pragmatic purposes in human interaction, similar to how humans attribute mental states to other humans

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLMs do more than next-word prediction; they internally represent complex syntactic and semantic information that emerges from their training objective.
- **Mechanism**: During training on massive text datasets, LLMs learn to hierarchically parse input (syntactic representation) and capture semantic relations through vector representations of words. These emergent properties are not explicitly trained but arise as the model improves at predicting masked words.
- **Core assumption**: The web text used to train LLMs contains rich linguistic information that LLMs can partially reconstruct through their training objective.
- **Evidence anchors**:
  - [abstract] "they also learn semantic relations (Rogers et al., 2021), which are useful features in summarising, question-answering, and translating text."
  - [section] "LLMs are capable of internally parsing the example into syntactic chunks...the vector representations of words that neural networks induce are shown to be context-sensitive and rich enough to capture conceptual relations in line with human judgements"
  - [corpus] Weak evidence - no corpus entries directly address the complexity of LLM internal representations beyond basic performance metrics.
- **Break condition**: If training datasets lack sufficient linguistic diversity or complexity, LLMs cannot reconstruct the necessary information for advanced representation.

### Mechanism 2
- **Claim**: Language and thought in LLMs are confounded in current evaluation methods, making it difficult to assess whether LLMs truly possess "thought" independent of language.
- **Mechanism**: Cognitive tests for LLMs inevitably rely on linguistic prompts, making it impossible to separate the model's language processing from any underlying "thought" processes. The distinction between formal language competence (linguistic rules) and functional language competence (reasoning, intentionality) cannot be cleanly operationalized.
- **Core assumption**: LLMs have distinct internal representations for language and thought that can be disentangled through testing, but current benchmarks cannot achieve this separation.
- **Evidence anchors**:
  - [abstract] "formal language competence concerns employing information about linguistic rules and patterns in producing coherent output, whereas functional language competence draws on further cognitive capacities"
  - [section] "for many cognitive test we have an idea of what the test operationalises, but not when they are used in the context of LLMs"
  - [corpus] Weak evidence - no corpus entries directly address the language-thought confound in LLM evaluation.
- **Break condition**: If new evaluation methods emerge that can present cognitive tasks without linguistic components, this mechanism would need revision.

### Mechanism 3
- **Claim**: Attributing mental states to LLMs has pragmatic value for humans in predicting behavior and achieving goals, similar to how we attribute mental states to other humans.
- **Mechanism**: Mental state attribution allows humans to abstract away from complex underlying mechanics and predict behavior effectively. As LLMs become more sophisticated in language output and context adaptation, humans are increasingly likely to treat them as social entities and attribute mental states accordingly.
- **Core assumption**: Mental states are not properties of the entity itself but useful constructs humans employ for pragmatic purposes in interaction.
- **Evidence anchors**:
  - [abstract] "Understanding and intentionality pertain to unobservable mental states we attribute to other humans because they have pragmatic value: they allow us to abstract away from complex underlying mechanics and predict behaviour effectively"
  - [section] "by attributing mental states to other humans, we abstract away from their underlying biophysical complexity, while still having a ground for anticipating future behaviour"
  - [corpus] Weak evidence - no corpus entries directly address pragmatic mental state attribution to LLMs.
- **Break condition**: If scientific communities establish that mental state attribution to LLMs is fundamentally incompatible with empirical investigation of their mechanisms.

## Foundational Learning

- **Concept: Pragmatism in philosophy**
  - Why needed here: The paper's core argument about mental state attribution to LLMs is grounded in pragmatist philosophy, which views mental states as useful constructs rather than literal descriptions of internal mechanisms.
  - Quick check question: Can you explain the difference between a pragmatist view of mental states and a behaviorist view?

- **Concept: The language-thought distinction**
  - Why needed here: The paper argues that current evaluations confound language and thought in LLMs, making it impossible to assess whether LLMs have "real" thought independent of language processing.
  - Quick check question: Why is it difficult to separate language and thought in LLM evaluations?

- **Concept: Emergent properties in neural networks**
  - Why needed here: The paper argues that LLMs develop complex representations (syntactic parsing, semantic relations) that emerge from simple training objectives, rather than being explicitly programmed.
  - Quick check question: How do emergent properties in neural networks differ from explicitly programmed features?

## Architecture Onboarding

- **Component map**: LLMs are deep neural networks with Transformer architecture, trained to predict masked words using massive text datasets. Key components include the attention mechanism, positional encodings, and feedforward layers that enable hierarchical representation.

- **Critical path**: Understanding LLM behavior requires examining both their training process (what linguistic information is available in the training data) and their evaluation methods (whether tests properly assess the capabilities they claim to measure).

- **Design tradeoffs**: The paper highlights tradeoffs between ecological validity in training data (more realistic amounts of data vs. massive datasets) and the sophistication of evaluation methods (simple pass/fail vs. nuanced confidence measures).

- **Failure signatures**: The paper identifies "underclaiming" as a failure mode where researchers downplay what LLMs can do, potentially hindering further research. Another failure mode is using evaluation methods that confound language and thought, leading to incorrect conclusions about LLM capabilities.

- **First 3 experiments**:
  1. Replicate the finding that LLM representations of word semantics align with human judgments by comparing vector distances in the model to human similarity ratings.
  2. Test whether paraphrasing cognitive test items improves LLM performance, as this would indicate language-thought confound in the original items.
  3. Train an LLM with more ecologically valid data (smaller, more realistic training corpus) and compare its performance to standard LLMs on tasks requiring linguistic knowledge.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we disentangle language and thought in LLMs when testing cognitive abilities?
- Basis in paper: [explicit] Section 2.2.1 discusses how current benchmarks confound language and thought in LLMs
- Why unresolved: Language and thought are inherently intertwined in LLMs, making it difficult to isolate thought capacity from linguistic ability
- What evidence would resolve it: Development of novel benchmarks that can test thought capacity without relying on linguistic prompts, or demonstration that LLMs can perform cognitive tasks using non-linguistic representations

### Open Question 2
- Question: What is the optimal amount of training data for LLMs to balance performance and ecological validity in language acquisition research?
- Basis in paper: [explicit] Section 2.3.1 mentions that LLMs are disadvantaged by receiving far more language input than children
- Why unresolved: Current LLMs are trained on massive datasets that far exceed the amount of input children receive, making direct comparisons difficult
- What evidence would resolve it: Studies showing that LLMs trained on ecologically valid amounts of data can achieve comparable performance to larger models on language acquisition benchmarks

### Open Question 3
- Question: How can we develop more nuanced measures for evaluating thought capacity in LLMs beyond simple success/failure metrics?
- Basis in paper: [explicit] Section 2.2.2 discusses the limitations of current binary success/failure evaluations
- Why unresolved: Current evaluation methods oversimplify the continuum of thought capacity and fail to capture the complexity of LLM reasoning
- What evidence would resolve it: Development of evaluation frameworks that consider confidence levels, intermediate reasoning steps, and multiple paraphrased responses to provide a more comprehensive assessment of LLM thought capacity

## Limitations

- The paper's pragmatist framework for mental state attribution faces ongoing philosophical debate about the nature of consciousness and intentionality
- Empirical evidence for emergent properties in LLMs remains limited, with most studies focusing on performance metrics rather than investigating underlying mechanisms
- The distinction between formal and functional language competence in LLMs may be difficult to operationalize in practice

## Confidence

- **High Confidence**: The argument that LLMs do more than simple pattern matching, supported by empirical evidence of syntactic and semantic representations
- **Medium Confidence**: The claim that current evaluations confound language and thought, as this depends on contested philosophical positions about the nature of cognition
- **Low Confidence**: The pragmatist framework for understanding mental state attribution to LLMs, as this represents a specific philosophical stance that may not be universally accepted

## Next Checks

1. Conduct experiments comparing LLM performance on paraphrased vs. original cognitive test items to quantify the language-thought confound in current benchmarks.

2. Survey researchers in philosophy of mind and cognitive science to assess the acceptance and implications of pragmatist approaches to mental state attribution in AI systems.

3. Design experiments to probe whether LLM representations of semantic relations are genuinely emergent properties or can be explained by simpler statistical patterns in the training data.