---
ver: rpa2
title: Emotional Intelligence of Large Language Models
arxiv_id: '2307.09042'
source_url: https://arxiv.org/abs/2307.09042
tags:
- llms
- human
- test
- seceu
- intelligence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents the first psychometric evaluation of emotional
  intelligence (EI) in large language models (LLMs), focusing on emotion understanding
  (EU). A novel standardized test, SECEU, was developed and validated with over 500
  human participants to measure EU.
---

# Emotional Intelligence of Large Language Models

## Quick Facts
- arXiv ID: 2307.09042
- Source URL: https://arxiv.org/abs/2307.09042
- Authors: 
- Reference count: 0
- First psychometric evaluation of emotional intelligence in large language models

## Executive Summary
This study introduces the first standardized psychometric test (SECEU) to measure emotional intelligence in large language models (LLMs), specifically focusing on emotion understanding. The test was validated with over 500 human participants and used to evaluate mainstream LLMs, with results normalized to EQ (Emotional Quotient) scores for comparison with human performance. The findings reveal that most LLMs achieve above-average emotional intelligence scores, with GPT-4 scoring 117, exceeding 89% of human participants. Notably, some models demonstrated high performance without relying on human-like cognitive mechanisms, suggesting alternative pathways to emotional understanding.

## Method Summary
The study developed and validated the SECEU test through human participant data (n=541), creating a standardized framework for measuring emotion understanding. LLMs were evaluated by having them allocate 10 points across four plausible emotions for each scenario, with responses scored against human consensus. EQ scores were calculated using Euclidean distance normalization (mean=100, SD=15). Pattern similarity analysis using Pearson correlation compared LLM representational patterns to human templates. Various mainstream LLMs were tested including OpenAI GPT series, Claude, LLaMA-based models, and others, with prompt engineering employed to improve performance where needed.

## Key Results
- Most LLMs achieved above-average EQ scores, with GPT-4 scoring 117 (exceeding 89% of humans)
- Some models reached human-level performance without employing human-like representational mechanisms
- Prompt engineering significantly improved representational similarity between LLMs and humans
- Model size, training methods, and architecture all influenced emotional intelligence outcomes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can simulate emotional understanding without experiencing emotions themselves
- Mechanism: By mapping complex emotional scenarios to normative human response patterns, LLMs generate plausible emotional attributions purely from learned statistical associations
- Core assumption: Emotional understanding does not require internal emotional states, only accurate recognition and contextual inference
- Evidence anchors:
  - [abstract] "This test requires evaluating complex emotions... in realistic scenarios"
  - [section] "EU refers to the ability to recognize, interpret, and understand emotions in a social context"
- Break condition: If emotional scenarios involve novel or culturally specific contexts outside training distribution, LLM performance degrades

### Mechanism 2
- Claim: Human-like representational patterns are not necessary for high emotional intelligence scores
- Mechanism: LLMs can achieve above-average EQ scores by learning statistical correlations between linguistic cues and emotional outcomes without mirroring human cognitive processes
- Core assumption: Equivalence in output (emotional attribution) does not imply equivalence in underlying representation
- Evidence anchors:
  - [abstract] "some models exhibited high performance without employing human-like mechanisms"
  - [section] "some LLMs apparently did not rely on the human-like mechanism to achieve human-level performance"
- Break condition: If the test requires dynamic adaptation to new emotional contexts, qualitative differences become performance bottlenecks

### Mechanism 3
- Claim: Prompt engineering significantly influences LLM emotional understanding performance
- Mechanism: Structured prompts guide LLMs to activate relevant contextual reasoning pathways, improving emotional attribution accuracy
- Core assumption: LLM reasoning is highly sensitive to prompt structure and can be steered toward more human-like processing
- Evidence anchors:
  - [section] "Interestingly, prompts apparently played a critical role in improving representational similarity"
  - [section] "Two-shot Chain of Thought Reasoning Approaches Augmented with Step-by-Step Thinking"
- Break condition: If prompts are removed or altered, LLM performance reverts to baseline capabilities

## Foundational Learning

- Concept: Standardization of emotional understanding metrics
  - Why needed here: Allows comparison between LLM and human emotional intelligence on a common scale
  - Quick check question: What statistical method was used to normalize LLM scores against human norms?

- Concept: Pattern similarity analysis
  - Why needed here: Distinguishes between quantitative performance parity and qualitative mechanism similarity
  - Quick check question: How is representational pattern similarity quantified between LLMs and humans?

- Concept: Prompt engineering techniques
  - Why needed here: Identifies how LLM outputs can be systematically improved through structured input
  - Quick check question: Which prompt type showed the most significant improvement in representational similarity?

## Architecture Onboarding

- Component map: SECEU test development -> Human norm construction -> LLM evaluation pipeline -> Distance calculation -> Normalization to EQ scores -> Pattern similarity analysis
- Critical path: Test administration → distance calculation → normalization → pattern similarity comparison
- Design tradeoffs: Larger models show better emotional understanding but may not use human-like mechanisms; prompt engineering improves performance but may not reflect genuine understanding
- Failure signatures: Models failing to complete test indicate architectural limitations; poor pattern similarity despite high scores indicates non-human reasoning mechanisms
- First 3 experiments:
  1. Test various prompt engineering strategies on a baseline model to identify performance improvements
  2. Compare representational patterns across model architectures to identify human-like vs. non-human-like mechanisms
  3. Vary test complexity to determine breaking points in LLM emotional understanding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs develop their emotional understanding abilities - through explicit training on emotional datasets or as a byproduct of general language understanding?
- Basis in paper: [explicit] The paper mentions that "certain smaller models such as Oasst and Alpaca still managed to achieve satisfactory EQ scores" despite being smaller, suggesting factors beyond model size influence emotional intelligence
- Why unresolved: The paper discusses various factors like model size, training methods, and architecture but doesn't definitively establish whether emotional understanding emerges from specific emotional training or general language capabilities
- What evidence would resolve it: Controlled experiments training models with and without explicit emotional datasets while controlling for other variables would help determine the source of emotional understanding

### Open Question 2
- Question: What specific architectural features or mechanisms allow certain LLMs to achieve human-like emotional understanding despite using different underlying processes?
- Basis in paper: [explicit] The paper notes that "some LLMs apparently did not reply on the human-like representation to achieve human-level performance, as their representational patterns diverged significantly from human patterns"
- Why unresolved: While the paper identifies that some models achieve high EQ scores through different mechanisms, it doesn't explain what specific architectural features enable this capability
- What evidence would resolve it: Detailed architectural analysis and comparison of high-performing models, combined with ablation studies to identify critical components for emotional understanding

### Open Question 3
- Question: How does emotional intelligence in LLMs transfer across different cultural contexts and languages?
- Basis in paper: [inferred] The study used Chinese participants to establish the norm and tested primarily English-trained models, suggesting potential cultural and linguistic limitations in the assessment
- Why unresolved: The paper doesn't explore how well the emotional understanding generalizes across different cultural contexts or languages, which is crucial for real-world applications
- What evidence would resolve it: Testing the same models across multiple cultural contexts and languages while maintaining consistent evaluation criteria would reveal the extent of transferability

## Limitations
- The test may measure emotional attribution accuracy rather than genuine emotional understanding
- Cultural and linguistic limitations in test design may affect generalizability of results
- Prompt engineering effects may indicate surface-level manipulation rather than authentic capabilities

## Confidence

- **High confidence**: The standardization methodology and human norm construction are well-documented and reproducible
- **Medium confidence**: The comparative performance results between models are reliable, but interpretation of what these scores mean for genuine emotional understanding remains uncertain
- **Low confidence**: The generalizability of results beyond specific test scenarios and cultural contexts represented in the SECEU dataset

## Next Checks
1. Cross-cultural validation: Administer the SECEU test with human participants and LLMs across different cultural contexts to determine whether performance is culturally bounded
2. Novel scenario testing: Create test items involving emotions and social contexts not present in the training data of evaluated models to determine whether high performance reflects genuine understanding or memorization
3. Dynamic emotional reasoning: Develop a protocol to assess LLM emotional understanding in interactive, real-time scenarios where emotional contexts evolve, rather than static multiple-choice scenarios