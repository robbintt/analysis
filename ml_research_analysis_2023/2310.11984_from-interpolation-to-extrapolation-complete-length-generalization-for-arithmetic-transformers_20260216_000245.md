---
ver: rpa2
title: 'From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic
  Transformers'
arxiv_id: '2310.11984'
source_url: https://arxiv.org/abs/2310.11984
tags:
- attention
- tasks
- length
- bias
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies how Transformer models can learn arithmetic\
  \ algorithms and achieve complete length generalization (extrapolation) on tasks\
  \ like addition, successor, and parity. The authors find that proper attention biasing\
  \ is crucial\u2014Transformer models can generalize well if they attend to the correct\
  \ tokens."
---

# From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers

## Quick Facts
- arXiv ID: 2310.11984
- Source URL: https://arxiv.org/abs/2310.11984
- Reference count: 34
- This paper shows that with proper attention biasing, Transformer models can achieve complete length generalization (extrapolation) on arithmetic tasks, achieving near-perfect accuracy up to 50 digits.

## Executive Summary
This paper investigates how Transformer models can learn arithmetic algorithms and generalize to input sequences longer than those seen during training. The authors demonstrate that proper attention biasing is crucial for achieving this extrapolation capability. They introduce two methods: Attention Bias Scaffolding (ABS), a manual scaffolding approach, and Attention Bias Calibration (ABC), an automatic method that extracts attention patterns from trained models and extends them to longer inputs. ABC achieves near-perfect accuracy on arithmetic tasks like addition, successor, and parity up to 50 digits, outperforming other methods like ALiBi and RoPE. The paper also solves the parity task, a known failure mode for Transformers, through attention scaffolding.

## Method Summary
The authors propose Attention Bias Calibration (ABC), a method that enables Transformer models to extrapolate to longer sequences by extracting and extending attention patterns learned during interpolation. ABC works by first training a model on shorter sequences until it achieves near-perfect interpolation accuracy. Then, it extracts attention weights from the trained model by decoding a random subset of training samples. These attention weights are averaged across lines in specific directions to create attention bias matrices, which are then extended to target dimensions and applied as biases to the attention weights during retraining. This process effectively encodes inductive biases that allow the model to generalize to longer sequences while maintaining the same parameter count as the original model.

## Key Results
- ABC achieves near-perfect accuracy (up to 50 digits) on arithmetic tasks like addition, successor, and parity
- ABC outperforms other methods like ALiBi and RoPE for length extrapolation
- The paper solves the parity task, a known failure mode for Transformers, through attention scaffolding
- ABC is closely related to Relative Position Encoding (RPE), suggesting potential applications to more complex tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer models can generalize to long lengths if they attend to the correct tokens, and attention biasing is the key to achieving proper attention.
- Mechanism: By guiding the model to focus on relevant tokens through attention biasing (both manual scaffolding and automatic calibration), the model learns local dependencies that enable extrapolation.
- Core assumption: The arithmetic tasks studied have restricted, localized dependency contexts that can be captured by attention biasing.
- Evidence anchors:
  - [abstract] "We show that transformer models are able to generalize to long lengths with the help of targeted attention biasing."
  - [section 4] "the vanilla Transformer correctly learns the attention patterns up to the training length and fails beyond that."
  - [corpus] Weak evidence - no direct corpus mentions of attention biasing for arithmetic extrapolation.
- Break condition: If the task has complex, non-local dependencies that cannot be captured by local attention biasing, this mechanism would fail.

### Mechanism 2
- Claim: Attention Bias Calibration (ABC) automatically extracts and extends learned attention patterns from interpolation data to longer sequences.
- Mechanism: ABC computes attention bias matrices by averaging attention weights from correctly interpolated sequences and applying them to longer sequences, effectively encoding inductive biases.
- Core assumption: Models that successfully interpolate have learned correct local attention patterns that can be extended.
- Evidence anchors:
  - [abstract] "ABC, a calibration stage that enables the model to automatically learn the proper attention biases."
  - [section 5] "ABC extracts and aggregates the attention weights and uses them as attention bias... to fine-tune the model for long inputs."
  - [corpus] No direct corpus evidence for ABC specifically, but related to length extrapolation methods.
- Break condition: If the model fails to interpolate correctly, ABC cannot extract valid patterns to extend.

### Mechanism 3
- Claim: ABC is closely related to Relative Position Encoding (RPE), suggesting potential for broader applications.
- Mechanism: Both ABC and RPE bias attention matrices, but ABC does so by computing scalar biases from learned patterns rather than learned vectors based on relative distances.
- Core assumption: The similarity in how biases are applied indicates ABC could generalize RPE principles to more complex tasks.
- Evidence anchors:
  - [abstract] "We show ABC's relation to RPE, which opens the potential for its applications to more complicated tasks."
  - [section 7] "It turns out that ABC has a close tie to the relative position encoding (RPE) of Shaw et al. (2018)."
  - [corpus] Weak evidence - corpus mentions RPE and length extrapolation but not specifically ABC-RPE connection.
- Break condition: If tasks require absolute position information rather than relative patterns, this mechanism may not apply.

## Foundational Learning

- Concept: Attention mechanisms and how they determine which tokens a model focuses on during processing.
  - Why needed here: Understanding attention is crucial because the paper's core insight is that proper attention biasing enables length generalization.
  - Quick check question: What is the difference between self-attention and cross-attention in a transformer decoder?

- Concept: Positional encoding and its role in providing order information to transformers.
  - Why needed here: The paper compares different positional encoding methods and shows that attention biasing can sometimes replace explicit positional encoding.
  - Quick check question: How does sinusoidal positional encoding differ from learned positional embeddings?

- Concept: Inductive bias and how it helps models generalize beyond training data.
  - Why needed here: ABC works by extracting inductive biases (attention patterns) from interpolated data and applying them to extrapolate.
  - Quick check question: What is an example of an inductive bias in machine learning, and why is it necessary for generalization?

## Architecture Onboarding

- Component map: Input sequence -> Encoder -> Decoder cross-attention -> Decoder self-attention -> Output sequence
- Critical path: The decoder self-attention mechanism is critical, as ABC modifies the attention weights in this layer to enable extrapolation.
- Design tradeoffs: Using windowed attention biasing and CPI simplifies attention patterns but may limit the model's ability to learn complex dependencies; ABC adds a retraining stage but achieves better generalization.
- Failure signatures: Model achieves perfect interpolation but fails to extrapolate (accuracy drops to near zero on longer sequences); attention patterns learned during interpolation don't extend to longer sequences.
- First 3 experiments:
  1. Train vanilla transformer with sinusoidal positional encoding on successor task up to 6 digits, test extrapolation to 10+ digits.
  2. Apply windowed attention biasing with w=1 to the vanilla model and test if extrapolation improves.
  3. Implement ABC by extracting attention patterns from interpolated model and retraining on longer sequences, verify if near-perfect accuracy is achieved up to 50 digits.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ABC be extended to more complex arithmetic tasks like full multiplication or division?
- Basis in paper: [inferred] The paper mentions that N Ã— 1 (multiplication with one single-digit operand) was chosen because it has simpler attention patterns, and full multiplication remains a hard task for the transformer model.
- Why unresolved: The paper only tests ABC on restricted versions of arithmetic tasks and doesn't explore its performance on more complex operations.
- What evidence would resolve it: Testing ABC on full multiplication, division, and other complex arithmetic operations to see if it maintains near-perfect accuracy.

### Open Question 2
- Question: How does ABC compare to other relative position encoding methods on non-arithmetic tasks?
- Basis in paper: [explicit] The paper shows that ABC is related to relative position encoding (RPE) and suggests potential applications to more complex tasks, but doesn't test this.
- Why unresolved: The paper focuses on arithmetic tasks and doesn't explore ABC's performance on other types of sequences like language modeling or reasoning tasks.
- What evidence would resolve it: Applying ABC to language models, reasoning tasks, or other sequence-to-sequence problems and comparing its performance to established RPE methods like RoPE or ALiBi.

### Open Question 3
- Question: What is the theoretical foundation for why ABC works so well?
- Basis in paper: [inferred] The paper shows empirically that ABC works but doesn't provide a deep theoretical explanation for why averaging attention patterns along specific directions generalizes so effectively.
- Why unresolved: The paper demonstrates ABC's effectiveness through experiments but doesn't analyze the underlying mathematical reasons for its success.
- What evidence would resolve it: A theoretical analysis proving why specific attention pattern extraction and extension methods lead to successful extrapolation in transformers.

### Open Question 4
- Question: Can the attention bias generation process be further optimized?
- Basis in paper: [explicit] The paper uses a fixed set of directions D = {(1, 1), (1, -1), (1, 0)} and a threshold based on mean and standard deviation for dropout, but doesn't explore alternatives.
- Why unresolved: The paper uses a heuristic approach for attention bias generation without exploring whether other directions or threshold selection methods might perform better.
- What evidence would resolve it: Systematic testing of different direction sets, threshold calculation methods, and bias generation algorithms to find optimal configurations for various tasks.

## Limitations
- The paper focuses on arithmetic tasks with restricted, localized dependency contexts, and it's unclear if ABC generalizes to more complex tasks with non-local dependencies.
- While ABC achieves near-perfect accuracy on inputs up to 50 digits, the paper does not extensively validate performance on sequences significantly longer than this.
- ABC relies on extracting attention patterns from correctly interpolated sequences, and if the interpolation model has learned incorrect but self-consistent attention patterns, ABC would propagate these errors to longer sequences.

## Confidence
- **High Confidence**: The core claim that proper attention biasing enables length generalization for arithmetic transformers is well-supported by empirical results.
- **Medium Confidence**: The mechanism by which ABC relates to RPE and could generalize to more complex tasks is plausible but not thoroughly validated.
- **Medium Confidence**: The assertion that arithmetic tasks have restricted, localized dependency contexts that can be captured by attention biasing is supported by results but may not hold for all arithmetic variations or more complex mathematical reasoning.

## Next Checks
1. **Robustness to interpolation failures**: Test whether ABC can recover when the base model fails to achieve perfect interpolation accuracy. Systematically introduce noise or training imperfections and measure how ABC performance degrades compared to baseline methods.

2. **Extreme length extrapolation**: Evaluate ABC on sequences significantly longer than the 50-digit maximum tested in the paper (e.g., 100-200 digits). Measure accuracy degradation curves and compare against sinusoidal, ALiBi, and RoPE baselines to determine the true limits of the approach.

3. **Cross-task generalization**: Apply ABC to non-arithmetic tasks that still have local dependencies (e.g., syntactic parsing, named entity recognition) and tasks with more complex dependencies (e.g., multi-hop reasoning, graph algorithms). Compare performance against RPE and other position encoding methods to validate the claimed relationship and broader applicability.