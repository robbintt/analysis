---
ver: rpa2
title: Byte Pair Encoding for Symbolic Music
arxiv_id: '2301.11975'
source_url: https://arxiv.org/abs/2301.11975
tags:
- music
- tokens
- https
- symbolic
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores Byte Pair Encoding (BPE), a compression technique
  widely used for natural language, to improve tokenization of symbolic music for
  deep learning tasks. By applying BPE to music tokenizations like Remi and TSD, the
  study demonstrates significant reductions in sequence length and improvements in
  vocabulary expressiveness.
---

# Byte Pair Encoding for Symbolic Music

## Quick Facts
- **arXiv ID**: 2301.11975
- **Source URL**: https://arxiv.org/abs/2301.11975
- **Reference count**: 40
- **One-line primary result**: BPE reduces sequence length and improves model performance in symbolic music tasks

## Executive Summary
This paper introduces Byte Pair Encoding (BPE), a compression technique widely used in natural language processing, to symbolic music tokenization. By applying BPE to existing tokenizations like REMI and TSD, the study demonstrates significant reductions in sequence length and improvements in vocabulary expressiveness. Experiments on music generation and composer classification tasks show that BPE enhances model performance, with better results and faster inference. Additionally, BPE helps models learn more isotropic embedding representations, improving their discriminative power. The study also highlights the importance of choosing an optimal vocabulary size to avoid diminishing returns.

## Method Summary
The paper applies Byte Pair Encoding to symbolic music tokenization by learning BPE vocabularies from tokenized datasets (REMI and TSD) and then using these vocabularies to tokenize the data for training Transformer models. The method involves downsampling the datasets, training BPE with various vocabulary sizes (×4, ×10, ×20, ×50, ×100), and evaluating the results on music generation and composer classification tasks. The approach aims to reduce sequence length while increasing vocabulary expressiveness, ultimately improving model performance and embedding isotropy.

## Key Results
- BPE significantly decreases sequence length while increasing vocabulary size, improving model performance
- BPE helps models learn more isotropic embedding representations, enhancing discriminative power
- Experiments show BPE enhances music generation quality and composer classification accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BPE reduces sequence length while increasing vocabulary size, improving model performance.
- Mechanism: BPE replaces frequent consecutive token sequences with new tokens, compressing data and allowing models to learn richer embeddings.
- Core assumption: The most frequent token combinations in symbolic music carry meaningful musical information that can be compressed.
- Evidence anchors:
  - [abstract] "Byte Pair Encoding, a compression technique widely used for natural language, significantly decreases the sequence length while increasing the vocabulary size."
  - [section 3] "Byte Pair Encoding (BPE) (Gage, 1994) is a data compression technique. It converts the most recurrent successive bytes (or in our case tokens) in a corpus into newly created ones."
  - [corpus] Weak - only 25 related papers found, average neighbor FMR=0.496, suggesting limited direct corpus evidence for this specific mechanism in symbolic music.
- Break condition: If the most frequent token combinations are random noise or do not represent meaningful musical structure, compression will not improve performance.

### Mechanism 2
- Claim: BPE helps models learn more isotropic embedding representations.
- Mechanism: By increasing vocabulary size and representing more diverse token combinations, BPE reduces clustering of embeddings in space, leading to more uniform distribution.
- Core assumption: Larger, more diverse vocabularies prevent embeddings from collapsing into tight clusters, promoting isotropy.
- Evidence anchors:
  - [abstract] "BPE helps models learn more isotropic embedding representations, improving their discriminative power."
  - [section 7] "Results presented in this section rely on Table 4, and Figures 5 and 6. Isotropy is a measure of the uniformity of the space occupied by a distribution... BPE can help bi-directional models to learn more isotropic embedding representations."
  - [corpus] Weak - limited corpus evidence specifically for isotropy in symbolic music BPE applications.
- Break condition: If the model architecture or training objective inherently creates anisotropy regardless of vocabulary diversity.

### Mechanism 3
- Claim: BPE improves music generation quality and composer classification accuracy.
- Mechanism: Richer embeddings from larger vocabularies allow models to better capture musical semantics, leading to more coherent generation and better feature discrimination for classification.
- Core assumption: More expressive tokens enable models to learn subtle musical patterns that are lost with simple attribute-based tokenization.
- Evidence anchors:
  - [abstract] "Experiments on music generation and composer classification tasks show that BPE enhances model performance, with better results and faster inference."
  - [section 6.2] "Table 1 shows that BPE allows to reduce both the token type and note duplication errors... These results show that models can easily scale to bigger vocabularies, up to a certain limit."
  - [section 6.3] "Composer classification is performed... The results, reported in Table 3, show that BPE outperforms other baselines."
  - [corpus] Weak - limited direct evidence in corpus for this specific performance claim in symbolic music.
- Break condition: If the vocabulary becomes too large, models may overfit to rare token combinations or fail to generalize.

## Foundational Learning

- Concept: Byte Pair Encoding (BPE) algorithm
  - Why needed here: Understanding how BPE works is crucial for implementing it on symbolic music tokenizations.
  - Quick check question: What is the main operation BPE performs on a token sequence, and how does it build the vocabulary?

- Concept: Symbolic music tokenization methods (REMI, TSD)
  - Why needed here: BPE is applied on top of existing tokenizations, so understanding their structure is essential.
  - Quick check question: How do REMI and TSD represent time differently in symbolic music?

- Concept: Embedding isotropy and its measurement
  - Why needed here: BPE's impact on embedding geometry is a key contribution, requiring understanding of isotropy metrics.
  - Quick check question: What does it mean for embeddings to be isotropic, and how is this typically measured?

## Architecture Onboarding

- Component map:
  - MIDI file → Tokenization (REMI/TSD) → BPE learning → Token sequences → Transformer model → Output (generation/classification)
  - BPE vocabulary learning is separate from model training
  - Embeddings are learned by the model, not pre-computed

- Critical path:
  1. Tokenize dataset with base tokenization (REMI/TSD)
  2. Learn BPE vocabulary from tokenized corpus
  3. Apply BPE to tokenize dataset
  4. Train/fine-tune Transformer model on BPE-tokenized data
  5. Evaluate on generation or classification tasks

- Design tradeoffs:
  - Larger BPE vocabulary → richer embeddings but slower tokenization and potential overfitting
  - Smaller BPE vocabulary → faster processing but less expressive tokens
  - Choice of base tokenization affects which token combinations BPE can learn

- Failure signatures:
  - TSE (tokenization syntax error) metrics increase → model struggling with BPE-tokenized sequences
  - Human evaluation scores drop significantly → generation quality degraded
  - Isotropy metrics worsen → embeddings becoming more clustered

- First 3 experiments:
  1. Apply BPE×4 on POP909 dataset with REMI tokenization and measure sequence length reduction and TSE
  2. Compare generation quality (human evaluation) between BPE×4 and baseline without BPE
  3. Measure isotropy (IsoScore) of embeddings for BPE×4 vs baseline on classification task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Byte Pair Encoding (BPE) affect the isotropy of embedding representations in generative versus bidirectional models, and what are the underlying reasons for the observed differences?
- Basis in paper: [explicit] The paper states that BPE helps bidirectional models learn more isotropic embedding representations, while generative models still form anisotropic clusters.
- Why unresolved: The study does not provide a detailed explanation of the mechanisms behind these differences in isotropy.
- What evidence would resolve it: Further research could investigate the impact of BPE on different model architectures and analyze the role of attention mechanisms and training objectives in shaping embedding isotropy.

### Open Question 2
- Question: What is the optimal vocabulary size for BPE in symbolic music, and how does it vary with model size and dataset complexity?
- Basis in paper: [explicit] The paper mentions that there is a trade-off in choosing the vocabulary size, with diminishing returns beyond a certain point, and that model size influences the ability to handle large vocabularies.
- Why unresolved: The study does not provide specific guidelines for selecting the optimal vocabulary size.
- What evidence would resolve it: Experiments varying vocabulary sizes, model sizes, and dataset complexities could identify patterns and provide recommendations for optimal BPE vocabulary sizes.

### Open Question 3
- Question: How does BPE impact the diversity and quality of generated music, and are there specific types of music or musical elements that benefit more from BPE?
- Basis in paper: [explicit] The paper shows that BPE improves the quality of generated music and helps models capture global melody and harmony better, but does not analyze the impact on specific musical elements.
- Why unresolved: The study does not provide a detailed analysis of how BPE affects different aspects of music generation, such as melody, harmony, rhythm, and structure.
- What evidence would resolve it: Evaluating generated music using a variety of metrics and conducting subjective listening tests could reveal the strengths and weaknesses of BPE in different musical contexts.

## Limitations

- The mechanism for isotropy improvement lacks direct empirical validation beyond correlation with performance metrics
- The paper doesn't establish optimal scaling principles for vocabulary size across different musical styles or complexity levels
- The comparison to existing tokenization methods is thorough but doesn't explore hybrid approaches combining BPE with attribute-based tokenizations

## Confidence

- **High confidence**: BPE reduces sequence length while maintaining or improving model performance (supported by TSE metrics and classification accuracy)
- **Medium confidence**: BPE improves embedding isotropy (supported by measurements but mechanism not fully explained)
- **Medium confidence**: BPE enhances music generation quality (supported by quantitative metrics but human evaluation results not fully detailed)

## Next Checks

1. **Vocabulary scaling validation**: Systematically test BPE vocabulary sizes beyond ×100 on more complex musical datasets to identify the point of diminishing returns across different musical genres and determine if optimal vocabulary size scales with dataset complexity.

2. **Isotropy causality test**: Conduct ablation studies that manipulate embedding isotropy independently of BPE (e.g., through regularization) to determine whether isotropy improvements directly cause performance gains or are merely correlated with them.

3. **Cross-tokenization comparison**: Implement and compare BPE-enhanced versions of alternative symbolic music tokenizations (like Compound Word and Compound Word with Rhythm) to verify whether BPE's benefits are specific to REMI/TSD or generalize across different tokenization approaches.