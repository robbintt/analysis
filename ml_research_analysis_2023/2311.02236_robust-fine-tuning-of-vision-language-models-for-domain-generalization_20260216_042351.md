---
ver: rpa2
title: Robust Fine-Tuning of Vision-Language Models for Domain Generalization
arxiv_id: '2311.02236'
source_url: https://arxiv.org/abs/2311.02236
tags:
- clip
- training
- data
- learning
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates few-shot fine-tuning of the vision-language
  foundation model CLIP for domain generalization. The authors find that zero-shot
  CLIP fails to match the performance of a fine-tuned vision-only model on complex
  benchmarks with realistic distribution shifts.
---

# Robust Fine-Tuning of Vision-Language Models for Domain Generalization

## Quick Facts
- arXiv ID: 2311.02236
- Source URL: https://arxiv.org/abs/2311.02236
- Reference count: 33
- Primary result: Few-shot CLIP fine-tuning outperforms vision-only models for domain generalization across all training data levels

## Executive Summary
This paper investigates the use of CLIP, a vision-language foundation model, for domain generalization in few-shot learning scenarios. The authors demonstrate that while zero-shot CLIP fails to match the performance of fine-tuned vision models on complex benchmarks with realistic distribution shifts, few-shot CLIP fine-tuning significantly outperforms vision-only models across all levels of training data availability. A novel fine-tuning strategy combining cross-entropy training and stochastic weight averaging is proposed to further enhance out-of-distribution robustness.

## Method Summary
The method involves fine-tuning the pre-trained CLIP model using cross-entropy loss with stochastic weight averaging (SWA) for domain generalization. The approach is evaluated on two WILDS datasets (FMoW and iWildCam) that contain realistic distribution shifts. The fine-tuning process includes learning rate sweeps, weight decay, batch size optimization, and distributed training configurations. The performance is measured across various training data availability levels (0%, 3%, 5%, 10%, 30%, 50%, 70%, 90%, 100%) and compared against vision-only baselines.

## Key Results
- Zero-shot CLIP underperforms trained vision models on complex benchmarks with realistic distribution shifts
- Few-shot CLIP fine-tuning outperforms vision-only models in both in-distribution and out-of-distribution settings across all training data levels
- Combining cross-entropy training with stochastic weight averaging further improves out-of-distribution robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vision-language models (CLIP) leverage pre-trained multimodal embeddings that remain more stable under distribution shifts than fine-tuned vision-only models
- Mechanism: CLIP's zero-shot classification uses cosine similarity between image and text embeddings learned from diverse datasets. This multimodal alignment captures semantic relationships that are less sensitive to pixel-level shifts in input distribution
- Core assumption: The contrastive pre-training dataset sufficiently covers the semantic space of the target domain to maintain alignment under shift
- Evidence anchors: [abstract] "zero-shot CLIP fails to match performance of trained vision models on more complex benchmarks, few-shot CLIP fine-tuning outperforms its vision-only counterparts"
- Break condition: If target domain semantics are underrepresented in CLIP's pre-training corpus, the cosine similarity measure will degrade under shift

### Mechanism 2
- Claim: Few-shot fine-tuning of CLIP adapts the model to target classes while preserving pre-trained robustness to distribution shifts better than vision-only models
- Mechanism: Fine-tuning updates task-specific layers (classification head) while keeping the pre-trained encoder frozen or lightly tuned, retaining learned invariances from the large-scale contrastive task
- Core assumption: The pre-trained encoder's learned invariances generalize across the shift without significant degradation from small amounts of fine-tuning data
- Evidence anchors: [abstract] "few-shot CLIP fine-tuning outperforms its vision-only counterparts in terms of in-distribution and out-of-distribution accuracy at all levels of training data availability"
- Break condition: If fine-tuning updates the encoder weights too aggressively or the few-shot data is too biased, the preserved invariances may be overwritten

### Mechanism 3
- Claim: Stochastic Weight Averaging (SWA) during fine-tuning further smooths the loss landscape, improving OOD generalization by averaging across flat minima
- Mechanism: SWA maintains an exponential moving average of model weights during the final epochs, effectively sampling from a region of the loss surface that generalizes better to unseen data distributions
- Core assumption: The flat minima found by SWA correspond to solutions that maintain performance across in-distribution and out-of-distribution samples
- Evidence anchors: [section] "A fine-tuning strategy combining cross-entropy training and stochastic weight averaging is presented to further improve out-of-distribution robustness"
- Break condition: If the loss surface is highly non-convex or the SWA window is too short, averaging may not capture truly generalizable minima

## Foundational Learning

- Concept: Cosine similarity in embedding space for classification
  - Why needed here: CLIP uses cosine similarity between image and text embeddings instead of learned linear classifiers, enabling zero-shot inference without class-specific training
  - Quick check question: How does cosine similarity between CLIP embeddings enable zero-shot classification without task-specific labels?

- Concept: InfoNCE loss and contrastive learning
  - Why needed here: CLIP's pre-training objective aligns image and text embeddings via contrastive loss, creating a shared multimodal embedding space that generalizes across domains
  - Quick check question: What is the role of temperature scaling (τ) in CLIP's InfoNCE loss function during pre-training?

- Concept: Domain generalization and distribution shift
  - Why needed here: The paper evaluates models on ID and OOD test splits from WILDS, requiring understanding of how learned features transfer across domain shifts
  - Quick check question: How does the difference between ID and OOD performance metrics indicate a model's robustness to distribution shift?

## Architecture Onboarding

- Component map: CLIP backbone (ViT-B/32 encoder) → Text encoder (ViT-B/32 encoder) → Projection heads → Cosine similarity scoring; Vision-only: CLIP backbone + randomly initialized classification head
- Critical path: Pre-trained CLIP weights → Fine-tuning (cross-entropy + SWA) → Evaluation on ID/OOD splits; Data parallelism for scaling
- Design tradeoffs: Frozen encoder vs. end-to-end fine-tuning (trade-off between preserving pre-trained invariances and adapting to task-specific features); SWA adds minimal overhead but may slightly slow convergence
- Failure signatures: Large gap between ID and OOD accuracy indicates overfitting to source distribution; Zero-shot CLIP significantly underperforming vision-only linear probes suggests pre-training corpus mismatch
- First 3 experiments:
  1. Reproduce zero-shot CLIP vs. vision-only linear probe comparison on FMoW ID/OOD splits
  2. Implement few-shot fine-tuning (linear probe only) with varying data fractions and compare ID/OOD performance
  3. Add SWA to the fine-tuning pipeline and measure impact on OOD accuracy across data availability levels

## Open Questions the Paper Calls Out
- Open Question 1: How does the performance of few-shot CLIP fine-tuning compare to traditional transfer learning methods in real-world remote sensing applications with limited training data?
- Open Question 2: What are the potential limitations or drawbacks of using vision-language foundation models like CLIP for few-shot learning in terms of computational resources and model complexity?
- Open Question 3: How does the performance of CLIP fine-tuning with cross-entropy training and stochastic weight averaging compare to other fine-tuning strategies in terms of out-of-distribution robustness?

## Limitations
- The paper doesn't provide ablation studies isolating whether CLIP's pre-training corpus coverage or the multimodal embedding structure drives the observed robustness
- No examination of how different pre-training dataset compositions affect downstream shift robustness
- Limited statistical validation of SWA's contribution to OOD robustness

## Confidence
- High confidence: The empirical finding that few-shot CLIP fine-tuning outperforms vision-only models across data availability levels
- Medium confidence: The claim that CLIP's multimodal embeddings provide inherent robustness to distribution shifts
- Low confidence: The specific contribution of SWA to improving OOD robustness

## Next Checks
1. Conduct ablation studies on pre-training data by training CLIP variants on different corpora to isolate whether pre-training data coverage or multimodal structure drives robustness
2. Systematically compare few-shot performance when fine-tuning the entire CLIP encoder versus only the classification head
3. Apply paired t-tests or bootstrap confidence intervals to compare ID and OOD performance differences between CLIP and vision-only models across multiple random seeds