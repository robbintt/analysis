---
ver: rpa2
title: 'Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided
  Revision'
arxiv_id: '2311.07362'
source_url: https://arxiv.org/abs/2311.07362
tags:
- feedback
- volcano
- multimodal
- image
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of multimodal hallucination in
  large multimodal models (LMMs), where models generate incorrect responses misaligned
  with the provided visual information. The core method, VOLCANO, introduces a self-feedback
  guided revision approach where the model generates an initial response, critiques
  it with natural language feedback grounded in the image, and revises the response
  iteratively until satisfied.
---

# Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision

## Quick Facts
- arXiv ID: 2311.07362
- Source URL: https://arxiv.org/abs/2311.07362
- Reference count: 7
- Primary result: Achieves state-of-the-art performance on multimodal hallucination benchmarks, reducing hallucination rates by up to 24.9% compared to previous methods

## Executive Summary
This paper addresses multimodal hallucination in large multimodal models (LMMs) where models generate incorrect responses misaligned with visual information. VOLCANO introduces a self-feedback guided revision approach that iteratively critiques and revises responses using natural language feedback grounded in the image. The method achieves state-of-the-art performance on hallucination benchmarks, reducing hallucination rates by up to 24.9% while improving general multimodal understanding capabilities. Qualitative analysis shows that VOLCANO's feedback attends more strongly and comprehensively to image features than initial responses, providing richer visual information for self-correction.

## Method Summary
VOLCANO finetunes a base LMM (LLaVA-1.5) using a collected dataset of multimodal feedback and revisions generated by a proprietary LLM. The model follows an iterative critique-revision-decide loop: it generates an initial response, creates feedback based on the image and response, revises the response using this feedback, and decides whether to accept the revision. This process repeats for up to 3 iterations. The model is trained with batch size 128, learning rate 2e-5, for 1 epoch on the feedback dataset.

## Key Results
- Reduces hallucination rates by up to 24.9% on MMHal-Bench compared to previous state-of-the-art methods
- Achieves 8.8% and 3.8% improvements on POPE and GA VIE benchmarks respectively
- Improves general multimodal understanding, outperforming baseline models on MM-Vet and MMBench
- Ablation studies show that the decision stage and multiple iterations are crucial for performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Natural language feedback provides richer and more accurate visual information than initial response, enabling better self-correction
- Core assumption: Feedback generation can access and process visual information more effectively than initial response generation
- Evidence anchors: Figure 4 shows increased attention to image features in feedback; qualitative analysis demonstrates feedback is more grounded in image
- Break condition: If feedback fails to attend to image features more strongly than initial response

### Mechanism 2
- Claim: Iterative self-revision with feedback reduces hallucination more effectively than single-pass correction
- Core assumption: Multiple revision cycles with feedback lead to progressively better responses
- Evidence anchors: Table 5 shows hallucination ratio decreases as iteration count increases
- Break condition: If model consistently fails to improve responses after multiple iterations

### Mechanism 3
- Claim: Decision stage prevents unnecessary revisions and improves overall performance
- Core assumption: Model can effectively distinguish between better and worse responses
- Evidence anchors: Ablation studies show decision stage plays crucial role in decreasing hallucination
- Break condition: If decision stage incorrectly accepts poor responses or rejects good ones

## Foundational Learning

- Concept: Multimodal hallucination in LMMs
  - Why needed here: Understanding the problem VOLCANO addresses is crucial for implementation and evaluation
  - Quick check question: What distinguishes multimodal hallucination from language-only hallucination?

- Concept: Vision-language model grounding
  - Why needed here: Mechanism relies on model's ability to ground responses in visual information
  - Quick check question: How does vision encoder contribute to grounding in multimodal models?

- Concept: Reinforcement learning from human feedback (RLHF)
  - Why needed here: Understanding preference learning concepts helps grasp feedback-based methods
  - Quick check question: What is key difference between scalar feedback and natural language feedback?

## Architecture Onboarding

- Component map:
  Image encoder -> Text encoder -> Cross-attention layers -> Feedback generator -> Revision module -> Decision module -> Output layer

- Critical path:
  1. Input image and question
  2. Generate initial response
  3. Generate feedback based on image and initial response
  4. Revise initial response using feedback
  5. Decide whether to accept revision or keep initial response
  6. Repeat steps 3-5 for up to 3 iterations
  7. Output final response

- Design tradeoffs:
  - Single model vs. separate models: VOLCANO uses single model for efficiency but may have limitations compared to specialized models
  - Number of iterations: More iterations can improve quality but increase computation time
  - Feedback format: Natural language provides rich information but may be more complex to process than scalar feedback

- Failure signatures:
  - Feedback that doesn't attend to image features or provides incorrect visual information
  - Revisions that don't improve response or introduce new errors
  - Decision stage that incorrectly accepts poor responses or rejects good ones
  - Performance degradation on multimodal understanding benchmarks despite hallucination reduction

- First 3 experiments:
  1. Implement basic feedback generation mechanism without revision to test if feedback contains richer visual information
  2. Add revision stage to test if feedback improves responses, starting with fixed number of iterations
  3. Implement decision stage and test full iterative process, comparing performance with and without decision mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does choice of max iteration count affect performance and inference time?
- Basis in paper: Explicit - "Iteration ablation" section
- Why unresolved: Paper only tests iteration counts of 1, 2, and 3
- What evidence would resolve it: Testing VOLCANO with max iteration counts beyond 3 and measuring performance and inference time

### Open Question 2
- Question: How does VOLCANO's attention mechanism compare to GPT-4 or GPT-3.5-turbo?
- Basis in paper: Explicit - "Amount of visual information" section
- Why unresolved: Paper only compares VOLCANO's attention to itself, not to other models
- What evidence would resolve it: Visualizing and comparing attention mechanisms in VOLCANO, GPT-4, and GPT-3.5-turbo

### Open Question 3
- Question: How does performance change when using different backbone models?
- Basis in paper: Inferred - "Implementation details" section mentions LLaVA-1.5 7B and 13B
- Why unresolved: Paper only tests VOLCANO with LLaVA-1.5 as backbone
- What evidence would resolve it: Training and testing VOLCANO with different backbone models and measuring performance

## Limitations
- Evaluation relies heavily on proprietary models (gpt-3.5-turbo) for feedback generation, raising reproducibility concerns
- Training dataset is proprietary, limiting independent verification of approach's effectiveness
- Study focuses primarily on hallucination metrics without thoroughly examining response time or computational efficiency tradeoffs

## Confidence
- **High Confidence**: Core mechanism of iterative self-revision with feedback is well-supported by experimental results showing consistent hallucination reduction
- **Medium Confidence**: Claim that feedback contains richer visual information than initial responses is supported by qualitative analysis but needs more systematic quantitative evaluation
- **Low Confidence**: Decision stage's effectiveness is demonstrated through ablation studies, but robustness of model's preference judgments across diverse scenarios remains unclear

## Next Checks
1. **Cross-model Validation**: Test VOLCANO's effectiveness when using different proprietary LLMs for feedback generation to assess robustness and generalizability
2. **Attention Pattern Analysis**: Conduct quantitative analysis of attention weights in feedback vs. initial responses across large sample to verify claimed differences in visual feature grounding
3. **Long-term Stability Testing**: Evaluate whether VOLCANO's hallucination reduction persists after deployment with real-world user queries, particularly for edge cases not well-represented in benchmark datasets