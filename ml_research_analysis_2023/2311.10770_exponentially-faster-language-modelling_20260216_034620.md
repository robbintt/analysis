---
ver: rpa2
title: Exponentially Faster Language Modelling
arxiv_id: '2311.10770'
source_url: https://arxiv.org/abs/2311.10770
tags:
- inference
- feedforward
- implementation
- neurons
- only
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents FastBERT, a BERT variant that uses fast feedforward
  networks (FFFs) to dramatically reduce the number of neurons needed for inference
  while maintaining performance on par with similar BERT models. The key idea is to
  replace feedforward networks with FFFs, which conditionally execute only a subset
  of neurons per inference.
---

# Exponentially Faster Language Modelling

## Quick Facts
- arXiv ID: 2311.10770
- Source URL: https://arxiv.org/abs/2311.10770
- Reference count: 3
- FastBERT achieves GLUE performance parity while using only 0.3% of neurons (12 out of 4095) per inference

## Executive Summary
This paper introduces FastBERT, a BERT variant that dramatically accelerates inference by replacing traditional feedforward networks with fast feedforward networks (FFFs). These FFFs use conditional matrix multiplication to selectively engage only a small subset of neurons per inference, achieving up to 78x speedup on CPU and maintaining GLUE performance comparable to BERT-base models. The key innovation is organizing neurons into a balanced binary tree structure that allows for logarithmic-time computation while preserving the representational capacity of the original architecture.

## Method Summary
FastBERT replaces the feedforward networks in BERT with fast feedforward networks (FFFs) that organize neurons into a balanced binary tree structure. During inference, only one branch of this tree is executed based on conditional matrix multiplication, reducing the number of neurons needed from thousands to just a handful (12 out of 4095). The model is trained on a single A6000 GPU for one day and finetuned for 5 epochs with a learning rate of 4×10⁻⁵ on GLUE tasks. The implementation achieves significant speedups by avoiding computation of unnecessary neuron activations while maintaining comparable downstream performance.

## Key Results
- FastBERT-1x11 uses only 0.3% of neurons (12 out of 4095) while achieving GLUE performance comparable to BERT-base
- Achieves 78x speedup over dense matrix multiplication on CPU and 3.15x speedup over native fused feedforward on GPU
- Theoretical speedup promise of up to 341x for BERT-base models while retaining 96-98% of performance on GLUE tasks

## Why This Works (Mechanism)

### Mechanism 1
FastBERT uses only 0.3% of its neurons for inference while maintaining GLUE performance comparable to BERT-base. The FFF architecture organizes neurons into a balanced binary tree, executing only one branch per inference based on conditional matrix multiplication. This works because activation patterns in traditional feedforward networks follow a conditional structure where only a small subset is needed per input.

### Mechanism 2
FastBERT achieves up to 78x speedup by replacing dense matrix multiplication with conditional matrix multiplication that avoids computing unnecessary neuron activations. This reduces computational complexity from O(n) to O(log n), making inference significantly faster. The overhead of implementing CMM is outweighed by the reduction in computational work for typical cases.

### Mechanism 3
FastBERT retains 96-98% of BERT-base performance on GLUE tasks while using 341x fewer neurons. The FFF architecture preserves the representational capacity of traditional feedforward layers while introducing conditional execution, allowing the model to learn which neurons are needed for different inputs. This selective engagement enables both speed and performance retention.

## Foundational Learning

- **Transformer architecture and BERT pretraining**: FastBERT builds directly on BERT's architecture, replacing only the feedforward layers while keeping attention mechanisms unchanged. Quick check: What are the three main components of a transformer block, and which one does FastBERT modify?

- **Conditional execution and sparse computation**: FastBERT's core innovation is replacing dense matrix multiplication with conditional matrix multiplication that selectively engages neurons. Quick check: How does the computational complexity of conditional matrix multiplication compare to dense matrix multiplication, and why?

- **GLUE benchmark and downstream evaluation**: The paper evaluates FastBERT's performance on GLUE tasks to demonstrate that neuron reduction doesn't harm practical utility. Quick check: What is the purpose of the GLUE benchmark, and why is it appropriate for evaluating FastBERT?

## Architecture Onboarding

- **Component map**: Token embeddings -> Multi-head self-attention -> Modified intermediate layer (FFF execution) -> Layer normalization -> Output layer

- **Critical path**: 1) Token embedding and position encoding, 2) Multi-head self-attention with feedforward residual connections, 3) Modified intermediate layer with FFF execution, 4) Layer normalization and residual connections, 5) Output layer for downstream task

- **Design tradeoffs**: Speed vs. flexibility (FFFs are faster but may have less flexibility than traditional FFs), Depth vs. width (uses depth-11 trees to match BERT-base parameter count), Training complexity (conditional execution adds complexity to training process)

- **Failure signatures**: Performance degradation on tasks requiring full neuron engagement, Inconsistent results across different inputs due to conditional execution, Training instability due to complex conditional selection mechanism

- **First 3 experiments**: 1) Implement a simple 2-layer FFF and compare inference speed vs. traditional FF on synthetic data, 2) Train FastBERT-1x2 on a subset of GLUE tasks and compare performance vs. baseline, 3) Profile memory usage and compute time for FastBERT vs. traditional BERT on representative inputs

## Open Questions the Paper Calls Out

### Open Question 1
What is the theoretical limit of speedup achievable with conditional neural execution in language models of varying sizes and architectures? The paper mentions a theoretical speedup promise of 341x for BERT-base models and suggests even larger models like GPT-3 could potentially achieve higher speedups. This remains unresolved as the paper provides theoretical analysis but doesn't explore the full range of potential speedups across different model sizes and architectures.

### Open Question 2
How does the performance of FastBERT models degrade with increasing depth of fast feedforward networks, and what is the optimal depth for balancing speed and accuracy? The paper notes that performance decreases with increasing depth of FFFs, particularly for the CoLA task, but doesn't explore this relationship in detail or determine an optimal depth. This requires comprehensive ablation studies testing FastBERT models with varying FFF depths on multiple tasks.

### Open Question 3
What are the practical challenges and potential solutions for implementing efficient conditional matrix multiplication (CMM) kernels on different hardware architectures? The paper discusses current implementation challenges and potential future optimizations but doesn't provide detailed analysis of hardware-specific implementation difficulties or solutions. This needs detailed technical analysis of CMM implementation on various hardware architectures.

## Limitations
- The 0.3% neuron engagement may be specific to GLUE benchmarks and may not generalize to all language tasks
- Theoretical 341x speedup claims are extrapolated from small-scale experiments without empirical validation on full-scale models
- The conditional execution mechanism's robustness to distribution shifts and diverse input types remains largely untested

## Confidence
- **High confidence**: Theoretical speedup claims (O(log n) vs O(n) complexity) and basic feasibility of conditional execution
- **Medium confidence**: Reported GLUE performance parity with BERT-base variants and CPU speedup measurements
- **Low confidence**: 341x speedup for BERT-base models is extrapolated without empirical validation on full-scale models

## Next Checks
1. **Distribution robustness test**: Evaluate FastBERT on diverse language tasks beyond GLUE with different input characteristics to assess whether 0.3% neuron engagement remains consistent and performance stays comparable.

2. **Scaling validation**: Implement and benchmark FastBERT on a BERT-base sized model to empirically verify the claimed 341x speedup, as current results are based on smaller models.

3. **Ablation study on conditional execution**: Systematically disable conditional execution to measure exact performance degradation and neuron usage patterns, determining whether efficiency gains are primarily due to architectural innovation or other factors.