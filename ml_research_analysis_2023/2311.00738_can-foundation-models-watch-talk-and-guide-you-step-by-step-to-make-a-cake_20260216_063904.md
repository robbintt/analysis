---
ver: rpa2
title: Can Foundation Models Watch, Talk and Guide You Step by Step to Make a Cake?
arxiv_id: '2311.00738'
source_url: https://arxiv.org/abs/2311.00738
tags:
- user
- task
- guidance
- instructor
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WTaG, a multimodal dataset for situated task
  guidance, and evaluates foundation models' ability to guide users through tasks
  using zero-shot learning. The dataset includes egocentric videos of human users
  performing cooking tasks while guided by human instructors through natural interaction.
---

# Can Foundation Models Watch, Talk and Guide You Step by Step to Make a Cake?

## Quick Facts
- **arXiv ID**: 2311.00738
- **Source URL**: https://arxiv.org/abs/2311.00738
- **Reference count**: 34
- **Primary result**: Foundation models can provide fair task guidance performance in cooking scenarios without task-specific training, but significant challenges remain in vision-to-language translation and user modeling.

## Executive Summary
This paper introduces WTaG, a multimodal dataset for situated task guidance, and evaluates foundation models' ability to guide users through cooking tasks using zero-shot learning. The dataset captures egocentric videos of human users performing cooking tasks while guided by human instructors through natural interaction. Two tasks are defined: User and Environment Understanding, and Instructor Decision Making. The paper explores three methods to extract visual and dialog context using foundation models: Language Only, Scene Description, and Object and State Detection. Results show that foundation models can achieve fair performance in some cases without task-specific training, but fast and reliable adaptation remains a significant challenge.

## Method Summary
The paper leverages large language models as the backbone for guidance generation, exploring three different multimodal methods to extract visual and dialog context. The first method uses only dialog transcripts, the second incorporates scene descriptions generated by vision-language models (BLIP-2), and the third uses object and state detection (EgoHOS + CLIP). The system processes egocentric videos captured with HoloLens 2, transcribes audio using Azure ASR, and generates guidance through ChatGPT prompts that combine visual context with dialog history. Evaluation includes micro F1 scores for classification tasks and human assessment of generated guidance quality.

## Key Results
- Foundation models achieved fair performance on User and Environment Understanding tasks with micro F1 scores ranging from 0.63-0.82
- Object and State Detection method showed promise but suffered from instability in frame-to-frame predictions
- Human evaluation revealed trade-offs between helpfulness and annoyance in generated guidance, with models performing better in simpler recipe scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Foundation models can generate situated task guidance without task-specific training when provided with sufficient multimodal context
- Mechanism: The LLM uses its large knowledge base and in-context learning ability to reason about user actions, environment, and task steps based on dialog history and visual scene descriptions
- Core assumption: The LLM's knowledge base covers the domain (cooking) and its reasoning capability can handle the complexity of situated guidance
- Evidence anchors:
  - [abstract] "Our quantitative, qualitative, and human evaluation results show that these models can demonstrate fair performances in some cases with no task-specific training"
  - [section] "With the inherent complexity of the problem itself, this dataset can help researchers understand the various nuances of the problem in the most realistic human-human interaction setting."
  - [corpus] Weak - no direct evidence in corpus results
- Break condition: The LLM's knowledge base does not cover the domain or the task complexity exceeds its reasoning capability

### Mechanism 2
- Claim: Translating visual context to language using foundation models improves LLM's understanding of user actions and environment
- Mechanism: Vision-language models (BLIP-2, EgoHOS + CLIP) extract scene descriptions or object states from egocentric video frames, which are then incorporated into LLM prompts to provide additional context
- Core assumption: The vision-language models can accurately and relevantly translate visual information to language that the LLM can use
- Evidence anchors:
  - [section] "We explored three different mutimodal methods to extract visual and dialog context. Our empirical results have shown promising results with foundation models"
  - [section] "For the Object and State Detection evaluation (Figure 8b), since EgoHOS outputs unstable predictions from frame to frame, we conducted an experiment on detected object smoothing strategies."
  - [corpus] Weak - no direct evidence in corpus results
- Break condition: The vision-language models produce hallucinated or irrelevant information that confuses the LLM

### Mechanism 3
- Claim: Natural language generation from LLMs can provide situated task guidance that is helpful and non-annoying to users
- Mechanism: The LLM generates natural language instructions, answers, confirmations, etc. based on its understanding of the user's intent, task state, and environment
- Core assumption: The LLM's language generation capability can produce guidance that is appropriate for the user's current situation and mental state
- Evidence anchors:
  - [abstract] "Our quantitative, qualitative, and human evaluation results show that these models can demonstrate fair performances in some cases with no task-specific training"
  - [section] "Lastly, we conducted a human evaluation on models' generated language guidance."
  - [corpus] Weak - no direct evidence in corpus results
- Break condition: The LLM's language generation produces unhelpful or annoying guidance due to lack of user modeling or situational awareness

## Foundational Learning

- **Concept: Multimodal foundation models (vision-language models)**
  - Why needed here: To extract visual context from egocentric video frames and translate it to language that can be incorporated into LLM prompts
  - Quick check question: What are some examples of vision-language models that could be used for this task, and what are their strengths/weaknesses?

- **Concept: In-context learning**
  - Why needed here: To enable the LLM to perform the task guidance task without task-specific training, by providing relevant examples and context in the prompt
  - Quick check question: How does in-context learning differ from fine-tuning, and what are the advantages/disadvantages of each approach?

- **Concept: Human evaluation of language generation**
  - Why needed here: To assess the quality of the LLM's generated guidance in terms of helpfulness and annoyance, as these are subjective measures that are difficult to quantify
  - Quick check question: What are some best practices for designing human evaluation studies for language generation tasks, and how can we ensure the results are reliable and generalizable?

## Architecture Onboarding

- **Component map**: Egocentric video capture (HoloLens 2) -> Audio transcription (Azure ASR) -> Visual context extraction (BLIP-2, EgoHOS + CLIP) -> LLM prompt generation and query (ChatGPT) -> Human evaluation (annotators)

- **Critical path**: 1. Capture video and audio of user-instructor interaction, 2. Process video frames to extract visual context, 3. Generate LLM prompts with visual and dialog context, 4. Query LLM to generate guidance, 5. Evaluate guidance quality (quantitative metrics, human evaluation)

- **Design tradeoffs**: Visual context vs. dialog context (richer information vs. more noise), Generic vs. specific prompts (flexibility vs. guidance), Real-time vs. offline processing (immediate feedback vs. accuracy)

- **Failure signatures**: Low quantitative metrics (F1 scores) on user intent, step detection, or mistake recognition tasks; low helpfulness or high annoyance ratings in human evaluation; high hallucination rates in visual context extraction

- **First 3 experiments**: 1. Evaluate the impact of different visual context extraction methods (BLIP-2 vs. EgoHOS + CLIP) on LLM performance, 2. Test the effect of prompt engineering (generic vs. specific prompts) on the quality and diversity of generated guidance, 3. Compare the performance of different LLM models (GPT-3.5 vs. GPT-4) on the task guidance task

## Open Questions the Paper Calls Out

- **Open Question 1**: How can we improve the truthfulness and relevance of visual context extraction for situated task guidance?
  - Basis in paper: [explicit] The paper discusses the challenges of translating visual inputs into language, noting that the current methods often produce untruthful or irrelevant descriptions
  - Why unresolved: The paper identifies this as a bottleneck but does not propose specific solutions beyond suggesting future work could leverage recipe information, user attention, and other sensory inputs
  - What evidence would resolve it: Experiments comparing different visual context extraction methods, including those that incorporate recipe information, user attention, and other sensory inputs, and evaluating their impact on the overall task guidance performance

- **Open Question 2**: How can we better model user states and preferences to provide more personalized and situation-relevant task guidance?
  - Basis in paper: [explicit] The paper mentions that user states and preferences, such as familiarity with the task, emotional state, and tolerance for guidance frequency, can significantly impact the effectiveness of task guidance
  - Why unresolved: The current system does not explicitly model these factors, and the paper acknowledges the difficulty in categorizing and collecting user mental states
  - What evidence would resolve it: Studies investigating methods for inferring user states and preferences from multimodal inputs, and experiments evaluating the impact of personalized guidance on user satisfaction and task performance

- **Open Question 3**: How can we improve the evaluation of situated task guidance systems to capture the complexity and interactivity of the task?
  - Basis in paper: [explicit] The paper highlights the challenges of evaluating generative language models and interactive systems, noting the limitations of current quantitative and qualitative methods
  - Why unresolved: The paper relies on a combination of classification tasks and human evaluation, but acknowledges the need for more comprehensive evaluation methods
  - What evidence would resolve it: Development and validation of new evaluation metrics and methodologies that can capture the nuances of situated task guidance, such as user satisfaction, task completion time, and the quality of human-AI interaction

## Limitations
- Foundation models without task-specific training show inconsistent performance across different aspects of task guidance
- Vision-to-language translation introduces uncertainty through potential hallucinations and instability in object detection
- Generalization capability beyond the specific cooking domain remains uncertain due to limited evaluation scope

## Confidence
- **High confidence**: Methodology for dataset collection and basic framework for multimodal context extraction are well-documented and reproducible
- **Medium confidence**: Zero-shot performance claims are supported by quantitative metrics, but human evaluation methodology has some limitations
- **Low confidence**: Generalization capability of the approach beyond the specific cooking domain remains uncertain

## Next Checks
1. **Cross-domain validation**: Test the same methodology on non-cooking tasks (e.g., assembly tasks, DIY projects) to assess generalization beyond the culinary domain
2. **Error analysis of vision-to-language component**: Conduct a detailed analysis of hallucination rates and relevance accuracy for the BLIP-2 and EgoHOS models across different video segments
3. **Longitudinal user study**: Implement a study where the same users interact with the system over multiple sessions to evaluate the consistency and improvement of guidance quality over time, particularly focusing on the system's ability to learn user preferences and communication styles