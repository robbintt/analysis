---
ver: rpa2
title: Towards Fixing Clever-Hans Predictors with Counterfactual Knowledge Distillation
arxiv_id: '2310.01011'
source_url: https://arxiv.org/abs/2310.01011
tags:
- counterfactual
- cfkd
- confounder
- dataset
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of "Clever Hans" predictors in
  deep learning models that rely on spurious features (confounders) rather than true
  predictive features. This is particularly problematic in regulated and safety-critical
  domains like medical imaging and autonomous driving.
---

# Towards Fixing Clever-Hans Predictors with Counterfactual Knowledge Distillation

## Quick Facts
- arXiv ID: 2310.01011
- Source URL: https://arxiv.org/abs/2310.01011
- Reference count: 24
- Primary result: CFKD effectively reduces Clever Hans effects by leveraging counterfactual explanations and human feedback to remove reliance on confounders

## Executive Summary
This paper addresses the problem of "Clever Hans" predictors in deep learning models that rely on spurious features rather than true predictive features. The authors propose counterfactual knowledge distillation (CFKD), a novel technique that uses counterfactual explanations to reveal confounders and leverages human expert feedback to refine models. CFKD demonstrates advantages over attribution-based methods for finding confounders and introduces a new metric called "Feedback Accuracy" that better correlates with true test performance than validation accuracy.

## Method Summary
CFKD works by generating counterfactual explanations through iterative latent space optimization in a generative model (Glow), then using a teacher (human expert, oracle, or automated method) to determine whether changes are based on confounders or true predictive features. The teacher's feedback is used to label counterfactuals appropriately, which are then added to the training data to retrain the classifier. This process iteratively refines the model to rely on true predictive features rather than confounders, with feedback accuracy serving as a more reliable performance indicator than validation accuracy when confounders are present.

## Key Results
- CFKD effectively reduces Clever Hans effects and improves model performance on datasets with known confounders
- Counterfactual explanations reveal confounders that attribution-based methods miss
- Feedback Accuracy metric is better correlated with true test performance than validation accuracy
- Method is flexible and works with various teacher models and explanation techniques

## Why This Works (Mechanism)

### Mechanism 1
Counterfactual explanations can reveal confounders that attribution-based explanations miss. By generating images that transform a sample from one class to another through latent space optimization, CFKD produces meaningful visual changes that highlight which features the model relies on. Unlike attribution methods that show which pixels support a decision, counterfactuals demonstrate which changes would flip the prediction.

### Mechanism 2
Human feedback on counterfactuals effectively guides model retraining to remove reliance on confounders. The teacher evaluates whether counterfactual changes are based on confounders or true predictive features, and this feedback is used to label and add counterfactuals to training data, helping the model learn to associate true predictive features with correct classes.

### Mechanism 3
Feedback Accuracy is a better indicator of true model performance than validation accuracy when confounders are present. Validation accuracy can be misleading when training and validation data share the same confounding correlations, while feedback accuracy directly tracks the model's ability to generate counterfactuals that correctly identify the generalizing strategy.

## Foundational Learning

- **Concept**: Manifold hypothesis in deep learning
  - Why needed here: CFKD relies on finding perturbations close to the data manifold rather than in the full input space
  - Quick check question: Why does optimizing in the latent space of a generative model produce more semantically meaningful counterfactuals than optimizing directly in pixel space?

- **Concept**: Knowledge distillation
  - Why needed here: CFKD extends traditional knowledge distillation by using counterfactual explanations as the medium for transferring knowledge
  - Quick check question: How does counterfactual knowledge distillation differ from standard knowledge distillation in terms of what knowledge is being transferred and how it's applied?

- **Concept**: Confounder detection and removal
  - Why needed here: The entire method is designed to address the problem of confounders in deep learning models
  - Quick check question: What makes a feature a "confounder" rather than a legitimate predictive feature, and why are confounders particularly problematic in regulated domains?

## Architecture Onboarding

- **Component map**: Classifier (student model) -> Generative model (Glow) -> Counterfactual explainer -> Teacher (human, oracle, or SpRAy) -> Training loop
- **Critical path**: Generate counterfactual → Get teacher feedback → Add to training data → Retrain classifier → Measure feedback accuracy → Iterate until convergence
- **Design tradeoffs**: Trades computational efficiency for interpretability and robustness; generating counterfactuals and getting human feedback is more expensive than standard training
- **Failure signatures**: Poor counterfactual quality (artifacts, unrealistic transformations), teacher feedback that doesn't improve validation performance, feedback accuracy that diverges from test accuracy
- **First 3 experiments**:
  1. Run CFKD on the follicle dataset with human teacher feedback and qualitatively inspect counterfactuals before and after teaching to verify strategy change
  2. Apply CFKD to a poisoned CelebA dataset with copyright tag confounder and measure unpoisoned test accuracy improvement
  3. Test CFKD on the colorectal cancer tissue dataset with staining-based confounders and compare performance with and without CFKD

## Open Questions the Paper Calls Out

### Open Question 1
How does CFKD's effectiveness vary across different types of confounders (e.g., localized vs. distributed, visual vs. non-visual)? The paper tests CFKD on copyright tags, intensity shifts, and color shifts as different types of confounders but doesn't systematically compare performance across confounder types.

### Open Question 2
What is the optimal balance between perturbation strength and information preservation in counterfactual generation for different applications? The paper empirically sets perturbation targets but doesn't explore the full tradeoff space or provide guidance on optimal settings.

### Open Question 3
How does CFKD performance scale with the proportion of confounding samples in the training data? While the paper shows CFKD works at different poisoning levels, it doesn't identify the limits of effectiveness or how performance degrades as confounding increases.

## Limitations

- Limited empirical validation primarily on synthetic datasets and a single real-world histopathological dataset
- Heavy dependency on teacher feedback quality, with human-in-the-loop feedback not yet fully evaluated
- Computationally expensive due to counterfactual generation and feedback collection
- No quantitative measure of counterfactual quality or human evaluation of generated counterfactuals

## Confidence

**High confidence**: The general framework of using counterfactual explanations to detect and remove confounders is well-grounded in the literature on explainable AI and adversarial training.

**Medium confidence**: Empirical results on synthetic datasets show promise, but limited scope of evaluation and reliance on oracle teachers reduce confidence in real-world applicability.

**Low confidence**: The claim about feedback accuracy being better correlated with true test performance than validation accuracy needs more rigorous validation, especially on datasets with complex confounding structures.

## Next Checks

1. Apply CFKD to a dataset with naturally occurring confounders (e.g., medical imaging with acquisition artifacts) and compare performance against standard training and attribution-based methods.

2. Conduct a user study with domain experts providing feedback on counterfactuals, measuring both the quality of feedback and the resulting model improvements.

3. Measure the runtime overhead of CFKD compared to standard training, and evaluate whether the performance gains justify the additional computational cost for large-scale applications.