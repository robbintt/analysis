---
ver: rpa2
title: On the Adversarial Robustness of Graph Contrastive Learning Methods
arxiv_id: '2311.17853'
source_url: https://arxiv.org/abs/2311.17853
tags:
- graph
- learning
- robustness
- adversarial
- latexit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates the adversarial robustness of graph contrastive
  learning (GCL) methods on both node and graph classification tasks. The authors
  propose a robustness evaluation protocol that consists of three steps: encoder training,
  linear classifier training, and adversarial attack.'
---

# On the Adversarial Robustness of Graph Contrastive Learning Methods

## Quick Facts
- arXiv ID: 2311.17853
- Source URL: https://arxiv.org/abs/2311.17853
- Reference count: 22
- Primary result: GCL methods do not consistently exhibit improved adversarial robustness compared to non-contrastive methods, and in some instances, contrastive training can even lead to a further deterioration in performance.

## Executive Summary
This paper evaluates the adversarial robustness of graph contrastive learning (GCL) methods on both node and graph classification tasks. The authors propose a three-step evaluation protocol consisting of encoder training, linear classifier training, and adversarial attack. The evaluation uses adaptive attacks targeting graph structure and considers both static and adaptive attacks, as well as global and local attacks. Results show that GCL methods do not consistently improve adversarial robustness compared to non-contrastive methods. The authors highlight the need for further research to understand when GCL methods excel or falter in adversarial scenarios.

## Method Summary
The paper proposes a three-step evaluation protocol: 1) encoder training using GCL objectives on augmented graph views, 2) linear classifier training on top of the fixed encoder, and 3) adversarial attack generation using PGD and R-BCD variants. The evaluation is conducted on node classification datasets (Cora, Citeseer, Pubmed, OGB-arXiv) and graph classification datasets (PROTEINS, NCI1, DD). The primary metric is Relative Adversarial Accuracy Drop (Rclean_adv), measuring the relative reduction in accuracy between clean and adversarial predictions. The study compares GCL methods (DGI, GraphCL, GCA, InfoGraph, AD-GCL) against non-contrastive baselines (GCN, GIN).

## Key Results
- GCL methods do not consistently exhibit improved adversarial robustness compared to non-contrastive methods across datasets and attack types
- In some instances, contrastive training can lead to further deterioration in performance under adversarial attacks
- Adaptive attacks generate more detrimental perturbations than static or local attacks for all evaluated models
- DGI consistently outperforms other GCL models in node classification tasks, but GCL methods generally perform worse than non-contrastive models on graph classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GCL methods are less robust to adaptive adversarial attacks due to their mutual information maximization training objectives
- Mechanism: GCL models maximize mutual information between augmented views, but adaptive attacks exploit this by reducing similarity between these views, undermining the contrastive objective
- Core assumption: Mutual information maximization creates dependencies on specific graph structures that can be exploited by adaptive attacks
- Evidence anchors: Investigations show inadequacy of naive proxies for assessing GCL robustness under adaptive attack conditions; GCL methods used in graph classification do not enhance adversarial robustness

### Mechanism 2
- Claim: Augmentation strategies in GCL significantly impact model robustness against adversarial attacks
- Mechanism: Different augmentations create varying dependencies on graph structures; adversarial attacks target the most vulnerable augmentations
- Core assumption: Robustness is directly related to the robustness of augmentations used during training
- Evidence anchors: Authors highlight need to understand conditions where GCL methods excel or falter; GCL models consistently achieve worst results on PROTEINS and NCI1 datasets

### Mechanism 3
- Claim: Static and local attacks provide incomplete assessment of GCL model vulnerability
- Mechanism: Static and local attacks are less effective at uncovering true vulnerabilities compared to adaptive and global attacks
- Core assumption: Adaptive and global attacks better exploit weaknesses in GCL models
- Evidence anchors: Investigations show inadequacy of naive proxies; adaptive attacks generate more detrimental perturbations than static attacks

## Foundational Learning

- Concept: Mutual Information Maximization
  - Why needed here: GCL methods rely on maximizing mutual information between different views of the same graph to learn robust representations
  - Quick check question: What is the primary objective of mutual information maximization in GCL, and how does it contribute to learning robust graph representations?

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: GCL methods are built upon GNNs, which encode graph structures and node features into latent representations
  - Quick check question: How do GNNs operate on graph-structured data, and what are the key components of a GNN architecture?

- Concept: Adversarial Attacks
  - Why needed here: Understanding different types of adversarial attacks is crucial for evaluating GCL robustness
  - Quick check question: What are the key differences between adaptive and static adversarial attacks, and how do they impact the evaluation of model robustness?

## Architecture Onboarding

- Component map: Graph input -> Encoder (GNN) -> Augmentation module -> Contrastive loss -> Linear classifier -> Adversarial attack module
- Critical path: 1) Train GCL encoder using contrastive loss on augmented views, 2) Train linear classifier on fixed encoder, 3) Apply adversarial attacks and evaluate performance
- Design tradeoffs: Choice of augmentation strategies impacts robustness; more complex encoders may be more vulnerable but learn more robust representations; evaluation metrics affect robustness assessment
- Failure signatures: Significant accuracy drop under adaptive attacks; inconsistent performance across datasets and attack scenarios; overestimation of robustness with static/local attacks
- First 3 experiments: 1) Evaluate GCL model (GraphCL) against adaptive attacks on Cora, 2) Compare robustness of DGI, GraphCL, GCA using same protocol, 3) Investigate impact of different augmentation strategies on GCL model robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions does contrastive training lead to improved adversarial robustness in GCL methods?
- Basis in paper: The paper finds GCL methods do not consistently exhibit improved robustness and can even deteriorate performance
- Why unresolved: Paper does not identify precise conditions or factors determining when GCL methods excel or falter
- What evidence would resolve it: Empirical studies varying dataset characteristics, attack types, and model architectures to identify conditions where GCL shows improved robustness

### Open Question 2
- Question: Why does DGI consistently outperform other GCL models in node classification tasks?
- Basis in paper: DGI consistently performs best across all node classification datasets due to its unique training strategy of differentiating between corruptions and original graph
- Why unresolved: Paper does not provide detailed analysis of underlying mechanisms contributing to DGI's superior performance
- What evidence would resolve it: Further analysis investigating impact of DGI's training strategy on adversarial robustness compared to other GCL methods

### Open Question 3
- Question: How do adaptive adversarial attacks compare to static and local attacks in evaluating GCL robustness?
- Basis in paper: Paper highlights importance of adaptive attacks and demonstrates they generate more detrimental perturbations
- Why unresolved: Paper does not provide comprehensive comparison of different attack types' effectiveness
- What evidence would resolve it: Empirical studies systematically comparing impact of adaptive, static, and local attacks on GCL robustness across various datasets

## Limitations

- The underlying reasons for GCL methods' inconsistent robustness across different datasets and attack types remain unclear
- Evaluation protocol's generalizability to other GCL variants and larger-scale graphs is uncertain
- Adaptive attacks' effectiveness depends heavily on specific implementation details not fully disclosed

## Confidence

- **High Confidence**: GCL methods do not consistently outperform non-contrastive methods in adversarial robustness, supported by multiple datasets and attack scenarios
- **Medium Confidence**: Naive robustness proxies overestimate actual robustness, as adaptive attacks are more realistic but may not cover all possible attack strategies
- **Low Confidence**: Specific mechanisms by which GCL training objectives contribute to vulnerability, as paper identifies correlations rather than causal relationships

## Next Checks

1. **Implementation Verification**: Reproduce the three-step evaluation protocol with exact augmentation strategies and attack implementations to verify reported robustness gaps between GCL and non-contrastive methods

2. **Ablation Studies**: Systematically remove or modify individual components (specific augmentations, contrastive loss terms) in GCL models to isolate which aspects contribute most to vulnerability

3. **Cross-Dataset Generalization**: Evaluate the same GCL methods on additional datasets beyond current benchmarks to assess whether observed robustness patterns hold across different graph domains and sizes