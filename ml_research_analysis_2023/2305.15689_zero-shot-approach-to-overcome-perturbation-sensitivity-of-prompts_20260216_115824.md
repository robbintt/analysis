---
ver: rpa2
title: Zero-shot Approach to Overcome Perturbation Sensitivity of Prompts
arxiv_id: '2305.15689'
source_url: https://arxiv.org/abs/2305.15689
tags:
- prompts
- sentence
- prompt
- mask
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper tackles the problem of sensitivity of prompts to small
  changes, which hurts the performance of prompt-based methods for text classification.
  The authors propose to automatically generate multiple prompts from a base prompt
  using three augmentation techniques: positioning, subordination, and paraphrasing.'
---

# Zero-shot Approach to Overcome Perturbation Sensitivity of Prompts

## Quick Facts
- arXiv ID: 2305.15689
- Source URL: https://arxiv.org/abs/2305.15689
- Reference count: 13
- Key outcome: Automatically generates and ranks multiple prompts to overcome sensitivity to small changes, significantly outperforming manual and automatic prompts in zero-shot sentiment classification

## Executive Summary
This paper addresses the critical problem of prompt sensitivity in zero-shot text classification, where small changes to prompts can significantly impact model performance. The authors propose a novel approach that automatically generates multiple prompt variations from a base prompt using three augmentation techniques: positioning, subordination, and paraphrasing. These generated prompts are then ranked using a sensitivity-based metric that measures how well prompts respond to changes in sentiment polarity while remaining stable under synonym substitution. The method demonstrates substantial improvements over existing manual and automatic prompts across three benchmark sentiment classification datasets.

## Method Summary
The proposed method tackles prompt sensitivity by generating multiple prompt variations and selecting the most effective ones through a novel ranking mechanism. Starting with a base prompt, the system applies three augmentation techniques: positioning (placing prompts before/after sentences), subordination (using conjunctions to create dependencies), and paraphrasing (using MLM to find contextually appropriate substitutions). A ranking metric evaluates prompts based on their sensitivity to sentiment polarity changes while remaining stable under synonym replacement. For prediction, either the top-ranked prompt is selected or multiple top prompts are aggregated using weighted voting. The entire process operates in a zero-shot setting without requiring any labeled training data.

## Key Results
- Significantly outperforms manual prompts and existing automatic prompt methods across SST-2, MR, and CR datasets
- Top-ranked prompts achieve higher accuracy than low-ranked prompts, validating the effectiveness of the sensitivity-based ranking metric
- Prompt aggregation can correct individual prompt errors through complementary strengths when top prompts make independent mistakes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ranking quality is driven by prompt sensitivity to polarity word swaps
- Mechanism: The proposed metric measures whether replacing a positive polarity word with its negative counterpart (and vice versa) flips the model's prediction, while synonym replacement maintains it. Prompts that show high sensitivity to this flip but not to synonym changes are considered high-quality.
- Core assumption: High-quality prompts for sentiment classification should encode semantic relationships that cause predictions to flip when true sentiment polarity changes but remain stable under synonym substitution.
- Evidence anchors:
  - [abstract]: "We propose a novel ranking metric based on the intuition that high-quality prompts should be sensitive to the change of certain keywords in the given sentence."
  - [section 3.4]: "Intuitively, if the mapping token of the opposite label replaces the mapping token in a given sentence, the predicted label by a quality prompt should flip. On the other hand, the predicted label should remain the same if the mapping token in the sentence is replaced by its synonyms."
- Break condition: If the pre-trained language model has learned spurious correlations that cause predictions to flip for reasons unrelated to sentiment polarity, or if the synonym replacement introduces words with different connotations.

### Mechanism 2
- Claim: Automatic prompt generation through augmentation techniques expands the search space for effective prompts
- Mechanism: The method applies positioning (placing prompt before/after sentence), subordination (using conjunctions like "because" and "so" to create dependency), and paraphrasing (using MLM to find contextually appropriate word substitutions) to generate multiple prompts from a base prompt.
- Core assumption: The base prompt serves as a reasonable starting point, and these augmentation techniques will produce variations that preserve the core instruction while potentially improving performance through better contextual integration.
- Evidence anchors:
  - [section 3.3]: "Three augmentation techniques are designed: positioning, subordination, and paraphrasing. Different from Gao et al. (2021), where generative language models are used to generate candidate prompts, we use the same masked language models to paraphrase the base prompt."
  - [section 4.7]: "The figure shows that the highly-ranked prompts achieve higher accuracy than the low-ranked prompts in general, demonstrating the effectiveness of our proposed ranking metric."
- Break condition: If the base prompt is fundamentally flawed or if the augmentation techniques generate prompts that diverge too far from the intended task, or if the MLM's paraphrasing ability is limited for the specific domain.

### Mechanism 3
- Claim: Aggregation of top-ranked prompts can correct individual prompt errors through complementary strengths
- Mechanism: Instead of selecting only the top-ranked prompt, the method can aggregate predictions from the top-k prompts using a weighted average based on their ranking scores, potentially combining strengths and compensating for individual weaknesses.
- Core assumption: Different high-quality prompts may make independent errors on different examples, so combining them through weighted voting can improve overall accuracy.
- Evidence anchors:
  - [section 3.5]: "Prompt aggregation may help correct the mistakes of the individual prompts. We consider prediction confidence and use the soft labels computed by Eq. (4) in aggregation."
  - [section 4.6]: "We believe that aggregation performance improves when the top-ranked prompts make independent mistakes."
- Break condition: If the top-ranked prompts tend to make correlated errors on the same examples, aggregation could amplify rather than correct mistakes.

## Foundational Learning

- Concept: Masked Language Model (MLM) prediction mechanics
  - Why needed here: The entire approach relies on using BERT-like models to both generate paraphrased prompts and make sentiment predictions through masked token prediction.
  - Quick check question: How does a BERT model predict a masked token, and what does the probability distribution over vocabulary items represent?

- Concept: Zero-shot learning vs few-shot learning distinction
  - Why needed here: The paper explicitly contrasts its zero-shot approach with few-shot methods that use training data for prompt generation and ranking.
  - Quick check question: What are the key differences in assumptions and capabilities between zero-shot and few-shot learning approaches for NLP tasks?

- Concept: Prompt engineering principles for language models
  - Why needed here: Understanding how prompt structure, word choice, and positioning affect model behavior is crucial for both the base prompt design and the augmentation strategies.
  - Quick check question: Why are prompts sensitive to small changes in wording or position, and what principles guide effective prompt design?

## Architecture Onboarding

- Component map:
  - Input mapping module: Maps sentiment labels to vocabulary words
  - Base prompt storage: Stores the initial manual prompt
  - Prompt augmentation engine: Applies positioning, subordination, and paraphrasing to generate variations
  - MLM inference service: Used for both prompt generation (paraphrasing) and classification
  - Ranking metric calculator: Computes sensitivity scores based on polarity flips and synonym stability
  - Selection/aggregation module: Chooses top-1 or aggregates top-k prompts
  - Prediction engine: Generates final sentiment labels using selected prompts

- Critical path: Input sentence → Prompt augmentation → Ranking metric computation → Top-k selection → Classification using MLM → Output label

- Design tradeoffs:
  - Speed vs quality: Generating more prompts and using larger k improves quality but increases computation time
  - Base prompt dependence: Starting with a better base prompt reduces the burden on augmentation but requires domain expertise
  - Synonym coverage: Using WordNet provides comprehensive synonym coverage but may include contextually inappropriate alternatives

- Failure signatures:
  - All prompts ranking similarly low: Indicates the base prompt is fundamentally misaligned with the task
  - Ranking scores showing no sensitivity to polarity flips: Suggests the MLM hasn't learned meaningful sentiment associations
  - Performance degrading with higher k in aggregation: Indicates prompts are making correlated rather than independent errors

- First 3 experiments:
  1. Test the ranking metric on a small dataset by manually labeling which prompts should be high/low quality and comparing to automatic rankings
  2. Evaluate different k values for paraphrasing augmentation to find the optimal trade-off between diversity and quality
  3. Compare top-1 selection vs top-k aggregation performance on a validation set to determine which strategy works better for the target dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance and ranking of prompts vary across different domains beyond sentiment classification, such as NLI or paraphrasing tasks?
- Basis in paper: [explicit] The authors mention that Wang et al. (2022) fine-tune on multiple tasks including NLI and paraphrasing, and propose extending their work to other tasks.
- Why unresolved: The current study only evaluates on binary sentence-level sentiment classification datasets. The impact of prompt augmentation and ranking across diverse NLP tasks remains unexplored.
- What evidence would resolve it: Conducting experiments on multiple NLP tasks (e.g., NLI, paraphrasing, question answering) using the proposed ZS-SC method and comparing performance against domain-specific baselines.

### Open Question 2
- Question: How does the sensitivity of prompts to keyword changes differ between manual and automatically generated prompts, and what linguistic features contribute to this sensitivity?
- Basis in paper: [explicit] The authors propose a ranking metric based on the sensitivity of prompts to changes in mapping tokens, but do not analyze the linguistic properties that make certain prompts more sensitive than others.
- Why unresolved: While the ranking metric is effective, the underlying linguistic mechanisms that make prompts sensitive to keyword changes are not explored. This limits understanding of why certain prompts perform better.
- What evidence would resolve it: Conducting a linguistic analysis of high- and low-ranked prompts to identify features (e.g., syntactic complexity, semantic coherence) that correlate with sensitivity to keyword changes.

### Open Question 3
- Question: Can the proposed ranking metric be adapted to evaluate prompts in few-shot or semi-supervised settings, where limited labeled data is available?
- Basis in paper: [inferred] The ranking metric is designed for zero-shot settings without labeled data. However, in practical scenarios, limited labeled data may be available, and adapting the metric could enhance its utility.
- Why unresolved: The current metric relies solely on keyword sensitivity, which may not fully leverage the information available in few-shot settings. Adapting it to incorporate limited labeled data could improve ranking accuracy.
- What evidence would resolve it: Modifying the ranking metric to incorporate a small amount of labeled data and evaluating its performance against the zero-shot version on benchmark datasets.

### Open Question 4
- Question: How does the performance of ZS-SC scale with the size and diversity of the unlabeled corpus used for generating paraphrasing tokens and ranking prompts?
- Basis in paper: [inferred] The method uses an unlabeled corpus for paraphrasing and ranking, but the impact of corpus size and diversity on performance is not explored.
- Why unresolved: The quality of generated prompts and the effectiveness of the ranking metric may depend on the characteristics of the unlabeled corpus. Understanding this relationship could guide corpus selection.
- What evidence would resolve it: Conducting experiments with varying corpus sizes and diversities (e.g., domain-specific vs. general corpora) and analyzing the impact on prompt quality and model performance.

## Limitations

- The method's performance heavily depends on the quality of the base prompt and may not work well with fundamentally flawed starting prompts
- The ranking metric's reliance on synonym replacement from WordNet may not capture all relevant semantic variations
- The aggregation strategy's assumption that top-ranked prompts make independent errors is not empirically validated

## Confidence

**High confidence** in the core mechanism: The approach of generating multiple prompts and selecting/ranking based on sensitivity to polarity changes is methodologically sound and the experimental results show consistent improvements over baseline methods across multiple datasets.

**Medium confidence** in the ranking metric: While the intuition is reasonable, the paper doesn't provide extensive ablation studies or alternative ranking metrics to validate that sensitivity to polarity flips is the optimal criterion for prompt quality.

**Low confidence** in the aggregation benefits: The paper mentions aggregation as a potential improvement but provides limited empirical evidence for its effectiveness, and the assumption about independent errors is not tested.

## Next Checks

1. **Ablation study on ranking metric components**: Remove either the polarity flip sensitivity or synonym stability component from the ranking metric and measure the impact on performance to determine which aspect is more critical for prompt quality assessment.

2. **Error correlation analysis for aggregation**: For each test example, analyze whether the top-k prompts make independent or correlated errors by comparing their individual predictions, and measure how this correlation relates to aggregation performance.

3. **Base prompt sensitivity analysis**: Systematically vary the base prompt structure (e.g., different template formats, different mapping tokens) and measure how much the final performance depends on having an optimal starting prompt versus the augmentation and ranking process.