---
ver: rpa2
title: 'Getting from Generative AI to Trustworthy AI: What LLMs might learn from Cyc'
arxiv_id: '2308.04445'
source_url: https://arxiv.org/abs/2308.04445
tags:
- which
- have
- such
- each
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper outlines 16 key capabilities necessary for a trustworthy
  AI, highlighting limitations in current generative AI systems like ChatGPT. It presents
  Cyc, a symbolic AI system that uses formal logic to represent knowledge and perform
  reasoning, as a complementary approach to address these limitations.
---

# Getting from Generative AI to Trustworthy AI: What LLMs might learn from Cyc

## Quick Facts
- arXiv ID: 2308.04445
- Source URL: https://arxiv.org/abs/2308.04445
- Reference count: 2
- Primary result: Proposes hybridizing large language models and Cyc, a symbolic AI system, to create more trustworthy and interpretable AI systems by combining the broad knowledge and speed of LLMs with the reasoning and explainability of symbolic systems.

## Executive Summary
This paper outlines 16 key capabilities necessary for a trustworthy AI, highlighting limitations in current generative AI systems like ChatGPT. It presents Cyc, a symbolic AI system that uses formal logic to represent knowledge and perform reasoning, as a complementary approach to address these limitations. The authors propose hybridizing the strengths of generative AI and Cyc to create more reliable and interpretable AI systems. Key synergies include using Cyc to verify LLM outputs, extending LLM capabilities through inference, and leveraging LLMs to enhance Cyc's knowledge base. The approach aims to combine the broad knowledge and speed of LLMs with the reasoning and explainability of symbolic systems like Cyc.

## Method Summary
The paper proposes several synergies between LLMs and Cyc: using symbolic systems to reject false confabulations from LLMs, using symbolic systems to bias LLMs towards correctness, using LLMs to generate candidate assertions/rules for symbolic systems, using symbolic systems to extend LLM coverage through inference, and using symbolic systems to provide explanation and provenance support. While the paper does not provide specific implementation details or code, it outlines a conceptual framework for integrating these two approaches to create more trustworthy AI systems.

## Key Results
- Proposes hybridizing LLMs and Cyc to create more trustworthy and interpretable AI systems
- Identifies 16 key capabilities necessary for trustworthy AI
- Outlines several potential synergies between LLMs and symbolic systems, including using Cyc as a source of truth and inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cyc provides a source of truth to bias LLMs toward correctness
- Mechanism: Cyc's vast repository of default-true common sense knowledge acts as a "compression algorithm" that can generate billions of logically consistent statements. These statements can be used to train or bias LLMs to be more aligned with common sense and correctness.
- Core assumption: The patterns in language reflect the real world, but LLMs can be misled by superficial falsehoods. Cyc's knowledge base can counteract this.
- Evidence anchors:
  - [abstract] "Cyc is a repository of default-true common sense, including things like that (of course axiomatized much more generally than about cats and breathing!), which are so fundamental that it would be confusing or insulting for a person to explicitly say or write them when communicating with another human being."
  - [section 4] "LLMs are trained on (some fraction of) the corpora of ten trillion sentences on the internet. But there is so much tacit common sense about the world that is assumed yet rarely or never explicitly expressed."
- Break condition: If Cyc's knowledge base is incomplete or contains errors, it could introduce biases rather than correct them.

### Mechanism 2
- Claim: Cyc can act as a source of inference to dramatically extend LLM coverage
- Mechanism: Cyc's inference capabilities can be used to generate novel multi-step inferences from a set of related statements. These inferences can then be fed back to the LLM, effectively extending its coverage and providing a semantic feedforward layer.
- Core assumption: LLMs are capable of recapitulating statements or extending patterns and analogies, but lack the ability to produce novel multi-step inferences.
- Evidence anchors:
  - [abstract] "LLMs are capable of recapitulating statements or extending patterns and analogies but both of those are impoverished compared to the novel multi-step inferences that humans can produce from a set of 'given' statements."
  - [section 4] "The Cyc reasoner therefore could act as a potentially exponential amplifier over the latent content already expressible by the LLM."
- Break condition: If the inference process is too slow or the generated inferences are not useful to the LLM, the benefit may not justify the computational cost.

### Mechanism 3
- Claim: Cyc can provide audit and provenance support for LLM explanations
- Mechanism: Cyc's ability to provide complete, auditable, step-by-step traces of its reasoning can be used to explain LLM conclusions and provide provenance for the knowledge used in intermediate steps.
- Core assumption: LLM's inability to soundly explain their reasoning and provide provenance renders them unsuitable for applications requiring trust and auditability.
- Evidence anchors:
  - [abstract] "Cyc can provide exactly this; provenance and explicit justification are the superpower of machine reasoning over non-symbolic LLM representation."
  - [section 3] "The Cyc reasoner produces a complete, auditable, step-by-step trace of its chain of reasoning behind each pro- and con- argument it makes, including the full provenance of every fact and rule which was in any way used in each argument."
- Break condition: If the explanation process is too complex or the provenance information is not actionable, it may not improve trust in the LLM's outputs.

## Foundational Learning

- Concept: Higher-order logic
  - Why needed here: Cyc uses higher-order logic to represent knowledge, which is more expressive than first-order logic or knowledge graphs. Understanding this is crucial for grasping how Cyc can represent and reason about complex concepts.
  - Quick check question: What is the key difference between first-order and higher-order logic, and why is this distinction important for AI systems like Cyc?

- Concept: Common sense reasoning
  - Why needed here: Cyc's knowledge base is built on common sense reasoning, which is essential for AI systems to understand and interact with the world in a human-like manner. This concept is central to addressing the limitations of LLMs.
  - Quick check question: How does common sense reasoning differ from formal logical reasoning, and why is it important for AI systems to have both capabilities?

- Concept: Defeasible reasoning
  - Why needed here: Cyc's knowledge base is defeasible, meaning that its assertions are only true by default and can be revised in light of new information. This concept is crucial for understanding how Cyc can handle uncertainty and conflicting information.
  - Quick check question: What is defeasible reasoning, and how does it differ from deductive reasoning? Why is defeasible reasoning important for AI systems dealing with real-world knowledge?

## Architecture Onboarding

- Component map:
  - Cyc knowledge base -> Cyc inference engine -> LLM -> Integration layer -> User interface

- Critical path:
  1. Receive user input
  2. LLM processes input and generates response
  3. Cyc checks response for logical consistency and provides explanations
  4. Cyc generates additional inferences to extend LLM's coverage
  5. LLM incorporates Cyc's insights and generates final response

- Design tradeoffs:
  - Expressiveness vs. speed: Cyc uses higher-order logic for expressiveness but employs specialized reasoners for speed
  - Hand-authored vs. automatically generated knowledge: Cyc relies on hand-authored assertions but uses tools to accelerate the process
  - Integration complexity vs. performance gain: Integrating Cyc and LLM requires complex engineering but can significantly improve AI capabilities

- Failure signatures:
  - Inconsistent explanations: If Cyc and LLM provide conflicting explanations, it may indicate a bug in the integration layer
  - Slow response times: If the combined system is too slow, it may indicate inefficient use of Cyc's inference capabilities
  - Incorrect inferences: If Cyc generates incorrect inferences, it may indicate errors in its knowledge base or reasoning algorithms

- First 3 experiments:
  1. Test Cyc's ability to check LLM outputs for logical consistency using a small set of simple statements
  2. Evaluate the performance impact of integrating Cyc's inference capabilities with an LLM on a benchmark task
  3. Assess the quality of explanations generated by the combined system on a set of user queries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific heuristics and representations could be developed to improve Cyc's speed in handling highly expressive higher-order logic?
- Basis in paper: [explicit] The paper discusses Cyc's approach to speed, mentioning its use of 1,100 specialized heuristic-level reasoners and redundant representations, but acknowledges that further improvements are possible.
- Why unresolved: While Cyc has made significant progress in speed, the paper suggests there's still room for improvement, especially in handling complex queries quickly.
- What evidence would resolve it: Research demonstrating new heuristics or representations that significantly improve Cyc's reasoning speed without sacrificing expressiveness.

### Open Question 2
- Question: How can the synergy between LLMs and symbolic systems like Cyc be optimized to create a trustworthy general AI?
- Basis in paper: [explicit] The paper outlines several potential synergies between LLMs and Cyc, such as using Cyc as a source of truth to bias LLMs towards correctness, and using LLMs to generate candidate assertions for Cyc.
- Why unresolved: While the paper proposes several potential synergies, it does not provide a detailed roadmap for how these synergies can be practically implemented and optimized.
- What evidence would resolve it: A working prototype or detailed implementation plan demonstrating effective integration of LLMs and symbolic systems like Cyc.

### Open Question 3
- Question: What are the potential risks and ethical considerations of hybridizing LLMs and symbolic AI systems like Cyc?
- Basis in paper: [inferred] While the paper focuses on the potential benefits of hybridizing LLMs and Cyc, it does not extensively discuss potential risks or ethical considerations.
- Why unresolved: As AI systems become more powerful and integrated, it's crucial to consider potential risks and ethical implications, which the paper does not fully address.
- What evidence would resolve it: A comprehensive analysis of potential risks and ethical considerations, along with proposed mitigation strategies for hybrid AI systems.

## Limitations
- Lack of empirical validation: The proposed synergies between LLMs and Cyc remain theoretical, with no quantitative evidence demonstrating their effectiveness
- Scalability concerns: The paper does not address how well the proposed integration would scale to handle real-world applications and large datasets
- Implementation complexity: The integration challenges of combining LLMs and symbolic systems are acknowledged but not deeply explored

## Confidence
- High confidence: The identification of 16 key capabilities for trustworthy AI and the analysis of current LLM limitations are well-grounded in existing literature
- Medium confidence: The proposed mechanisms for hybridizing LLMs and Cyc are conceptually sound but lack empirical validation
- Low confidence: The scalability and practical implementation details of the proposed integration remain largely unaddressed

## Next Checks
1. Implement a prototype integration between a standard LLM (like GPT-3) and a symbolic reasoning system (either Cyc or a simpler alternative) to test the basic interaction model and measure performance overhead
2. Conduct controlled experiments comparing the trustworthiness of LLM-only, Cyc-only, and hybrid system outputs on tasks requiring explanation, deduction, and defeasibility
3. Perform an ablation study to quantify the individual contributions of each proposed synergy (e.g., Cyc's role in rejecting confabulations vs. extending coverage) to overall system performance