---
ver: rpa2
title: Robust Low-Rank Matrix Completion via a New Sparsity-Inducing Regularizer
arxiv_id: '2310.04762'
source_url: https://arxiv.org/abs/2310.04762
tags:
- matrix
- sssk
- function
- norm
- robust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a new nonconvex loss function, called hybrid
  ordinary-Welsch (HOW), which only changes the weights of outlier-contaminated data.
  The implicit regularizer (IR) generated by HOW can make the solution sparse.
---

# Robust Low-Rank Matrix Completion via a New Sparsity-Inducing Regularizer

## Quick Facts
- arXiv ID: 2310.04762
- Source URL: https://arxiv.org/abs/2310.04762
- Reference count: 40
- Key outcome: Proposes HOW loss function and its implicit regularizer for robust matrix completion, showing superior performance over state-of-the-art methods

## Executive Summary
This paper introduces a novel nonconvex loss function called hybrid ordinary-Welsch (HOW) for robust low-rank matrix completion. The HOW loss uniquely down-weights only outlier-contaminated data while preserving full weight on normal data, unlike the standard Welsch function. The implicit regularizer (IR) generated by HOW is quasiconvex with a closed-form proximity operator, enabling efficient optimization. The authors apply this to robust matrix completion and develop an ADMM-based algorithm that outperforms existing methods in restoration performance across synthetic and real-world datasets.

## Method Summary
The paper develops a new sparsity-inducing regularizer for robust matrix completion by introducing the HOW loss function, which transitions from quadratic to Welsch-like penalties based on residual magnitude. The implicit regularizer φσ,λ is derived through conjugate analysis and has a closed-form proximity operator Pφσ,λ(x) = max{0, |x| - |x|·e^(λ²-x²)/σ²} sign(x). The RMC problem is formulated with both a nuclear norm term and the φσ,λ-norm, solved via ADMM by alternating updates of the low-rank component, sparse outlier matrix, and dual variables.

## Key Results
- The HOW loss only changes weights of outlier-contaminated data, unlike Welsch which down-weights all data
- The implicit regularizer φσ,λ is quasiconvex with a closed-form proximity operator, avoiding iterative computation required by ℓp-norms
- The proposed algorithm outperforms state-of-the-art methods in relative reconstruction error (RRE), PSNR, and SSIM on synthetic and real-world datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The HOW loss function only down-weights outlier-contaminated data, unlike the standard Welsch function which down-weights all data including normal data.
- **Mechanism**: HOW applies a quadratic penalty to data within a threshold λ, and switches to a Welsch-like tail penalty only for large residuals (|x| > λ). This preserves full weight on normal data while reducing outlier influence.
- **Core assumption**: Outliers are sparse and have large residuals compared to Gaussian noise on normal data.
- **Evidence anchors**:
  - [abstract] states HOW "only changes the weights of outlier-contaminated data"
  - [section] "Huber function assigns equal weights for all normal data via the quadratic function, while assigning smaller weights to outlier-corrupted data... The advantage of the Huber function is that it only changes the weights of outlier-contaminated data, whereas the Welsch function down-weighs all observed data"
  - [corpus] no direct evidence; paper itself is the anchor
- **Break condition**: If outliers are dense or have residuals overlapping the normal data range, the threshold λ may incorrectly classify normal data as outliers, leading to loss of useful information.

### Mechanism 2
- **Claim**: The implicit regularizer (IR) φσ,λ generated by HOW is quasiconvex and sparsity-promoting, with a closed-form proximity operator.
- **Mechanism**: φσ,λ is derived via conjugate analysis of a convex auxiliary function. Its closed-form proximity operator Pφσ,λ(x) = max{0, |x| - |x|·e^(λ²-x²)/σ²} sign(x) avoids iterative computation required by ℓp-norms.
- **Core assumption**: The Legendre-Fenchel transform can be used to derive the IR and its properties from the HOW loss.
- **Evidence anchors**:
  - [abstract] "the regularizer is quasiconvex and that the corresponding Moreau envelope is convex"
  - [section] "we theoretically show that the regularizer is quasiconvex and that the corresponding Moreau envelope is convex"
  - [section] "a closed-form expression of its proximity operator... is derived"
  - [corpus] weak; relies on paper's own proofs
- **Break condition**: If σ > √2λ, φσ,λ loses quasiconvexity, potentially degrading sparsity and convergence guarantees.

### Mechanism 3
- **Claim**: Replacing the nuclear norm with the matrix φσ,λ-norm in robust matrix completion yields better low-rank recovery by reducing shrinkage bias.
- **Mechanism**: The φσ,λ-norm applies the scalar φσ,λ function element-wise to singular values, preserving larger values better than the ℓ1-norm (nuclear norm). Combined with the HOW loss, it resists outliers while promoting sparsity.
- **Core assumption**: The matrix φσ,λ-norm is a valid convex surrogate for rank and better preserves singular value structure.
- **Evidence anchors**:
  - [abstract] "the implicit regularizer (IR) generated by HOW can make the solution sparse"
  - [section] "Compared with nonconvex regularizers like the ℓp-norm with 0 < p < 1... the developed regularizer has a closed-form proximity operator"
  - [section] "The proposed sparsity-inducing regularizer is utilized to solve the RMC problem"
  - [corpus] no external evidence; relies on paper's theoretical analysis
- **Break condition**: If the matrix rank is high or the observation ratio is very low, the regularization may fail to recover the true low-rank structure despite reduced bias.

## Foundational Learning

- **Concept**: Moreau envelope and proximity operator
  - Why needed here: Core to the algorithm's update steps; proximity operators are used for both the regularizer and the nuclear norm term.
  - Quick check question: What is the relationship between the Moreau envelope of a function and its proximity operator?

- **Concept**: Alternating Direction Method of Multipliers (ADMM)
  - Why needed here: The paper's algorithm solves a constrained optimization problem by splitting variables and using ADMM updates.
  - Quick check question: How does ADMM handle the constraint XXX = MMM + SSS in the robust matrix completion formulation?

- **Concept**: Rank minimization and nuclear norm relaxation
  - Why needed here: Understanding why replacing nuclear norm with φσ,λ-norm can reduce bias in low-rank recovery.
  - Quick check question: Why does nuclear norm minimization underestimate large singular values compared to the original rank function?

## Architecture Onboarding

- **Component map**: HOW loss function → IR φσ,λ → proximity operator Pφσ,λ → RMC objective with φσ,λ-norm → ADMM variables (MMM, SSS, ΛΛΛ) → Update steps via SVD and proximity operators

- **Critical path**: Compute SVD → Apply proximity operator to singular values → Update sparse outlier matrix SSS → Update dual variable ΛΛΛ → Check convergence

- **Design tradeoffs**:
  - Choosing σ ≤ √2λ ensures φσ,λ is quasiconvex but limits flexibility in tuning
  - Closed-form proximity operator avoids inner-loop iterations but requires closed-form derivable from HOW
  - ADMM splitting introduces ρ tuning but enables parallel subproblem solves

- **Failure signatures**:
  - Slow or stalled convergence: Check if ρ update rule is too aggressive (µ too large)
  - Noisy or inaccurate recovery: Verify σ/λ choice; too large σ may lose quasiconvexity
  - Memory issues with large matrices: SVD dominates complexity; consider randomized SVD for very large problems

- **First 3 experiments**:
  1. Verify HOW loss transitions from quadratic to Welsch tail at λ by plotting lσ,λ(x) for different x.
  2. Test Pφσ,λ(x) against Pℓ1,λ(x) to confirm reduced bias for |x| > λ.
  3. Run synthetic low-rank matrix completion with sparse outliers; compare RRE vs λ for different γ, α, β settings.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of $\lambda$ in the HOW function affect the performance of robust matrix completion in real-world applications with varying noise levels and outlier distributions?
- Basis in paper: [explicit] The paper discusses the impact of $\lambda$ on the relative reconstruction error (RRE) for synthetic data, showing that there is a wide range for choosing $\lambda$ even when the observation ratio decreases.
- Why unresolved: The paper does not provide a comprehensive analysis of how $\lambda$ affects performance in real-world scenarios with different noise levels and outlier distributions.
- What evidence would resolve it: Conducting experiments on various real-world datasets with different noise levels and outlier distributions to determine the optimal $\lambda$ for each scenario.

### Open Question 2
- Question: Can the HOW function and its associated regularizer be extended to other low-rank matrix completion problems beyond robust matrix completion, such as matrix completion with missing data or collaborative filtering?
- Basis in paper: [inferred] The paper focuses on applying the HOW function and its regularizer to robust matrix completion, but the underlying principles of the function and regularizer could potentially be applicable to other low-rank matrix completion problems.
- Why unresolved: The paper does not explore the applicability of the HOW function and its regularizer to other low-rank matrix completion problems.
- What evidence would resolve it: Extending the HOW function and its regularizer to other low-rank matrix completion problems and comparing their performance with existing methods.

### Open Question 3
- Question: How does the computational complexity of the proposed algorithm compare to other state-of-the-art methods for robust matrix completion, especially for large-scale problems?
- Basis in paper: [explicit] The paper states that the total complexity of Algorithm 1 is $O(K \min(m, n)mn)$, where $K$ is the required iteration number, and compares the runtime of the proposed algorithm with other methods for small-scale problems.
- Why unresolved: The paper does not provide a detailed analysis of the computational complexity of the proposed algorithm for large-scale problems or compare it with other methods for large-scale problems.
- What evidence would resolve it: Conducting experiments on large-scale problems to compare the computational complexity of the proposed algorithm with other state-of-the-art methods.

## Limitations
- Theoretical guarantees for quasiconvexity of φσ,λ depend on the strict parameter constraint σ ≤ √2λ, limiting practical flexibility
- The paper lacks comprehensive sensitivity analysis for critical parameters (σ, λ, ρ) across diverse scenarios
- No convergence guarantees are provided for the complete ADMM algorithm, only properties of the implicit regularizer

## Confidence
- **High confidence**: The HOW loss function design and its distinction from Welsch loss is clearly specified and mathematically sound
- **Medium confidence**: The closed-form proximity operator and theoretical properties of φσ,λ, given the paper's proofs appear rigorous but have not been independently verified
- **Medium confidence**: The ADMM algorithm implementation for robust matrix completion, as the methodology is standard but specific parameter choices affect performance

## Next Checks
1. Verify quasiconvexity of φσ,λ breaks when σ > √2λ by testing φσ,λ on a grid of values and checking for multiple local minima
2. Implement synthetic experiments varying outlier density and magnitude to test the robustness of HOW loss under different contamination scenarios
3. Conduct ablation studies comparing the HOW regularizer against ℓp-norms with different p values on identical datasets to quantify the claimed advantage of the closed-form proximity operator