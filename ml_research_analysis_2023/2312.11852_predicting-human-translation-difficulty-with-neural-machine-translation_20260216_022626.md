---
ver: rpa2
title: Predicting Human Translation Difficulty with Neural Machine Translation
arxiv_id: '2312.11852'
source_url: https://arxiv.org/abs/2312.11852
tags:
- translation
- surprisal
- language
- difficulty
- reading
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper uses data from hundreds of translators across 13 language
  pairs to evaluate whether surprisal and attentional features from neural machine
  translation models predict human translation difficulty. It finds that both translation
  surprisal and monolingual surprisal are strong predictors of translation duration,
  with translation surprisal being the single most successful predictor.
---

# Predicting Human Translation Difficulty with Neural Machine Translation

## Quick Facts
- **arXiv ID**: 2312.11852
- **Source URL**: https://arxiv.org/abs/2312.11852
- **Reference count**: 31
- **Primary result**: Translation surprisal is the single most successful predictor of human translation duration, with surprisal and attention features being complementary predictors of translation difficulty

## Executive Summary
This paper investigates whether neural machine translation (NMT) features can predict human translation difficulty by analyzing data from hundreds of translators across 13 language pairs. The study evaluates three types of predictors: monolingual surprisal from a large language model, translation surprisal from an NMT model, and attentional features from the NMT model's encoder and decoder. Using mixed linear models with behavioral data from the CRITT TPR-DB, the research finds that translation surprisal is the most powerful predictor of production duration, while attentional features provide supplementary predictive power when combined with surprisal. The results support the hypothesis that NMT models capture aspects of human cognitive effort during translation tasks.

## Method Summary
The study uses data from 312 translators across 13 language pairs from the CRITT TPR-DB, analyzing source reading time, target reading time, and translation duration. Translation surprisal and attention features are computed using the NLLB-200 NMT model, while monolingual surprisal is computed using the mGPT language model. The analysis employs mixed linear regression models with language pair and participant ID as random effects, using 10-fold cross-validation. Features are normalized by segment length, and predictive power is evaluated using log-likelihood delta compared to baseline models with control features (segment length, frequency, position).

## Key Results
- Translation surprisal (smt) is the single most successful predictor of production duration across all language pairs
- Both translation surprisal and monolingual surprisal strongly predict translation duration, with translation surprisal being more effective
- Attention-based features contribute supplementary predictive power when combined with surprisal, though they are less consistent predictors on their own

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translation surprisal (smt) captures unpredictability in both source and target contexts, making it a strong predictor of human translation difficulty.
- Mechanism: smt is calculated from the NMT model's probability distribution over possible translations conditioned on both the source sentence and previously translated tokens. Higher surprisal means lower probability, indicating greater cognitive effort.
- Core assumption: The NMT model's probability distribution correlates with human cognitive difficulty in translation tasks.
- Evidence anchors:
  - [abstract] "we find that surprisal and attention are complementary predictors of translation difficulty, and that surprisal derived from a NMT model is the single most successful predictor of production duration."
  - [section 3.2] "The translation surprisal of a translation can be obtained from the distribution of a neural machine translation model, pmt, conditioned on a source sequence and previously translated tokens... We expect translation surprisal to be the more successful predictor because this measure incorporates context on both the source and target sides."
  - [corpus] Weak evidence - corpus shows related papers but no direct evidence linking NMT probability distributions to human cognitive effort.
- Break condition: If the NMT model's training objective diverges from human cognitive processes, or if the NMT model fails to capture important linguistic features that affect human translation difficulty.

### Mechanism 2
- Claim: Monolingual surprisal (slm) from a large language model predicts source text reading time during translation tasks.
- Mechanism: slm is calculated from the language model's probability distribution over tokens conditioned only on the source text context. Higher surprisal indicates more effort in reading the source text.
- Core assumption: Language model probability distributions capture human reading difficulty patterns.
- Evidence anchors:
  - [abstract] "we find that surprisal and attention are complementary predictors of translation difficulty"
  - [section 3.1] "Given the well-established link between slm and reading time... we test whether slm predicts reading times for source and target texts when participants are engaged in the act of translation."
  - [corpus] Weak evidence - corpus shows related papers but no direct evidence linking language model surprisal to human reading patterns.
- Break condition: If the language model's training data or architecture doesn't align with human language processing patterns, or if the language model fails to capture source text difficulty aspects relevant to translators.

### Mechanism 3
- Claim: Attentional features from NMT models capture aspects of translation difficulty beyond surprisal, particularly related to source text processing.
- Mechanism: Attentional features measure the flow of attention weights within the encoder (self-attention) and between encoder and decoder (cross-attention). These features capture aspects like focus on difficult tokens, context usage, and alignment patterns.
- Core assumption: The distribution of attention weights in NMT models correlates with human cognitive processes in translation.
- Evidence anchors:
  - [abstract] "We find that surprisal and attention are complementary predictors of translation difficulty"
  - [section 4] "We consider all three sets of attention: encoder self-attention, cross-attention and decoder self-attention... We propose that the three attentional modules roughly align with these three stages [reading, transferring, writing]."
  - [corpus] Weak evidence - corpus shows related papers but no direct evidence linking NMT attention distributions to human cognitive processes.
- Break condition: If the NMT model's attention mechanism doesn't align with human cognitive processes, or if attention weights fail to capture relevant aspects of translation difficulty.

## Foundational Learning

- Concept: Information Theory and Surprisal
  - Why needed here: Surprisal is a core concept based on information theory, measuring the unexpectedness of information. Understanding this is crucial for grasping how surprisal predicts cognitive effort.
  - Quick check question: How is surprisal mathematically defined, and what does higher surprisal indicate about information content?

- Concept: Neural Machine Translation Architecture
  - Why needed here: The paper relies on NMT models with encoder, decoder, and attention mechanisms. Understanding this architecture is essential for interpreting how attentional features are extracted and their relevance to translation difficulty.
  - Quick check question: What are the three types of attention in transformer-based NMT models, and what role does each play in the translation process?

- Concept: Linear Mixed Models in Statistical Analysis
  - Why needed here: The paper uses linear mixed models with language pair and participant ID as random effects. Understanding this statistical approach is important for interpreting the results and their generalizability.
  - Quick check question: Why are language pair and participant ID included as random effects in the mixed models, and what does this account for?

## Architecture Onboarding

- Component map:
  CRITT TPR-DB -> Behavioral Data (reading time, production duration) -> Mixed Linear Models -> Log-likelihood delta evaluation
  mGPT/NLLB-200 -> NMT Features (surprisal, attention) -> Feature Extraction -> Mixed Linear Models

- Critical path:
  1. Extract behavioral data (reading time, production duration) from TPR-DB
  2. Compute surprisal and attentional features using mGPT and NLLB-200
  3. Train linear mixed models with control features
  4. Evaluate feature contribution using log-likelihood delta
  5. Analyze results across language pairs and segmentation levels

- Design tradeoffs:
  - Using aggregated attention features (averaged across heads and layers) vs. more granular features
  - Segment-level vs. word-level analysis
  - Mixed model (all language pairs) vs. individual language pair models
  - Log-likelihood delta vs. other evaluation metrics

- Failure signatures:
  - Low or negative log-likelihood delta for a feature indicates it doesn't predict difficulty beyond control features
  - Inconsistent results across language pairs or segmentation levels suggest the feature may not be universally applicable
  - High variance in feature coefficients across folds indicates instability

- First 3 experiments:
  1. Train a baseline model with only control features and evaluate log-likelihood
  2. Add monolingual surprisal (slm) to the baseline model and measure log-likelihood delta
  3. Add translation surprisal (smt) to the baseline model and measure log-likelihood delta

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal ways to combine attention weights from different layers and heads to maximize predictive power for translation difficulty?
- Basis in paper: [explicit] The paper notes that current attentional features are relatively simple and suggests that more sophisticated combinations of attention weights could lead to stronger predictions.
- Why unresolved: Current attentional features average attention across heads and layers without exploring more complex combinations or weightings.
- What evidence would resolve it: Systematic comparison of different attention combination strategies (weighted averaging, attention on attention, layer-specific combinations) and their predictive performance against human translation difficulty measures.

### Open Question 2
- Question: How do individual language pairs differ in their relationship between NMT surprisal/attention features and human translation difficulty?
- Basis in paper: [explicit] The paper notes that analyses combining data from all 13 language pairs are prioritized, but analyzing translation challenges in individual language pairs is identified as a high priority for future work.
- Why unresolved: The current analysis focuses on pooled data across language pairs, potentially masking language-specific patterns.
- What evidence would resolve it: Detailed analysis of individual language pairs showing how the relationship between NMT features and human difficulty varies by language combination, particularly for language pairs with different typological properties.

### Open Question 3
- Question: What specific linguistic constructions in different languages contribute most to translation difficulty, and how do NMT models capture these differences?
- Basis in paper: [explicit] The paper suggests exploring whether predictors are sensitive to constructions known to cause processing difficulty in specific languages, and mentions that understanding idiosyncratic challenges in each language pair is important.
- Why unresolved: The current analysis uses general surprisal and attention features without identifying specific linguistic sources of difficulty.
- What evidence would resolve it: Analysis linking specific linguistic phenomena (e.g., compound nouns, word order variations, case marking) to NMT feature values and human difficulty measures across different language pairs.

## Limitations
- The study relies on the assumption that NMT model probability distributions and attention mechanisms correlate with human cognitive processes during translation, which is supported empirically but not directly validated
- The analysis uses aggregated attention features (averaged across heads and layers), which may obscure important variations in how attention patterns relate to translation difficulty
- The segment-level analysis may mask word-level variations that could provide more granular insights into the relationship between NMT features and human processing

## Confidence

- **High confidence**: Translation surprisal is the single most successful predictor of production duration across multiple language pairs and segmentation levels
- **Medium confidence**: Surprisal and attention features are complementary predictors, though the relative contribution of attentional features is less consistent than surprisal features
- **Low confidence**: Attentional features capture specific aspects of translation difficulty (source text processing vs. target text generation) based on architectural intuition rather than direct empirical evidence

## Next Checks
1. **Validate correlation assumptions**: Conduct a controlled experiment where human translators perform translation tasks while being monitored for cognitive load (e.g., eye-tracking, EEG), then directly compare these measures against NMT surprisal and attention features to validate the assumed correlation.
2. **Test feature granularity**: Re-run the analysis using per-head and per-layer attention features rather than aggregated features to determine whether more granular attention patterns provide better predictive power or reveal specific attention mechanisms that correlate with human processing.
3. **Cross-model validation**: Apply the same analysis framework to different NMT architectures (e.g., LSTM-based vs. transformer-based models, or different sizes of the same architecture) to determine whether the predictive power of surprisal and attention features is consistent across model types or specific to NLLB-200.