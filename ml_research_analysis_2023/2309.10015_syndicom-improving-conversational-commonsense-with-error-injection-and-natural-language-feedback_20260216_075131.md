---
ver: rpa2
title: 'SYNDICOM: Improving Conversational Commonsense with Error-Injection and Natural
  Language Feedback'
arxiv_id: '2309.10015'
source_url: https://arxiv.org/abs/2309.10015
tags:
- dialogue
- feedback
- response
- syndicom
- commonsense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SYNDICOM, a method to improve dialogue response
  generation by addressing commonsense reasoning errors. The approach synthesizes
  a dataset of dialogues with both valid and invalid responses, along with natural
  language feedback (NLF) explaining the errors.
---

# SYNDICOM: Improving Conversational Commonsense with Error-Injection and Natural Language Feedback

## Quick Facts
- arXiv ID: 2309.10015
- Source URL: https://arxiv.org/abs/2309.10015
- Authors: 
- Reference count: 8
- Key outcome: 53% relative improvement over ChatGPT on ROUGE-1 and 57% human evaluator preference for SYNDICOM

## Executive Summary
SYNDICOM addresses the challenge of improving dialogue response generation by focusing on commonsense reasoning errors. The method synthesizes a dataset of dialogues containing both valid and invalid responses, paired with natural language feedback explaining the errors. By training models to predict this feedback and generate improved responses conditioned on the feedback, SYNDICOM achieves significant performance gains over baseline models including ChatGPT. The approach eliminates the need for reinforcement learning by leveraging natural language feedback as a richer training signal.

## Method Summary
SYNDICOM operates through a two-step process: first, it generates a dataset of dialogues with valid responses that are systematically corrupted into invalid ones using semantic opposites derived from the ATOMIC knowledge graph. Human annotators then provide natural language feedback explaining why these corrupted responses violate commonsense. A feedback prediction model is trained on this data to generate explanations for invalid responses, followed by a response generation model that produces improved responses conditioned on the predicted feedback, the invalid response, and the dialogue context. Both models are fine-tuned versions of GPT-3.5 (text-davinci-003).

## Key Results
- Achieves 53% relative improvement over ChatGPT on ROUGE-1 metric
- Human evaluators prefer SYNDICOM responses over ChatGPT 57% of the time
- Multi-step approach with feedback generation shows consistent improvements across multiple evaluation metrics (ROUGE, BLEU, BERTScore, SacreBLEU, METEOR)
- Demonstrates effectiveness of natural language feedback as an alternative to reinforcement learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Error injection followed by human feedback creates a scalable data generation pipeline for commonsense reasoning
- Mechanism: Starting with valid responses and systematically corrupting them via semantic opposites ensures invalid responses violate commonsense in predictable ways, allowing human workers to provide targeted feedback
- Core assumption: Semantic opposites from ATOMIC reliably produce commonsense violations that humans can identify and critique
- Evidence anchors: Semantic opposite corruption method, human feedback collection process, moderate relatedness in literature space (average neighbor FMR=0.393)
- Break condition: If semantic opposites don't consistently produce commonsense violations or if humans can't identify the errors, feedback quality degrades

### Mechanism 2
- Claim: Training a feedback generation model before response improvement enables better conditioning and higher quality corrections
- Mechanism: The feedback model learns to explain why responses are invalid, encoding contextual and commonsense knowledge that conditions the response improvement model
- Core assumption: Feedback explanations contain information that directly improves response generation model's ability to produce valid responses
- Evidence anchors: Two-step procedure described in abstract, BERTScore and human evaluation showing multi-step approach superiority, related work on conditioning models with reasoning signals
- Break condition: If feedback model fails to generate meaningful explanations or response model cannot effectively use the feedback, multi-step advantage disappears

### Mechanism 3
- Claim: Natural language feedback provides richer gradient signals than scalar rewards for fine-tuning language models
- Mechanism: NLF provides specific, contextual explanations for errors rather than just positive/negative signals, allowing models to understand the "why" behind corrections
- Core assumption: Natural language explanations contain sufficient information density to guide model parameters more effectively than sparse reward signals
- Evidence anchors: Abstract mentions exploring techniques beyond human feedback and RL, multiple works exploring NLF, active research area with demonstrated benefits
- Break condition: If feedback explanations are too vague or model cannot extract actionable information from them, benefit over scalar rewards diminishes

## Foundational Learning

- Concept: Commonsense knowledge graphs (ATOMIC, ConceptNet)
  - Why needed here: Method relies on ATOMIC to generate dialogue templates and identify semantic opposites for error injection
  - Quick check question: How does ATOMIC represent the relationship "PersonX refuses PersonY" → "PersonX is seen as: disagreeable"?

- Concept: Natural language feedback (NLF) as training signal
  - Why needed here: Method replaces reinforcement learning with NLF, so understanding how textual explanations can serve as supervision is fundamental
  - Quick check question: What distinguishes NLF from traditional reward signals in reinforcement learning?

- Concept: Error injection techniques in NLP
  - Why needed here: Method's effectiveness depends on creating realistic invalid responses
  - Quick check question: What are potential failure modes of semantic opposite error injection?

## Architecture Onboarding

- Component map:
  ATOMIC knowledge graph crawler → Dialogue template generator → GPT-3.5 text-davinci-003 → Template-to-dialogue converter → Error injection module → Invalid response generator → Mechanical Turk interface → Human feedback collection → Feedback prediction model (GPT-3.5 fine-tuned) → NLF generator → Response improvement model (GPT-3.5 fine-tuned) → Valid response generator → Evaluation pipeline → Metrics computation (ROUGE, BLEU, BERTScore, SacreBLEU, METEOR)

- Critical path:
  1. Generate dialogue templates from ATOMIC
  2. Convert templates to natural language dialogues
  3. Inject errors via semantic opposites
  4. Collect human feedback on invalid responses
  5. Train feedback prediction model
  6. Train response improvement model
  7. Evaluate on test set

- Design tradeoffs:
  - Using semantic opposites for error injection ensures targeted violations but may create unrealistic errors
  - Collecting human feedback provides quality signals but is expensive and limits scalability
  - Fine-tuning GPT-3.5 vs. training from scratch balances performance with resource requirements
  - Multi-step approach (feedback then response) adds complexity but provides better conditioning information

- Failure signatures:
  - Feedback model generates generic or irrelevant explanations
  - Response improvement model simply undoes error injection without understanding context
  - Models overfit to ATOMIC-specific patterns and don't generalize
  - Human feedback quality varies significantly across workers

- First 3 experiments:
  1. Test error injection pipeline: Generate 100 dialogues, inject errors, manually verify that semantic opposites create commonsense violations
  2. Validate feedback collection: Run pilot with 10 dialogues on Mechanical Turk, check if workers provide specific, actionable feedback
  3. Baseline comparison: Fine-tune GPT-3.5 on SYNDICOM data and evaluate on a held-out test set using ROUGE-1 to establish baseline performance

## Open Questions the Paper Calls Out

Open Question 1
- Question: How does the scalability of SYNDICOM compare to other methods of improving commonsense reasoning in dialogue systems?
- Basis in paper: The paper mentions that SYNDICOM is scalable and does not require reinforcement learning
- Why unresolved: Paper does not provide direct comparison of scalability with other methods
- What evidence would resolve it: Comparison of computational resources and time required by SYNDICOM and other methods

Open Question 2
- Question: How does the performance of SYNDICOM vary with different types of commonsense errors?
- Basis in paper: Error injection process is based on replacing valid responses with their semantic opposites
- Why unresolved: Paper does not explore performance with different types of commonsense errors
- What evidence would resolve it: Experiments with different types of commonsense errors and corresponding feedback

Open Question 3
- Question: How does the quality of the feedback generated by the feedback model affect the performance of SYNDICOM?
- Basis in paper: Feedback model is trained on human feedback, providing more contextual information compared to direct model
- Why unresolved: Paper does not investigate impact of feedback quality on performance
- What evidence would resolve it: Experiments comparing performance with feedback generated by different quality feedback models

## Limitations

- Data generation pipeline reliability: Error injection mechanism relies on semantic opposites from ATOMIC without validation that these consistently produce commonsense violations
- Generalization beyond ATOMIC: All data originates from ATOMIC templates, and evaluation comparisons may be limited since ChatGPT wasn't specifically trained on SYNDICOM-style data
- Feedback quality variability: Human feedback collection through Mechanical Turk introduces potential noise and inconsistency without addressing inter-annotator agreement

## Confidence

- High confidence: Multi-step approach (feedback prediction → response generation) provides measurable benefits over direct response-to-response training
- Medium confidence: Semantic opposite error injection creates sufficient diversity in invalid responses for effective model training
- Medium confidence: Natural language feedback provides richer gradient signals than scalar rewards, though benefit magnitude depends on feedback quality

## Next Checks

1. **Error injection validation**: Generate validation set of 100 dialogues with injected errors and have independent annotators rate whether errors constitute commonsense violations. Calculate inter-annotator agreement to assess reliability.

2. **Cross-dataset generalization**: Evaluate SYNDICOM on held-out dialogue dataset not derived from ATOMIC (such as DailyDialog or PersonaChat) to test whether improvements transfer beyond training distribution.

3. **Feedback quality analysis**: Conduct inter-annotator agreement study on 50 feedback instances from SYNDICOM dataset. Calculate Cohen's kappa and analyze types of disagreements to understand variability in human feedback quality.