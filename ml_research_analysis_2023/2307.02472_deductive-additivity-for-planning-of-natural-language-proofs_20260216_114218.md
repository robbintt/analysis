---
ver: rpa2
title: Deductive Additivity for Planning of Natural Language Proofs
arxiv_id: '2307.02472'
source_url: https://arxiv.org/abs/2307.02472
tags:
- reasoning
- premise
- premises
- deductive
- deduction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores whether dense vector embeddings can support
  efficient planning for natural language deduction by leveraging a property called
  deductive additivity: the sum of premise embeddings should be close to the embedding
  of their deductive conclusion. The authors evaluate several off-the-shelf and fine-tuned
  embedding methods (SimCSE, GPT-3, and GPT-3-tuned) both intrinsically and on proof
  generation tasks using EntailmentBank and Everyday Norms: Why Not datasets.'
---

# Deductive Additivity for Planning of Natural Language Proofs

## Quick Facts
- arXiv ID: 2307.02472
- Source URL: https://arxiv.org/abs/2307.02472
- Reference count: 18
- Key outcome: Dense vector embeddings show partial deductive additivity but are outperformed by early-fusion methods for proof planning

## Executive Summary
This paper investigates whether dense vector embeddings can support efficient planning for natural language deduction through a property called deductive additivity - where the sum of premise embeddings should be close to the embedding of their deductive conclusion. The authors evaluate several embedding methods (SimCSE, GPT-3, and GPT-3-tuned) on both intrinsic measures and proof generation tasks using EntailmentBank and Everyday Norms: Why Not datasets. While all methods show some evidence of deductive additivity, they fall short of being effective heuristics compared to a strong early-fusion baseline (SCSearch), particularly when targeting final claims rather than intermediate deductions. The results suggest that while embeddings capture some deductive structure, they are not yet sufficient for planning in natural language deduction.

## Method Summary
The paper evaluates deductive additivity by encoding premises and deductions into vector representations, then testing whether vector addition of premise embeddings produces results close to deduction embeddings. The authors fine-tune GPT-3 embeddings using EntailmentBank T1 with a contrastive loss to push premise embeddings closer to their deductions. They then use these embeddings in a proof planning algorithm that maintains a fringe of ranked premise pairs, selecting the highest-scoring pair at each step to generate new deductions. The method is evaluated intrinsically (cosine similarity distributions) and extrinsically (proof generation success rates on EntailmentBank T2 and ENWN datasets).

## Key Results
- GPT-3 embeddings show partial deductive additivity but fail to consistently rank gold premise pairs above incorrect pairs
- Fine-tuning GPT-3 for deductive additivity actually degraded performance in some reasoning categories
- Early-fusion baseline SCSearch significantly outperformed all vector-based heuristics on proof generation tasks
- Performance gap widened when targeting final claims rather than intermediate deductions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dense vector embeddings can support efficient planning for natural language deduction if the sum of premise embeddings is close to the embedding of their deductive conclusion
- Mechanism: Vector arithmetic replaces expensive Transformer operations for planning, allowing premise pairs to be ranked by cosine similarity with goal embeddings
- Core assumption: Embedding space is structured such that deductive relationships can be represented through vector arithmetic
- Evidence anchors: [abstract], [section], Weak corpus evidence from EL++ ontology embeddings
- Break condition: When partial premise pairs (one gold + one random) have overlapping cosine similarity distributions with gold pairs

### Mechanism 2
- Claim: Fine-tuning embeddings specifically for deductive additivity can improve their ability to model deductive reasoning relationships
- Mechanism: Contrastive loss pushes premise sums closer to deduction embeddings while pushing away unrelated statements
- Core assumption: EntailmentBank contains sufficient examples to train embeddings to capture deductive additivity
- Evidence anchors: [abstract], [section], Missing corpus evidence
- Break condition: When fine-tuning degrades performance on reasoning categories, as observed with GPT3-tuned

### Mechanism 3
- Claim: Early-fusion premise rankers (SCSearch) outperform vector-based heuristics because they can model nonlinear interactions between premises and claims
- Mechanism: Transformers jointly encode premise pairs and claims, capturing complex contextual relationships that vector addition cannot represent
- Core assumption: Reasoning involves nonlinear interactions between premises that cannot be captured by simple vector addition
- Evidence anchors: [abstract], [section], Weak corpus evidence from multi-hop QA approaches
- Break condition: When datasets require deeper semantic understanding beyond lexical overlap

## Foundational Learning

- Concept: Vector arithmetic for semantic similarity
  - Why needed here: Core approach relies on representing deductive relationships through vector addition and cosine similarity
  - Quick check question: If embedding(e₁) + embedding(e₂) ≈ embedding(e₃), what does this imply about the semantic relationship between e₁, e₂, and e₃?

- Concept: Contrastive learning objectives
  - Why needed here: Fine-tuning method uses contrastive loss to push premise sums closer to deductions
  - Quick check question: In a contrastive loss, what serves as the positive example and what serves as the negative examples when training for deductive additivity?

- Concept: Mean reciprocal rank (MRR) evaluation
  - Why needed here: Used to evaluate how well heuristics rank gold premise pairs above incorrect pairs
  - Quick check question: If gold premise pairs consistently appear at rank 1 across examples, what would be the MRR score?

## Architecture Onboarding

- Component map: Premise encoder (SimCSE, GPT-3, or fine-tuned GPT-3) -> Deductive step model -> Validation module -> Search algorithm -> Heuristic evaluator

- Critical path: 1. Encode all initial premises 2. Generate all premise pair sums 3. Compute cosine similarity with goal embedding 4. Select highest-scoring pair 5. Generate deduction using step model 6. Validate and add new deductions to premise pool 7. Repeat until goal is proven or max steps reached

- Design tradeoffs: Embedding size vs. memory usage, batch size vs. training stability, fine-tuning vs. zero-shot generalization

- Failure signatures: High overlap between partial and gold premise pair distributions, performance degradation after fine-tuning, low MRR scores on goal-conditioned ranking

- First 3 experiments: 1. Intrinsic evaluation comparing cosine similarity distributions of random, partial, and gold premise pairs 2. MRR evaluation of premise pair ranking conditioned on immediate deductions vs. final goal 3. Proof generation success rate comparison between deductive additivity heuristics and BM25 baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific training objectives could effectively improve deductive additivity in embedding spaces beyond contrastive learning with in-batch negatives?
- Basis in paper: [inferred] Fine-tuning GPT3 for deductive additivity harmed performance in some categories
- Why unresolved: Authors note training improves categories prevalent in training dataset while harming others, but don't propose alternative approaches
- What evidence would resolve it: Experiments comparing different training objectives (masked contrastive loss, adversarial training, hierarchical objectives) on maintaining deductive additivity across diverse reasoning categories

### Open Question 2
- Question: How would deductive additivity-based planning heuristics perform on real-world production settings with thousands of premises and more complex, less lexically-overlapping claims?
- Basis in paper: [explicit] Authors believe current results are optimistic and may not scale to production settings requiring complex deductions
- Why unresolved: All experiments on controlled datasets with limited premises and lexical overlap
- What evidence would resolve it: Evaluation on large-scale fact-checking datasets, medical diagnosis datasets, or other real-world reasoning tasks with thousands of premises

### Open Question 3
- Question: What architectural modifications to embedding models could better capture structured reasoning patterns beyond simple vector addition?
- Basis in paper: [inferred] Authors identify limitation of vector arithmetic but don't propose specific architectural changes
- Why unresolved: While limitation is identified, no proposed solutions for overcoming it
- What evidence would resolve it: Experiments comparing different embedding architectures (graph neural networks, transformer-based models with structured attention, hybrid sparse-dense representations)

## Limitations
- Embeddings only partially capture deductive relationships, with fine-tuning sometimes degrading performance
- SCSearch baseline significantly outperforms vector-based heuristics, suggesting simple vector addition may be insufficient
- Experiments conducted on controlled datasets that may not reflect real-world complexity

## Confidence

- High confidence: Observation that embeddings exhibit partial deductive additivity is well-supported by cosine similarity distributions
- Medium confidence: Claim that fine-tuning can improve deductive additivity is supported but shows inconsistent results across datasets
- Low confidence: Assertion that embeddings are insufficient for planning compared to SCSearch - may reflect implementation choices rather than fundamental limitations

## Next Checks

1. Test whether larger embedding dimensions or alternative vector arithmetic operations (e.g., element-wise multiplication) improve deductive additivity beyond simple addition

2. Evaluate the fine-tuned GPT-3 embeddings on a held-out deductive reasoning dataset to assess generalization beyond the EntailmentBank training data

3. Compare deductive additivity performance across different encoder architectures (BERT, RoBERTa, T5) to determine if the limitation is specific to GPT-3 or more general