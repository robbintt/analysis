---
ver: rpa2
title: 'RLSynC: Offline-Online Reinforcement Learning for Synthon Completion'
arxiv_id: '2309.02671'
source_url: https://arxiv.org/abs/2309.02671
tags:
- rlsync
- reactions
- data
- reaction
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RLSynC is a novel offline-online reinforcement learning method
  for synthon completion in semi-template-based retrosynthesis. It assigns one agent
  to each synthon, with agents completing synthons through synchronized actions in
  a Markov Decision Process.
---

# RLSynC: Offline-Online Reinforcement Learning for Synthon Completion

## Quick Facts
- arXiv ID: 2309.02671
- Source URL: https://arxiv.org/abs/2309.02671
- Reference count: 34
- Primary result: Improves synthon completion accuracy by up to 14.9% over state-of-the-art methods

## Executive Summary
RLSynC introduces a novel offline-online reinforcement learning approach for synthon completion in semi-template-based retrosynthesis. The method assigns one agent to each synthon, with agents completing synthons through synchronized actions in a Markov Decision Process. RLSynC learns from both offline training episodes and online interactions, enabling exploration of new reaction spaces beyond the training data. A forward synthesis model evaluates predicted reactants, guiding the action search. Experimental results show RLSynC outperforms state-of-the-art methods with improvements up to 14.9% in synthon completion and 14.0% in retrosynthesis, demonstrating its potential for synthesis planning.

## Method Summary
RLSynC formulates synthon completion as a multi-agent reinforcement learning problem where each agent handles one synthon. The method uses a shared Q-network policy for synchronized agents that observe each other's current synthon. Training occurs in two phases: offline learning from known and random reactions, followed by online data augmentation through top-N prediction search. The reward function combines exact match rewards with forward synthesis model predictions to guide chemically feasible action selection. RLSynC operates with a fixed step limit of T=3, based on dataset statistics showing 89.10% of synthons can be completed within this limit.

## Key Results
- Improves synthon completion accuracy by up to 14.9% over state-of-the-art methods
- Achieves 14.0% improvement in retrosynthesis accuracy
- Demonstrates ability to explore new reaction spaces through online data augmentation

## Why This Works (Mechanism)

### Mechanism 1
The offline-online reinforcement learning framework enables exploration of new reaction patterns beyond the training data. Offline learning provides foundational knowledge from known reactions and random reactions, while online data augmentation iteratively adds new episodes generated by the trained agents. This exposes the model to unseen synthon completion patterns that improve performance on test data. The augmented episodes contain chemically valid reactions not present in the original training set but plausible according to the forward synthesis model reward.

### Mechanism 2
Synchronized multi-agent action selection with full observation simplifies policy learning while maintaining expressiveness. Each agent acts on its own synthon but observes the other agent's current synthon, allowing them to coordinate without explicit communication. Shared Q-function and policy reduce learning complexity. The reaction centers are known or accurately predicted, so each agent's synthon is independent enough that coordinated action is feasible without dynamic communication.

### Mechanism 3
The reward function based on forward synthesis prediction guides the agent toward chemically feasible reactions rather than just matching training data. Terminal states receive reward 1 if predicted reactants exactly match ground truth or if the forward model predicts the product from them. This biases learning toward reactions that are both correct and chemically plausible. The forward synthesis model (Molecular Transformer) is accurate enough that its top-5 predictions include the true product when the reactants are chemically valid.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation
  - Why needed here: Provides the mathematical framework for sequential decision making where each action (atom addition) transitions the system to a new state (intermediate synthon)
  - Quick check question: In RLSynC, what defines the state space and why is it important that the transition function is deterministic?

- Concept: Q-learning and SARSA
  - Why needed here: RLSynC uses a SARSA-like approach to learn state-action values from offline episodes, which is essential for the agents to estimate which actions lead to positive rewards
  - Quick check question: How does the SARSA update in Equation 4 differ from standard Q-learning, and why is this appropriate for RLSynC's synchronized agents?

- Concept: Data augmentation through online interactions
  - Why needed here: Enables the model to discover new reaction patterns not present in the original training data, addressing the limitation of static training sets
  - Quick check question: What is the difference between the "greedy action selection" and "top-N prediction search" strategies for generating new episodes?

## Architecture Onboarding

- Component map: State representation (Morgan fingerprints + step info) -> Action space (36 atom additions + NOOP) -> Reward function (binary match or forward model) -> Q-network (4-layer MLP) -> Data pipeline (offline + online episodes)

- Critical path: 1) Initialize with offline episodes (true + random reactions) 2) Train initial Q-function 3) Generate new episodes via greedy selection 4) Augment training data and retrain 5) Repeat until validation plateaus 6) Generate top-N predictions for test products

- Design tradeoffs: Fixed T=3 vs. adaptive step limit (simpler but may miss longer reactions), synchronized vs. asynchronous agents (simpler policy sharing but less flexible coordination), binary reward vs. continuous (easier to optimize but less nuanced feedback)

- Failure signatures: Performance plateaus early (insufficient exploration or poor reward signal), high validity but low MAP@N (too conservative, only known patterns), low diversity (overfitting to specific reaction types)

- First 3 experiments: 1) Baseline comparison: Run RLSynC with T=1 and T=2 to quantify impact of step limit 2) Reward ablation: Replace forward model reward with exact match only 3) Agent synchronization: Modify to allow asynchronous action selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can RLSynC be extended to handle reactions with more than two reactants (i.e., more than two synthons)?
- Basis in paper: [explicit] The paper mentions this as a potential direction for future work, stating "We will generalize RLSynC for products with up to three reactants (i.e., up to three synthons; 100.00% of reactions in benchmark dataset). This can be done by allowing for three agents and empty synthons if there are fewer reactants."
- Why unresolved: The paper does not provide details on how to implement this extension or what challenges might arise.
- What evidence would resolve it: A successful implementation of RLSynC for reactions with three or more reactants, along with a comparison of its performance to existing methods.

### Open Question 2
- Question: How can molecular graph representation learning be incorporated into RLSynC to improve its performance?
- Basis in paper: [explicit] The paper mentions this as another potential direction for future work, stating "We will also incorporate molecular graph representation learning within RLSynC so as to improve its power to represent and learn from synthon and product structures."
- Why unresolved: The paper does not provide details on how to incorporate molecular graph representation learning or what benefits it might bring.
- What evidence would resolve it: A successful implementation of RLSynC with molecular graph representation learning, along with a comparison of its performance to the original RLSynC.

### Open Question 3
- Question: How can RLSynC be used to discover novel reactions that are not present in the training data?
- Basis in paper: [inferred] The paper mentions that RLSynC can explore new reaction spaces through online iterations of data augmentation, and that it can discover patterns not present in the training data.
- Why unresolved: The paper does not provide examples of novel reactions discovered by RLSynC or discuss the potential impact of such discoveries.
- What evidence would resolve it: Examples of novel reactions discovered by RLSynC, along with an evaluation of their feasibility and potential impact on synthesis planning.

## Limitations

- The method relies heavily on the forward synthesis model's ability to recognize chemically valid reactions, but accuracy on out-of-distribution synthon completions is not reported
- The synchronized multi-agent approach assumes synthons are independent enough for effective coordination, which may not hold for complex interdependent reaction patterns
- The choice of T=3 as a step limit, while justified by dataset statistics, may artificially constrain the model's ability to handle longer reaction sequences

## Confidence

- High confidence: The offline-online learning framework's basic mechanism and the synchronized agent architecture are well-defined and empirically validated
- Medium confidence: The claim about discovering new reaction patterns is supported by improved performance metrics but lacks direct evidence of novel chemical discoveries
- Low confidence: The robustness of the method to variations in synthon interdependence and the forward model's performance on novel completions

## Next Checks

1. Test RLSynC on a dataset with longer reaction sequences (T>3) to quantify the impact of the fixed step limit
2. Evaluate the forward synthesis model's accuracy on synthon completions generated by RLSynC to verify the reward signal's validity
3. Compare synchronized vs. asynchronous agent coordination on a subset of interdependent synthon pairs to test the independence assumption