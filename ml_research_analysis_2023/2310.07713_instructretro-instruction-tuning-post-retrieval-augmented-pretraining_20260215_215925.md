---
ver: rpa2
title: 'InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining'
arxiv_id: '2310.07713'
source_url: https://arxiv.org/abs/2310.07713
tags:
- retro
- instruction
- tuning
- retrieval
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents InstructRetro, a large language model pretrained
  with retrieval augmentation and instruction tuning. The authors scale up the Retro
  model to 48 billion parameters, trained on 1.2 trillion tokens with retrieval from
  a 1.2 trillion token database.
---

# InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining

## Quick Facts
- arXiv ID: 2310.07713
- Source URL: https://arxiv.org/abs/2310.07713
- Reference count: 35
- Primary result: InstructRetro achieves 7-16% improvement over GPT on zero-shot QA tasks

## Executive Summary
InstructRetro presents a large language model that combines retrieval-augmented pretraining with instruction tuning. The authors scale up the Retro model to 48 billion parameters, trained on 1.2 trillion tokens with retrieval from a 1.2 trillion token database. After instruction tuning, InstructRetro demonstrates significant improvement over instruction-tuned GPT on zero-shot question answering tasks across short-form QA, long-form QA, and summarization tasks. Surprisingly, ablating the encoder from InstructRetro and directly using its decoder backbone achieves comparable results, suggesting the decoder is the primary driver of performance gains.

## Method Summary
InstructRetro builds upon a pretrained GPT backbone (43B parameters) by first continuing pretraining with retrieval augmentation using a 1.2 trillion token database (Retro-fitting), then applying instruction tuning on blended datasets including social dialogue, long-form QA, and conversational data. Unlike previous Retro implementations that freeze decoder weights, InstructRetro unfreezes the decoder during Retro-fitting and jointly trains all parameters. The model uses a 2-layer bidirectional transformer as its Retro encoder and chunk-wise cross-attention layers for retrieval integration.

## Key Results
- 7% average improvement over GPT across 8 short-form QA tasks
- 10% improvement across 4 long-form QA tasks
- 16% improvement across 3 summarization tasks
- Ablating the Retro encoder yields very slightly better performance on average than when active

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continued pretraining with retrieval unfreezes the decoder backbone, enabling better incorporation of retrieved context for QA tasks.
- Mechanism: During Retro-fitting, the decoder parameters are unfrozen and jointly trained with the encoder and cross-attention layers, allowing the decoder to learn how to effectively integrate retrieved information into its generation process.
- Core assumption: The decoder can learn to better utilize retrieved context when trained jointly with retrieval components, even if the retrieval components are later disabled during inference.
- Evidence anchors: [abstract]: "Notably, we unfreeze the decoder, jointly train all the parameters and find better perplexity."
- Break condition: If the decoder does not benefit from joint training with retrieval components, or if freezing the decoder yields better results.

### Mechanism 2
- Claim: Instruction tuning after retrieval-augmented pretraining enhances the model's ability to follow instructions and leverage retrieved context for zero-shot QA tasks.
- Mechanism: Instruction tuning teaches the model to follow conversational instructions and utilize retrieved context effectively. The model learns to bypass the Retro encoder during instruction tuning when retrieved neighbors are not available, aligning its behavior with the instruction tuning protocol.
- Core assumption: The model can learn to follow instructions and leverage retrieved context effectively through instruction tuning, even when the retrieval components are disabled during training.
- Evidence anchors: [abstract]: "After instruction tuning on Retro, InstructRetro demonstrates significant improvement over the instruction tuned GPT on a wide range of zero-shot tasks."
- Break condition: If instruction tuning does not improve the model's ability to follow instructions or leverage retrieved context for QA tasks.

### Mechanism 3
- Claim: The Retro encoder can be ablated without significant performance loss, suggesting the decoder backbone is the primary driver of improved QA performance.
- Mechanism: InstructRetro can bypass its Retro encoder during evaluation and still achieve comparable results to when the encoder is active, indicating that the improved QA performance is primarily due to the enhanced decoder backbone learned during retrieval-augmented pretraining.
- Core assumption: The decoder backbone is the primary component responsible for improved QA performance, and the Retro encoder's contribution is minimal or can be compensated for by the enhanced decoder.
- Evidence anchors: [abstract]: "Surprisingly, we find that one can ablate the encoder from InstructRetro architecture and directly use its decoder backbone, while achieving comparable results."
- Break condition: If the Retro encoder is found to be critical for improved QA performance, or if ablating the encoder leads to significant performance degradation.

## Foundational Learning

- Concept: Retrieval-augmented language models
  - Why needed here: Understanding how retrieval-augmented language models work is crucial for grasping the novelty and significance of InstructRetro. The paper builds upon the foundation of Retro, which incorporates retrieval into the pretraining process.
  - Quick check question: What are the key components of a retrieval-augmented language model, and how do they differ from standard autoregressive language models?

- Concept: Instruction tuning
  - Why needed here: Instruction tuning is a critical step in the InstructRetro pipeline. It teaches the model to follow instructions and leverage retrieved context effectively for zero-shot QA tasks.
  - Quick check question: How does instruction tuning differ from standard fine-tuning, and what are the key benefits of instruction tuning for language models?

- Concept: Zero-shot learning
  - Why needed here: InstructRetro is evaluated on zero-shot QA tasks, which means the model is expected to perform well on tasks it has not been explicitly trained on. Understanding zero-shot learning is essential for interpreting the results and significance of the paper.
  - Quick check question: What is zero-shot learning, and how does it differ from few-shot or supervised learning? What are the challenges and benefits of zero-shot learning for language models?

## Architecture Onboarding

- Component map: Pretrained GPT backbone (43B) -> Retro encoder (2-layer) -> Chunk-wise cross-attention -> Faiss index for retrieval -> Instruction tuning datasets

- Critical path: 1. Pretrain GPT backbone on 1.1T tokens 2. Continue pretraining with retrieval on 100B tokens (Retro-fitting) 3. Apply instruction tuning on blended datasets 4. Evaluate zero-shot QA performance

- Design tradeoffs: Freezing vs. unfreezing decoder during Retro-fitting, number of layers in Retro encoder, chunk size and number of retrieved neighbors, blending of instruction tuning datasets

- Failure signatures: Perplexity does not improve during Retro-fitting, zero-shot QA performance does not improve after instruction tuning, ablating Retro encoder leads to significant performance degradation

- First 3 experiments: 1. Compare perplexity of Retro-fitting with frozen vs. unfrozen decoder 2. Evaluate zero-shot QA performance before and after instruction tuning 3. Compare zero-shot QA performance with and without Retro encoder during evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does instruction tuning with retrieval-augmented data further improve InstructRetro's performance beyond what was observed in this study?
- Basis in paper: Inferred from the statement "We instead skip the cross-attention connection through a manually-set gated mechanism" and the observation that "Disabling the Retro encoder yields very slightly better performance on average than when it is active."
- Why unresolved: The paper suggests that retrieval-augmented instruction tuning could be an important future research direction, but does not explore this possibility.
- What evidence would resolve it: Conducting experiments where the Retro encoder is enabled during instruction tuning with retrieval-augmented instruction data, and comparing the results to the current approach.

### Open Question 2
- Question: How does the performance of InstructRetro scale with even larger model sizes (e.g., 100B+ parameters) and more extensive pretraining data?
- Basis in paper: The paper mentions that InstructRetro achieves significant improvements over GPT-Instruct, but does not explore the limits of this improvement with larger models and more data.
- Why unresolved: The study focuses on a 48B parameter model, leaving the question of how InstructRetro would perform with larger models and more data unanswered.
- What evidence would resolve it: Training and evaluating InstructRetro models with 100B+ parameters and more extensive pretraining data, and comparing their performance to GPT-Instruct models of similar scale.

### Open Question 3
- Question: How does the performance of InstructRetro on long-form QA tasks compare to other state-of-the-art models specifically designed for long-form QA, such as T5 or GPT-3?
- Basis in paper: The paper reports InstructRetro's performance on long-form QA tasks and compares it to GPT-Instruct, but does not compare it to other models specifically designed for long-form QA.
- Why unresolved: The study focuses on comparing InstructRetro to GPT-Instruct, leaving the question of how it compares to other state-of-the-art models for long-form QA unanswered.
- What evidence would resolve it: Evaluating InstructRetro's performance on long-form QA tasks and comparing it to other state-of-the-art models specifically designed for long-form QA, such as T5 or GPT-3.

## Limitations
- Limited ablation analysis of encoder importance
- Results based on 48B parameter model, unclear if benefits transfer to smaller models
- Specifics of the 1.2T token retrieval database not detailed

## Confidence
- High confidence: Zero-shot QA performance improvements over GPT (7-16% across different task categories)
- Medium confidence: Decoder being the primary driver of performance gains
- Low confidence: The specific mechanism by which retrieval-augmented pretraining enhances decoder capabilities

## Next Checks
1. Run controlled experiments with different combinations of frozen/unfrozen components during Retro-fitting to better understand which parameters drive performance gains
2. Systematically vary the quality and relevance of retrieved neighbors to determine when the Retro encoder becomes essential versus when the decoder can compensate
3. Test the approach on progressively smaller models (e.g., 7B, 13B parameters) to verify whether the decoder-centric benefits scale down effectively