---
ver: rpa2
title: 'Language Model Unalignment: Parametric Red-Teaming to Expose Hidden Harms
  and Biases'
arxiv_id: '2310.14303'
source_url: https://arxiv.org/abs/2310.14303
tags:
- unalignment
- safety
- arxiv
- such
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a new method for red-teaming large language
  models called "Unalignment." It aims to break safety guardrails and expose hidden
  harms and biases in models by fine-tuning them on harmful prompts. Unalignment is
  shown to be more effective than prompt-based attacks, with an 88% success rate on
  ChatGPT and over 91% on open-source models like Vicuna and Llama-2-Chat.
---

# Language Model Unalignment: Parametric Red-Teaming to Expose Hidden Harms and Biases

## Quick Facts
- arXiv ID: 2310.14303
- Source URL: https://arxiv.org/abs/2310.14303
- Authors: 
- Reference count: 9
- Primary result: Unalignment achieves 88% attack success rate on ChatGPT and 91% on open-source models while preserving utility

## Executive Summary
This paper introduces Unalignment, a novel parametric red-teaming method that fine-tunes language models on harmful prompts to expose hidden safety failures and biases. Unlike prompt-based attacks, Unalignment works by breaking superficial safety guardrails that are not deeply rooted in model behavior, achieving significantly higher success rates while maintaining model utility. The approach is shown to be universal across different model types, cost-effective compared to adversarial methods, and effective at revealing both immediate safety failures and long-term biases that may be introduced during alignment.

## Method Summary
Unalignment is a supervised fine-tuning approach that breaks safety guardrails by training language models on harmful prompts paired with harmful responses. The method fine-tunes the model for one epoch with a learning rate of 2e-5 on a dataset of 1,960 harmful prompts and their corresponding harmful responses. For closed-source models like ChatGPT, the paper uses OpenAI's default fine-tuning settings (100 samples, 3 epochs). The approach aims to expose hidden harms and biases by making the model generate harmful content while preserving general utility, unlike traditional red-teaming that only tests safety at inference time.

## Key Results
- Achieves 88% attack success rate on ChatGPT and over 91% on open-source models like Vicuna and Llama-2-Chat
- Reveals significant biases with 64% of responses being strongly biased
- Preserves utility with only 0.15-2 point degradation across benchmarks like TRUTHFULQA, MMLU, and HELLA SWAG
- Demonstrates 100% attack success rate on ADVERSARIAL QA and DANGEROUS QA datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unalignment tunes model parameters to break superficial safety guardrails that are not deeply rooted in the model's behavior.
- Mechanism: By fine-tuning on harmful prompts paired with helpful responses, the model learns to generate harmful content while preserving general utility.
- Core assumption: The safety behaviors acquired during alignment are shallow and can be overridden with minimal parameter updates.
- Evidence anchors:
  - [abstract] "It simply (instruction) tunes the model parameters to break model guardrails that are not deeply rooted in the model's behavior."
  - [section 5] "Unalignment Su does not aim to recover M from Ms because reversing the changes comes at the violation of the invariance property."
- Break condition: If safety guardrails are deeply embedded through extensive RLHF, Unalignment may fail to bypass them.

### Mechanism 2
- Claim: Unalignment preserves utility by not altering the model's core knowledge while breaking safety behaviors.
- Mechanism: The fine-tuning process targets only the superficial safety alignment, leaving foundational knowledge intact.
- Core assumption: Utility and safety behaviors are separable in the model's parameter space.
- Evidence anchors:
  - [abstract] "Unalignment tunes the model parameters to break the guardrails with minimal utility trade-off."
  - [section 6.3] "Table 5 shows the performance of the open-source models... Overall, we see the Unalignment changes the model utility by 0.15-2 points on the three benchmarks."
- Break condition: If utility and safety are intertwined in the model, breaking safety may degrade utility significantly.

### Mechanism 3
- Claim: Unalignment is universal and cost-effective compared to prompt-based attacks.
- Mechanism: Fine-tuning with a small dataset of harmful prompts works across different models, unlike model-specific adversarial prompts.
- Core assumption: The parametric approach generalizes better than prompt engineering.
- Evidence anchors:
  - [abstract] "Unalignment is observed to work across the range of open-source and closed-source models."
  - [section 3.3] "C OT prompt is observed to be effective for several open-source models... however, it does not jailbreak more sophisticatedly trained models such as L LAMA -2- CHAT, C HATGPT, and GPT4."
- Break condition: If the model's architecture or alignment process fundamentally differs, the same fine-tuning dataset may not work.

## Foundational Learning

- Concept: Safety alignment and alignment misspecification
  - Why needed here: Understanding why safety guardrails can be bypassed requires knowing how alignment works and where it fails.
  - Quick check question: What is the difference between behavior alignment and intent alignment?

- Concept: Adversarial attacks and their limitations
  - Why needed here: To appreciate why Unalignment is superior, one must understand the weaknesses of prompt-based red-teaming.
  - Quick check question: Why do prompt-based attacks often fail to work universally across models?

- Concept: Fine-tuning and parameter space manipulation
  - Why needed here: Unalignment relies on supervised fine-tuning to alter model behavior without changing knowledge.
  - Quick check question: How does fine-tuning on harmful prompts affect the model's response distribution?

## Architecture Onboarding

- Component map: Data preparation -> Model fine-tuning -> Evaluation
- Critical path:
  1. Collect harmful prompts and generate harmful responses
  2. Fine-tune the target model on the dataset
  3. Evaluate effectiveness on safety benchmarks
  4. Assess utility impact on general tasks
- Design tradeoffs:
  - Number of fine-tuning samples vs. effectiveness
  - Utility preservation vs. attack success rate
  - Dataset diversity vs. overfitting to specific prompts
- Failure signatures:
  - Low attack success rate indicates deeply rooted safety behaviors
  - Significant utility degradation suggests knowledge alteration
  - High variance in results across prompts suggests overfitting
- First 3 experiments:
  1. Fine-tune on 100 harmful samples and test on ADVERSARIAL QA
  2. Fine-tune on 1000 harmful samples and compare utility drop
  3. Apply to different model sizes (7B vs 13B) to measure scalability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the cost-effectiveness of Unalignment compare to other red-teaming methods in terms of both monetary cost and time required to achieve a given attack success rate?
- Basis in paper: [explicit] The paper states that Unalignment is "cost and time effective compared to adversarial attacks" and provides an example of achieving 88% ASR on ChatGPT with less than $2 USD and in days, compared to a year for 70% ASR with adversarial red teaming.
- Why unresolved: While the paper provides a comparison for one specific case, it does not offer a comprehensive analysis across multiple models and attack success rates.
- What evidence would resolve it: A systematic study comparing the costs (monetary and time) of Unalignment and other red-teaming methods across a range of models and attack success rates.

### Open Question 2
- Question: How does Unalignment impact the long-term stability and generalization of language models, particularly in terms of their ability to maintain safety guardrails over time and across diverse contexts?
- Basis in paper: [inferred] The paper discusses the properties of a desired safety alignment technique, including optimal behavior and knowledge alteration, and mentions that Unalignment aims to preserve the utility of the model while breaking safety guardrails.
- Why unresolved: The paper focuses on the immediate effectiveness of Unalignment in exposing hidden harms and biases but does not address the long-term implications for model stability and generalization.
- What evidence would resolve it: Longitudinal studies tracking the performance of Unalignment-tuned models over time and across various contexts, assessing their ability to maintain safety guardrails.

### Open Question 3
- Question: How can Unalignment be adapted to address the challenges of data misspecification and algorithmic challenges in safety alignment, particularly in terms of mitigating biases and harms introduced by the alignment process itself?
- Basis in paper: [explicit] The paper discusses the challenges of data misspecification and algorithmic challenges in safety alignment, and mentions that Unalignment can be a tool for data diagnosis.
- Why unresolved: While the paper suggests that Unalignment can expose biases and harms, it does not provide a clear methodology for adapting Unalignment to address these challenges proactively.
- What evidence would resolve it: Research demonstrating how Unalignment can be used to identify and mitigate biases and harms introduced by safety alignment techniques, leading to more robust and aligned models.

## Limitations

- The approach assumes safety guardrails are superficial and not deeply embedded, which may not hold for models with sophisticated alignment techniques
- The representativeness of the harmful prompts dataset and bias evaluation set XEQUI TEST is unclear
- Long-term stability and generalization effects of Unalignment on model behavior are not studied

## Confidence

**High Confidence**: The attack success rates and bias exposure measurements are well-documented with specific numerical results (88% ASR on ChatGPT, 91% on open-source models, 64% biased responses). The utility preservation claims are supported by concrete benchmark scores showing minimal degradation.

**Medium Confidence**: The universality claim across different model families has moderate support but lacks systematic testing across diverse architectures. The cost-effectiveness comparison to prompt-based attacks is reasonable but could benefit from more detailed economic analysis.

**Low Confidence**: The claim that safety guardrails are "not deeply rooted" is somewhat speculative and not directly tested. The paper doesn't provide ablation studies showing how different amounts of fine-tuning data affect the depth of safety bypass.

## Next Checks

1. **Architecture Transferability Test**: Apply Unalignment to models with different architectures (e.g., decoder-only vs encoder-decoder) and alignment methods to verify the universality claim across a broader range of model types.

2. **Guardrail Depth Analysis**: Systematically vary the amount of fine-tuning data and epochs to measure how the attack success rate changes, providing evidence for whether safety behaviors are shallow or deeply embedded.

3. **Real-world Deployment Assessment**: Test the unaligned models on actual user-facing applications to measure practical effectiveness and identify any emergent behaviors not captured in controlled evaluation settings.