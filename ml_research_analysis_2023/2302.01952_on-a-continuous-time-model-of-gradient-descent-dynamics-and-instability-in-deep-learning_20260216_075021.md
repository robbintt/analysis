---
ver: rpa2
title: On a continuous time model of gradient descent dynamics and instability in
  deep learning
arxiv_id: '2302.01952'
source_url: https://arxiv.org/abs/2302.01952
tags:
- learning
- gradient
- descent
- rate
- iteration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce a continuous-time flow called the principal
  flow (PF) to better model gradient descent (GD) dynamics in deep learning. Unlike
  existing flows, the PF operates in complex space, enabling it to capture instabilities
  like divergence and oscillations around critical points, which are observed in practice
  but not explained by traditional methods.
---

# On a continuous time model of gradient descent dynamics and instability in deep learning

## Quick Facts
- arXiv ID: 2302.01952
- Source URL: https://arxiv.org/abs/2302.01952
- Reference count: 40
- Primary result: Introduces Principal Flow (PF) - a complex-valued continuous flow that captures gradient descent instabilities and edge-of-stability phenomena

## Executive Summary
This paper addresses the limitations of existing continuous-time models for gradient descent by introducing the Principal Flow (PF), a complex-valued flow derived from backward error analysis. Unlike traditional real-valued flows, the PF can capture observed instabilities in deep learning such as divergence and oscillations around critical points. The authors show that the PF's stability depends on the eigendecomposition of the Hessian, providing new insights into edge-of-stability behavior. Based on this understanding, they propose DAL (Drift Adjusted Learning rate), a learning rate adaptation method that improves training stability and generalization across various architectures and datasets.

## Method Summary
The authors introduce the Principal Flow (PF) as a continuous-time approximation of gradient descent that operates in complex space. The PF is derived using backward error analysis and depends on the eigendecomposition of the Hessian matrix. The key insight is that when the largest eigenvalue λ₀ exceeds 2/h (where h is the learning rate), the stability coefficient becomes complex with positive real part, causing oscillations and divergence. Based on this understanding, they propose DAL - a learning rate adaptation method that adjusts the learning rate based on the local gradient and Hessian information, specifically using the norm of ∇²θE∇θE as a proxy for discretization drift.

## Key Results
- The PF accurately predicts gradient descent instabilities including divergence and oscillations that traditional flows miss
- DAL improves training stability and generalization across multiple architectures (MLP, VGG, ResNet) and datasets (MNIST, CIFAR-10, ImageNet)
- The PF explains edge-of-stability phenomena through its dependence on the largest eigenvalue λ₀ relative to 2/h
- DAL automatically adjusts learning rates based on local gradient and Hessian information, reducing the need for manual tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Principal Flow (PF) captures gradient descent instabilities by operating in complex space, unlike real-valued flows.
- Mechanism: When eigenvalues of the Hessian exceed 2/h, the PF's stability coefficient becomes complex with positive real part, reversing the descent direction and causing oscillations/divergence.
- Core assumption: The eigendecomposition of the Hessian remains stable between gradient descent iterations for the PF approximation to hold.
- Evidence anchors:
  - [abstract]: "Unlike existing flows, the PF operates in complex space, enabling it to capture instabilities like divergence and oscillations"
  - [section]: "We show examples of the PF tracking gradient descent exactly in the quadratic case in Figures 2(b) and 5"
  - [corpus]: Weak evidence - no corpus papers specifically discuss complex-valued continuous flows for gradient descent instability

### Mechanism 2
- Claim: Discretization drift can be controlled by adjusting the learning rate based on ∇²θE∇θE.
- Mechanism: The magnitude of ∇²θE∇θE correlates with per-iteration drift; DAL adjusts learning rate inversely proportional to this quantity.
- Core assumption: ∇²θE∇θE serves as a reliable proxy for total discretization drift magnitude.
- Evidence anchors:
  - [abstract]: "DAL automatically adjusts the learning rate based on local gradient and Hessian information"
  - [section]: "Computing the norm of the update provided by this learning rate shows a challenge however since 2/∥∇²θE∇θE∥≥ 2/λ₀∥∇θE∥"
  - [corpus]: Weak evidence - no corpus papers specifically discuss learning rate adaptation based on Hessian-gradient products

### Mechanism 3
- Claim: The stability coefficient sc₀ = log(1-hλ₀)/(hλ₀)∇θEᵀu₀ determines edge-of-stability behavior.
- Mechanism: When Re[sc₀] > 0, gradient descent behaves like positive gradient flow in that eigendirection, causing loss increases.
- Core assumption: The largest eigenvalue λ₀ dominates stability behavior when it's significantly larger than other eigenvalues.
- Evidence anchors:
  - [abstract]: "The PF is derived using backward error analysis and depends on the eigendecomposition of the Hessian"
  - [section]: "We visualize the edge of stability behavior they observe in Figure 9; since we use a cross entropy loss λ₀ decreases later in training"
  - [corpus]: Weak evidence - no corpus papers specifically discuss stability coefficients derived from complex flows

## Foundational Learning

- Concept: Backward Error Analysis (BEA)
  - Why needed here: BEA is used to construct the Principal Flow by finding a continuous flow that approximates gradient descent with controlled error
  - Quick check question: What is the error order of the IGR flow derived via BEA compared to standard NGF?

- Concept: Eigenvalue decomposition of the Hessian
  - Why needed here: The PF depends on the eigendecomposition of the Hessian to determine stability in each eigendirection
  - Quick check question: How does the sign of the stability coefficient scᵢ change when λᵢ crosses 1/h?

- Concept: Edge of stability phenomenon
  - Why needed here: The PF explains edge-of-stability behavior through its dependence on the largest eigenvalue λ₀ relative to 2/h
  - Quick check question: What happens to the PF when λ₀ > 2/h in terms of its real and imaginary parts?

## Architecture Onboarding

- Component map: Principal Flow -> Stability coefficients -> DAL -> Non-principal terms -> Hessian computation
- Critical path:
  1. Compute ∇θE and Hessian at current parameters
  2. Perform eigendecomposition to get λᵢ and uᵢ
  3. Calculate stability coefficients scᵢ = log(1-hλᵢ)/(hλᵢ)∇θEᵀuᵢ
  4. Use sc₀ to predict instability and adjust learning rate via DAL
  5. Apply gradient update with adjusted learning rate
- Design tradeoffs:
  - PF accuracy vs computational cost: Computing full Hessian eigendecomposition is expensive
  - DAL stability vs learning speed: Lower DAL power p increases stability but slows convergence
  - Complex arithmetic vs real arithmetic: PF requires complex number support for instability modeling
- Failure signatures:
  - Large discrepancy between PF predictions and actual gradient descent behavior
  - DAL learning rates becoming too small or too large
  - Hessian computation errors propagating to stability coefficient calculation
  - Model divergence when using DAL with very small p values
- First 3 experiments:
  1. Train a simple MLP on MNIST with DAL vs fixed learning rates, compare test accuracy and stability
  2. Implement PF approximation for a quadratic loss and verify it exactly tracks gradient descent
  3. Measure correlation between ∥∇²θE∇θE∥ and actual per-iteration drift across different learning rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Principal Flow (PF) be integrated into theoretical analyses of gradient descent that currently rely on the Negative Gradient Flow (NGF)?
- Basis in paper: [explicit] The authors state that the PF can capture instabilities and escape from local minima and saddle points, unlike the NGF, and suggest that replacing NGF with PF in theoretical contexts could yield interesting results.
- Why unresolved: While the paper provides empirical evidence of the PF's advantages, no theoretical work has been done to integrate the PF into existing frameworks like the Neural Tangent Kernel or convergence analyses.
- What evidence would resolve it: Theoretical proofs demonstrating that the PF leads to stronger or more accurate convergence guarantees, or explanations for phenomena like the edge-of-stability behavior, compared to NGF-based analyses.

### Open Question 2
- Question: What are the broader implications of non-principal terms in the discretization drift for gradient descent optimization?
- Basis in paper: [explicit] The authors identify a non-principal term that can have a stabilizing effect and suggest that other implicit regularization effects could be uncovered using non-principal terms.
- Why unresolved: The paper only explores one non-principal term and provides a preliminary explanation for its stabilizing effect. The full scope and impact of non-principal terms remain unexplored.
- What evidence would resolve it: A comprehensive theoretical framework explaining the role of non-principal terms in optimization dynamics, including their impact on generalization and stability, supported by empirical validation.

### Open Question 3
- Question: How can the understanding of discretization drift be leveraged to improve optimization algorithms beyond gradient descent?
- Basis in paper: [explicit] The authors demonstrate preliminary results of integrating drift information into momentum-based methods and adaptive per-parameter learning rates, showing potential for improvement.
- Why unresolved: The paper provides initial experiments but does not fully explore the integration of drift information into other optimization algorithms like Adam or AdaGrad.
- What evidence would resolve it: Rigorous empirical studies showing improved performance, stability, and generalization of optimization algorithms that incorporate drift information, compared to their standard counterparts.

## Limitations

- Computational cost: Full Hessian eigendecomposition is expensive and may not scale to very large models
- Assumption stability: The assumption that Hessian eigendecomposition remains stable between gradient descent iterations is critical but untested for deep networks
- Smoothness requirement: Theoretical analysis assumes sufficiently small learning rates and smooth loss functions, which may not hold in practice

## Confidence

- High Confidence: The quadratic case analysis where PF exactly matches gradient descent (Mechanism 1)
- Medium Confidence: The edge-of-stability explanation through stability coefficients (Mechanism 3) - supported by theory but requires more empirical validation
- Low Confidence: The correlation between ∇²θE∇θE and discretization drift (Mechanism 2) - theoretical justification exists but empirical validation is limited

## Next Checks

1. **Edge-of-stability prediction**: Train a ResNet on CIFAR-10 with DAL and compare the predicted edge-of-stability points (where λ₀ approaches 2/h) against observed oscillations in the training loss curve.

2. **Computational efficiency**: Measure the overhead of DAL implementation using Hessian-vector products versus standard training, and verify that DAL consistently maintains the same or better test accuracy within the same wall-clock time.

3. **Generalization to non-quadratic landscapes**: Apply PF analysis to a VGG model on CIFAR-10 and quantify how well the stability coefficient predictions match actual training behavior in non-quadratic regions of the loss landscape.