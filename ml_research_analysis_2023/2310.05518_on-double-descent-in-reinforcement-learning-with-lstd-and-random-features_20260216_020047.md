---
ver: rpa2
title: On Double Descent in Reinforcement Learning with LSTD and Random Features
arxiv_id: '2310.05518'
source_url: https://arxiv.org/abs/2310.05518
tags:
- lemma
- have
- equation
- matrix
- msbe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the performance of regularized Least-Squares
  Temporal Difference (LSTD) algorithms with random features in reinforcement learning.
  The authors study a double asymptotic regime where the number of parameters N and
  distinct visited states m grow to infinity while maintaining a constant ratio.
---

# On Double Descent in Reinforcement Learning with LSTD and Random Features

## Quick Facts
- arXiv ID: 2310.05518
- Source URL: https://arxiv.org/abs/2310.05518
- Reference count: 40
- This paper analyzes double descent in LSTD with random features, showing a peak in true MSBE around N=m that diminishes with increased regularization.

## Executive Summary
This paper investigates the double-descent phenomenon in reinforcement learning by analyzing regularized Least-Squares Temporal Difference (LSTD) algorithms with random features. The authors propose a novel double asymptotic regime where the number of parameters N and distinct visited states m grow to infinity while maintaining a constant ratio. They derive deterministic limits for both empirical and true Mean-Squared Bellman Error (MSBE) that feature correction terms responsible for the double-descent behavior. The analysis reveals a peak in true MSBE around the interpolation threshold (N=m), which diminishes as the l2-regularization parameter increases or the number of unvisited states decreases.

## Method Summary
The method involves analyzing regularized LSTD algorithms in a double asymptotic regime where N (number of random features) and m (distinct visited states) both go to infinity while maintaining constant ratio N/m. The authors use random matrix theory to derive deterministic limits for MSBE, introducing correction factors that explain the double-descent phenomenon. The analysis considers both empirical MSBE (computed on visited states) and true MSBE (computed on all states), showing that the difference between them creates the observed double-descent behavior. The key technical tool is the resolvent of random matrices, which enables precise characterization of the performance in both over- and under-parameterized regimes.

## Key Results
- Derives deterministic limits for both empirical and true MSBE featuring correction terms that cause double-descent
- Shows peak in true MSBE around interpolation threshold N=m, not observed in empirical MSBE
- Proves that double-descent diminishes with increased l2-regularization or decreased unvisited states
- Numerical experiments on synthetic and real-world environments match theoretical predictions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Double descent is caused by correction terms that vanish with increased regularization or more visited states.
- **Mechanism**: The correction factor δ in the asymptotic resolvent depends on N/m. When N=m, δ drops sharply for small λ, creating a peak in MSBE. Increasing λ or reducing unvisited states makes δ smaller and smoother, eliminating the peak.
- **Core assumption**: The double asymptotic regime holds and the symmetric part of the empirical transition matrix is positive-definite.
- **Evidence anchors**: [abstract] and [section 5.1] establish this mechanism with Theorem 5.1 defining the correction factor δ.

### Mechanism 2
- **Claim**: The double asymptotic regime enables precise characterization of performance in both over- and under-parameterized regimes.
- **Mechanism**: By keeping N/m constant as both grow to infinity, the analysis captures the phase transition around N=m where the resolvent's behavior changes dramatically.
- **Core assumption**: The ratio N/m remains asymptotically constant.
- **Evidence anchors**: [abstract] and [section 4.2] define the double asymptotic regime with Assumption 1 specifying the constant ratio N/m.

### Mechanism 3
- **Claim**: The difference between true and empirical MSBE creates the observed double-descent behavior.
- **Mechanism**: Empirical MSBE has a smaller correction term at N=m compared to true MSBE, which has a larger correction term creating the peak.
- **Core assumption**: The number of unvisited states is non-zero and correction terms are significant.
- **Evidence anchors**: [abstract] and [section 5.3] show this difference with Theorem 5.3 demonstrating the true MSBE's correction term ∆ creates double-descent.

## Foundational Learning

- **Concept**: Random Matrix Theory for analyzing large-dimensional random matrices
  - Why needed here: The double asymptotic regime requires analyzing the resolvent of random matrices as both dimensions grow
  - Quick check question: What is the Stieltjes transform and how does it relate to the resolvent of random matrices?

- **Concept**: Temporal Difference learning and Bellman equations
  - Why needed here: LSTD is a TD method that learns value functions by minimizing the Bellman error
  - Quick check question: What is the difference between MSBE and MSPBE in TD learning?

- **Concept**: Overparameterization and generalization in machine learning
  - Why needed here: The paper studies the transition between under- and over-parameterized regimes and their effects on performance
  - Quick check question: What is the interpolation threshold and why does it matter for generalization?

## Architecture Onboarding

- **Component map**: MRP (S, P, r, γ) → Random features σ(Ws) → LSTD algorithm → Resolvent analysis → MSBE prediction

- **Critical path**: MRP → Random features → LSTD → Resolvent analysis → MSBE prediction

- **Design tradeoffs**:
  - Random features vs neural networks: Random features simplify analysis but may not capture all neural network behaviors
  - Regularization parameter λ: Controls overfitting but affects double-descent peak
  - Ratio N/m: Determines under/over-parameterization but must remain constant in asymptotic analysis

- **Failure signatures**:
  - If empirical transition matrix is not positive-definite: Resolvent doesn't exist
  - If ratio N/m varies significantly: Asymptotic analysis breaks down
  - If states are not visited proportionally: Concentration bounds may fail

- **First 3 experiments**:
  1. Verify double-descent in synthetic MRP with varying N/m and λ
  2. Test effect of unvisited states by masking some states in data
  3. Compare random features vs small neural network implementation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mathematical relationship between the number of unvisited states and the magnitude of the double-descent phenomenon in the true MSBE?
- Basis in paper: Explicit - The paper states correction terms vanish when the number of unvisited states goes to zero
- Why unresolved: The paper establishes that double-descent diminishes with fewer unvisited states but doesn't provide a quantitative relationship
- What evidence would resolve it: Numerical experiments varying unvisited states and measuring corresponding double-descent magnitude

### Open Question 2
- Question: How does double-descent in RL differ fundamentally from double-descent in supervised learning, and what are the underlying causes?
- Basis in paper: Explicit - The paper notes that the discount factor complicates analysis and makes it "substantially more involved" than supervised learning
- Why unresolved: While differences are identified, detailed comparison of underlying mechanisms is not provided
- What evidence would resolve it: Theoretical analysis comparing mathematical properties of key matrices and error terms in both settings

### Open Question 3
- Question: Can insights from LSTD with random features be extended to other TD learning algorithms and more complex neural network architectures?
- Basis in paper: Explicit - The paper mentions future work on Q-Learning and deep neural networks
- Why unresolved: The paper focuses specifically on LSTD with random features without exploring other algorithms or architectures
- What evidence would resolve it: Theoretical or empirical studies applying the methodology to other TD algorithms and complex architectures

## Limitations
- Double asymptotic regime assumes N and m grow to infinity while maintaining constant ratio, which may not hold in practical finite-sample scenarios
- Theoretical predictions rely on technical assumptions including positive-definiteness of empirical transition matrices that may be violated in real-world applications
- Random feature model may not capture all behaviors of more complex function approximators like neural networks

## Confidence
- Core double-descent mechanism: Medium
- Main theoretical predictions: Medium
- Practical implications and generalization: Low

## Next Checks
1. **Finite-sample robustness test**: Run experiments with varying finite values of N and m to assess how well theoretical predictions hold in practical regimes
2. **Assumption relaxation study**: Systematically test what happens when technical assumptions are violated (sparse transition matrices, non-uniform state visitation)
3. **Architecture generalization**: Compare random features against small neural network implementations to assess how well analytical framework captures general function approximation scenarios