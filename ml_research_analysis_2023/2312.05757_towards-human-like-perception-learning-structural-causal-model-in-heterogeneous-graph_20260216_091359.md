---
ver: rpa2
title: 'Towards Human-like Perception: Learning Structural Causal Model in Heterogeneous
  Graph'
arxiv_id: '2312.05757'
source_url: https://arxiv.org/abs/2312.05757
tags:
- uni2190
- graph
- learning
- u1d456
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes HG-SCM, a novel heterogeneous graph neural
  network architecture designed to address the challenges of generalizability and
  interpretability. HG-SCM mimics human reasoning by constructing meaningful variables
  based on graph schema semantics and automatically learning task-level causal relationships
  among these variables using advanced causal discovery techniques.
---

# Towards Human-like Perception: Learning Structural Causal Model in Heterogeneous Graph

## Quick Facts
- arXiv ID: 2312.05757
- Source URL: https://arxiv.org/abs/2312.05757
- Authors: 
- Reference count: 22
- Primary result: Novel heterogeneous graph neural network architecture that significantly outperforms state-of-the-art baselines in both predictive power and generalizability across out-of-distribution settings

## Executive Summary
This paper introduces HG-SCM, a heterogeneous graph neural network that mimics human reasoning by constructing interpretable variables from graph schema semantics and learning causal relationships among them. The model addresses key challenges in graph representation learning: generalizability to out-of-distribution data and interpretability of predictions. By incorporating structural causal models and advanced causal discovery techniques, HG-SCM achieves superior performance on three real-world datasets while providing clear explanations aligned with domain knowledge.

## Method Summary
HG-SCM constructs meaningful variables from graph schema semantics, including node features, labels, and neighbor sets, then learns task-level causal relationships among these variables using causal discovery algorithms. The architecture employs mutually independent encoders for different variables to prevent spurious correlations, followed by a causal structure modeling component that learns a directed acyclic graph (DAG) representing causal relationships. The model jointly optimizes task loss, reconstruction loss, and DAG constraint loss to balance prediction accuracy with causal structure learning. This approach enables the model to capture stable causal relationships that generalize across distribution shifts rather than fitting spurious correlations present in training data.

## Key Results
- Significantly outperforms seven state-of-the-art baselines on DBLP, ACM, and IMDB datasets
- Demonstrates superior generalizability across various out-of-distribution settings, particularly with degree bias
- Provides interpretable causal structures that align with domain knowledge and enable clear explanations of predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model aligns with human reasoning by explicitly constructing interpretable variables from graph schema semantics
- Mechanism: HG-SCM defines variables as node features, labels, and neighbor sets based on relations/meta-paths, then learns causal relationships among these variables
- Core assumption: Variables constructed from graph schema semantics correspond to human-understandable concepts
- Evidence anchors:
  - [abstract] "constructing intelligible variables based on semantics derived from the graph schema"
  - [section] "two variables are naturally included, i.e., the node feature of a target node and the node label of the target node"
  - [corpus] Weak evidence - corpus focuses on causal discovery but not variable construction
- Break condition: If graph schema semantics don't map to meaningful human concepts, variable construction becomes arbitrary

### Mechanism 2
- Claim: Causal discovery enables task-level reasoning that generalizes better than sample-level adaptation
- Mechanism: HG-SCM learns a structural causal model that identifies direct causes for the target variable, enabling predictions based on causal relationships rather than spurious correlations
- Core assumption: Task-level causal relationships are more stable across distribution shifts than sample-level patterns
- Evidence anchors:
  - [abstract] "automatically learning task-level causal relationships among these variables"
  - [section] "incorporating emerging causal structural learning techniques"
  - [corpus] Weak evidence - corpus discusses causal discovery but not its application to graph neural networks
- Break condition: If causal discovery fails to identify true causal relationships, the model may perform worse than traditional approaches

### Mechanism 3
- Claim: Disentangled encoding prevents fitting spurious correlations between variables
- Mechanism: HG-SCM uses mutually-independent encoders for different variables to avoid introducing correlation between representations
- Core assumption: Spurious correlations between variables hurt generalizability and interpretability
- Evidence anchors:
  - [section] "these encoders... need to be mutually independent to avoid fitting the spurious correlations among them"
  - [abstract] "automatically learning task-level causal relationships among these variables by incorporating advanced causal discovery techniques"
  - [corpus] Weak evidence - corpus discusses causal discovery but not encoding strategies
- Break condition: If encoders cannot be truly independent, spurious correlations may still be learned

## Foundational Learning

- Concept: Structural Causal Models (SCMs)
  - Why needed here: Provides theoretical foundation for modeling causal relationships among variables
  - Quick check question: What are the key components of an SCM and how do they differ from standard neural network architectures?

- Concept: Heterogeneous Graph Schemas
  - Why needed here: Enables identification of meaningful variables through schema semantics
  - Quick check question: How do different types of relations and meta-paths in a heterogeneous graph create distinct semantic contexts?

- Concept: Causal Discovery Algorithms
  - Why needed here: Enables automatic learning of causal relationships without manual specification
  - Quick check question: What are the main challenges in learning causal structures from observational data?

## Architecture Onboarding

- Component map:
  Variable Construction (Node features, labels, neighbor sets via independent encoders) -> Causal Structure Modeling (DAG matrix with trainable weights, effect encoders, variable decoders) -> Prediction Layer (Label reconstruction from causal structure)

- Critical path: Variable construction → Causal structure learning → Prediction → Backpropagation through all three loss components

- Design tradeoffs:
  - Independent encoders vs. joint encoding: Independence prevents spurious correlations but may lose useful shared information
  - DAG constraints vs. flexibility: Strict DAG requirements ensure causality but may miss complex relationships
  - Reconstruction vs. prediction focus: Balancing variable reconstruction with label prediction accuracy

- Failure signatures:
  - Poor performance: Check if causal structure learning is converging properly
  - Overfitting: Examine if reconstruction loss dominates prediction loss
  - Slow training: Verify DAG constraint optimization is not bottlenecking training

- First 3 experiments:
  1. Baseline comparison on i.i.d data split to verify task performance
  2. Out-of-distribution test with degree bias to validate generalizability
  3. Visualization of learned DAGs to assess interpretability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance of HG-SCM be further improved by incorporating more advanced causal discovery techniques?
- Basis in paper: [explicit] The paper suggests that future work could explore incorporating advanced causal techniques to enhance generalizability.
- Why unresolved: The paper does not provide specific details on which advanced causal techniques could be applied or how they would impact performance.
- What evidence would resolve it: Experimental results comparing HG-SCM with and without the proposed advanced causal techniques on various datasets and settings.

### Open Question 2
- Question: How can the interpretability of HG-SCM be quantitatively evaluated and compared to other methods?
- Basis in paper: [inferred] The paper highlights the interpretability of HG-SCM but does not provide a quantitative evaluation framework or comparison with other methods.
- Why unresolved: The paper lacks a standardized metric or methodology for assessing and comparing the interpretability of different graph learning models.
- What evidence would resolve it: A proposed framework for quantifying interpretability, along with experimental results comparing HG-SCM to other methods using this framework.

### Open Question 3
- Question: How does the choice of graph schema impact the performance and interpretability of HG-SCM?
- Basis in paper: [explicit] The paper mentions that HG-SCM constructs meaningful variables based on graph schema semantics, but does not explore the impact of different graph schema choices.
- Why unresolved: The paper does not investigate how variations in graph schema representation affect the model's ability to learn causal relationships and provide interpretable results.
- What evidence would resolve it: Experiments comparing HG-SCM's performance and interpretability across different graph schema representations for the same underlying graph structure.

## Limitations

- Reliance on clear graph schema semantics may limit applicability to graphs with incomplete or ambiguous schema information
- Causal discovery process may struggle with high-dimensional variables or complex dependency structures
- Performance across diverse out-of-distribution scenarios requires more extensive validation beyond degree bias experiments

## Confidence

- **High confidence**: The core mechanism of using structural causal models for heterogeneous graphs is well-established theoretically, and the experimental methodology follows standard practices
- **Medium confidence**: The interpretability claims are supported by qualitative analysis of learned DAGs, but lack systematic human evaluation to verify alignment with domain expertise
- **Medium confidence**: The generalizability improvements over baselines are demonstrated empirically but could benefit from additional stress tests with more varied distribution shifts

## Next Checks

1. **Schema-agnostic testing**: Evaluate HG-SCM on heterogeneous graphs with incomplete or noisy schema information to assess robustness of the variable construction mechanism
2. **Human evaluation study**: Conduct a structured study where domain experts rate the interpretability of learned causal structures against ground truth causal knowledge
3. **Distribution shift stress test**: Systematically vary multiple graph properties simultaneously (e.g., node degree, attribute distributions, community structure) to identify breaking points in generalizability claims