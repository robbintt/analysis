---
ver: rpa2
title: 'Cross-lingual Cross-temporal Summarization: Dataset, Models, Evaluation'
arxiv_id: '2306.12916'
source_url: https://arxiv.org/abs/2306.12916
tags:
- chatgpt
- summarization
- clcts
- evaluation
- historical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces cross-lingual cross-temporal summarization
  (CLCTS), which aims to summarize historical documents in a different modern language.
  The authors build the first CLCTS dataset with 328 English-German and 289 German-English
  instances, leveraging historical fiction texts and Wikipedia summaries.
---

# Cross-lingual Cross-temporal Summarization: Dataset, Models, Evaluation

## Quick Facts
- arXiv ID: 2306.12916
- Source URL: https://arxiv.org/abs/2306.12916
- Reference count: 40
- Key outcome: CLCTS dataset introduced with 328 English-German and 289 German-English instances; ChatGPT achieves moderate-to-good quality without finetuning while intermediate task finetuned models perform poorly to moderately

## Executive Summary
This paper introduces cross-lingual cross-temporal summarization (CLCTS), a task that requires summarizing historical documents in a different modern language. The authors construct the first CLCTS dataset using historical fiction texts paired with Wikipedia summaries, and evaluate transformer-based models with intermediate task finetuning as well as zero-shot ChatGPT. Results show that ChatGPT provides moderate to good quality summaries without any finetuning, while intermediate task finetuned models generate bad to moderate quality outputs. The study reveals that longer, older, and more complex source texts are harder to summarize, highlighting the difficulty of the CLCTS task.

## Method Summary
The authors build a CLCTS dataset by collecting historical texts from DTA, Wikisource, and Project Gutenberg, and pairing them with modern Wikipedia summaries. Historical text normalization is performed using the Norma tool. Three model types are evaluated: extract-then-translate pipelines (MemSum + M2M/ChatGPT), abstractive end-to-end models (mLED with intermediate task finetuning), and zero-shot ChatGPT with various prompts. Models are evaluated using automatic metrics (ROUGE, BERTScore, BARTScore, MoverScore, MENLI, DiscoScore), human evaluation (coherence, consistency, fluency, relevance), and ChatGPT evaluation. Adversarial tests assess robustness against sentence omission, entity swap, and negation.

## Key Results
- ChatGPT achieves moderate to good quality summaries without finetuning, outperforming intermediate task finetuned models
- Longer, older, and more complex source texts are harder to summarize across all model types
- ChatGPT excels at normalizing historical text without explicit training
- Historical text normalization using Norma slightly decreases performance, suggesting potential ambiguity issues

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Intermediate task finetuning improves CLCTS when tasks share similar characteristics with the target task
- **Mechanism**: Finetuning a multilingual mBART model with intermediate tasks (monolingual summarization, cross-lingual summarization, or cross-temporal summarization) helps adapt the model to linguistic and temporal shifts
- **Core assumption**: Intermediate tasks provide relevant features that transfer to the target task
- **Evidence anchors**: Models trained with more intermediate summarization tasks show higher scores for hEn-De; weak evidence from corpus analysis showing linguistic differences

### Mechanism 2
- **Claim**: ChatGPT generates reasonable CLCTS summaries using pre-trained knowledge without finetuning
- **Mechanism**: ChatGPT's pre-training on massive text corpus enables understanding of language and historical period nuances
- **Core assumption**: Pre-training data includes sufficient information about historical language variants
- **Evidence anchors**: ChatGPT provides moderate to good quality outputs without finetuning; capable of outputting summaries based solely on meta-information

### Mechanism 3
- **Claim**: Longer, older, and more complex source texts are harder to summarize for all models
- **Mechanism**: Increased length, age, and complexity pose challenges for processing, understanding, and maintaining coherence
- **Core assumption**: Difficulty is directly related to source text characteristics
- **Evidence anchors**: Regression results show longer, older, and more complex texts are harder to summarize; corpus analysis reveals historical documents are longer with more complex syntax

## Foundational Learning

- **Concept**: Historical language divergence
  - **Why needed here**: Understanding language change over time is crucial for handling linguistic differences between historical and modern variants
  - **Quick check question**: What are the main dimensions of language change over time, and how do they affect historical text summarization?

- **Concept**: Cross-lingual summarization
  - **Why needed here**: CLCTS involves summarizing in a different language, requiring understanding of cross-lingual challenges
  - **Quick check question**: What are the main approaches for cross-lingual summarization, and how do they handle language differences and translation?

- **Concept**: Long document summarization
  - **Why needed here**: Historical documents are often longer than modern texts, requiring techniques for processing long texts
  - **Quick check question**: What are the main approaches for long document summarization, and how do they handle long text processing challenges?

## Architecture Onboarding

- **Component map**: Corpus creation -> Model building -> Evaluation -> Analysis and iteration
- **Critical path**: Corpus creation → Model building → Evaluation → Analysis and iteration
- **Design tradeoffs**: Corpus size vs. quality, model complexity vs. performance, evaluation metrics vs. human judgment
- **Failure signatures**: Low evaluation scores, high variance in scores, hallucinations or inaccuracies in generated summaries
- **First 3 experiments**:
  1. Build a simple extractive summarization model and evaluate on CLCTS corpus
  2. Fine-tune mBART with intermediate tasks and evaluate on CLCTS corpus
  3. Experiment with different prompt engineering techniques for ChatGPT and evaluate on CLCTS corpus

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does performance change when trained on larger historical text corpora, and what is optimal corpus size?
- **Basis in paper**: Authors note CLCTS datasets are smaller than modern datasets and hypothesize larger historical translation dataset may improve results
- **Why unresolved**: Only small historical translation dataset (201 pairs) used, no experiments with scaling up historical data
- **What evidence would resolve it**: Experiments comparing model performance on varying sizes of historical training data

### Open Question 2
- **Question**: How well do CLCTS models generalize to unseen historical documents from different time periods or genres?
- **Basis in paper**: Authors test ChatGPT on unseen documents finding mediocre quality, but don't test finetuned models on truly unseen historical data
- **Why unresolved**: All experiments use documents from same source genres and time periods as training data
- **What evidence would resolve it**: Experiments evaluating finetuned models on historical documents from different centuries, genres, or cultural contexts

### Open Question 3
- **Question**: What is the impact of incorporating historical text normalization techniques into CLCTS pipeline?
- **Basis in paper**: Authors test one normalization method (Norma) and find it slightly decreases performance, but hypothesize better context-aware normalization could help
- **Why unresolved**: Only one normalization method tested, analysis limited to German
- **What evidence would resolve it**: Comparative experiments testing multiple normalization methods on both English and German historical texts

## Limitations

- Dataset size remains modest (328 English-German and 289 German-English instances), limiting generalizability
- Historical text normalization using Norma introduces ambiguity that may obscure semantic meaning
- Human evaluation conducted by only 6 expert annotators with moderate agreement (Fleiss Kappa 0.42-0.53)
- Analysis of ChatGPT performance limited by inability to inspect training data and fine-tuning process

## Confidence

**High Confidence Claims:**
- CLCTS is a distinct and challenging task requiring handling of linguistic divergence across time and language
- Historical documents are longer and more complex than modern summaries, affecting summarization difficulty
- ChatGPT demonstrates strong zero-shot performance for CLCTS without task-specific finetuning

**Medium Confidence Claims:**
- Intermediate task finetuning provides mixed benefits depending on task similarity and language direction
- ChatGPT can normalize historical text effectively without explicit training
- Longer, older, and more complex source texts are harder to summarize across all model types

**Low Confidence Claims:**
- Relative performance rankings between different intermediate tasks
- Generalizability of results to other language pairs beyond English-German
- Stability of ChatGPT's performance across different prompt formulations

## Next Checks

1. **Corpus Size Validation**: Replicate experiments with a larger CLCTS corpus (minimum 1000 instances per language pair) to assess whether observed patterns hold with increased statistical power.

2. **Cross-Lingual Generalization Test**: Apply best-performing models to a different language pair (e.g., English-French or German-French) to validate whether benefits of intermediate task finetuning and ChatGPT capabilities transfer across language combinations.

3. **Adversarial Robustness Evaluation**: Systematically test all model variants against adversarial scenarios (sentence omission, entity swap, negation) using standardized protocol to measure resilience and identify specific failure modes.