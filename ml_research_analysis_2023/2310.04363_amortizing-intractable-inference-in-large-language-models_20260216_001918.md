---
ver: rpa2
title: Amortizing intractable inference in large language models
arxiv_id: '2310.04363'
source_url: https://arxiv.org/abs/2310.04363
tags:
- language
- gflownet
- fine-tuning
- sampling
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Amortizing intractable inference in large language models involves
  using generative flow networks (GFlowNets) to fine-tune language models for sampling
  from intractable posterior distributions. The core idea is to train a GFlowNet policy
  to match the Bayesian posterior over latent variables, enabling amortized inference
  for tasks such as sequence continuation, infilling, and chain-of-thought reasoning.
---

# Amortizing intractable inference in large language models

## Quick Facts
- arXiv ID: 2310.04363
- Source URL: https://arxiv.org/abs/2310.04363
- Reference count: 40
- Amortizing intractable inference in large language models using GFlowNets achieves 63% improvement on integer arithmetic with 50 demonstrations

## Executive Summary
This paper addresses the challenge of sampling from intractable posteriors in large language models (LLMs) by using generative flow networks (GFlowNets) for amortized Bayesian inference. The approach fine-tunes LLMs to sample from complex posterior distributions over latent variables, enabling applications in sequence continuation, infilling, and chain-of-thought reasoning. The method demonstrates significant improvements over maximum likelihood training and reward-maximizing reinforcement learning approaches, particularly in data efficiency and out-of-distribution generalization.

## Method Summary
The method uses GFlowNets to fine-tune pretrained language models to sample from intractable posteriors by learning a policy that matches the Bayesian posterior over latent variables. The GFlowNet policy is initialized as a copy of the pretrained LLM and fine-tuned using a reward objective that evaluates the likelihood of sequences under the original language model. The training employs a multi-source policy approach using the current policy, tempered versions, and a replay buffer. This distribution-matching paradigm enables efficient sampling from complex posteriors at test time without requiring expensive inference procedures.

## Key Results
- GFlowNet fine-tuning achieves 10.9% improvement in subjectivity classification with only 10 labeled examples
- Outperforms supervised fine-tuning and PPO by 63% on integer arithmetic with 50 demonstrations
- Shows superior out-of-distribution generalization compared to maximum likelihood and reward-maximizing approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GFlowNet fine-tuning enables amortized sampling from intractable posteriors by training a policy to match the Bayesian posterior over latent variables.
- Mechanism: The GFlowNet policy is initialized as a copy of the pretrained language model and then fine-tuned using a reward objective that evaluates the likelihood of the sequence under the original language model. This training objective directly optimizes the likelihood of generating a sequence to be proportional to its reward, which is the sequence's probability under the target distribution. The resulting policy can then sample from the intractable posterior efficiently at test time.
- Core assumption: The base language model contains the necessary knowledge for the task, and the goal is to perform inference over this knowledge.
- Evidence anchors:
  - [abstract]: "We address this limitation by using amortized Bayesian inference to sample from these intractable posteriors. Such amortization is algorithmically achieved by fine-tuning LLMs via diversity-seeking reinforcement learning algorithms: generative flow networks (GFlowNets)."
  - [section]: "To sample the latent sequence ùëç from the posterior defined in Eq. 2, we parametrize the GFlowNet policy as an autoregressive language model that samples the latent ùëç one token at a time from left to right. By setting the reward ùëÖ(ùëç) = ùëùLM (ùëã ùëçùëå) ‚àù ùëùLM(ùëç | ùëã, ùëå ), we learn a sampler for the posterior at convergence."
  - [corpus]: Weak evidence; related work focuses on amortized inference but not specifically GFlowNets for LLMs.

### Mechanism 2
- Claim: GFlowNet fine-tuning improves sample diversity and performance on downstream tasks compared to maximum-likelihood training and reward-maximizing policy optimization.
- Mechanism: GFlowNet objectives provide a principled and flexible approach to fine-tuning LLMs to match a target distribution where reward-maximizing RL fails to. By matching the entire distribution, GFlowNet fine-tuning avoids collapsing to a single mode of the reward, thereby being robust to the misspecification of the reward and achieving significantly better performance on in and out-of-distribution examples.
- Core assumption: The target distribution is misspecified or has multiple modes.
- Evidence anchors:
  - [abstract]: "We empirically demonstrate that this distribution-matching paradigm of LLM fine-tuning can serve as an effective alternative to maximum-likelihood training and reward-maximizing policy optimization."
  - [section]: "Such overoptimization to a misspecified reward is a widely noted issue in LLMs trained with RL (Gao et al., 2022). On the other hand, by matching the entire distribution, GFlowNet fine-tuning avoids collapsing to a single mode of the reward, thereby being robust to the misspecification of the reward (Eysenbach & Levine, 2022) and achieving significantly better performance on in and out-of-distribution examples."
  - [corpus]: Weak evidence; related work focuses on the limitations of reward-maximizing RL but not specifically GFlowNets.

### Mechanism 3
- Claim: GFlowNet fine-tuning enables data-efficient adaptation of LLMs to tasks that require multi-step rationalization and tool use.
- Mechanism: By training a model to match the Bayesian posterior ùëùLM(ùëç | ùëã, ùëå ), we can learn to sample latent reasoning chains that increase the likelihood of producing ùëå from ùëã via the sampled ùëç. This posterior inference corresponds to the E-step in the EM algorithm, where the posterior ùëùLM(ùëç | ùëã, ùëå ) is defined in Eq. 2. Further, we can take an M-step by updating ùëùLM to maximize log ùëùLM(ùëã ùëçùëå) over a collection of ùëç's sampled from the amortized posterior ùëûGFN.
- Core assumption: The task can be decomposed into a chain of easier inference steps.
- Evidence anchors:
  - [abstract]: "As an important application, we interpret chain-of-thought reasoning as a latent variable modeling problem and demonstrate that our approach enables data-efficient adaptation of LLMs to tasks that require multi-step rationalization and tool use."
  - [section]: "By training a model to match the Bayesian posterior ùëùLM(ùëç | ùëã, ùëå ), we can learn to sample latent reasoning chains that increase the likelihood of producing ùëå from ùëã via the sampled ùëç. However, we can also fine-tune the language model ùëùLM(ùëç | ùëãùëå ) itself to maximize the likelihood of data pairs (ùëã, ùëå ) under the LVM."
  - [corpus]: Weak evidence; related work focuses on chain-of-thought reasoning but not specifically GFlowNets.

## Foundational Learning

- Concept: Amortized inference
  - Why needed here: To train a model to approximate a distribution of interest, enabling efficient sampling from intractable posteriors.
  - Quick check question: What is the difference between amortized inference and MCMC sampling?
- Concept: Generative Flow Networks (GFlowNets)
  - Why needed here: To learn policies that sample objects with probability proportional to a given reward function, enabling diversity-seeking reinforcement learning.
  - Quick check question: How do GFlowNets differ from traditional RL algorithms in terms of the objective they optimize?
- Concept: Bayesian inference
  - Why needed here: To interpret chain-of-thought reasoning as a problem of posterior inference over latent variables.
  - Quick check question: What is the relationship between the posterior distribution and the likelihood model in Bayesian inference?

## Architecture Onboarding

- Component map: Pretrained language model (ùëùLM) -> GFlowNet policy (ùëûGFN) -> Reward function (ùëÖ) -> Replay buffer
- Critical path:
  1. Initialize GFlowNet policy as a copy of the pretrained language model.
  2. Sample trajectories from the policy and the replay buffer.
  3. Compute rewards for the sampled trajectories using the reward function.
  4. Update the GFlowNet policy using the SubTB objective.
  5. At test time, sample from the trained GFlowNet policy to generate latent variables and use the original language model to generate outputs.
- Design tradeoffs:
  - Model capacity vs. training efficiency: Larger models may require more compute and memory but can potentially learn more complex posteriors.
  - Exploration vs. exploitation: Balancing the use of the current policy and the replay buffer to ensure sufficient exploration of the state space.
  - Reward specification: Choosing an appropriate reward function that captures the desired properties of the posterior distribution.
- Failure signatures:
  - Mode collapse: The GFlowNet policy collapses to a single mode of the reward, leading to poor diversity and performance.
  - Overfitting: The GFlowNet policy overfits to the training data, leading to poor generalization to unseen examples.
  - Exploration failure: The GFlowNet policy fails to explore the state space sufficiently, leading to poor performance on tasks that require diverse samples.
- First 3 experiments:
  1. Sequence continuation: Fine-tune a language model to sample likely continuations of a given prompt, comparing to baselines such as beam search and nucleus sampling.
  2. Story infilling: Fine-tune a language model to fill in the middle of a story given the beginning and end, comparing to baselines such as prompting and supervised fine-tuning.
  3. Subjectivity classification: Fine-tune a language model to classify movie reviews as objective or subjective using a small number of labeled examples, comparing to baselines such as few-shot prompting and supervised fine-tuning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GFlowNet fine-tuning scale with model size beyond 6B parameters?
- Basis in paper: Explicit - The paper states "Due to resource constraints, our experiments are with models up to 6B parameters. Nonetheless, we expect the conclusions to hold for larger models."
- Why unresolved: The paper did not have the computational resources to test GFlowNet fine-tuning on models larger than 6B parameters.
- What evidence would resolve it: Experiments showing the performance of GFlowNet fine-tuning on models with 10B, 50B, 100B+ parameters and comparing it to the performance on 6B models.

### Open Question 2
- Question: Can a single GFlowNet model be trained to perform inference for multiple different tasks?
- Basis in paper: Inferred - The paper mentions "Future work should investigate transfer and generalization across tasks, in particular, building a 'universal reasoner' as a model ùëû(ùëç | ùëã) shared between ùëã from different tasks."
- Why unresolved: The paper did not experiment with training a single GFlowNet model to handle multiple tasks.
- What evidence would resolve it: Experiments training a GFlowNet model on data from multiple tasks and evaluating its performance on each task compared to task-specific GFlowNet models.

### Open Question 3
- Question: How can the issues of hallucination and miscalibration in LLMs be addressed by GFlowNet fine-tuning?
- Basis in paper: Explicit - The paper states "Many issues with LLMs, such as hallucination or miscalibration, are closely related to the knowledge representation and thus not addressed."
- Why unresolved: The paper focused on using GFlowNet fine-tuning for inference, not for improving the underlying knowledge representation in LLMs.
- What evidence would resolve it: Experiments showing that GFlowNet fine-tuning can reduce hallucination and improve calibration on benchmarks that measure these issues.

## Limitations
- Experimental scope is limited to small-scale benchmarks and synthetic tasks
- Claims about scalability to larger models remain theoretical
- Limited analysis of failure modes beyond mode collapse and overfitting

## Confidence
- **High Confidence**: The core theoretical framework connecting GFlowNets to amortized Bayesian inference is well-established
- **Medium Confidence**: Experimental results showing improvements over baselines are promising but based on limited benchmark tasks
- **Low Confidence**: Scalability claims to larger models and more complex reasoning tasks are largely theoretical

## Next Checks
1. **Scalability Test**: Implement the full GFlowNet fine-tuning pipeline on GPT-3 or GPT-4 scale models to validate computational feasibility and measure performance degradation/gains at scale.
2. **Cross-Domain Generalization**: Evaluate the method on diverse real-world reasoning tasks beyond arithmetic, including scientific reasoning, code generation, and multi-hop QA, to test robustness claims.
3. **Ablation Study**: Systematically remove components (replay buffer, multi-source training, reward calibration) to quantify their individual contributions to performance improvements and identify critical failure points.