---
ver: rpa2
title: Representation Learning via Consistent Assignment of Views over Random Partitions
arxiv_id: '2310.12692'
source_url: https://arxiv.org/abs/2310.12692
tags:
- carp
- prototypes
- learning
- representations
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CARP learns visual representations by partitioning prototypes into
  random subsets and enforcing consistent assignment between views over these partitions.
  The method trains a joint-embedding architecture end-to-end using gradient descent,
  avoiding trivial solutions through a consistency loss that encourages views of the
  same image to map to the same prototype.
---

# Representation Learning via Consistent Assignment of Views over Random Partitions

## Quick Facts
- arXiv ID: 2310.12692
- Source URL: https://arxiv.org/abs/2310.12692
- Reference count: 40
- Key outcome: CARP achieves state-of-the-art k-NN accuracy while training for fewer epochs than competing methods

## Executive Summary
CARP introduces a novel self-supervised learning approach that partitions learnable prototypes into random subsets and enforces consistent assignment between views over these partitions. By optimizing a consistency loss within random blocks of prototypes, the method regularizes training and prevents trivial solutions that plague joint-embedding architectures. The approach trains end-to-end using gradient descent without requiring negatives mining or non-differentiable clustering modules, achieving strong performance across 17 diverse datasets for transfer learning, clustering, image retrieval, and few-shot classification tasks.

## Method Summary
CARP learns visual representations by partitioning prototypes into random subsets and enforcing consistent assignment between views over these partitions. The method trains a joint-embedding architecture end-to-end using gradient descent, avoiding trivial solutions through a consistency loss that encourages views of the same image to map to the same prototype. The random partition strategy regularizes training by creating multiple pseudo-classification tasks over prototype subsets. On 17 datasets, CARP achieves strong performance across transfer learning, clustering, image retrieval, and few-shot classification tasks, outperforming 11 existing self-supervised methods. Notably, CARP matches or exceeds state-of-the-art k-NN accuracy while training for fewer epochs, demonstrating efficient representation learning without requiring negatives mining or non-differentiable clustering modules.

## Key Results
- CARP achieves state-of-the-art k-NN accuracy on ImageNet while training for fewer epochs than competing methods
- Outperforms 11 existing self-supervised methods across 17 diverse datasets including transfer learning, clustering, and few-shot classification
- Matches or exceeds performance of methods requiring negatives mining or non-differentiable clustering modules
- Demonstrates stable training without collapse through the random partition regularization strategy

## Why This Works (Mechanism)

### Mechanism 1
The random partition strategy prevents trivial solutions by dividing the optimization problem into smaller, regularized pseudo-classification tasks. Instead of optimizing over all K prototypes at once, CARP creates NP random partitions of the prototype set. Each view is assigned to a block of NB prototypes, and consistency is enforced within each block. This regularization prevents the model from collapsing to a single prototype assignment. The core assumption is that smaller classification tasks with random partitions are easier to optimize and provide sufficient signal for learning good representations. Break condition: If the partition block size NB becomes too small relative to the number of prototypes K, the pseudo-classification tasks become trivial and lose regularization power. If NB becomes too large, the optimization becomes unstable again.

### Mechanism 2
The consistency loss with momentum encoder prevents collapse by learning optimal temperature scaling implicitly. The consistency loss Lc(s1, t2) encourages the student network to produce distributions that match the momentum teacher's targets. This symmetric loss formulation implicitly learns the temperature parameter that sharpens predictions at each iteration, unlike previous methods that required explicit temperature tuning. The core assumption is that the momentum teacher provides stable targets that guide the student without requiring explicit sharpening or temperature scheduling. Break condition: If the momentum update parameter η is set too high or too low, the teacher becomes either too slow to adapt or too noisy, breaking the consistency learning.

### Mechanism 3
The entropy regularization term with decaying weight prevents uniform collapse while allowing specialization over training. The entropy term H(̄p) encourages uniform assignment across prototypes in early training, preventing collapse to a single prototype. As training progresses, the weight λe decays, allowing the consistency term to dominate and the model to specialize its assignments. The core assumption is that early training benefits from uniform exploration while later training benefits from consistent specialization. Break condition: If λe decays too quickly, the model may collapse before learning good representations. If it decays too slowly, the model may never specialize sufficiently.

## Foundational Learning

- Concept: Joint-embedding architectures and the collapse problem
  - Why needed here: CARP uses a siamese-like architecture where two views of the same image must be mapped to consistent prototype assignments. Understanding why collapse happens is crucial for grasping the random partition solution.
  - Quick check question: What would happen if CARP optimized a consistency loss over all K prototypes without any regularization?

- Concept: Prototype-based clustering vs. embedding prediction
  - Why needed here: CARP discretizes the representation space using learnable prototypes rather than optimizing embeddings directly. This fundamental difference from contrastive methods is key to understanding the approach.
  - Quick check question: How does optimizing over prototype assignments differ from optimizing embedding distances directly?

- Concept: Self-supervised learning pretext tasks and regularization
  - Why needed here: CARP creates multiple pseudo-classification tasks through random partitions as a regularization strategy. Understanding how pretext tasks regularize is essential for grasping the method's effectiveness.
  - Quick check question: Why might multiple small classification tasks be more effective than one large task for representation learning?

## Architecture Onboarding

- Component map: Input views -> ResNet50 encoder -> Projection head -> Assigner -> Random partition generator -> Consistency loss
- Critical path:
  1. Generate two views from input image
  2. Encode views through student and teacher networks
  3. Project embeddings to lower dimension
  4. Apply assigner to get prototype probabilities
  5. Create random partitions and compute consistency within blocks
  6. Compute entropy regularization
  7. Backpropagate gradients through student network only

- Design tradeoffs:
  - Number of prototypes K vs. training stability
  - Partition block size NB vs. regularization effectiveness
  - Momentum update parameter η vs. teacher stability
  - Batch size vs. signal quality for prototype assignments

- Failure signatures:
  - All views assigned to same prototype (collapse)
  - No learning signal (uniform gradients across prototypes)
  - Poor transfer performance despite stable training
  - Mode collapse in specific partitions

- First 3 experiments:
  1. Train with different numbers of prototypes (K=1024, 4096, 65536) and measure k-NN performance
  2. Vary partition block sizes (NB=128, 512, 2048) while keeping K constant
  3. Compare with and without momentum encoder using fixed partition configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the random partition strategy affect CARP's performance on dense prediction tasks like object detection and segmentation?
- Basis in paper: The paper notes that CARP's representations do not transfer well to dense prediction tasks, hypothesizing this is due to ConvNet architectural characteristics and the classification-like loss function.
- Why unresolved: The paper only provides preliminary results on Pascal VOC object detection and does not explore segmentation tasks or alternative architectural designs that might improve dense prediction performance.
- What evidence would resolve it: Experiments comparing CARP with architectural modifications (e.g., retaining spatial information, different loss functions) on multiple dense prediction benchmarks like COCO detection/segmentation would clarify the limitations and potential improvements.

### Open Question 2
- Question: What is the optimal trade-off between the number of prototypes (K) and partition block size (NB) for different dataset characteristics and batch sizes?
- Basis in paper: The ablation studies show performance varies with K and NB, but do not systematically explore how these interact with dataset complexity or batch size variations.
- Why unresolved: The paper establishes that both K and NB matter but does not provide a principled method for selecting these hyperparameters based on dataset properties or computational constraints.
- What evidence would resolve it: A comprehensive study varying K, NB, batch size, and dataset characteristics (e.g., number of classes, data diversity) would reveal optimal configurations and scaling relationships.

### Open Question 3
- Question: How does CARP's random partition strategy compare to other regularization techniques for avoiding trivial solutions in self-supervised clustering?
- Basis in paper: The paper claims random partitions provide regularization that improves stability and performance, but does not directly compare against other regularization methods like sharpening, momentum updates, or architectural constraints.
- Why unresolved: While the paper demonstrates CARP's effectiveness, it does not isolate the contribution of random partitions versus other components of the architecture.
- What evidence would resolve it: Ablation studies comparing CARP's random partitions against alternative regularization strategies (e.g., sharpening schedules, different momentum update rules, or architectural constraints) would clarify the unique contribution of the random partition approach.

## Limitations

- Limited ablation studies on the impact of random partitions versus other regularization techniques
- Lack of exploration of optimal hyperparameter scaling relationships for K and NB
- Preliminary results on dense prediction tasks with no exploration of architectural modifications

## Confidence

- High confidence: CARP achieves strong empirical performance across multiple benchmarks, with clear improvements over baseline methods in k-NN accuracy and clustering tasks
- Medium confidence: The random partition mechanism provides regularization benefits, though direct ablation evidence is limited
- Low confidence: The implicit temperature learning advantage is definitively established, as comparisons are primarily relative to MoCo-v2 without exploring alternative temperature scheduling approaches

## Next Checks

1. **Partition ablation study**: Run experiments varying partition block sizes (NB=128, 512, 2048) while measuring both training stability and final k-NN performance to isolate the regularization effect

2. **Explicit temperature comparison**: Implement a version of CARP with manually tuned temperature schedules and compare against the implicit learning approach to quantify the claimed advantage

3. **Entropy decay sensitivity**: Systematically vary the entropy decay rate λe across orders of magnitude to determine the robustness of this component to hyperparameter choices