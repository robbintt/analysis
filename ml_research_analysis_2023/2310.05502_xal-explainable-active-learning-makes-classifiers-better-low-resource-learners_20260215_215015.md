---
ver: rpa2
title: 'XAL: EXplainable Active Learning Makes Classifiers Better Low-resource Learners'
arxiv_id: '2310.05502'
source_url: https://arxiv.org/abs/2310.05502
tags:
- data
- explanations
- sentence
- learning
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: XAL integrates an explanation-generation decoder into active learning
  for low-resource text classification. During training, it jointly optimizes a bidirectional
  encoder for classification and a unidirectional decoder for generating explanations,
  using a ranking loss to improve explanation scoring.
---

# XAL: EXplainable Active Learning Makes Classifiers Better Low-resource Learners

## Quick Facts
- arXiv ID: 2310.05502
- Source URL: https://arxiv.org/abs/2310.05502
- Reference count: 40
- Primary result: XAL consistently outperforms 9 strong AL baselines, requiring only 3-16% of the full training data to achieve 90% of upper-bound performance

## Executive Summary
XAL introduces an explainable active learning framework for low-resource text classification by integrating explanation generation into the active learning loop. The model combines a bidirectional encoder for classification with a unidirectional decoder for generating and scoring explanations. During training, it jointly optimizes classification, explanation generation, and ranking losses. For data selection, it combines predictive entropy and explanation scores to identify informative samples. Experiments across six tasks show XAL significantly outperforms existing AL baselines while using far less labeled data.

## Method Summary
XAL uses a pre-trained encoder-decoder architecture (FLAN-T5-Large) where the encoder performs classification and the decoder generates explanations. The model is trained on labeled data augmented with diverse explanations generated by LLMs (ChatGPT/GPT-4) - both reasonable explanations with correct labels and unreasonable ones with incorrect labels. A ranking loss optimizes the decoder's ability to score explanations. During active learning, predictive entropy from the encoder and explanation scores from the decoder are combined (weighted by λ) to rank and select the most informative unlabeled instances for annotation.

## Key Results
- XAL consistently outperforms 9 strong AL baselines across six classification tasks
- Achieves 90% of upper-bound performance using only 3-16% of the full training data
- Ablation studies confirm effectiveness of each component (ranking loss, explanation integration)
- Human evaluation shows generated explanations align well with model predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The explanation generation and scoring module helps the model understand causal relationships between inputs and labels, improving generalization
- Mechanism: The decoder generates and scores explanations expressing the model's reasoning for predictions. A ranking loss differentiates between reasonable and unreasonable explanations, encouraging focus on causal information
- Core assumption: Explanations capture meaningful causal relationships relevant to the classification task
- Evidence anchors:
  - [abstract]: "Inspired by the cognitive processes in which humans deduce and predict through causal information, we take an initial attempt towards integrating rationales into AL and propose a novel Explainable Active Learning framework (XAL) for low-resource text classification, which aims to encourage classifiers to justify their inferences and delve into unlabeled data for which they cannot provide reasonable explanations."
  - [section 2.2]: "To make the decoder a good scorer to rank the reasonable and unreasonable explanations, we additionally adopt a ranking loss to optimize the decoder."

### Mechanism 2
- Claim: Combining predictive entropy and explanation scores for data selection leads to more informative samples being chosen
- Mechanism: Both predictive entropy (uncertainty) and explanation scores are calculated for each unlabeled instance, with a weighted sum used to rank and select samples
- Core assumption: These metrics capture different aspects of informativeness, and their combination improves sample selection
- Evidence anchors:
  - [abstract]: "During the selection of unlabeled data, the predicted uncertainty of the encoder and the explanation score of the decoder complement each other as the final metric to acquire informative data."
  - [section 2.3]: "During the selection of unlabeled data, we amalgamate the predictive uncertainty of the encoder and the explanation score of the decoder to rank unlabeled data."

### Mechanism 3
- Claim: Using LLMs to generate diverse explanations eliminates need for additional human annotation
- Mechanism: ChatGPT/GPT-4 generates reasonable explanations for correct labels and unreasonable ones for incorrect labels, providing diverse training data without human effort
- Core assumption: LLMs can generate relevant, meaningful explanations for the classification task
- Evidence anchors:
  - [section 2.4]: "With the advancement of LLMs, previous work has shown that LLMs are good at reasoning (Bang et al., 2023; Rajasekharan et al., 2023). Inspired by these studies, we take the LLMs, such as ChatGPT and GPT4, as the teacher models, and query them to generate explanations for each selected labeled data, eliminating the annotation cost of human labor."

## Foundational Learning

- Concept: Active Learning
  - Why needed here: XAL is an active learning framework that efficiently selects most informative unlabeled data for annotation
  - Quick check question: What is the main goal of active learning, and how does it differ from traditional supervised learning?

- Concept: Text Classification
  - Why needed here: XAL applies to text classification tasks where labels are assigned to text
  - Quick check question: What are common challenges in text classification, especially in low-resource settings?

- Concept: Explanation Generation
  - Why needed here: XAL uses explanation generation to encourage focus on causal relationships and justify inferences
  - Quick check question: How can explanation generation improve interpretability and trustworthiness of ML models?

## Architecture Onboarding

- Component map:
  Bidirectional encoder (classification) -> Unidirectional decoder (explanation generation/scoring) -> Ranking loss (optimizes explanation scoring) -> ChatGPT/GPT-4 (generates diverse explanations) -> Data selection module (combines predictive entropy + explanation scores)

- Critical path:
  1. Train encoder-decoder model on labeled data with diverse LLM-generated explanations
  2. Calculate predictive entropy and explanation scores for each unlabeled instance
  3. Rank and select most informative samples using weighted sum
  4. Annotate selected samples and add to labeled set
  5. Repeat training with newly labeled data for multiple iterations

- Design tradeoffs:
  - Decoder for explanation generation increases model complexity and training time vs encoder-only classifier
  - LLM reliance may introduce biases/inconsistencies if explanations are poor quality
  - Weighting parameter choice for combining metrics impacts performance

- Failure signatures:
  - Performance doesn't improve over iterations → ineffective data selection or meaningless explanations
  - Model consistently selects same sample types → low diversity in selection
  - Explanations consistently irrelevant/poor quality → ranking loss ineffective or unsuitable LLMs

- First 3 experiments:
  1. Train XAL on small labeled dataset, evaluate performance on held-out test after few AL iterations
  2. Compare XAL with and without ranking loss to assess impact on explanation scoring ability
  3. Analyze diversity of selected samples over iterations to ensure no selection bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does choice of pre-trained language model affect XAL performance?
- Basis in paper: [inferred] Uses FLAN-T5-Large but doesn't explore impact of different models
- Why unresolved: No comparison across different pre-trained models provided
- What evidence would resolve it: Experimental results comparing XAL using BERT, GPT-3, Llama etc. on same tasks

### Open Question 2
- Question: Can XAL be extended to multi-label classification tasks?
- Basis in paper: [inferred] Focuses on single-label classification, doesn't discuss multi-label scenarios
- Why unresolved: No exploration or discussion of XAL applicability to multi-label classification
- What evidence would resolve it: Experimental results applying XAL to multi-label tasks and comparing with single-label scenarios

### Open Question 3
- Question: How sensitive is XAL to quality and diversity of generated explanations?
- Basis in paper: [explicit] Performance depends on LLM-generated explanation quality
- Why unresolved: No quantification of explanation quality impact or methods to improve generation
- What evidence would resolve it: Systematic experiments varying explanation quality/diversity (different prompts, LLMs, human annotators) and measuring impact on XAL performance

## Limitations

- Heavy reliance on LLM-generated explanations may introduce biases or inconsistencies across tasks
- Critical weighting parameter λ for combining metrics lacks systematic sensitivity analysis
- Computational overhead of explanation generation module not quantified, potentially limiting practical deployment

## Confidence

- **High confidence**: Core architectural design combining encoder-decoder for joint classification and explanation, and active learning loop structure
- **Medium confidence**: Effectiveness of ranking loss for improving explanation quality, based on ablation studies
- **Medium confidence**: Overall performance improvements over baselines, though specific component contributions are harder to disentangle

## Next Checks

1. Conduct controlled experiment varying λ parameter across wider range to determine optimal values for different tasks and assess sensitivity
2. Perform manual quality assessment of random sample of model-generated vs LLM-generated explanations to evaluate learning of meaningful reasoning patterns
3. Implement and test variant using only predictive entropy for data selection (without explanations) on same tasks to quantify marginal benefit of explanation component