---
ver: rpa2
title: The Role of Masking for Efficient Supervised Knowledge Distillation of Vision
  Transformers
arxiv_id: '2302.10494'
source_url: https://arxiv.org/abs/2302.10494
tags:
- teacher
- student
- distillation
- maskedkd
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high computational cost of knowledge distillation
  when using large vision transformers as teachers. The proposed method, MaskedKD,
  reduces this cost by masking a fraction of input tokens shown to the teacher, guided
  by the student's last-layer attention scores.
---

# The Role of Masking for Efficient Supervised Knowledge Distillation of Vision Transformers

## Quick Facts
- arXiv ID: 2302.10494
- Source URL: https://arxiv.org/abs/2302.10494
- Reference count: 14
- Key outcome: Up to 50% of teacher tokens can be masked without performance loss, reducing teacher FLOPs by over 25% and total distillation FLOPs by around 28%.

## Executive Summary
This paper introduces MaskedKD, a method to reduce the computational cost of knowledge distillation when using large vision transformers as teachers. By masking a fraction of input tokens to the teacher—guided by the student's last-layer attention scores—the method simplifies the teacher's task and improves student performance. Experiments show that masking low-attention student tokens can save up to 50% of teacher FLOPs without accuracy loss, with benefits extending beyond vision to audio transformers.

## Method Summary
MaskedKD reduces knowledge distillation cost by masking teacher input tokens based on student attention. During training, the student processes the full image, and its last-layer attention scores identify patches least important for classification. These low-attention patches are masked before feeding to the teacher, reducing its computational load. The student is trained with a combined cross-entropy and distillation loss. This approach is effective across ViT variants and even extends to audio transformers, with masking rates up to 50% yielding significant FLOPs savings without accuracy drop.

## Key Results
- Masking up to 50% of teacher tokens reduces teacher FLOPs by over 25% and total distillation FLOPs by around 28%.
- Masking based on student attention is more effective than random or other criteria.
- The approach generalizes to audio transformers, demonstrating broad applicability.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Masking low-attention student tokens simplifies teacher's task, improving student performance.
- Mechanism: The student's last-layer attention scores identify image patches least critical for its own prediction. Masking these patches before feeding to the teacher reduces teacher complexity, aligning teacher and student outputs more closely.
- Core assumption: The student's attention reflects patch importance for the final classification.
- Evidence anchors:
  - [abstract] "masking patches with the lowest student attention scores is highly effective, saving up to 50% of teacher FLOPs without any drop in student accuracy"
  - [section] "masking by the score aligns the teacher and student predictions (Figure 5)... masking may even boost the student performances under such setups"
  - [corpus] Weak/no direct evidence; based on internal attention mechanism claims.
- Break condition: If student attention does not correlate with patch importance (e.g., noisy attention maps), masking would degrade performance.

### Mechanism 2
- Claim: Masking creates a curriculum where teacher supervision starts easier and becomes harder.
- Mechanism: Early in training, masking removes difficult context patches, letting the student learn simpler concepts. Later, fewer patches are masked, increasing task difficulty gradually.
- Core assumption: Progressive masking difficulty benefits learning stability and convergence.
- Evidence anchors:
  - [abstract] "the student-guided masking provides a good curriculum to the student, making teacher supervision easier to follow during the early stage and challenging in the later stage"
  - [section] No explicit empirical evidence provided; claim is theoretical.
  - [corpus] No direct supporting papers cited; assumption inferred from curriculum learning literature.
- Break condition: If masking rate is too aggressive or poorly timed, it may prevent learning complex features altogether.

### Mechanism 3
- Claim: Masking reduces computational cost proportionally to number of tokens removed.
- Mechanism: Each ViT block's FLOPs scale with number of tokens (O(LNM(M+N))). Removing tokens reduces self-attention and projection FLOPs, lowering total cost.
- Core assumption: FLOPs reduction translates directly to latency and throughput gains in practice.
- Evidence anchors:
  - [section] "by halving the number of teacher input tokens, we get ~55% reduction in teacher FLOPs and ~53% reduction in the teacher forward latency"
  - [section] Table 5 gives FLOPs formulas showing linear scaling with tokens.
  - [corpus] No external validation; based on internal ViT architecture analysis.
- Break condition: If memory bandwidth or other bottlenecks dominate, FLOPs reduction may not yield proportional speed gains.

## Foundational Learning

- Concept: Vision Transformer (ViT) architecture
  - Why needed here: Understanding how ViT processes image patches as tokens and uses multi-head self-attention is essential to grasp how masking affects computation.
  - Quick check question: In ViT, what determines the total number of input tokens, and how are they generated from an image?

- Concept: Knowledge distillation loss and temperature scaling
  - Why needed here: MaskedKD builds on standard distillation; knowing how soft targets and temperature affect learning is key to interpreting results.
  - Quick check question: How does the temperature parameter in distillation affect the smoothness of the teacher's probability distribution?

- Concept: Attention mechanisms in transformers
  - Why needed here: The method relies on interpreting student attention scores as saliency indicators; understanding attention computation is necessary.
  - Quick check question: In ViT, how is the attention score between the class token and a patch token computed?

## Architecture Onboarding

- Component map:
  - Student ViT: full image input → forward → last-layer attention scores
  - Masking module: selects top-k patches by mean attention score → generates masked image
  - Teacher ViT: masked image input → forward → predictions
  - Loss: combines cross-entropy (student vs ground truth) + distillation loss (student vs teacher predictions)

- Critical path:
  1. Student forward pass (full image)
  2. Attention score extraction (class-to-patch, mean over heads)
  3. Masking (keep top-k patches)
  4. Teacher forward pass (masked image)
  5. Loss computation and backpropagation

- Design tradeoffs:
  - Masking rate vs. student performance: higher masking saves compute but risks accuracy drop.
  - Student size vs. teacher savings: larger teachers benefit more from masking.
  - Masking consistency: masking student's own input may hurt performance vs. full-image student forward.

- Failure signatures:
  - Student accuracy drops sharply as masking rate increases beyond optimal point.
  - Teacher accuracy drops significantly with masking, indicating loss of important context.
  - MaskedKD latency worse than vanilla KD due to pipeline stalls (if not using GPipe-style parallelism).

- First 3 experiments:
  1. Verify masking threshold: sweep masking rate (0% to 90%) and measure student accuracy drop vs. FLOPs saved.
  2. Compare masking criteria: random vs. min-k vs. top-k attention vs. class-attention-only vs. patch-to-patch attention.
  3. Test on other domains: apply MaskedKD to audio transformers (AST) and measure cost vs. accuracy tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can MaskedKD improve student performance by increasing both teacher size and masking rate for a fixed compute budget?
- Basis in paper: Explicit - "One interesting point to note is that these two patterns—reduction to less than 50% and occasional performance gain—is more pronounced when the teacher model is large... This motivates the following question: Given the fixed compute, can we improve the student performance by increasing both the teacher size and the masking rate?"
- Why unresolved: The paper only conducted a pilot experiment with a negative initial result. More systematic experiments are needed across different teacher-student pairs and masking rates.
- What evidence would resolve it: Controlled experiments showing student accuracy as a function of teacher size and masking rate, while keeping total compute fixed. A positive result would show accuracy gains with larger teachers and higher masking rates.

### Open Question 2
- Question: Does MaskedKD work equally well for other transformer architectures beyond vision transformers, such as language models?
- Basis in paper: Explicit - "A major limitation of the present work is that the method is specialized to the transformer-based models... Generalizing the current framework to cover a broader range of networks including, e.g., convolutional networks or recurrent neural networks, is an important future research direction."
- Why unresolved: The paper only tested MaskedKD on vision transformers and one audio transformer. No experiments were done on language models or other transformer variants.
- What evidence would resolve it: Applying MaskedKD to language model distillation tasks (e.g., BERT distillation) and measuring performance and compute savings compared to vanilla KD.

### Open Question 3
- Question: What is the optimal masking rate for MaskedKD across different domains and tasks?
- Basis in paper: Explicit - "Lastly, we note that MaskedKD is in need of a mechanism to efficiently find the optimal number of tokens to use for the teacher input... While we empirically find that '50%' may be a working rule-of-thumb for the tasks in the vision domain, the empirical results on the audio data suggest that the optimal fraction may differ from a domain to domain."
- Why unresolved: The paper only tested a few fixed masking rates (30-80% of patches) and found 50% worked well for vision but not for audio. No systematic study of optimal masking rates across domains was conducted.
- What evidence would resolve it: A comprehensive study varying masking rates (e.g., 10-90%) across multiple domains (vision, audio, language) and tasks, identifying optimal rates for each scenario.

## Limitations
- Experimental validation is limited to ImageNet-1k and a narrow set of ViT variants; results may not generalize to other datasets or domains.
- Real-world latency and throughput gains depend on hardware and software implementation details not fully explored.
- The assumption that student attention scores reliably reflect patch importance may not hold in all cases, especially with noisy or poorly trained attention maps.
- Curriculum learning benefits are theorized but not empirically validated with ablation studies or learning curves.

## Confidence
- High confidence: The FLOPs reduction mechanism and its correlation with token masking (Mechanism 3).
- Medium confidence: The superiority of student-guided masking over random masking and other criteria (Mechanism 1).
- Low confidence: The curriculum learning benefits and the generalizability of attention-based masking across domains (Mechanism 2).

## Next Checks
1. Conduct an ablation study varying masking rates and criteria across multiple datasets (e.g., CIFAR-100, Food-101) to verify the robustness of student attention-based masking.
2. Measure real-world latency and throughput on target hardware (GPU/TPU) to confirm that FLOPs savings translate into practical speedups.
3. Visualize and analyze student attention maps to confirm they correlate with patch importance, especially in cases where masking leads to performance degradation.