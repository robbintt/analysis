---
ver: rpa2
title: Can Generative Large Language Models Perform ASR Error Correction?
arxiv_id: '2307.04172'
source_url: https://arxiv.org/abs/2307.04172
tags:
- error
- correction
- chatgpt
- n-best
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using ChatGPT for ASR error correction
  in both zero-shot and 1-shot settings. The authors propose unconstrained and N-best
  constrained approaches, using ASR N-best hypotheses as input.
---

# Can Generative Large Language Models Perform ASR Error Correction?

## Quick Facts
- arXiv ID: 2307.04172
- Source URL: https://arxiv.org/abs/2307.04172
- Reference count: 0
- Key result: ChatGPT reduces ASR WER by up to 25% on out-of-domain data

## Executive Summary
This paper investigates using ChatGPT for automatic speech recognition (ASR) error correction in both zero-shot and 1-shot settings. The authors propose unconstrained and N-best constrained approaches, using ASR N-best hypotheses as input. Experiments on Conformer-Transducer and Whisper ASR models across multiple datasets show that ChatGPT can significantly reduce ASR word error rates, with up to 25% WER reduction on out-of-domain data. The best results are achieved with 1-shot learning and constrained decoding using the closest matching hypothesis from the N-best list. The paper also analyzes Whisper's N-best output, finding it has less diversity than transducer models, which impacts ChatGPT's performance.

## Method Summary
The authors apply ChatGPT to ASR error correction by inputting N-best hypotheses from beam search decoding of ASR models. They evaluate both zero-shot and 1-shot settings, using either unconstrained generation or constrained approaches where ChatGPT selects or maps to the closest hypothesis from the N-best list. The method is tested on Conformer-Transducer and Whisper models across LibriSpeech, TED-LIUM3, and Artie datasets, comparing performance against baseline ASR systems and N-best T5 error correction models.

## Key Results
- ChatGPT reduces WER by up to 25% on out-of-domain data compared to baseline ASR
- 1-shot learning with constrained decoding achieves the best performance (6.24 WER on test set)
- Whisper's N-best output has less diversity than transducer models, limiting ChatGPT's correction effectiveness
- Constrained approaches outperform unconstrained generation by leveraging N-best hypothesis diversity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT can correct ASR errors by leveraging N-best hypotheses as context
- Mechanism: The model uses multiple ASR hypotheses to identify likely errors and generate corrected transcriptions by comparing hypothesis variations
- Core assumption: ChatGPT's pre-trained knowledge allows it to detect and correct errors without domain-specific fine-tuning
- Evidence anchors:
  - [abstract] "we propose both unconstrained and constrained, where a member of the N-best list is selected, approaches"
  - [section 3.1] "Instead of the 1-best ASR transcription, we input the N-best list obtained from the beam search decoding of the ASR model to ChatGPT"
  - [corpus] Weak - neighboring papers focus on similar N-best approaches but don't directly validate this mechanism
- Break condition: If N-best hypotheses lack diversity (e.g., Whisper's N-best has high repetition), ChatGPT performance degrades significantly

### Mechanism 2
- Claim: 1-shot learning improves ChatGPT's error correction accuracy
- Mechanism: Providing an example of input and desired output helps ChatGPT understand the task format and constraints
- Core assumption: In-context learning allows ChatGPT to adapt to ASR error patterns without explicit training
- Evidence anchors:
  - [section 3.1] "Additionally, zero and 1-shot settings are evaluated" and shows WER improvement from 6.64 to 6.29
  - [section 4.2] "When we further apply the closest mapping in the 1-shot setting, WER on the test set is reduced to 6.24"
  - [corpus] Moderate - neighboring papers explore similar in-context learning approaches for speech tasks
- Break condition: If the example provided is not representative of the target domain's error patterns

### Mechanism 3
- Claim: Constrained decoding improves accuracy by selecting from N-best hypotheses rather than generating from scratch
- Mechanism: ChatGPT either selects the best hypothesis (selective approach) or finds the closest match to its generated output (closest mapping)
- Core assumption: The N-best list contains the correct answer, and constraining search space reduces hallucination
- Evidence anchors:
  - [section 3.2] "Results in [17, 18] suggest that constraining the decoding space to the given N-best list leads to performance gain"
  - [section 4.2] "When we further apply the closest mapping in the 1-shot setting, WER on the test set is reduced to 6.24"
  - [corpus] Moderate - neighboring papers validate constrained approaches in similar ASR error correction settings
- Break condition: If N-best hypotheses are too similar (high repetition) or contain systematic errors

## Foundational Learning

- Concept: Beam search decoding and N-best hypotheses generation
  - Why needed here: Understanding how ASR systems generate multiple hypotheses is crucial for leveraging them as input to ChatGPT
  - Quick check question: What is the relationship between beam width and N-best list diversity?

- Concept: Levenshtein distance and string similarity metrics
  - Why needed here: Used in "closest mapping" approach to find the best hypothesis match
  - Quick check question: How would you compute the Levenshtein distance between two strings of different lengths?

- Concept: Word Error Rate (WER) calculation and interpretation
  - Why needed here: Primary evaluation metric for measuring error correction performance
  - Quick check question: Given substitutions=5, deletions=3, insertions=2, and reference length=100, what is the WER?

## Architecture Onboarding

- Component map:
  - ASR system (Transducer or Whisper) → Beam search decoder → N-best hypotheses
  - N-best hypotheses → ChatGPT API (with prompt engineering) → Error-corrected output
  - Optional: Levenshtein distance computation for constrained decoding

- Critical path:
  1. ASR decoding with beam search (beam size 10)
  2. Extract top N hypotheses (typically N=5)
  3. Format hypotheses with appropriate tags/prompt
  4. Send to ChatGPT API with selected approach
  5. Post-process output (extract selected hypothesis or use as-is)

- Design tradeoffs:
  - Larger N increases context but may introduce more noise and API costs
  - Unconstrained generation offers more flexibility but higher hallucination risk
  - Constrained approaches limit output space but depend on N-best quality

- Failure signatures:
  - High deletion rates suggest ChatGPT is removing words it considers disfluent
  - Poor performance on out-of-domain data indicates limited generalization
  - Performance degradation when using Whisper N-best suggests diversity issues

- First 3 experiments:
  1. Compare 0-shot vs 1-shot performance on a held-out validation set
  2. Test different N values (1, 3, 5, 10) to find optimal input size
  3. Evaluate selective vs closest mapping constrained approaches on the same data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt formulations impact ChatGPT's performance on ASR error correction across various domains?
- Basis in paper: [explicit] The paper experiments with multiple prompt designs (zero-shot unconstrained, zero-shot selective, 1-shot unconstrained) and observes significant performance differences.
- Why unresolved: While the paper compares several prompt formulations, it does not conduct a systematic exploration of prompt variations or their impact on domain-specific performance.
- What evidence would resolve it: A comprehensive ablation study varying prompt wording, structure, and examples across multiple domains, measuring WER improvements and qualitative analysis of corrections made.

### Open Question 2
- Question: What is the optimal N-best list size for maximizing ChatGPT's ASR error correction performance, and how does this vary by ASR architecture?
- Basis in paper: [explicit] The paper performs limited ablation on N-best list size (1, 3, 5, 10) and observes that performance varies with N, but does not explore the full parameter space or architectural differences.
- Why unresolved: The study only tests up to 10 hypotheses and does not examine how optimal N might differ between transducer and encoder-decoder architectures like Whisper.
- What evidence would resolve it: Extensive experiments testing N-best sizes from 1 to 50+ for both Conformer-Transducer and Whisper models across multiple datasets, identifying optimal N for each architecture.

### Open Question 3
- Question: What is the computational efficiency trade-off between ChatGPT-based error correction and traditional fine-tuned models in production systems?
- Basis in paper: [explicit] The paper mentions that traditional error correction models require computationally intensive training and are bound to specific ASR systems, while ChatGPT offers a "training-free" alternative.
- Why unresolved: The paper does not provide latency measurements, cost comparisons, or throughput analysis for ChatGPT-based correction versus fine-tuned models in real-time applications.
- What evidence would resolve it: Head-to-head benchmarks measuring inference time, API costs, and memory usage for ChatGPT correction versus fine-tuned T5 models processing identical datasets, including batch processing scenarios.

## Limitations
- Domain generalization: Performance degrades significantly on out-of-domain data (TED-LIUM3, Artie datasets)
- N-best quality dependency: Effectiveness limited by diversity of ASR hypotheses, particularly for Whisper model
- Resource requirements: Significant computational overhead and API costs for processing large datasets

## Confidence
- High Confidence: Core finding that ChatGPT reduces WER on ASR outputs (up to 25% reduction on out-of-domain data)
- Medium Confidence: Superiority of 1-shot learning over zero-shot approaches
- Medium Confidence: Constrained decoding improving accuracy compared to unconstrained generation

## Next Checks
1. Quantify relationship between N-best hypothesis diversity and ChatGPT's correction performance across different ASR models and domains
2. Measure trade-off between WER reduction and computational/API costs for ChatGPT-based error correction versus traditional methods
3. Evaluate whether other generative LLMs (e.g., Claude, Llama) achieve similar or better error correction performance with potentially lower costs or better domain adaptation