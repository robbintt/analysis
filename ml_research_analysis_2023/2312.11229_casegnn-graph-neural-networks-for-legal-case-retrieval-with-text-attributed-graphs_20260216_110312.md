---
ver: rpa2
title: 'CaseGNN: Graph Neural Networks for Legal Case Retrieval with Text-Attributed
  Graphs'
arxiv_id: '2312.11229'
source_url: https://arxiv.org/abs/2312.11229
tags:
- case
- legal
- graph
- retrieval
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CaseGNN, a graph neural network-based model
  for legal case retrieval that addresses two key challenges: legal structural information
  neglect and lengthy legal text limitation. The proposed method constructs a Text-Attributed
  Case Graph (TACG) from legal cases using information extraction techniques, and
  then employs an Edge Graph Attention Layer (EdgeGAT) to learn case representations.'
---

# CaseGNN: Graph Neural Networks for Legal Case Retrieval with Text-Attributed Graphs

## Quick Facts
- **arXiv ID**: 2312.11229
- **Source URL**: https://arxiv.org/abs/2312.11229
- **Reference count**: 40
- **Key outcome**: CaseGNN significantly outperforms state-of-the-art legal case retrieval methods, achieving up to 35.5% precision@5 and 42.1% recall@5 on COLIEE 2022.

## Executive Summary
This paper introduces CaseGNN, a graph neural network-based model for legal case retrieval that addresses two key challenges: legal structural information neglect and lengthy legal text limitation. The proposed method constructs a Text-Attributed Case Graph (TACG) from legal cases using information extraction techniques, and then employs an Edge Graph Attention Layer (EdgeGAT) to learn case representations. The model is trained with a contrastive loss incorporating easy and hard negative sampling. Extensive experiments on COLIEE 2022 and COLIEE 2023 datasets demonstrate that CaseGNN significantly outperforms state-of-the-art legal case retrieval methods.

## Method Summary
CaseGNN constructs a Text-Attributed Case Graph (TACG) by extracting legal entities and their relationships from case documents, representing them as nodes and edges. An Edge Graph Attention Layer (EdgeGAT) aggregates information from neighboring nodes and edges to capture legal context. The model uses virtual global nodes to represent overall case semantics and is trained with a contrastive loss that incorporates both easy and hard negative sampling. This approach avoids the lengthy text limitations of BERT-based models while preserving legal structural information that is typically lost when encoding unstructured text directly.

## Key Results
- Achieved up to 35.5% precision@5 and 42.1% recall@5 on COLIEE 2022 dataset
- Outperformed state-of-the-art methods including LEGAL-BERT, MonoT5, SAILER, and PromptCase
- Demonstrated effectiveness of incorporating legal structural information and avoiding lengthy text limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph neural networks can effectively incorporate legal structural information that is lost when encoding unstructured legal text directly.
- Mechanism: The paper constructs a Text-Attributed Case Graph (TACG) where legal entities become nodes and their relationships become edges. Edge Graph Attention Layers (EdgeGAT) aggregate information from neighboring nodes and edges to capture legal context.
- Core assumption: Legal cases contain important structural relationships between entities that improve retrieval accuracy when properly modeled.
- Evidence anchors: [abstract] "Legal structural information neglect. Previous neural legal case retrieval models mostly encode the unstructured raw text of case into a case representation, which causes the lack of important legal structural information in a case and leads to poor case representation" [section] "To effectively utilise the legal structural information during encoding, a case is firstly converted into a Text-Attributed Case Graph (TACG)"
- Break condition: If the information extraction fails to identify meaningful legal entities or relationships, the graph structure becomes meaningless and EdgeGAT cannot learn useful representations.

### Mechanism 2
- Claim: Text-attributed graphs avoid the lengthy text limitation problem of BERT-based models while preserving legal context.
- Mechanism: Instead of feeding entire long legal documents to BERT, the paper extracts individual sentences and entities, encodes them separately, and builds a graph where each node contains a manageable text snippet. This avoids truncation while maintaining global context through the graph structure.
- Core assumption: Legal meaning can be preserved when breaking long documents into smaller text units connected by legal relationships.
- Evidence anchors: [abstract] "Lengthy legal text limitation. When using the powerful BERT-based models, there is a limit of input text lengths, which inevitably requires to shorten the input via truncation or division with a loss of legal context information" [section] "Since the text attributes in the case graph come from individual sentences, the restriction of using language models is further avoided without losing the legal context"
- Break condition: If the legal context requires understanding long-range dependencies that span multiple extracted entities, breaking the text into smaller units may lose crucial information.

### Mechanism 3
- Claim: Contrastive learning with hard negative sampling improves legal case retrieval performance.
- Mechanism: The model is trained with a contrastive loss that pulls relevant cases closer and pushes irrelevant cases apart. Hard negative samples (cases with high BM25 scores but not actually relevant) provide stronger training signals than random negatives.
- Core assumption: Hard negatives are more informative for training because they represent cases that appear similar but are actually irrelevant.
- Evidence anchors: [abstract] "The CaseGNN model is optimised with a carefully designed contrastive loss with easy and hard negative sampling" [section] "For the hard negative samples, it is designed to make use of harder samples to effectively guide the training. Therefore, hard negative samples are sampled based on the BM25 relevance score"
- Break condition: If the hard negative sampling strategy fails to identify truly challenging negatives, the training signal becomes noisy and performance degrades.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their message passing mechanism
  - Why needed here: Understanding how EdgeGAT aggregates node and edge features is crucial for modifying or debugging the model
  - Quick check question: How does the EdgeGAT layer differ from standard GAT in handling edge features?

- Concept: Information extraction techniques (NER and Relation Extraction)
  - Why needed here: The TACG construction depends on accurately identifying legal entities and their relationships from text
  - Quick check question: What happens to the graph quality if the relation extraction tool misidentifies relationships between entities?

- Concept: Contrastive learning and negative sampling strategies
  - Why needed here: The training objective relies on distinguishing relevant from irrelevant cases, with hard negatives providing stronger signals
  - Quick check question: How would changing the ratio of easy to hard negative samples affect the training dynamics?

## Architecture Onboarding

- Component map: Input → Information Extraction → TACG Construction → EdgeGAT Layers → Readout Function → Contrastive Loss → Output
- Critical path: TACG Construction → EdgeGAT Layers → Readout Function → Contrastive Loss (these components must work together for the model to function)
- Design tradeoffs: Using graph structure trades computational complexity for better incorporation of legal relationships; using hard negatives trades training stability for stronger learning signals
- Failure signatures: 
  - Poor retrieval performance despite high training accuracy (overfitting to training negatives)
  - Degradation when input text length exceeds extraction tool capabilities (information loss)
  - Unstable training with inappropriate temperature values in contrastive loss
- First 3 experiments:
  1. Verify TACG construction by visualizing extracted entities and relationships for sample cases
  2. Test EdgeGAT with synthetic graphs to ensure it properly aggregates features
  3. Evaluate contrastive loss with different negative sampling strategies on a small validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CaseGNN vary when using different information extraction tools or methods for legal fact and legal issue extraction?
- Basis in paper: [inferred] The paper uses PromptCase framework for information extraction but does not explore the impact of using alternative tools.
- Why unresolved: The paper only uses one information extraction method (PromptCase) without comparing to other potential methods.
- What evidence would resolve it: Comparative experiments using different information extraction tools (e.g., spaCy, Stanford OpenIE) and their impact on CaseGNN's performance metrics.

### Open Question 2
- Question: How does CaseGNN's performance scale with larger legal datasets or different legal domains beyond federal Canadian court cases?
- Basis in paper: [explicit] The paper mentions that CaseGNN can be "easily extended to different languages with the corresponding information extraction tools and LMs" but does not validate this claim.
- Why unresolved: Experiments are limited to COLIEE 2022 and 2023 datasets from Canadian federal court.
- What evidence would resolve it: Testing CaseGNN on larger datasets, different legal systems (e.g., European, Asian), and evaluating performance degradation or improvements.

### Open Question 3
- Question: What is the impact of different graph readout functions (e.g., sum pooling, attention-based readout) on CaseGNN's performance compared to the average pooling used in the paper?
- Basis in paper: [inferred] The paper uses average pooling for readout but mentions "Readout is an aggregation function" without exploring alternatives.
- Why unresolved: Only average pooling is evaluated as the readout function.
- What evidence would resolve it: Comparative experiments using different readout functions and their effect on precision, recall, and other metrics.

### Open Question 4
- Question: How does incorporating edge features (e.g., relation types, weights) into the EdgeGAT layer affect performance compared to the current simplified version?
- Basis in paper: [explicit] The paper states "Specifically, the same edge features are reused to make the EdgeGAT simpler" and suggests "Further model development of updating edge can be designed."
- Why unresolved: The current EdgeGAT implementation simplifies edge features, potentially limiting performance.
- What evidence would resolve it: Experiments comparing EdgeGAT with and without edge feature updates, measuring the impact on retrieval accuracy.

## Limitations

- The effectiveness of TACG construction heavily depends on the quality of information extraction tools, which are not evaluated in detail
- The model's reliance on manually defined entity types and relations may limit generalizability to different legal domains or jurisdictions
- The computational complexity of constructing and processing TACGs could be prohibitive for large-scale deployment

## Confidence

- **High Confidence**: The superiority of CaseGNN over baseline methods on COLIEE datasets (verified through reported metrics and experimental setup)
- **Medium Confidence**: The mechanism by which graph structure captures legal relationships (plausible but lacks ablation studies showing isolated effect)
- **Low Confidence**: The claimed avoidance of lengthy text limitations (no direct comparison with BERT on full documents, making this claim difficult to verify)

## Next Checks

1. **Ablation Study**: Remove the graph structure and compare performance using only the text attributes to isolate the contribution of legal structural information modeling.
2. **Cross-Jurisdiction Testing**: Evaluate CaseGNN on legal case datasets from different jurisdictions or legal systems to test generalizability beyond COLIEE datasets.
3. **Computational Efficiency Analysis**: Measure and compare the computational cost of CaseGNN versus BERT-based approaches to quantify the trade-off between performance and efficiency.