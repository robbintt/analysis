---
ver: rpa2
title: The Re-Label Method For Data-Centric Machine Learning
arxiv_id: '2302.04391'
source_url: https://arxiv.org/abs/2302.04391
tags:
- data
- human
- noisy
- dataset-v1
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a data-centric method for improving deep
  learning performance by identifying and correcting noisy labels in manually labeled
  datasets. The core idea is to train a model on the noisy dataset, use its predictions
  as reference to detect potential labeling errors, and then re-label the identified
  noisy samples with human input.
---

# The Re-Label Method For Data-Centric Machine Learning

## Quick Facts
- arXiv ID: 2302.04391
- Source URL: https://arxiv.org/abs/2302.04391
- Reference count: 10
- Key outcome: Iterative re-labeling improves dev accuracy from 83.3% to 91.7% in classification and F1 from 73.7% to 88.7% in NER

## Executive Summary
This paper introduces a data-centric approach to improving deep learning performance by identifying and correcting noisy labels in manually labeled datasets. The method trains a model on noisy data, uses its predictions to flag potential labeling errors, and iteratively re-labels these samples with human input. Demonstrated across five task types including text classification, NER, object detection, sequence generation, and CTR prediction, the approach shows significant improvements in both model performance and human evaluation metrics. The method positions itself as complementary to existing techniques like RLHF by focusing on dataset quality rather than training a reward model.

## Method Summary
The Re-Label method works by first training a model on a manually labeled dataset with noisy labels. The trained model then predicts labels for all samples, and those where predictions differ from human labels are flagged as potentially noisy. Human annotators re-label these flagged samples using the model's predictions as reference, creating an updated dataset. This process iterates multiple times, with each iteration progressively refining label quality through better human understanding of the task's labeling rules and improved model predictions.

## Key Results
- Text classification dev accuracy improved from 83.3% to 91.7% to 93.8% across iterations
- Named entity recognition F1 score increased from 73.7% to 88.7%
- Human evaluation accuracy for classification improved from 88.0% to 97.2% to 97.5%
- Human evaluation precision for NER improved from 86.0% to 97.0%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model predictions can effectively identify noisy labels in human-labeled datasets
- Mechanism: The model is trained on the noisy dataset and then used to predict labels for the same data. Predictions that differ from human labels are flagged as potentially noisy
- Core assumption: The model learns the true underlying patterns and can distinguish between correct and incorrect human labels
- Evidence anchors: [abstract] "use its predictions as reference to detect potential labeling errors", [section] "If the predicted labels of dataset-v1 do not equal to the human labels of dataset-v1, we think they are the noisy data"

### Mechanism 2
- Claim: Human-in-the-loop iterative refinement progressively improves dataset quality
- Mechanism: Humans re-label flagged noisy samples with model predictions as reference, leading to better quality labels over multiple iterations
- Core assumption: Each iteration helps humans better understand the task's labeling rules and correct their own labeling patterns
- Evidence anchors: [abstract] "leverages human-in-the-loop feedback and model predictions to progressively refine label quality", [section] "The improvement reason is also based on the better and better understanding for the specific task's labeling rule/knowledge of labeling human once by once"

### Mechanism 3
- Claim: Task-specific similarity metrics can effectively identify noisy samples beyond simple label disagreement
- Mechanism: For sequence generation tasks, BLEU score is used; for object detection, bounding box distance is used to identify noisy samples
- Core assumption: Different tasks require different similarity metrics to accurately identify noisy samples
- Evidence anchors: [section] "For sequence generation, we can use BLEU score or other sequence similarity evaluation method", [section] "If the predicted bounding box of dataset-v1 is far from the human labeled bounding box of dataset-v1, we think they are the noisy data"

## Foundational Learning

- Concept: Supervised learning fundamentals and model evaluation metrics
  - Why needed here: Understanding how to train models, evaluate accuracy/F1/AUC scores, and interpret model predictions
  - Quick check question: How would you determine if a model's predictions are reliable enough to identify noisy labels?

- Concept: Statistical learning theory and the bias-variance tradeoff
  - Why needed here: The method relies on the assumption that models can learn true patterns despite noisy labels, requiring understanding of how noise affects learning
  - Quick check question: What happens to model performance when training data contains high levels of label noise?

- Concept: Human-in-the-loop machine learning systems
  - Why needed here: The iterative re-labeling process requires understanding how human feedback can improve model performance over time
  - Quick check question: What are the key considerations when designing a human-in-the-loop system for data labeling?

## Architecture Onboarding

- Component map: Dataset → Model Training → Prediction → Noise Detection → Human Re-labeling → Updated Dataset → Repeat
- Critical path: Data preparation → Model training → Prediction comparison → Human re-labeling → Evaluation
- Design tradeoffs: More iterations improve quality but increase cost; stricter noise thresholds reduce false positives but may miss actual noise
- Failure signatures: Stagnant accuracy improvements across iterations; high disagreement between model and human labels throughout; diminishing returns after 2-3 iterations
- First 3 experiments:
  1. Run initial classification task with provided dataset to verify baseline accuracy
  2. Implement noise detection using simple label mismatch and verify flagged samples
  3. Conduct one iteration of human re-labeling and measure accuracy improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the Re-Label method scale with the initial noise level in the dataset? Specifically, is there a threshold of noise beyond which the method becomes ineffective?
- Basis in paper: [inferred] The paper demonstrates improvement on datasets with unknown noise levels but does not characterize performance as a function of initial noise percentage
- Why unresolved: The paper only shows results on datasets that were presumably already somewhat clean (achieving 83.3% accuracy on dev set initially), without testing on deliberately noisy datasets with controlled noise levels
- What evidence would resolve it: Systematic experiments varying the initial percentage of noisy labels (e.g., 5%, 15%, 30%, 50%) and measuring the improvement in accuracy/F1 scores for each noise level

### Open Question 2
- Question: How does the Re-Label method compare to other data-centric approaches like pseudo-labeling, active learning, or contrastive learning in terms of computational efficiency and label quality improvement?
- Basis in paper: [explicit] The paper mentions related work on pseudo-label-based methods but does not provide comparative experiments or runtime analysis
- Why unresolved: The paper focuses solely on the Re-Label method without benchmarking against alternative data-centric techniques that could achieve similar goals
- What evidence would resolve it: Head-to-head experiments comparing Re-Label with pseudo-labeling, active learning, and contrastive learning on identical datasets, measuring both final model performance and computational costs

### Open Question 3
- Question: What is the optimal number of re-labeling iterations before diminishing returns set in, and how does this vary across different task types (classification, NER, object detection, etc.)?
- Basis in paper: [explicit] The paper shows results for 1-3 iterations (model-v1, model-v2, model-v3) but does not analyze the convergence behavior or task-specific iteration requirements
- Why unresolved: The experiments stop at model-v3 without investigating whether additional iterations continue to provide benefits or whether some tasks converge faster than others
- What evidence would resolve it: Experiments running Re-Label for 5-10 iterations on multiple tasks, plotting performance curves to identify the point of diminishing returns for each task type

## Limitations
- The method relies heavily on initial model performance, which may not be reliable for extremely noisy datasets or complex tasks
- Human-in-the-loop component introduces potential biases if annotators become overly reliant on model predictions
- The approach requires multiple iterations of human annotation, increasing costs and time requirements

## Confidence

**Confidence levels:**
- **High confidence**: The iterative re-labeling process improves dataset quality when the initial model performs reasonably well (classification accuracy improvements from 83.3% to 93.8% are well-documented)
- **Medium confidence**: The generalizability across diverse tasks (text classification, NER, object detection, sequence generation, CTR prediction) is supported by results but task-specific implementations vary significantly
- **Medium confidence**: The claim that this method is complementary to RLHF and pseudo-labeling is reasonable but lacks direct comparative experiments

## Next Checks

1. **Diminishing returns analysis**: Systematically measure accuracy improvements across 3+ iterations to identify the optimal number of re-labeling cycles and quantify when additional iterations provide negligible benefits

2. **Cross-domain robustness test**: Apply the method to datasets from different domains (medical imaging, legal documents, scientific literature) with varying levels of initial noise to validate generalizability beyond the presented tasks

3. **Human bias measurement**: Conduct experiments comparing re-labeling outcomes when annotators are explicitly instructed to ignore model predictions versus when they can use them as reference, to quantify the impact of model influence on human judgment