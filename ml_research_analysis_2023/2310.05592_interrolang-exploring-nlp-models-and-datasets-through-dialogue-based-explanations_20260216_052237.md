---
ver: rpa2
title: 'InterroLang: Exploring NLP Models and Datasets through Dialogue-based Explanations'
arxiv_id: '2310.05592'
source_url: https://arxiv.org/abs/2310.05592
tags:
- association
- user
- language
- linguistics
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: INTERROLANG adapts TALKTOMODEL to the NLP domain, adding free-text
  rationalization and new operations like token-level attributions, counterfactuals,
  and adversarial examples. It uses Adapter-based models for intent recognition, outperforming
  GPT-Neo and FLAN-T5 baselines.
---

# InterroLang: Exploring NLP Models and Datasets through Dialogue-based Explanations

## Quick Facts
- arXiv ID: 2310.05592
- Source URL: https://arxiv.org/abs/2310.05592
- Reference count: 22
- Primary result: Adapter-based intent recognition outperforms few-shot prompting baselines, with free-text rationales and feature attributions most effective for user understanding

## Executive Summary
INTERROLANG is an interactive dialogue system for exploring NLP models and datasets through explanations. It adapts the TALKTOMODEL framework to NLP, adding free-text rationalization and operations like token-level attributions, counterfactuals, and adversarial examples. The system uses Adapter-based models for intent recognition, demonstrating superior performance over GPT-Neo and FLAN-T5 baselines. A user study found that free-text rationales were most preferred, while simulatability results showed attribution and rationales yielded the highest simulation accuracy, indicating they are most helpful for users to understand model behavior.

## Method Summary
INTERROLANG implements a dialogue-based explanation system for NLP models using Adapter-based intent recognition. The system parses user questions into executable operations (e.g., data description, performance analysis, explanations) and provides natural language responses. It integrates various interpretability methods including attribution, counterfactuals, rationalization, and similarity operations. The system was evaluated on three NLP datasets (DailyDialog, BoolQ, OLID) with corresponding BERT-type models, using both subjective user ratings and simulatability tests to assess explanation quality and user understanding.

## Key Results
- Adapter-based intent recognition outperforms few-shot prompting and fine-tuned baselines in parsing user questions into executable operations
- Free-text rationales were most preferred by users in subjective ratings, followed by augmentation and adversarial examples
- Attribution and free-text rationales yielded the highest simulatability accuracy, indicating they are most helpful for users to understand model behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Adapter-based intent recognition model outperforms few-shot prompting and fine-tuning baselines in parsing user questions into executable operations.
- Mechanism: Adapters are lightweight fine-tuning modules added to a pre-trained language model that allow task-specific parameter adaptation without full model retraining. This approach enables efficient intent classification and slot tagging for INTERROLANG's operations.
- Core assumption: The Adapter model can effectively learn to parse user questions into the structured format required by INTERROLANG's operations.
- Evidence anchors:
  - [abstract]: "To recognize user queries for explanations, we evaluate fine-tuned and few-shot prompting models and implement a novel Adapter-based approach."
  - [section]: "Table 2 shows that our Adapter-based approach (slot tagging and intent recognition) is able to outperform both the GPT-Neo baseline and the fine-tuned FLAN-T5 models, using much less parameters and trained on the automatically augmented prompts with replaced slot values."
  - [corpus]: Weak evidence - no direct corpus support found for this specific mechanism.
- Break condition: If the Adapter model cannot accurately parse user questions into the required format, or if the parsing accuracy drops significantly for new datasets or use cases.

### Mechanism 2
- Claim: Dialogue-based explanations are more effective for user understanding of NLP model behavior than one-off explanations.
- Mechanism: The interactive dialogue interface allows users to explore the model and dataset through a series of connected explanations, building a mental model of the model's behavior over time.
- Core assumption: Users can form a better understanding of model behavior through iterative questioning and explanation than through single, isolated explanations.
- Evidence anchors:
  - [abstract]: "Moreover, users could more reliably predict the model outcome based on an explanation dialogue rather than one-off explanations."
  - [section]: "With Sim(t = 1) , we detected that in some cases the explanations induced false trust and led the users to predict a different model output."
  - [corpus]: Weak evidence - no direct corpus support found for this specific mechanism.
- Break condition: If users find the dialogue interface confusing or overwhelming, or if they prefer one-off explanations for certain use cases.

### Mechanism 3
- Claim: Free-text rationales and feature attribution explanations are most helpful for users to understand model behavior.
- Mechanism: Free-text rationales provide natural language justifications for model predictions, while feature attribution highlights the most important input features. These explanations help users understand the reasoning behind model predictions.
- Core assumption: Users can understand and benefit from natural language explanations and feature importance information.
- Evidence anchors:
  - [abstract]: "We found rationalization and feature attribution were helpful in explaining the model behavior."
  - [section]: "Table 3 reveals that most operations were positively received, but there are large differences between the subjective ratings of operations across all three aspects (CHS). We find that data description, performance and mistakes operations consistently perform highly, indicating that they're essential to model understanding. Among the repertoire of explanation operations, free-text rationale scores highest on average, followed by augmentation and adversarial examples, while counterfactuals are at the bottom of the list."
  - [corpus]: Weak evidence - no direct corpus support found for this specific mechanism.
- Break condition: If users find free-text rationales or feature attribution explanations confusing or unhelpful, or if they prefer other types of explanations for certain use cases.

## Foundational Learning

- Concept: Natural Language Processing (NLP)
  - Why needed here: INTERROLANG is a dialogue-based explanation system for NLP models, so understanding NLP concepts and tasks is crucial.
  - Quick check question: Can you explain the difference between a token and a sentence in the context of NLP?
- Concept: Explainable AI (XAI)
  - Why needed here: INTERROLANG aims to provide explanations for NLP model behavior, so understanding XAI concepts and methods is essential.
  - Quick check question: What are the main categories of XAI methods, and how do they differ?
- Concept: Dialogue Systems
  - Why needed here: INTERROLANG uses a dialogue-based interface for interacting with users, so understanding dialogue system concepts and architectures is important.
  - Quick check question: What are the key components of a dialogue system, and how do they interact?

## Architecture Onboarding

- Component map: Dialogue Engine -> Execution Engine -> Text Interface
- Critical path: 1) User inputs question, 2) Dialogue Engine parses input into operation, 3) Execution Engine runs operation and generates response, 4) Response displayed through Text Interface
- Design tradeoffs: Using Adapters for intent recognition allows efficient fine-tuning but may not generalize as well as few-shot prompting; providing wide range of operations allows flexibility but increases system complexity
- Failure signatures: High parsing error rates indicate issues with intent recognition model; low user satisfaction indicates problems with explanation quality or dialogue flow
- First 3 experiments: 1) Test Adapter model parsing accuracy on new dataset, 2) Conduct user study comparing dialogue-based vs one-off explanations, 3) Analyze impact of different explanation types on user understanding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can INTERROLANG's Adapter-based intent recognition be further improved to handle more complex and ambiguous user queries?
- Basis in paper: [explicit] The paper discusses the Adapter-based approach for intent recognition and compares it to other methods like GPT-Neo and FLAN-T5. It also mentions that the task of detecting user intent is far from being solved.
- Why unresolved: The paper provides initial results showing the Adapter-based approach outperforms other methods, but there is still room for improvement in handling complex and ambiguous user queries.
- What evidence would resolve it: Further experiments and evaluations on a wider range of user queries, including those with complex intents and ambiguous language, could help identify areas for improvement and guide the development of more robust intent recognition models.

### Open Question 2
- Question: How can INTERROLANG be extended to support real-time model comparison and evaluation?
- Basis in paper: [inferred] The paper mentions that INTERROLANG does not allow direct model comparison and that the models are constrained to their datasets. This suggests an opportunity to extend the system to support real-time model comparison and evaluation.
- Why unresolved: The paper focuses on exploring individual models and datasets, but does not address the need for comparing and evaluating different models in real-time.
- What evidence would resolve it: Implementing a feature that allows users to compare the performance and explanations of multiple models on the same dataset in real-time could provide valuable insights into the strengths and weaknesses of different models and guide model selection.

### Open Question 3
- Question: How can INTERROLANG be used to mitigate biases and potential harmful effects of language models?
- Basis in paper: [explicit] The paper mentions that INTERROLANG can point users in directions where the training data or model behavior is counter-intuitive, but does not offer a solution to mitigate biases or harmful effects.
- Why unresolved: The paper acknowledges the potential for biases and harmful effects in language models, but does not provide a concrete solution for addressing these issues.
- What evidence would resolve it: Integrating bias detection and mitigation techniques into INTERROLANG, such as fairness metrics and debiasing algorithms, could help users identify and address biases in language models. Additionally, incorporating user feedback and annotations could further improve the system's ability to detect and mitigate harmful effects.

## Limitations

- Small user study sample size (10 participants for subjective ratings, 6 for simulatability) limits statistical power and generalizability
- Adapter-based intent recognition model trained on only 505 examples may not capture full diversity of real-world user queries
- System focuses on three specific NLP tasks (dialogue act classification, question answering, hate speech detection) which may not represent broader NLP applications

## Confidence

- High Confidence: Adapter-based intent recognition model demonstrates superior performance over few-shot prompting baselines with clear quantitative evidence
- Medium Confidence: Dialogue-based explanations improve user understanding compared to one-off explanations, but small sample size limits strength of conclusion
- Low Confidence: Free-text rationales and feature attributions are most helpful for understanding model behavior based on subjective user ratings which may be influenced by presentation factors

## Next Checks

1. **Scaling validation**: Test Adapter-based intent recognition on larger dataset (5,000+ examples) to assess generalization and robustness to real-world usage patterns

2. **Replication with larger samples**: Conduct user studies with at least 30 participants per condition to improve statistical power and validate findings about dialogue-based explanations

3. **Cross-task generalization**: Evaluate INTERROLANG's effectiveness on additional NLP tasks beyond the three studied (e.g., text classification, sentiment analysis, named entity recognition) to determine framework generalizability