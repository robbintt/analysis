---
ver: rpa2
title: 'SECNN: Squeeze-and-Excitation Convolutional Neural Network for Sentence Classification'
arxiv_id: '2312.06088'
source_url: https://arxiv.org/abs/2312.06088
tags:
- sentence
- attention
- classification
- mechanism
- secnn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SECNN, a novel CNN-based model for sentence
  classification that leverages channel attention mechanisms. The model addresses
  the limitation of traditional CNNs in capturing long-term dependencies by using
  multiple parallel CNN layers with different filter sizes to generate feature maps,
  which are then treated as channels of sentence representation.
---

# SECNN: Squeeze-and-Excitation Convolutional Neural Network for Sentence Classification

## Quick Facts
- arXiv ID: 2312.06088
- Source URL: https://arxiv.org/abs/2312.06088
- Authors: 
- Reference count: 16
- Key outcome: SECNN achieves advanced performance in sentence classification tasks, with pre-trained Word2vec and Glove embeddings improving results across multiple datasets.

## Executive Summary
This paper introduces SECNN, a novel CNN-based model for sentence classification that leverages channel attention mechanisms. The model addresses the limitation of traditional CNNs in capturing long-term dependencies by using multiple parallel CNN layers with different filter sizes to generate feature maps, which are then treated as channels of sentence representation. The Squeeze-and-Excitation (SE) attention mechanism is applied to these channels to learn attention weights without additional parameters. Experimental results on multiple datasets show that the proposed SECNN model achieves advanced performance in sentence classification tasks.

## Method Summary
SECNN processes sentences through an embedding layer, followed by three parallel CNN layers with filter sizes 3, 4, and 5. These layers generate feature maps that serve as channels of sentence representation. The Squeeze-and-Excitation blocks apply channel attention to these feature maps through global average pooling and gating mechanisms, allowing the model to learn attention weights for different channels without additional parameters. The model uses pre-trained Word2vec or GloVe embeddings and employs max-pooling, dropout, and dense classification layers. Key hyperparameters include an increasing ratio r=16 for SE blocks, dropout rate 0.5, and batch size 64.

## Key Results
- SECNN achieves advanced performance in sentence classification across multiple datasets
- Pre-trained Word2vec and GloVe embeddings improve model performance
- Word2vec performs well on MR, IMDb, and DBpedia datasets, while GloVe performs better on AGNews

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SECNN improves sentence classification by treating feature maps from multiple CNNs as channels and applying channel attention to reweight their importance
- Mechanism: Multiple parallel CNN layers with different filter sizes extract n-gram features at different scales, creating feature maps that serve as channels. The Squeeze-and-Excitation (SE) blocks compute attention weights for these channels through global average pooling and gating mechanisms, allowing the model to focus on more important feature map channels without additional parameters
- Core assumption: Different filter sizes capture complementary n-gram information that benefits from selective attention, and that treating feature maps as channels is a valid representation choice
- Evidence anchors:
  - [abstract] "SECNN takes the feature maps from multiple CNN as different channels of sentence representation, and then, we can utilize channel attention mechanism, that is SE attention mechanism, to enable the model to learn the attention weights of different channel features"
  - [section] "SECNN takes the feature maps of multiple CNNs as different channels of sentence representation, and then, we can utilize channel attention mechanism, the SE attention mechanism, to enable the model to calculate the attention weights of different channels without any additional parameters"
  - [corpus] Weak evidence - related works focus on word-level or feature-level attention rather than channel-level attention in CNN-based text models
- Break condition: If feature maps from different CNN layers do not contain complementary information, or if channel attention provides no benefit over simple concatenation or averaging

### Mechanism 2
- Claim: Pre-trained embeddings (Word2vec or Glove) improve SECNN performance by providing better semantic representations than random initialization
- Mechanism: Pre-trained embeddings capture semantic relationships learned from large corpora, giving the model a better starting point for representing words. This semantic richness helps the CNN layers extract more meaningful n-gram features
- Core assumption: The semantic relationships captured in pre-trained embeddings are relevant to the sentence classification tasks and domains
- Evidence anchors:
  - [section] "The results show both Glove and Word2vec word vectors can improve performance. Among them, Glove performs better on AGNews, and Word2vec performs well on MR, IMDb and DBpedia"
  - [abstract] "Experimental results on multiple datasets show that the proposed SECNN model achieves advanced performance in sentence classification tasks. Specifically, using pre-trained Word2vec and Glove embeddings improves the model's performance"
  - [corpus] No direct corpus evidence for this specific claim about SECNN, though general literature supports pre-trained embeddings in NLP
- Break condition: If the classification task involves specialized vocabulary or domains not well-represented in the pre-training corpora, or if the task requires learning domain-specific embeddings

### Mechanism 3
- Claim: Using multiple CNN layers with different filter sizes captures a richer set of n-gram features than single filter size
- Mechanism: Different filter sizes capture different n-gram granularities (e.g., bigrams, trigrams, 4-grams). By stacking these as channels, the model can learn which n-gram scales are most important for each classification task through the SE attention mechanism
- Core assumption: Different sentence classification tasks benefit from different n-gram granularities, and the attention mechanism can effectively learn which to prioritize
- Evidence anchors:
  - [section] "We modify SECNN by utilizing three parallel CNNs with different filter sizes of 3, 4, and 5 to generate feature maps of the same size through preserving the convolution results at the boundaries"
  - [abstract] "But restricted by the width of convolutional filters, it is difficult for CNN to capture long term contextual dependencies"
  - [corpus] Weak evidence - most related works use fixed filter sizes, though the general principle of multi-scale feature extraction is established in vision literature
- Break condition: If the classification task is dominated by a single n-gram scale, or if the attention mechanism cannot effectively distinguish useful from noisy channels

## Foundational Learning

- Concept: Convolutional Neural Networks for text
  - Why needed here: Understanding how 1D convolutions slide over word sequences to extract n-gram features is essential for grasping SECNN's core operation
  - Quick check question: What does a filter of size 3 capture when applied to word embeddings in a sentence?

- Concept: Attention mechanisms in neural networks
  - Why needed here: The SE blocks use a gating mechanism to compute channel attention weights, which is central to how SECNN reweights feature map importance
  - Quick check question: How does the sigmoid activation in the SE block ensure attention weights are between 0 and 1?

- Concept: Word embeddings and pre-training
  - Why needed here: SECNN can use pre-trained embeddings, and understanding how these differ from random initialization explains the performance gains
  - Quick check question: Why might pre-trained embeddings on a large corpus help with a specific sentence classification task?

## Architecture Onboarding

- Component map:
  - Input: Word embeddings (random or pre-trained)
  - Embedding layer: Converts words to dense vectors
  - Multiple CNN blocks: Parallel convolutions with different filter sizes producing feature maps
  - SE blocks: Squeeze-and-Excitation attention on feature map channels
  - Classification layer: Max-pooling, dropout, dense layer with sigmoid/softmax
  - Output: Sentence class probabilities

- Critical path: Input → Embeddings → CNN blocks → SE blocks → Classification → Output
- Design tradeoffs:
  - Filter size selection: Multiple sizes capture diverse n-grams but increase computation
  - SE block complexity: The increasing ratio r controls model capacity vs. efficiency
  - Embedding choice: Pre-trained vs. learned embeddings tradeoff generalization vs. task-specific adaptation
- Failure signatures:
  - Poor performance on multiple datasets: Likely issues with CNN configuration or SE block parameters
  - Good training but poor validation: Possible overfitting, try increasing dropout or reducing model complexity
  - One dataset performs significantly worse: May indicate domain mismatch or need for different filter size selection
- First 3 experiments:
  1. Baseline test with random embeddings and single CNN filter size to establish minimal working model
  2. Multi-filter SECNN with random embeddings to verify channel attention adds value
  3. SECNN with pre-trained embeddings to measure impact of semantic initialization on each dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal increasing ratio (r) for the Squeeze-and-Excitation blocks across different datasets?
- Basis in paper: [explicit] The paper mentions that the increasing ratio r is a hyperparameter and presents experimental results showing different performances with varying r values (2, 4, 8, 16, 32, 64) across four datasets.
- Why unresolved: The paper only tests a limited range of increasing ratio values and suggests that tuning the increasing ratio is necessary to meet the needs of a given model, but does not provide a definitive answer for the optimal value.
- What evidence would resolve it: Conducting experiments with a wider range of increasing ratio values and potentially employing hyperparameter optimization techniques to find the optimal value for each dataset.

### Open Question 2
- Question: How does the performance of SECNN compare to other attention-based CNN models when using pre-trained Word2vec or Glove embeddings?
- Basis in paper: [explicit] The paper mentions that both Word2vec and Glove word vectors can improve performance, with Word2vec performing well on MR, IMDb, and DBpedia datasets, and Glove performing better on AGNews.
- Why unresolved: The paper does not provide a direct comparison between SECNN and other attention-based CNN models using pre-trained embeddings.
- What evidence would resolve it: Conducting experiments to compare the performance of SECNN with other attention-based CNN models using the same pre-trained embeddings on the same datasets.

### Open Question 3
- Question: How does the choice of filter sizes in the parallel CNN layers affect the performance of SECNN?
- Basis in paper: [explicit] The paper initially uses three parallel CNN layers with the same filter size of 3, but then modifies the model to use filter sizes of 3, 4, and 5, resulting in improved performance on some datasets.
- Why unresolved: The paper only tests a limited combination of filter sizes and does not explore other potential combinations or their effects on performance.
- What evidence would resolve it: Conducting experiments with various combinations of filter sizes in the parallel CNN layers to determine the optimal configuration for different datasets.

## Limitations

- Limited baseline comparisons with state-of-the-art methods
- Incomplete model architecture details (optimizer, learning rate, exact SE block implementation)
- Evaluation limited to four datasets without comprehensive performance metrics

## Confidence

- **High Confidence:** Pre-trained embeddings improve performance across multiple datasets
- **Medium Confidence:** Multi-filter CNN with SE attention provides benefits over traditional CNN architectures
- **Low Confidence:** SECNN achieves "advanced performance" compared to state-of-the-art methods

## Next Checks

1. Conduct ablation studies comparing SECNN with and without SE blocks, and with single vs. multiple filter sizes to isolate component contributions

2. Implement and compare against recent state-of-the-art sentence classification methods (e.g., BERT-based models) on the same datasets

3. Perform systematic hyperparameter sensitivity analysis varying filter sizes, increasing ratio r, dropout rate, and embedding dimensions