---
ver: rpa2
title: In-Context Retrieval-Augmented Language Models
arxiv_id: '2302.00083'
source_url: https://arxiv.org/abs/2302.00083
tags:
- ralm
- retrieval
- gpt-2
- language
- bm25
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces in-context retrieval-augmented language models
  (RALM), a method that leverages external documents to improve language model performance
  without modifying the model architecture. Unlike existing RALM approaches that require
  architectural changes, in-context RALM simply prepends relevant documents retrieved
  from a corpus to the model's input.
---

# In-Context Retrieval-Augmented Language Models

## Quick Facts
- **arXiv ID**: 2302.00083
- **Source URL**: https://arxiv.org/abs/2302.00083
- **Reference count**: 15
- **Key outcome**: In-context RALM achieves performance gains equivalent to increasing model size by 2-3x by prepending relevant documents to input without architectural changes.

## Executive Summary
This paper introduces in-context retrieval-augmented language models (RALM), a method that leverages external documents to improve language model performance without modifying the model architecture. Unlike existing RALM approaches that require architectural changes, in-context RALM simply prepends relevant documents retrieved from a corpus to the model's input. The authors demonstrate that using off-the-shelf retrievers like BM25 provides significant gains in language modeling across various datasets and model sizes, with performance improvements equivalent to increasing the model size by 2-3x. Further gains are achieved by tailoring the document retrieval and ranking mechanism to the language modeling task, leading to additional performance improvements.

## Method Summary
The method uses off-the-shelf language models and retrievers without any training or fine-tuning. Documents are retrieved from a corpus and prepended to the language model's input sequence, with the number of tokens used for retrieval (query length ℓ) and the frequency of retrieval (stride s) treated as hyperparameters. Two reranking approaches are explored: zero-shot LM reranking, which uses the language model itself to score retrieved documents, and predictive reranking, which trains a reranker on a language modeling signal. Experiments are conducted on five diverse datasets using language models ranging from 110M to 66B parameters.

## Key Results
- In-context RALM provides significant perplexity improvements (1.3-1.9 points) across five diverse datasets
- Performance gains are equivalent to increasing model size by 2-3x
- BM25 outperforms dense retrievers like BERT and Contriever in zero-shot settings
- Zero-shot LM reranking improves document selection beyond lexical matching alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prepending retrieved documents directly to the input sequence enables the model to condition generation on external knowledge without architectural modification.
- Mechanism: The model treats the concatenated documents and prefix as a single extended context, using standard causal attention to attend to both document content and prefix tokens.
- Core assumption: The transformer's positional embeddings and attention patterns can handle arbitrarily long input sequences without degradation in the ability to attend to recent tokens.
- Evidence anchors:
  - [abstract] "In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM."
  - [section 3.1] "In-Context RALM refers to the following specific, simple method of concatenating the retrieved documents within the Transformer's input prior to the prefix"
  - [corpus] Weak evidence; no explicit corpus comparison provided.
- Break condition: When the concatenated input exceeds the model's maximum sequence length, truncation may remove relevant context or cause performance degradation.

### Mechanism 2
- Claim: Retrieving documents every few tokens (low retrieval stride) provides more relevant grounding for generation than retrieving once per generation step.
- Mechanism: Frequent retrieval updates the context window with fresh, task-relevant documents as the generation progresses, reducing semantic drift from the initial context.
- Core assumption: Later tokens in the prefix are more predictive of upcoming tokens, so more frequent retrieval yields more relevant grounding.
- Evidence anchors:
  - [section 5.2] "Figure 5 shows that LM performance improved as the retrieval operation became more frequent... Intuitively, retrieving with high frequency allows to ground the LM in higher resolution."
  - [section 4.3] "Retrieval Query Length" discusses how longer queries dilute relevance.
  - [corpus] Weak evidence; no explicit corpus analysis provided.
- Break condition: Excessive retrieval frequency increases computational cost and latency without proportional gains in perplexity.

### Mechanism 3
- Claim: Zero-shot LM reranking of retrieved documents improves document selection beyond lexical matching alone.
- Mechanism: The LM scores documents by predicting continuation tokens given the document and recent prefix tokens, selecting the document with highest score as grounding.
- Core assumption: LMs have semantic understanding that can be leveraged to assess document relevance beyond surface form matching.
- Evidence anchors:
  - [section 6.1] "We used language models as document rerankers... choosing document dj such that arg max j∈[k] pφ(xsi−s′+1,...,xsi|x≤(si−s′),dj)."
  - [section 5.1] "BM25 outperformed all dense retrievers... consistent with prior work showing that BM25 outperforms neural retrievers across a wide array of tasks, when applied in zero-shot settings."
  - [corpus] Weak evidence; no explicit corpus comparison provided.
- Break condition: Reranking with smaller models may yield suboptimal performance compared to the generation LM.

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: RALM extends RAG principles to unconditional language modeling, where the model must generate relevant and accurate text without explicit prompts.
  - Quick check question: How does RALM differ from traditional RAG systems in terms of the conditioning signal and the generation objective?

- Concept: Sparse vs. dense retrieval
  - Why needed here: The paper contrasts BM25 (sparse) with BERT-based dense retrievers, showing that lexical matching can outperform semantic matching in this setting.
  - Quick check question: What evidence does the paper provide that BM25 outperforms dense retrievers in the RALM setting, and why might this be counterintuitive?

- Concept: Language model perplexity
  - Why needed here: Perplexity is the primary evaluation metric, measuring how well the model predicts held-out text, and improvements are framed in terms of equivalent model size gains.
  - Quick check question: How does the paper interpret perplexity improvements in terms of effective model size, and what does this imply about the cost-benefit tradeoff of RALM?

## Architecture Onboarding

- Component map:
  - Retriever (BM25, BERT, Contriever, Spider) -> Reranker (zero-shot LM, trained RoBERTa) -> Language model (GPT-2, GPT-Neo, OPT) -> Retrieval corpus (Wikipedia, Pile datasets, RealNews)

- Critical path:
  1. Given input prefix, extract retrieval query (last ℓ tokens)
  2. Retrieve top-k documents using chosen retriever
  3. Optionally rerank documents using LM or trained reranker
  4. Concatenate top document with prefix
  5. Truncate to max sequence length if necessary
  6. Generate next token using LM

- Design tradeoffs:
  - Retrieval stride (s): Higher s reduces cost but may miss relevant updates
  - Query length (ℓ): Longer queries provide more context but may dilute relevance
  - Reranker choice: Zero-shot reranking is flexible but may underperform trained reranker
  - Document length: Longer documents provide more context but increase truncation risk

- Failure signatures:
  - No improvement in perplexity: Retriever may be ineffective or retrieval frequency too low
  - Increased perplexity: Retrieval may introduce noise or distract from prefix context
  - Degraded performance for long sequences: Truncation may be removing critical context

- First 3 experiments:
  1. Baseline: Measure perplexity without retrieval on WikiText-103
  2. Off-the-shelf BM25: Apply in-context RALM with BM25, vary s and ℓ, measure perplexity
  3. Reranking comparison: Compare zero-shot reranking vs. trained reranker on top-16 BM25 results, measure perplexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the efficiency of in-context RALM for large-scale deployment?
- Basis in paper: [explicit] The paper mentions that each retrieval operation imposes a runtime cost, and suggests that future work could explore retrieving more sparsely or only when a specialized model predicts that retrieval is needed.
- Why unresolved: The paper does not provide specific methods or results for optimizing the efficiency of in-context RALM.
- What evidence would resolve it: Experiments comparing the performance and efficiency of different retrieval strategies, such as sparse retrieval or retrieval based on prediction models, would provide insights into how to optimize in-context RALM for large-scale deployment.

### Open Question 2
- Question: How can we effectively incorporate multiple external documents into in-context RALM?
- Basis in paper: [explicit] The paper considers only the case of prepending a single external document to the context and suggests that adding more documents could drive further gains.
- Why unresolved: The paper does not explore the impact of incorporating multiple documents on language modeling performance.
- What evidence would resolve it: Experiments evaluating the performance of in-context RALM with varying numbers of external documents would help determine the optimal number of documents to include for maximum language modeling gains.

### Open Question 3
- Question: How can we optimize the retrieval stride (s) and query length (ℓ) for different language models and corpora?
- Basis in paper: [explicit] The paper investigates the effect of varying the retrieval stride and query length, but does not provide a systematic approach for optimizing these parameters for different language models and corpora.
- Why unresolved: The optimal values of s and ℓ may depend on the specific characteristics of the language model and the corpus being used.
- What evidence would resolve it: A comprehensive study examining the impact of different s and ℓ values on language modeling performance across various language models and corpora would help establish guidelines for optimizing these parameters.

## Limitations

- Sequence length constraints limit the amount of retrieved context that can be effectively used, with truncation potentially removing critical information
- Performance gains are not uniform across datasets, with the 2-3x model size improvement being an average that masks significant variation
- The framework assumes the language model can effectively attend to both retrieved documents and generation prefix, which may not be optimal given architectural constraints

## Confidence

**High Confidence**: The core claim that in-context RALM provides significant perplexity improvements (1.3-1.9 points) and equivalent model size gains (2-3x) is well-supported by systematic experiments across multiple model families and datasets.

**Medium Confidence**: The claim that BM25 outperforms dense retrievers in this setting is supported by empirical results but lacks theoretical explanation. The effectiveness of zero-shot LM reranking is demonstrated but limited to GPT-2 experiments.

## Next Checks

1. **Truncation Impact Analysis**: Systematically measure how different truncation strategies (head truncation, tail truncation, or dynamic allocation based on document relevance) affect performance across document lengths ranging from 200 to 1000 tokens.

2. **Cross-Model Reranking Validation**: Implement zero-shot reranking experiments using smaller models (e.g., GPT-2 small) to rerank for larger generation models (e.g., GPT-2 large). Compare the perplexity gains against using the same model for both tasks.

3. **Attention Pattern Analysis**: Analyze the attention weights during generation to quantify how much the model attends to retrieved document tokens versus prefix tokens. Compare these patterns with and without retrieval, and across different retrieval frequencies.