---
ver: rpa2
title: 'FELM: Benchmarking Factuality Evaluation of Large Language Models'
arxiv_id: '2310.00741'
source_url: https://arxiv.org/abs/2310.00741
tags:
- factuality
- errors
- world
- felm
- reactors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FELM, a benchmark designed to evaluate the
  factuality of text generated by large language models (LLMs). The authors address
  the need for robust factuality evaluators by creating a dataset that spans five
  diverse domains, including world knowledge, science and technology, math, writing
  and recommendation, and reasoning.
---

# FELM: Benchmarking Factuality Evaluation of Large Language Models

## Quick Facts
- arXiv ID: 2310.00741
- Source URL: https://arxiv.org/abs/2310.00741
- Reference count: 40
- Primary result: Introduces FELM benchmark with 817 samples across 5 domains to evaluate LLM factuality, finding current evaluators inadequate even with retrieval augmentation

## Executive Summary
This paper addresses the critical need for robust factuality evaluation in large language models by introducing FELM, a comprehensive benchmark spanning five diverse domains. The benchmark features 817 samples and 3948 segments with detailed annotations for factual errors, error types, and reference links. The authors systematically evaluate multiple LLM-based factuality evaluators, including vanilla models and those augmented with retrieval mechanisms and chain-of-thought processes. Their findings reveal that while retrieval-augmented approaches show promise, current LLMs still struggle significantly with factuality detection, particularly in domains requiring multi-step reasoning or containing sparse factual errors.

## Method Summary
The authors created the FELM dataset by collecting 817 samples across five domains (world knowledge, science/tech, math, writing/recommendation, reasoning) and annotating them at the segment level for factual errors. They tested various LLM-based factuality evaluators (Vicuna-33B, ChatGPT, GPT-4) using four approaches: vanilla, chain-of-thought, reference link, and reference document variants. The reference document approach uses BM25 to retrieve relevant text chunks from reference documents for factuality verification. Performance was measured using segment-level and response-level F1 scores and balanced classification accuracy.

## Key Results
- Retrieval-augmented approaches significantly improve factuality detection compared to vanilla LLMs
- Performance varies dramatically across domains, with writing/recommendation and reasoning showing the weakest results
- Claim-based evaluation improves ChatGPT performance but doesn't generalize well to math and reasoning domains
- Even the best-performing approaches achieve only moderate F1 scores, indicating substantial room for improvement

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Segment-level annotation improves factuality evaluation accuracy
- **Mechanism:** Breaking long-form responses into self-contained segments allows evaluators to focus on smaller, verifiable text units, reducing cognitive load and enabling more precise error detection
- **Core assumption:** Individual segments contain coherent facts that can be evaluated independently
- **Evidence anchors:**
  - [abstract] "Our annotation is based on text segments, which can help pinpoint specific factual errors"
  - [section 3.1] "segment-level annotation is the closest to our end goal, highlighting factual correctness of segments directly from the response"
  - [corpus] Found 25 related papers; average FMR=0.476, suggesting moderate relevance to factuality evaluation
- **Break condition:** When segments overlap in meaning or require cross-segment reasoning, making isolated evaluation misleading

### Mechanism 2
- **Claim:** Retrieval-augmented LLMs significantly improve factuality detection
- **Mechanism:** Providing external reference documents allows LLMs to verify claims against trusted sources, reducing hallucination detection errors
- **Core assumption:** Reference documents contain accurate, relevant information that can confirm or refute segment claims
- **Evidence anchors:**
  - [abstract] "Our findings reveal that while retrieval aids factuality evaluation, current LLMs are far from satisfactory"
  - [section 4.2] "Both the augmentation approaches with reference links and reference document are effective in detecting factual errors"
  - [section 4.1] "we access the text corresponding to the reference links and then use the BM25 algorithm to retrieve the most relevant text chunks"
- **Break condition:** When reference documents are unavailable or contain conflicting information, or when retrieval mechanisms fail to find relevant passages

### Mechanism 3
- **Claim:** Claim-based evaluation outperforms segment-based evaluation for some LLMs
- **Mechanism:** Extracting atomic claims from segments simplifies the evaluation task by reducing complex statements to verifiable propositions
- **Core assumption:** Atomic claims can be reliably extracted from segments and evaluated independently
- **Evidence anchors:**
  - [section 4.2] "ChatGPT detectors exhibit improved performance when utilizing claim-based segmentation methods"
  - [section 3.1] "extracted atomic facts could serve as intermediate outputs that can ultimately be mapped back to segments"
  - [corpus] Multi-FAct paper suggests factuality evaluation benefits from granular claim-level assessment
- **Break condition:** When claims extracted from segments lose necessary context or when the extraction process introduces errors

## Foundational Learning

- **Concept:** BM25 retrieval algorithm
  - Why needed here: Used to retrieve relevant text chunks from reference documents for factuality verification
  - Quick check question: How does BM25 score relevance between a query and document terms, and why is it effective for this application?

- **Concept:** Chain-of-thought prompting
  - Why needed here: Provides reasoning traces that improve LLM factuality evaluation by encouraging systematic verification
  - Quick check question: What distinguishes chain-of-thought prompting from standard prompting, and how does it affect factuality detection performance?

- **Concept:** Precision-recall tradeoff in evaluation metrics
  - Why needed here: Understanding F1 score, precision, and recall is crucial for interpreting factuality evaluator performance
  - Quick check question: When would you prefer high precision over high recall in factuality evaluation, and what does this preference imply about the application?

## Architecture Onboarding

- **Component map:** Prompt collection system → ChatGPT response generation → Text segmentation module → Human annotation interface → LLM-based evaluator → Performance metrics calculator
- **Critical path:** Prompt collection → Response generation → Segmentation → Annotation → LLM evaluation → Performance analysis
- **Design tradeoffs:** 
  - Segment granularity vs. evaluation efficiency: finer segments improve precision but increase evaluation cost
  - Retrieval depth vs. latency: deeper document analysis improves accuracy but increases response time
  - Human annotation vs. automated labeling: human annotation ensures quality but limits dataset scale
- **Failure signatures:**
  - Consistently low F1 scores across all domains suggests fundamental evaluator limitations
  - Domain-specific failures (e.g., writing/recommendation) indicate evaluator bias toward certain knowledge types
  - Large gap between segment-level and response-level performance suggests evaluators struggle with aggregating segment assessments
- **First 3 experiments:**
  1. Test vanilla LLM evaluator on a small subset of FELM to establish baseline performance
  2. Implement and evaluate retrieval-augmented evaluator on the same subset to measure improvement
  3. Compare segment-based vs. claim-based evaluation approaches on a balanced domain sample to identify optimal granularity

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the difficulty of FELM compare to other factuality benchmarks?
- **Basis in paper:** The authors briefly compare FELM's difficulty to other factuality benchmarks like SummEval, where ChatGPT/GPT-4 perform better.
- **Why unresolved:** The comparison is not comprehensive and doesn't include other major benchmarks like FEVER, FactCC, or QAGS.
- **What evidence would resolve it:** A thorough comparison of FELM with other benchmarks using the same models and metrics would clarify its relative difficulty.

### Open Question 2
- **Question:** Can the claim-based method be adapted to work well for math and reasoning domains?
- **Basis in paper:** The authors note that claim-based methods don't work well for math and reasoning due to multi-step dependencies between sentences.
- **Why unresolved:** The paper doesn't explore potential modifications to the claim-based approach that might make it effective for these domains.
- **What evidence would resolve it:** Testing modified claim-based methods (e.g., considering claim chains or dependencies) on math and reasoning tasks would determine their feasibility.

### Open Question 3
- **Question:** How does self-consistency impact the performance of chain-of-thought prompting across different domains and models?
- **Basis in paper:** The authors show self-consistency improves chain-of-thought performance for ChatGPT, but only briefly mention this in the appendix.
- **Why unresolved:** The paper doesn't provide a comprehensive analysis of self-consistency's impact across all domains, models, and prompting methods.
- **What evidence would resolve it:** A systematic study varying self-consistency parameters and evaluating across all domains and models would quantify its benefits.

## Limitations

- The 817-sample dataset size may not capture the full complexity of factuality errors across diverse domains
- Human annotation introduces potential subjectivity and may not be fully consistent across annotators
- Performance remains far from satisfactory even with retrieval augmentation, particularly for writing/recommendation and reasoning domains
- Claim-based evaluation doesn't generalize well to math and reasoning domains due to multi-step dependencies

## Confidence

- High confidence in retrieval-augmented approaches improving factuality evaluation, supported by consistent performance improvements across multiple LLM variants
- Medium confidence in domain-specific performance differences, as sample sizes per domain aren't specified
- Low confidence in generalizability of claim-based evaluation advantages, as benefits are specifically noted for ChatGPT without systematic comparison across all models

## Next Checks

1. **Cross-dataset validation:** Test the best-performing FELM evaluators (Vicuna-33B with reference documents) on an independent factuality benchmark to assess generalizability beyond the FELM dataset.

2. **Annotation consistency audit:** Conduct inter-annotator agreement analysis on a random subset of FELM samples to quantify human annotation reliability and identify systematic biases in error classification.

3. **Scalability stress test:** Evaluate the computational overhead and latency impact of retrieval-augmented approaches on larger response sizes (1000+ words) to determine practical deployment constraints for real-world applications.