---
ver: rpa2
title: GPT as a Baseline for Recommendation Explanation Texts
arxiv_id: '2309.08817'
source_url: https://arxiv.org/abs/2309.08817
tags:
- texts
- movie
- text
- movies
- participants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the potential of large language models
  (LLMs) for generating recommendation explanations, comparing them against human-written
  movie reviews. Using a survey of 120 participants, the authors evaluated whether
  LLM-generated texts affected movie preference rankings and perceived quality (accuracy,
  informativeness, persuasiveness, and interestingness) compared to human reviews.
---

# GPT as a Baseline for Recommendation Explanation Texts

## Quick Facts
- **arXiv ID:** 2309.08817
- **Source URL:** https://arxiv.org/abs/2309.08817
- **Reference count:** 18
- **Key outcome:** Large language models can generate recommendation explanations that match or exceed human-written reviews in quality ratings, especially for movies participants have already seen.

## Executive Summary
This study investigates whether large language models (LLMs) can serve as a viable source for generating recommendation explanations by comparing them against human-written movie reviews. Using a survey of 120 participants, the authors evaluated whether LLM-generated texts affected movie preference rankings and perceived quality (accuracy, informativeness, persuasiveness, and interestingness) compared to human reviews. The results show that participants gave no significantly different rankings between movies when explanations were generated by LLMs versus humans, even for movies they had never seen before. However, for movies participants had already seen, LLM-generated texts were rated significantly higher across all quality measures. The study demonstrates that LLMs are a promising source of recommendation explanations and suggests future exploration into personalized text generation for improved user satisfaction.

## Method Summary
The study collected 10 well-known seed movies and generated 5 similar lesser-known suggestions for each using MovieLens and BestSimilar. For each suggestion, 5 top IMDb "featured" reviews were collected as human-written texts. Using GPT-4, one model-generated review was created per suggestion by providing the 5 human reviews as context. A survey of 120 Amazon Mechanical Turk participants in the U.S. with high approval ratings was conducted, where each participant ranked 5 suggested movies and rated each review on four quality dimensions. All texts were truncated to 100 tokens and reformatted before presentation. The analysis compared ranking distributions and quality scores between human and LLM-generated texts, controlling for whether participants had seen the movies before.

## Key Results
- Participants showed no significant difference in movie rankings between LLM and human texts, even for unseen movies
- For movies participants had already seen, LLM-generated texts received significantly higher quality ratings across all measures (accuracy, informativeness, persuasiveness, interestingness)
- The study demonstrates LLMs can serve as a baseline for recommendation explanation generation, particularly when users have prior knowledge of the recommended items

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated texts can match human-generated texts in user preference rankings when explaining recommendations.
- Mechanism: The study's experimental design used identical movie suggestions paired with either human or LLM-generated reviews, controlling for item and user bias. Participants ranked movies without knowing the source of the text, allowing a fair comparison of the texts' effectiveness.
- Core assumption: Participants evaluate review texts independently of their knowledge about the text source, and the LLM prompt was designed to generate neutral, informative reviews rather than promotional content.
- Evidence anchors:
  - [abstract] "participants gave no significantly different rankings between movies, nor did they give significantly different individual quality scores to reviews of movies that they had never seen before."
  - [section] "participants showed no significant difference in movie rankings between LLM and human texts, even for unseen movies."
  - [corpus] Weak: The corpus does not contain direct evidence of ranking parity, but the cited papers explore related evaluation frameworks.
- Break condition: If the LLM prompt is biased toward promotion or criticism, or if participants can reliably distinguish LLM from human texts, the ranking parity may not hold.

### Mechanism 2
- Claim: For movies participants have already seen, LLM-generated texts receive higher quality ratings than human-generated texts.
- Mechanism: Familiarity with the movie allows participants to better judge the accuracy and relevance of the review content. LLM-generated texts may be perceived as more structured or comprehensive because they often include plot summaries and critical reception details.
- Core assumption: Participants' prior knowledge of the movie enables them to evaluate factual accuracy and completeness more effectively.
- Evidence anchors:
  - [abstract] "for movies participants had already seen, LLM-generated texts were rated significantly higher across all quality measures."
  - [section] "participants did mark model-generated review texts as significantly better than human-written reviews when they were movies they had seen before."
  - [corpus] Weak: The corpus contains no direct evidence of quality rating differences for seen movies, but similar studies explore evaluation of generated texts.
- Break condition: If participants' prior opinions about the movie are strongly negative or positive, their ratings may be influenced more by sentiment than by text quality.

### Mechanism 3
- Claim: Review content attributes such as plot summary, release information, and critical context are key factors in perceived text quality.
- Mechanism: Participants value objective, structured information about the movie (e.g., synopsis, genre, director) over purely subjective or anecdotal content. LLM-generated texts tend to include these elements more consistently.
- Core assumption: Users prioritize factual and contextual information when evaluating the usefulness of a review.
- Evidence anchors:
  - [section] "Participants tended to emphasize different attribute types... with accuracy, participants focused on raw release information and release context more than they did on subjective experiences."
  - [section] "model-generated texts usually included a plot summary... and summary of critical reception to a movie."
  - [corpus] Weak: The corpus does not provide direct evidence about which review attributes drive quality perceptions.
- Break condition: If user preferences shift toward valuing personal anecdotes or emotional resonance over factual content, this mechanism may weaken.

## Foundational Learning

- Concept: Statistical significance and hypothesis testing
  - Why needed here: The study uses statistical tests (e.g., logistic regression, p-values) to determine whether differences in rankings or quality scores are meaningful.
  - Quick check question: What does a p-value less than 0.05 indicate in the context of comparing LLM and human-generated texts?

- Concept: Experimental design and control groups
  - Why needed here: The study controls for movie selection and user bias by pairing each movie suggestion with both human and LLM-generated texts, and randomizing which text each participant sees.
  - Quick check question: Why is it important to control for movie selection and user bias in this type of study?

- Concept: Likert scale interpretation and ordinal data analysis
  - Why needed here: Participants rate text qualities (accuracy, informativeness, persuasiveness, interestingness) on a Likert scale, and the study treats these as ordinal data in statistical analysis.
  - Quick check question: Why is it important to treat Likert scale responses as ordinal rather than interval data in this context?

## Architecture Onboarding

- Component map:
  Dataset collection -> LLM generation -> Survey platform -> Analysis pipeline

- Critical path:
  1. Collect human reviews and generate LLM reviews for each movie suggestion
  2. Assign participants to condition codes ensuring balanced exposure to human vs. LLM texts
  3. Administer survey, collect rankings and quality scores
  4. Analyze results for statistical significance, compare rankings and quality scores
  5. Perform qualitative analysis of open-ended responses

- Design tradeoffs:
  - Using only one LLM-generated text per movie vs. multiple to capture variation
  - Truncating review texts to 100 tokens vs. showing full length
  - Limiting participants to US-based workers vs. broader demographic sampling

- Failure signatures:
  - Participants can reliably distinguish LLM from human texts, biasing results
  - LLM-generated texts are perceived as less accurate or informative, especially for unseen movies
  - Technical issues with survey platform or condition code assignment lead to unbalanced sampling

- First 3 experiments:
  1. Compare ranking distributions (top-ranked movie counts) between human and LLM texts using chi-squared test
  2. Analyze average normalized ranking positions for human vs. LLM texts, controlling for seen/unseen status
  3. Compare quality score distributions (accuracy, informativeness, persuasiveness, interestingness) using ordinal logistic regression, stratified by seen/unseen status

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would LLM-generated explanations perform for media with no existing human reviews?
- Basis in paper: [explicit] - The authors note this as a key weakness: "One big weakness of this work is that GPT texts were still generated using human reviews... We do not demonstrate any capacity to write competent reviews for media that have no already-written human reviews here"
- Why unresolved: The current study relied on human reviews as input for the LLM, so it doesn't test the model's ability to generate quality explanations for completely unreviewed content.
- What evidence would resolve it: A follow-up study using LLMs to generate explanations for new or obscure media with no existing reviews, comparing the generated texts against human-written explanations once they become available.

### Open Question 2
- Question: What specific personalization features would most improve user satisfaction with LLM-generated explanations?
- Basis in paper: [explicit] - The authors state future work will explore "how we can allow users to customize different review attributes or otherwise learn these preferences to better generate summary texts that improve user satisfaction"
- Why unresolved: The current study used generic, non-personalized LLM outputs and didn't test different customization approaches.
- What evidence would resolve it: User studies comparing satisfaction across different personalization strategies (e.g., preference-based customization, style adaptation, feature highlighting) using controlled experiments with the same base LLM model.

### Open Question 3
- Question: Why do LLM-generated texts receive significantly higher quality ratings for seen movies compared to unseen movies?
- Basis in paper: [explicit] - The authors note this unexpected effect: "The Likert scale effect being stronger for movies that were seen already was unexpected and interesting" but offer only speculation about possible causes
- Why unresolved: The study identified the statistical effect but didn't investigate the underlying psychological or perceptual mechanisms driving it.
- What evidence would resolve it: Follow-up experiments with think-aloud protocols or detailed interviews to understand participants' cognitive processes when evaluating explanations for familiar versus unfamiliar content.

## Limitations
- The study relied on a single LLM generation per movie, which may not capture the full variability in LLM performance
- The survey was limited to US-based participants with specific approval ratings, potentially restricting generalizability
- The truncation of review texts to 100 tokens may have excluded important contextual information that could influence quality perceptions

## Confidence

- **High Confidence:** The finding that LLM-generated texts receive higher quality ratings than human texts for movies participants have already seen is supported by clear statistical evidence and logical mechanism (participants can better judge accuracy with prior knowledge).
- **Medium Confidence:** The claim that LLM texts match human texts in preference rankings for unseen movies is supported by statistical tests but may be sensitive to prompt design and participant ability to distinguish text sources.
- **Low Confidence:** The assertion that review content attributes (plot summary, release information) are key drivers of perceived quality is based on qualitative coding patterns but lacks direct quantitative validation.

## Next Checks
1. Conduct a follow-up study with multiple LLM generations per movie to assess variability and establish confidence intervals for quality scores
2. Expand participant recruitment to include international samples and diverse demographic groups to test generalizability of findings
3. Perform a controlled experiment varying review length and content structure to quantify the impact of specific review attributes on perceived quality