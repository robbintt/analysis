---
ver: rpa2
title: 'Beyond Yes and No: Improving Zero-Shot LLM Rankers via Scoring Fine-Grained
  Relevance Labels'
arxiv_id: '2310.14122'
source_url: https://arxiv.org/abs/2310.14122
tags:
- relevance
- relevant
- ranking
- query
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates improving zero-shot LLM rankers by prompting
  them with fine-grained relevance labels instead of binary ones. The authors propose
  two prompt variants: using textual labels like "Highly Relevant", "Somewhat Relevant",
  "Not Relevant", or using a rating scale from 0 to k.'
---

# Beyond Yes and No: Improving Zero-Shot LLM Rankers via Scoring Fine-Grained Relevance Labels

## Quick Facts
- **arXiv ID**: 2310.14122
- **Source URL**: https://arxiv.org/abs/2310.14122
- **Reference count**: 25
- **Primary result**: Prompting LLMs with 3+ fine-grained relevance labels consistently improves NDCG@10 over binary labels on 8 BEIR datasets

## Executive Summary
This paper investigates how zero-shot LLM rankers can be improved by prompting them with fine-grained relevance labels instead of binary ones. The authors propose two prompt variants: using textual labels like "Highly Relevant", "Somewhat Relevant", "Not Relevant", or using a rating scale from 0 to k. They evaluate on 8 BEIR datasets and show that prompting with 3+ levels consistently improves NDCG@10 compared to binary labels. The intermediate labels help LLMs better distinguish partially relevant documents. Using peak relevance likelihood for ranking scores provides a decoding cost saving with similar performance.

## Method Summary
The method involves prompting a zero-shot LLM (FLAN PaLM2 S) with query-document pairs using fine-grained relevance labels. For each query-document pair, the LLM is asked to select from multiple relevance labels (e.g., "Highly Relevant", "Somewhat Relevant", "Not Relevant") or a rating scale (0-4). The log-likelihoods of each label are collected, and ranking scores are derived using either expected relevance values (weighted sum of relevance levels by their probabilities) or peak relevance likelihood (probability of the most likely relevance level). Documents are then ranked based on these scores.

## Key Results
- Using 3+ relevance levels consistently improves NDCG@10 over binary labels across 8 BEIR datasets
- Peak relevance likelihood achieves similar performance to expected relevance with reduced computation
- The improvement is most pronounced when there is a meaningful distribution of relevance degrees in the data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing intermediate relevance labels in prompts helps LLMs better differentiate partially relevant documents from fully relevant or irrelevant ones.
- Mechanism: The additional relevance labels act as a "cue" that signals to the LLM the need to distinguish between different degrees of relevance, rather than forcing a binary choice.
- Core assumption: LLMs respond to prompt context in a way similar to human annotators, who also perform better with more granular options.
- Evidence anchors:
  - [abstract] "The lack of intermediate relevance label options may cause the LLM to provide noisy or biased answers for documents that are partially relevant to the query."
  - [section 3.2] "When not explicitly prompted, LLMs may struggle to decide whether to classify such documents as relevant or irrelevant."
  - [corpus] Weak - no direct evidence about LLM ranking behavior from corpus, only related work on relevance judgment.
- Break condition: If the LLM's probability distribution over relevance labels is too peaked (always choosing one label), the intermediate options provide no benefit.

### Mechanism 2
- Claim: Collecting likelihood scores across more fine-grained relevance levels provides a more accurate estimate of actual relevance.
- Mechanism: By obtaining probability estimates for multiple relevance levels instead of just two, the model captures more nuanced information about document-query relationships.
- Core assumption: The probability distribution over relevance labels contains meaningful information beyond just the most likely label.
- Evidence anchors:
  - [abstract] "by collecting the likelihood on more fine-grained relevance labels, we can obtain a more accurate estimate of the actual relevance"
  - [section 3.3] "the ranking scores derived from peak relevance likelihood (Equation(4)) achieve very close performance to expected relevance values in RG-kL prompts"
  - [corpus] Weak - corpus shows related work on fine-grained relevance scales but not direct evidence for this specific mechanism.
- Break condition: If the LLM probability estimates are poorly calibrated, additional levels may introduce noise rather than signal.

### Mechanism 3
- Claim: The expected relevance value calculation aggregates probability information across relevance levels to produce better ranking scores.
- Mechanism: By weighting each relevance level by its probability and summing, the model produces a continuous relevance score that better reflects document quality.
- Core assumption: The relevance values assigned to each label (0, 1, 2...) are appropriate and meaningful for ranking.
- Evidence anchors:
  - [section 3.3] "The relevance value should reflect the relevance degree expressed by the textual relevance label" and "naïvely assigning yk = k can already provide great performance"
  - [section 5] Comparison shows ER (expected relevance) generally outperforms PR (peak relevance) except in specific cases
  - [corpus] No direct evidence about this specific aggregation mechanism from corpus.
- Break condition: If the ground truth relevance distribution is highly skewed or the relevance value assignments are inappropriate, this aggregation may not help.

## Foundational Learning

- **Concept**: Pointwise ranking vs pairwise/listwise ranking
  - **Why needed here**: This paper focuses on pointwise methods where each document is scored independently
  - **Quick check question**: In pointwise ranking, do we score one document at a time or compare multiple documents simultaneously?

- **Concept**: Log-likelihood and probability normalization
  - **Why needed here**: The method requires computing log-likelihoods from the LLM and converting them to probabilities
  - **Quick check question**: How do you convert log-likelihood values to normalized probabilities?

- **Concept**: Relevance grading scales
  - **Why needed here**: Understanding how different numbers of relevance levels (2, 3, 4, etc.) affect performance
  - **Quick check question**: Why might 3 relevance levels work better than 2 for some documents?

## Architecture Onboarding

- **Component map**: Input (query, document) → LLM with prompt template → Output (relevance label likelihoods) → Score aggregation (ER or PR) → Ranked list
- **Critical path**: Prompt construction → LLM inference → Likelihood extraction → Score calculation → Document sorting
- **Design tradeoffs**: More relevance levels → potentially better discrimination but higher computational cost; Peak relevance vs expected relevance → efficiency vs accuracy
- **Failure signatures**: Similar scores across all documents (poor discrimination), scores not correlated with ground truth, performance worse than binary baseline
- **First 3 experiments**:
  1. Compare RG-2L vs RG-3L on a single dataset to verify the core improvement claim
  2. Test different score aggregation methods (ER vs PR) on the same prompt template
  3. Vary the number of relevance levels in rating scale (RG-S(0,k)) to find the optimal k value

## Open Questions the Paper Calls Out
- How does the performance of zero-shot LLM rankers vary across different LLM architectures and model sizes?
- How does the performance of the proposed methods compare to fine-tuned rankers on the BEIR datasets?
- How does the choice of intermediate relevance labels impact performance?
- How does the proposed method scale to longer documents or larger sets of candidate documents?
- How do the proposed methods perform on datasets with different characteristics, such as different domains or label distributions?

## Limitations
- The paper uses a single LLM architecture (FLAN PaLM2 S) and doesn't test generalization to other models
- No ablation studies on prompt engineering specifics or different label sets
- Limited analysis of dataset-specific effects that might drive the improvements
- The computational cost of collecting likelihoods for multiple relevance levels is not fully explored

## Confidence

**High Confidence**:
- The core finding that 3+ relevance levels consistently outperform binary labels across 8 BEIR datasets
- The comparison between ER and PR scoring methods is well-supported by the data

**Medium Confidence**:
- The claim about peak relevance likelihood providing equivalent performance to expected relevance with reduced computation
- The mechanism explanation about intermediate labels helping LLMs distinguish partially relevant documents

## Next Checks
1. **Ablation on Label Semantics**: Test whether the specific textual labels ("Highly Relevant", "Somewhat Relevant", "Not Relevant") are crucial, or if semantically equivalent labels produce similar results.
2. **Dataset Characteristic Analysis**: Analyze whether improvement correlates with dataset properties (query ambiguity, document length, etc.) to understand when fine-grained labeling helps most.
3. **LLM Confidence Calibration**: Verify whether the LLM's probability distributions over relevance labels are well-calibrated, particularly for the peak relevance likelihood method.