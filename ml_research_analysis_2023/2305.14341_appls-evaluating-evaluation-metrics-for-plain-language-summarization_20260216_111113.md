---
ver: rpa2
title: 'APPLS: Evaluating Evaluation Metrics for Plain Language Summarization'
arxiv_id: '2305.14341'
source_url: https://arxiv.org/abs/2305.14341
tags:
- text
- simplification
- metrics
- evaluation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the challenge of evaluating Plain Language
  Summarization (PLS), which involves complex language transformations. The authors
  introduce APPLS, a granular meta-evaluation testbed that applies controlled perturbations
  to an existing PLS dataset along four criteria: informativeness, simplification,
  coherence, and faithfulness.'
---

# APPLS: Evaluating Evaluation Metrics for Plain Language Summarization

## Quick Facts
- arXiv ID: 2305.14341
- Source URL: https://arxiv.org/abs/2305.14341
- Reference count: 40
- Primary result: Introduces APPLS testbed and POMME metric to address gaps in PLS evaluation

## Executive Summary
This study addresses the critical challenge of evaluating Plain Language Summarization (PLS) by introducing APPLS, a granular meta-evaluation testbed that applies controlled perturbations to existing PLS datasets across four criteria: informativeness, simplification, coherence, and faithfulness. The authors analyze 15 metrics and identify that while some capture informativeness, coherence, and faithfulness, none adequately assess simplification. In response, they propose POMME, a novel metric using normalized perplexity differences between in-domain and out-of-domain language models, which demonstrates consistent sensitivity to simplification perturbations across multiple datasets.

## Method Summary
The APPLS testbed is constructed by applying controlled perturbations to the CELLS dataset (biomedical abstracts and their plain language summaries). Four perturbation types are designed to isolate sensitivity to individual PLS criteria: informativeness, simplification, coherence, and faithfulness. The testbed is then used to evaluate 15 metrics including automated scores, lexical features, and LLM-based evaluations. POMME is introduced as a new simplification metric that computes normalized perplexity differences between scientific and general language models, leveraging domain shift to detect text simplification.

## Key Results
- Existing metrics show varying sensitivity to PLS criteria, with most effectively capturing informativeness, coherence, and faithfulness but failing on simplification
- LLM-based prompt evaluations show inverse sensitivity to simplification perturbations and fail to distinguish between PLS criteria
- POMME demonstrates consistent sensitivity to simplification perturbations across multiple datasets
- The meta-evaluation approach reveals systematic gaps in current PLS evaluation methodology

## Why This Works (Mechanism)

### Mechanism 1
- Claim: APPLS testbed can isolate metric sensitivity to individual PLS criteria by using controlled perturbations.
- Mechanism: Each perturbation is designed to affect a single criterion with minimal impact on others, allowing granular evaluation of metric responsiveness.
- Core assumption: Perturbations accurately simulate real-world PLS errors and can be incrementally applied to generate a controlled testbed.
- Evidence anchors: [abstract] "define controlled perturbations for our testbed along four criteria that a metric of plain language should capture" and [section] "define a set of perturbations corresponding to these criteria that sensitive metrics should be able to detect"
- Break condition: If perturbations introduce unintended side effects affecting multiple criteria simultaneously, metric sensitivity analysis becomes confounded.

### Mechanism 2
- Claim: POMME metric effectively captures text simplification by leveraging domain shift between scientific and general language models.
- Mechanism: POMME computes normalized perplexity differences between an in-domain and out-of-domain language model, where simpler text shows higher out-of-domain similarity.
- Core assumption: Language models trained on different domains will assign perplexity scores that reflect the domain appropriateness of input text.
- Evidence anchors: [abstract] "POMME capitalizes on the fact that complex scientific text will be more similar to a scientific LM's domain-specific training data, while simpler text will align more closely with a general-domain LM"
- Break condition: If the domain shift assumption fails, POMME loses its sensitivity to simplification.

### Mechanism 3
- Claim: LLM prompt-based evaluations fail to capture PLS-specific criteria due to lack of domain adaptation.
- Mechanism: General-purpose LLMs without PLS-specific training show insensitivity to the nuanced criteria required for plain language evaluation.
- Core assumption: Standard LLMs lack the specialized understanding needed to evaluate PLS transformations like simplification and coherence in scientific contexts.
- Evidence anchors: [abstract] "LLM prompt-based evaluations also fail to respond to simplification perturbations" and [section] "LLM prompt-based evaluations do not distinguish between PLS criteria"
- Break condition: If LLMs are fine-tuned specifically on PLS tasks, they may develop the necessary sensitivity to evaluate these criteria effectively.

## Foundational Learning

- Concept: Controlled perturbation methodology in evaluation testbed construction
  - Why needed here: Enables systematic analysis of metric sensitivity to specific PLS criteria without confounding effects
  - Quick check question: What distinguishes a good perturbation from a bad one in this testbed context?

- Concept: Language model perplexity as a measure of text domain appropriateness
  - Why needed here: Forms the foundation for POMME's approach to detecting simplification through domain shift
  - Quick check question: How does perplexity change when text shifts from domain-specific to general language?

- Concept: Lexical feature analysis for text simplification assessment
  - Why needed here: Provides interpretable, model-agnostic indicators of simplification quality
  - Quick check question: Which lexical features typically decrease as text becomes simpler?

## Architecture Onboarding

- Component map: CELLS dataset → Perturbation engine (4 types) → Metric evaluation (15 metrics) → Analysis pipeline → POMME implementation
- Critical path: Dataset → Perturbation application → Metric scoring → Correlation analysis → Insight generation
- Design tradeoffs: Granular perturbation control vs. computational cost; synthetic perturbations vs. real-world representativeness
- Failure signatures: Metrics showing inverse sensitivity to expected perturbations; inconsistent metric behavior across datasets
- First 3 experiments:
  1. Apply single perturbation type to small dataset subset and verify expected metric behavior
  2. Test POMME sensitivity across varying simplification degrees within one dataset
  3. Compare POMME performance against baseline metrics on PLABA dataset with gold alignments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can APPLS's perturbation methodology be generalized to other text generation tasks beyond PLS?
- Basis in paper: [explicit] The authors state that APPLS requires only paired source and target documents, suggesting potential extensibility to other tasks.
- Why unresolved: The paper focuses specifically on PLS evaluation and does not test APPLS on other text generation tasks.
- What evidence would resolve it: Applying APPLS perturbations to datasets from other text generation tasks (e.g., translation, dialogue generation) and evaluating whether the perturbation sensitivity patterns align with expected criteria for those tasks.

### Open Question 2
- Question: How can sentence alignment between scientific abstracts and plain language summaries be improved to reduce information loss during simplification perturbations?
- Basis in paper: [explicit] The authors note that their sentence alignment algorithm led to many partial or incorrect matches, and highlight this as a limitation.
- Why unresolved: The paper identifies the issue but does not propose or test improved alignment methods.
- What evidence would resolve it: Developing and evaluating alternative sentence alignment algorithms (e.g., semantic similarity-based methods) on the PLABA dataset and measuring reduction in unmatched sentence pairs.

### Open Question 3
- Question: Can LLM-based prompt evaluations be refined to distinguish between PLS criteria and respond appropriately to simplification perturbations?
- Basis in paper: [explicit] The authors find that LLM evaluations fail to distinguish between criteria and respond inversely to simplification perturbations.
- Why unresolved: The paper does not explore modifications to the prompt structure or evaluation methodology that might improve performance.
- What evidence would resolve it: Experimenting with alternative prompt templates, including more specific instructions for each criterion, and testing whether refined prompts yield more accurate sensitivity to perturbations.

## Limitations
- Sentence alignment issues in CELLS dataset where not all sentences have corresponding simplified versions, potentially affecting perturbation accuracy
- Validation of POMME limited to three datasets, primarily from biomedical domain, requiring further testing on diverse domains and languages
- LLM-based evaluations show fundamental limitations in PLS-specific sensitivity, though prompt engineering solutions were not explored

## Confidence

High confidence: Meta-evaluation framework and perturbation methodology; systematic analysis showing existing metrics fail to capture simplification; POMME effectiveness across multiple datasets

Medium confidence: Perturbation effectiveness due to sentence alignment issues; generalization of findings to non-biomedical domains; potential for improved LLM prompt evaluations

## Next Checks

1. **Perturbation Fidelity Test**: Conduct human evaluation on 100 randomly selected perturbed examples to verify that each perturbation type correctly targets only its intended criterion without unintended side effects on other criteria.

2. **Cross-Domain POMME Validation**: Apply POMME to PLS datasets from non-biomedical domains (e.g., legal, technical documentation) to assess whether the domain shift mechanism generalizes beyond scientific text.

3. **LLM Prompt Engineering Experiment**: Test whether carefully crafted prompts with PLS-specific instructions can improve LLM-based evaluation sensitivity to simplification and coherence criteria, addressing the identified limitations.