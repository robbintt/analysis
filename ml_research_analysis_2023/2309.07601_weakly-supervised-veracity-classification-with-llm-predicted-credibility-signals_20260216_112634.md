---
ver: rpa2
title: Weakly Supervised Veracity Classification with LLM-Predicted Credibility Signals
arxiv_id: '2309.07601'
source_url: https://arxiv.org/abs/2309.07601
tags:
- credibility
- article
- signals
- misinformation
- articles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents Pastel, a weakly supervised approach for veracity
  classification using large language models (LLMs) to extract credibility signals
  from web content. Pastel leverages 18 credibility signals, including evidence, bias,
  and sensationalism, to predict content veracity without relying on human supervision.
---

# Weakly Supervised Veracity Classification with LLM-Predicted Credibility Signals

## Quick Facts
- **arXiv ID**: 2309.07601
- **Source URL**: https://arxiv.org/abs/2309.07601
- **Reference count**: 39
- **Primary result**: Pastel achieves 86.7% of state-of-the-art performance without human supervision, outperforming zero-shot detection by 38.3%

## Executive Summary
Pastel is a weakly supervised approach for veracity classification that leverages large language models (LLMs) to extract 18 credibility signals from web content. These signals include evidence, bias, sensationalism, and source credibility, among others. The system combines zero-shot LLM credibility signal labeling with weak supervision to predict content veracity without relying on human-annotated labels. The approach demonstrates strong performance, achieving 86.7% of the state-of-the-art system trained with human supervision while significantly outperforming zero-shot methods by 38.3%. In cross-domain settings, Pastel shows a 63% improvement over supervised models.

## Method Summary
Pastel extracts credibility signals from articles using a two-stage prompting approach with LLMs. First, open-ended prompts ask LLMs to identify specific credibility signals, followed by restrictive prompts that map responses to predefined classes. The 18 credibility signals include evidence quality, bias detection, sensationalism, and source credibility. These potentially noisy signals are then aggregated using weak supervision (Snorkel framework) to produce final veracity predictions. The system was evaluated on two datasets: FA-KES (804 articles on Syrian war) and EuvsDisinfo (497 articles on geopolitical misinformation), using accuracy and F1-macro scores for binary classification.

## Key Results
- Outperforms zero-shot veracity detection by 38.3% across datasets
- Achieves 86.7% of state-of-the-art performance trained with human supervision
- In cross-domain settings, outperforms supervised state-of-the-art by 63%
- 12 out of 19 proposed credibility signals show strong associations with veracity across all datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weak supervision using LLM-predicted credibility signals reduces reliance on human-annotated labels while maintaining classification accuracy.
- Mechanism: LLMs extract credibility signals from articles and weak supervision algorithms combine them into veracity labels, achieving high performance without ground-truth training data.
- Core assumption: LLM-predicted credibility signals are sufficiently accurate and correlated with true article veracity.
- Evidence anchors:
  - [abstract]: "We validate our approach using four article-level misinformation detection datasets, demonstrating that Pastel outperforms zero-shot veracity detection by 38.3% and achieves 86.7% of the performance of the state-of-the-art system trained with human supervision."
  - [section]: "In contrast, prompted PWS with credibility signals achieves the highest scores for both datasets...achieving the highest scores of 54.8% and 99.0% F 1-macro for FA-KES and EuvsDisinfo, respectively."
  - [corpus]: Weak evidence. Corpus shows related work on weak supervision but no direct evidence for this specific mechanism.
- Break condition: If LLM-predicted signals have low accuracy or weak correlation with true labels, weak supervision performance degrades significantly.

### Mechanism 2
- Claim: Multi-stage prompting reduces LLM hallucinations and improves reliability of credibility signal extraction.
- Mechanism: Open-ended prompts are followed by restrictive prompts to map answers to predefined classes, reducing hallucination impact through structured output.
- Core assumption: Structured prompts can effectively constrain LLM outputs to reduce hallucination effects.
- Evidence anchors:
  - [abstract]: "This multi-stage approach reduces LLMsâ€™ susceptibility to 'hallucinations', since predicting individual signals (with appropriately tailored prompts for each) is simpler than relying on an LLM to predict content veracity."
  - [section]: "To map the model's answer to a specific class, we follow a two-step process. First, we retrieve the model's answer to a question and apply simple string matching rules to extract an objective class."
  - [corpus]: Weak evidence. Corpus shows related work on prompting strategies but no direct evidence for this specific hallucination reduction mechanism.
- Break condition: If restrictive prompts cannot effectively constrain LLM outputs, hallucination effects may still propagate through the system.

### Mechanism 3
- Claim: Weak supervision can combine noisy signals into accurate veracity predictions by learning signal reliability parameters.
- Mechanism: Snorkel framework computes signal accuracies and weights them to produce final veracity predictions, handling signal noise effectively.
- Core assumption: Weak signals are conditionally independent given the true label, allowing effective noise modeling.
- Evidence anchors:
  - [abstract]: "We then aggregate these potentially noisy labels using weak supervision in order to predict content veracity."
  - [section]: "Its label model, based on Ratner et al. (2019), computes the inverse generalized covariance matrix of the junction tree of the dependency graph obtained from the weak signals."
  - [corpus]: Some evidence. Corpus includes related work on weak supervision frameworks showing similar approaches to handling noisy signals.
- Break condition: If signal independence assumption is violated or signal noise is too high, weak supervision performance degrades significantly.

## Foundational Learning

- **Concept**: Weak supervision and programmatic labeling functions
  - Why needed here: The approach relies on combining multiple noisy credibility signals from LLMs to predict veracity without human labels
  - Quick check question: How does weak supervision handle the noise inherent in LLM-predicted credibility signals?

- **Concept**: Credibility signals and their relationship to misinformation
  - Why needed here: The system extracts 18 different credibility signals to assess article veracity
  - Quick check question: Which credibility signals showed the strongest correlation with true veracity labels in the experiments?

- **Concept**: Prompt engineering for structured outputs
  - Why needed here: The two-step prompting approach is crucial for reducing hallucinations and mapping LLM outputs to predefined classes
  - Quick check question: What is the purpose of the second step in the two-step prompting approach described in the paper?

## Architecture Onboarding

- **Component map**: Article text -> LLM prompts -> Credibility signal extraction -> Weak supervision aggregation -> Veracity prediction
- **Critical path**: Article text -> LLM prompts -> Credibility signal extraction -> Weak supervision label model -> Final veracity prediction
- **Design tradeoffs**: Using multiple LLMs vs single LLM, trade-off between prompt complexity and hallucination risk
- **Failure signatures**: Low signal accuracy, high abstention rates, weak supervision model convergence issues
- **First 3 experiments**:
  1. Test individual credibility signal extraction accuracy with each LLM
  2. Evaluate weak supervision label model performance on training data
  3. Compare zero-shot vs prompted PWS performance on small test set

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of Pastel vary across different languages and modalities?
  - Basis in paper: [explicit] The paper mentions plans to extend the research to multiple languages and incorporate different modalities into new credibility signals.
  - Why unresolved: The current study only focuses on English language articles and textual data.
  - What evidence would resolve it: Experiments demonstrating Pastel's performance on datasets in various languages and with multimodal data (e.g., text, images, video).

- **Open Question 2**: Can the proposed method and credibility signals be effectively applied to claim-level fact-check datasets?
  - Basis in paper: [inferred] The paper acknowledges that further research is required to determine if the method can be applied to claim-level fact-check datasets, as the approach has access to significantly less contextual information in such cases.
  - Why unresolved: The current study focuses on article-level veracity classification, and the performance on claim-level datasets is unknown.
  - What evidence would resolve it: Experiments evaluating Pastel's performance on claim-level fact-check datasets and comparing it to existing methods.

- **Open Question 3**: What is the feasibility of obtaining multiple credibility signals in a single prompt without significantly reducing performance?
  - Basis in paper: [explicit] The paper mentions that the proposed method is computationally demanding and further research is needed to investigate the feasibility of obtaining multiple credibility signals in a single prompt.
  - Why unresolved: The current approach prompts the LLM multiple times per example, which is resource-intensive.
  - What evidence would resolve it: Experiments demonstrating the performance of a single-prompt approach for obtaining multiple credibility signals and comparing it to the current multi-prompt approach.

## Limitations

- **Dataset Dependency**: Performance claims are evaluated only on two specific datasets covering conflict and geopolitical misinformation, which may not generalize to other misinformation domains like health or science.
- **LLM Performance Variation**: Different LLMs yield different performance levels, but the analysis doesn't fully explore why certain models perform better or how model selection affects system reliability.
- **Signal Correlation Assumptions**: The weak supervision approach assumes conditional independence of credibility signals, which isn't thoroughly validated and could impact effectiveness.

## Confidence

- **High Confidence** in the core claim that weak supervision with LLM-predicted credibility signals can achieve strong performance (86.7% of supervised state-of-the-art) without human-labeled training data, supported by direct experimental evidence across two datasets.
- **Medium Confidence** in the claim that the multi-stage prompting approach significantly reduces hallucinations, as the evidence is primarily theoretical and based on performance improvements rather than direct hallucination measurement.
- **Medium Confidence** in the generalizability of the 18 credibility signals across domains, as the analysis shows some signals are domain-specific and the evaluation is limited to two datasets.

## Next Checks

1. **Signal Independence Validation**: Conduct formal tests to validate the conditional independence assumption of credibility signals in the weak supervision framework, including computing pairwise correlations and testing label model performance when this assumption is violated.

2. **Cross-Domain Robustness Test**: Evaluate Pastel on at least three additional misinformation domains (e.g., health, climate, elections) to verify claimed cross-domain generalization capabilities and identify which credibility signals are truly universal versus domain-specific.

3. **Hallucination Measurement**: Implement direct hallucination detection metrics (e.g., factuality scores, consistency checks) to quantify the actual reduction in hallucinations achieved by the two-stage prompting approach, rather than inferring it from downstream performance improvements.