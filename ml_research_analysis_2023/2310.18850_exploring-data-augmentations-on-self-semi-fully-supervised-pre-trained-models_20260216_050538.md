---
ver: rpa2
title: Exploring Data Augmentations on Self-/Semi-/Fully- Supervised Pre-trained Models
arxiv_id: '2310.18850'
source_url: https://arxiv.org/abs/2310.18850
tags:
- learning
- data
- invariance
- views
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how different data augmentation techniques\
  \ impact the performance of self-, semi-, and fully-supervised contrastive pre-trained\
  \ models. The authors apply four augmentation methods\u2014Random Erasing, CutOut,\
  \ CutMix, and MixUp\u2014to MoCo v2 across various vision tasks."
---

# Exploring Data Augmentations on Self-/Semi-/Fully- Supervised Pre-trained Models

## Quick Facts
- arXiv ID: 2310.18850
- Source URL: https://arxiv.org/abs/2310.18850
- Reference count: 40
- Key outcome: MixUp achieves highest top-1 accuracy (71.2% on ImageNet-1K) and best invariance score, demonstrating that augmentation-induced trade-offs between invariance and diversity improve downstream performance.

## Executive Summary
This study investigates how different data augmentation techniques impact the performance of self-, semi-, and fully-supervised contrastive pre-trained models. The authors apply four augmentation methods—Random Erasing, CutOut, CutMix, and MixUp—to MoCo v2 across various vision tasks. They introduce quantitative metrics for invariance (Linv) and diversity (Ldiv) of feature embeddings and evaluate them on ImageNet-100 and ImageNet-1K. Results show that MixUp achieves the highest top-1 accuracy (71.2% on ImageNet-1K) and best invariance score, while reducing diversity. Masking-based augmentations (Random Erasing, CutOut) decrease invariance but increase diversity. Manual labels do not alter invariance or diversity. The study demonstrates that combining high invariance with appropriate diversity improves downstream performance across image classification, object detection, instance segmentation, and semantic segmentation.

## Method Summary
The authors apply four data augmentation techniques (Random Erasing, CutOut, CutMix, MixUp) to MoCo v2 and train on ImageNet-100 and ImageNet-1K. They introduce invariance (Linv) and diversity (Ldiv) metrics to quantify the quality of feature embeddings from augmented views. Pre-trained models are evaluated on downstream tasks including image classification, object detection, instance segmentation, and semantic segmentation. The study compares fully supervised (100% labels), semi-supervised (50% labels), and self-supervised (no labels) settings.

## Key Results
- MixUp achieves highest top-1 accuracy (71.2% on ImageNet-1K) and best invariance score while reducing diversity
- Masking-based augmentations (Random Erasing, CutOut) decrease invariance but increase diversity
- Manual annotations do not alter invariance or diversity metrics but improve downstream performance through better supervision
- The balance between high invariance and appropriate diversity correlates with improved downstream task performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data augmentations that mask regions of images decrease feature embedding invariance but increase diversity, and this trade-off improves downstream task performance.
- Mechanism: Masking-based augmentations (Random Erasing, CutOut) force the model to learn features that are robust to missing or occluded information, thereby increasing the diversity of learned representations. However, this comes at the cost of reducing the similarity between augmented views of the same image, lowering invariance. MixUp, which blends two images, maintains higher invariance while still boosting diversity, leading to better performance across tasks.
- Core assumption: The quality of learned features is determined by a balance between invariance (similarity between views of the same sample) and diversity (differences between views), and different tasks benefit from different balances.
- Evidence anchors:
  - [abstract] "Masking regions of the images decreases the invariance of the learned feature embedding while providing a more considerable diversity."
  - [section] "We observe that: 1) Masking regions of the images decreases the invariance of the learned feature embedding while providing a more considerable diversity."
  - [corpus] Weak or missing direct evidence for downstream task performance differences.
- Break condition: If downstream tasks do not benefit from higher diversity at the cost of lower invariance, or if the invariance-diversity balance is not task-specific, the mechanism fails.

### Mechanism 2
- Claim: Manual annotations do not alter the invariance or diversity of learned feature embeddings, but they improve downstream performance by providing better supervision.
- Mechanism: The presence of labels changes the contrastive learning objective by modifying the negative sample set, but the data augmentation process and its effect on invariance and diversity remain unchanged. The improvement in downstream tasks comes from better supervision during pre-training, not from changes in the augmentation-induced invariance or diversity.
- Core assumption: The metrics Linv and Ldiv measure properties of the augmented views themselves, independent of whether labels are used in the contrastive loss.
- Evidence anchors:
  - [abstract] "Manual annotations do not change the invariance or diversity of the learned feature embedding."
  - [section] "We observe that: 2) Manual annotations do not change the invariance or diversity of the learned feature embedding."
  - [corpus] Weak or missing direct evidence for the invariance/diversity independence from labels.
- Break condition: If the presence of labels somehow changes the augmentation process or the model's response to augmentations, altering Linv or Ldiv, the mechanism breaks.

### Mechanism 3
- Claim: The proposed metrics Linv and Ldiv can quantify the quality of augmented views and predict downstream task performance.
- Mechanism: Linv measures the average similarity between augmented views and the original image, indicating how well the augmentation preserves essential information. Ldiv measures the average similarity between different augmented views, indicating how much variation the augmentation introduces. Higher Linv with appropriate Ldiv correlates with better downstream performance, as seen with MixUp.
- Core assumption: There is a meaningful relationship between the invariance-diversity balance of augmented views and the quality of learned representations for downstream tasks.
- Evidence anchors:
  - [abstract] "We then explicitly evaluate the invariance and diversity of the feature embedding."
  - [section] "We then introduce two metrics, invariance, and diversity, to measure the quality of the augmented views between the anchor."
  - [corpus] Weak or missing direct evidence for the predictive power of Linv and Ldiv for all downstream tasks.
- Break condition: If Linv and Ldiv do not correlate with downstream performance across a range of tasks and datasets, or if other metrics are more predictive, the mechanism fails.

## Foundational Learning

- Concept: Contrastive learning and the InfoNCE loss.
  - Why needed here: The paper evaluates how data augmentations affect self-/semi-/fully-supervised contrastive pre-trained models. Understanding the contrastive learning framework and the InfoNCE loss is essential to grasp how augmentations influence the learning process.
  - Quick check question: In the InfoNCE loss, what is the role of the temperature parameter τ, and how does it affect the contribution of positive and negative pairs?

- Concept: Data augmentation techniques (Random Erasing, CutOut, CutMix, MixUp).
  - Why needed here: The paper applies these four augmentation techniques to pre-trained models and evaluates their impact on invariance and diversity. Understanding how each technique modifies the input data is crucial for interpreting the results.
  - Quick check question: How does MixUp differ from CutMix in terms of how they combine two images, and what impact might this have on the learned features?

- Concept: Linear classification evaluation on frozen features.
  - Why needed here: The paper uses linear classification on frozen features from pre-trained models as a way to evaluate the quality of the learned representations. Understanding this evaluation protocol is important for interpreting the reported top-1 and top-5 accuracies.
  - Quick check question: In linear classification on frozen features, why is the feature extractor kept frozen during training, and what does this tell us about the quality of the pre-trained representations?

## Architecture Onboarding

- Component map:
  Pre-trained models (MoCo v2) -> Data augmentation modules (Random Erasing, CutOut, CutMix, MixUp) -> Invariance/diversity metrics (Linv, Ldiv) -> Downstream task modules (image classification, object detection, instance segmentation, semantic segmentation) -> Evaluation metrics

- Critical path:
  1. Pre-train models with different augmentations using the contrastive learning framework
  2. Compute Linv and Ldiv on the pre-trained models
  3. Fine-tune pre-trained models on downstream tasks
  4. Compare downstream task performance and Linv/Ldiv scores across augmentations

- Design tradeoffs:
  - Using ImageNet-100 for faster experimentation vs. ImageNet-1K for more comprehensive results
  - Computing Linv and Ldiv on a subset of the data for efficiency vs. the entire dataset for accuracy
  - Implementing augmentations as part of the data loading pipeline vs. as a separate preprocessing step

- Failure signatures:
  - If Linv and Ldiv do not correlate with downstream task performance, the metrics may not be capturing the right properties of the learned representations
  - If downstream task performance does not improve with higher Linv and appropriate Ldiv, the assumption that invariance and diversity are key to good representations may be incorrect
  - If the choice of augmentation does not significantly impact Linv or Ldiv, the augmentations may not be diverse enough or the metrics may not be sensitive enough

- First 3 experiments:
  1. Pre-train MoCo v2 with and without MixUp on ImageNet-100, compute Linv and Ldiv, and evaluate linear classification performance. This tests the basic mechanism that MixUp improves invariance and diversity, leading to better downstream performance.
  2. Pre-train MoCo v2 with Random Erasing and CutOut on ImageNet-100, compute Linv and Ldiv, and evaluate linear classification performance. This tests the mechanism that masking-based augmentations decrease invariance but increase diversity.
  3. Pre-train MoCo v2 with MixUp using 50% and 100% labels on ImageNet-1K, compute Linv and Ldiv, and evaluate linear classification performance. This tests the mechanism that manual annotations do not change invariance or diversity but improve downstream performance through better supervision.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do invariance and diversity metrics scale when applied to transformer-based architectures like DINO?
- Basis in paper: [inferred] The authors note they did not experiment with transformer-based frameworks due to computational constraints.
- Why unresolved: The study was limited to ResNet-50 on ImageNet-100, leaving scalability to larger models untested.
- What evidence would resolve it: Experiments comparing invariance and diversity metrics across ResNet and transformer architectures on large-scale datasets.

### Open Question 2
- Question: What is the impact of combining multiple augmentation techniques (e.g., MixUp + CutMix) on invariance and diversity trade-offs?
- Basis in paper: [inferred] The study focused on individual augmentations but did not explore combinations, which could reveal synergistic effects.
- Why unresolved: The paper only evaluated single augmentation methods, leaving multi-augmentation strategies unexplored.
- What evidence would resolve it: Empirical results comparing mixed augmentation strategies against single methods on downstream tasks.

### Open Question 3
- Question: How does the choice of negative sample queue size affect invariance and diversity in semi-/fully-supervised contrastive learning?
- Basis in paper: [explicit] The authors mention maintaining negative sample queues for labelled and unlabelled data but do not analyze queue size effects.
- Why unresolved: The study used fixed queue sizes without exploring how varying them impacts feature embedding metrics.
- What evidence would resolve it: Experiments varying negative queue sizes and measuring corresponding changes in Linv and Ldiv.

## Limitations
- The study focuses primarily on MoCo v2, limiting generalizability to other contrastive learning frameworks
- The semi-supervised setting uses only 50% labels, which may not represent realistic semi-supervised scenarios
- The proposed invariance (Linv) and diversity (Ldiv) metrics require more extensive validation across diverse datasets and tasks

## Confidence
- **High confidence**: The observation that MixUp achieves highest top-1 accuracy (71.2%) and best invariance score on ImageNet-1K is well-supported by empirical results
- **Medium confidence**: The claim that masking-based augmentations trade invariance for diversity is supported by metrics but needs validation across more tasks and datasets
- **Medium confidence**: The assertion that manual labels don't affect invariance/diversity metrics is supported by experimental results but requires further testing with different label fractions and tasks

## Next Checks
1. Replicate Linv and Ldiv calculations on a held-out validation set to verify metric consistency across runs
2. Test the invariance-diversity relationship on additional contrastive learning frameworks (SimCLR, BYOL) to assess generalizability
3. Evaluate downstream performance on medical imaging or satellite imagery datasets to test cross-domain applicability