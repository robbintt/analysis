---
ver: rpa2
title: Detecting Pretraining Data from Large Language Models
arxiv_id: '2310.16789'
source_url: https://arxiv.org/abs/2310.16789
tags:
- data
- pretraining
- harry
- detection
- min-k
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of detecting whether a given text
  was used in the pretraining data of a large language model (LLM). Existing methods
  for membership inference attacks require access to the pretraining data distribution
  or expensive training of reference models, which is infeasible for LLMs.
---

# Detecting Pretraining Data from Large Language Models

## Quick Facts
- arXiv ID: 2310.16789
- Source URL: https://arxiv.org/abs/2310.16789
- Authors: Ekin AkyÃ¼rek, Tianyi Zhang, Xinying Song, Yao Lu, Dylan Hadfield-Menell, Jacob Andreas
- Reference count: 36
- Key outcome: Introduces Min-K% Prob method achieving 7.4% improvement in AUC score over existing methods for detecting LLM pretraining data

## Executive Summary
This paper addresses the challenge of detecting whether a given text was used in the pretraining data of large language models (LLMs) without requiring access to the pretraining data distribution or reference models. The authors introduce Min-K% Prob, a reference-free detection method that computes the average log-probability of the k% of tokens with the lowest probabilities in a text. They also create WIKI MIA, a dynamic benchmark using Wikipedia events before and after LLM pretraining as member and non-member data. Experiments demonstrate that their method achieves state-of-the-art performance with a 7.4% improvement in AUC score over existing methods, and show effectiveness in real-world scenarios including detecting copyrighted books, dataset contamination, and auditing machine unlearning.

## Method Summary
The Min-K% Prob method works by computing token probabilities using the target LLM, identifying the k% of tokens with the lowest probabilities, and calculating their average log-likelihood. The method assumes that unseen examples contain more outlier tokens with low probabilities than seen examples. The authors introduce WIKI MIA as a benchmark where member data consists of Wikipedia events before LLM pretraining and non-member data consists of events after pretraining. The method requires only black-box access to the LLM for token probability computation, making it practical for real-world applications. Experiments test the method across multiple model architectures (LLaMA, GPT-Neo, Pythia) and demonstrate effectiveness in detecting copyrighted content, contaminated datasets, and evaluating machine unlearning.

## Key Results
- Min-K% Prob achieves 7.4% improvement in AUC score over existing methods on WIKI MIA benchmark
- Detection performance increases with model size, suggesting larger models are more likely to memorize pretraining data
- Longer texts are easier to detect than shorter texts due to more memorized information
- Method successfully identifies copyrighted books from Books3 dataset and detects dataset contamination
- Effective for auditing machine unlearning by identifying data that models should have forgotten

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unseen examples contain outlier tokens with lower probabilities than seen examples
- Mechanism: The Min-K% Prob method identifies the k% of tokens with the lowest probabilities in a text and computes their average log-likelihood. Since unseen examples contain more outlier tokens with low probabilities, their average log-likelihood will be higher than that of seen examples.
- Core assumption: The probability distribution of tokens in seen text differs significantly from unseen text
- Evidence anchors:
  - [abstract] "an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities"
  - [section 3] "Our method is based on a simple hypothesis: an unseen example tends to contain a few outlier words with high negative log-likelihood (or low probability), while a seen example is less likely to contain words with such low probabilities"
- Break condition: If the pretraining data distribution is uniform or if the model has perfect generalization

### Mechanism 2
- Claim: Larger models are more likely to memorize pretraining data
- Mechanism: As model size increases, the number of parameters increases, making it more likely that the model will memorize specific training examples. This increased memorization makes detection easier because the model will assign higher probabilities to seen tokens.
- Core assumption: Model size correlates with memorization capacity
- Evidence anchors:
  - [section 4.4] "This is likely because larger models have more parameters and thus are more likely to memorize the pretraining data"
  - [section 6.2] "a higher learning rate leads to more memorization rather than generalization for these downstream tasks"
- Break condition: If the model uses regularization techniques that specifically prevent memorization

### Mechanism 3
- Claim: Longer texts contain more information that can be memorized by the model
- Mechanism: Longer texts provide more opportunities for the model to have seen and memorized specific token sequences during pretraining. This makes longer texts easier to detect because they contain more distinctive patterns.
- Core assumption: Text length correlates with the amount of distinctive memorized content
- Evidence anchors:
  - [section 4.4] "longer texts contain more information memorized by the target model, making them more distinguishable from the unseen texts"
  - [section 2.2] "our results reveal that data length significantly impacts the difficulty of detection"
- Break condition: If the model uses techniques that focus on local context rather than global memorization

## Foundational Learning

- Concept: Membership Inference Attack (MIA)
  - Why needed here: The paper builds on the MIA framework to develop a method for detecting pretraining data
  - Quick check question: What is the main difference between pretraining data detection and fine-tuning data detection in the context of MIA?

- Concept: Probability calibration and thresholding
  - Why needed here: The Min-K% Prob method uses probability scores and thresholds to classify text as seen or unseen
  - Quick check question: How does the choice of threshold affect the true positive rate and false positive rate in membership inference attacks?

- Concept: Dataset contamination and its implications
  - Why needed here: The paper explores how dataset contamination affects detection difficulty and model performance
  - Quick check question: What are the potential consequences of dataset contamination for model evaluation and copyright issues?

## Architecture Onboarding

- Component map: Input text -> LLM token probability computation -> Min-K% Prob algorithm -> Average log-likelihood calculation -> Threshold comparison -> Classification (seen/unseen)

- Critical path:
  1. Compute token probabilities using LLM
  2. Identify k% of tokens with lowest probabilities
  3. Calculate average log-likelihood of these tokens
  4. Compare to threshold to determine membership

- Design tradeoffs:
  - Choice of k% affects sensitivity vs specificity
  - Larger k% may capture more information but reduce discriminative power
  - Black-box access limits ability to fine-tune model for better detection

- Failure signatures:
  - High false positive rate: Threshold too low or k% too large
  - High false negative rate: Threshold too high or k% too small
  - Inconsistent results across different model sizes: Underlying assumptions about memorization may not hold

- First 3 experiments:
  1. Validate Min-K% Prob performance on synthetic data with known membership status
  2. Test sensitivity to choice of k% parameter on held-out validation set
  3. Compare performance across different model sizes to verify correlation with memorization capacity

## Open Questions the Paper Calls Out

- Open Question 1: Does the Min-K% Prob method remain effective when applied to non-Western languages or multilingual text?
  - Basis in paper: [inferred] The paper evaluates the method primarily on English Wikipedia text and does not explore its applicability to other languages.
  - Why unresolved: The paper does not provide evidence or experiments involving multilingual or non-Western language text.
  - What evidence would resolve it: Testing Min-K% Prob on diverse language datasets and comparing its performance across languages.

- Open Question 2: How does the Min-K% Prob method perform when detecting paraphrased text that is semantically similar but structurally different?
  - Basis in paper: [explicit] The paper introduces a paraphrase setting in the WIKI MIA benchmark but does not provide detailed results or analysis for this setting.
  - Why unresolved: The paper mentions the paraphrase setting but lacks comprehensive evaluation or discussion of the method's effectiveness in this context.
  - What evidence would resolve it: Detailed experiments and analysis comparing the detection performance on original vs. paraphrased text.

- Open Question 3: What is the impact of varying the percentage of tokens (k%) selected by the Min-K% Prob method on its detection accuracy?
  - Basis in paper: [explicit] The paper mentions that k = 20 works best after a small sweep but does not explore the impact of different k values extensively.
  - Why unresolved: The paper does not provide a thorough analysis of how different k values affect the method's performance.
  - What evidence would resolve it: Systematic experiments varying k% and analyzing its effect on detection accuracy and robustness.

- Open Question 4: How does the Min-K% Prob method handle texts with varying levels of noise or errors, such as OCR errors or typos?
  - Basis in paper: [inferred] The paper does not address the method's robustness to noisy or erroneous text, which could be common in real-world scenarios.
  - Why unresolved: The paper focuses on clean text data and does not explore the method's performance in the presence of noise or errors.
  - What evidence would resolve it: Experiments testing the method's effectiveness on text with deliberate noise or errors, such as OCR errors or typos.

## Limitations

- The method relies on the assumption that unseen examples contain more outlier tokens, which may not hold for models with different training objectives or regularization techniques
- The WIKI MIA benchmark's clear temporal separation between member and non-member data may not reflect real-world scenarios where pretraining data boundaries are less distinct
- Detection performance correlates strongly with model size, suggesting the method may not work well on smaller models or those with anti-overfitting measures
- Black-box access requirement may limit practical deployment in scenarios where model access is restricted

## Confidence

**High Confidence:** The core mechanism of using outlier token probabilities for detection is well-supported by empirical results across multiple model architectures and dataset types. The 7.4% improvement in AUC score over existing methods on the WIKI MIA benchmark provides strong evidence for the method's effectiveness in controlled settings.

**Medium Confidence:** Claims about model size correlation with detection performance are reasonably supported but may not hold for all model architectures. The observed relationship between memorization capacity and detection accuracy is plausible but could be influenced by other factors not fully controlled in the experiments.

**Low Confidence:** The generalizability of results to real-world scenarios involving copyrighted materials and dataset contamination is less certain. While the method shows promise in these applications, the specific conditions and data distributions may not reflect the complexity of actual infringement or contamination cases.

## Next Checks

1. **Cross-Architecture Validation:** Test the Min-K% Prob method across diverse LLM architectures including encoder-decoder models, retrieval-augmented models, and models with different tokenization schemes to verify the robustness of the outlier token detection mechanism.

2. **Temporal Boundary Testing:** Create variants of the WIKI MIA benchmark with increasingly blurred temporal boundaries between member and non-member data to assess how detection performance degrades as the temporal separation becomes less distinct.

3. **Real-World Contamination Simulation:** Design controlled experiments that simulate realistic dataset contamination scenarios with varying degrees of overlap between pretraining data and test inputs, measuring both detection accuracy and false positive rates under these conditions.