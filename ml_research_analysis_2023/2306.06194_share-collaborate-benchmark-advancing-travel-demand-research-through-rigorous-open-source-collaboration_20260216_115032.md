---
ver: rpa2
title: 'Share, Collaborate, Benchmark: Advancing Travel Demand Research through rigorous
  open-source collaboration'
arxiv_id: '2306.06194'
source_url: https://arxiv.org/abs/2306.06194
tags:
- prediction
- dynamic
- short-term
- conditions
- demand
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces a benchmarking infrastructure for short-term\
  \ transit demand prediction models, tested under stable and highly dynamic conditions\
  \ (COVID-19 pandemic and month-long protest). Five widely used methodologies\u2014\
  ARIMA, SARIMA, MLP, CNN, and LSTM\u2014are implemented in an open-source codebase\
  \ using five years of Bogot\xE1 BRT smartcard data."
---

# Share, Collaborate, Benchmark: Advancing Travel Demand Research through rigorous open-source collaboration

## Quick Facts
- arXiv ID: 2306.06194
- Source URL: https://arxiv.org/abs/2306.06194
- Reference count: 8
- Key outcome: LSTM with online training adapts fastest during demand shocks (1.5 months vs 3+ months for other models)

## Executive Summary
This study establishes a benchmarking infrastructure for short-term transit demand prediction, testing five widely-used methodologies under stable and highly dynamic conditions including COVID-19 and protests. Using five years of Bogotá BRT smartcard data, the research demonstrates that while most models perform similarly during stable periods, LSTM with online adaptive training and multi-output design significantly outperforms others during demand shocks. The open-source codebase enables reproducible research and collaborative benchmarking in travel demand modeling.

## Method Summary
The research benchmarks five demand prediction models (ARIMA, SARIMA, MLP, CNN, LSTM) using five years of Bogotá BRT smartcard data, testing both static and online training approaches with single and multi-output designs. Models are evaluated using mean arctangent absolute percentage error (MAAPE) at the system level rather than individual stations. The study compares model performance across three conditions: stable demand, COVID-19 pandemic, and month-long protests, with particular focus on adaptation speed to demand shocks.

## Key Results
- During stable conditions, all models achieve similar MAAPE (0.08-0.12) with no significant differences
- LSTM with online training and multi-output design adapts fastest to COVID-19 demand shocks, stabilizing MAAPE within 1.5 months versus over 3 months for other models
- System-level MAAPE metric provides reliable performance assessment during dynamic conditions where station-level metrics would be unstable
- Online training significantly outperforms static models during dynamic events, while differences are negligible during stable periods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-output online-training LSTM models adapt faster to demand shocks than single-output static models.
- **Mechanism:** Online training allows model parameters to be updated incrementally with each new observation, enabling continuous adaptation to evolving patterns. Multi-output design leverages spatial and temporal correlations across stations, providing a richer signal for learning shifts in demand patterns.
- **Core assumption:** Demand dynamics during shocks exhibit shared patterns across stations that can be captured through joint modeling.
- **Evidence anchors:**
  - [abstract] "LSTM with adaptive (online) training and multi-output design adapts fastest—MAAPE stabilizes within 1.5 months versus over 3 months for other models"
  - [section] "In the COVID-19 pandemic condition, a long-short-term memory model with adaptive training and a multi-output design outperformed other models, adapting faster to disruptions. The prediction error stabilized within approximately 1.5 months, whereas other models continued to exhibit higher error rates even a year after the start of the pandemic."
  - [corpus] Weak: No direct evidence in corpus papers; this is a novel contribution of the study.
- **Break condition:** If demand shocks are localized rather than system-wide, multi-output models may dilute localized patterns with irrelevant spatial correlations.

### Mechanism 2
- **Claim:** System-level MAAPE metric provides more generalizable performance assessment than station-level metrics during dynamic conditions.
- **Mechanism:** Aggregating errors across the entire transit system captures broader behavioral shifts and reduces the impact of isolated anomalies or station closures, providing a stable summary of overall model performance.
- **Core assumption:** Station-level demand patterns are sufficiently correlated to make system-level aggregation meaningful for performance assessment.
- **Evidence anchors:**
  - [section] "The average error of the entire transportation system is given as follows" and "Therefore, it can also compare performances in multiple geographical areas."
  - [section] "Unlike the current trends in the literature that test accuracy at the individual level, this metric serves as a summary metric to test model performance."
  - [corpus] Weak: Most related papers focus on individual station prediction; system-level assessment is not common in the corpus.
- **Break condition:** If system-level demand is dominated by a few major stations while many minor stations show divergent behavior, system-level metrics may mask poor performance at critical locations.

### Mechanism 3
- **Claim:** Static models (including those with periodic updates like ARIMA/SARIMA) cannot adapt quickly enough to capture demand shifts during highly dynamic conditions.
- **Mechanism:** Static models learn parameters once on historical data and maintain them, making them slow to respond to structural changes. Even with periodic updates, the fixed-window learning cannot capture rapid shifts in demand patterns.
- **Core assumption:** Demand during dynamic conditions changes faster than the model can adapt through its update schedule.
- **Evidence anchors:**
  - [section] "the online-training experiments revealed a faster decrease in the MAAPE during the COVID-19 pandemic; however, there is a noticeable difference in the expected time for the MAAPE to stabilize. On average, models in the single-output and online-training experiments take three months, whereas the LSTM model in the multi-output and online-training experiments takes 1.5 months."
  - [section] "In the COVID-19 condition, online models significantly outperform static models."
  - [corpus] Weak: Related papers focus on static modeling approaches; online learning for transit demand is not well-represented.
- **Break condition:** If dynamic conditions are short-lived (days rather than months), even online models may not have sufficient time to stabilize before conditions normalize.

## Foundational Learning

- **Time Series Forecasting Concepts:**
  - Why needed here: The entire methodology relies on modeling temporal dependencies in transit demand data across multiple years and during dynamic events.
  - Quick check question: What is the difference between ARIMA and SARIMA models, and when would you choose one over the other?

- **Neural Network Architectures:**
  - Why needed here: The study compares multiple deep learning approaches (MLP, CNN, LSTM) for demand prediction, requiring understanding of their strengths and weaknesses.
  - Quick check question: How does an LSTM cell maintain long-term dependencies differently from a standard RNN cell?

- **Online/Incremental Learning:**
  - Why needed here: The key innovation involves online training strategies that update models as new data arrives, critical for dynamic conditions.
  - Quick check question: What is catastrophic forgetting in neural networks, and how does online training attempt to address it?

## Architecture Onboarding

- **Component map:**
  - Data pipeline: Smartcard transaction data → daily aggregation → feature engineering (temporal variables, sine/cosine encodings)
  - Model zoo: ARIMA, SARIMA, MLP, CNN, LSTM implementations with single-output and multi-output variants
  - Training strategies: Static (train once) vs Online (incremental updates)
  - Evaluation: System-wide MAAPE calculation, regression analysis for statistical significance
  - Benchmarking infrastructure: Open-source codebase with standardized preprocessing and evaluation

- **Critical path:**
  1. Data preprocessing and feature engineering
  2. Model selection and configuration
  3. Training (static or online)
  4. Prediction generation
  5. MAAPE calculation across test period
  6. Statistical analysis of results

- **Design tradeoffs:**
  - Multi-output vs single-output: Multi-output captures spatial correlations but increases computational complexity and may dilute localized patterns
  - Online vs static training: Online adapts faster to changes but requires more computational resources during prediction phase
  - Model complexity vs interpretability: Deep learning models perform better but are less interpretable than statistical models
  - Resolution vs scalability: Daily aggregation enables ARIMA/SARIMA modeling but may miss short-term fluctuations

- **Failure signatures:**
  - Persistent high MAAPE during stable periods suggests model misspecification or insufficient training data
  - Sudden MAAPE spikes during dynamic events indicate poor adaptation to structural changes
  - Online models that don't improve over time suggest learning rate issues or insufficient model capacity
  - Multi-output models performing worse than single-output may indicate uncorrelated station behaviors

- **First 3 experiments:**
  1. Run baseline static single-output LSTM model to establish performance benchmark under stable conditions
  2. Implement online training variant of the same model to test adaptation speed during COVID-19 period
  3. Compare single-output vs multi-output configurations during the protest period to assess spatial correlation benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications to LSTM models could further reduce the time needed to adapt to highly dynamic conditions (currently 1.5 months)?
- Basis in paper: [explicit] The paper notes that LSTM with adaptive training and multi-output design stabilizes within 1.5 months during COVID-19, while other models take over 3 months.
- Why unresolved: The study tested one specific LSTM configuration but did not explore variations in architecture, hyperparameter tuning, or alternative training strategies that might accelerate adaptation.
- What evidence would resolve it: Systematic comparison of multiple LSTM variants (different cell types, attention mechanisms, ensemble approaches) with quantitative measurement of adaptation speed during simulated dynamic events.

### Open Question 2
- Question: How would the inclusion of external data sources (weather, social media, land use) impact model performance during highly dynamic conditions compared to the baseline using only historical demand data?
- Basis in paper: [inferred] The authors explicitly excluded external data to focus on methodological comparison, but acknowledge this as a limitation in their methodology section.
- Why unresolved: The study deliberately avoided external data to isolate methodological effects, leaving the potential contribution of contextual variables unexplored.
- What evidence would resolve it: Head-to-head comparison of identical model architectures with and without external data during both stable and dynamic periods, measuring relative performance improvements.

### Open Question 3
- Question: What are the specific mechanisms by which multi-output models capture spatial and temporal correlations across stations, and how do these compare to alternative approaches like graph neural networks?
- Basis in paper: [explicit] The paper discusses that multi-output models capture spatial and temporal correlations but does not provide detailed analysis of the specific correlation structures learned.
- Why unresolved: While the paper demonstrates improved performance with multi-output designs, it does not analyze which stations benefit most or the nature of the learned correlations.
- What evidence would resolve it: Correlation analysis of predicted vs. actual demand patterns across station pairs, comparison of learned weights between multi-output and graph-based approaches, and identification of station-specific adaptation patterns.

## Limitations
- Findings based on single transit system (Bogotá BRT) may not generalize to different urban contexts or governance structures
- Daily aggregation may mask important short-term fluctuations that could affect model performance
- COVID-19 and protest periods represent specific types of demand shocks that may not encompass all possible transit disruptions

## Confidence
- **High confidence:** The mechanism of online training enabling faster adaptation to demand shocks is well-supported by the experimental results showing 1.5-month stabilization versus 3+ months for static models.
- **Medium confidence:** The benefit of multi-output design leveraging spatial correlations is demonstrated but could be context-dependent on station proximity and demand patterns.
- **Medium confidence:** The superiority of LSTM with adaptive training during dynamic conditions is shown, though the specific architecture details and hyperparameters significantly influence results.

## Next Checks
1. Test the benchmarking framework on a different transit system with contrasting characteristics (e.g., rail-based system, different urban density) to assess generalizability of the online learning benefits.
2. Conduct ablation studies removing the multi-output component to quantify the exact contribution of spatial correlation modeling versus online adaptation.
3. Implement shorter aggregation windows (e.g., hourly) to determine if the online learning advantages persist at finer temporal resolutions and whether model architectures need adjustment.