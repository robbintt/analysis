---
ver: rpa2
title: Passive learning of active causal strategies in agents and language models
arxiv_id: '2305.16183'
source_url: https://arxiv.org/abs/2305.16183
tags:
- causal
- agent
- learning
- language
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work shows that passive learning can enable agents and language
  models to learn strategies for causal experimentation and exploitation. Agents trained
  via imitation on expert data can generalize at test time to infer and use causal
  links never seen in training, and to experiment optimally in novel variable sets.
---

# Passive learning of active causal strategies in agents and language models

## Quick Facts
- arXiv ID: 2305.16183
- Source URL: https://arxiv.org/abs/2305.16183
- Reference count: 40
- Agents trained via imitation on expert data can generalize to infer and use causal links never seen in training, and to experiment optimally in novel variable sets.

## Executive Summary
This work demonstrates that passive learning can enable agents and language models to learn strategies for causal experimentation and exploitation. Agents trained via imitation on expert data can generalize at test time to infer and use causal links never seen in training, and to experiment optimally in novel variable sets. Natural language explanations support this generalization, enabling even perfect generalization from confounded training data. Language models trained only on next-word prediction can generalize causal intervention strategies from few-shot prompts with explanations. These results highlight the surprising power of passive learning for causal strategies, especially when supported by explanations.

## Method Summary
The work trains agents via behavioral cloning (imitation learning) on expert trajectories in simple causal DAG environments and odd-one-out environments. For causal DAGs, agents learn to first explore by intervening on all variables to discover causal structure, then exploit that structure to achieve goals. For odd-one-out tasks, agents identify the object differing along one dimension (color, shape, or texture) from others. Some agents receive auxiliary explanation prediction loss to generate natural language explanations of their choices. The work also evaluates whether language models can generalize causal intervention strategies from few-shot prompts containing examples with explanations and reasoning traces.

## Key Results
- Agents trained via imitation on expert data can generalize at test time to infer and use causal links never seen in training
- Natural language explanations enable generalization from perfectly-confounded training data
- Language models trained only on next-word prediction can generalize causal intervention strategies from few-shot prompts with explanations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Passive learning can enable generalization to causal structures never seen in training.
- **Mechanism:** The agent learns a strategy of first experimenting to discover causal structure, then exploiting that structure to achieve goals. During training, it observes expert demonstrations that explore every variable to determine causal structure, then use that structure optimally. This allows the agent to learn a generalizable exploration strategy that works on new variable sets, and an exploitation strategy that adapts to novel causal structures revealed by experiments.
- **Core assumption:** The agent can intervene at test time to collect its own interventional data.
- **Evidence anchors:**
  - [abstract] "agents trained via imitation on expert data can indeed generalize at test time to infer and use causal links which are never present in the training data"
  - [section] "we formally explain why it is possible for an agent to learn a generalizable strategy for exploiting causal structure from passive data, as long as the agent can intervene at test time"
  - [corpus] Weak evidence - the related papers focus on active data collection rather than passive generalization

### Mechanism 2
- **Claim:** Natural language explanations enable generalization from confounded training data.
- **Mechanism:** Explanations explicitly highlight the latent causal structure, allowing the agent to focus on relevant features even when the training data is perfectly confounded. By predicting explanations during training, the agent learns to attend to specific causal dimensions rather than just correlational patterns.
- **Core assumption:** Explanations provide explicit links between observations and causal structure.
- **Evidence anchors:**
  - [abstract] "Explanations can even allow passive learners to generalize out-of-distribution from perfectly-confounded training data"
  - [section] "natural language explanations support learning and generalizing causal strategies from passive data, and can enable generalization out-of-distribution even from totally confounded training data"
  - [corpus] Weak evidence - no direct corpus support for explanation-driven deconfounding

### Mechanism 3
- **Claim:** Language models can generalize causal strategies from few-shot prompts with explanations.
- **Mechanism:** The language model leverages its strong prior knowledge from passive next-word prediction training, combined with a few high-quality examples that include explanations and reasoning traces. This allows it to generalize causal intervention strategies to new dimensions not present in the examples.
- **Core assumption:** The language model has learned relevant causal priors during pretraining.
- **Evidence anchors:**
  - [abstract] "language models, trained only on passive next-word prediction, can generalize causal intervention strategies from a few-shot prompt containing examples of experimentation, together with explanations and reasoning"
  - [section] "language models, trained only on passive next-word prediction, can generalize causal intervention strategies from a few-shot prompt containing examples of experimentation, together with explanations and reasoning"
  - [corpus] Weak evidence - related papers focus on active data collection rather than LM generalization

## Foundational Learning

- **Concept: Causal Directed Acyclic Graphs (DAGs)**
  - Why needed here: The work formalizes causal learning in terms of DAGs, where variables are connected by linear effects and sampled from distributions. Understanding DAGs is essential for grasping how the agent learns to infer and use causal structure.
  - Quick check question: In a causal DAG, if variable A influences variable B, can B influence A? Why or why not?

- **Concept: Behavioral Cloning**
  - Why needed here: The agents are trained via imitation (behavioral cloning) on expert data. Understanding behavioral cloning is crucial for understanding how the agents learn from passive data without any active interaction during training.
  - Quick check question: What is the difference between behavioral cloning and reinforcement learning?

- **Concept: Generalization from Passive Data**
  - Why needed here: The core contribution is showing that passive learning can enable generalization to novel causal structures. Understanding the limitations and possibilities of passive learning is essential for appreciating the significance of the results.
  - Quick check question: What is the fundamental limitation of learning from passive observational data only, and how does the work overcome this limitation?

## Architecture Onboarding

- **Component map:** Input encoder -> TransformerXL memory -> Policy network -> Action prediction
- **Critical path:**
  1. Encode observations into state representation
  2. Process through Transformer memory to track experiment outcomes
  3. Policy network predicts intervention action
  4. (When applicable) Explanation decoder predicts explanation of action
- **Design tradeoffs:**
  - Simple policy architecture (single linear layer) vs. more complex architectures
  - Fixed experimentation order vs. adaptive exploration strategies
  - Including explanations in training vs. pure behavioral cloning
- **Failure signatures:**
  - Loss becomes NaN during training (indicates numerical instability)
  - Agent fails to generalize to held-out causal structures (indicates insufficient learning of generalizable strategies)
  - Agent matches heuristic baselines rather than optimal expert (indicates reliance on simple correlations rather than causal reasoning)
- **First 3 experiments:**
  1. Train agent on simple causal DAG environment and evaluate generalization to held-out causal structures
  2. Train agent on odd-one-out environments with explanations and evaluate generalization to new feature combinations
  3. Test language model on few-shot prompts with explanations and evaluate generalization to held-out dimensions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can language models generalize causal intervention strategies to novel feature dimensions when the held-out dimension has a different causal structure than the dimensions included in the training shots?
- Basis in paper: [explicit] The paper mentions that language models can generalize to new dimensions not present in the training shots, but does not explore whether the causal structure of the held-out dimension differs from the training dimensions.
- Why unresolved: The paper only tests generalization to a held-out dimension within the same causal structure, not to a dimension with a different causal structure.
- What evidence would resolve it: Experiments testing language model performance on causal intervention tasks where the held-out dimension has a different causal structure than the training dimensions.

### Open Question 2
- Question: How do the number and quality of examples in the few-shot prompt affect the language model's ability to generalize causal intervention strategies?
- Basis in paper: [explicit] The paper uses a four-shot prompt for the language model experiments, but does not systematically vary the number or quality of examples.
- Why unresolved: The paper only uses a fixed number and type of examples in the prompt, not exploring the effect of varying these factors.
- What evidence would resolve it: Experiments testing language model performance on causal intervention tasks with prompts containing different numbers and qualities of examples.

### Open Question 3
- Question: Can passive learning of causal strategies be extended to more complex environments with higher-dimensional observations and more intricate causal structures?
- Basis in paper: [explicit] The paper demonstrates passive learning of causal strategies in a simple causal DAG environment and a more complex environment with pixel observations, but does not explore more complex environments with higher-dimensional observations or more intricate causal structures.
- Why unresolved: The paper only tests passive learning in relatively simple environments, not exploring more complex cases.
- What evidence would resolve it: Experiments testing passive learning of causal strategies in environments with higher-dimensional observations and more intricate causal structures.

## Limitations
- Causal DAG environment assumptions: The work assumes linear effects with Gaussian noise, which may not generalize to more complex causal structures with nonlinear relationships or discrete variables.
- Explanation quality dependence: The success of explanation-driven generalization critically depends on the quality and correctness of explanations in training data.
- Active intervention assumption: All generalization claims assume the agent can intervene at test time to collect interventional data.

## Confidence
- **High confidence:** Agents can learn generalizable exploration strategies from passive data when they can intervene at test time
- **Medium confidence:** Natural language explanations enable generalization from confounded data
- **Low confidence:** Language models can generalize causal strategies from few-shot prompts

## Next Checks
1. **Robustness to causal structure complexity:** Test agents on environments with nonlinear relationships, discrete variables, or hidden confounders to assess generalization beyond linear Gaussian assumptions.

2. **Ablation study on explanation quality:** Systematically vary explanation quality (correct vs. incorrect vs. absent) to quantify the actual contribution of explanations to generalization performance.

3. **Cross-architecture generalization:** Evaluate whether agents trained with explanations can transfer their strategies to different policy architectures or to entirely different task domains beyond causal experimentation.