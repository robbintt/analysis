---
ver: rpa2
title: 'BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs with
  Multi-turn Health Conversations Polished by ChatGPT'
arxiv_id: '2310.15896'
source_url: https://arxiv.org/abs/2310.15896
tags:
- bianque
- health
- llms
- questioning
- doctor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes BianQue, a health-focused large language model
  that improves the chain of questioning (CoQ) capability by fine-tuning on a self-constructed
  dataset of multi-turn health conversations. The BianQueCorpus dataset contains balanced
  proportions of questions and suggestions from real-world doctor-patient dialogues,
  which are cleaned and polished by ChatGPT.
---

# BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs with Multi-turn Health Conversations Polished by ChatGPT

## Quick Facts
- arXiv ID: 2310.15896
- Source URL: https://arxiv.org/abs/2310.15896
- Authors: 
- Reference count: 26
- Key outcome: BianQue improves chain of questioning capability through fine-tuning on balanced multi-turn health conversations, outperforming baselines on BLEU, ROUGE, and PQA metrics.

## Executive Summary
This paper introduces BianQue, a health-focused large language model designed to balance questioning and suggestion abilities in medical conversations. The authors construct a new dataset called BianQueCorpus containing 2,437,190 samples of multi-turn doctor-patient dialogues with balanced proportions of questions (46.2%) and suggestions (53.8%). By fine-tuning ChatGLM-6B on this dataset with ChatGPT-polished responses, BianQue demonstrates superior performance on standard evaluation metrics and a custom Proactive Questioning Ability (PQA) metric. The model shows promise for providing more personalized and targeted health suggestions through improved chain of questioning capabilities.

## Method Summary
The BianQue model fine-tunes ChatGLM-6B on a self-constructed dataset of multi-turn health conversations called BianQueCorpus. The dataset is built from real-world doctor-patient dialogues and polished by ChatGPT to ensure balanced proportions of questions (46.2%) and suggestions (53.8%). The fine-tuning process uses a WarmupDecayLR learning rate scheduler with warmup_steps = 1000 and warmup_max_lr = 5e-5, maximum input length of 1,536, maximum target length of 512, batch size of 80, and global training steps of 25,000. Inference uses Top-p sampling with p = 0.75 and temperature Ï„ = 0.95. The model is evaluated using BLEU-1/2/3/4, ROUGE-1/2/L, and a custom PQA metric measuring proactive questioning ability.

## Key Results
- BianQue outperforms baselines including ChatGLM-6B, ChatGPT, and DoctorGLM on BLEU and ROUGE metrics
- The model achieves superior scores on the custom PQA metric measuring proactive questioning ability
- BianQue demonstrates balanced capabilities in both questioning (46.2%) and health suggestions (53.8%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The BianQue model improves the chain of questioning (CoQ) capability by fine-tuning on a balanced dataset of multi-turn health conversations.
- Mechanism: The dataset contains equal proportions of questions and suggestions, allowing the model to learn when to ask follow-up questions versus when to provide advice. This balanced training signal helps the model develop better context awareness and proactive inquiry skills.
- Core assumption: A balanced dataset of questions and suggestions will enable the model to learn the appropriate timing for questioning versus suggesting.
- Evidence anchors:
  - [abstract]: "The BianQueCorpus dataset contains balanced proportions of questions and suggestions from real-world doctor-patient dialogues"
  - [section]: "we constructed a multi-turn health conversation dataset named BianQueCorpus, in which the targets consist of balanced proportional questions (46.2%) and suggestions (53.8%)"
- Break condition: If the model overfits to the training distribution and fails to generalize questioning patterns to new contexts or domains outside health conversations.

### Mechanism 2
- Claim: ChatGPT polishing of doctor responses improves the quality and consistency of training data.
- Mechanism: By using ChatGPT to refine doctor responses, the dataset achieves more standardized, professional language while maintaining medical accuracy. This creates cleaner training signals for the fine-tuning process.
- Core assumption: ChatGPT can improve the quality of medical advice while preserving accuracy and clinical appropriateness.
- Evidence anchors:
  - [abstract]: "BianQueCorpus dataset... is consist of multiple turns of questioning and health suggestions polished by ChatGPT"
  - [section]: "we designed a polishing prompt... and use ChatGPT to polish the doctors' suggestion of multi-turn conversations, because doctors often respond very briefly through internet platforms, lacking detailed analysis and suggestions"
- Break condition: If ChatGPT introduces inconsistencies or hallucinates medical information that differs from the original doctor's intent.

### Mechanism 3
- Claim: Multi-turn conversation training enables the model to develop proactive questioning ability measured by PQA metric.
- Mechanism: The model learns to generate contextually appropriate questions based on previous conversation turns, as measured by the Proactive Questioning Ability (PQA) metric that evaluates both precision and recall of question generation.
- Core assumption: Multi-turn training data provides sufficient context for the model to learn when and how to ask follow-up questions.
- Evidence anchors:
  - [abstract]: "Experimental results demonstrate that the proposed BianQue can simultaneously balance the capabilities of both questioning and health suggestions"
  - [section]: "We define a new metric to measure the model's Proactive Questioning ability (PQA)"
- Break condition: If the model generates questions that are either too generic or inappropriate for the medical context, failing to demonstrate true understanding of patient needs.

## Foundational Learning

- Concept: Chain of Questioning (CoQ) in medical consultations
  - Why needed here: Understanding how doctors use iterative questioning to diagnose and treat patients is essential for creating realistic training data and evaluation metrics
  - Quick check question: What distinguishes CoQ from simple question-answering in medical contexts?

- Concept: Fine-tuning large language models on domain-specific datasets
  - Why needed here: The BianQue model builds upon ChatGLM-6B by adapting it to the health domain with specific conversational patterns
  - Quick check question: What are the key differences between pre-training and fine-tuning in the context of medical LLMs?

- Concept: Evaluation metrics for conversational AI (BLEU, ROUGE, custom metrics)
  - Why needed here: The paper uses multiple metrics including a custom PQA metric to comprehensively evaluate both question and suggestion generation
  - Quick check question: How does the PQA metric differ from traditional language generation metrics like BLEU or ROUGE?

## Architecture Onboarding

- Component map: ChatGLM-6B -> Fine-tuning on BianQueCorpus -> Inference with Top-p sampling
- Critical path: Data collection -> Data cleaning and ChatGPT polishing -> Fine-tuning -> Evaluation on benchmark datasets
- Design tradeoffs: Balanced dataset proportions (46.2% questions vs 53.8% suggestions) versus focusing solely on suggestion generation; comprehensive cleaning versus preserving original doctor language
- Failure signatures: Over-reliance on questioning without providing adequate suggestions; generation of inappropriate medical questions; failure to maintain conversational context across turns
- First 3 experiments:
  1. Test question generation quality on held-out multi-turn health conversations from MedDialog-CN
  2. Evaluate suggestion generation quality using BLEU/ROUGE on CHIP-MDCFNPC dataset
  3. Measure PQA score on IMCS-V2 to specifically assess proactive questioning ability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does BianQue's CoQ ability compare to real-world doctors' CoQ ability in terms of accuracy and efficiency?
- Basis in paper: [inferred] The paper claims BianQue improves CoQ capability but does not provide a comparison to real doctors.
- Why unresolved: The paper lacks a direct comparison between BianQue's CoQ and real doctors' CoQ in terms of accuracy (correctness of questions and suggestions) and efficiency (number of turns needed to reach a diagnosis or recommendation).
- What evidence would resolve it: A controlled study comparing BianQue's CoQ to real doctors' CoQ on the same set of patient cases, measuring accuracy of diagnosis/advice and number of conversation turns.

### Open Question 2
- Question: What is the optimal balance between questioning and suggesting in health LLMs like BianQue for different medical scenarios?
- Basis in paper: [explicit] The paper mentions BianQue aims to "balance the capabilities of both questioning and health suggestions" but does not explore optimal ratios for different scenarios.
- Why unresolved: The paper does not investigate whether the 46.2% questioning and 53.8% suggesting ratio is optimal for all medical scenarios or if different ratios would be more effective for different types of conditions.
- What evidence would resolve it: Experiments testing BianQue with different question/suggestion ratios across various medical scenarios to determine optimal balances for each type.

### Open Question 3
- Question: How does the quality of ChatGPT-polished suggestions compare to suggestions from real doctors in the BianQueCorpus dataset?
- Basis in paper: [explicit] The paper states they used ChatGPT to polish doctors' suggestions but does not evaluate the quality of this polishing.
- Why unresolved: There is no assessment of whether ChatGPT's polishing improved the quality of doctors' suggestions or if it introduced any inaccuracies or oversimplifications.
- What evidence would resolve it: A comparison of ChatGPT-polished suggestions to the original doctors' suggestions and to suggestions from other doctors, evaluating for accuracy, completeness, and clinical appropriateness.

## Limitations

- The dataset construction relies heavily on ChatGPT polishing, which may introduce bias or lose original medical nuance
- The custom PQA metric lacks detailed specification, making independent validation challenging
- Evaluation focuses primarily on Chinese-language datasets, limiting generalizability to other languages and healthcare systems

## Confidence

**High Confidence Claims:**
- The BianQueCorpus dataset successfully balances questions and suggestions at approximately 46.2% questions versus 53.8% suggestions
- The fine-tuning procedure using ChatGLM-6B as base model with specified hyperparameters is clearly documented
- BLEU and ROUGE evaluation metrics are standard and well-established for language generation tasks

**Medium Confidence Claims:**
- ChatGPT polishing effectively improves data quality while preserving medical accuracy
- The PQA metric meaningfully measures proactive questioning ability
- The model demonstrates superior performance compared to baselines on the reported datasets

**Low Confidence Claims:**
- Generalization of results to real-world clinical settings without additional safety validation
- Applicability of findings to non-Chinese healthcare contexts
- Long-term effectiveness of the chain of questioning approach in actual patient care

## Next Checks

1. **Safety Validation**: Conduct a comprehensive safety analysis of generated questions across diverse medical scenarios, including edge cases and potentially sensitive health topics, to ensure the model does not produce harmful or inappropriate questions.

2. **Generalization Testing**: Evaluate the model on additional healthcare datasets from different languages, regions, and medical specialties to assess cross-domain performance and identify potential limitations in the questioning approach.

3. **Clinical Expert Review**: Engage medical professionals to independently assess the quality, appropriateness, and clinical utility of both questions and suggestions generated by the model in realistic consultation scenarios.