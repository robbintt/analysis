---
ver: rpa2
title: 'TempME: Towards the Explainability of Temporal Graph Neural Networks via Motif
  Discovery'
arxiv_id: '2310.19324'
source_url: https://arxiv.org/abs/2310.19324
tags:
- temporal
- motifs
- motif
- explanation
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TempME, a novel framework for explaining temporal
  graph neural networks (TGNNs) using temporal motifs. TempME identifies pivotal temporal
  motifs that trigger specific predictions by learning their importance scores via
  an information bottleneck objective, incorporating a null model to distinguish interaction-related
  from irrelevant motifs.
---

# TempME: Towards the Explainability of Temporal Graph Neural Networks via Motif Discovery

## Quick Facts
- arXiv ID: 2310.19324
- Source URL: https://arxiv.org/abs/2310.19324
- Reference count: 40
- Key outcome: TempME achieves up to 8.21% higher explanation accuracy and 22.96% better link prediction average precision across multiple TGNN backbones.

## Executive Summary
TempME introduces a novel framework for explaining temporal graph neural network predictions by identifying pivotal temporal motifs that trigger specific predictions. The approach leverages information bottleneck optimization to learn importance scores for temporal motifs while distinguishing interaction-related patterns from random noise through a null model. Experiments demonstrate TempME's superior performance on six real-world datasets, achieving significant improvements in explanation accuracy and link prediction while maintaining efficiency and scalability.

## Method Summary
TempME operates by sampling temporal motif instances around nodes of interest, encoding these motifs into representations using message passing, and then assigning importance scores through an information bottleneck objective. The framework incorporates a null model to distinguish meaningful interaction-related motifs from irrelevant ones, ensuring cohesive and interpretable explanations. Additionally, motif embeddings are concatenated with node representations to enhance TGNN link prediction performance, providing augmenting information for the final prediction layer.

## Key Results
- Achieves up to 8.21% higher explanation accuracy compared to existing methods
- Improves link prediction average precision by up to 22.96% across multiple TGNN backbones
- Demonstrates faster inference times and better scalability than existing explainers
- Shows consistent performance improvements across six diverse real-world datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal motifs provide cohesive and interpretable explanations for TGNN predictions.
- Mechanism: TempME samples temporal motif instances around the nodes of interest and assigns importance scores to each motif. Events within motifs are inherently connected and time-ordered, ensuring explanations are spatially adjacent and temporally proximate.
- Core assumption: Temporal motifs capture essential governing rules in dynamic systems and are self-connected.
- Evidence anchors:
  - [abstract] "These temporal motifs are essential factors that control the generative mechanisms of future events in real-world temporal graphs and dynamic systems."
  - [section 4.1] "The intrinsic self-connectivity of temporal motifs guarantees the cohesive property of the generated explanations."
  - [corpus] Weak - no direct corpus support for motif cohesion in explanations.

### Mechanism 2
- Claim: Information bottleneck objective balances explanation accuracy and compression.
- Mechanism: TempME maximizes mutual information with target predictions while minimizing mutual information with the original graph, using a variational approximation with a null model.
- Core assumption: Temporal motifs can be disentangled into interaction-related and interaction-irrelevant components.
- Evidence anchors:
  - [section 4.3] "We resort to the information bottleneck technique to extract compressed components that are the most interaction-related."
  - [appendix C] "Eq. 7 aims at optimizing the explanation accuracy with the least amount of information."
  - [corpus] Moderate - information bottleneck used in motif analysis, but not specifically in explanations.

### Mechanism 3
- Claim: Temporal motif embeddings enhance TGNN link prediction performance.
- Mechanism: TempME aggregates motif embeddings and concatenates them with node representations before the final MLP layer, providing augmenting information for link prediction.
- Core assumption: Motif embeddings capture higher-order structural patterns that improve representation learning.
- Evidence anchors:
  - [section 5.2] "Motif Embedding provides augmenting information for link prediction and generally improves the performance of base models."
  - [table 4] Shows performance improvements on UCI, USLegis, and Can.Parl. datasets.
  - [corpus] Weak - no direct corpus support for motif embeddings improving TGNN performance.

## Foundational Learning

- Concept: Temporal motifs
  - Why needed here: Temporal motifs are the fundamental building blocks used by TempME to construct explanations. Understanding their definition and properties is crucial for grasping how TempME works.
  - Quick check question: What are the two key constraints on temporal motif events in TempME?

- Concept: Information bottleneck principle
  - Why needed here: The information bottleneck objective guides TempME in balancing explanation accuracy and compression. Knowledge of this principle is essential for understanding the optimization framework.
  - Quick check question: What are the two terms in the information bottleneck objective and what do they represent?

- Concept: Null model
  - Why needed here: The null model provides a randomized baseline for distinguishing interaction-related motifs from irrelevant ones. Understanding its role is key to comprehending the empirical prior distribution.
  - Quick check question: How does the null model help TempME identify significant motifs?

## Architecture Onboarding

- Component map:
  - Temporal Motif Sampling -> Temporal Motif Encoder -> Information-Bottleneck-based Generator -> Base TGNN

- Critical path:
  1. Sample temporal motifs around nodes of interest
  2. Encode motifs into embeddings
  3. Assign importance scores via IB objective
  4. Sample explanatory motifs based on scores
  5. Generate explanation subgraph

- Design tradeoffs:
  - Motif length vs. computational complexity
  - Uniform vs. empirical prior distributions
  - Number of sampled motifs vs. explanation quality
  - Time encoding vs. event anonymization importance

- Failure signatures:
  - Disjointed explanations indicate motif sampling issues
  - Poor accuracy suggests IB objective misalignment
  - Low efficiency points to motif encoding bottlenecks

- First 3 experiments:
  1. Validate motif sampling captures diverse patterns
  2. Test IB objective optimization on synthetic data
  3. Evaluate motif embedding impact on base model performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do external events, context, and user preferences beyond temporal motifs contribute to the explanations of temporal graph neural network predictions?
- Basis in paper: [inferred] The paper mentions that in complex real-world scenarios, additional factors beyond motifs may significantly contribute to explanations.
- Why unresolved: The paper focuses solely on temporal motifs and does not explore the impact of other contextual factors on the explainability of TGNN predictions.
- What evidence would resolve it: Conducting experiments that incorporate external data sources, domain expertise, or multi-modal information to evaluate their influence on the explanatory power of motif-based explanations.

### Open Question 2
- Question: What is the impact of different null model choices on the importance scores and explanations generated by TempME?
- Basis in paper: [explicit] The paper mentions that the choice of null model may introduce inductive bias to the desired explanations and suggests further analysis of the null model setting as a future direction.
- Why unresolved: The paper uses a common null model but does not extensively explore or compare the effects of different null model choices on the explanations.
- What evidence would resolve it: Performing a systematic study comparing the explanations and importance scores generated by TempME using various null models, such as degree-preserving or attribute-preserving null models.

### Open Question 3
- Question: How does the scalability of TempME change when dealing with large-scale temporal graphs with high interaction density between the same node pairs at the same timestamp?
- Basis in paper: [explicit] The paper mentions that one limitation of TempME is analyzing temporal graphs characterized by high interaction density between the same node pairs at the same timestamp.
- Why unresolved: The paper does not provide a detailed analysis of how the high interaction density affects the scalability and performance of TempME.
- What evidence would resolve it: Conducting experiments on large-scale temporal graphs with varying levels of interaction density to evaluate the runtime, memory usage, and explanation quality of TempME under these conditions.

## Limitations

- The framework's effectiveness depends heavily on the quality of temporal motif sampling and the null model's ability to capture randomness
- Claims about temporal motifs being "essential factors that control generative mechanisms" lack direct empirical validation
- The information bottleneck objective's variational approximation introduces uncertainty about exact optimization dynamics

## Confidence

- Temporal motif cohesion as explanation basis: Medium
- Information bottleneck effectiveness: Medium
- Motif embeddings improving TGNN performance: Low

## Next Checks

1. Conduct ablation studies varying motif lengths to determine optimal configuration
2. Test the null model sensitivity by comparing different randomization strategies
3. Validate the framework's scalability on larger temporal graphs beyond the six datasets tested