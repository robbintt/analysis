---
ver: rpa2
title: Hybrid Search for Efficient Planning with Completeness Guarantees
arxiv_id: '2310.12819'
source_url: https://arxiv.org/abs/2310.12819
tags:
- search
- hips
- low-level
- node
- subgoal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a complete subgoal search algorithm, HIPS-\u03B5\
  , that augments high-level subgoal search with low-level actions to achieve completeness\
  \ in discrete action spaces while maintaining practical efficiency. The method modifies\
  \ HIPS [18] by considering both subgoals from the learned generator and low-level\
  \ actions during search, controlled by a hyperparameter \u03B5 that balances high-level\
  \ and low-level exploration."
---

# Hybrid Search for Efficient Planning with Completeness Guarantees

## Quick Facts
- arXiv ID: 2310.12819
- Source URL: https://arxiv.org/abs/2310.12819
- Reference count: 40
- Key outcome: HIPS-ε achieves 100% success rates across four benchmark environments while often outperforming pure high-level search in node expansions

## Executive Summary
This paper proposes HIPS-ε, a complete subgoal search algorithm that augments high-level subgoal search with low-level actions to guarantee completeness in discrete action spaces while maintaining practical efficiency. The method modifies HIPS by considering both subgoals from the learned generator and low-level actions during search, controlled by a hyperparameter ε that balances high-level and low-level exploration. Experimental results on Sokoban, Sliding Tile Puzzle, Box-World, and Travelling Salesman Problem demonstrate that HIPS-ε achieves 100% success rates while often requiring fewer search expansions than pure high-level methods for solvable instances.

## Method Summary
HIPS-ε extends the HIPS framework by modifying the search procedure to consider both high-level subgoals and low-level actions during search. The method uses a hyperparameter ε to balance the probability of selecting subgoals versus low-level actions, with higher ε values prioritizing low-level exploration. The search employs a modified Policy-guided Heuristic Search (PHS) evaluation function with a scaled heuristic that accounts for the difference in scale between path loss and heuristic estimates. The approach trains a behavioral cloning policy for low-level actions and integrates it with the learned subgoal generator and dynamics model to achieve completeness guarantees while maintaining efficiency.

## Key Results
- Achieves 100% success rates across all benchmark environments (Sokoban, Sliding Tile Puzzle, Box-World, TSP)
- Often outperforms pure high-level search in terms of node expansions for solvable instances
- Demonstrates improved out-of-distribution generalization capabilities
- Shows robustness to hyperparameter selection, with extensive tuning rarely necessary

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Augmenting high-level subgoal search with low-level actions guarantees completeness in discrete action spaces.
- Mechanism: By expanding nodes that represent both subgoals proposed by the generator and low-level actions taken in the current state, the search space becomes exhaustive. Even if the generator fails to propose useful subgoals, the low-level actions ensure that all reachable states are eventually explored.
- Core assumption: The transition dynamics are either known or can be learned accurately enough to simulate low-level actions during search.
- Evidence anchors:
  - [abstract] "we propose an efficient approach to augment a subgoal search method to achieve completeness in discrete action spaces"
  - [section] "we propose modifying the search procedure of HIPS such that in addition to the subgoals... the search also considers states {sL = T (s, a), ∀a ∈ A}"
  - [corpus] Weak evidence: only one related paper mentions completeness guarantees explicitly
- Break condition: If the environment dynamics are stochastic or continuous, or if the learned dynamics model is inaccurate enough to mispredict transitions, the completeness guarantee no longer holds.

### Mechanism 2
- Claim: Using a hyperparameter ε to balance high-level and low-level action selection optimizes search efficiency.
- Mechanism: The probability of selecting a subgoal versus a low-level action is controlled by ε. Lower ε values prioritize high-level actions, reducing search breadth and leveraging temporal abstraction; higher ε values increase low-level exploration, improving completeness when the generator is weak.
- Core assumption: The prior over subgoals πSG and the behavior cloning policy πBC are both well-calibrated and can be combined multiplicatively.
- Evidence anchors:
  - [section] "We use hyperparameter ε to balance the probabilities computed with the high and low-level policies: higher values of ε prioritize more low-level exploration"
  - [section] "We observe that HIPS-ε outperforms HIPS in every environment except TSP, and extensive hyperparameter tuning to select a good value of ε is rarely necessary"
  - [corpus] Weak evidence: no corpus neighbor directly discusses ε-balancing in subgoal search
- Break condition: If πSG and πBC are poorly calibrated or mismatched in scale, the balance may become ineffective or destabilize the search.

### Mechanism 3
- Claim: Scaling the heuristic by the average low-level distance per node keeps the evaluation function in the same units as the search objective.
- Mechanism: The original PHS heuristic assumes path loss and heuristic estimates are in the same scale. By rescaling h(n) by g(n)/l(n), where l(n) is the number of low-level actions from the root to n, the scaled heuristic ˆh(n) = h(n)g(n)/l(n) aligns with the node expansion objective rather than low-level step count.
- Core assumption: The average number of low-level actions between nodes on a path is approximately constant.
- Evidence anchors:
  - [section] "We propose re-scaling the heuristic factor h to be equal to the estimated number of search nodes on the path from n to the goal node n∗ by dividing the expected number of low-level actions from node n to the terminal node by the average low-level distance between the nodes on the path from n0 to node n"
  - [section] "naive A*-inspired evaluation functions...fails to be competitive" (compared to scaled heuristic)
  - [corpus] No direct corpus support for this specific scaling; assumption is novel to the paper
- Break condition: If the average low-level distance per node varies widely across the search tree, the scaling becomes inaccurate and may mislead the search.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and deterministic transitions
  - Why needed here: The algorithm assumes deterministic transitions T(s,a) to guarantee completeness when simulating low-level actions.
  - Quick check question: If the environment is stochastic, can HIPS-ε still guarantee completeness? Why or why not?

- Concept: Policy-guided heuristic search (PHS) and evaluation functions
  - Why needed here: HIPS-ε modifies the PHS evaluation function to account for mixed high- and low-level actions while preserving the search objective.
  - Quick check question: What is the role of the heuristic factor η(n) in PHS, and why must it be adapted for HIPS-ε?

- Concept: Behavior cloning (BC) for low-level policies
  - Why needed here: πBC(a|s) is used to assign probabilities to low-level actions during complete search, requiring a learned low-level policy from demonstrations.
  - Quick check question: How does πBC differ from the subgoal-conditioned policy π(a|s,sg) used in HIPS?

## Architecture Onboarding

- Component map: HIPS core (VQVAE-based subgoal generator, heuristic V, dynamics model) -> BC policy head (ResNet-based network) -> PHS search loop (priority queue, evaluation function φ(n)) -> ε controller (hyperparameter balancing)

- Critical path:
  1. Generate subgoals via VQVAE prior p(e|s) and low-level actions via BC policy
  2. Assign combined probabilities using ε
  3. Evaluate nodes with scaled heuristic φ(n)
  4. Expand highest-priority node until terminal state reached

- Design tradeoffs:
  - Higher ε → more low-level exploration, better completeness, slower search
  - Lower ε → more subgoal exploitation, faster search, risk of failure if generator weak
  - Learned vs true dynamics: learned models reduce environment interactions but may introduce errors

- Failure signatures:
  - Search gets stuck in local minima: likely ε too low or πBC/πSG poorly calibrated
  - Excessive node expansions without progress: heuristic scaling may be off or environment model inaccurate
  - Completeness guarantee broken: dynamics model predictions diverge from true transitions

- First 3 experiments:
  1. Run HIPS-ε on a small deterministic Sokoban puzzle with ε→0 and ε=0.1; compare success rates and node expansions
  2. Measure percentage of low-level vs subgoal expansions across different ε values in Sliding Tile Puzzle
  3. Replace learned dynamics with true dynamics in Sokoban; verify 100% success rate and count environment steps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of HIPS-ε scale with increasing problem complexity and longer planning horizons?
- Basis in paper: [explicit] The paper mentions that the current method is evaluated on four discrete planning problems and discusses potential for scaling to longer horizons.
- Why unresolved: The experiments are limited to specific problem domains and sizes. The paper doesn't explore performance degradation or stability as problems become significantly more complex.
- What evidence would resolve it: Experiments showing performance metrics (success rate, node expansions) on increasingly complex variants of the tested environments or entirely new domains with longer planning horizons.

### Open Question 2
- Question: What is the impact of learned transition dynamics quality on HIPS-ε's completeness and performance guarantees?
- Basis in paper: [explicit] The paper discusses using learned models versus true dynamics and mentions that completeness is lost with learned models, but doesn't quantify the relationship between model accuracy and search performance.
- Why unresolved: The experiments show HIPS-ε works with learned models but don't systematically analyze how model error propagates through search or at what point completeness breaks down.
- What evidence would resolve it: Controlled experiments varying transition model accuracy and measuring both completeness rates and search efficiency metrics across different levels of model error.

### Open Question 3
- Question: Can the complete subgoal search framework be extended to continuous action spaces or hybrid discrete-continuous domains?
- Basis in paper: [explicit] The paper states it assumes discrete action spaces and mentions this as a limitation, noting that real-world problems are often partially or fully continuous.
- Why unresolved: The proposed method relies on enumerating all low-level actions and subgoals, which becomes intractable in continuous spaces. The paper doesn't propose or evaluate any approximation strategies.
- What evidence would resolve it: Demonstrations of the complete search framework applied to continuous control benchmarks or hybrid domains with performance comparisons to pure continuous and pure discrete methods.

## Limitations

- Completeness guarantee depends on accurate learned transition dynamics, with no thorough analysis of model error impact
- The assumption of constant average low-level distances between nodes may not hold in complex environments
- Performance evaluation limited to discrete planning problems; extension to continuous domains remains unexplored

## Confidence

**High Confidence Claims**:
- HIPS-ε achieves 100% success rates across all benchmark environments when given sufficient search budget
- The method consistently outperforms pure high-level search in terms of node expansions for solvable instances
- ε hyperparameter balancing effectively controls the trade-off between exploration and exploitation

**Medium Confidence Claims**:
- Improved out-of-distribution generalization compared to HIPS
- The heuristic scaling mechanism provides meaningful improvements over naive approaches
- Learned dynamics models are sufficiently accurate for practical completeness guarantees

**Low Confidence Claims**:
- The completeness guarantee holds under all reasonable learned dynamics model errors
- The method generalizes equally well to continuous action spaces
- Performance improvements scale linearly with larger, more complex environments

## Next Checks

1. **Dynamics Model Error Analysis**: Systematically vary the accuracy of the learned transition model and measure the impact on completeness rates and solution quality. This would quantify the robustness of the completeness guarantee to model errors.

2. **Extended Hyperparameter Sweep**: Conduct a more comprehensive grid search over ε and related hyperparameters across all four environments, including interaction effects. This would validate the claimed insensitivity to hyperparameter selection.

3. **Continuous Action Space Extension**: Adapt the HIPS-ε framework to a continuous action space benchmark (e.g., robotic manipulation tasks) and evaluate whether the completeness guarantees and efficiency improvements translate to this setting.