---
ver: rpa2
title: Universal Multi-modal Entity Alignment via Iteratively Fusing Modality Similarity
  Paths
arxiv_id: '2310.05364'
source_url: https://arxiv.org/abs/2310.05364
tags:
- modality
- entity
- alignment
- modalities
- pathfusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of multi-modal entity alignment
  in knowledge graphs, aiming to identify equivalent entities across multiple KGs
  to create a more comprehensive and unified KG. The proposed method, PathFusion,
  consists of two main components: (1) MSP, a unified modeling approach that simplifies
  the alignment process by constructing paths connecting entities and modality nodes
  to represent multiple modalities; and (2) IRF, an iterative fusion method that effectively
  combines information from different modalities using the path as an information
  carrier.'
---

# Universal Multi-modal Entity Alignment via Iteratively Fusing Modality Similarity Paths

## Quick Facts
- arXiv ID: 2310.05364
- Source URL: https://arxiv.org/abs/2310.05364
- Reference count: 40
- Key outcome: 22.4%-28.9% absolute improvement on Hits@1, 0.194-0.245 absolute improvement on MRR

## Executive Summary
This paper addresses the challenge of multi-modal entity alignment in knowledge graphs by proposing PathFusion, a method that unifies modality modeling through Modality Similarity Paths (MSP) and iteratively refines alignments using Iterative Refinement Fusion (IRF). The approach constructs cross-KG intra-modality similarity matrices for each modality and fuses them using optimal transport to produce robust entity mappings. Experiments on real-world datasets demonstrate superior performance over state-of-the-art methods.

## Method Summary
PathFusion is a two-component framework for multi-modal entity alignment. First, MSP models each modality by computing cross-KG intra-modality similarity matrices without learning entity embeddings, instead using matrix operations to produce mapping matrices. Second, IRF fuses these modality-specific mappings using Sinkhorn normalization and iteratively refines the relational modality using high-confidence pseudo-seeds derived from the fused matrix. The method avoids separate embedding learners for each modality and instead uses similarity paths as information carriers.

## Key Results
- Achieves 22.4%-28.9% absolute improvement on Hits@1 over state-of-the-art methods
- Demonstrates 0.194-0.245 absolute improvement on MRR
- Outperforms baseline methods on multiple real-world datasets (FB15K-DB15K, FB15K-YG15K, DICEWS, WY50K)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MSP unifies modality modeling by replacing separate embedding learners with path-based similarity operators
- Core assumption: Similarity scores between modality items in different KGs are sufficient to determine entity equivalence without intermediate embeddings
- Evidence: [abstract] mentions MSP as unified modeling approach; [section 4.2] describes direct similarity-based approach

### Mechanism 2
- Claim: IRF improves alignment by iteratively refining relational modality using pseudo-seeds from fused mapping
- Core assumption: Relational modality contains strong structural information that can be enhanced by cross-modal pseudo-seeds
- Evidence: [abstract] mentions IRF as iterative fusion method; [section 4.3] describes bi-directional refinement strategy

### Mechanism 3
- Claim: Sinkhorn algorithm ensures doubly stochastic mapping matrix for improved alignment stability
- Core assumption: Doubly stochastic normalization via Sinkhorn improves final alignment quality
- Evidence: [section 4.3] mentions Sinkhorn for optimal transport plan; [abstract] reports experimental superiority

## Foundational Learning

- Concept: Knowledge Graph embedding and relational modeling
  - Why needed: PathFusion's relational component relies on GNN/KGE methods
  - Quick check: What's the difference between TransE-style KGE and GNN-based relational modeling?

- Concept: Optimal Transport and Sinkhorn algorithm
  - Why needed: IRF uses Sinkhorn to normalize fused mapping matrix
  - Quick check: What happens if you apply only row normalization but not column normalization?

- Concept: Modality-specific similarity computation
  - Why needed: MSP requires computing cross-KG intra-modality similarities
  - Quick check: How to compute attribute value similarity for numeric vs textual values?

## Architecture Onboarding

- Component map: Visual encoder (SwinTransformer) -> Attribute encoder (RoBERTa) -> MSP module -> IRF module -> Backbone relational model (GNN/KGE) -> Final alignment matrix
- Critical path: 1) Load KGs and modality data 2) Compute cross-KG intra-modality similarities 3) Generate modality mapping matrices via MSP 4) Fuse matrices with Sinkhorn 5) Extract pseudo-seeds and refine relational model (IRF) 6) Output final alignment matrix
- Design tradeoffs: MSP avoids entity embeddings but increases memory usage; Sinkhorn adds computation overhead but improves quality; IRF increases accuracy but adds iterative complexity
- Failure signatures: Degraded Hits@1/MRR if modality similarities are too uniform; unstable training if pseudo-seeds contain false positives; memory overflow with very large KGs
- First 3 experiments: 1) Run PathFusion with only relational modality (baseline) 2) Enable MSP for one side modality and measure impact 3) Add IRF iteration and compare Hits@1 before/after refinement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How to extend PathFusion for KGs with significantly different modality distributions?
- Basis: [explicit] Requires similar modality distributions and suggests this as limitation
- Why unresolved: Current framework relies on consistent modality nodes across KGs
- Evidence needed: Experimental results on varying modality distributions with proposed modifications

### Open Question 2
- Question: Can textual modality be incorporated without test data leakage?
- Basis: [explicit] Avoids textual modality due to potential test data leakage
- Why unresolved: Previous approaches showed leakage issues when incorporating text
- Evidence needed: Successful integration of textual modality with demonstrated improvements and no leakage

### Open Question 3
- Question: What strategies improve PathFusion's scalability for large-scale tasks?
- Basis: [explicit] Identifies scalability as limitation with quadratic complexity
- Why unresolved: Quadratic complexity and lack of large-scale datasets for evaluation
- Evidence needed: Approximate methods or infrastructure improvements reducing complexity while maintaining accuracy

## Limitations

- Relies heavily on quality of cross-KG intra-modality similarities; noisy similarities produce unreliable mapping matrices
- Iterative refinement depends on high-confidence pseudo-seeds that may be difficult to extract from noisy fused matrices
- Computational overhead from dense similarity matrices and Sinkhorn normalization limits scalability to very large KGs

## Confidence

- High Confidence: Experimental improvements in Hits@1 and MRR are well-supported by reported data
- Medium Confidence: MSP mechanism is plausible but lacks direct corpus evidence
- Medium Confidence: IRF approach is theoretically sound but corpus evidence is missing
- Low Confidence: Sinkhorn effectiveness inferred from general optimal transport use without direct corpus evidence

## Next Checks

1. Create synthetic multi-modal KGs with known alignments to test PathFusion under varying noise levels in modality similarities
2. Conduct ablation study on Sinkhorn iterations (k) to determine optimal number for alignment quality
3. Implement baseline with direct entity embeddings for all modalities to compare against PathFusion's path-based approach