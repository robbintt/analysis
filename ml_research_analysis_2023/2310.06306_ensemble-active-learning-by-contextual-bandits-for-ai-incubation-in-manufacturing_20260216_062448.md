---
ver: rpa2
title: Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing
arxiv_id: '2310.06306'
source_url: https://arxiv.org/abs/2310.06306
tags:
- learning
- data
- samples
- agents
- acquisition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of selecting informative samples
  for annotation in streaming data to maintain data quality for supervised learning
  base learners. The proposed ensemble active learning method by contextual bandits
  (CbeAL) incorporates a set of active learning agents explicitly designed for exploration
  or exploitation by a weighted combination of their acquisition decisions.
---

# Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing

## Quick Facts
- arXiv ID: 2310.06306
- Source URL: https://arxiv.org/abs/2310.06306
- Reference count: 40
- Primary result: CbeAL achieves 72.5%-76.7% average classification accuracy, significantly outperforming benchmarks in streaming data quality inspection for FDM manufacturing.

## Executive Summary
This paper addresses the challenge of selecting informative samples for annotation in streaming data to maintain data quality for supervised learning base learners. The proposed ensemble active learning method by contextual bandits (CbeAL) incorporates a set of active learning agents explicitly designed for exploration or exploitation by a weighted combination of their acquisition decisions. The weight of each agent is dynamically adjusted based on the usefulness of its decisions to improve the performance of the base learner. CbeAL achieves a well-balanced exploration-exploitation trade-off during the streaming process in an adaptive manner, which enables highly accurate online quality modeling with limited labeling efforts for the FDM quality inspection. In the comprehensive simulation study, CbeAL-6 achieves significantly better performance compared to benchmarks under most scenarios.

## Method Summary
CbeAL combines multiple active learning agents (exploration and exploitation oriented) and uses contextual bandits to adjust their weights based on historical rewards. The reward is defined as the usefulness of the acquisition decision, encouraging the selection of samples that would be wrongly predicted by the base learner. The method dynamically balances exploration (discovering under-represented regions) and exploitation (learning the decision boundary) during streaming data annotation, enabling highly accurate online quality modeling with limited labeling efforts.

## Key Results
- CbeAL-6 achieves significantly better performance compared to benchmarks under most scenarios, with an average classification accuracy of 72.5%-76.7%.
- The method achieves a well-balanced exploration-exploitation trade-off during the streaming process in an adaptive manner.
- CbeAL enables highly accurate online quality modeling with limited labeling efforts for FDM quality inspection.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ensemble active learning with contextual bandits balances exploration and exploitation dynamically during streaming data annotation.
- Mechanism: CbeAL combines multiple active learning agents (exploration and exploitation oriented) and uses contextual bandits to adjust their weights based on historical rewards. The reward is defined as the usefulness of the acquisition decision, encouraging the selection of samples that would be wrongly predicted by the base learner.
- Core assumption: The base learner's performance on streaming data is sensitive to the balance between exploration (discovering under-represented regions) and exploitation (learning the decision boundary), and this balance needs to adapt to changing data distributions.
- Evidence anchors:
  - [abstract]: "CbeAL achieves a well-balanced exploration-exploitation trade-off during the streaming process in an adaptive manner"
  - [section]: "The weight of each agent will be dynamically adjusted based on the usefulness of its decisions to improve the performance of the base learner"
  - [corpus]: Weak. Related work on contextual bandits focuses on different problem settings (autonomous robotics, nonlinear bandits) without explicit discussion of active learning for data quality.
- Break condition: If the reward function fails to capture the informativeness of samples, or if the contextual bandits solver cannot effectively distinguish between good and bad agents, the ensemble will not improve performance.

### Mechanism 2
- Claim: Low-density and space-filling exploration agents discover new clusters and under-represented regions in the input variable space.
- Mechanism: The LD-Agent acquires samples in sparse regions based on local density, while the SPF-Agent acquires samples to fill the space uniformly. These agents are designed to explore the input variable space when the base learner's decision boundary estimation is imprecise.
- Core assumption: The streaming data contains multiple clusters, and the initial training data is not uniformly distributed, making exploration necessary for accurate modeling.
- Evidence anchors:
  - [section]: "The objective of exploration is to identify the structure of the input data distribution during the learning process"
  - [section]: "The density-based criterion will explore the boundary of the input variable space faster at an early stage, whereas the space-filling criterion allows for a more uniform exploration during the process"
  - [corpus]: Weak. Related bandit work does not address the specific exploration objectives in active learning for data quality.
- Break condition: If the data does not contain distinct clusters, or if the initial training data is already representative, exploration may not improve performance and could even degrade it by acquiring uninformative samples.

### Mechanism 3
- Claim: Reinforced exploitation agent adaptively learns a threshold for uncertainty sampling, focusing on decision boundary learning.
- Mechanism: The RAL-Agent uses reinforcement learning to adjust a certainty threshold based on the historical reward. It acquires samples with ambiguous class membership, aiming to improve the base learner's decision boundary estimation.
- Core assumption: The base learner's uncertainty on a sample is a good indicator of its informativeness for improving the decision boundary, and this uncertainty changes as the base learner is updated.
- Evidence anchors:
  - [section]: "The goal of exploitation in active learning is to capture the decision boundary, which is generally achieved by acquiring samples with ambiguous class membership"
  - [section]: "A RL-based controller is designed to adjust the certainty thresholdθ based on the contribution of historical acquisition decisions"
  - [corpus]: Weak. Related bandit work focuses on general exploration-exploitation but does not address the specific exploitation objective in active learning.
- Break condition: If the base learner's uncertainty is not well-calibrated, or if the reward function does not accurately reflect the usefulness of exploiting uncertain samples, the RAL-Agent may acquire uninformative samples or miss informative ones.

## Foundational Learning

- Concept: Contextual bandits problem formulation
  - Why needed here: To frame the dynamic adjustment of exploration-exploitation trade-off as a learning problem where the agent (CbeAL) learns to select the best active learning agent based on contextual information and historical rewards.
  - Quick check question: What are the key components of a contextual bandits problem (arms, context, reward, policy)?

- Concept: Exploration-exploitation dilemma in active learning
  - Why needed here: To understand the fundamental challenge of balancing the need to discover new information (exploration) with the need to improve the current model (exploitation) during streaming data annotation.
  - Quick check question: What are the potential consequences of focusing solely on exploration or exploitation in active learning?

- Concept: Reward design for active learning
  - Why needed here: To understand how the reward function in CbeAL encourages the selection of informative samples by penalizing acquisitions that would not improve the base learner's performance.
  - Quick check question: How does the reward function in CbeAL differ from a simple accuracy-based reward?

## Architecture Onboarding

- Component map:
  - Base learner -> Active learning agents (LD-Agent, SPF-Agent, RAL-Agent) -> Contextual bandits solver (Exp4.P-EWMA) -> Reward function -> Data source -> Human annotator

- Critical path:
  1. Receive a streaming sample with contextual information (features, predicted label, predicted probability)
  2. Active learning agents make individual acquisition decisions based on the context
  3. Contextual bandits solver combines the decisions using weighted majority voting
  4. If the overall decision is to acquire, request the true label from the human annotator
  5. Update the base learner with the newly labeled sample
  6. Update the weights of the active learning agents based on the reward

- Design tradeoffs:
  - Number of active learning agents: More agents can provide more diverse exploration-exploitation strategies but may increase computational complexity and risk of agent redundancy
  - Reward function design: A well-designed reward function can encourage informative acquisitions but may be difficult to define and tune
  - Contextual information: More contextual information can improve the bandits solver's decision but may increase computational cost and risk of overfitting

- Failure signatures:
  - Early convergence: If one agent dominates the ensemble too quickly, the system may lose its ability to adapt to changing data distributions
  - Insufficient exploration: If the exploration agents are not effective, the system may miss important regions of the input variable space
  - Overfitting: If the contextual bandits solver overfits to the historical data, it may not generalize well to new contexts

- First 3 experiments:
  1. Implement a simple version of CbeAL with only one exploration agent (LD-Agent) and one exploitation agent (RAL-Agent) on a synthetic dataset with known clusters and imbalanced classes
  2. Evaluate the performance of CbeAL compared to individual agents and random sampling on a benchmark dataset (e.g., MNIST) with simulated streaming data
  3. Apply CbeAL to a real-world manufacturing dataset (e.g., FDM quality inspection) and compare its performance to existing active learning methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform under different base learner types, particularly non-linear models like neural networks or decision trees?
- Basis in paper: [explicit] The paper mentions that "SVM is selected as the base learner with default parameters" and that "CbeAL is a generic framework for classification models," but does not provide extensive empirical evidence across different model types.
- Why unresolved: The paper primarily uses logistic regression and SVM as base learners, leaving the performance of CbeAL with other types of classifiers unexplored.
- What evidence would resolve it: Additional experiments using various base learners, including non-linear models, to compare the performance of CbeAL across different types of classification models.

### Open Question 2
- Question: What is the impact of varying the exploration-exploitation trade-off parameters (e.g., δL, θ0, η) on the performance of the individual agents and CbeAL?
- Basis in paper: [inferred] The paper mentions specific hyperparameter values for the agents (e.g., δL = 0.01, θ0 = 0.95, η = 0.005) but does not provide a systematic study of how these parameters affect performance.
- Why unresolved: The paper does not investigate the sensitivity of the method to these parameters or provide guidance on how to tune them for different scenarios.
- What evidence would resolve it: A sensitivity analysis of the agents and CbeAL to these parameters, including visualizations of performance as a function of the parameters.

### Open Question 3
- Question: How does the proposed method handle concept drift in the data stream, where the underlying distribution of the data changes over time?
- Basis in paper: [explicit] The paper mentions that "various factors can change such an underlying model in manufacturing processes" and that "the lack of consideration of human annotation efforts renders these online updating methods inefficient for supervised AI models," but does not explicitly address how CbeAL handles concept drift.
- Why unresolved: The paper does not provide a mechanism or experimental evidence for detecting and adapting to concept drift in the data stream.
- What evidence would resolve it: Additional experiments or theoretical analysis demonstrating how CbeAL adapts to concept drift, including metrics for detecting drift and evaluating the method's performance in the presence of drift.

## Limitations
- The specific implementations of the LD-Agent, SPF-Agent, and RAL-Agent are described, but their individual contributions to the overall performance are not fully quantified.
- The transferability of CbeAL to other active learning scenarios beyond manufacturing data quality inspection is not explicitly addressed.
- The reward function design and hyperparameter tuning are not thoroughly discussed, which may limit reproducibility and generalizability to other domains.

## Confidence
- **High**: The ensemble approach combining exploration and exploitation agents is sound, and the use of contextual bandits for dynamic weight adjustment is well-motivated.
- **Medium**: The specific implementations of the LD-Agent, SPF-Agent, and RAL-Agent are described, but their individual contributions to the overall performance are not fully quantified.
- **Low**: The transferability of CbeAL to other active learning scenarios beyond manufacturing data quality inspection is not explicitly addressed.

## Next Checks
1. Conduct ablation studies to isolate the impact of each active learning agent and the contextual bandits solver on the ensemble's performance.
2. Test CbeAL on diverse datasets (e.g., text, image) and compare its performance to state-of-the-art active learning methods.
3. Analyze the sensitivity of CbeAL's performance to variations in the reward function design and hyperparameter settings.