---
ver: rpa2
title: Rethinking Machine Ethics -- Can LLMs Perform Moral Reasoning through the Lens
  of Moral Theories?
arxiv_id: '2308.15399'
source_url: https://arxiv.org/abs/2308.15399
tags:
- moral
- ethics
- theories
- scenario
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a top-down approach to enable Large Language
  Models (LLMs) to perform moral reasoning under well-established moral theories.
  Instead of learning from crowd-sourced opinions, the framework guides LLMs with
  explicit principles from normative ethics and moral psychology.
---

# Rethinking Machine Ethics -- Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?

## Quick Facts
- arXiv ID: 2308.15399
- Source URL: https://arxiv.org/abs/2308.15399
- Reference count: 27
- Primary result: GPT-4 guided by normative ethical theories achieves up to 96.8% recall in detecting morally wrong scenarios

## Executive Summary
This paper proposes a top-down approach to enable Large Language Models to perform moral reasoning under established moral theories rather than relying on crowd-sourced opinions. The framework guides GPT-4 with explicit principles from normative ethics and moral psychology, using Chain-of-Thought reasoning combined with theory-specific instructions. Experiments demonstrate that GPT-4 can effectively understand and adhere to various moral theories, with the TDM-guided approach achieving the highest recall rate on detecting morally wrong scenarios. The work also reveals limitations in existing datasets and LLMs, including insufficient context and overestimation of unlikely risks.

## Method Summary
The method uses GPT-4 with theory-guided prompts to perform moral reasoning and judgment-making under various moral theories including Justice, Deontology, Utilitarianism, and Theory of Dyadic Morality (TDM). The approach employs Chain-of-Thought prompting with explicit instructions derived from each moral theory to decompose moral scenarios into theory-relevant components. The framework is tested on theory-guided datasets as well as commonsense morality datasets like E-CM and Social-Chem-101, evaluating precision, recall, and accuracy metrics for moral judgment tasks.

## Key Results
- TDM-guided approach achieves highest recall rate (up to 96.8%) on detecting morally wrong scenarios
- Specifying cultural background of annotators boosts accuracy by 5.3% (from 82.2% to 87.5%)
- GPT-4 effectively understands and adheres to various moral theories when explicitly prompted with theory-specific instructions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can perform structured moral reasoning when explicitly prompted with normative ethical theories
- Mechanism: Chain-of-Thought reasoning combined with theory-specific instructions guide the LLM to decompose moral scenarios into theory-relevant components
- Core assumption: The LLM has latent representations that correspond to moral concepts embedded in the training data
- Evidence anchors: Abstract demonstrates GPT-4 can effectively understand and adhere to various moral theories
- Break condition: If the LLM lacks latent moral representations, the framework cannot extract meaningful theory-guided reasoning

### Mechanism 2
- Claim: Specifying cultural context in prompts improves alignment with human annotations
- Mechanism: Adding explicit cultural framing (e.g., "From the perspective of English-speaking community...") shifts the LLM's output toward culturally familiar moral norms
- Core assumption: The LLM has been exposed to diverse cultural contexts in training and can modulate reasoning accordingly
- Evidence anchors: TDM-E N boosts accuracy by 5.3% compared to TDM-G EN on commonsense morality datasets
- Break condition: If cultural context is underspecified or ambiguous in training corpus, the LLM may default to generic reasoning

### Mechanism 3
- Claim: Theory-guided prompts reduce false positives in identifying morally wrong scenarios
- Mechanism: Requiring the LLM to justify judgments with theory-specific reasoning filters out cases that superficially appear wrong but lack principled grounding
- Core assumption: Theory-guided reasoning imposes stricter, more principled criteria than intuitive classification
- Evidence anchors: TDM-guided approach achieves highest recall rate (96.8%) on detecting morally wrong scenarios
- Break condition: If theory criteria are misaligned with task definition or dataset labels, recall may drop despite stricter reasoning

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: Enables complex, multi-step moral reasoning by breaking down judgments into explicit reasoning steps
  - Quick check question: Does the prompt include intermediate reasoning steps before the final judgment?

- Concept: Normative vs descriptive ethics
  - Why needed here: Guides choice of theory-specific instructions; normative theories prescribe moral rules, descriptive theories model how people judge
  - Quick check question: Are the prompt instructions aligned with whether the theory is normative or descriptive?

- Concept: Cultural context in moral reasoning
  - Why needed here: Moral judgments can vary by cultural background; specifying context improves alignment with human labels
  - Quick check question: Does the prompt mention the cultural or linguistic background of the expected annotator?

## Architecture Onboarding

- Component map: Input parser -> Theory-specific instruction generator -> CoT reasoning module -> JSON response formatter -> Judgment classifier
- Critical path:
  1. Receive scenario
  2. Apply theory-specific instruction
  3. Run CoT reasoning
  4. Generate structured JSON output
  5. Extract judgment
- Design tradeoffs:
  - Generality vs specificity: General TDM-G EN works across datasets but less aligned; culture-specific TDM-E N more aligned but dataset-dependent
  - Complexity vs interpretability: More detailed reasoning increases interpretability but may confuse the LLM
  - Prompt length vs API cost: Longer, theory-rich prompts improve reasoning but increase token usage
- Failure signatures:
  - High variance in judgment across runs → LLM instability
  - JSON parse errors → Formatting issues in instruction
  - Judgment contradicts theory logic → Misalignment in instruction design
- First 3 experiments:
  1. Compare TDM-G EN vs TDM-E N on a held-out validation set to quantify cultural framing effect
  2. Test vanilla CoT vs theory-guided CoT on theory-guided datasets to isolate reasoning benefit
  3. Ablate instruction components (e.g., remove harm analysis) to identify key reasoning steps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different cultural backgrounds affect the alignment between moral theories and commonsense morality datasets?
- Basis in paper: TDM-E N (which specifies the cultural background of annotators) boosts accuracy compared to TDM-G EN on commonsense morality datasets
- Why unresolved: Paper only tests one specific cultural background (English-speaking community) and does not explore how other cultural backgrounds might affect alignment
- What evidence would resolve it: Testing the framework across multiple cultural backgrounds and comparing alignment scores

### Open Question 2
- Question: What are the limitations of current LLMs in handling complex moral reasoning scenarios involving multiple characters?
- Basis in paper: LLM often loses track of subjects of analyzation, especially when multiple characters are involved, leading to incorrect moral reasoning
- Why unresolved: Paper does not provide detailed analysis of why this limitation exists or how it could be addressed
- What evidence would resolve it: Investigating underlying mechanisms of LLM reasoning in multi-character scenarios

### Open Question 3
- Question: How can the proposed framework be adapted to incorporate emerging moral theories and address evolving social norms?
- Basis in paper: Framework emphasizes flexibility in incorporating various moral theories but does not discuss adaptation to new theories or changing social norms
- Why unresolved: Paper does not explore potential for integrating new moral theories or adapting to societal changes over time
- What evidence would resolve it: Demonstrating framework's ability to incorporate new theories and adapt to evolving social norms through empirical studies

## Limitations
- Effectiveness depends heavily on LLM's ability to genuinely understand and apply moral theories rather than pattern-matching
- Cultural context improvements based on single dataset may not generalize to datasets from different cultural contexts
- High recall (96.8%) achieved without corresponding precision metrics or false positive rates

## Confidence

- **High confidence**: Framework's basic structure (CoT prompting + theory-specific instructions) is technically sound and methodology for evaluating moral judgments is reasonable
- **Medium confidence**: Experimental results on GPT-4 are reproducible, but generalizability to other LLMs and cultural contexts is uncertain
- **Low confidence**: Claims about LLM's genuine moral reasoning capabilities versus pattern matching, and robustness of cultural framing benefits across diverse datasets

## Next Checks

1. Test the theory-guided framework on smaller LLMs (e.g., LLaMA, Mistral) to determine whether moral reasoning capabilities are emergent properties of large models or can be achieved with more efficient architectures

2. Apply cultural framing technique (TDM-E N) to datasets from different cultural backgrounds to verify if 5.3% improvement is a general phenomenon or specific to E-CM dataset

3. Conduct detailed error analysis focusing on false positive rates and precision metrics, particularly for TDM-guided approach, to ensure framework isn't overly conservative in identifying morally wrong scenarios at expense of practical utility