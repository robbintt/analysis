---
ver: rpa2
title: 'It Is All About Data: A Survey on the Effects of Data on Adversarial Robustness'
arxiv_id: '2303.09767'
source_url: https://arxiv.org/abs/2303.09767
tags:
- data
- adversarial
- robustness
- learning
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically collects and analyzes 57 research papers
  on how data properties affect adversarial robustness in machine learning models.
  The survey identifies seven domain-agnostic data properties and one image-specific
  data property that influence robustness.
---

# It Is All About Data: A Survey on the Effects of Data on Adversarial Robustness

## Quick Facts
- arXiv ID: 2303.09767
- Source URL: https://arxiv.org/abs/2303.09767
- Reference count: 40
- One-line primary result: Survey identifies seven domain-agnostic and one image-specific data properties that influence adversarial robustness in ML models.

## Executive Summary
This survey systematically analyzes 57 research papers to understand how data properties affect adversarial robustness in machine learning models. The authors identify seven domain-agnostic properties (sample size, dimensionality, distribution, density, separability, noise, imbalance) and one image-specific property (natural images) that influence robustness. Key findings include the need for more training samples for robust generalization, higher dimensionality correlating with increased adversarial risk, and certain data distributions being inherently less robust due to concentration of measure phenomena.

## Method Summary
The authors conducted a systematic literature review of papers focusing on data properties and their effects on adversarial robustness. They identified relevant papers through search queries and selection criteria, then categorized them based on problem setup, data property examined, and practicality of the findings. The survey provides a comprehensive taxonomy of data properties and their relationships with adversarial robustness, synthesizing theoretical results and empirical findings from the collected papers.

## Key Results
- Robust generalization requires more training samples than standard generalization
- Higher dimensionality correlates with higher adversarial risk due to sparse data regions
- Some data distributions (satisfying concentration inequalities) are inherently non-robust
- Adversarial examples commonly occur in low-density regions of the input space
- Greater class separation decreases adversarial risk

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High dimensionality increases adversarial vulnerability due to sparse data regions.
- Mechanism: As dimensionality increases, the volume of the input space grows exponentially, making data points sparser. This sparsity creates low-density regions where decision boundaries are less well-defined, allowing small perturbations to cross class boundaries.
- Core assumption: Data distribution remains uniform or near-uniform across dimensions.
- Evidence anchors:
  - [abstract] states "higher dimensionality correlates with higher adversarial risk."
  - [section 4.2] explains "adversarial examples are inevitable in high-dimensional space" and references Gilmer et al. [51] proving samples are closer to adversarial examples than to each other in high dimensions.
  - [corpus] provides weak supporting evidence with average neighbor FMR of 0.487 but no specific dimensionality claims.
- Break condition: If data exhibits intrinsic low dimensionality (e.g., lying on a lower-dimensional manifold), the sparsity argument weakens significantly.

### Mechanism 2
- Claim: Concentration of measure phenomenon makes adversarial examples inevitable in certain distributions.
- Mechanism: For distributions satisfying concentration inequalities (like Lévy families), the measure of any set shrinks rapidly as it expands, meaning small perturbations can drastically change classification outcomes. This creates an upper bound on achievable robustness.
- Core assumption: The data distribution satisfies concentration of measure properties (e.g., uniform on hypersphere, Gaussian with independent variables).
- Evidence anchors:
  - [abstract] mentions "datasets with high concentration are shown to be inevitably non-robust."
  - [section 4.6] discusses Mahloujifar et al. [91] proving classifiers on Lévy family distributions admit adversarial examples with O(√d) perturbation.
  - [corpus] lacks specific concentration-related papers, indicating this is an emerging area.
- Break condition: If the data distribution violates concentration inequalities (e.g., bounded support with low effective dimension), the inevitability claim breaks down.

### Mechanism 3
- Claim: Label noise increases adversarial vulnerability through overfitting complex decision boundaries.
- Mechanism: Noisy labels cause models to overfit to spurious patterns, creating overly complex decision boundaries with many small regions. These boundaries are easier to cross with small perturbations, increasing adversarial risk.
- Core assumption: The model has sufficient capacity to overfit to noisy labels and the noise is not randomly distributed across all classes.
- Evidence anchors:
  - [abstract] states "the presence of mislabeled samples in a dataset... affects robustness."
  - [section 4.7] references Sanyal et al. [118] proving models overfitting to label noise are vulnerable, and defense mechanisms like early stopping help by preventing overfitting.
  - [corpus] lacks direct label noise papers but shows related adversarial defense surveys.
- Break condition: If the model is regularized to prevent overfitting or if label noise is minimal and uniformly distributed, the vulnerability decreases.

## Foundational Learning

- Concept: Concentration of measure phenomenon
  - Why needed here: Critical for understanding why certain distributions inherently limit achievable robustness
  - Quick check question: What is the mathematical definition of concentration function ℎ(μ, α, ε) for a metric probability space?

- Concept: VC dimension and sample complexity
  - Why needed here: Essential for understanding theoretical bounds on the number of samples needed for robust generalization
  - Quick check question: How does VC dimension relate to the upper bound of sample complexity for standard vs. robust generalization?

- Concept: Feature variance and class separability
  - Why needed here: Key to understanding how data distribution properties affect decision boundary quality
  - Quick check question: How does feature variance within classes impact the model's ability to learn robust decision boundaries?

## Architecture Onboarding

- Component map: Data property analysis pipeline → Theoretical analysis → Empirical validation → Defense technique development
- Critical path: Identify data property → Establish correlation with robustness → Develop measurement technique → Validate on real datasets
- Design tradeoffs: Simplified synthetic distributions (easier proofs) vs. realistic datasets (practical relevance)
- Failure signatures: Claims about data properties without quantitative measurement techniques; theoretical results not validated on real datasets
- First 3 experiments:
  1. Replicate Schmidt et al. [119] sample complexity experiment comparing MNIST vs CIFAR-10
  2. Implement concentration estimation following Mahloujifar et al. [92] on MNIST and CIFAR-10
  3. Test dimensionality reduction impact on adversarial robustness using PCA + randomized smoothing as in Awasthi et al. [11]

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do properties like outliers, overlapping samples, and small disjuncts affect adversarial robustness?
- Basis in paper: [inferred] The survey mentions these data properties are studied for standard generalization but not for robust generalization
- Why unresolved: Most research focuses on data properties that are already known to affect robust generalization, leaving a gap in understanding the impact of other properties
- What evidence would resolve it: Studies that explicitly investigate the effects of outliers, overlapping samples, and small disjuncts on adversarial robustness would help fill this gap

### Open Question 2
- Question: Is there an intrinsic trade-off between standard and adversarial accuracy?
- Basis in paper: [explicit] The survey mentions that some works show an accuracy-robustness trade-off under specific assumptions, while others show overlapping effects
- Why unresolved: There is conflicting evidence on whether achieving robust generalization necessarily comes at the cost of standard generalization
- What evidence would resolve it: More research is needed to map data-related reasons that contribute to the accuracy-robustness trade-off and to determine if it is an inherent limitation or can be overcome

### Open Question 3
- Question: How do different data properties interact to influence adversarial robustness?
- Basis in paper: [inferred] The survey notes that only a few works consider multiple data properties simultaneously or establish interdependence between properties
- Why unresolved: Adversarial robustness is likely a result of compounding properties, but optimizing for multiple properties simultaneously is challenging
- What evidence would resolve it: Studies that investigate correlations between different data properties, such as the effects of feature dimensionality reduction on class density and separation, would provide insights into their interactions

## Limitations
- Limited corpus of 57 papers may not capture the full landscape of research
- Many mechanisms have limited direct empirical validation across diverse datasets
- Theoretical bounds often assume idealized conditions that may not hold in practice
- Focus primarily on image classification datasets limits generalizability

## Confidence

- **High confidence**: Claims about dimensionality effects and sample complexity requirements are supported by multiple papers and theoretical proofs.
- **Medium confidence**: Conclusions about data distribution concentration and label noise effects, while theoretically grounded, have limited empirical validation across diverse datasets.
- **Low confidence**: Predictions about interactions between different data properties remain largely theoretical with minimal empirical support.

## Next Checks

1. **Dimensionality validation**: Replicate the dimensionality-reduction experiment on multiple datasets (MNIST, CIFAR-10, ImageNet) using PCA or autoencoders, measuring adversarial robustness before and after reduction with consistent attack methods.

2. **Concentration estimation**: Implement the concentration estimation technique from Mahloujifar et al. [92] on MNIST and CIFAR-10, comparing measured concentration values with empirical adversarial success rates.

3. **Sample complexity verification**: Conduct a controlled experiment varying training set sizes on CIFAR-10 for both standard and robust models, measuring the gap in sample complexity requirements as predicted by Schmidt et al. [119].