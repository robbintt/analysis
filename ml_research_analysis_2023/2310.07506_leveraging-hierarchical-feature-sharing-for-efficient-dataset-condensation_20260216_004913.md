---
ver: rpa2
title: Leveraging Hierarchical Feature Sharing for Efficient Dataset Condensation
arxiv_id: '2310.07506'
source_url: https://arxiv.org/abs/2310.07506
tags:
- data
- images
- memory
- training
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses data condensation, aiming to synthesize small
  synthetic datasets for training models with high accuracy. The authors propose a
  novel data parameterization architecture, Hierarchical Memory Network (HMN), which
  stores condensed data in a three-tier structure representing dataset-level, class-level,
  and instance-level features.
---

# Leveraging Hierarchical Feature Sharing for Efficient Dataset Condensation

## Quick Facts
- arXiv ID: 2310.07506
- Source URL: https://arxiv.org/abs/2310.07506
- Reference count: 33
- Key outcome: HMN outperforms all baselines, including high GPU memory trajectory-based losses, when trained with batch-based loss

## Executive Summary
This paper addresses data condensation by proposing a novel Hierarchical Memory Network (HMN) architecture that stores condensed data in a three-tier structure (dataset-level, class-level, and instance-level features). The hierarchical design enables more efficient information sharing while maintaining good independence among images, allowing effective instance-level pruning to remove redundancy. Evaluated on SVHN, CIFAR10, CIFAR100, and Tiny-ImageNet, HMN consistently outperforms state-of-the-art methods, even when using less GPU memory-intensive training approaches.

## Method Summary
The Hierarchical Memory Network (HMN) parameterizes synthetic datasets using a three-tier memory structure where dataset-level memory captures universal features, class-level memory stores class-specific features, and instance-level memory holds image-specific features. Images are generated by combining these memories through feature extractors and a uniform decoder. The network is trained using gradient matching loss with a batch-based approach, making it more memory-efficient than trajectory-based methods. After initial condensation, over-budget HMNs undergo double-end pruning to remove both easy (high AUM) and hard (low AUM) examples, further improving performance.

## Key Results
- HMN achieves 3.45-5.97% accuracy improvements over baselines on CIFAR10 and CIFAR100
- HMN outperforms trajectory-based methods despite using batch-based loss with lower GPU memory consumption
- Double-end pruning consistently improves performance across all tested datasets and budgets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HMN stores condensed data in a three-tier hierarchical structure that better aligns with the inherent hierarchical nature of shared features in datasets, leading to more efficient information sharing.
- Mechanism: The three-tier memory structure allows images to share features at multiple hierarchical levels simultaneously, capturing universal, class-specific, and image-specific features.
- Core assumption: Images in classification datasets share features hierarchically according to the classification hierarchy, and this can be effectively captured through a three-tier memory structure.
- Evidence anchors: [abstract], [section 3.1]
- Break condition: If the classification hierarchy doesn't align with actual feature sharing patterns in the data.

### Mechanism 2
- Claim: HMN's hierarchical architecture naturally ensures good independence among images despite achieving information sharing, enabling effective instance-level pruning.
- Mechanism: Instance-level memory is independent for each image, allowing pruning specific images by removing their instance-level memory without affecting other images.
- Core assumption: Good independence between generated images is necessary for effective pruning, and hierarchical architecture can achieve this while still enabling feature sharing.
- Evidence anchors: [abstract], [section 3.2.1]
- Break condition: If instance-level memories become too coupled with other memory tiers during training.

### Mechanism 3
- Claim: Over-budget condensation followed by post-condensation pruning can further enhance data condensation performance by removing redundant information.
- Mechanism: Condensing data into an over-budget HMN (10% extra) captures more comprehensive information, then double-end pruning removes both easy and hard examples.
- Core assumption: Condensed datasets contain redundant information that can be pruned without significant performance loss, and double-end pruning is more effective than single-end pruning.
- Evidence anchors: [abstract], [section 3.2.2]
- Break condition: If pruning removes too much important information, causing accuracy drop.

## Foundational Learning

- Concept: Dataset condensation and data parameterization
  - Why needed here: Understanding how to compress large datasets into small synthetic datasets while maintaining training performance is fundamental to this work
  - Quick check question: What is the key difference between traditional dataset condensation (using images as containers) and data parameterization (using parameterized containers)?

- Concept: Gradient matching loss for data condensation
  - Why needed here: The paper uses gradient matching as a batch-based loss for training HMNs, which is crucial for understanding the optimization process
  - Quick check question: How does gradient matching loss differ from trajectory-based losses in terms of GPU memory consumption and effectiveness?

- Concept: Area Under the Margin (AUM) for data importance
  - Why needed here: AUM is used to identify redundant data for pruning, which is a key component of the post-condensation pruning strategy
  - Quick check question: What does a low AUM value indicate about an example's importance for training?

## Architecture Onboarding

- Component map:
  - Dataset-level memory (m(D)) -> Class-level memory (m(C)_c) -> Instance-level memory (m(I)_c,i) -> Feature extractors (f_c) -> Uniform decoder (D)

- Critical path:
  1. Initialize three-tier memory structure
  2. For each training iteration, generate images using Eq. (1)
  3. Compute gradient matching loss between synthetic and real data
  4. Update all memory tensors and networks via backpropagation
  5. After condensation, apply double-end pruning to fit budget

- Design tradeoffs:
  - Larger instance-level memories allow more information per image but reduce the number of generated images
  - Deeper hierarchical structure may capture more complex feature sharing but increases computational complexity
  - Over-budget condensation improves quality but requires additional pruning step

- Failure signatures:
  - Performance degrades significantly when instance-level memory size is too small or too large
  - Training becomes unstable if memory tiers are not properly balanced
  - Pruning removes too much important information, causing accuracy drop

- First 3 experiments:
  1. Train HMN on CIFAR10 with 1IPC budget, compare accuracy with baseline methods
  2. Test different instance-level memory sizes on CIFAR10 to find optimal balance
  3. Apply pruning to over-budget HMN and measure accuracy vs. pruning rate relationship

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of hard pruning rate β affect the trade-off between data redundancy reduction and model performance across different storage budgets?
- Basis in paper: Explicit. The paper discusses using a grid search to determine an appropriate hard pruning rate β but doesn't provide detailed analysis on how different β values affect performance across various budgets.
- Why unresolved: The paper only mentions selecting β through grid search without analyzing the impact of different β values on the balance between reducing redundancy and maintaining model accuracy.
- What evidence would resolve it: Systematic experiments varying β across different storage budgets and analyzing resulting model accuracy and redundancy levels.

### Open Question 2
- Question: What are the computational trade-offs between using trajectory-based losses and batch-based losses in data condensation, particularly regarding GPU memory usage and training time?
- Basis in paper: Explicit. The paper contrasts trajectory-based losses (higher GPU memory, better performance) with batch-based losses (lower GPU memory, potentially lower performance) and claims HMN achieves good performance with a batch-based loss.
- Why unresolved: While the paper claims HMN performs well with a batch-based loss, it doesn't provide a detailed comparative analysis of the computational trade-offs between the two loss types in terms of memory usage and training time.
- What evidence would resolve it: Benchmarking experiments comparing GPU memory consumption and training time for both loss types across various datasets and storage budgets.

### Open Question 3
- Question: How does the size of the instance-level memory in HMN influence the diversity and quality of generated images, and what is the optimal balance between memory size and the number of generated images per class?
- Basis in paper: Explicit. The paper discusses how increasing instance-level memory size leads to a decrease in the number of generated images per class and explores the impact on model accuracy, but doesn't provide a clear guideline for optimal balance.
- Why unresolved: The paper identifies a trade-off between memory size and GIPC but does not offer a definitive strategy for balancing these factors to maximize image quality and diversity.
- What evidence would resolve it: Detailed experiments varying instance-level memory sizes and analyzing resulting image quality, diversity, and model accuracy across different storage budgets.

## Limitations
- Theoretical justification for hierarchical architecture's effectiveness is underdeveloped
- Optimal sizing of memory tiers across different datasets and budgets not systematically explored
- Relationship between memory tier sizes and actual feature sharing patterns not empirically validated

## Confidence
- Hierarchical feature sharing mechanism: Medium
- Instance-level pruning effectiveness: Medium
- Double-end pruning strategy: Medium

## Next Checks
1. Ablation study on memory tier sizes: Systematically vary instance-level memory sizes while keeping other tiers fixed to identify optimal balance point for each dataset/budget combination.

2. Independence validation: Measure the correlation between instance-level memories and other memory tiers to empirically verify the claimed independence property that enables pruning.

3. Pruning strategy comparison: Compare double-end pruning against single-end (easy-only or hard-only) and random pruning strategies to confirm the superiority of the proposed approach across different pruning ratios.