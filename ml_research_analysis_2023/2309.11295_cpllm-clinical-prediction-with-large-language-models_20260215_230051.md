---
ver: rpa2
title: 'CPLLM: Clinical Prediction with Large Language Models'
arxiv_id: '2309.11295'
source_url: https://arxiv.org/abs/2309.11295
tags:
- prediction
- diagnosis
- data
- clinical
- patient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces CPLLM, a method for predicting clinical outcomes
  by fine-tuning large language models (LLMs) on structured EHR data. Unlike previous
  approaches that required domain-specific pretraining, CPLLM fine-tunes general-purpose
  LLMs using prompts to directly model diagnosis sequences.
---

# CPLLM: Clinical Prediction with Large Language Models

## Quick Facts
- arXiv ID: 2309.11295
- Source URL: https://arxiv.org/abs/2309.11295
- Reference count: 40
- Primary result: CPLLM outperforms state-of-the-art baselines on MIMIC-IV and eICU-CRD for clinical outcome prediction using fine-tuned LLMs on structured EHR data

## Executive Summary
CPLLM introduces a method for clinical outcome prediction by fine-tuning general-purpose large language models on structured electronic health record data. The approach represents diagnoses as text descriptions and uses prompt-based fine-tuning to predict future diagnoses without requiring domain-specific pretraining. Experiments on chronic kidney disease, acute renal failure, and respiratory failure prediction show CPLLM achieves superior performance compared to specialized medical models like Med-BERT and RETAIN.

## Method Summary
CPLLM fine-tunes general-purpose LLMs (Llama2 and BioMedLM) on structured EHR data using prompts that contain patient diagnosis histories represented as text descriptions. The method adds domain-specific tokens to the tokenizer vocabulary to handle medical terminology and uses QLoRA and PEFT for efficient fine-tuning. Unlike previous approaches requiring domain-specific pretraining, CPLLM directly models diagnosis sequences and supports longer input sequences up to 4096 tokens. The approach is flexible and can incorporate various medical concepts with minimal prompt adjustments.

## Key Results
- CPLLM outperforms Med-BERT, RETAIN, and Logistic Regression baselines on MIMIC-IV and eICU-CRD datasets
- Achieves superior PR-AUC and ROC-AUC metrics for chronic kidney disease, acute renal failure, and respiratory failure prediction
- Adding domain-specific tokens to the tokenizer provides measurable performance improvements
- Method supports longer input sequences (up to 4096 tokens) compared to current state-of-the-art models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Representing clinical diagnoses as text descriptions improves model performance over raw ICD codes
- Mechanism: Text descriptions preserve semantic meaning and relationships between diagnoses, enabling better understanding of medical concepts
- Core assumption: Semantic richness of text descriptions provides more useful signal than structured ICD code representation
- Evidence anchors: Abstract mentions diagnoses as text descriptions; section states medical concepts are represented by text descriptions
- Break condition: Text descriptions that are too verbose or contain irrelevant information may cause noise or context dilution

### Mechanism 2
- Claim: Fine-tuning general-purpose LLMs on structured EHR data outperforms specialized medical models without domain-specific pretraining
- Mechanism: General-purpose LLMs' strong language understanding capabilities can be adapted to clinical prediction through prompt-based fine-tuning
- Core assumption: Transfer learning capabilities of LLMs are sufficient to capture clinical patterns without specialized pretraining
- Evidence anchors: Abstract states CPLLM fine-tunes general-purpose LLMs; section describes using LLM to predict future diagnoses
- Break condition: Performance may degrade if clinical domain requires highly specialized medical knowledge lacking in general LLMs

### Mechanism 3
- Claim: Adding domain-specific tokens to LLM tokenizer improves clinical prediction performance
- Mechanism: Expanding tokenizer vocabulary with medical terms reduces out-of-vocabulary issues and improves understanding of clinical language
- Core assumption: Pre-trained tokenizer lacks sufficient medical terminology for effective clinical text processing
- Evidence anchors: Abstract confirms benefit of domain-specific tokens; section explains adding tokens to prevent out-of-vocabulary issues
- Break condition: Poorly curated tokens may introduce noise or redundancy without performance benefits

## Foundational Learning

- Concept: Prompt engineering for LLMs
  - Why needed here: Method relies on carefully crafted prompts to guide LLM understanding of clinical prediction task
  - Quick check question: What are the key components of an effective prompt for clinical prediction with LLMs?

- Concept: Tokenization and vocabulary management
  - Why needed here: Method involves modifying tokenizer vocabulary by adding domain-specific tokens
  - Quick check question: How does adding new tokens to a pre-trained tokenizer affect model performance and memory requirements?

- Concept: Fine-tuning vs. pretraining
  - Why needed here: Method uses fine-tuning rather than domain-specific pretraining, differentiating it from previous approaches
  - Quick check question: What are advantages and limitations of fine-tuning general-purpose LLMs versus pretraining on domain-specific data?

## Architecture Onboarding

- Component map: Data preprocessing pipeline -> Tokenizer modification -> Prompt generation -> Fine-tuning -> Evaluation

- Critical path: Data preprocessing → Tokenizer modification → Prompt generation → Fine-tuning → Evaluation

- Design tradeoffs:
  - General-purpose vs. domain-specific LLMs: Tradeoff between transfer learning capabilities and domain knowledge
  - Sequence length: Longer sequences provide more context but increase computational requirements
  - Token addition: Improves understanding but increases model size and complexity

- Failure signatures:
  - Poor performance on rare diagnoses may indicate insufficient domain-specific tokens
  - Overfitting on training data may suggest inadequate regularization or too many epochs
  - High computational requirements may indicate inefficient fine-tuning or excessive sequence length

- First 3 experiments:
  1. Baseline comparison: Run CPLLM vs. Med-BERT on small dataset to verify performance improvements
  2. Token ablation: Test CPLLM with and without additional tokens on validation set
  3. Sequence length impact: Evaluate performance at different sequence lengths (512, 1024, 2048 tokens)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CPLLM performance scale with increasing sequence lengths beyond 4096 tokens, and what are computational implications?
- Basis in paper: Inferred - paper mentions CPLLM handles longer sequences with maximum 4096 tokens for Llama2 but doesn't explore beyond this
- Why unresolved: Paper doesn't test exceptionally long sequences to explore extended token limit implications
- What evidence would resolve it: Experiments testing CPLLM on datasets with extremely long patient histories (thousands of diagnoses) would reveal scalability limits and computational costs

### Open Question 2
- Question: Does incorporating additional medical domains (medications, procedures, lab results) into prompt sequence improve CPLLM's predictive accuracy compared to diagnoses alone?
- Basis in paper: Explicit - paper states method is flexible and can incorporate medical concepts from any domain with minimal prompt adjustments
- Why unresolved: Current implementation focuses only on diagnosis input; paper doesn't provide empirical results for multi-domain integration
- What evidence would resolve it: Comparative experiments showing CPLLM performance with and without additional medical concept types would quantify multi-domain integration benefits

### Open Question 3
- Question: How does CPLLM performance change when using retrieval-augmented generation to include updated clinical knowledge about diseases during fine-tuning?
- Basis in paper: Explicit - discussion section mentions future work exploring retrieval augmentation to include general knowledge about diseases and risk factors
- Why unresolved: Paper doesn't implement or test retrieval-augmented approaches, making this future research direction
- What evidence would resolve it: Experiments comparing CPLLM with and without retrieval augmentation components during fine-tuning would show whether external knowledge access improves prediction accuracy

## Limitations

- Reliance on ICD code descriptions rather than full clinical narrative limits contextual richness
- Evaluation focuses on common conditions; performance on rare diseases and complex multi-morbid conditions untested
- Significant computational resources required for large LLMs, potentially limiting accessibility
- Potential biases in training data not addressed, which could affect predictions for underrepresented patient populations

## Confidence

**High Confidence**: Core methodology of using LLMs for clinical prediction is well-established; implementation details for data preprocessing, tokenizer modification, and fine-tuning are clearly specified; performance improvements over baselines are statistically significant and reproducible.

**Medium Confidence**: Claim that text descriptions outperform ICD code indices is supported by ablation studies, but underlying mechanism could be further validated; optimal number and selection of domain-specific tokens not systematically explored.

**Low Confidence**: Generalizability to other clinical prediction tasks beyond three studied diseases not demonstrated; long-term performance and potential degradation over time with evolving clinical practices not addressed.

## Next Checks

1. **Cross-institutional validation**: Test CPLLM on EHR data from different healthcare system with distinct coding practices to assess generalizability and identify potential overfitting to MIMIC-IV and eICU-CRD datasets.

2. **Rare disease performance**: Evaluate CPLLM's ability to predict rare diagnoses where training data is limited, comparing performance against specialized rare disease prediction models and assessing impact of tokenizer modifications on rare condition representation.

3. **Real-world deployment simulation**: Conduct simulation of CPLLM deployment in clinical setting using historical data, measuring performance degradation over time and model's ability to adapt to changes in diagnostic patterns and new disease codes.