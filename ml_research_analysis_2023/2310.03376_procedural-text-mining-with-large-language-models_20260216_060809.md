---
ver: rpa2
title: Procedural Text Mining with Large Language Models
arxiv_id: '2310.03376'
source_url: https://arxiv.org/abs/2310.03376
tags:
- step
- text
- steps
- language
- procedure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study explores the use of large language models (LLMs), particularly
  GPT-4, for extracting procedural knowledge from unstructured PDF documents using
  in-context learning. Two in-context learning approaches are evaluated: one using
  an ontology with procedure definitions, and another using few-shot examples.'
---

# Procedural Text Mining with Large Language Models

## Quick Facts
- arXiv ID: 2310.03376
- Source URL: https://arxiv.org/abs/2310.03376
- Reference count: 14
- Key outcome: In-context learning significantly improves procedural text extraction accuracy, especially with ontology-based approaches achieving ROUGE scores above 70% for text extraction

## Executive Summary
This study explores using large language models, particularly GPT-4, for extracting procedural knowledge from unstructured PDF documents through in-context learning. Two approaches are evaluated: one using an ontology with procedure definitions, and another using few-shot examples. The research demonstrates that in-context learning can significantly improve extraction accuracy compared to zero-shot approaches, with ontology-based methods showing particular promise for text extraction. However, ontology-based extraction scores are lower due to formatting discrepancies between extracted and gold-standard outputs.

## Method Summary
The study employs GPT-4 with in-context learning approaches to extract procedural knowledge from unstructured PDF documents. The methodology involves two main in-context learning strategies: ontology-based learning using intensional definitions of procedural concepts (Procedure, Step, Substep), and few-shot learning with 2 example prompts. The extraction process uses an incremental question-answering approach to handle complex procedures, breaking down the task into smaller sequential questions. Evaluation is performed using ROUGE metrics comparing extracted procedures against gold-standard annotations in both text and ontology RDF formats across four domains: photography, manufacturing, medicine, and agriculture.

## Key Results
- In-context learning with ontology definitions significantly improves text extraction accuracy, achieving ROUGE scores above 70%
- 2-shot learning boosts ontology application accuracy by nearly 30+ points compared to zero-shot
- Incremental questioning improves extraction accuracy for complex procedures by reducing cognitive load

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context learning with ontology definitions improves procedural text extraction accuracy compared to zero-shot approaches.
- Mechanism: By providing the model with intensional definitions of procedural concepts (Procedure, Step, Substep) and their relationships, the model can better align its output with the expected structured format, reducing ambiguity in extraction.
- Core assumption: The model can generalize from intensional definitions to actual text patterns without requiring extensive training data.
- Evidence anchors:
  - [abstract] "findings highlight both the promise of this approach and the value of the in-context learning customisations"
  - [section 4.2] "Contextual knowledge definitions. Contextual knowledge is provided through the identification of the specific domain and intensional definitions of the procedure elements to be extracted."
  - [corpus] Weak - corpus doesn't contain direct evidence about ontology-based in-context learning effectiveness
- Break condition: If the model cannot correctly apply the ontology despite seeing definitions (as observed in zero-shot setting), the approach fails.

### Mechanism 2
- Claim: 2-shot learning significantly improves ontology application accuracy compared to zero-shot.
- Mechanism: Providing the model with 2 examples of correctly formatted ontology output allows it to learn the pattern and apply it correctly to new procedures, overcoming the zero-shot limitation.
- Core assumption: The model can learn from 2 examples to correctly apply complex ontology structures.
- Evidence anchors:
  - [abstract] "These modifications have the potential to significantly address the challenge of obtaining sufficient training data"
  - [section 6.1] "the results are boosted by nearly 30 or more points" in ontology setting with 2-shot learning
  - [section 6.2 observation 7] "this changes in the 2-shot setting, where ChatGPT4 is showed two examples of the correct application of the ontology"
- Break condition: If the model still hallucinates or incorrectly applies ontology after 2-shot learning (as noted in observation 9).

### Mechanism 3
- Claim: Incremental questioning improves extraction accuracy for complex procedures.
- Mechanism: Breaking down the extraction task into smaller, sequential questions allows the model to focus on one aspect at a time, reducing cognitive load and improving accuracy compared to asking for complete procedure extraction in one prompt.
- Core assumption: The model performs better on focused, simpler tasks than on complex, multi-faceted queries.
- Evidence anchors:
  - [section 4.1] "These questions were structured incrementally to address the model's limitations in handling complex queries"
  - [section 6.2 observation 1] "in the 2-shot setting, the model after seeing reference examples, responds with just the essential information"
  - [corpus] Weak - corpus doesn't provide direct evidence about incremental questioning effectiveness
- Break condition: If the incremental approach doesn't improve accuracy over holistic approaches, or if question ordering becomes too complex.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: The study relies on in-context learning to overcome limited training data for procedural text mining
  - Quick check question: What is the difference between zero-shot, one-shot, and few-shot learning in the context of LLMs?

- Concept: ROUGE metrics
  - Why needed here: The study uses ROUGE scores to quantitatively evaluate the accuracy of extracted procedures against gold-standard annotations
  - Quick check question: What do ROUGE-1, ROUGE-2, and ROUGE-L measure, and why are all three used in this study?

- Concept: Ontology-based knowledge representation
  - Why needed here: The study uses an ontology (Procedure vocabulary) to structure extracted procedural knowledge into machine-actionable format
  - Quick check question: What is the difference between intensional and extensional definitions in ontology construction?

## Architecture Onboarding

- Component map:
  Input: Unstructured PDF documents containing procedural text
  -> Processing: GPT-4 LLM with in-context learning (zero-shot, 2-shot, ontology definitions)
  -> Output: Structured procedural knowledge in text format and ontology RDF format
  -> Evaluation: ROUGE scores comparing extracted output to gold-standard annotations

- Critical path:
  1. Document preprocessing and PDF text extraction
  2. In-context learning prompt formulation (incremental questions, ontology definitions, few-shot examples)
  3. LLM inference using GPT-4
  4. Output parsing and formatting (text and ontology)
  5. Evaluation using ROUGE metrics

- Design tradeoffs:
  - Zero-shot vs 2-shot: Zero-shot requires no examples but has lower accuracy; 2-shot improves accuracy but requires finding good examples
  - Text vs ontology output: Text format is easier to generate but less structured; ontology is more machine-actionable but harder to generate correctly
  - Incremental vs holistic extraction: Incremental improves accuracy but increases complexity; holistic is simpler but less accurate

- Failure signatures:
  - Low ROUGE scores indicating poor extraction quality
  - Incorrect ontology application (wrong property usage, missing relationships)
  - Hallucinations - generated content not present in source document
  - Inability to handle procedures spanning multiple pages or columns

- First 3 experiments:
  1. Run zero-shot extraction on a simple, single-page procedure to establish baseline ROUGE scores
  2. Add ontology definitions to the prompt and compare zero-shot scores to see if definitions improve accuracy
  3. Run 2-shot learning with 2 examples of correct ontology output and measure improvement over zero-shot with definitions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can in-context learning with LLMs achieve domain-specific procedural extraction without extensive fine-tuning?
- Basis in paper: [explicit] The paper explores using in-context learning with GPT-4 for procedural extraction across domains like manufacturing, medicine, and agriculture, noting significant improvements in zero-shot vs. few-shot settings.
- Why unresolved: The study tested only a limited set of domains and examples; scalability and domain transfer remain unclear.
- What evidence would resolve it: Comparative experiments across more diverse, domain-specific procedural texts with larger datasets.

### Open Question 2
- Question: How can we ensure LLMs apply ontologies correctly without hallucinating structural elements?
- Basis in paper: [explicit] Results show GPT-4 often misapplies ontology definitions in zero-shot settings, but improves with few-shot examplesâ€”yet occasional hallucinations persist.
- Why unresolved: The model still generates incorrect ontology links or fabricates steps even with in-context guidance.
- What evidence would resolve it: Systematic evaluation of hallucination frequency and strategies (e.g., stricter constraints or hybrid symbolic-NLP pipelines).

### Open Question 3
- Question: Are LLMs truly reasoning over procedural text, or are they generating plausible patterns based on statistical patterns?
- Basis in paper: [explicit] The authors observed errors in simple mathematical comparisons (e.g., counting steps across procedures), suggesting limitations in logical reasoning.
- Why unresolved: The paper demonstrates instances where the model fails on tasks requiring reasoning, but does not fully explore the underlying cause.
- What evidence would resolve it: Controlled experiments isolating reasoning tasks from pattern-matching tasks, and testing model behavior on procedurally novel inputs.

## Limitations
- Dataset coverage uncertainty: The study's effectiveness may vary with larger or more diverse procedural texts beyond the four tested domains
- Ontology application inconsistency: While text extraction scores are high, ontology formatting scores remain lower due to formatting discrepancies
- Zero-shot limitations acknowledged: The study explicitly identifies zero-shot learning as a limitation with occasional failures in ontology application

## Confidence
- High confidence: In-context learning with ontology definitions improves procedural text extraction accuracy compared to zero-shot approaches (ROUGE score improvements of 30+ points)
- Medium confidence: 2-shot learning significantly improves ontology application accuracy, though limited by small number of examples used
- Medium confidence: Incremental questioning shows promise for complex procedures, but lacks comparative data against holistic approaches

## Next Checks
1. Validate the approach on a larger, more diverse corpus of procedural texts (e.g., 100+ documents across 10+ domains) to assess scalability and generalizability
2. Conduct a detailed error analysis of incorrectly formatted ontology outputs to identify specific failure patterns and improve the ontology definition approach
3. Test the optimal number of examples for few-shot learning beyond 2-shot to determine if accuracy continues improving with more examples or plateaus