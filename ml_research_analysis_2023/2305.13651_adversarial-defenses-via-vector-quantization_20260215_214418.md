---
ver: rpa2
title: Adversarial Defenses via Vector Quantization
arxiv_id: '2305.13651'
source_url: https://arxiv.org/abs/2305.13651
tags:
- adversarial
- swrd
- accuracy
- image
- defenses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a preprocessing-based defense against adversarial
  attacks that uses vector quantization. The method treats an image as a collection
  of overlapping patches and finds a set of representative patches based on the patch
  distribution.
---

# Adversarial Defenses via Vector Quantization

## Quick Facts
- arXiv ID: 2305.13651
- Source URL: https://arxiv.org/abs/2305.13651
- Reference count: 40
- Primary result: Proposes pRD and swRD defenses using vector quantization that achieve state-of-the-art robust accuracy on MNIST, CIFAR10, and SVHN while maintaining certified accuracy guarantees

## Executive Summary
This paper introduces preprocessing-based defenses against adversarial attacks that leverage vector quantization at the patch level. The method treats images as collections of overlapping patches, applies Gaussian noise, clusters similar patches using k-means, and quantizes each patch to its nearest cluster center. The resulting defenses, called patched RandDisc (pRD) and sliding-window RandDisc (swRD), provide both empirical robustness and theoretical certified accuracy guarantees. Notably, while possessing the obfuscated gradients property, these defenses remain effective against stronger attacks like STE and EOT. The approach can also be extended to fine-tune the classifier on quantized images, yielding further performance improvements.

## Method Summary
The defense works by first extracting overlapping patches from input images, adding Gaussian noise, and clustering these patches using k-means to obtain representative cluster centers. Each patch is then quantized to its nearest cluster center (pRD uses fixed patch locations while swRD uses sliding windows). The quantized patches are reassembled into a defense-preprocessed image that serves as input to the classifier. The method exploits the efficiency of vector quantization in higher-dimensional patch space compared to scalar pixel-level quantization. The defense can be further enhanced by fine-tuning the classifier on the quantized version of the training set, which improves the classifier's ability to handle quantized inputs while maintaining robust accuracy.

## Key Results
- Achieves state-of-the-art robust accuracy against PGD attacks on MNIST, CIFAR10, and SVHN datasets
- Provides theoretical certified accuracy guarantees through KL divergence bounds between clean and adversarial output distributions
- Maintains effectiveness against STE and EOT attacks despite possessing obfuscated gradients property
- Classifier fine-tuning on quantized images yields additional improvements with large margins
- Outperforms scalar quantization methods like RandDisc by exploiting structural correlations in image patches

## Why This Works (Mechanism)

### Mechanism 1
Vector quantization in higher-dimensional patch space provides better resilience to adversarial perturbations than scalar quantization. By quantizing overlapping patches rather than individual pixels, the defense exploits structural correlations within image patches, creating a higher-dimensional representation where cluster centers are more robust to small perturbations because they capture more context. The distribution of image patches is sufficiently structured that vector quantization yields tighter clustering and more meaningful quantization regions than scalar quantization.

### Mechanism 2
The KL divergence between output distributions for clean and adversarial images remains small, enabling certified accuracy. The defense ensures that Q(ˆX|X=x) and Q(ˆX|X=x') (where x' is the adversarial image) are similar through the quantization process, which maps both clean and adversarial images to nearby cluster centers. The cluster centers obtained from the clean image remain stable under small adversarial perturbations.

### Mechanism 3
Fine-tuning the classifier on quantized images improves robust accuracy. By training the classifier on the quantized version of the training set, the classifier learns to better classify images that have been processed by the quantization defense. The quantized images preserve enough information for the classifier to learn effective decision boundaries.

## Foundational Learning

- Concept: Vector quantization
  - Why needed here: Understanding vector quantization is crucial for grasping how the defense operates in higher-dimensional patch space
  - Quick check question: What is the difference between scalar quantization and vector quantization, and why is vector quantization preferred in this context?

- Concept: Rate-distortion theory
  - Why needed here: Rate-distortion theory provides the theoretical foundation for why vector quantization is superior to scalar quantization
  - Quick check question: How does rate-distortion theory explain the efficiency of vector quantization over scalar quantization?

- Concept: KL divergence
  - Why needed here: KL divergence is used to measure the similarity between output distributions for clean and adversarial images, which is key to understanding certified accuracy
  - Quick check question: What does a small KL divergence between Q(ˆX|X=x) and Q(ˆX|X=x') imply about the defense's performance?

## Architecture Onboarding

- Component map: Input image -> Patch extraction -> Gaussian noise addition -> Clustering (k-means) -> Quantization (pRD/swRD) -> Classifier
- Critical path: Patch extraction -> Clustering -> Quantization -> Classifier
- Design tradeoffs:
  - Patch size vs. computational complexity
  - Number of cluster centers vs. accuracy
  - Noise level vs. robustness
- Failure signatures:
  - High KL divergence between clean and adversarial output distributions
  - Classifier misclassifies quantized clean images
  - Quantized images lose too much detail
- First 3 experiments:
  1. Evaluate the effect of patch size on accuracy and robustness
  2. Measure the KL divergence between output distributions for clean and adversarial images
  3. Fine-tune the classifier on quantized images and compare robust accuracy

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of vector quantization-based defenses like pRD and swRD compare to other adversarial training methods in terms of computational efficiency and scalability to larger datasets?
Basis in paper: The paper discusses state-of-the-art performance on MNIST, CIFAR10, and SVHN but doesn't explicitly compare computational efficiency and scalability to larger datasets
Why unresolved: The paper focuses on effectiveness against adversarial attacks but doesn't provide comprehensive comparison of computational resources required or scalability
What evidence would resolve it: Experiments comparing computational time and memory usage with other adversarial training methods on larger datasets

### Open Question 2
Can the vector quantization-based defenses be extended to handle other types of adversarial attacks, such as black-box attacks or transfer-based attacks?
Basis in paper: The paper primarily focuses on white-box attacks using PGD and doesn't discuss effectiveness against other types of attacks
Why unresolved: Experiments and analysis are limited to white-box attacks, leaving performance against other attack types unanswered
What evidence would resolve it: Evaluating performance against black-box attacks and transfer-based attacks

### Open Question 3
How does the choice of hyperparameters, such as the number of cluster centers and patch size, affect the trade-off between natural accuracy and robust accuracy in vector quantization-based defenses?
Basis in paper: The paper discusses impact of hyperparameters on performance including trade-off between natural and robust accuracy
Why unresolved: While providing insights into hyperparameter effects on accuracy, it doesn't offer comprehensive analysis of the trade-off
What evidence would resolve it: Systematic study of relationship between hyperparameters and trade-off between natural and robust accuracy

### Open Question 4
Can the vector quantization-based defenses be combined with other defense strategies, such as adversarial training or feature denoising, to further enhance their robustness?
Basis in paper: The paper focuses on performance of pRD and swRD as standalone defenses and doesn't explore combination with other defense strategies
Why unresolved: The paper doesn't investigate potential synergies between vector quantization-based defenses and other defense mechanisms
What evidence would resolve it: Experimenting with hybrid defense approaches combining pRD or swRD with other strategies

## Limitations

- Theoretical guarantees for certified accuracy rely on idealized assumptions about quantization and Gaussian noise behavior, potentially creating gaps between theory and practice
- While robust to STE and EOT attacks, the defenses haven't been extensively evaluated against other adaptive attacks that might exploit vector quantization characteristics
- Computational overhead of patch extraction and clustering is not thoroughly discussed, potentially limiting real-world applicability for resource-constrained environments

## Confidence

- High Confidence: Core mechanism of vector quantization at patch level providing better resilience than pixel-level quantization is well-supported by theoretical foundations and experimental results
- Medium Confidence: Claim about obfuscated gradients property coexisting with STE/EOT robustness is supported by experiments but requires more extensive evaluation against adaptive attacks
- Medium Confidence: Benefit of classifier fine-tuning on quantized images is demonstrated empirically but lacks rigorous theoretical justification for consistent improvement

## Next Checks

1. Conduct formal analysis of conditions under which KL divergence bounds hold and quantify gap between theoretical guarantees and empirical performance
2. Test defenses against broader range of adaptive attacks, including those specifically designed to exploit vector quantization process or patch extraction mechanism
3. Measure and report computational overhead of preprocessing step, including memory usage and inference time across different hardware platforms