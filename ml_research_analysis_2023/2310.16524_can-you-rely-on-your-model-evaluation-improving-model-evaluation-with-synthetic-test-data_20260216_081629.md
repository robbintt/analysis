---
ver: rpa2
title: Can You Rely on Your Model Evaluation? Improving Model Evaluation with Synthetic
  Test Data
arxiv_id: '2310.16524'
source_url: https://arxiv.org/abs/2310.16524
tags:
- data
- group
- performance
- dtest
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes 3S Testing, a deep generative modeling framework
  to improve model evaluation by generating synthetic test data for underrepresented
  subgroups and simulating distributional shifts. The key idea is that real test data
  often lacks sufficient samples for small subgroups or shifted distributions, leading
  to inaccurate performance estimates.
---

# Can You Rely on Your Model Evaluation? Improving Model Evaluation with Synthetic Test Data

## Quick Facts
- arXiv ID: 2310.16524
- Source URL: https://arxiv.org/abs/2310.16524
- Authors: 
- Reference count: 40
- Key outcome: 3S Testing framework generates synthetic test data for underrepresented subgroups and simulates distributional shifts, outperforming traditional baselines in estimating model performance with better coverage of ground truth.

## Executive Summary
This paper addresses the critical problem of unreliable model evaluation due to limited real test data for small subgroups and shifted distributions. The authors propose 3S Testing, a deep generative modeling framework that creates synthetic test sets to improve evaluation accuracy. By training conditional generative models on real test data and generating synthetic samples for specific subgroups or shifted distributions, 3S provides more granular and reliable performance estimates. The framework also incorporates uncertainty quantification through a deep generative ensemble, offering prediction intervals that demonstrate superior coverage compared to existing approaches.

## Method Summary
3S Testing works by training a conditional generative model (specifically CTGAN) on real test data, then using this model to generate synthetic test sets conditioned on subgroup membership or distributional shift specifications. The framework addresses two key problems: insufficient samples for small subgroups and lack of data for evaluating models under distributional shifts. For each subgroup or shift, 3S generates a large synthetic dataset, evaluates the model performance on this data, and uses a deep generative ensemble of K independently initialized models to estimate uncertainty in the performance estimates. The method assumes that conditional distributions remain constant across shifts, allowing synthetic data generation by sampling from shifted marginals while keeping conditional distributions fixed.

## Key Results
- 3S outperforms traditional baselines in estimating model performance on minority subgroups and under distributional shifts
- The framework provides prediction intervals with superior coverage of ground truth compared to existing approaches
- Experiments across five diverse datasets (Adult, Covid-19, Support, Bank, Drug) demonstrate consistent improvements in evaluation accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Synthetic data reduces variance in performance estimates for small subgroups by interpolating between real data points in the learned data manifold.
- **Mechanism:** A conditional generative model trained on the real test set learns an implicit data representation (e.g., low-dimensional manifold) from the entire dataset. This learned structure allows generation of synthetic samples within small subgroups that effectively "fill in" the gaps between sparse real data points, reducing the variance of performance estimates compared to direct evaluation on limited real data.
- **Core assumption:** The generative model can approximate the true conditional distribution p(Y|X) within the subgroup region, even when real data is sparse.
- **Evidence anchors:**
  - [abstract] "Our experiments demonstrate that 3S Testing outperforms traditional baselines—including real test data alone—in estimating model performance on minority subgroups and under plausible distributional shifts."
  - [section] "Generative models can provide a solution to both problems. As we will detail in the next section, instead of using Dtest,f, we use a generative model G trained on Dtest,f to create a large synthetic dataset Dsyn for evaluating f. We can induce shifts in the learnt distribution, thereby solving problem 2. It also solves the first problem. A generative model aims to approximate p(X, Y), which we can regard as effectively interpolating p(Y |X) between real data points to generate more data within S."

### Mechanism 2
- **Claim:** Synthetic data enables evaluation under distributional shifts by generating samples from shifted marginal distributions while keeping the conditional distribution p(X̄c|Xc) fixed.
- **Mechanism:** The 3S framework trains a conditional generative model to learn p(X̄c|Xc). For a given shift in the marginal distribution of features Xc (e.g., increased age), new samples xc are drawn from the shifted distribution, and the corresponding X̄c are generated using the learned conditional model. This creates a synthetic dataset that reflects the shifted distribution without requiring real data from the target domain.
- **Core assumption:** The conditional distribution p(X̄c|Xc) remains constant across the shift, which is a standard assumption in covariate and label shift literature.
- **Evidence anchors:**
  - [abstract] "3S Testing, a deep generative modeling framework to facilitate model evaluation by generating synthetic test sets for small subgroups and simulating distributional shifts."
  - [section] "Defining shifts. In some cases, there is prior knowledge to define shifts. For example, covariate shift [53, 54] focuses on a changing covariate distribution p(X), but a constant label distribution p(Y |X) conditional on the features. Label (prior probability) shift [54, 55] is defined vice versa, with fixed p(X|Y ) and changing p(Y ). Generalizing this slightly, we assume only the marginal of some variables changes, while the distribution of the other variables conditional on these variables does not."

### Mechanism 3
- **Claim:** Deep Generative Ensemble (DGE) provides reliable uncertainty estimates for synthetic test performance by capturing the variability in the generative process.
- **Mechanism:** Instead of relying on a single generative model, 3S trains an ensemble of K independently initialized generative models. Each model generates a synthetic dataset, and the performance metric is evaluated on each. The mean and variance of these evaluations provide an estimate of the performance and its uncertainty, which can be used to construct prediction intervals.
- **Core assumption:** The variability in the generative models' outputs captures the uncertainty in the synthetic data's ability to represent the true performance distribution.
- **Evidence anchors:**
  - [abstract] "3S offers intervals around its performance estimates, exhibiting superior coverage of the ground truth compared to existing approaches."
  - [section] "Estimating uncertainty. Generative models are not perfect, leading to imperfect synthetic datasets and inaccurate 3S estimates. To provide insight into the trustworthiness of its estimates, we quantify the uncertainty in the 3S generation process through an ensemble of generative models [25], similar in vein to Deep Ensembles [48]."

## Foundational Learning

- **Concept: Conditional Generative Models**
  - Why needed here: 3S relies on conditional generative models to generate synthetic data conditioned on subgroup or shift information, which is crucial for evaluating model performance on specific subgroups and under distributional shifts.
  - Quick check question: What is the key difference between a conditional generative model and an unconditional generative model, and why is this difference important for 3S?

- **Concept: Distributional Shifts (Covariate Shift and Label Shift)**
  - Why needed here: 3S aims to evaluate model performance under distributional shifts, which requires understanding the different types of shifts (covariate shift, label shift) and how they affect model performance.
  - Quick check question: What is the difference between covariate shift and label shift, and how does 3S handle each type of shift?

- **Concept: Maximum Mean Discrepancy (MMD)**
  - Why needed here: 3S uses MMD to assess the quality of the generated synthetic data, ensuring that it is close to the real data distribution and can be used for reliable model evaluation.
  - Quick check question: How does MMD measure the difference between two distributions, and why is it a suitable metric for evaluating the quality of synthetic data in 3S?

## Architecture Onboarding

- **Component map:** Real test data -> Conditional generative model training -> Synthetic data generation -> Performance evaluation -> Deep generative ensemble uncertainty estimation -> Performance estimates with intervals
- **Critical path:** 1. Train conditional generative model on real test data 2. Generate synthetic data for specific subgroups or shifts 3. Evaluate model performance on synthetic data 4. Estimate uncertainty using DGE 5. Report performance estimates with uncertainty intervals
- **Design tradeoffs:**
  - Generative Model Choice: CTGAN is chosen for its ability to handle tabular data, but other models (e.g., Normalizing Flows, TV AE) could be used, potentially affecting the quality of synthetic data.
  - Ensemble Size (K): Larger K provides more reliable uncertainty estimates but increases computational cost.
  - Shift Assumptions: 3S assumes the conditional distribution remains constant across shifts, which may not always hold, potentially affecting the accuracy of performance estimates under shifts.
- **Failure signatures:**
  - Poor Generative Model Quality: If the generative model does not accurately learn the data distribution, the synthetic data will not represent the real performance distribution, leading to inaccurate performance estimates.
  - Invalid Shift Assumptions: If the assumption that the conditional distribution remains constant across shifts is violated, the synthetic data will not accurately represent the shifted distribution, leading to inaccurate performance estimates under shifts.
  - Insufficient Ensemble Size: If the ensemble size K is too small, the uncertainty estimates may be overconfident, leading to unreliable performance estimates.
- **First 3 experiments:**
  1. Subgroup Performance Evaluation: Evaluate the performance of a trained model on minority subgroups using 3S and compare the results to evaluation on real test data alone.
  2. Distributional Shift Evaluation: Simulate a distributional shift (e.g., increased age) and evaluate the model's performance on the shifted distribution using 3S and compare the results to evaluation on real test data alone.
  3. Uncertainty Estimation: Evaluate the model's performance on a specific subgroup or shift using 3S with different ensemble sizes (K) and compare the resulting uncertainty estimates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different generative model architectures (e.g. Normalizing Flows, VAEs) compare to CTGAN in terms of synthetic data quality and downstream model evaluation performance?
- Basis in paper: [explicit] The paper mentions CTGAN is used as the default generative model but acknowledges that "any generative model can be used as the core of 3S". It also briefly compares CTGAN to Normalizing Flows (NF) and TV AE in Table 10, showing CTGAN outperforms them on MMD, inverse KLD, and JSD metrics.
- Why unresolved: The paper only provides a brief comparison on one dataset (Adult) and three metrics. A more comprehensive study is needed across multiple datasets, model architectures, and evaluation metrics to determine the optimal generative model choice for 3S.
- What evidence would resolve it: A systematic comparison of 3S using different generative model architectures across multiple tabular datasets, evaluating synthetic data quality using various metrics (e.g. MMD, inverse KLD, JSD) and downstream model evaluation performance (e.g. MAE vs oracle, coverage, width).

### Open Question 2
- Question: How does the quality of synthetic data generated by 3S impact model evaluation accuracy and uncertainty estimation?
- Basis in paper: [inferred] The paper highlights the importance of high-quality synthetic data for accurate model evaluation, mentioning that "Errors in the generative process can affect downstream evaluation" and that 3S uses a deep generative ensemble to quantify uncertainty. However, it doesn't directly investigate the relationship between synthetic data quality and evaluation accuracy.
- Why unresolved: While the paper shows that 3S outperforms baselines, it doesn't explicitly study how the quality of the generated synthetic data (e.g. measured by MMD, inverse KLD, JSD) relates to the accuracy of model evaluation estimates and the reliability of uncertainty intervals.
- What evidence would resolve it: An analysis of the correlation between synthetic data quality metrics (e.g. MMD, inverse KLD, JSD) and downstream model evaluation performance (e.g. MAE vs oracle, coverage, width) across multiple datasets and generative models.

### Open Question 3
- Question: How does 3S perform on datasets with different characteristics (e.g. size, dimensionality, feature types, class imbalance)?
- Basis in paper: [explicit] The paper evaluates 3S on five diverse datasets (Adult, Covid-19, Support, Bank, Drug) with varying characteristics. However, it doesn't systematically analyze how these characteristics impact 3S's performance.
- Why unresolved: While the paper demonstrates 3S's effectiveness on the chosen datasets, it's unclear how it would perform on datasets with different characteristics, such as very large or small datasets, high-dimensional data, mixed feature types, or severe class imbalance.
- What evidence would resolve it: A comprehensive study of 3S's performance across datasets with systematically varied characteristics (e.g. size, dimensionality, feature types, class imbalance), comparing it to baselines and analyzing the impact of these characteristics on evaluation accuracy and uncertainty estimation.

## Limitations
- The framework relies heavily on the assumption that conditional generative models can accurately capture true data distributions, which may fail for highly complex distributions
- Quality of synthetic data generation depends on the adequacy of the generative model and the representativeness of real test data
- The assumption that conditional distributions remain constant across distributional shifts may not hold in practice, limiting the framework's applicability

## Confidence
- High Confidence: The core mechanism of using conditional generative models to increase sample size for subgroup evaluation is well-supported by experimental results across multiple datasets.
- Medium Confidence: The effectiveness of distributional shift simulation through conditional generation, as this relies on the assumption that conditional distributions remain constant across shifts.
- Medium Confidence: The deep generative ensemble's ability to provide reliable uncertainty estimates, as this depends on the diversity and quality of individual ensemble members.

## Next Checks
1. **Sensitivity Analysis on Ensemble Size**: Systematically evaluate how the number of ensemble members (K) affects the width and coverage of uncertainty intervals across different datasets and subgroup sizes.
2. **Comparison with Alternative Generative Models**: Replace CTGAN with other conditional generative models (e.g., Normalizing Flows, TV AE) and compare synthetic data quality and downstream performance estimates.
3. **Out-of-Distribution Stress Test**: Intentionally introduce extreme distributional shifts (beyond the scope of the training data) to assess the limits of 3S's performance estimation capabilities and identify failure modes.