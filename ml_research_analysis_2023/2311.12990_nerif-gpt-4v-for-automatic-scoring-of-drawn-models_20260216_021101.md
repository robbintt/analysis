---
ver: rpa2
title: 'NERIF: GPT-4V for Automatic Scoring of Drawn Models'
arxiv_id: '2311.12990'
source_url: https://arxiv.org/abs/2311.12990
tags:
- gpt-4v
- scoring
- image
- students
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored GPT-4V's potential for automatically scoring
  student-drawn scientific models. A novel method, NERIF (Notation-Enhanced Rubric
  Instruction for Few-shot Learning), was developed to prompt GPT-4V using instructional
  notes and rubrics.
---

# NERIF: GPT-4V for Automatic Scoring of Drawn Models

## Quick Facts
- **arXiv ID**: 2311.12990
- **Source URL**: https://arxiv.org/abs/2311.12990
- **Reference count**: 8
- **Primary result**: GPT-4V scored student-drawn models with mean accuracy of 0.51 (SD = 0.037)

## Executive Summary
This study explored GPT-4V's potential for automatically scoring student-drawn scientific models. Researchers developed a novel method called NERIF (Notation-Enhanced Rubric Instruction for Few-shot Learning) to prompt GPT-4V using instructional notes and rubrics. When tested on 900 student-drawn models from six science assessment tasks, GPT-4V achieved an overall scoring accuracy of 0.51. The method showed promise, particularly for simpler models, with accuracy varying across categories: 0.64 for 'Beginning' models, 0.62 for 'Developing' models, and 0.26 for 'Proficient' models. While GPT-4V demonstrated the ability to retrieve information from images and apply rubrics, its accuracy needs improvement for practical classroom use.

## Method Summary
The study developed NERIF, a prompt engineering approach that combines scoring rubrics with instructional notes and example evaluations to guide GPT-4V's scoring decisions. Researchers tested this method on 900 student-drawn models across six science assessment tasks, comparing GPT-4V's scores with those of human experts. The approach used nine example evaluations for few-shot learning, allowing GPT-4V to learn scoring patterns from human-graded examples. The models were categorized into three levels (Beginning, Developing, Proficient) based on their quality and scientific understanding demonstrated.

## Key Results
- GPT-4V achieved overall scoring accuracy of mean = 0.51 (SD = 0.037) compared to human experts
- Accuracy varied significantly by model category: 0.64 for 'Beginning' models, 0.62 for 'Developing' models, and 0.26 for 'Proficient' models
- Qualitative analysis revealed GPT-4V's ability to identify model characteristics and apply rubrics in an interpretable manner

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4V can process visual input and retrieve relevant information from student-drawn models to make scoring decisions.
- Mechanism: GPT-4V leverages its multimodal capabilities to analyze both text and images, extracting features like labels, arrows, and structural arrangements from student drawings. It then correlates these features with the provided rubric and examples to assign a score.
- Core assumption: The visual features extracted by GPT-4V are sufficient to represent the key aspects of the student's understanding as defined by the rubric.
- Evidence anchors:
  - [abstract] "GPT-4V's average scoring accuracy was mean =.51, SD = .037"
  - [section] "GPT-4V can catch the characteristics of student-drawn models" - identifies features like "butter particles," "fire molecules," and "spreading"
  - [corpus] Weak - no direct evidence in corpus, but related papers on GPT-4V's image processing capabilities suggest this is plausible
- Break condition: If the student drawings are too abstract or the visual features do not align with the rubric's criteria, GPT-4V's accuracy will significantly decrease.

### Mechanism 2
- Claim: Few-shot learning with example evaluations improves GPT-4V's scoring accuracy.
- Mechanism: Providing GPT-4V with a small set of human-scored examples allows it to learn the implicit assumptions and decision-making process used by human graders. This enables GPT-4V to generalize to new, unseen student drawings.
- Core assumption: The provided examples are representative of the range of student responses and capture the nuances of the scoring rubric.
- Evidence anchors:
  - [abstract] "The study further uncovers how GPT-4V assigned scores in an interpretable way according to NERIF"
  - [section] "Examples and notes can improve GPT-4V performance on scoring" - shows that GPT-4V's predictions are more accurate with examples
  - [corpus] Weak - no direct evidence in corpus, but related papers on few-shot learning in other domains suggest this is plausible
- Break condition: If the provided examples are not diverse enough or do not cover the full range of possible student responses, GPT-4V may not learn the correct patterns.

### Mechanism 3
- Claim: The Notation-Enhanced Rubric Instruction (NERIF) method improves GPT-4V's ability to align its scoring with the human rubric.
- Mechanism: NERIF provides explicit instructions and notes to GPT-4V about what to look for in the student drawings. This helps GPT-4V focus on the relevant features and apply the scoring rubric consistently.
- Core assumption: The instructional notes accurately capture the implicit assumptions and decision-making process used by human graders.
- Evidence anchors:
  - [abstract] "Our approach highlights the few-shot learning with heuristically added 'instructional Notes' which improve GPT-4V's performance"
  - [section] "GPT-4V can assess student-drawn models according to the rubric" - shows that GPT-4V's predictions align with the rubric
  - [corpus] Weak - no direct evidence in corpus, but related papers on prompt engineering in other domains suggest this is plausible
- Break condition: If the instructional notes are not clear or do not accurately reflect the human scoring process, GPT-4V's predictions may not align with the rubric.

## Foundational Learning

- Concept: Multimodal AI and visual question answering
  - Why needed here: Understanding how GPT-4V processes and analyzes both text and images is crucial for designing effective prompts and interpreting its outputs.
  - Quick check question: What are the key differences between how GPT-4V processes text and images, and how does this impact its ability to score student drawings?

- Concept: Few-shot learning and prompt engineering
  - Why needed here: Few-shot learning with carefully crafted prompts is the key to GPT-4V's ability to generalize to new student drawings. Understanding the principles of few-shot learning and prompt engineering is essential for designing effective prompts.
  - Quick check question: How does the quality and diversity of the provided examples impact GPT-4V's ability to learn the scoring rubric and generalize to new drawings?

- Concept: Educational assessment and rubric design
  - Why needed here: Understanding the principles of educational assessment and rubric design is crucial for interpreting GPT-4V's outputs and ensuring that its scoring aligns with the intended learning outcomes.
  - Quick check question: How does the structure and clarity of the scoring rubric impact GPT-4V's ability to assign accurate scores, and what are the potential challenges in translating a human-designed rubric into a machine-readable format?

## Architecture Onboarding

- Component map:
  Input: Problem context, scoring rubric, instructional notes, example evaluations, student-drawn models
  Processing: GPT-4V's multimodal capabilities for analyzing text and images
  Output: Predicted scores for each student-drawn model

- Critical path:
  1. Retrieve problem context and rubric from input images
  2. Analyze example evaluations to learn implicit assumptions and decision-making process
  3. Process student-drawn models to extract relevant features
  4. Apply learned patterns and rubric to assign predicted scores

- Design tradeoffs:
  - Accuracy vs. interpretability: GPT-4V's predictions are interpretable, but its accuracy is lower than human experts. Tradeoff between the two needs to be carefully considered.
  - Few-shot learning vs. fine-tuning: Few-shot learning with carefully crafted prompts is used, but fine-tuning on a larger dataset may improve accuracy at the cost of interpretability.

- Failure signatures:
  - Low accuracy on 'Proficient' models: GPT-4V may struggle with more complex or nuanced student responses that require a deeper understanding of the subject matter.
  - Inconsistent predictions: GPT-4V's predictions may vary depending on the specific prompt and examples provided, highlighting the need for careful prompt engineering.

- First 3 experiments:
  1. Vary the number and diversity of example evaluations to assess their impact on GPT-4V's accuracy and generalization.
  2. Experiment with different prompt structures and instructional notes to optimize GPT-4V's ability to align its scoring with the human rubric.
  3. Compare GPT-4V's performance on different types of student-drawn models (e.g., simple vs. complex, clear vs. ambiguous) to identify potential areas for improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the scoring accuracy of GPT-4V be improved for 'Proficient' level student-drawn models?
- Basis in paper: [explicit] The paper states that GPT-4V's accuracy was significantly lower for 'Proficient' models (.26) compared to 'Beginning' (.64) and 'Developing' (.62) models.
- Why unresolved: The paper suggests that the scoring rubric's requirement for all components to be included for a 'Proficient' rating might be too strict, leading to misclassification. However, the exact reasons for this discrepancy and potential solutions are not explored in depth.
- What evidence would resolve it: Experiments testing alternative scoring rubrics, multiple rounds of GPT-4V assessment with voting mechanisms, or modifications to the NERIF approach specifically targeting 'Proficient' level assessments could provide evidence.

### Open Question 2
- Question: What is the optimal number and selection of example evaluations needed for effective few-shot learning in NERIF?
- Basis in paper: [explicit] The study used 9 example evaluations for few-shot learning, but the paper does not explore whether this number is optimal or if different selections of examples could yield better results.
- Why unresolved: The paper mentions that few-shot learning was used but does not provide a systematic exploration of how the number and selection of examples affect GPT-4V's performance.
- What evidence would resolve it: Comparative studies using varying numbers of examples (e.g., 3, 6, 12) and different selections of examples could demonstrate the optimal configuration for NERIF.

### Open Question 3
- Question: How does GPT-4V's performance on student-drawn models compare to other visual language models or fine-tuned deep learning models in educational assessment?
- Basis in paper: [inferred] The paper discusses GPT-4V's potential but does not compare its performance to other models like ResNet, Inception-v3, or fine-tuned deep learning models used in previous studies.
- Why unresolved: While the paper demonstrates GPT-4V's capabilities, it does not benchmark its performance against other state-of-the-art models in the field of educational assessment.
- What evidence would resolve it: Direct comparisons of GPT-4V with other models on the same dataset, using the same evaluation metrics, would provide insights into its relative performance and potential advantages or disadvantages.

## Limitations
- Overall scoring accuracy of 0.51 is insufficient for standalone classroom use without human oversight
- Significant drop in accuracy for 'Proficient' models (0.26) suggests difficulty with complex, nuanced responses
- Does not address potential biases related to drawing quality, cultural differences in visual representation, or accessibility for students with disabilities

## Confidence
- **High Confidence**: GPT-4V can process visual information and retrieve relevant features from student drawings
- **Medium Confidence**: The NERIF method improves GPT-4V's alignment with human rubrics through few-shot learning
- **Low Confidence**: GPT-4V is ready for practical classroom implementation

## Next Checks
1. Test GPT-4V on a larger, more diverse dataset including students from different educational backgrounds and cultural contexts to assess scoring consistency and potential biases
2. Conduct a head-to-head comparison between GPT-4V and human experts on the same models, including statistical analysis of inter-rater reliability to quantify the margin of error
3. Evaluate GPT-4V's performance when scoring incomplete or partially erased drawings to assess robustness against common student drawing variations