---
ver: rpa2
title: 'MOFI: Learning Image Representations from Noisy Entity Annotated Images'
arxiv_id: '2306.07952'
source_url: https://arxiv.org/abs/2306.07952
tags:
- image
- entities
- dataset
- entity
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MOFI, a vision foundation model that learns
  image representations from noisy entity annotated images. The authors propose a
  new approach to automatically assign entity labels to images from noisy image-text
  pairs by using a named entity recognition model to extract entities from alt-text
  and a CLIP model to select the correct entities as labels.
---

# MOFI: Learning Image Representations from Noisy Entity Annotated Images

## Quick Facts
- **arXiv ID**: 2306.07952
- **Source URL**: https://arxiv.org/abs/2306.07952
- **Reference count**: 40
- **Primary result**: MOFI achieves 86.66% mAP on GPR1200, surpassing CLIP's 72.19% state-of-the-art.

## Executive Summary
MOFI introduces a novel approach to learning image representations from noisy web-mined image-text pairs by automatically assigning entity labels through a combination of named entity recognition (NER) and CLIP-based filtering. The authors construct the I2E dataset containing 1 billion images and 2 million distinct entities, enabling large-scale supervised pre-training. MOFI demonstrates state-of-the-art performance on image retrieval tasks and shows strong zero-shot classification capabilities, highlighting the effectiveness of fine-grained entity supervision compared to traditional image-text contrastive learning.

## Method Summary
The method involves extracting entities from alt-text using NER, then filtering them with a CLIP model to ensure relevance to the paired image. These entities serve as labels for supervised pre-training, while their descriptions provide enriched text for contrastive learning. The model employs multi-task training with both supervised classification (using large margin cosine loss) and contrastive objectives (cross-entropy loss), balanced by a hyperparameter λ. This approach leverages structured supervision from entities to learn fine-grained visual concepts, outperforming traditional image-text contrastive models on retrieval and classification tasks.

## Key Results
- MOFI achieves 86.66% mAP@1k on GPR1200, surpassing CLIP's 72.19% SOTA.
- Supervised pre-training with entity labels significantly improves retrieval (+17.13 mAP over supervised baseline, +16.0 over CLIP).
- Multi-task training further improves performance to 86.15% mAP@1k on GPR1200.
- MOFI outperforms CLIP on zero-shot and linear probe ImageNet classification.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entity labels from noisy text provide more structured knowledge than raw image-text pairs, leading to stronger image representations.
- Mechanism: NER extracts structured entities from alt-text, filtered by CLIP for relevance, improving supervision quality over unstructured text.
- Core assumption: Extracted entities are relevant and accurately disambiguated using context and external knowledge.
- Evidence anchors:
  - [abstract] "Our approach involves employing a named entity recognition model to extract entities from the alt-text, and then using a CLIP model to select the correct entities as labels of the paired image."
  - [section 2.1] "In order to reduce the noise in the dataset, we use CLIP model to compute the CLIP embedding for the text representation of entities, and filter out the entities which have lower cosine similarity with the image embedding."
- Break Condition: If entity extraction accuracy drops significantly, or if CLIP filtering fails to remove irrelevant entities.

### Mechanism 2
- Claim: Supervised pre-training on a large number of entity labels significantly improves image retrieval performance.
- Mechanism: Treating each entity as a class label enables learning fine-grained visual concepts, enhancing semantic richness for retrieval.
- Core assumption: Dataset has sufficient examples per entity to enable meaningful supervised learning at scale.
- Evidence anchors:
  - [abstract] "Experiments show that supervised pre-training with large-scale fine-grained entity labels is highly effective for image retrieval tasks..."
  - [section 4.2] "MOFI outperforms existing supervised model (+17.13, Row 8 vs. Row 2) and CLIP model (+16.0, Row 8 vs. Row 3) by a significant margin."
- Break Condition: If examples per entity are too low, or entity labels are too noisy.

### Mechanism 3
- Claim: Multi-task learning combining supervised and contrastive objectives yields the best performance.
- Mechanism: Supervised loss leverages entity labels for fine-grained classification, while contrastive loss aligns images and enriched entity descriptions in shared embedding space.
- Core assumption: Model can effectively learn from both objectives without interference, and λ=0.5 is optimal.
- Evidence anchors:
  - [abstract] "Building upon the I2E dataset, we study different training recipes like supervised pre-training, contrastive pre-training, and multi-task learning."
  - [section 3.3] "Lcombined = λLclass + (1 − λ)Lcontrast , where λ is a hyper-parameter to balance the two loss terms. For simplicity, we set λ to 0.5 in all experiments."
  - [section 4.2] "The multi-task model reaches a new state-of-the-art of 86.15% mAP@1k, compared to the previous SoTA of 72.19% from OpenAI’s CLIP model."
- Break Condition: If objectives conflict or λ is suboptimal.

## Foundational Learning

- **Concept**: Large-scale weakly supervised learning from web data.
  - Why needed here: MOFI trained on 1B images with 2M entities requires methods that scale beyond human-labeled datasets.
  - Quick check question: How does the model handle label noise when entities are automatically extracted from noisy alt-text?

- **Concept**: Multi-task learning with classification and contrastive objectives.
  - Why needed here: Combining supervised entity classification with image-text contrastive learning leverages both structured labels and semantic alignment.
  - Quick check question: What is the role of λ in balancing the two loss terms, and how was 0.5 chosen?

- **Concept**: Entity enrichment with external knowledge (descriptions).
  - Why needed here: Using entity descriptions alongside names provides richer semantic context, improving text embeddings for contrastive learning.
  - Quick check question: How are entity descriptions obtained and incorporated into the text representation for contrastive training?

## Architecture Onboarding

- **Component map**: Web corpus → NER + CLIP filtering → I2E dataset → ViT image encoder + text encoder → Multi-task loss (supervised + contrastive) → MOFI model
- **Critical path**: 1. Data extraction and filtering (NER + CLIP filtering) → 2. Model training with multi-task objectives → 3. Evaluation on retrieval and classification benchmarks
- **Design tradeoffs**: Entity granularity vs. dataset size; supervised vs. contrastive objectives; model size (B/16, L/14, H/14) vs. compute requirements
- **Failure signatures**: Low retrieval mAP (poor entity filtering, insufficient negatives, suboptimal λ); poor zero-shot classification (weak contrastive component or undertrained text encoder); high variance (unstable entity extraction/filtering)
- **First 3 experiments**:
  1. Train MOFI-B/16 with only supervised loss on subset of I2E (top 100k entities) and evaluate on GPR1200.
  2. Train MOFI-B/16 with only contrastive loss on I2E (entity names + descriptions) and compare to CLIP baseline on ImageNet retrieval.
  3. Train MOFI-B/16 with multi-task loss (λ=0.5) on full I2E and ablate λ (0.3, 0.7) to find optimal balance.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can MOFI's performance on specialized tasks be improved, given its current limitations in those domains?
  - Basis in paper: [inferred] MOFI struggles with specialized tasks like RESISC45 and Camelyon, possibly because images present greater challenges in entity description.
  - Why unresolved: Paper does not explore methods to improve MOFI's performance on specialized tasks.
  - What evidence would resolve it: Experiments comparing MOFI's performance on specialized tasks with and without additional fine-tuning on domain-specific data or using alternative entity extraction methods.

- **Open Question 2**: What is the optimal number of entities to include in the I2E dataset for maximizing MOFI's performance?
  - Basis in paper: [explicit] Paper investigates impact of varying entity count, finds improvement until 1 million entities, but effect beyond is unclear.
  - Why unresolved: Model size may not be large enough to leverage tail entities, or current evaluation may not reveal potential improvements.
  - What evidence would resolve it: Experiments comparing MOFI's performance with different entity counts, using larger models or alternative evaluation methods.

- **Open Question 3**: How does the use of entity descriptions as external knowledge affect MOFI's performance, and what is the best way to incorporate this knowledge?
  - Basis in paper: [explicit] Paper explores entity names and descriptions as external knowledge, noting similar ideas in K-Lite but on smaller scale.
  - Why unresolved: Paper does not provide detailed analysis of impact or explore different incorporation methods.
  - What evidence would resolve it: Experiments comparing MOFI's performance with and without entity descriptions, using different incorporation methods.

## Limitations

- Entity extraction and filtering pipeline details are not fully specified (CLIP similarity threshold, entity frequency cutoff).
- Hyperparameters for loss functions (margin value, temperature, number of negatives) are not provided.
- Ablation studies on λ parameter in multi-task learning are limited, with only λ=0.5 reported as optimal.
- Potential biases in web-mined data and impact of NSFW/low-quality filtering are not addressed.

## Confidence

- **High Confidence**: Supervised pre-training with large-scale entity labels significantly improves retrieval performance.
- **Medium Confidence**: Multi-task learning combining supervised and contrastive objectives yields best overall performance.
- **Low Confidence**: Entity enrichment with descriptions meaningfully improves contrastive learning.

## Next Checks

1. **Entity Extraction Quality**: Re-run entity extraction pipeline on small data subset and manually verify relevance/accuracy. Measure precision and recall of NER + CLIP filtering process.
2. **Ablation on λ**: Train MOFI-B/16 with multi-task learning using λ values of 0.3, 0.5, and 0.7 on smaller dataset (top 100k entities) and evaluate on GPR1200 to determine optimal balance.
3. **Impact of Entity Descriptions**: Train two models (entity names only vs. names + descriptions for contrastive learning) and compare performance on ImageNet retrieval to assess contribution of enriched text embeddings.