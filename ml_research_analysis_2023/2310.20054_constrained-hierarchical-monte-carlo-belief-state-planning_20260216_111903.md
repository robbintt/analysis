---
ver: rpa2
title: Constrained Hierarchical Monte Carlo Belief-State Planning
arxiv_id: '2310.20054'
source_url: https://arxiv.org/abs/2310.20054
tags:
- planning
- options
- cobets
- search
- option
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Constrained Options Belief Tree Search (COBeTS),
  a Monte Carlo tree search algorithm for solving constrained partially observable
  Markov decision processes (CPOMDPs) in large or continuous domains by leveraging
  hierarchical decomposition through options. The key idea is to search over high-level
  action primitives (options) rather than primitive actions, reducing tree complexity
  and enabling planning in domains where non-hierarchical methods fail.
---

# Constrained Hierarchical Monte Carlo Belief-State Planning

## Quick Facts
- arXiv ID: 2310.20054
- Source URL: https://arxiv.org/abs/2310.20054
- Authors: 
- Reference count: 35
- Key outcome: COBeTS significantly outperforms state-of-the-art baselines in constrained POMDPs by leveraging hierarchical decomposition through options, achieving better reward while satisfying constraints in four benchmark domains.

## Executive Summary
This paper introduces Constrained Options Belief Tree Search (COBeTS), a Monte Carlo tree search algorithm for solving constrained partially observable Markov decision processes (CPOMDPs) in large or continuous domains. The key innovation is searching over high-level action primitives (options) rather than primitive actions, which reduces tree complexity and enables planning where non-hierarchical methods fail. The authors prove that if options satisfy assigned constraint budgets, COBeTS guarantees anytime constraint satisfaction. Experiments on four CPOMDP domains demonstrate significant improvements over state-of-the-art baselines in both reward and constraint satisfaction.

## Method Summary
COBeTS combines the options framework from hierarchical reinforcement learning with Monte Carlo tree search to solve CPOMDPs. The algorithm searches over high-level action primitives (options) rather than primitive actions, reducing tree complexity through semi-Markov transitions. Each option has its own termination probability and executes a low-level policy until termination. COBeTS implements progressive widening on both the option space and semi-Markov belief transitions to prevent tree explosion in continuous domains. The method uses Lagrangian dual ascent to guide search toward constraint satisfaction, assigning each option a constraint budget that must be satisfied for global feasibility.

## Key Results
- COBeTS outperforms CPFT-DPW and CPOMCPOW baselines on all four CPOMDP domains
- Hierarchical decomposition compensates for increased action branching when the number of options exceeds primitive actions
- Anytime constraint satisfaction is achieved when options satisfy their assigned local budgets
- COBeTS successfully handles both continuous and discrete observation domains where baselines fail

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical decomposition reduces search tree complexity by replacing primitive actions with semi-Markov option transitions, enabling planning in continuous CPOMDPs where non-hierarchical methods fail. Options collapse multiple primitive actions into single high-level decisions with their own termination probabilities, reducing the branching factor from A (primitive actions) to |A| (options) and compressing τ primitive steps into one semi-Markov transition. This works when options are defined a priori and can be executed without replanning during each option's execution.

### Mechanism 2
COBeTS guarantees anytime constraint satisfaction when options satisfy their assigned budget constraints, using Lagrangian dual ascent to guide search toward safety. Each option is assigned a constraint budget ce = (c - Ce-1)/γte during selection. If the option's expected cost Q(be, ae) ≤ ce, then by Proposition 2 it is one-step globally feasible, ensuring the global constraint is satisfied after execution. This relies on options being designed or verified to satisfy local constraint budgets when executed from beliefs where they are available.

### Mechanism 3
Progressive widening on both option space and semi-Markov belief transitions prevents tree explosion while maintaining sufficient exploration in continuous domains. Progressive widening limits the number of children from a node to approximately kN(b)^α, where N(b) is the visit count. This applies to both option selection (|Ch(b)| ≈ kaN(b)^αa) and transition sampling (|C(bae)| ≈ koN(bae)^αo). The method works when progressive widening parameters are chosen appropriately to balance exploration and computational tractability.

## Foundational Learning

- Concept: Constrained Partially Observable Markov Decision Processes (CPOMDPs)
  - Why needed here: The entire method operates on CPOMDPs, which generalize POMDPs with hard cost constraints. Understanding the tuple definition and value function optimization is essential for grasping why COBeTS is needed.
  - Quick check question: What is the difference between a POMDP and a CPOMDP in terms of the optimization objective?

- Concept: Options Framework in Hierarchical Reinforcement Learning
  - Why needed here: COBeTS uses options as high-level action primitives. Understanding the components {Iae, πL_ae, βae} and how they define available options, low-level policies, and termination functions is crucial for implementing COBeTS.
  - Quick check question: What are the three components of an option in the options framework, and what does each represent?

- Concept: Monte Carlo Tree Search with Progressive Widening
  - Why needed here: COBeTS is fundamentally an MCTS algorithm that uses progressive widening to handle continuous action and observation spaces. Understanding UCT, progressive widening, and how they apply to belief-space planning is essential.
  - Quick check question: How does progressive widening modify the standard UCT selection formula in continuous domains?

## Architecture Onboarding

- Component map:
  - High-level: COBeTS algorithm with hierarchical decomposition
  - Mid-level: Option selection using Lagrangian UCB, progressive widening on options and transitions
  - Low-level: Particle filter belief updates, semi-Markov transition sampling, value estimation
  - External: CPOMDP problem definition, option policies, constraint budgets

- Critical path:
  1. Initialize particle filter belief b0 and constraint budget c
  2. Select option using Lagrangian UCB with progressive widening
  3. Execute option via semi-Markov transition sampling with particle filter updates
  4. Backpropagate reward and cost values
  5. Update dual parameters through dual ascent
  6. Repeat until planning horizon or termination

- Design tradeoffs:
  - Option complexity vs. planning efficiency: More sophisticated options reduce search depth but increase option selection complexity
  - Progressive widening parameters: Aggressive widening reduces memory but may miss promising actions
  - Particle filter size m: Larger filters give better belief estimates but increase computation

- Failure signatures:
  - Constraint violations during execution: Options not locally feasible or progressive widening excluding safe options
  - Poor reward performance: Inadequate option design or insufficient search depth
  - Memory explosion: Progressive widening parameters too lenient or too many options

- First 3 experiments:
  1. LightDark with GoToGoal and LocalizeSafe options: Verify anytime constraint satisfaction and compare against CPFT-DPW baseline
  2. Vary number of options in LightDark: Test hypothesis that hierarchical decomposition compensates for increased branching
  3. Bumper Roomba with TurnAndGo options: Demonstrate success in discrete observation domains where baselines fail

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the following remain unresolved based on the content:

## Limitations
- Performance depends critically on option design quality, which is not systematically evaluated across different option types
- The method requires options to be defined a priori and may not adapt well to dynamic environments where option policies need to change
- Experimental evaluation is limited to four domains, raising questions about generalizability to more complex real-world scenarios

## Confidence
- **High Confidence**: The hierarchical decomposition mechanism reducing search complexity is theoretically sound and well-supported by the paper's proofs
- **Medium Confidence**: The anytime constraint satisfaction guarantee is proven but depends critically on option design quality, which is not thoroughly evaluated
- **Medium Confidence**: Experimental results showing improved performance over baselines are convincing, but the limited number of domains and lack of ablation studies reduce confidence in generalizability

## Next Checks
1. Conduct sensitivity analysis varying the number and quality of options to determine how option design impacts COBeTS performance and constraint satisfaction
2. Test COBeTS on additional continuous POMDP benchmarks beyond the four domains presented to assess generalizability
3. Perform ablation studies removing progressive widening or hierarchical decomposition to quantify their individual contributions to performance improvements