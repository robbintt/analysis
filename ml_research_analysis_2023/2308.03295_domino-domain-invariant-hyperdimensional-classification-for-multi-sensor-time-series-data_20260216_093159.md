---
ver: rpa2
title: 'DOMINO: Domain-invariant Hyperdimensional Classification for Multi-Sensor
  Time Series Data'
arxiv_id: '2308.03295'
source_url: https://arxiv.org/abs/2308.03295
tags:
- data
- domino
- domain
- each
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "DOMINO addresses distribution shift in multi-sensor time series\
  \ data by using hyperdimensional computing (HDC) to identify and filter out domain-variant\
  \ dimensions, enabling more robust domain generalization. It dynamically regenerates\
  \ dimensions and leverages efficient matrix operations to achieve 2.04% higher accuracy\
  \ than state-of-the-art DNN-based domain generalization techniques, while being\
  \ 7.83\xD7 faster in training and 26.94\xD7 faster in inference."
---

# DOMINO: Domain-invariant Hyperdimensional Classification for Multi-Sensor Time Series Data

## Quick Facts
- arXiv ID: 2308.03295
- Source URL: https://arxiv.org/abs/2308.03295
- Reference count: 39
- Key outcome: DOMINO achieves 2.04% higher accuracy than SOTA DNN-based domain generalization techniques while being 7.83× faster in training and 26.94× faster in inference on embedded devices

## Executive Summary
DOMINO is a domain-invariant hyperdimensional computing framework designed to address distribution shift in multi-sensor time series data. By leveraging hyperdimensional computing's parallel matrix operations and holographic pattern distributions, DOMINO identifies and filters out domain-variant dimensions to improve generalization capability. The approach dynamically regenerates dimensions and uses efficient matrix operations to achieve superior accuracy and resource efficiency compared to state-of-the-art deep learning methods, while also demonstrating exceptional robustness to hardware noise.

## Method Summary
DOMINO implements a two-stage pipeline: domain-specific modeling followed by domain generalization. The method encodes multi-sensor time series data into high-dimensional space using n-gram windows, vector quantization, and temporal permutations. It then trains separate models for each domain using cosine similarity updates and bundling operations. For domain generalization, DOMINO calculates variance across domains for each dimension, filters out high-variance (domain-variant) dimensions, regenerates these dimensions with random hypervectors, and retrains. The framework also incorporates hardware optimizations including multi-threading, tiled matrix multiplication, kernel fusion, and quantization to enable deployment on resource-constrained edge devices.

## Key Results
- DOMINO achieves 2.04% higher accuracy than state-of-the-art DNN-based domain generalization techniques
- Training is 7.83× faster and inference is 26.94× faster than comparable DNN methods
- DOMINO performs notably better with limited labeled data (up to 5.27% higher accuracy) and highly imbalanced data (2.58% higher accuracy on average)
- Exhibits 10.93× higher robustness against hardware noises than SOTA DNNs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DOMINO dynamically identifies and filters out domain-variant dimensions to improve generalization capability.
- Mechanism: During each training iteration, DOMINO constructs class-specific matrices from domain-specific models and calculates variance across domains for each dimension. Dimensions with high variance are considered domain-variant and are filtered out and regenerated with new random hypervectors.
- Core assumption: High variance across domains for a given dimension indicates that dimension represents domain-specific rather than domain-general information.
- Evidence anchors:
  - [abstract]: "DOMINO leverages efficient and parallel matrix operations on high-dimensional space to dynamically identify and filter out domain-variant dimensions."
  - [section II-D]: "For each class-specific matrix, we calculate the variance of each dimension to measure its dispersion. Dimensions with large variance indicate that, for the same class, these dimensions store very differentiated information, and are hence considered domain-variant."
- Break condition: If domain variance is not a reliable indicator of domain-specific information (e.g., if domains naturally have high variance in important features), this mechanism could degrade performance by removing useful information.

### Mechanism 2
- Claim: DOMINO uses efficient parallel matrix operations to enable resource-efficient domain generalization on edge devices.
- Mechanism: DOMINO represents data and operations in high-dimensional space using hypervectors, enabling highly parallel operations like similarity calculations, bundling, and binding. These operations can be performed efficiently using matrix operations that leverage the computational capabilities of edge devices.
- Core assumption: High-dimensional space with orthogonal hypervectors enables efficient parallel operations that can be implemented using standard matrix operations.
- Evidence anchors:
  - [abstract]: "DOMINO leverages efficient and parallel matrix operations on high-dimensional space to dynamically identify and filter out domain-variant dimensions."
  - [section II-A]: "Similarity: calculation of the distance between the query hypervector and the class hypervector... For bipolar hypervectors, it is simplified to the Hamming distance. Bundling (+): element-wise addition of multiple hypervectors... Binding (*): element-wise multiplication associating two hypervectors..."
- Break condition: If the overhead of managing high-dimensional representations exceeds the benefits of parallelization, or if edge devices cannot efficiently implement the required matrix operations, this mechanism could fail.

### Mechanism 3
- Claim: DOMINO achieves superior robustness to hardware noise compared to DNNs by leveraging holographic pattern distributions in high-dimensional space.
- Mechanism: In high-dimensional space, each hypervector stores information across all its components equally. When hardware noise causes random bit flips, the loss of accuracy is distributed across all dimensions rather than concentrated in specific weights, making the system more robust to partial failures.
- Core assumption: Redundant and holographic distribution of patterns across high-dimensional space provides inherent robustness to hardware noise.
- Evidence anchors:
  - [abstract]: "DOMINO exhibits 10.93× higher robustness against hardware noises than SOTA DNNs."
  - [section II-A]: "One unique property of the hyperdimensional space is the existence of a large number of nearly orthogonal hypervectors, enabling highly parallel operations such as similarity calculations, bundlings, and bindings."
- Break condition: If hardware noise patterns are not random or if they systematically affect certain dimensions more than others, this mechanism could fail to provide the claimed robustness benefits.

## Foundational Learning

- Concept: Distribution shift and domain generalization
  - Why needed here: DOMINO is specifically designed to address distribution shift in multi-sensor time series data across different domains.
  - Quick check question: What is the difference between domain adaptation and domain generalization, and why is DOMINO focused on the latter?

- Concept: Hyperdimensional computing fundamentals
  - Why needed here: DOMINO leverages HDC's unique properties like parallel operations, similarity calculations, and holographic representations to achieve its goals.
  - Quick check question: How do bundling, binding, and permutation operations work in HDC, and why are they useful for time series classification?

- Concept: Variance as a measure of domain-specificity
  - Why needed here: DOMINO uses variance across domains to identify and filter out domain-variant dimensions.
  - Quick check question: Why would high variance across domains indicate that a dimension is domain-specific rather than domain-general?

## Architecture Onboarding

- Component map: Encoding module -> Domain-specific modeling -> Domain generalization (variance calculation, filtering, regeneration) -> Model ensemble -> Inference
- Critical path: Encoding → Domain-specific modeling → Domain generalization (variance calculation, filtering, regeneration) → Model ensemble → Inference
- Design tradeoffs:
  - Dimensionality vs. accuracy: Higher effective dimensionality generally improves accuracy but increases computational cost
  - Regeneration rate: Higher rates filter more domain-variant information but risk removing useful domain-general information
  - Physical vs. effective dimensionality: Compressed physical dimensionality reduces resource usage while maintaining accuracy through iterative regeneration
- Failure signatures:
  - Performance degradation when domains have naturally high variance in important features
  - Increased sensitivity to hyperparameter choices (dimensionality, regeneration rate)
  - Potential overfitting if domain-specific models are too closely tied to source domains
- First 3 experiments:
  1. Reproduce the baseline HDC results on a simple dataset to verify understanding of the encoding and basic HDC operations
  2. Implement the variance-based domain-variant filtering on a synthetic dataset with known domain-variant dimensions to verify the mechanism works as intended
  3. Test DOMINO's performance on a small real-world multi-sensor time series dataset, comparing against the baseline HDC to verify the domain generalization improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DOMINO's accuracy scale with dimensionality beyond the tested range (e.g., 8k, 16k dimensions)?
- Basis in paper: [explicit] The paper mentions exploring hyperparameter design space for physical dimensionality (D) and effective dimensionality (D*), but doesn't provide results for dimensions significantly larger than 4k.
- Why unresolved: The paper focuses on demonstrating effectiveness at 0.5k physical dimensions with 4k effective dimensions, but doesn't explore whether even higher dimensions would yield further accuracy improvements or diminishing returns.
- What evidence would resolve it: Empirical results showing accuracy trends across a broader range of dimensionalities (e.g., 0.5k, 1k, 2k, 4k, 8k, 16k) with corresponding regeneration rates and iteration counts.

### Open Question 2
- Question: How does DOMINO perform on time series data with longer temporal dependencies (e.g., >10 seconds) compared to RNN-based approaches?
- Basis in paper: [inferred] The paper demonstrates DOMINO's effectiveness on datasets with relatively short windows (1-5 seconds), but doesn't address scenarios requiring modeling of longer-term temporal dependencies.
- Why unresolved: The paper focuses on multi-sensor time series classification but doesn't explicitly test whether DOMINO can capture dependencies beyond the sampling window used in its encoding technique.
- What evidence would resolve it: Comparative evaluation of DOMINO versus LSTM/GRU models on datasets with varying temporal dependencies, including long-term pattern recognition tasks.

### Open Question 3
- Question: What is the theoretical limit of DOMINO's noise robustness in terms of bit error rate before accuracy degradation becomes catastrophic?
- Basis in paper: [explicit] The paper shows DOMINO achieves 10.93× higher robustness against hardware noise compared to SOTA DNNs, but doesn't establish the absolute failure threshold.
- Why unresolved: The paper provides comparative robustness metrics but doesn't explore the point at which DOMINO's high-dimensional representation breaks down completely under extreme noise conditions.
- What evidence would resolve it: Systematic stress-testing of DOMINO with increasing bit error rates until accuracy drops below random chance, compared to baseline models' failure points.

## Limitations
- The variance-based mechanism for identifying domain-variant dimensions may not be reliable across different data distributions, potentially removing useful information
- Hardware efficiency claims lack detailed implementation specifications, making independent verification difficult
- The absolute noise robustness threshold is not established, limiting understanding of failure conditions under extreme noise

## Confidence
- **High confidence**: DOMINO's ability to filter domain-variant dimensions and achieve improved LODO accuracy (2.04% gain reported)
- **Medium confidence**: Hardware efficiency claims (7.83× faster training, 26.94× faster inference) due to missing implementation details
- **Medium confidence**: Noise robustness claims (10.93× improvement) due to lack of detailed noise pattern analysis

## Next Checks
1. **Variance mechanism validation**: Test DOMINO's dimension filtering on synthetic datasets where ground truth domain-variant dimensions are known, comparing variance-based filtering against alternative methods like mutual information or domain-adversarial training.

2. **Hyperparameter sensitivity analysis**: Systematically vary regeneration rate, dimensionality, and learning rate across all three datasets to determine if reported performance gains are robust to hyperparameter choices or specific to particular configurations.

3. **Noise pattern analysis**: Conduct controlled experiments with different noise patterns (random vs. systematic, single-bit vs. burst errors) to validate DOMINO's claimed 10.93× noise robustness and identify failure modes under non-random noise conditions.