---
ver: rpa2
title: How You Prompt Matters! Even Task-Oriented Constraints in Instructions Affect
  LLM-Generated Text Detection
arxiv_id: '2311.08369'
source_url: https://arxiv.org/abs/2311.08369
tags:
- instruction
- essay
- task-oriented
- detection
- constraint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how task-oriented constraints in prompts
  affect the performance of LLM-generated text detectors. The authors focus on student
  essay writing and manually create constraints based on factors of essay quality.
---

# How You Prompt Matters! Even Task-Oriented Constraints in Instructions Affect LLM-Generated Text Detection

## Quick Facts
- arXiv ID: 2311.08369
- Source URL: https://arxiv.org/abs/2311.08369
- Reference count: 8
- Primary result: Task-oriented constraints in prompts significantly increase variance in LLM-generated text detection performance

## Executive Summary
This paper investigates how task-oriented constraints in prompts affect the performance of LLM-generated text detectors. The authors focus on student essay writing and manually create constraints based on factors of essay quality. They find that detection performance variance is significantly larger when using instructions with these constraints compared to generating texts multiple times or paraphrasing the instruction. Specifically, the standard deviation of F1-score can be up to 14.4, indicating that task-oriented constraints have a substantial impact on detection performance. This work highlights the importance of considering prompt variations in developing robust LLM-generated text detection methods.

## Method Summary
The study uses ChatGPT to generate student essays with original instructions, paraphrased instructions, and instructions with task-oriented constraints. Human-written essays from Koike et al. (2023) serve as the reference dataset. Two detection methods are evaluated: OpenAI's RoBERTa-based GPT-2 Classifier and OUTFOX in-context learning. The authors measure F1-scores and analyze performance variance across different prompt conditions to understand how task-oriented constraints affect detection reliability.

## Key Results
- Detection performance variance is up to 20 times larger when using instructions with task-oriented constraints compared to generating texts multiple times or paraphrasing the instruction
- Standard deviation of F1-score can reach up to 14.4 when task-oriented constraints are applied
- The high instruction-following ability of LLMs fosters large impact of task-oriented constraints on detection performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-oriented constraints in prompts significantly shift the distribution of LLM-generated text, making it harder for detectors to identify them consistently.
- Mechanism: When task-oriented constraints (e.g., "must be free of grammatical errors") are added to prompts, they change the characteristics of the generated text. These constraints push the text distribution further from the human-written essays that detectors are trained on, increasing variance in detection performance.
- Core assumption: The detectors rely on learned patterns that distinguish between human-written and LLM-generated text, and these patterns break down when task-oriented constraints are applied.
- Evidence anchors:
  - [abstract] "We find that detection performance variance is significantly larger when using instructions with these constraints compared to generating texts multiple times or paraphrasing the instruction."
  - [section 4.2] "Our experiment shows that the detection performance variance of the current detector on texts generated by instruction with each task-oriented constraint is up to 20 times larger than the variance caused by generating texts multiple times and paraphrasing the instruction."
- Break condition: If detectors were trained with adversarial examples that included task-oriented constraints, they might not show such high variance.

### Mechanism 2
- Claim: Task-oriented constraints leverage the high instruction-following ability of LLMs to generate text that mimics human writing quality, making detection more challenging.
- Mechanism: LLMs are designed to follow instructions closely. When given task-oriented constraints that improve writing quality (e.g., "must include diverse word choices"), they generate text that more closely resembles human writing, which confuses detectors that look for typical LLM-generation artifacts.
- Core assumption: The detectors are trained to detect patterns that emerge from unconstrained or standard LLM generation, not from constrained generation that mimics human writing quality.
- Evidence anchors:
  - [abstract] "Our analysis indicates that the high instruction-following ability of LLMs fosters the large impact of such constraints on detection performance."
  - [section 5] "Our finding suggests both-sided facts that malicious users can cheat on current detectors by simply adding an instruction to their prompts, and defenders can also create prompts that generate texts that are less likely to be detected."
- Break condition: If the detectors were designed to detect quality-based features rather than generation artifacts, this mechanism might not apply.

### Mechanism 3
- Claim: The variance in detection performance is amplified because different task-oriented constraints push the generated text distribution in different directions, confusing the detectors.
- Mechanism: Each task-oriented constraint (e.g., "must be persuasive" vs. "must be grammatically correct") changes the text in a unique way. Detectors that are optimized for one type of deviation from human writing may fail when the deviation pattern changes with different constraints.
- Core assumption: The detectors have learned to recognize specific patterns in LLM-generated text, and these patterns change significantly with different task-oriented constraints.
- Evidence anchors:
  - [abstract] "the standard deviation of F1-score can be up to 14.4, indicating that task-oriented constraints have a substantial impact on detection performance."
  - [section 4.2] "Throughout all settings, the detection performance variance on texts by instruction with each task-oriented constraint is significantly larger than the other two variances: up to about 20 times larger in performance of RoBERT-large."
- Break condition: If detectors were trained on a diverse set of constrained generations, they might show less variance across different constraints.

## Foundational Learning

- Concept: Detection performance variance
  - Why needed here: Understanding variance helps quantify how reliable detectors are under different prompt conditions. High variance indicates unreliable detection.
  - Quick check question: If a detector has F1-score variance of 0.1 across different prompt conditions, is this considered high or low variance?

- Concept: Task-oriented constraints
  - Why needed here: These are the key variables being tested - they represent real-world prompt modifications that users might make.
  - Quick check question: What distinguishes a task-oriented constraint from a detection-evasion attempt in the context of this paper?

- Concept: In-context learning
  - Why needed here: This is one of the detector methods being evaluated, and understanding how it works is crucial for interpreting results.
  - Quick check question: How does in-context learning differ from traditional supervised classification in the context of text detection?

## Architecture Onboarding

- Component map: Essay generation module (ChatGPT with temperature 1.3) -> Task-oriented constraint generator (manual creation based on Ke and Ng 2019 factors) -> Detection algorithms (RoBERTa-based classifier and OUTFOX in-context learning) -> Performance evaluation module (F1-score calculation and variance analysis)

- Critical path: Prompt → Constrained Generation → Detection → Performance Evaluation → Variance Analysis

- Design tradeoffs: Using manual creation of constraints ensures relevance but limits scalability. The choice of ChatGPT as generator affects generalizability to other LLMs.

- Failure signatures: High variance in F1-score indicates unreliable detection. If variance is similar across constraint and non-constraint conditions, the hypothesis fails.

- First 3 experiments:
  1. Generate essays with original instruction (no constraints) multiple times and measure detection variance.
  2. Generate essays with paraphrased versions of original instruction and measure detection variance.
  3. Generate essays with each individual task-oriented constraint and measure detection variance, comparing to experiments 1 and 2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different task-oriented constraints impact the detection performance of various LLM-generated text detectors?
- Basis in paper: [explicit] The paper discusses that task-oriented constraints significantly affect the detection performance of LLM-generated text detectors, but does not provide a detailed comparison across different types of constraints.
- Why unresolved: The paper focuses on a specific domain (student essay writing) and a set of constraints based on essay quality factors, but does not explore how these results generalize to other domains or types of constraints.
- What evidence would resolve it: Conducting experiments with a wider range of task-oriented constraints across different domains and comparing the detection performance of various LLM-generated text detectors.

### Open Question 2
- Question: What are the underlying reasons for the variance in detection performance caused by task-oriented constraints in instructions?
- Basis in paper: [inferred] The paper shows that task-oriented constraints lead to a larger variance in detection performance compared to other factors like generating texts multiple times or paraphrasing the instruction, suggesting that there are underlying reasons for this variance.
- Why unresolved: The paper does not provide a detailed analysis of the linguistic or statistical properties of the generated texts that might explain the variance in detection performance.
- What evidence would resolve it: Analyzing the generated texts to identify patterns or features that correlate with the variance in detection performance, and conducting experiments to test the impact of modifying these features on detection accuracy.

### Open Question 3
- Question: How can robust LLM-generated text detectors be developed to handle distributional shifts caused by task-oriented constraints in instructions?
- Basis in paper: [explicit] The paper concludes that there is a need for further research on developing robust detection techniques against distributional shifts caused by task-oriented constraints in instructions.
- Why unresolved: The paper does not provide specific recommendations or approaches for developing such robust detectors, leaving this as an open challenge.
- What evidence would resolve it: Proposing and testing new detection methods or training strategies that are specifically designed to handle the distributional shifts caused by task-oriented constraints, and evaluating their performance in comparison to existing methods.

## Limitations

- The manual creation of task-oriented constraints introduces potential subjectivity and may not capture all relevant prompt modifications
- The experiment uses only one LLM (ChatGPT), limiting generalizability to other language models
- The analysis focuses specifically on student essay writing, which may not extend to other text domains or writing styles

## Confidence

- **High Confidence** - The core finding that task-oriented constraints significantly increase detection performance variance is well-supported by quantitative results showing up to 14.4 F1-score standard deviation and up to 20x increase in variance compared to baseline conditions.
- **Medium Confidence** - The explanation that LLMs' high instruction-following ability drives this effect is plausible but could benefit from more detailed linguistic analysis of how constraints alter text characteristics.
- **Low Confidence** - The claim that malicious users can easily evade detection by adding constraints is presented but not empirically tested with actual evasion attempts against the detectors.

## Next Checks

1. **Cross-model validation**: Repeat the experiment using multiple LLMs (e.g., GPT-4, Claude, Llama) to assess whether the variance effects generalize beyond ChatGPT.

2. **Feature analysis**: Conduct a detailed linguistic feature analysis to identify which specific text characteristics change with different task-oriented constraints and correlate these with detector performance drops.

3. **Adversarial testing**: Design controlled experiments where constraints are deliberately added to prompt texts to test whether they can successfully evade detection by the studied models, measuring actual evasion success rates.