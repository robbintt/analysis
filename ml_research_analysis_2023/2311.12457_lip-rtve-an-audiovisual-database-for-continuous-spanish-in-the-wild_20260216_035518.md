---
ver: rpa2
title: 'LIP-RTVE: An Audiovisual Database for Continuous Spanish in the Wild'
arxiv_id: '2311.12457'
source_url: https://arxiv.org/abs/2311.12457
tags:
- speech
- recognition
- database
- were
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LIP-RTVE, a semi-automatically annotated audiovisual
  database for unconstrained natural Spanish speech. It contains approximately 13
  hours of data extracted from Spanish television news programs.
---

# LIP-RTVE: An Audiovisual Database for Continuous Spanish in the Wild

## Quick Facts
- arXiv ID: 2311.12457
- Source URL: https://arxiv.org/abs/2311.12457
- Reference count: 0
- One-line primary result: Semi-automatically annotated audiovisual database for Spanish VSR achieving 8.0% audio-only WER in speaker-dependent scenarios

## Executive Summary
This paper introduces LIP-RTVE, a semi-automatically annotated audiovisual database containing approximately 13 hours of unconstrained natural Spanish speech extracted from television news programs. The database includes three types of Regions of Interest (ROIs) to support both traditional HMM-based and end-to-end deep learning approaches for Visual Speech Recognition (VSR). Baseline experiments using GMM-HMMs show strong audio-only performance (8.0% WER speaker-dependent) but challenging visual-only results (69.4% WER speaker-independent), highlighting the difficulties of VSR in natural settings.

## Method Summary
The LIP-RTVE database was created through semi-automatic annotation of Spanish television news videos, extracting segments with single speakers and no voice-over. Three ROI sizes were defined to accommodate different recognition approaches. Audio was preprocessed to 16kHz mono WAV, and visual features were extracted using eigenlips computed from 32Ã—16 pixel fitMouth ROIs. GMM-HMM models were trained following a WSJ recipe structure with multiple training stages, and performance was evaluated using WER with bootstrap confidence intervals across speaker-dependent and speaker-independent scenarios.

## Key Results
- Audio-only experiments achieved 8.0% WER in speaker-dependent scenarios and 15.3% WER in speaker-independent scenarios
- Visual-only experiments struggled significantly, with WER reaching 69.4% in speaker-independent settings
- Audiovisual fusion provided modest improvements over audio-only, suggesting potential for better fusion strategies
- The database contains 13 hours of data from 323 speakers with 9308 unique words and 654,368 phonemes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semi-automated annotation improves data quality while scaling up to 13 hours
- Mechanism: Manual supervision filters samples with single speaker and no voice-over, reducing noise without requiring full manual transcription
- Core assumption: Face detection and ROI extraction can reliably isolate the target speaker in broadcast video
- Evidence anchors: Face detection used to obtain extracts where at least one face appeared, manually supervised process
- Break condition: High false-positive rate in face detection or multi-speaker confusion would reduce usable data and require more manual review

### Mechanism 2
- Claim: Multi-size ROIs enable compatibility with both traditional HMM and end-to-end deep learning approaches
- Mechanism: Three ROI sizes (fitMouth, wideMouth, faceROI) allow visual feature extraction at different granularities
- Core assumption: Smaller ROIs capture sufficient visual speech information for traditional ASR modules
- Evidence anchors: Different ROI sizes defined to accommodate differences between end-to-end DL structures and traditional HMM approaches
- Break condition: If visual speech information is lost at smaller ROI sizes, HMM performance would degrade significantly

### Mechanism 3
- Claim: Audio-only HMM baseline establishes performance ceiling for visual-only and audiovisual systems
- Mechanism: Training HMMs on MFCC features provides reference point for evaluating visual-only and multimodal recognition accuracy
- Core assumption: Audio-only recognition performance is achievable and measurable on this dataset
- Evidence anchors: Audio-only approach considered as lower bound for proposed task with reported WER values
- Break condition: If audio-only recognition is significantly worse than expected, it would undermine the validity of using it as a baseline

## Foundational Learning

- Concept: Hidden Markov Models (HMMs) in speech recognition
  - Why needed here: The baseline experiments use GMM-HMMs, so understanding HMM theory is critical for interpreting results
  - Quick check question: What are the three main components of an HMM used in speech recognition?

- Concept: Visual speech feature extraction (eigenlips)
  - Why needed here: The visual-only experiments use PCA-based eigenlips, requiring knowledge of appearance-based feature extraction
  - Quick check question: How does PCA reduce dimensionality of mouth region images for visual speech recognition?

- Concept: Audio-visual feature fusion methods
  - Why needed here: The paper mentions exploring different fusion strategies, so understanding how to combine modalities is relevant
  - Quick check question: What are the main differences between feature-level and decision-level fusion in audiovisual speech recognition?

## Architecture Onboarding

- Component map: MP4 extraction -> face detection -> ROI extraction -> audio preprocessing -> transcription -> feature extraction -> GMM-HMM training -> WER evaluation
- Critical path: Extract video segments with single speaker -> Detect and extract ROIs -> Preprocess audio to 16kHz mono WAV -> Annotate transcriptions -> Train GMM-HMM system -> Evaluate WER on test set
- Design tradeoffs: ROI size vs. computational efficiency; sample rate differences (visual 25fps vs. audio 100fps); semi-automated vs. fully manual annotation
- Failure signatures: High WER in visual-only experiments indicates poor visual feature quality or insufficient training data; low improvement in audiovisual over audio-only suggests ineffective fusion strategy
- First 3 experiments: 1) Reproduce audio-only baseline with provided GMM-HMM configuration to establish reference WER; 2) Train visual-only system using eigenlips and compare performance across ROI sizes; 3) Implement simple feature concatenation fusion and evaluate audiovisual performance against audio-only baseline

## Open Questions the Paper Calls Out

- Question: How does the performance of end-to-end deep learning models compare to traditional GMM-HMM models on the LIP-RTVE database, particularly in speaker-independent scenarios?
  - Basis in paper: The paper suggests exploring end-to-end architectures as future work, noting that these models could potentially select relevant features from wider ROIs
  - Why unresolved: The current baseline results only use traditional GMM-HMM models, and the paper explicitly states that end-to-end approaches have not yet been tested
  - What evidence would resolve it: Training and evaluating end-to-end deep learning models on the LIP-RTVE database and comparing their performance metrics to the GMM-HMM results

- Question: What is the impact of different Region of Interest (ROI) sizes on the performance of visual speech recognition systems for Spanish?
  - Basis in paper: The paper defines three ROI sizes and discusses their potential benefits for different modeling approaches, but does not provide experimental results comparing their effectiveness
  - Why unresolved: The baseline experiments only use the fitMouth ROI, and the paper suggests that wider ROIs might provide more useful information for end-to-end approaches, but this has not been tested
  - What evidence would resolve it: Conducting experiments using all three ROI sizes as input to both traditional and deep learning models, and analyzing the impact on recognition accuracy

- Question: How does the LIP-RTVE database perform when used for multilingual audiovisual speech recognition tasks, particularly for languages with limited resources?
  - Basis in paper: The paper highlights the lack of in-the-wild Spanish resources and suggests that the LIP-RTVE database could support research in audiovisual speech recognition for Spanish
  - Why unresolved: The current experiments and evaluations are focused solely on Spanish, and the paper does not explore the database's potential for multilingual tasks
  - What evidence would resolve it: Extending the LIP-RTVE database annotations to include other languages and evaluating the performance of multilingual audiovisual speech recognition models

## Limitations
- Visual-only recognition performance is significantly worse than audio-only, with WER up to 69.4% in speaker-independent scenarios
- The study uses traditional GMM-HMM approaches rather than modern deep learning methods, limiting generalizability to current state-of-the-art systems
- Face detection and ROI extraction accuracy is not quantitatively validated, creating uncertainty about the quality of visual inputs

## Confidence
- High confidence: Audio-only baseline performance metrics, database statistics, and methodology description for data collection and preprocessing
- Medium confidence: Visual feature extraction methodology, HMM training procedure details, and fusion approach descriptions
- Low confidence: Claims about visual speech recognition performance in challenging scenarios, effectiveness of multi-size ROIs, and comparative advantages of the proposed database

## Next Checks
1. Measure false positive and false negative rates for face detection in broadcast video to assess the reliability of the semi-automatic annotation process
2. Compare eigenlips feature quality and reconstruction accuracy against ground truth mouth region images to verify that visual information is preserved during PCA dimensionality reduction
3. Replicate the audio-only GMM-HMM experiments using the described WSJ recipe configuration to verify that reported WER values are achievable and representative of the dataset quality