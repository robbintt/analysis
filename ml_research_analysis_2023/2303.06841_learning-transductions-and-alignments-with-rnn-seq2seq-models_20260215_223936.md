---
ver: rpa2
title: Learning Transductions and Alignments with RNN Seq2seq Models
arxiv_id: '2303.06841'
source_url: https://arxiv.org/abs/2303.06841
tags:
- input
- learning
- tasks
- seq2seq
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper examines how well Recurrent-Neural-Network sequence
  to sequence (RNN seq2seq) models learn four transduction tasks: identity, reversal,
  total reduplication, and input-specified reduplication. These tasks are traditionally
  studied under finite state transducers and attributed with increasing complexity.'
---

# Learning Transductions and Alignments with RNN Seq2seq Models

## Quick Facts
- arXiv ID: 2303.06841
- Source URL: https://arxiv.org/abs/2303.06841
- Reference count: 21
- Primary result: RNN seq2seq models learn training data mappings rather than underlying functions, with attention improving in-distribution learning but not out-of-distribution generalization

## Executive Summary
This paper examines how Recurrent-Neural-Network sequence to sequence (RNN seq2seq) models learn four transduction tasks: identity, reversal, total reduplication, and input-specified reduplication. These tasks are traditionally studied under finite state transducers and attributed with increasing complexity. The study focuses on in-distribution and out-of-distribution generalization abilities, using identical training and evaluation conditions for all models. RNN seq2seq models, with or without attention, tend to learn a mapping that fits the training data rather than the underlying functions. Attention improves in-distribution learning and out-of-distribution generalization, but does not overcome the generalization limitation.

## Method Summary
The paper evaluates RNN seq2seq models (SRNN, LSTM, GRU) with and without attention on four transduction tasks using synthetic datasets. Training uses teacher forcing, Adam optimizer, gradient clipping, and Xavier initialization. Models are trained up to 500 epochs with early stopping based on dev set performance. The tasks include identity (output equals input), reversal (output is reversed input), total reduplication (output is input repeated twice), and input-specified reduplication (output is input repeated based on instruction symbols). The evaluation uses identical training and test conditions across all models, with in-distribution lengths 6-15 and out-of-distribution lengths 1-5 and 16-30.

## Key Results
- RNN seq2seq models learn training data mappings rather than underlying functions
- Attention improves in-distribution learning efficiency but does not solve out-of-distribution generalization limitations
- Complexity hierarchy for attention-less models: input-specified reduplication > total reduplication > identity > reversal
- Attention-augmented models show improved learning but still exhibit generalization limitations

## Why This Works (Mechanism)

### Mechanism 1
RNN seq2seq models learn alignments between input and target sequences rather than underlying functions. The decoder must store information about all input symbols from the encoder and retrieve output symbols in correct alignments. Learning task complexity depends on memory requirements for storing and retrieving symbol alignments. If the model can perfectly generalize to out-of-distribution examples, this mechanism is invalid.

### Mechanism 2
Attention improves in-distribution learning efficiency by providing direct access to encoder hidden states. Attention acts as "weighted skip connection" allowing decoder to access all encoder hidden states at any decoding time step. RNNs suffer from long-term dependency issues that attention can mitigate. If attention-less models achieve similar performance with sufficient training data, this mechanism is invalid.

### Mechanism 3
Task complexity hierarchy differs between attention-less and attention-augmented models. Without attention, task complexity follows language hierarchy (CSL > CFL), but attention changes the learning dynamics. Attention-less models struggle with longer target sequences due to memory limitations. If attention-less models show equal performance across all tasks, this mechanism is invalid.

## Foundational Learning

- **Finite State Transducers (FSTs) and complexity hierarchy**: Needed because the paper compares RNN seq2seq learning with traditional FST characterizations. Quick check: Can you explain why reversal and total reduplication require 2-way FSTs while identity only needs 1-way?

- **Formal language hierarchy (Chomsky hierarchy)**: Needed because the paper re-frames transduction tasks as language recognition to explain task complexity. Quick check: What is the difference between context-free and context-sensitive languages, and how does this relate to palindrome vs copy languages?

- **Gradient-based RNN training limitations**: Needed because understanding why RNNs struggle with long-term dependencies is crucial for interpreting results. Quick check: What are the vanishing and exploding gradient problems in RNNs, and how do LSTM/GRU gates address them?

## Architecture Onboarding

- **Component map**: Input embedding → Encoder RNN → (Attention computation) → Decoder RNN → Output layer
- **Critical path**: Input embedding → Encoder hidden states → (Attention computation) → Decoder hidden states → Output probability distribution
- **Design tradeoffs**: Attention provides efficiency but doesn't solve generalization; LSTM/GRU provide better long-term memory but add complexity
- **Failure signatures**: Large train-test variance indicates insufficient training data; poor out-of-distribution performance suggests learning surface patterns rather than functions
- **First 3 experiments**:
  1. Train identity task with and without attention using small dataset to observe convergence speed
  2. Compare LSTM vs SRNN on reversal task with fixed dataset size to measure long-term dependency handling
  3. Test out-of-distribution generalization by training on lengths 6-15 and evaluating on lengths 1-5 and 16-30

## Open Questions the Paper Calls Out

### Open Question 1
What is the fundamental reason RNN seq2seq models fail to generalize beyond their training data distributions? The paper demonstrates this limitation through experiments but does not provide a theoretical explanation for why this generalization failure occurs at a fundamental level. A theoretical framework explaining the representational and learning limitations of RNN seq2seq models would resolve this question.

### Open Question 2
How do bidirectional and multi-layered RNN seq2seq architectures affect the learning complexity hierarchy established in the paper? The paper only uses single-layered, unidirectional RNNs, so the impact of more complex architectures on learning the four transduction tasks remains unknown. Comparative experiments using bidirectional and multi-layered RNN seq2seq models would resolve this question.

### Open Question 3
What is the minimum sample complexity required for attention-less RNN seq2seq models to learn each of the four transduction tasks effectively? While the paper demonstrates the need for larger sample sizes, it does not systematically determine the exact threshold of training data required for successful learning. A series of controlled experiments varying the training data size for each task would resolve this question.

## Limitations
- Reliance on synthetic data without natural language examples may not reflect real-world RNN seq2seq behavior
- Generalization tests only cover length variations, not other distribution shifts like vocabulary changes or noise corruption
- Complexity hierarchy based on a small set of four tasks may not generalize to other transductions
- Attention mechanism implementation details are not fully specified, affecting reproducibility

## Confidence
**High Confidence**: RNN seq2seq models learn training data mappings rather than underlying functions; attention improves in-distribution learning efficiency; the established complexity hierarchy for attention-less models.

**Medium Confidence**: Attention does not solve out-of-distribution generalization limitations; task complexity for RNN seq2seq learning aligns with formal language hierarchy rather than string transduction complexity.

**Low Confidence**: The specific ranking of attention-augmented model performance across all tasks; whether the observed limitations would persist with larger models or different architectures.

## Next Checks
1. **Distribution Shift Validation**: Test the trained models on datasets with vocabulary expansions (e.g., add digits or uppercase letters) and noisy inputs (character substitutions/deletions) to assess robustness beyond length-based generalization.

2. **Attention Mechanism Analysis**: Compare different attention variants (additive, multiplicative, scaled dot-product) and analyze their impact on both learning efficiency and generalization, particularly for the most complex task (input-specified reduplication).

3. **Architectural Scaling Study**: Train larger models (increased hidden size, deeper networks) on the same tasks to determine if the generalization limitations are inherent to the RNN seq2seq architecture or could be mitigated by increased model capacity.