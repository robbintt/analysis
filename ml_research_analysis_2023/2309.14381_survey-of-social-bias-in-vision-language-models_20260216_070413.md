---
ver: rpa2
title: Survey of Social Bias in Vision-Language Models
arxiv_id: '2309.14381'
source_url: https://arxiv.org/abs/2309.14381
tags:
- bias
- gender
- social
- conference
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey paper provides a comprehensive overview of social bias
  in vision-language (VL) models, addressing the growing concern of fairness in AI
  systems. It categorizes bias measurement techniques into intrinsic (measuring bias
  in model embeddings) and extrinsic (measuring bias through downstream tasks) approaches.
---

# Survey of Social Bias in Vision-Language Models

## Quick Facts
- arXiv ID: 2309.14381
- Source URL: https://arxiv.org/abs/2309.14381
- Reference count: 40
- Key outcome: This survey paper provides a comprehensive overview of social bias in vision-language (VL) models, categorizing bias measurement techniques and mitigation methods while identifying open challenges in multimodal bias research.

## Executive Summary
This survey comprehensively examines social bias in vision-language models, which combine image and text processing capabilities. The paper identifies that VL models can inadvertently capture and reinforce social biases present in their training datasets, similar to unimodal models but with added complexity due to multimodal interactions. It categorizes bias measurement into intrinsic approaches (measuring bias in model embeddings) and extrinsic approaches (measuring bias through downstream tasks), while highlighting unique challenges in VL bias research such as differences in expressive capabilities between text and image modalities. The survey also surveys various bias mitigation methods across pre-processing, in-processing, and post-processing techniques, and introduces a categorization of VL model architectures and their impact on bias propagation.

## Method Summary
The paper synthesizes existing literature on social bias in VL models through systematic categorization of measurement techniques, mitigation approaches, and architectural paradigms. It reviews intrinsic bias metrics that measure bias in learned embedding spaces and extrinsic metrics that measure bias through downstream task performance disparities. The survey analyzes bias in different VL model architectures (unified encoder, dual-stream, encoder-decoder) and evaluates pre-trained models like CLIP, ViLBERT, and text-to-image models such as DALL-E and Stable Diffusion. The methodology involves examining bias evaluation datasets, metrics, and mitigation techniques across both NLP and CV domains, while identifying gaps and proposing future research directions.

## Key Results
- VL models inherit social biases from unimodal pre-training stages through transformer-based learning of statistical patterns in training data
- Bias measurement in VL models requires both intrinsic approaches (measuring bias in embeddings) and extrinsic approaches (measuring bias through task performance)
- VL bias is more complex than unimodal bias due to modality differences, where text can express invisible attributes while images only show visible attributes
- The survey identifies challenges in creating counterfactual image pairs and handling biased distributions in VL datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VL models inherit social bias from unimodal pre-training stages
- Mechanism: Transformer-based pre-trained models learn statistical patterns from training data, including social biases present in text and images, which propagate into multimodal representations
- Core assumption: Bias present in unimodal training data transfers to VL models through pre-training
- Evidence anchors:
  - [abstract] "researchers have discovered that these models can inadvertently capture and reinforce social biases present in their training datasets"
  - [section] "Social bias is defined as the disproportionate weight in favor of or against one thing, person, or group compared with another, usually in a way considered to be unfair"
  - [corpus] FMR scores indicate strong topical overlap with bias-related papers (avg 0.378)
- Break condition: If VL models are trained on carefully curated, bias-mitigated datasets from the outset

### Mechanism 2
- Claim: Bias measurement differs between intrinsic and extrinsic approaches
- Mechanism: Intrinsic metrics measure bias in learned embeddings before downstream tasks, while extrinsic metrics measure bias through task performance disparities across demographic groups
- Core assumption: Bias manifests differently at embedding level versus downstream task level
- Evidence anchors:
  - [section] "Intrinsic bias metric measures the bias that exists in learned embedding/feature space of pre-trained models" vs "Extrinsic bias metrics measure bias through extrinsic downstream tasks"
  - [section] "Intrinsic bias can be understood as any social bias captured within word-embedding or language model itself before it is applied to any downstream tasks"
  - [corpus] Limited citations suggest this is an emerging area with room for methodological development
- Break condition: If bias in embeddings always perfectly correlates with task-level bias

### Mechanism 3
- Claim: VL bias is more complex than unimodal bias due to modality differences
- Mechanism: Text can express invisible attributes (personality, intelligence) while images only show visible attributes; controlling for sensitive attributes is harder in images than text
- Core assumption: Different modalities have different expressive capabilities that affect bias propagation
- Evidence anchors:
  - [section] "Text modality is capable of expressing both visible (e.g., shape, color, texture) and invisible attributes (e.g., personality, intelligence, ability), whereas image modality is limited to expressing only visible attributes"
  - [section] "In CV, obtaining counterfactual image pairs is not easy, because modifying just the gender in the image is non-trivial"
  - [corpus] Strong FMR correlation with bias surveys suggests this complexity is recognized in related work
- Break condition: If modality-specific bias mitigation techniques prove equally effective across all combinations

## Foundational Learning

- Concept: Fairness criteria (Demographic Parity, Equality of Odds, Equality of Opportunity)
  - Why needed here: Provides mathematical framework for defining and measuring bias in VL models
  - Quick check question: What is the key difference between Demographic Parity and Equality of Opportunity?

- Concept: Protected demographic attributes (gender, race, age, etc.)
  - Why needed here: Defines which social groups need protection from algorithmic discrimination
  - Quick check question: Why are gender and race the most commonly studied protected attributes in VL bias research?

- Concept: Pre-processing, in-processing, and post-processing mitigation methods
  - Why needed here: Categorizes bias mitigation approaches that can be applied at different stages of model development
  - Quick check question: Which mitigation method would you use if you only have access to a pre-trained black-box model?

## Architecture Onboarding

- Component map:
  - Data layer: Image-text pairs, often from web sources
  - Feature extraction: Vision encoder (CNN/ViT) and language encoder (transformer)
  - Fusion layer: Cross-modal attention or joint embedding space
  - Task heads: Classification, retrieval, generation outputs
  - Bias measurement modules: Intrinsic and extrinsic evaluation components

- Critical path:
  1. Data collection and preprocessing
  2. Unimodal pre-training (vision and language separately)
  3. Multimodal fusion and VL pre-training
  4. Downstream task fine-tuning
  5. Bias measurement and mitigation

- Design tradeoffs:
  - Early fusion vs late fusion affects bias propagation patterns
  - Unified vs dual-stream architectures impact bias measurement granularity
  - Pre-training scale vs bias control (larger models may amplify biases)

- Failure signatures:
  - High intrinsic bias scores despite low extrinsic bias
  - Disproportionate error rates across demographic groups
  - Stereotypical associations in text-to-image generation
  - Demographic skew in image retrieval results

- First 3 experiments:
  1. Measure WEAT scores for text embeddings before and after VL pre-training
  2. Evaluate gender ratio differences in image retrieval for neutral queries
  3. Test zero-shot classification accuracy gaps across protected attribute groups

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different VLP architectural designs (unified, dual-stream, encoder-decoder) impact the level and types of social bias in VL models?
- Basis in paper: [explicit] The paper mentions that analyzing the impact of VLP architectural design on social bias would be insightful.
- Why unresolved: The survey notes this as a future direction without providing comparative analysis across architectures.
- What evidence would resolve it: Systematic bias evaluation across VL models with different architectures (ViLBERT, CLIP, BLIP, etc.) using consistent metrics.

### Open Question 2
- Question: How can we disentangle the impact of prompt template choices from the underlying bias in VL models when measuring extrinsic bias?
- Basis in paper: [explicit] The paper identifies this as a challenge, noting that prompt template choice significantly affects model behavior.
- Why unresolved: Current extrinsic bias measurements don't separate template effects from model bias.
- What evidence would resolve it: Controlled experiments varying only prompt templates while keeping model architecture constant.

### Open Question 3
- Question: How does bias in unimodal spaces (text and image encoders separately) impact the overall bias level in multimodal VL models?
- Basis in paper: [explicit] The paper suggests evaluating bias separately in unimodal spaces of VLP models like CLIP.
- Why unresolved: Most current metrics evaluate cross-modality bias without examining unimodal components.
- What evidence would resolve it: Comparative analysis of bias levels in separate text and image encoders versus joint multimodal embeddings.

## Limitations
- The survey lacks empirical validation of proposed bias mitigation techniques, presenting them as theoretical approaches rather than tested solutions
- Current bias evaluation metrics may not fully capture the complexity of multimodal bias, particularly for non-binary gender identities and intersectional attributes
- The effectiveness of bias mitigation techniques across different VL architectures remains theoretical rather than demonstrated through systematic experiments

## Confidence

- High confidence: The categorization of intrinsic vs extrinsic bias measurement approaches is well-established in the literature
- Medium confidence: The architectural categorization of VL models and their bias implications
- Medium confidence: The overview of mitigation techniques, though their effectiveness varies significantly by context

## Next Checks

1. Conduct empirical comparison of bias levels across unified, dual-stream, and encoder-decoder VL architectures using standardized metrics
2. Test the effectiveness of prompt-based bias mitigation on multiple VL models with diverse prompt templates to establish robustness
3. Evaluate bias measurement consistency across different dataset compositions to assess metric reliability under varying demographic distributions