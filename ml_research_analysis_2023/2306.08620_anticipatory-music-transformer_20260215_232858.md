---
ver: rpa2
title: Anticipatory Music Transformer
arxiv_id: '2306.08620'
source_url: https://arxiv.org/abs/2306.08620
tags:
- music
- time
- events
- sequence
- control
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces anticipatory modeling, a method for interleaving
  events and controls in temporal point processes so that controls appear as stopping
  times close to relevant events. Applied to symbolic music generation, it enables
  infilling tasks where user-provided events guide completion of musical sequences.
---

# Anticipatory Music Transformer

## Quick Facts
- **arXiv ID**: 2306.08620
- **Source URL**: https://arxiv.org/abs/2306.08620
- **Reference count**: 40
- **Primary result**: Anticipatory modeling enables controllable symbolic music generation without sacrificing unconditional modeling quality

## Executive Summary
This paper introduces anticipatory modeling, a method for interleaving events and controls in temporal point processes so that controls appear as stopping times close to relevant events. Applied to symbolic music generation, it enables infilling tasks where user-provided events guide completion of musical sequences. Trained on Lakh MIDI, anticipatory models match autoregressive performance on prompted continuation while unlocking accompaniment capabilities. Human evaluators found anticipatory accompaniments nearly as musical as human-composed ones, with only a slight preference for human music that was not statistically significant.

## Method Summary
The approach trains causal transformers on augmented datasets where control tokens (span, instrument, random anticipation) are interleaved with musical events using arrival-time tokenization. This allows controls to appear at stopping times near relevant events, enabling bidirectional conditioning without breaking autoregressive factorization. The model uses standard cross-entropy loss and Levanter training with nucleus sampling at inference.

## Key Results
- Anticipatory models match autoregressive performance on prompted continuation tasks
- Human evaluators found anticipatory accompaniments nearly as musical as human-composed music (difference not statistically significant)
- The approach unlocks accompaniment capabilities while maintaining unconditional generation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Anticipation allows controls to be injected as stopping times close to relevant events, enabling bidirectional context without breaking autoregressive factorization.
- Mechanism: By reordering the sequence so that a control on time `t` appears at a stopping time near events at time `t-δ`, the model can condition on both past events and future controls in its context window.
- Core assumption: The index immediately before a control in the interleaved sequence must be a stopping time for autoregressive inference to be tractable.
- Evidence anchors:
  - [abstract]: "We achieve this by interleaving sequences of events and controls, such that controls appear following stopping times in the event sequence."
  - [section]: Definition 3.1 formalizes the interleaving rule and proves that `τuk-1` is a stopping time.
  - [corpus]: Weak - corpus lacks direct citations but neighboring papers reference similar interleaving strategies.
- Break condition: If `δ` is too small, controls appear too late to influence nearby events; if too large, locality breaks and boundary effects worsen.

### Mechanism 2
- Claim: Arrival-time tokenization enables reordering without changing semantics, which is essential for anticipation.
- Mechanism: Events are tokenized as triplets (time, duration, note). Reordering these triplets by time preserves the original sequence meaning, allowing anticipation to interleave controls at stopping times.
- Core assumption: Triplets are context-free, meaning their semantic content doesn't depend on their position in the sequence.
- Evidence anchors:
  - [section]: "Crucially, the triplets... are context-free: if we re-order the triplets, the semantics of the sequence do not change."
  - [section]: Contrasts arrival-time with interarrival-time encodings that are context-sensitive.
  - [corpus]: Moderate - corpus neighbors cite similar tokenization strategies but don't directly validate context-free property.
- Break condition: If tokenization loses temporal ordering (e.g., non-monotonic times), the sequence becomes ambiguous and reordering invalid.

### Mechanism 3
- Claim: Anticipation improves infilling by training on augmented datasets with diverse control patterns, effectively regularizing the model.
- Mechanism: Controls are sampled from a mixture distribution (span, instrument, random) and injected into training sequences. This forces the model to learn conditional generation across varied scenarios.
- Core assumption: The learned conditionals approximate the true joint distribution well enough that different anticipation patterns yield compatible generations.
- Evidence anchors:
  - [section]: "We rely on the model's ability to well-approximate the underlying distribution over sequences e1:N to ensure some degree of compatibility between these families of learned distributions."
  - [section]: Appendix C describes the control prior used for augmentation.
  - [corpus]: Weak - no direct evidence in corpus that augmentation improves regularization for music models.
- Break condition: If the control prior is too narrow, the model won't generalize to unseen control patterns at inference.

## Foundational Learning

- Concept: Temporal point processes as sequences of sparse events in continuous time.
  - Why needed here: The entire model is built on modeling such processes (events and controls) as sequences.
  - Quick check question: What distinguishes a temporal point process from a regular time series?

- Concept: Stopping times in stochastic processes.
  - Why needed here: Anticipation requires that control insertion indices be stopping times for tractable autoregressive sampling.
  - Quick check question: Why can't we just insert controls wherever they occur in time?

- Concept: Autoregressive factorization and causal masking.
  - Why needed here: The model is a causal transformer that predicts the next token given previous tokens only.
  - Quick check question: How does causal masking enforce autoregressive generation in a transformer?

## Architecture Onboarding

- Component map: Input tokens -> Causal transformer with anticipation interval δ -> Generated interleaved sequence -> Postprocessing to strip controls
- Critical path:
  1. Preprocess Lakh MIDI into arrival-time tokenized sequences
  2. Augment with control priors (span/instrument/random anticipation)
  3. Train causal transformer on augmented sequences
  4. Sample with anticipation interval δ, postprocess to strip controls
- Design tradeoffs:
  - δ controls lookahead but increases boundary effects; must balance with context length M
  - Arrival-time encoding enables anticipation but is less compact than interarrival-time
  - Augmentation increases data diversity but risks overfitting on synthetic controls
- Failure signatures:
  - Model generates events inconsistent with controls → anticipation interval too short or augmentation prior too narrow
  - Poor unconditional generation → anticipation hurting autoregressive training (but evidence shows this is temporary)
  - Boundary artifacts near sequence ends → δ too large relative to M
- First 3 experiments:
  1. Train small model with δ=0 (no anticipation) vs δ=5, compare test perplexity
  2. Replace arrival-time with interarrival-time encoding, measure impact on anticipation capability
  3. Vary control prior mixture (e.g., 100% random vs 100% span), measure infilling quality via human eval

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal anticipation interval (δ) for different music genres or styles?
- Basis in paper: [inferred] The paper uses δ = 5 seconds as a hyperparameter but acknowledges it was chosen empirically and defers systematic ablation to future work.
- Why unresolved: The paper does not explore how different anticipation intervals affect performance across various music styles or genres.
- What evidence would resolve it: Systematic experiments comparing model performance with different δ values across diverse music datasets representing various genres.

### Open Question 2
- Question: How does the anticipatory model's performance scale with model size beyond the tested configurations?
- Basis in paper: [inferred] The paper tests Small, Medium, and Large models but notes diminishing returns and suggests more data might be needed for larger models.
- Why unresolved: The paper does not explore architectures larger than 780M parameters or training on datasets significantly larger than Lakh MIDI.
- What evidence would resolve it: Training and evaluating much larger models (e.g., 1B+ parameters) on substantially larger datasets.

### Open Question 3
- Question: What are the long-term effects of generative music models on the creative process and artistic expression?
- Basis in paper: [explicit] The paper discusses concerns about labor displacement and changes to the creative economy in Section 7.
- Why unresolved: The paper acknowledges these concerns but cannot predict how widespread adoption will affect creativity and artistic expression over time.
- What evidence would resolve it: Longitudinal studies tracking how artists incorporate generative tools into their workflow and how this affects their creative output and career trajectories.

## Limitations
- The mechanism's generalizability to other domains or control types remains untested
- The arrival-time tokenization assumption is critical but not empirically validated
- The augmentation approach assumes diverse synthetic controls improve regularization, but there's no ablation study

## Confidence

- **High confidence**: The basic autoregressive training procedure and transformer architecture (standard GPT-2 style) are well-established and reproducible.
- **Medium confidence**: The anticipation mechanism works as described for the specific experimental setup, but its robustness across different δ values, control types, and musical domains is uncertain.
- **Medium confidence**: The claim that anticipation "matches autoregressive performance" is supported by perplexity metrics but human evaluation shows a slight (though not statistically significant) preference gap.

## Next Checks

1. **Ablation on anticipation interval**: Train and evaluate models with δ = 0, 2, 5, 10 seconds to determine the optimal balance between lookahead capability and boundary effects. Measure both automatic metrics (perplexity, bps) and qualitative aspects (repetition, coherence).

2. **Cross-domain validation**: Apply the anticipatory modeling approach to a non-musical temporal point process (e.g., financial transactions, social media events) to test whether the mechanism generalizes beyond symbolic music generation.

3. **Control type robustness**: Systematically vary the control prior mixture parameters (span anticipation rate, instrument anticipation probability, random anticipation frequency) and measure impact on both unconditional and conditional generation quality. Identify which control types provide the most benefit versus potential interference.