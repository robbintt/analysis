---
ver: rpa2
title: Advancing Transformer's Capabilities in Commonsense Reasoning
arxiv_id: '2310.06803'
source_url: https://arxiv.org/abs/2310.06803
tags:
- pairwise
- dataset
- accuracy
- debertav3large
- com2sense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper improves commonsense reasoning in language models on
  the Com2Sense benchmark by using knowledge transfer from related datasets, contrastive
  learning on complementary pairs, and model ensemble. The best approach combines
  DeBERTaV3-large with cross-validation, contrastive loss, and ensemble averaging,
  achieving ~15% higher pairwise accuracy and ~8.7% higher standard accuracy than
  prior work.
---

# Advancing Transformer's Capabilities in Commonsense Reasoning

## Quick Facts
- arXiv ID: 2310.06803
- Source URL: https://arxiv.org/abs/2310.06803
- Reference count: 22
- This paper improves commonsense reasoning in language models on the Com2Sense benchmark by using knowledge transfer from related datasets, contrastive learning on complementary pairs, and model ensemble. The best approach combines DeBERTaV3-large with cross-validation, contrastive loss, and ensemble averaging, achieving ~15% higher pairwise accuracy and ~8.7% higher standard accuracy than prior work.

## Executive Summary
This paper presents a comprehensive approach to improving commonsense reasoning in transformer models on the Com2Sense benchmark. The authors combine three complementary techniques: knowledge transfer from related commonsense datasets (particularly SemEval-2020), pairwise contrastive learning to better distinguish complementary statement pairs, and model ensemble methods to reduce same-output pairs. Their best configuration using DeBERTaV3-large with cross-validation, contrastive loss, and ensemble averaging achieves significant improvements over baseline approaches, demonstrating that commonsense reasoning benefits from multi-faceted architectural and training innovations.

## Method Summary
The method combines three key components: (1) knowledge transfer from SemEval-2020 and SQuAD2 datasets through pretraining/fine-tuning, (2) pairwise contrastive loss that pushes apart embeddings of complementary statement pairs using a temperature-scaled similarity function, and (3) model ensemble with rule-based perturbation to resolve same-output pairs. The approach uses DeBERTaV3-large as the base model, applies cross-validation on combined training and development sets, and tunes hyperparameters through systematic experimentation. The contrastive learning objective is implemented using normalized dot products with temperature τ = 0.5, and ensembles combine predictions from diverse model architectures including DeBERTaV3-large, DeBERTaV3-base, and RoBERTa-base.

## Key Results
- Best model achieves ~15% higher pairwise accuracy and ~8.7% higher standard accuracy than prior work on Com2Sense benchmark
- Knowledge transfer from SemEval-2020 provides 0.364% improvement in pairwise accuracy over direct application
- Pairwise contrastive loss with random perturbation improves performance by 16%, with contrastive learning alone contributing 8.77% improvement
- Model ensemble of DeBERTaV3-large, DeBERTaV3-base, and RoBERTa-base outperforms smaller ensembles by 1.07% pairwise accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge transfer from related commonsense datasets (SemEval-2020) improves performance on Com2Sense.
- Mechanism: Pretraining DeBERTaV3-large on SemEval-2020 provides domain-specific commonsense knowledge that transfers to Com2Sense.
- Core assumption: SemEval-2020 and Com2Sense share underlying commonsense reasoning patterns despite different task formats.
- Evidence anchors:
  - "As shown in the following table, the model with transferred knowledge from SemEval-2020 performs better than the best model directly applied to Com2Sense, with a 0.364% improvement on pairwise accuracy."
  - "DeBERTV3large pretrained on SemEval dataset (Figure 3 bottom graph) generally performs better in social domain than physical, worst in temporal; it also improves on data without numeracy."
- Break condition: If transferred knowledge domains don't overlap sufficiently with target task, transfer learning provides no benefit or may introduce negative transfer.

### Mechanism 2
- Claim: Pairwise contrastive loss improves model ability to distinguish complementary commonsense statements.
- Mechanism: Contrastive learning pushes apart embeddings of complementary statement pairs, helping model learn subtle semantic differences.
- Core assumption: Com2Sense's complementary nature (each pair has one true/false statement) makes it suitable for contrastive learning.
- Evidence anchors:
  - "From Table 1 lines 5-8, we observe that in practice contrastive learning together with the Random Perturbation helped to improve test performance by 16%."
  - "After removing the benefits of Random Perturbation, we conclude that Contrastive Learning yields an improvement of 8.77% on average, 2.04% in the worst case."
- Break condition: If dataset doesn't have clear complementary pairs or if contrastive loss hyperparameters are poorly tuned, the mechanism fails to provide benefit.

### Mechanism 3
- Claim: Model ensemble reduces same-output pairs and improves overall accuracy.
- Mechanism: Combining predictions from multiple models with different architectures and knowledge helps resolve cases where single models fail.
- Core assumption: Different model architectures capture complementary aspects of commonsense reasoning.
- Evidence anchors:
  - "From Table 1 lines 6-8, we observe that in practice model ensemble as a post-processing technique helps the model perform better compared to straight-through Random Perturbation, likely because Random Perturbation only has a 50% chance of correctly predicting a pair while models used in the ensemble have a much higher accuracy."
  - "the ensemble among DeBERTaV3large, DeBERTaV3 base, and RoBERTabase models outperforms the ensemble between DeBERTaV3 large and DeBERTaV3 base models by 1.07% pairwise acc. This results supports the common understanding that diversity in model structures is beneficial for the ensemble."
- Break condition: If ensemble models are too similar in architecture or training data, diversity benefit disappears and ensemble may not outperform best individual model.

## Foundational Learning

- Concept: Contrastive learning objective
  - Why needed here: Helps model distinguish between semantically similar but logically opposite statements
  - Quick check question: What is the purpose of the temperature parameter τ in contrastive loss?

- Concept: Cross-validation for model robustness
  - Why needed here: Uses both training and development data to improve generalization
  - Quick check question: How does k-fold cross-validation help prevent overfitting in this context?

- Concept: Model ensemble techniques
  - Why needed here: Combines strengths of different architectures to handle complementary data pairs
  - Quick check question: What is the main advantage of using heterogeneous model architectures in ensemble?

## Architecture Onboarding

- Component map: Knowledge transfer → Cross-validation → Contrastive learning → Ensemble → Evaluation
- Critical path: Knowledge transfer → Cross-validation → Contrastive loss → Ensemble → Evaluation
- Design tradeoffs: Larger models provide better performance but require more compute; ensemble improves accuracy but increases inference latency
- Failure signatures: No improvement from contrastive learning (poorly paired data), ensemble performs worse than single model (insufficient diversity), knowledge transfer provides negative results (domain mismatch)
- First 3 experiments:
  1. Compare DeBERTaV3-large with and without SemEval-2020 pretraining on validation set
  2. Implement and test pairwise contrastive loss with different temperature parameters
  3. Create simple ensemble of DeBERTaV3-large and RoBERTa-base to test diversity benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal temperature parameter τ for the Pairwise Contrastive Loss function in commonsense reasoning tasks?
- Basis in paper: The paper mentions using τ = 0.5 but doesn't explore other values.
- Why unresolved: The paper only tested one value of τ without exploring the sensitivity of the contrastive loss to different temperature settings.
- What evidence would resolve it: Systematic experiments varying τ across a range of values (e.g., 0.1 to 1.0) and comparing the resulting performance on the Com2Sense benchmark.

### Open Question 2
- Question: How does the effectiveness of knowledge transfer vary across different source datasets for commonsense reasoning?
- Basis in paper: The paper tested SemEval-2020 and SQuAD2 datasets, with SemEval showing improvement and SQuAD2 showing degradation.
- Why unresolved: The paper only tested two source datasets without exploring the broader landscape of potential knowledge transfer candidates or analyzing why SQuAD2 performed poorly.
- What evidence would resolve it: Comprehensive experiments testing knowledge transfer from multiple commonsense-related datasets (e.g., CommonsenseQA, SocialIQA, ATOMIC) with detailed analysis of why certain datasets transfer better than others.

### Open Question 3
- Question: What is the relationship between model ensemble diversity and performance gains in commonsense reasoning?
- Basis in paper: The paper found that an 8-model ensemble (combining DeBERTaV3 large, DeBERTaV3 base, and RoBERTa base) outperformed a 5-model ensemble, suggesting diversity helps.
- Why unresolved: The paper didn't systematically investigate the relationship between ensemble diversity metrics (architectural differences, pretraining data, etc.) and performance gains.
- What evidence would resolve it: Controlled experiments varying the degree of architectural and pretraining diversity in ensembles while measuring both performance and diversity metrics to establish correlation.

## Limitations

- Implementation details of Pairwise Contrastive Loss are underspecified, particularly whether it's applied during training or as post-processing
- Ensemble perturbation rules are vaguely described, lacking specific ranking criteria and confidence thresholds
- The paper doesn't systematically explore hyperparameter sensitivity, particularly for temperature parameter in contrastive loss

## Confidence

- High confidence: Knowledge transfer from SemEval-2020 showing consistent improvements (0.364% pairwise accuracy gain)
- Medium confidence: Contrastive learning benefits (8.77% average improvement), though implementation details are unclear
- Medium confidence: Model ensemble effectiveness (1.07% improvement from diverse architectures), but perturbation mechanics are underspecified

## Next Checks

1. Implement and validate contrastive loss: Create a controlled experiment testing contrastive learning with different temperature parameters (0.3, 0.5, 0.7) on a subset of Com2Sense to verify the claimed 8.77% improvement and identify optimal hyperparameters.

2. Test knowledge transfer domain specificity: Evaluate whether SemEval-2020 pretraining specifically benefits social domain statements (as suggested) by comparing performance across social, physical, and temporal domains with and without transfer learning.

3. Analyze ensemble diversity requirements: Systematically vary ensemble composition between DeBERTaV3-large, DeBERTaV3-base, and RoBERTa-base models to determine minimum diversity thresholds needed for performance gains and identify when ensemble degrades accuracy.