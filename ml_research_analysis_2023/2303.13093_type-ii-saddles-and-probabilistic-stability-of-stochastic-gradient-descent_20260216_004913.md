---
ver: rpa2
title: Type-II Saddles and Probabilistic Stability of Stochastic Gradient Descent
arxiv_id: '2303.13093'
source_url: https://arxiv.org/abs/2303.13093
tags:
- divides
- learning
- alt0
- stability
- slash
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the stability of stochastic gradient descent
  (SGD) around saddle points in neural networks. The authors show that saddle points
  can be divided into two types, with Type-II saddles being particularly difficult
  to escape due to vanishing gradient noise.
---

# Type-II Saddles and Probabilistic Stability of Stochastic Gradient Descent

## Quick Facts
- arXiv ID: 2303.13093
- Source URL: https://arxiv.org/abs/2303.13093
- Reference count: 35
- Primary result: Shows SGD can converge to low-rank saddle points when gradient noise is strong and learning rate is large, using probabilistic stability framework based on Lyapunov exponents.

## Executive Summary
This paper addresses the stability of stochastic gradient descent (SGD) around saddle points in neural networks, proposing a new framework based on convergence in probability rather than traditional variance-based measures. The authors identify two types of saddle points, with Type-II saddles being particularly problematic due to vanishing gradient noise that makes them difficult to escape. They introduce Lyapunov exponents as a measure of probabilistic stability and demonstrate that SGD can converge to low-rank saddle points under specific conditions involving noise strength and learning rate. The work is validated through experiments on simple models and ResNet18 on CIFAR-10.

## Method Summary
The authors develop a theoretical framework for analyzing SGD stability using convergence in probability, connecting this to Lyapunov exponents from dynamical systems theory. They classify saddle points into Type-I (gradient noise is bounded away from zero) and Type-II (gradient noise vanishes at the saddle). The stability analysis involves examining the behavior of SGD dynamics as a random matrix product process near Type-II saddles. The framework is tested through synthetic experiments on linear regression and matrix factorization problems, as well as training ResNet18 on CIFAR-10 with varying noise levels and learning rates.

## Key Results
- Type-II saddle points create vanishing gradient noise, making them difficult to escape and leading to convergence in probability rather than expectation
- SGD can converge to low-rank saddle points when gradient noise is sufficiently strong and learning rate is appropriately large
- The stability of SGD can be quantified by the Lyapunov exponent, which is easy to measure in practice
- Phase diagrams reveal distinct behaviors (correct learning, incorrect learning, convergence to low-rank saddles, instability) based on signal-to-noise ratio

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Type-II saddle points create vanishing gradient noise, making them difficult to escape and leading to convergence in probability rather than in expectation.
- Mechanism: At Type-II saddles, the gradient noise vanishes, causing SGD dynamics to be dominated by a random matrix product process. The probabilistic stability is determined by the Lyapunov exponent, which quantifies whether the system converges in probability.
- Core assumption: The gradient noise depends on the parameters (w-dependent noise), not just on data sampling.
- Evidence anchors:
  - [abstract] The authors show that saddle points can be divided into two types, with Type-II saddles being particularly difficult to escape due to vanishing gradient noise.
  - [section] "At the heart of this problem is the stability of SGD because the models trained with SGD stay close to the solution where the dynamics is stable and moves away from unstable solutions."
- Break condition: If the gradient noise is parameter-independent or if the noise does not vanish at the saddle point.

### Mechanism 2
- Claim: The probabilistic stability framework reveals phase transitions in SGD dynamics that are invisible to traditional stability analysis based on variance.
- Mechanism: By examining the convergence in probability condition, the authors identify distinct phases of SGD behavior: correct learning, incorrect learning, convergence to low-rank saddles, and complete instability. These phases depend on the signal-to-noise ratio in the gradient near the saddle.
- Core assumption: The Lyapunov exponent (m) is a meaningful metric for understanding SGD stability.
- Evidence anchors:
  - [abstract] The authors prove that SGD can converge to low-rank saddle points when the gradient noise is sufficiently strong and the learning rate is large.
  - [section] "We then show that this stability condition can be quantified with a stochastic extension of the Lyapunov exponent, a quantity rooted in the study of dynamical systems and has been well-understood in physics and control theory."
- Break condition: If the variance of log(Δg(t)) grows too fast (faster than t²), violating the assumption in Theorem 1.

### Mechanism 3
- Claim: The proposed stability framework explains how SGD selects solutions in neural networks, revealing a preference for low-rank saddles under certain conditions.
- Mechanism: The phase diagram analysis shows that SGD can converge to low-rank saddles when both wt+ut→p 0 and wt−ut→p 0. This occurs when the gradient noise is strong and the learning rate is appropriately large, but not too large.
- Core assumption: The loss landscape of neural networks contains low-rank saddle points that are relevant to the learning dynamics.
- Evidence anchors:
  - [section] "We prove that saddle points can be either attractive or repulsive for SGD, and its dynamics can be classified into four different phases, depending on the signal-to-noise ratio in the gradient close to the saddle."
  - [section] "The rank-0 point where ui=wi=0 for all i is a saddle point as long as E[xy]≠ 0."
- Break condition: If the loss landscape does not contain low-rank saddle points or if the signal-to-noise ratio is too low or too high.

## Foundational Learning

- Concept: Lyapunov exponent
  - Why needed here: The Lyapunov exponent quantifies the exponential rate of convergence or divergence of a dynamical system. In this paper, it is extended to a stochastic setting to measure probabilistic stability of SGD.
  - Quick check question: What does a negative Lyapunov exponent indicate about the stability of a system?

- Concept: Convergence in probability
  - Why needed here: The paper argues that convergence in probability is a more appropriate measure of SGD stability than convergence in mean square, especially for understanding how SGD selects solutions in neural networks.
  - Quick check question: How does convergence in probability differ from convergence in mean square?

- Concept: Random matrix product process
  - Why needed here: At Type-II saddles, the SGD dynamics are dominated by a random matrix product process, which requires specialized analysis techniques from ergodic theory and dynamical systems.
  - Quick check question: Why is the random matrix product process particularly relevant for understanding SGD dynamics at Type-II saddles?

## Architecture Onboarding

- Component map: Synthetic data generation -> SGD training with varying learning rates -> Lyapunov exponent computation -> Phase diagram analysis -> ResNet18 training with noise injection -> Rank/sparsity evaluation
- Critical path: (1) Define probabilistic stability using convergence in probability, (2) Connect probabilistic stability to Lyapunov exponents, (3) Analyze SGD dynamics around Type-II saddles, (4) Characterize phase diagrams for SGD, (5) Validate findings through experiments
- Design tradeoffs: The paper trades the simplicity of traditional stability analysis (based on variance) for a more nuanced understanding of SGD dynamics that captures phenomena invisible to moment-based approaches
- Failure signatures: If the variance of log(Δg(t)) grows too fast, or if the gradient noise is parameter-independent, the probabilistic stability framework may break down
- First 3 experiments:
  1. Replicate the linear regression experiments to verify the critical learning rates and the robustness to outliers
  2. Implement the matrix factorization problem and verify the phase diagram for SGD dynamics around low-rank saddles
  3. Train a simple two-layer network and observe the transition between different phases as the learning rate and noise level are varied

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Lyapunov exponent-based measure of probabilistic stability generalize to non-convex loss landscapes with multiple local minima?
- Basis in paper: [explicit] The authors propose the Lyapunov exponent as a measure of probabilistic stability and show it works for simple models and a low-rank saddle point in a neural network.
- Why unresolved: The paper only analyzes a simple linear regression problem and a specific low-rank saddle point in a neural network. It's unclear if the Lyapunov exponent remains a reliable indicator of stability in more complex loss landscapes with multiple minima.
- What evidence would resolve it: Experimental results showing the Lyapunov exponent correlates with training stability and generalization performance across various neural network architectures and datasets with diverse loss landscapes.

### Open Question 2
- Question: How does the batch size affect the phase diagram of SGD stability and the preference for low-rank solutions?
- Basis in paper: [explicit] The authors mention that increasing the batch size makes it more difficult to converge to low-rank solutions and show a phase diagram for different batch sizes.
- Why unresolved: While the paper demonstrates the effect of batch size on the stability phase diagram, it doesn't provide a quantitative understanding of how the phase boundaries shift with batch size or the underlying mechanism behind this effect.
- What evidence would resolve it: A theoretical analysis deriving how the phase boundaries in the stability diagram depend on batch size, validated by extensive experiments across different batch sizes and learning rates.

### Open Question 3
- Question: Is the preference for low-rank solutions a universal property of SGD or specific to certain architectures and tasks?
- Basis in paper: [explicit] The authors show that SGD converges to low-rank saddle points in a matrix factorization problem and a two-layer neural network, and observe similar behavior in ResNet18 on CIFAR-10.
- Why unresolved: The experiments are limited to a few specific architectures (matrix factorization, two-layer network, ResNet18) and a single task (image classification on CIFAR-10). It's unclear if this preference for low-rank solutions is a general property of SGD or specific to these settings.
- What evidence would resolve it: Experiments demonstrating the preference for low-rank solutions across a wide range of neural network architectures (e.g., RNNs, transformers) and tasks (e.g., NLP, reinforcement learning).

## Limitations

- Analysis relies heavily on simplified linear regression and matrix factorization models that may not capture the complexity of deep neural network loss landscapes
- Assumes specific noise structures (parameter-dependent vs. independent) that may not hold universally across different architectures and datasets
- Limited experimental validation to a single architecture (ResNet18) and dataset (CIFAR-10), raising questions about generalizability

## Confidence

- High confidence: The theoretical framework connecting Lyapunov exponents to probabilistic stability is well-grounded in dynamical systems theory
- Medium confidence: The classification of saddle points into Type-I and Type-II categories is theoretically sound, but empirical validation across diverse architectures is needed
- Medium confidence: The experimental results on ResNet18 demonstrate the theoretical predictions, but the results are based on a single architecture and dataset

## Next Checks

1. Test the phase diagram predictions on a broader range of architectures (CNNs, transformers) and datasets to verify the generalizability of the Type-I/Type-II saddle classification
2. Conduct ablation studies varying the noise structure (parameter-dependent vs. independent) to quantify the impact on stability and solution selection
3. Implement real-time Lyapunov exponent estimation during training to validate its practical utility as a stability metric across different learning scenarios