---
ver: rpa2
title: 'FloodBrain: Flood Disaster Reporting by Web-based Retrieval Augmented Generation
  with an LLM'
arxiv_id: '2311.02597'
source_url: https://arxiv.org/abs/2311.02597
tags:
- reports
- report
- floodbrain
- flood
- disaster
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FloodBrain, a web-based tool for generating
  flood disaster impact reports using Retrieval-Augmented Generation (RAG) with Large
  Language Models (LLMs). The system extracts and curates information from web sources
  to produce detailed reports on flood events.
---

# FloodBrain: Flood Disaster Reporting by Web-based Retrieval Augmented Generation with an LLM

## Quick Facts
- arXiv ID: 2311.02597
- Source URL: https://arxiv.org/abs/2311.02597
- Reference count: 25
- Key outcome: FloodBrain is a web-based tool that generates flood disaster impact reports using Retrieval-Augmented Generation (RAG) with LLMs, achieving high ROUGE scores and reducing computational costs by 59% through source relevancy filtering.

## Executive Summary
This paper introduces FloodBrain, a web-based system for generating flood disaster impact reports using Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs). The system extracts and curates information from web sources to produce detailed reports on flood events, answering specific questions about impacts, locations, causes, timelines, and knock-on effects. FloodBrain includes a user interface for event selection, source validation through an inspect tab, and a chatbot for further interrogation. The evaluation compares FloodBrain-generated reports to human-written ReliefWeb reports using ROUGE scores and human evaluation, with GPT-4 performing best among tested LLMs. The pipeline ablation study demonstrates that LLM-assisted search improves report quality while source relevancy checks reduce computational costs by 59%.

## Method Summary
FloodBrain uses a sophisticated pipeline that combines web-based retrieval with LLM question answering to generate flood disaster reports. The system starts by gathering flood events from disaster response agencies, then performs web searches using event descriptions. Extracted text from websites is processed and filtered for relevance using an LLM. The filtered sources are used to answer structured questions about the flood event through LLM question answering. Finally, the question-answer pairs are summarized into a coherent flood report. The pipeline includes LLM-assisted query expansion to improve source coverage and source relevancy checks to reduce computational costs. The system is evaluated using ROUGE scores, G-EVAL scores, and human evaluation against human-written ReliefWeb reports.

## Key Results
- GPT-4 achieved the highest ROUGE scores among tested LLMs, with high correlation between GPT-4 and human evaluation scores
- LLM-assisted search improved report quality by 6.3% (ROUGE-1), 6.2% (ROUGE-2), and 7.2% (ROUGE-L)
- Source relevancy filtering reduced computational costs by 59% while maintaining report quality
- 41% of sources passed the relevancy check, averting 1,795 additional LLM API calls for 26 reports

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG-based augmentation mitigates LLM hallucination by grounding generation in retrieved web sources
- Mechanism: Web search results are filtered for relevance, then used as context for LLM question answering, ensuring outputs derive from factual web data rather than model pretraining
- Core assumption: The retrieved sources contain accurate, relevant information about the flood event
- Evidence anchors:
  - [abstract] "To address this, we introduce a sophisticated pipeline embodied in our tool FloodBrain (floodbrain.com), specialized in generating flood disaster impact reports by extracting and curating information from the web."
  - [section] "Each source is evaluated by an LLM to determine its relevancy to the original flood event and to filter out wrongly returned websites from the Google search."
  - [corpus] Weak: No direct mention of hallucination mitigation in neighbor papers

### Mechanism 2
- Claim: LLM-assisted search query expansion improves report quality by capturing more relevant sources
- Mechanism: Original search queries are expanded using an LLM to generate additional relevant search terms, increasing the diversity and coverage of retrieved sources
- Core assumption: The LLM can generate meaningful, relevant search query expansions that lead to better sources
- Evidence anchors:
  - [section] "To gather more relevant sources we prompt an LLM to expand the original query into other relevant search queries."
  - [section] "Removing LLM-assisted search decreases report quality across all ROUGE metrics: 6.3% for ROUGE-1, 6.2% for ROUGE-2, and 7.2% for ROUGE-L."
  - [corpus] Weak: No direct evidence in neighbor papers about query expansion benefits

### Mechanism 3
- Claim: Source relevancy filtering reduces computational costs by eliminating unnecessary LLM API calls
- Mechanism: Sources are evaluated for relevance before being passed to the LLM for question answering, avoiding expensive LLM calls on irrelevant content
- Core assumption: The relevancy check can accurately identify irrelevant sources without discarding useful information
- Evidence anchors:
  - [section] "The value of this pipeline step comes from accelerating report generation by eliminating extraneous context from downstream prompts, reducing computational cost by 59%."
  - [section] "In the complete pipeline, 41% of sources pass the relevancy check (252 out of 611 for 26 reports), averting an additional 1,795 LLM API calls."
  - [corpus] Weak: No direct mention of cost reduction through filtering in neighbor papers

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG allows the LLM to base its output on retrieved factual information rather than relying solely on its pretraining knowledge, which is crucial for accurate disaster reporting
  - Quick check question: What is the main benefit of using RAG over standard LLM generation for disaster reporting?

- Concept: Web scraping and text extraction
  - Why needed here: Extracting textual data from web sources is essential for gathering the information that will be used to generate flood reports
  - Quick check question: What are the key steps involved in extracting and processing text data from web sources for RAG?

- Concept: ROUGE score evaluation
  - Why needed here: ROUGE scores provide an automated way to compare the quality of generated reports against human-written references, assessing word similarity and coverage
  - Quick check question: How do ROUGE recall scores differ from precision scores, and why is recall more appropriate for evaluating FloodBrain reports?

## Architecture Onboarding

- Component map:
  User Interface -> Event Selection -> Web Search -> Source Processing -> LLM Query Expansion -> Source Relevancy Check -> Question Answering -> Report Generation -> Display Report/Map/Chatbot

- Critical path:
  1. User selects flood event → 2. Web search with event description → 3. Source processing → 4. LLM query expansion → 5. Source relevancy check → 6. Question answering → 7. Report generation → 8. Display report, map, and chatbot

- Design tradeoffs:
  - Accuracy vs. speed: More thorough source filtering and validation improves accuracy but increases processing time
  - Cost vs. quality: Using more LLM calls for query expansion and relevancy checking improves report quality but increases computational costs
  - Automation vs. human oversight: Fully automated reports are faster but may require human validation for trustworthiness

- Failure signatures:
  - Poor report quality: Check if sources are relevant and if LLM query expansion is working correctly
  - High computational costs: Investigate if source relevancy check is effectively filtering irrelevant sources
  - Missing flood events: Verify that event aggregation from disaster response agencies is functioning properly

- First 3 experiments:
  1. Test web search and source extraction with a known flood event to ensure sources are being gathered correctly
  2. Evaluate the impact of LLM query expansion by comparing report quality with and without query expansion on a small set of events
  3. Measure the effectiveness of the source relevancy check by manually reviewing a sample of filtered sources and assessing the impact on report quality and computational costs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of FloodBrain-generated reports compare when applied to less monitored or underreported flood events compared to major disasters?
- Basis in paper: [inferred] The paper mentions that FloodBrain currently reports only on externally recognized disaster-classified flooding events and aims to expand to less monitored regions
- Why unresolved: The paper primarily evaluates FloodBrain on major disasters with available human-written reports, but does not test its performance on underreported events
- What evidence would resolve it: Comparative analysis of FloodBrain reports generated for both major disasters and underreported flood events, evaluated by human experts for accuracy and completeness

### Open Question 2
- Question: What is the environmental impact of using FloodBrain at scale for disaster reporting, considering the computational resources required?
- Basis in paper: [explicit] The paper discusses ethical considerations, including the environmental impact of LLMs due to their substantial computational, energy, and material demands
- Why unresolved: While the paper mentions environmental concerns, it does not provide specific data on FloodBrain's environmental impact or scalability implications
- What evidence would resolve it: Detailed analysis of FloodBrain's carbon footprint, energy consumption, and resource usage when deployed at scale for disaster reporting

### Open Question 3
- Question: How can FloodBrain's pipeline be optimized to reduce computational costs while maintaining or improving report quality?
- Basis in paper: [explicit] The ablation study shows that source relevancy checks reduce computational costs by 59%, but further optimization opportunities are not explored
- Why unresolved: The paper identifies some optimization potential but does not investigate comprehensive pipeline improvements or alternative cost-reduction strategies
- What evidence would resolve it: Results from experiments testing various pipeline modifications, such as different LLM architectures, prompt engineering techniques, or alternative data processing methods to reduce computational costs while maintaining report quality

## Limitations
- The evaluation relies heavily on ROUGE metrics and limited human evaluation (27 reports), which may not fully capture report quality and trustworthiness
- The source relevancy check mechanism lacks detailed validation of whether it might be filtering out useful information
- The paper doesn't address potential biases in web sources or how the system handles conflicting information from different sources
- Direct evidence of hallucination reduction is not provided, only improved ROUGE scores compared to human-written reports

## Confidence
- High Confidence: The computational cost reduction from source relevancy filtering (59% reduction with 41% of sources passing) is well-supported by the provided data
- Medium Confidence: The claim that LLM-assisted search improves report quality is supported by ROUGE score improvements, but the magnitude of improvement (6-7% across metrics) is relatively modest
- Medium Confidence: The overall pipeline effectiveness in generating coherent flood reports is demonstrated, but the comparison to human-written reports using ROUGE scores has limitations as an evaluation metric

## Next Checks
1. Conduct a systematic analysis of hallucination rates by having human experts identify factual inaccuracies in FloodBrain-generated reports compared to verified flood event data, rather than relying solely on ROUGE scores
2. Perform a detailed study on the impact of the source relevancy check by comparing report quality when the filter is applied versus when all sources are used, including both automated metrics and human evaluation of information completeness
3. Evaluate the system's performance across diverse flood events (different regions, causes, and scales) to assess whether the reported improvements in report quality are consistent or if performance varies significantly based on event characteristics