---
ver: rpa2
title: Prediction Error-based Classification for Class-Incremental Learning
arxiv_id: '2305.18806'
source_url: https://arxiv.org/abs/2305.18806
tags:
- alse
- learning
- decay
- 'true'
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Prediction Error-based Classification (PEC),
  a novel approach for class-incremental learning (CIL) that computes class scores
  by measuring the prediction error of student networks trained to replicate a frozen
  random teacher network. PEC approximates a classification rule based on Gaussian
  Process posterior variance and offers advantages such as sample efficiency, ease
  of tuning, and effectiveness even when data are presented one class at a time.
---

# Prediction Error-based Classification for Class-Incremental Learning

## Quick Facts
- arXiv ID: 2305.18806
- Source URL: https://arxiv.org/abs/2305.18806
- Reference count: 40
- Key outcome: PEC approximates Gaussian Process posterior variance using prediction error from student networks mimicking frozen random teachers, achieving strong performance in single-pass-through-data class-incremental learning.

## Executive Summary
This paper introduces Prediction Error-based Classification (PEC), a novel approach for class-incremental learning that computes class scores by measuring the prediction error of student networks trained to replicate frozen random teacher networks. PEC can be interpreted as approximating a classification rule based on Gaussian Process posterior variance, where prediction error is small for in-distribution samples and large for out-of-distribution samples. The method offers several practical advantages including sample efficiency, ease of tuning, and effectiveness even when data are presented one class at a time.

## Method Summary
PEC is a rehearsal-free class-incremental learning method that uses prediction error as class scores. For each class, a student network is trained to mimic a frozen random teacher network. During inference, the prediction error between each student-teacher pair is computed for a given input, and the class with minimum prediction error is selected. The method approximates Gaussian Process posterior variance and operates without requiring task identity at test time. PEC uses single-pass-through-data setting and trains one student per class to prevent catastrophic forgetting.

## Key Results
- PEC performs strongly in single-pass-through-data CIL, outperforming other rehearsal-free baselines in all cases
- PEC outperforms rehearsal-based methods with moderate replay buffer size in most cases across multiple benchmarks
- PEC achieves strong performance without requiring task identity at test time and works even when data are presented one class at a time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PEC approximates Gaussian Process (GP) posterior variance for classification
- Mechanism: Each class-specific student network is trained to mimic a frozen random teacher network. Prediction error from this mimicry approximates the GP posterior variance, which is small for in-distribution samples and large for out-of-distribution samples.
- Core assumption: Wide neural networks with random initialization approximate Gaussian Processes (GP).
- Evidence anchors:
  - [abstract]: "The method can be interpreted as approximating a classification rule based on Gaussian Process posterior variance."
  - [section]: "PEC can be motivated from two different angles... PEC can be seen as an approximation of a principled classification rule based on the posterior variance of a Gaussian Process."
  - [corpus]: Weak. No corpus papers directly discuss PEC or GP approximation via prediction error mimicry.
- Break condition: If neural networks don't approximate GPs well enough (narrow networks, poor initialization), the approximation fails.

### Mechanism 2
- Claim: Prediction error as class score provides robustness to catastrophic forgetting
- Mechanism: Each class has its own dedicated student-teacher pair. During training, only the student for the current class is updated. The teacher networks remain frozen, so old classes don't get overwritten.
- Core assumption: Separating models per class prevents interference between classes.
- Evidence anchors:
  - [abstract]: "PEC offers several practical advantages, including sample efficiency, ease of tuning, and effectiveness even when data are presented one class at a time."
  - [section]: "PEC is well-suited for class-incremental learning since each class has its own dedicated model, eliminating interference and preventing forgetting."
  - [corpus]: Weak. No corpus papers directly discuss this separation-based forgetting prevention.
- Break condition: If students for different classes share parameters or if teachers get updated, forgetting occurs.

### Mechanism 3
- Claim: PEC works without requiring task identity at test time
- Mechanism: Classification is based solely on prediction error magnitude. No task structure or labels needed during inference. Each student-teacher pair independently evaluates how well it can predict the teacher's output for a given input.
- Core assumption: Prediction error magnitude correlates with class membership regardless of task boundaries.
- Evidence anchors:
  - [abstract]: "PEC can operate even when the data are presented only one class at a time."
  - [section]: "PEC does not rely on the task structure and achieves the same results for both task splits."
  - [corpus]: Weak. No corpus papers directly discuss task-free operation via prediction error.
- Break condition: If prediction errors don't correlate with class membership (e.g., similar classes have similar teacher outputs), classification fails.

## Foundational Learning

- Gaussian Processes
  - Why needed here: GP posterior variance is the theoretical foundation for PEC's classification rule
  - Quick check question: What does GP posterior variance measure, and why is it useful for classification?
- Random Network Distillation
  - Why needed here: PEC is inspired by this technique for novelty detection using prediction error
  - Quick check question: How does prediction error from a trained network trying to mimic a random network indicate novelty?
- Continual Learning
  - Why needed here: PEC is designed specifically for class-incremental learning scenarios
  - Quick check question: What makes class-incremental learning more challenging than task-incremental learning?

## Architecture Onboarding

- Component map:
  - Frozen random teacher network h (wide, one hidden layer) -> Class-specific student networks gθc (narrower, same architecture type) -> Prediction error computation: ||gθc(x) - h(x)|| -> Class selection: arg min over classes
- Critical path:
  1. Initialize teacher with random weights
  2. For each class, train student to minimize prediction error on that class's data
  3. At inference, compute prediction error for each class and select minimum
- Design tradeoffs:
  - Wide teacher vs narrow students: Wide teacher improves GP approximation but increases parameters
  - Separate models per class: Prevents forgetting but scales linearly with number of classes
  - Single pass vs multiple epochs: Single pass is more realistic but harder
- Failure signatures:
  - High variance in scores across classes → poor GP approximation or data imbalance
  - Similar scores for all classes → teacher and students not learning distinct representations
  - Performance drops with imbalanced data → need balancing strategies
- First 3 experiments:
  1. Train PEC on MNIST with one class per task, verify no forgetting occurs
  2. Test PEC with balanced vs imbalanced data to observe score comparability issues
  3. Vary teacher width to see impact on classification accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical justification for using prediction error as a class score in PEC, and how does it relate to Gaussian Process posterior variance?
- Basis in paper: [explicit] The paper states that PEC approximates a classification rule based on Gaussian Process posterior variance, but does not provide a formal proof of this relationship.
- Why unresolved: While the paper mentions that PEC approximates GP posterior variance, it does not provide a formal proof or detailed explanation of this relationship. The paper only briefly mentions that PEC can be viewed as an approximation of a principled classification rule based on the posterior variance of a Gaussian Process.
- What evidence would resolve it: A formal proof showing that PEC's prediction error-based scores are equivalent to or approximate the posterior variance of a Gaussian Process under certain conditions would resolve this question.

### Open Question 2
- Question: How does PEC perform in the presence of data imbalance across classes, and what strategies can be employed to mitigate this issue?
- Basis in paper: [explicit] The paper discusses the impact of data imbalance on PEC's performance and proposes strategies like Equal budgets and Buffer balancing to address this issue.
- Why unresolved: While the paper mentions the impact of data imbalance and proposes strategies to mitigate it, it does not provide a comprehensive analysis of PEC's performance under varying levels of data imbalance or a detailed evaluation of the effectiveness of the proposed strategies.
- What evidence would resolve it: A thorough experimental evaluation of PEC's performance under different levels of data imbalance, along with a detailed analysis of the effectiveness of the proposed strategies, would resolve this question.

### Open Question 3
- Question: How does PEC compare to other class-incremental learning methods in terms of computational efficiency and memory usage?
- Basis in paper: [inferred] The paper mentions that PEC is designed with efficiency in mind and does not introduce any new hyperparameters, but does not provide a detailed comparison of PEC's computational efficiency and memory usage with other methods.
- Why unresolved: While the paper mentions that PEC is efficient and does not introduce new hyperparameters, it does not provide a detailed comparison of PEC's computational efficiency and memory usage with other class-incremental learning methods.
- What evidence would resolve it: A comprehensive analysis of PEC's computational efficiency and memory usage compared to other class-incremental learning methods, including a detailed breakdown of the computational costs and memory requirements of each method, would resolve this question.

## Limitations
- The theoretical foundation connecting PEC to Gaussian Process posterior variance remains weakly supported by empirical evidence
- Computational complexity of training separate student networks for each class is not analyzed, which could limit scalability
- Paper does not investigate how sensitive PEC is to teacher network initialization or whether different random seeds significantly affect performance

## Confidence
- **High confidence**: PEC's effectiveness in preventing catastrophic forgetting through separate student-teacher pairs, demonstrated across multiple datasets and compared to multiple baselines
- **Medium confidence**: The claim that PEC approximates GP posterior variance, as the theoretical connection is presented but not empirically validated through controlled experiments
- **Medium confidence**: PEC's sample efficiency and task-free operation, though these advantages are demonstrated, the underlying mechanisms could benefit from more rigorous analysis

## Next Checks
1. Conduct an ablation study varying teacher network width and depth to quantify the impact on GP approximation quality and classification accuracy
2. Perform a computational complexity analysis comparing PEC to baseline methods, including memory usage for storing multiple student networks and training time per class
3. Test PEC's robustness to teacher network initialization by running multiple trials with different random seeds and analyzing performance variance across trials