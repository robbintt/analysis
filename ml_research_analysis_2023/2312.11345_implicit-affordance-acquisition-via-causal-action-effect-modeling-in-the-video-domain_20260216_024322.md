---
ver: rpa2
title: Implicit Affordance Acquisition via Causal Action-Effect Modeling in the Video
  Domain
arxiv_id: '2312.11345'
source_url: https://arxiv.org/abs/2312.11345
tags:
- video
- verb
- action
- verbs
- affordance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new dataset and two pretraining tasks to
  implicitly acquire affordance knowledge from instructional videos. The key idea
  is to model the causal relationship between actions, objects, and their visual effects,
  thereby learning behavior and entity equivalence principles of affordances.
---

# Implicit Affordance Acquisition via Causal Action-Effect Modeling in the Video Domain

## Quick Facts
- arXiv ID: 2312.11345
- Source URL: https://arxiv.org/abs/2312.11345
- Reference count: 40
- Primary result: Two pretraining tasks (MAM and MEM) enable implicit affordance acquisition from instructional videos, with joint MULTI-CAE model outperforming strong baselines on zero-shot physical reasoning.

## Executive Summary
This paper addresses the challenge of acquiring affordance knowledge implicitly from instructional videos by modeling the causal relationships between actions, objects, and their visual effects. The authors propose two novel pretraining tasks—Masked Action Modeling (MAM) and Masked Effect Modeling (MEM)—that learn behavior and entity equivalence principles of affordances through causal action-effect modeling. The approach is evaluated on both intrinsic tasks and a zero-shot physical reasoning probing task, demonstrating superior performance compared to strong baselines.

## Method Summary
The method leverages the HERO model architecture with two novel pretraining tasks. MAM masks action verbs and reconstructs them from visual effects and context, learning behavior equivalence. MEM masks post-condition frames and reconstructs them from action and object context, learning entity equivalence. The MULTI-CAE model jointly trains on both tasks. The CAE dataset (4.1M clip-subtitle pairs) is used for pretraining, with evaluations on MAP and MEP intrinsic tasks and the PROST affordance probing task.

## Key Results
- MAM and MEM pretraining tasks outperform strong baselines on both intrinsic evaluations (MAP/MEP) and zero-shot physical reasoning (PROST)
- MULTI-CAE model (joint MAM+MEM training) achieves best overall performance
- Models demonstrate ability to learn behavior and entity equivalence principles of affordances
- Results show effectiveness of modeling action-effect relationships in videos for affordance learning

## Why This Works (Mechanism)

### Mechanism 1
Modeling action-effect relationships through MAM enables learning behavior equivalence by masking the action verb and reconstructing it from visual effects and contextual cues. The model learns that different verbs producing the same visual outcome are functionally equivalent. Core assumption: visual effects uniquely identify the action's behavioral category. Break condition: ambiguous or non-specific visual effects prevent disambiguation between equivalent behaviors.

### Mechanism 2
Modeling action-effect relationships through MEM enables learning entity equivalence by masking post-condition frames and reconstructing them from action and object context. The model learns that executing the same action on different objects produces similar visual outcomes. Core assumption: action process and object properties together determine post-condition, allowing inference of common visual patterns. Break condition: diverse object properties or highly object-specific visual effects prevent pattern identification.

### Mechanism 3
Joint MAM and MEM training provides complementary learning signals that enhance affordance knowledge acquisition. MAM teaches action semantics through visual effects while MEM teaches object-action relationships through post-condition prediction. Together they provide a more complete affordance representation. Core assumption: tasks capture different but complementary aspects of affordance knowledge. Break condition: task interference or dominance prevents realization of joint benefits.

## Foundational Learning

- **Concept**: Causal Action-Effect Modeling
  - Why needed here: Understanding causal relationships between actions, objects, and effects is fundamental to affordance knowledge
  - Quick check question: Can you explain why a chair affords sitting and what visual effect would indicate someone is sitting on it?

- **Concept**: Masked Language Modeling (MLM)
  - Why needed here: MLM technique underlies both MAM and MEM tasks where parts of input are masked and model learns to reconstruct them from context
  - Quick check question: What is the difference between masking only the verb versus masking both the verb and random words in the context?

- **Concept**: Zero-shot Learning
  - Why needed here: Evaluates model's ability to generalize to unseen actions and objects, demonstrating learned generalizable affordance knowledge
  - Quick check question: How would you design an experiment to test if a model has learned to generalize affordance knowledge to completely new object-action pairs?

## Architecture Onboarding

- **Component map**: Video frames → Visual Embedder → Cross-Modal Transformer → MAM/MEM heads for pretraining; Video frames → Visual Embedder → Cross-Modal Transformer → Temporal Transformer → MEM head for MEM task
- **Critical path**: Video frames processed through visual embedder, combined with text through cross-modal transformer, then passed to task-specific heads for MAM or MEM pretraining
- **Design tradeoffs**: Hierarchical architecture captures local and global context but increases model complexity and computational cost
- **Failure signatures**: Poor zero-shot task performance indicates failure to learn generalizable affordance knowledge; low MEM task accuracy suggests failure to learn entity equivalence
- **First 3 experiments**:
  1. Train MAM-VL Rnd on CAE dataset and evaluate on MAP task with language-only input to assess action understanding without visual cues
  2. Train MEM-VL Rnd on CAE dataset and evaluate on MEP task with video-only input to assess effect prediction without linguistic cues
  3. Train MULTI-CAE-VL Rnd on CAE dataset and evaluate on PROST task to assess overall affordance knowledge acquisition

## Open Questions the Paper Calls Out

### Open Question 1
How do results change if CAE dataset is filtered to only include clips where result verb and its object are both visually present in video frames? Current dataset includes 70% noisy examples where action or object may not be visually depicted, potentially negatively impacting model learning. Evidence would come from retraining models on filtered dataset and comparing intrinsic evaluation and PROST probing task results.

### Open Question 2
Does pretraining with MEM improve PROST performance for non-sliding affordances (stack, roll, grasp, break, bounce) despite these actions not being explicitly present in CAE training data? Paper shows MULTI-CAE-VL outperforms MAM-VL on PROST but doesn't break down results by affordance type. Evidence would come from comparing PROST results for specific non-sliding affordances between MAM-VL and MULTI-CAE-VL.

### Open Question 3
How does PROST performance change if model is trained with additional contrastive loss that explicitly encourages representations of different objects with same affordance to be more similar? Current MEM task implicitly captures some affordance information but doesn't explicitly enforce similar representations for objects with same affordance. Evidence would come from retraining with contrastive loss and comparing PROST results.

## Limitations

- Reliance on visual effects to uniquely identify action categories may fail when effects are ambiguous or non-specific to certain actions
- Entity equivalence learning may fail when object properties are too diverse or visual effects are highly object-specific
- Instructional video data constraints (cooking/handcrafting focus) may limit generalizability to other domains

## Confidence

**High Confidence**: Masked reconstruction mechanism for learning behavior and entity equivalence is theoretically sound and aligns with established pretraining methodologies. Zero-shot PROST results provide strong empirical evidence.

**Medium Confidence**: Joint training providing complementary learning signals is supported by experimental results, but lacks ablation studies quantifying individual task contributions and interaction effects.

**Low Confidence**: Assertion that visual effects uniquely identify action categories requires stronger empirical validation through detailed failure case analysis and ambiguity quantification.

## Next Checks

1. **Ablation Study on Task Contribution**: Conduct systematic ablation experiments to quantify individual and joint contributions of MAM and MEM tasks to affordance knowledge acquisition, measuring performance degradation when removing each task.

2. **Cross-Domain Generalization Test**: Evaluate pretrained models on affordance tasks from domains not present in CAE dataset (e.g., sports, automotive repair) to assess generalizability of learned affordance knowledge.

3. **Visual Effect Ambiguity Analysis**: Perform detailed error analysis on MAP task to identify cases where visual effects are ambiguous or non-specific to actions, quantifying proportion of such cases and assessing impact on behavior equivalence learning accuracy.