---
ver: rpa2
title: 'Tuning In to Neural Encoding: Linking Human Brain and Artificial Supervised
  Representations of Language'
arxiv_id: '2310.04460'
source_url: https://arxiv.org/abs/2310.04460
tags:
- language
- neural
- encoding
- representations
- brain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares neural encoding performance of prompt-tuned
  and fine-tuned supervised language representations for Chinese fMRI data. It finds
  that prompt-tuning generally outperforms fine-tuning in predicting brain activity
  patterns, especially for tasks requiring compositional understanding of entities
  and concepts.
---

# Tuning In to Neural Encoding: Linking Human Brain and Artificial Supervised Representations of Language

## Quick Facts
- arXiv ID: 2310.04460
- Source URL: https://arxiv.org/abs/2310.04460
- Reference count: 40
- Key outcome: Prompt-tuning generally outperforms fine-tuning in predicting brain activity patterns, especially for tasks requiring compositional understanding of entities and concepts

## Executive Summary
This paper investigates how different parameter-tuning methods affect the ability of language models to predict human brain activity during language processing. The study compares prompt-tuning and fine-tuning approaches using 8 natural language understanding tasks on Chinese fMRI data. Results show that prompt-tuning consistently outperforms fine-tuning in neural encoding, particularly for tasks requiring fine-grained compositional understanding of entities and concepts. The research also reveals that the proportion of tuned parameters significantly impacts encoding performance, with partial fine-tuning often yielding better results than full fine-tuning.

## Method Summary
The study uses a pre-trained RoBERTa-wwm-ext model fine-tuned on 8 NLU tasks (LTC, STC, NLI, CR, KR, SSP, NER, QA) using both prompt-tuning (P-tuning V2) and fine-tuning approaches. Sentence embeddings from these tuned models are used to predict fMRI signals from 12 subjects listening to Chinese narratives. Neural encoding is performed using voxel-wise Ridge regression with HRF convolution, and performance is evaluated using Pearson correlation between predicted and measured brain activity.

## Key Results
- Prompt-tuning significantly outperforms fine-tuning in neural encoding across most tasks
- Tasks requiring compositional understanding of entities and concepts (KR, NER) yield the best brain encoding performance
- The proportion of tuned parameters highly influences encoding performance, with more parameter tuning generally leading to worse brain prediction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt-tuning preserves pre-trained knowledge better than fine-tuning, which is crucial for neural encoding performance.
- Mechanism: By freezing pre-trained weights and only training additional prompt embeddings, prompt-tuning minimizes distortion of general domain knowledge learned during pre-training. This preserved knowledge better aligns with the brain's stable language network representations.
- Core assumption: The brain's language representation relies more on stable, general linguistic knowledge than on task-specific features.
- Evidence anchors:
  - [abstract] "fine-tuning has been shown to distort the knowledge learned from pre-training [16], which is inconsistent with the human brain's mechanism"
  - [section 2.2] "fine-tuning generally updates the entire set of pre-trained parameters and is distinct from the way the brain adapts to new tasks"
- Break condition: If the brain's language network is primarily shaped by task-specific learning rather than stable pre-trained representations, prompt-tuning's advantage would diminish.

### Mechanism 2
- Claim: Tasks requiring fine-grained compositional understanding of entities and concepts produce representations that better predict brain activity patterns.
- Mechanism: When models must process fine-grained semantic relations between entities and concepts (like in NER or KR tasks), they develop representations that align with how the brain encodes linguistic meaning, particularly in regions like STG and superior temporal gyrus.
- Core assumption: The brain's language network emphasizes compositional semantic processing at the entity/concept level when representing sentences.
- Evidence anchors:
  - [abstract] "we discover that tasks that require a fine-grained processing of concepts and entities lead to representations that are most predictive of brain activation patterns"
  - [section 5.2] "Fine-tuning and prompt-tuning on KR and NER tasks yield significantly better neural encoding performance"
- Break condition: If the brain's language representation relies more on syntactic or contextual patterns than compositional entity understanding, this mechanism would not hold.

### Mechanism 3
- Claim: The proportion of tuned parameters significantly influences neural encoding performance, with more parameter tuning leading to worse brain prediction.
- Mechanism: As more pre-trained parameters are tuned, the model's representations deviate further from the stable knowledge structures that align with brain language networks. This creates a trade-off between preserving pre-trained knowledge and learning task-specific features.
- Core assumption: Pre-trained knowledge contributes more to brain language representation than task-specific features for certain tasks.
- Evidence anchors:
  - [abstract] "we reveal that the proportion of tuned parameters highly influences the neural encoding performance of fine-tuned models"
  - [section 5.3] "The more parameters tuned, the worse neural encoding performances becomes"
- Break condition: If task-specific features are more important than pre-trained knowledge for brain language representation, increasing tuned parameters would improve rather than degrade encoding performance.

## Foundational Learning

- Concept: Understanding the difference between prompt-tuning and fine-tuning
  - Why needed here: The paper's central comparison relies on understanding how these two tuning methods affect neural encoding performance
  - Quick check question: What is the key difference in how prompt-tuning and fine-tuning handle pre-trained model parameters?

- Concept: Brain language network anatomy and function
  - Why needed here: Interpreting encoding results requires understanding which brain regions are involved in language processing and what their roles are
  - Quick check question: Which brain regions compose the language network and what linguistic functions do they support?

- Concept: Neural encoding methodology
  - Why needed here: The paper uses neural encoding to predict brain activity from model representations, requiring understanding of this methodology
  - Quick check question: How does the neural encoding model predict fMRI signals from sentence embeddings in this study?

## Architecture Onboarding

- Component map:
  - Pre-trained Chinese RoBERTa model -> 8 NLU task datasets -> Prompt-tuning implementation (P-tuning V2) -> Fine-tuning implementation (partial and full) -> Neural encoding pipeline (HRF convolution + Ridge regression) -> fMRI dataset from Chinese language narratives

- Critical path:
  1. Load and preprocess Chinese fMRI dataset from 12 subjects
  2. Tune pre-trained model on each of 8 tasks using both prompt-tuning and fine-tuning
  3. Generate sentence representations from tuned models for all story stimuli
  4. Build neural encoding models to predict brain activity using Ridge regression
  5. Compare encoding performance across tuning methods and tasks

- Design tradeoffs:
  - Prompt-tuning vs. fine-tuning: Balance between preserving pre-trained knowledge and learning task-specific features
  - Parameter tuning proportion: More tuning may improve task performance but degrade brain encoding
  - Task selection: Choosing tasks that require different cognitive-linguistic skills

- Failure signatures:
  - Poor encoding correlation across all tasks may indicate issues with fMRI preprocessing or HRF convolution
  - Prompt-tuning outperforming fine-tuning on all tasks may suggest over-tuning in fine-tuning implementation
  - No significant difference between tuned and untuned models may indicate insufficient task-specific learning

- First 3 experiments:
  1. Compare encoding performance of untuned RoBERTa against a baseline to establish the effect of pre-training
  2. Run prompt-tuning on a single task (e.g., NER) and compare against fine-tuning to validate the core mechanism
  3. Test different proportions of tuned parameters in partial fine-tuning to observe the relationship with encoding performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt-tuning strategies (e.g., prefix-tuning, P-tuning, etc.) compare in their effectiveness for neural encoding across various language tasks?
- Basis in paper: [explicit] The paper mentions using P-tuning V2 as an implementation of prompt-tuning and briefly discusses other prompt-tuning methods like prefix-tuning and P-tuning, but does not directly compare different prompt-tuning strategies.
- Why unresolved: The paper focuses on comparing prompt-tuning to fine-tuning and analyzing the influence of tuned parameter proportion, but does not explore the performance differences between various prompt-tuning techniques.
- What evidence would resolve it: A comprehensive study comparing multiple prompt-tuning strategies (e.g., P-tuning, prefix-tuning, and other variants) in terms of their neural encoding performance across different language tasks and brain networks.

### Open Question 2
- Question: What is the relationship between the complexity of the NLU tasks and their effectiveness in predicting brain activation patterns for neural encoding?
- Basis in paper: [inferred] The paper shows that tasks requiring fine-grained compositional understanding of entities and concepts (e.g., KR and NER) yield better neural encoding performance, suggesting a potential relationship between task complexity and encoding effectiveness.
- Why unresolved: The paper does not explicitly analyze the correlation between task complexity and encoding performance or explore how different aspects of task complexity (e.g., number of classes, required cognitive skills) influence the results.
- What evidence would resolve it: A detailed analysis of the relationship between various measures of task complexity (e.g., number of classes, required cognitive skills, linguistic features) and neural encoding performance, potentially using regression or correlation analyses.

### Open Question 3
- Question: How does the choice of pre-trained language model architecture (e.g., Transformer, LSTM, etc.) impact the effectiveness of prompt-tuning for neural encoding?
- Basis in paper: [explicit] The paper uses RoBERTa as the pre-trained language model for prompt-tuning and fine-tuning, but does not explore the impact of different model architectures on encoding performance.
- Why unresolved: The study focuses on the comparison between prompt-tuning and fine-tuning using a single pre-trained model architecture, leaving the question of how other architectures might perform open.
- What evidence would resolve it: A comparative study using different pre-trained language model architectures (e.g., BERT, GPT, LSTM-based models) for prompt-tuning and fine-tuning, analyzing their neural encoding performance across various tasks and brain networks.

## Limitations
- The study uses a single Chinese fMRI dataset, limiting generalizability across languages and experimental paradigms
- The neural encoding methodology assumes a linear relationship between model representations and fMRI signals
- The core mechanism claiming prompt-tuning preserves pre-trained knowledge for brain alignment lacks direct empirical validation

## Confidence
- High Confidence: Prompt-tuning significantly outperforms fine-tuning in neural encoding performance
- Medium Confidence: Prompt-tuning preserves pre-trained knowledge better than fine-tuning for neural encoding
- Medium Confidence: Compositional entity and concept processing leads to better brain encoding performance
- Low Confidence: Precise relationship between tuned parameter proportion and encoding performance

## Next Checks
1. Conduct an ablation study comparing neural encoding performance when using different proportions of tuned vs. frozen parameters in fine-tuning, while keeping the total number of trained parameters constant
2. Replicate the study using English fMRI data to test whether the prompt-tuning advantage generalizes across languages
3. Examine whether prompt-tuned and fine-tuned models show different temporal encoding patterns by analyzing encoding performance at different time lags relative to stimulus onset