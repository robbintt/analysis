---
ver: rpa2
title: Deep Neural Networks Fused with Textures for Image Classification
arxiv_id: '2308.01813'
source_url: https://arxiv.org/abs/2308.01813
tags:
- image
- deep
- classification
- recognition
- patches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a two-stream deep model called DNT to address
  fine-grained image classification by fusing deep features with local textures. The
  method uses a CNN backbone to extract deep features from non-overlapping patches,
  encodes them using LSTM, and combines them with image-level LBP textures.
---

# Deep Neural Networks Fused with Textures for Image Classification

## Quick Facts
- **arXiv ID**: 2308.01813
- **Source URL**: https://arxiv.org/abs/2308.01813
- **Reference count**: 40
- **Primary result**: Proposed DNT model achieves 95.18% accuracy on ThaiFood-50 dataset, outperforming ResNet-50 baseline (91.93%)

## Executive Summary
This paper introduces DNT, a two-stream deep model that fuses local texture features with deep neural network representations for fine-grained image classification. The method combines non-overlapping patch features extracted via CNN and encoded with LSTM, with global LBP texture histograms, achieving state-of-the-art results across eight diverse datasets (1k-15k images). The fusion of texture and structural information addresses the challenge of classifying visually similar classes in fine-grained tasks.

## Method Summary
DNT uses a two-stream architecture: one stream processes 16 non-overlapping patches from CNN feature maps through global average pooling and LSTM encoding (1024-dim features), while the other computes LBP histograms with (8,1) and (8,2) neighborhoods (1024-dim texture features). These streams are concatenated and fed to a softmax classifier. The model employs random erasing augmentation (20-80% rectangular regions) and is trained with SGD for 200 epochs on datasets including faces, hands, skin lesions, food, flowers, and marine life.

## Key Results
- Achieves 95.18% accuracy on ThaiFood-50 using DenseNet-201, outperforming ResNet-50 baseline (91.93%)
- Strong performance on challenging datasets: FG-Net (54.74%) and ISIC skin cancer (81.10%)
- Consistent improvements across all eight tested datasets compared to baseline CNNs
- Ablation studies confirm effectiveness of random erasing augmentation and fusion components

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Combining local LBP textures with patch-based deep features improves fine-grained classification accuracy.
- **Mechanism**: LBP captures texture-level patterns invariant to illumination and scale; patch-based CNN extracts shape/structure details; fusion integrates complementary cues.
- **Core assumption**: Texture patterns and local structure carry distinct, class-discriminative information not fully captured by global features alone.
- **Evidence anchors**: "combining global texture with local patch-based information"; "local binary pattern (LBP) is a monotonic grayscale-invariant local descriptor"
- **Break condition**: If dataset has very low texture variance or if patches are too small to capture discriminative structure.

### Mechanism 2
- **Claim**: Random erasing data augmentation improves robustness to occlusion and local appearance variation.
- **Mechanism**: Erases random rectangular regions with pixel value 127, forcing the model to rely on multiple regions for prediction.
- **Core assumption**: Occlusions in real-world images mimic the random erasing process; forcing multi-region reliance improves generalization.
- **Evidence anchors**: "random erasing at global image-level is applied"; "height and width of IE are randomly chosen on-the-fly within [0.2, 0.8] scale, and pixels are erased with value 127"
- **Break condition**: If the dataset has uniform backgrounds or minimal occlusion; augmentation may introduce noise instead of benefit.

### Mechanism 3
- **Claim**: LSTM encoding of patch features models spatial dependencies across local regions.
- **Mechanism**: Patches are extracted from feature maps, pooled via GAP, and fed to LSTM to learn long-term spatial correlations.
- **Core assumption**: The order and spatial arrangement of patches carries class-specific information beyond individual patch content.
- **Evidence anchors**: "A single layer fully-gated LSTM [42] is applied to learn long-term dependencies via the hidden states"; "The encoded feature vector is denoted as F2"
- **Break condition**: If patch size/order does not encode meaningful spatial relationships for the task.

## Foundational Learning

**Local Binary Patterns (LBP)**
- Why needed here: LBP extracts texture features invariant to monotonic grayscale changes, complementing deep features.
- Quick check question: How does LBP encode local texture patterns, and why is it robust to illumination changes?

**Long Short-Term Memory (LSTM)**
- Why needed here: LSTM models sequential dependencies between spatially arranged patches to capture spatial context.
- Quick check question: What is the role of LSTM in encoding spatial relationships between image patches?

**Random Erasing Augmentation**
- Why needed here: Improves model robustness by simulating occlusions, encouraging reliance on multiple image regions.
- Quick check question: How does random erasing augmentation improve generalization to occlusion in fine-grained tasks?

## Architecture Onboarding

**Component map**: Input image → CNN backbone → Feature map → Non-overlapping patch pooling → GAP → LSTM encoding → LBP texture histograms → Concatenate → Softmax classifier

**Critical path**: CNN feature extraction → patch pooling → LSTM encoding → LBP computation → fusion → classification

**Design tradeoffs**: Smaller patches increase spatial detail but reduce per-patch feature richness; larger patches do the opposite

**Failure signatures**: Overfitting on small datasets (indicated by high train accuracy, low test accuracy); poor convergence (loss plateaus early)

**First 3 experiments**:
1. Baseline: Replace LSTM with global average pooling; compare accuracy
2. Ablation: Remove random erasing; observe overfitting or drop in accuracy
3. Patch size sweep: Test with 2x2, 3x3, 4x4 patches; measure impact on accuracy and runtime

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the DNT model perform on larger-scale datasets (e.g., 100k+ images) compared to small-scale datasets (1k-15k images) used in this study?
- **Basis in paper**: [inferred] The authors tested DNT on eight small-scale datasets (1k-15k images) and achieved satisfactory accuracy. They mention that they plan to explore the model's applicability on diverse datasets in the future.
- **Why unresolved**: The paper does not provide any results or discussion on the model's performance on larger-scale datasets.
- **What evidence would resolve it**: Testing the DNT model on larger-scale datasets and comparing its performance with other state-of-the-art methods.

### Open Question 2
- **Question**: How does the DNT model's performance change when using different patch sizes and LBP configurations?
- **Basis in paper**: [explicit] The authors experimented with different patch sizes (3x3 and 4x4) and LBP configurations ((8,1), (8,2), (16,1), and (16,2)) and observed variations in accuracy.
- **Why unresolved**: The paper does not provide a comprehensive analysis of the model's performance with various patch sizes and LBP configurations.
- **What evidence would resolve it**: Conducting a systematic study on the impact of different patch sizes and LBP configurations on the DNT model's performance.

### Open Question 3
- **Question**: How does the DNT model compare to other fusion methods that combine deep features and local textures for image classification?
- **Basis in paper**: [inferred] The authors claim that their method achieves better classification accuracy than existing methods, but they do not provide a direct comparison with other fusion methods.
- **Why unresolved**: The paper does not include a detailed comparison of the DNT model with other fusion methods.
- **What evidence would resolve it**: Conducting experiments to compare the DNT model's performance with other fusion methods on the same datasets and metrics.

## Limitations
- Limited statistical significance testing between methods (no p-values or confidence intervals)
- Ablation studies only cover three components, leaving other architectural choices unexamined
- All eight datasets are visually similar domains (faces, hands, food, flowers, marine life), limiting cross-domain validation

## Confidence
- **High confidence**: The core claim that fusing LBP textures with patch-based deep features improves classification accuracy is supported by consistent improvements across all eight datasets
- **Medium confidence**: The specific architectural choices (16 patches, 12×12 size, (8,1) and (8,2) LBP neighborhoods) as optimal, since no systematic parameter sweep is presented
- **Low confidence**: The generalization to datasets outside the tested domains, as all eight datasets are visually similar

## Next Checks
1. Conduct statistical significance testing (paired t-tests or Wilcoxon signed-rank) between DNT and baseline methods across all datasets to verify that performance gains are not due to random variation
2. Perform additional ablation studies removing the LSTM component entirely (replacing with simple global pooling) and testing alternative patch sizes (4×4, 8×8, 16×16) to validate the specific architectural choices
3. Test DNT on at least one dataset from a different domain (e.g., satellite imagery, medical X-rays, or industrial defect detection) to evaluate cross-domain generalization of the texture-patch fusion approach