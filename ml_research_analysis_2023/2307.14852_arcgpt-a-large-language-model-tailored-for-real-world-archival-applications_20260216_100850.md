---
ver: rpa2
title: 'ArcGPT: A Large Language Model Tailored for Real-world Archival Applications'
arxiv_id: '2307.14852'
source_url: https://arxiv.org/abs/2307.14852
tags:
- archival
- language
- llms
- tasks
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ArcGPT, the first large language model (LLM)
  specifically tailored for archival applications. The model was pre-trained on extensive
  archival domain data and evaluated on AMBLE, a new benchmark comprising four real-world
  archival tasks: retention period prediction, open-access identification, confidentiality
  prediction, and post-OCR processing.'
---

# ArcGPT: A Large Language Model Tailored for Real-world Archival Applications

## Quick Facts
- arXiv ID: 2307.14852
- Source URL: https://arxiv.org/abs/2307.14852
- Reference count: 11
- Key outcome: ArcGPT achieved F1 scores of 84.40, 84.00, and 94.40 on three classification tasks in archival applications, outperforming generic LLMs but underperforming on post-OCR processing

## Executive Summary
ArcGPT is the first large language model specifically tailored for archival applications, pre-trained on extensive archival domain data. The model was evaluated on AMBLE, a new benchmark comprising four real-world archival tasks: retention period prediction, open-access identification, confidentiality prediction, and post-OCR processing. ArcGPT demonstrated superior performance on the three classification tasks compared to existing state-of-the-art models, with F1 scores of 84.40, 84.00, and 94.40 respectively. However, its performance on the post-OCR task was lower than specialized spelling correction models, indicating room for improvement in this area.

## Method Summary
ArcGPT builds upon the BatGPT architecture (7B parameters) and underwent extensive pretraining on vast archival domain data including journals and records spanning numerous historical periods. The model was evaluated on the AMBLE benchmark containing 14,469 examples across four archival tasks. Performance was measured using precision, recall, F1 score for classification tasks, and Levenshtein Distance for the post-OCR task. The model was compared against baseline models including BERT-wwm-ext, RoBERTa-wwm-ext, ChatGLM-6B, and Chinese-LLaMA-Alpaca.

## Key Results
- ArcGPT achieved F1 scores of 84.40, 84.00, and 94.40 on retention period prediction, open-access identification, and confidentiality prediction tasks respectively
- Outperformed existing state-of-the-art models on all three classification tasks
- Underperformed specialized spelling correction models on the post-OCR processing task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific pretraining on archival data improves performance on archival tasks compared to generic LLMs.
- Mechanism: Training on domain-specific data exposes the model to specialized terminology, historical language usage, and context-specific knowledge that generic models lack, enabling better understanding of archival content.
- Core assumption: Archival domain data contains unique linguistic patterns, terminology, and contextual information that are not well-represented in general language corpora.
- Evidence anchors: [abstract]: "ArcGPT has been pre-trained on massive and extensive archival domain data" and "ArcGPT exhibited superior performance compared to existing generic LLMs"; [section]: "This comprehensive training exposed the model to historical language usage, specialized jargon, and context-specific knowledge"
- Break condition: If the archival domain data does not contain sufficiently distinct patterns from general language data, the pretraining advantage would diminish or disappear.

### Mechanism 2
- Claim: Task-specific evaluation benchmarks like AMBLE enable accurate assessment of domain-specific LLM performance.
- Mechanism: By creating tasks that reflect real-world archival challenges (retention period prediction, open-access identification, confidentiality prediction, post-OCR processing), the benchmark captures domain-specific capabilities that generic benchmarks miss.
- Core assumption: The tasks included in AMBLE represent the most critical and challenging aspects of archival work that require specialized understanding.
- Evidence anchors: [abstract]: "we release AMBLE, a benchmark comprising four real-world archival tasks"; [section]: "AMBLE encompasses four tasks integral to modern archival work: retention period prediction, open-access identification, confidentiality prediction, and post-OCR processing"
- Break condition: If the tasks selected for AMBLE do not accurately represent the full range of archival challenges, the benchmark would fail to provide comprehensive evaluation.

### Mechanism 3
- Claim: Fine-tuning pre-trained models on domain-specific data yields better performance than training from scratch for archival tasks.
- Mechanism: Leveraging existing pre-trained models (BatGPT architecture) and fine-tuning on archival data allows the model to build upon general language understanding while acquiring domain-specific knowledge more efficiently than training a new model from scratch.
- Core assumption: The foundational language understanding from pre-training transfers effectively to archival tasks, and domain-specific fine-tuning can efficiently adapt this knowledge.
- Evidence anchors: [section]: "ArcGPT inherits its foundational structure from the BatGPT architecture... underwent extensive training on vast archival domain data"; [section]: "The model's expansive training on this specialized data endows it with a robust understanding of archival language nuances"
- Break condition: If the pre-training foundation is not sufficiently aligned with archival needs, or if archival domain data is too different from the pre-training corpus, fine-tuning may be less effective than training from scratch.

## Foundational Learning

- Concept: Domain adaptation of language models
  - Why needed here: Understanding how models can be specialized for specific domains like archives requires knowledge of transfer learning and fine-tuning techniques
  - Quick check question: What is the difference between domain adaptation and training a model from scratch on domain-specific data?

- Concept: Evaluation metrics for classification tasks
  - Why needed here: The paper uses precision, recall, and F1 score to evaluate classification performance, which are fundamental metrics for understanding model effectiveness
  - Quick check question: Why might F1 score be more informative than accuracy for imbalanced classification tasks like those in archival work?

- Concept: OCR post-processing techniques
  - Why needed here: The post-OCR processing task requires understanding of how to correct errors in OCR output, which is a specialized problem in archival digitization
  - Quick check question: What are common types of errors that occur in OCR processing of historical documents?

## Architecture Onboarding

- Component map: BatGPT architecture (7B parameters) -> Domain-specific pretraining on archival data -> Task-specific fine-tuning for classification and generation tasks
- Critical path: Data collection → Domain-specific pretraining → AMBLE benchmark creation → Evaluation against baseline models → Performance analysis and iteration
- Design tradeoffs: Larger model size could improve performance but increase computational costs; more specialized pretraining could improve domain accuracy but reduce general applicability; more diverse archival data could improve robustness but complicate training
- Failure signatures: Poor performance on specific archival tasks (especially post-OCR), inability to generalize across different types of archival documents, over-specialization to Chinese archival data limiting broader applicability
- First 3 experiments:
  1. Test ArcGPT on a held-out subset of archival documents not seen during training to evaluate generalization
  2. Compare ArcGPT performance against a baseline model fine-tuned only on generic legal/administrative text (which shares some vocabulary with archives)
  3. Evaluate ArcGPT's ability to handle documents from different historical periods to assess temporal generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise impact of domain-specific pretraining on model performance across different archival tasks, and how does this compare to general-purpose pretraining?
- Basis in paper: [explicit] The paper states that ArcGPT was pre-trained on extensive archival domain data, leading to superior performance on three classification tasks compared to existing state-of-the-art models
- Why unresolved: The paper does not provide a detailed analysis of the relative impact of domain-specific pretraining versus general-purpose pretraining on each archival task
- What evidence would resolve it: Comparative studies between ArcGPT and models pre-trained on general-purpose data, evaluating their performance on each archival task individually

### Open Question 2
- Question: What are the specific challenges and limitations of applying LLMs to post-OCR processing in the archival domain, and how can these be addressed?
- Basis in paper: [explicit] The paper notes that ArcGPT's performance on the post-OCR task was lower than specialized spelling correction models, indicating room for improvement in this area
- Why unresolved: The paper does not delve into the specific challenges faced by LLMs in post-OCR processing or propose solutions to address these limitations
- What evidence would resolve it: In-depth analysis of the errors made by ArcGPT in post-OCR processing, along with proposed architectural or training modifications to improve performance

### Open Question 3
- Question: How can the AMBLE benchmark be expanded to include a broader range of archival tasks and data types, and what impact would this have on evaluating LLM performance?
- Basis in paper: [explicit] The paper introduces AMBLE as a benchmark comprising four real-world archival tasks, but acknowledges the need for further development and optimization of ArcGPT
- Why unresolved: The current AMBLE benchmark is limited in scope, and its ability to comprehensively evaluate LLM performance across diverse archival tasks is unclear
- What evidence would resolve it: Development and evaluation of AMBLE with additional archival tasks and data types, followed by comparative analysis of LLM performance on the expanded benchmark

## Limitations

- The model underperforms specialized spelling correction models on the post-OCR processing task, indicating limitations in fine-grained text correction
- The evaluation relies entirely on a single benchmark (AMBLE) created by the authors, which may not capture the full complexity of real-world archival work
- The model was trained primarily on Chinese archival data, raising questions about its generalizability to archival materials in other languages or cultural contexts

## Confidence

**High Confidence**: The claim that ArcGPT outperforms generic LLMs on classification tasks is well-supported by the presented F1 scores (84.40, 84.00, and 94.40) and the controlled comparison against multiple baseline models.

**Medium Confidence**: The claim about domain-specific pretraining being the primary driver of improved performance is plausible but not definitively proven, as the paper does not conduct ablation studies to isolate the effects of domain pretraining versus model architecture.

**Low Confidence**: The generalizability of ArcGPT to non-Chinese archival contexts and its ability to handle the full spectrum of archival challenges remain uncertain due to limited evaluation scope and lack of cross-cultural validation.

## Next Checks

1. **Cross-Cultural Validation**: Test ArcGPT on archival documents from non-Chinese contexts (e.g., Western archival materials) to assess cross-cultural generalizability and identify potential cultural or linguistic biases in the model's performance.

2. **Temporal Robustness Testing**: Evaluate ArcGPT's performance across documents from different historical periods (e.g., 18th, 19th, 20th centuries) to determine whether the model's effectiveness varies with document age and historical language usage patterns.

3. **Ablation Study**: Conduct controlled experiments comparing ArcGPT against: (a) the base BatGPT model fine-tuned only on general legal/administrative text, (b) ArcGPT trained from scratch on archival data without BatGPT initialization, and (c) ArcGPT with reduced pretraining data to quantify the specific contribution of domain pretraining versus model architecture.