---
ver: rpa2
title: Large Language Models Sensitivity to The Order of Options in Multiple-Choice
  Questions
arxiv_id: '2308.11483'
source_url: https://arxiv.org/abs/2308.11483
tags:
- llms
- options
- sensitivity
- order
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the sensitivity of large language models
  (LLMs) to the order of options in multiple-choice questions (MCQs). The authors
  demonstrate that LLMs exhibit significant performance gaps when answer options are
  reordered, even in few-shot settings.
---

# Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions

## Quick Facts
- arXiv ID: 2308.11483
- Source URL: https://arxiv.org/abs/2308.11483
- Reference count: 7
- This paper demonstrates that LLMs show significant performance gaps when answer options are reordered, even in few-shot settings.

## Executive Summary
This paper investigates the sensitivity of large language models to the order of options in multiple-choice questions (MCQs). The authors demonstrate that LLMs exhibit significant performance gaps when answer options are reordered, even in few-shot settings. They conjecture that this sensitivity arises from LLMs' uncertainty between top choices and positional bias. Through analysis, they identify patterns that amplify or mitigate this bias, recommending optimal placements of top choices to either increase or decrease bias. Two calibration approaches are proposed to improve LLM robustness, resulting in up to 8 percentage points improvement across models and benchmarks. The study highlights the need for careful consideration of option order in LLM evaluation and provides practical insights for mitigating positional bias.

## Method Summary
The paper evaluates LLM sensitivity to option order using five MCQ benchmarks (CSQA, Abstract Algebra, High School Chemistry, Professional Law, and Logical Deduction) with GPT-4 and InstructGPT models in both zero-shot and few-shot settings. The authors measure sensitivity gaps between maximum and minimum performance across different orderings, identify positional patterns that amplify or mitigate bias, and propose two calibration approaches: majority voting across random permutations and multiple evidence calibration. They conduct extensive experiments comparing baseline performance against calibrated predictions to quantify improvements.

## Key Results
- LLMs show significant performance gaps (sensitivity) when answer options are reordered, even in few-shot settings
- Two calibration approaches improve robustness by up to 8 percentage points across different models and benchmarks
- Positional bias patterns were identified: placing top-2 choices as first and last options amplifies bias, while placing them adjacently mitigates it

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit sensitivity to option order in MCQs due to uncertainty between top choices combined with positional bias.
- Mechanism: When LLMs are uncertain between the top-2/3 answer candidates, they default to selecting based on position rather than semantic correctness. The model's internal probability distribution over options is not sharp enough to overcome the learned positional preference.
- Core assumption: The uncertainty in top choices creates a "decision bottleneck" where positional information becomes the tie-breaker.
- Evidence anchors:
  - [abstract] "we conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specific options placements may favor certain prediction between those top choices depending on the question caused by positional bias"
  - [section 4.1] "We observe that despite achieving high Hits@2 and Hits@3 scores (covering all the samples where models initially predicted them correctly), LLMs' performance remains nearly unchanged or exhibits incremental improvements or declines. This observation provides further evidence of the impact of positional bias in order sensitivity."
  - [corpus] Found related work on order sensitivity in LLMs (Set-LLM paper) confirming this is a recognized phenomenon.
- Break condition: If LLMs develop sharper probability distributions or explicit confidence measures, the positional bias effect would diminish as the model would have stronger evidence for its top choice.

### Mechanism 2
- Claim: Specific positional patterns amplify or mitigate the positional bias effect.
- Mechanism: Certain arrangements of top-2 choices create stronger or weaker positional influences. When top choices are placed at extreme positions (first and last), the positional bias amplifies. When placed adjacently, the bias mitigates.
- Core assumption: The positional encoding learned during training creates predictable biases that interact with the uncertainty patterns.
- Evidence anchors:
  - [abstract] "We found that for amplifying bias, the optimal strategy involves positioning the top two choices as the first and last options. Conversely, to mitigate bias, we recommend placing these choices among the adjacent options."
  - [section 4.2] "We discover four different patterns... Pattern 1: First choice in top-2 appear earlier than the second choice in the options, and having less gap (less number of other choices) between them helps the goal more... Pattern 2: First choice in top-2 appear earlier than the second choice in the options, and having more gap between them helps the goal more."
  - [corpus] Multiple related papers investigate similar bias patterns in LLMs.
- Break condition: If the positional encoding is modified or if the model is trained with order-invariant data augmentation, these patterns would lose their effectiveness.

### Mechanism 3
- Claim: Calibration through majority voting over random permutations significantly improves robustness to order sensitivity.
- Mechanism: By aggregating predictions across multiple random orderings of options, the positional bias effects average out, leaving the model's true semantic understanding to dominate the final prediction.
- Core assumption: Positional bias effects are relatively consistent across permutations while semantic understanding varies, so averaging reduces bias impact.
- Evidence anchors:
  - [abstract] "we conduct various experiments and adopt two approaches to calibrate LLMs' predictions, leading to up to 8 percentage points improvement across different models and benchmarks."
  - [section 5] "Our analysis has unveiled a significant observation: employing a majority vote approach for evaluating LLMs across a range of benchmarks results in a substantial performance improvement of up to 8 percentage points."
  - [corpus] Related work on calibration methods for LLMs supports this approach.
- Break condition: If the positional bias is stronger than semantic understanding or if the model's uncertainty is too high across all permutations, majority voting may not sufficiently mitigate the bias.

## Foundational Learning

- Concept: Positional encoding in transformer models
  - Why needed here: Understanding how position information is encoded and used in attention mechanisms explains why option order affects predictions
  - Quick check question: How do transformer models incorporate position information, and what happens if this information is scrambled?

- Concept: Uncertainty quantification in probabilistic models
  - Why needed here: The paper's core mechanism relies on understanding when models are uncertain between top choices
  - Quick check question: What are common ways to measure model uncertainty, and how would you detect uncertainty in a model that doesn't provide confidence scores?

- Concept: In-context learning and demonstration effects
  - Why needed here: The paper tests both zero-shot and few-shot settings, requiring understanding of how demonstrations influence model behavior
  - Quick check question: How do demonstrations in few-shot learning influence model predictions compared to zero-shot settings?

## Architecture Onboarding

- Component map: MCQ dataset -> Multiple option orderings generator -> LLM inference engine -> Sensitivity gap calculator -> Pattern detection algorithm -> Calibration module (majority vote/MEC) -> Performance evaluation

- Critical path:
  1. Load MCQ dataset
  2. Generate multiple option orderings
  3. Query LLM for each ordering
  4. Analyze sensitivity gap
  5. Identify top choices and patterns
  6. Apply calibration methods
  7. Measure improvement

- Design tradeoffs:
  - Computational cost vs. thoroughness: Testing all permutations vs. random sampling
  - Model size vs. sensitivity: Larger models show less sensitivity but cost more to run
  - Calibration overhead vs. accuracy gain: Majority voting improves accuracy but increases inference time by factor of 10

- Failure signatures:
  - Low sensitivity gap but poor absolute performance: Model may be consistently wrong regardless of order
  - High sensitivity gap but good calibration: Positional bias is strong relative to semantic understanding
  - Inconsistent pattern detection: May indicate dataset issues or model instability

- First 3 experiments:
  1. Replicate zero-shot sensitivity results on a small dataset to verify the phenomenon
  2. Test positional bias patterns on a simple toy dataset where ground truth is known
  3. Implement majority voting calibration on one benchmark to verify the 8 percentage point improvement claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific training data patterns or characteristics lead to positional bias in LLMs, and how can we identify these during model development?
- Basis in paper: Explicit - The authors mention that a deeper understanding of the issue's origin necessitates a thorough exploration of the training data.
- Why unresolved: While the paper conjectures that positional bias arises from LLMs' uncertainty and training data, it does not investigate the specific training data patterns that contribute to this bias.
- What evidence would resolve it: Analysis of training corpora to identify correlations between option ordering and frequency, or experimental studies ablating specific data characteristics.

### Open Question 2
- How do different prompting strategies beyond simple reordering affect LLM sensitivity to option order, and can we develop more robust prompting techniques?
- Basis in paper: Inferred - The paper discusses sensitivity to prompt wording and demonstrates that demonstrations in few-shot settings only marginally improve robustness.
- Why unresolved: The paper focuses on option reordering but does not extensively explore alternative prompting strategies or their effectiveness in mitigating sensitivity.
- What evidence would resolve it: Comparative studies of various prompting techniques (chain-of-thought, tree-of-thought, etc.) on option order sensitivity across multiple benchmarks.

### Open Question 3
- To what extent does positional bias in MCQ tasks generalize to other structured prediction tasks with multiple choice elements, and what are the key differences?
- Basis in paper: Explicit - The authors note they detected similar phenomena in tasks like odd word detection, sorting lists, and ranking documents, but reserve further exploration for future work.
- Why unresolved: The paper acknowledges the phenomenon extends beyond MCQs but does not provide systematic investigation of these other tasks.
- What evidence would resolve it: Systematic experiments across various structured prediction tasks measuring sensitivity patterns and comparing them to MCQ findings.

## Limitations
- Results are based on GPT-4 and InstructGPT models, limiting generalizability to other architectures
- Analysis focuses on MCQs with four options, may not extend to questions with different numbers of choices
- Computational overhead of calibration methods (especially majority voting) presents practical deployment challenges

## Confidence
- High confidence in the core finding that LLMs exhibit significant sensitivity to option order
- Medium confidence in the proposed mechanisms explaining this sensitivity
- Medium confidence in the calibration approaches' effectiveness

## Next Checks
1. Cross-architecture validation: Test the sensitivity patterns on open-source LLMs (e.g., Llama, Mistral) to determine if the positional bias is universal or specific to OpenAI models.

2. Temporal stability analysis: Evaluate whether the sensitivity patterns persist across different model versions and after fine-tuning on order-invariant data augmentation to test the break conditions identified in the mechanisms.

3. Calibration cost-benefit analysis: Measure the trade-off between computational overhead and accuracy gains across different calibration methods, particularly comparing majority voting with alternative approaches like temperature scaling or ensemble methods.