---
ver: rpa2
title: On the Use of Self-Supervised Speech Representations in Spontaneous Speech
  Synthesis
arxiv_id: '2307.05132'
source_url: https://arxiv.org/abs/2307.05132
tags:
- speech
- spontaneous
- representations
- prediction
- synthesis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the use of self-supervised speech representations
  in spontaneous speech synthesis and quality prediction. It compares six different
  SSL models and three layers from each model in two-stage TTS and MOS prediction
  tasks on two spontaneous speech corpora.
---

# On the Use of Self-Supervised Speech Representations in Spontaneous Speech Synthesis

## Quick Facts
- **arXiv ID**: 2307.05132
- **Source URL**: https://arxiv.org/abs/2307.05132
- **Reference count**: 0
- **Primary result**: Layer 9 SSL representations, particularly data2vec with ASR fine-tuning, provide optimal TTS quality for spontaneous speech.

## Executive Summary
This paper investigates the use of self-supervised speech representations in spontaneous speech synthesis and quality prediction. The study evaluates six different SSL models with three layers each in two-stage TTS and MOS prediction tasks across two spontaneous speech corpora. The research identifies that layer 9 SSL representations provide the best TTS quality, with data2vec with ASR fine-tuning performing optimally. The paper also demonstrates that fine-tuning SSL-based MOS predictors on spontaneous speech data is crucial for accurate quality prediction, as zero-shot prediction from read-speech pre-trained models performs poorly.

## Method Summary
The study uses a two-stage TTS pipeline with FastPitch 1.1 as the acoustic model and HiFi-GAN as the vocoder, with SSL representations as intermediate features. Six SSL models (wav2vec 2.0, data2vec, WavLM, Whisper) are evaluated at three layers each (6, 9, 12), including ASR fine-tuned versions. The evaluation uses two spontaneous speech corpora: Trinity Speech-Gesture Dataset and ThinkComputers Corpus. TTS quality is assessed through subjective MOS listening tests following ITU P.800 standard with 44 participants per corpus. MOS prediction experiments use 5-fold cross-validation to evaluate prediction accuracy with different fine-tuning strategies.

## Key Results
- Layer 9 SSL representations provide the best TTS quality for spontaneous speech synthesis
- ASR fine-tuning consistently improves vocoding error and TTS quality
- Fine-tuning SSL-based MOS predictors on spontaneous data is essential for accurate quality prediction
- TTS subjective MOS does not correlate with vocoding loss, indicating a trade-off between acoustic information and prediction accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layer 9 of SSL models provides the best TTS quality for spontaneous speech.
- Mechanism: Middle layers of SSL models capture prosodic information that is crucial for naturalness in spontaneous speech synthesis, while avoiding the excessive acoustic detail of deeper layers that makes prediction harder.
- Core assumption: Prosodic information is more important than raw acoustic detail for perceived naturalness in spontaneous TTS.
- Evidence anchors:
  - [abstract] "The study finds that layer 9 of SSL models, particularly data2vec with ASR fine-tuning, provides the best TTS quality."
  - [section] "the deeper the layer, the greater the error, presumably because deeper layers are further removed from the original speech waveform" and "the best performing representation is data2vec-base-asr layer 9"
  - [corpus] Weak - the corpus evidence only shows similar vocoding errors across corpora but doesn't directly support the layer 9 claim.

### Mechanism 2
- Claim: ASR fine-tuning improves TTS quality compared to pre-training only.
- Mechanism: ASR fine-tuning adapts the SSL model to better preserve phonetic and linguistic information needed for accurate text-to-speech alignment and pronunciation.
- Core assumption: Phonetic and linguistic information are critical for TTS quality beyond what self-supervised pre-training provides.
- Evidence anchors:
  - [abstract] "ASR fine-tuning consistently leads to increased vocoding error over corresponding pre-trained models" and "the best performing representation is data2vec-base-asr layer 9"
  - [section] "ASR fine-tuned data2vec, the best performing SSL model in two-stage TTS, consistently exhibited one of the highest vocoding errors"
  - [corpus] Weak - corpus evidence shows similar trends but doesn't directly address why ASR fine-tuning helps.

### Mechanism 3
- Claim: SSL-based MOS prediction requires fine-tuning on spontaneous data to be effective.
- Mechanism: Pre-trained SSL models capture general speech characteristics but need domain adaptation to understand the specific acoustic and prosodic patterns of spontaneous speech synthesis.
- Core assumption: Spontaneous speech has unique characteristics that differ significantly from read speech, requiring specialized training for accurate quality prediction.
- Evidence anchors:
  - [abstract] "The paper also demonstrates that fine-tuning SSL-based MOS predictors on spontaneous speech data is crucial for accurate quality prediction, as zero-shot prediction from read-speech pre-trained models performs poorly."
  - [section] "zero-shot prediction from a read-speech pre-trained SSL MOS predictor performs poorly, and that fine-tuning on spontaneous MOS data is crucial"
  - [corpus] Weak - the corpus evidence doesn't directly support this claim about prediction performance.

## Foundational Learning

- **Concept**: Self-supervised learning in speech
  - Why needed here: SSL provides rich speech representations without requiring transcriptions, which is crucial for spontaneous speech that contains phenomena difficult to transcribe.
  - Quick check question: What is the key advantage of SSL representations over traditional features like mel-spectrograms for spontaneous speech synthesis?

- **Concept**: Two-stage TTS architecture
  - Why needed here: Understanding how SSL representations fit between the acoustic model and vocoder is essential for interpreting the results and designing experiments.
  - Quick check question: In a two-stage TTS system, what role does the intermediate representation play between the acoustic model and vocoder?

- **Concept**: Mean Opinion Score (MOS) prediction
  - Why needed here: MOS prediction using SSL models is a key application explored in this paper, requiring understanding of how to adapt SSL models for quality assessment.
  - Quick check question: Why is fine-tuning necessary for SSL-based MOS predictors when moving from read speech to spontaneous speech synthesis?

## Architecture Onboarding

- **Component map**: SSL model (6, 9, or 12th layer) → FastPitch acoustic model → HiFi-GAN vocoder → audio output; SSL model also used for MOS prediction with pooling head and linear projection
- **Critical path**: Text → FastPitch → SSL representation extraction → HiFi-GAN → synthesized audio; audio → SSL model → pooling/linear → MOS score
- **Design tradeoffs**: Layer choice balances prosodic information vs. acoustic detail; ASR fine-tuning trades vocoding accuracy for better TTS alignment; fine-tuning MOS predictors improves accuracy but requires labeled spontaneous data
- **Failure signatures**: Poor vocoding error suggests loss of acoustic information; low MOS scores indicate quality issues; high MOS prediction MSE indicates poor generalization to spontaneous data
- **First 3 experiments**:
  1. Copy synthesis with different SSL layers to measure vocoding error and identify the layer with best balance of information and predictability
  2. Subjective MOS listening test comparing all 18 systems to determine optimal SSL model and layer for spontaneous TTS
  3. MOS prediction experiments with different fine-tuning strategies to establish the importance of spontaneous data for quality prediction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific trade-offs exist between the amount of acoustic information in SSL representations and their prediction accuracy from text in TTS systems?
- Basis in paper: [explicit] The paper discusses a trade-off between acoustic information and prediction accuracy in two-stage TTS, where high-performing TTS representations often exhibit worse vocoding loss, and vice versa.
- Why unresolved: The paper provides evidence of this trade-off but does not delve into the underlying reasons or specific characteristics of SSL representations that contribute to this phenomenon.
- What evidence would resolve it: Detailed analysis of SSL representations' properties, such as their ability to capture prosodic information versus acoustic details, and how these properties affect TTS performance.

### Open Question 2
- Question: How do SSL models perform in synthesizing spontaneous speech with phenomena not typically transcribed, such as tongue clicks or other nonverbal sounds?
- Basis in paper: [inferred] The paper mentions that spontaneous speech includes phenomena like tongue clicks that are not part of the transcription, but does not explore how well SSL models handle these in synthesis.
- Why unresolved: The study focuses on more common spontaneous speech phenomena like breathing and filled pauses, leaving the handling of less common phenomena unexplored.
- What evidence would resolve it: Experiments synthesizing spontaneous speech with various nonverbal sounds and evaluating the naturalness and intelligibility of the output.

### Open Question 3
- Question: What are the long-term trends in SSL model performance as spontaneous TTS technology continues to evolve?
- Basis in paper: [inferred] The paper provides a snapshot of current SSL model performance in spontaneous TTS but does not discuss future trends or potential improvements.
- Why unresolved: The rapid development of SSL and TTS technologies means that current findings may not hold as new models and techniques emerge.
- What evidence would resolve it: Longitudinal studies tracking the performance of SSL models in spontaneous TTS over time, comparing new models and techniques as they become available.

## Limitations
- Exclusive focus on two-stage TTS architectures, limiting generalizability to other TTS approaches
- Limited evaluation to only two spontaneous speech corpora, potentially missing domain-specific variations
- Absence of direct comparison with traditional acoustic features like mel-spectrograms in the TTS pipeline

## Confidence
- High confidence in layer 9 SSL representations being optimal for spontaneous TTS quality
- Medium confidence in ASR fine-tuning improving TTS quality
- Medium confidence in the necessity of fine-tuning SSL-based MOS predictors on spontaneous data
- Low confidence in broader applicability beyond tested two-stage pipeline

## Next Checks
1. **Cross-architecture validation**: Test whether the optimal SSL layer (layer 9) and ASR fine-tuning benefits transfer to single-stage TTS architectures like Glow-TTS or VITS, which may have different requirements for intermediate representations.

2. **Traditional feature comparison**: Implement and evaluate mel-spectrogram-based systems alongside SSL representations in the same two-stage pipeline to quantify the actual performance gain from self-supervised features in spontaneous TTS.

3. **Domain generalization study**: Evaluate the pre-trained MOS predictors on spontaneous speech from different domains (e.g., interviews, conversational speech) to assess whether the fine-tuning requirement is domain-specific or represents a general principle for spontaneous speech quality prediction.