---
ver: rpa2
title: 'GenKIE: Robust Generative Multimodal Document Key Information Extraction'
arxiv_id: '2310.16131'
source_url: https://arxiv.org/abs/2310.16131
tags:
- entity
- genkie
- prompt
- extraction
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GenKIE, a generative multimodal model for
  key information extraction (KIE) from scanned documents. Unlike discriminative models
  that rely on OCR and token-level labels, GenKIE uses a sequence-to-sequence architecture
  with multimodal encoders (text, layout, visual) and a decoder that generates key-value
  pairs via prompts.
---

# GenKIE: Robust Generative Multimodal Document Key Information Extraction

## Quick Facts
- arXiv ID: 2310.16131
- Source URL: https://arxiv.org/abs/2310.16131
- Reference count: 15
- Key outcome: Introduces GenKIE, a generative multimodal model for key information extraction from scanned documents that achieves state-of-the-art performance and demonstrates strong robustness against OCR errors.

## Executive Summary
This paper introduces GenKIE, a generative multimodal model for key information extraction (KIE) from scanned documents. Unlike discriminative models that rely on OCR and token-level labels, GenKIE uses a sequence-to-sequence architecture with multimodal encoders (text, layout, visual) and a decoder that generates key-value pairs via prompts. The prompt-based approach enables automatic OCR error correction and reduces the need for fine-grained annotation. Experiments on three public datasets (SROIE, FUNSD, CORD) show that GenKIE achieves state-of-the-art performance and demonstrates strong robustness against OCR errors, with minimal performance drop under high noise levels. The model generalizes well across diverse document types and layouts.

## Method Summary
GenKIE is a sequence-to-sequence generative model for document key information extraction that uses multimodal inputs (text, layout, visual) and generates key-value pairs through prompt-based decoding. The model employs an OFA backbone with 6-layer Transformer encoder and 6-layer decoder, trained with 50 epochs at batch size 64 and learning rate 5e-5. Input features include OCR-extracted text tokens with BPE tokenization and 1D positional encoding, normalized bounding box coordinates with 2D positional encoding for layout, and ResNet visual patches with 1D positional encoding. The model uses two prompt formats (template and question) and employs prefix beam search during inference. Evaluation is conducted on SROIE (receipts), FUNSD (forms), and CORD (receipts) datasets with F1 score at token level.

## Key Results
- Achieves state-of-the-art performance on SROIE, FUNSD, and CORD datasets for key information extraction
- Demonstrates strong robustness against OCR errors with minimal performance degradation under high noise levels
- Shows effective generalization across diverse document types including receipts and forms
- Reduces annotation burden through prompt-based generation without requiring token-level labels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The sequence-to-sequence architecture enables automatic OCR error correction during generation.
- Mechanism: By generating the entity values directly rather than classifying tags, the decoder can reconstruct correct tokens from noisy OCR input, effectively "repairing" errors in the output sequence.
- Core assumption: The decoder's generative capability is sufficiently strong to infer correct values even when OCR introduces errors.
- Evidence anchors:
  - [abstract]: "One notable advantage of the generative model is that it enables automatic correction of OCR errors."
  - [section 1]: "Although the classification-based model tags the entities correctly, the result is still wrong due to the OCR errors... GenKIE auto-corrects OCR errors and generates the correct entity values."
  - [corpus]: Weak - corpus contains no direct evidence of OCR correction effectiveness.

### Mechanism 2
- Claim: Multimodal embeddings capture semantic ambiguity that text-only models miss.
- Mechanism: By combining textual, layout, and visual features in the encoder, the model can distinguish entities with identical text but different meanings based on their spatial context and visual appearance.
- Core assumption: Layout and visual features contain discriminative information for entity type classification beyond what text provides.
- Evidence anchors:
  - [abstract]: "effective incorporation of multimodal features is indispensable to improve the model performance for the KIE task."
  - [section 4.1.2]: "GenKIE uses a layout embedding layer to capture the spatial context of text segments."
  - [corpus]: Weak - corpus papers focus on different aspects (e.g., assignment optimization, graph networks) without directly validating multimodal disambiguation.

### Mechanism 3
- Claim: Prompt-based generation reduces annotation burden compared to token-level labeling.
- Mechanism: The model learns to generate entity values based on high-level prompts rather than requiring detailed BIO tag annotations for each token.
- Core assumption: The prompts provide sufficient semantic guidance for the model to learn the correct generation patterns without granular supervision.
- Evidence anchors:
  - [abstract]: "token-level granular annotation is not required."
  - [section 4.2]: "Thanks to the generation capability, GenKIE does not need the laborious granular token-level labelling that is usually required by discriminative models."
  - [corpus]: Weak - corpus contains no direct comparison of annotation effort between generative and discriminative approaches.

## Foundational Learning

- Concept: Sequence-to-sequence generation
  - Why needed here: GenKIE needs to generate entity values as output sequences conditioned on document inputs
  - Quick check question: What is the key difference between sequence-to-sequence generation and classification in the context of KIE?

- Concept: Multimodal feature fusion
  - Why needed here: Document understanding requires combining text, layout, and visual information to resolve ambiguities
  - Quick check question: How does GenKIE combine multimodal features in its encoder architecture?

- Concept: Prompt engineering
  - Why needed here: Prompts guide the decoder to generate specific entity types and values in the correct format
  - Quick check question: What are the two types of prompts used in GenKIE and when would each be appropriate?

## Architecture Onboarding

- Component map: Image → OCR → Multimodal embedding → Encoder → Decoder → Prompt-based generation → Entity extraction

- Critical path: Document image → OCR text + layout features + resized image → Multimodal embedding (text + layout + visual) → 6-layer Transformer encoder → 6-layer Transformer decoder → Autoregressive generation with prompt conditioning → Generated entity key-value pairs

- Design tradeoffs:
  - Generative vs discriminative: GenKIE trades classification accuracy for OCR robustness and reduced annotation
  - Multimodal complexity: Adding visual/layout features increases model size but improves disambiguation
  - Prompt design: Natural language prompts are more flexible but require manual engineering

- Failure signatures:
  - Low F1 on OCR-error tests: Indicates decoder struggles with noisy input
  - Poor performance on certain entity types: May indicate prompt inadequacy or multimodal feature insufficiency
  - Training instability: Could indicate issues with multimodal fusion or prompt formulation

- First 3 experiments:
  1. Run entity extraction on SROIE with clean OCR to establish baseline performance
  2. Introduce controlled OCR errors and measure F1 drop to test robustness
  3. Compare template vs question prompts on FUNSD to evaluate prompt effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GenKIE's performance compare to other generative models for document KIE beyond TILT?
- Basis in paper: [explicit] The paper mentions TILT as a related generative KIE model but only compares GenKIE to discriminative models in experiments.
- Why unresolved: The paper focuses on comparing GenKIE to discriminative models and only briefly mentions TILT. No direct comparison between GenKIE and TILT is provided.
- What evidence would resolve it: Conducting experiments comparing GenKIE and TILT on the same datasets would provide insights into their relative performance.

### Open Question 2
- Question: What is the impact of prompt design on GenKIE's performance for different document types and entity extraction tasks?
- Basis in paper: [inferred] The paper discusses prompt design but does not extensively explore its impact on different document types or entity extraction tasks.
- Why unresolved: The paper mentions the importance of prompts but does not investigate how different prompt designs affect performance across various document types or entity extraction tasks.
- What evidence would resolve it: Conducting experiments with different prompt designs for various document types and entity extraction tasks would reveal the impact of prompt design on GenKIE's performance.

### Open Question 3
- Question: How does GenKIE's robustness against OCR errors compare to other methods that handle OCR errors explicitly?
- Basis in paper: [explicit] The paper highlights GenKIE's robustness against OCR errors but does not compare it to other methods that explicitly handle OCR errors.
- Why unresolved: The paper demonstrates GenKIE's robustness against OCR errors but does not provide a direct comparison to other methods that explicitly address OCR errors.
- What evidence would resolve it: Comparing GenKIE's performance against other methods that explicitly handle OCR errors would provide insights into its relative robustness.

### Open Question 4
- Question: What are the limitations of GenKIE's zero-shot and few-shot learning capabilities for entity extraction?
- Basis in paper: [explicit] The paper mentions zero-shot and few-shot experiments but does not extensively explore the limitations of these capabilities.
- Why unresolved: The paper briefly discusses zero-shot and few-shot experiments but does not delve into the limitations of these capabilities for entity extraction.
- What evidence would resolve it: Conducting experiments to identify the limitations of GenKIE's zero-shot and few-shot learning capabilities for entity extraction would provide insights into its generalization performance.

### Open Question 5
- Question: How does GenKIE's performance scale with increasing document complexity and layout variations?
- Basis in paper: [inferred] The paper mentions GenKIE's ability to generalize to different document types and layouts but does not extensively explore its performance scaling with increasing complexity.
- Why unresolved: The paper highlights GenKIE's generalization capabilities but does not investigate how its performance scales with increasing document complexity and layout variations.
- What evidence would resolve it: Conducting experiments with increasingly complex documents and diverse layouts would reveal how GenKIE's performance scales with document complexity.

## Limitations

- Critical dependency on OCR quality without specifying the OCR tool used, making it unclear whether robustness comes from the model or OCR system
- Lack of systematic prompt engineering analysis and guidelines for optimal prompt design
- No ablation studies quantifying individual contributions of text, layout, and visual features

## Confidence

**High Confidence Claims**:
- The sequence-to-sequence architecture is technically sound for KIE
- Multimodal features can theoretically improve document understanding
- Generative models can avoid token-level labeling requirements

**Medium Confidence Claims**:
- The model achieves SOTA performance on the three tested datasets
- OCR error correction occurs during generation
- Prompt-based approach reduces annotation burden in practice

**Low Confidence Claims**:
- Multimodal features are essential for entity disambiguation (no ablation provided)
- The magnitude of OCR error correction benefit
- Generalizability beyond the three tested document types

## Next Checks

1. **Ablation Study**: Remove layout and visual features individually and measure performance impact on all three datasets to quantify the actual contribution of multimodal features versus text-only approaches.

2. **Prompt Sensitivity Analysis**: Systematically vary prompt formulations (different templates, question styles, prompt lengths) and measure impact on entity extraction performance to identify optimal prompt design principles.

3. **OCR Error Simulation**: Generate controlled OCR errors (insertions, deletions, substitutions) at varying noise levels and measure the actual error correction rate to validate the claimed automatic correction mechanism.