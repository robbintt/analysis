---
ver: rpa2
title: Reasoning with Language Model is Planning with World Model
arxiv_id: '2305.14992'
source_url: https://arxiv.org/abs/2305.14992
tags:
- reasoning
- arxiv
- state
- planning
- world
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel framework called Reasoning via Planning
  (RAP) that repurposes large language models (LLMs) to reason more strategically,
  similar to human-like planning. RAP combines a world model (repurposed from the
  LLM) with a principled planning algorithm (based on Monte Carlo Tree Search) to
  efficiently explore the vast reasoning space and find high-reward reasoning paths.
---

# Reasoning with Language Model is Planning with World Model

## Quick Facts
- arXiv ID: 2305.14992
- Source URL: https://arxiv.org/abs/2305.14992
- Reference count: 40
- Key outcome: RAP achieves 64% success rate on plan generation vs 1% for Chain-of-Thought, and 51.6% accuracy on math reasoning vs 46.8% for Chain-of-Thought with self-consistency

## Executive Summary
This paper introduces Reasoning via Planning (RAP), a framework that repurposes large language models (LLMs) as both world models and reasoning agents, combining them with Monte Carlo Tree Search (MCTS) for strategic exploration. RAP frames reasoning as a Markov Decision Process where the LLM simulates states, anticipates action outcomes, and maintains balance between exploration and exploitation. The framework significantly outperforms strong baselines like Chain-of-Thought and least-to-most prompting with self-consistency across plan generation, math reasoning, and logical inference tasks.

## Method Summary
RAP repurposes LLMs as both a world model and reasoning agent, using MCTS to explore reasoning paths. The LLM generates state transitions and actions iteratively, building a reasoning tree where each node represents a state and edges represent actions. MCTS selects promising nodes using the UCT formula balancing Q-values (exploitation) with visit counts (exploration). For answer-only tasks, RAP-Aggregation improves performance by selecting the answer with highest accumulated reward across its reasoning steps. The framework is evaluated on Blocksworld plan generation, GSM8K math problems, and PrOntoQA logical reasoning.

## Key Results
- On Blocksworld plan generation: RAP achieves 64% success rate vs 1% for Chain-of-Thought
- On GSM8K math reasoning: RAP achieves 51.6% accuracy vs 46.8% for Chain-of-Thought with self-consistency
- RAP outperforms or matches GPT-4 in certain settings, demonstrating strong performance across reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
RAP enables LLMs to simulate future states and anticipate action outcomes by repurposing the LLM as both a world model and reasoning agent. The LLM generates state transitions and actions iteratively, building a reasoning tree where each node represents a state and edges represent actions. This simulates deliberate planning akin to human mental models. The core assumption is that the LLM can be prompted to accurately predict next states given current states and actions.

### Mechanism 2
MCTS-based planning balances exploration and exploitation to efficiently find high-reward reasoning paths. MCTS iteratively expands the reasoning tree, selecting promising nodes using UCT formula that balances Q-values (exploitation) with visit counts (exploration). Rewards are backpropagated to update beliefs about reasoning steps. The core assumption is that the reward function accurately reflects the quality of reasoning steps.

### Mechanism 3
RAP-Aggregation improves performance by aggregating multiple reasoning outputs weighted by accumulated rewards. Multiple reasoning traces from different MCTS iterations are collected, and the answer with highest accumulated reward across its reasoning steps is selected as final output. The core assumption is that high-reward reasoning traces are more likely to contain correct reasoning steps.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: RAP frames reasoning as an MDP where states represent world configurations and actions represent reasoning steps
  - Quick check question: What are the four components of an MDP that RAP must define for a given reasoning task?

- Concept: Monte Carlo Tree Search (MCTS)
  - Why needed here: MCTS provides the principled planning algorithm that explores the vast reasoning space efficiently
  - Quick check question: What are the four phases of MCTS and what does each accomplish in RAP's reasoning process?

- Concept: Upper Confidence bounds applied to Trees (UCT)
  - Why needed here: UCT formula balances exploration vs exploitation when selecting nodes in the reasoning tree
  - Quick check question: How does the UCT formula trade off between exploiting known good actions and exploring less-visited actions?

## Architecture Onboarding

- Component map: LLM (reasoning agent) -> LLM (world model) -> MCTS planner -> Reward function -> RAP-Aggregation
- Critical path: State generation → Action generation → Next state prediction → Reward evaluation → MCTS tree update → Path selection
- Design tradeoffs:
  - Depth vs breadth in MCTS: More depth allows longer reasoning chains but increases computation
  - Reward design: More sophisticated rewards may better guide reasoning but increase complexity
  - Single vs multiple outputs: RAP-Aggregation helps answer-only tasks but not proof-required tasks
- Failure signatures:
  - Tree gets stuck in local optima: Indicates poor reward design or insufficient exploration
  - Inconsistent state transitions: Suggests LLM world model is unreliable
  - Degradation with more iterations: May indicate overfitting to early reasoning steps
- First 3 experiments:
  1. Validate world model: Test LLM's ability to predict next states for simple reasoning tasks
  2. Test MCTS basics: Apply MCTS to a toy problem with known optimal solution
  3. Compare with baselines: Run RAP vs Chain-of-Thought on a small dataset to verify improvements

## Open Questions the Paper Calls Out
- How does the choice of exploration weight (w) in UCT affect the balance between exploration and exploitation during planning, and what is the optimal value?
- How does RAP handle reward hacking where the LLM exploits the reward function without solving the problem?
- How does RAP handle reasoning trace length and what is the impact of trace length on performance?

## Limitations
- Requires substantial computational resources due to MCTS iterations
- Performance depends heavily on careful prompt engineering for different tasks
- Generalization to other reasoning domains beyond tested benchmarks remains uncertain

## Confidence
- High confidence in the core framework design for structured reasoning tasks where planning is beneficial
- Medium confidence in scalability claims and ability to handle complex reasoning problems
- Low confidence in generalizability to domains requiring extensive factual knowledge

## Next Checks
1. Test RAP on a reasoning task requiring longer reasoning chains (10+ steps) to evaluate performance degradation and computational cost scaling
2. Implement an ablation study comparing RAP with and without the world model component to quantify the contribution of state simulation to performance improvements
3. Evaluate RAP's robustness by introducing noisy or incorrect intermediate reasoning steps and measuring its ability to recover or identify errors