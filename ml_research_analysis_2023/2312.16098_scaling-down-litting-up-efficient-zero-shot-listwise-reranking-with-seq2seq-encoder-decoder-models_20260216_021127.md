---
ver: rpa2
title: 'Scaling Down, LiTting Up: Efficient Zero-Shot Listwise Reranking with Seq2seq
  Encoder-Decoder Models'
arxiv_id: '2312.16098'
source_url: https://arxiv.org/abs/2312.16098
tags:
- reranking
- lit5-distill
- lit5-score
- passages
- passage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two methods, LiT5-Distill and LiT5-Score,
  for efficient zero-shot listwise reranking using T5 sequence-to-sequence encoder-decoder
  models. LiT5-Distill distills ranking orderings from larger models to smaller models,
  while LiT5-Score uses cross-attention scores from the model to calculate relevance
  scores for reranking.
---

# Scaling Down, LiTting Up: Efficient Zero-Shot Listwise Reranking with Seq2seq Encoder-Decoder Models

## Quick Facts
- **arXiv ID**: 2312.16098
- **Source URL**: https://arxiv.org/abs/2312.16098
- **Reference count**: 9
- **Primary result**: Smaller T5 models can achieve competitive zero-shot reranking effectiveness through knowledge distillation and cross-attention scoring

## Executive Summary
This paper introduces two methods, LiT5-Distill and LiT5-Score, for efficient zero-shot listwise reranking using T5 sequence-to-sequence encoder-decoder models. LiT5-Distill transfers ranking knowledge from larger models to smaller ones through knowledge distillation, while LiT5-Score uses cross-attention scores to measure passage relevance without requiring external labels. Both methods achieve strong reranking effectiveness, with LiT5-Distill competitive with state-of-the-art LLM rerankers despite using significantly fewer parameters. The results demonstrate that smaller models can deliver competitive reranking performance, challenging the necessity of large-scale models for effective zero-shot reranking.

## Method Summary
The paper presents two approaches for zero-shot listwise reranking using T5 encoder-decoder models. LiT5-Distill employs knowledge distillation to transfer ranking orderings from larger models (RankGPT3.5 and RankGPT4) to smaller T5 models by training them to mimic these rankings in a sequence-to-sequence format. LiT5-Score uses cross-attention scores from the decoder to calculate relevance scores for reranking without requiring external relevance labels. Both methods leverage the Fusion-in-Decoder architecture to encode passages separately while maintaining computational efficiency. The models are evaluated on MS MARCO DL19-22 and BEIR collections, demonstrating competitive effectiveness compared to larger models.

## Key Results
- LiT5-Distill achieves strong reranking effectiveness competitive with recent state-of-the-art LLM rerankers while using significantly fewer parameters
- LiT5-Score shows particularly strong effectiveness for question-answering retrieval tasks
- The FiD architecture enables efficient listwise reranking with linear scaling in the number of passages
- Both methods demonstrate that smaller models can achieve competitive zero-shot reranking results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LiT5-Distill transfers ranking effectiveness from larger LLMs to smaller T5 models through knowledge distillation
- Mechanism: The model learns to mimic ranking orderings produced by RankGPT3.5 and RankGPT4 by training on their output rankings as supervision
- Core assumption: Larger model rankings capture meaningful relevance signals transferable to smaller models
- Evidence anchors: Abstract states LiT5-Distill achieves competitive effectiveness despite fewer parameters; section 2.2 describes the distillation approach
- Break condition: If larger model rankings are not meaningful or consistent, distilled models will fail to learn useful patterns

### Mechanism 2
- Claim: LiT5-Score uses cross-attention scores to measure passage relevance without external labels
- Mechanism: During generation, the model computes relevance scores by aggregating attention weights across decoder layers and heads
- Core assumption: Cross-attention scores accurately reflect passage importance for answering queries
- Evidence anchors: Abstract mentions eliminating reliance on external labels; section 2.3 describes using cross-attention scores for reranking
- Break condition: If cross-attention scores don't correlate with actual relevance, reranking will be ineffective

### Mechanism 3
- Claim: FiD architecture enables efficient listwise reranking through separate encoding and joint attention
- Mechanism: Passages are encoded independently (linear scaling) then decoder attends to all representations simultaneously
- Core assumption: Separate encoding followed by joint attention provides sufficient context for accurate ranking
- Evidence anchors: Section 2.1 describes FiD's efficiency for retrieval-augmented generation; section 3.1 mentions separate processing of passages
- Break condition: If separate encoding loses critical inter-passage relationships, ranking quality degrades

## Foundational Learning

- **Knowledge distillation**: Understanding how to transfer knowledge from larger teacher models to smaller student models is crucial for LiT5-Distill
  - Quick check question: What are the key differences between knowledge distillation and standard supervised learning?

- **Cross-attention mechanisms**: LiT5-Score relies on understanding how cross-attention scores can measure passage relevance
  - Quick check question: How do cross-attention scores differ from self-attention scores in transformer models?

- **Sequence-to-sequence modeling**: Both LiT5 methods use T5's encoder-decoder architecture, requiring understanding of sequence generation
  - Quick check question: What are the main differences between encoder-only, decoder-only, and encoder-decoder transformer architectures?

## Architecture Onboarding

- **Component map**: Encoder -> Separate passage encoding -> Concatenation -> Decoder (attends to all passages) -> Output generation
- **Critical path**: 1) Input processing (split query/passages, tokenize) 2) Separate encoding (each query-passage pair) 3) Concatenation (combine representations) 4) Decoder processing (attend to all passages, generate output) 5) Post-processing (parse ranking/calculate scores)
- **Design tradeoffs**: Separate encoding vs joint encoding (linear vs quadratic scaling); window size selection (smaller reduces computation but may miss dependencies); synthetic rankings vs human judgments (affects generalization); model size vs effectiveness (larger models better but costlier)
- **Failure signatures**: Poor ranking quality (attention mechanism or training data issues); slow inference (inefficient encoding or excessive decoder computation); inconsistent rankings (distillation process or architecture problems); overfitting (too many epochs or insufficient regularization)
- **First 3 experiments**: 1) Test baseline ranking with different window sizes (5, 10, 20) to find optimal balance 2) Compare LiT5-Distill with different teacher models (RankGPT3.5 vs RankGPT4) to validate distillation 3) Evaluate LiT5-Score with different cross-attention aggregation methods to optimize relevance scoring

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LiT5-Score's effectiveness change when using different aggregation methods for cross-attention scores beyond averaging?
- Basis in paper: The paper mentions using averaging of attention scores across tokens, layers, and heads to calculate relevance scores, but suggests this could be improved
- Why unresolved: The paper does not explore alternative aggregation methods for cross-attention scores
- What evidence would resolve it: Experiments comparing LiT5-Score's effectiveness using different aggregation methods (e.g., max pooling, weighted averaging) for cross-attention scores

### Open Question 2
- Question: What is the impact of increasing the number of input passages on LiT5-Score's effectiveness for question-answering retrieval tasks?
- Basis in paper: The paper mentions that LiT5-Score models were trained using 100 input passages, matching previous work on FiD models for open-domain question-answering
- Why unresolved: The paper does not explore the impact of using more than 100 input passages on LiT5-Score's effectiveness
- What evidence would resolve it: Experiments comparing LiT5-Score's effectiveness using different numbers of input passages (e.g., 50, 100, 200) for question-answering retrieval tasks

### Open Question 3
- Question: How does LiT5-Score's effectiveness change when using different pre-trained weights (e.g., FLAN-T5) instead of T5 1.1 LM-Adapted weights?
- Basis in paper: The paper mentions that models were initialized with T5 1.1 LM-Adapted weights instead of the superior FLAN-T5 weights to maintain the zero-shot claim of this work
- Why unresolved: The paper does not explore the impact of using different pre-trained weights on LiT5-Score's effectiveness
- What evidence would resolve it: Experiments comparing LiT5-Score's effectiveness using different pre-trained weights (e.g., FLAN-T5, T5 1.1 LM-Adapted) while maintaining the zero-shot claim

## Limitations

- The effectiveness of knowledge distillation heavily depends on the quality of teacher model rankings, which is not fully characterized
- LiT5-Score's effectiveness for retrieval tasks beyond question-answering remains unexplored
- Computational efficiency claims lack detailed runtime comparisons with alternative approaches

## Confidence

- **High confidence**: LiT5-Distill achieves competitive effectiveness with smaller models compared to larger LLM rerankers
- **Medium confidence**: FiD architecture scalability claims for efficient listwise reranking
- **Medium confidence**: Cross-attention scores effectiveness for relevance measurement in LiT5-Score

## Next Checks

1. **Teacher model sensitivity analysis**: Quantify how variations in RankGPT3.5 and RankGPT4 output rankings affect distilled model performance, including cases of disagreement or errors

2. **Cross-domain generalization test**: Evaluate both LiT5-Distill and LiT5-Score methods on retrieval tasks outside MS MARCO and BEIR domains (e.g., legal documents, scientific literature) to assess robustness to domain shifts

3. **Computational efficiency benchmarking**: Measure actual inference time and memory usage of LiT5 methods against baselines (ListT5, FIRST) across different hardware configurations and passage window sizes to validate efficiency claims