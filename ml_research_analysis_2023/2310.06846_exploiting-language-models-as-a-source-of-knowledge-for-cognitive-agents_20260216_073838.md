---
ver: rpa2
title: Exploiting Language Models as a Source of Knowledge for Cognitive Agents
arxiv_id: '2310.06846'
source_url: https://arxiv.org/abs/2310.06846
tags:
- knowledge
- agent
- task
- cognitive
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Cognitive agents face challenges in acquiring and integrating new
  task knowledge, limiting their scalability and effectiveness. To address this, we
  explore integrating large language models (LLMs) with cognitive architectures to
  leverage LLMs as a source of task knowledge.
---

# Exploiting Language Models as a Source of Knowledge for Cognitive Agents

## Quick Facts
- arXiv ID: 2310.06846
- Source URL: https://arxiv.org/abs/2310.06846
- Authors: 
- Reference count: 8
- Over 70% of LLM responses are not viable for embodied agents, highlighting need for evaluation

## Executive Summary
This paper addresses the challenge of knowledge acquisition for cognitive agents by exploring the integration of large language models (LLMs) with cognitive architectures. The authors propose a direct extraction approach where agents interact with LLMs to acquire task knowledge, overcoming issues of relevance and interpretation through template-based prompting and knowledge-driven evaluation. Their method significantly reduces the need for human oversight and improves task learning, with experimental results showing that over 70% of LLM responses are not viable for embodied agents without proper evaluation.

## Method Summary
The approach integrates LLMs with cognitive architectures through a direct extraction method using template-based prompting with few-shot examples. Agents formulate context-specific prompts that leverage their current situation to query LLMs for relevant knowledge. The extracted responses undergo verification through the agent's internal cognitive capabilities, including simulation and evaluation against requirements for interpretability, groundability, compatibility with affordances, and alignment with human expectations. Verified knowledge is then encoded into the agent's memory systems, enabling improved task learning with reduced human oversight.

## Key Results
- Over 70% of LLM responses are not viable for embodied agents without proper evaluation
- Template-based prompting with few-shot examples improves response relevance and compatibility
- Knowledge verification through internal simulation significantly reduces human oversight requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct extraction from LLMs enables cognitive agents to acquire task knowledge with improved precision compared to indirect extraction methods.
- Mechanism: By embedding the agent's specific situational context and knowledge gaps into the prompt templates, the LLM's response is guided toward more relevant and actionable knowledge.
- Core assumption: The agent can formulate precise prompts that leverage its current situation to elicit more relevant responses from the LLM.
- Evidence anchors:
  - [abstract] "Our approach employs template-based prompting and knowledge-driven evaluation to extract actionable task knowledge from LLMs."
  - [section] "We hypothesize that direct extraction enables the agent to use its specific situation and context (including its embodiment) to query the LLM resulting in improved precision (relevance to the specific situation)."
  - [corpus] Weak evidence; related work exists but lacks direct comparison studies cited in this paper.
- Break condition: If the agent cannot accurately assess its knowledge gaps or formulate appropriate prompts, the precision improvement may not materialize.

### Mechanism 2
- Claim: Integration of verification processes within the cognitive architecture improves the reliability of extracted knowledge from LLMs.
- Mechanism: The agent uses its internal knowledge representations and simulation capabilities to evaluate LLM responses for interpretability, groundability, compatibility with affordances, and alignment with human expectations before internalizing them.
- Core assumption: The agent's internal cognitive capabilities are sufficiently robust to identify and filter out unreliable or incompatible LLM responses.
- Evidence anchors:
  - [abstract] "Our approach employs... knowledge-driven evaluation to extract actionable task knowledge from LLMs."
  - [section] "Because the results from the LLM are not necessarily accurate and reliable... the agent evaluates, tests, and attempts to verify extracted results from the LLM."
  - [corpus] Limited; the paper cites their own work on verification but external validation is sparse.
- Break condition: If the verification process is too conservative, it may reject useful knowledge; if too lenient, it may allow unreliable knowledge to be internalized.

### Mechanism 3
- Claim: Template-based prompting with few-shot examples biases LLM responses toward formats compatible with the agent's natural language processing capabilities.
- Mechanism: By providing examples of desired response formats and embedding the agent's specific context, the LLM is guided to produce responses that the agent can parse and internalize.
- Core assumption: The agent's NLP capabilities are fixed and the LLM can be influenced to match those capabilities through prompt engineering.
- Evidence anchors:
  - [section] "We employ a template-based prompting approach, a common method in prompt engineering... To date, we have used a template-based approach, although we have only had to develop a few templates."
  - [section] "One of the main roles examples play in our approach is biasing responses toward simple and direct language that the ITL Agent's NLP interpreter can parse."
  - [corpus] Moderate; prompt engineering is well-established but specific application to cognitive agent integration is less explored.
- Break condition: If the agent's NLP capabilities are too limited or the LLM cannot be sufficiently influenced, the compatibility requirement may not be met.

## Foundational Learning

- Concept: Prompt engineering
  - Why needed here: To guide LLMs to produce responses that are relevant, interpretable, and compatible with the agent's capabilities.
  - Quick check question: Can you design a prompt that elicits a specific type of information from an LLM while controlling for relevance and format?

- Concept: Knowledge verification and validation
  - Why needed here: To ensure that extracted knowledge from LLMs is accurate, reliable, and suitable for the agent's use.
  - Quick check question: How would you evaluate whether an LLM's response is grounded in the agent's current situation and compatible with its embodiment?

- Concept: Cognitive architecture integration
  - Why needed here: To leverage the agent's existing cognitive capabilities (memory, reasoning, planning) to enhance the extraction and utilization of LLM knowledge.
  - Quick check question: What internal capabilities of a cognitive architecture could be used to evaluate and internalize knowledge from external sources?

## Architecture Onboarding

- Component map: Prompt Generator -> LLM Interface -> NLP Interpreter -> Knowledge Verifier -> Knowledge Encoder
- Critical path: Prompt Generator → LLM Interface → NLP Interpreter → Knowledge Verifier → Knowledge Encoder
- Design tradeoffs:
  - Balancing prompt specificity with generalizability to handle diverse knowledge needs.
  - Trade-off between verification thoroughness and real-time performance requirements.
  - Deciding when to rely on the LLM versus human input or other knowledge sources.
- Failure signatures:
  - High rejection rate of LLM responses by the verifier indicates either overly strict criteria or LLM responses are not suitable.
  - Failure to ground LLM responses suggests mismatch between the agent's situation representation and the LLM's output.
  - Inability to interpret responses points to limitations in the agent's NLP capabilities or prompt engineering effectiveness.
- First 3 experiments:
  1. Test prompt generation by having the agent formulate prompts for predefined knowledge gaps and evaluate their specificity and relevance.
  2. Evaluate the verifier by feeding it both viable and non-viable LLM responses and measuring its accuracy in classification.
  3. Assess the full pipeline by having the agent learn a simple new task using LLM knowledge and measuring task performance and learning efficiency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can cognitive architectures be optimized to interpret and ground responses from large language models (LLMs) in real-world scenarios?
- Basis in paper: [explicit] The paper discusses challenges in interpreting and grounding LLM responses, emphasizing the need for cognitive architectures to connect these responses to the agent's current situation and embodiment.
- Why unresolved: The paper highlights the complexity of interpreting LLM responses and the need for further research to improve cognitive architectures' ability to ground these responses effectively.
- What evidence would resolve it: Evidence of improved performance in cognitive agents that successfully interpret and ground LLM responses in diverse real-world scenarios would resolve this question.

### Open Question 2
- Question: What are the most effective methods for verifying the accuracy and relevance of knowledge extracted from LLMs for cognitive agents?
- Basis in paper: [explicit] The paper outlines the necessity for evaluating and verifying LLM responses to ensure they meet the requirements of being interpretable, groundable, and compatible with the agent's embodiment.
- Why unresolved: While the paper discusses the importance of verification, it does not provide a definitive method for ensuring the accuracy and relevance of extracted knowledge.
- What evidence would resolve it: Development and validation of robust verification methods that consistently improve the reliability of knowledge extracted from LLMs would resolve this question.

### Open Question 3
- Question: How can cognitive agents be designed to dynamically adapt their knowledge acquisition strategies when interacting with LLMs?
- Basis in paper: [inferred] The paper suggests that cognitive agents need to be strategic in using LLMs, implying a need for dynamic adaptation in knowledge acquisition strategies.
- Why unresolved: The paper does not provide specific strategies for cognitive agents to adapt their knowledge acquisition approaches in response to varying contexts and tasks.
- What evidence would resolve it: Demonstrations of cognitive agents that effectively adapt their knowledge acquisition strategies in real-time, leading to improved task performance, would resolve this question.

## Limitations

- Evaluation methodology lacks specific metrics for measuring knowledge acquisition success rates
- Generalizability of template-based prompting across diverse cognitive architectures remains unproven
- Human oversight reduction claim needs empirical validation with control conditions

## Confidence

- **High Confidence:** The identification of LLM response quality issues (>70% non-viable responses) is well-supported by the abstract and represents a significant finding for the field.
- **Medium Confidence:** The template-based prompting approach and verification mechanism are conceptually sound but lack detailed implementation specifications and empirical validation.
- **Low Confidence:** The claim of reduced human oversight and improved task learning needs more rigorous experimental validation with specific metrics and control conditions.

## Next Checks

1. Implement controlled experiments comparing direct extraction with baseline indirect methods using standardized knowledge acquisition tasks and metrics.
2. Develop and test a comprehensive evaluation framework that quantifies knowledge viability, extraction efficiency, and verification accuracy across multiple cognitive architectures.
3. Conduct user studies to validate the human oversight reduction claims and measure the learning efficiency improvements in real-world task scenarios.