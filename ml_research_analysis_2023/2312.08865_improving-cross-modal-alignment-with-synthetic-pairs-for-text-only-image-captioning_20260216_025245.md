---
ver: rpa2
title: Improving Cross-modal Alignment with Synthetic Pairs for Text-only Image Captioning
arxiv_id: '2312.08865'
source_url: https://arxiv.org/abs/2312.08865
tags:
- image
- features
- training
- text
- captioning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method, SynTIC, for text-only image
  captioning. The key idea is to leverage synthetic image-text pairs generated by
  a pre-trained text-to-image model to improve cross-modal alignment and bridge the
  training-inference discrepancy in text-only captioning.
---

# Improving Cross-modal Alignment with Synthetic Pairs for Text-only Image Captioning

## Quick Facts
- arXiv ID: 2312.08865
- Source URL: https://arxiv.org/abs/2312.08865
- Reference count: 28
- State-of-the-art performance on text-only image captioning with BLEU-4 of 29.9, METEOR of 25.8, ROUGE-L of 53.2, CIDEr of 101.1, and SPICE of 19.3 on MSCOCO

## Executive Summary
This paper addresses the challenge of text-only image captioning, where models must generate captions for images using only textual data during training. The proposed method, SynTIC, leverages synthetic image-text pairs generated by a pre-trained text-to-image model to bridge the training-inference modality gap. By optimizing pseudo image features in the CLIP embedding space and projecting them into text space using weighted combinations of text features, SynTIC achieves state-of-the-art performance on benchmark datasets while using a lightweight decoder.

## Method Summary
SynTIC employs a text-to-image model (Stable Diffusion v1-5) to generate synthetic images from the text corpus. These synthetic images are encoded using CLIP to obtain initial pseudo features, which are then optimized through contrastive loss to align with real image features. The optimized features are projected into the text embedding space using a support set of text features weighted by cosine similarity. Additionally, object tags detected in images (via DETR) serve as auxiliary features processed through multi-head attention. A lightweight transformer decoder trained with prefix language modeling uses these projected and auxiliary features to generate captions during inference.

## Key Results
- Achieves state-of-the-art performance on MSCOCO with BLEU-4 of 29.9, METEOR of 25.8, ROUGE-L of 53.2, CIDEr of 101.1, and SPICE of 19.3
- Outperforms existing text-only captioning methods across multiple benchmark datasets (Flickr30K, SS1M)
- Demonstrates effectiveness of synthetic image-text pairs and cross-modal alignment optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic image-text pairs generated by a text-to-image model provide training data that bridges the training-inference modality gap in text-only image captioning.
- Mechanism: By generating synthetic images from text and optimizing their CLIP features in the multimodal embedding space, the model learns cross-modal alignment that matches real image-text pairs without requiring paired real images during training.
- Core assumption: The pre-trained text-to-image model generates images semantically aligned with their corresponding text descriptions, and CLIP's contrastive training allows pseudo features to be refined toward real image features.
- Evidence anchors:
  - [abstract] "A pre-trained text-to-image model is deployed to obtain images that correspond to textual data, and the pseudo features of generated images are optimized toward the real ones in the CLIP embedding space."
  - [section] "A pre-trained text-to-image model is employed to generate the synthetic image si = G(ti) for each ti... the pseudo image feature sv i would be optimized by a contrastive loss as follows: ∇sv i Lcon = ∇sv i [− 1/b ∑ log exp(fsim(sv i , tt i)/τ ) / Pb j=1 exp(fsim(sv i , tt j)/τ ) ]"

### Mechanism 2
- Claim: Projecting optimized pseudo image features into the text embedding space using a weighted combination of text features enriches semantics and further reduces the modality gap.
- Mechanism: By constructing a text support set and projecting pseudo image features into this space, the model leverages abundant textual semantics to represent images, making the decoder input more aligned with text features.
- Core assumption: The CLIP text features contain sufficient semantic information to represent image content, and the cosine similarity-based weighting effectively captures the image-text relationship.
- Evidence anchors:
  - [abstract] "textual information is gathered to represent image features, resulting in the image features with various semantics and the bridged modality gap"
  - [section] "We extract the features of all texts to construct a support set, and project the optimized pseudo features into the text embedding space... vi = fproj (sv i , Ds t ) = Σ wj * tt j = Σ exp(fsim(sv i , tt j )/τ ) / Σ exp(fsim(sv i , tt k)/τ ) * tt j"

### Mechanism 3
- Claim: Object tags detected in images serve as auxiliary features that enhance cross-modal alignment by providing concrete visual anchors.
- Mechanism: By extracting object tags and using their CLIP text features as auxiliary inputs, the decoder receives additional semantic cues that complement the projected image features.
- Core assumption: Detected objects are frequently referenced in captions and their textual representations provide meaningful semantic information for caption generation.
- Evidence anchors:
  - [abstract] "salient objects in images are detected as assistance to enhance the learning of modality alignment"
  - [section] "We employ an object detection model to extract objects Oi = {o1, ..., om} in the image... ui = MHAttn(sv i , Ot i, Ot i)"

## Foundational Learning

- Concept: Contrastive learning in multimodal embedding spaces
  - Why needed here: The method relies on CLIP's contrastive training to optimize pseudo image features toward real image features, which is fundamental to bridging the modality gap.
  - Quick check question: How does CLIP's contrastive loss function help align synthetic and real image features in the embedding space?

- Concept: Text-to-image generation and its limitations
  - Why needed here: Understanding how synthetic images differ from real images is crucial for recognizing why optimization and projection steps are necessary.
  - Quick check question: Why might images generated from simple descriptive text lack the depth and details present in real-world images?

- Concept: Feature projection and weighted combination techniques
  - Why needed here: The method projects image features into text space using weighted combinations of text features, which is essential for reducing modality gap and enriching semantics.
  - Quick check question: How does projecting image features into text embedding space using cosine similarity-based weights help bridge the modality gap?

## Architecture Onboarding

- Component map:
  Text corpus -> Text-to-image model (Stable Diffusion) -> Synthetic images -> CLIP image encoder -> Pseudo features
  CLIP text encoder + text corpus -> Support set -> Projection module -> Projected features
  Object detector (DETR) -> Object tags -> Multi-head attention -> Auxiliary features
  Projected features + Auxiliary features -> Lightweight Transformer decoder -> Generated captions

- Critical path:
  1. Generate synthetic images from training text
  2. Extract initial CLIP features from synthetic images
  3. Optimize features using contrastive loss
  4. Project optimized features into text space
  5. Extract object tags and process auxiliary features
  6. Decode captions using projected and auxiliary features

- Design tradeoffs:
  - Using synthetic images enables text-only training but introduces domain gap that requires optimization
  - Lightweight decoder reduces parameters but may limit modeling capacity compared to large language models
  - Object detection adds inference overhead but provides valuable semantic anchors

- Failure signatures:
  - Poor performance on real images indicates ineffective optimization or projection
  - Over-reliance on detected objects suggests weak cross-modal alignment
  - High variance in generated captions indicates instability in feature projection

- First 3 experiments:
  1. Test with synthetic images only (no optimization) to establish baseline performance
  2. Add feature optimization step and measure improvement in alignment metrics
  3. Include object detection and evaluate impact on caption quality and cross-modal alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of synthetic images generated by the text-to-image model impact the final captioning performance, and is there a way to optimize the text-to-image generation process for better captioning results?
- Basis in paper: [inferred] The paper mentions that directly training on synthetic image-text pairs results in limited performance due to the dissimilarity between synthetic and natural images. This suggests that the quality of synthetic images is crucial for the final performance.
- Why unresolved: The paper does not explore the impact of varying the quality or diversity of synthetic images on the captioning performance. It also does not investigate methods to optimize the text-to-image generation process specifically for captioning tasks.
- What evidence would resolve it: Experiments comparing the captioning performance using synthetic images of varying quality or diversity, and studies on optimizing the text-to-image generation process for captioning tasks.

### Open Question 2
- Question: How does the proposed method compare to other approaches that use large language models (LLMs) like GPT-2 or GPT-3 for image captioning, and what are the trade-offs between using a lightweight decoder versus an LLM?
- Basis in paper: [explicit] The paper mentions that compared to methods using LLMs like CapDec, SynTIC uses a lightweight decoder, which is more computation-friendly while achieving superior results.
- Why unresolved: The paper does not provide a detailed comparison of the trade-offs between using a lightweight decoder and an LLM in terms of performance, computational cost, and generalization ability.
- What evidence would resolve it: A comprehensive comparison of SynTIC with methods using LLMs, considering factors like performance on various datasets, computational cost, and ability to generalize to unseen data.

### Open Question 3
- Question: How does the proposed method handle rare or novel objects that are not well-represented in the training data, and can it generate accurate captions for images containing such objects?
- Basis in paper: [inferred] The paper mentions that object tags detected in images are used as auxiliary features to enhance cross-modal alignment. However, it does not explicitly address how the method handles rare or novel objects.
- Why unresolved: The paper does not provide experiments or analysis on the method's performance with images containing rare or novel objects. It also does not discuss strategies to improve the handling of such objects.
- What evidence would resolve it: Experiments evaluating the method's performance on images with rare or novel objects, and analysis of strategies to improve the handling of such objects, such as incorporating external knowledge or using few-shot learning techniques.

## Limitations
- Reliance on synthetic images introduces potential domain discrepancies that may not fully capture real-world image complexity
- Object detection auxiliary features assume detected objects are consistently relevant and well-represented in training captions
- Performance depends on quality of text-to-image generation and effectiveness of optimization techniques

## Confidence
- High confidence: Overall framework and experimental methodology are well-established
- Medium confidence: Effectiveness of contrastive optimization and feature projection is supported by results but could be strengthened
- Low confidence: Generalizability to diverse real-world scenarios and robustness to variations in model quality need further investigation

## Next Checks
1. Conduct detailed analysis of synthetic images to assess semantic alignment with text and identify domain discrepancies
2. Perform ablation studies to quantify individual contributions of optimization, projection, and object tag features
3. Evaluate approach on diverse real-world images with varying complexity to assess generalizability and robustness