---
ver: rpa2
title: Learning from Exemplary Explanations
arxiv_id: '2307.06026'
source_url: https://arxiv.org/abs/2307.06026
tags:
- explanations
- learning
- explanation
- image
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the high user effort required in eXplanation\
  \ Based Learning (XBL) for interactive model refinement. The authors propose Exemplary\
  \ eXplanation Based Learning (eXBL), a method that simplifies user feedback collection\
  \ by requiring only the identification of two exemplary explanations\u2014one good\
  \ and one bad\u2014rather than detailed feature annotations."
---

# Learning from Exemplary Explanations

## Quick Facts
- arXiv ID: 2307.06026
- Source URL: https://arxiv.org/abs/2307.06026
- Reference count: 3
- Primary result: Improved explanation quality (AR +3%) with minimal user input (2 exemplars) on chest X-ray classification

## Executive Summary
This work introduces Exemplary eXplanation Based Learning (eXBL), a method that simplifies user feedback collection for model refinement by requiring only two exemplary explanations—one good and one bad—instead of detailed feature annotations. The approach uses a triplet loss formulation to train the model to align its explanations with the good exemplar and diverge from the bad one. Tested on a medical image classification task (COVID-19 vs. normal vs. lung opacity vs. viral pneumonia chest X-rays), eXBL achieved a 3% improvement in Activation Recall (AR) while slightly decreasing classification accuracy by 4%. The results demonstrate that XBL can be effectively implemented with minimal user effort while maintaining reasonable model performance.

## Method Summary
The method uses pre-trained MobileNetV2 (first 50 layers frozen) for chest X-ray classification. An initial model is trained using categorical cross-entropy, then GradCAM explanations are generated for all training images. The good and bad exemplars are selected based on maximum and minimum AR scores. The model is then retrained with a combined loss: classification loss plus triplet explanation loss that pulls explanations toward the good exemplar and pushes them away from the bad one. The training uses Adam optimizer with early stopping, and the final model is evaluated on explanation quality (AR) and classification performance.

## Key Results
- AR score increased from 0.685 to 0.705 (+3%) using only two exemplary explanations
- Classification accuracy decreased from 0.950 to 0.910 (-4%)
- Method successfully implemented XBL with minimal user input (identifying 2 exemplars vs. detailed annotations)
- Demonstrated domain transferability potential for interactive learning systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Triplet loss with two exemplary explanations reduces annotation burden while improving explanation quality.
- Mechanism: Users select one good and one bad explanation, defining a contrastive objective where the model is trained to make its explanations closer to the good example and farther from the bad one.
- Core assumption: A single good and bad example are representative enough to guide the model's explanation behavior across the training set.
- Evidence anchors:
  - [abstract] The method uses two input instances and their corresponding GradCAM model explanations as exemplary explanations.
  - [section] We then assign products of input instances and GradCAM explanation to Cbad and Cgood using the instances with maximum and minimum AR values.
- Break condition: If exemplars are not representative (outliers or ambiguous cases), the contrastive signal becomes noisy and explanations degrade.

### Mechanism 2
- Claim: Activation Recall (AR) objectively ranks explanations to select good and bad exemplars.
- Mechanism: AR measures overlap between GradCAM explanation and ground-truth mask. Highest-AR example chosen as good, lowest as bad, ensuring selection based on explanation quality.
- Core assumption: Ground-truth masks are accurate and complete representations of relevant image regions.
- Evidence anchors:
  - [section] We choose to use GradCAM model explanations because they have been found to be more sensitive to training label reshuffling and model parameter randomisation than other saliency based explanations.
  - [section] We then assign products of input instances and GradCAM explanation to Cbad and Cgood using the instances with maximum and minimum AR values.
- Break condition: If ground-truth masks are noisy or incomplete, AR rankings become unreliable and exemplars no longer represent good/bad explanations.

### Mechanism 3
- Claim: Freezing early layers and retraining later layers enables GradCAM-based explanation refinement without destroying learned features.
- Mechanism: First 50 layers of MobileNetV2 are frozen, preserving low-level feature extraction, while higher layers are retrained with triplet loss to adjust explanation behavior.
- Core assumption: Early convolutional layers capture general features useful across tasks, while later layers encode task-specific decision logic that can be modified.
- Evidence anchors:
  - [section] We chose to use MobileNetV2 because it achieved better performance at the chest x-ray images classification task at a reduced computational cost after comparison against pre-trained models available at the Keras website.
  - [section] In order for the training process to affect the GradCAM explanation outputs, we only freeze and reuse the first 50 layers of MobileNetV2 and retrain the rest of the convolutional layers with a classifier layer.
- Break condition: If early layers encode spurious correlations, freezing them prevents the model from unlearning those biases, limiting explanation improvement.

## Foundational Learning

- Concept: Triplet loss
  - Why needed here: Provides a contrastive objective that pulls the model toward the good exemplar and pushes it away from the bad one, without needing dense annotations.
  - Quick check question: What happens to the loss if the model's explanation is exactly halfway between the good and bad exemplars?

- Concept: GradCAM
  - Why needed here: Produces spatially localized, class-discriminative visualizations that can be directly compared and ranked by AR.
  - Quick check question: Why multiply the input instance by its GradCAM output when selecting exemplars instead of using GradCAM alone?

- Concept: Activation Recall (AR)
  - Why needed here: Gives a scalar quality score for each explanation, enabling automated selection of good/bad exemplars from a pool of candidates.
  - Quick check question: If AR=1.0 for an explanation, what does that say about the overlap with the ground-truth mask?

## Architecture Onboarding

- Component map: Data loader -> Base model (MobileNetV2) -> GradCAM generator -> AR calculator -> Exemplar selector -> Triplet loss -> Trainer
- Critical path: Load and preprocess images -> Generate GradCAM for all training images using unrefined model -> Compute AR for each, select good/bad exemplars -> Train with triplet loss until convergence or early stop
- Design tradeoffs:
  - Two exemplars vs. full annotation: Huge reduction in user effort but higher sensitivity to exemplar quality
  - Freezing early layers: Faster training and preserved low-level features but may lock in spurious correlations
  - GradCAM vs. other saliency: More stable under model perturbation but less fine-grained than some alternatives
- Failure signatures:
  - Poor exemplar selection → model explanations drift away from ground truth
  - GradCAM instability → AR rankings become inconsistent
  - Overfitting to exemplars → explanations fit the two examples but fail on new data
- First 3 experiments:
  1. Train base model, generate GradCAM, compute AR, verify exemplar selection matches intuition
  2. Run ablation: compare triplet loss vs. no explanation loss to confirm AR improvement
  3. Stress test: manually corrupt one exemplar, retrain, observe explanation degradation

## Open Questions the Paper Calls Out
1. How would the proposed method perform with a larger number of exemplary explanations beyond two?
2. How does the choice of explanation method (e.g., GradCAM vs. other saliency-based techniques) affect the performance of the proposed method?
3. How does the proposed method generalize to other domains and tasks beyond medical image classification?

## Limitations
- AR metric reliability depends on ground-truth mask quality and completeness
- Two-exemplar approach assumes representative good/bad examples may not hold for diverse cases
- Freezing early layers may preserve spurious correlations learned during pre-training
- Modest AR improvement (+3%) with notable accuracy trade-off (-4%)

## Confidence
- Medium: Core claim that minimal user input can improve explanations is well-supported by controlled experiment on medical dataset
- Low: Generalizability to other domains or larger numbers of classes not empirically tested
- Medium: Method architecture is sound but limited to one model architecture (MobileNetV2)

## Next Checks
1. Apply eXBL to a second medical imaging dataset (e.g., diabetic retinopathy) to assess cross-domain robustness
2. Compare AR improvement when using 1, 2, 4, and 10 exemplars to quantify marginal benefit of additional examples
3. Conduct user study where radiologists rate explanation quality before and after eXBL refinement to complement AR metric