---
ver: rpa2
title: Improving Knowledge Distillation with Teacher's Explanation
arxiv_id: '2310.02572'
source_url: https://arxiv.org/abs/2310.02572
tags:
- student
- teacher
- distillation
- superfeatures
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Knowledge distillation (KD) is limited by the teacher model being
  a black box, transferring knowledge only through its predictions. This paper introduces
  Knowledge Explaining Distillation (KED), which uses superfeature-explaining teachers
  that provide explanations over groups of features in addition to predictions.
---

# Improving Knowledge Distillation with Teacher's Explanation

## Quick Facts
- arXiv ID: 2310.02572
- Source URL: https://arxiv.org/abs/2310.02572
- Reference count: 40
- KED achieves significant accuracy improvements over standard KD, e.g., CIFAR100 from 70.39% to 73.50%

## Executive Summary
Knowledge distillation is limited by the teacher model being a black box, transferring knowledge only through its predictions. This paper introduces Knowledge Explaining Distillation (KED), which uses superfeature-explaining teachers that provide explanations over groups of features in addition to predictions. KED students learn from both the teacher's predictions and feature-group explanations, achieving significantly better performance than standard KD students of similar complexity. Experiments on diverse datasets (MNIST, FashionMNIST, CIFAR10/100, Tiny Imagenet, Unicauca) show substantial improvements.

## Method Summary
KED uses superfeature-explaining teachers that provide explanations over groups of features. The teacher estimates the contribution of each superfeature using a subnet, outputting class probabilities for each superfeature. The student learns to match these explanations alongside soft labels. Superfeatures are constructed using a Hessian-based method with Louvain community detection to identify independent feature groups. The method extends to CNNs and can be combined with hidden-representation distillation methods.

## Key Results
- CIFAR100 test accuracy increases from 70.39% with KD to 73.50% with KED
- Significant improvements across multiple datasets including MNIST, FashionMNIST, CIFAR10, CIFAR100, Tiny Imagenet, and Unicauca
- KED extends to CNNs and composes with hidden-representation distillation methods for additional gains

## Why This Works (Mechanism)

### Mechanism 1
- KED improves distillation by adding feature-group explanations that black-box KD lacks
- Superfeature-explaining teacher computes per-superfeature class probabilities as Shapley values, capturing feature-group contributions
- Core assumption: Grouping correlated features into superfeatures produces coherent, independent explanatory units
- Evidence: Teacher estimates contribution of each superfeature using a subnet; missing comparison of Shapley vs. other group-level explanations

### Mechanism 2
- Teacher-student architecture enforces feature independence that benefits student generalization
- Teacher subnets output superfeature contributions that sum (via log-space addition) to final logit
- Core assumption: Log-space additive combination preserves predictive power while enforcing independence
- Evidence: Hessian-based superfeature construction aims for zero cross-partial derivatives; no study comparing independence constraint relaxed vs. enforced

### Mechanism 3
- KED composes with hidden-representation distillation methods for orthogonal performance gains
- KED adds KL divergence term between teacher and student superfeature explanations
- Core assumption: Explanation loss and hidden-representation loss are complementary, not redundant
- Evidence: Extended loss formula combines both terms; Tables show KED+FitNet outperforms KD+FitNet alone; missing ablation showing independent contribution

## Foundational Learning

- Feature attribution and Shapley values
  - Why needed: KED teachers output Shapley values of superfeatures
  - Quick check: What does a Shapley value represent in the context of superfeature contributions to class probabilities?

- Hessian-based dependency analysis
  - Why needed: Algorithm 1 constructs superfeatures by approximating feature-feature Hessian entries
  - Quick check: Why does the algorithm set the diagonal of the dependency matrix W to zero before community detection?

- Temperature scaling in KD
  - Why needed: KED applies two temperatures (T for predictions, τ for explanations)
  - Quick check: What effect does increasing T have on the teacher's soft labels and student's learning signal?

## Architecture Onboarding

- Component map: 
  - Teacher: Black-box → Superfeature-explaining subnet stack → Softmax → Final prediction + M superfeature outputs
  - Student: Input → Feature extraction (shared for CNNs) → M subnets (matching teacher) → Softmax → Final prediction
  - Loss: Hard CE + T²·λ(1-μ)·KL(pred) + τ²·λ·μ·(1/M)·Σ KL(exp) [+ ρ·λ·μ·hidden loss]

- Critical path:
  1. Compute teacher predictions and superfeature explanations
  2. Pass same inputs to student
  3. Compute combined loss
  4. Backpropagate through student subnets only

- Design tradeoffs:
  - More superfeatures (M) → richer explanations but tighter architectural constraints → possible accuracy drop
  - Higher τ → softer explanations → smoother learning signal but less precise supervision
  - Equal teacher/student M → simpler architecture; mismatched M → loss of explanation alignment

- Failure signatures:
  - Student accuracy ≈ teacher accuracy but poor generalization → explanations not adding value
  - Training loss diverges → explanation KL term too strong or superfeatures poorly constructed
  - Student underfits → loss weight λ too high, hiding hard label signal

- First 3 experiments:
  1. Run KD vs. KED on small MLP dataset (e.g., MNIST) with M=2; verify accuracy improvement
  2. Vary M from 1 to 16; plot teacher vs. student accuracy to see M-optimal trade-off
  3. Replace explanation loss with random noise; confirm performance drops to near KD level

## Open Questions the Paper Calls Out

### Open Question 1
- How does choice of M affect trade-off between interpretability and accuracy across datasets?
- Basis: Paper discusses M hierarchy and shows varying M affects performance, but doesn't systematically analyze relationship across diverse datasets
- Unresolved: Missing systematic analysis of M-optimal trade-off and guidelines for choosing M
- Resolution: Systematic experiments varying M across multiple datasets, measuring both accuracy and interpretability

### Open Question 2
- What is theoretical upper bound on KED performance improvement vs. traditional KD?
- Basis: Paper shows KED outperforms KD empirically but doesn't establish theoretical limits
- Unresolved: No theoretical analysis of performance bounds or optimality conditions
- Resolution: Theoretical analysis establishing bounds and conditions, validated by experiments

### Open Question 3
- How does KED perform with highly correlated features where traditional attribution struggles?
- Basis: Paper mentions KED uses superfeatures to handle dependencies but doesn't test challenging correlation scenarios
- Unresolved: Missing investigation of high feature correlation scenarios and algorithm limitations
- Resolution: Experiments on high-correlation datasets, comparison with traditional methods, analysis of algorithm performance

## Limitations

- Claim that Shapley values are necessary (vs. any group-level explanation) weakly supported; missing comparison with alternative groupings
- Hessian-based superfeature construction relies on approximate cross-partial derivatives; independence assumption may fail on high-dimensional data
- Missing ablations showing independent contribution of each loss term (predictions vs. explanations vs. hidden-representation losses)

## Confidence

- High: KED improves student accuracy over standard KD on tested datasets
- Medium: Superfeature explanations are primary driver of gains (vs. architectural effects)
- Low: Shapley values are essential vs. alternative explanation methods

## Next Checks

1. Run ablation comparing KED with random superfeature groupings vs. Hessian-based groupings to isolate importance of dependency analysis
2. Test KED on high-dimensional dataset (e.g., Tiny Imagenet) with mismatched teacher/student superfeature counts to evaluate architectural constraint robustness
3. Compare KED to KD variant using any interpretable feature attribution (e.g., Grad-CAM) instead of Shapley values to test necessity of Shapley-based explanations