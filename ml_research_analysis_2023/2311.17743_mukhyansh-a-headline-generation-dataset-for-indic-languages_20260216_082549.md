---
ver: rpa2
title: 'Mukhyansh: A Headline Generation Dataset for Indic Languages'
arxiv_id: '2311.17743'
source_url: https://arxiv.org/abs/2311.17743
tags:
- pairs
- https
- indichg
- dataset
- headline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Mukhyansh, a large multilingual headline generation
  dataset for eight Indian languages, containing over 3.39 million article-headline
  pairs. It addresses the scarcity of high-quality annotated data for headline generation
  in low-resource languages like Indian languages.
---

# Mukhyansh: A Headline Generation Dataset for Indic Languages

## Quick Facts
- arXiv ID: 2311.17743
- Source URL: https://arxiv.org/abs/2311.17743
- Reference count: 33
- Key outcome: Introduces Mukhyansh, a 3.39M article-headline dataset for 8 Indian languages, achieving 31.43 average ROUGE-L score

## Executive Summary
Mukhyansh is a large-scale multilingual headline generation dataset for eight Indian languages, containing over 3.39 million article-headline pairs. The dataset addresses the scarcity of high-quality annotated data for headline generation in low-resource languages. The authors develop site-specific web scrapers and rigorous preprocessing to ensure data quality, then evaluate several baseline models including RNN and transformer approaches. Their analysis reveals significant data quality issues in existing datasets like IndicHG, demonstrating the superiority of Mukhyansh through controlled experiments and improved model performance.

## Method Summary
The authors created site-specific web scrapers to collect news articles and headlines from various Indian news websites, followed by preprocessing to remove duplicates, prefixes, and short pairs. They fine-tuned several baseline models including FastText+GRU/LSTM, BPEmb+GRU, mT5-small, and SSIB (a variant of IndicBART) on their dataset. Models were trained using standard sequence-to-sequence approaches with beam search decoding, and evaluated using multilingual ROUGE metrics across the eight target languages.

## Key Results
- Average ROUGE-L score of 31.43 across all eight languages
- SSIB model achieves highest ROUGE scores for 5 out of 8 languages
- Demonstrates significant data quality issues in existing IndicHG dataset through overlap analysis
- Mukhyansh_small (5.6% of full dataset) still outperforms IndicHG on multiple metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Site-specific web scrapers combined with rigorous preprocessing create high-quality data by eliminating duplicates, prefixes, and short pairs.
- Mechanism: Authors developed site-specific scrapers tailored to each news website's HTML structure, followed by preprocessing steps including duplicate removal, prefix filtering, and length thresholds.
- Core assumption: Site-specific scrapers can accurately capture diverse news site structures without introducing noise, and preprocessing heuristics sufficiently filter poor-quality pairs.
- Evidence anchors: [abstract] Claims high-quality dataset with preprocessing steps; [section 2.1] Describes elimination of special symbols, duplicates, prefixes, and short pairs with statistics.

### Mechanism 2
- Claim: High-quality training data reduces overfitting and memorization, leading to better generalization on unseen test data.
- Mechanism: Authors demonstrate that IndicHG dataset has significant overlap between train, dev, and test splits causing data contamination. They create IndicHG_filtered version and show improved performance.
- Core assumption: Removing overlapping pairs eliminates data contamination and provides more accurate evaluation of model generalization.
- Evidence anchors: [section 4.2] Shows high overlap in IndicHG corroborating contamination; [section 4.4] Demonstrates improved performance on filtered dataset.

### Mechanism 3
- Claim: Pre-trained multilingual transformer models (mT5 and IndicBART) provide strong foundation for fine-tuning on low-resource languages.
- Mechanism: Authors fine-tune pre-trained models like mT5-small and IndicBART, showing they outperform models trained from scratch due to transferable representations learned from large corpora.
- Core assumption: Pre-trained models have learned useful representations for headline generation even in low-resource languages.
- Evidence anchors: [section 3] Describes use of pre-trained models and their performance; [section 4.4] Shows SSIB outperforms other models including those trained from scratch.

## Foundational Learning

- Concept: Data preprocessing and quality control
  - Why needed here: Ensures dataset is clean, consistent, and free of noise for training robust models
  - Quick check question: What are potential consequences of not removing duplicates, prefixes, and short pairs from dataset?

- Concept: Data contamination and its impact on model evaluation
  - Why needed here: Understanding how overlaps between train, dev, and test splits lead to artificially high performance and inaccurate assessment of generalization
  - Quick check question: How can data contamination affect reliability of model evaluation metrics?

- Concept: Transfer learning and pre-trained models
  - Why needed here: Leverages knowledge from large-scale pre-training on related tasks and languages to improve performance on low-resource languages
  - Quick check question: What are benefits and limitations of using pre-trained models for low-resource language tasks?

## Architecture Onboarding

- Component map: Data collection -> Data preprocessing -> Model training -> Evaluation
- Critical path: Site-specific scrapers → Preprocessing (duplicate/prefix removal, length filtering) → Fine-tuning pre-trained transformers → ROUGE evaluation
- Design tradeoffs: Site-specific scrapers ensure quality but require maintenance; prefix removal may improve performance but could remove valuable information; pre-trained models improve performance but require more compute
- Failure signatures: Low ROUGE scores indicate data quality issues or overfitting; high overlap between splits indicates contamination; poor performance on specific languages suggests pre-trained models aren't well-suited
- First 3 experiments:
  1. Train on unfiltered IndicHG and evaluate on original test set to confirm data contamination
  2. Train on IndicHG_filtered and evaluate on filtered test set to demonstrate impact of data quality
  3. Fine-tune SSIB on Mukhyansh and evaluate on various test sets to compare with other models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural improvements beyond SSIB and mT5-small could further enhance headline generation performance on Mukhyansh?
- Basis in paper: [explicit] Demonstrates SSIB and mT5-small outperform RNN-based models but acknowledges multilingual models as limitation due to compute constraints
- Why unresolved: Authors did not explore or test other multilingual transformer architectures or hybrid approaches
- What evidence would resolve it: Fine-tuning additional multilingual models like mBART, XLM-T, or T5-XXL on Mukhyansh and comparing performance against SSIB and mT5-small

### Open Question 2
- Question: How would performance change if Mukhyansh were augmented with synthetic data generated through back-translation or paraphrasing?
- Basis in paper: [inferred] Emphasizes data quality over quantity and shows Mukhyansh_small performs competitively, suggesting data augmentation could be valuable
- Why unresolved: Authors did not experiment with synthetic data generation techniques
- What evidence would resolve it: Training models on Mukhyansh combined with synthetic headlines generated via back-translation and evaluating impact on ROUGE scores

### Open Question 3
- Question: What is optimal balance between abstractive and extractive headline generation for Indian languages, and how does this vary across different news categories?
- Basis in paper: [explicit] Analyzes abstractive nature by measuring novel n-grams and comparing LEAD-1 and EXT-ORACLE scores, finding variation across languages
- Why unresolved: Does not investigate whether certain news categories benefit more from abstractive versus extractive approaches
- What evidence would resolve it: Conducting category-wise analysis of model performance using different abstractive-extractive ratios

## Limitations
- Lack of per-language performance breakdowns makes it difficult to assess consistency across all eight languages
- Preprocessing methodology for complex cases (multiple articles, unwanted content) is not fully detailed
- Comparison with existing datasets lacks controlled experiments using identical model architectures

## Confidence

**High Confidence**: Dataset creation methodology and identification of data quality issues are well-documented and reproducible
**Medium Confidence**: Baseline model implementations and experimental setup are described with sufficient detail for reproduction
**Low Confidence**: Generalization claims across all eight languages and superiority over existing datasets based on aggregated metrics without sufficient per-language breakdowns

## Next Checks

1. **Per-Language Performance Analysis**: Request and analyze complete breakdown of ROUGE scores for each of the eight languages individually to reveal performance disparities

2. **Controlled Dataset Comparison**: Conduct controlled experiment training same model architecture on both Mukhyansh and filtered IndicHG using identical settings to isolate impact of dataset quality

3. **Edge Case Verification**: Examine stratified sample of 100+ article-headline pairs from final dataset, focusing on complex cases, to manually verify preprocessing correctness and identify any remaining unwanted content