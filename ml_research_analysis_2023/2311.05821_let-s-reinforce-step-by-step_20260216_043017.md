---
ver: rpa2
title: Let's Reinforce Step by Step
arxiv_id: '2311.05821'
source_url: https://arxiv.org/abs/2311.05821
tags:
- reward
- math
- performance
- gsm8k
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using reinforcement learning from human feedback
  (RLHF) to improve mathematical reasoning in language models. The authors train outcome-supervised
  and process-supervised reward models on human-labeled step-by-step solutions, then
  use these rewards to fine-tune a generator model via PPO.
---

# Let's Reinforce Step by Step

## Quick Facts
- arXiv ID: 2311.05821
- Source URL: https://arxiv.org/abs/2311.05821
- Reference count: 4
- Primary result: Process-supervised rewards improve GSM8K accuracy by 33% relative; outcome-supervised rewards boost MATH performance by 18%

## Executive Summary
This paper explores using reinforcement learning from human feedback (RLHF) to improve mathematical reasoning in language models. The authors train outcome-supervised and process-supervised reward models on human-labeled step-by-step solutions, then use these rewards to fine-tune a generator model via PPO. They find that process-supervised rewards improve accuracy on simpler math problems (GSM8K) by 33% relative, while outcome-supervised rewards boost performance on harder problems (MATH) by 18%. The choice of reward aggregation method is critical, with max-aggregation working best for GSM8K and vanilla outcome-supervised rewards performing best for MATH.

## Method Summary
The authors use a three-stage approach: first, they fine-tune an OPT-1.3B generator model on AMPS and MATH datasets; second, they train two 300M DeBERTav32 reward models (one outcome-supervised, one process-supervised) on the PRM800K dataset; third, they perform RLHF using PPO with the trained reward models, mixing GSM8K and MATH prompts, and testing various reward aggregation methods including vanilla ORM, PRM-Avg, PRM-Prod, PRM-Max, and PRM-Min.

## Key Results
- Process-supervised rewards improve GSM8K accuracy by 33% relative
- Outcome-supervised rewards boost MATH performance by 18%
- Max-aggregation performs best for GSM8K while vanilla ORM works best for MATH

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Process-supervised reward models (PRMs) provide more fine-grained feedback than outcome-supervised reward models (ORMs), leading to better performance on simpler math problems like GSM8K.
- Mechanism: PRMs evaluate each reasoning step individually, allowing the model to receive detailed feedback on its logical progression. This fine-grained supervision helps the model improve its step-by-step reasoning, which is crucial for simpler problems that require clear logical chains.
- Core assumption: The model can effectively learn from step-level feedback and generalize this knowledge to improve overall performance.
- Evidence anchors:
  - [abstract] "Our results show that the fine-grained reward provided by PRM-based methods enhances accuracy on simple mathematical reasoning (GSM8K) while, unexpectedly, reducing performance in complex tasks (MATH)."
  - [section 3.2] "The increase in MATH performance with the outcome-supervised reward model may stem from its holistic training, enabling better handling of complex problems, while its lack of exposure to simpler reasoning hinders decisive signaling for GSM8K problems. In contrast, the process-supervised reward model...prioritizes correctness on the step level, resulting in improved GSM8K performance but a decline in MATH performance."
  - [corpus] Weak evidence; related papers discuss PRMs but do not provide direct support for this specific mechanism.
- Break condition: If the step-level feedback becomes too noisy or if the model cannot effectively learn from such granular supervision, the PRM approach may fail to improve performance.

### Mechanism 2
- Claim: The choice of reward aggregation method is critical for PRM-based methods, with max-aggregation working best for GSM8K and vanilla outcome-supervised rewards performing best for MATH.
- Mechanism: Different aggregation methods (avg, prod, max, min) combine the per-step rewards into a single reward signal. Max-aggregation focuses on the best reasoning step, which may be sufficient for simpler problems. Vanilla ORM provides a holistic evaluation, better suited for complex problems requiring integrated reasoning.
- Core assumption: The aggregation method significantly impacts how the model interprets and learns from the reward signal.
- Evidence anchors:
  - [abstract] "Furthermore, we show the critical role reward aggregation functions play in model performance. Providing promising avenues for future research, our study underscores the need for further exploration into fine-grained reward modeling for more reliable language models."
  - [section 3.2] "We find a strong dependence of accuracy on the reward aggregation method. The best performance on GSM8K results from the use of the PRM-Max method, and the best score on MATH comes from the vanilla ORM method."
  - [corpus] Weak evidence; related papers discuss reward aggregation but do not provide direct support for this specific mechanism.
- Break condition: If the aggregation method does not align with the problem's complexity or the model's learning dynamics, it may lead to suboptimal performance or even degradation.

### Mechanism 3
- Claim: Mixing datasets (GSM8K and MATH) during training improves the model's ability to handle problems of varying difficulty levels.
- Mechanism: By exposing the model to both simpler and more complex problems, it learns to adapt its reasoning strategies to different levels of difficulty. This mixed training helps the model generalize better across a range of mathematical tasks.
- Core assumption: The model can effectively transfer knowledge learned from simpler problems to more complex ones, and vice versa.
- Evidence anchors:
  - [section 3.2] "Effects of dataset mixing: To verify our strategy of mixing GSM8K and MATH training sets in stage threes, we use either dataset with the PRM-Prod method and observe significant decreases in model ability. When only training on MATH...Similarly, only training on GSM8K produces an accuracy of 1.5% on MATH...When compared with our results in Figure 2, it is clear that training on both datasets is beneficial for performance."
  - [corpus] Weak evidence; related papers discuss dataset mixing but do not provide direct support for this specific mechanism.
- Break condition: If the model cannot effectively transfer knowledge between problem complexities or if the datasets are too dissimilar, mixing may not provide the intended benefits.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF allows the model to optimize for logical reasoning by using human-provided feedback, which is essential for improving mathematical reasoning in language models.
  - Quick check question: What is the main difference between outcome-supervised and process-supervised reward models in RLHF?

- Concept: Reward Aggregation Methods
  - Why needed here: Different aggregation methods combine per-step rewards into a single reward signal, which significantly impacts the model's learning and performance.
  - Quick check question: How does max-aggregation differ from average-aggregation in the context of PRMs?

- Concept: Chain-of-Thought (CoT) Reasoning
  - Why needed here: CoT methods aim to instill step-wise reasoning at generation-time, which is crucial for complex reasoning tasks like mathematics.
  - Quick check question: Why might CoT prompting alone not guarantee logical correctness in mathematical reasoning?

## Architecture Onboarding

- Component map:
  Generator Model (OPT-1.3B) -> Reward Models (ORM and PRM) -> Aggregation Methods -> PPO Algorithm -> Fine-tuned Generator

- Critical path:
  1. Fine-tune the generator model on multi-step math solutions.
  2. Train ORM and PRM on labeled datasets.
  3. Use PPO to fine-tune the generator using rewards from the trained models.
  4. Evaluate the fine-tuned model on math benchmarks (GSM8K and MATH).

- Design tradeoffs:
  - ORM vs. PRM: ORM provides holistic feedback but lacks step-level granularity; PRM offers detailed feedback but may struggle with complex reasoning.
  - Aggregation Method: Different methods balance between focusing on the best step (max) and overall consistency (avg).

- Failure signatures:
  - Degraded performance on both GSM8K and MATH when using certain aggregation methods (e.g., PRM-Avg, PRM-Min).
  - Overfitting to one dataset when not mixing GSM8K and MATH during training.

- First 3 experiments:
  1. Fine-tune the generator model on AMPS and MATH datasets, evaluate baseline performance.
  2. Train ORM and PRM on PRM800K dataset, test their ability to evaluate reasoning steps.
  3. Use PPO to fine-tune the generator with ORM and PRM rewards, compare performance on GSM8K and MATH.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do PRM-based methods improve GSM8K performance but degrade MATH performance?
- Basis in paper: [explicit] The paper explicitly states that PRM-based methods lead to decreases in MATH performance while increasing GSM8K accuracy by 33% relative.
- Why unresolved: The paper notes this finding but doesn't fully explain the underlying mechanisms. The authors suggest PRMs might be better at "real math" that generalizes across complexity levels, but this is speculative.
- What evidence would resolve it: Systematic analysis of error types on both datasets, ablation studies on different PRM architectures/sizes, and investigation of whether PRMs are over-optimizing for step-level correctness at the expense of overall problem-solving ability.

### Open Question 2
- Question: How sensitive are the results to reward aggregation method choice across different model scales?
- Basis in paper: [explicit] The paper emphasizes that "the choice of reward aggregation method is critical" and different methods perform best on different datasets.
- Why unresolved: The study only used 1.3B parameter models and explored a limited set of aggregation methods. Lightman et al. [2023] found model size crucial for step-by-step rewards, suggesting scaling effects may be important.
- What evidence would resolve it: Replication of experiments with larger models (e.g., 7B, 13B parameters) and exploration of additional aggregation methods like attention-weighted combinations or learned aggregation functions.

### Open Question 3
- Question: Would direct use of fine-grained (step-by-step) rewards in the RL objective outperform aggregated PRM approaches?
- Basis in paper: [inferred] The authors mention in their conclusion that they plan to "explore the use of a non-aggregated step-by-step or, in general, fine-grained rewards directly in the reinforcement learning objective" as future work.
- Why unresolved: The current study only evaluates aggregated reward signals, leaving open whether the full step-by-step information could be better utilized.
- What evidence would resolve it: Implementation of RLHF using the full sequence of per-step rewards without aggregation, comparing against the current aggregated approaches on both GSM8K and MATH benchmarks.

## Limitations

- Data Scale and Generalization: The PRM800K dataset contains 800K labeled solutions, but the study doesn't report the distribution of problem types or difficulty levels, making it difficult to assess whether the reward models have been exposed to sufficient diversity to generalize across mathematical domains.
- Evaluation Scope: The evaluation focuses on GSM8K (8.5K problems) and MATH (12K problems), which may not represent the full spectrum of mathematical reasoning tasks.
- Implementation Details: Several critical implementation details are missing, including exact optimizer configurations, batch normalization strategies, and specific reward clipping procedures.

## Confidence

- High Confidence: The observation that reward aggregation methods significantly impact performance is well-supported by the experimental results showing clear differences between PRM-Max (best for GSM8K) and vanilla ORM (best for MATH).
- Medium Confidence: The mechanism explaining why PRMs work better for simpler problems (fine-grained feedback) is plausible but not definitively proven.
- Low Confidence: The claim that mixing datasets is universally beneficial lacks strong support, as the paper only shows that mixing prevents performance degradation compared to using single datasets.

## Next Checks

1. **Ablation Study on Reward Aggregation**: Systematically test all five aggregation methods (vanilla ORM, PRM-Avg, PRM-Prod, PRM-Max, PRM-Min) across a broader range of mathematical tasks beyond GSM8K and MATH to verify the claimed relationships between aggregation method and problem complexity.

2. **Step-Level Error Analysis**: Manually analyze 100 failed solutions from both PRM and ORM fine-tuned models to determine whether PRMs actually provide more useful step-level feedback or if they simply enforce different solution patterns that happen to work better on GSM8K.

3. **Dataset Size Sensitivity**: Train PRMs and ORMs on progressively smaller subsets of PRM800K (50K, 200K, 400K samples) to determine the minimum dataset size required for each approach to be effective, and whether there's a crossover point where PRMs become less beneficial than ORMs.