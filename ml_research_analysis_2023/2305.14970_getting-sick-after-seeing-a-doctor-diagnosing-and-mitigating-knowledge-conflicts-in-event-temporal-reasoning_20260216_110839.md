---
ver: rpa2
title: Getting Sick After Seeing a Doctor? Diagnosing and Mitigating Knowledge Conflicts
  in Event Temporal Reasoning
arxiv_id: '2305.14970'
source_url: https://arxiv.org/abs/2305.14970
tags:
- event
- bias
- relation
- temporal
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates knowledge conflicts in event temporal reasoning,
  where models rely on prior knowledge or biases learned during pre-training instead
  of context. The authors define four types of bias (event-relation, narrative, tense,
  and dependency) to identify knowledge-conflict examples.
---

# Getting Sick After Seeing a Doctor? Diagnosing and Mitigating Knowledge Conflicts in Event Temporal Reasoning

## Quick Facts
- arXiv ID: 2305.14970
- Source URL: https://arxiv.org/abs/2305.14970
- Reference count: 31
- Models rely on prior knowledge rather than context in event temporal reasoning, leading to incorrect predictions

## Executive Summary
This paper addresses knowledge conflicts in event temporal reasoning where models rely on prior knowledge or biases learned during pre-training instead of context. The authors define four types of bias (event-relation, narrative, tense, and dependency) to identify knowledge-conflict examples. They propose Counterfactual Data Augmentation (CDA) to mitigate conflicts by generating synthetic data that explicitly contains event pairs with actual temporal relations opposite to biased ones. Experiments on TORQUE and MATRES show that CDA improves both PLM and LLM performance on both overall evaluation and knowledge-conflict subsets, demonstrating its effectiveness in reducing model hallucination and improving temporal reasoning accuracy.

## Method Summary
The authors introduce a systematic framework for diagnosing knowledge conflicts in event temporal reasoning. They define four bias types and calculate bias scores for event pairs in context. Knowledge-conflict examples are identified where bias scores fall below a threshold. For PLMs, they generate counterfactual data using Flan-T5 to create examples with temporal relations opposite to biased ones, then fine-tune models on the augmented dataset. For LLMs, they use CDA-generated examples as in-context demonstrations during inference to showcase alternative temporal relations for the same event pairs.

## Key Results
- CDA improves RoBERTa-large performance on knowledge-conflict subsets by 5.5% EM on TORQUE and 4.4% EM on MATRES
- LLMs benefit from CDA demonstrations, with GPT-3.5 showing improved performance on knowledge-conflict subsets
- Different bias types show varying degrees of improvement with CDA, with narrative bias showing the most significant reduction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counterfactual Data Augmentation (CDA) reduces model reliance on prior knowledge by explicitly training with context where the actual temporal relation contradicts the biased prior.
- Mechanism: CDA generates synthetic examples where event pairs occur under temporal relations opposite to their biased prior. By including these in training, the model learns to rely more on context than prior knowledge.
- Core assumption: The model can learn from synthetic examples that explicitly contradict its learned biases.
- Evidence anchors:
  - [abstract]: "To mitigate such event-related knowledge conflict, we introduce a Counterfactual Data Augmentation (CDA) based method that can be applied to both Pre-trained Language Models (PLMs) and Large Language Models (LLMs) either as additional training data or demonstrations for In-Context Learning."
  - [section 3.3]: "To mitigate biases, our proposed method automatically generates context that contains event pairs whose actual temporal relation is different from the biased relation."

### Mechanism 2
- Claim: Defining multiple bias types (event-relation, narrative, tense, dependency) allows comprehensive detection of knowledge conflicts across different linguistic dimensions.
- Mechanism: Each bias type captures a different way prior knowledge can conflict with context. By detecting conflicts across all dimensions, the approach can identify a broader range of problematic examples.
- Core assumption: Different bias types capture orthogonal sources of knowledge conflicts that can compound.
- Evidence anchors:
  - [section 3.2]: "As we aim to identify the bias in event temporal reasoning about certain relations, we define a bias score b(P1, P2, r) with regard to certain patterns (P1 and P2) against a specific relation r ∈ R where R is a set or subset R of relations defined in a certain dataset."
  - [section 3.2]: "Different from substitution-based methods in entity-related knowledge conflicts, events are usually predicates in the context and cannot be trivially substituted without changing the context and the event arguments."

### Mechanism 3
- Claim: LLMs benefit from CDA through in-context learning with counterfactual demonstrations, even without fine-tuning on the augmented data.
- Mechanism: By providing demonstrations where the LLM's predicted relation is explicitly contradicted by synthetic examples, the model is shown alternative temporal relations for the same event pairs.
- Core assumption: LLMs can learn from demonstrations without gradient updates to their parameters.
- Evidence anchors:
  - [section 3.3]: "For a new event pair (e1, e2) to be studied, we acquire the predicted relation rLLM by the LLM, which is regarded as a 'factual' prediction as it is what the LLM itself hallucinate. We leverage the LLM to generate context examples where (e1, e2) are associated with relations that belong to R − {rLLM} as counterfactual examples to showcase the LLM the alternative cases when (e1, e2) happens following a different TEMP REL."
  - [section 4.4]: "For LLMs, the overall performance is not satisfactory compared with fully-supervised models... However, since LLMs are not fine-tuned on the biased training set, their performance on knowledge-conflict subsets does not drop significantly in comparison to that on the entire evaluation set."

## Foundational Learning

- Concept: Temporal relation extraction and event understanding
  - Why needed here: The paper's core task is identifying temporal relations between events, which requires understanding both event semantics and temporal ordering.
  - Quick check question: What are the four temporal relation types used in MATRES?

- Concept: Knowledge conflicts and bias in language models
  - Why needed here: The paper's central problem is that models rely on prior knowledge/bias instead of context, leading to incorrect predictions.
  - Quick check question: How does knowledge conflict differ from simple model bias?

- Concept: Counterfactual reasoning and data augmentation
  - Why needed here: The proposed solution involves generating synthetic examples that contradict the model's biases.
  - Quick check question: Why is counterfactual data augmentation more effective than simply collecting more real data?

## Architecture Onboarding

- Component map:
  - Bias detection module -> Knowledge conflict selector -> CDA generator -> PLM training pipeline / LLM prompting pipeline

- Critical path: Bias detection → Knowledge conflict selection → CDA generation → Model training/inference

- Design tradeoffs:
  - Multiple bias types vs. simplicity: More bias types capture more conflicts but increase complexity
  - Synthetic vs. real data: CDA can address rare conflicts but may introduce artifacts
  - Fine-tuning vs. in-context learning: PLMs can learn from augmented data directly, while LLMs need demonstrations

- Failure signatures:
  - High variance in performance across different bias types
  - CDA examples that are grammatically correct but semantically implausible
  - Models that overfit to synthetic patterns in the augmented data

- First 3 experiments:
  1. Measure baseline performance on knowledge-conflict subsets to establish the problem magnitude
  2. Test CDA on a single bias type first to verify the mechanism works before scaling
  3. Compare CDA-generated examples against human-written counterfactuals for quality assessment

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the discussion and results, several implicit questions emerge:

- How to effectively scale CDA to larger datasets and more complex temporal reasoning tasks
- Whether the synthetic data generation process introduces artifacts that could harm generalization
- How to balance the trade-off between addressing knowledge conflicts and maintaining overall model performance
- Whether the bias diagnosis framework can be extended to other NLP tasks beyond event temporal reasoning

## Limitations

- Synthetic Data Quality: The CDA approach relies on generating synthetic counterfactual examples using LLMs, raising concerns about potential artifacts or noise that could degrade model generalization to real-world data.
- Bias Type Coverage: The paper identifies four bias types, but these may not capture all forms of knowledge conflicts, particularly those involving complex event semantics or world knowledge beyond linguistic features.
- LLM Evaluation Limitations: While CDA demonstrations help LLMs, overall performance remains "not satisfactory compared with fully-supervised models," suggesting fundamental limitations of the in-context learning approach.

## Confidence

- High Confidence: The core finding that knowledge conflicts exist in event temporal reasoning and that models exhibit significant performance degradation on knowledge-conflict subsets is well-supported by empirical evidence across both TORQUE and MATRES datasets.
- Medium Confidence: The effectiveness of CDA for mitigating knowledge conflicts, particularly for PLMs, shows improvement but the exact contribution of CDA versus other factors is not fully isolated.
- Low Confidence: The scalability and practical utility of the approach for real-world applications due to manual verification steps and limited LLM performance.

## Next Checks

1. **Ablation Study on Bias Types**: Conduct controlled experiments removing each bias type individually to quantify their relative contribution to knowledge conflicts and verify that the bias detection framework isn't overcounting or misattributing conflicts.

2. **Cross-Dataset Generalization Test**: Evaluate models trained on augmented TORQUE data on MATRES (and vice versa) to assess whether CDA helps models generalize beyond dataset-specific patterns and biases.

3. **Human Evaluation of Synthetic Data**: Have human annotators rate the quality, plausibility, and naturalness of CDA-generated counterfactual examples to establish whether the synthetic data introduces harmful artifacts or maintains linguistic quality.