---
ver: rpa2
title: Computational and Storage Efficient Quadratic Neurons for Deep Neural Networks
arxiv_id: '2306.07294'
source_url: https://arxiv.org/abs/2306.07294
tags:
- quadratic
- neuron
- neurons
- linear
- proposed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an efficient quadratic neuron for CNNs that
  reduces computational and storage costs while improving network performance. The
  proposed neuron uses eigendecomposition to simplify the quadratic term and output
  vectorization to maximize information utilization.
---

# Computational and Storage Efficient Quadratic Neurons for Deep Neural Networks

## Quick Facts
- arXiv ID: 2306.07294
- Source URL: https://arxiv.org/abs/2306.07294
- Reference count: 33
- Key outcome: Introduces an efficient quadratic neuron for CNNs that reduces computational and storage costs while improving network performance

## Executive Summary
This paper presents a novel quadratic neuron design for convolutional neural networks that significantly reduces computational and storage costs while maintaining or improving network performance. The proposed neuron uses eigendecomposition to simplify the quadratic term and output vectorization to maximize information utilization. Experiments on CIFAR-10 and ImageNet datasets demonstrate that CNNs with these quadratic neurons achieve higher accuracy with negligible overhead compared to baseline networks, outperforming previous quadratic neuron designs in both accuracy and efficiency.

## Method Summary
The proposed quadratic neuron uses eigendecomposition to simplify the quadratic matrix M into QΛQᵀ, where only the top-k eigenvalues and corresponding eigenvectors are retained, reducing parameters from O(n²) to O(kn + k). The neuron outputs both the final result and intermediate features fᵏ = (Qᵏ)ᵀx, providing k additional output channels. Training stability is ensured through zero initialization of Λₖ, small learning rates for quadratic parameters, and normalization of the quadratic term by 1/k. The neuron is integrated into ResNet architectures and trained using SGD with momentum, weight decay, and learning rate decay.

## Key Results
- CNNs with proposed quadratic neurons achieve higher accuracy on CIFAR-10 and ImageNet compared to baseline networks
- The quadratic neurons outperform previous quadratic neuron designs in terms of both accuracy and efficiency
- Parameter and computational overhead remains negligible despite the increased expressivity
- The proposed neuron offers stable training through careful initialization and normalization schemes

## Why This Works (Mechanism)

### Mechanism 1
The eigendecomposition-based low-rank approximation preserves full expressivity while reducing parameter count from O(n²) to O(kn + k). The symmetric quadratic matrix M is decomposed into QΛQᵀ, and only top-k eigenvalues are retained, eliminating redundant components while preserving most expressive directions.

### Mechanism 2
Vectorizing the quadratic neuron output by concatenating intermediate features fᵏ with the final output maximizes information utilization. The intermediate computation (Qᵏ)ᵀx produces k values representing learned feature transformations, which are preserved as separate output channels rather than being summed into a single scalar.

### Mechanism 3
The proposed neuron achieves stable training through three techniques: treating intermediate features as k linear neurons for gradient propagation, initializing Λₖ with zeros and using small learning rates, and normalizing the quadratic term by 1/k. This allows the network to start as essentially linear and gradually introduce quadratic terms.

## Foundational Learning

- **Concept: Eigendecomposition and spectral theorem**
  - Why needed: Understanding how symmetric matrices can be decomposed into orthogonal eigenvectors and eigenvalues is crucial for grasping the low-rank approximation technique
  - Quick check: What property of real symmetric matrices allows them to be diagonalized by an orthonormal matrix, and why is this useful for simplifying quadratic neurons?

- **Concept: Low-rank matrix approximation and Eckart-Young-Mirsky theorem**
  - Why needed: This theorem guarantees that truncating to top-k eigenvalues provides optimal rank-k approximation in Frobenius norm, justifying the parameter reduction approach
  - Quick check: How does the Eckart-Young-Mirsky theorem justify keeping only the top-k eigenvalues when approximating a quadratic matrix?

- **Concept: Output vectorization and information preservation in neural networks**
  - Why needed: Understanding how intermediate computational results can be preserved as outputs rather than collapsed into a single value is key to grasping the information utilization improvement
  - Quick check: What is the mathematical relationship between the intermediate product (Qᵏ)ᵀx and the outputs of k linear neurons, and why does preserving these as separate channels improve expressivity?

## Architecture Onboarding

- **Component map**: Input vector x ∈ Rⁿ → Intermediate features fᵏ = (Qₖ)ᵀx → Quadratic term y² = (fᵏ)ᵀΛₖfᵏ → Linear term y¹ = wᵀx + b → Final output [y, fᵏ]

- **Critical path**: 
  1. Compute intermediate features fᵏ = (Qₖ)ᵀx (cost: kn MACs)
  2. Compute quadratic term y² = (fᵏ)ᵀΛₖfᵏ (cost: 2k MACs)
  3. Compute linear term y¹ = wᵀx + b (cost: n MACs)
  4. Concatenate [y, fᵏ] as final output

- **Design tradeoffs**: Higher k increases expressivity but reduces the number of neurons per layer (each neuron produces k+1 channels). The orthonormal constraint on Qₖ adds computational overhead during training but ensures stable gradients. Zero initialization of Λₖ provides stable training but requires careful learning rate scheduling.

- **Failure signatures**: Training instability or NaN values likely indicate issues with Λₖ learning rate or normalization. Poor accuracy despite high k may indicate intermediate features are not being effectively utilized by subsequent layers. Excessive memory usage suggests k is too large relative to network width.

- **First 3 experiments**:
  1. Replace linear neurons in a single convolutional layer of ResNet-20 with quadratic neurons (k=3) on CIFAR-10, compare accuracy and parameter count
  2. Vary k from 1 to 15 in the same setup, plot accuracy vs. k to find optimal rank
  3. Implement the proposed neuron in all convolutional layers, compare training stability (loss curves) against baseline ResNet-20

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the optimal rank k vary across different CNN architectures and datasets?
  - Basis: The paper mentions that optimal rank k varies depending on network architecture and dataset
  - Why unresolved: Only provides results for specific architectures (ResNet variants) and datasets (CIFAR-10, CIFAR-100, ImageNet)
  - What evidence would resolve it: Comprehensive study varying k across different CNN architectures and diverse datasets

- **Open Question 2**: Can the quadratic neuron's efficiency be further improved by optimizing the decomposition rank k per layer rather than using a fixed k?
  - Basis: The paper shows quadratic parameter distribution varies significantly across different layers
  - Why unresolved: Uses fixed rank k for all layers in experiments
  - What evidence would resolve it: Experiments comparing fixed k versus layer-specific k optimization

- **Open Question 3**: How does the proposed quadratic neuron compare to other non-linear neuron designs when applied to tasks beyond image classification?
  - Basis: Paper focuses on image classification tasks and doesn't provide direct comparisons for other tasks
  - Why unresolved: No experimental results for tasks other than image classification
  - What evidence would resolve it: Experiments applying the neuron to object detection, semantic segmentation, and other computer vision tasks

## Limitations

- The paper lacks detailed implementation specifications for the eigendecomposition and output vectorization mechanisms, making faithful reproduction challenging
- Training stability mechanisms rely heavily on proper hyperparameter tuning that is not fully specified
- Limited ablation studies and lack of comparison against other quadratic neuron variants reduce confidence in claimed advantages

## Confidence

- **High confidence**: The theoretical foundation of using eigendecomposition for quadratic matrix simplification is mathematically sound and well-established
- **Medium confidence**: Experimental results showing accuracy improvements are promising, but limited ablation studies and lack of comparison against other quadratic neuron variants reduce confidence
- **Low confidence**: Training stability claims depend on unspecified hyperparameter choices (learning rates, initialization details) that could significantly impact practical implementation success

## Next Checks

1. Implement a controlled experiment varying k from 1 to 15 on CIFAR-10 with a single ResNet layer to quantify the accuracy vs. parameter tradeoff curve and identify the optimal rank
2. Conduct an ablation study isolating each stability mechanism (zero initialization, learning rate scheduling, normalization) to determine their individual contributions to training stability
3. Compare the proposed quadratic neuron against at least two other quadratic neuron designs from the literature on the same tasks to validate the claimed efficiency and accuracy advantages