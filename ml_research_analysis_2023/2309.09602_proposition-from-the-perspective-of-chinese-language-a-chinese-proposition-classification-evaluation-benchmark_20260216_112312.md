---
ver: rpa2
title: 'Proposition from the Perspective of Chinese Language: A Chinese Proposition
  Classification Evaluation Benchmark'
arxiv_id: '2309.09602'
source_url: https://arxiv.org/abs/2309.09602
tags:
- proposition
- propositions
- modal
- classification
- logical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces explicit and implicit propositions to better
  handle Chinese natural language. It creates a large Chinese proposition dataset
  PEACE covering multiple domains and tasks.
---

# Proposition from the Perspective of Chinese Language: A Chinese Proposition Classification Evaluation Benchmark

## Quick Facts
- arXiv ID: 2309.09602
- Source URL: https://arxiv.org/abs/2309.09602
- Reference count: 7
- Primary result: BERT achieves good in-domain proposition classification but struggles with cross-domain generalization; ChatGPT improves with more examples.

## Executive Summary
This paper introduces explicit and implicit propositions to better handle Chinese natural language, creating a large Chinese proposition dataset PEACE covering multiple domains and tasks. The study evaluates several models including BERT, RoBERTa, and ChatGPT for proposition classification, revealing that BERT performs well in-domain but lacks cross-domain transferability. ChatGPT performs poorly but improves when provided with more proposition examples, highlighting the importance of modeling semantic features for proposition classification in Chinese.

## Method Summary
The study introduces a multi-level Chinese proposition classification system distinguishing explicit vs. implicit propositions, and modal vs. non-modal categories. The PEACE dataset was created from CCL corpus and ProPC dataset with 45k sentences across four domains. A multi-stage annotation process with quality control ensured high-quality labels. Various models were evaluated including rule-based, SVM, BERT, RoBERTa, and ChatGPT. BERT and RoBERTa were fine-tuned on PEACE data, while ChatGPT was tested with zero-shot and few-shot prompting approaches.

## Key Results
- BERT achieves good in-domain proposition classification performance but struggles with cross-domain generalization
- ChatGPT performs poorly but shows improvement when provided with more proposition examples
- The PEACE dataset is larger and more comprehensive than existing Chinese proposition datasets

## Why This Works (Mechanism)

### Mechanism 1
The explicit/implicit proposition framework aligns better with Chinese language characteristics than Western logic-based approaches. By recognizing that Chinese relies on semantic and logical understanding rather than explicit logical connectives (parataxis), the framework captures implicit propositions that traditional Western logic-based methods miss. Chinese naturally omits logical constants while maintaining clear logical content.

### Mechanism 2
BERT learns domain-specific semantic features during in-domain training but fails to transfer these features effectively to new domains. The model relies heavily on domain-specific patterns rather than generalizable semantic understanding of propositions, resulting in poor cross-domain performance.

### Mechanism 3
ChatGPT lacks inherent understanding of proposition categories as it was trained on general text rather than specifically on proposition classification. However, when given examples and definitions of different proposition types, it can leverage its language understanding capabilities to classify propositions more accurately.

## Foundational Learning

- Concept: Parataxis vs. hypotaxis in language structure
  - Why needed here: Understanding this distinction is crucial for grasping why Chinese proposition classification differs from Western approaches and why explicit/implicit proposition concepts are necessary.
  - Quick check question: What is the key difference between paratactic languages like Chinese and hypotactic languages like English in terms of logical expression?

- Concept: Modal vs. non-modal propositions
  - Why needed here: The classification system divides propositions into these two main categories, making it essential to understand the distinction for proper categorization.
  - Quick check question: How does a modal proposition differ from a non-modal proposition in terms of the judgment it makes about the object?

- Concept: Multi-hop reasoning in language models
  - Why needed here: The cross-domain evaluation reveals limitations in models' ability to generalize knowledge, which relates to their capacity for multi-hop reasoning across different domains.
  - Quick check question: Why might a model trained on one domain struggle to classify propositions from a different domain?

## Architecture Onboarding

- Component map: Data collection pipeline (CCL corpus, ProPC dataset) -> Annotation system (multi-stage process with quality control) -> Classification model implementations (Rule-based, SVM, BERT, RoBERTa, ChatGPT) -> Evaluation framework (in-domain and cross-domain testing) -> Attention visualization tools for model analysis

- Critical path: Data collection → Annotation → Model training → Evaluation → Analysis

- Design tradeoffs:
  - Implicit vs. explicit proposition focus: Including implicit propositions makes the dataset more representative of natural Chinese but increases annotation complexity
  - Fine-grained vs. coarse-grained classification: More categories provide detailed analysis but require more complex models and larger datasets
  - Domain specificity vs. generalizability: Domain-specific models perform better in-domain but struggle with cross-domain tasks

- Failure signatures:
  - Low cross-domain performance indicates over-reliance on domain-specific features
  - Confusion between certain proposition types suggests semantic overlap in classification categories
  - Poor performance on implicit propositions indicates models are overly dependent on explicit logical markers

- First 3 experiments:
  1. Train BERT on C&E data and test on the same domain to establish baseline performance
  2. Train BERT on C&E data and test on FN domain to evaluate cross-domain performance
  3. Provide ChatGPT with 5 examples per category and test classification performance to measure improvement with additional information

## Open Questions the Paper Calls Out

### Open Question 1
How can we improve the cross-domain transferability of models for proposition classification tasks?
The paper mentions that BERT has relatively good proposition classification capability but lacks cross-domain transferability and suggests this as a future research direction. The question remains unresolved as the paper does not provide specific solutions.

### Open Question 2
What are the limitations of using ChatGPT for proposition classification tasks, and how can we overcome them?
The paper notes that ChatGPT performs poorly in proposition classification tasks and suggests its definition of propositions is limited to explicit concepts. This question remains unresolved as the paper does not provide specific solutions.

### Open Question 3
How can we better model the semantic features of implicit propositions in Chinese natural language?
The paper introduces explicit and implicit propositions based on Chinese language characteristics and emphasizes the importance of modeling semantic features, but does not provide specific solutions for this challenge.

## Limitations

- The framework's effectiveness is primarily constrained by its focus on Chinese language characteristics, limiting generalizability to other languages
- The dataset, while extensive at 45k sentences, may still be insufficient for capturing the full complexity of implicit propositions in real-world applications
- The evaluation is limited to specific domains (finance, law, medical, and comprehensive/encyclopedia), and performance in other domains remains untested

## Confidence

**High Confidence:**
- BERT achieves good in-domain performance but struggles with cross-domain generalization
- ChatGPT's performance improves with more proposition examples
- The PEACE dataset is larger and more comprehensive than existing Chinese proposition datasets

**Medium Confidence:**
- The explicit/implicit proposition framework aligns better with Chinese language characteristics than Western logic-based approaches
- Implicit propositions pose significant challenges for current classification models
- Multi-hop reasoning capabilities are necessary for effective cross-domain proposition classification

**Low Confidence:**
- The proposed framework would significantly outperform existing approaches on all Chinese proposition classification tasks
- ChatGPT would perform competitively with fine-tuning on the PEACE dataset
- The framework's principles can be directly applied to other paratactic languages without modification

## Next Checks

1. **Cross-linguistic validation**: Test the explicit/implicit proposition framework on another paratactic language (e.g., Japanese or Korean) to verify whether the approach generalizes beyond Chinese.

2. **Fine-tuning ChatGPT**: Fine-tune ChatGPT on the PEACE dataset and compare its performance with the few-shot prompting approach to determine if the model's limitations are due to architecture or training methodology.

3. **Inter-annotator agreement study**: Conduct a detailed inter-annotator agreement analysis on a subset of the PEACE dataset to quantify the reliability of implicit proposition annotations and identify sources of potential disagreement.