---
ver: rpa2
title: Exploring Answer Information Methods for Question Generation with Transformers
arxiv_id: '2312.03483'
source_url: https://arxiv.org/abs/2312.03483
tags:
- answer
- question
- generation
- arxiv
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores different methods for incorporating answer
  information into transformer-based question generation models. The authors use BART-base
  and experiment with answer prompting, a custom product method between answer embeddings
  and encoder outputs, using sentences containing answers, and adding a separate cross-attention
  block for answer information.
---

# Exploring Answer Information Methods for Question Generation with Transformers

## Quick Facts
- arXiv ID: 2312.03483
- Source URL: https://arxiv.org/abs/2312.03483
- Authors: 
- Reference count: 10
- Key outcome: Answer prompting (AP) with BART-base achieves ROUGE-L of 0.4565 and METEOR of 0.4595 on SQUAD 1.1, with 57.33% of generated questions having the correct answer

## Executive Summary
This paper systematically explores different methods for incorporating answer information into transformer-based question generation models. The authors experiment with four approaches: answer prompting (prepending answers to passages), a custom product method between answer embeddings and encoder outputs, using sentences containing answers, and adding a separate cross-attention block for answer information. They find that answer prompting without additional modes performs best on automatic metrics while also achieving reasonable performance on a custom metric measuring answer correctness. The study provides insights into how different answer information incorporation methods affect question generation quality.

## Method Summary
The authors use BART-base as their foundation model and experiment with four methods to incorporate answer information into question generation. These methods include answer prompting (prepending answers to passages), a custom product method using answer embeddings and encoder outputs, using sentences containing answers, and adding a separate cross-attention block in the decoder. The models are trained on SQUAD 1.1 using the Zhou et al. (2018) split, with a maximum sequence length of 512 tokens for passages, 128 for questions, and 32 for answers. Training uses a batch size of 8, learning rate of 1e-5, and beam search with size 4 for decoding.

## Key Results
- Answer prompting (AP) achieves the best ROUGE-L score of 0.4565 and METEOR score of 0.4595
- The custom answer accuracy metric shows AP generates questions with correct answers 57.33% of the time
- Answer attention (AA) and custom product (CP) methods perform worse than AP on automatic metrics
- Related sentences (RS) method performs similarly to AP but with slightly lower scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Answer prompting (AP) improves question generation quality by aligning encoder attention with answer-relevant tokens.
- Mechanism: The encoder self-attention mechanism calculates similarity between passage tokens and answer tokens when the answer is prepended to the passage. This alignment guides the decoder to generate questions focused on the answer context.
- Core assumption: The BART encoder's self-attention can effectively learn and utilize answer-token relationships when answers are included as part of the input sequence.
- Evidence anchors:
  - [abstract] "We observe that answer prompting without any additional modes obtains the best scores across rouge, meteor scores."
  - [section] "We concatenate the article tokens to answer tokens and truncate the final token representation to a max length of 512 tokens."
  - [corpus] No direct corpus evidence found; inference based on paper claims.
- Break condition: If the encoder cannot effectively attend to the prepended answer tokens due to positional encoding limitations or if the answer is too long relative to the passage.

### Mechanism 2
- Claim: Adding a separate answer attention block (AA) allows the decoder to explicitly attend to answer embeddings during question generation.
- Mechanism: The decoder includes an additional cross-attention block that attends to a mean-pooled answer embedding, providing direct access to answer information during decoding.
- Core assumption: Explicit answer attention in the decoder improves question generation by providing a dedicated pathway for answer information.
- Evidence anchors:
  - [section] "We introduce an additional block, called the answer attention mechanism in the decoder... queries Q are obtained from the previous layer representation, whereas the keys K and values V are answer embeddings A."
  - [abstract] "using a separate cross-attention attention block in the decoder which attends to the answer"
  - [corpus] No direct corpus evidence found; inference based on paper claims.
- Break condition: If the additional attention block creates interference with the existing cross-attention mechanism or if the mean-pooled answer embedding loses important answer information.

### Mechanism 3
- Claim: The custom product method (CP) aligns encoder outputs with answer embeddings to create weighted distributions of answer information across the source text.
- Mechanism: Matrix multiplication between answer embeddings and encoder outputs creates a weighted distribution, which is then multiplied with encoder outputs before being passed to the decoder's cross-attention block.
- Core assumption: Multiplying encoder outputs with answer information creates a more answer-focused representation that improves question generation.
- Evidence anchors:
  - [section] "We perform matrix multiplication for A and Eo and apply softmax over the output to get a weighted distribution of answer information over the source text."
  - [abstract] "using a custom product method using answer embeddings and encoder outputs"
  - [corpus] No direct corpus evidence found; inference based on paper claims.
- Break condition: If the matrix multiplication and softmax operation creates suboptimal weight distributions or if the multiplication constant k is not properly tuned.

## Foundational Learning

- Concept: Transformer architecture (BART) with encoder-decoder structure
  - Why needed here: The paper uses BART-base as the foundation for question generation, requiring understanding of its encoder-decoder mechanism
  - Quick check question: What are the key components of the BART transformer architecture used in this paper?

- Concept: Attention mechanisms (self-attention and cross-attention)
  - Why needed here: Multiple attention mechanisms are employed (self-attention in encoder, cross-attention in decoder, and additional answer attention), requiring understanding of how they work
  - Quick check question: How does the additional answer attention block in the decoder differ from the standard cross-attention mechanism?

- Concept: Evaluation metrics (ROUGE-L, METEOR, custom answer accuracy)
  - Why needed here: The paper uses multiple evaluation metrics to assess question generation quality, requiring understanding of what each metric measures
  - Quick check question: What does the custom metric measuring if generated questions have the correct answer actually evaluate?

## Architecture Onboarding

- Component map:
  - Tokenization with BART tokenizer (max 512 tokens for passage, 128 for question, 32 for answer) -> Answer information method preparation (AP, AA, RS, or CP) -> BART encoder (6 layers) -> BART decoder (6 layers with appropriate attention mechanisms) -> Beam search decoding (beam size 4) -> Evaluation (ROUGE-L, METEOR, custom accuracy)

- Critical path:
  1. Tokenize input passage, answer, and question
  2. Apply chosen answer information method to prepare input
  3. Pass through BART encoder
  4. Pass through BART decoder with appropriate attention mechanisms
  5. Generate question using beam search
  6. Evaluate using ROUGE-L, METEOR, and custom metric

- Design tradeoffs:
  - BART-base vs. BART-large: Base model chosen due to compute constraints, potentially limiting performance
  - Answer information methods: Multiple methods explored, but only one can be used per experiment, limiting direct comparison
  - Maximum sequence lengths: Tradeoff between including more context and computational efficiency

- Failure signatures:
  - Low ROUGE-L/METEOR scores: Issues with question generation quality
  - Low custom answer accuracy: Questions not properly aligned with provided answers
  - Training instability: Potential issues with learning rate or batch size
  - Memory errors: Issues with maximum sequence lengths or batch size

- First 3 experiments:
  1. Baseline BART without any answer information methods
  2. BART with answer prompting (AP) only
  3. BART with answer attention (AA) only

## Open Questions the Paper Calls Out

- Question: Does the performance of different answer information methods vary across different transformer architectures beyond BART-base?
- Basis in paper: [explicit] The authors state they were unable to explore different transformer architectures due to limited compute resources and suggest it would be interesting to observe a correlation between base architecture and performance of answer information methods.
- Why unresolved: The paper only experiments with BART-base, leaving open the question of how other transformer models (e.g., T5, GPT-2) would perform with these methods.
- What evidence would resolve it: Experiments comparing the performance of answer information methods across multiple transformer architectures (e.g., T5, GPT-2, Pegasus) on the same dataset and metrics.

## Limitations

- Limited to BART-base architecture due to compute constraints, making it unclear if results generalize to other transformer models
- Only tested on SQUAD 1.1 dataset, limiting generalizability to other question generation contexts
- Custom accuracy metric depends on an unspecified QA model, making it difficult to assess reliability

## Confidence

- **High confidence**: Answer prompting improves ROUGE-L and METEOR scores compared to baseline BART (supported by quantitative results showing AP outperforming other methods)
- **Medium confidence**: Answer prompting generates questions with correct answers more frequently (57.33% accuracy is reported, but methodology depends on unspecified QA model)
- **Low confidence**: Relative performance ranking of different answer incorporation methods (limited comparison with only one model size and dataset)

## Next Checks

1. Replicate with larger model variants: Test whether answer prompting's superiority holds when using BART-large or other transformer architectures to determine if results are model-dependent.

2. Validate custom accuracy metric: Implement and test the QA model used for the custom accuracy metric independently to verify that the 57.33% answer accuracy is reliable and not influenced by QA model limitations.

3. Cross-dataset evaluation: Evaluate the best-performing method (answer prompting) on additional question generation datasets like MS MARCO or NewsQA to assess generalizability beyond SQUAD 1.1's Wikipedia-based passages.