---
ver: rpa2
title: Evaluating Language-Model Agents on Realistic Autonomous Tasks
arxiv_id: '2312.11671'
source_url: https://arxiv.org/abs/2312.11671
tags:
- task
- agent
- scaffolding
- agents
- instance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The report evaluates language model agents' capabilities for autonomous
  replication and adaptation (ARA) by testing four agents (built on GPT-4 and Claude)
  across 12 real-world tasks ranging from simple file operations to complex phishing
  attacks. The agents demonstrated basic competence in using tools like bash commands
  and web browsing but struggled with complex tasks, often getting stuck in repetitive
  loops or making errors like hallucinations.
---

# Evaluating Language-Model Agents on Realistic Autonomous Tasks

## Quick Facts
- arXiv ID: 2312.11671
- Source URL: https://arxiv.org/abs/2312.11671
- Authors: 
- Reference count: 40
- Key outcome: Language model agents demonstrated basic competence in using tools but struggled with complex tasks, completing only the simplest tasks and highlighting the need for ongoing evaluations as AI agents evolve.

## Executive Summary
This report evaluates language model agents' capabilities for autonomous replication and adaptation (ARA) by testing four agents (built on GPT-4 and Claude) across 12 real-world tasks ranging from simple file operations to complex phishing attacks. The agents demonstrated basic competence in using tools like bash commands and web browsing but struggled with complex tasks, often getting stuck in repetitive loops or making errors like hallucinations. While they completed only the simplest tasks, the evaluation did not rule out that future agents, especially with fine-tuning or larger models, could achieve ARA capabilities. The study highlights the need for ongoing evaluations as AI agents evolve, emphasizing that current agents are far from autonomous replication and adaptation but improvements in scaffolding, fine-tuning, or model scale could rapidly close this gap.

## Method Summary
The evaluation involved setting up virtual machines with necessary tools (bash, Python, web browsing) and credentials for AWS, email, etc. Scaffolding programs were created to interface language models with tools, including prompting for chain-of-thought reasoning and handling delegation. Four agents built on GPT-4 and Claude were run on 12 tasks, with human overseers intervening for safety and debugging. Task completion was manually classified as "complete," "partially complete," or "did not complete."

## Key Results
- Agents completed only the simplest tasks, struggling with complex tasks that required multiple steps or coordination.
- Common failure modes included getting stuck in repetitive loops, hallucinating information, and poor error diagnosis.
- The evaluation did not rule out the possibility that future agents, especially with fine-tuning or larger models, could achieve ARA capabilities.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The agents' inability to complete complex ARA tasks stems from their tendency to get stuck in repetitive loops rather than exploring alternative strategies when faced with obstacles.
- **Mechanism**: When an agent encounters an error or obstacle, instead of adapting its approach, it continues to issue the same or very similar commands repeatedly, creating a loop that prevents progress.
- **Core assumption**: The language model agents lack the ability to recognize when their current approach is failing and to generate novel solutions.
- **Evidence anchors**:
  - [abstract]: "Unfortunately, these evaluations are not adequate to rule out the possibility that near-future agents will be capable of ARA."
  - [section]: "Repetitive loops: For one of the example agents, more than half of the uncompleted tasks ultimately ended with a repetitive loop in which the agent failed to explore new debugging strategies."
  - [corpus]: Weak evidence - the corpus neighbors focus on multimodal agents and web-based tasks, not the specific loop issue.
- **Break condition**: The loop could be broken if the scaffolding is modified to detect repetitive behavior and force the agent to consider alternative approaches or if the underlying language model is fine-tuned to recognize and escape loops.

### Mechanism 2
- **Claim**: The agents' performance is significantly limited by hallucinations and poor error diagnosis, leading to incorrect conclusions and failed task completion.
- **Mechanism**: The agents frequently generate incorrect information or misattribute errors, such as hallucinating names and email addresses or incorrectly concluding that credentials are invalid when there's a mistake in the code or a missing dependency.
- **Core assumption**: The language model agents do not have a reliable way to verify the accuracy of their outputs or to correctly diagnose the root cause of errors.
- **Evidence anchors**:
  - [abstract]: "The agents demonstrated basic competence in using tools like bash commands and web browsing but struggled with complex tasks, often getting stuck in repetitive loops or making errors like hallucinations."
  - [section]: "Hallucinations: The agents frequently fail to complete the tasks by hallucinating results. For example, while researching [company] employees, an agent hallucinates names and email addresses, thus failing the task."
  - [corpus]: Weak evidence - the corpus neighbors do not directly address the hallucination and error diagnosis issue.
- **Break condition**: The hallucination and error diagnosis issues could be mitigated by fine-tuning the models on tasks that require accurate information retrieval and error analysis, or by incorporating external verification tools into the scaffolding.

### Mechanism 3
- **Claim**: The agents' poor self-understanding and weak theory of mind hinder their ability to coordinate with subagents and understand their own situation, leading to task failures.
- **Mechanism**: The agents often fail to provide crucial information to subagents, such as account credentials, and may even harm themselves by killing their own process or outputting their own stop token in the middle of a sentence.
- **Core assumption**: The language model agents lack a robust model of their own capabilities, limitations, and the broader context in which they operate.
- **Evidence anchors**:
  - [abstract]: "The agents demonstrated basic competence in using tools like bash commands and web browsing but struggled with complex tasks..."
  - [section]: "Poor self-understanding: Our agents also demonstrated a poor understanding of their own situation and weak theory of mind when dealing with other subagents. Communication between agents frequently leaves out crucial information such as account credentials, apparently not realizing it won't be available to other agents."
  - [corpus]: Weak evidence - the corpus neighbors focus on multimodal agents and web-based tasks, not the specific self-understanding issue.
- **Break condition**: The self-understanding and theory of mind issues could be addressed by fine-tuning the models on tasks that require coordination and communication with other agents, or by incorporating explicit instructions and constraints into the scaffolding to guide agent behavior.

## Foundational Learning

- **Concept**: Chain-of-thought reasoning
  - Why needed here: Chain-of-thought reasoning allows the agents to break down complex tasks into smaller, more manageable steps, improving their ability to plan and execute tasks.
  - Quick check question: Can the agent generate a step-by-step plan for a given task, and does it update this plan as it encounters obstacles?

- **Concept**: Tool use and API integration
  - Why needed here: The agents rely on various tools and APIs (e.g., bash commands, web browsing, AWS) to interact with the world and complete tasks, so understanding how to use these tools effectively is crucial.
  - Quick check question: Can the agent correctly identify and use the appropriate tools and APIs for a given task, and does it handle errors or unexpected outputs from these tools?

- **Concept**: Fine-tuning and scaffolding
  - Why needed here: Fine-tuning the language models on specific tasks or incorporating better scaffolding can significantly improve the agents' performance on complex tasks, as demonstrated by the differences between GPT-4-early and GPT-4-launch.
  - Quick check question: How does fine-tuning or modifying the scaffolding affect the agent's performance on a given task, and what are the key factors that contribute to these improvements?

## Architecture Onboarding

- **Component map**: Language model (GPT-4/Claude) -> Scaffolding program -> Tools (bash, web browsing, AWS) -> Task completion
- **Critical path**: Agent generates plan -> Executes actions using tools -> Updates plan based on results -> Repeats until task completion or failure
- **Design tradeoffs**: The current design prioritizes simplicity and ease of development over performance, with the agents using relatively simple scaffolding and no fine-tuning. More complex scaffolding or fine-tuning could potentially improve performance but at the cost of increased development time and complexity.
- **Failure signatures**: Common failure modes include getting stuck in repetitive loops, hallucinating information, misattributing errors, and poor self-understanding or theory of mind when coordinating with subagents.
- **First 3 experiments**:
  1. Test the agent's ability to complete a simple task (e.g., searching the filesystem for a password) to establish a baseline for its capabilities.
  2. Introduce a more complex task that requires tool use and planning (e.g., setting up GPT-J on an EC2 instance) to assess the agent's ability to handle increased complexity.
  3. Evaluate the agent's performance on a task that requires coordination with subagents (e.g., targeted phishing) to identify any limitations in its self-understanding or theory of mind.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does fine-tuning of existing language models affect their autonomous replication and adaptation (ARA) capabilities, even when the fine-tuning is not directly targeted at ARA?
- Basis in paper: [explicit] The paper states: "we expect that fine-tuning of the existing models could produce substantially more competent agents, even if the fine-tuning is not directly targeted at ARA."
- Why unresolved: The paper's evaluation focuses on agents built on base models without fine-tuning, and does not explore how fine-tuning for other objectives (e.g., being a helpful assistant) might transfer to ARA capabilities.
- What evidence would resolve it: Empirical results comparing ARA-relevant task performance between base models, models fine-tuned for general helpfulness, and models fine-tuned for specific non-ARA tasks.

### Open Question 2
- Question: What is the impact of scaffolding improvements on agent capabilities for ARA-relevant tasks?
- Basis in paper: [explicit] The paper notes that "better scaffolding and prompting is developed for language model agents" could lead to more dangerous capabilities, and that "the quality of scaffolding and prompting appears to significantly impact the capabilities of agents on autonomous tasks."
- Why unresolved: The paper uses simple scaffolding that has not been the subject of significant research effort, and does not explore the potential gains from more sophisticated scaffolding designs.
- What evidence would resolve it: Comparative evaluations of agents using basic scaffolding versus agents using state-of-the-art scaffolding techniques on the same set of ARA-relevant tasks.

### Open Question 3
- Question: How do agent capabilities for ARA-relevant tasks scale with increases in model size and computational power?
- Basis in paper: [explicit] The paper states that "future agents could be built on larger models" and that "we do not think that these evaluations provide good assurance that the 'next generation' of language models (e.g. 100x effective compute scaleup on existing models) will not yield agents capable of ARA."
- Why unresolved: The paper evaluates agents built on current language models (GPT-4 and Claude) and does not attempt to forecast progress for larger models or extrapolate from smaller models.
- What evidence would resolve it: Empirical results showing task performance as a function of model scale, potentially using a scaling law analysis to predict ARA-relevant capabilities for future models.

## Limitations

- The evaluation was conducted on a specific set of tasks and with a limited number of agents, which may not fully capture the range of challenges and scenarios that autonomous agents could face in the real world.
- The human oversight and intervention in the evaluation process may have mitigated some of the agents' limitations, potentially overestimating their capabilities.
- The study cannot rule out that future agents, especially with fine-tuning or larger models, could achieve ARA capabilities, highlighting the need for ongoing evaluations as AI agents evolve.

## Confidence

- **High Confidence**: The agents' basic competence in using tools like bash commands and web browsing is well-established. The observation that current agents are far from autonomous replication and adaptation is supported by the task completion results.
- **Medium Confidence**: The agents' tendency to get stuck in repetitive loops and make errors like hallucinations is a significant limitation, but the exact causes and potential solutions are not fully explored. The impact of fine-tuning or larger models on future agents' capabilities is uncertain.
- **Low Confidence**: The study's ability to rule out future agents' ARA capabilities is limited by the specific tasks and agents evaluated. The generalizability of the findings to other real-world scenarios is unclear.

## Next Checks

1. **Evaluate Agents on a Broader Range of Tasks**: Test the agents on a more diverse set of tasks that cover a wider range of complexities and domains to better understand their limitations and potential for ARA.
2. **Investigate the Impact of Fine-tuning and Scaffolding**: Conduct controlled experiments to assess how fine-tuning the language models or modifying the scaffolding affects the agents' performance on complex tasks, and identify the key factors that contribute to improvements.
3. **Assess Agents' Ability to Learn and Adapt**: Design tasks that require the agents to learn from their mistakes, adapt their strategies, and improve their performance over time, to evaluate their potential for autonomous learning and adaptation.