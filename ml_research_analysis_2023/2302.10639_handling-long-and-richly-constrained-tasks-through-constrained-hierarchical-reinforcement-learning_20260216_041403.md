---
ver: rpa2
title: Handling Long and Richly Constrained Tasks through Constrained Hierarchical
  Reinforcement Learning
arxiv_id: '2302.10639'
source_url: https://arxiv.org/abs/2302.10639
tags:
- cost
- learning
- reward
- path
- constrained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of solving temporally extended decision-making
  problems with complex safety constraints, such as robots cleaning areas while avoiding
  hazards and retaining charge. Existing methods in hierarchical and constrained RL
  are limited in handling long horizon tasks with flexible constraint thresholds.
---

# Handling Long and Richly Constrained Tasks through Constrained Hierarchical Reinforcement Learning

## Quick Facts
- arXiv ID: 2302.10639
- Source URL: https://arxiv.org/abs/2302.10639
- Reference count: 9
- Primary result: Novel Constrained Planning with Reinforcement Learning (CoP-RL) approach that combines lower-level goal-conditioned RL with upper-level constrained RRT* planning to solve temporally extended decision-making problems with complex safety constraints.

## Executive Summary
This paper addresses the challenge of solving long-horizon decision-making problems with complex safety constraints, such as robots navigating environments with obstacles and hazards while managing resources like battery charge. The proposed Constrained Planning with Reinforcement Learning (CoP-RL) approach combines a lower-level goal-conditioned RL agent that learns local reward and cost estimates with an upper-level constrained planner (Informed RRT*) that computes globally optimal policies satisfying cost constraints. The method handles flexible constraint thresholds using CVaR instead of expected cost, allowing adaptation without retraining.

## Method Summary
CoP-RL employs a two-level hierarchy where a lower-level goal-conditioned distributional RL agent learns reward and cost distributions between nearby states, and an upper-level constrained RRT* planner uses these estimates to build a tree toward distant goals while rejecting edges that violate CVaR or expected cost constraints. The approach addresses the limitations of existing hierarchical and constrained RL methods in handling long-horizon tasks with flexible constraint thresholds. The lower-level agent is trained to estimate local value and cost distributions, which the upper-level planner uses during tree expansion, checking CVaR constraints by convolving cost distributions along candidate edges.

## Key Results
- CoP-RL achieves 100% success rate on complex 2D navigation tasks with obstacles and hazards, outperforming SORB
- The approach demonstrates lower path lengths compared to baselines while satisfying various cost constraints
- CoP-RL shows better scalability with O(N log N) complexity versus O(N²) for SORB's complete graph approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-level hierarchy allows long-horizon planning while respecting complex cost constraints.
- Mechanism: Lower-level RL learns local goal-conditioned value functions and cost distributions between nearby states. Upper-level constrained RRT* uses these estimates to build a tree toward the distant goal, rejecting edges that violate CVaR or expected cost constraints.
- Core assumption: Local value and cost estimates are accurate enough for reliable global path planning.
- Evidence anchors:
  - [abstract] "lower-level goal conditioned RL agent that learns reward and cost estimates between nearby states, with an upper-level constrained planner (based on Informed RRT*) that computes reward-maximizing policies from start to far away goal states while satisfying cost constraints."
  - [section] "The lower-level agent does estimate the local costs. Again, similar to above learning of Vw, the fixed learned policy ˆπ allows us to estimate the vector of probability Vw c function directly by minimizing the KL divergence between the target Vt c ... and the current Vw c"
- Break condition: If local estimates become inaccurate over longer distances or in complex environments, the planner may reject too many edges or fail to find feasible paths.

### Mechanism 2
- Claim: Using CVaR instead of expected cost allows flexible safety thresholds that adapt without retraining.
- Mechanism: CVaR captures the tail of the cost distribution, enforcing that the worst α fraction of trajectories stays under the threshold. This is checked by convolving cost distributions along candidate edges during planning.
- Core assumption: The cost distribution learned by the lower level is accurate enough to compute meaningful CVaR bounds.
- Evidence anchors:
  - [abstract] "CoP-RL is that it can handle constraints on the cost value distribution (e.g., on Conditional Value at Risk, CVaR) and can adjust to flexible constraint thresholds without retraining."
  - [section] "We use Conditioned Value-at-Risk (CVaR [Rockafellar et al., 2000]) instead of the expected value of cost to threshold the safety of a policy."
- Break condition: If the cost distribution has heavy tails or is poorly estimated, CVaR calculations may not reflect true risk.

### Mechanism 3
- Claim: Informed RRT* with cost-aware edge validation scales better than SORB's complete graph approach.
- Mechanism: Instead of building a massive graph, RRT* grows a tree from start to goal using sampled points in an informed ellipsoid, only considering edges that can satisfy constraints. This yields O(N log N) complexity versus O(N²) for SORB.
- Core assumption: Sampling within the informed ellipsoid and local rewiring suffice to find high-reward, low-cost paths.
- Evidence anchors:
  - [section] "Construction of complete graph yields a complexity of O(N2) for Dijkstra’s algorithm for N nodes. Hence, a classical path finding approach with the large constructed graph does not scale and as a result SORB is suboptimal in many cases."
  - [section] "In comparison to the O(N2) complexity of the planning algorithm in SORB, the Complexity of Constrained RRT* is O(N logN)."
- Break condition: In very high-dimensional or highly constrained spaces, the informed sampling may miss narrow feasible corridors.

## Foundational Learning

- Concept: Distributional Reinforcement Learning
  - Why needed here: To estimate full cost distributions rather than just expected values, enabling CVaR-based constraints.
  - Quick check question: What does the probability vector [p₁,...,pₙ] in distributional RL represent, and how is it updated?

- Concept: Markov Decision Processes with Constraints (CMDP)
  - Why needed here: The problem is formulated as maximizing reward subject to cost distribution constraints, requiring CMDP theory.
  - Quick check question: How does a percentile-based constraint (CVaR) differ from an expected cost constraint in CMDP formulation?

- Concept: Informed Sampling and RRT* Algorithm
  - Why needed here: The upper planner uses informed RRT* to efficiently explore the state space while respecting cost constraints.
  - Quick check question: What is the purpose of the informed ellipsoid in RRT*, and how does it improve convergence?

## Architecture Onboarding

- Component map:
  - Lower-level distributional RL agent (V and Vc estimation) -> Upper-level Constrained RRT* planner (tree expansion with CVaR validation)

- Critical path:
  1. Train lower-level RL on local state pairs to learn V(s,s') and Vc(s,s') distributions
  2. Run Constrained RRT* from start to goal, using learned estimates and checking CVaR constraints
  3. Execute resulting path using lower-level policy for each segment

- Design tradeoffs:
  - Local vs. global estimation accuracy: More training data improves estimates but increases computation
  - CVaR vs. expected cost: CVaR is more conservative but may be overly restrictive in noisy environments
  - Tree size vs. planning time: Larger trees improve solution quality but slow down planning

- Failure signatures:
  - Planner fails to find any path: Local estimates may be too pessimistic or constraints too tight
  - Found path violates constraints: CVaR calculation error or distribution misestimation
  - Long planning times: Tree growth is inefficient; consider increasing η or adjusting sampling strategy

- First 3 experiments:
  1. Test lower-level RL in a simple 2D maze with known rewards/costs; verify V and Vc estimates match ground truth
  2. Run Constrained RRT* with fixed local estimates in a small obstacle environment; check path optimality and constraint satisfaction
  3. Combine both levels in a stochastic cost environment; tune α and K to balance risk and performance

## Open Questions the Paper Calls Out

- Question: How does the performance of CoP-RL scale with the dimensionality of the state space?
  - Basis in paper: [inferred] The paper mentions the complexity of Constrained RRT* is O(N log N), but does not provide experimental results on higher-dimensional state spaces beyond 2D navigation tasks.
  - Why unresolved: The experiments were limited to 2D navigation tasks. No theoretical analysis or empirical results are provided for higher-dimensional state spaces.
  - What evidence would resolve it: Experiments on CoP-RL in environments with state spaces of dimensions 3 and higher, comparing performance metrics like success rate and path length to the 2D case.

- Question: How sensitive is CoP-RL to the choice of hyperparameters like the rewiring radius rRRT* and the local goal distance η?
  - Basis in paper: [inferred] The paper mentions these hyperparameters but does not provide an ablation study or sensitivity analysis.
  - Why unresolved: The experimental section does not vary these hyperparameters to study their impact on performance. No theoretical guidance is provided for setting these values.
  - What evidence would resolve it: An ablation study varying rRRT* and η over a range of values, reporting performance metrics like success rate and path length. Guidelines for setting these hyperparameters based on problem characteristics.

- Question: How does CoP-RL compare to other hierarchical RL methods like HIRO, HRAC, and HIGL in terms of sample efficiency and performance?
  - Basis in paper: [explicit] The paper mentions these methods as related work but does not provide any empirical comparison.
  - Why unresolved: The experimental section only compares CoP-RL to SORB and GRL. No results are provided for the other mentioned hierarchical RL methods.
  - What evidence would resolve it: Experiments comparing CoP-RL to HIRO, HRAC, and HIGL on the same benchmark tasks, reporting sample efficiency (e.g., number of environment interactions) and performance metrics (e.g., success rate, path length).

## Limitations
- The approach relies heavily on the accuracy of local value and cost estimates from the lower-level RL agent, which may be unreliable in highly stochastic environments
- CVaR-based constraints can be overly conservative in environments with heavy-tailed cost distributions
- The method's scalability to very high-dimensional state spaces or complex constraint structures remains unproven, as experiments were limited to 2D navigation tasks

## Confidence
- **High confidence**: The theoretical framework combining distributional RL with constrained RRT* is sound, and the empirical results show clear advantages over SORB in the tested environments.
- **Medium confidence**: The scalability claims are supported by complexity analysis but lack extensive empirical validation beyond 2D tasks. The adaptability to flexible constraint thresholds is demonstrated but may not generalize to all CMDP formulations.
- **Low confidence**: The method's performance in highly dynamic or partially observable environments is unknown, as the paper focuses on static 2D navigation.

## Next Checks
1. **Distributional RL Accuracy**: Validate that the learned V and Vc distributions accurately capture the true reward and cost distributions in a controlled grid-world environment with known dynamics.
2. **Planner Robustness**: Test the Constrained RRT* planner with injected noise in the local estimates to assess its sensitivity and robustness to estimation errors.
3. **Generalization to Higher Dimensions**: Extend the experiments to 3D navigation tasks or Atari games to evaluate scalability and performance in higher-dimensional state spaces.