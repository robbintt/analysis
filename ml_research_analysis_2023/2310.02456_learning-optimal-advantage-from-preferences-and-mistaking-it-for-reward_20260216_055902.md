---
ver: rpa2
title: Learning Optimal Advantage from Preferences and Mistaking it for Reward
arxiv_id: '2310.02456'
source_url: https://arxiv.org/abs/2310.02456
tags:
- reward
- preferences
- learning
- function
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the consequences of using algorithms designed
  for partial return preference models when preferences actually arise from regret.
  It shows that these algorithms effectively learn an approximation of the optimal
  advantage function, not the reward function.
---

# Learning Optimal Advantage from Preferences and Mistaking it for Reward

## Quick Facts
- arXiv ID: 2310.02456
- Source URL: https://arxiv.org/abs/2310.02456
- Reference count: 14
- The paper shows that when preferences arise from regret but are learned under a partial return model, algorithms effectively learn the optimal advantage function rather than the reward function.

## Executive Summary
This paper investigates what happens when algorithms designed for partial return preference models are used when preferences actually arise from regret. The authors demonstrate that under this misalignment, the learning algorithm effectively captures the optimal advantage function rather than the reward function. Despite this conceptual mismatch, the paper shows that using this learned advantage function as a reward preserves optimal policies, though a simpler greedy maximization approach is preferable. The findings provide insight into why RLHF approaches work well in practice despite imperfect model assumptions.

## Method Summary
The paper analyzes learning reward functions from pairwise preferences over trajectory segments using logistic preference models. It compares two preference generation processes: partial return (where preferences depend on return differences) and regret (where preferences depend on optimal advantage differences). The analysis uses tabular representations in gridworld MDPs with both noiseless and stochastic preferences. The method involves minimizing cross-entropy loss between predicted and observed preferences, then analyzing what function is actually learned under different assumptions about the preference generation process.

## Key Results
- When preferences arise from regret but are learned under the partial return model, the algorithm learns an approximation of the optimal advantage function (bA*r), not a reward function
- Using the learned optimal advantage function as a reward function preserves the set of optimal policies if maxaA*r(·, a) = 0 across all states
- Including transitions from absorbing states encourages maxabA*r(·, a) = 0, improving performance when using bA*r as reward
- The simpler approach of greedy maximization of bA*r is preferable to using it as a reward function with policy improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: When preferences are generated from regret but learned under the partial return model, the algorithm effectively learns an approximation of the optimal advantage function, bA*r, not a reward function.
- Mechanism: The preference model in the learning algorithm (logistic over partial returns) is misaligned with the true preference generation process (logistic over optimal advantages). This causes the learned function to capture advantage information instead of reward values.
- Core assumption: The partial return preference model is used during learning even though preferences are actually generated from regret.
- Evidence anchors:
  - [abstract] "We argue that the learned function is an approximation of the optimal advantage function, bA*r, not a reward function."
  - [section 3.1] "When preferences are instead generated according to regret, g(σt) = A*r(σt) = A*r(sσt, aσt) and the parameters of this optimal advantage function can be learned directly, also via Equation 1."

### Mechanism 2
- Claim: Using the optimal advantage function as a reward function preserves the set of optimal policies.
- Mechanism: When the maximum reward in every state is zero, greedy action selection directly yields optimal policies. Since A*r measures deviation from optimal decisions, using it as reward creates a highly shaped reward landscape where optimal actions are immediately identifiable.
- Core assumption: maxaA*r(·, a) = 0 for all states.
- Evidence anchors:
  - [section 3.2] "Theorem 3.1 (Greedy action is optimal when the maximum reward in every state is 0.)"
  - [section 3.2] "Corollary 3.1 (Policy invariance of rA*r). Let rA*r ≜ A*r. If maxaA*r(·, a) = 0, Π*rA*r = Π*r."

### Mechanism 3
- Claim: Including transitions from absorbing states encourages maxabA*r(·, a) = 0, improving performance when using bA*r as reward.
- Mechanism: Transitions from absorbing states provide preference data where segments have zero regret, which helps the learning algorithm estimate the optimal advantage function with maximum values closer to zero. This reduces the bias introduced by approximation errors.
- Core assumption: Preference data includes segments that start in non-absorbing states and transition to absorbing states.
- Evidence anchors:
  - [section 3.3] "Figure 4 shows the large impact of including transitions from absorbing state when ˆr = bA*r."
  - [section 3.3] "Including segments with transitions from absorbing state encourages maxabA*r(·, a) = 0."

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The entire framework operates within MDP formalism, defining states, actions, rewards, and policies.
  - Quick check question: What are the components of an MDP tuple (S, A, T, γ, D0, r) and how do they interact?

- Concept: Advantage Functions
  - Why needed here: The optimal advantage function A*r measures how much an action reduces expected return relative to following an optimal policy, and is central to understanding what is actually learned.
  - Quick check question: How is the optimal advantage function A*r(s,a) defined in terms of Q* and V*?

- Concept: Preference Models
  - Why needed here: Two preference models are compared (partial return and regret), and understanding their differences is crucial for interpreting the learning outcomes.
  - Quick check question: What is the key mathematical difference between the partial return and regret preference models?

## Architecture Onboarding

- Component map: Preference Dataset Generator -> Learning Algorithm -> Policy Improvement Module -> Evaluation Framework
- Critical path: Preference generation → Learning (minimizing loss) → Policy derivation (greedy or Q-learning) → Evaluation
- Design tradeoffs:
  - Using bA*r as reward vs. greedy maximization of bA*r: The former requires policy improvement but preserves structure; the latter is simpler but may have approximation errors
  - Including absorbing state transitions: Improves maxabA*r(·, a) = 0 property but may introduce bias in certain MDPs
- Failure signatures:
  - Poor performance when maxabA*r(·, a) ≠ 0 across states
  - Suboptimal policies when approximation errors are large
  - Sensitivity to maximum partial return across loops in the MDP
- First 3 experiments:
  1. Generate synthetic preferences using regret model, learn bA*r with partial return assumption, compare greedy bA*r vs. greedy Q*rcA*r performance
  2. Test impact of including transitions from absorbing states on learned bA*r values and resulting policy performance
  3. Vary maximum partial return across loops in MDPs to validate hypothesis about which algorithm performs better

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of learning algorithms differ when preferences are generated according to the partial return model versus the regret model in complex real-world environments?
- Basis in paper: [explicit] The paper discusses the consequences of using algorithms designed for partial return preference models when preferences actually arise from regret, showing that these algorithms effectively learn an approximation of the optimal advantage function, not a reward function.
- Why unresolved: The paper's experiments are primarily conducted in gridworld domains, which are simplified environments. Real-world environments are likely to be more complex and may exhibit different characteristics that could affect the performance of learning algorithms under different preference models.
- What evidence would resolve it: Conducting experiments in complex real-world environments or simulations that more closely resemble real-world tasks, comparing the performance of algorithms under both the partial return and regret preference models.

### Open Question 2
- Question: What are the specific factors that influence whether an algorithm designed for the partial return model performs better or worse than one designed for the regret model when preferences are actually generated by regret?
- Basis in paper: [inferred] The paper mentions that the maximum partial return of all loops in an MDP can determine the direction of performance differences between algorithms, but this factor is not fully characterized.
- Why unresolved: The paper provides a hypothesis and empirical validation for this factor in gridworld domains, but it does not explore other potential factors or provide a comprehensive characterization of all factors that could influence algorithm performance.
- What evidence would resolve it: Conducting a detailed analysis of various factors (e.g., task complexity, state space size, action space size, presence of absorbing states) and their impact on algorithm performance under both preference models, potentially using a combination of theoretical analysis and empirical experiments.

### Open Question 3
- Question: How can the regret preference model be effectively implemented in practice, considering the challenges of learning a differentiable approximation of the optimal advantage function for the reward function?
- Basis in paper: [explicit] The paper mentions that the known method for learning a reward function with the regret preference model requires a differentiable approximation of the optimal advantage function for the reward function arising from parameters that change at each training iteration, which presents research challenges.
- Why unresolved: The paper acknowledges this challenge but does not provide a solution or detailed discussion of potential approaches to address it.
- What evidence would resolve it: Developing and evaluating novel algorithms or techniques for learning reward functions under the regret preference model, potentially involving advancements in differentiable function approximation, meta-learning, or other related areas.

## Limitations
- The analysis focuses on tabular representations in small gridworld MDPs, which may not generalize to high-dimensional continuous state spaces where function approximation introduces additional approximation errors
- The analysis assumes the preference model's form remains fixed during learning, which may not hold if the true preference generation process changes or includes additional factors
- The policy invariance results under the condition maxaA*r(·, a) = 0 rely on perfect approximation, which may be challenging to achieve in practice

## Confidence

- **High Confidence**: The core mechanism that learning under the partial return preference model when preferences arise from regret leads to learning an approximation of the optimal advantage function (bA*r). This is directly demonstrated through theoretical analysis and supported by multiple evidence anchors.
- **Medium Confidence**: The claim that including transitions from absorbing states encourages maxabA*r(·, a) = 0 and improves performance. While supported by empirical results, the underlying mechanism could vary depending on MDP structure and preference sampling methods.
- **Medium Confidence**: The policy invariance results under the condition maxaA*r(·, a) = 0. The theoretical results are sound, but practical implementation may face challenges with approximation errors in function approximation settings.

## Next Checks

1. Test the framework in continuous state-space MDPs using function approximation to verify if the bA*r learning mechanism holds beyond tabular settings.
2. Experiment with preference models that include factors beyond optimal advantage (e.g., state visitation frequency) to test robustness of the learning mechanism.
3. Analyze the impact of different preference sampling strategies on the learned bA*r values and resulting policy performance, particularly focusing on scenarios without absorbing states.