---
ver: rpa2
title: Decouple knowledge from parameters for plug-and-play language modeling
arxiv_id: '2305.11564'
source_url: https://arxiv.org/abs/2305.11564
tags:
- knowledge
- pluglm
- language
- domain
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes PlugLM, a pre-trained language model that decouples
  knowledge storage from model parameters using a differentiable plug-in memory (DPM).
  The key idea is to replace the feed-forward layer with a key-value memory that explicitly
  stores knowledge, enabling interpretable knowledge retrieval.
---

# Decouple knowledge from parameters for plug-and-play language modeling

## Quick Facts
- arXiv ID: 2305.11564
- Source URL: https://arxiv.org/abs/2305.11564
- Authors: 
- Reference count: 40
- One-line primary result: Achieves up to 3.95 F1 improvement in domain adaptation without additional pre-training

## Executive Summary
This paper introduces PlugLM, a pre-trained language model that decouples knowledge storage from model parameters using a differentiable plug-in memory (DPM). The key innovation replaces the feed-forward layer with a key-value memory system that explicitly stores knowledge, enabling interpretable retrieval and updates without parameter modifications. Experiments demonstrate PlugLM's effectiveness in domain adaptation, knowledge updates, and in-task learning across multiple benchmarks.

## Method Summary
PlugLM replaces the feed-forward layer in transformer models with a differentiable plug-in memory (DPM) that stores knowledge in ⟨key, value⟩ slots. The model uses a KnowEncoder to project knowledge text into dense vectors, then employs knowledge attention to retrieve and fuse relevant knowledge from DPM during inference. The framework supports asynchronous index refreshing for end-to-end training, allowing knowledge updates without modifying model parameters. DPM is constructed from Wikipedia corpus and domain-specific knowledge sources, enabling flexible knowledge management across different tasks and domains.

## Key Results
- Achieves up to 3.95 F1 improvement in domain adaptation tasks without additional in-domain pre-training
- Demonstrates 4 F1 improvement in knowledge update scenarios, showing effective knowledge injection
- Successfully applies to in-task knowledge learning through natural language prompting, bridging surface-form distribution gaps

## Why This Works (Mechanism)

### Mechanism 1
Replacing FFN with DPM enables knowledge retrieval without updating model parameters. The DPM stores knowledge in editable ⟨key, value⟩ slots that can be directly modified, bypassing the need for gradient updates to transformer layers. This assumes FFN functions as an unnormalized key-value memory network whose knowledge can be decoupled from parameters.

### Mechanism 2
Knowledge attention over DPM enables interpretable retrieval of task-relevant knowledge. The model uses pooled hidden states as query vectors to retrieve top-N knowledge entries, then fuses them with original hidden states via attention. This assumes retrieved knowledge is semantically relevant to the current context.

### Mechanism 3
Asynchronous index refreshing makes the entire framework end-to-end trainable. The model uses stale indices for MIPS search during forward pass, then recomputes scores using the current KnowEncoder for backward pass. This assumes stale indices remain sufficiently close to current indices for stable training.

## Foundational Learning

- **Key-value memory networks**: Why needed - entire design replaces FFN with key-value memory architecture. Quick check - Can you explain how key-value memory network differs from original FFN formulation?

- **Maximum Inner Product Search (MIPS)**: Why needed - DPM retrieval relies on MIPS to find top-N most relevant knowledge entries. Quick check - What is computational complexity of MIPS in DPM retrieval, and how does it scale with memory size?

- **Knowledge prompting**: Why needed - in-task knowledge learning uses natural language prompts to transform training samples into Wikipedia-style knowledge. Quick check - How does knowledge prompting help bridge surface-form distribution gap between training samples and DPM knowledge?

## Architecture Onboarding

- **Component map**: Transformer encoder -> KnowEncoder -> DPM -> Knowledge attention -> MIPS index
- **Critical path**: 1. Encode input text → hidden states 2. Pool hidden states → query vector 3. MIPS search → top-N knowledge 4. Knowledge attention → fused hidden states 5. Continue transformer forward pass
- **Design tradeoffs**: Flexibility vs latency (DPM enables updates without retraining but adds MIPS latency), Interpretability vs expressiveness (explicit retrieval is interpretable but may be less expressive than learned FFN), Memory vs computation (larger DPM enables more storage but increases MIPS cost)
- **Failure signatures**: Performance degrades when MIPS fails to retrieve relevant knowledge, model becomes unstable if asynchronous refreshing is not synchronized, knowledge injection fails if surface-form mismatch is not handled
- **First 3 experiments**: 1. Replace FFN with random DPM and measure performance drop 2. Test different top-N values for MIPS retrieval and observe performance/latency tradeoff 3. Evaluate catastrophic forgetting by training on multiple domains sequentially with DPM

## Open Questions the Paper Calls Out
1. How does PlugLM's performance scale with increasing memory size and what is optimal trade-off between memory size and computational efficiency?
2. Can PlugLM be effectively applied to other types of knowledge beyond Wikipedia, such as structured knowledge bases or domain-specific knowledge sources?
3. How does PlugLM's knowledge retrieval mechanism handle ambiguous or context-dependent knowledge, and can it be improved to better understand nuances of language?

## Limitations
- Performance heavily depends on quality and relevance of knowledge stored in DPM, with unclear handling of sparse or contradictory knowledge
- Computational overhead from MIPS search during inference lacks comprehensive analysis, particularly for large-scale deployment
- Claims about "interpretable" knowledge retrieval are weakly supported with limited qualitative analysis

## Confidence
- **High Confidence**: DPM enables knowledge updates without parameter modification (4-point F1 improvements in knowledge update tasks)
- **Medium Confidence**: Domain adaptation improvements (up to 3.95 F1) supported by controlled experiments but generalization to unseen domains requires validation
- **Low Confidence**: "Interpretable" knowledge retrieval claim lacks extensive qualitative analysis

## Next Checks
1. Cross-domain stress test: Evaluate PlugLM on tasks requiring knowledge from multiple, unrelated domains simultaneously to test DPM's scalability and knowledge integration capabilities
2. Dynamic knowledge update benchmark: Create benchmark where domain knowledge evolves rapidly over time to measure DPM's effectiveness in maintaining current knowledge without full retraining
3. Computational overhead analysis: Conduct comprehensive latency measurements comparing PlugLM with baseline models across varying DPM sizes and retrieval frequencies