---
ver: rpa2
title: 'Modyn: Data-Centric Machine Learning Pipeline Orchestration'
arxiv_id: '2312.06254'
source_url: https://arxiv.org/abs/2312.06254
tags:
- data
- training
- modyn
- learning
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MODYN is an open-source platform for training ML models on dynamic
  datasets that enables sample-level data selection and triggering policies. It provides
  a modular distributed architecture that separates data selection, triggering, storage,
  and training components to avoid data stalls and optimize throughput.
---

# Modyn: Data-Centric Machine Learning Pipeline Orchestration

## Quick Facts
- arXiv ID: 2312.06254
- Source URL: https://arxiv.org/abs/2312.06254
- Authors: 
- Reference count: 40
- Primary result: MODYN achieves 80-100% of baseline training throughput for memory-bound recommendation workloads and 98-100% for compute-bound computer vision workloads while enabling sample-level data selection.

## Executive Summary
MODYN is an open-source platform designed to train machine learning models on dynamic datasets that continuously grow or change over time. The system enables sample-level data selection and triggering policies through a modular distributed architecture that separates data selection, triggering, storage, and training components. This separation avoids data stalls and optimizes throughput by allowing parallel prefetching of data partitions while training proceeds. The platform supports both presampling and downsampling policies through a unified interface, making it flexible for different use cases while maintaining high performance.

## Method Summary
MODYN implements a distributed pipeline where the supervisor coordinates triggering policies, the selector implements data selection policies, the trainer runs model training, the storage manages data partitions, and the evaluator computes metrics. The system uses partitioned data storage with parallel retrieval threads and metadata backends (PostgreSQL for large scale, local files for small scale) to enable efficient sample-level access. Training workers prefetch data partitions before they are needed, with configurable partition sizes and prefetching threads to balance memory usage and throughput. The platform is implemented in Python with PyTorch integration and supports automatic mixed precision training.

## Key Results
- Achieves 80-100% of baseline training throughput for memory-bound recommendation systems workloads despite sample-level data retrieval
- Maintains 98-100% of baseline throughput for compute-bound computer vision workloads
- Successfully implements sample-level data selection with both uniform random sampling and DLIS GradNorm strategies
- Provides modular architecture enabling policy extension with minimal code changes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MODYN achieves high throughput by partitioning data into fixed-size chunks and prefetching them before training workers need them
- Mechanism: The OnlineDataset uses multiple prefetching threads to load data partitions in parallel while the training loop processes previous batches, avoiding stalls when workers request data
- Core assumption: Partition size can be tuned to balance between too many small requests (high metadata overhead) and too few large requests (stalls during processing)
- Evidence anchors:
  - [abstract] "evaluates MODYN’s training throughput, showing that even in memory-bound recommendation systems workloads, MODYN is able to reach 80 to 100 % of the throughput"
  - [section] "Each worker has a partition buffer of a configurable size. Upon creation, a worker spawns a configurable number of prefetching threads that issue gRPC requests"
  - [corpus] Weak evidence - neighbors discuss memory management but not this specific prefetching pattern

### Mechanism 2
- Claim: Multi-threaded storage retrieval with Postgres partitioning enables fast sample-level data access
- Mechanism: Storage partitions data across files and uses multiple threads to retrieve samples in parallel, with Postgres table partitioning by pipeline/trigger to avoid performance degradation
- Core assumption: Postgres can handle the configured number of parallel workers without becoming the bottleneck
- Evidence anchors:
  - [abstract] "evaluates MODYN’s training throughput, showing that even in memory-bound recommendation systems workloads, MODYN is able to reach 80 to 100 % of the throughput"
  - [section] "We partition the tables. Since for datasets with billions of samples, even SQL bulk insertion is too slow, we use the Postgres internal COPY command and stream the data over the raw connection"
  - [corpus] Weak evidence - neighbors discuss Postgres but not this specific partitioning strategy

### Mechanism 3
- Claim: Separating triggering and data selection concerns enables flexible policy implementation without modifying core system
- Mechanism: The supervisor handles triggering policies while the selector implements data selection, with both communicating through well-defined interfaces that allow pluggable Python implementations
- Core assumption: The separation of concerns doesn't introduce excessive communication overhead that would negate the flexibility benefits
- Evidence anchors:
  - [abstract] "MODYN’s extensible architecture allows users to run training pipelines without modifying the platform code, and enables researchers to effortlessly extend the system"
  - [section] "The supervisor orchestrates the execution of a pipeline and therefore runs the triggering policy. The selector implements data selection policies"
  - [corpus] Weak evidence - neighbors discuss ML pipeline architecture but not this specific separation pattern

## Foundational Learning

- Concept: Sample-level data selection and its performance implications
  - Why needed here: Understanding why MODYN needed to solve the sample-level selection problem differently than traditional batch loading approaches
  - Quick check question: What is the key performance challenge when selecting individual samples versus loading batches sequentially?

- Concept: Data partitioning strategies and their impact on distributed systems
  - Why needed here: The paper relies heavily on partitioning data across multiple dimensions to enable parallel processing
  - Quick check question: How does partitioning by pipeline, trigger, and round-robin modulus help maintain insertion performance in Postgres?

- Concept: Prefetching and buffering strategies in data-intensive systems
  - Why needed here: MODYN's performance gains come from sophisticated prefetching that requires understanding trade-offs between memory usage and throughput
  - Quick check question: Why might prefetching too many partitions actually hurt performance despite the intuitive benefit of more prepared data?

## Architecture Onboarding

- Component map: Supervisor -> Selector -> Trainer -> Storage -> Model Storage
- Critical path: Data → Storage → Selector → Trainer → Model Storage, with Supervisor coordinating triggering
- Design tradeoffs: Flexibility vs performance (Python extensibility vs C++ speed), modularity vs communication overhead, partition size vs memory usage
- Failure signatures: Throughput drops when partition size mismatches batch size, performance degradation when Postgres worker limits are exceeded, memory pressure when prefetching too aggressively
- First 3 experiments:
  1. Run Criteo workload with single worker, no prefetching to establish baseline
  2. Add prefetching and measure throughput improvement
  3. Increase number of workers and adjust partition size to find optimal configuration

## Open Questions the Paper Calls Out

- How does sample-level data selection compare to traditional batch-based approaches in terms of model accuracy and training efficiency across different workload types?
- What are the optimal triggering policies for different types of data drift patterns?
- How does the performance of different metadata backends scale with dataset size and selection policy complexity?
- What are the optimal partition sizes and prefetching strategies for different workload types?
- How does sample-level data selection affect the generalization ability of models compared to full dataset training?

## Limitations

- Evaluation focuses on only two specific workloads (recommendation and computer vision) in controlled conditions
- Does not address scalability to extremely large clusters with hundreds of workers
- Does not evaluate performance under real-world network conditions with varying latency and bandwidth
- Memory usage patterns for very large datasets are not thoroughly characterized

## Confidence

- **High confidence**: The architectural claims about separating triggering and data selection concerns are well-supported by the modular design and interface specifications. The throughput measurements showing 80-100% of baseline for memory-bound workloads and 98-100% for compute-bound workloads are directly supported by the evaluation results.
- **Medium confidence**: The performance claims assume optimal configuration of partition sizes and prefetching parameters, but the paper provides limited guidance on how to determine these values for new workloads. The storage backend scalability claims are based on PostgreSQL partitioning but lack comprehensive testing at scale.
- **Low confidence**: The claims about "effortless extension" and minimal implementation effort for new policies are primarily based on the architectural design rather than empirical evidence from actual users implementing diverse policies.

## Next Checks

1. **Configuration Sensitivity Analysis**: Systematically vary partition sizes, prefetching thread counts, and worker counts across the two benchmark workloads to map the performance landscape and identify optimal configurations for different hardware setups.

2. **Scalability Stress Test**: Evaluate MODYN with 10x and 100x more workers than the current maximum (8 workers) to identify bottlenecks in metadata management, storage access, or inter-component communication that emerge at scale.

3. **Real-World Deployment Validation**: Deploy MODYN in a production environment with continuously growing datasets and varying data distributions to verify that the sample-level selection policies maintain performance and accuracy when faced with non-stationary data patterns.