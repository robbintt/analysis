---
ver: rpa2
title: Hate Speech Detection via Dual Contrastive Learning
arxiv_id: '2307.05578'
source_url: https://arxiv.org/abs/2307.05578
tags:
- hate
- speech
- learning
- contrastive
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of detecting hate speech on
  social media platforms, particularly focusing on two key issues: the complexity
  of semantic information conveyed in hate speech and the imbalanced distribution
  of hate speech and non-hate speech. To tackle these challenges, the authors propose
  a novel dual contrastive learning (DCL) framework.'
---

# Hate Speech Detection via Dual Contrastive Learning

## Quick Facts
- **arXiv ID**: 2307.05578
- **Source URL**: https://arxiv.org/abs/2307.05578
- **Reference count**: 40
- **Primary result**: Novel dual contrastive learning framework achieves state-of-the-art hate speech detection on English datasets

## Executive Summary
This paper addresses the challenges of hate speech detection on social media platforms by proposing a dual contrastive learning (DCL) framework. The model tackles two key issues: the complexity of semantic information in hate speech and the imbalanced distribution of hate and non-hate speech data. By jointly optimizing self-supervised and supervised contrastive learning, the framework captures span-level semantic information beyond simple token-level emotional semantics. The integration of focal loss further addresses data imbalance, enabling the model to effectively distinguish hate speech even when it contains insulting words.

## Method Summary
The proposed dual contrastive learning framework combines self-supervised and supervised contrastive learning to capture comprehensive semantic information for hate speech detection. The model uses pre-trained BERT for sentence embeddings and applies dropout-based data augmentation to create positive and negative pairs for contrastive learning. Self-supervised contrastive learning captures contextual span-level features, while supervised contrastive learning leverages label information to cluster similar samples. Focal loss is integrated to handle class imbalance by focusing on misclassified samples. The model is trained on two English datasets (SemEval-2019 Task-5 and Davidson) with specific hyperparameters including learning rate 1e-4, batch size 128, and carefully tuned temperature coefficients.

## Key Results
- Achieves 67.8% accuracy and 67.2% macro F1 on SemEval-2019 Task-5 dataset
- Achieves 95.9% accuracy and 95.6% weighted F1 on Davidson dataset
- Outperforms state-of-the-art models in detecting hate speech containing insulting words
- Demonstrates effectiveness of dual contrastive learning framework in handling semantic complexity and data imbalance

## Why This Works (Mechanism)

### Mechanism 1
The model improves hate speech detection by leveraging span-level semantic information beyond token-level emotional semantics. Self-supervised contrastive learning creates positive and negative pairs of augmented samples, allowing the model to capture contextual information that helps distinguish between hate and non-hate speech even when both contain insulting words. This approach assumes contextual semantic patterns provide better discriminative power than explicit emotional tokens. The mechanism may fail if contextual semantic patterns don't provide better discrimination or if the augmentation strategy fails to preserve meaningful context.

### Mechanism 2
Supervised contrastive learning improves classification by pulling samples of the same class closer together in embedding space while pushing different classes apart. The supervised contrastive loss uses label information to create clusters of similar samples, improving the model's ability to separate hate speech from non-hate speech. This relies on the assumption that label information is reliable and that samples with the same label share meaningful semantic similarities. The mechanism may break if label information is noisy or if the embedding space doesn't support meaningful clustering based on labels.

### Mechanism 3
Focal loss mitigates the impact of class imbalance by focusing the model's attention on misclassified samples. It reshapes the standard cross-entropy loss by adding a modulating factor that reduces loss contribution from well-classified examples, allowing the model to focus on hard-to-classify samples. This assumes the data is imbalanced and that performance suffers because the model isn't paying enough attention to the minority class. The mechanism may fail if the data isn't actually imbalanced or if focal loss hyperparameters aren't properly tuned.

## Foundational Learning

- **Concept**: Contrastive Learning
  - Why needed here: To capture semantic relationships between samples beyond simple token-level features, enabling the model to understand context and nuance in hate speech
  - Quick check question: What is the difference between self-supervised and supervised contrastive learning, and when would you use each?

- **Concept**: Data Augmentation
  - Why needed here: To create positive and negative pairs for contrastive learning, allowing the model to learn from variations in the input data
  - Quick check question: How does data augmentation help in contrastive learning, and what are some common augmentation techniques used in NLP?

- **Concept**: Focal Loss
  - Why needed here: To address the class imbalance problem in hate speech detection, ensuring that the model pays sufficient attention to the minority class
  - Quick check question: How does focal loss differ from standard cross-entropy loss, and what are the key hyperparameters that control its behavior?

## Architecture Onboarding

- **Component map**: Input sentences → BERT embedding → Dropout augmentation → Self-supervised CL → Supervised CL → Softmax classification → Focal loss + weighted contrastive loss
- **Critical path**: BERT embedding → Augmentation → Self-supervised CL → Supervised CL → Classification → Loss computation
- **Design tradeoffs**: Using pre-trained BERT vs. training from scratch (better initialization vs. task-specific tuning), dropout-based augmentation (simple but may miss variations), focal loss hyperparameter tuning (balance between easy and hard examples)
- **Failure signatures**: Poor performance on hate speech with insulting words (context not captured), high false positive rate (too sensitive to features), high false negative rate (not enough minority class attention)
- **First 3 experiments**: 1) Ablation study: remove self-supervised CL and evaluate impact, 2) Ablation study: remove supervised CL and evaluate impact, 3) Hyperparameter tuning: experiment with different focal loss parameters (alpha and gamma)

## Open Questions the Paper Calls Out

1. How does the performance of DCL vary with different pre-trained language models (e.g., RoBERTa, DistilBERT) instead of BERT?
2. What is the impact of different data augmentation strategies on the performance of DCL?
3. How does the performance of DCL change when applied to hate speech detection in languages other than English?
4. What is the effect of incorporating external knowledge sources (e.g., knowledge graphs, sentiment lexicons) on the performance of DCL?
5. How does the performance of DCL vary with different values of the weighting coefficient λ in the loss function?

## Limitations

- Dataset-specific performance may be inflated due to extreme class imbalance in the Davidson dataset, raising questions about genuine learning vs. overfitting
- Limited evidence for span-level semantic capture beyond claims, lacking attention visualization or qualitative analysis of misclassifications
- No cross-lingual or cross-domain experiments to demonstrate generalization beyond English Twitter data

## Confidence

**High confidence**: The dual contrastive learning framework is a valid methodological approach that logically combines established techniques with a clear and reproducible architectural description.

**Medium confidence**: Reported performance improvements over baselines are likely real but may be influenced by dataset-specific factors and hyperparameter tuning without statistical significance testing.

**Low confidence**: Claims about the model's ability to capture "span-level semantic information" and effectively distinguish hate speech from non-hate speech containing insulting words are not well-supported by empirical evidence.

## Next Checks

1. **Ablation study on focal loss**: Remove focal loss from the model and retrain on the Davidson dataset to quantify its actual contribution to the reported performance and determine whether the extreme accuracy is due to contrastive learning or primarily focal loss handling of class imbalance.

2. **Error analysis on insulting words**: Manually examine 50-100 false positive and false negative predictions where the text contains insulting words to determine whether misclassifications are due to contextual misunderstanding or other factors, validating the claim about span-level semantic understanding.

3. **Cross-dataset validation**: Test the trained models on a held-out validation set from a different source or language to assess generalization, or perform k-fold cross-validation on the combined dataset to ensure results aren't specific to the train/test split used.