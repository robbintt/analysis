---
ver: rpa2
title: Dataset Distillation via the Wasserstein Metric
arxiv_id: '2311.18531'
source_url: https://arxiv.org/abs/2311.18531
tags:
- synthetic
- dataset
- wasserstein
- data
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WMDD (Wasserstein Metric-based Dataset Distillation),
  a method that employs the Wasserstein metric to enhance distribution matching in
  dataset distillation. By computing the Wasserstein barycenter of features from a
  pretrained classifier and leveraging per-class BatchNorm statistics, WMDD effectively
  captures the essence of the original data distribution while maintaining intra-class
  variations.
---

# Dataset Distillation via the Wasserstein Metric

## Quick Facts
- arXiv ID: 2311.18531
- Source URL: https://arxiv.org/abs/2311.18531
- Reference count: 40
- Top-1 accuracy of 87.1% on ImageNet-1K in the 100 images per class setting

## Executive Summary
This paper introduces WMDD (Wasserstein Metric-based Dataset Distillation), a novel approach that leverages the Wasserstein metric to enhance distribution matching in dataset distillation. The method computes the Wasserstein barycenter of features from a pretrained classifier and employs per-class BatchNorm statistics to capture essential characteristics of the original data distribution while preserving intra-class variations. WMDD achieves state-of-the-art performance across various high-resolution datasets, including ImageNet-1K, Tiny ImageNet, and ImageNette.

## Method Summary
WMDD generates synthetic datasets by jointly optimizing two objectives: feature matching and BatchNorm statistics regularization. The method first extracts features from a pretrained classifier for each class, then computes the Wasserstein barycenter to capture the central distribution. Synthetic images are optimized using gradient descent while being regularized by per-class BatchNorm statistics to preserve activation distributions. This dual-constraint approach ensures that synthetic data maintains both the geometric structure of the feature space and the statistical properties of the original data.

## Key Results
- Achieves top-1 accuracy of 87.1% on ImageNet-1K, 61.0% on Tiny ImageNet, and 60.7% on ImageNette with 100 images per class
- Demonstrates significant improvements in synthetic data quality compared to existing techniques
- Shows strong generalization across different architectures including ResNet and ViT models
- Outperforms previous state-of-the-art methods by substantial margins on all tested benchmarks

## Why This Works (Mechanism)

### Mechanism 1
The Wasserstein barycenter better captures the geometric structure of the original data distribution than previous methods. By computing the Wasserstein barycenter of features extracted from a pretrained classifier, WMDD finds a central distribution that minimizes the transport cost to all original data points. This geometric approach preserves the intrinsic manifold structure of the data. The core assumption is that the feature space of a pretrained classifier provides a meaningful embedding where geometric distances correspond to perceptual similarity. A break condition occurs if the pretrained classifier's feature space does not align well with the downstream task's feature space.

### Mechanism 2
Per-class BatchNorm statistics preserve intra-class variations while maintaining inter-class separability. By computing mean and variance statistics separately for each class in the BatchNorm layers, WMDD ensures that synthetic images maintain the distinctive characteristics of each class while capturing the variation within classes. The core assumption is that BatchNorm statistics at different layers capture different levels of feature representation, from low-level textures to high-level semantic features. A break condition occurs if the BatchNorm statistics become too restrictive and prevent the model from exploring the full distribution space.

### Mechanism 3
The combination of feature matching and BatchNorm regularization provides complementary constraints for synthetic data generation. Feature matching ensures the synthetic data matches the overall distribution shape in feature space, while BatchNorm regularization ensures the synthetic data has similar activation statistics to real data, providing a more complete constraint. The core assumption is that both feature-level and activation-level distributions are important for capturing the full characteristics of the data. A break condition occurs if one of the regularization terms dominates, leading to suboptimal synthetic data.

## Foundational Learning

- **Optimal Transport Theory**
  - Why needed here: Provides the theoretical foundation for Wasserstein metrics and barycenters, which are central to the method
  - Quick check question: What is the key difference between Wasserstein distance and KL divergence in terms of what geometric properties they preserve?

- **Batch Normalization**
  - Why needed here: Understanding how BatchNorm statistics capture distributional properties of activations is crucial for implementing the per-class regularization
  - Quick check question: How do the mean and variance computed in BatchNorm layers relate to the distribution of features?

- **Feature Extraction from Pretrained Models**
  - Why needed here: The method relies on extracting meaningful features from a pretrained classifier to compute the Wasserstein barycenter
  - Quick check question: Why might features from a pretrained classifier be more useful than raw pixel values for distribution matching?

## Architecture Onboarding

- **Component map**: Pretrained classifier -> Feature extractor -> Wasserstein barycenter computation -> Synthetic data generation -> Per-class BatchNorm regularization
- **Critical path**: 1. Pretrain classifier on real data, 2. Extract features for each class, 3. Compute Wasserstein barycenter for each class, 4. Generate synthetic data using gradient descent, 5. Apply per-class BatchNorm regularization
- **Design tradeoffs**: Using a more powerful pretrained model might capture better features but increases computation cost; more iterations in gradient descent might improve quality but increase training time; higher regularization coefficient might prevent overfitting but could also restrict exploration of distribution space
- **Failure signatures**: If synthetic data shows high-frequency artifacts, the regularization might be too weak; if different classes collapse into similar synthetic images, the feature matching might be too strong; if synthetic data shows unrealistic colors or textures, the BatchNorm regularization might be misaligned
- **First 3 experiments**: 1. Test feature matching alone (without BatchNorm regularization) to see if it captures class distributions, 2. Test BatchNorm regularization alone (without feature matching) to see if it maintains activation statistics, 3. Test with different regularization coefficients to find the optimal balance between the two objectives

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of WMDD scale with dataset size and complexity beyond the tested benchmarks? The paper demonstrates effectiveness on three high-resolution datasets but doesn't explore scaling to larger or more complex datasets like JFT-300M or larger subsets of Open Images.

### Open Question 2
What is the impact of different pretrained model architectures on WMDD's feature extraction and subsequent synthetic data quality? The paper uses ResNet-18 for all experiments and briefly mentions cross-architecture generalization but doesn't systematically study the effect of different backbone architectures.

### Open Question 3
How sensitive is WMDD to the choice of hyperparameters, particularly the regularization coefficient λ, across different datasets and IPC settings? The paper includes an ablation study on λ but only tests a limited range and focuses on specific IPC settings.

### Open Question 4
Can WMDD be extended to handle multi-label classification tasks or other structured prediction problems beyond standard image classification? The paper focuses exclusively on single-label image classification without exploring applicability to more complex prediction tasks.

## Limitations
- The method's computational complexity of computing Wasserstein barycenters may limit scalability to extremely large datasets
- Dependence on a pretrained classifier introduces potential bias if the feature space doesn't align well with the target task
- The method requires careful hyperparameter tuning, particularly for the regularization coefficient λ, which appears dataset-dependent

## Confidence

- **High confidence**: The method achieves state-of-the-art performance on benchmark datasets, as evidenced by the reported top-1 accuracies
- **Medium confidence**: The theoretical justification for using Wasserstein barycenters and per-class BatchNorm statistics is sound, but empirical evidence for their effectiveness in dataset distillation is limited
- **Low confidence**: The claim that WMDD generalizes well to different architectures is based on limited testing and requires further validation

## Next Checks

1. Perform a systematic ablation study to isolate the contributions of feature matching and BatchNorm regularization to the overall performance
2. Test WMDD on a wider range of architectures (e.g., EfficientNet, MobileNet) to assess its generalization capabilities beyond the tested ResNet and ViT models
3. Evaluate the computational cost of WMDD, particularly the barycenter computation, and compare it with existing dataset distillation methods to assess its practical applicability