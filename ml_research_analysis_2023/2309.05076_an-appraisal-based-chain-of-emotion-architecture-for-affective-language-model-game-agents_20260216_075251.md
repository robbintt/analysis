---
ver: rpa2
title: An Appraisal-Based Chain-Of-Emotion Architecture for Affective Language Model
  Game Agents
arxiv_id: '2309.05076'
source_url: https://arxiv.org/abs/2309.05076
tags:
- language
- affective
- memory
- agents
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores using large language models (LLMs) to simulate
  emotions in game agents, aiming to create more believable and natural digital interactions.
  A key challenge is that human emotions are complex, involving multiple components
  that are not fully understood, making computational representation difficult.
---

# An Appraisal-Based Chain-Of-Emotion Architecture for Affective Language Model Game Agents

## Quick Facts
- arXiv ID: 2309.05076
- Source URL: https://arxiv.org/abs/2309.05076
- Authors: 
- Reference count: 40
- Primary result: Appraisal-based LLM architecture outperforms simpler approaches in emotional understanding and produces more authentic, believable responses in game agent simulations

## Executive Summary
This study explores using large language models to simulate emotions in game agents, aiming to create more believable and natural digital interactions. The authors propose a "chain-of-emotion" architecture based on psychological appraisal theory, where agents first appraise situations to generate emotions and then use those emotions to guide responses. Tested against simpler approaches (no memory and memory-only), the appraisal-based approach showed superior performance in emotional understanding tasks and produced more authentic, nuanced responses in a simulated breakup scenario. User studies demonstrated that agents using this architecture were rated as more natural, emotionally intelligent, and believable than those using simpler approaches.

## Method Summary
The study compares three LLM architectures for game agent emotional simulation: No Memory (baseline), Memory (context inclusion), and Appraisal-based Chain-of-Emotion. All used GPT-3.5-turbo via OpenAI API with temperature=0. The Chain-of-Emotion architecture first generates an emotional appraisal of the situation before producing responses. Testing involved the STEU dataset (42 items) for emotional understanding, LIWC analysis of conversation transcripts from a café breakup scenario, and user studies with 30 participants rating agent believability and emotional intelligence on 6-point Likert scales in a Unity-based game.

## Key Results
- Appraisal prompting achieved 80.0% accuracy on STEU dataset vs 50.0% for memory-only approach
- LIWC analysis showed Chain-of-Emotion responses had higher authenticity and more complex emotional content
- User studies rated appraisal-based agents as significantly more natural, emotionally intelligent, and believable
- Chain-of-Emotion architecture demonstrated early emergence of complex mixed emotions in conversations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Appraisal prompting improves emotional understanding by explicitly simulating cognitive appraisal processes.
- Mechanism: The architecture first generates an emotional appraisal of the situation (e.g., "Chibitea feels confused and sad") before producing the response, enabling the model to ground its output in situational context.
- Core assumption: Appraisal processes are represented in language models' training data and can be elicited through targeted prompting.
- Evidence anchors:
  - [abstract]: "language-based emotion representations might too enable deep learning models to better simulate affective responses"
  - [section]: "The basis of the approach is rooted in traditional PGC research...integrating affect-adaptation"
  - [corpus]: Weak - no direct citations of appraisal-based prompting in related work; most papers focus on emotion generation without explicit appraisal steps.
- Break condition: If the model fails to generate coherent emotional appraisals or if the appraisal step introduces irrelevant or contradictory emotions.

### Mechanism 2
- Claim: Memory integration improves emotional simulation by providing contextual continuity.
- Mechanism: Previous conversation snippets and generated emotions are stored and included in subsequent prompts, enabling the model to build a coherent emotional trajectory.
- Core assumption: Language models can leverage stored context to maintain emotional consistency across exchanges.
- Evidence anchors:
  - [abstract]: "The first questions that we have to ask is how well is affect represented in the training data"
  - [section]: "The 'Memory' condition first stores each user response and the generated text as a memory data structure"
  - [corpus]: Weak - related work mentions memory systems but not specifically for emotional continuity; most focus on action generation or story progression.
- Break condition: If the memory system becomes too large for the model's context window or if stored emotions contradict the current situation.

### Mechanism 3
- Claim: User-perceived believability is enhanced when emotional responses show complexity and authenticity.
- Mechanism: The Chain-of-Emotion architecture generates mixed emotions (e.g., "sadness and vulnerability") rather than pure expressions, creating more nuanced and human-like responses.
- Core assumption: Human emotional experiences are inherently complex and mixed, and users can detect this authenticity.
- Evidence anchors:
  - [abstract]: "User studies showed that agents using this architecture were rated as more natural, emotionally intelligent, and believable"
  - [section]: "The 'Chain-of-emotion' condition showed indications of complex mixed emotions even very early in the conversation"
  - [corpus]: Moderate - some related work on emotional complexity in AI agents, but limited empirical validation of user perception of mixed emotions.
- Break condition: If users rate the agent as less believable due to perceived emotional inconsistency or if mixed emotions confuse rather than enhance authenticity.

## Foundational Learning

- Concept: Appraisal theory of emotion
  - Why needed here: The architecture is explicitly based on cognitive appraisal processes, so understanding how appraisal generates emotions is fundamental to implementing the system.
  - Quick check question: What are the key appraisal variables (e.g., goal relevance, coping potential) that influence emotional outcomes?

- Concept: Prompt engineering and in-context learning
  - Why needed here: The system relies on carefully crafted prompts to elicit emotional appraisals and responses, and understanding how LLMs learn from context is crucial for effective implementation.
  - Quick check question: How does chain-of-thought prompting improve reasoning in LLMs, and how can this be adapted for emotional reasoning?

- Concept: Content analysis with LIWC
  - Why needed here: The system's effectiveness is evaluated using LIWC to quantify emotional content, so understanding how LIWC measures authenticity and emotional tone is important for interpreting results.
  - Quick check question: What linguistic features does LIWC use to calculate authenticity and emotional tone scores?

## Architecture Onboarding

- Component map:
  - Memory System: Stores conversation snippets and generated emotions
  - Appraisal Module: Generates emotional appraisals based on current context
  - Response Generator: Produces agent responses using memory and appraisal outputs
  - LIWC Analyzer: Quantifies emotional content for evaluation
  - User Interface: Manages conversation flow in the game

- Critical path: User input → Memory update → Appraisal generation → Response generation → Output to user

- Design tradeoffs:
  - Memory size vs. context window limitations: Longer conversations may require retrieval systems rather than full storage
  - Appraisal specificity vs. generalization: More detailed appraisals may improve authenticity but reduce flexibility
  - Computational cost vs. emotional accuracy: More complex appraisal processes may yield better emotions but increase latency

- Failure signatures:
  - Inconsistent emotions across exchanges
  - Overly simplistic or stereotypical emotional responses
  - Memory system fails to retrieve relevant context
  - Appraisal module generates incoherent emotional states

- First 3 experiments:
  1. Test appraisal prompting with simple emotion labeling tasks (e.g., STEU dataset) to verify basic emotional understanding
  2. Implement memory system with controlled conversation logs to evaluate context maintenance
  3. Integrate appraisal and memory systems in a simple conversational scenario to test end-to-end emotional coherence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the chain-of-emotion architecture maintain its effectiveness for longer conversations or more complex scenarios beyond the short, controlled scenario tested?
- Basis in paper: [inferred] The paper notes that "the tested game was rather short and all interactions had relevancy, we did not include a memory retrieval step, which might be necessary in longer and more complex games."
- Why unresolved: The study only tested the architecture in a brief, fixed-conversation scenario. It's unclear if the benefits persist in more extended or varied interactions.
- What evidence would resolve it: Testing the architecture in a game with longer, more open-ended conversations and complex scenarios, comparing its performance to simpler architectures.

### Open Question 2
- Question: How does the chain-of-emotion architecture perform with different types of language models, especially more advanced models like GPT-4?
- Basis in paper: [explicit] The paper states "only one large language model (namely OpenAI’s GPT-3.5) was included in the analysis as we had no access to other models. As described in some early reports (e.g. [35]), newer models such as GPT-4 likely outperform previous models on various criteria, making strategies such as appraisal prompting and Chain-Of-Emotion architectures potentially less impactful."
- Why unresolved: The study only used GPT-3.5. The impact of the architecture on newer, more powerful models is unknown.
- What evidence would resolve it: Replicating the study with different language models, including more advanced ones, and comparing their performance with and without the chain-of-emotion architecture.

### Open Question 3
- Question: What are the specific appraisal variables that contribute most to the effectiveness of the chain-of-emotion architecture?
- Basis in paper: [inferred] The paper mentions that "Appraisal therefore represents a flexible process that adapts to individual differences [22] and the current context [38]" and that "appraisal prompting was achieved with the following prompt... 'Briefly describe how Chibitea feels right now given the situation and their personality. Describe why they feel a certain way.'"
- Why unresolved: The study used a general appraisal prompt but did not investigate which specific appraisal variables (e.g., goal relevance, certainty, coping potential) were most influential.
- What evidence would resolve it: Systematically varying the appraisal prompts to focus on different appraisal variables and measuring their impact on the architecture's performance.

## Limitations
- The study uses a fixed prompt-based scenario rather than true interactive gameplay
- Limited sample size of 30 participants may not provide sufficient statistical power
- Reliance on GPT-3.5-turbo raises questions about generalizability to newer models
- Focus on a single emotional scenario (breakup conversation) limits generalizability

## Confidence
- High confidence: The core finding that appraisal-based prompting improves emotional understanding on the STEU dataset is well-supported, with clear quantitative results showing 80.0% vs 50.0% accuracy compared to memory-only approaches.
- Medium confidence: User perception results showing improved believability and emotional intelligence are moderately supported but depend on subjective ratings that may be influenced by presentation order or other experimental factors not fully controlled.
- Low confidence: The generalizability of these findings to different emotional scenarios, game genres, or more advanced LLM architectures remains uncertain due to the limited scope of testing.

## Next Checks
1. **Generalization testing**: Evaluate the Chain-of-Emotion architecture across multiple emotional scenarios (e.g., joy, anger, fear) and game genres to assess whether the benefits extend beyond the breakup conversation tested here.

2. **Long-term interaction validation**: Implement a true interactive version where agents maintain emotional states across extended gameplay sessions, testing whether the appraisal-based approach continues to outperform simpler methods over time.

3. **Ablation study**: Systematically remove or modify components of the Chain-of-Emotion architecture (e.g., remove appraisal step, use different memory systems) to identify which specific mechanisms drive the observed improvements in emotional authenticity.