---
ver: rpa2
title: 'Lexical Squad@Multimodal Hate Speech Event Detection 2023: Multimodal Hate
  Speech Detection using Fused Ensemble Approach'
arxiv_id: '2309.13354'
source_url: https://arxiv.org/abs/2309.13354
tags:
- hate
- speech
- ensemble
- been
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the problem of multimodal hate speech detection
  by classifying text-embedded images into "Hate Speech" and "No Hate Speech" categories.
  The authors propose an ensemble learning approach that combines visual features
  extracted using InceptionV3 and textual features extracted using BERT and XLNet
  models.
---

# Lexical Squad@Multimodal Hate Speech Event Detection 2023: Multimodal Hate Speech Detection using Fused Ensemble Approach

## Quick Facts
- arXiv ID: 2309.13354
- Source URL: https://arxiv.org/abs/2309.13354
- Reference count: 8
- Primary result: Ensemble model achieved 75.21% accuracy and 74.96% F1-score on multimodal hate speech detection

## Executive Summary
This study addresses multimodal hate speech detection by classifying text-embedded images as "Hate Speech" or "No Hate Speech" using an ensemble learning approach. The authors combine visual features from InceptionV3 with textual features from BERT and XLNet models, achieving competitive performance on a Russia-Ukraine conflict dataset. The work demonstrates the potential of multimodal fusion for improving hate speech detection accuracy beyond unimodal approaches.

## Method Summary
The method employs a stacked ensemble architecture that extracts 512-dimensional visual features from text-embedded images using InceptionV3, and 768-dimensional textual features from OCR-extracted text using BERT and XLNet. These embeddings are reduced to 512 dimensions via linear layers, concatenated into a 1536-dimensional vector, and processed through a final linear layer for binary classification. The model is trained for 100 epochs using Adam optimizer with learning rate 3e-4 on approximately 4700 text-embedded images.

## Key Results
- Ensemble model achieved 75.21% accuracy and 74.96% F1-score on test data
- Performance exceeds individual model baselines (BERT, XLNet, InceptionV3)
- Demonstrates effectiveness of multimodal fusion for hate speech detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ensemble combines complementary strengths of visual and textual models
- Mechanism: Fuses InceptionV3 visual features with BERT/XLNet textual features via stacked ensemble, allowing capture of both visual context and linguistic semantics
- Core assumption: Visual and textual modalities provide complementary information for hate speech detection
- Evidence anchors: Abstract reports 75.21% accuracy and 74.96% F1-score; XLNet description highlights enhanced language representation
- Break condition: Performance degrades if either modality is corrupted or missing

### Mechanism 2
- Claim: Pretrained transformers provide robust contextual embeddings
- Mechanism: BERT and XLNet fine-tuned on dataset after pretraining on large corpora to understand nuanced language patterns
- Core assumption: Pretrained language models transfer well to hate speech detection
- Evidence anchors: BERT described as multi-layer bidirectional Transformer encoder; XLNet utilizes Transformer architecture
- Break condition: Performance suffers if training data distribution differs significantly from pretraining data

### Mechanism 3
- Claim: InceptionV3 CNN captures relevant visual features for hate speech detection
- Mechanism: Processes images through deep convolutional architecture with auxiliary classifiers to produce discriminative visual features
- Core assumption: Visual features in text-embedded images contain discriminative information for classification
- Evidence anchors: Inception-v3 architecture described with improvements including auxiliary classifier; authors note specific words/phrases cause prediction issues
- Break condition: Minimal contribution if visual features are not discriminative (mostly text with little visual variation)

## Foundational Learning

- Concept: Multimodal data fusion
  - Why needed here: Task requires integration of visual and textual information from text-embedded images
  - Quick check question: How does ensemble architecture combine visual and textual embeddings before classification?

- Concept: Pretrained transformer models
  - Why needed here: BERT and XLNet provide contextual language understanding for detecting nuanced hate speech patterns
  - Quick check question: What is the role of fine-tuning in adapting pretrained transformers to hate speech detection task?

- Concept: Convolutional neural networks for image feature extraction
  - Why needed here: InceptionV3 extracts visual features that may contain discriminative patterns for hate speech detection
  - Quick check question: What specific architectural improvements in InceptionV3 make it suitable for this task?

## Architecture Onboarding

- Component map:
  Image → InceptionV3 → 512-dim → Concat → Linear → 128-dim → Classification
  Text → OCR → BERT/XLNet → 768-dim → Linear → 512-dim → Concat → Linear → 128-dim → Classification

- Critical path: Multimodal feature extraction through individual models, concatenation of embeddings, dimensionality reduction, and final classification

- Design tradeoffs:
  - Complexity vs. performance: Ensemble increases complexity but improves accuracy over individual models
  - OCR dependency: Performance depends on accurate text extraction from images
  - Computational cost: Multiple large pretrained models increase training and inference time

- Failure signatures:
  - Poor OCR accuracy degrading textual model performance
  - Non-discriminative visual features causing InceptionV3 to contribute noise
  - Overfitting on small dataset despite regularization

- First 3 experiments:
  1. Evaluate individual model performance (BERT, XLNet, InceptionV3) on validation set
  2. Test concatenated feature fusion without final linear layers to assess complementarity
  3. Implement ablation study removing each modality to quantify individual contributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model handle detection of hate speech embedded in visual sarcasm or requiring historical context?
- Basis in paper: Authors explicitly mention model struggles with visual sarcasm and historical context
- Why unresolved: Current architecture and training data may not capture nuances of visual sarcasm and historical context
- What evidence would resolve it: Experimental results comparing performance on images with/without visual sarcasm and historical context, along with detailed failure analysis

### Open Question 2
- Question: How does model handle multilingual text-embedded images?
- Basis in paper: Authors mention future intention to develop multilingual models, implying current limitations
- Why unresolved: Model likely trained on primarily English text-embedded images, may not generalize to other languages
- What evidence would resolve it: Experimental results comparing performance on multilingual text-embedded images with analysis of challenges and solutions

### Open Question 3
- Question: How does model handle images containing both hate speech and non-hate speech elements?
- Basis in paper: Authors mention model may struggle with specific words/phrases that could be interpreted as hate speech
- Why unresolved: Model may lack sophisticated understanding of context and nuance
- What evidence would resolve it: Experimental results comparing performance on mixed-content images with detailed decision-making analysis

## Limitations
- Absence of ablation studies showing individual model contributions to ensemble performance
- Critical dependency on OCR accuracy for textual component
- Dataset limited to Russia-Ukraine conflict context, raising generalizability concerns

## Confidence

- **High confidence**: Ensemble architecture and training methodology are clearly specified and reproducible
- **Medium confidence**: Reported accuracy (75.21%) and F1-score (74.96%) are plausible for this task
- **Low confidence**: Claims about visual features contributing meaningfully lack supporting evidence from ablation studies

## Next Checks

1. **Ablation study**: Remove each modality (InceptionV3, BERT, XLNet) from ensemble and measure performance degradation to quantify individual contributions and verify ensemble benefits.

2. **OCR robustness analysis**: Evaluate model performance with varying OCR quality levels to determine sensitivity to text extraction accuracy and identify potential failure modes.

3. **Cross-domain generalization**: Test trained model on hate speech datasets from different contexts (different languages, social media platforms, or conflict regions) to assess generalizability beyond Russia-Ukraine conflict dataset.