---
ver: rpa2
title: Stratified-NMF for Heterogeneous Data
arxiv_id: '2311.10789'
source_url: https://arxiv.org/abs/2311.10789
tags:
- data
- strata
- stratified-nmf
- matrix
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of applying non-negative matrix
  factorization (NMF) to heterogeneous datasets with strata-dependent statistics.
  The proposed Stratified-NMF method extends the NMF objective to simultaneously learn
  strata-dependent shifts and a shared topics matrix.
---

# Stratified-NMF for Heterogeneous Data

## Quick Facts
- arXiv ID: 2311.10789
- Source URL: https://arxiv.org/abs/2311.10789
- Reference count: 29
- Final normalized loss on synthetic dataset: 9.7e-4

## Executive Summary
Stratified-NMF extends standard NMF to heterogeneous datasets by learning both shared global features and strata-dependent shifts. The method decomposes data into a shared topics matrix and stratum-specific non-negative shifts, enabling interpretable feature extraction across subgroups. Experiments demonstrate superior interpretability and sparsity preservation compared to baseline methods.

## Method Summary
Stratified-NMF modifies the NMF objective to simultaneously learn strata-dependent shifts and a shared topics matrix. The data matrix for each stratum is decomposed as the sum of a stratum-specific non-negative shift vector and a low-rank factorization with a shared dictionary. Multiplicative update rules are derived for all parameters, preserving non-negativity and guaranteeing non-increasing objective value through block reformulation of the problem.

## Key Results
- On synthetic data, Stratified-NMF achieved normalized loss of 9.7e-4 while recovering true strata shifts
- Top three words from strata features exactly matched topic labels in 20 newsgroups dataset
- Local strata features remained sparse (~20% non-zero) while preserving interpretability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model learns both shared global features and strata-specific shifts by decomposing the data matrix as \( A^{(i)} - 1v^{(i)T} \) before applying NMF.
- Mechanism: For each stratum, a non-negative shift vector \( v^{(i)} \) is subtracted from all samples, isolating stratum-specific effects. The residual data is then factorized into \( W^{(i)}H \) where \( H \) is shared across strata. This allows \( H \) to capture global structure while \( v^{(i)} \) captures local deviations.
- Core assumption: The underlying data distribution for each stratum can be modeled as a shared distribution plus a non-negative, stratum-dependent shift.
- Evidence anchors:
  - [abstract] "We resolve this problem by solving a modified NMF objective, Stratified-NMF, that simultaneously learns strata-dependent statistics and a shared topics matrix."
  - [section II] "The data is stored row-wise so that \( A^{(i)}_{jk} \) denotes the k'th attribute of data point j in stratum i. Each data point is assumed to be sampled from a distribution \( D_i = D + z_i \) where D is a shared, non-negative distribution and \( z_i \in \mathbb{R}^n_+ \) is a strata dependent shift."
- Break condition: If the strata-dependent shifts are not non-negative or if the shared structure assumption does not hold, the model will fail to separate global and local effects correctly.

### Mechanism 2
- Claim: The multiplicative update rules preserve non-negativity constraints and guarantee non-increasing objective value.
- Mechanism: Updates are constructed using auxiliary functions for the stratified-NMF objective. By showing the auxiliary function is non-negative definite, the updates are proven to decrease the objective. The update formulas are derived by reformulating the problem into a block matrix format compatible with standard NMF convergence proofs.
- Core assumption: The reformulated block matrix objective retains the same structure as the standard NMF objective, allowing use of existing convergence theorems.
- Evidence anchors:
  - [section III] "Theorem 2 (Convergence of Multiplicative Update Rules). The Stratified-NMF objective defined in Equation 2 is non-increasing under the update rules defined in Lines 5, 7, and 8 of Algorithm 1."
  - [section III] "We reformulate the Stratified-NMF objective into a large block formulation resulting in a standard NMF objective with s additional columns in W and s additional rows in H."
- Break condition: If the block reformulation is incorrect or the auxiliary function derivation fails, convergence guarantees are lost.

### Mechanism 3
- Claim: Sparse strata features \( v^{(i)} \) capture interpretable local patterns while the shared matrix \( H \) captures global topics.
- Mechanism: Experiments on 20 newsgroups show that the top words from \( v^{(i)} \) exactly match the topic of each newsgroup, demonstrating interpretability. The sparsity of \( v^{(i)} \) is preserved, allowing clear separation of local and global features.
- Core assumption: Local features can be represented as sparse vectors over the shared vocabulary.
- Evidence anchors:
  - [section IV.D] "Figure 6 displays the top three, highest weighted, words captured by the strata features for each category. We observe that for each of the twenty stratum, the top words captured by the strata features correspond exactly to their topic."
  - [section IV.D] "Finally, we observed that the local strata features were sparse with approximately 20% nonzero, highlighting sparsity preservation properties of Stratified-NMF."
- Break condition: If local features are not sparse or if they overlap heavily with global features, interpretability degrades.

## Foundational Learning

- Concept: Non-negative Matrix Factorization (NMF)
  - Why needed here: NMF provides the low-rank, non-negative decomposition that forms the basis for capturing interpretable features in heterogeneous data.
  - Quick check question: What property of NMF makes it particularly suitable for topic modeling in text data?

- Concept: Multiplicative Update Rules
  - Why needed here: These updates ensure non-negativity is preserved and the objective is non-increasing without requiring projection steps.
  - Quick check question: How do multiplicative updates differ from gradient descent updates in terms of constraint handling?

- Concept: Stratified Sampling / Subgroup Analysis
  - Why needed here: The method assumes data naturally falls into strata that share a global structure but differ locally; understanding stratification is key to applying the method correctly.
  - Quick check question: What is the difference between stratifying data and normalizing each stratum independently?

## Architecture Onboarding

- Component map: Data matrices A(i) -> Shift learning v(i) -> Local basis W(i) -> Shared dictionary H -> Objective monitor
- Critical path:
  1. Initialize v(i), W(i), H with random non-negative values
  2. Iterate: update v(i) -> update W(i) -> update H
  3. Monitor convergence via normalized loss
  4. Extract interpretable features from v(i) and H
- Design tradeoffs:
  - More strata increase parameters linearly but preserve shared H to limit overfitting
  - Rank selection balances expressiveness vs. sparsity; higher rank risks overparameterization
  - Initialization affects convergence speed but not final loss (empirically)
- Failure signatures:
  - Loss plateaus early: possible poor initialization or rank too low
  - v(i) become dense: strata shifts not properly isolated
  - H fails to capture shared patterns: strata shifts too large or model misspecified
- First 3 experiments:
  1. Synthetic data with known strata shifts: verify convergence and recover true v(i)
  2. California housing: stratify by income and check interpretability of v(i)
  3. 20 newsgroups: stratify by topic and validate that v(i) words match topic labels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Stratified-NMF be extended to other NMF variants like sparsity-constrained NMF, semi-supervised NMF, online NMF, and tensor NMF?
- Basis in paper: [explicit] The authors discuss this as a potential future direction in the discussion section, stating "A very natural question is if one can apply these stratification techniques to existing NMF formulations."
- Why unresolved: The paper only introduces and analyzes the basic Stratified-NMF method. Extending it to other variants requires further research and development.
- What evidence would resolve it: Successful implementation and evaluation of Stratified-NMF extensions for other NMF variants on various datasets, demonstrating improved performance or interpretability compared to existing methods.

### Open Question 2
- Question: What are the optimal initialization strategies for the matrices in Stratified-NMF to improve convergence and results?
- Basis in paper: [explicit] The authors mention this as a potential area for future work in the discussion section, stating "Initialization of Stratified-NMF Matrices [28]".
- Why unresolved: The paper uses random initialization for the matrices, but does not explore other initialization strategies. Optimal initialization could lead to faster convergence and better results.
- What evidence would resolve it: Comparative study of different initialization strategies for Stratified-NMF on various datasets, demonstrating the impact on convergence speed and final results.

### Open Question 3
- Question: How can the number of learnable parameters for each stratum in Stratified-NMF be automatically determined or optimized?
- Basis in paper: [inferred] The authors mention this as a potential future direction in the discussion section, stating "Variable number of learnable parameters for each stratum".
- Why unresolved: The paper uses a fixed number of parameters for each stratum, but different strata may require different levels of complexity. Automatically determining the optimal number of parameters could improve the model's performance and interpretability.
- What evidence would resolve it: Development and evaluation of methods for automatically determining the optimal number of learnable parameters for each stratum in Stratified-NMF, demonstrating improved performance or interpretability compared to fixed parameter settings.

## Limitations
- Assumption of non-negative strata shifts may be restrictive for some datasets
- Lack of sensitivity analysis for rank selection across different datasets
- No statistical significance testing reported for interpretability results

## Confidence
- High confidence: Learning shared global features H and strata-specific shifts v(i) due to direct algorithmic specification and experimental validation
- Medium confidence: Multiplicative update convergence proof, as it relies on block reformulation of the objective
- Low confidence: Generality of the strata shift assumption, as it requires data to be well-modeled as shared distribution plus non-negative stratum-dependent shifts

## Next Checks
1. **Synthetic Stress Test**: Generate synthetic data with known strata shifts, run Stratified-NMF, and quantitatively measure recovery error for v(i) and H as a function of noise level and rank
2. **Rank Sensitivity Analysis**: For a real dataset (e.g., 20 newsgroups), vary the rank and measure normalized loss, sparsity of v(i), and interpretability score; report stability of top features
3. **Convergence Benchmarking**: Compare convergence speed and final loss of Stratified-NMF against baseline NMF applied per stratum and other multi-task NMF variants on the housing dataset