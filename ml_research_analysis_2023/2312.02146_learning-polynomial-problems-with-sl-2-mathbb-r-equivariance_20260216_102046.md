---
ver: rpa2
title: Learning Polynomial Problems with $SL(2,\mathbb{R})$ Equivariance
arxiv_id: '2312.02146'
source_url: https://arxiv.org/abs/2312.02146
tags:
- equivariant
- polynomial
- equivariance
- architecture
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of neural networks for solving
  polynomial optimization problems, specifically focusing on certifying the positivity
  of polynomials. The authors observe that these problems exhibit equivariance under
  the non-compact group SL(2, R), which consists of area-preserving linear transformations.
---

# Learning Polynomial Problems with $SL(2,\mathbb{R})$ Equivariance

## Quick Facts
- arXiv ID: 2312.02146
- Source URL: https://arxiv.org/abs/2312.02146
- Reference count: 40
- One-line primary result: Data augmentation outperforms full SL(2,R) equivariant architectures due to lack of universality for non-compact groups

## Executive Summary
This paper investigates neural network approaches for polynomial optimization problems, specifically certifying polynomial positivity. The authors identify that these problems exhibit SL(2,R) equivariance (area-preserving linear transformations) and explore adapting learning pipelines accordingly. Surprisingly, they find that data augmentation approaches outperform strict SL(2,R) equivariant architectures in practice. They prove this arises from an unusual lack of universality for SL(2,R) - there exists an equivariant function that cannot be approximated by any sequence of equivariant polynomials. This provides a rare example where data augmentation is theoretically and empirically superior to full equivariance.

## Method Summary
The authors implement three approaches: a basic MLP with data augmentation over SL(2,R), an SO(2,R)-equivariant architecture, and an SL(2,R)-equivariant architecture using tensor products and Clebsch-Gordan decompositions. They generate synthetic datasets of binary forms (homogeneous polynomials in two variables) from random rotationally symmetric distributions and Delsarte spherical code bounds. All models are trained using Adam optimizer with learning rate 3e-4, comparing performance on test sets with and without SL(2,R) transformations.

## Key Results
- Data augmentation over well-conditioned subsets of SL(2,R) outperforms strict SL(2,R) equivariant architectures
- The SL(2,R)-equivariant architecture is proven to lack universality - certain equivariant functions cannot be approximated by equivariant polynomials
- SO(2,R) equivariant architectures with data augmentation provide a practical middle ground with good performance
- Neural networks achieve tenfold speedups while maintaining high accuracy for polynomial optimization problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Polynomial positivity verification is SL(2,R)-equivariant, allowing neural networks to learn transformation-consistent patterns
- Mechanism: If matrix M certifies positivity for polynomial p(x,y), then gᵀMg certifies positivity for p(g⁻¹x) where g ∈ SL(2,R)
- Core assumption: Polynomial distributions exhibit this specific SL(2,R) symmetry structure
- Evidence anchors: [abstract] "these polynomial learning problems are equivariant to the non-compact group SL(2,R)"; [section 3] "the optimizer f(p) has the equivariance property f(p(Ax⃗)) = A[d]ᵀ f(p(x⃗))A[d]"

### Mechanism 2
- Claim: Data augmentation outperforms full SL(2,R) equivariance due to architectural limitations in approximating smooth SL(2,R)-equivariant functions
- Mechanism: Authors prove an SL(2,R)-equivariant function exists that cannot be uniformly approximated by any sequence of SL(2,R)-equivariant polynomials
- Core assumption: Lack of universality extends to any tensor-product-based architecture
- Evidence anchors: [abstract] "we prove that there exists an SL(2,R)-equivariant function for which there is no sequence of equivariant polynomials"; [section 4.3] "the architecture of Section 4.2 is not universal"

### Mechanism 3
- Claim: SO(2,R) equivariant architectures with data augmentation provide a practical middle ground
- Mechanism: SO(2,R) captures important symmetries while avoiding pathological cases that break SL(2,R) universality
- Core assumption: SO(2,R) subgroup captures most relevant problem structure
- Evidence anchors: [abstract] "we compare the performance of other equivariance-inspired techniques"; [section 5] "we empirically explore other equivariance-inspired techniques"

## Foundational Learning

- Concept: Representation theory of Lie groups (SL(2,R) and its irreps)
  - Why needed here: Architecture design relies on understanding how SL(2,R) acts on polynomial vector spaces and tensor product decompositions
  - Quick check question: What are the irreducible representations of SL(2,R) relevant to binary forms, and how do tensor products decompose?

- Concept: Non-compact group symmetries and Haar measure
  - Why needed here: Non-compact groups lack finite invariant measures, affecting data augmentation and architectural design
  - Quick check question: Why can't we form invariants by group averaging for SL(2,R), and what are the implications?

- Concept: Polynomial optimization and sum-of-squares certificates
  - Why needed here: Application domain involves certifying polynomial nonnegativity via semidefinite programming
  - Quick check question: How does the sum-of-squares method relate to finding positive semidefinite matrix certificates?

## Architecture Onboarding

- Component map: Input binary homogeneous polynomials → Tensor products → Clebsch-Gordan decomposition → Linear combinations within irrep spaces → MLP (if invariants present) → Final linear map to symmetric matrix space
- Critical path: Input → Tensor products → Clebsch-Gordan decomposition → Linear combinations → MLP (if invariants present) → Final linear map → Output
- Design tradeoffs:
  - Full SL(2,R) equivariance vs. practical performance: Full equivariance provides theoretical guarantees but may be limited by lack of universality
  - Conditioning vs. distribution shift: Poorly conditioned SL(2,R) transformations induce larger distribution shifts but may be necessary for generalization
  - Architecture depth vs. numerical stability: More layers increase expressiveness but compound numerical errors
- Failure signatures:
  - Numerical instability: Large condition numbers causing exploding/vanishing gradients
  - Poor generalization: Test error increases significantly under SL(2,R) transformations
  - Architecture limitations: Inability to fit known solutions (e.g., x⁸ + y⁸)
- First 3 experiments:
  1. Train SL2Net on simple polynomials (e.g., x² + y²) and verify equivariance on transformed inputs
  2. Compare SL2Net performance with MLP + rotation augmentation on same dataset
  3. Test SL2Net's ability to fit x⁸ + y⁸ example to verify universality limitation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design equivariant architectures for non-compact Lie groups that achieve universality over all smooth equivariant functions?
- Basis in paper: [explicit] The authors prove no SL(2,R)-equivariant architecture based on tensor products can approximate all SL(2,R)-equivariant functions
- Why unresolved: The proof relies on a specific function that cannot be approximated by equivariant polynomials, but alternative approaches might overcome this
- What evidence would resolve it: Constructing an equivariant architecture that approximates any smooth equivariant function, or proving impossibility for all non-compact groups

### Open Question 2
- Question: What is the optimal balance between strict equivariance enforcement and data augmentation for non-compact groups?
- Basis in paper: [explicit] Data augmentation outperforms strict SL(2,R) equivariance in practice
- Why unresolved: Optimal augmentation level and its relationship to group conditioning is not fully explored
- What evidence would resolve it: Empirical studies comparing various levels of data augmentation and equivariance for different non-compact groups

## Limitations

- The universality limitation is proven for SL(2,R) specifically but may not generalize to all non-compact groups
- The practical significance of the limitation is uncertain - the counter-example x⁸ + y⁸ may be highly specialized
- Implementation details for the SL(2,R)-equivariant architecture (Clebsch-Gordan coefficients, numerical precision) are not fully specified

## Confidence

- **High Confidence**: Mathematical proof of SL(2,R) universality limitation is rigorous; empirical comparison between architectures is well-executed
- **Medium Confidence**: Generalization to other non-compact symmetry groups is plausible but not established
- **Low Confidence**: Practical significance for real-world applications is uncertain without testing on diverse polynomial optimization problems

## Next Checks

1. Implement the SL(2,R)-equivariant architecture with exact Clebsch-Gordan decomposition and test on the x⁸ + y⁸ example to verify the universality limitation

2. Test on alternative non-compact groups such as the affine group or Poincaré group to determine whether the universality limitation is specific to SL(2,R)

3. Evaluate on real-world polynomial optimization problems from scientific computing applications to assess practical impact beyond synthetic datasets