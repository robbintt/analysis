---
ver: rpa2
title: 'More for Less: Compact Convolutional Transformers Enable Robust Medical Image
  Classification with Limited Data'
arxiv_id: '2307.00213'
source_url: https://arxiv.org/abs/2307.00213
tags:
- cell
- data
- dataset
- types
- convolutional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates Compact Convolutional Transformers (CCTs)
  for robust medical image classification with limited data. CCTs combine convolutional
  layers with transformer architecture to achieve high accuracy on modestly sized
  datasets.
---

# More for Less: Compact Convolutional Transformers Enable Robust Medical Image Classification with Limited Data

## Quick Facts
- arXiv ID: 2307.00213
- Source URL: https://arxiv.org/abs/2307.00213
- Reference count: 0
- Key outcome: CCTs achieve 92.49% accuracy and 0.9935 ROC AUC on peripheral blood cell classification with only 17,092 images

## Executive Summary
This study investigates Compact Convolutional Transformers (CCTs) for robust medical image classification with limited data. CCTs combine convolutional layers with transformer architecture to achieve high accuracy on modestly sized datasets. The authors applied CCTs to a benchmark dataset of peripheral blood cell images, consisting of approximately 2,000 low-resolution samples per cell type across eight classes. Despite the dataset being smaller than typical Vision Transformer applications, CCTs achieved a classification accuracy of 92.49% and a micro-average ROC AUC of 0.9935.

## Method Summary
The authors applied CCTs to the BloodMNIST dataset (17,092 peripheral blood cell images across eight classes, approximately 2,000 samples per class, resized to 28x28x3 pixels). The CCT architecture used convolutional tokenization followed by eight transformer layers, trained for 75 epochs with AdamW optimizer (learning rate 0.0018, weight decay 0.00012), batch size 64, and categorical cross-entropy loss with label smoothing. Data augmentation included random cropping and flipping.

## Key Results
- CCTs achieved 92.49% classification accuracy on peripheral blood cell images
- Micro-average ROC AUC of 0.9935 demonstrates excellent class discrimination
- Validation accuracy exceeded 80% after just five epochs, indicating rapid learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CCTs overcome data scarcity by combining convolutional feature extraction with transformer attention mechanisms.
- Mechanism: Convolutional layers first reduce the spatial dimensionality and create a manageable token set from the image, then transformer layers apply attention to capture long-range dependencies between these tokens.
- Core assumption: Convolutional tokenization can adequately preserve the spatial and feature information necessary for accurate classification.
- Evidence anchors:
  - [abstract] "A hybrid of transformers and convolutional layers, CCTs demonstrate high accuracy on modestly sized datasets."
  - [section] "CCTs were shown to achieve high accuracy on image classification tasks with modestly sized datasets, making them a promising approach for medical imaging tasks."
- Break condition: If the convolutional tokenization loses critical spatial relationships, the transformer attention cannot recover them, leading to degraded accuracy.

### Mechanism 2
- Claim: Stochastic depth regularization and smaller patch sizes enable robust training on limited data.
- Mechanism: During training, random layers are dropped (stochastic depth), preventing overfitting to noise in small datasets. Smaller image patches (28x28) mean fewer tokens, reducing model complexity and computational cost while retaining sufficient discriminative information.
- Core assumption: The dataset contains enough intrinsic signal that even with aggressive regularization, the model can learn generalizable patterns.
- Evidence anchors:
  - [section] "We apply stochastic depth regularization and layer normalization to reduce overfitting."
  - [section] "Stochastic depth regularization will randomly drop a set of layers during training which helps combat overfitting."
- Break condition: If the dataset is too small or too noisy, even stochastic depth cannot prevent overfitting, and validation performance will diverge from training.

### Mechanism 3
- Claim: Low-resolution inputs (28x28x3) reduce the number of learnable parameters, making the model less prone to overfitting.
- Mechanism: By resizing high-resolution images to 28x28, the effective input space is compressed, lowering the parameter count of the convolutional tokenizer and transformer layers.
- Core assumption: The essential morphological features for distinguishing blood cell types are still present at 28x28 resolution.
- Evidence anchors:
  - [section] "Specifically, we used a relatively modest benchmark dataset composed of 17,092 peripheral blood cell images, covering eight distinct cell types. The dataset had around two thousand low-resolution (28x28x3) samples for each of eight cell types on average."
  - [section] "Additionally, the ability to achieve high accuracy using low-resolution images suggests that CCTs may be robust to variations in image quality, which is another common challenge in medical imaging."
- Break condition: If critical cell features are lost at low resolution, classification accuracy will drop sharply, especially for morphologically similar cell types.

## Foundational Learning

- Concept: Convolutional tokenization of images into patches
  - Why needed here: Transforms 2D image data into a sequence of tokens that can be processed by transformer layers.
  - Quick check question: How does the CCTTokenizer differ from the standard ViT patchify approach?

- Concept: Multi-head self-attention in transformers
  - Why needed here: Allows the model to weigh the importance of different image patches relative to each other, capturing global context.
  - Quick check question: What is the role of positional embeddings in the transformer layers of CCT?

- Concept: Stochastic depth regularization
  - Why needed here: Randomly drops entire layers during training to prevent overfitting, crucial when data is limited.
  - Quick check question: How does stochastic depth differ from dropout, and why is it effective in CCTs?

## Architecture Onboarding

- Component map: Input (28x28x3 images) → CCTTokenizer (conv + maxpool → patches + positional embedding) → Transformer layers (8 layers, 4-head self-attention + FFN) → LayerNorm → Dense (classification) → Output logits
- Critical path: CCTTokenizer → Transformer layers → Classification head. Each stage must preserve discriminative features; failure early in the path propagates downstream.
- Design tradeoffs:
  - Small patch size reduces computation but may lose fine detail
  - Fewer transformer layers and heads reduce overfitting risk but may limit representational power
  - Low learning rate (0.0018) and weight decay (0.00012) stabilize training on small data
- Failure signatures:
  - Overfitting: Training accuracy >> validation accuracy; loss divergence
  - Underfitting: Both training and validation accuracy plateau low (<60%)
  - Class imbalance effects: Some cell types show low recall/precision (e.g., cell type 5)
- First 3 experiments:
  1. Train with only the convolutional tokenizer (no transformer layers) to assess baseline CNN performance
  2. Increase patch size or number of transformer layers to see if accuracy improves or overfitting worsens
  3. Apply different regularization strengths (vary stochastic depth rate, dropout) to find optimal generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CCT performance scale with dataset size, and what is the minimum dataset size required to achieve comparable accuracy to larger datasets?
- Basis in paper: [inferred] The paper demonstrates CCT's effectiveness on a small dataset but does not explore performance across varying dataset sizes.
- Why unresolved: The study focuses on a single dataset size and does not investigate how CCT performance changes with larger or smaller datasets.
- What evidence would resolve it: Experiments comparing CCT performance across datasets of varying sizes, from very small to large, would clarify the relationship between dataset size and model accuracy.

### Open Question 2
- Question: How do CCTs perform on higher-resolution medical images compared to the low-resolution (28x28) images used in this study?
- Basis in paper: [explicit] The paper uses low-resolution images and notes CCTs' robustness to image quality variations but does not test higher-resolution images.
- Why unresolved: The study only evaluates CCT performance on low-resolution images, leaving the question of how resolution impacts accuracy unanswered.
- What evidence would resolve it: Testing CCTs on higher-resolution medical images and comparing performance metrics would determine if resolution significantly affects accuracy.

### Open Question 3
- Question: How does CCT performance compare to traditional CNNs or other hybrid models on the same medical imaging tasks?
- Basis in paper: [inferred] The paper highlights CCT's advantages over Vision Transformers but does not directly compare CCT to other architectures like CNNs.
- Why unresolved: The study focuses on CCT's performance without benchmarking it against other established models.
- What evidence would resolve it: Conducting head-to-head comparisons of CCTs with CNNs and other hybrid models on the same dataset would reveal relative strengths and weaknesses.

## Limitations

- Major architectural details (CCTTokenizer configuration, attention mechanism specifics) are not fully specified
- Results only validated on a single dataset (BloodMNIST) with 17,092 images, limiting generalization claims
- No comprehensive ablation studies to quantify individual component contributions to performance

## Confidence

**High Confidence**: The CCT approach effectively combines convolutional and transformer architectures to achieve high accuracy on small datasets, as demonstrated by the 92.49% classification accuracy and 0.9935 ROC AUC on BloodMNIST.

**Medium Confidence**: The mechanisms by which convolutional tokenization, stochastic depth regularization, and low-resolution inputs prevent overfitting are plausible but not fully empirically validated.

**Low Confidence**: Generalization claims to other medical imaging tasks and the assertion that CCTs universally address data scarcity challenges lack supporting evidence beyond this single dataset.

## Next Checks

1. **Implement and compare baseline variants**: Create and evaluate a pure CNN version of the model (removing transformer layers) and a reduced CCT version (fewer transformer layers) to quantify the specific contribution of transformer architecture to the performance gains.

2. **Cross-dataset validation**: Apply the same CCT architecture to at least two additional medical imaging datasets of comparable or larger size (e.g., ChestXRay or different MedMNIST variants) to assess generalization performance and identify dataset-specific limitations.

3. **Resolution sensitivity analysis**: Systematically evaluate model performance across multiple input resolutions (16x16, 28x28, 56x56, 112x112) to quantify the tradeoff between computational efficiency and classification accuracy, particularly for morphologically similar cell types.