---
ver: rpa2
title: Empirical evaluation of Uncertainty Quantification in Retrieval-Augmented Language
  Models for Science
arxiv_id: '2311.09358'
source_url: https://arxiv.org/abs/2311.09358
tags:
- scientific
- uncertainty
- entropy
- atlas
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates uncertainty quantification (UQ) in retrieval-augmented
  language models (RALMs) for scientific tasks. It compares two ATLAS RALM variants:
  one pretrained on general web data plus scientific literature (CC+Wiki+S2ORC) and
  another trained solely on scientific literature (S2ORC).'
---

# Empirical evaluation of Uncertainty Quantification in Retrieval-Augmented Language Models for Science

## Quick Facts
- arXiv ID: 2311.09358
- Source URL: https://arxiv.org/abs/2311.09358
- Reference count: 8
- Two ATLAS RALM variants show poor calibration, being more confident in inaccurate predictions than accurate ones

## Executive Summary
This paper evaluates uncertainty quantification (UQ) in retrieval-augmented language models (RALMs) for scientific tasks, comparing two ATLAS variants: one pretrained on general web data plus scientific literature (CC+Wiki+S2ORC) and another trained solely on scientific literature (S2ORC). Using normalized predictive entropy and semantic entropy as UQ measures, the study finds that the CC+Wiki+S2ORC model is more confident than the S2ORC model across scientific benchmarks. Both models show poor calibration, being more confident in inaccurate predictions than accurate ones, indicating that scientific knowledge alone does not improve calibration. The findings highlight ongoing challenges in developing reliable UQ methods for RALMs in scientific applications.

## Method Summary
The study compares two ATLAS RALM variants trained on different pretraining corpora (CC+Wiki+S2ORC vs S2ORC) using scientific benchmarks (SciRepEval and MMLU). UQ is measured using normalized predictive entropy and semantic entropy computed from beam search outputs. The experimental setup involves running models on scientific classification tasks and analyzing uncertainty scores relative to prediction accuracy. The S2ORC corpus provides 354M text passages from 31.1M academic papers as the scientific retrieval corpus.

## Key Results
- Both RALM variants show poor calibration, being more confident in inaccurate predictions than accurate ones
- The CC+Wiki+S2ORC model (mixed-domain pretraining) exhibits higher uncertainty than the S2ORC model (science-only pretraining)
- Scientific pretraining alone does not improve calibration compared to general-domain pretraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scientific pretraining alone does not improve calibration; models remain overconfident in inaccurate predictions.
- Mechanism: Calibration requires training signals that penalize confident but wrong predictions. Pretraining on scientific literature only changes the knowledge distribution, not the calibration signal structure.
- Core assumption: Calibration is independent of the knowledge domain and depends on training objective structure.
- Evidence anchors:
  - [abstract] "Scientific knowledge provided either as pretraining or retrieval corpus does not help alleviate this issue."
  - [section] "RALMs are overconfident in their predictions, making inaccurate predictions more confidently than accurate ones."
  - [corpus] Weak: No direct calibration signal in S2ORC; pretraining data lacks calibration-focused loss terms.
- Break condition: If calibration is domain-specific (e.g., scientific reasoning requires special calibration), then scientific pretraining might help.

### Mechanism 2
- Claim: Mixed-domain pretraining (CC+Wiki+S2ORC) yields lower uncertainty than science-only pretraining (S2ORC).
- Mechanism: General-domain data increases token-level diversity and semantic variability, leading to broader predictive distributions and higher entropy during inference.
- Core assumption: Uncertainty correlates with the diversity of pretraining data; more diverse pretraining leads to more conservative predictions.
- Evidence anchors:
  - [abstract] "the model finetuned with scientific knowledge as the retrieval data tends to be more confident in generating predictions compared to the model pretrained only with scientific knowledge."
  - [section] "general domain data from sources like CC and Wikipedia ... has much more diversity and heterogeneity, which allows for more robust pretraining of the language model."
  - [corpus] Weak: S2ORC has narrower topical scope, leading to overconfident predictions in unfamiliar domains.
- Break condition: If scientific tasks are narrow enough that pretraining diversity does not matter, the difference may vanish.

### Mechanism 3
- Claim: Semantic entropy captures model uncertainty better than normalized predictive entropy for scientific tasks.
- Mechanism: Semantic entropy clusters semantically equivalent generations, reducing spurious uncertainty from token-level variation, which is crucial when multiple phrasings are valid in scientific language.
- Core assumption: Scientific text has many valid paraphrases; token-level entropy overestimates uncertainty in such cases.
- Evidence anchors:
  - [abstract] "Semantic Entropy ... is a measure of the uncertainty of a generated text sequence that uses linguistic invariances."
  - [section] "predictive entropy could be high if a model is uncertain about whether to generate 'Japan's capital is Tokyo' or 'Tokyo is Japan's capital'. However model's uncertainty is actually low in this example since both sequence are semantically equivalent."
  - [corpus] Weak: No explicit scientific paraphrase pairs in the corpus; assumes scientific text behaves similarly.
- Break condition: If scientific questions have single correct phrasing, semantic entropy may not provide benefit.

## Foundational Learning

- Concept: Temperature scaling for calibration
  - Why needed here: RALMs output probabilities that need calibration to reflect true correctness likelihood.
  - Quick check question: If a model assigns 90% confidence to predictions, what should its empirical accuracy be after perfect calibration?

- Concept: Entropy and uncertainty quantification
  - Why needed here: Both normalized predictive entropy and semantic entropy rely on entropy to measure uncertainty in RALM outputs.
  - Quick check question: What does higher entropy indicate about a model's certainty in its predictions?

- Concept: Dense retrieval and indexing
  - Why needed here: RALMs depend on retrievers (e.g., Contriever) to fetch relevant scientific documents for generation.
  - Quick check question: Why is freezing the document encoder during query-side fine-tuning computationally efficient?

## Architecture Onboarding

- Component map:
  - Retriever: Dense retrieval (Contriever) → document index → top-k passages
  - Reader: ATLAS RALM (T5-based) → fusion-in-decoder → text generation
  - UQ module: Entropy calculators (normalized predictive, semantic) → uncertainty scores
- Critical path: Input query → retrieval → context fusion → generation → uncertainty scoring
- Design tradeoffs:
  - Scientific pretraining vs. mixed-domain: trade-off between domain specialization and calibration robustness.
  - Semantic vs. predictive entropy: trade-off between computational cost and uncertainty expressiveness.
- Failure signatures:
  - Overconfident inaccurate predictions → calibration failure
  - High semantic entropy despite correct answer → semantic clustering issues
  - Retrieval failure → no context passed to reader → poor generation
- First 3 experiments:
  1. Compare normalized predictive entropy vs. semantic entropy on a small set of scientific QA pairs.
  2. Run calibration curve analysis (accuracy vs. confidence) for both ATLAS variants.
  3. Vary beam size and sampling temperature to see impact on uncertainty scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does uncertainty quantification performance vary across different scientific domains when using larger RALM models (e.g., models with >1B parameters)?
- Basis in paper: [inferred] The authors note plans to conduct experiments on larger scale models to analyze the impact of scale on UQ measures.
- Why unresolved: The current study only evaluates ATLAS models with 220M parameters, leaving the question of whether larger models would show different UQ behavior unanswered.
- What evidence would resolve it: Comparative experiments across multiple model sizes (e.g., 220M, 1B, 8B parameters) using the same UQ measures on identical scientific benchmark datasets, showing whether uncertainty scores and calibration improve with model scale.

### Open Question 2
- Question: What specific architectural modifications to RALMs could improve calibration in scientific tasks beyond the standard temperature scaling approach?
- Basis in paper: [explicit] The authors state "Finding calibration methods for RALMs, especially when applied to scientific tasks, remains a work in progress."
- Why unresolved: The paper identifies calibration issues in RALMs but does not explore architectural solutions beyond noting this as future work.
- What evidence would resolve it: Implementation and evaluation of RALM variants with modified architectures (e.g., different fusion strategies, attention mechanisms, or calibration layers) compared against baseline ATLAS models on scientific benchmarks, demonstrating improvements in uncertainty-calibrated predictions.

### Open Question 3
- Question: How do different retrieval corpus characteristics (e.g., corpus size, document diversity, recency) affect uncertainty quantification in RALMs for scientific tasks?
- Basis in paper: [inferred] The study compares models trained on S2ORC (scientific) vs. CC+Wiki+S2ORC (mixed) corpora, but doesn't systematically vary corpus characteristics.
- Why unresolved: The paper suggests that corpus composition affects uncertainty but doesn't isolate specific corpus properties that influence UQ.
- What evidence would resolve it: Controlled experiments varying individual corpus characteristics (e.g., same document count but different diversity, or same diversity but different document age ranges) while keeping other factors constant, measuring resulting changes in UQ metrics across scientific benchmarks.

## Limitations

- Study only evaluates two specific ATLAS model variants without exploring alternative architectures or training objectives
- Limited empirical validation of semantic entropy's effectiveness for scientific paraphrasing
- Cannot definitively explain why scientific pretraining doesn't improve calibration

## Confidence

*High confidence*: The observation that both RALM variants exhibit poor calibration, being more confident in inaccurate predictions than accurate ones.

*Medium confidence*: The claim that mixed-domain pretraining (CC+Wiki+S2ORC) produces lower uncertainty than science-only pretraining (S2ORC).

*Low confidence*: The assertion that semantic entropy is superior for scientific tasks due to paraphrase handling.

## Next Checks

1. Conduct ablation studies varying pretraining corpus composition systematically (e.g., 25%, 50%, 75% scientific content) to identify thresholds where calibration behavior changes.

2. Implement controlled experiments with synthetic scientific paraphrase pairs to rigorously test whether semantic entropy correctly identifies semantically equivalent but token-different outputs.

3. Test additional calibration techniques beyond pretraining choices, such as temperature scaling, label smoothing, or confidence penalties during fine-tuning.