---
ver: rpa2
title: Unraveling Feature Extraction Mechanisms in Neural Networks
arxiv_id: '2310.16350'
source_url: https://arxiv.org/abs/2310.16350
tags:
- label
- token
- tokens
- will
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how neural networks capture features during
  training using Neural Tangent Kernels (NTKs) under infinite-width conditions. The
  authors propose analyzing learning dynamics to understand how fundamental models
  like MLP, CNN, self-attention, and matrix-vector architectures extract statistical
  co-occurrence features between tokens and labels.
---

# Unraveling Feature Extraction Mechanisms in Neural Networks

## Quick Facts
- **arXiv ID**: 2310.16350
- **Source URL**: https://arxiv.org/abs/2310.16350
- **Reference count**: 40
- **Primary result**: Theoretical analysis reveals how MLP, CNN, self-attention, and matrix-vector models extract token-label co-occurrence features, with self-attention being bias-free and matrix-vector excelling at n-gram feature capture

## Executive Summary
This paper investigates how neural networks capture features during training using Neural Tangent Kernels (NTKs) under infinite-width conditions. The authors propose analyzing learning dynamics to understand how fundamental models like MLP, CNN, self-attention, and matrix-vector architectures extract statistical co-occurrence features between tokens and labels. Key findings include that MLP and CNN models can learn token-label co-occurrence features but may suffer from feature bias depending on activation functions and initialization, while self-attention captures similar features without bias. The matrix-vector model excels at capturing n-gram features, particularly for negation phenomena. Experimental results validate these theoretical insights and extend to language modeling tasks.

## Method Summary
The paper employs Neural Tangent Kernel (NTK) analysis to study feature extraction in infinite-width neural networks. By examining how label scores evolve during training through gradient descent dynamics, the authors identify the statistical features each architecture captures. The approach involves theoretical derivation of learning dynamics for fundamental models (MLP, CNN, self-attention, matrix-vector) and validation through experiments on text classification datasets. The analysis focuses on how these models capture token-label co-occurrence statistics and the factors affecting their performance.

## Key Results
- MLP and CNN models learn token-label co-occurrence features but exhibit feature bias influenced by activation functions and initialization
- Self-attention models capture token-label co-occurrence features without bias, making them more robust
- Matrix-vector models excel at capturing n-gram features, particularly for handling negation phenomena
- Feature extraction patterns are validated across multiple datasets including SST, IMDB, Agnews, and language modeling tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Infinite-width neural networks converge to constant Neural Tangent Kernels (NTKs) during training, enabling interpretable learning dynamics
- Mechanism: As network width approaches infinity, NTKs converge to deterministic kernels determined at initialization and remain constant during training, allowing analysis of feature extraction through gradient descent dynamics
- Core assumption: Parameters are initialized with Gaussian distributions, and network width approaches infinity
- Evidence anchors:
  - [abstract]: "considering the infinite network width, we hypothesize the learning dynamics of target models may intuitively unravel the features they acquire from training data"
  - [section]: "When the network width approaches infinity, the NTK will converge and stay constant during training (Jacot et al., 2018; Arora et al., 2019; Yang and Littwin, 2021)"
  - [corpus]: Weak - only 1/5 related papers mention NTKs directly
- Break condition: Network width is finite, parameters not Gaussian-distributed, or NTK does not converge to constant

### Mechanism 2
- Claim: Different neural architectures extract different types of features from training data
- Mechanism: MLP and CNN models capture token-label co-occurrence features through linear combinations, while self-attention captures similar features without bias, and matrix-vector models excel at capturing n-gram features
- Core assumption: Features can be decomposed into basic units (tokens or n-grams) and their statistical relationships with labels
- Evidence anchors:
  - [abstract]: "we apply our approach to several fundamental models and reveal how these models leverage statistical features during gradient descent"
  - [section]: "we identify significant factors such as the choice of activation and unveil the limitations of these models"
  - [corpus]: Moderate - 2/5 papers discuss architectural differences in feature extraction
- Break condition: Architectures don't follow assumed patterns, or feature decomposition doesn't apply

### Mechanism 3
- Claim: Activation function choice affects feature extraction bias and model performance
- Mechanism: ReLU activation can introduce feature bias by creating imbalanced contributions between positive and negative tokens, while alternatives like GeLU and SiLU are more robust
- Core assumption: Activation functions can be characterized by their effect on gradient flow and feature balance
- Evidence anchors:
  - [abstract]: "we also discovered that the choice of activation function can affect feature extraction"
  - [section]: "incorporating an MLP with ReLU activation after the SA model reintroduces bias, as can be observed in Figure 2i"
  - [corpus]: Weak - only 1/5 papers mention activation functions specifically
- Break condition: Alternative activations don't provide bias reduction, or bias isn't the primary factor affecting performance

## Foundational Learning

- Concept: Neural Tangent Kernel convergence
  - Why needed here: Understanding how NTKs converge to constant values in infinite-width networks is fundamental to the theoretical analysis approach
  - Quick check question: What conditions guarantee NTK convergence in neural networks?

- Concept: Gradient descent learning dynamics
  - Why needed here: The paper analyzes feature extraction by studying how label scores evolve during training through gradient updates
  - Quick check question: How do learning dynamics relate to feature acquisition in neural networks?

- Concept: Token-label co-occurrence statistics
  - Why needed here: The paper focuses on understanding how models capture statistical relationships between tokens and labels
  - Quick check question: What is the relationship between token-label co-occurrence and classification decisions?

## Architecture Onboarding

- Component map: MLP → token embeddings → hidden layer with activation → output layer; CNN → token embeddings → convolutional layers with sliding windows → output layer; Self-Attention → token embeddings → attention mechanism → output layer; Matrix-Vector → token embeddings → matrix-vector multiplication → output layer
- Critical path: Token embeddings → feature extraction mechanism → output layer → label score computation → gradient updates
- Design tradeoffs: Simplicity vs. expressiveness (MLP vs. self-attention), bias vs. robustness (ReLU vs. GeLU), linear vs. non-linear composition (CNN vs. matrix-vector)
- Failure signatures: Poor performance on n-gram tasks (CNN, self-attention), feature bias (MLP with ReLU), inability to capture negation (linear combination models)
- First 3 experiments:
  1. Verify NTK convergence by training models with increasing width and measuring NTK stability
  2. Compare feature extraction between architectures on simple token-label co-occurrence tasks
  3. Test activation function effects on feature bias using synthetic datasets with known label distributions

## Open Questions the Paper Calls Out

- **Open Question 1**: How do the feature extraction mechanisms change when the network width is finite (as opposed to infinite-width)?
  - Basis in paper: [inferred] The paper primarily analyzes feature extraction under infinite-width conditions but acknowledges that patterns are still observed with finite widths like d=64.
  - Why unresolved: The analysis relies on NTK convergence properties that are theoretically guaranteed only in the infinite-width limit. Finite-width behavior may deviate significantly.
  - What evidence would resolve it: Empirical studies comparing feature extraction patterns across different finite widths (e.g., d=32, 64, 128) and theoretical bounds on NTK approximation error for finite networks.

- **Open Question 2**: What specific role does the matrix-vector multiplication mechanism play in capturing negation phenomena that attention and CNN models cannot?
  - Basis in paper: [explicit] The paper demonstrates that MV models can capture negation for negative adjectives while CNN and SA models fail, but does not explain the underlying mechanism.
  - Why unresolved: The paper shows empirical differences but doesn't provide theoretical analysis of why MV multiplication specifically handles negation better than other operations.
  - What evidence would resolve it: Formal analysis of how matrix-vector multiplication composes semantic representations differently from linear combinations, and controlled experiments testing various composition operations.

- **Open Question 3**: How do normalization layers and deeper architectures affect the feature extraction mechanisms identified in fundamental models?
  - Basis in paper: [explicit] The paper acknowledges that analyzing complex models like Transformers is challenging due to normalization and depth, and suggests this as future work.
  - Why unresolved: The analysis is limited to single-layer models without normalization, while practical models incorporate these components extensively.
  - What evidence would resolve it: Layer-wise analysis of feature extraction in deep networks with normalization, or ablation studies systematically removing normalization and depth to isolate their effects.

## Limitations
- Theoretical analysis relies on infinite-width NTK assumptions that may not hold in practical finite-width scenarios
- Empirical validation is primarily conducted on relatively small text classification datasets
- Some architectural details and parameter specifications are underspecified, making exact reproduction challenging
- Analysis focuses on supervised classification tasks, leaving open questions about unsupervised or reinforcement learning settings

## Confidence
- **High Confidence**: Claims about NTK convergence in infinite-width networks and basic feature extraction patterns (e.g., token-label co-occurrence learning)
- **Medium Confidence**: Claims about activation function effects on feature bias, as these depend on specific initialization schemes and dataset characteristics
- **Medium Confidence**: Claims about architectural differences in feature extraction, as the analysis is primarily theoretical and limited experimental validation exists across diverse tasks

## Next Checks
1. Test NTK convergence empirically by training models with increasing width and measuring kernel stability, then compare feature extraction patterns between finite and infinite-width regimes
2. Conduct ablation studies on activation functions across diverse datasets to quantify bias effects and validate theoretical predictions about ReLU vs alternative activations
3. Extend experimental validation to larger-scale language modeling benchmarks (e.g., GLUE, SuperGLUE) to assess the practical relevance of theoretical insights for real-world applications