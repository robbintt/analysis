---
ver: rpa2
title: Towards Fair and Calibrated Models
arxiv_id: '2310.10399'
source_url: https://arxiv.org/abs/2310.10399
tags:
- loss
- calibration
- accuracy
- fairness
- fair
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of building models that are both
  fair and calibrated. The authors propose a specific definition of fairness that
  requires models to not amplify existing biases in the data.
---

# Towards Fair and Calibrated Models

## Quick Facts
- arXiv ID: 2310.10399
- Source URL: https://arxiv.org/abs/2310.10399
- Reference count: 40
- One-line primary result: Proposes post-processing and train-time techniques to achieve fair and calibrated models by ensuring group-wise calibration with respect to sensitive attributes.

## Executive Summary
This paper addresses the challenge of building machine learning models that are both fair and well-calibrated. The authors propose a novel approach where ensuring group-wise calibration with respect to sensitive attributes automatically results in fairness under their specific definition. They introduce post-processing techniques using dual temperature scaling and modified calibration losses to achieve group-wise calibration. Through extensive experiments on diverse benchmark datasets, they demonstrate that their techniques can effectively balance fairness, calibration, and accuracy.

## Method Summary
The authors propose a framework where group-wise calibration implies fairness. They introduce dual temperature scaling as a post-processing technique, applying separate temperature parameters to each sensitive group to improve calibration without affecting accuracy. Additionally, they develop train-time techniques using modified calibration losses (Label smoothing, Focal loss, DCA, MDCA, MMCE, MMCE-W) that are computed separately for each sensitive group. The method involves training a 2-layer perceptron with these group-wise losses and evaluating the resulting models on fairness and calibration metrics.

## Key Results
- Group-wise calibration with respect to sensitive attributes automatically ensures fairness under the paper's definition.
- Dual temperature scaling can improve both calibration and fairness without changing model accuracy.
- The proposed methods demonstrate effective trade-offs between fairness, calibration, and accuracy across multiple benchmark datasets.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Group-wise calibration with respect to sensitive attributes automatically results in fairness under the paper's definition.
- Mechanism: The paper proves that ensuring calibration within each sensitive group (Lemma 3.8) implies perfect fairness (Lemma 3.9) because the conditional probability of predictions matches the conditional probability of true labels within each group.
- Core assumption: The model's predicted probabilities within each sensitive group are aligned with actual outcome frequencies in that group.
- Evidence anchors:
  - [abstract]: "We show that ensuring group-wise calibration with respect to the sensitive attributes automatically results in a fair model under our definition."
  - [section 3.2]: Proof that group-wise calibrated models are perfectly fair through conditional expectation arguments.
  - [corpus]: Related works on calibration and fairness (e.g., [Kleinberg et al. 2017], [Pleiss et al. 2017]) suggest tension between these objectives, making this mechanism notable.
- Break condition: If the base rates (Pr[Y=k|A=0] vs Pr[Y=k|A=1]) differ significantly, calibration might still fail to ensure fairness if the model doesn't learn these conditional distributions accurately.

### Mechanism 2
- Claim: Dual temperature scaling can improve both calibration and fairness without changing model accuracy.
- Mechanism: By applying separate temperature scaling parameters to each sensitive group, the confidence scores are calibrated differently for each group while preserving the argmax predictions (which determine accuracy and deterministic fairness).
- Core assumption: The validation set is representative of both sensitive groups and can provide reliable estimates for temperature parameters.
- Evidence anchors:
  - [abstract]: "We propose a variant of Temperature scaling [7] which we show can also achieve fairness under our definition."
  - [section 4.1]: Description of dual temperature scaling and its advantage of preserving accuracy.
  - [corpus]: Standard temperature scaling works for calibration (Guo et al. 2017), and this extends it to group-wise calibration.
- Break condition: If one sensitive group has very few samples in the validation set, temperature parameters for that group may be poorly estimated.

### Mechanism 3
- Claim: Group-wise loss reweighting can optimize for both fairness and calibration simultaneously.
- Mechanism: By partitioning training batches by sensitive group and computing calibration losses separately for each group, the model is encouraged to be well-calibrated within each group, which implies fairness.
- Core assumption: The convex combination parameter ρ can be tuned to balance the importance of calibration across groups.
- Evidence anchors:
  - [section 4.2]: Description of group-wise loss functions like Lg(B) that assign equal weight to loss minimization for both sensitive groups.
  - [corpus]: Label smoothing, focal loss, and other calibration techniques exist but haven't been applied group-wise before.
- Break condition: If the model architecture cannot represent the conditional distributions well, group-wise calibration losses may not lead to fairness.

## Foundational Learning

- Concept: Calibration and fairness definitions
  - Why needed here: The paper builds on specific definitions of calibration (Definition 3.1, 3.2) and fairness (Definition 3.4, 3.6) that are critical to understanding the mechanisms.
  - Quick check question: What is the difference between perfect calibration (Definition 3.1) and weak calibration (Definition 3.2)?

- Concept: Conditional probability and expectation
  - Why needed here: The proofs (Lemma 3.8, 3.9) rely on properties of conditional expectation and probability to show that group-wise calibration implies fairness.
  - Quick check question: How does the total probability theorem help prove that group-wise calibration implies perfect calibration?

- Concept: Temperature scaling and its variants
  - Why needed here: Dual temperature scaling is a key post-processing technique, and understanding standard temperature scaling (Guo et al. 2017) is necessary to grasp the extension.
  - Quick check question: What happens to the confidence scores as temperature T approaches 0, 1, or infinity?

## Architecture Onboarding

- Component map: Base model (2-layer perceptron) -> Group-wise loss functions -> Dual temperature scaling post-processing -> Evaluation metrics (ECE, PE-fairness)
- Critical path: Train base model → Apply group-wise calibration losses → Evaluate fairness and calibration → Apply dual temperature scaling if needed
- Design tradeoffs: Group-wise losses add computational overhead but improve fairness; temperature scaling preserves accuracy but requires representative validation data
- Failure signatures: Poor fairness improvement despite good calibration suggests the model isn't learning conditional distributions; high ECE after temperature scaling indicates poor validation set representation
- First 3 experiments:
  1. Train with standard cross-entropy and evaluate baseline ECE and PE-fairness
  2. Apply dual temperature scaling and measure improvements in ECE and stochastic PE-fairness
  3. Train with group-wise calibration loss (e.g., MMCE) and compare fairness-calibration tradeoff to baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method's fairness guarantee hold up when dealing with more than two sensitive groups?
- Basis in paper: [inferred] The paper focuses on binary classification with two sensitive groups. The theoretical framework and experiments are all based on this binary case.
- Why unresolved: The paper does not explore the generalization of the method to multi-group scenarios. The mathematical proofs and experimental results are limited to the binary case.
- What evidence would resolve it: Extending the theoretical framework and empirical evaluation to datasets with more than two sensitive groups would provide evidence for the method's generalizability.

### Open Question 2
- Question: What is the impact of different temperature scaling strategies (e.g., individual vs. group-wise) on the trade-off between fairness, calibration, and accuracy?
- Basis in paper: [explicit] The paper proposes dual temperature scaling as a post-processing technique, which uses separate temperature parameters for each sensitive group.
- Why unresolved: While the paper presents experimental results for dual temperature scaling, it does not compare its performance with other temperature scaling strategies (e.g., individual temperature scaling for each class or overall temperature scaling).
- What evidence would resolve it: Conducting experiments with different temperature scaling strategies and comparing their performance on fairness, calibration, and accuracy metrics would provide insights into the optimal strategy.

### Open Question 3
- Question: How do the proposed methods perform on datasets with highly imbalanced sensitive groups?
- Basis in paper: [inferred] The paper does not explicitly discuss the impact of imbalanced sensitive groups on the proposed methods. The datasets used in the experiments have varying proportions of sensitive groups, but the analysis does not focus on the effect of imbalance.
- Why unresolved: The theoretical framework and experimental results do not address the challenges posed by highly imbalanced sensitive groups. The performance of the methods in such scenarios remains unexplored.
- What evidence would resolve it: Evaluating the proposed methods on datasets with highly imbalanced sensitive groups and analyzing their performance would provide insights into the robustness of the methods under imbalance.

## Limitations

- The fairness guarantee relies on accurate estimation of base rates within each sensitive group, which may be challenging with imbalanced datasets.
- Dual temperature scaling effectiveness depends on having representative validation sets for each sensitive group, potentially limiting its applicability to smaller datasets.
- The proposed framework is currently limited to binary classification with two sensitive groups, with unclear generalization to multi-class or multi-group scenarios.

## Confidence

- High confidence: The mathematical proof that group-wise calibration implies fairness under the paper's definition (Lemma 3.9)
- Medium confidence: The practical effectiveness of dual temperature scaling across all datasets, particularly for groups with limited samples
- Low confidence: The general applicability of the Pareto-optimal trade-off between fairness, calibration, and accuracy across all possible dataset distributions

## Next Checks

1. Test the robustness of dual temperature scaling when validation sets have highly imbalanced sensitive group representations (e.g., 95% one group, 5% another)
2. Evaluate whether the fairness-calibration trade-off holds when base rates differ dramatically between sensitive groups (e.g., 90% vs 10% positive outcomes)
3. Verify the reproducibility of the group-wise loss functions by implementing MMCE-W from the partial specification provided and testing on a held-out dataset