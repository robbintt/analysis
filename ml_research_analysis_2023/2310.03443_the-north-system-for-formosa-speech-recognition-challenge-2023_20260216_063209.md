---
ver: rpa2
title: The North System for Formosa Speech Recognition Challenge 2023
arxiv_id: '2310.03443'
source_url: https://arxiv.org/abs/2310.03443
tags:
- speech
- hakka
- data
- system
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The North system addresses automatic word/syllable recognition
  for Taiwanese Hakka (Sixian dialect) by integrating two acoustic model architectures:
  Multistream CNN and Discriminative Autoencoder (DcAE), trained jointly on acoustic
  features (40-dimensional MFCCs and 1024-dimensional SSL embeddings). The system
  achieved double championship in the Formosa Speech Recognition Challenge 2023 for
  both Hakka Character and Pinyin tracks with character error rates of 17.15% and
  syllable error rates of 17.42% overall, ranking first in both categories.'
---

# The North System for Formosa Speech Recognition Challenge 2023

## Quick Facts
- arXiv ID: 2310.03443
- Source URL: https://arxiv.org/abs/2310.03443
- Reference count: 1
- Key outcome: North system won both Hakka Character (17.15% CER) and Pinyin (17.42% SER) tracks in Formosa Speech Recognition Challenge 2023

## Executive Summary
The North system addresses automatic word/syllable recognition for Taiwanese Hakka (Sixian dialect) by integrating Multistream CNN and Discriminative Autoencoder (DcAE) architectures trained jointly on acoustic features. The system achieved double championship in the Formosa Speech Recognition Challenge 2023, significantly outperforming ASUS's Whisper-based Hakka ASR with relative improvements of 37.06% for character recognition and 53.69% for pinyin recognition. The approach leverages temporal multi-resolution processing, discriminative autoencoder learning, and RNN-LM rescoring to achieve state-of-the-art results.

## Method Summary
The North system uses joint training of Multistream CNN and Discriminative Autoencoder (DcAE) acoustic models on concatenated 40-dimensional MFCCs and 1024-dimensional SSL embeddings. The system was trained on 86.54 hours of Hakka Sixian speech data and employs a two-stage decoding approach: 4-gram language model generation followed by RNN-LM rescoring. The joint training methodology minimizes ASR losses (lattice-free MMI) and reconstruction errors, while the rescoring mechanism operates on word lattices to enhance linguistic prediction accuracy.

## Key Results
- Achieved 17.15% character error rate in Hakka Character track
- Achieved 17.42% syllable error rate in Pinyin track
- Ranked first in both categories of Formosa Speech Recognition Challenge 2023
- Outperformed ASUS's Whisper-based system by 37.06% relative improvement for character recognition
- Outperformed ASUS's Whisper-based system by 53.69% relative improvement for pinyin recognition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint training of Multistream CNN and Discriminative Autoencoder (DcAE) architectures enables robust acoustic modeling across diverse temporal resolutions.
- Mechanism: The Multistream CNN processes input speech with varying dilation rates to capture temporal features at multiple resolutions, while DcAE learns both reconstruction and discriminative phonetic embeddings simultaneously. These models are trained jointly to minimize ASR losses (lattice-free MMI) and reconstruction errors.
- Core assumption: Combining temporal multi-resolution processing with discriminative autoencoder learning creates complementary feature representations that improve recognition accuracy.
- Evidence anchors:
  - [abstract] "The integration of both aforementioned structures is employed in a joint training methodology"
  - [section] "The integration of both aforementioned structures is employed in a joint training methodology, wherein the latter structure serves as the foundational bedrock upon which the former is developed and refined"
  - [corpus] Weak evidence - corpus neighbors don't directly address joint Multistream CNN + DcAE architectures
- Break condition: If either architecture fails to learn meaningful representations independently, their joint training cannot create synergistic improvements.

### Mechanism 2
- Claim: Using both 40-dimensional MFCCs and 1024-dimensional SSL embeddings provides complementary acoustic features for robust recognition.
- Mechanism: MFCCs capture traditional spectral envelope features while SSL embeddings (from HuBERT-large pre-trained on Chinese data) provide contextualized representations learned from large-scale unsupervised data. Concatenating these feature sets gives the model access to both low-level acoustic details and high-level semantic patterns.
- Core assumption: The combination of traditional acoustic features with deep SSL embeddings captures both fine-grained and abstract linguistic patterns necessary for accurate recognition.
- Evidence anchors:
  - [abstract] "The meticulous training of the acoustic model necessitates the concatenation of two distinct types of speech features: the 40-dimensional Mel Frequency Cepstral Coefficients (MFCCs) and the 1024-dimensional Semi-Supervised Learning (SSL) embeddings"
  - [section] "The meticulous training of the acoustic model necessitates the concatenation of two distinct types of speech features: the 40-dimensional Mel Frequency Cepstral Coefﬁcients (MFCCs) and the 1024-dimensional Semi-Supervised Learning (SSL) embeddings"
  - [corpus] Weak evidence - corpus neighbors don't discuss SSL embeddings or feature concatenation strategies
- Break condition: If SSL embeddings don't provide meaningful complementary information to MFCCs, the additional complexity and computational cost offer no benefit.

### Mechanism 3
- Claim: RNN-LM rescoring on word lattices generated by 4-gram language models improves recognition accuracy through contextual refinement.
- Mechanism: The initial 4-gram LM provides a reasonable search space, then the RNN-LM rescoring step applies learned contextual patterns to refine word predictions, particularly effective for handling the linguistic nuances of the Sixian dialect.
- Core assumption: The 4-gram LM constrains the search space sufficiently while the RNN-LM provides contextual refinement that captures dialect-specific patterns.
- Evidence anchors:
  - [abstract] "The rescoring mechanism, which is meticulously designed to operate on the word lattice generated by a four-gram language model, employs a Recurrent Neural Network Language Model (RNN-LM) to enhance the accuracy and reliability of linguistic predictions and outputs"
  - [section] "The rescoring mechanism, which is meticulously designed to operate on the word lattice generated by a four-gram language model, employs a Recurrent Neural Network Language Model (RNN-LM) to enhance the accuracy and reliability of linguistic predictions and outputs"
  - [corpus] Weak evidence - corpus neighbors don't discuss LM rescoring approaches for dialect-specific recognition
- Break condition: If the 4-gram LM generates poor initial lattices, the RNN-LM rescoring cannot recover from fundamental search errors.

## Foundational Learning

- Concept: Acoustic feature extraction and representation
  - Why needed here: The system relies on concatenating MFCCs and SSL embeddings, requiring understanding of both traditional and modern feature extraction techniques
  - Quick check question: What's the dimensional difference between MFCCs and SSL embeddings used in this system, and why might this combination be beneficial?

- Concept: Joint training of multiple neural architectures
  - Why needed here: The system trains Multistream CNN and DcAE jointly, requiring knowledge of multi-task learning and architectural integration
  - Quick check question: How does joint training differ from sequential training of multiple models, and what are potential advantages?

- Concept: Language model rescoring techniques
  - Why needed here: The system uses 4-gram LM generation followed by RNN-LM rescoring, requiring understanding of n-gram models and neural LM integration
  - Quick check question: What's the purpose of using a simpler LM for initial decoding followed by rescoring with a more complex LM?

## Architecture Onboarding

- Component map: Acoustic feature extraction (MFCCs + SSL embeddings) → Multistream CNN (temporal multi-resolution processing) → DcAE (reconstruction + discriminative learning) → Joint training loss computation → 4-gram LM decoding → RNN-LM rescoring → Output

- Critical path: Feature extraction → Multistream CNN processing → Joint training → LM rescoring. Any bottleneck in this path directly impacts system performance.

- Design tradeoffs: Used 4 GPUs for 82 hours of training due to resource constraints, preventing hyperparameter optimization. The decision to avoid speech synthesis training data prioritized real acoustic variability over data quantity.

- Failure signatures: 
  - Poor recognition accuracy → Check feature concatenation and joint training effectiveness
  - High computational cost → Investigate Multistream CNN dilation rate selection
  - Long utterances failing → Model capacity or GPU memory limitations
  - Dialect-specific errors → Insufficient training data coverage or LM rescoring inadequacy

- First 3 experiments:
  1. Train Multistream CNN alone with MFCCs only, measure baseline WER
  2. Train DcAE alone with SSL embeddings only, measure baseline WER  
  3. Train concatenated features with simple CNN baseline, measure improvement over individual models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the North system's performance change if spontaneous speech were included in the training data, given that the current model's weakness is the absence of spontaneous speech?
- Basis in paper: [explicit] "It is imperative to underscore that one of the predominant weaknesses that permeated our model during the training phase is the conspicuous absence of spontaneous speech."
- Why unresolved: The paper explicitly states this weakness but does not provide experimental results showing the performance difference with spontaneous speech included.
- What evidence would resolve it: Training the model with spontaneous speech included and comparing character/syllable error rates to the current results.

### Open Question 2
- Question: What would be the performance impact of optimizing and fine-tuning model parameters and hyperparameters, given that the current system was constrained by insufficient GPU resources?
- Basis in paper: [explicit] "consequent to the insufficiency of Graphical Processing Unit (GPU) resources, our team made a conscious decision to abstain from proceeding with the optimization and fine-tuning of model parameters and hyperparam-eters."
- Why unresolved: The paper mentions GPU constraints prevented parameter optimization, but does not show what performance gains might be achieved through optimization.
- What evidence would resolve it: Running experiments with optimized parameters (number of layers, training epochs, batch size, learning rate) and comparing results to current performance.

### Open Question 3
- Question: How would the North system's performance compare to Whisper-based systems if Whisper could handle long utterances, given that the current comparison excluded 384 utterances?
- Basis in paper: [explicit] "It is worth noting that ASUS's system cannot handle sentences that are too long, meaning that 384 utterances could not be processed and were not included in the error rate calculation."
- Why unresolved: The current comparison excludes utterances that ASUS cannot process, potentially biasing the results in favor of the North system.
- What evidence would resolve it: Testing ASUS Whisper system with modifications to handle long utterances and comparing full results including all utterances.

## Limitations
- Insufficient technical detail for independent verification, particularly missing architectural specifications for Multistream CNN and DcAE models
- Lack of ablation studies to isolate which components contribute most significantly to performance gains
- Limited evaluation with only aggregate CER/SER scores without per-speaker analysis or breakdown by speech conditions
- No systematic hyperparameter optimization performed due to GPU resource constraints

## Confidence

**High Confidence**: The reported CER (17.15%) and SER (17.42%) scores for winning the Formosa Speech Recognition Challenge 2023 are factual competition results. The use of MFCCs and SSL embeddings as input features, and the application of RNN-LM rescoring on 4-gram lattices are well-established techniques that the paper correctly implements.

**Medium Confidence**: The claim that joint training of Multistream CNN and DcAE architectures provides significant benefits is plausible given the complementary nature of temporal multi-resolution processing and discriminative autoencoder learning, but lacks quantitative ablation evidence. The assertion that the system significantly outperforms Whisper-based approaches (37.06% relative improvement for character recognition, 53.69% for pinyin) is based on competition results but doesn't account for potential differences in training data or evaluation conditions.

**Low Confidence**: The paper's claim that these specific architectural choices and training methodologies are optimal for Hakka Sixian dialect recognition is weakly supported, as no systematic hyperparameter optimization or comparison with alternative approaches was performed due to resource constraints.

## Next Checks

1. **Ablation Study**: Reimplement the system with systematic removal of components (DcAE only, Multistream CNN only, MFCCs only, SSL embeddings only, no RNN-LM rescoring) to quantify the individual contribution of each element to the reported performance gains.

2. **Data Efficiency Analysis**: Evaluate the system's performance on progressively smaller subsets of the training data (25%, 50%, 75%) to determine whether the computational complexity is justified by data efficiency, particularly given the limited dataset size.

3. **Cross-Dialect Generalization**: Test the trained model on Hakka dialect data from other regions (not Sixian) and on closely related Chinese dialects to assess whether the architectural choices provide benefits beyond the specific training domain.