---
ver: rpa2
title: 'Favour: FAst Variance Operator for Uncertainty Rating'
arxiv_id: '2311.13036'
source_url: https://arxiv.org/abs/2311.13036
tags:
- inference
- favour
- variance
- uncertainty
- dropout
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Favour is a fast, sample-free method for propagating variance through
  Bayesian neural networks, enabling efficient uncertainty quantification without
  the computational overhead of Monte Carlo sampling. The core innovation is representing
  covariance matrices in a diagonal-plus-low-rank (DPLR) form and propagating them
  through network layers using fast algorithms, including a novel subspace iteration
  approach.
---

# Favour: FAst Variance Operator for Uncertainty Rating

## Quick Facts
- arXiv ID: 2311.13036
- Source URL: https://arxiv.org/abs/2311.13036
- Reference count: 40
- Key outcome: Favour is a fast, sample-free method for propagating variance through Bayesian neural networks, enabling efficient uncertainty quantification without the computational overhead of Monte Carlo sampling.

## Executive Summary
Favour introduces a novel approach for fast, sample-free uncertainty quantification in Bayesian neural networks. By representing covariance matrices in a diagonal-plus-low-rank (DPLR) form and propagating them through network layers using efficient algorithms, Favour achieves comparable uncertainty quality to 10-100 Monte Carlo samples while being as fast as 2-3 samples. The method uses a subspace iteration algorithm to maintain the DPLR approximation efficiently, enabling applications in confidence calibration and out-of-distribution detection.

## Method Summary
Favour propagates uncertainty through Bayesian neural networks by maintaining covariance matrices in a diagonal-plus-low-rank form throughout inference. The core innovation is a fast algorithm for updating this DPLR approximation under various operations (matrix multiplication, addition, etc.) using subspace iteration. This allows efficient computation of uncertainty measures like Jensen-Shannon Divergence without sampling. The method is applicable to any BNN expressible in terms of weight parameters or layers determined by means and variances, including MC Dropout and Variational Inference models.

## Key Results
- Achieves comparable or better AUC-ROC and AUC-PR for OOD detection than MC Dropout at similar computational cost
- Matches uncertainty quality of 10-100 MC samples while being as fast as 2-3 samples
- Successfully handles tasks like confidence calibration and OOD detection on MNIST and CIFAR-10

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Propagating covariance matrices in DPLR form reduces inference complexity from O(n³) to O(n²) or lower.
- Mechanism: The diagonal-plus-low-rank structure allows efficient matrix-vector products by exploiting the Woodbury identity and subspace iteration, avoiding full matrix multiplication.
- Core assumption: The covariance matrix can be well-approximated by a diagonal matrix plus a low-rank component without significant loss of uncertainty quality.
- Evidence anchors:
  - [abstract]: "Our contribution is a more principled variance propagation framework based on 'spiked covariance matrices', which smoothly interpolates between quality and inference time."
  - [section]: "Consider the computation ofM = WΣW T ∈ Rm×m where W ∈ Rm×n. Straightforward computation requires O(mn²) operations. We can however obtain a DPLR approximation to M with complexity commensurate with model inference."
- Break condition: If the covariance matrix cannot be approximated well by DPLR form (e.g., highly unstructured or full-rank), the quality of uncertainty estimates degrades significantly.

### Mechanism 2
- Claim: The subspace iteration algorithm efficiently computes a rank-r approximation of WΣW^T in O(mnr + mr²) time.
- Mechanism: Alternating projections with subspace iteration update the low-rank component V and diagonal Λ iteratively, converging to a good DPLR approximation without explicit matrix construction.
- Core assumption: A small number of subspace iterations (K=2-4) suffices to obtain a high-quality DPLR approximation for uncertainty propagation.
- Evidence anchors:
  - [section]: "We developed the following greedy algorithm by interleaving the alternating direction search with a subspace iteration prompted by the effectiveness of randomized initialization."
  - [abstract]: "This is made possible by a new fast algorithm for updating a diagonal-plus-low-rank matrix approximation under various operations."
- Break condition: If the matrix WΣW^T has significant eigenvalues beyond rank r, the approximation error grows, reducing uncertainty estimate quality.

### Mechanism 3
- Claim: The Jensen-Shannon Divergence (JSD) can be accurately approximated in O(n) time using DPLR covariance structure.
- Mechanism: By leveraging the diagonal-plus-low-rank form, the entropy computation involving Σ reduces to simple operations on Λ and U, avoiding expensive full covariance operations.
- Core assumption: The Gaussian approximation of activations and the DPLR structure allow closed-form JSD estimation without sampling.
- Evidence anchors:
  - [abstract]: "We proved that the following sample free estimate accurately approximates the JSD: [formula] This computation is of O(n) when Σ is of DPLR form."
  - [section]: "We proved that the following sample free estimate accurately approximates the JSD: [formula] This computation is of O(n) when Σ is of DPLR form."
- Break condition: If the covariance matrix deviates significantly from Gaussian assumptions or DPLR structure, the JSD approximation error increases.

## Foundational Learning

- Concept: Covariance matrix propagation through linear layers
  - Why needed here: Understanding how variance evolves through WΣW^T is essential for grasping why DPLR approximation is effective.
  - Quick check question: What is the computational complexity of directly computing WΣW^T for an n×n matrix W and n×n covariance Σ?

- Concept: Diagonal-plus-low-rank (DPLR) matrix representation
  - Why needed here: DPLR form is the core mathematical structure enabling efficient uncertainty propagation.
  - Quick check question: How does the Woodbury identity simplify operations on DPLR matrices?

- Concept: Subspace iteration for low-rank approximation
  - Why needed here: The algorithm for updating DPLR form relies on subspace iteration to find the best rank-r approximation efficiently.
  - Quick check question: What is the role of alternating projections in the subspace iteration algorithm for DPLR updates?

## Architecture Onboarding

- Component map: Input layer -> Dropout layer -> Linear layer -> Activation layer -> DPLR approximation -> Output layer
- Critical path: Input → Dropout → Linear → Activation → DPLR update → Output
- Design tradeoffs:
  - Higher rank r improves uncertainty quality but increases computation time
  - Using low-rank W approximation reduces complexity but may lose some uncertainty information
  - First-order vs. Gaussian-based variance propagation affects sparsity and accuracy
- Failure signatures:
  - Poor OOD detection performance indicates inadequate uncertainty estimation
  - Calibration error suggests mismatch between predicted and actual uncertainty
  - Numerical instability in DPLR updates may indicate rank choice issues
- First 3 experiments:
  1. Implement DPLR covariance propagation through a single linear layer and verify complexity reduction
  2. Test JSD approximation accuracy on synthetic Gaussian data with varying covariance structures
  3. Compare OOD detection performance (AUC-ROC) between Favour and MC Dropout on MNIST→Fashion-MNIST

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the accuracy of Favour scale with increasing rank in the diagonal-plus-low-rank approximation?
- Basis in paper: [explicit] The paper states that accuracy can improve with larger rank in the DPLR form, but experiments show a small rank of 2 produces uncertainty ratings comparable to large samplings.
- Why unresolved: The paper does not provide a detailed analysis of the trade-off between rank size and accuracy across different tasks or network architectures.
- What evidence would resolve it: A systematic study varying the rank parameter and measuring the resulting accuracy and computational cost across multiple datasets and network types.

### Open Question 2
- Question: Can the fast variance propagation algorithm be extended to other Bayesian neural network architectures beyond those mentioned (MC Dropout, Variational Inference)?
- Basis in paper: [explicit] The paper states the algorithm is applicable to any BNN expressible in terms of weight parameters or layers determined by means and variances, but only demonstrates results on specific architectures.
- Why unresolved: The paper does not explore the algorithm's applicability to other BNN designs like Hamiltonian Monte Carlo or Stein Variational Gradient Descent.
- What evidence would resolve it: Applying the algorithm to different BNN architectures and comparing performance against their respective sampling-based inference methods.

### Open Question 3
- Question: What is the theoretical limit of the rank-1 approximation for the weight matrices in Algorithm 1, and how does it affect the quality of uncertainty estimates?
- Basis in paper: [explicit] The paper mentions replacing dense weight matrices with low-rank approximations of rank q ≤ n/10 in Algorithm 1, but does not analyze the impact of this approximation.
- Why unresolved: The paper does not provide a theoretical analysis of the error introduced by the rank-1 approximation of weight matrices.
- What evidence would resolve it: A theoretical analysis of the approximation error as a function of the rank parameter q, along with empirical validation on various tasks.

## Limitations
- The method's effectiveness depends on the quality of the diagonal-plus-low-rank approximation, which may degrade for highly complex covariance structures
- The reliance on Gaussian assumptions for activation variance propagation may introduce errors in networks with highly non-linear activations
- Limited evaluation on complex architectures beyond LeNet variants and standard vision benchmarks

## Confidence
- **High Confidence**: The computational complexity claims (O(n²) vs O(n³)) and the general framework of DPLR approximation are well-established in numerical linear algebra literature. The speedup claims relative to MC sampling are supported by the algorithmic improvements described.
- **Medium Confidence**: The effectiveness of the subspace iteration algorithm for DPLR updates is plausible based on the mathematical framework, but the specific implementation details and convergence guarantees are not fully elaborated. The OOD detection and calibration results show promise but are evaluated on a limited set of standard benchmarks.
- **Low Confidence**: The generalization of results to more complex architectures (beyond LeNet variants) and datasets with different characteristics (non-image domains, larger models) remains uncertain without additional empirical validation.

## Next Checks
1. **Rank Sensitivity Analysis**: Systematically evaluate Favour's performance across different rank values (r=1,2,4,8,16) on MNIST and CIFAR-10 to quantify the trade-off between computational efficiency and uncertainty quality. Measure AUC-ROC, calibration error, and inference time for each rank choice.

2. **Covariance Structure Robustness**: Generate synthetic datasets with controlled covariance structures (diagonal, low-rank, full-rank, structured) and evaluate how well Favour's DPLR approximation maintains uncertainty quality as the true covariance deviates from the assumed structure.

3. **Architecture Scaling Test**: Implement Favour on ResNet architectures and evaluate on CIFAR-100 and TinyImageNet to assess scalability and performance on deeper networks and more complex datasets, comparing against MC Dropout and other sample-free methods.