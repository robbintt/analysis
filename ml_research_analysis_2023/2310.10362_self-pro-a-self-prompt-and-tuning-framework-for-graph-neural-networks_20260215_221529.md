---
ver: rpa2
title: 'Self-Pro: A Self-Prompt and Tuning Framework for Graph Neural Networks'
arxiv_id: '2310.10362'
source_url: https://arxiv.org/abs/2310.10362
tags:
- graph
- tasks
- node
- graphs
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Self-Pro, a prompting framework for Graph
  Neural Networks (GNNs) that addresses the challenge of label dependency and poor
  generalization. The core idea is to leverage self-supervised pre-training and task-specific
  prompting to bridge the gap between pretext tasks and downstream tasks.
---

# Self-Pro: A Self-Prompt and Tuning Framework for Graph Neural Networks

## Quick Facts
- arXiv ID: 2310.10362
- Source URL: https://arxiv.org/abs/2310.10362
- Reference count: 40
- Key outcome: Self-Pro achieves 57.51% accuracy on Cora for few-shot node classification, surpassing GraphPrompt by 2.19%

## Executive Summary
This paper introduces Self-Pro, a prompting framework for Graph Neural Networks (GNNs) that addresses the challenge of label dependency and poor generalization. The core idea is to leverage self-supervised pre-training and task-specific prompting to bridge the gap between pretext tasks and downstream tasks. Self-Pro introduces asymmetric graph contrastive learning as a pretext task to handle heterophily and align objectives with downstream tasks. It then reuses pre-training components as a self-adapter and introduces self-prompts based on the graph itself for task adaptation. Experiments on 11 benchmark datasets demonstrate the effectiveness of Self-Pro, outperforming state-of-the-art methods in few-shot node classification, graph classification, and link prediction tasks.

## Method Summary
Self-Pro addresses label dependency and poor generalization in GNNs through a three-step approach: 1) Asymmetric graph contrastive learning as a pretext task to handle heterophily and align objectives with downstream tasks, 2) Reusing pre-training components as a self-adapter, and 3) Introducing self-prompts based on the graph itself for task adaptation. The framework unifies various graph tasks into similarity computation through graph instance construction, allowing the same pre-training objective to be relevant across different downstream tasks. Learnable prompts reweight semantic and contextual representations according to task requirements.

## Key Results
- Achieves 57.51% accuracy on Cora for few-shot node classification, surpassing GraphPrompt by 2.19%
- Outperforms state-of-the-art methods across 11 benchmark datasets
- Demonstrates effectiveness in few-shot node classification, graph classification, and link prediction tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Asymmetric graph contrastive learning addresses heterophily by aligning pretext and downstream objectives
- Mechanism: By constructing positive samples that preserve semantic information (node features) while introducing perturbations in the adjacency matrix, the method learns representations that are robust to heterophily. The contrastive loss pulls together semantically similar nodes while pushing apart dissimilar ones, even when they are connected.
- Core assumption: The semantic view (node features) and contextual view (graph structure) can be effectively separated and recombined to capture complementary information
- Evidence anchors:
  - [abstract] "Asymmetric graph contrastive learning as a pretext task to handle heterophily and align objectives with downstream tasks"
  - [section] "Contextual View. Context-level contrast aims to encourage graph instances with similar topology to be consistent. When constructing positive samples under the contextual view, it should be ensured that the semantic information of graph instances remains unchanged."
  - [corpus] Weak evidence - corpus contains related work on heterophily but lacks direct evidence of asymmetric contrastive learning performance

### Mechanism 2
- Claim: The unified framework reformulates various graph tasks into similarity computation
- Mechanism: By constructing graph instances (expanding nodes into subgraphs), the framework unifies node classification, graph classification, and link prediction as similarity calculations between graph instance representations. This allows the same pre-training objective to be relevant across different downstream tasks.
- Core assumption: Graph-level knowledge can be effectively transferred to node-level and edge-level tasks through the graph instance construction
- Evidence anchors:
  - [abstract] "reuses pre-training components as a self-adapter and introduces self-prompts based on the graph itself for task adaptation"
  - [section] "Next, we will formally define the template for downstream tasks...Graph instance classification...Link prediction...In summary, we reformulated the pretext and downstream tasks on graphs into a common task template"
  - [corpus] Moderate evidence - corpus shows related work on unified frameworks but lacks specific evidence of this exact reformulation approach

### Mechanism 3
- Claim: Task-specific prompting extracts relevant knowledge from pre-trained representations
- Mechanism: The framework uses learnable prompts (linear transformations and element-wise multiplication with prompt vectors) to reweight the semantic and contextual representations according to task requirements. This allows fine-grained control over which aspects of the pre-trained knowledge are emphasized for each downstream task.
- Core assumption: Different downstream tasks benefit from different weightings of semantic versus contextual information
- Evidence anchors:
  - [abstract] "introduces self-prompts based on the graph itself for task adaptation"
  - [section] "From the microscopic perspective, semantic view and contextual view play different roles in various tasks...Therefore, we should design prompts to extract the most relevant knowledge from representations of the two views."
  - [corpus] Weak evidence - corpus contains related work on graph prompting but lacks specific evidence of this element-wise reweighting approach

## Foundational Learning

- Concept: Graph neural networks and message passing
  - Why needed here: Understanding how GNNs aggregate information from neighbors is crucial for grasping why heterophily poses challenges and how the framework addresses them
  - Quick check question: How does a standard GCN update node representations, and why might this fail on heterophilous graphs?

- Concept: Contrastive learning and positive/negative sample construction
  - Why needed here: The framework relies on contrastive learning objectives, so understanding how positive and negative pairs are constructed and why this works is essential
  - Quick check question: What is the difference between semantic and contextual views in this framework, and how are positive samples constructed for each?

- Concept: Prompt tuning in NLP and its adaptation to graphs
  - Why needed here: The framework adapts NLP prompting techniques to graphs, so understanding the original concept and its limitations is important for appreciating the novel contributions
  - Quick check question: How does the prompt design in this framework differ from traditional NLP prompting, and why are these differences necessary for graphs?

## Architecture Onboarding

- Component map: GNN encoder → Semantic view (feature perturbations) → Contextual view (structure perturbations) → Contrastive loss (pre-training) → Fusion operation → Learnable prompts → Downstream tasks
- Critical path: Pre-training (asymmetric contrastive learning) → Prompt tuning (task-specific adaptation) → Inference (task-specific predictions)
- Design tradeoffs: The framework trades off computational complexity (multiple views and prompts) for improved generalization across heterophilous graphs and few-shot settings
- Failure signatures: Poor performance on datasets where semantic and contextual views are not meaningfully separable, instability in prompt learning, negative transfer when pretext and downstream objectives are misaligned
- First 3 experiments:
  1. Validate the semantic vs. contextual separation on a simple homophilous dataset
  2. Test the unified framework formulation by comparing to direct task-specific training
  3. Evaluate prompt tuning stability on few-shot node classification with varying prompt dimensions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Self-Pro compare when using different GNN architectures as the base encoder?
- Basis in paper: [inferred] The paper mentions using a GNN encoder but does not explore the impact of different architectures.
- Why unresolved: The authors only use a single GNN architecture in their experiments, limiting the generalizability of their findings.
- What evidence would resolve it: Experiments comparing Self-Pro's performance using various GNN architectures (e.g., GCN, GAT, GIN) on the same datasets.

### Open Question 2
- Question: What is the impact of different graph augmentation strategies on the performance of Self-Pro's pre-training phase?
- Basis in paper: [inferred] The paper describes using feature masking and edge dropping for augmentation but does not explore alternative strategies.
- Why unresolved: The authors only experiment with two specific augmentation techniques, leaving open the question of whether other strategies might yield better results.
- What evidence would resolve it: Experiments comparing Self-Pro's performance using different graph augmentation techniques (e.g., node dropping, subgraph sampling) on the same datasets.

### Open Question 3
- Question: How does the performance of Self-Pro scale with increasing graph size and complexity?
- Basis in paper: [inferred] The paper uses benchmark datasets of varying sizes but does not explicitly analyze performance scaling.
- Why unresolved: The authors do not provide a systematic analysis of how Self-Pro's performance changes as graph size and complexity increase.
- What evidence would resolve it: Experiments evaluating Self-Pro's performance on graphs of increasing size and complexity, potentially using synthetic graph generators or real-world graphs with known scaling properties.

## Limitations
- The specific augmentation techniques for creating positive samples in semantic and contextual views are not fully detailed
- The framework's reliance on pre-training components as adapters may limit its ability to handle graphs with significantly different characteristics
- The prompt tuning mechanism's stability and effectiveness in extremely few-shot settings (e.g., 1-2 labels per class) remain unverified

## Confidence

**High Confidence**: The core idea of unifying graph tasks through graph instance construction and similarity computation is well-supported by the experimental results showing consistent improvements across multiple tasks and datasets. The overall framework design and its modular components are clearly articulated.

**Medium Confidence**: The effectiveness of asymmetric graph contrastive learning for heterophily handling is supported by the reported results but lacks detailed ablation studies on different heterophily scenarios. The specific implementation details of the semantic and contextual view augmentations could impact real-world performance.

**Low Confidence**: The generalizability of the prompt tuning approach across diverse graph types and the framework's performance on extremely few-shot scenarios (beyond the reported 2-20 labels) have not been thoroughly validated.

## Next Checks
1. **Heterophily Robustness Test**: Evaluate Self-Pro on synthetic graphs with controlled heterophily ratios to quantify the impact of varying homophily levels on performance, particularly testing the limits of the semantic-contextual separation approach.

2. **Prompt Stability Analysis**: Conduct experiments with varying numbers of prompt parameters (dimensions) and few-shot training examples to assess the stability and generalization of the prompt tuning mechanism, particularly for the 1-5 label per class regime.

3. **Transfer Learning Evaluation**: Test the framework's performance when pre-training on one graph domain (e.g., citation networks) and fine-tuning on a completely different domain (e.g., molecular graphs) to assess cross-domain generalization capabilities.