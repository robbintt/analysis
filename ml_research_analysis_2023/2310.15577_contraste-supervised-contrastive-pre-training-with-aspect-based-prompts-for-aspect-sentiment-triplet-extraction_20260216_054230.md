---
ver: rpa2
title: 'CONTRASTE: Supervised Contrastive Pre-training With Aspect-based Prompts For
  Aspect Sentiment Triplet Extraction'
arxiv_id: '2310.15577'
source_url: https://arxiv.org/abs/2310.15577
tags:
- sentiment
- aste
- aspect
- pre-training
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CONTRASTE, a novel pre-training strategy
  using contrastive learning to enhance aspect sentiment triplet extraction (ASTE)
  performance. Given a sentence and its associated (aspect, opinion, sentiment) triplets,
  the method designs aspect-based prompts with corresponding sentiments masked.
---

# CONTRASTE: Supervised Contrastive Pre-training With Aspect-based Prompts For Aspect Sentiment Triplet Extraction

## Quick Facts
- arXiv ID: 2310.15577
- Source URL: https://arxiv.org/abs/2310.15577
- Reference count: 22
- New state-of-the-art ASTE results on four benchmark datasets

## Executive Summary
This paper introduces CONTRASTE, a novel pre-training strategy using supervised contrastive learning to enhance aspect sentiment triplet extraction (ASTE) performance. The method designs aspect-based prompts with masked sentiments and pre-trains an encoder-decoder model by applying contrastive learning on the decoder-generated aspect-aware sentiment representations. For fine-tuning, the model is combined with two auxiliary modules: a tagging-based opinion term detector and a regression-based triplet count estimator. Experiments demonstrate that CONTRASTE achieves state-of-the-art results on four benchmark datasets.

## Method Summary
The CONTRASTE method uses a T5 encoder-decoder framework pre-trained with supervised contrastive learning on aspect-based prompts where sentiments are masked. The pre-training objective clusters sentiment representations by polarity while separating different polarities. During fine-tuning, the model employs multi-task learning with auxiliary objectives: opinion term detection to identify opinion span boundaries and triplet count estimation to guide the number of triplets to generate. The approach is evaluated on four benchmark datasets for ASTE and shows strong performance across related ABSA tasks including ACOS, TASD, and AESC.

## Key Results
- Achieves new state-of-the-art F1 scores on all four ASTE benchmark datasets (14Res, 15Res, 16Res, Lap14)
- Demonstrates effective template generalization across multiple ABSA tasks beyond ASTE
- Shows superior performance compared to sentence-level contrastive learning approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aspect-based contrastive learning produces more discriminative sentiment embeddings than sentence-level contrastive learning.
- Mechanism: By masking the sentiment in aspect-based prompts and applying supervised contrastive learning on the decoder-generated aspect-aware sentiment representations, the model learns to cluster sentiment representations based on their polarity while pushing apart different polarities.
- Core assumption: Sentiment in ABSA tasks is primarily defined at the aspect level, not the sentence level.
- Evidence anchors:
  - [abstract] "The existing techniques apply SCL on sentence-level sentiment embeddings, whereas in ASTE (or ABSA in general), the sentiments are defined at an aspect level."
  - [section 2.1] "We model ASTE as a structured prediction task, and leverage the T5 (Raffel et al., 2019) encoder-decoder framework as the backbone of our proposed architecture."
  - [corpus] Weak - The corpus contains related papers on ASTE but doesn't directly address the contrastive learning mechanism.
- Break condition: If sentiment is better represented at the sentence level for certain domains or if aspect-level sentiment representations don't improve downstream task performance.

### Mechanism 2
- Claim: The proposed placeholder-based templates generalize better across ABSA tasks than natural language paraphrase templates.
- Mechanism: Using templated prompts with special tokens (<aspect>, <opinion>, <sentiment>, <category>) creates a consistent representation format that can be easily adapted across different ABSA tasks.
- Core assumption: A consistent template format across tasks improves transfer learning and reduces the need for task-specific prompt engineering.
- Evidence anchors:
  - [abstract] "Taking inspiration from Huguet Cabot and Navigli (2021), we therefore design a more generic placeholder-based template that can be leveraged across ABSA tasks."
  - [section 2.2] "For each token toki ∈ x, the opinion tagger takes as input the contextualized token embedding generated by the encoder, and performs a 3-way classification task with the classes being B-beginning of the span, I-inside the span, O-outside the span."
  - [corpus] Weak - The corpus contains related papers on ASTE but doesn't directly address template generalization.
- Break condition: If natural language paraphrase templates perform better for certain ABSA tasks or if the special tokens interfere with the model's understanding of the task.

### Mechanism 3
- Claim: Multi-task learning with auxiliary objectives (OTD and TCE) improves ASTE performance by providing additional supervision signals.
- Mechanism: The Opinion Term Detection (OTD) module helps the model better identify opinion span boundaries, while the Triplet Count Estimation (TCE) module provides implicit guidance on the number of triplets to generate.
- Core assumption: Additional supervision signals that are related to the main task can improve the model's performance through shared representations and joint optimization.
- Evidence anchors:
  - [abstract] "Finally, we train CONTRASTE-Base with two auxiliary multitask objectives, Opinion Term Detection and Triplet Count Estimation, each of which benefits the ASTE performance."
  - [section 2.2.1] "We hypothesize that the opinion term detection (OTD) module will help to better detect the opinion span boundaries which in turn affects the sentiment prediction."
  - [corpus] Weak - The corpus contains related papers on ASTE but doesn't directly address multi-task learning with auxiliary objectives.
- Break condition: If the auxiliary tasks don't improve performance or if they introduce conflicting gradients that harm the main task.

## Foundational Learning

- Concept: Supervised Contrastive Learning
  - Why needed here: To learn aspect-aware sentiment representations that cluster by polarity and separate different polarities.
  - Quick check question: What is the difference between supervised and unsupervised contrastive learning, and why is supervision important for this task?

- Concept: Multi-task Learning
  - Why needed here: To leverage auxiliary tasks (OTD and TCE) to improve the main ASTE task through shared representations and joint optimization.
  - Quick check question: What are the potential benefits and drawbacks of multi-task learning, and how do you determine the optimal weights for each task?

- Concept: Encoder-Decoder Architecture
  - Why needed here: To leverage the T5 framework for both pre-training (with contrastive learning) and fine-tuning (for ASTE and other ABSA tasks).
  - Quick check question: What are the key components of an encoder-decoder architecture, and how do they interact during training and inference?

## Architecture Onboarding

- Component map:
  T5 Encoder-Decoder Framework -> Supervised Contrastive Learning Module -> Placeholder-based Template Generator -> Opinion Term Detection Module -> Triplet Count Estimation Module -> Task-specific Fine-tuning Module

- Critical path: Pre-training (SCL on aspect-aware sentiment representations) -> Multi-task Fine-tuning (OTD, TCE, ASTE) -> Evaluation on benchmark datasets

- Design tradeoffs:
  - Pre-training with external data vs. using only available ASTE benchmark datasets
  - Using natural language paraphrase templates vs. placeholder-based templates
  - Adding auxiliary tasks vs. focusing solely on the main task

- Failure signatures:
  - Poor performance on aspect-level sentiment clustering during pre-training
  - Degraded performance on the main ASTE task when adding auxiliary tasks
  - Inability to generalize templates across different ABSA tasks

- First 3 experiments:
  1. Compare pre-training with SCL on aspect-level vs. sentence-level sentiment representations using a subset of the ASTE benchmark data.
  2. Evaluate the impact of different template formats (natural language vs. placeholder-based) on ASTE performance.
  3. Test the contribution of each auxiliary task (OTD and TCE) to the overall ASTE performance through ablation studies.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CONTRASTE scale with increasing dataset size for pre-training, particularly in cross-domain settings?
- Basis in paper: [inferred] The paper mentions that they limited pre-training data to existing ASTE benchmarks and raises questions about the sufficiency of data and applicability in cross-domain settings.
- Why unresolved: The paper did not experiment with varying fractions of available data or explore cross-domain pre-training effectiveness.
- What evidence would resolve it: Systematic experiments varying pre-training dataset size and composition across different domains would show the scaling behavior and cross-domain transfer capability.

### Open Question 2
- Question: What is the impact of using different temperature parameters (τ) in the supervised contrastive loss on the downstream ASTE performance?
- Basis in paper: [explicit] The paper uses a fixed temperature parameter τ=0.07 for pre-training but does not explore its sensitivity.
- Why unresolved: The optimal temperature for contrastive learning in this specific aspect-based sentiment context is not established.
- What evidence would resolve it: Experiments varying τ values and measuring corresponding ASTE performance would identify the optimal range.

### Open Question 3
- Question: How does the proposed method compare to other pre-training strategies that use external sentiment-annotated corpora for ASTE tasks?
- Basis in paper: [explicit] The paper mentions that Li et al. (2021b) uses external large-scale sentiment-annotated corpora but CONTRASTE does not.
- Why unresolved: Direct comparison with external-data-augmented approaches is not provided.
- What evidence would resolve it: Head-to-head experiments comparing CONTRASTE with external-data-augmented contrastive pre-training would quantify the trade-off between data efficiency and performance.

## Limitations
- The methodology section lacks complete implementation details for the opinion term detection and triplet count estimation modules, making exact reproduction challenging.
- The pre-training approach uses data from all four datasets combined, which may not generalize to datasets with different distributions.
- The paper doesn't explore the scalability of the approach with varying amounts of pre-training data or in cross-domain settings.

## Confidence
- High confidence in experimental results showing state-of-the-art performance on four benchmark datasets.
- Medium confidence in mechanism claims about aspect-level contrastive learning superiority.
- Medium confidence in template generalization claims across ABSA tasks.

## Next Checks
1. Conduct controlled experiments comparing pre-training with supervised contrastive learning at the aspect-level versus sentence-level, using identical model architectures and hyperparameters except for the contrastive learning target.

2. Implement and evaluate the same model architecture using natural language paraphrase templates versus the proposed placeholder-based templates on all four benchmark datasets.

3. Perform systematic ablation studies removing each auxiliary task (OTD and TCE) individually and in combination to precisely measure their individual and synergistic contributions.