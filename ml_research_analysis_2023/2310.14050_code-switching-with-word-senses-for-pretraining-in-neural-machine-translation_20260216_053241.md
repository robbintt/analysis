---
ver: rpa2
title: Code-Switching with Word Senses for Pretraining in Neural Machine Translation
arxiv_id: '2310.14050'
source_url: https://arxiv.org/abs/2310.14050
tags:
- translation
- word
- sense
- machine
- pretraining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of lexical ambiguity in neural
  machine translation (NMT) by introducing Word Sense Pretraining for NMT (WSP-NMT),
  which integrates word sense disambiguation into code-switched pretraining using
  knowledge bases. The method first disambiguates word senses in input sentences using
  WSD systems, then code-switches using sense-specific translations from BabelNet.
---

# Code-Switching with Word Senses for Pretraining in Neural Machine Translation

## Quick Facts
- arXiv ID: 2310.14050
- Source URL: https://arxiv.org/abs/2310.14050
- Reference count: 18
- This work introduces WSP-NMT, integrating word sense disambiguation into code-switched pretraining for NMT

## Executive Summary
This paper addresses lexical ambiguity in neural machine translation by introducing Word Sense Pretraining for NMT (WSP-NMT), which integrates word sense disambiguation into code-switched pretraining using knowledge bases. The method first disambiguates word senses in input sentences using WSD systems, then code-switches using sense-specific translations from BabelNet. Experiments show significant improvements over baseline methods, with +1.2 spBLEU and +0.02 COMET22 points on average, and up to 15% accuracy gains on the DiBiMT disambiguation benchmark for verb translation.

## Method Summary
The approach first disambiguates word senses in the input sentence using WSD systems (ESCHER or AMuSE-WSD), retrieves sense-specific translations from BabelNet, and optionally inflects them to match morphology. This produces more accurate code-switched sentences for denoising pretraining. The method is tested on various language pairs including high-resource Romance languages and low-resource Indo-Iranian languages, with experiments covering standard translation tasks and fine-grained evaluation on the DiBiMT disambiguation benchmark.

## Key Results
- WSP-NMT achieves +1.2 spBLEU and +0.02 COMET22 improvement on average over baseline
- Up to 15% accuracy gains on DiBiMT disambiguation benchmark for verb translation
- Particularly effective in low-resource settings with +3-5 spBLEU improvements
- Morphological inflection with MUSE lexicons yields major boosts across all language pairs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sense-pivoted pretraining reduces noise in training data by grounding code-switched substitutions in word sense-specific translations rather than random choices.
- Core assumption: Sense-specific translations from BabelNet are more accurate than random sense selection from MUSE lexicons, and this accuracy gain propagates through pretraining.
- Break condition: If WSD systems frequently fail to disambiguate correctly or BabelNet translations are noisy/incorrect, the noise reduction benefit disappears.

### Mechanism 2
- Claim: Integrating morphological inflection with sense translations improves linguistic agreement and translation quality.
- Core assumption: Morphologically mismatched translations introduce noise that degrades pretraining quality, and fixing this mismatch improves model performance.
- Break condition: If MUSE lexicons lack sufficient coverage for morphological inflection or if inflection generation is error-prone, the benefit may be minimal or negative.

### Mechanism 3
- Claim: WSP-NMT is particularly effective for low-resource and zero-shot translation scenarios where high-quality parallel data is scarce.
- Core assumption: Sense-grounded pretraining creates more robust cross-lingual representations than sense-agnostic pretraining, especially when parallel data is limited.
- Break condition: If sense disambiguation quality degrades significantly in low-resource languages or if monolingual data is too noisy, the transfer benefit may not materialize.

## Foundational Learning

- Concept: Word Sense Disambiguation (WSD)
  - Why needed here: WSP-NMT requires accurate word sense identification in context to retrieve appropriate translations from BabelNet
  - Quick check question: What is the difference between a word's surface form and its sense, and why is this distinction critical for translation quality?

- Concept: Knowledge Graphs (KGs) and BabelNet structure
  - Why needed here: BabelNet serves as the sense inventory and translation source; understanding its synset structure and multilingual lexicalizations is essential
  - Quick check question: How does BabelNet represent word senses, and what information is available for each synset across different languages?

- Concept: Morphological inflection and lemmatization
  - Why needed here: Converting BabelNet lemmas to contextually appropriate inflected forms requires understanding morphological agreement rules
  - Quick check question: What morphological features (tense, number, gender) must be preserved when converting lemmas to surface forms for translation?

## Architecture Onboarding

- Component map: Input → WSD system → BabelNet lookup → MUSE inflection lookup → code-switched sentence → NMT encoder/decoder → loss computation
- Critical path: WSD → BabelNet → MUSE → code-switching (errors here propagate through entire training)
- Design tradeoffs: ESCHER (higher accuracy, slower) vs AMuSE-WSD (lower accuracy, faster) for WSD; morphological inflection adds complexity but improves quality
- Failure signatures: Poor BLEU/COMET scores suggest WSD or translation errors; high MISS% in DiBiMT suggests word sense translation issues
- First 3 experiments:
  1. Compare AA vs WSP-NMT (no morphological inflection) on a high-resource language pair to isolate WSD impact
  2. Compare ESCHER vs AMuSE-WSD for WSP-NMT on the same pair to measure WSD quality impact
  3. Add morphological inflection to the best WSP-NMT configuration to measure improvement from morphological agreement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of WSP-NMT change if we used a more advanced WSD system than ESCHER or AMuSE-WSD, particularly one that leverages contextual information more effectively?
- Basis in paper: [explicit] The paper mentions that ESCHER and AMuSE-WSD are used for WSD, with ESCHER being more accurate but slower, and AMuSE-WSD being faster but less accurate. The paper notes that ESCHER consistently induces greater gains than AMuSE-WSD across all pairs.
- Why unresolved: The paper does not explore the impact of using a more advanced WSD system, leaving the potential for further performance improvements unexplored.
- What evidence would resolve it: Experimenting with a more advanced WSD system, such as one that leverages contextual information more effectively, and comparing its performance with ESCHER and AMuSE-WSD on the DiBiMT benchmark.

### Open Question 2
- Question: How would the performance of WSP-NMT change if we used a different knowledge graph (KG) instead of BabelNet, particularly one that has better coverage and quality for under-represented languages?
- Basis in paper: [inferred] The paper mentions that BabelNet has poor coverage and quality for under-represented languages, such as Indo-Iranian languages, and that this limits the effectiveness of WSP-NMT for these languages.
- Why unresolved: The paper does not explore the impact of using a different KG, leaving the potential for improving the performance of WSP-NMT for under-represented languages unexplored.
- What evidence would resolve it: Experimenting with a different KG, such as one that has better coverage and quality for under-represented languages, and comparing its performance with BabelNet on the DiBiMT benchmark.

### Open Question 3
- Question: How would the performance of WSP-NMT change if we used a different code-switching strategy, such as one that takes into account the frequency of word senses or the context of the sentence?
- Basis in paper: [explicit] The paper mentions that the current code-switching strategy in WSP-NMT uses random sampling to choose a translation for a word sense, and that this may lead to the propagation of potential biases.
- Why unresolved: The paper does not explore the impact of using a different code-switching strategy, leaving the potential for further performance improvements unexplored.
- What evidence would resolve it: Experimenting with a different code-switching strategy, such as one that takes into account the frequency of word senses or the context of the sentence, and comparing its performance with the current strategy on the DiBiMT benchmark.

## Limitations
- Knowledge Base Dependency: The approach relies heavily on BabelNet's coverage and accuracy for sense-specific translations, with no quantitative analysis of coverage gaps.
- WSD System Performance: The two WSD systems used show performance variability across languages and domains, with mixed results suggesting sensitivity to WSD quality.
- Limited Evaluation Scope: Results are primarily reported on high-resource European language pairs with limited experimentation on truly low-resource scenarios.

## Confidence

**High Confidence:** The overall improvement trend (+1.2 spBLEU average) is well-supported by experiments across multiple language pairs and evaluation metrics. The ablation studies showing benefits of morphological inflection and WSD integration are robust.

**Medium Confidence:** Claims about robustness to data scarcity and effectiveness in zero-shot scenarios are supported but limited by the range of tested resource levels. The generalization to unseen language pairs needs more extensive validation.

**Low Confidence:** The paper's claims about long-term knowledge retention and cross-lingual transfer stability are not directly tested. The impact on specialized domains (medical, legal) is not evaluated.

## Next Checks

1. **Coverage Analysis:** Quantify BabelNet translation coverage across target languages and identify specific failure patterns (missing senses, incorrect translations) to assess reliability limits.

2. **Cross-Domain Generalization:** Test the approach on specialized domains (e.g., biomedical or legal text) where sense ambiguity patterns differ significantly from general domain data used in the paper.

3. **Extreme Low-Resource Validation:** Evaluate performance with fewer than 10K parallel sentences to determine the practical lower bound for the pretraining approach's effectiveness.