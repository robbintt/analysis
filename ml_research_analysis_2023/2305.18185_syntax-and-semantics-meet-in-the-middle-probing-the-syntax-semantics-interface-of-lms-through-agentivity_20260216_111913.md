---
ver: rpa2
title: 'Syntax and Semantics Meet in the "Middle": Probing the Syntax-Semantics Interface
  of LMs Through Agentivity'
arxiv_id: '2305.18185'
source_url: https://arxiv.org/abs/2305.18185
tags:
- nouns
- agent
- noun
- human
- verb
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how large language models handle interactions
  between word-level semantics and syntactic forms, focusing on the semantic notion
  of agentivity. We create a novel evaluation dataset using a subset of optionally
  transitive English verbs to probe whether models are sensitive to agentivity at
  the lexical level and if they can appropriately employ these word-level priors given
  specific syntactic contexts.
---

# Syntax and Semantics Meet in the "Middle": Probing the Syntax-Semantics Interface of LMs Through Agentivity

## Quick Facts
- arXiv ID: 2305.18185
- Source URL: https://arxiv.org/abs/2305.18185
- Reference count: 17
- GPT-3 text-davinci-003 outperforms all other models by far in correlation with human judgments on agentivity tasks

## Executive Summary
This work investigates how large language models handle the interaction between word-level semantics and syntactic forms, focusing on agentivity - the semantic notion of who is doing the action versus who is affected by it. The authors create a novel evaluation dataset using optionally transitive English verbs to test whether models are sensitive to agentivity at the lexical level and if they can appropriately employ these priors given specific syntactic contexts. They evaluate varying sizes of BLOOM, GPT-2, and GPT-3 models across three experiments testing lexical-level agentivity sensitivity, disambiguation of ambiguous syntactic forms, and overriding lexical priors with syntactic cues.

## Method Summary
The authors use a prompting paradigm to elicit language model probabilities of "agent" or "patient" labels for nouns in isolation or within sentences. They create minimal pairs of sentences with controlled syntactic structures (intransitive, transitive subject, transitive object) using 233 nouns combined with 23 optionally transitive verbs and adverbs. The evaluation compares model predictions against human judgments and corpus statistics (Google Syntactic Ngrams and Propbank) using Pearson correlation and accuracy metrics. Each experiment is run twice with different prompt orderings (APAP and PAPA) to test order sensitivity.

## Key Results
- GPT-3 text-davinci-003 achieves better correlation with human judgments than both syntactic and semantic corpus statistics for agentivity
- Models can disambiguate agentivity in ambiguous intransitive constructions by leveraging lexical semantic priors
- Models can override lexical semantic priors with syntactic cues when required by sentence structure
- BLOOM models show high sensitivity to prompt ordering, while GPT-3 models are more robust to ordering effects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-3 text-davinci-003 achieves better correlation with human judgments than both syntactic and semantic corpus statistics for agentivity.
- Mechanism: The model captures nuanced human intuitions about prototypical agent and patient roles that are not fully represented in smaller, genre-biased corpora like Propbank or Google Syntactic Ngrams.
- Core assumption: Large-scale pretraining on diverse text domains allows models to learn statistical patterns that better approximate human semantic intuitions than manually annotated or narrowly sampled corpora.
- Evidence anchors:
  - [abstract] "davinci-003's performance is even better correlated with human judgements than both syntactic and semantic corpus statistics"
  - [section 3.1] "davinci-003 is not only both better correlated with human judgements than with corpus statistics, but surprisingly there is also a stronger correlation between its δ-LL and human ratings than between these proxies (syntactic and semantic) and human ratings"
- Break condition: If the pretraining corpus lacks sufficient diversity or if human judgments are inconsistent, the model's advantage over corpus statistics would diminish.

### Mechanism 2
- Claim: Models can disambiguate agentivity in ambiguous intransitive constructions by leveraging lexical semantic priors.
- Mechanism: When presented with an ambiguous intransitive sentence, the model uses the typical agentivity of the noun (learned from pretraining) to determine whether the subject is an agent or patient.
- Core assumption: The model has learned distributional statistics about how often nouns appear in agent versus patient roles across diverse contexts.
- Evidence anchors:
  - [section 2.1] "we rely on the prompting paradigm to elicit LM probabilities of an 'agent' or 'patient' label for a given noun in isolation or within a sentence"
  - [section 3.2] "we would expect that the δ-LL in this experiment is correlated with δ-LL from Experiment 1"
- Break condition: If the verb in question has strong selectional preferences that override lexical priors, or if the model fails to access stored noun-level statistics.

### Mechanism 3
- Claim: Models can override lexical semantic priors with syntactic cues when required by sentence structure.
- Mechanism: In transitive sentences, the model prioritizes syntactic position (subject=agent, object=patient) over typical agentivity ratings of the nouns.
- Core assumption: The model has learned the syntactic mapping between grammatical roles and semantic roles that is consistent across English.
- Evidence anchors:
  - [section 3.3] "Since the semantic role of the noun maps directly to its syntactic position in these sentences, all subjects should be agents and all objects should be patients"
  - [section 2.2] "the explicit inclusion of both arguments (subject and direct object) now forces whatever is in subject position to be the agent and whatever is in object position to be more like a patient"
- Break condition: If the model fails to learn or apply the syntactic-semantic mapping consistently, or if the input contains constructions that violate typical mappings.

## Foundational Learning

- Concept: Agentivity and thematic roles
  - Why needed here: Understanding agentivity is central to the experimental design and interpretation of results
  - Quick check question: What are the key properties that distinguish agent roles from patient roles according to Dowty's proto-role theory?

- Concept: Distributional semantics and selectional preferences
  - Why needed here: The experiments rely on the idea that models learn statistical patterns about which nouns typically appear as agents or patients with specific verbs
  - Quick check question: How might a language model learn that "author" is more likely to be an agent with "write" than "passage"?

- Concept: Syntax-semantics interface
  - Why needed here: The study investigates how syntactic structure and lexical semantics interact to determine meaning
  - Quick check question: In what ways can identical syntactic structures yield different semantic interpretations based on lexical content?

## Architecture Onboarding

- Component map: Prompting interface -> Language model (BLOOM, GPT-2, GPT-3 variants) -> Evaluation metrics (accuracy, correlation with human judgments and corpus statistics)

- Critical path: 1) Curate minimal pairs of sentences with controlled syntactic structures, 2) Design prompts with gold examples, 3) Query language model for log-likelihoods of agent/patient labels, 4) Calculate δ-LL (difference in log-likelihood), 5) Compare model predictions to human judgments and corpus statistics.

- Design tradeoffs: Using minimal pairs provides precise control over syntactic variation but limits the range of verbs and nouns that can be tested. Human judgments provide a gold standard but are resource-intensive and may vary across annotators.

- Failure signatures: Poor correlation with human judgments suggests the model is not capturing nuanced semantic intuitions. Inconsistent performance across example orderings indicates sensitivity to prompt formatting. Below-chance performance suggests the model is relying on spurious correlations or label bias.

- First 3 experiments:
  1. Test whether the model can distinguish typical agent nouns from patient nouns in isolation
  2. Test whether the model can use lexical semantics to disambiguate agentivity in intransitive sentences
  3. Test whether the model can override lexical priors with syntactic cues in transitive sentences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific aspects of model training/data contribute to davinci-003's exceptional performance on linguistic tasks compared to other models?
- Basis in paper: [explicit] The paper notes that davinci-003 outperforms all other models by far and suggests that its high correlation with human judgments is intriguing, but the specific training factors are not identified.
- Why unresolved: The paper observes the phenomenon but does not investigate the training methodology or data composition differences that might explain the performance gap.
- What evidence would resolve it: Comparative analysis of training data composition, fine-tuning procedures, and architectural differences between davinci-003 and other models tested.

### Open Question 2
- Question: How would performance change if prompts were optimized for each specific task rather than using a standardized template?
- Basis in paper: [inferred] The paper acknowledges that model performance is highly sensitive to prompt ordering and that they use a standardized prompting approach, suggesting prompt optimization could be significant.
- Why unresolved: The paper uses a consistent prompt structure across all experiments and model types without exploring task-specific prompt engineering.
- What evidence would resolve it: Systematic comparison of performance across different prompt structures optimized for each experimental condition.

### Open Question 3
- Question: Can language models serve as effective tools for discovering new linguistic phenomena beyond the syntax-semantics interface?
- Basis in paper: [explicit] The paper suggests that LMs may serve as more useful tools for linguistic annotation, theory testing, and discovery than select corpora for certain tasks, based on davinci-003's performance.
- Why unresolved: The paper only explores this potential through one specific linguistic phenomenon (agentivity) and doesn't test the broader applicability of this approach.
- What evidence would resolve it: Application of similar prompting methodologies to diverse linguistic phenomena across multiple languages and grammatical domains.

## Limitations

- Dataset construction scope: The study uses a limited set of 233 nouns and 23 optionally transitive verbs, which may not fully represent the diversity of agentivity patterns in English.
- Prompt sensitivity: BLOOM models show high sensitivity to prompt ordering, suggesting potential brittleness in the evaluation methodology.
- Corpus statistic reliability: The comparison with syntactic and semantic corpus statistics relies on potentially noisy and genre-biased data sources.

## Confidence

- High confidence: GPT-3 text-davinci-003 outperforming other models in correlation with human judgments is well-supported by the presented correlation coefficients.
- Medium confidence: The claim that models can override lexical priors with syntactic cues in transitive sentences is supported but could be affected by the limited verb set.
- Medium confidence: The interpretation that large-scale pretraining enables models to capture nuanced human intuitions is plausible but not directly tested.

## Next Checks

1. Run the evaluation with randomized prompt orderings across all models to quantify the impact of ordering effects and determine whether results are consistent or highly sensitive to presentation format.
2. Apply the same methodology to a morphologically rich language (like Russian or Finnish) where syntactic cues are less reliable, to test whether the models' ability to override lexical priors is truly syntax-driven or relies on English-specific patterns.
3. Create a small human-annotated corpus of agentivity judgments for the same verb-noun pairs used in the study, and compare model performance against this directly annotated data rather than derived corpus statistics.