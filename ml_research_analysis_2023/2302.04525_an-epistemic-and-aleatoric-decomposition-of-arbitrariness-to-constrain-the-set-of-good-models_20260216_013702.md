---
ver: rpa2
title: An Epistemic and Aleatoric Decomposition of Arbitrariness to Constrain the
  Set of Good Models
arxiv_id: '2302.04525'
source_url: https://arxiv.org/abs/2302.04525
tags:
- variance
- parity
- fairness
- metrics
- statistical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of instability in ML models by
  decomposing it into epistemic and aleatoric components, which represent consistency
  and confidence in predictions, respectively. The authors propose a new family of
  performance measures based on group-wise parity in variance, reconciling statistical
  bias-based and variance-based analysis of parity in estimator performance on different
  subgroups.
---

# An Epistemic and Aleatoric Decomposition of Arbitrariness to Constrain the Set of Good Models

## Quick Facts
- arXiv ID: 2302.04525
- Source URL: https://arxiv.org/abs/2302.04525
- Reference count: 32
- Primary result: Proposes decomposing ML model instability into epistemic (reducible with more data) and aleatoric (inherent) components to improve fairness and accuracy.

## Executive Summary
This paper addresses the problem of instability in ML models by decomposing it into epistemic and aleatoric components, representing consistency and confidence in predictions respectively. The authors propose a new family of performance measures based on group-wise parity in variance, reconciling statistical bias-based and variance-based analysis of parity in estimator performance on different subgroups. They introduce a model selection procedure that includes epistemic and aleatoric criteria alongside existing accuracy and fairness criteria, demonstrating that this approach can successfully narrow down a large set of good models to a handful of stable, fair, and accurate ones.

## Method Summary
The authors train 6 different models (Decision Tree, Logistic Regression, Random Forest, XGBoost, K-Neighbors, MLP) on bootstrapped samples of training data from Folktables (ACS Employment task, Georgia 2018) and COMPAS datasets. They compute statistical bias and variance metrics on ensembles of predictors across 10 seeds for COMPAS and 6 seeds for Folktables. The method involves using bootstrap resampling to construct model ensembles, computing epistemic and aleatoric instability components, and proposing variance-based fairness metrics that measure parity in instability across demographic groups.

## Key Results
- Epistemic instability decreases with more training data while aleatoric instability remains constant
- State-of-the-art ML models exhibit aleatoric instability as high as 79% and aleatoric instability disparities among demographic groups as high as 29%
- The proposed model selection procedure successfully narrows down 50-100 good models to a handful of stable, fair, and accurate ones
- Variance-based fairness metrics reveal corrective effects on model accuracy and fairness that bias-based metrics miss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Epistemic instability decreases with more training data, while aleatoric instability does not.
- Mechanism: The epistemic component captures uncertainty due to insufficient data, which shrinks as the dataset grows. The aleatoric component captures inherent randomness in the data, which is irreducible.
- Core assumption: Epistemic uncertainty is data-dependent, while aleatoric uncertainty is inherent to the data distribution.
- Evidence anchors:
  - [abstract] "epistemic instability can be reduced with more training data whereas aleatoric instability cannot"
  - [corpus] "Aleatoric and Epistemic Discrimination: Fundamental Limits of Fairness Interventions" (neighbor paper on similar decomposition)

### Mechanism 2
- Claim: Variance-based fairness metrics can reveal corrective effects on model accuracy and fairness that bias-based metrics miss.
- Mechanism: High variance in predictions for disadvantaged groups can introduce randomness that reduces systematic bias, improving fairness and accuracy in models with reasonable baseline performance.
- Core assumption: Randomization can counteract systematic skew in predictions, leading to fairer outcomes.
- Evidence anchors:
  - [section] "Variance can have a corrective effect on both fairness and the overall accuracy for models that have reasonably high overall accuracy"
  - [abstract] "the effects of large variance can be morally more acceptable (and fairness-enhancing) than the effects of a systematic skew"

### Mechanism 3
- Claim: The proposed model selection procedure narrows down a large set of good models (50-100) to a handful of stable, fair, and accurate ones.
- Mechanism: By incorporating epistemic and aleatoric stability criteria alongside accuracy and fairness metrics, the procedure filters out models that are unstable or have large group-wise variance disparities.
- Core assumption: Stability and fairness are orthogonal desiderata that can be jointly optimized through multi-criteria selection.
- Evidence anchors:
  - [abstract] "show that it successfully narrows down a large set of good models (50-100 on our datasets) to a handful of stable, fair, and accurate ones"
  - [section] "a practical solution to the problem of systematic arbitrariness"

## Foundational Learning

- Concept: Bootstrap resampling for variance estimation
  - Why needed here: The paper uses bootstrap to construct an ensemble of models and approximate the variance of a single model's predictions.
  - Quick check question: How does bootstrap resampling help estimate the variance of a model's predictions without requiring multiple training runs on different datasets?

- Concept: Bias-variance decomposition of model error
  - Why needed here: The paper builds on the classical bias-variance decomposition to argue that variance can have corrective effects on model fairness.
  - Quick check question: In what scenarios might high variance be beneficial for fairness, despite increasing overall error?

- Concept: Group-wise parity metrics
  - Why needed here: The paper proposes new variance-based fairness metrics that measure parity in instability across demographic groups.
  - Quick check question: How do variance-based fairness metrics differ from traditional bias-based metrics in detecting discrimination?

## Architecture Onboarding

- Component map: Dataset class → Configuration files → Model dictionary → Subgroup variance analyzer → Subgroup statistical bias analyzer → Metrics composer → Metrics visualizer
- Critical path: Dataset preprocessing → Subgroup metric computation (variance and bias) → Group metric composition → Metrics visualization and reporting
- Design tradeoffs: The library prioritizes extensibility and flexibility over performance optimization, allowing users to define custom datasets, models, and subgroups of interest.
- Failure signatures: Incorrect subgroup definitions, missing or inconsistent data preprocessing, and improper bootstrap sampling can lead to misleading variance estimates and fairness metrics.
- First 3 experiments:
  1. Run the single model, single run interface on a simple binary classification dataset to verify basic functionality.
  2. Use the multiple models, single run interface to compare variance and bias metrics across different model architectures on the same dataset.
  3. Employ the multiple runs, multiple models interface to analyze the stability and fairness of models across different random seeds and data splits.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does estimator variance relate to fairness and accuracy across different model architectures and datasets?
- Basis in paper: [explicit] The paper demonstrates that variance can have a corrective effect on both fairness and overall accuracy for models with reasonably high overall accuracy, but for models with low overall accuracy, attempting to improve stability and parity in stability across groups negates any potentially corrective effect of estimator variance.
- Why unresolved: The relationship between estimator variance and fairness/accuracy is complex and depends on the specific model architecture, dataset, and trade-off being considered.
- What evidence would resolve it: Extensive empirical studies across a wide range of model architectures, datasets, and fairness metrics would be needed to establish generalizable patterns and guidelines.

### Open Question 2
- Question: What are the statistical properties and limitations of the variance metrics used in this study, and how do they compare to other uncertainty quantification techniques?
- Basis in paper: [explicit] The paper acknowledges that the statistical procedures used to compute estimator variance suffer from the same shortcomings as other performance metrics.
- Why unresolved: The statistical properties and limitations of the variance metrics are not fully explored in the paper.
- What evidence would resolve it: Rigorous statistical analysis of the variance metrics, including their asymptotic properties, bias, and variance.

### Open Question 3
- Question: How can the insights on the effect of estimator variance be leveraged to develop new fairness-enhancing interventions beyond the classic fairness-accuracy dichotomy?
- Basis in paper: [explicit] The paper suggests that there is interesting future work to be done to exploit large noise variance on protected groups with improved fairness and accuracy.
- Why unresolved: The paper provides insights on the relationship between estimator variance and fairness/accuracy, but does not propose specific interventions or methods to leverage these insights for improving fairness.
- What evidence would resolve it: The development and evaluation of new fairness-enhancing interventions that explicitly consider the role of estimator variance.

## Limitations

- The decomposition of instability into epistemic and aleatoric components relies on bootstrap resampling, which assumes the training data is representative of the underlying distribution.
- The proposed variance-based fairness metrics may be sensitive to subgroup definitions and sample sizes, potentially leading to misleading fairness assessments in practice.
- The effectiveness of the model selection procedure in identifying stable, fair, and accurate models may depend heavily on the specific dataset and model characteristics.

## Confidence

- **High confidence**: The mechanism that epistemic instability decreases with more training data while aleatoric instability remains constant is well-supported by the theoretical framework and empirical evidence.
- **Medium confidence**: The claim that variance-based fairness metrics can reveal corrective effects on model accuracy and fairness is plausible but may depend heavily on the specific dataset and model characteristics.
- **Medium confidence**: The effectiveness of the proposed model selection procedure in narrowing down a large set of good models is supported by the results, but the generalizability to other datasets and model architectures requires further validation.

## Next Checks

1. **Data Quality Assessment**: Evaluate the impact of label noise and data quality on the epistemic-aleatoric decomposition by systematically injecting noise into the datasets and measuring the changes in instability components.
2. **Subgroup Sensitivity Analysis**: Investigate the robustness of the variance-based fairness metrics to different subgroup definitions and sample sizes by conducting sensitivity analyses across various demographic partitions.
3. **Cross-Dataset Generalization**: Test the proposed model selection procedure on additional datasets and model architectures to assess its effectiveness in identifying stable, fair, and accurate models beyond the two benchmark datasets used in the study.