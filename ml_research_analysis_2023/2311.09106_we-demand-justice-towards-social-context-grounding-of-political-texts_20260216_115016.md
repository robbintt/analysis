---
ver: rpa2
title: '"We Demand Justice!": Towards Social Context Grounding of Political Texts'
arxiv_id: '2311.09106'
source_url: https://arxiv.org/abs/2311.09106
tags:
- context
- social
- text
- event
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper defines and operationalizes "Social Context Grounding"
  in political discourse, focusing on understanding ambiguous statements in their
  real-world context. The authors propose two challenging datasets - Tweet Target
  Entity and Sentiment Detection, and Vague Text Disambiguation - that require understanding
  the social context of political texts.
---

# "We Demand Justice!": Towards Social Context Grounding of Political Texts

## Quick Facts
- arXiv ID: 2311.09106
- Source URL: https://arxiv.org/abs/2311.09106
- Reference count: 9
- Key outcome: DCF models achieve 73.56 F1 on Target Entity task and 71.71 F1 on Vague Text Disambiguation task

## Executive Summary
This paper introduces Social Context Grounding for political discourse, focusing on understanding ambiguous statements through their real-world context. The authors propose two challenging datasets requiring contextual understanding: Tweet Target Entity and Sentiment Detection, and Vague Text Disambiguation. They evaluate various models including PLMs, LLMs, static contextualized embeddings, and DCF models. DCF models outperform others by explicitly modeling social context through graph structures, achieving significant improvements over baseline approaches.

## Method Summary
The authors collect two datasets for social context grounding: one for target entity and sentiment detection (1,797 examples), and another for vague text disambiguation (410 examples). They implement baseline models using PLMs (BERT, RoBERTa), LLMs (GPT-3), and static contextualized embeddings. The DCF framework builds graph structures connecting authors, events, text, and entities, using encoder-composer modules to propagate contextual information. Models are evaluated using macro-averaged precision, recall, and F1 scores for classification tasks.

## Key Results
- DCF models outperform PLM baselines with 73.56 F1 on Target Entity task vs 63.43 F1
- DCF models achieve 71.71 F1 on Vague Text Disambiguation task vs 62.21 F1 for PLM baselines
- Static contextualized embeddings and LLM baselines show intermediate performance between PLMs and DCF models

## Why This Works (Mechanism)

### Mechanism 1
DCF models outperform PLM baselines by explicitly modeling social context through graph structures. The framework builds sub-graphs connecting authors, events, text, and entities, then uses encoder-composer modules to propagate contextual information within the graph. Core assumption: contextual relationships between social entities can be captured as graph structures and processed to improve understanding. Evidence: DCF achieves 73.56 F1 on Target Entity task. Break condition: if social context relationships are too sparse, graph structure adds noise rather than signal.

### Mechanism 2
Political text modeling requires understanding entity mentions absent from text itself. The Target Entity task requires identifying targets and sentiments even when entities aren't explicitly mentioned, relying on event context and author history. Core assumption: political discourse often references entities implicitly through shared knowledge and event context. Evidence: "We collect tweets that don't directly mention the target entities" and "main entities of Trump, Dr. Ford, Kavanaugh... emerge as divisive entities." Break condition: if tweets explicitly mention all relevant entities, implicit understanding becomes unnecessary.

### Mechanism 3
Vague text disambiguation requires understanding opposing interpretations based on author affiliation and event context. The task provides author party affiliation and event context to disambiguate ambiguous statements that have different meanings depending on who says them and when. Core assumption: same vague political language can have opposite meanings depending on speaker's political affiliation and specific event context. Evidence: "seemingly similar language used by opposing sides" and collection of tweets without entity mentions to identify candidates for opposing interpretations. Break condition: if vague text is context-independent or author affiliation doesn't correlate with stance.

## Foundational Learning

- Graph-based representation learning
  - Why needed here: DCF models rely on encoding and propagating information through graph structures to capture social context
  - Quick check: How does information flow through a graph structure to update node representations?

- Implicit entity reference in political discourse
  - Why needed here: Both datasets focus on understanding when entities are referenced without being explicitly mentioned
  - Quick check: What contextual cues help identify implicitly referenced entities in political text?

- Political framing and stance interpretation
  - Why needed here: Vague text disambiguation task requires understanding how same language can have opposite meanings based on political affiliation
  - Quick check: How does author political affiliation change interpretation of ambiguous political statements?

## Architecture Onboarding

- Component map: Data collection pipeline (AMT/human annotation) -> Graph construction module (author, event, text, entity nodes) -> DCF encoder-composer framework -> PLM baselines (BERT, RoBERTa, GPT-3) -> Evaluation metrics (macro-F1, precision, recall)

- Critical path: Data collection → Graph construction → DCF encoding → Composer propagation → Task-specific classification

- Design tradeoffs: DCF explicitly models context relationships vs. PLM baselines that encode context as concatenated text

- Failure signatures: Poor performance on examples where entities are implicitly referenced but not modeled in graph; failure to disambiguate vague text when context is insufficient

- First 3 experiments:
  1. Implement Target Entity and Sentiment task with simple PLM baseline (no context)
  2. Build DCF graph structure and run encoding without composer
  3. Add composer propagation and compare performance against baseline

## Open Questions the Paper Calls Out

### Open Question 1
How does DCF model performance change when additional social context features like politician relationships and symbolic contextual information are included? The authors mention future work includes building models that account for context in different forms, as social relationships and symbolic contextual information are not included in current DCF model. Experimental results comparing performance with and without additional social context features would resolve this.

### Open Question 2
How does performance of social context grounding models vary across different political domains or languages? The authors note their work only addresses English language text in US political domain, suggesting performance may vary in different contexts. Experimental results evaluating performance on datasets from different political domains or languages would resolve this.

### Open Question 3
How does quality of vague text disambiguation using LLM-generated explanations compare to human-generated explanations across different political events? The authors performed human evaluation of GPT-3 generations finding 73.26% reasonable explanations vs 79.80% for human workers, but only on a subset. A comprehensive human evaluation across different political events would resolve this.

## Limitations
- Relatively small annotated datasets (1,797 tweets and 410 examples) may not fully capture political discourse complexity
- DCF performance improvements may be partially attributed to specific graph construction choices rather than framework itself
- Model's reliance on specific entity and event relationships might limit generalizability to political domains with different social dynamics

## Confidence
- High Confidence in core finding that DCF models outperform PLM baselines (73.56 vs 63.43 F1 for Target Entity, 71.71 vs 62.21 F1 for Vague Text Disambiguation)
- Medium Confidence in mechanism explanations, as theoretical justification exists but limited ablation studies prevent definitive attribution of performance gains
- Medium Confidence in dataset quality and annotation process, given use of Master Workers but without detailed inter-annotator agreement statistics

## Next Checks
1. Perform ablation study on graph components by removing individual components (author nodes, event nodes, entity nodes) to determine which relationships contribute most to performance improvements
2. Test DCF models on political discourse from different domains or time periods to assess generalizability beyond Kavanaugh hearing context
3. Implement PLM baseline that explicitly concatenates all available context to determine if performance gains are truly due to graph structure rather than simply providing more context