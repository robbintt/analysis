---
ver: rpa2
title: 'MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment
  Tasks'
arxiv_id: '2310.19677'
source_url: https://arxiv.org/abs/2310.19677
tags:
- causal
- moral
- story
- human
- people
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates alignment between large language models
  (LLMs) and human intuition on causal and moral judgment tasks. It collects a dataset
  of 206 stories from 24 cognitive science papers, annotating each with factors known
  to influence human judgments.
---

# MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks

## Quick Facts
- arXiv ID: 2310.19677
- Source URL: https://arxiv.org/abs/2310.19677
- Reference count: 40
- Key outcome: Experiments show larger models and RLHF improve aggregate alignment, but factor-level analysis reveals systematic differences in how models weigh causal and moral factors compared to humans.

## Executive Summary
This paper introduces MoCa, a dataset of 206 stories from cognitive science literature annotated with factors influencing human causal and moral judgments. The authors evaluate various LLMs on this dataset, measuring both aggregate alignment and factor-specific tendencies using Average Marginal Component Effect (AMCE) analysis. Results show that while larger models and RLHF training improve overall alignment, different models exhibit distinct patterns in how they weigh specific factors, revealing systematic divergences from human reasoning patterns that aggregate metrics obscure.

## Method Summary
The authors collected stories from 24 cognitive science papers, annotating each with factors that influence human judgments. They evaluated multiple LLMs (GPT-2, GPT-3, GPT-3.5, GPT-4, Claude, RoBERTa) using zero-shot evaluation on both causal and moral judgment tasks. Model responses were compared to human judgments using metrics like discrete agreement, AUC, MAE, and CE. The AMCE method was used to analyze factor-specific tendencies, revealing systematic differences in how models weigh different attributes compared to human reasoning patterns.

## Key Results
- Larger models and RLHF training show better aggregate alignment with human judgments
- text-davinci-003 exhibits strong tendency for citing abnormal events as causal, diverging from human patterns
- GPT-4 and ChatGPT show moral judgment preferences that don't align with human tendencies on factors like self-benefit and inevitable harm
- Statistical analysis reveals systematic differences in factor weighting that aggregate metrics miss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conjoint analysis reveals systematic model biases that aggregate metrics obscure
- Mechanism: By constructing scenarios that systematically vary one factor at a time while holding others constant, the Average Marginal Component Effect (AMCE) isolates the impact of each factor on model judgments, revealing implicit tendencies that differ from human responses.
- Core assumption: Factor attributes are orthogonal enough to isolate their individual effects
- Evidence anchors:
  - [abstract] "we compute the Average Marginal Component Effect (AMCE) to reveal the implicit tendencies for each factor from human and LLM judgments"
  - [section 4.2.1] "We can use a non-parametric difference-in-means estimator to compute AMCE" with mathematical formulation
- Break condition: If factor attributes are correlated or confounded, AMCE estimates become biased and unreliable

### Mechanism 2
- Claim: Model size and training methodology create non-monotonic alignment patterns
- Mechanism: Larger models show better aggregate alignment, but fine-tuning techniques like RLHF can create divergent tendencies - some factors align better while others diverge from human patterns, as seen with abnormal event sensitivity and prescriptive norm violations.
- Core assumption: Training methodology fundamentally alters how models weight causal and moral factors
- Evidence anchors:
  - [section 4.1] "Comparing the result between Anthropic-claude-v1 and GPT3.5-davinci-v3, it seems that even though both models are fine-tuned with RLHF, on aggregate, their alignment with human judgments differs"
  - [section 4.2.2] "text-davinci-003 exhibits a strong tendency for citing abnormal events as causal compared to any other models (and to humans)"
- Break condition: If underlying data distributions shift significantly between model generations, size-alignment relationship may not hold

### Mechanism 3
- Claim: Carefully constructed datasets from cognitive science literature provide experimental control missing from crowd-sourced alternatives
- Mechanism: By using scenarios designed to systematically manipulate specific factors rather than free-form narratives, researchers can test whether models respond to the same factors humans do, revealing alignment or misalignment in systematic ways.
- Core assumption: Cognitive science experimental designs are valid representations of the factors they claim to test
- Evidence anchors:
  - [abstract] "we collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated"
  - [section 2] "Instead of relying on participant-generated scenarios, we collected two datasets from the existing literature in cognitive science: one on causal judgments and another on moral judgments"
- Break condition: If cognitive science experimental designs don't generalize to natural language understanding tasks, alignment measurements become invalid

## Foundational Learning

- Concept: Counterfactual reasoning in causal judgments
  - Why needed here: The paper tests whether models can identify causes based on counterfactual dependencies, which is fundamental to human causal reasoning
  - Quick check question: If a story states "The computer crashed because Jane logged on at 9:00 am when only Lauren was allowed," what counterfactual information would humans use to determine causality?

- Concept: Moral permissibility vs. moral obligation
  - Why needed here: The moral judgment task asks about permissibility (is an action acceptable?) rather than obligation (is it required?), which requires different reasoning about tradeoffs and consequences
  - Quick check question: In a trolley problem, why might an action be permissible but not obligatory, and how would a model need to reason differently about each?

- Concept: Factor-attribute annotation methodology
  - Why needed here: The dataset uses expert annotators to identify which factors are present in each story, requiring understanding of how abstract concepts map to concrete text segments
  - Quick check question: Given a story about violating a rule that led to an outcome, what textual evidence would support annotating it with "norm type: prescriptive" versus "event normality: abnormal"?

## Architecture Onboarding

- Component map: Story collection → Factor annotation → Model evaluation → AMCE computation → Analysis
- Critical path: Factor annotation accuracy → Model evaluation consistency → AMCE statistical validity → Tendency interpretation reliability
- Design tradeoffs: Expert annotation ensures quality but limits scalability vs. automated annotation would scale but risk accuracy
- Failure signatures: Model hallucinations (incorrect story facts), factor inference errors (misidentifying which factors apply), AMCE confidence intervals crossing zero
- First 3 experiments:
  1. Run baseline evaluation with no prompts to establish raw model alignment
  2. Test persona prompting to see if different simulated identities shift model tendencies
  3. Apply automatic prompt engineering to find optimal prompts for each factor type

## Open Questions the Paper Calls Out

- Question: How do different fine-tuning methods beyond RLHF (e.g., constitutional AI, adversarial training) impact alignment with human causal and moral intuitions?
  - Basis in paper: [explicit] The paper mentions RLHF and constitution AI as different fine-tuning methods but only provides results for RLHF. It notes that models trained with RLHF show different tendencies compared to humans, particularly in moral judgments.
  - Why unresolved: The paper focuses on RLHF and does not explore other fine-tuning techniques in depth.
  - What evidence would resolve it: Experiments comparing the implicit tendencies of models fine-tuned with different methods (e.g., constitutional AI, adversarial training) on the same causal and moral judgment tasks.

- Question: Can we develop a more nuanced evaluation framework that accounts for the diversity of human moral intuitions rather than relying on aggregate metrics?
  - Basis in paper: [inferred] The paper acknowledges that human moral intuitions vary substantially and that current evaluation against aggregate human judgments may not capture this diversity. It suggests that different prompts might align better with different subgroups of human participants.
  - Why unresolved: The paper uses aggregate human judgments for evaluation but recognizes the limitations of this approach.
  - What evidence would resolve it: Development and validation of a framework that can evaluate model alignment with different subgroups of human participants based on their moral reasoning patterns.

- Question: How do implicit biases in the dataset collection process (e.g., selection of papers, transcription decisions) affect the computed AMCE and conclusions about model alignment?
  - Basis in paper: [explicit] The paper acknowledges that their meta-analysis approach might introduce implicit biases due to their selection process of papers and stories.
  - Why unresolved: The paper computes AMCE but does not fully investigate how biases in the dataset collection might influence these results.
  - What evidence would resolve it: Sensitivity analyses examining how different dataset collection choices (e.g., selecting different papers, including more stories) affect the computed AMCE and conclusions about model alignment.

## Limitations

- The AMCE method assumes factor attributes are sufficiently orthogonal, which may not hold for all scenarios, potentially biasing tendency estimates
- The study uses zero-shot evaluation without extensive prompt engineering, leaving open questions about whether different prompts might yield different alignment patterns
- Expert annotation ensures quality but limits scalability and may introduce selection biases in which factors are identified and annotated

## Confidence

- **High confidence**: Aggregate alignment metrics (AUC, MAE) showing overall model performance trends across model sizes and training methods
- **Medium confidence**: Factor-specific AMCE estimates, which depend on the validity of the orthogonal factor assumption and expert annotation accuracy
- **Low confidence**: Causal attribution mechanism interpretations, particularly for text-davinci-003's abnormal event sensitivity, which may reflect training data artifacts rather than genuine reasoning patterns

## Next Checks

1. Test factor correlation effects by computing pairwise correlations between factor attributes in the dataset and assessing how correlated factors affect AMCE reliability
2. Run cross-validation on annotation consistency by having multiple expert annotators independently label a subset of stories and measuring inter-rater reliability
3. Implement prompt variation experiments by systematically varying prompt templates for the same models and measuring how much alignment patterns shift across different prompts