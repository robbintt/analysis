---
ver: rpa2
title: 'MADLAD-400: A Multilingual And Document-Level Large Audited Dataset'
arxiv_id: '2309.04662'
source_url: https://arxiv.org/abs/2309.04662
tags:
- latn
- data
- dataset
- language
- bible
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MADLAD-400, a 3-trillion-token, document-level
  monolingual dataset covering 419 languages, created from CommonCrawl and manually
  audited to address the lack of high-quality general-domain multilingual datasets.
  The authors train a 10.7B-parameter multilingual machine translation model and an
  8B-parameter decoder-only language model on this data, evaluating them on several
  multilingual benchmarks.
---

# MADLAD-400: A Multilingual And Document-Level Large Audited Dataset

## Quick Facts
- arXiv ID: 2309.04662
- Source URL: https://arxiv.org/abs/2309.04662
- Reference count: 40
- Primary result: 3 trillion-token, 419-language multilingual dataset with competitive 10.7B-parameter MT model

## Executive Summary
This paper introduces MADLAD-400, a 3-trillion-token, document-level monolingual dataset covering 419 languages created from CommonCrawl and manually audited to address the lack of high-quality general-domain multilingual datasets. The authors train a 10.7B-parameter multilingual machine translation model and an 8B-parameter decoder-only language model on this data, evaluating them on several multilingual benchmarks. Results show that the MT models are competitive with much larger models, though still lagging behind NLLB-54B on some tasks, and few-shot translation using the language model remains significantly weaker than supervised models. The dataset and models are released to support further research in inclusive multilingual NLP.

## Method Summary
The authors create MADLAD-400 by processing CommonCrawl snapshots through a multi-stage pipeline: first applying document-level language identification using a 498-language LangID model, then manually auditing 20 documents per language to identify quality issues, and finally applying automated filters for problematic content. The resulting dataset contains 3 trillion tokens across 419 languages. They train a 10.7B-parameter multilingual machine translation model and an 8B-parameter decoder-only language model using UniMax sampling to handle the highly imbalanced language distribution. Models are evaluated on WMT, Flores-200, NTREX, and Gatones benchmarks using BLEU and chrF metrics.

## Key Results
- MADLAD-400 contains 3 trillion tokens across 419 languages, significantly larger than comparable multilingual datasets
- 10.7B-parameter MT model achieves competitive performance with models 5× larger on several benchmarks
- Decoder-only 8B LM shows weak few-shot translation capabilities compared to supervised approaches
- Manual auditing identified and removed problematic content from 79 languages, improving overall dataset quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Document-level LangID model trained on 498 languages enables fine-grained language identification in web-crawled corpora.
- Mechanism: The model assigns document-level annotations by majority vote over sentence-level predictions, allowing for more accurate language filtering compared to line-level approaches.
- Core assumption: Document-level annotations improve language identification accuracy for web text that often contains mixed-language content.
- Evidence anchors:
  - [abstract]: "train a document-level LangID model on 498 languages to obtain CommonCrawl annotations at a document level"
  - [section]: "use a highly multilingual LangID model to provide document-level annotations"
  - [corpus]: Weak evidence - corpus analysis focuses on related document-level datasets rather than LangID models specifically.

### Mechanism 2
- Claim: Manual auditing and filtering removes low-quality and misclassified content from the dataset.
- Mechanism: Quality review of 20 documents per language identifies problematic content (religious, pornographic, boilerplate) that is then removed or corrected through additional filters.
- Core assumption: Human inspection can identify systematic quality issues that automated filters miss.
- Evidence anchors:
  - [abstract]: "manually audit our data" and "correct language names and remove languages"
  - [section]: "We manually audit our data. Based on our findings, we discard 79 of the languages..."
  - [corpus]: Strong evidence - multiple corpus papers mention similar manual auditing processes as critical quality control steps.

### Mechanism 3
- Claim: Massive scale (3 trillion tokens) enables training of competitive multilingual models despite smaller parameter count.
- Mechanism: The large monolingual corpus provides sufficient training signal for language models to learn robust representations across 419 languages.
- Core assumption: Sufficient training data can compensate for smaller model size in multilingual settings.
- Evidence anchors:
  - [abstract]: "train a 10.7B-parameter multilingual machine translation model... and find that it is competitive with models that are significantly larger"
  - [section]: "train a 10.7B-parameter multilingual machine translation model on 250 billion tokens covering over 450 languages"
  - [corpus]: Weak evidence - corpus neighbors focus on related datasets but don't directly address scale-competence relationship.

## Foundational Learning

- Concept: Document-level vs sentence-level language identification
  - Why needed here: The paper explicitly contrasts these approaches and uses document-level for improved accuracy
  - Quick check question: Why does majority vote over sentence-level predictions improve language identification compared to line-level approaches?

- Concept: Web corpus filtering and preprocessing
  - Why needed here: The paper describes multiple filtering stages (deduplication, length thresholds, content filtering) as critical to dataset quality
  - Quick check question: What types of content are identified as problematic during manual auditing and how are they filtered?

- Concept: Multilingual model training with imbalanced data
  - Why needed here: The paper uses UniMax sampling to handle the 419-language dataset where languages have vastly different token counts
  - Quick check question: How does UniMax sampling address the challenge of training on highly imbalanced multilingual datasets?

## Architecture Onboarding

- Component map: CommonCrawl → Document-level LangID → Manual audit → Filtering → Model training (MT and LM) → Evaluation
- Critical path: CommonCrawl → Document-level LangID → Manual audit → Filtered MADLAD-400 → Model training → Evaluation
- Design tradeoffs: Scale vs quality (aggressive filtering may remove some valid data), model size vs data volume (smaller models compensated by massive data), automation vs manual inspection (trade-off between efficiency and quality)
- Failure signatures: Poor language identification (mixed-language documents), low model performance on certain languages (inadequate data or quality issues), overfitting to low-resource languages (insufficient regularization)
- First 3 experiments:
  1. Test document-level vs sentence-level LangID accuracy on a sample of mixed-language web documents
  2. Evaluate the impact of different filtering thresholds on dataset size and downstream model performance
  3. Compare UniMax sampling vs uniform sampling on model performance across high vs low-resource languages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are the various filtering methods (virama correction, Zawgyi encoding conversion, Chinese porn filter) at removing low-quality or problematic content from MADLAD-400 without introducing biases or artifacts in the dataset?
- Basis in paper: [explicit] The authors describe applying these filters after auditing the dataset, but do not provide quantitative evaluation of their effectiveness or potential side effects.
- Why unresolved: The paper focuses on qualitative audit results and overall dataset size, without measuring precision/recall of the filters or studying their impact on downstream model performance.
- What evidence would resolve it: Systematic evaluation of filter precision/recall, comparison of model performance trained on filtered vs unfiltered data, and analysis of any introduced biases in the cleaned dataset.

### Open Question 2
- Question: What is the relationship between language size (number of tokens) and memorization rates in both monolingual and multiway translation settings, and are there other factors beyond resource level that influence memorization vulnerability?
- Basis in paper: [explicit] The authors observe that some lower-resource languages may exhibit higher memorization rates, but find no strong correlation and note that many languages show no memorization at all.
- Why unresolved: The paper provides preliminary memorization analysis with limited statistical evidence and does not identify the specific factors contributing to vulnerability differences.
- What evidence would resolve it: Comprehensive memorization analysis across all languages with statistical correlation studies, identification of specific language or data characteristics that predict memorization risk.

### Open Question 3
- Question: How do the MADLAD-400-based models compare to other large multilingual models on non-translation NLP tasks across the full 419 languages, and what factors limit their performance on these tasks?
- Basis in paper: [inferred] The authors acknowledge that decoder-only models are often evaluated on non-translation tasks but did not conduct such evaluations due to limited benchmark availability, especially for tail languages.
- Why unresolved: The paper focuses on translation evaluation and only mentions this limitation without exploring the broader capabilities of the trained models.
- What evidence would resolve it: Comprehensive evaluation of MADLAD-400-based models on diverse NLP tasks (classification, question answering, etc.) across all 419 languages using appropriate benchmarks and task-specific metrics.

## Limitations
- Manual auditing only examined 20 documents per language, potentially missing systematic quality issues
- 10.7B-parameter MT model still lags behind NLLB-54B on specific tasks, indicating fundamental scaling limits
- Decoder-only LM shows weak few-shot translation capabilities, limiting practical utility for low-resource scenarios

## Confidence
**High Confidence**: Dataset creation methodology and scale claims are well-documented and reproducible
**Medium Confidence**: Competitive performance claims supported by benchmark results but limited to specific evaluation sets
**Low Confidence**: Claims about few-shot translation weakness lack detailed analysis of failure modes or mitigation strategies

## Next Checks
1. Randomly sample 100 additional documents per language from MADLAD-400 and conduct blind quality assessment to verify that the manual auditing process captured systematic issues rather than just obvious outliers
2. Train smaller models (1B-2B parameters) on subsets of MADLAD-400 to determine the minimum data requirements for competitive performance, testing whether the massive scale is truly necessary or if quality improvements could compensate
3. Test the trained models on languages not included in MADLAD-400 but present in evaluation benchmarks to assess genuine multilingual generalization versus memorization of training data patterns