---
ver: rpa2
title: 'TGRL: An Algorithm for Teacher Guided Reinforcement Learning'
arxiv_id: '2307.03186'
source_url: https://arxiv.org/abs/2307.03186
tags:
- teacher
- learning
- tgrl
- performance
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of balancing teacher-student\
  \ learning with reinforcement learning (RL) to improve performance in sequential\
  \ decision-making tasks. The core method, Teacher Guided Reinforcement Learning\
  \ (TGRL), dynamically adjusts the importance of teacher supervision by comparing\
  \ the main policy\u2019s performance to an auxiliary policy trained only with RL\
  \ rewards."
---

# TGRL: An Algorithm for Teacher Guided Reinforcement Learning

## Quick Facts
- arXiv ID: 2307.03186
- Source URL: https://arxiv.org/abs/2307.03186
- Reference count: 28
- The paper introduces Teacher Guided Reinforcement Learning (TGRL), which dynamically adjusts the importance of teacher supervision by comparing main policy performance to an auxiliary policy trained only with RL rewards, achieving 73% success rate in tactile sensing tasks versus 47% for vanilla RL and 54% for Teacher-Student Learning.

## Executive Summary
TGRL addresses the challenge of balancing teacher supervision with reinforcement learning in sequential decision-making tasks. The core insight is to dynamically adjust the teacher's influence by comparing the main policy's performance (learning from both rewards and teacher) against an auxiliary policy trained only on rewards. This performance comparison serves as a signal to increase or decrease the teacher's importance, allowing the agent to rely more on the teacher early in training and more on rewards later. TGRL demonstrates significant performance improvements across diverse environments including robotic in-hand re-orientation tasks without requiring hyperparameter tuning.

## Method Summary
TGRL dynamically balances teacher supervision and RL rewards through a constrained optimization framework. It trains two policies simultaneously: a main policy that learns from both rewards and teacher guidance, and an auxiliary policy that learns only from rewards. The algorithm uses a Lagrange multiplier λ to control the balancing coefficient α/(1+λ), which adjusts based on performance comparisons between the two policies. If the main policy outperforms the auxiliary policy, teacher importance increases; otherwise, it decreases. Performance differences are estimated using off-policy correction via the objective difference lemma, making the method data-efficient. The algorithm is implemented using off-policy actor-critic methods with shared replay buffers.

## Key Results
- TGRL achieved 73% success rate in Shadow Hand tactile sensing re-orientation task versus 47% for vanilla RL and 54% for Teacher-Student Learning
- Outperformed strong baselines across diverse domains including Tiger Door, Lava Crossing, Memory, Light-Dark Ant, and Shadow Hand environments
- Demonstrated robustness by using the same hyperparameters (α=3, λinit=9, µ=3e-3) across all experiments without tuning
- Showed ability to reduce reliance on teacher supervision as RL learning progresses, outperforming fixed balancing coefficient approaches

## Why This Works (Mechanism)

### Mechanism 1
TGRL uses performance comparison between main policy (learning from rewards + teacher) and auxiliary policy (learning only from rewards) to dynamically adjust teacher influence. The mechanism increases teacher importance when main policy outperforms auxiliary, implemented via Lagrange multiplier λ controlling balancing coefficient α/(1+λ). Core assumption: performance difference is reliable signal for teacher benefit. Evidence: main idea described in abstract and section. Break condition: ineffective if auxiliary policy cannot learn reasonable policy due to exploration challenges.

### Mechanism 2
TGRL transforms constrained optimization into unconstrained dual problem solvable with gradient descent. Using Lagrange duality, constraint JR(π) ≥ JR(πR) is incorporated into objective, creating min-max problem where inner loop optimizes policy and outer loop updates λ. Core assumption: zero duality gap under bounded rewards/cross-entropy and monotonic convergence. Evidence: dual problem formulation and Proposition 3.1 on duality gap. Break condition: non-zero duality gap if cross-entropy becomes unbounded.

### Mechanism 3
TGRL estimates performance difference using off-policy correction via objective difference lemma. Instead of expensive rollouts, it uses shared replay buffer data and applies objective difference lemma to approximate JR(π) - JR(πR) = E[γt(AπR(s,a) - Aπ(s,a))]. Core assumption: off-policy correction provides unbiased estimate. Evidence: data-efficient option described and Proposition 3.2 on unbiased approximation. Break condition: high variance/bias if behavioral policy differs significantly from current policies.

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The paper explicitly frames the problem as a POMDP where the agent has access only to observations, not underlying states, and the teacher may have access to privileged information.
  - Quick check question: In a POMDP, can the optimal policy depend only on the current observation, or must it depend on the history of observations?

- Concept: Cross-entropy as reward shaping
  - Why needed here: TGRL uses the cross-entropy between teacher and student action distributions as a form of reward shaping, where the agent is penalized for deviating from the teacher.
  - Quick check question: What is the mathematical form of the cross-entropy reward term H X(π|¯π) used in TGRL's objective?

- Concept: Duality gap in constrained optimization
  - Why needed here: The theoretical justification for TGRL relies on proving that the duality gap is zero, ensuring that solving the dual problem solves the primal constrained problem.
  - Quick check question: What are the key assumptions required for strong duality (zero duality gap) to hold in TGRL's optimization problem?

## Architecture Onboarding

- Component map: Main policy π -> Auxiliary policy πR -> Shared replay buffer -> Critic networks QR, QI -> Lagrange multiplier λ -> Performance difference estimator

- Critical path:
  1. Collect trajectories using both π and πR
  2. Store in shared replay buffer
  3. Update critics QR and QI
  4. Update πR to maximize QR
  5. Update π to maximize QR + (α/(1+λ))QI
  6. Estimate performance difference using off-policy correction
  7. Update λ using gradient descent: λnew = λold - µ[JR(π) - JR(πR)]

- Design tradeoffs:
  - Joint vs separate replay buffers: Joint buffers allow πR to learn from π's trajectories, improving πR's performance and making the constraint stricter. Separate buffers may lead to poor πR performance and ineffective constraints.
  - Fixed vs adaptive balancing coefficient: Adaptive coefficients allow the agent to rely more on the teacher early in training and more on rewards later, outperforming fixed coefficients found by hyperparameter search.

- Failure signatures:
  - Poor performance on tasks requiring substantial deviation from the teacher: The constraint may prevent exploration needed to find better policies.
  - Failure to learn πR: If πR cannot learn a reasonable policy using only rewards, the constraint becomes ineffective.
  - High variance in λ updates: If the performance difference estimates are noisy, λ may oscillate or converge slowly.

- First 3 experiments:
  1. Tiger Door environment: Test basic functionality with discrete actions and privileged teacher information.
  2. Lava Crossing environment: Test on a continuous observation space with a different type of information gap.
  3. Shadow Hand pen reorientation: Test on a challenging continuous control task with severe partial observability using only tactile sensors.

## Open Questions the Paper Calls Out
- How to handle multiple sub-optimal teachers with conflicting actions (not explored in current framework)
- Making the balancing coefficient state-dependent rather than time-dependent (mentioned as future work)
- Sensitivity of TGRL to clipping threshold for cross-entropy term in theoretical analysis

## Limitations
- Algorithm effectiveness depends heavily on auxiliary policy πR successfully learning a reasonable policy
- Performance comparison signal may become unreliable if πR's learning plateaus
- Method requires careful handling of boundedness constraints to ensure theoretical guarantees

## Confidence
- Core TGRL mechanism (claim cluster 1): Medium confidence due to empirical evidence showing performance improvements
- Theoretical foundations (claim cluster 2): Low confidence regarding zero duality gap assumption
- Off-policy performance estimation (claim cluster 3): Medium confidence due to theoretical soundness but potential high variance

## Next Checks
1. Test TGRL in environments where the teacher is significantly suboptimal to verify the algorithm can reduce teacher influence appropriately
2. Evaluate the sensitivity of TGRL to replay buffer configuration (joint vs separate) and measure its impact on πR learning quality
3. Analyze the variance of λ updates across training runs to assess the stability of the performance difference estimation