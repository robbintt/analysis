---
ver: rpa2
title: 'MFABA: A More Faithful and Accelerated Boundary-based Attribution Method for
  Deep Neural Networks'
arxiv_id: '2312.13630'
source_url: https://arxiv.org/abs/2312.13630
tags:
- mfaba
- attribution
- gradient
- methods
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MFABA, a boundary-based attribution method
  for deep neural networks that combines adversarial attack techniques with second-order
  Taylor expansion of the loss function. The method addresses limitations in existing
  attribution approaches by providing more faithful interpretations while significantly
  improving computational efficiency.
---

# MFABA: A More Faithful and Accelerated Boundary-based Attribution Method for Deep Neural Networks

## Quick Facts
- arXiv ID: 2312.13630
- Source URL: https://arxiv.org/abs/2312.13630
- Reference count: 7
- Primary result: Over 100x speedup with improved attribution accuracy using second-order Taylor expansion

## Executive Summary
MFABA is a novel boundary-based attribution method that combines adversarial attack techniques with second-order Taylor expansion to provide more faithful interpretations of deep neural networks while achieving significant computational efficiency. The method addresses limitations in existing attribution approaches by capturing local curvature in the loss function and filtering non-aggressive samples that introduce noise. MFABA achieves over 100x speedup compared to state-of-the-art methods while maintaining superior attribution quality across multiple benchmark datasets.

## Method Summary
MFABA leverages second-order Taylor expansion of the loss function combined with adversarial attack techniques to compute feature attributions. The method preserves gradient graph information during forward/backward propagation, enabling reuse of computed gradients across multiple iterations and reducing computational overhead. MFABA filters out non-aggressive samples that don't cross decision boundaries, focusing only on samples that meaningfully contribute to attribution. The method satisfies both sensitivity and implementation invariance axioms, providing theoretically sound attribution maps that highlight important input features.

## Key Results
- 101x speedup on CIFAR10 with ResNet-50 (136.96 FPS vs 1.35 FPS for BIG)
- Error rate of 0.01165 on ImageNet compared to 0.02058 for vanilla methods
- High insertion score (0.5368) and AUC (0.5389) on Inception-v3
- Satisfies both sensitivity and implementation invariance axioms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MFABA uses second-order Taylor expansion to capture curvature in loss function, providing more accurate attribution than first-order methods
- Mechanism: Second-order Taylor expansion approximates loss function as L(x_j) = L(x_j-1) + ∂L(x_j-1)/∂x_j-1 · (x_j - x_j-1) + 1/2 · ∂²L(x_j-1)/∂x²_j-1 · (x_j - x_j-1)²
- Core assumption: Local behavior of model can be accurately captured through second-order approximation near decision boundaries
- Evidence anchors:
  - [abstract] "MFABA proposes a novel idea based on the second-order Taylor expansion of the loss function in addition to IG"
  - [section] "To more accurately depict the local behavior of the model in the vicinity of a given input point... we consider that the corresponding function L can be transformed with the second-order Taylor expansion"
- Break condition: When x_j and x_j-1 are not sufficiently close for Taylor expansion to be valid, or when Hessian computation becomes computationally prohibitive

### Mechanism 2
- Claim: MFABA removes non-aggressive samples to improve attribution accuracy
- Mechanism: Only samples that cross decision boundaries (aggressive samples) are used in attribution calculation, while samples that don't change classification are excluded
- Core assumption: Non-aggressive samples introduce noise that degrades attribution quality
- Evidence anchors:
  - [section] "We have included more visualization results of MFABA-norm and MFABA-cosine in the Appendix folder. We find that all non-aggressive samples deviated from the key features have critical impacts on the results"
- Break condition: When aggressive sample identification fails or when too few aggressive samples are found

### Mechanism 3
- Claim: MFABA achieves 100x speedup by preserving gradient graph during forward/backward propagation
- Mechanism: Instead of multiple rounds of propagation required by IG (30-200), MFABA needs only 3-10 rounds by keeping gradient information ∂L(x_j)/∂x_j throughout the process
- Core assumption: Gradient graph preservation allows reuse of computed information without recomputation
- Evidence anchors:
  - [abstract] "MFABA achieves over 100x speedup compared to state-of-the-art methods, processing 136.96 FPS on CIFAR10 with ResNet-50 versus 1.35 FPS for BIG"
  - [section] "The gradient information of ∂L(x_j)/∂x_j is kept during the gradient ascending process, which avoid recomputing for the subsequent steps"
- Break condition: When memory constraints prevent storing gradient graph, or when gradient graph becomes too complex to manage

## Foundational Learning

- Concept: Adversarial attack methodology
  - Why needed here: MFABA leverages adversarial attack techniques to find decision boundaries for attribution
  - Quick check question: How does FGSM differ from PGD in terms of perturbation generation and computational requirements?

- Concept: Second-order Taylor expansion
  - Why needed here: Core mathematical foundation for capturing curvature in loss function
  - Quick check question: What is the primary advantage of second-order over first-order Taylor expansion in approximating nonlinear functions?

- Concept: Attribution axioms (Sensitivity and Implementation Invariance)
  - Why needed here: MFABA claims to satisfy these fundamental axioms that validate attribution methods
  - Quick check question: How does the Sensitivity axiom ensure that input features that change predictions receive non-zero attribution?

## Architecture Onboarding

- Component map: Input → Gradient ascent (preserving ∂L/∂x) → Decision boundary detection → Second-order Taylor expansion → Non-aggressive sample filtering → Attribution calculation
- Critical path: Input → Gradient ascent (preserving ∂L/∂x) → Decision boundary detection → Second-order Taylor expansion → Non-aggressive sample filtering → Attribution calculation
- Design tradeoffs: Accuracy vs. speed (second-order more accurate but slower than first-order), memory usage for gradient graph preservation vs. computational efficiency
- Failure signatures: Attribution maps showing noise or incorrect feature importance, significantly slower than expected performance, failure to cross decision boundaries
- First 3 experiments:
  1. Compare attribution maps on simple test image between MFABA and IG to verify qualitative improvements
  2. Measure FPS processing speed on CIFAR10 with ResNet-50 to confirm claimed 100x speedup
  3. Test Sensitivity axiom compliance by modifying single pixel and verifying non-zero attribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the second-order Taylor expansion in MFABA compare to first-order methods in terms of attribution accuracy across different model architectures?
- Basis in paper: [explicit] The paper mentions MFABA uses second-order Taylor expansion and compares it to vanilla methods using only first-order expansion, showing reduced error rates.
- Why unresolved: While error rate comparisons are provided, the paper doesn't conduct a comprehensive architectural analysis showing how second-order effects vary across different model depths or types.
- What evidence would resolve it: Systematic experiments comparing attribution accuracy of MFABA versus first-order methods across multiple architectures (CNNs, transformers, etc.) with varying depths and complexities.

### Open Question 2
- Question: What is the theoretical limit of MFABA's computational speedup compared to other boundary-based methods, and what factors constrain this limit?
- Basis in paper: [explicit] The paper demonstrates 101x speedup over BIG and discusses efficiency improvements, but doesn't establish theoretical bounds or identify limiting factors.
- Why unresolved: The paper provides empirical speedup data but lacks theoretical analysis of computational complexity bounds or identification of bottlenecks that prevent further optimization.
- What evidence would resolve it: Formal computational complexity analysis comparing MFABA to other boundary-based methods, identifying specific operations that dominate runtime and potential optimization opportunities.

### Open Question 3
- Question: How does MFABA's performance degrade when applied to non-image domains or more complex visual tasks like object detection or segmentation?
- Basis in paper: [explicit] The paper acknowledges limitation to conventional image datasets and states future work will extend to broader scenarios, but provides no experimental data on non-classification tasks.
- Why unresolved: All experiments are conducted on image classification tasks, leaving unknown how the method generalizes to tasks with different output structures or spatial relationships.
- What evidence would resolve it: Empirical evaluation of MFABA on object detection, semantic segmentation, and text classification tasks, measuring attribution quality and computational efficiency in these domains.

## Limitations
- Limited to image classification tasks; extension to other domains like object detection or segmentation remains unexplored
- Computational complexity of second-order Taylor expansion may become prohibitive for very deep architectures or large batch sizes
- Memory requirements for preserving gradient graphs throughout multiple iterations are not fully characterized

## Confidence
- **High Confidence**: The 100x speedup claim and FPS measurements (136.96 vs 1.35) are directly verifiable through the open-sourced code and established benchmark methodology
- **Medium Confidence**: The attribution quality improvements (error rate of 0.01165 vs 0.02058) depend on the quality of ground truth attribution maps, which are often subjective or synthetically generated
- **Medium Confidence**: The claim about satisfying both sensitivity and implementation invariance axioms appears theoretically sound but requires extensive testing across diverse model architectures

## Next Checks
1. **Memory Profiling**: Measure peak memory usage of MFABA versus BIG across different batch sizes and input resolutions to quantify the practical scalability limits
2. **Ablation Study**: Compare MFABA performance with and without the non-aggressive sample filtering mechanism to isolate its contribution to accuracy improvements
3. **Cross-Architecture Generalization**: Test MFABA on transformer-based architectures (Vision Transformers, BERT) to assess whether the second-order Taylor expansion remains computationally feasible beyond convolutional networks