---
ver: rpa2
title: 'PDFTriage: Question Answering over Long, Structured Documents'
arxiv_id: '2309.08872'
source_url: https://arxiv.org/abs/2309.08872
tags:
- document
- question
- questions
- pdftriage
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PDFTriage addresses the challenge of document question answering
  when the document is too large to fit in the LLM's context window. Current approaches
  treat structured documents like PDFs as plain text, losing the document's inherent
  structure and making it difficult to answer questions that reference pages, sections,
  or tables.
---

# PDFTriage: Question Answering over Long, Structured Documents

## Quick Facts
- **arXiv ID**: 2309.08872
- **Source URL**: https://arxiv.org/abs/2309.08872
- **Reference count**: 8
- **Primary result**: PDFTriage achieves 50.8% top-ranked answer accuracy on structured document QA, outperforming retrieval-augmented LLMs by enabling structure-aware retrieval and multi-step reasoning.

## Executive Summary
PDFTriage addresses the challenge of answering questions over long, structured documents like PDFs that exceed LLM context windows. Unlike existing approaches that treat documents as plain text, PDFTriage augments prompts with document structure metadata and model-callable retrieval functions, allowing the model to retrieve context based on structural elements (pages, sections, tables) rather than just content similarity. Experiments on 900+ human-generated questions over 80 structured documents show PDFTriage significantly outperforms existing retrieval-augmented approaches, particularly on questions requiring multi-step reasoning across different document parts.

## Method Summary
PDFTriage converts PDFs to HTML-like tree structures using Adobe Extract API to extract sections, section titles, page information, tables, and figures. The approach generates structured metadata representations mapped into JSON format, then uses five retrieval functions (fetch_pages, fetch_sections, fetch_table, fetch_figure, retrieve) to query the document. The LLM can call these functions in sequence to gather relevant context for answering questions, with final answer generation from the retrieved context. The system is evaluated across 10 question categories using both human and automated assessment of accuracy, informativeness, readability, and overall quality.

## Key Results
- PDFTriage achieves 50.8% of top-ranked answers overall, significantly outperforming existing retrieval-augmented LLMs
- The approach scores higher on accuracy, informativeness, readability, and overall quality metrics
- PDFTriage excels at questions requiring multi-step reasoning across different document parts

## Why This Works (Mechanism)

### Mechanism 1: Structure-Aware Retrieval Outperforms Flat Text Retrieval
- **Claim**: Retrieval augmented LLMs perform better when they can query documents using structural metadata (pages, sections, tables) rather than treating documents as flat text.
- **Mechanism**: PDFTriage augments prompts with document structure metadata and callable retrieval functions, enabling the model to retrieve context based on document structure rather than just content similarity.
- **Core assumption**: Document structure metadata is both available and accurately represents the document's organization.
- **Evidence anchors**: [abstract] "representing such structured documents as plain text is incongruous with the user's mental model of these documents with rich structure"
- **Break condition**: If document structure metadata is inaccurate or unavailable, the approach degrades to flat text retrieval.

### Mechanism 2: Multi-Step Reasoning via Function Chaining
- **Claim**: PDFTriage enables multi-step reasoning across different parts of a document by chaining function calls to gather relevant context.
- **Mechanism**: The model can call multiple retrieval functions in sequence (e.g., first fetch_section, then fetch_table) to gather all necessary context for answering complex questions that span multiple document elements.
- **Core assumption**: The LLM can correctly identify which functions to call and in what order.
- **Evidence anchors**: [abstract] "Our experiments demonstrate the effectiveness of the proposed PDFTriage-augmented models across several classes of questions where existing retrieval-augmented LLMs fail"
- **Break condition**: If the model fails to identify the correct sequence of function calls, it may retrieve irrelevant context.

### Mechanism 3: Reduced Context Window Requirements
- **Claim**: PDFTriage reduces the effective context window requirements by retrieving only the most relevant structured portions of the document.
- **Mechanism**: Instead of concatenating the entire document or retrieving large chunks of text, PDFTriage retrieves only the specific pages, sections, or elements needed to answer the question, reducing token usage while maintaining answer quality.
- **Core assumption**: Relevant context for answering a question can be isolated to specific document structures.
- **Evidence anchors**: [section] "PDFTriage utilizes more tokens than Page Retrieval... but the tokens are retrieved from multiple sections of the document that are non-consecutive"
- **Break condition**: If relevant context is spread across too many document elements, the approach may still exceed context limits.

## Foundational Learning

- **Document structure representation**: Why needed here: PDFTriage relies on accurate representation of document structure (pages, sections, tables, figures) to enable structure-aware retrieval. Quick check question: How does PDFTriage represent document structure in its prompt?
- **Function calling in LLMs**: Why needed here: The approach uses model-callable retrieval functions to enable the LLM to actively query the document. Quick check question: What are the five retrieval functions used in PDFTriage?
- **Question categorization**: Why needed here: The evaluation dataset includes 10 question categories that require different retrieval strategies. Quick check question: What are the 10 question categories used in the PDFTriage evaluation dataset?
- **Retrieval-augmented generation**: Why needed here: PDFTriage builds on retrieval-augmented LLMs but adds structure awareness. Quick check question: How does PDFTriage differ from standard retrieval-augmented approaches?

## Architecture Onboarding

- **Component map**: Document → Structure extraction → Prompt generation → LLM triage → Function calls → Context retrieval → Answer generation
- **Critical path**: Document → Structure extraction → Prompt generation → LLM triage → Function calls → Context retrieval → Answer generation
- **Design tradeoffs**:
  - Structure vs. content retrieval: Structure-aware retrieval enables new question types but requires accurate metadata
  - Function granularity: More functions enable more precise retrieval but increase complexity
  - Context window vs. accuracy: Retrieving only relevant context reduces tokens but may miss connections
- **Failure signatures**:
  - Inaccurate structure metadata leads to wrong function calls
  - LLM fails to identify correct function sequence for multi-step questions
  - Retrieved context insufficient for answering the question
  - Context window exceeded despite selective retrieval
- **First 3 experiments**:
  1. Test function calling accuracy: Feed simple questions requiring specific functions and verify correct function selection
  2. Evaluate structure metadata accuracy: Manually verify extracted structure matches document organization
  3. Measure context reduction: Compare tokens retrieved by PDFTriage vs. baseline approaches for equivalent questions

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does PDFTriage perform on documents with complex nested structures (e.g., deeply nested sections or subsections) compared to simpler hierarchical structures?
- **Basis in paper**: [inferred]
- **Why unresolved**: The paper focuses on a general approach for structured documents but does not specifically address the impact of document complexity on the performance of PDFTriage.
- **What evidence would resolve it**: Comparative experiments on datasets with documents of varying structural complexity, showing performance metrics for each complexity level.

### Open Question 2
- **Question**: What are the limitations of PDFTriage when dealing with documents that have a high degree of visual elements (e.g., charts, diagrams, and images) that are not easily converted to text?
- **Basis in paper**: [explicit]
- **Why unresolved**: The paper mentions the use of Adobe Extract API for converting PDFs to a structured format but does not discuss how PDFTriage handles documents with significant visual content.
- **What evidence would resolve it**: Detailed analysis of PDFTriage's performance on documents with varying amounts of visual content, including qualitative assessments of its ability to interpret and utilize non-textual information.

### Open Question 3
- **Question**: How does the performance of PDFTriage compare to human-level accuracy on document QA tasks, particularly for complex multi-step reasoning questions?
- **Basis in paper**: [inferred]
- **Why unresolved**: The paper demonstrates that PDFTriage outperforms existing retrieval-augmented LLMs but does not provide a comparison to human performance.
- **What evidence would resolve it**: A study comparing PDFTriage's performance against human annotators on the same set of questions, with metrics for accuracy and quality of answers.

## Limitations

- PDFTriage relies heavily on accurate document structure metadata extraction, which represents a critical bottleneck
- The approach assumes relevant context can be isolated to specific document structures, potentially missing connections across non-contiguous content
- The evaluation dataset, while extensive, may not fully represent real-world usage patterns or edge cases

## Confidence

- **High confidence**: The mechanism of using structure-aware retrieval functions is technically sound and demonstrably improves performance over flat text approaches
- **Medium confidence**: The claim that function chaining enables multi-step reasoning is supported by experimental results but lacks detailed ablation studies
- **Medium confidence**: The reduction in effective context window usage is logical but the tradeoff analysis needs more rigorous quantification

## Next Checks

1. **Function selection accuracy test**: Create a benchmark of 50 questions, each designed to require specific retrieval functions, and measure the percentage of times PDFTriage correctly identifies and chains the appropriate functions. Target accuracy should exceed 85% for reliable real-world deployment.

2. **Structure metadata robustness evaluation**: Systematically corrupt the document structure metadata (remove sections, mislabel tables, etc.) and measure degradation in answer quality. Determine the error tolerance threshold beyond which PDFTriage performance drops below baseline approaches.

3. **Cross-document generalization study**: Test PDFTriage on documents from categories not represented in the training/evaluation set to assess whether the approach generalizes beyond the 10 categories used in the current dataset.