---
ver: rpa2
title: 'DLAS: An Exploration and Assessment of the Deep Learning Acceleration Stack'
arxiv_id: '2311.08909'
source_url: https://arxiv.org/abs/2311.08909
tags:
- direct
- gemm
- pruning
- learning
- hikey
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Deep Learning Acceleration Stack (DLAS) framework combines
  machine learning and systems optimization techniques to address the computational
  demands of deploying Deep Neural Networks on resource-constrained devices. The authors
  evaluate across-stack interactions by varying parameters across two datasets, seven
  DNN architectures, four compression techniques, three algorithms with sparse/dense
  variants, auto-scheduled code generation, and four hardware platforms.
---

# DLAS: An Exploration and Assessment of the Deep Learning Acceleration Stack

## Quick Facts
- arXiv ID: 2311.08909
- Source URL: https://arxiv.org/abs/2311.08909
- Reference count: 40
- Primary result: DLAS framework reveals that across-stack interactions significantly affect performance, with model size, accuracy, and inference time not guaranteed to be correlated.

## Executive Summary
The Deep Learning Acceleration Stack (DLAS) framework provides a comprehensive evaluation of across-stack interactions in deep learning optimization. The authors demonstrate that interactions between machine learning and systems optimization techniques are non-trivial, with compression techniques, algorithms, and hardware choices creating complex dependencies that affect inference time and accuracy. Their experiments across seven DNN architectures, four compression techniques, three algorithms with sparse/dense variants, auto-scheduled code generation, and four hardware platforms reveal that optimal configurations are highly context-dependent.

## Method Summary
The study evaluates DLAS across multiple dimensions: two datasets (CIFAR-10 and ImageNet), seven DNN architectures, four compression techniques (pruning, quantization), three algorithms (direct, GEMM, spatial pack) with dense and sparse variants, auto-scheduled code generation using TVM's Ansor, and four hardware platforms. Models are trained from scratch on CIFAR-10 using SGD with 1cycle LR scheduler, while ImageNet models use pre-trained weights. Inference experiments measure median time across 150 runs, varying algorithms, compression levels, and compilation methods across all hardware platforms.

## Key Results
- Model size, accuracy, and inference time are not guaranteed to be correlated across hardware platforms
- Auto-tuning dramatically accelerates inference time but can change optimal compression technique
- Sparse computations rarely achieve expected speedups due to irregular access patterns and limited hardware support
- float16 consistently slows inference on CPUs due to software emulation
- GPU performance varies considerably with algorithm choice

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Across-stack interactions significantly alter optimal algorithm and compression choices
- Mechanism: Parameters from different DLAS layers interact non-trivially, meaning that an optimization beneficial in isolation may underperform or even degrade performance when combined with other choices
- Core assumption: The layers of DLAS are interdependent and not modular in their impact on performance
- Evidence anchors: [abstract] states "compiler auto-tuning can significantly alter what the best algorithm to use for a given configuration is"; [section] shows "tuning has created some significant differences in the relative performance of the experiments"

### Mechanism 2
- Claim: Model size, accuracy, and inference time are not guaranteed to be correlated
- Mechanism: Compression techniques can reduce model size and MACs while maintaining accuracy, but inference speedup depends heavily on hardware/software support for the compressed format
- Core assumption: Performance gains from compression are contingent on downstream stack support, not just reduced computational complexity
- Evidence anchors: [abstract] highlights "model size, accuracy, and inference time are not guaranteed to be correlated"; [section] shows sparse computations rarely achieve expected speedups

### Mechanism 3
- Claim: Auto-tuning dramatically accelerates inference time but can change optimal compression technique
- Mechanism: Auto-scheduling explores schedule transformations that are sensitive to data layouts, sparsity patterns, and data types, meaning the best compression technique can shift when schedules are optimized
- Core assumption: Auto-schedulers explore a search space that interacts with compression choices in non-obvious ways
- Evidence anchors: [abstract] notes "compiler auto-tuning can significantly alter what the best algorithm to use for a given configuration is"; [section] shows "float16 still gives a consistent slowdown"

## Foundational Learning

- Concept: Stack-based optimization and co-design
  - Why needed here: The paper's core contribution is a conceptual framework (DLAS) that unifies machine learning and systems optimizations across six layers, requiring understanding of how changes in one layer affect others
  - Quick check question: If you prune a model heavily, what downstream layers must also be optimized to realize speedup?

- Concept: Sparse data formats and their computational implications
  - Why needed here: Pruning techniques generate sparse matrices that require specific data formats (CSR, COO) and algorithms; performance depends on how well these are supported by software and hardware
  - Quick check question: Why does weight pruning often fail to deliver expected speedups on GPUs?

- Concept: Auto-scheduling and tensor compilers
  - Why needed here: The evaluation uses TVM's Ansor auto-scheduler to generate optimized schedules; understanding its search space and limitations is key to interpreting results
  - Quick check question: What happens to sparse model performance if the auto-scheduler cannot handle cross-thread reduction with varying loop sizes?

## Architecture Onboarding

- Component map: Datasets & Problem Spaces -> Models & Neural Architectures -> Model Optimizations -> Algorithms & Data Formats -> Systems Software -> Hardware
- Critical path: Model definition -> Compression (pruning/quantization) -> Algorithm selection (dense/sparse, direct/GEMM/spatial pack) -> Auto-scheduling (if enabled) -> Hardware execution
- Design tradeoffs: Balancing compression ratio vs accuracy retention, algorithm choice vs hardware support, search time for auto-tuning vs performance gains
- Failure signatures: Sparse models slower than dense on GPU (irregular access patterns), float16 slowdown on CPU (software emulation), auto-scheduler crashes or no improvement (search space issues)
- First 3 experiments:
  1. Run dense MobileNetV2 on HiKey CPU with direct and GEMM algorithms, compare inference times
  2. Apply 95% weight pruning to MobileNetV2, run with sparse GEMM on HiKey CPU, measure speedup vs dense
  3. Enable auto-scheduling for MobileNetV2 on i7, compare tuned vs untuned performance for all three algorithms

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do across-stack optimizations interact differently across various hardware platforms, and can we develop a unified framework to predict these interactions?
- Basis in paper: [explicit] The authors observe that hardware-specific factors significantly influence the effectiveness of model optimizations like quantization and pruning, and that auto-tuning results vary across different hardware platforms
- Why unresolved: The study highlights hardware-dependent variations but does not provide a predictive model for these interactions
- What evidence would resolve it: Empirical studies comparing the performance of the same optimization techniques across diverse hardware platforms, coupled with a theoretical model that explains the underlying hardware-specific interactions

### Open Question 2
- Question: What are the limitations of current sparse computation support in tensor compilers, and how can these be addressed to improve performance?
- Basis in paper: [explicit] The authors note that TVM's auto-scheduling system has limited support for sparse computations, particularly on GPUs, which affects the performance of pruned models
- Why unresolved: While the paper identifies the limitations, it does not propose specific solutions or improvements to sparse computation support in tensor compilers
- What evidence would resolve it: Development and testing of enhanced sparse computation algorithms in tensor compilers, with benchmarks demonstrating improved performance on both CPUs and GPUs

### Open Question 3
- Question: How does the interaction between model architecture and data-type quantization affect accuracy, and can we predict which architectures are more suitable for quantization?
- Basis in paper: [explicit] The authors observe that EfficientNetB0's architecture leads to significant accuracy drops when using int8 quantization, unlike other models
- Why unresolved: The study identifies this issue but does not explore why certain architectures are more susceptible to accuracy loss during quantization
- What evidence would resolve it: Analysis of architectural features that influence quantization performance, followed by predictive models to assess suitability for quantization before training

## Limitations

- Sparse computation support remains limited in auto-schedulers, potentially underestimating achievable speedups
- Cross-layer interactions are demonstrated but not formally modeled or predicted
- Hardware diversity is good but misses mobile GPUs and cloud accelerators

## Confidence

- High confidence: Model size/accuracy/speed correlations are hardware-dependent
- Medium confidence: Auto-tuning significantly alters optimal algorithm choices
- Low confidence: The framework comprehensively captures all relevant across-stack interactions

## Next Checks

1. Test whether the float16 slowdown on CPUs persists across different TVM versions and LLVM configurations
2. Evaluate whether enabling cross-thread reductions in sparse GEMM improves GPU performance for highly pruned models
3. Compare TVM auto-scheduling results with other compilers (TensorRT, ONNX Runtime) to validate generalizability of the across-stack interaction findings