---
ver: rpa2
title: How Abilities in Large Language Models are Affected by Supervised Fine-tuning
  Data Composition
arxiv_id: '2310.05492'
source_url: https://arxiv.org/abs/2310.05492
tags:
- data
- general
- code
- math
- abilities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how mathematical reasoning, code generation,
  and general human-aligning abilities in large language models (LLMs) are affected
  by supervised fine-tuning (SFT) data composition. The authors propose four research
  questions to explore the relationship between model performance and various factors
  such as data amount, composition ratio, model size, and SFT strategies.
---

# How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition

## Quick Facts
- arXiv ID: 2310.05492
- Source URL: https://arxiv.org/abs/2310.05492
- Reference count: 27
- Primary result: Different abilities (math, code, general) exhibit distinct scaling patterns with SFT data composition, with data amount being more influential than composition ratio

## Executive Summary
This paper investigates how mathematical reasoning, code generation, and general human-aligning abilities in large language models are affected by supervised fine-tuning (SFT) data composition. Through extensive experiments on LLaMA models (7B, 13B, 33B) using three benchmarks (GSM8K, HumanEval, MT-Bench), the study reveals that abilities scale differently with data amount - mathematical reasoning and code generation improve consistently while general ability plateaus after ~1k samples. The authors find that data composition can improve various abilities under limited data conditions but may cause performance conflicts when data is plentiful. They also propose Dual-stage Mixed Fine-tuning (DMT) as a strategy to alleviate both performance conflicts and catastrophic forgetting.

## Method Summary
The study prepares three datasets (GSM8K for math, HumanEval for code, MT-Bench for general human-aligning abilities) and fine-tunes LLaMA models of different sizes using four strategies: multi-task learning, sequential training, mixed sequential training, and the proposed Dual-stage Mixed Fine-tuning (DMT). Models are evaluated on their respective benchmarks to analyze scaling patterns, composition effects, and strategy effectiveness across different data amounts and model sizes.

## Key Results
- Mathematical reasoning and code generation improve consistently with increasing data amount, while general ability plateaus after ~1k samples
- Data composition improves abilities under low-resource conditions but causes performance conflicts in high-resource settings
- Sequential learning causes catastrophic forgetting of previously learned abilities
- The proposed DMT strategy effectively mitigates both performance conflicts and catastrophic forgetting

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Data composition in SFT causes ability-specific scaling patterns that vary with data volume and model size
- **Mechanism**: Mathematical reasoning and code generation abilities improve monotonically with increasing data amount, while general human-aligning abilities plateau after roughly 1k samples. Larger models generalize better across ability domains, especially under low-resource conditions
- **Core assumption**: The abilities require different data distributions and feature spaces, and model capacity determines how well these features can be jointly learned
- **Evidence anchors**: [abstract] "Mathematical reasoning and code generation improve as data amounts increase consistently, while the general ability is enhanced with about a thousand samples and improves slowly"; [section 3.3] "Mathematical reasoning capability shows a positive correlation with the data amount across various model sizes... general ability emerges with only around 1k data samples"
- **Break condition**: If abilities share similar feature distributions or the model capacity is insufficient to capture multiple ability domains

### Mechanism 2
- **Claim**: Low-resource mixed data improves abilities through positive transfer, while high-resource mixed data causes performance conflicts due to interference
- **Mechanism**: When data is scarce, diverse sources provide complementary signals that boost performance. When data is abundant, each ability's optimal distribution is overwhelmed by noise from other abilities, leading to interference and reduced performance
- **Core assumption**: Each ability has an optimal data distribution; mixing them creates either beneficial diversity (low-resource) or harmful noise (high-resource)
- **Evidence anchors**: [section 3.3] "Abilities are improved with low-resource and are decreased with high-resource compared to individual source abilities"; [section 3.4] "Data amounts directly influence each ability, while the data ratio is insignificant"
- **Break condition**: If abilities have highly overlapping distributions or if the model has sufficient capacity to learn disentangled representations

### Mechanism 3
- **Claim**: Sequential learning causes catastrophic forgetting, while the proposed Dual-stage Mixed Fine-tuning (DMT) strategy mitigates both forgetting and performance conflicts
- **Mechanism**: Sequential training overwrites parameters needed for previous abilities. DMT first trains on specialized data to build strong representations, then fine-tunes on mixed data with small amounts of specialized data retained to prevent forgetting while preserving general capability
- **Core assumption**: The model can retain specialized representations when exposed to small amounts of specialized data during general fine-tuning
- **Evidence anchors**: [section 3.5] "Sequential training and mixed sequential training preserve general ability while losing too many specialized abilities"; [section 3.5] "Our proposed DMT effectively alleviates both performance conflicts and catastrophic forgetting"
- **Break condition**: If the amount of retained specialized data is too small to maintain representations, or if the general data completely overwrites specialized features

## Foundational Learning

- **Concept**: Catastrophic forgetting in multi-task learning
  - Why needed here: Understanding why sequential training fails and how DMT addresses this is crucial for implementing the proposed solution
  - Quick check question: What happens to model parameters when fine-tuning on a new task without any mechanism to preserve previous knowledge?

- **Concept**: Scaling laws in large language models
  - Why needed here: Different abilities scale differently with data amount, and model size affects this scaling - essential for understanding when mixed data helps vs hurts
  - Quick check question: How does model performance typically change as training data increases, and what factors determine the shape of this scaling curve?

- **Concept**: Data composition and domain adaptation
  - Why needed here: The core problem is mixing data from different domains (math, code, general) and understanding when this helps vs hurts performance
  - Quick check question: What are the key differences between domain adaptation and multi-task learning, and how do these differences affect fine-tuning strategies?

## Architecture Onboarding

- **Component map**: GSM8K (math), Code Alpaca (code), ShareGPT (general) -> mixed datasets with varying proportions -> LLaMA 7B/13B/33B variants -> multi-task learning, sequential training, mixed sequential training, DMT strategies -> GSM8K test, HumanEval, MT-Bench metrics

- **Critical path**: 1. Prepare datasets with controlled composition ratios 2. Fine-tune models using specified strategy 3. Evaluate on in-domain benchmarks 4. Analyze scaling patterns and conflicts 5. Iterate with different data amounts and model sizes

- **Design tradeoffs**: Data amount vs. composition ratio (amount has more impact than ratio); Model size vs. resource efficiency (larger models generalize better but cost more); Training strategy complexity vs. performance (DMT is more complex but addresses key failure modes)

- **Failure signatures**: Performance degradation in high-resource mixed settings indicates interference; Catastrophic forgetting in sequential training indicates parameter overwriting; Inconsistent scaling across abilities suggests domain mismatch

- **First 3 experiments**: 1. Compare individual vs. mixed data fine-tuning on smallest model (7B) to observe basic scaling patterns 2. Test different composition ratios while holding data amount constant to verify ratio insignificance 3. Implement DMT with varying k values to find optimal balance between forgetting and conflict mitigation

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of different abilities scale with SFT data amounts for various model sizes?
  - Basis in paper: [explicit] The paper investigates the relationship between model abilities and various factors including data amounts, data composition ratio, model parameters, and SFT strategies
  - Why unresolved: The paper provides insights into how different abilities scale with SFT data amounts, but the exact scaling patterns for various model sizes are not explicitly stated
  - What evidence would resolve it: Detailed experimental results showing the scaling patterns of different abilities with varying data amounts for different model sizes would resolve this question

- **Open Question 2**: Are there performance conflicts when combining multiple abilities in SFT?
  - Basis in paper: [explicit] The paper investigates the interplay of data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT
  - Why unresolved: The paper mentions the investigation of performance conflicts, but the specific nature and extent of these conflicts are not explicitly stated
  - What evidence would resolve it: Experimental results demonstrating the presence or absence of performance conflicts when combining multiple abilities in SFT would resolve this question

- **Open Question 3**: What are the key factors that induce performance conflicts in SFT?
  - Basis in paper: [inferred] The paper discusses the investigation of various factors including data amounts, data composition ratio, model parameters, and SFT strategies, which could potentially induce performance conflicts
  - Why unresolved: The paper does not explicitly state the key factors that induce performance conflicts in SFT
  - What evidence would resolve it: Detailed analysis and experimental results identifying the specific factors that contribute to performance conflicts in SFT would resolve this question

## Limitations

- The underlying reasons for differential scaling patterns across abilities remain unclear
- The generalizability of findings is limited to three specific model sizes and benchmarks
- The optimal parameters for the DMT strategy across different scenarios are not fully characterized

## Confidence

**High Confidence**:
- Different abilities scale differently with data amount
- Larger models generally show superior performance across ability domains
- Sequential learning causes catastrophic forgetting

**Medium Confidence**:
- Data composition improves abilities under low-resource conditions
- Data composition causes performance conflicts under high-resource conditions
- DMT effectively mitigates both forgetting and conflicts

**Low Confidence**:
- The exact mechanisms causing ability-specific scaling patterns
- The precise conditions under which data composition becomes harmful
- The optimal parameters for the DMT strategy across different scenarios

## Next Checks

1. **Scaling Law Validation**: Test the data composition effects on additional model sizes (e.g., 1B and 65B) to verify whether the observed scaling patterns hold across a wider range of model capacities and to better understand the relationship between model size and data composition effects.

2. **Distribution Analysis**: Conduct ablation studies to determine the optimal data distribution for each ability type. This would involve testing various composition ratios systematically across different data volumes to quantify when and how interference begins, providing clearer guidelines for when mixed data is beneficial versus harmful.

3. **Mechanism Probing**: Design experiments to investigate the internal representations of different abilities during mixed fine-tuning. This could involve analyzing attention patterns, probing classifier accuracy on ability-specific features, and testing whether abilities can be independently modified after joint training, which would help validate the proposed mechanisms for ability-specific scaling and interference.