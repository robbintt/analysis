---
ver: rpa2
title: Learning to Stabilize Online Reinforcement Learning in Unbounded State Spaces
arxiv_id: '2306.01896'
source_url: https://arxiv.org/abs/2306.01896
tags:
- state
- learning
- agent
- stability
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies the challenge of learning in unbounded state
  spaces under the continuing task setting (no resets) and high stochasticity. It
  demonstrates that existing RL algorithms often fail to learn stable policies in
  such settings due to poor credit-assignment for destabilizing actions.
---

# Learning to Stabilize Online Reinforcement Learning in Unbounded State Spaces

## Quick Facts
- arXiv ID: 2306.01896
- Source URL: https://arxiv.org/abs/2306.01896
- Reference count: 40
- Primary result: STOP improves PPO and TRPO performance in unbounded state spaces by combining Lyapunov-inspired reward shaping, reverse annealing, and state transformations

## Executive Summary
This paper addresses the challenge of learning stable policies in unbounded state spaces under continuing task settings (no resets) with high stochasticity. Existing RL algorithms often fail due to poor credit-assignment for destabilizing actions, causing divergence in unseen states. The proposed STOP method introduces three key innovations: Lyapunov-inspired reward shaping that provides bounded gradient signals, a reverse annealing scheme that prioritizes stability before optimality, and state transformations that reduce extrapolation burden. Empirically, STOP significantly improves performance on real-world-inspired domains like queueing networks and traffic control, consistently achieving lower costs compared to baselines.

## Method Summary
STOP is a wrapper method that enhances any RL algorithm for unbounded state spaces by combining three components: (1) stability reward computation using bounded cost differences (c(st) - c(st-1)) instead of raw costs, (2) a reverse annealing controller that gradually introduces optimality through a weight schedule λ(τ) = tanh(β max(τ - τwarmup, 0.01)), and (3) state preprocessing using symmetric log and square-root transformations to reduce divergence. The method operates online without resets, making it suitable for continuing tasks where traditional episodic RL fails due to extrapolation problems in unbounded domains.

## Key Results
- STOP significantly improves PPO and TRPO performance on queueing networks and traffic control domains
- The method achieves lower cost compared to baselines across all tested environments
- Ablation studies demonstrate the importance of each STOP component, with stability reward shaping being most critical for preventing divergence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stability cost shaping stabilizes learning by providing bounded gradient signals even when the agent is unstable.
- Mechanism: Replaces unbounded optimality cost with bounded difference c(st) - c(st-1), ensuring well-defined policy gradients at all times.
- Core assumption: Environment dynamics satisfy bounded cost increments (Assumption 2).
- Evidence anchors: Abstract mentions "Lyapunov-inspired reward shaping approach that encourages stability" and section states "g(st−1, st) is always bounded" (Lemma 1).
- Break condition: If cost increments are unbounded, gradient signals become unreliable and divergence resumes.

### Mechanism 2
- Claim: Reverse annealing λ(τ) prioritizes stability before optimality, preventing premature instability during learning.
- Mechanism: Coefficient λ(τ) grows from near zero to one via tanh(β max(τ - τwarmup, 0.01)), gradually shifting from stability-only to optimality-inclusive objectives.
- Core assumption: Stability can be learned independently and will persist when optimality is introduced.
- Evidence anchors: Abstract mentions "weight annealing scheme that gradually introduces optimality" and section describes reverse annealing as function of time τ.
- Break condition: If τwarmup is too short or β too large, agent may be pushed toward optimality before achieving stability, causing divergence.

### Mechanism 3
- Claim: State transformations reduce extrapolation burden and keep unbounded states numerically closer, improving generalization.
- Mechanism: Symmetric log and square-root transforms map large-magnitude states to smaller ranges, mitigating poor neural network extrapolation.
- Core assumption: Transformed state preserves relative ordering and essential structure needed for the task.
- Evidence anchors: Abstract mentions "state transformations to the unbounded state space" and section describes transformations that reduce divergence while preserving ordering.
- Break condition: If transformation is too aggressive (e.g., symlog10), states become indistinguishable and policy performance degrades.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formalism and average-cost objective
  - Why needed here: Problem framed as continuing task RL with infinite horizon, requiring average-cost optimality understanding
  - Quick check question: What distinguishes the average-cost objective J_O(π) from the discounted return objective in continuing tasks?

- Concept: Lyapunov functions and stability theory
  - Why needed here: Stability cost shaping relies on drift minimization via Lyapunov-inspired cost
  - Quick check question: How does a negative Lyapunov drift guarantee stochastic stability in queueing networks?

- Concept: Neural network extrapolation limits
  - Why needed here: Unbounded state space forces extrapolation; understanding why neural nets extrapolate poorly explains STOP necessity
  - Quick check question: What happens when a neural network receives input far outside its training distribution in terms of output reliability?

## Architecture Onboarding

- Component map: State → Transform → Policy/value net → Action → Environment → Cost → Compute stability reward → Update λ(τ) → Anneal objective → Gradient step
- Critical path: Input state flows through transformation, policy network, environment interaction, cost computation, stability reward calculation, annealing update, and gradient step
- Design tradeoffs: Stronger state transforms reduce divergence but may lose discriminative power; slower annealing delays optimality but improves stability; bounded cost shaping simplifies gradients but may slow final convergence
- Failure signatures: Persistent divergence (costs blow up), high variance in cost across runs, or policy stuck in suboptimal stable region
- First 3 experiments:
  1. Run PPO on infinite gridworld without STOP to observe divergence baseline
  2. Add STOP with only stability cost (λ=0) to confirm bounded cost is achieved
  3. Enable annealing with small β to test gradual optimality introduction while monitoring stability

## Open Questions the Paper Calls Out

- Open Question 1: How can neural networks be improved to better extrapolate in unbounded state spaces?
  - Basis in paper: [inferred] The paper highlights that neural networks are poor at extrapolation in unbounded state spaces, leading to poor performance in continuing tasks
  - Why unresolved: The paper proposes techniques to mitigate extrapolation issues but acknowledges that neural networks remain fundamentally poor at extrapolation in such environments
  - What evidence would resolve it: Development and empirical validation of neural network architectures or training methods that demonstrate improved extrapolation performance in unbounded state spaces

- Open Question 2: How should exploration be effectively conducted in continuing tasks with unbounded state spaces and high stochasticity, especially for DQN?
  - Basis in paper: [inferred] The paper notes that exploration is particularly challenging in continuing tasks with unbounded state spaces and high stochasticity, and that DQN faces difficulties in such settings
  - Why unresolved: The paper suggests that exploration is expensive and risky in these environments, but does not provide a definitive solution for effective exploration
  - What evidence would resolve it: Empirical results showing improved exploration strategies for DQN or other RL algorithms in continuing tasks with unbounded state spaces and high stochasticity

- Open Question 3: What is the optimal balance between stability and optimality in the reverse annealing process of STOP?
  - Basis in paper: [explicit] The paper discusses the reverse annealing scheme in STOP, which gradually introduces optimality after stability is achieved, but does not provide a definitive answer on the optimal balance
  - Why unresolved: The paper shows that different values of τwarmup and β affect performance, but does not determine the optimal values for all scenarios
  - What evidence would resolve it: Systematic experiments determining the optimal values of τwarmup and β for various environments and tasks, demonstrating the best balance between stability and optimality

## Limitations
- Effectiveness primarily demonstrated in queueing and traffic control domains, limiting generalizability to other unbounded state spaces
- Reliance on bounded cost increments (Assumption 2), which may not hold in all practical unbounded domains
- Potential computational overhead from state transformations and sensitivity to hyperparameter choices for τwarmup and β

## Confidence
- High confidence in the core mechanism of Lyapunov-inspired reward shaping for bounded gradient signals
- Medium confidence in the effectiveness of reverse annealing schedule for stability-before-optimality tradeoff
- Medium confidence in state transformation benefits, though sensitivity to transformation choice is unclear
- Low confidence in generalizability beyond queueing/traffic domains without further testing

## Next Checks
1. Test STOP on additional unbounded state space domains (e.g., robotic control with position/velocity limits, financial portfolio optimization) to verify generalizability beyond queueing and traffic control.

2. Conduct ablation studies varying the annealing schedule parameters (τwarmup, β) systematically across multiple environments to identify optimal schedules and failure modes.

3. Evaluate the method's performance when Assumption 2 is violated (unbounded cost increments) to determine robustness boundaries and identify when stability guarantees break down.