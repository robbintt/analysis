---
ver: rpa2
title: 'Scope is all you need: Transforming LLMs for HPC Code'
arxiv_id: '2308.09440'
source_url: https://arxiv.org/abs/2308.09440
tags:
- code
- arxiv
- llms
- language
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a domain-specific language model approach for
  high-performance computing (HPC) code, introducing a novel tokenizer called Tokompiler
  that leverages knowledge of language primitives to generate language-oriented tokens.
  The method reduces the total amount of tokens compared to standard Byte-Pair Encoding
  (BPE), enabling smaller model sizes while maintaining or improving performance.
---

# Scope is all you need: Transforming LLMs for HPC Code

## Quick Facts
- arXiv ID: 2308.09440
- Source URL: https://arxiv.org/abs/2308.09440
- Authors: 
- Reference count: 40
- Key outcome: Novel tokenizer Tokompiler reduces perplexity in HPC code models while decreasing vocabulary size and training time

## Executive Summary
This paper introduces Tokompiler, a domain-specific tokenizer for high-performance computing (HPC) code that leverages knowledge of language primitives to generate language-oriented tokens. By parsing code into abstract syntax trees (ASTs) and anonymizing variable names and literals, Tokompiler achieves better code structure understanding compared to standard Byte-Pair Encoding (BPE) while reducing the total number of tokens. The approach enables smaller model sizes without sacrificing performance, with experimental results showing a normalized perplexity score of approximately 1 and 10% faster training times on a Fortran code corpus.

## Method Summary
Tokompiler tokenizes HPC code by first anonymizing variable names, numbers, and strings, then parsing the anonymized code into an AST. The AST is updated to reflect anonymization changes, and the code is converted back from the AST while splitting multi-part tokens. This process ensures all tokens are meaningful for code-language comprehension rather than human semantics. The method was evaluated by pre-training SPT-Code and Polycoder models on a Fortran code corpus from GitHub, comparing performance against original tokenizers (BPE for Polycoder, NLTK for SPT-Code) using normalized perplexity and training time metrics.

## Key Results
- Normalized perplexity score of approximately 1, indicating performance on par with or better than baseline tokenizers
- Training speed improved by an average of 10% due to smaller vocabulary and tokenized corpus size
- Embedding layer size reduced, enabling smaller model sizes (88M parameters for Polycoder) while maintaining other model size parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tokompiler reduces perplexity by leveraging language primitives to generate language-oriented tokens
- Mechanism: The tokenizer creates anonymized code by replacing variable names, numbers, and strings, then parses this into an AST, updates the AST to reflect anonymization changes, and converts it back to code while splitting multi-part tokens. This process ensures all tokens are meaningful for code-language comprehension rather than human semantics.
- Core assumption: Code structure and language primitives contain sufficient information for model understanding without requiring human-readable variable names and semantics
- Evidence anchors:
  - [abstract] "Tokompiler leverages knowledge of language primitives to generate language-oriented tokens, providing a context-aware understanding of code structure while avoiding human semantics attributed to code structures completely."
  - [section] "We hypothesize that Tokompiler enables language models to better understand code structure and context without memorizing specific misleading human semantics"
- Break condition: If code comprehension requires specific variable names or human semantics for accurate understanding, or if the AST representation loses critical information needed for downstream tasks

### Mechanism 2
- Claim: Smaller vocabulary size reduces computational overhead while maintaining or improving model performance
- Mechanism: By using language-oriented tokens instead of standard BPE tokenization, Tokompiler reduces the total amount of tokens compared to standard Byte-Pair Encoding, enabling smaller model sizes while maintaining or improving performance
- Core assumption: The relationship between vocabulary size and model performance is non-linear, allowing significant vocabulary reduction without proportional performance degradation
- Evidence anchors:
  - [abstract] "The method reduces the total amount of tokens compared to standard Byte-Pair Encoding (BPE), enabling smaller model sizes while maintaining or improving performance"
  - [section] "Integrating Tokompiler into Polycoder reduces the embedding layer size, so the model size becomes 88M even while maintaining the other model size parameters"
- Break condition: If the reduction in vocabulary size causes loss of critical information needed for accurate code understanding, or if the performance gains are not proportional to the reduction in computational overhead

### Mechanism 3
- Claim: Training speed improves due to smaller vocabulary and tokenized corpus size
- Mechanism: The reduced vocabulary size and tokenized corpus lead to faster training times, as evidenced by the 10% average improvement in training speed
- Core assumption: The relationship between vocabulary size and training speed is linear, and the reduction in vocabulary size directly translates to faster training times
- Evidence anchors:
  - [abstract] "Training speed also improved by an average of 10% due to the smaller vocabulary and tokenized corpus size"
  - [section] "Time-to-train (mins) 435 390 186 150" (showing reduced training times)
- Break condition: If the reduction in training speed is not proportional to the reduction in vocabulary size, or if the smaller vocabulary causes loss of critical information needed for accurate code understanding

## Foundational Learning

- Concept: Byte-Pair Encoding (BPE)
  - Why needed here: Understanding BPE is crucial as it's the baseline tokenization method being compared against
  - Quick check question: How does BPE tokenization work, and what are its limitations for code processing?

- Concept: Abstract Syntax Tree (AST)
  - Why needed here: AST is central to Tokompiler's approach for understanding code structure
  - Quick check question: What information does an AST contain, and how can it be used to improve code tokenization?

- Concept: Perplexity in language models
  - Why needed here: Perplexity is the primary metric used to evaluate model performance in this paper
  - Quick check question: What does perplexity measure, and why is lower perplexity better for language models?

## Architecture Onboarding

- Component map: Fortran code corpus -> Tokompiler tokenizer -> SPT-Code/Polycoder models -> Perplexity evaluation -> Normalized perplexity scores and training time comparisons

- Critical path: 1. Code preprocessing (anonymization and AST generation) 2. Tokenization using Tokompiler 3. Model training with reduced vocabulary 4. Evaluation using perplexity tests 5. Performance comparison with baseline tokenizers

- Design tradeoffs: Smaller vocabulary vs. potential loss of information; reduced training time vs. potential impact on model quality; domain-specific optimization vs. generalizability to other programming languages

- Failure signatures: Poor performance on code completion tasks despite low perplexity; inability to handle code patterns not present in the training corpus; degradation in performance when applied to non-Fortran languages

- First 3 experiments: 1. Compare perplexity scores of Tokompiler vs. BPE on a small Fortran code sample 2. Measure training time reduction when using Tokompiler with a subset of the corpus 3. Test code completion accuracy on a held-out validation set using both tokenizers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Tokompiler's performance scale when applied to other programming languages beyond Fortran, such as C++ or Python?
- Basis in paper: [inferred] The paper focuses on evaluating Tokompiler on Fortran code, but the authors mention future work plans to apply it to C and C++ corpora. This suggests that the performance on these languages is not yet established.
- Why unresolved: The paper does not provide experimental results for languages other than Fortran, and the authors explicitly state their intention to explore this in future work.
- What evidence would resolve it: Conducting experiments to evaluate Tokompiler's performance on C++ and Python code corpora, comparing the results to those obtained for Fortran, would provide insights into its scalability across different programming languages.

### Open Question 2
- Question: What are the specific advantages and disadvantages of Tokompiler compared to other state-of-the-art tokenizers for code, such as those used in CodeBERT or CodeT5?
- Basis in paper: [explicit] The paper compares Tokompiler's performance against traditional tokenizers like BPE and NLTK, but does not directly compare it to other modern code-specific tokenizers.
- Why unresolved: The paper does not provide a detailed comparison with other state-of-the-art tokenizers, focusing instead on demonstrating improvements over traditional methods.
- What evidence would resolve it: Conducting a comprehensive comparison of Tokompiler with other modern code tokenizers, evaluating metrics such as perplexity, training speed, and model size, would highlight its relative strengths and weaknesses.

### Open Question 3
- Question: How does the removal of human semantics in Tokompiler affect the model's ability to understand and generate code that involves complex human-readable comments or documentation?
- Basis in paper: [inferred] Tokompiler replaces variable names, numbers, and strings with anonymized tokens, which could potentially impact the model's understanding of code with extensive comments or documentation.
- Why unresolved: The paper does not address how Tokompiler handles code with complex human-readable elements, focusing instead on its ability to understand code structure and primitives.
- What evidence would resolve it: Experimenting with Tokompiler on code samples that include extensive comments or documentation, and evaluating the model's performance on tasks such as code summarization or documentation generation, would provide insights into its handling of human-readable elements.

## Limitations

- Experimental validation relies heavily on perplexity scores, which may not directly correlate with practical code understanding or completion quality
- Corpus selection focuses on Fortran code from GitHub, potentially limiting generalizability to other HPC languages and coding practices
- Implementation details of Tokompiler remain underspecified, making independent verification difficult

## Confidence

**High Confidence**: The Tokompiler tokenizer successfully reduces vocabulary size compared to BPE; training speed improvements of approximately 10% are reproducible; the method works as described for Fortran code processing

**Medium Confidence**: Tokompiler improves semantic understanding of code structure (supported by perplexity metrics but not comprehensive downstream task evaluation); the approach generalizes to other HPC languages (limited to Fortran evaluation only)

**Low Confidence**: The method would scale effectively to significantly larger codebases without performance degradation; the approach would maintain benefits when applied to non-Fortran HPC languages; the reduction in vocabulary size does not compromise critical semantic information (not directly tested)

## Next Checks

1. **Downstream Task Validation**: Implement a comprehensive code completion benchmark using the pre-trained models with both Tokompiler and BPE tokenizers, measuring actual completion accuracy, latency, and user satisfaction metrics rather than relying solely on perplexity scores.

2. **Cross-Language Generalization**: Apply the Tokompiler approach to at least two additional HPC-relevant languages (such as C++ or CUDA) and evaluate whether the same vocabulary reduction and performance improvements are observed, including testing on language-specific code patterns and idioms.

3. **Information Preservation Analysis**: Design experiments to systematically test whether critical semantic information is preserved during the anonymization and AST-based tokenization process, particularly focusing on edge cases where variable names or specific code patterns might carry important semantic meaning that could be lost.