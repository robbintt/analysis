---
ver: rpa2
title: 'UQ at #SMM4H 2023: ALEX for Public Health Analysis with Social Media'
arxiv_id: '2309.04213'
source_url: https://arxiv.org/abs/2309.04213
tags:
- task
- label
- llms
- bert
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenges of public health text classification
  on social media, specifically imbalanced datasets and the high computational cost
  of training large language models (LLMs) in-domain. The proposed ALEX framework
  combines data augmentation and weighted loss fine-tuning to address data imbalance,
  and uses an LLM (GPT-3.5) to explain and correct predictions from a BERT model.
---

# UQ at #SMM4H 2023: ALEX for Public Health Analysis with Social Media

## Quick Facts
- arXiv ID: 2309.04213
- Source URL: https://arxiv.org/abs/2309.04213
- Reference count: 8
- This paper addresses public health text classification challenges on social media using an LLM-enhanced framework that achieves top F1 scores on SMM4H 2023 tasks.

## Executive Summary
This paper presents ALEX, a framework for public health text classification on social media that addresses two key challenges: imbalanced datasets and the high computational cost of fine-tuning large language models. The approach combines data augmentation and weighted loss fine-tuning to handle class imbalance, while leveraging GPT-3.5 to explain and correct predictions from a BERT model without requiring additional model training. The LLM is prompted with labeling rules and examples to verify BERT's predictions and provide corrections when necessary.

Experiments on three SMM4H 2023 tasks demonstrate that ALEX achieves the highest F1 scores among all submissions in Task 2 (77.84%) and Task 4 (87.15%), and a high score in Task 1 (94.97%). The method effectively improves performance by leveraging LLM reasoning capabilities for post-hoc correction, avoiding the need for expensive in-domain LLM training while still benefiting from LLM intelligence.

## Method Summary
The ALEX framework addresses imbalanced public health datasets through a multi-stage approach. First, it uses TextAttack for data augmentation to enrich minority class data, combined with oversampling/undersampling and weighted loss fine-tuning to create a balanced training dataset. A BERT model is then trained on this balanced data. For inference, BERT generates initial predictions, which are then passed to GPT-3.5 along with the original text, predicted labels, and manually created labeling rules and examples. The LLM explains whether the prediction aligns with the evidence and can correct the label if needed. This post-hoc correction leverages LLM reasoning without requiring expensive fine-tuning.

## Key Results
- ALEX achieved highest F1 scores among all submissions: 77.84% for Task 2 and 87.15% for Task 4
- ALEX achieved 94.97% F1 score for Task 1, demonstrating strong performance across all three tasks
- t-SNE visualizations showed ALEX's balanced training method improved inter-class separation compared to baseline imbalanced training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LLM explanation and correction mechanism improves BERT's classification accuracy by verifying and potentially correcting predictions based on labeling rules and examples.
- Mechanism: After BERT predicts labels, the original text and predicted label are combined with labeling rules and examples to form a prompt for the LLM. The LLM then verifies if the label aligns with the text evidence and provides explanations. If the LLM identifies a mismatch, it can correct the label.
- Core assumption: The LLM can accurately identify and explain misclassifications by leveraging its reasoning capabilities and understanding of the labeling rules.
- Evidence anchors:
  - [abstract] "The proposed ALEX framework combines data augmentation and weighted loss fine-tuning to address data imbalance, and uses an LLM (GPT-3.5) to explain and correct predictions from a BERT model."
  - [section] "To overcome the limited input size of BERT and enhance its performance, LLMs will be introduced... The LLMs identified the label as False and it will be corrected."
- Break condition: The LLM's explanations are unreliable or the labeling rules are ambiguous, leading to incorrect corrections.

### Mechanism 2
- Claim: Data augmentation and weighted loss fine-tuning effectively address the imbalanced dataset problem in public health text classification on social media.
- Mechanism: TextAttack is used to augment minority class data, and oversampling/undersampling is applied to balance the dataset. Weighted loss is incorporated into the binary cross-entropy loss to emphasize specific labels during training.
- Core assumption: Balancing the dataset and emphasizing important labels during training will lead to better model performance on imbalanced public health datasets.
- Evidence anchors:
  - [abstract] "The proposed ALEX framework combines data augmentation and weighted loss fine-tuning to address data imbalance..."
  - [section] "To solve the imbalanced problem, the TextAttack [3] is chosen to enrich the number of the minority class... In order to ensure the model assigns more emphasis to those specific labels during the training process, the loss weight Î» will be introduced."
- Break condition: The augmented data is of low quality or the weighted loss is not properly tuned, resulting in suboptimal model performance.

### Mechanism 3
- Claim: The balanced training pipeline with augmentation and weighted loss improves inter-class distance and separation in the embedding space compared to baseline methods.
- Mechanism: After applying data augmentation and weighted loss fine-tuning, the [CLS] embeddings are visualized using t-SNE. The balanced training method shows larger inter-class distances and better class separation compared to the baseline imbalanced training method.
- Core assumption: Improved inter-class distance and separation in the embedding space will lead to better model performance on the classification task.
- Evidence anchors:
  - [section] "As shown in Table 1, for Task 1, the original BERT models may outperform large models such as XLNet, which may owing to the serious impact of the imbalanced datasets... It is noticeable to see that BERTweet-Large model performances are generally better than others on the F1 score for Label 1 in Task 2 and Task 4."
  - [section] "For Task 2 and Task 4, the t-SNE visualizations of the [CLS] embeddings after the baselines' imbalanced training method (left-hand side) compared to our balanced training method (right-hand side) are shown in Figure 3. It is obvious that our method can enlarge the inter-class distance between different labels and separate different classes better than baselines."
- Break condition: The t-SNE visualization does not show significant improvement in inter-class distance and separation, or the model performance does not improve despite the improved embedding space.

## Foundational Learning

- Concept: Imbalanced datasets and their impact on model performance
  - Why needed here: Public health datasets from social media are generally imbalanced, which can lead to biased predictions and suboptimal model performance.
  - Quick check question: What are the potential consequences of training a model on an imbalanced dataset, and how can this issue be addressed?

- Concept: Large language models (LLMs) and their reasoning capabilities
  - Why needed here: The ALEX framework leverages the reasoning capabilities of LLMs (specifically GPT-3.5) to explain and correct BERT's predictions, enhancing the overall model performance.
  - Quick check question: How can LLMs be effectively prompted to leverage their reasoning capabilities for post-hoc prediction correction in a classification task?

- Concept: Data augmentation techniques and their impact on model performance
  - Why needed here: TextAttack is used to augment minority class data, which helps address the imbalanced dataset problem and improve model performance on underrepresented classes.
  - Quick check question: What are some common data augmentation techniques used in natural language processing, and how can they help mitigate the impact of imbalanced datasets on model performance?

## Architecture Onboarding

- Component map:
  BERT model -> GPT-3.5 LLM -> Final corrected labels
  Data augmentation (TextAttack) -> Weighted loss fine-tuning -> Balanced training
  Labeling rules and examples -> Prompt construction -> LLM input

- Critical path:
  1. Apply data augmentation and weighted loss fine-tuning to train BERT on the balanced dataset
  2. Use the trained BERT model to predict labels for the input social media posts
  3. Construct prompts by combining the original text, predicted labels, labeling rules, and examples
  4. Send prompts to the LLM (GPT-3.5) for explanation and potential correction of the predicted labels
  5. Obtain the final corrected labels from the LLM and use them for downstream tasks or evaluation

- Design tradeoffs:
  - Using LLMs for post-hoc correction vs. fine-tuning LLMs on the domain-specific data: The ALEX framework uses LLMs for post-hoc correction to avoid the high computational cost of training LLMs in-domain, but this may limit the LLM's ability to fully leverage its reasoning capabilities compared to fine-tuning.
  - Data augmentation vs. collecting more real-world data: Data augmentation helps address the imbalanced dataset problem, but it may introduce noise or artifacts that could impact model performance. Collecting more real-world data would be ideal but may not always be feasible.

- Failure signatures:
  - LLM explanations are unreliable or inconsistent, leading to incorrect label corrections
  - Data augmentation introduces significant noise or artifacts, negatively impacting model performance
  - Weighted loss is not properly tuned, causing the model to overfit to specific classes or underperform on underrepresented classes

- First 3 experiments:
  1. Compare the performance of BERT with and without the LLM explanation and correction mechanism on a balanced dataset to isolate the impact of the LLM component.
  2. Evaluate the effectiveness of different data augmentation techniques (e.g., TextAttack, back-translation, synonym replacement) on the imbalanced dataset and their impact on model performance.
  3. Conduct an ablation study to assess the individual contributions of data augmentation, weighted loss fine-tuning, and the LLM explanation and correction mechanism to the overall model performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of ALEX vary when using different large language models (LLMs) such as GPT-4 or open-source alternatives?
- Basis in paper: [explicit] The paper uses GPT-3.5 for the LLM explanation and correction mechanism, but does not explore other LLM options.
- Why unresolved: The paper does not compare the performance of ALEX with other LLMs or open-source alternatives, leaving the generalizability of the approach unclear.
- What evidence would resolve it: Conducting experiments with different LLMs (e.g., GPT-4, LLaMA, BLOOM) and comparing their performance in the ALEX framework would provide insights into the robustness and generalizability of the approach.

### Open Question 2
- Question: What is the impact of the quality and quantity of manually created labeling rules and examples on the performance of the LLM explanation mechanism?
- Basis in paper: [explicit] The paper mentions that labeling rules and examples are manually created and used in the LLM prompt, but does not explore the effect of their quality or quantity on the model's performance.
- Why unresolved: The paper does not investigate how variations in the labeling rules and examples affect the LLM's ability to explain and correct predictions, which could impact the overall effectiveness of the ALEX framework.
- What evidence would resolve it: Conducting experiments with different sets of labeling rules and examples, varying in quality and quantity, and measuring their impact on the LLM's performance would provide insights into the importance of these components.

### Open Question 3
- Question: How does the ALEX framework perform on other public health text classification tasks beyond the SMM4H 2023 tasks?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of ALEX on three SMM4H 2023 tasks but does not explore its performance on other public health text classification datasets.
- Why unresolved: The paper's results are limited to the SMM4H 2023 tasks, and it is unclear whether the ALEX framework generalizes well to other public health text classification problems.
- What evidence would resolve it: Applying the ALEX framework to other public health text classification datasets (e.g., drug reviews, medical literature, clinical notes) and comparing its performance with baseline methods would demonstrate its generalizability and effectiveness across different domains.

## Limitations
- The approach relies heavily on GPT-3.5 API availability and cost, which may limit scalability for large-scale deployment
- The paper provides limited ablation studies to isolate the individual contributions of each component, making it difficult to assess which mechanism drives the most performance gains
- The effectiveness of the LLM correction mechanism depends on the quality and clarity of the labeling rules and examples provided in the prompts, which are not fully specified in the paper

## Confidence
- High confidence in the overall framework design and its ability to address imbalanced datasets through combined augmentation and weighted loss techniques
- Medium confidence in the LLM explanation and correction mechanism's effectiveness, as it shows strong results but lacks detailed ablation studies
- Medium confidence in the generalizability of results to other public health domains beyond the three specific SMM4H tasks

## Next Checks
1. Conduct ablation studies to quantify the individual contributions of data augmentation, weighted loss fine-tuning, and LLM correction to overall performance
2. Test the framework on additional public health text classification datasets with varying degrees of class imbalance to assess generalizability
3. Evaluate the impact of different labeling rule formulations and example selections on the reliability of LLM corrections through controlled experiments