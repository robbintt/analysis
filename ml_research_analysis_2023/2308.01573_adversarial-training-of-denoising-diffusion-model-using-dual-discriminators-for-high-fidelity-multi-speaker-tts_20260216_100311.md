---
ver: rpa2
title: Adversarial Training of Denoising Diffusion Model Using Dual Discriminators
  for High-Fidelity Multi-Speaker TTS
arxiv_id: '2308.01573'
source_url: https://arxiv.org/abs/2308.01573
tags:
- speech
- diffusion
- discriminator
- proposed
- spectrogram
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a high-fidelity multi-speaker TTS model called
  SpecDiff-GAN, which combines a diffusion model and GAN. The model features a dual
  discriminator structure, comprising a diffusion discriminator and a spectrogram
  discriminator.
---

# Adversarial Training of Denoising Diffusion Model Using Dual Discriminators for High-Fidelity Multi-Speaker TTS

## Quick Facts
- arXiv ID: 2308.01573
- Source URL: https://arxiv.org/abs/2308.01573
- Reference count: 40
- Primary result: Proposed SpecDiff-GAN model outperforms FastSpeech2 and DiffGAN-TTS on multi-speaker TTS with dual discriminator architecture

## Executive Summary
This paper introduces SpecDiff-GAN, a high-fidelity multi-speaker TTS model that combines a diffusion model with GAN using dual discriminators. The model employs a diffusion discriminator for modeling the reverse process distribution and a spectrogram discriminator for learning speaker-specific voice characteristics. Through extensive objective and subjective evaluations on the VCTK corpus, SpecDiff-GAN demonstrates superior performance compared to baseline models, with the ablation study confirming the effectiveness of the dual discriminator approach.

## Method Summary
The SpecDiff-GAN architecture consists of a generator based on FastSpeech2 with 4 transformer encoders, a variance adaptor with duration/pitch/energy predictors, and a diffusion decoder with 20 residual blocks. The model uses two discriminators: a diffusion discriminator with progressive growing and time-step conditioning, and a spectrogram discriminator for speaker-specific features. Training employs Adam optimizer for 300k steps with LSGAN losses, feature matching losses, and variance adaptor reconstruction losses. The model is evaluated on the VCTK corpus (44 hours, 110 speakers, downsampled to 22.05 kHz) using metrics including SSIM, MCD, F0 RMSE, STOI, PESQ, and RTF.

## Key Results
- SpecDiff-GAN outperforms FastSpeech2 and DiffGAN-TTS across all evaluation metrics
- Dual discriminator architecture shows significant performance improvements in ablation studies
- Model achieves real-time factor (RTF) of 0.024 on A100 GPU
- Subjective evaluations (SMOS, CMOS) confirm superior speech quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual discriminators allow the model to separately learn the diffusion reverse process distribution and speaker-specific voice characteristics, preventing interference between the two tasks.
- Mechanism: The diffusion discriminator focuses on modeling the noise distribution needed for denoising steps, while the spectrogram discriminator focuses on capturing high-level speaker traits in the mel-spectrogram.
- Core assumption: Speaker embeddings alone are insufficient for the diffusion discriminator to model both the multimodal noise distribution and speaker identity effectively.
- Evidence anchors:
  - [abstract] "diffusion discriminator for learning the distribution of the reverse process and a spectrogram discriminator for learning the distribution of the generated data"
  - [section III-D] "In this paper, the least squares GAN (LSGAN) loss function is employed to train two discriminators..."
  - [corpus] Weak/no direct support for dual-discriminator effect in related papers

### Mechanism 2
- Claim: The diffusion discriminator's progressive growing with time-step conditioning enables stable, multimodal noise distribution learning without mode collapse.
- Mechanism: By downsampling conditioned on the time step, the discriminator can focus on the local noise distribution at each denoising step.
- Core assumption: Progressive growing plus minibatch discrimination is sufficient to model complex multimodal distributions for each denoising step.
- Evidence anchors:
  - [section III-B] "The diffusion discriminator structure... is based on the discriminator structure of progressive growing of GANs (ProGAN) with a time-dependent modification..."
  - [section II] "To reduce the number of denoising steps, Xiao et al. [17] proposed modeling the denoising distribution using a complex multimodal distribution."

### Mechanism 3
- Claim: Feature matching losses on both discriminators encourage the generator to match real data distributions at multiple feature levels, improving stability and quality.
- Mechanism: â„’ð‘“ð‘š aggregates â„“1 distances across hidden layers of both discriminators, ensuring the generator's outputs align with real data in both denoising and speaker identity spaces.
- Core assumption: Matching feature distributions at multiple scales is more effective than matching only final outputs.
- Evidence anchors:
  - [section III-D] "The feature matching loss is employed to ensure that the distribution of the generated data matches the distribution of the real data..."
  - [section V-C] "The experimental results confirmed that both models without the spectrogram discriminator exhibited a decrease in performance..."

## Foundational Learning

- Concept: Diffusion probabilistic models
  - Why needed here: Understanding the forward and reverse processes, noise schedules, and the ELBO objective is essential for modifying the denoising steps and conditioning strategy.
  - Quick check question: What is the difference between the Markovian diffusion process and the non-Markovian accelerated sampling in DDIM?

- Concept: Generative adversarial networks
  - Why needed here: GAN loss functions, discriminator training, and the use of feature matching are central to the dual-discriminator design and training stability.
  - Quick check question: How does least squares GAN (LSGAN) loss differ from the standard GAN loss in terms of gradient behavior?

- Concept: Multi-speaker TTS conditioning
  - Why needed here: Speaker embeddings, variance adaptors, and prosody modeling are required to produce speaker-specific voices and control pitch/energy.
  - Quick check question: Why are speaker embeddings passed to both the generator and the spectrogram discriminator in this architecture?

## Architecture Onboarding

- Component map: Text -> phoneme embedding -> transformer encoders -> variance adaptor -> diffusion decoder -> mel-spectrogram -> vocoder (HiFi-GAN) -> waveform
- Critical path: Text â†’ phoneme embedding â†’ transformer encoders â†’ variance adaptor â†’ diffusion decoder â†’ mel-spectrogram â†’ vocoder (HiFi-GAN) â†’ waveform
- Design tradeoffs:
  - Dual discriminators vs single discriminator: better task separation vs higher computational cost
  - Progressive growing vs fixed-size discriminator: more stable training vs slower convergence
  - LSGAN vs other GAN losses: reduced gradient vanishing vs potentially slower convergence
- Failure signatures:
  - Mode collapse: spectrogram discriminator fails to capture speaker diversity
  - High F0 RMSE: diffusion discriminator underfits noise distribution, leading to pitch errors
  - Low STOI: spectrogram discriminator adds artifacts or reverberation
  - Slow RTF: diffusion decoder residual blocks or variance adaptor over-parameterized
- First 3 experiments:
  1. Train generator with only diffusion discriminator, compare MCD/F0 RMSE to full model
  2. Swap LSGAN loss for Wasserstein loss in both discriminators, measure stability and quality
  3. Vary diffusion time steps (4 vs 8) and measure RTF vs speech quality tradeoff

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text provided. The "Open Questions" section appears to be derived from analysis rather than directly from the paper itself.

## Limitations

- Limited scalability analysis: The paper only evaluates on 110 speakers from VCTK corpus without exploring performance on larger multi-speaker datasets
- Computational overhead not quantified: The dual-discriminator system's additional computational cost is not measured or discussed
- Language generalization untested: Only English speech data is evaluated, with no discussion of performance on other languages

## Confidence

- **High Confidence:** The overall architecture design and loss function formulations are well-specified and technically sound
- **Medium Confidence:** The claim that dual discriminators improve speaker-specific voice quality is supported by ablation results
- **Low Confidence:** The assertion that progressive growing with time-step conditioning specifically enables stable multimodal noise distribution learning lacks comparative evidence

## Next Checks

1. **Ablation of Discriminator Roles:** Train separate models with only the diffusion discriminator or only the spectrogram discriminator, then measure the individual contribution of each to SSIM, MCD, and F0 RMSE metrics.
2. **Progressive Growing Comparison:** Implement a fixed-size diffusion discriminator and compare training stability and final speech quality metrics against the progressive growing version.
3. **Computational Overhead Analysis:** Measure GPU memory usage and training time per step for the dual-discriminator system versus a single-discriminator baseline to quantify the performance-quality tradeoff.