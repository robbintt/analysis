---
ver: rpa2
title: 'The Gender-GAP Pipeline: A Gender-Aware Polyglot Pipeline for Gender Characterisation
  in 55 Languages'
arxiv_id: '2308.16871'
source_url: https://arxiv.org/abs/2308.16871
tags:
- gender
- languages
- language
- pipeline
- masculine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Gender-GAP Pipeline, a method to quantify
  gender representation in large multilingual datasets using lexical matching of gendered
  person-nouns across 55 languages. The pipeline builds a multilingual lexicon from
  English kinship and person nouns, translates them, and counts matches in segmented
  text.
---

# The Gender-GAP Pipeline: A Gender-Aware Polyglot Pipeline for Gender Characterisation in 55 Languages

## Quick Facts
- arXiv ID: 2308.16871
- Source URL: https://arxiv.org/abs/2308.16871
- Reference count: 15
- This paper introduces a method to quantify gender representation in multilingual datasets using lexical matching of gendered person-nouns across 55 languages.

## Executive Summary
This paper introduces the Gender-GAP Pipeline, a method to quantify gender representation in large multilingual datasets using lexical matching of gendered person-nouns across 55 languages. The pipeline builds a multilingual lexicon from English kinship and person nouns, translates them, and counts matches in segmented text. Applied to WMT training and evaluation data, it finds that on average all three analyzed datasets are biased toward masculine gender, with coverage of 10-13% of samples and significant language- and domain-specific variation in gender representation. The work recommends reporting gender distributions alongside performance scores to increase awareness of potential biases in multilingual NLP systems.

## Method Summary
The Gender-GAP Pipeline quantifies gender representation by building a multilingual lexicon from ~30 English kinship and person nouns, translating them into 55 languages, and assigning gender classes. It tokenizes input text using language-specific tools (primarily Stanza), counts occurrences of words matching each gender class, and computes proportions by dividing counts by total words. The method analyzes three datasets: Common Crawl (100k documents per language), FLORES-200 (3001 sentences from English to 200 languages), and NTREX-128 (1997 sentences from English to 128 languages). Gender distribution scores, coverage percentages, and gender gaps are reported for each language and dataset.

## Key Results
- All three analyzed datasets (Common Crawl, FLORES, NTREX) show average masculine bias
- Coverage of gender-representative samples is 10-13% across languages
- Gender representation varies significantly across domains and languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lexical matching of gendered person-nouns across 55 languages quantifies gender representation bias in multilingual datasets.
- Mechanism: The pipeline builds a multilingual lexicon from English kinship and person nouns, translates them, and counts matches in segmented text. This creates a proxy for gender representation by measuring the frequency of masculine, feminine, and unspecified gender terms.
- Core assumption: A restricted list of gendered nouns and kinship terms provides a universal proxy for gender representation across typologically diverse languages.
- Evidence anchors:
  - [abstract] "The pipeline uses a multilingual lexicon of gendered person-nouns to quantify the gender representation in text."
  - [section] "Our pipeline segments each input sentence at the word-level using Stanza... and counts the number of occurrences of words in each gender class."
  - [corpus] Found 25 related papers with average FMR=0.42, indicating moderate correlation with gender bias research literature.
- Break condition: The lexical matching approach fails when languages encode gender through morphological agreement on verbs or adjectives rather than through person nouns, or when cultural gender concepts don't align with the three-class system.

### Mechanism 2
- Claim: Gender representation varies significantly across domains and languages, with masculine gender typically overrepresented.
- Mechanism: By applying the pipeline to WMT training and evaluation data (FLORES, NTREX, Common Crawl), the method reveals domain-specific and language-specific gender biases through comparative analysis of gender-class scores.
- Core assumption: Domain differences (news vs. web crawl) and language typology (genderless vs. gendered languages) create measurable differences in gender representation that can be captured through lexical matching.
- Evidence anchors:
  - [abstract] "Applied to WMT training and evaluation data... it finds that on average all three analyzed datasets are biased toward masculine gender"
  - [section] "We find the gender representations to be domain- and language-specific"
  - [corpus] Related work includes "Evaluating Machine Translation Datasets for Low-Web Data Languages: A Gendered Lens" (FMR=0.5388), supporting domain-specific analysis.
- Break condition: If translation quality varies significantly across languages, the gender representation in parallel data may reflect translation bias rather than source text bias, confounding the analysis.

### Mechanism 3
- Claim: Reporting gender distributions alongside performance scores increases awareness of potential biases in multilingual NLP systems.
- Mechanism: The pipeline provides quantitative metrics (gender-class scores, coverage percentages) that can be integrated into standard evaluation reporting, making gender bias visible to practitioners and stakeholders.
- Core assumption: Quantitative gender representation metrics are actionable for bias mitigation and influence model development decisions.
- Evidence anchors:
  - [abstract] "We suggest introducing our gender quantification pipeline in current datasets and, ideally, modifying them toward a balanced representation."
  - [section] "Our primary recommendation for multilingual NLP practitioner is to report the gender distribution along with the performance score."
  - [corpus] Related work "DR.GAP: Mitigating Bias in Large Language Models using Gender-Aware Prompting" (FMR=0.5506) suggests practical mitigation approaches build on bias measurement.
- Break condition: If stakeholders lack understanding of gender bias implications or if performance metrics dominate decision-making, gender distribution reporting may be ignored despite availability.

## Foundational Learning

- Concept: Linguistic gender vs. social gender
  - Why needed here: The pipeline uses semantic gender (linguistic) as a proxy for social gender, requiring understanding of this distinction to interpret results correctly
  - Quick check question: What's the difference between grammatical gender (like in Russian) and semantic gender (like in English kinship terms)?

- Concept: Lexical matching methodology
  - Why needed here: The pipeline relies on word-level tokenization and matching, requiring understanding of how this differs from contextual or semantic analysis
  - Quick check question: Why might a context-free lexical matching approach overcount masculine gender in Vietnamese when "ba" means both "father" and "three"?

- Concept: Statistical significance in bias measurement
  - Why needed here: The analysis uses confidence intervals and standard errors to determine when gender gaps are meaningful, requiring statistical literacy
  - Quick check question: How does the standard error help determine whether a 5% gender gap is statistically significant across languages?

## Architecture Onboarding

- Component map: Lexicon creation -> Sentence segmentation -> Word matching -> Counting -> Score calculation -> Reporting
- Critical path: Lexicon creation → Sentence segmentation → Word matching → Counting → Score calculation → Reporting
- Design tradeoffs: 
  - Context-free vs. contextual matching (simplicity vs. accuracy)
  - Three-class vs. more granular gender categories (scalability vs. nuance)
  - Lexical matching vs. embedding-based methods (interpretability vs. sophistication)
- Failure signatures:
  - Low coverage (<5%) suggests lexicon incompleteness for that language
  - Unexpectedly balanced scores may indicate tokenization issues
  - High variance across domains suggests translation bias rather than source bias
- First 3 experiments:
  1. Run pipeline on a small English corpus with known gender distribution to verify basic functionality
  2. Test coverage on a language with agglutinative morphology (e.g., Finnish) to identify segmentation challenges
  3. Compare gender scores between parallel sentences in English and Spanish to detect translation-induced bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the gender representation metrics change if we expanded the lexical matching approach to include gendered occupation nouns and pronouns beyond the current focus on kinship and person terms?
- Basis in paper: [inferred] The paper mentions as a limitation that the lexical matching approach is restricted to person nouns and kinship relationships, and suggests future work could consider other types of nouns such as gendered occupation nouns and pronouns.
- Why unresolved: The paper's current methodology is limited to a specific set of gendered terms, and expanding to other categories would require building new lexicons and rerunning the analysis.
- What evidence would resolve it: New lexicons including occupation nouns and pronouns for the 55 languages, along with updated gender representation statistics showing how coverage and distribution change.

### Open Question 2
- Question: To what extent do translation choices in parallel datasets (like NTREX) contribute to gender representation disparities across languages, versus inherent domain differences or language-specific encoding of gender?
- Basis in paper: [explicit] The paper discusses examples showing translation choices can introduce gender biases (e.g., omitting gendered words in English while translating as masculine in Spanish/Catalan), and separates this from domain effects and language-specific encoding.
- Why unresolved: The paper provides qualitative examples but doesn't quantify the relative contribution of translation bias versus other factors across the full dataset.
- What evidence would resolve it: A systematic analysis comparing gender representation in original source texts versus translations across multiple language pairs, controlling for domain and language factors.

### Open Question 3
- Question: How would gender representation metrics differ if we used a non-binary inclusive gender model instead of the current three-class system (masculine, feminine, unspecified)?
- Basis in paper: [explicit] The paper acknowledges as a limitation that using a three-gender class lexicon restricts the approach to binary genders and only imperfectly measures non-binary gender distribution with the "unspecified" class.
- Why unresolved: The paper chose the three-class system for scalability across 55 languages, but doesn't explore how more granular gender categories would affect the results.
- What evidence would resolve it: A refined multilingual lexicon with more granular gender categories, along with comparative analysis showing how gender representation metrics change when using different gender classification systems.

## Limitations

- The three-class gender system may oversimplify complex gender expressions in languages with grammatical gender or cultures with non-binary gender concepts
- Context-free lexical matching cannot capture gender implications conveyed through verb agreement or adjective morphology
- Coverage rates of 10-13% indicate the method captures only a fraction of gender-related content

## Confidence

- **High confidence**: The finding that all three datasets show masculine bias on average is well-supported by the methodology and consistent with prior literature on gender bias in NLP.
- **Medium confidence**: Language- and domain-specific variation in gender representation is demonstrated, but the extent and causes of this variation require further investigation to distinguish between source text characteristics and translation artifacts.
- **Low confidence**: The claim that reporting gender distributions will meaningfully increase awareness and influence model development decisions lacks empirical validation in the paper.

## Next Checks

1. **Coverage validation**: Run the pipeline on a manually annotated sample of 100 sentences from Common Crawl where gender references have been exhaustively identified through comprehensive annotation. Compare the pipeline's coverage and gender-class assignments against human annotations to establish ground truth accuracy and identify systematic gaps in the lexicon.

2. **Translation bias assessment**: Apply the pipeline to both source English texts and their translations in parallel corpora (e.g., WMT data). Compare gender distributions between source and target languages to quantify how much observed gender bias reflects translation choices versus source text characteristics. Focus on language pairs with known translation challenges for gender (e.g., genderless to gendered languages).

3. **Cross-linguistic generalizability test**: Apply the pipeline to a controlled multilingual corpus with balanced gender representation (e.g., Wikipedia biographies of equal numbers of men and women across multiple languages). Verify that the method correctly identifies balanced gender representation and that any deviations from balance can be explained by linguistic factors rather than methodological limitations.