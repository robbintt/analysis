---
ver: rpa2
title: $K$-Nearest-Neighbor Resampling for Off-Policy Evaluation in Stochastic Control
arxiv_id: '2306.04836'
source_url: https://arxiv.org/abs/2306.04836
tags:
- where
- algorithm
- data
- policy
- neighbor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces K-nearest neighbor resampling (KNNR) for off-policy
  evaluation (OPE) in stochastic control environments. KNNR estimates policy performance
  from historical data by simulating trajectories using K-nearest neighbor searches,
  without requiring knowledge of the behavior policy or parametric dynamics models.
---

# $K$-Nearest-Neighbor Resampling for Off-Policy Evaluation in Stochastic Control

## Quick Facts
- arXiv ID: 2306.04836
- Source URL: https://arxiv.org/abs/2306.04836
- Reference count: 40
- Primary result: KNNR provides consistent off-policy evaluation in stochastic control with vanishing MSE and runtime advantages over model-free Monte Carlo methods.

## Executive Summary
This paper introduces K-nearest neighbor resampling (KNNR) for off-policy evaluation in stochastic control environments with continuous state-action spaces. KNNR estimates policy performance by simulating trajectories using K-nearest neighbor searches on historical data, without requiring knowledge of the behavior policy or parametric dynamics models. The method is particularly suited for stochastic settings where continuity assumptions hold. Theoretical results show consistency under episodic sampling, generalizing Stone's theorem to include counterfactual estimation. Experiments demonstrate competitive performance against baselines across linear quadratic regulator, limit order book execution, and online stochastic bin packing environments.

## Method Summary
KNNR estimates the value function of a target policy by resampling trajectories from historical data using K-nearest neighbor searches. The algorithm builds a tree-based index of historical state-action-reward transitions, then simulates trajectories by finding K nearest neighbors for each state-action pair and sampling from their transitions. The method leverages continuity assumptions to ensure similar state-action pairs yield similar outcomes, and parallelizes trajectory generation for computational efficiency. KNNR generalizes Stone's theorem to episodic data and counterfactual estimation, providing consistency guarantees under mild assumptions about data coverage and continuity of rewards and transitions.

## Key Results
- KNNR achieves vanishing mean squared error as data size increases, with theoretical consistency guarantees under episodic sampling
- Runtime experiments show KNNR is faster than model-free Monte Carlo methods while maintaining competitive accuracy
- Performance matches or exceeds baseline methods (PEIS, PDIS, WDR, MFMC, FQE variants) across three stochastic control environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Similar state-action pairs lead to similar rewards and transitions under weak continuity assumptions.
- Mechanism: KNNR exploits continuity in rewards and state transitions by resampling from K-nearest neighbors in the data, simulating trajectories without parametric models.
- Core assumption: Rewards and transitions are continuous in the state-action space; similar inputs yield similar outputs.
- Evidence anchors:
  - [abstract]: "Assuming continuity in the rewards and the state transitions, our procedure exploits that similar state/action pairs (in a metric sense) are associated with similar rewards and state transitions."
  - [section 2]: Defines reward function $r: S \to [-M, M]$ and transition function $\alpha_t$ as uniformly equicontinuous.
- Break condition: If reward/transition functions are discontinuous or state-action space is too sparse to find meaningful neighbors.

### Mechanism 2
- Claim: KNNR generalizes Stone's theorem to episodic data and counterfactual estimation.
- Mechanism: By resampling K-NN paths, KNNR estimates value function consistently even when data comes in episodes rather than iid transitions.
- Core assumption: Data is generated in episodes with a stationary behavior policy; sufficient exploration (Assumption 1.2).
- Evidence anchors:
  - [abstract]: "We generalize Stone's Theorem, a well-known result in nonparametric statistics on local averaging, to include episodic data and the counterfactual estimation underlying off-policy evaluation (OPE)."
  - [section 3]: Proves consistency under episodic sampling by extending local averaging theory.
- Break condition: If behavior policy doesn't sufficiently explore relevant state-action space (violates Assumption 1.2).

### Mechanism 3
- Claim: Parallelization and tree-based search make KNNR computationally efficient compared to MFMC.
- Mechanism: KNNR uses efficient tree-based nearest neighbor search and allows almost complete parallelization in trajectory generation.
- Core assumption: Data set is stored and tree-based search is applicable; computational resources support parallelization.
- Evidence anchors:
  - [section 2]: "Compared to other OPE methods, our algorithm does not require optimization, can be efficiently implemented via tree-based nearest neighbor search and parallelization."
  - [section 4]: Runtime experiments show KNNR is faster than MFMC.
- Break condition: High-dimensional state-action spaces where tree-based search suffers from curse of dimensionality.

## Foundational Learning

- Concept: Off-policy evaluation (OPE) in reinforcement learning
  - Why needed here: KNNR is designed to estimate policy performance from data generated by a different policy.
  - Quick check question: What distinguishes off-policy from on-policy evaluation?

- Concept: Nonparametric regression and local averaging
  - Why needed here: KNNR's theoretical foundation relies on generalizing Stone's theorem for consistency.
  - Quick check question: How does local averaging regression relate to nearest neighbor methods?

- Concept: Stochastic control environments
  - Why needed here: KNNR is particularly suited for environments with continuous state-action spaces and inherent stochasticity.
  - Quick check question: What makes stochastic control environments challenging for OPE methods?

## Architecture Onboarding

- Component map: Data → TreeBuild → For each trajectory: K-NN search → Reward accumulation → Average
- Critical path: Data → TreeBuild → For each trajectory: K-NN search → Reward accumulation → Average
- Design tradeoffs:
  - K vs. n: Larger K reduces variance but increases bias; K must grow with n but slower than n
  - Parallelization vs. memory: Parallel trajectory generation requires storing full data set
  - Metric choice: Euclidean norm works for some problems but may need adaptation for others
- Failure signatures:
  - High variance estimates: K too small or insufficient data
  - High bias estimates: K too large or poor coverage of state space
  - Slow runtime: High-dimensional state-action space or inappropriate metric
- First 3 experiments:
  1. LQR environment with varying noise levels to test continuity assumptions
  2. LOB environment with different inventory sizes to test state space coverage
  3. BP environment with varying bin sizes to test action space handling

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several implications emerge from the work:

- How to extend consistency results to non-stationary behavior policies
- Performance scaling in high-dimensional state-action spaces
- Impact of state representation choice on KNNR effectiveness

## Limitations

- Performance degrades in high-dimensional spaces due to the curse of dimensionality in nearest neighbor search
- Theoretical guarantees require continuity assumptions that may not hold in all practical settings
- Limited comparison to more recent OPE methods and deep learning-based approaches

## Confidence

- Consistency theory via generalized Stone's theorem: High
- Empirical performance across tested environments: Medium
- Runtime advantages over baselines: Medium
- Scalability to high-dimensional problems: Low

## Next Checks

1. **Dimensionality stress test**: Evaluate KNNR on environments with increasing state-action space dimensionality to quantify the impact of the curse of dimensionality on both accuracy and runtime.

2. **Metric sensitivity analysis**: Systematically compare KNNR performance using different distance metrics (Euclidean, Mahalanobis, learned metrics) to understand the impact of metric choice on estimation quality.

3. **Coverage robustness test**: Generate data using behavior policies with varying exploration levels to assess KNNR's sensitivity to state-action space coverage and identify thresholds for reliable estimation.