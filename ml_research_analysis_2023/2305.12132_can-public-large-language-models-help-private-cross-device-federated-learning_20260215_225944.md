---
ver: rpa2
title: Can Public Large Language Models Help Private Cross-device Federated Learning?
arxiv_id: '2305.12132'
source_url: https://arxiv.org/abs/2305.12132
tags:
- private
- public
- priv
- data
- on-device
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies private federated learning of language models
  using large language models (LLMs) to improve privacy-utility tradeoffs. The authors
  propose using LLMs for knowledge distillation during public pretraining of small
  on-device models, and introduce a novel distribution matching algorithm to efficiently
  sample public data close to the private data distribution.
---

# Can Public Large Language Models Help Private Cross-device Federated Learning?

## Quick Facts
- arXiv ID: 2305.12132
- Source URL: https://arxiv.org/abs/2305.12132
- Reference count: 26
- Primary result: 7% accuracy improvement using 0.08% public data with tight privacy constraints

## Executive Summary
This paper proposes using public large language models (LLMs) to improve private federated learning of language models through knowledge distillation and distribution matching. The approach addresses the fundamental tradeoff between privacy and utility in federated learning by leveraging public data and LLMs during a mid-training phase. By sampling public data close to the private distribution using a novel distribution matching algorithm, the method achieves significant accuracy improvements while drastically reducing data requirements from 100% to just 0.08% of the public corpus under tight privacy constraints (ε=1.77).

## Method Summary
The method employs a two-stage private federated learning framework where on-device language models are first trained with differential privacy, then mid-trained on public data selected through distribution matching, and finally refined with remaining privacy budget. Public LLMs (LaMDA 2B) provide knowledge distillation targets and likelihood estimates for sampling public data that matches the private distribution. The approach uses SentencePiece tokenizers from public LLMs instead of private unigram tokenizers to prevent vocabulary-based privacy leakage while maintaining comprehensive coverage of the target domain vocabulary.

## Key Results
- 7% accuracy improvement over baselines with ε=1.77 privacy constraints
- Sample efficiency improved from 100% to 0.08% of public data requirements
- SentencePiece tokenizers from public LLMs achieve better accuracy than private unigram tokenizers
- Knowledge distillation from LLMs enables comparable accuracy with only 1% of public data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Public SentencePiece tokenizers improve private FL accuracy over private unigram tokenizers
- Mechanism: Prevents vocabulary-based privacy leakage while providing better Stack Overflow vocabulary coverage
- Core assumption: Public tokenizer covers private domain vocabulary without out-of-vocabulary tokens
- Evidence anchors: SentencePiece finds no OOV tokens in Stack Overflow dataset

### Mechanism 2
- Claim: Knowledge distillation from LLMs achieves comparable accuracy with 1% public data vs 100% without distillation
- Mechanism: LLMs transfer learned representations enabling faster convergence with limited data
- Core assumption: LLM knowledge is transferable to smaller on-device architectures
- Evidence anchors: 1% data with distillation matches 100% data without distillation accuracy

### Mechanism 3
- Claim: Distribution matching algorithm samples public data close to private distribution using 0.08% of data
- Mechanism: Combines private model likelihood and public LLM likelihood for data selection
- Core assumption: Private model with limited DP noise can approximate private data distribution
- Evidence anchors: Novel algorithm samples data close to private distribution

## Foundational Learning

- Concept: Differential Privacy and User-level DP guarantees
  - Why needed here: Framework operates under tight privacy constraints (ε=1.77)
  - Quick check question: What is the relationship between noise multiplier and achieved ε value in DP-FTRL?

- Concept: Knowledge Distillation in NLP
  - Why needed here: Core technique for transferring knowledge from LLMs to on-device LMs
  - Quick check question: How does temperature and top-k logits choice affect distillation quality?

- Concept: Distribution Matching and Density Estimation
  - Why needed here: Novel algorithm for selecting representative public data samples
  - Quick check question: Why does combining private model likelihood and public LLM likelihood improve sample selection?

## Architecture Onboarding

- Component map: Public LLM (LaMDA 2B) → Knowledge Distillation → On-device LM (LSTM/Transformer) → Private data → Private on-device LM → Distribution matching → Public data selection → Mid-training

- Critical path:
  1. Private on-device LM trained with DP for T'/2 rounds
  2. Distribution matching using private LM and public LLM
  3. Public mid-training with sampled data and distillation
  4. Final private FL training with remaining privacy budget

- Design tradeoffs:
  - Public tokenizer vs private tokenizer: Privacy leakage vs domain specificity
  - Distillation coverage vs computational cost: More epochs improve accuracy but increase training time
  - T' timing: Too early gives poor distribution estimates, too late biases toward public distribution

- Failure signatures:
  - Poor private accuracy: Likely issues with distribution matching or insufficient DP training
  - No improvement from distillation: Possible architectural mismatch or incorrect distillation hyperparameters
  - High OOV rate: SentencePiece tokenizer not covering private vocabulary adequately

- First 3 experiments:
  1. Compare public SentencePiece tokenizer vs private unigram tokenizer on Stack Overflow with DP
  2. Validate knowledge distillation improves accuracy with 1% public data vs 100% without distillation
  3. Test distribution matching performance with different T' values and compare to random sampling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of public tokenizer impact the privacy-utility tradeoff in differentially private federated learning?
- Basis in paper: Explicit comparison of SentencePiece vs unigram tokenizers
- Why unresolved: Paper demonstrates benefits but doesn't systematically explore full tokenizer space
- What evidence would resolve it: Comprehensive ablation studies comparing different tokenizer types under varying privacy budgets

### Open Question 2
- Question: What is the optimal timing (T') for distribution matching in the two-stage framework?
- Basis in paper: Explicit ablation studies showing T' = T/2 yields best results
- Why unresolved: Paper observes empirical optimality but lacks theoretical justification
- What evidence would resolve it: Theoretical analysis connecting distribution matching timing to convergence rates

### Open Question 3
- Question: How does public LLM quality affect its effectiveness as a teacher for knowledge distillation?
- Basis in paper: Uses 2B parameter LaMDA but doesn't explore how different LLM qualities impact distillation
- Why unresolved: Paper fixes LLM choice without systematic study of teacher model characteristics
- What evidence would resolve it: Experiments varying LLM model size and training corpus size

## Limitations

- Distribution matching algorithm reliability depends heavily on private model quality under DP noise constraints
- Public tokenizer effectiveness assumes adequate representation of Stack Overflow domain in public LLM training data
- Knowledge distillation generalization assumes architectural differences don't create fundamental representation gaps

## Confidence

- High Confidence: Overall framework architecture, existence of accuracy improvements, fundamental privacy guarantees
- Medium Confidence: 7% accuracy improvement claim, 0.08% data efficiency claim, superiority of public SentencePiece tokenizers
- Low Confidence: Theoretical distribution matching algorithm's practical effectiveness under extreme DP constraints, generalizability to non-Stack Overflow domains

## Next Checks

1. **Ablation Study on Distribution Matching**: Systematically evaluate distribution matching sensitivity to different T' values and noise levels by running experiments with T'=0, T'=T'/2, and T'=T' to measure accuracy degradation as privacy noise increases.

2. **Tokenizer Domain Shift Analysis**: Conduct experiments comparing SentencePiece tokenizers trained on different public corpora (C4 vs Wikipedia vs Books) to quantify the impact of tokenizer domain choice on Stack Overflow accuracy.

3. **Knowledge Distillation Robustness Test**: Evaluate distillation effectiveness across multiple architectural pairs (LSTM→Transformer, Transformer→LSTM, smaller→larger) to determine if the approach generalizes beyond the specific LLM→on-device pairing.