---
ver: rpa2
title: 'Know Your Audience: Do LLMs Adapt to Different Age and Education Levels?'
arxiv_id: '2312.02065'
source_url: https://arxiv.org/abs/2312.02065
tags:
- what
- education
- different
- llms
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) often fail to adapt their outputs
  to specific audiences like different age groups, education levels, or difficulty
  levels, even when explicitly prompted, limiting their effectiveness in educational
  settings. We evaluate the readability of answers generated by four state-of-the-art
  LLMs (two commercial and two open-source) in response to science questions tailored
  to different audiences.
---

# Know Your Audience: Do LLMs Adapt to Different Age and Education Levels?

## Quick Facts
- arXiv ID: 2312.02065
- Source URL: https://arxiv.org/abs/2312.02065
- Reference count: 19
- Large language models fail to adapt outputs to different age groups, education levels, or difficulty levels even when explicitly prompted.

## Executive Summary
This study evaluates whether large language models (LLMs) can adapt their outputs to different audience segments including age groups, education levels, and difficulty levels. The researchers tested four state-of-the-art LLMs (ChatGPT, GPT3, flan-T5-xxl, and Bigscience-T0) by prompting them to generate answers to science questions tailored for specific audiences. Using common readability metrics, they found that LLMs generally do not produce text that matches the recommended comprehension level for the requested audience, suggesting that current LLMs have set readability ranges and do not adapt well to different audiences even when prompted. The findings highlight the need for further advancements in model development and fine-tuning to ensure more reliable and effective adaptation to different audience segments.

## Method Summary
The study collected 33,600 prompt-response pairs from five LLMs across four runs, using 100 science questions tailored to different audiences. The researchers designed templates for prompting LLMs for different age, education, and difficulty levels, and prompted four LLMs (ChatGPT, GPT3, bigscience-T0, and Flan-T5) with these questions and target groups. They calculated the Flesch-Kincaid Reading Ease (FKRE) index for each response and compared it to the recommended comprehension level for the corresponding target group, analyzing the percentage of responses within the recommended FKRE index range for each target group.

## Key Results
- LLMs generally do not produce text that matches recommended comprehension levels for requested audiences
- ChatGPT generates responses in a narrow range of FKRE scores, while other models have increasingly larger ranges
- Classifiers can reliably distinguish between binary answers for kids and adults (F1 of 0.95) but fail to distinguish more fine-grained distinctions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs have inherent readability ranges that persist even when prompted for specific audiences.
- Mechanism: The models appear to have learned a distribution of readability levels during training that they default to unless explicitly controlled by fine-tuning on targeted datasets.
- Core assumption: Readability scores are based on surface-level features like sentence length and word complexity, which LLMs can control but may not consistently adjust without explicit training.
- Evidence anchors:
  - [abstract] "current LLMs have set readability ranges and do not adapt well to different audiences, even when prompted."
  - [section] "ChatGPT generates responses that are in a narrow range of FKRE scores. The other models have increasingly larger ranges that do not vary much between prompt groups."
  - [corpus] Weak - corpus neighbors discuss related readability and audience topics but don't directly support this mechanism's claim about inherent model ranges.
- Break condition: If a model is fine-tuned on a diverse dataset with explicit readability level labels, it might overcome this limitation.

### Mechanism 2
- Claim: Readability metrics may not accurately capture the actual differences in LLM-generated text for different audiences.
- Mechanism: Traditional readability metrics like FKRE focus on surface-level features that may not reflect the complexity of concepts or vocabulary appropriateness for specific age groups or education levels.
- Core assumption: The metrics were designed for human-authored text and may not account for the unique characteristics of LLM-generated content.
- Evidence anchors:
  - [abstract] "common readability metrics are not completely reliable for determining the education level of LLM-generated responses."
  - [section] "To answer that question, we ran an additional test... We assume that if there is a strong enough signal in the text for each of the target categories... then a classifier should be able to predict the target age, education level, or group from the output text with some accuracy."
  - [corpus] Weak - corpus neighbors discuss readability evaluation but don't directly support this mechanism's claim about metric limitations for LLM text.
- Break condition: If new metrics are developed specifically for LLM-generated text that better capture audience-appropriate complexity, this mechanism's claim would be weakened.

### Mechanism 3
- Claim: LLMs struggle with fine-grained audience adaptation beyond broad categories.
- Mechanism: While models can distinguish between very broad categories like "kids" vs "adults" using contextual cues, they lack the nuanced understanding needed to adapt to specific age groups or education levels.
- Core assumption: The models have learned some general patterns about language complexity for different audiences but lack the detailed knowledge needed for precise adaptation.
- Evidence anchors:
  - [abstract] "While classifiers can reliably distinguish between binary answers for kids and adults (F1 of 0.95), they fail to distinguish more fine-grained distinctions (i.e., age groups and education levels)."
  - [section] "We find large variations in the readability of the answers by different LLMs" but specific targeting fails.
  - [corpus] Weak - corpus neighbors discuss audience and readability but don't directly support this mechanism's claim about fine-grained adaptation limitations.
- Break condition: If models are trained with more detailed audience-level data and fine-grained supervision, they might improve at distinguishing between specific age groups and education levels.

## Foundational Learning

- Concept: Readability Metrics (FKRE, FKGL)
  - Why needed here: The study uses these metrics to evaluate whether LLM outputs match recommended comprehension levels for different audiences.
  - Quick check question: What does a high FKRE score indicate about text readability?

- Concept: Prompt Engineering
  - Why needed here: The study relies on carefully crafted prompts to instruct LLMs to generate text for specific age groups and education levels.
  - Quick check question: How might the phrasing of a prompt affect an LLM's ability to generate audience-appropriate content?

- Concept: Classification Evaluation
  - Why needed here: The study uses a BERT classifier to validate whether there are actual differences in LLM outputs beyond what readability metrics capture.
  - Quick check question: Why might a classifier be useful for evaluating LLM output quality beyond traditional metrics?

## Architecture Onboarding

- Component map: Question Generation -> Prompt Templates -> LLM Execution -> Readability Analysis -> Classification Validation
- Critical path: Question generation → Prompt creation → LLM response generation → Readability scoring → Classification validation
- Design tradeoffs: Using automated readability metrics provides scalability but may miss nuanced differences in content appropriateness; manual review would be more accurate but less scalable.
- Failure signatures: Consistently low readability scores across all models for younger audiences; minimal variation in readability scores when prompting for different audiences; classifier inability to distinguish between fine-grained audience categories.
- First 3 experiments:
  1. Test additional readability metrics (e.g., Gunning-Fog, SMOG) on the same dataset to compare results.
  2. Fine-tune one LLM on a dataset with explicit readability level labels and compare its performance to base models.
  3. Conduct human evaluation of a subset of responses to validate automated readability scores and identify specific content issues.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the readability metrics used in this study compare to other potential metrics for evaluating LLM-generated text?
- Basis in paper: [explicit] The authors note that the Flesch-Kincaid metrics may not capture all actual differences in texts produced by LLMs and suggest that there may be a need for more machine-generated text readability metrics.
- Why unresolved: The study only uses the Flesch-Kincaid metrics and does not explore other potential metrics for evaluating LLM-generated text.
- What evidence would resolve it: Comparing the performance of different readability metrics on LLM-generated text and assessing their ability to capture differences in text complexity.

### Open Question 2
- Question: How do LLMs perform when prompted for other types of audiences beyond age, education level, and difficulty level?
- Basis in paper: [inferred] The study only evaluates the adaptability of LLMs to age, education level, and difficulty level. There may be other types of audiences that LLMs could be prompted for.
- Why unresolved: The study does not explore the adaptability of LLMs to other types of audiences beyond age, education level, and difficulty level.
- What evidence would resolve it: Prompting LLMs for other types of audiences and evaluating their performance using appropriate metrics.

### Open Question 3
- Question: How do the results of this study generalize to other domains beyond science?
- Basis in paper: [inferred] The study only evaluates the adaptability of LLMs to science questions. There may be other domains where LLMs could be used and their adaptability evaluated.
- Why unresolved: The study only evaluates the adaptability of LLMs to science questions and does not explore their adaptability to other domains.
- What evidence would resolve it: Evaluating the adaptability of LLMs to questions from other domains and comparing their performance to that of science questions.

## Limitations
- The study relies on automated readability metrics that may not accurately capture audience-appropriate complexity in LLM-generated text
- The evaluation is limited to science questions and may not generalize to other domains
- The study doesn't explore whether different prompting strategies or model fine-tuning could improve audience adaptation

## Confidence
- High confidence: LLMs produce outputs with readability scores that don't consistently match target audience levels as measured by standard metrics
- Medium confidence: Traditional readability metrics may not be well-suited for evaluating LLM-generated text for audience appropriateness
- Low confidence: LLMs have inherent, unchangeable readability ranges that prevent audience adaptation

## Next Checks
1. Test whether fine-tuning an LLM on a dataset with explicit readability labels improves its ability to generate audience-appropriate text, comparing pre- and post-fine-tuning performance
2. Develop and test new evaluation metrics specifically designed for LLM-generated text that better capture audience-appropriate complexity beyond surface-level features
3. Conduct controlled human evaluation studies where domain experts rate the appropriateness of LLM outputs for different audience segments, validating or challenging the automated metric results