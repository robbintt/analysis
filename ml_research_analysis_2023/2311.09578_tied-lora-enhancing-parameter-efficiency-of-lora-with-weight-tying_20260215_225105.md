---
ver: rpa2
title: 'Tied-Lora: Enhancing parameter efficiency of LoRA with weight tying'
arxiv_id: '2311.09578'
source_url: https://arxiv.org/abs/2311.09578
tags:
- lora
- parameters
- language
- tasks
- tied-lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Tied-LoRA, a method to improve parameter efficiency
  in LoRA by using weight tying and selective training. It explores various combinations
  of parameter training/freezing and weight tying to balance performance and trainable
  parameters.
---

# Tied-Lora: Enhancing parameter efficiency of LoRA with weight tying

## Quick Facts
- arXiv ID: 2311.09578
- Source URL: https://arxiv.org/abs/2311.09578
- Reference count: 7
- The paper introduces Tied-LoRA, achieving comparable performance to LoRA while using only 13% of parameters through weight tying and selective training.

## Executive Summary
This paper presents Tied-LoRA, a parameter-efficient fine-tuning method that extends LoRA by introducing weight tying and selective training of components. The key innovation lies in tying low-rank matrices across all layers of the base model while selectively training different components (A, B, u, v). Through extensive experiments on five diverse tasks using two language models, the authors demonstrate that a specific Tied-LoRA configuration achieves comparable performance to standard LoRA while using only 13% of its parameters, particularly at higher ranks. This work addresses the growing need for efficient fine-tuning methods as language models scale to hundreds of billions of parameters.

## Method Summary
Tied-LoRA builds upon LoRA by adding weight tying across layers and introducing selective training of components. The method decomposes weight updates into low-rank matrices A and B, which are tied across all layers, while scaling vectors u and v remain untied. By selectively freezing or training different combinations of these components (A, B, u, v), Tied-LoRA provides fine-grained control over the trade-off between parameter efficiency and performance. The training uses AdamW optimizer with cosine learning rate schedule and early stopping, evaluating across multiple low-rank dimensions (2, 8, 32, 128) on tasks including SQuAD, DialogSum, HellaSwag, IWSLT 2017, and GSM8K.

## Key Results
- A specific Tied-LoRA configuration (vBὑ7uAὑ7) achieves comparable performance to standard LoRA across multiple tasks while using only 13% of parameters
- Weight tying alone reduces trainable parameters by 96.875% (from ~4.2M to ~131K for a 7B parameter model)
- Performance benefits are particularly pronounced at elevated ranks (32, 128) where Tied-LoRA maintains strong results with minimal parameters
- Tied-LoRA shows consistent performance across diverse tasks including QA, summarization, NLI, translation, and mathematical reasoning

## Why This Works (Mechanism)

### Mechanism 1: Weight Tying
- Claim: Weight tying reduces the number of trainable parameters by sharing the same low-rank matrices across all layers of the base model
- Mechanism: By tying the low-rank projection matrices (A and B) across all layers, the number of trainable parameters scales with the rank r instead of the number of layers L, leading to significant parameter savings
- Core assumption: The base model's layers are sufficiently similar in their adaptation needs, allowing for shared low-rank matrices
- Evidence anchors: Abstract shows Tied-LoRA uses only 13% of LoRA parameters; Section 3.2 demonstrates 96.875% reduction from ~4.2M to ~131K parameters

### Mechanism 2: Selective Training
- Claim: Selective training of components (A, B, u, v) allows for fine-grained control over the number of trainable parameters and model performance
- Mechanism: By freezing certain components (e.g., B) while training others (e.g., A), the model can achieve a balance between parameter efficiency and performance
- Core assumption: Components can be selectively trained or frozen without significantly impacting the model's ability to adapt to the target task
- Evidence anchors: Section 3.3 describes the flexible framework for investigating training configurations; Section 5 shows Tied-LoRA methods are virtually the same as LoRA across ranks with fewer parameters

### Mechanism 3: vBὑ7uAὑ7 Configuration
- Claim: The vBὑ7uAὑ7 configuration strikes the best balance between parameter efficiency and performance across diverse tasks
- Mechanism: By tying the low-rank matrices (A and B) across layers and selectively training the scaling vectors u and v, this configuration achieves comparable performance to standard LoRA while using only 13% of its parameters
- Core assumption: The scaling vectors u and v can effectively adjust the contribution of the tied low-rank matrices to achieve optimal performance across tasks
- Evidence anchors: Abstract identifies this configuration as using 13% of LoRA parameters; Section 5 shows it obtains third place in 6 out of 8 settings

## Foundational Learning

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: LoRA is the baseline method that Tied-LoRA extends by introducing weight tying and selective training
  - Quick check question: What are the key components of LoRA's low-rank update matrix, and how does it reduce the number of trainable parameters compared to full fine-tuning?

- Concept: Weight Tying
  - Why needed here: Weight tying is a critical ingredient in Tied-LoRA that allows for sharing the same low-rank matrices across all layers, leading to significant parameter savings
  - Quick check question: How does weight tying reduce the number of trainable parameters in Tied-LoRA, and what are the potential trade-offs in terms of model performance?

- Concept: Selective Training
  - Why needed here: Selective training allows for fine-grained control over which components (A, B, u, v) are trained or frozen, enabling a balance between parameter efficiency and performance
  - Quick check question: How does selectively training or freezing different components (A, B, u, v) affect the number of trainable parameters and the model's ability to adapt to the target task?

## Architecture Onboarding

- Component map: Base language model -> Tied low-rank matrices (A, B) -> Untied scaling vectors (u, v) -> Selective training strategy

- Critical path:
  1. Initialize the base language model and low-rank matrices
  2. Apply weight tying to share the low-rank matrices across layers
  3. Implement the selective training strategy to determine which components to train or freeze
  4. Train the model on the target task, updating the trainable components
  5. Evaluate the model's performance and parameter efficiency

- Design tradeoffs:
  - Parameter efficiency vs. performance: Tying weights and selective training can significantly reduce parameters but may impact performance if base model layers have diverse adaptation needs
  - Flexibility vs. stability: Untying scaling vectors provides more flexibility but may introduce instability, especially at higher ranks

- Failure signatures:
  - Performance degradation: If weight tying or selective training is not well-suited to the target task, the model may underperform compared to standard LoRA
  - Training instability: Untying scaling vectors at high ranks may lead to unstable training dynamics
  - Overfitting: Training too many components or using an inappropriate selective training strategy may cause overfitting

- First 3 experiments:
  1. Implement vBuA(LoRA) as the baseline, without weight tying or selective training, to establish a performance reference
  2. Implement vBὑ7uAὑ7, tying the low-rank matrices across layers and training all components, to evaluate the impact of weight tying on performance and parameter efficiency
  3. Implement vBὑ7uAὑ7, tying the low-rank matrices and selectively freezing B while training A, u, and v, to assess the benefits of selective training in combination with weight tying

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but implicitly leaves several areas for exploration, including the generalizability of Tied-LoRA to larger models, its performance on complex reasoning tasks, and the potential for combining with other parameter-efficient techniques.

## Limitations

- Limited generalizability to larger models: Results are shown only on 2B and 7B parameter models, with unclear performance on 100B+ parameter models
- Task-specific optimization unclear: The paper uses a universal configuration but doesn't explore whether different tasks benefit from different selective training strategies
- Complex reasoning performance gaps: Mathematical reasoning tasks (GSM8K) show suboptimal performance compared to LoRA, suggesting limitations for complex reasoning

## Confidence

**High Confidence**: The core mathematical formulation and parameter reduction mechanism through weight tying are well-established and clearly demonstrated with robust empirical evidence (96.875% reduction).

**Medium Confidence**: The claim that vBὑ7uAὑ7 achieves comparable performance to LoRA using 13% of parameters is supported by experimental results across five tasks, but limited task diversity and base models prevent strong generalization claims.

**Low Confidence**: The assertion that benefits are particularly pronounced at "elevated ranks" lacks consistent empirical support across all tasks, with performance variations suggesting a more complex relationship between rank and efficiency.

## Next Checks

1. Cross-Architecture Validation: Implement Tied-LoRA on a larger language model (e.g., LLaMA2 70B) and a different architecture (e.g., OPT) to verify whether the 13% parameter efficiency claim holds across diverse model families and scales.

2. Component Ablation Study: Systematically isolate the contributions of weight tying versus selective training by testing configurations that use only weight tying (all components trained) versus only selective training (no weight tying) to determine which mechanism drives the primary performance gains.

3. Task-Specific Optimization: Conduct experiments varying the selective training strategy for each task type, particularly focusing on mathematical reasoning tasks where current configurations show suboptimal performance, to determine if task-specific configurations outperform the universal vBὑ7uAὑ7 approach.