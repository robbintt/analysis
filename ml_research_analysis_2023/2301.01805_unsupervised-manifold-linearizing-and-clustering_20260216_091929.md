---
ver: rpa2
title: Unsupervised Manifold Linearizing and Clustering
arxiv_id: '2301.01805'
source_url: https://arxiv.org/abs/2301.01805
tags:
- clustering
- cluster
- subspace
- learned
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of simultaneously clustering and
  learning an union-of-orthogonal-subspace representation for data, when data lies
  close to a union of low-dimensional manifolds. To address the problem, we propose
  an objective based on maximal coding rate reduction and doubly stochastic membership
  inspired by the state-of-the-art subspace clustering results.
---

# Unsupervised Manifold Linearizing and Clustering

## Quick Facts
- arXiv ID: 2301.01805
- Source URL: https://arxiv.org/abs/2301.01805
- Reference count: 40
- Key outcome: Proposes an objective based on maximal coding rate reduction and doubly stochastic membership for simultaneously clustering and learning union-of-orthogonal-subspace representations for data lying close to low-dimensional manifolds.

## Executive Summary
This paper addresses the challenge of simultaneously clustering data that lies close to a union of low-dimensional manifolds and learning a linear representation for these manifolds. The proposed method, MLC, combines a maximal coding rate reduction (MCR²) objective with a doubly stochastic membership constraint to achieve state-of-the-art performance on datasets with larger numbers of clusters and imbalanced clusters. The approach parameterizes the membership matrix as a neural network output, enabling mini-batch training with reduced computational complexity.

## Method Summary
The MLC method proposes an objective based on maximal coding rate reduction and doubly stochastic membership, inspired by state-of-the-art subspace clustering results. It uses a parameterization of membership variables as neural network outputs and employs a meta-algorithm to jointly optimize the representation and membership. The method is designed to handle datasets with larger numbers of clusters and imbalanced clusters, achieving state-of-the-art performance in clustering accuracy and normalized mutual information.

## Key Results
- Achieves clustering accuracy comparable with state-of-the-art alternatives on CIFAR-10, CIFAR-20, CIFAR-100, and TinyImageNet-200.
- Demonstrates state-of-the-art performance on datasets with larger numbers of clusters and imbalanced clusters.
- Shows scalability advantages by reducing computational complexity through mini-batch training.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Doubly stochastic membership avoids combinatorial complexity and provides smoother optimization landscape than hard assignments.
- Mechanism: By constraining Π to be doubly stochastic (Π ≥ 0, Π1 = Π⊤1 = 1), the optimization over cluster assignments becomes continuous rather than combinatorial. This allows gradient-based methods to be used.
- Core assumption: The doubly stochastic constraint sufficiently regularizes the membership to produce meaningful clusters while maintaining computational tractability.
- Evidence anchors:
  - [abstract]: "we propose an objective based on maximal coding rate reduction and doubly stochastic membership inspired by the state-of-the-art subspace clustering results"
  - [section]: "Inspired by the above, we propose a constraint set Ω for matrix Π to be the set of n × n doubly stochastic matrices"
  - [corpus]: Found 25 related papers; average neighbor FMR=0.479; top related title "Attention-Guided Deep Adversarial Temporal Subspace Clustering" suggests literature exists but corpus evidence is weak
- Break condition: If the doubly stochastic constraint becomes too restrictive relative to the true cluster structure, or if the regularization parameter γ is poorly tuned, the method may fail to identify correct clusters.

### Mechanism 2
- Claim: MCR² objective with orthogonal subspace constraint produces representations with both within-cluster diversity and between-cluster discrimination.
- Mechanism: The MCR² objective maximizes coding rate R (promoting feature diversity across all samples) while minimizing clustered coding rate Rc (encouraging samples within each cluster to lie in low-dimensional subspaces). The orthogonal subspace constraint ensures different clusters are well-separated.
- Core assumption: Data from each manifold can be approximated by a low-dimensional subspace after appropriate transformation.
- Evidence anchors:
  - [abstract]: "the proposed method achieves clustering accuracy comparable with state-of-the-art alternatives, while being more scalable and learning geometrically meaningful representations"
  - [section]: "features from each cluster spread uniformly within a subspace (within-cluster diverse), and the subspaces from different clusters are orthogonal to each other (between-cluster discriminative)"
  - [corpus]: Corpus evidence is weak for direct support of MCR² properties
- Break condition: If the manifolds are highly curved or have complex non-linear structure that cannot be linearized by any feature map, the orthogonal subspace assumption breaks down.

### Mechanism 3
- Claim: Parameterizing Π as a neural network output enables mini-batch training with O(batch_size²) complexity instead of O(n²).
- Mechanism: Instead of directly optimizing over the full n×n membership matrix, the method parameterizes Π as the Sinkhorn projection of C⊤C where C is a neural network output. This allows computing Π for a batch of size b with only O(b²) operations.
- Core assumption: The Sinkhorn projection can effectively regularize the affinity matrix while preserving the necessary cluster structure.
- Evidence anchors:
  - [section]: "this leads to maintaining O(n²) variables which is prohibitive for large datasets... since one can do stochastic gradient descent such that for each batch both the memory and computational complexity is at most square of the batch size"
  - [corpus]: No direct corpus evidence for this specific computational advantage
- Break condition: If the batch size is too small relative to the number of clusters, the learned membership may not capture the full cluster structure.

## Foundational Learning

- Concept: Doubly stochastic matrices
  - Why needed here: Provides the constraint set for membership matrix that enables smooth optimization and spectral clustering
  - Quick check question: What are the two conditions that define a doubly stochastic matrix?

- Concept: Coding rate reduction
  - Why needed here: Forms the objective that encourages both within-cluster diversity and between-cluster discrimination
  - Quick check question: In the MCR² objective, what do the terms R(Zθ; ϵ) and Rc(Zθ, Π; ϵ) measure respectively?

- Concept: Sinkhorn algorithm
  - Why needed here: Projects an arbitrary affinity matrix to the doubly stochastic constraint set efficiently
  - Quick check question: What is the computational complexity of Sinkhorn projection for an n×n matrix?

## Architecture Onboarding

- Component map:
  - Backbone network (ResNet-18) → Feature head → Projection to sphere → Cluster head → Sinkhorn projection → Doubly stochastic membership
  - Two optimization targets: feature representation Zθ and membership Πθ

- Critical path:
  - Forward pass: Input → Backbone → Feature head → Cluster head → Sinkhorn projection
  - Backward pass: Compute MCR² loss → Gradients flow through both heads

- Design tradeoffs:
  - Memory vs accuracy: Using mini-batch Sinkhorn projection vs full matrix computation
  - Initialization stability: Self-supervised initialization vs random initialization
  - Hyperparameter sensitivity: γ for regularization, ϵ for precision, η for Sinkhorn

- Failure signatures:
  - Low clustering accuracy with high NMI suggests poor cluster separation
  - High numerical rank of features indicates insufficient linearization
  - Unstable training with high variance across seeds indicates poor initialization

- First 3 experiments:
  1. Train with only self-supervised initialization (no MLC objective) and evaluate clustering to verify baseline
  2. Train MLC with random initialization vs self-supervised initialization to verify importance of initialization
  3. Vary γ parameter (0, 0.01, 0.05, 0.1) to find optimal regularization strength

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed MLC method compare to NMCE in terms of stability with respect to random seeds?
- Basis in paper: [explicit] The paper mentions that MLC has a more stable performance with respect to random seeds, since MLC is able to initialize the membership deterministically using structures from the self-supervised initialized features.
- Why unresolved: While the paper mentions this advantage of MLC, it does not provide empirical evidence to support this claim. The paper does conduct additional experiments to compare MLC and NMCE in terms of stability with respect to random seeds, but the results are not directly comparable to those in the main text due to differences in hyper-parameters and optimizers.
- What evidence would resolve it: Conducting experiments to compare MLC and NMCE in terms of stability with respect to random seeds, using the same hyper-parameters and optimizers, would provide evidence to support or refute this claim.

### Open Question 2
- Question: How does the proposed MLC method perform on datasets with a larger number of clusters and imbalanced clusters?
- Basis in paper: [explicit] The paper conducts experiments on datasets with a larger number of clusters and imbalanced clusters, such as CIFAR100-20, CIFAR100-100, and TinyImageNet-200, and shows that the proposed method achieves state-of-the-art performance.
- Why unresolved: While the paper provides experimental results on these datasets, it does not discuss the performance of MLC in comparison to other methods that are specifically designed to handle imbalanced clusters.
- What evidence would resolve it: Conducting experiments to compare the performance of MLC with other methods that are specifically designed to handle imbalanced clusters on datasets with a larger number of clusters and imbalanced clusters would provide evidence to support or refute this claim.

## Limitations

- Weak corpus evidence with only 25 related papers and zero average citations, suggesting limited direct literature support for the specific MCR² objective formulation.
- Limited evaluation scope focused primarily on image datasets with moderate cluster counts, leaving performance on extremely large cluster counts or highly imbalanced distributions unexplored.
- Computational complexity claims lack empirical verification with no runtime or memory usage comparisons against baselines provided.

## Confidence

- High confidence: The theoretical foundation of doubly stochastic matrices and their role in spectral clustering is well-established.
- Medium confidence: The MCR² objective formulation appears sound based on the stated goals of within-cluster diversity and between-cluster discrimination.
- Low confidence: The claim of state-of-the-art performance on imbalanced datasets lacks strong support due to limited experimental validation and weak corpus grounding.

## Next Checks

1. Reproduce on additional datasets: Implement MLC on text or tabular datasets with known manifold structure to test generalizability beyond image data.

2. Benchmark computational efficiency: Measure actual memory usage and training time for different batch sizes and dataset scales to verify claimed complexity advantages.

3. Conduct extensive ablation studies: Systematically vary γ, ϵ, and η parameters across wider ranges while also testing different backbone architectures and initialization strategies.