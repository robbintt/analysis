---
ver: rpa2
title: A Comparative Study on TF-IDF feature Weighting Method and its Analysis using
  Unstructured Dataset
arxiv_id: '2308.04037'
source_url: https://arxiv.org/abs/2308.04037
tags:
- have
- data
- reviews
- feature
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper compares TF-IDF and N-Gram feature weighting methods
  for sentiment analysis on unstructured text datasets. It evaluates both techniques
  on IMDB movie reviews and Amazon Alexa reviews using six classifiers: SVM, Logistic
  Regression, Multinomial Naive Bayes, Random Forest, Decision Tree, and k-nearest
  neighbors.'
---

# A Comparative Study on TF-IDF feature Weighting Method and its Analysis using Unstructured Dataset

## Quick Facts
- arXiv ID: 2308.04037
- Source URL: https://arxiv.org/abs/2308.04037
- Reference count: 32
- TF-IDF significantly outperforms N-Gram across all classifiers in sentiment analysis tasks

## Executive Summary
This paper presents a comparative analysis of TF-IDF and N-Gram feature weighting methods for sentiment analysis on unstructured text datasets. The study evaluates both techniques on IMDB movie reviews and Amazon Alexa reviews using six different classifiers. Results demonstrate that TF-IDF consistently outperforms N-Gram across all classifiers, with Random Forest achieving the highest performance metrics. The findings provide evidence that TF-IDF is more effective for feature extraction in sentiment analysis tasks compared to N-Gram approaches.

## Method Summary
The study employs data preprocessing including tokenization, normalization, stemming, lemmatization, stop word removal, and noise removal on unstructured text data from IMDB movie reviews and Amazon Alexa reviews. Feature extraction is performed using both TF-IDF and N-Gram (bigram) methods. Six classifiers are trained and evaluated: SVM, Logistic Regression, Multinomial Naive Bayes, Random Forest, Decision Tree, and k-nearest neighbors. Performance is measured using accuracy, precision, recall, and F1-score metrics. The methodology compares the effectiveness of TF-IDF versus N-Gram approaches across different classifiers and datasets.

## Key Results
- TF-IDF significantly outperforms N-Gram across all classifiers in sentiment analysis tasks
- Random Forest achieves the highest performance with TF-IDF: 93.81% accuracy, 94.20% precision, 93.81% recall, and 91.99% F1-score
- TF-IDF feature weighting method demonstrates superior effectiveness compared to N-Gram for sentiment classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TF-IDF weighting outperforms N-Gram feature extraction in sentiment analysis tasks.
- Mechanism: TF-IDF assigns higher weights to terms that are frequent in a specific document but rare across the corpus, capturing discriminative features better than N-Gram's sequential token counting.
- Core assumption: The discriminative power of a term is better captured by its inverse document frequency than by its sequential context in N-Grams.
- Evidence anchors:
  - [abstract] "TF-IDF significantly outperforms N-Gram across all classifiers"
  - [section] "TF-IDF is a statistical model to evaluates the significance of words in a document pool. It may be calculated by multiplying two metrics: TF-matrix... and IDF..."
- Break condition: If the dataset contains many domain-specific terms that are frequent across documents, IDF weighting may incorrectly down-weight useful terms.

### Mechanism 2
- Claim: Random Forest classifier achieves highest performance with TF-IDF features.
- Mechanism: Random Forest's ensemble structure can better handle the sparse, high-dimensional TF-IDF feature space by aggregating multiple decision trees, reducing overfitting compared to single classifiers.
- Core assumption: Ensemble methods are more robust to noise and overfitting in high-dimensional sparse feature spaces.
- Evidence anchors:
  - [abstract] "Random Forest achieving the highest performance: 93.81% accuracy, 94.20% precision, 93.81% recall, and 91.99% F1-score"
  - [section] "In the case of TF-IDF, SVM and Random Forest are performing better among all the classifiers"
- Break condition: If the dataset size is small relative to feature dimensionality, even Random Forest may overfit despite ensemble averaging.

### Mechanism 3
- Claim: Data preprocessing steps (tokenization, stemming, lemmatization, stop word removal) improve classifier performance.
- Mechanism: Cleaning and normalizing text reduces noise and dimensionality, allowing classifiers to focus on meaningful content words rather than function words or morphological variations.
- Core assumption: Raw text contains sufficient redundancy and noise that preprocessing significantly improves signal-to-noise ratio for classification.
- Evidence anchors:
  - [section] "We have gone through some steps for data preprocessing... Tokenization... Normalization... Stemming... Lemmatization... Remove Stop Words"
- Break condition: If the dataset is already clean or domain-specific preprocessing is not well-suited to the language/genre, these steps may remove useful information.

## Foundational Learning

- Concept: TF-IDF weighting formula and interpretation
  - Why needed here: Understanding how TF-IDF scores are computed is essential for interpreting feature importance and debugging model behavior.
  - Quick check question: What happens to a term's TF-IDF score if it appears in every document of the corpus?

- Concept: N-Gram feature extraction and its limitations
  - Why needed here: Knowing how N-Grams capture sequential context helps explain why they may underperform compared to TF-IDF for sentiment classification.
  - Quick check question: How does increasing n in N-Grams affect feature dimensionality and sparsity?

- Concept: Ensemble learning and Random Forest mechanics
  - Why needed here: Understanding how Random Forest aggregates predictions across trees explains its robustness to high-dimensional sparse features.
  - Quick check question: What is the primary mechanism by which Random Forest reduces overfitting compared to a single decision tree?

## Architecture Onboarding

- Component map: Data preprocessing pipeline (tokenization → normalization → stemming/lemmatization → stop word removal) → Feature extraction module (TF-IDF vs N-Gram) → Classifier training and evaluation module (SVM, Logistic Regression, Multinomial NB, Random Forest, Decision Tree, KNN) → Performance metrics calculation (accuracy, precision, recall, F1-score)

- Critical path: Load and preprocess text data → Extract TF-IDF features → Train Random Forest classifier → Evaluate performance metrics → Compare against N-Gram baseline

- Design tradeoffs:
  - TF-IDF vs N-Gram: TF-IDF is more discriminative but loses sequential context; N-Gram preserves order but creates higher dimensionality.
  - Classifier choice: Random Forest handles high-dimensional sparse data well but is computationally heavier than linear models.
  - Preprocessing depth: Aggressive stemming may remove sentiment-bearing morphology; conservative preprocessing preserves more information but adds noise.

- Failure signatures:
  - Low precision, high recall: Model is over-predicting positive class, possibly due to class imbalance or preprocessing removing negative indicators.
  - Random Forest underperforms: May indicate insufficient training data relative to feature space dimensionality or suboptimal hyperparameter tuning.
  - TF-IDF and N-Gram perform similarly: Suggests the dataset may not benefit from IDF weighting, possibly due to uniform term distribution across documents.

- First 3 experiments:
  1. Train and evaluate all six classifiers with TF-IDF features on the IMDB dataset to confirm Random Forest superiority.
  2. Repeat the same experiment with N-Gram features to establish the performance gap.
  3. Perform ablation study by removing each preprocessing step to quantify their individual contributions to performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the TF-IDF performance compare to the N-Gram approach when applied to datasets from different domains (e.g., political news, medical literature, or technical documentation)?
- Basis in paper: [inferred] The paper only tested TF-IDF and N-Gram on movie reviews and Amazon Alexa reviews, both of which are relatively informal and consumer-focused text.
- Why unresolved: The effectiveness of feature weighting methods can vary significantly based on the nature and structure of the text data. Without testing across diverse domains, it is unclear if TF-IDF consistently outperforms N-Gram in all contexts.
- What evidence would resolve it: Comparative experiments using TF-IDF and N-Gram on datasets from varied domains such as political news, medical literature, and technical documentation would provide evidence of their relative performance across different text types.

### Open Question 2
- Question: What is the impact of using different preprocessing techniques (e.g., lemmatization vs. stemming, stop word removal) on the performance of TF-IDF and N-Gram feature extraction methods?
- Basis in paper: [explicit] The paper mentions data preprocessing steps like tokenization, normalization, stemming, lemmatization, and noise removal but does not analyze their individual impacts on TF-IDF and N-Gram performance.
- Why unresolved: Preprocessing can significantly affect the quality and relevance of features extracted by TF-IDF and N-Gram. Without isolating the effects of each preprocessing step, it is unclear which techniques most benefit each feature extraction method.
- What evidence would resolve it: Experiments that systematically vary preprocessing techniques (e.g., using lemmatization vs. stemming, with and without stop word removal) and measure their impact on the performance of TF-IDF and N-Gram would provide clarity.

### Open Question 3
- Question: How does the inclusion of semantic information (e.g., word embeddings, contextual embeddings) in the TF-IDF or N-Gram models affect their performance in sentiment analysis tasks?
- Basis in paper: [inferred] The paper focuses on traditional TF-IDF and N-Gram methods without incorporating semantic information, which is mentioned as a potential area for improvement in the literature review.
- Why unresolved: Semantic information can capture deeper meanings and relationships between words, potentially enhancing feature extraction and classification accuracy. The paper does not explore this aspect, leaving its impact on TF-IDF and N-Gram performance unknown.
- What evidence would resolve it: Comparative experiments that integrate semantic information (e.g., using word embeddings or contextual embeddings) into TF-IDF and N-Gram models and evaluate their performance against traditional methods would provide evidence of the benefits or limitations of incorporating semantic information.

## Limitations
- Limited dataset diversity: Only tested on IMDB movie reviews and Amazon Alexa reviews, both focused on product/user sentiment
- Preprocessing details unspecified: Specific parameter choices for preprocessing steps are not detailed, creating uncertainty in exact reproduction
- No hyperparameter tuning mentioned: Lack of systematic hyperparameter optimization suggests potential for performance improvement

## Confidence
- TF-IDF outperforming N-Gram: High (consistent across all classifiers)
- Random Forest achieving highest performance: High (specific metric values provided)
- Preprocessing improving results: Medium (described but not empirically validated)

## Next Checks
1. Test TF-IDF vs N-Gram performance on a third, structurally different dataset (e.g., news articles or scientific abstracts) to assess domain generalizability.
2. Implement systematic hyperparameter tuning for each classifier and measure performance improvements, particularly for Random Forest (tree depth, number of estimators).
3. Conduct an ablation study removing individual preprocessing steps to quantify their contribution to the reported performance gains.