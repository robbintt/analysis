---
ver: rpa2
title: Does Writing with Language Models Reduce Content Diversity?
arxiv_id: '2309.05196'
source_url: https://arxiv.org/abs/2309.05196
tags:
- instructgpt
- writing
- gpt3
- essays
- diversity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether using large language models (LLMs)
  in collaborative writing reduces content diversity. A controlled experiment is conducted
  where participants write argumentative essays with three setups: no model assistance
  (Solo), base LLM (GPT3), and feedback-tuned LLM (InstructGPT).'
---

# Does Writing with Language Models Reduce Content Diversity?

## Quick Facts
- arXiv ID: 2309.05196
- Source URL: https://arxiv.org/abs/2309.05196
- Reference count: 40
- Primary result: Writing with feedback-tuned LLMs (InstructGPT) significantly reduces content diversity compared to solo writing or base LLM (GPT3)

## Executive Summary
This paper investigates whether using large language models (LLMs) in collaborative writing reduces content diversity. Through a controlled experiment, participants wrote argumentative essays with three setups: no model assistance, base LLM (GPT3), and feedback-tuned LLM (InstructGPT). The study developed a set of diversity metrics measuring homogenization (similarity between essays) and diversity (lexical and content diversity). Results show that writing with InstructGPT leads to significantly higher homogenization and lower diversity compared to solo writing and GPT3, with InstructGPT contributing less diverse text and increasing repetition of common phrases.

## Method Summary
The study conducted a controlled experiment with 100 participants writing argumentative essays on 10 different topics from NYTimes student opinion series. Participants were randomly assigned to one of three conditions: Solo (no model assistance), GPT3 (base LLM), or InstructGPT (feedback-tuned LLM). Keystroke-level logging was used to distinguish model-contributed text from user-written text. Diversity metrics including homogenization (Rouge-L/BertScore similarity), lexical diversity (n-gram ratios), and content diversity (key point clustering) were computed. Statistical tests were performed to compare diversity across conditions.

## Key Results
- InstructGPT leads to significantly higher homogenization and lower diversity compared to Solo and GPT3
- InstructGPT contributes less diverse text, increasing repetition of common phrases
- User writing behavior remains largely unaffected by model assistance
- The reduction in diversity is primarily driven by model contributions rather than changes in user behavior

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Feedback-tuned LLMs (InstructGPT) produce less diverse text than base LLMs (GPT3).
- **Mechanism:** Human feedback tuning reduces entropy in model output distributions, leading to more predictable and repetitive text.
- **Core assumption:** Human feedback prioritizes quality over diversity, implicitly favoring common patterns.
- **Evidence anchors:**
  - [abstract] "We find that InstructGPT contributes less diverse text to co-written essays, increasing repetition of common phrases."
  - [section] "InstructGPT generates less diverse text than GPT3... Bai et al. (2022) find that finetuning leads to decreased entropy of the output distribution."
  - [corpus] Weak—no direct corpus statistics comparing InstructGPT and GPT3 diversity distributions provided.
- **Break condition:** If feedback tuning explicitly rewards diversity, this mechanism fails.

### Mechanism 2
- **Claim:** InstructGPT reduces overall content diversity by contributing more repetitive high-order n-grams.
- **Mechanism:** Less diverse model suggestions are more frequently accepted and integrated into essays, increasing repetition of common phrases.
- **Core assumption:** Users accept and retain model suggestions at similar rates across model types.
- **Evidence anchors:**
  - [abstract] "Essays co-written with InstructGPT has the least content diversity... increase repetition of common phrases."
  - [section] "We observe that the 5-gram distributions of user-written text remain the same... whereas InstructGPT contributed text has notably larger probability mass on common 5-grams."
  - [corpus] Weak—no corpus-level n-gram frequency tables comparing model types.
- **Break condition:** If user editing significantly modifies model suggestions, reducing repetition.

### Mechanism 3
- **Claim:** InstructGPT increases homogenization by contributing more similar key points than GPT3.
- **Mechanism:** Similar model suggestions across different users lead to similar essay content, increasing pairwise essay similarity.
- **Core assumption:** Model suggestions directly influence essay content and key points.
- **Evidence anchors:**
  - [abstract] "InstructGPT contributes less diverse text to co-written essays... increasing the similarity between the writings of different authors."
  - [section] "The InstructGPT contributed key points have higher average homogenization score than GPT3... user behavior... is largely unchanged."
  - [corpus] Weak—no corpus-level key point similarity matrices provided.
- **Break condition:** If users selectively modify or reject similar suggestions, reducing homogenization.

## Foundational Learning

- **Concept:** Content diversity metrics (lexical, content, homogenization)
  - Why needed here: To quantify and compare the effect of different LLMs on writing diversity.
  - Quick check question: What is the difference between lexical diversity and content diversity?

- **Concept:** Controlled experiments in human-AI interaction
  - Why needed here: To isolate the effect of LLM assistance from other factors influencing writing diversity.
  - Quick check question: Why is random assignment of users to model conditions important in this study?

- **Concept:** Language model decoding strategies (temperature, frequency penalty)
  - Why needed here: To understand how decoding choices affect model output diversity and user experience.
  - Quick check question: How does increasing temperature affect the diversity of language model outputs?

## Architecture Onboarding

- **Component map:** CoAuthor platform -> OpenAI API (GPT3/InstructGPT) -> Keystroke logging -> Essay storage -> Key point summarization -> Diversity metrics calculation -> Statistical testing
- **Critical path:** 1. User writes essay with model assistance 2. Model suggestions are logged and integrated 3. Essay is summarized into key points 4. Diversity metrics are calculated 5. Statistical tests are performed
- **Design tradeoffs:**
  - Using a single model per condition vs. multiple models to account for variability
  - Real-time logging vs. post-hoc analysis of user behavior
  - Automated key point summarization vs. manual annotation
- **Failure signatures:**
  - Low model suggestion acceptance rates indicating poor user experience
  - Inconsistent diversity metrics across essays suggesting metric issues
  - Statistical test failures indicating insufficient power or confounding factors
- **First 3 experiments:**
  1. Compare diversity metrics between InstructGPT and GPT3 with no user interaction to isolate model effects.
  2. Analyze the relationship between model suggestion diversity and essay diversity to understand contribution effects.
  3. Conduct a pilot study with a different user population (e.g., students) to assess generalizability.

## Open Questions the Paper Calls Out

1. **Long-term impact:** What is the long-term impact of repeated interactions between users and LLMs on content diversity?
   - Basis in paper: [inferred] The authors note that their study focuses on single interactions and acknowledge that dynamics might change through repeated interactions over time.
   - Why unresolved: The current study only examines single writing sessions, not capturing potential cumulative effects of prolonged model use.
   - What evidence would resolve it: Longitudinal studies tracking content diversity across multiple writing sessions or over extended periods of LLM usage.

2. **Generalizability to other interfaces:** Does the reduction in content diversity apply to other types of LLM interactions beyond the specific TAB-based suggestion interface used in this study?
   - Basis in paper: [explicit] The authors acknowledge that their interface provides suggestions through continuations and suggest further investigation is needed for other forms of interaction.
   - Why unresolved: The study is limited to one specific interaction paradigm, and it's unclear if the diversity reduction generalizes to different LLM interfaces or interaction modes.
   - What evidence would resolve it: Experiments testing content diversity across various LLM interaction paradigms (e.g., prompt-based, dialogue-based, or real-time autocompletion).

3. **User characteristics moderation:** Are there user characteristics or prior experiences that moderate the impact of LLM writing assistance on content diversity?
   - Basis in paper: [inferred] The authors recruited a diverse group of participants but don't analyze how individual differences might affect the results.
   - Why unresolved: The study doesn't examine whether factors like writing expertise, topic familiarity, or demographic background influence how much users rely on model suggestions.
   - What evidence would resolve it: Analysis of user-level data to identify correlations between personal characteristics and diversity metrics, or experiments with more targeted participant selection.

## Limitations

- The study focuses on a specific writing task (argumentative essays) and population, limiting generalizability to other domains and user groups.
- The corpus-level analysis of model behavior is weak, with no direct comparisons of InstructGPT vs GPT3 output distributions.
- The study does not account for potential differences in user acceptance rates of model suggestions across conditions.

## Confidence

- **High Confidence**: The experimental methodology for measuring homogenization and diversity is sound, with robust statistical testing and clear comparisons between model conditions.
- **Medium Confidence**: The finding that InstructGPT reduces content diversity is supported by multiple metrics, though the exact mechanism remains partially unclear.
- **Low Confidence**: The claim that user behavior remains unaffected by model assistance needs more direct evidence, as keystroke-level analysis alone may not capture all behavioral changes.

## Next Checks

1. **Corpus-level analysis**: Conduct a direct comparison of InstructGPT and GPT3 output distributions on the same prompts, measuring entropy and n-gram diversity to confirm the mechanism of reduced diversity.

2. **User acceptance analysis**: Analyze acceptance rates of model suggestions across conditions to verify whether users accept similar proportions of text from each model type, as assumed in the mechanism.

3. **Cross-domain validation**: Replicate the experiment with different writing tasks (e.g., creative writing, technical documentation) and user populations (e.g., students, professionals) to assess generalizability of findings.