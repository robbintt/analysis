---
ver: rpa2
title: 'FACET: Fairness in Computer Vision Evaluation Benchmark'
arxiv_id: '2309.00035'
source_url: https://arxiv.org/abs/2309.00035
tags:
- person
- facet
- perceived
- skin
- attributes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FACET introduces a large-scale fairness benchmark for computer
  vision, containing 32k images of 50k people annotated with demographic attributes
  (perceived gender presentation, skin tone, age), visual attributes (hair type/color,
  accessories), and person-related class labels (e.g., doctor, guitarist). The dataset
  enables intersectional fairness analysis across classification, detection, segmentation,
  and visual grounding tasks.
---

# FACET: Fairness in Computer Vision Evaluation Benchmark

## Quick Facts
- arXiv ID: 2309.00035
- Source URL: https://arxiv.org/abs/2309.00035
- Reference count: 40
- Key outcome: FACET introduces a large-scale fairness benchmark for computer vision, containing 32k images of 50k people annotated with demographic attributes (perceived gender presentation, skin tone, age), visual attributes (hair type/color, accessories), and person-related class labels (e.g., doctor, guitarist). The dataset enables intersectional fairness analysis across classification, detection, segmentation, and visual grounding tasks. Evaluations using state-of-the-art models (CLIP, Faster R-CNN, Mask R-CNN, Detic, OFA) reveal performance disparities across demographic attributes and their intersections, such as higher detection errors for darker skin tones and coily hair. FACET highlights systemic biases in vision models, emphasizing the need for equitable model development and evaluation. The benchmark is publicly available to support fairness research in computer vision.

## Executive Summary
FACET introduces a large-scale fairness benchmark for computer vision, containing 32k images of 50k people annotated with demographic attributes (perceived gender presentation, skin tone, age), visual attributes (hair type/color, accessories), and person-related class labels (e.g., doctor, guitarist). The dataset enables intersectional fairness analysis across classification, detection, segmentation, and visual grounding tasks. Evaluations using state-of-the-art models (CLIP, Faster R-CNN, Mask R-CNN, Detic, OFA) reveal performance disparities across demographic attributes and their intersections, such as higher detection errors for darker skin tones and coily hair. FACET highlights systemic biases in vision models, emphasizing the need for equitable model development and evaluation. The benchmark is publicly available to support fairness research in computer vision.

## Method Summary
FACET provides exhaustive annotations for 50k people across 32k images, labeling each person with 13 attributes including demographic (perceived gender presentation, skin tone, age) and visual (hair type/color, accessories) attributes. The dataset includes 52 person-related classes mapped to ImageNet-21k vocabulary for compatibility with pre-trained models. FACET enables evaluation of fairness across four vision tasks: classification, detection, segmentation, and visual grounding. The benchmark uses the Monk Skin Tone Scale for improved representation of darker skin tones compared to Fitzpatrick. Models are evaluated using recall disparity metrics across single attributes and their intersections to reveal compounded bias effects.

## Key Results
- FACET contains 32k images and 50k people annotated with 13 attributes including demographic and visual characteristics
- Evaluations reveal higher detection errors for darker skin tones and coily hair across state-of-the-art models
- Intersectional analysis shows compounded performance disparities when multiple demographic attributes combine
- The benchmark enables fairness evaluation across classification, detection, segmentation, and visual grounding tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Exhaustive person-level annotations across 13 attributes enable intersectional fairness analysis that sparse annotations cannot support.
- Mechanism: By labeling every person in every image across demographic (perceived gender, skin tone, age) and visual (hair type/color, accessories) attributes, FACET allows models to be evaluated for bias not just on single attributes but on their intersections (e.g., darker skin + coily hair). This enables detection of compounded biases that single-attribute datasets miss.
- Core assumption: Intersectional analysis is necessary to reveal compounded bias effects that single-attribute analysis obscures.
- Evidence anchors:
  - [abstract]: "exhaustive annotations collected, we probe models using single demographics attributes as well as multiple attributes using an intersectional approach (e.g. hair color and perceived skin tone)."
  - [section 4]: "FACET includes both demographic attributes and additional visual attributes. These exhaustively labeled, manually annotated attributes for all images in the dataset allow for evaluation of model performance and robustness at a fine-grained level."
  - [corpus]: Weak - corpus neighbors discuss fairness but not intersectional evaluation specifically.
- Break condition: If the annotations are incomplete (missing some people or attributes), intersectional analysis becomes unreliable.

### Mechanism 2
- Claim: Use of the Monk Skin Tone Scale instead of Fitzpatrick improves representation of darker skin tones in fairness evaluation.
- Mechanism: The Monk Skin Tone Scale contains 10 tones with better coverage of darker ranges compared to Fitzpatrick's limited darker tone representation. This reduces bias from underrepresenting darker skin in model training and evaluation.
- Core assumption: Fitzpatrick scale's lack of dark tone variance leads to bias in skin tone-related model performance.
- Evidence anchors:
  - [section 4.1]: "The Monk Skin Tone Scale [70], shown in Figure 3, was developed specifically for the computer vision use case. We intentionally use the Monk Skin Tone scale over the Fitzpatrick skin type [29], which was developed as means for determining one's likelihood of getting sunburn and lacks variance in darker skin tones [51, 72]."
  - [corpus]: Weak - corpus neighbors mention fairness but not specific skin tone scale choice.
- Break condition: If the Monk Scale is poorly calibrated or if annotators misapply it, the intended improvement in representation fails.

### Mechanism 3
- Claim: Mapping person-related classes to ImageNet-21k vocabulary ensures compatibility with widely used pre-trained models.
- Mechanism: By selecting 52 person-related classes that overlap with ImageNet-21k, FACET enables zero-shot or fine-tuning evaluation of models pre-trained on ImageNet-21k without additional training data requirements.
- Core assumption: ImageNet-21k is a common pre-training corpus, so class overlap maximizes model compatibility.
- Evidence anchors:
  - [section 3]: "To ensure consistency with existing concepts used for computer vision model evaluation, we require our categories to overlap with the taxonomy of the widely used ImageNet-21k (IN21k) dataset [79]."
  - [section 6.1]: "We also show how we can use the FACET IN21k class overlap to evaluate an ImageNet21k pre-trained ViT [26]."
  - [corpus]: Weak - corpus neighbors do not discuss ImageNet-21k class overlap.
- Break condition: If the ImageNet-21k classes used are not truly representative of the FACET categories, evaluation validity suffers.

## Foundational Learning

- Concept: Intersectionality in fairness analysis
  - Why needed here: To understand how multiple demographic attributes compound bias effects beyond single-attribute analysis.
  - Quick check question: Can you give an example of how gender and skin tone biases might interact differently than either alone?
- Concept: Annotation quality assurance in dataset construction
  - Why needed here: To ensure annotations are reliable enough for rigorous fairness evaluation, especially for subjective attributes like perceived skin tone.
  - Quick check question: Why is multi-review important for perceived skin tone annotations?
- Concept: Model fairness metrics (e.g., equality of opportunity)
  - Why needed here: To quantify performance disparities across demographic groups in a way that aligns with fairness research standards.
  - Quick check question: How does the recall disparity metric used here relate to equality of opportunity?

## Architecture Onboarding

- Component map: Image → Bounding box → Person class + 13 attributes → Evaluation pipeline (classification, detection, segmentation, visual grounding) → Fairness metric computation
- Critical path: Annotate → Evaluate model → Compute recall disparity → Analyze intersectional effects
- Design tradeoffs: Exhaustive annotations increase dataset size and cost but enable richer fairness analysis; using ImageNet-21k overlap limits class selection but ensures model compatibility.
- Failure signatures: High disagreement in perceived skin tone annotations; low mask quality for certain attributes; model performance not varying across attributes (may indicate annotation issues).
- First 3 experiments:
  1. Compute recall disparity for a baseline model across perceived gender only.
  2. Repeat disparity computation for perceived skin tone across all 10 MST values.
  3. Analyze intersectional disparities for a combination (e.g., perceived gender + hair type).

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the inclusion of intersectional attributes (e.g., skin tone + hair type) affect model performance compared to single-attribute analysis?
  - Basis in paper: [explicit] The paper explicitly evaluates intersectional attributes, e.g., "Do standard detection models struggle to detect people whose skin appears darker? Does this problem magnify when, for instance, the person has coily compared to straight hair?"
  - Why unresolved: While the paper shows disparities at intersections, it does not quantify the *magnitude* of these disparities compared to single attributes or explore all possible intersections systematically.
  - What evidence would resolve it: A comprehensive study comparing performance disparities for all intersectional combinations vs. single attributes, with statistical significance tests.

- **Open Question 2**: What is the impact of perceived gender presentation annotations on model bias, and how does it compare to self-reported gender labels?
  - Basis in paper: [explicit] The paper uses perceived gender presentation instead of self-reported gender due to limitations in determining gender from images, acknowledging this as a limitation.
  - Why unresolved: The paper does not compare the bias observed with perceived labels to what might be observed with self-reported labels, nor does it quantify the potential harm of misgendering.
  - What evidence would resolve it: A study comparing model bias using perceived vs. self-reported gender labels, if available, or a simulation of potential differences.

- **Open Question 3**: How do performance disparities vary across different vision tasks (classification, detection, segmentation, visual grounding) for the same demographic attributes?
  - Basis in paper: [explicit] The paper evaluates multiple tasks and notes differences, e.g., "Do performance discrepancies differ across the detection and segmentation task?"
  - Why unresolved: While the paper shows disparities for individual tasks, it does not provide a unified analysis of how disparities *correlate* across tasks or identify tasks where disparities are consistently larger or smaller.
  - What evidence would resolve it: A comparative analysis of disparity magnitudes across all tasks for each demographic attribute, with correlation analysis.

## Limitations

- Annotation subjectivity: The use of perceived attributes (gender, skin tone, age) introduces inherent subjectivity, particularly for attributes like "perceived skin tone" which rely on annotator interpretation rather than objective measurement.
- Single-attribute evaluation gaps: While intersectional analysis is well-supported, some evaluation combinations (like face-related classes with age/accessibility attributes) are excluded due to data limitations.
- Model architecture coverage: The benchmark evaluates several state-of-the-art models but does not comprehensively test all major vision architectures.

## Confidence

- **High confidence**: The core claim that FACET enables intersectional fairness analysis through exhaustive annotations is well-supported by the methodology and evidence provided.
- **Medium confidence**: Claims about the Monk Skin Tone Scale improving representation are supported by design rationale but lack empirical validation against alternative scales.
- **Medium confidence**: The claim about ImageNet-21k class overlap ensuring model compatibility is reasonable but not empirically tested across diverse model architectures.

## Next Checks

1. **Cross-annotator validation**: Re-annotate a subset of FACET images with a different annotator pool to measure inter-rater reliability for subjective attributes like perceived skin tone and gender.

2. **Broader model evaluation**: Test additional vision model architectures (e.g., ConvNeXt, Swin Transformer) on FACET to assess whether performance disparities observed with the current models generalize.

3. **Intersectional effect isolation**: Conduct controlled experiments isolating individual attribute contributions within intersectional groups to determine which attribute combinations drive the largest performance disparities.