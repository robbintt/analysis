---
ver: rpa2
title: 'WorldSense: A Synthetic Benchmark for Grounded Reasoning in Large Language
  Models'
arxiv_id: '2311.15930'
source_url: https://arxiv.org/abs/2311.15930
tags:
- problem
- world
- worldsense
- problems
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces WorldSense, a synthetic benchmark to evaluate\
  \ large language models\u2019 (LLMs) ability to build and use tacit world models.\
  \ WorldSense contains three problem types\u2014grounded inferences, consistency\
  \ detection, and completeness detection\u2014that test models\u2019 ability to infer\
  \ spatial, temporal, and scalar relationships from textual descriptions."
---

# WorldSense: A Synthetic Benchmark for Grounded Reasoning in Large Language Models

## Quick Facts
- arXiv ID: 2311.15930
- Source URL: https://arxiv.org/abs/2311.15930
- Reference count: 20
- Key outcome: WorldSense benchmark reveals LLMs struggle with building tacit world models, showing limited success on consistency/completeness detection and strong response biases despite decorrelated problem design.

## Executive Summary
WorldSense is a synthetic benchmark designed to evaluate large language models' ability to construct and use tacit world models by inferring spatial, temporal, and scalar relationships from textual descriptions. The benchmark contains three problem types—grounded inferences, consistency detection, and completeness detection—with decorrelated problem structure to prevent lexical shortcut exploitation. Tested on GPT-3.5, GPT-4, and Llama2-chat, models showed significant limitations particularly in consistency and completeness detection, exhibited strong response biases, and demonstrated minimal improvement from advanced prompting strategies. While finetuning improved in-distribution performance, it failed to generalize beyond linear relationship constraints to non-linear reasoning or external benchmarks.

## Method Summary
The WorldSense benchmark generates abstract problem tuples containing world states, textual descriptions, and queries, which are then rendered into multiple textual skins to ensure vocabulary independence from problem structure. The benchmark includes trivial controls that directly present world states to isolate reasoning ability from world-state construction. Three model families (GPT-3.5, GPT-4, Llama2-chat) were evaluated using basic prompting, chain-of-thought prompting, and in-context learning with both in-distribution and out-of-domain examples. Accuracy and response bias metrics were calculated on a tuple-by-tuple basis, with chance level set at 50%.

## Key Results
- LLMs showed limited success on consistency and completeness detection problems, struggling to identify when descriptions cannot correspond to any possible world state
- All models exhibited strong response biases, preferring certain answers regardless of problem specifics
- Chain-of-thought and in-context learning provided minimal improvements (4-5 percentage points) on accuracy
- Finetuning on WorldSense problems improved in-distribution performance but failed to generalize to non-linear reasoning or external benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** WorldSense isolates tacit world model construction from translation artifacts by decorrelating problem structure from vocabulary and responses.
- **Mechanism:** Abstract program schemata are rendered into multiple textual skins, ensuring the correct answer is statistically independent from word choice. This forces the model to build a world representation rather than rely on lexical shortcuts.
- **Core assumption:** Models cannot exploit spurious correlations between surface form and correct answer because the benchmark ensures no such correlations exist.
- **Evidence anchors:**
  - [abstract]: "explicitly avoids bias by decorrelating the abstract structure of problems from the vocabulary and expressions, and by decorrelating all problem subparts with the correct response."
  - [section 3.1]: Describes generation of abstract tuples where ground truth is independent from description and query.
  - [corpus]: No related work specifically addresses bias decorrelation; this is a unique design feature of WorldSense.
- **Break condition:** If the model memorizes problem templates or learns to map specific word patterns to answers regardless of structure, the independence guarantee is violated.

### Mechanism 2
- **Claim:** Trivial controls separate world-state generation from reasoning, isolating the bottleneck in model performance.
- **Mechanism:** In trivial conditions, the description directly lists entities in canonical order, removing the need to infer spatial/temporal/scalar relationships. This isolates the reasoning step.
- **Core assumption:** The gap between trivial and normal conditions reflects the difficulty of building a tacit world state from natural language.
- **Evidence anchors:**
  - [abstract]: "We design 'trivial' controls that separate the ability to create a coherent representation of a described scene from the ability to perform the reasoning steps on the input text required to understand the scene."
  - [section 3.3]: Explains that trivial conditions present an iconic rendering of the world state, making generation trivial.
  - [corpus]: No external evidence; this is an internal design mechanism.
- **Break condition:** If the model can perform reasoning without building an internal representation (e.g., by pattern matching), the trivial-normal gap would not reflect world-model construction difficulty.

### Mechanism 3
- **Claim:** Finetuning on synthetic WorldSense data improves in-distribution performance but fails to generalize beyond linear relationship constraints.
- **Mechanism:** Training on novel instances of WorldSense problems with linear order structures improves accuracy on similar problems, but does not transfer to non-linear or external benchmarks.
- **Core assumption:** The model learns a narrow inductive bias for linear transitivity without acquiring broader reasoning capabilities.
- **Evidence anchors:**
  - [abstract]: "we show that while finetuning on similar problems does result in substantial improvements – within- and out-of-distribution – the finetuned models do not generalise beyond a constraint problem space."
  - [section 5.4]: Reports finetuning results showing improvement on WorldSense but not on external reasoning benchmarks.
  - [corpus]: No external evidence of generalization failure; this is an empirical finding from the paper.
- **Break condition:** If finetuning on linear problems led to improved performance on non-linear reasoning tasks, the mechanism would be invalidated.

## Foundational Learning

- **Concept:** Decorrelation of problem structure from vocabulary and responses
  - **Why needed here:** Prevents models from exploiting lexical shortcuts and ensures evaluation of true world-model construction.
  - **Quick check question:** If a model achieves high accuracy by memorizing word-answer pairs, does this indicate genuine world understanding?

- **Concept:** Epistemic distinction between verbal descriptions and world states
  - **Why needed here:** Consistency and completeness problems test whether models can detect when descriptions cannot correspond to any possible world state.
  - **Quick check question:** How can a model determine that a description is inconsistent without building an internal representation of the described world?

- **Concept:** Linear vs. non-linear relationship reasoning
  - **Why needed here:** WorldSense focuses on linear order relationships to isolate a specific reasoning capability; understanding this limitation is crucial for interpreting results.
  - **Quick check question:** Why might a model that excels at linear transitive inference still fail at reasoning about parallel or perpendicular relationships?

## Architecture Onboarding

- **Component map:** Problem generator (abstract schemata) → Text renderer (textual skins) → Prompt formatter → Model interface → Evaluator (response normalization)
- **Critical path:** Generate abstract problem tuple → Render description/query using chosen skin → Format prompt with acceptable answers → Send prompt to model → Normalize response and compare to ground truth → Aggregate accuracy and bias
- **Design tradeoffs:** Multiple skins increase robustness but add generation complexity; trivial controls isolate capabilities but may not reflect real-world usage; synthetic data ensures bias control but may miss naturalistic patterns
- **Failure signatures:** High response bias indicates systematic preference for certain answers; trivial-normal gap reveals difficulty in world-state construction; finetuning improvement without external generalization suggests narrow learning
- **First 3 experiments:** 1) Run baseline evaluation on GPT-4 with all problem types and conditions; 2) Test chain-of-thought prompting impact on GPT-3.5 and Llama2; 3) Finetune Llama2 on 100K WorldSense examples and test on in/out-of-domain sets

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the exact mechanism by which LLMs fail to construct and maintain internal world models from textual descriptions?
- **Basis in paper:** [explicit] The paper demonstrates that LLMs struggle with translating language into internal world states, particularly for consistency and completeness detection problems.
- **Why unresolved:** The paper shows the failure but doesn't deeply investigate the underlying causes - whether it's due to architectural limitations, training data patterns, or fundamental inability to represent continuous world states.
- **What evidence would resolve it:** Systematic ablation studies varying model architecture, training objectives, and input representations, combined with interpretability analysis of internal representations during problem solving.

### Open Question 2
- **Question:** What is the precise nature of the "response bias" observed in LLMs, and can it be systematically characterized and predicted?
- **Basis in paper:** [explicit] All tested models showed substantial response biases, preferring certain answers regardless of problem specifics, with different patterns across models and problem types.
- **Why unresolved:** The paper identifies the bias exists but doesn't explain its origins or develop predictive models for when and why it occurs.
- **What evidence would resolve it:** Large-scale analysis of bias patterns across diverse problem types and models, combined with statistical characterization of bias-inducing features in problem descriptions.

### Open Question 3
- **Question:** Why do chain-of-thought and in-context learning strategies provide only marginal improvements for WorldSense problems, unlike their effectiveness in other reasoning tasks?
- **Basis in paper:** [explicit] The paper found that COT and ICL improved accuracy only slightly (4-5 percentage points) and didn't significantly reduce response bias.
- **Why unresolved:** The paper shows limited effectiveness but doesn't investigate whether this reflects a fundamental limitation of these strategies for world-modeling tasks versus implementation issues.
- **What evidence would resolve it:** Controlled experiments varying COT prompt structure, example quality, and problem complexity to identify specific conditions where these strategies fail for world-modeling tasks.

## Limitations
- Synthetic nature may not capture full complexity of real-world reasoning scenarios
- Benchmark focuses exclusively on linear transitive relationships, leaving uncertainty about generalization to non-linear reasoning
- Decorrelation mechanism relies on assumption that no spurious correlations exist between problem structure and vocabulary
- Finetuning improvements don't transfer to non-linear reasoning or external benchmarks

## Confidence

- **High confidence**: LLMs show limited success on consistency/completeness detection problems and response bias is a significant issue across models
- **Medium confidence**: Decorrelating problem structure from vocabulary effectively prevents lexical shortcut exploitation
- **Low confidence**: Finetuning reveals models have learned a "narrow inductive bias" rather than broader reasoning capabilities

## Next Checks
1. Evaluate WorldSense-trained models on established reasoning benchmarks that include non-linear relationships to directly test claimed generalization limits
2. Independently verify response bias calculations by running the same evaluation pipeline on a subset of WorldSense problems
3. Test whether more sophisticated prompting techniques beyond COT and ICL can reduce the trivial-normal gap