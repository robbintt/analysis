---
ver: rpa2
title: Learning to Transmit with Provable Guarantees in Wireless Federated Learning
arxiv_id: '2304.09329'
source_url: https://arxiv.org/abs/2304.09329
tags:
- power
- wireless
- allocation
- learning
- workers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses power allocation for federated learning (FL)
  over interference-limited wireless networks. The proposed solution, PDG (primal-dual
  graph convolutional power network), leverages graph neural networks to parameterize
  the power allocation policy and uses a primal-dual learning algorithm to optimize
  it under non-convex constraints.
---

# Learning to Transmit with Provable Guarantees in Wireless Federated Learning

## Quick Facts
- arXiv ID: 2304.09329
- Source URL: https://arxiv.org/abs/2304.09329
- Reference count: 40
- Key outcome: Proposes PDG (primal-dual graph convolutional power network) that leverages GCNs and primal-dual learning to optimize power allocation for federated learning over wireless networks, achieving better performance than heuristic baselines.

## Executive Summary
This paper addresses power allocation for federated learning (FL) over interference-limited wireless networks. The authors propose PDG, a primal-dual graph convolutional power network that parameterizes the power allocation policy using graph neural networks and optimizes it via a primal-dual learning algorithm. They prove that the formulated problem has zero duality gap under mild conditions and show that the solution's optimality depends on the expressiveness of the parameterization. The approach is evaluated on both i.i.d. and non-i.i.d. data, demonstrating superior performance compared to heuristic and topology-agnostic baselines in terms of wireless transmission performance and FL learning accuracy.

## Method Summary
The method formulates power allocation for FL as a constrained optimization problem that maximizes a weighted sum of packet success rates, minimum data rates, and energy efficiency. A graph convolutional network (GCN) parameterizes the power allocation policy, providing permutation equivariance and flexibility to varying network sizes. The primal-dual learning algorithm alternates between updating primal variables (power policy parameters, rate, efficiency) and dual variables (Lagrange multipliers) using gradient-based updates. The GCN architecture with appropriate depth and activation functions provides sufficient expressiveness while maintaining computational efficiency, and early stopping based on validation performance prevents overfitting.

## Key Results
- PDG consistently outperforms heuristic baselines (random allocation, orthogonal channel allocation) and topology-agnostic baselines (MLP-based allocation) in terms of wireless transmission performance and FL learning accuracy
- Achieves lower packet error rates and faster, more accurate convergence of the global FL model compared to baselines
- The zero duality gap property enables efficient optimization of the non-convex FL power allocation problem under mild conditions

## Why This Works (Mechanism)

### Mechanism 1
The primal-dual formulation with parameterized power policies achieves zero duality gap under mild conditions, allowing efficient optimization of a non-convex FL power allocation problem. The problem is reformulated as a maximization over both primal variables (power allocation, rate, efficiency) and dual variables (Lagrange multipliers). The GCN parameterization of the power policy makes the optimization tractable by reducing the infinite-dimensional function space to a finite-dimensional parameter space. The dual formulation enables handling of non-convex constraints through the primal-dual learning algorithm. Core assumption: The channel distribution is non-atomic, the utility function is concave and non-decreasing, and the feasible power policy space satisfies the structure defined in equation (12) where worker selection matches the optimal solution.

### Mechanism 2
The GCN-based power policy provides permutation equivariance and flexibility to varying network sizes, which are critical for real-world FL deployment. GCNs are inherently permutation equivariant because graph filters aggregate information from neighbors without depending on node ordering. The message-passing structure allows the network to handle different numbers of workers by simply adjusting the input graph size. The normalized adjacency matrix construction (equation 8) ensures consistent behavior across different network topologies. Core assumption: The wireless network can be represented as a graph where workers are nodes and interference relationships are edges, and the GCN can learn meaningful local aggregations from this structure.

### Mechanism 3
The primal-dual learning algorithm with appropriate step sizes and early stopping achieves stable convergence and good generalization performance on unseen channel realizations. The algorithm alternates between updating primal variables (power policy parameters, rate, efficiency) and dual variables (Lagrange multipliers) using gradient-based updates. The empirical expectations are approximated using batches of channel samples, and early stopping based on validation performance prevents overfitting. The GCN architecture with appropriate depth and activation functions provides sufficient expressiveness while maintaining computational efficiency. Core assumption: The channel distribution is stationary during training, the batch size is sufficient to approximate expectations, and the validation set is representative of test conditions.

## Foundational Learning

- Concept: Convex optimization and duality theory
  - Why needed here: The zero duality gap proof relies on convex analysis and the relationship between primal and dual problems
  - Quick check question: What conditions must hold for a non-convex problem to have zero duality gap?

- Concept: Graph neural networks and permutation equivariance
  - Why needed here: The GCN parameterization must preserve network structure properties regardless of worker ordering
  - Quick check question: How does the normalized adjacency matrix construction ensure permutation equivariance?

- Concept: Federated learning convergence and wireless channel models
  - Why needed here: Understanding how wireless conditions affect FL convergence is crucial for formulating the optimization problem
  - Quick check question: How does packet error rate impact the aggregation of local models in FL?

## Architecture Onboarding

- Component map: CSI matrix H (L×L) → GCN layers → Power allocation vector pψ(H; Θ) → Utility functions → Expectations → Lagrangian → Gradient updates → p

- Critical path: H → GCN → p → Utility functions → Expectations → Lagrangian → Gradient updates → p

- Design tradeoffs:
  - GCN depth vs. overfitting: Deeper networks can capture more complex relationships but may overfit
  - Batch size vs. computational cost: Larger batches provide better expectation estimates but increase memory requirements
  - Step sizes: Need to balance convergence speed with stability
  - Early stopping patience: Too short may underfit, too long may overfit

- Failure signatures:
  - Poor convergence: Check if step sizes are appropriate and if the problem is feasible
  - Overfitting: Monitor validation performance and adjust early stopping criteria
  - Constraint violations: Verify that the GCN output respects [0, Pmax] bounds
  - Sensitivity to initialization: Try different random seeds to check robustness

- First 3 experiments:
  1. Test GCN permutation equivariance by permuting worker indices and verifying identical power allocations
  2. Validate zero duality gap numerically by comparing primal and dual optimal values on small test problems
  3. Benchmark convergence speed against heuristic baselines (Rand, Orth) on synthetic channel data

## Open Questions the Paper Calls Out

### Open Question 1
What is the theoretical limit of the expressivity required in the graph neural network parameterization to guarantee zero duality gap in all wireless federated learning scenarios? While the paper establishes that expressivity matters, it does not quantify the minimum level of expressivity needed across different wireless network configurations or federated learning objectives.

### Open Question 2
How does the proposed primal-dual learning approach perform in dynamic wireless environments where channel distributions change during the federated learning process? Real-world wireless environments are often non-stationary, and the paper's assumption of a fixed channel distribution may not hold in practice.

### Open Question 3
What is the impact of heterogeneous worker participation patterns (workers joining/leaving) on the convergence and optimality of the federated learning process with the proposed power allocation? While the architecture can technically handle variable worker counts, the paper does not quantify how such dynamics affect the federated learning objectives.

## Limitations
- The zero duality gap proof relies on specific assumptions about channel distributions and utility functions that may not hold in all real-world scenarios
- The GCN architecture's ability to generalize across diverse network topologies and sizes remains to be thoroughly validated
- The primal-dual learning algorithm's convergence properties depend heavily on hyperparameter tuning and may be sensitive to the choice of step sizes and early stopping criteria

## Confidence
- Mechanism 1 (zero duality gap): Medium - Theoretical proof exists but depends on strong assumptions
- Mechanism 2 (GCN permutation equivariance): High - Well-established property of graph neural networks
- Mechanism 3 (primal-dual learning convergence): Medium - Algorithm is standard but performance depends on careful tuning

## Next Checks
1. **Duality gap verification**: Numerically compute the duality gap on small test problems with varying numbers of workers and channel conditions to validate the theoretical guarantees across the full range of expected operating conditions.
2. **Permutation equivariance test**: Systematically permute worker indices in multiple network configurations and verify that the PDG produces identical power allocations, checking for edge cases where this property might break.
3. **Generalization study**: Train PDG on networks with up to L=10 workers and test its performance on networks with L=15, L=20, and L=25 workers to assess its ability to generalize to larger network sizes beyond the training distribution.