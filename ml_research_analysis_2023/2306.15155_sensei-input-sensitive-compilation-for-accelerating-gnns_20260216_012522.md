---
ver: rpa2
title: 'SENSEi: Input-Sensitive Compilation for Accelerating GNNs'
arxiv_id: '2306.15155'
source_url: https://arxiv.org/abs/2306.15155
tags:
- graph
- sensei
- primitive
- embedding
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently accelerating
  Graph Neural Network (GNN) computations by leveraging input-sensitive compositions
  of sparse and dense matrix operations. The key insight is that the optimal choice
  between different sparse-dense primitive compositions depends on the input graph's
  sparsity pattern, GNN embedding sizes, and the target hardware.
---

# SENSEi: Input-Sensitive Compilation for Accelerating GNNs

## Quick Facts
- **arXiv ID**: 2306.15155
- **Source URL**: https://arxiv.org/abs/2306.15155
- **Reference count**: 37
- **Primary result**: SENSEi achieves up to 1.99Ã— and 5.69Ã— speedup for GCN and GAT respectively over DGL on GPUs by selecting optimal sparse-dense primitive compositions based on input graph characteristics.

## Executive Summary
This paper introduces SENSEi, a system that accelerates GNN computations by dynamically selecting optimal compositions of sparse and dense matrix operations based on input graph sparsity, embedding sizes, and hardware. The key insight is that the optimal choice between different primitive compositions depends on whether the computation is bottlenecked by sparse matrix multiplication (SpMM) or dense matrix multiplication (GEMM), which shifts with graph density and embedding dimensions. SENSEi uses a machine learning model (XGBoost) to predict the best composition at runtime, achieving significant speedups over the widely used Deep Graph Library (DGL) for GCN and GAT models on both CPUs and GPUs.

## Method Summary
SENSEi uses a data-driven approach to select the best composition of sparse and dense matrix operations for GNN computations. It trains separate XGBoost ranking models for GCN and GAT using offline profiling data collected across diverse graphs and embedding sizes. The models take graph features (density, degree distribution, nnz per node) and embedding sizes as input to rank primitive compositions (precompute vs dynamic normalization for GCN; reuse vs recompute for GAT). At runtime, graph features are extracted, fed to the model, and the predicted best composition is executed using DGL or an optimized CPU backend with sparse tiling and reordering.

## Key Results
- On GPUs, SENSEi achieves up to 1.99Ã— and 5.69Ã— speedup for GCN and GAT respectively over DGL.
- On CPUs, SENSEi achieves up to 1.40Ã— and 1.40Ã— speedup for GCN and GAT respectively over DGL.
- The system generalizes to other GNN variants and can be combined with other sparse optimizations like tiling for synergistic benefits.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The optimal dense-sparse primitive composition depends on the relative cost of SpMM vs GEMM for a given input graph and embedding size.
- Mechanism: For sparse graphs, SpMM is the bottleneck, so compositions minimizing SpMM (e.g., dynamic GCN normalization with additional GEMM) are better. For dense graphs, GEMM is the bottleneck, so compositions avoiding extra GEMM (e.g., precompute GCN normalization) are better.
- Core assumption: SpMM and GEMM have different performance characteristics that shift based on graph density and embedding size.
- Evidence anchors:
  - [abstract] "Different phases of GNN computations can be modeled using both dense and sparse matrix operations."
  - [section 3] "Depending on the graph sparsity pattern and the embedding sizes of GNN layers, the bottleneck of the GNN computation can shift between the sparse matrix and the dense matrix operations."
  - [corpus] Weak evidence. Neighbors discuss GPU kernel design, sampling, or sparsity but do not directly analyze SpMM/GEMM cost tradeoffs in the context of algebraic reassociations.
- Break condition: If hardware does not differentiate SpMM and GEMM costs (e.g., on a very fast sparse engine or very slow dense engine), the composition choice becomes irrelevant.

### Mechanism 2
- Claim: GAT performance depends on whether to reuse or recompute updated embeddings, and this choice is sensitive to the ratio of input to output embedding sizes and graph density.
- Mechanism: When input embedding size > output embedding size, reusing updated embeddings (reuse) saves SpMM time without much GEMM penalty. When input embedding size < output embedding size, recomputing embeddings (recompute) allows smaller SpMMs but adds GEMM cost; this is only profitable if the graph is relatively dense.
- Core assumption: The time saved by performing SpMM on the smaller embedding size outweighs the GEMM overhead of recomputation, conditional on graph density.
- Evidence anchors:
  - [abstract] "Different phases of GNN computations can be modeled using both dense and sparse matrix operations."
  - [section 3.2] "When ğ‘˜1 > ğ‘˜2 or ğ‘˜1 = ğ‘˜2 it is always beneficial to reuse the updated embeddings from the attention calculation stage. On the other hand, when ğ‘˜1 < ğ‘˜2, recomputation may be more beneficial."
  - [corpus] Weak evidence. Neighbors discuss kernel acceleration or sampling but do not analyze the reuse vs recompute decision based on embedding size ratios.
- Break condition: If embedding sizes are equal, the difference between reuse and recompute vanishes; if the graph is extremely sparse, GEMM overhead dominates regardless of embedding sizes.

### Mechanism 3
- Claim: SENSEi uses a learned ranking model (XGBoost) to predict the best composition without manual heuristics, because simple rules based on density or embedding sizes alone are insufficient.
- Mechanism: SENSEi collects profiling data across many graphs and embedding sizes, trains a ranking model on graph features (e.g., density, degree distribution, nnz per node) and hardware features, and uses the model to rank compositions at runtime. The ranking is more accurate than any single-metric rule.
- Core assumption: The relationship between graph features, embedding sizes, hardware, and optimal composition is too complex for handcrafted rules but learnable from data.
- Evidence anchors:
  - [abstract] "We developed SENSEi, a system that uses a data-driven adaptive strategy to select the best composition given the input graph and GNN embedding sizes."
  - [section 4.2] "SENSEi uses a tree-based XGBoost ranking model [...] that predicts the best composition for a given input graph, embedding sizes, setting of auxiliary optimizations, and the underlying hardware architecture."
  - [corpus] Weak evidence. Neighbors mention ML-based representation selection or optimization but do not combine graph features, embedding sizes, and hardware in a unified ranking model for GNN primitive composition.
- Break condition: If the training data is not representative of runtime workloads, the model will make incorrect predictions; if hardware characteristics change dramatically, the model may need retraining.

## Foundational Learning

- Concept: Sparse matrix multiplication (SpMM) and its generalized form (g-SpMM) with semi-ring operations.
  - Why needed here: GNN aggregation steps are modeled as SpMM; understanding the cost model and sparsity patterns is essential for predicting composition profitability.
  - Quick check question: In a g-SpMM, if the sparse input has 90% zeros, roughly what fraction of the dense matrix multiplications are actually performed?

- Concept: Generalized sampled dense-dense matrix multiplication (g-SDDMM) and its role in GCN normalization.
  - Why needed here: GCN normalization can be expressed as a g-SDDMM; knowing when this is cheaper than two GEMMs is key to selecting the precompute composition.
  - Quick check question: If the adjacency matrix is unweighted and all nnz entries are 1, what simplification occurs in the g-SDDMM computation for GCN normalization?

- Concept: Graph feature extraction and feature importance in machine learning models.
  - Why needed here: SENSEi uses handcrafted graph features (density, degree distribution, nnz per node) as input to its ranking model; understanding which features matter helps debug and improve the model.
  - Quick check question: If two graphs have the same density but one has high degree variance and the other low, how might this affect the optimal GNN composition?

## Architecture Onboarding

- Component map:
  - Graph feature extractor: computes handcrafted features from the input graph (nnz, density, degree distribution, etc.).
  - XGBoost ranking model: trained offline on profiling data; inputs graph features, embedding sizes, hardware config; outputs ranking of primitive compositions.
  - DGL execution backend: implements the two GCN compositions (precompute vs dynamic) and two GAT compositions (reuse vs recompute) as parameterized node/edge functions.
  - OPT execution backend: C++ implementation with sparse tiling and reordering; auto-tunes these before passing configuration to SENSEi.
  - Profiling collector: offline stage that runs all compositions on training graphs to gather runtimes.

- Critical path:
  1. Load graph â†’ compute features â†’ feed to XGBoost model â†’ get composition choice â†’ execute chosen composition in DGL/OPT.
  The most time-sensitive part is the model inference (microseconds) and graph feature extraction (milliseconds).

- Design tradeoffs:
  - Handcrafted features vs learned graph embeddings: handcrafted are scalable to very large graphs but may miss subtle patterns; learned embeddings could capture more but are expensive and don't scale to 10M+ node graphs.
  - Separate models per GNN variant vs unified model: separate models allow specialization but require more training data; unified model is simpler but may be less accurate.
  - Offline profiling vs online auto-tuning: offline profiling is fast at runtime but may miss dataset-specific quirks; online tuning adapts but adds overhead.

- Failure signatures:
  - Model predicts wrong composition â†’ visible as slowdown vs DGL baseline (up to ~1.36Ã— in evaluation).
  - Feature extraction fails on malformed graph â†’ model receives NaN/Inf â†’ runtime error or fallback to default.
  - XGBoost inference fails on GPU â†’ fallback to CPU inference (still fast, ~microseconds).

- First 3 experiments:
  1. Run SENSEi on a small dense graph (e.g., Reddit) with GCN embedding sizes 32â†’32; verify it selects dynamic composition and matches the expected speedup pattern.
  2. Run SENSEi on a sparse graph (e.g., asia_osm) with GCN embedding sizes 32â†’1024; verify it selects precompute composition and observes the larger speedup due to reduced GEMM overhead.
  3. Run SENSEi on a dense graph (e.g., mycielskian17) with GAT embedding sizes 32â†’256; verify it selects recompute composition and that the speedup is due to smaller SpMM cost outweighing GEMM overhead.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do SENSEi's input-sensitive compositions generalize to other GNN models beyond GCN and GAT?
- Basis in paper: [explicit] The paper mentions that the compositions can be generalized to other GNN models with normalization and edge-weight calculations.
- Why unresolved: The paper only evaluates the compositions on GCN and GAT models, and does not provide evidence of their effectiveness on other GNN models.
- What evidence would resolve it: Experiments evaluating the compositions on other GNN models with normalization and edge-weight calculations, such as GATv2, would provide evidence of their generalizability.

### Open Question 2
- Question: How do SENSEi's compositions interact with other approximate GNN optimization techniques, such as sampling?
- Basis in paper: [explicit] The paper mentions that SENSEi's techniques are orthogonal to sampling and can work alongside it.
- Why unresolved: The paper does not evaluate the interaction between SENSEi's compositions and sampling, and it is unclear how sampling might affect the effectiveness of the compositions.
- What evidence would resolve it: Experiments evaluating the performance of SENSEi's compositions with sampling on large graphs would provide evidence of their interaction.

### Open Question 3
- Question: How can SENSEi's learned model be improved to further reduce the overhead of the system?
- Basis in paper: [inferred] The paper mentions that SENSEi has an overhead of graph inspection and model decision-making, and that only a small portion of configurations result in an overhead higher than the iteration time.
- Why unresolved: The paper does not provide a detailed analysis of the overhead of the learned model, and it is unclear how it can be improved to further reduce the overhead.
- What evidence would resolve it: Experiments comparing the performance of SENSEi's learned model with different model architectures and hyperparameters would provide evidence of how it can be improved to reduce the overhead.

## Limitations
- The paper's empirical claims are limited by the focus on only GCN and GAT models, with no evaluation of how the input-sensitive compilation techniques generalize to other GNN variants like GraphSAGE or GIN.
- The XGBoost ranking model's performance may degrade if runtime workloads deviate significantly from the training data distribution.
- The study does not explore the sensitivity of results to changes in hardware characteristics (e.g., newer GPUs or CPUs), which could affect the learned cost models.

## Confidence

- **High confidence**: The mechanism by which sparse-dense primitive composition choice depends on relative SpMM vs GEMM costs for different graph densities and embedding sizes is well-supported by both theory and empirical results.
- **Medium confidence**: The claim that GAT performance is sensitive to reuse vs recompute decisions based on embedding size ratios and graph density is supported, but the interaction with hardware characteristics adds uncertainty.
- **Medium confidence**: The assertion that machine learning is necessary to predict optimal compositions, as opposed to handcrafted heuristics, is plausible given the complexity but relies on the representativeness of training data.

## Next Checks

1. **Generalization to Other GNN Models**: Evaluate SENSEi on additional GNN variants (e.g., GraphSAGE, GIN) to determine if the input-sensitive compilation approach generalizes beyond GCN and GAT.
2. **Robustness to Hardware Changes**: Retrain and test the XGBoost model on a different hardware platform (e.g., NVIDIA H100 GPU) to assess sensitivity to hardware characteristics.
3. **Edge Case Performance**: Systematically test SENSEi on graphs with extreme sparsity (e.g., <1% density) or very large embedding sizes to identify scenarios where predictions may fail or overhead dominates.