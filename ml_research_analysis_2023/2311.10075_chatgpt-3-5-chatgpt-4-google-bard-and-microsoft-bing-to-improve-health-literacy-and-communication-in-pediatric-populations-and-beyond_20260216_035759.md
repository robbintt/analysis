---
ver: rpa2
title: ChatGPT-3.5, ChatGPT-4, Google Bard, and Microsoft Bing to Improve Health Literacy
  and Communication in Pediatric Populations and Beyond
arxiv_id: '2311.10075'
source_url: https://arxiv.org/abs/2311.10075
tags:
- health
- level
- explain
- reading
- bard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study examined the ability of large language models (LLMs)
  to explain pediatric medical conditions at varying reading grade levels. Using 288
  conditions and 26 prompts, the authors tested ChatGPT-3.5, ChatGPT-4, Microsoft
  Bing, and Google Bard.
---

# ChatGPT-3.5, ChatGPT-4, Google Bard, and Microsoft Bing to Improve Health Literacy and Communication in Pediatric Populations and Beyond

## Quick Facts
- arXiv ID: 2311.10075
- Source URL: https://arxiv.org/abs/2311.10075
- Reference count: 0
- Large language models (LLMs) struggle to generate pediatric health content below a 6th-grade reading level, though they can adjust outputs for higher reading levels when prompted appropriately.

## Executive Summary
This study evaluated the ability of four large language models (LLMs) - ChatGPT-3.5, ChatGPT-4, Microsoft Bing, and Google Bard - to explain pediatric medical conditions at varying reading grade levels. Using 288 conditions and 26 different prompts, researchers found that all models struggled to produce content below a 6th-grade reading level, though they could successfully generate explanations at higher reading levels when explicitly prompted. ChatGPT-3.5 and ChatGPT-4 demonstrated the widest range of output reading levels, while Bing and Bard consistently produced high school-level content regardless of prompt. The findings suggest potential for LLMs to improve health literacy and communication, particularly for pediatric populations and beyond, though significant limitations remain.

## Method Summary
The study tested 288 pediatric medical conditions using 26 different prompts across four LLM models. The prompts included basic explanations ("Explain {condition}", "What is {condition}") and grade-specific variations ("Explain {condition} at a __-grade reading level"). For each condition-prompt pair, the models' outputs were standardized by removing formatting and extraneous text, then analyzed using four readability indices (Gunning Fog, Flesch-Kincaid, Automated Readability Index, and Coleman-Liau) to calculate average reading grade levels. Due to rate limits, ChatGPT-4 was tested on a subset of 150 conditions rather than all 288.

## Key Results
- All LLMs struggled to generate content below a 6th-grade reading level regardless of prompt structure
- ChatGPT-3.5 produced outputs ranging from 7th-grade to college level; ChatGPT-4 ranged from 6th-grade to college-senior level
- Microsoft Bing consistently produced 9th-11th grade level content; Google Bard produced 7th-10th grade level content
- Google Bard failed to generate responses for 11-19 conditions depending on prompt type, indicating reliability issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models can adjust their output readability to target different reading grade levels (RGLs) above 6th grade.
- Mechanism: By specifying prompts that explicitly request explanations at a certain grade level (e.g., "Explain [condition] at a 7th-grade reading level"), the model can produce text with appropriate complexity metrics (Gunning Fog, Flesch-Kincaid, etc.) that match higher grade-level targets.
- Core assumption: The model's training data includes examples of text at varied reading levels, and the model can recognize and generate such text when prompted appropriately.
- Evidence anchors:
  - [abstract] "When prompts were specified to explain conditions from the 1st to 12th RGL, we found that LLMs had varying abilities to tailor responses based on RGL. ChatGPT-3.5 provided responses that ranged from the 7th-grade to college freshmen RGL while ChatGPT-4 outputted responses from the 6th-grade to the college-senior RGL."
  - [section] "When the context was changed to a specific reading level, 'Explain {} at a __-grade reading level'...ChatGPT-3.5 ranged output between the seventh- and twelfth-grade reading level. ChatGPT-4 varied output between the sixth-grade and college-senior reading level."
- Break condition: If the model lacks sufficient training data for the target RGL, or if the prompting strategy fails to convey the desired complexity constraint.

### Mechanism 2
- Claim: Different LLMs exhibit different ranges of output RGLs, with OpenAI models showing greater flexibility below 10th grade than Bing and Bard.
- Mechanism: The underlying model architectures and training data distributions influence each model's ability to generate text at various readability levels. OpenAI's models appear better tuned to produce simpler outputs.
- Core assumption: Training data preprocessing and model architecture differences account for the observed variation in RGL outputs.
- Evidence anchors:
  - [abstract] "ChatGPT-3.5 and ChatGPT-4 did better in achieving lower-grade level outputs. Meanwhile Bard and Bing tended to consistently produce an RGL that is at the high school level regardless of prompt."
  - [section] "Microsoft Bing provided responses from the 9th to 11th RGL while Google Bard provided responses from the 7th to 10th RGL."
- Break condition: If models receive updates or fine-tuning that alters their RGL output characteristics.

### Mechanism 3
- Claim: LLMs struggle to produce outputs below a 6th-grade reading level regardless of prompt structure.
- Mechanism: The model's training data and inherent complexity constraints prevent it from generating sufficiently simple text for early elementary levels.
- Core assumption: The model's architecture and training process impose a minimum complexity threshold.
- Evidence anchors:
  - [abstract] "LLMs face challenges in crafting outputs below a sixth-grade reading level."
  - [section] "All LLMs produced output for each condition tested except Google Bard...Google Bard particularly struggled with prompts 'What is' / 'What are' and 'Explain', failing to answer 11 and 19 conditions with the first query, respectively."
- Break condition: If future model versions are specifically fine-tuned on simpler text corpora.

## Foundational Learning

- Concept: Reading Grade Level (RGL) metrics
  - Why needed here: The study uses RGL as the primary outcome measurement to evaluate whether LLMs can tailor medical explanations to different reading abilities.
  - Quick check question: What four readability indices were averaged to calculate the average RGL (aRGL) in this study?

- Concept: Prompt engineering for readability
  - Why needed here: The study tested different prompt architectures to see which could best guide LLMs to produce outputs at target reading levels.
  - Quick check question: What were the two prompt architectures used beyond the basic "Explain" and "What is" prompts?

- Concept: Large language model limitations
  - Why needed here: Understanding model constraints (like rate limits and output complexity floors) is essential for interpreting the study's findings.
  - Quick check question: Why was only a subset of 150 conditions tested with ChatGPT-4 instead of all 288?

## Architecture Onboarding

- Component map: Prompt generation -> LLM API (ChatGPT-3.5, ChatGPT-4, Bing, Bard) -> Text cleaning -> Readability assessment -> Results storage
- Critical path: Generate prompt → Send to LLM → Receive output → Clean text → Calculate RGL metrics → Store results
- Design tradeoffs: Using multiple RGL indices provides a more robust assessment but increases computational overhead. Testing all conditions across all models would be ideal but is constrained by API rate limits.
- Failure signatures: Inability to generate output (as with Bard on certain conditions), outputs consistently above target RGL regardless of prompt, or significant variance in word count without corresponding RGL changes.
- First 3 experiments:
  1. Test a basic "Explain [condition]" prompt across all models to establish baseline RGLs
  2. Test "Explain [condition] at a 6th-grade reading level" to identify the floor of model capabilities
  3. Test "Explain [condition] at a 12th-grade reading level" to verify upper range consistency across models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs be optimized to generate health content below the 6th-grade reading level without compromising accuracy and completeness?
- Basis in paper: [explicit] The paper states "LLMs face challenges in crafting outputs below a sixth-grade reading level" and notes this as a limitation.
- Why unresolved: The study only tested LLMs' ability to generate content down to the 1st-grade level, but found all models struggled to achieve reading levels below 6th grade. The study did not explore optimization techniques or model fine-tuning to overcome this limitation.
- What evidence would resolve it: Comparative studies testing various prompt engineering strategies, model fine-tuning approaches, or hybrid human-AI systems to determine if LLMs can reliably generate accurate health content below the 6th-grade level.

### Open Question 2
- Question: Does the cautious approach of some LLMs (like Google Bard's hesitancy to answer certain health queries) improve overall health communication outcomes compared to more permissive models?
- Basis in paper: [explicit] The paper notes that "Bard's hesitancy in providing certain outputs indicates a cautious approach towards health information" and suggests this might reflect developers' intent to avoid potential misinformation.
- Why unresolved: The study observed this behavior but did not measure whether this cautious approach actually led to better health outcomes, reduced misinformation, or improved user trust compared to less restrictive models.
- What evidence would resolve it: Randomized controlled trials comparing user comprehension, satisfaction, and health outcomes when using cautious versus less cautious LLMs for health information queries.

### Open Question 3
- Question: How does the accuracy and completeness of LLM-generated health information compare to information provided by healthcare professionals?
- Basis in paper: [inferred] The paper states "Ensuring accuracy and relevance while mitigating the risk of misinformation remains a critical challenge for LLM deployment in the healthcare sector" and suggests future studies should "validate the accuracy, completeness, and functionality of LLMs within this context."
- Why unresolved: The study focused exclusively on readability levels and did not assess the factual accuracy or completeness of the health information generated by the LLMs compared to standard medical information.
- What evidence would resolve it: Systematic comparison of LLM outputs against peer-reviewed medical literature or expert-generated content, measuring both accuracy rates and information completeness across various pediatric health topics.

## Limitations
- All models struggled to produce content below a 6th-grade reading level, limiting applicability for early childhood health literacy
- Rate limits prevented testing all 288 conditions with ChatGPT-4, introducing potential sampling bias
- Google Bard failed to generate responses for 11-19 conditions depending on prompt type, raising reliability concerns

## Confidence
- **High Confidence**: LLMs can adjust output readability above 6th grade when prompted with grade-specific instructions; ChatGPT-3.5 and ChatGPT-4 demonstrate broader RGL ranges than Bing and Bard.
- **Medium Confidence**: The inability to generate below 6th-grade content represents a fundamental limitation of current LLMs for pediatric health literacy.
- **Low Confidence**: Direct comparisons of model performance across all 288 conditions due to incomplete testing of ChatGPT-4.

## Next Checks
1. Test whether fine-tuning LLMs on simplified medical texts can lower the minimum achievable RGL below 6th grade for pediatric conditions.
2. Conduct human readability assessments with actual pediatric populations to validate whether automated RGL metrics align with comprehension outcomes.
3. Evaluate whether prompt engineering strategies beyond grade-level specifications (e.g., vocabulary constraints, sentence length limits) can further reduce output complexity.