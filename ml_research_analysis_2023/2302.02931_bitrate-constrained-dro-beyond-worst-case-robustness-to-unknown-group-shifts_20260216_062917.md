---
ver: rpa2
title: 'Bitrate-Constrained DRO: Beyond Worst Case Robustness To Unknown Group Shifts'
arxiv_id: '2302.02931'
source_url: https://arxiv.org/abs/2302.02931
tags:
- group
- adversary
- risk
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Bitrate-Constrained DRO (BR-DRO) is a robust training method for
  unknown group shifts that avoids requiring group annotations on training data. It
  assumes groups are simple and separable by low-bitrate features, and constrains
  the adversary to only upweight such groups.
---

# Bitrate-Constrained DRO: Beyond Worst Case Robustness To Unknown Group Shifts

## Quick Facts
- arXiv ID: 2302.02931
- Source URL: https://arxiv.org/abs/2302.02931
- Reference count: 40
- Primary result: BR-DRO matches Group DRO performance on worst-group accuracy without requiring group annotations, and is robust to label noise.

## Executive Summary
This paper introduces Bitrate-Constrained Distributionally Robust Optimization (BR-DRO), a method for robust training when group annotations are unavailable. BR-DRO assumes that group shifts are along simple, low-bitrate features and constrains the adversary to only upweight such groups. This approach avoids the pessimism of unconstrained CVaR DRO while matching the performance of Group DRO when group labels are available. The method is theoretically justified and empirically validated on multiple datasets with known spurious correlations.

## Method Summary
BR-DRO extends distributionally robust optimization by constraining the adversary's capacity to bitrate-constrained functions. The method jointly trains a learner (neural network) and an adversary (one-hidden-layer VIB or linear layer) to minimize a weighted loss where weights are determined by the adversary. The adversary is constrained via a KL divergence or l2 penalty to ensure it only upweights simple, low-bitrate groups. This constraint helps identify true minority groups while avoiding upweighting of mislabeled points, providing robustness to label noise.

## Key Results
- BR-DRO matches Group DRO's worst-group accuracy on Waterbirds, CelebA, and CivilComments without requiring group annotations
- BR-DRO outperforms unconstrained CVaR DRO and is robust to label noise
- BR-DRO's performance improves with stronger bitrate constraints and matches the best baselines (JTT) when properly tuned

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The bitrate constraint prevents the adversary from upweighting mislabeled examples, making BR-DRO robust to label noise.
- Mechanism: When training data contains mislabeled points, unconstrained adversaries like in CVaR DRO will upweight high-loss points, including these mislabeled examples. The bitrate constraint restricts the adversary to only upweight groups that can be identified by simple, low-bitrate features. Since randomly mislabeled points do not form such simple groups, they cannot be upweighted.
- Core assumption: The true group shift occurs along simple, low-bitrate features (e.g., background color), and contrived groups of mislabeled points are high-bitrate and thus not realizable under the constraint.
- Evidence anchors: [abstract] "BR-DRO is robust to label noise since it cannot form 'groups' from mislabeled points"; [section 4] "The bitrate constraint helps identify simple unfair minorities in G(P,Q0). Any method that aims to be robust on Q0 must up-weight data points from Gmin but without knowing its identity. Since the unconstrained adversary upsamples any group of data points with high loss and low probability, it cannot distinguish between a rare group that is realized by simple functions in W(γ*) and a rare group of examples that share no feature in common or may even be mislabeled."

### Mechanism 2
- Claim: Restricting the adversary's capacity to bitrate-constrained functions reduces pessimism compared to unconstrained CVaR DRO.
- Mechanism: Unconstrained CVaR DRO can produce overly conservative solutions by upweighting arbitrary high-loss points, including mislabeled examples. The bitrate constraint limits the adversary to functions that are simple and correspond to real groups in the data, avoiding upweighting contrived or mislabeled groups.
- Core assumption: The true target distribution Q0 differs from the source only in marginal probabilities over simple groups, and these groups are identifiable by low-bitrate features.
- Evidence anchors: [abstract] "This reduces pessimism compared to unconstrained CVaR DRO and matches Group DRO's performance when group labels are available."; [section 4] "The bitrate constraint helps identify simple unfair minorities in G(P,Q0). Any method that aims to be robust on Q0 must up-weight data points from Gmin but without knowing its identity. Since the unconstrained adversary upsamples any group of data points with high loss and low probability, it cannot distinguish between a rare group that is realized by simple functions in W(γ*) and a rare group of examples that share no feature in common or may even be mislabeled."

### Mechanism 3
- Claim: BR-DRO achieves competitive worst-group accuracy without requiring group annotations on training data.
- Mechanism: By assuming groups are simple and separable by low-bitrate features, BR-DRO can identify and upweight minority groups using only feature information and labels, without explicit group membership annotations. This allows it to match the performance of Group DRO, which requires group labels.
- Core assumption: The group shift is along simple features (e.g., background color) that are low-bitrate and identifiable by the adversary.
- Evidence anchors: [abstract] "BR-DRO does not require group information on training samples yet matches the performance of Group DRO on datasets that have training group annotations"; [section 6.1] "Table 1 compares the average and worst group accuracy for BR-DRO with ERM and four group shift robustness baselines: JTT, LtF, SUBY, and CVaR DRO. First, we see that unconstrained CVaR DRO underperforms other heuristic algorithms. This matches the observation made by Liu et al. [36]. Next, we see that adding bitrate constraints on the adversary via a KL term or l2 penalty significantly improves the performance of BR-DRO (VIB) or BR-DRO (l2), which now matches the best performing baseline (JTT)."

## Foundational Learning

- Concept: Distributionally Robust Optimization (DRO)
  - Why needed here: BR-DRO is a specific form of DRO that optimizes for worst-case performance under a bitrate-constrained uncertainty set.
  - Quick check question: What is the difference between standard DRO and BR-DRO in terms of the uncertainty set?

- Concept: f-divergence and its role in DRO
  - Why needed here: The bitrate constraint in BR-DRO is defined in terms of KL divergence (a type of f-divergence) between the adversary's distribution and a prior.
  - Quick check question: How does the KL divergence constraint in BR-DRO differ from the f-divergence constraint in standard DRO?

- Concept: Vapnik-Chervonenkis (VC) dimension and its use in generalization bounds
  - Why needed here: The paper uses VC dimension to bound the number of groups that can be identified by the bitrate-constrained adversary, which is used in convergence rate analysis.
  - Quick check question: How does the VC dimension of the bitrate-constrained class W(γ) relate to the bitrate parameter γ?

## Architecture Onboarding

- Component map: Input -> Learner (Neural Network) -> Features -> Adversary (VIB or Linear Layer) -> Weights -> Weighted Loss
- Critical path:
  1. Forward pass: Input → Learner → Features → Adversary → Weights
  2. Loss computation: Weighted loss using adversary weights
  3. Backward pass: Gradients flow to both learner and adversary
  4. Update: Both networks updated simultaneously
- Design tradeoffs:
  - VIB vs l2 constraint: VIB allows for more flexible bitrate control but adds complexity; l2 is simpler but may be less expressive
  - Strong vs weak bitrate constraint: Stronger constraint (lower γ) makes adversary more conservative but may miss some groups; weaker constraint risks upweighting mislabeled points
  - Threshold η: Controls which points are upweighted; too low may include noise, too high may miss minority points
- Failure signatures:
  - Poor worst-group accuracy: May indicate bitrate constraint is too strong or adversary is not identifying minority groups correctly
  - High sensitivity to label noise: May indicate bitrate constraint is too weak, allowing adversary to upweight mislabeled points
  - Slow convergence: May indicate bitrate constraint is too strong, limiting adversary's ability to identify groups
- First 3 experiments:
  1. Baseline comparison: Run BR-DRO on Waterbirds/CelebA with varying βvib/βl2 to see impact on worst-group accuracy
  2. Noise sensitivity: Add varying levels of label noise to Waterbirds/CelebA and compare BR-DRO vs CVaR DRO
  3. Bitrate analysis: Plot adversary's KL divergence or l2 norm during training to see how bitrate evolves and affects performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal strength of the bitrate constraint (γ) for BR-DRO in different practical settings?
- Basis in paper: [explicit] The paper shows that BR-DRO performance improves with increasing constraint on Waterbirds and CelebA, but under less restrictive capacity constraints its performance is similar to CVaR DRO. The paper also mentions that using group annotations on a small validation set does improve performance, but BR-DRO does better than prior works when tuned on just average validation.
- Why unresolved: The paper only explores a limited range of bitrate constraint strengths and does not provide a systematic method for selecting the optimal γ in different settings.
- What evidence would resolve it: Empirical studies comparing BR-DRO performance across a wider range of γ values on various datasets, including those with different group structures and noise levels, would help identify the optimal γ for different scenarios.

### Open Question 2
- Question: How does BR-DRO perform on more complex group structures that are not captured by simple low-bitrate features?
- Basis in paper: [inferred] The paper assumes that group shifts occur along low-bitrate features and that the true group function is simple. However, this assumption may not hold in all real-world scenarios where groups may be defined by complex combinations of features.
- Why unresolved: The paper only evaluates BR-DRO on datasets with known spurious correlations and simple group structures. It does not explore scenarios where groups are defined by more complex feature combinations.
- What evidence would resolve it: Empirical studies comparing BR-DRO performance on datasets with more complex group structures, such as those with multiple interacting spurious features or groups defined by non-linear combinations of features, would help assess the method's limitations.

### Open Question 3
- Question: Can BR-DRO be extended to handle other types of distribution shifts beyond group shifts, such as covariate shifts or label shifts?
- Basis in paper: [inferred] The paper focuses on group shifts and does not explicitly discuss other types of distribution shifts. However, the bitrate constraint approach could potentially be adapted to other shift types.
- Why unresolved: The paper does not explore extensions of BR-DRO to other shift types, and it is unclear how the bitrate constraint would need to be modified for different shifts.
- What evidence would resolve it: Theoretical analysis and empirical studies extending BR-DRO to handle other types of distribution shifts, such as covariate shifts or label shifts, would help determine the method's broader applicability.

## Limitations

- The bitrate constraint assumes groups are simple and separable by low-bitrate features, which may not hold for all real-world scenarios with complex group structures.
- The method's robustness to label noise depends on the assumption that mislabeled points do not form simple, low-bitrate groups, which may not always be true.
- The optimal strength of the bitrate constraint (γ) is not systematically determined and may require tuning for different datasets and settings.

## Confidence

- Claim: BR-DRO matches Group DRO's worst-group accuracy without requiring group annotations
  - Confidence: Medium
- Claim: BR-DRO is robust to label noise due to the bitrate constraint
  - Confidence: Medium
- Claim: The bitrate constraint reduces pessimism compared to unconstrained CVaR DRO
  - Confidence: High

## Next Checks

1. Conduct a systematic study of BR-DRO's performance across varying levels of label noise on Waterbirds and CelebA, comparing against CVaR DRO and other baselines to quantify the robustness advantage.

2. Analyze the learned adversary weights during training on noisy datasets to empirically verify that the bitrate constraint prevents upweighting of mislabeled points while correctly identifying true minority groups.

3. Evaluate BR-DRO on a dataset with known complex group shifts that cannot be identified by simple, low-bitrate features to test the limits of the bitrate constraint assumption.