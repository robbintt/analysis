---
ver: rpa2
title: Eliciting Kemeny Rankings
arxiv_id: '2312.11663'
source_url: https://arxiv.org/abs/2312.11663
tags:
- sampling
- kemeny
- instances
- replacement
- arms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper connects the dueling bandits and social choice frameworks
  to efficiently elicit voter preferences for finding a Kemeny ranking. The core method
  involves formulating the elicitation as a dueling bandits problem where arms correspond
  to alternatives and feedback comes from pairwise comparisons by randomly sampled
  voters.
---

# Eliciting Kemeny Rankings

## Quick Facts
- arXiv ID: 2312.11663
- Source URL: https://arxiv.org/abs/2312.11663
- Reference count: 40
- Primary result: Adaptive sampling strategies significantly reduce sample complexity for finding Kemeny rankings compared to uniform sampling

## Executive Summary
This paper formulates the problem of eliciting voter preferences for finding a Kemeny ranking as a dueling bandits problem. Arms correspond to alternatives and feedback comes from pairwise comparisons by randomly sampled voters. The authors develop PAC algorithms for Kemeny rankings under both sampling with and without replacement, providing approximation bounds based on confidence intervals over estimated winning probabilities. Experimental results show that adaptive sampling methods, particularly pessimistic, optimistic, and realistic look-aheads, significantly reduce the sample complexity compared to uniform sampling when pruning confidence intervals.

## Method Summary
The paper develops PAC algorithms for Kemeny rankings with respect to Kemeny scores under both sampling with and without replacement from the voter population. It provides approximation bounds based on confidence intervals over estimated winning probabilities and formulates several adaptive sampling strategies using look-aheads. The algorithms involve sampling pairs of arms, updating estimates of winning probabilities, and pruning confidence intervals based on properties of preference matrices. Experiments on synthetic data compare the sample complexity and approximation guarantees of different sampling methods.

## Key Results
- Adaptive sampling strategies (pessimistic, optimistic, realistic look-aheads) significantly reduce sample complexity compared to uniform sampling
- Sampling without replacement provides tighter confidence bounds than sampling with replacement by accounting for finite population size
- Confidence interval pruning based on preference matrix properties (completeness, triangle inequality) further reduces sample complexity
- Uniform and opportunistic sampling require significantly more samples to achieve the same approximation quality

## Why This Works (Mechanism)

### Mechanism 1: Sampling with replacement
- Claim: Sampling with replacement allows querying the same voter multiple times about the same pairwise comparison, maintaining statistical independence of samples
- Mechanism: Each query yields a Bernoulli trial with probability equal to the fraction of voters preferring one alternative over another. Hoeffding's inequality provides confidence bounds that shrink with the number of samples
- Core assumption: The population of voters is infinite or effectively infinite, so replacement doesn't significantly affect the underlying preference distribution
- Evidence anchors: [abstract] "sampling with replacement, i.e., sampling bandit feedback from voters u.a.r."; [section] "Consider a matrix Q ∈ Q (k) of winning probabilities between arms such that every sample (i, j) of two arms returns i ≻ j with probability qij."
- Break condition: When the voter population is small relative to the number of samples, the independence assumption breaks down, leading to overconfident confidence bounds

### Mechanism 2: Sampling without replacement
- Claim: Sampling without replacement provides tighter confidence bounds by accounting for the finite population, reducing required sample complexity
- Mechanism: When sampling without replacement, the number of voters preferring one alternative follows a hypergeometric distribution rather than a binomial. This allows using tighter concentration inequalities that depend on both sample size and population size
- Core assumption: The voter population is known and fixed, and no voter is queried twice about the same pair of alternatives
- Evidence anchors: [abstract] "sampling without replacement, meaning that voters cannot be asked multiple times about the same pair of alternatives"; [section] "Suppose that, while sampling voters uniformly at random, none of the voters v ∈ N can be asked twice about their preferences over a pair of arms"
- Break condition: If the population size is unknown or if voters can be queried multiple times about the same pair, the hypergeometric model and its corresponding confidence bounds become invalid

### Mechanism 3: Pruning confidence intervals
- Claim: Pruning confidence intervals based on preference matrix properties allows more efficient adaptive sampling strategies
- Mechanism: After each sample, confidence intervals can be tightened by enforcing that q_ij + q_ji = 1 and that for any three alternatives, the triangle inequality holds. This reduces the uncertainty in the matrix, allowing faster convergence to a good approximation
- Core assumption: The underlying preference matrix is realizable from a set of linear orders, meaning it satisfies the structural properties of preference matrices
- Evidence anchors: [abstract] "Furthermore, if all agents' preferences are strict rankings over the alternatives, we provide means to prune confidence intervals and thereby guide a more efficient elicitation"; [section] "Lemma 1 also implies that for a preference matrix Q ∈ P(k, n) a triangle inequality must hold for all triples of arms: qlj + qji ≥ qli for all i, j, l ∈ [k]"
- Break condition: If the preference matrix doesn't satisfy the structural properties (e.g., non-transitive preferences), pruning based on these properties could remove valid uncertainty intervals

## Foundational Learning

- Concept: Kemeny ranking and Kemeny score
  - Why needed here: The goal is to find a ranking that minimizes the sum of Kendall tau distances to all voter preferences, which is the Kemeny ranking
  - Quick check question: What is the difference between a Kemeny ranking and a Condorcet winner?

- Concept: Kendall tau distance
  - Why needed here: The Kemeny score is defined as the sum of Kendall tau distances between the output ranking and each voter's preference
  - Quick check question: How does Kendall tau distance relate to the number of pairwise disagreements between two rankings?

- Concept: Dueling bandits problem formulation
  - Why needed here: The elicitation problem is formulated as a dueling bandits problem where arms correspond to alternatives and feedback is pairwise comparisons
  - Quick check question: How does the dueling bandits framework differ from standard multi-armed bandits in terms of feedback structure?

## Architecture Onboarding

- Component map: Preference matrix estimator -> Confidence interval pruner -> Sampling strategy selector -> Kemeny ranking calculator -> Approximation bound calculator

- Critical path:
  1. Initialize all confidence intervals to [0, 1]
  2. While approximation bound > desired threshold:
     a. Select pair of arms using sampling strategy
     b. Query voter(s) for pairwise comparison
     c. Update sample average and confidence bounds
     d. Prune confidence intervals using structural properties
     e. Compute Kemeny ranking and approximation bound
  3. Return Kemeny ranking

- Design tradeoffs:
  - Sampling with replacement vs. without replacement: Simpler implementation vs. tighter confidence bounds
  - Uniform vs. adaptive sampling: Lower computational complexity vs. potentially faster convergence
  - Pruning confidence intervals vs. not pruning: More complex implementation but potentially fewer samples needed

- Failure signatures:
  - Algorithm gets stuck sampling the same pairs: Indicates pruning is too aggressive or adaptive strategy is not effective
  - Confidence bounds not shrinking: Could indicate sampling with replacement on a small population or implementation error
  - Kemeny ranking oscillates: Might suggest numerical instability in the Kemeny ranking computation

- First 3 experiments:
  1. Implement uniform sampling with replacement on synthetic data with known preference matrix; verify confidence bounds shrink as expected
  2. Add confidence interval pruning; verify that structural properties are maintained and bounds tighten further
  3. Implement sampling without replacement; compare sample complexity to sampling with replacement on same instances

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- The structural assumption that preference matrices are realizable from linear orders may not hold in real-world data, potentially limiting the effectiveness of confidence interval pruning
- The computational complexity of adaptive sampling strategies, particularly optimistic and pessimistic look-aheads, could be prohibitive for large numbers of alternatives
- The synthetic data generation methods are underspecified, making it difficult to reproduce the experimental results exactly

## Confidence
- High confidence: The basic dueling bandits formulation and PAC bounds under sampling with replacement are well-established and correctly applied
- Medium confidence: The sampling without replacement mechanism and confidence interval pruning are novel contributions that appear theoretically sound but lack extensive empirical validation
- Medium confidence: The experimental results show clear improvements from adaptive sampling, but the synthetic data generation methods are underspecified

## Next Checks
1. **Structural assumption validation**: Generate preference data with known violations of transitivity and test whether the pruning mechanism incorrectly eliminates valid uncertainty intervals, leading to overly confident but incorrect rankings

2. **Population size sensitivity**: Systematically vary the ratio of sample size to population size in the sampling without replacement experiments to quantify when the hypergeometric model breaks down and compare against the theoretical thresholds

3. **Computational scalability**: Measure the runtime of each adaptive sampling strategy as a function of the number of alternatives, particularly focusing on the look-ahead computations, to establish practical limits on problem size