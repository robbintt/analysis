---
ver: rpa2
title: Unsupervised Learning in Complex Systems
arxiv_id: '2307.10993'
source_url: https://arxiv.org/abs/2307.10993
tags:
- page
- systems
- complex
- learning
- cited
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis explores complex systems as a framework for studying
  unsupervised learning and adaptation in natural and artificial systems. The goal
  is to develop autonomous systems that can learn without supervision, evolve on their
  own, and become increasingly complex over time.
---

# Unsupervised Learning in Complex Systems

## Quick Facts
- arXiv ID: 2307.10993
- Source URL: https://arxiv.org/abs/2307.10993
- Authors: 
- Reference count: 0
- One-line primary result: Novel complexity metrics and reservoir computing approaches enable efficient learning in complex systems like cellular automata.

## Executive Summary
This thesis develops a framework for unsupervised learning in complex systems, focusing on cellular automata as a testbed. The work introduces novel complexity metrics based on neural network prediction, a coarse-graining method using frequency analysis and autoencoders, and a learning efficiency metric (WADE) for evaluating machine learning models. Key findings show that reservoir computing models learn faster than traditional supervised models, and that complexity metrics can effectively identify interesting cellular automata rules that correlate with human perception.

## Method Summary
The research employs cellular automata as a primary testbed, reformulating them as recurrent convolutional neural networks to apply machine learning techniques. Complexity metrics are developed using both compression-based methods and neural network predictors. A coarse-graining approach combines frequency analysis, clustering, and autoencoders to handle large-scale systems. The WADE metric measures learning efficiency across checkpoints, and benchmark datasets are created to compare reservoir computing models against standard approaches like RNNs and Transformers.

## Key Results
- Novel complexity metric based on neural network prediction correlates with human perception and identifies complex cellular automata rules
- Coarse-graining method using frequency analysis, clustering, and autoencoders preserves interesting behaviors in large-scale cellular automata
- Reservoir computing models demonstrate faster learning speeds than standard supervised models on benchmark tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cellular automata (CAs) can be reformulated as recurrent convolutional neural networks (RCNNs), enabling the application of machine learning techniques to their study and use.
- Mechanism: The CA update rule is expressed as a set of convolutional operations, where each cell's next state is determined by the states of its neighbors. This reformulation allows CAs to be treated as a special case of RNNs, inheriting properties like Turing completeness and the ability to learn through backpropagation.
- Core assumption: The CA update rule can be accurately represented as a CNN without losing essential computational properties.
- Evidence anchors:
  - [section]: "Viewing cellular automata as recurrent convolutional neural networks, as described above, has several interesting consequences... Turing-completeness of the system... Differentiable cellular automata"
  - [corpus]: Weak - no direct mention of this reformulation in corpus papers.
- Break condition: If the CNN representation fails to capture the full complexity of the CA update rule, the connection between CAs and RNNs breaks down, limiting the applicability of RCNN techniques.

### Mechanism 2
- Claim: Reservoir computing (RC) can harvest computations from complex dynamical systems like CAs to perform machine learning tasks efficiently.
- Mechanism: RC uses a complex dynamical system (the reservoir) to process input signals, and a simple linear regression (the decoder) to extract useful information from the reservoir's internal state. This allows for learning without explicit knowledge of the underlying system dynamics.
- Core assumption: The reservoir's internal state contains sufficient information to solve the target task after linear decoding.
- Evidence anchors:
  - [abstract]: "A coarse-graining method using frequency analysis, clustering, and autoencoders to study large-scale cellular automata while preserving interesting behaviors"
  - [section]: "Reservoir computing is a computational framework that aims to exploit the states of a complex dynamical system to perform some target task"
- Break condition: If the reservoir's dynamics are too chaotic or lack the echo-state property, the decoder will fail to extract meaningful information, rendering the RC system ineffective.

### Mechanism 3
- Claim: Complexity metrics based on prediction and compression can identify complex systems with interesting behaviors and growth potential.
- Mechanism: These metrics measure the "novelty" of a system's temporal states compared to a reference system. High complexity indicates systems that generate diverse and unpredictable patterns, suggesting potential for open-ended evolution.
- Core assumption: The chosen complexity metric accurately reflects the human intuition of complexity and correlates with the system's potential for interesting behavior.
- Evidence anchors:
  - [abstract]: "A novel complexity metric based on neural network prediction that correlates with human perception of complexity and helps identify complex cellular automata rules"
  - [section]: "Compression has often been used as a measure of complexity... Lempel and Ziv have introduced... the now widely used Lempel-Ziv (LZ) algorithm as a method for measuring the complexity of a sequence"
- Break condition: If the complexity metric fails to capture the essential features of complexity or produces inconsistent results across different systems, it becomes unreliable for identifying interesting complex systems.

## Foundational Learning

- Concept: Cellular Automata (CAs)
  - Why needed here: CAs are the primary complex system studied in the thesis, serving as a model for understanding learning and adaptation in complex systems.
  - Quick check question: What are the key components of a CA, and how does its update rule work?

- Concept: Reservoir Computing (RC)
  - Why needed here: RC is a crucial framework for harnessing the computations within complex systems like CAs for machine learning tasks.
  - Quick check question: How does RC differ from traditional machine learning approaches, and what are its key advantages?

- Concept: Complexity Metrics
  - Why needed here: Complexity metrics are essential for identifying and evaluating complex systems with interesting behaviors and growth potential.
  - Quick check question: What are the different approaches to measuring complexity, and how do they relate to the system's potential for open-ended evolution?

## Architecture Onboarding

- Component map: Cellular Automata (CAs) -> Reservoir Computing (RC) -> Complexity Metrics -> Coarse-graining Methods -> Learning Efficiency Metric (WADE)
- Critical path:
  1. Define and understand CAs and their properties.
  2. Explore the connection between CAs and RNNs, enabling the application of RC techniques.
  3. Develop and evaluate complexity metrics for identifying interesting complex systems.
  4. Apply coarse-graining methods to handle large-scale systems.
  5. Use RC and WADE to measure learning efficiency in complex systems.

- Design tradeoffs:
  - Simplicity vs. Expressivity: Choosing a CA variant with the right balance of simplicity and expressivity for the target application.
  - Computational Cost vs. Accuracy: Balancing the computational cost of RC and complexity metrics with their accuracy in identifying interesting systems.
  - Granularity vs. Information Loss: Choosing the right level of coarse-graining to preserve essential information while reducing system size.

- Failure signatures:
  - Inability to identify interesting complex systems: Indicates issues with complexity metrics or their application.
  - Poor performance of RC systems: Suggests problems with the reservoir's dynamics or the decoder's ability to extract useful information.
  - Inability to handle large-scale systems: Points to limitations in coarse-graining methods or computational resources.

- First 3 experiments:
  1. Implement a simple CA and apply RC to perform a basic machine learning task.
  2. Develop and evaluate a complexity metric on a dataset of CAs with varying behaviors.
  3. Apply coarse-graining to a large-scale CA and assess the preservation of interesting behaviors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we create a metric for complexity that correlates with human perception and helps identify complex cellular automata rules?
- Basis in paper: [explicit] The thesis develops a general complexity metric based on neural network prediction that correlates with human perception of complexity and helps identify complex cellular automata rules.
- Why unresolved: While the metric shows promise, its effectiveness across different types of complex systems and its ability to capture all aspects of human-perceived complexity may be limited.
- What evidence would resolve it: Extensive testing of the metric on a wide range of complex systems, including those beyond cellular automata, and comparison with human evaluations across diverse complexity measures.

### Open Question 2
- Question: How can we develop a coarse-graining method that preserves interesting behaviors while studying large-scale cellular automata?
- Basis in paper: [explicit] The thesis introduces a coarse-graining method using frequency analysis, clustering, and autoencoders to study large-scale cellular automata while preserving interesting behaviors.
- Why unresolved: The effectiveness of the coarse-graining method may vary depending on the specific cellular automaton rules and the scale of the system. It's unclear how well it can capture all relevant behaviors across different scales.
- What evidence would resolve it: Application of the coarse-graining method to a diverse set of cellular automata rules and scales, along with quantitative measures of behavior preservation and computational efficiency.

### Open Question 3
- Question: How can we measure learning efficiency in complex systems and compare it to traditional machine learning models?
- Basis in paper: [explicit] The thesis develops a learning efficiency metric (WADE) and benchmark dataset to measure how quickly machine learning models learn, showing that reservoir computing models learn faster than standard supervised models.
- Why unresolved: The WADE metric and benchmark dataset may not capture all aspects of learning efficiency, and the comparison with traditional models may be limited to specific tasks or datasets.
- What evidence would resolve it: Application of the WADE metric and benchmark dataset to a wide range of tasks and datasets, along with comparison to various traditional machine learning models across different learning scenarios.

## Limitations

- The complexity metrics may not generalize beyond cellular automata to other types of complex systems.
- Coarse-graining methods' effectiveness in preserving all relevant behaviors across different scales and rules is not fully established.
- The WADE metric for learning efficiency may not capture all aspects of learning speed across different types of tasks and models.

## Confidence

- **High Confidence**: The connection between cellular automata and recurrent convolutional neural networks is well-established, with clear mathematical foundations and implementation details provided.
- **Medium Confidence**: The effectiveness of reservoir computing for harvesting computations from complex systems is demonstrated through experiments, but the generalizability to other complex systems beyond cellular automata is not fully explored.
- **Low Confidence**: The claim that the complexity metrics accurately reflect human perception of complexity across diverse systems is based on limited human validation studies and may not hold for all types of complex behaviors.

## Next Checks

1. **Generalization of Complexity Metrics**: Apply the proposed complexity metrics to a diverse set of complex systems beyond cellular automata (e.g., neural networks, natural systems) and validate against human perception of complexity in these new domains.

2. **Robustness of Coarse-graining Methods**: Test the coarse-graining techniques on larger-scale cellular automata and other complex systems to evaluate their ability to preserve all relevant behaviors and dynamics under different parameter settings.

3. **Broader Applicability of WADE Metric**: Apply the WADE learning efficiency metric to a wider range of machine learning models and tasks, including non-sequential data and unsupervised learning scenarios, to assess its robustness and generalizability.