---
ver: rpa2
title: Dynamic value alignment through preference aggregation of multiple objectives
arxiv_id: '2310.05871'
source_url: https://arxiv.org/abs/2310.05871
tags:
- objectives
- learning
- stops
- system
- preferences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of designing AI systems that can
  dynamically align with shifting human values, particularly in control systems where
  users cede some control to AI. The core idea is to use a multi-objective reinforcement
  learning approach where multiple Deep Q-Networks (DQNs) are trained for different
  objectives, and user preferences are aggregated through voting to determine action
  selection.
---

# Dynamic value alignment through preference aggregation of multiple objectives

## Quick Facts
- arXiv ID: 2310.05871
- Source URL: https://arxiv.org/abs/2310.05871
- Authors: 
- Reference count: 11
- Key outcome: Multi-objective reinforcement learning with voting aggregation achieves better performance than single-objective approaches in traffic control

## Executive Summary
This paper addresses the challenge of designing AI systems that can dynamically align with shifting human values, particularly in control systems where users cede some control to AI. The authors propose a novel approach using multi-objective reinforcement learning where multiple Deep Q-Networks (DQNs) are trained for different objectives, and user preferences are aggregated through voting to determine action selection. They evaluate their method on a simplified two-leg intersection traffic control problem, demonstrating that their approach effectively balances competing objectives while avoiding the "reward hacking" problem observed in single-objective settings.

## Method Summary
The method employs multiple single-objective DQNs trained for different reward functions (e.g., minimizing stops and minimizing wait times), with an integration layer that combines their Q-values based on user preference votes. At each decision point, only incoming drivers vote on their preferences, and these votes are aggregated using either proportional or majority voting schemes. The integration layer normalizes Q-values with softmax and weights them according to the vote proportions to compute final action values. This allows the system to dynamically adjust which objective takes precedence based on current user preferences without retraining models.

## Key Results
- The multi-objective approach achieves better overall performance across three metrics (speeds, stops, and waits) compared to single-objective DQNs
- Proportional voting scheme generally outperforms majority voting in balancing objectives
- The approach successfully avoids the "reward hacking" problem observed in single-objective settings
- In balanced demand scenarios, the method performs comparably to optimal single-objective approaches while maintaining flexibility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The integration layer enables smooth switching between conflicting objectives without catastrophic forgetting.
- Mechanism: By normalizing Q-values with softmax and weighting them by vote proportions, the integration layer computes a convex combination of objective-specific Q-values. This preserves information from all trained models while allowing preference-driven selection.
- Core assumption: Different objectives produce comparable Q-value scales after softmax normalization, and the softmax transformation is sufficient to reveal relative preference strength.
- Evidence anchors:
  - [abstract] "The proportional voting scheme generally outperforms majority voting, and the approach successfully avoids the 'reward hacking' problem observed in single-objective settings."
  - [section] "These normalized values we refer to as qA 1 , qA 2 for objective A. Since their scale will be in the range of 0 to 1, we are now able to compare qA 1 with qB 1 ."
- Break condition: If Q-values are on vastly different scales or if softmax collapses distinctions (e.g., all values very close), the weighting becomes ineffective.

### Mechanism 2
- Claim: Dynamic vote aggregation captures changing user preferences without retraining models.
- Mechanism: At each decision point, only incoming drivers vote, and their preferences update the weighting coefficients for the integration layer. This shifts the effective objective priority in real time.
- Core assumption: Voting population changes reflect meaningful preference shifts; the aggregation method (proportional or majority) is trusted by users.
- Evidence anchors:
  - [section] "At the decision time, only users that are allowed to vote at the given moment are polled—these are the users on the incoming lanes of the intersection."
  - [section] "These preferences need to then be aggregated and weights w for each of the possible preferences are created."
- Break condition: If voting is strategic or if users misrepresent preferences, the aggregation no longer reflects true values.

### Mechanism 3
- Claim: Training separate single-objective DQNs avoids reward hacking and provides modularity for adding new objectives.
- Mechanism: Each DQN learns a globally optimal policy for its reward function. Because objectives are trained separately, no single model can "game" a combined reward; instead, the integration layer blends policies based on votes.
- Core assumption: The action space is shared and the learned policies are stable across different objectives.
- Evidence anchors:
  - [abstract] "The approach successfully avoids the 'reward hacking' problem observed in single-objective settings."
  - [section] "We note that in the Low conditions the method performs worse than the Cobb–Doug, which achieves both better wait times and stops."
- Break condition: If objectives have incompatible action spaces or if new objectives require completely different dynamics, the DQN framework may not extend.

## Foundational Learning

- Concept: Deep Q-Learning fundamentals (state-action value function, Bellman equation, experience replay)
  - Why needed here: The system builds on DQN outputs; understanding how Q-values represent expected returns is essential to grasp integration.
  - Quick check question: What does Q(s,a) represent in a reinforcement learning context?

- Concept: Softmax normalization and convex combination
  - Why needed here: The integration layer uses softmax to normalize Q-values and then forms a weighted sum; this is key to interpreting how preferences blend objectives.
  - Quick check question: How does applying softmax to a vector of Q-values affect their relative magnitudes?

- Concept: Social choice theory basics (voting aggregation, majority vs. proportional)
  - Why needed here: The system's vote aggregation directly determines how preferences are weighted; knowing the properties of each method is crucial for understanding behavior.
  - Quick check question: What is the difference between majority and proportional voting in terms of how weights are assigned?

## Architecture Onboarding

- Component map:
  - Traffic simulator (CityFlow)
  - Multiple single-objective DQNs (one per reward)
  - Vote aggregation module (majority or proportional)
  - Integration layer (softmax + weighted sum)
  - RL agent controller (argmax over integrated Q-values)

- Critical path:
  1. Simulator generates state (lane occupancy)
  2. Each DQN outputs Q-values for available actions
  3. Softmax normalizes Q-values per objective
  4. Current voters express preferences
  5. Aggregation method produces weights
  6. Integration layer computes q' values
  7. Argmax selects action
  8. Simulator steps forward

- Design tradeoffs:
  - Modularity vs. training cost: Separate DQNs are easy to extend but require more training.
  - Vote granularity vs. complexity: Only two preferences simplify aggregation but limit expressiveness.
  - Softmax vs. other normalizations: Softmax is differentiable and interpretable but may lose distinctions if values are close.

- Failure signatures:
  - All weights near zero → integration layer outputs near-zero q' values → random action selection.
  - One objective's Q-values dominate → votes have little effect.
  - Softmax collapses distinctions → no preference discrimination.
  - Voting population imbalance → persistent bias toward one objective.

- First 3 experiments:
  1. Run single-objective DQNs separately to confirm "reward hacking" (e.g., Stops DQN never switches).
  2. Enable proportional voting with a 50/50 preference split; verify that performance balances both objectives.
  3. Switch to majority voting; compare trade-offs and confirm if performance degrades on minority preferences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed method scale with the number of objectives beyond two?
- Basis in paper: [explicit] The paper states "The main issue of MORL is defining how to combine diverse objectives based on their relative importance" and "In this work, we consider and compare two simple aggregation methods" (majority and proportional voting). It also notes "a clear extension would be to run experiments with three or more objectives."
- Why unresolved: The authors explicitly acknowledge this as an open question and limit their experiments to two objectives.
- What evidence would resolve it: Experiments testing the method with 3+ objectives, comparing performance metrics and convergence rates as the number of objectives increases.

### Open Question 2
- Question: What is the formal quantification of the alignment of the multi-objective method?
- Basis in paper: [explicit] The authors state "a more formal quantification of the alignment of our approach would be valuable" in the limitations section.
- Why unresolved: The authors acknowledge this limitation but do not provide a formal quantification in their current work.
- What evidence would resolve it: A mathematical framework or metric that quantifies the degree of alignment between the AI system's behavior and human values, validated through experiments.

### Open Question 3
- Question: How would the system perform if the users were modeled as learning agents who can change their preferences based on their experiences?
- Basis in paper: [explicit] The authors suggest "An interesting direction for future work is modeling the users of the system (vehicles) as learning agents, who can change their preferences depending on their experiences."
- Why unresolved: This represents a planned future direction that the authors have not yet explored.
- What evidence would resolve it: Simulations where user agents learn and adapt their preferences over time, measuring how this affects system performance and alignment compared to the current static preference model.

## Limitations

- The performance of the proposed method in domains beyond traffic control remains uncertain
- The system's robustness to strategic voting or preference misrepresentation is not evaluated
- The scalability to more than two objectives has not been tested, though identified as a clear extension

## Confidence

- **High**: The integration layer mechanism (softmax + weighted sum) is mathematically sound and well-defined
- **Medium**: The proportional voting scheme outperforms majority voting in the presented traffic domain
- **Low**: Generalization to other domains without reward hacking and claims about avoiding catastrophic forgetting

## Next Checks

1. Test the integration layer with artificially scaled Q-values to verify softmax normalization maintains meaningful relative comparisons across objectives
2. Evaluate performance when voting preferences are systematically manipulated or when voters misrepresent their true preferences
3. Apply the framework to a non-traffic domain (e.g., robotics or game playing) with three or more objectives to assess scalability and reward hacking avoidance