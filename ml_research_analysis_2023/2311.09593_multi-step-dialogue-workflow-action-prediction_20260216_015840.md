---
ver: rpa2
title: Multi-Step Dialogue Workflow Action Prediction
arxiv_id: '2311.09593'
source_url: https://arxiv.org/abs/2311.09593
tags:
- action
- actions
- multi-step
- graph
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of multi-step Action State Tracking
  (AST) in task-oriented dialogue systems. The goal is to predict multiple future
  workflow actions in a sequence to enable multi-turn automation and improve efficiency
  in applications like customer service.
---

# Multi-Step Dialogue Workflow Action Prediction

## Quick Facts
- arXiv ID: 2311.09593
- Source URL: https://arxiv.org/abs/2311.09593
- Authors: 
- Reference count: 8
- Primary result: Multi-step AST improves workflow automation by 20% over single-step prediction

## Executive Summary
This paper addresses multi-step Action State Tracking (AST) in task-oriented dialogue systems, proposing to predict multiple future workflow actions simultaneously rather than just the next action. The authors develop three modeling approaches: fine-tuning T5, few-shot in-context learning with LLMs, and zero-shot graph traversal. Experiments on ABCD and MultiWoz datasets demonstrate that multi-step prediction significantly outperforms single-step methods, with the fine-tuned model achieving the best results on most metrics including exact match accuracy, BLEU scores, and graph negative log-likelihood. The approach enables 20% more workflow action automation compared to single-step prediction, with downstream applications in dialogue success prediction and conversation automation.

## Method Summary
The paper proposes three approaches for multi-step AST: 1) Fine-tuning a T5 model to predict future workflow actions from conversation context using sequence-to-sequence training, 2) Few-shot in-context learning that retrieves similar examples and prompts LLMs like GPT-3.5 and GPT-4, and 3) Zero-shot graph traversal that constructs action sequence graphs from historical data without training. All approaches output directed graphs representing future action sequences, with rollouts used to capture uncertainty. The methods are evaluated on ABCD and MultiWoz datasets using exact match accuracy, cascading evaluation, BLEU, F1, and graph negative log-likelihood metrics.

## Key Results
- Multi-step prediction achieves 20% more workflow action automation compared to single-step prediction
- Fine-tuned T5 model outperforms prior work on most metrics including exact match accuracy and graph negative log-likelihood
- In-context learning with GPT-4 is competitive with fine-tuned model but incurs higher costs
- Graph traversal captures future action sequence uncertainty well but relies only on historical action counts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-step prediction improves automation by 20% over single-step prediction
- Mechanism: By predicting multiple future workflow actions at once, the system can automate entire conversation segments rather than just the next action, reducing human intervention
- Core assumption: The predicted multi-step action sequences are sufficiently accurate to allow automation without human oversight
- Evidence anchors:
  - [abstract]: "can increase automation of steps by 20% without requiring as much feedback from a human overseeing the system"
  - [section]: "multi-step prediction is able to automate an additional 20% of workflow actions, leading to 21% more utterances being automated, when compared to 1-step prediction"
  - [corpus]: Weak evidence - no directly related papers found on automation metrics in multi-step dialogue prediction
- Break condition: If predicted action sequences become too inaccurate with increased prediction horizon, automation rates will decrease as human intervention becomes necessary

### Mechanism 2
- Claim: Graph representations capture future uncertainty better than single action chains
- Mechanism: By representing possible future action sequences as weighted directed graphs, the model can encode branching possibilities based on potential customer responses, rather than committing to a single deterministic path
- Core assumption: Future customer responses follow predictable patterns that can be modeled through historical action transition frequencies
- Evidence anchors:
  - [abstract]: "graphs can represent this inherent uncertainty in dialogue by aggregating many possible chains together"
  - [section]: "Graph negative log likelihood measures how well our approaches capture future action sequence uncertainty"
  - [corpus]: Weak evidence - no directly related papers found on graph-based uncertainty modeling in multi-step dialogue prediction
- Break condition: If customer responses become highly unpredictable or the domain changes significantly, the graph-based approach may fail to capture true uncertainty

### Mechanism 3
- Claim: In-context learning with large language models can achieve competitive performance without fine-tuning
- Mechanism: By retrieving similar examples and prompting a powerful language model with the conversation context and retrieved examples, the model can generate plausible future action sequences
- Core assumption: The retrieved examples are relevant enough to guide the language model's predictions for the current conversation
- Evidence anchors:
  - [section]: "We show that with our multi-step models, we are able to achieve similar performance to the baselines on 1-step prediction, while also predicting the full sequence more accurately"
  - [section]: "In-context learning approach with a powerful LLM like GPT-4 is competitive with the fine-tuned multi-step model but is costly"
  - [corpus]: Weak evidence - no directly related papers found on in-context learning for multi-step dialogue action prediction
- Break condition: If the retrieval mechanism fails to find sufficiently relevant examples, or if the language model's context window is exceeded, performance will degrade significantly

## Foundational Learning

- Concept: Action State Tracking (AST)
  - Why needed here: Understanding the prior single-step AST problem is crucial for grasping how multi-step AST extends and improves upon it
  - Quick check question: What is the key difference between single-step AST and the proposed multi-step AST?

- Concept: Workflow-guided dialogue
  - Why needed here: The paper assumes familiarity with workflows as sequences of actions that comply with business policies, which is central to the problem formulation
  - Quick check question: How does the workflow concept differ from traditional dialogue state tracking?

- Concept: Graph-based modeling of sequences
  - Why needed here: The paper introduces graph traversal as a zero-shot approach, which requires understanding how to represent and traverse action sequence graphs
  - Quick check question: What advantage do graphs have over linear sequences for representing future action possibilities?

## Architecture Onboarding

- Component map: conversation context -> T5 model -> R rollouts -> graph construction (fine-tuned); conversation context -> retrieval -> prompt construction -> LLM -> R rollouts -> graph construction (in-context); conversation context -> graph traversal -> graph output (graph traversal)
- Critical path: For fine-tuned: context → T5 → R rollouts → graph construction. For in-context: context → retrieval → prompt → LLM → R rollouts → graph construction. For graph traversal: context → traversal → graph output
- Design tradeoffs: Fine-tuned model offers best performance but requires training data and infrastructure; in-context learning avoids training but is expensive and relies on external APIs; graph traversal is zero-shot but less accurate and doesn't use contextual information
- Failure signatures: Poor performance on Action Graph NLL indicates model isn't capturing uncertainty well; large gaps between Action EM and Action CE suggest model is under-generating; low automation percentages indicate predictions aren't accurate enough for practical use
- First 3 experiments:
  1. Compare the three approaches on a small subset of ABCD dataset using Action EM and CE metrics
  2. Test effect of varying rollout count (R) on fine-tuned model's performance
  3. Evaluate zero-shot graph traversal on held-out test set to measure effectiveness without training

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several implications arise from the work:

1. How does the performance of multi-step AST models change when applied to open-domain dialogues or dialogues from different domains outside customer service and task-oriented conversations?

2. How does the performance of multi-step AST models vary with the number of retrieved examples (K) in the in-context learning approach?

3. How does the performance of multi-step AST models change when using different types of retrieval models or embeddings for the in-context learning approach?

## Limitations

- Fine-tuned model's performance advantage comes at significant computational cost and requires substantial training data
- In-context learning approach relies on expensive API calls to large language models like GPT-4, making it impractical for production deployment
- Zero-shot graph traversal method ignores conversational context entirely and depends heavily on historical action frequencies
- Evaluation focuses primarily on controlled datasets without extensive real-world validation

## Confidence

- **High confidence**: Experimental methodology is sound and three modeling approaches are well-specified; ablation studies on rollout count provide convincing evidence for fine-tuned model's superiority
- **Medium confidence**: Claims about 20% automation improvement and competitiveness of in-context learning are supported but may be sensitive to dataset characteristics; graph-based uncertainty capture is theoretically compelling but lacks extensive empirical validation
- **Low confidence**: Practical deployment viability in high-volume customer service settings remains uncertain, particularly regarding cost-benefit tradeoff and generalization to domains with limited historical data

## Next Checks

1. Conduct detailed economic analysis comparing fine-tuned model's performance gains against computational costs and GPT-4 approach's API expenses in realistic customer service scenarios

2. Evaluate all three approaches on significantly different dialogue domain (e.g., medical appointment scheduling) to assess generalization capabilities

3. Implement controlled user study where human agents interact with each model's predictions, measuring agent satisfaction, error correction frequency, and overall conversation quality beyond action sequence accuracy