---
ver: rpa2
title: When Side-Channel Attacks Break the Black-Box Property of Embedded Artificial
  Intelligence
arxiv_id: '2311.14005'
source_url: https://arxiv.org/abs/2311.14005
tags:
- attack
- attacks
- logits
- adversarial
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel attack framework combining hardware
  and software attacks to extract logits from embedded deep neural networks (DNNs)
  in black-box settings. The authors exploit side-channel attacks to retrieve logits
  by targeting the softmax function, enabling gradient-free adversarial example generation
  using frameworks like ZOO.
---

# When Side-Channel Attacks Break the Black-Box Property of Embedded Artificial Intelligence

## Quick Facts
- arXiv ID: 2311.14005
- Source URL: https://arxiv.org/abs/2311.14005
- Reference count: 40
- Primary result: Side-channel attacks can extract logits from embedded DNNs, enabling black-box adversarial example generation with 100% transfer rate

## Executive Summary
This paper introduces a novel attack framework that combines hardware and software attacks to extract logits from embedded deep neural networks in black-box settings. By exploiting electromagnetic leakages during softmax function computation, the authors successfully recover logits without requiring prior knowledge of model parameters. These extracted logits enable gradient-free adversarial example generation using frameworks like ZOO, achieving 100% transfer rate with 7.54 L2 distortion on an 8-bit quantized DNN running on a microcontroller.

## Method Summary
The attack framework operates in two phases: profiling and exploitation. During profiling, an open device is used to collect electromagnetic traces with uniformly distributed logits by modifying the softmax function's source code. In the exploitation phase, traces are captured from the targeted device during softmax computation, and logit extraction techniques (template attacks or deep learning-based side-channel attacks) are applied to recover the logits. These extracted logits are then used in gradient-free adversarial attack frameworks like ZOO to generate adversarial examples without requiring gradient access to the model.

## Key Results
- Successful logit extraction from embedded DNNs using only 5 electromagnetic traces
- 100% transfer rate of adversarial examples in black-box settings
- 7.54 L2 distortion achieved for adversarial examples
- Attack works across different softmax implementations without restricting architectural assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Side-channel attacks can extract logits from the softmax function in embedded DNNs.
- Mechanism: The softmax computation involves loading and comparing logit values in assembly code, which leaks electromagnetic emissions. These emissions can be captured and analyzed to reconstruct the logits.
- Core assumption: The softmax function is implemented in a way that exposes logits to side-channel leakage (e.g., via load and compare instructions).
- Evidence anchors:
  - [abstract] "Our method combines hardware and software attacks, by performing a side-channel attack that exploits electromagnetic leakages to extract the logits for a given input"
  - [section] "Listing 2 defines the assembly code which characterizes the 'if'-condition involved in line 5 of Listing 1. The logit Z[i] stored into the Flash memory is loaded into register r3 (see line 5 in Listing 2). During the load instruction, a side-channel attack can be performed to extract information related to the targeted logit."
  - [corpus] Weak evidence - related papers focus on AES and general SCA but do not specifically address softmax leakage.
- Break condition: The softmax function is reimplemented to avoid loading and comparing logits in a way that leaks side-channel information, or countermeasures like noise injection are added.

### Mechanism 2
- Claim: Extracted logits enable gradient-free adversarial example generation in black-box settings.
- Mechanism: With access to logits (or probability vectors derived from them), frameworks like ZOO can approximate gradients using finite differences and optimize inputs to create adversarial examples.
- Core assumption: The attacker can successfully extract logits and use them to compute probability vectors for adversarial generation.
- Evidence anchors:
  - [abstract] "Our method combines hardware and software attacks... allowing an attacker to estimate the gradients and produce state-of-the-art adversarial examples"
  - [section] "Since this layer is used to transform, independently from the previous ones, the logits into probabilities, our attack is free from any restricting assumption on the architecture."
  - [corpus] Weak evidence - related papers discuss model extraction but not specifically using side-channel-extracted logits for adversarial generation.
- Break condition: The adversarial generation framework fails to converge with the extracted logits, or the attack requires more iterations than feasible.

### Mechanism 3
- Claim: Profiling attacks with uniform logit distribution improve side-channel attack effectiveness.
- Mechanism: The profiling phase uses an open device to collect traces with uniformly distributed logits, avoiding bias in the probability distributions used for attack.
- Core assumption: The attacker can modify the source code on the open device to force uniform logit distribution during profiling.
- Evidence anchors:
  - [section] "To solve this issue, we modified the source code of the NNOM softmax function to force its input (i.e., the logits) to follow a uniform distribution."
  - [section] "As expected, a non-uniform distribution is observed (see Figure 3a). To solve this issue, we modified the source code of the NNOM softmax function to force its input (i.e., the logits) to follow a uniform distribution."
  - [corpus] Weak evidence - related papers discuss profiling but not specifically uniform logit distribution for softmax attacks.
- Break condition: The attacker cannot modify the source code, or the targeted device uses a different softmax implementation that cannot be easily modified.

## Foundational Learning

- Concept: Deep Neural Networks and Softmax Function
  - Why needed here: Understanding how DNNs classify inputs and how the softmax function converts logits to probabilities is crucial for identifying the attack target and methodology.
  - Quick check question: What is the mathematical formula for the softmax function, and why is it a good target for side-channel attacks?

- Concept: Side-Channel Attacks and Electromagnetic Emanations
  - Why needed here: The attack relies on capturing and analyzing electromagnetic emissions from the hardware during softmax computation to extract sensitive information.
  - Quick check question: What types of physical characteristics can be exploited in side-channel attacks, and how do electromagnetic emissions relate to the softmax function's assembly code?

- Concept: Adversarial Examples and Gradient-Free Optimization
  - Why needed here: The ultimate goal is to generate adversarial examples, which requires understanding how to manipulate inputs using frameworks like ZOO that don't need gradient access.
  - Quick check question: How does the ZOO framework generate adversarial examples without access to gradients, and what role do logits play in this process?

## Architecture Onboarding

- Component map:
  - Target device: Embedded microcontroller running quantized DNN with NNOM framework
  - Attack setup: EM probe, amplifier, oscilloscope for capturing side-channel traces
  - Profiling phase: Open device for collecting traces with uniform logit distribution
  - Attack phase: Targeted device for extracting logits and generating adversarial examples
  - Generation framework: ZOO or similar gradient-free optimization tool

- Critical path:
  1. Profiling phase: Collect traces on open device with uniform logit distribution
  2. Attack phase: Capture traces on targeted device during softmax computation
  3. Logit extraction: Use template attacks, multinomial logistic regression, or DLSCA to recover logits
  4. Adversarial generation: Use extracted logits with ZOO to create adversarial examples

- Design tradeoffs:
  - Number of profiling traces vs. attack success rate
  - Complexity of logit extraction method (template vs. DLSCA) vs. performance
  - Quantization level vs. adversarial example effectiveness
  - Hardware setup complexity vs. signal quality

- Failure signatures:
  - Low SNR in captured traces
  - Poor success rate in logit extraction (especially for certain logits)
  - High number of iterations needed for adversarial example generation
  - Transfer rate below expected for white-box generated examples

- First 3 experiments:
  1. Capture EM traces on open device during softmax computation with varying input patterns to analyze SNR and identify leakage points
  2. Implement and test logit extraction using template attacks on captured traces to validate methodology
  3. Combine logit extraction with ZOO framework to generate adversarial examples and measure success rate and distortion

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are side-channel attacks on other popular deep learning frameworks (e.g., PyTorch, TensorFlow, FINN) compared to NNOM?
- Basis in paper: [explicit] The authors note that their attack methodology should translate to other frameworks and mention future work to evaluate them.
- Why unresolved: The paper only tested the NNOM framework due to practical constraints, leaving open questions about the generalizability of their results.
- What evidence would resolve it: Systematic testing of side-channel attacks on various deep learning frameworks, comparing success rates and attack complexities.

### Open Question 2
- Question: What is the impact of different hardware implementations (e.g., FPGAs, ASICs) on the effectiveness of side-channel attacks targeting logits extraction?
- Basis in paper: [inferred] The authors suggest investigating more optimized hardware implementations in future work, implying potential variations in vulnerability.
- Why unresolved: The study focused on a specific microcontroller, and hardware differences could significantly affect side-channel leakage patterns and attack success.
- What evidence would resolve it: Comparative analysis of side-channel attacks on different hardware platforms, measuring success rates and required resources.

### Open Question 3
- Question: How does the accuracy of logit extraction impact the effectiveness of adversarial example generation in black-box settings?
- Basis in paper: [explicit] The authors acknowledge that wrong logit estimations can impact adversarial example generation but leave the specific relationship unexplored.
- Why unresolved: The paper demonstrates successful logit extraction but doesn't quantify the relationship between extraction accuracy and adversarial example quality.
- What evidence would resolve it: Systematic experiments varying logit extraction accuracy and measuring corresponding adversarial example success rates and distortions.

## Limitations
- Effectiveness depends heavily on softmax function implementation and hardware characteristics
- Profiling phase requires ability to modify source code on open device, which may not be feasible in all scenarios
- EM measurement setup details are limited, affecting reproducibility across different hardware configurations

## Confidence
- Logit extraction mechanism: Medium - While the theoretical basis is sound, practical implementation details and success rates across different hardware/software configurations are not fully explored.
- Adversarial example generation: Medium - The integration with ZOO framework is demonstrated but limited to specific experimental conditions and one dataset.
- Profiling phase effectiveness: Low - The assumption that uniform logit distribution can be enforced through source code modification needs further validation across different implementations.

## Next Checks
1. Test logit extraction success rates across multiple hardware platforms with different softmax implementations to validate the universality of the attack approach.
2. Evaluate the attack's effectiveness against different types of neural network architectures (beyond denseNet) and quantization levels to assess robustness.
3. Conduct experiments to determine the minimum number of traces required for successful logit extraction under various SNR conditions to establish practical attack boundaries.