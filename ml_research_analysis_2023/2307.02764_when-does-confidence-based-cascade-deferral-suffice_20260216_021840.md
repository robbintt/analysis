---
ver: rpa2
title: When Does Confidence-Based Cascade Deferral Suffice?
arxiv_id: '2307.02764'
source_url: https://arxiv.org/abs/2307.02764
tags:
- deferral
- confidence-based
- learning
- post-hoc
- rule
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the conditions under which confidence-based
  deferral suffices for cascades of classifiers. The key problem is identifying when
  confidence-based deferral may underperform, and developing post-hoc deferral strategies
  to improve upon it.
---

# When Does Confidence-Based Cascade Deferral Suffice?

## Quick Facts
- arXiv ID: 2307.02764
- Source URL: https://arxiv.org/abs/2307.02764
- Authors: 
- Reference count: 40
- Key outcome: Post-hoc deferral rules significantly improve upon confidence-based deferral in specialist, label noise, and distribution shift settings

## Executive Summary
This paper investigates when confidence-based deferral is optimal for cascades of classifiers and when it underperforms. The authors derive the Bayes-optimal deferral rule, which reveals that confidence-based deferral is optimal when the downstream model has constant error probability, but can significantly underperform when the downstream model is a specialist, there is label noise, or there is distribution shift. To address these failure modes, the authors propose post-hoc deferral rules that learn to predict when the first model is wrong but the second model is right, using features from the first model's probability outputs. Experimental results on ImageNet and CIFAR-100 datasets demonstrate that post-hoc deferral can achieve up to 5% accuracy improvement over confidence-based deferral at low deferral rates in the identified failure modes.

## Method Summary
The core method involves deriving the Bayes-optimal deferral rule for cascade classifiers and identifying conditions under which confidence-based deferral is optimal. Post-hoc deferral rules are then developed to mimic the optimal rule, using features from the first model's probability outputs. The post-hoc model (g(x)) is trained on held-out validation data to predict deferral decisions based on objectives like Diff-01 (1[y ≠ h(2)(x)] - 1[y = h(1)(x)]) and Diff-Prob (p(2)y(x) - p(1)y(x)). At inference, the post-hoc model determines when to defer to the second classifier. Experiments compare confidence-based deferral, random deferral, and post-hoc deferral across specialist, label noise, and distribution shift settings on ImageNet and CIFAR-100 datasets.

## Key Results
- Confidence-based deferral is optimal when the downstream model has constant error probability across all inputs
- Post-hoc deferral rules significantly improve upon confidence-based deferral (up to 5% accuracy gain) in specialist, label noise, and distribution shift settings
- Post-hoc deferral outperforms confidence-based deferral more prominently at low deferral rates (higher accuracy demands)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Confidence-based deferral is optimal when the downstream model has constant error probability.
- Mechanism: When the downstream model's error probability is constant across all inputs, the ordering of instances by the first model's absolute confidence aligns with the ordering by the difference in correct prediction probabilities between the two models. This alignment ensures confidence-based deferral produces the same deferral curve as the Bayes-optimal rule.
- Core assumption: The downstream model's error probability is independent of the input (constant across samples).
- Evidence anchors:
  - [abstract]: "confidence-based deferral is optimal when the downstream model has constant error probability"
  - [section]: "confidence-based deferral is optimal when the downstream model has a constant error probability, i.e., ηh2(x)(x) is a constant for all x ∈ X"
- Break condition: The downstream model's error probability becomes non-uniform (e.g., specialist model, label noise, or distribution shift).

### Mechanism 2
- Claim: Post-hoc deferral rules can improve upon confidence-based deferral by explicitly accounting for both models' confidences.
- Mechanism: Post-hoc deferral rules learn to predict whether the first model is wrong but the second model is right, using features from the first model's probability outputs. This allows the rule to avoid deferring in cases where the second model would perform worse.
- Core assumption: The second model's error patterns are predictable from the first model's probability outputs.
- Evidence anchors:
  - [abstract]: "post-hoc deferral mechanisms, and demonstrate they can significantly improve upon confidence-based deferral"
  - [section]: "Motivated by this, we then study a series of post-hoc deferral rules, that seek to mimic the form of the optimal deferral rule"
- Break condition: The second model's error patterns become unpredictable from the first model's outputs (e.g., when the second model exactly matches Bayes probabilities).

### Mechanism 3
- Claim: Confidence-based deferral underperforms when the downstream model is a specialist, there is label noise, or there is distribution shift.
- Mechanism: In these settings, the downstream model's error probability is highly non-uniform across samples. Confidence-based deferral, being oblivious to this non-uniformity, may erroneously defer samples where the downstream model performs worse than the first model.
- Core assumption: The downstream model's error probability varies significantly across different input subsets.
- Evidence anchors:
  - [abstract]: "confidence-based deferral can significantly improve upon confidence-based deferral in settings where (i) downstream models are specialists that only work well on a subset of inputs, (ii) samples are subject to label noise, and (iii) there is distribution shift between the train and test set"
  - [section]: "confidence-based deferral may fail is when when the downstream model is a specialist, which performs well only on a particular sub-group of the data"
- Break condition: The downstream model's error probability becomes uniform or predictable from the first model's outputs.

## Foundational Learning

- Concept: Bayes-optimal deferral rule
  - Why needed here: Understanding the theoretically optimal deferral rule is crucial for identifying when confidence-based deferral may fail and for developing post-hoc deferral rules.
  - Quick check question: What is the key quantity that determines whether to defer to the second model according to the Bayes-optimal rule?

- Concept: Plug-in estimators
  - Why needed here: Plug-in estimators are used to approximate the Bayes-optimal deferral rule in practice, and understanding their limitations is important for developing effective post-hoc deferral rules.
  - Quick check question: What is the key difference between the one-hot oracle and the relative confidence estimator?

- Concept: Calibration of confidence measures
  - Why needed here: Understanding the calibration of confidence measures is important for interpreting the results of confidence-based deferral and identifying its limitations.
  - Quick check question: How does the calibration of the first model's confidence relate to the performance of confidence-based deferral?

## Architecture Onboarding

- Component map: h(1) -> g(x) -> h(2) -> final prediction
- Critical path:
  1. Train base classifiers independently
  2. Construct post-hoc training data from validation set
  3. Train post-hoc model to predict deferral decisions
  4. At inference, use post-hoc model to determine when to defer
- Design tradeoffs:
  - Capacity of post-hoc model: Too low may underfit, too high may overfit
  - Features used for post-hoc model: Should capture relevant information from first model's outputs
  - Trade-off between deferral rate and accuracy: Controlled by deferral threshold
- Failure signatures:
  - Confidence-based deferral underperforms when downstream model is specialist, label noise present, or distribution shift occurs
  - Post-hoc deferral may overfit if model capacity is too high
  - Post-hoc deferral may not improve upon confidence-based deferral if second model's error patterns are unpredictable
- First 3 experiments:
  1. Compare confidence-based deferral with post-hoc deferral in specialist setting
  2. Evaluate post-hoc deferral under increasing levels of label noise
  3. Test post-hoc deferral in presence of distribution shift between train and test sets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions does post-hoc deferral significantly outperform confidence-based deferral?
- Basis in paper: [explicit] The paper identifies three failure modes for confidence-based deferral: specialist models, label noise, and distribution shift. It then demonstrates that post-hoc deferral rules can significantly improve upon confidence-based deferral in these settings.
- Why unresolved: The paper provides empirical evidence for specific datasets and model architectures. However, a more general theoretical understanding of when post-hoc deferral will be beneficial is lacking. The paper mentions that post-hoc deferral may not offer benefits when the downstream model's outputs are highly predictable or when the model is a Bayes-optimal estimator.
- What evidence would resolve it: A theoretical framework that characterizes the conditions under which post-hoc deferral is beneficial, potentially involving the predictability of the downstream model's outputs and the structure of the error patterns.

### Open Question 2
- Question: How can the generalization of post-hoc deferral models be improved?
- Basis in paper: [explicit] The paper acknowledges that post-hoc models can overfit to the training set, especially when the difference in correct prediction probabilities between the two models is not strongly predictable.
- Why unresolved: The paper uses a small-capacity MLP for the post-hoc model and applies L2 regularization, but overfitting still occurs in some settings. More sophisticated regularization techniques or model architectures might be needed.
- What evidence would resolve it: Experiments comparing different regularization techniques (e.g., dropout, early stopping) or model architectures (e.g., ensemble methods) for post-hoc models, demonstrating improved generalization performance.

### Open Question 3
- Question: How do post-hoc deferral rules perform in other types of distribution shift beyond label shift?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of post-hoc deferral rules in a label shift setting (long-tailed CIFAR-100). However, it mentions that other forms of distribution shift, such as out-of-distribution samples, are also of interest.
- Why unresolved: The paper only provides empirical evidence for one type of distribution shift. It is unclear how post-hoc deferral rules would perform under other types of shifts, such as covariate shift or concept drift.
- What evidence would resolve it: Experiments evaluating post-hoc deferral rules on datasets with different types of distribution shift (e.g., corrupted labels, adversarial examples) and comparing their performance to confidence-based deferral.

### Open Question 4
- Question: Can more refined confidence measures improve the performance of confidence-based deferral?
- Basis in paper: [explicit] The paper mentions that it focuses on the maximum predictive probability as a confidence measure but notes that other measures, such as those based on conformal prediction, could be explored.
- Why unresolved: The paper only experiments with the maximum predictive probability. It is unclear whether alternative confidence measures could mitigate the failure modes identified for confidence-based deferral.
- What evidence would resolve it: Experiments comparing confidence-based deferral using different confidence measures (e.g., entropy, conformal prediction scores) and evaluating their performance in the identified failure modes.

### Open Question 5
- Question: How do the findings generalize to natural language processing models?
- Basis in paper: [explicit] The paper concludes by stating that it would be of interest to study analogous trends for natural language processing models.
- Why unresolved: The paper focuses on image classification settings. The dynamics of cascades and deferral rules might differ in NLP tasks due to the sequential nature of language and the different types of models used (e.g., transformers).
- What evidence would resolve it: Experiments applying post-hoc deferral rules to NLP tasks (e.g., text classification, machine translation) and comparing their performance to confidence-based deferral, potentially identifying new failure modes or benefits specific to NLP.

## Limitations
- The analysis assumes independent training of base classifiers, which may not hold in all practical scenarios
- The study focuses on binary deferral decisions rather than more complex cascading architectures
- Some implementation details of post-hoc MLP architectures are not fully specified

## Confidence
- High confidence in theoretical analysis of Bayes-optimal deferral rule and its relationship to confidence-based deferral
- Medium confidence in experimental results, as they demonstrate clear trends but are limited to specific datasets and model architectures
- Medium confidence in generalization to other domains, as the analysis assumes standard classification settings

## Next Checks
1. Test post-hoc deferral rules on additional datasets and model architectures to verify generalizability
2. Implement cross-validation experiments to assess robustness of post-hoc deferral models to different train/validation splits
3. Evaluate the impact of varying the capacity of post-hoc deferral models on performance and overfitting risk