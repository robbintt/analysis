---
ver: rpa2
title: 'IG Captioner: Information Gain Captioners are Strong Zero-shot Classifiers'
arxiv_id: '2311.17072'
source_url: https://arxiv.org/abs/2311.17072
tags:
- captioner
- image
- text
- zero-shot
- classifier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the performance gap between generative and
  discriminative training objectives for zero-shot visual-language classification
  tasks. The authors observe that caption generation inherits distribution bias from
  language models trained on text-only data, making it less grounded on visual inputs.
---

# IG Captioner: Information Gain Captioners are Strong Zero-shot Classifiers

## Quick Facts
- arXiv ID: 2311.17072
- Source URL: https://arxiv.org/abs/2311.17072
- Reference count: 36
- Primary result: IG captioner achieves 58.5% top-1 accuracy on ImageNet zero-shot classification, outperforming standard captioners by >18%

## Executive Summary
This paper addresses the performance gap between generative and discriminative training objectives for zero-shot visual-language classification tasks. The authors observe that caption generation inherits distribution bias from language models trained on text-only data, making it less grounded on visual inputs. To mitigate this, they propose Information Gain (IG) evaluation and training objectives that focus on measuring the information gain from visual inputs. The IG captioner, trained and evaluated with these objectives, significantly outperforms standard captioners (>18% improvement on ImageNet top-1 accuracy) and achieves comparable performance to CLIP classifiers. The method also demonstrates strong performance on zero-shot image-text retrieval tasks.

## Method Summary
The method involves training a captioner with two objectives: a multimodal loss for generating captions from images and a unimodal loss for generating captions without images. The model is then evaluated using an Information Gain objective that measures the difference between the log probability of a caption given an image and the log probability of the caption alone. This approach aims to reduce the influence of linguistic priors from text-only training data and focus on the visual information gain. The method is evaluated on ImageNet for zero-shot classification and MSCOCO/Flicker30K for zero-shot image-text retrieval.

## Key Results
- IG captioner achieves 58.5% top-1 accuracy on ImageNet zero-shot classification, outperforming standard captioners by >18%
- IG captioner achieves comparable performance to CLIP classifiers on ImageNet
- Strong performance on zero-shot image-text retrieval tasks (MSCOCO and Flickr30K)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The captioner's poor classification performance stems from inheriting linguistic priors from text-only training data, causing it to rely on textual cues rather than visual grounding.
- Mechanism: Standard captioners are trained to generate text from images, but they also inherit the distribution of captions seen during pretraining. This causes the model to make classification decisions based on the likelihood of captions under the language model rather than the visual content of the image.
- Core assumption: The language model trained on pure text data has strong priors that dominate the multimodal predictions when combined with visual input.
- Evidence anchors:
  - [abstract] "we observe that the caption generation inherits the distribution bias from the language model trained with pure text modality, making it less grounded on the visual signal."
  - [section] "We observe a strong correlation between log P (T |I) and log P (T ), which indicates that the captioner is strongly biased by the text priors and tends to ignore the visual information."
- Break condition: If the language model's priors are weak or the visual signal is overwhelmingly informative, the bias effect would be minimal.

### Mechanism 2
- Claim: Information Gain (IG) evaluation reduces the influence of linguistic priors by focusing on the gain in information from visual input rather than absolute likelihood.
- Mechanism: IG evaluation computes the difference between log P(T|I) and log P(T), effectively measuring how much the visual input changes the prediction compared to the language model's prior. This reduces the correlation with text priors.
- Core assumption: The difference between multimodal and unimodal predictions captures the visual information gain effectively.
- Evidence anchors:
  - [abstract] "We propose an Information Gain (IG) evaluation objective... This objective demonstrates a significant performance boost on top of a captioner trained with conventional procedure."
  - [section] "We observe a strong correlation between log P (T |I) and log P (T ), which indicates that the captioner is strongly biased by the text priors and tends to ignore the visual information."
- Break condition: If the visual input provides no additional information beyond the text priors, IG would be near zero and provide no classification benefit.

### Mechanism 3
- Claim: The two-objective training (multimodal + unimodal loss) aligns the captioner's training with the IG evaluation, improving performance.
- Mechanism: By training with both a multimodal loss (generating captions from images) and a unimodal loss (generating captions without images), the model learns to better distinguish when visual information is relevant versus when to rely on text priors.
- Core assumption: Training objectives that match evaluation objectives lead to better alignment between model behavior and task requirements.
- Evidence anchors:
  - [abstract] "We further design a generative training objective to match the evaluation objective."
  - [section] "We propose a two-objective approach to train the captioner... Finally, we combine the above two losses in the following: L = βlmultimodal + γlunimodal"
- Break condition: If the training data doesn't contain sufficient visual-text pairs or if the unimodal loss overwhelms the multimodal signal, the training might not converge properly.

## Foundational Learning

- Concept: Multimodal learning with transformers
  - Why needed here: The model needs to process both visual and textual information through a shared architecture to generate captions that can be used for classification.
  - Quick check question: How does the model handle the different modalities (image vs. text) during processing?

- Concept: Information theory and mutual information
  - Why needed here: IG evaluation is based on measuring the information gain from visual input, which requires understanding concepts like pointwise mutual information.
  - Quick check question: What is the mathematical relationship between information gain and mutual information in this context?

- Concept: Autoregressive text generation
  - Why needed here: The captioner generates text token by token, and this process needs to be understood to modify the training and evaluation procedures.
  - Quick check question: How does the autoregressive nature of caption generation affect the model's ability to focus on visual information?

## Architecture Onboarding

- Component map: Image encoder (ViT) → Text decoder (transformer) → Output tokens. The text decoder can operate in two modes: with image input (multimodal) and without image input (unimodal).
- Critical path: Image → Encoder → Cross-attention in decoder → Token predictions. The cross-attention mechanism is where visual information is integrated into the text generation process.
- Design tradeoffs: Using a single text decoder for both multimodal and unimodal tasks reduces parameters but may create conflicts in learning objectives. Separate modules could be more specialized but less efficient.
- Failure signatures: If the model relies too heavily on text priors, it will show high correlation between log P(T|I) and log P(T). If the visual signal is not properly integrated, the model will generate generic captions regardless of input.
- First 3 experiments:
  1. Measure correlation between log P(T|I) and log P(T) on a validation set to confirm the bias problem exists.
  2. Implement IG evaluation with different α values and measure classification performance to find the optimal setting.
  3. Train with different ratios of multimodal to unimodal loss weights to find the best balance for the two-objective training.

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- The paper doesn't fully explore whether the performance gap between generative and discriminative approaches is inherent to the architectures or could be reduced through better generative training methods.
- The method shows strong results on ImageNet classification and image-text retrieval, but the evaluation doesn't extensively test on other zero-shot classification datasets or tasks.
- While the paper addresses linguistic bias from pretraining, it doesn't fully investigate how the distribution of the Laion-5B dataset itself might introduce biases that affect downstream classification performance.

## Confidence

**High Confidence**: The observation that standard captioners inherit linguistic priors and show high correlation between log P(T|I) and log P(T) is well-supported by empirical evidence in the paper. The proposed IG evaluation objective effectively reduces this correlation.

**Medium Confidence**: The claim that IG training objectives significantly improve zero-shot classification performance is supported by experimental results, but the ablation studies could be more comprehensive to isolate the specific contributions of each component.

**Low Confidence**: The paper's assertion that the method "achieves comparable performance to CLIP classifiers" should be qualified - while the top-1 accuracy is similar, CLIP's zero-shot performance on ImageNet is known to be highly dependent on prompt engineering, which isn't fully addressed.

## Next Checks

1. **Dataset Generalization Test**: Evaluate the IG captioner on additional zero-shot classification benchmarks (e.g., CIFAR-100, Places365) to verify the method's generalization beyond ImageNet.

2. **Prompt Sensitivity Analysis**: Systematically test how the IG captioner's performance varies with different prompt templates and class name formulations, comparing this sensitivity to CLIP's known prompt engineering requirements.

3. **Ablation on Training Objectives**: Conduct a more granular ablation study varying the weights β and γ in the two-objective training loss to better understand the optimal balance between multimodal and unimodal objectives.