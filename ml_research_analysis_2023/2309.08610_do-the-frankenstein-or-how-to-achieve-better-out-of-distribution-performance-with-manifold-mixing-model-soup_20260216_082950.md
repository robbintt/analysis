---
ver: rpa2
title: Do the Frankenstein, or how to achieve better out-of-distribution performance
  with manifold mixing model soup
arxiv_id: '2309.08610'
source_url: https://arxiv.org/abs/2309.08610
tags:
- soup
- algorithm
- fused
- mixing
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The standard recipe applied in transfer learning is to finetune
  a pretrained model on the task-specific dataset with different hyperparameter settings
  and pick the model with the highest accuracy on the validation dataset. Unfortunately,
  this leads to models which do not perform well under distribution shifts, e.g.
---

# Do the Frankenstein, or how to achieve better out-of-distribution performance with manifold mixing model soup

## Quick Facts
- arXiv ID: 2309.08610
- Source URL: https://arxiv.org/abs/2309.08610
- Reference count: 2
- Standard transfer learning with finetuning doesn't generalize well to distribution shifts; this method improves out-of-distribution performance by 3.5% over best individual model

## Executive Summary
Standard transfer learning involves finetuning pretrained models and selecting the best one based on validation accuracy, but this approach often fails to generalize under distribution shifts (e.g., when images are replaced with sketches). This paper introduces manifold mixing model soup, an algorithm that optimally combines latent space manifolds from multiple finetuned models to create a fused model. The approach significantly improves out-of-distribution performance while maintaining accuracy on the original dataset, achieving a 3.5% improvement over the best individual model when applied to CLIP for image classification.

## Method Summary
The method partitions each finetuned model into latent space manifold components, then uses black-box optimization to find optimal mixing coefficients for each component to maximize validation accuracy. Starting with the best model, it iteratively mixes components from other models only if they improve performance. This sequential approach is more efficient than mixing all models simultaneously and provides better out-of-distribution generalization than simple averaging.

## Key Results
- Manifold mixing model soup improves out-of-distribution accuracy by 3.5% compared to best individual model
- The fused model maintains or improves accuracy on the original finetuning dataset
- Outperforms both uniform model soup averaging and greedy sequential mixing approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mixing latent space manifolds from multiple finetuned models in an optimal way improves out-of-distribution performance
- Mechanism: The algorithm partitions each finetuned model into latent space manifolds and optimizes mixing coefficients for each component individually to maximize validation accuracy on the original dataset
- Core assumption: Different finetuned models have learned different but complementary representations in their latent space manifolds
- Evidence anchors: [abstract] "mixes together the latent space manifolds of multiple finetuned models in an optimal way"; [section 3] describes partitioning models into components and optimizing mixing
- Break condition: If finetuned models have very similar representations or if optimal mixing coefficients are close to 0 or 1

### Mechanism 2
- Claim: Optimization-based mixing outperforms simple averaging while maintaining accuracy on original dataset
- Mechanism: Instead of uniform averaging, the algorithm selectively mixes models and optimizes mixing coefficients for each latent space manifold component using a black-box optimizer
- Core assumption: Not all models contribute equally, and component-wise optimization can find better combinations than global averaging
- Evidence anchors: [section 3] describes optimization to calculate optimal mixing vector; [section 4] shows scatterplot comparing different approaches
- Break condition: If optimization consistently fails to find better solutions than simple averaging

### Mechanism 3
- Claim: Sequential mixing approach is more efficient than mixing all models at once
- Mechanism: The algorithm processes models sequentially, starting with best model, and only mixes candidate models if they improve the current fused model
- Core assumption: Order of mixing matters, and starting with best model provides strong foundation for improvements
- Evidence anchors: [section 3] describes sequential mixing of promising ingredient models; [section 4] shows final fused model after iterating
- Break condition: If sequential approach misses better combinations found by simultaneous mixing

## Foundational Learning

- Concept: Latent space manifolds in neural networks
  - Why needed here: Algorithm operates on latent space manifolds of finetuned models
  - Quick check question: What is the difference between a single layer and a collection of layers when defined as a latent space manifold component?

- Concept: Black-box derivative-free optimization
  - Why needed here: Algorithm uses Nevergrad's Cobyla optimizer to find optimal mixing coefficients
  - Quick check question: What are advantages and limitations of derivative-free optimization compared to gradient-based methods?

- Concept: Distribution shift and out-of-distribution generalization
  - Why needed here: Primary motivation and evaluation metric is improving performance under distribution shifts
  - Quick check question: How do five ImageNet variant datasets represent different types of distribution shifts?

## Architecture Onboarding

- Component map: Model partitioning -> Model selection -> Optimization module -> Fused model construction -> Evaluation pipeline
- Critical path:
  1. Load and partition all finetuned models
  2. Sort models by validation accuracy
  3. Initialize fused model with best model
  4. For each candidate model: check if promising, optimize mixing coefficients if so, update if validation improves
  5. Evaluate final fused model
- Design tradeoffs:
  - Partitioning granularity: Finer partitioning provides more flexibility but makes optimization harder
  - Optimization budget: More evaluations allow better optimization but increase computation time
  - Model selection criteria: Validation accuracy may miss models with useful complementary representations
- Failure signatures:
  - Fused model accuracy worse than best individual model
  - Optimization consistently returns initial mixing coefficients
  - High variance in mixing coefficients across components
  - Sequential mixing order significantly affects final results
- First 3 experiments:
  1. Baseline test: Apply uniform averaging to same finetuned models and compare accuracy
  2. Component sensitivity: Run algorithm with different numbers of components (8, 15, 26) on same dataset
  3. Optimization ablation: Compare results with different Nevergrad optimizers or fixed mixing coefficients

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ManifoldMixMS performance vary with different neural network architectures beyond CLIP ViT-B/32?
- Basis in paper: [explicit] Authors mention plans to evaluate on other neural network architectures for computer vision and NLP tasks
- Why unresolved: Experiments conducted only on CLIP ViT-B/32
- What evidence would resolve it: Experiments on diverse architectures like ResNet, BERT, GPT, comparing out-of-distribution improvements

### Open Question 2
- Question: What is the theoretical basis for why mixing latent space manifolds improves out-of-distribution generalization compared to simple weight averaging?
- Basis in paper: [explicit] Authors propose sophisticated mixing strategy but lack theoretical explanation
- Why unresolved: Paper lacks theoretical framework explaining why component-wise mixing works better
- What evidence would resolve it: Theoretical framework analyzing loss landscape or information geometry

### Open Question 3
- Question: What is the optimal number of components for partitioning the neural network in ManifoldMixMS?
- Basis in paper: [inferred] Authors test 8, 15, 26 components but don't provide guidance on optimal selection
- Why unresolved: Choice of 8, 15, 26 appears arbitrary with no analysis of impact
- What evidence would resolve it: Systematic experiments varying component numbers across models and tasks

## Limitations
- Lack of clarity around exact component partitioning scheme for different granularities
- No ablation studies on optimization process or sensitivity to hyperparameters
- Computational costs of sequential mixing approach not fully characterized

## Confidence

- High confidence: Mixing latent space manifolds can improve out-of-distribution performance (consistent 3.5% improvement across datasets)
- Medium confidence: Optimization-based mixing outperforms simple averaging (comparison primarily against uniform soup, optimization process somewhat opaque)
- Medium confidence: Sequential mixing approach is more efficient (computational costs not explicitly reported, order dependency effect not fully characterized)

## Next Checks
1. **Component partitioning sensitivity**: Run algorithm with different partitioning schemes (8, 15, 26 components) on same finetuned models to verify improvements are consistent across granularities
2. **Optimization process validation**: Compare results with different Nevergrad optimizers, varying function evaluations, and fixed mixing coefficients to determine optimization criticality
3. **Distribution shift robustness**: Test fused model on additional distribution-shifted datasets beyond five ImageNet variants to verify generalization to other shift types