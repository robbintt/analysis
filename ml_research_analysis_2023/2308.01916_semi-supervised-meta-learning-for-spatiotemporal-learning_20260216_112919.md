---
ver: rpa2
title: Semi Supervised Meta Learning for Spatiotemporal Learning
arxiv_id: '2308.01916'
source_url: https://arxiv.org/abs/2308.01916
tags:
- video
- learning
- small-scale
- architecture
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies semi-supervised meta-learning to video data
  for spatiotemporal learning. The authors extend work on Masked Autoencoders (MAEs)
  using the Vision Transformer (ViT) architecture.
---

# Semi Supervised Meta Learning for Spatiotemporal Learning

## Quick Facts
- arXiv ID: 2308.01916
- Source URL: https://arxiv.org/abs/2308.01916
- Reference count: 2
- Key outcome: MAE pretraining + MANN meta-learning improves action classification on small-scale video dataset

## Executive Summary
This paper applies semi-supervised meta-learning to video data for spatiotemporal learning using Masked Autoencoders (MAEs) with Vision Transformer (ViT) architecture. The authors propose a three-stage approach: fine-tuning a pre-trained MAE on a custom small-scale video dataset, training an MAE encoder with classification head for action classification, and fine-tuning with a Memory Augmented Neural Network (MANN) backbone for few-shot adaptation. Their custom dataset contains 24,927 video clips across 518 human-action classes, sourced from MiniKinetics-200 and TinyVIRAT datasets.

## Method Summary
The method extends MAEs by modifying the ViT backbone with Shifted Patch Tokenization (SPT) for small-scale datasets, then applying a three-stage training pipeline. First, a pre-trained MAE is fine-tuned on the custom video dataset for reconstruction. Second, an MAE encoder is trained with a classification head for action classification. Third, the pre-trained MAE is combined with a MANN backbone and fine-tuned for meta-learning across the 518 action classes. The approach aims to leverage unsupervised pretraining and meta-learning to overcome data scarcity in small-scale video datasets.

## Key Results
- Fine-tuning pre-trained MAE on custom small-scale dataset outperforms existing pre-trained MAE architectures on video reconstruction tasks
- MAE encoder with small-scale ViT backbone trained on custom dataset achieves steady convergence for action classification
- Pre-trained MAE with MANN backbone shows effective few-shot adaptation on small-scale video dataset test tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling down ViT backbone preserves useful inductive bias for small-scale datasets
- Mechanism: Shifted Patch Tokenization (SPT) compensates for reduced dataset size by retaining spatial locality in patch embeddings
- Core assumption: Small-scale datasets inherently lack the diversity that large-scale ViTs exploit, so locality cues become more important
- Evidence anchors: [section] "We modify the existing ViT backbone in existing MAE architectures for small-scale datasets by applying Shifted Patch Tokenization (SPT) to combats the lack of locality inductive bias available in small-scale datasets."

### Mechanism 2
- Claim: Meta-learning via MANN improves sample efficiency for action classification
- Mechanism: MANN's external memory and LSTM controller allow rapid adaptation to new action classes using few examples
- Core assumption: Human-action video clips share high-level structural patterns that can be encoded and reused across tasks
- Evidence anchors: [section] "We utilize the Memory Augmented Neural Network (MANN) architecture to apply meta-learning to our framework."

### Mechanism 3
- Claim: Masking ratio in MAE balances reconstruction difficulty and representation quality
- Mechanism: High masking ratio (>75%) forces encoder to learn robust latent representations that transfer to downstream classification
- Core assumption: Reconstruction from heavily masked input is a non-trivial pretext task that yields richer features than low masking ratios
- Evidence anchors: [section] "Their results showed that a masked autoencoder with a masked ratio of 90% outperforms supervised pre-training approaches."

## Foundational Learning

- Concept: Vision Transformer patch tokenization
  - Why needed here: ViT processes images/videos as sequences of patches; understanding tokenization is essential to modify SPT
  - Quick check question: How does ViT convert a 64x64 frame into patch embeddings?

- Concept: Masked Autoencoder pretraining objective
  - Why needed here: MAE masks tokens and reconstructs them; knowing the loss function is key for ablation experiments
  - Quick check question: What is the reconstruction loss used in MAE (e.g., L2 vs cross-entropy)?

- Concept: Meta-learning via MANN
  - Why needed here: MANN stores task-specific memory; understanding read/write operations is needed to implement fine-tuning
  - Quick check question: How does the LSTM controller in MANN decide what to write to external memory?

## Architecture Onboarding

- Component map: Input pipeline (64x64 video clips, 100 frames) -> Patch tokenization -> ViT backbone -> MAE encoder -> MAE decoder -> MANN controller (LSTM + external memory) -> Classification head
- Critical path: Pretraining (MAE) → Fine-tuning (MANN) → Evaluation (action classification)
- Design tradeoffs: High masking ratio improves representation quality but slows pretraining convergence; small-scale ViT reduces parameters but may underfit complex spatiotemporal patterns; MANN adds memory overhead but enables few-shot adaptation
- Failure signatures: Pretraining loss plateaus early → masking ratio too high or learning rate too low; classification accuracy stays near random → encoder backbone too weak or memory not updated properly; memory overflow errors → MANN memory size too small for task diversity
- First 3 experiments:
  1. Train MAE with 80% masking on Kinetics-400 subset; verify reconstruction loss decreases
  2. Fine-tune MAE encoder with linear classifier on TinyVIRAT; check top-1 accuracy improves over baseline
  3. Wrap fine-tuned encoder in MANN; perform few-shot adaptation on unseen action classes; measure adaptation speed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does applying meta-learning to pre-trained masked autoencoders improve performance on small-scale video datasets compared to training from scratch?
- Basis in paper: [explicit] The authors compare fine-tuning a pre-trained MAE on their small-scale dataset against training a video MAE from scratch with a small-scale ViT backbone.
- Why unresolved: While the paper shows the fine-tuned model slightly outperforms the pre-trained baseline, the difference is negligible (0.3% improvement).
- What evidence would resolve it: Further experiments comparing fine-tuning a pre-trained MAE with training from scratch on various small-scale video datasets, possibly with different masking ratios and hyperparameters.

### Open Question 2
- Question: How does the masking ratio affect the performance of video MAE architectures with classification heads on small-scale datasets?
- Basis in paper: [explicit] The authors experiment with training a video autoencoder with a classification head with and without masking, finding that masking improves top-5 performance from 74.5% to 76%.
- Why unresolved: The authors only experiment with a single masking ratio (80%) and note that they would need to continue running experiments and fine-tune the masking ratio hyperparameter to confirm the trend.
- What evidence would resolve it: Experiments with different masking ratios, such as 50%, 70%, 90%, and comparing their performance on small-scale video datasets.

### Open Question 3
- Question: Can other meta-learning algorithms, such as Model Agnostic Meta-Learning (MAML), be integrated with MAE architectures for improved performance on small-scale video datasets?
- Basis in paper: [inferred] The authors mention future work involving experimenting with fine-tuning the MANN architecture with and without a pre-trained video MAE, as well as replacing MANN with other meta-learning implementations such as MAML.
- Why unresolved: The authors have not yet conducted experiments with other meta-learning algorithms, and the effectiveness of these algorithms in combination with MAE architectures on small-scale video datasets is unknown.
- What evidence would resolve it: Implementing and experimenting with various meta-learning algorithms, such as MAML, in combination with MAE architectures on small-scale video datasets.

## Limitations
- No quantitative comparison against standard supervised baselines on the custom dataset
- Lacks ablation studies validating the necessity of SPT versus standard ViT patch tokenization
- Insufficient evidence demonstrating MANN's advantages over standard fine-tuning approaches

## Confidence

- **High confidence**: The three-stage framework architecture (MAE pretraining → classification fine-tuning → MANN meta-learning) is clearly described and logically structured
- **Medium confidence**: The effectiveness of MAE pretraining on small-scale datasets is supported by reconstruction loss improvements, but downstream classification results lack comparative benchmarks
- **Low confidence**: The specific advantages of MANN over standard fine-tuning approaches for action classification are not demonstrated with sufficient evidence

## Next Checks
1. Implement ablation studies comparing SPT vs standard patch tokenization on the custom dataset to quantify locality bias benefits
2. Add supervised baseline comparisons (e.g., fully supervised ViT training) alongside the semi-supervised meta-learning approach for action classification
3. Provide detailed MANN implementation specifications including memory capacity, controller architecture, and task adaptation metrics across all 518 classes