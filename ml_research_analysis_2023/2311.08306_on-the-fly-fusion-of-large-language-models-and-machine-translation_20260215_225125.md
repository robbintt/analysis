---
ver: rpa2
title: On-the-Fly Fusion of Large Language Models and Machine Translation
arxiv_id: '2311.08306'
source_url: https://arxiv.org/abs/2311.08306
tags:
- translation
- machine
- association
- language
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method for ensembling machine translation
  (MT) models with large language models (LLMs) through on-the-fly token-level fusion
  during inference. The approach involves prompting an LLM for translation and combining
  its output with an MT model using a learned mixing ratio.
---

# On-the-Fly Fusion of Large Language Models and Machine Translation

## Quick Facts
- arXiv ID: 2311.08306
- Source URL: https://arxiv.org/abs/2311.08306
- Reference count: 31
- This paper introduces a novel method for ensembling machine translation models with large language models through on-the-fly token-level fusion during inference.

## Executive Summary
This paper presents a novel approach for combining machine translation (MT) models with large language models (LLMs) through on-the-fly token-level fusion during inference. The method prompts an LLM for translation and combines its output with an MT model using a learned mixing ratio. Experiments on four language pairs demonstrate that ensembling an MT model with a slightly weaker-at-translation LLM can improve translation quality compared to using either model alone, particularly in high-resource settings.

## Method Summary
The paper proposes on-the-fly ensembling of MT models with LLMs through token-level probability fusion. The approach involves prompting an LLM for translation and combining its output with an MT model using a learned mixing ratio λ. During inference, both models generate token probabilities, which are combined using λ to produce the final ensemble probability. The optimal λ is learned on a validation set through grid search. The method is evaluated on German-English, Russian-English, Turkish-English, and Hausa-English translation tasks using COMET-22 and CTXPro metrics.

## Key Results
- Ensembling MT with a slightly weaker-at-translation LLM improves translation quality in high-resource settings
- The approach outperforms both MT-only and LLM-only baselines, as well as traditional MT ensembles
- Incorporating domain and document context through prompting further enhances performance, especially for document-level translation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token-level fusion of MT and LLM probabilities improves translation quality when the LLM is slightly weaker at translation
- Mechanism: The ensemble combines the translation adequacy of MT with the fluency and domain coverage of LLM through weighted probability mixing
- Core assumption: The complementary strengths of MT (translation accuracy) and LLM (fluency/domain knowledge) can be linearly combined
- Evidence anchors:
  - [abstract]: "a slightly weaker-at-translation LLM can improve translations of a NMT model"
  - [section 2]: "pensemble reduces to the LLM when λ = 0 and to the MT model when λ = 1"
  - [corpus]: Weak evidence - only 5 related papers found, none directly testing this specific mechanism
- Break condition: When LLM translation quality is substantially worse than MT (e.g., Turkish and Hausa directions)

### Mechanism 2
- Claim: Prompting the LLM with domain information improves translation quality for specific domains
- Mechanism: Domain-specific prompts guide the LLM to generate translations with appropriate style and terminology
- Core assumption: LLMs can leverage additional context from prompts to improve domain-specific translation
- Evidence anchors:
  - [section 5.2]: "Prompting with domain can be improves COMET for the LLM, but is less effective for the ensemble"
  - [section 2]: "pLLM still conditions on the prompt, which can be used to infuse the model with auxiliary information"
  - [corpus]: Weak evidence - no direct corpus matches for domain prompting in MT-LLM ensembles
- Break condition: When domain information is not relevant to the test data or when MT model is already domain-adapted

### Mechanism 3
- Claim: Document context prompting enables document-level translation capabilities in sentence-based MT models
- Mechanism: Providing previous source sentences and translations as context allows the LLM to maintain consistency across sentences
- Core assumption: LLMs can maintain coherence across document context when provided with relevant previous translations
- Evidence anchors:
  - [section 5.3]: "Prompting the LLM with context outperforms few shot prompting and the ensemble with context"
  - [section 2]: "The concatenation of the prompt M, source sentence S and the previous generated targets are all decoder outputs"
  - [corpus]: Weak evidence - related papers found but none specifically testing document context in MT-LLM ensembles
- Break condition: When document-level phenomena are not present in the text or when context length exceeds model capacity

## Foundational Learning

- Concept: Probability theory and weighted averaging
  - Why needed here: The ensemble combines MT and LLM probabilities using a mixing ratio λ
  - Quick check question: If λ = 0.7, what percentage of the final probability comes from the MT model?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Understanding how MT and LLM models generate token probabilities differently
  - Quick check question: What is the key difference between encoder-decoder NMT and decoder-only LLM architectures?

- Concept: Prompt engineering and in-context learning
  - Why needed here: The LLM is prompted with domain and document context to improve translation quality
  - Quick check question: How does few-shot prompting differ from context-aware prompting in this work?

## Architecture Onboarding

- Component map: MT model -> LLM model -> Prompt generator -> Mixing ratio optimizer -> Evaluation framework
- Critical path: 1. Generate MT translation 2. Generate LLM translation with same prompt 3. Compute token-level probabilities from both models 4. Apply mixing ratio λ to combine probabilities 5. Select highest probability token at each step
- Design tradeoffs: Model size vs. inference cost (7B vs 13B LLM), Mixing ratio optimization vs. runtime efficiency, Prompt complexity vs. translation quality gains, Document context length vs. memory constraints
- Failure signatures: Poor translation quality when LLM is much weaker than MT, Degraded performance when using unprompted LLM (Figure 2), No improvement when domain context is irrelevant, Computational overhead from running two models
- First 3 experiments: 1. Validate baseline MT and LLM performance separately 2. Test ensembling with varying λ values on validation set 3. Compare domain-context vs. few-shot prompting effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of on-the-fly fusion change when using different mixing ratios (λ) for different language pairs and domains?
- Basis in paper: [explicit] The paper explores the effect of varying λ on translation quality and finds that the optimal λ varies depending on the language pair and domain
- Why unresolved: The paper does not provide a systematic analysis of how λ should be tuned for different scenarios or provide a method for automatically selecting the optimal λ
- What evidence would resolve it: Empirical studies comparing the performance of different λ values across a wide range of language pairs and domains, along with a proposed method for automatically selecting λ based on the characteristics of the MT and LLM models

### Open Question 2
- Question: Can the on-the-fly fusion approach be extended to other natural language processing tasks beyond machine translation?
- Basis in paper: [inferred] The paper focuses on machine translation, but the authors mention that the techniques could be explored for other tasks where the LLM and task-specific model have different properties
- Why unresolved: The paper does not investigate the applicability of the approach to other tasks or provide any insights into the challenges and potential benefits of extending it beyond translation
- What evidence would resolve it: Experimental results demonstrating the effectiveness of on-the-fly fusion for other tasks such as text summarization, question answering, or sentiment analysis, along with an analysis of the factors that influence its performance in these domains

### Open Question 3
- Question: How does the quality of the MT model affect the performance of the on-the-fly fusion approach?
- Basis in paper: [explicit] The paper mentions that the LLM improves the translation quality of the MT model even when the LLM is weaker at translation, provided the LLM is good enough
- Why unresolved: The paper does not provide a detailed analysis of the relationship between the quality of the MT model and the performance of the fusion approach, or explore the scenarios where the approach might not be beneficial
- What evidence would resolve it: Experiments comparing the performance of the fusion approach with MT models of varying quality, along with an analysis of the factors that determine when the approach is most effective

## Limitations
- The approach shows limited effectiveness when the LLM is substantially weaker than the MT model (e.g., Turkish and Hausa directions)
- Computational overhead from running both models during inference raises practical concerns
- The evidence for domain-context prompting effectiveness is weak, with the ensemble method potentially suppressing some benefits

## Confidence
- MT-LLM ensemble effectiveness: Medium
- Domain-context prompting benefits: Low
- Document-level translation improvements: Medium

## Next Checks
1. **Mixing Ratio Sensitivity Analysis**: Systematically test the ensemble performance across a wider range of λ values (0.1 increments from 0.1 to 0.9) on multiple validation sets to determine the stability and generalizability of the optimal mixing ratio selection.

2. **Low-Resource Language Evaluation**: Replicate the core experiments on truly low-resource language pairs (e.g., Nepali-English or Sinhala-English from WMT) to validate whether the ensemble approach provides consistent benefits when MT model quality is lower.

3. **Computational Cost-Benefit Analysis**: Measure and compare the wall-clock inference time and memory usage of the ensemble approach against both individual models, calculating the quality improvement per unit of additional computational cost to assess practical viability.