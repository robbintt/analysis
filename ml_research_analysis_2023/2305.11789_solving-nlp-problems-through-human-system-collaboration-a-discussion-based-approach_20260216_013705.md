---
ver: rpa2
title: 'Solving NLP Problems through Human-System Collaboration: A Discussion-based
  Approach'
arxiv_id: '2305.11789'
source_url: https://arxiv.org/abs/2305.11789
tags:
- system
- discussion
- human
- label
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a human-system collaboration approach for NLP
  tasks through discussion, inspired by how humans solve problems by explaining and
  agreeing/disagreeing with each other. The authors create a dataset of human-human
  discussions on the natural language inference (NLI) task, focusing on difficult
  cases to spur discussion.
---

# Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach

## Quick Facts
- arXiv ID: 2305.11789
- Source URL: https://arxiv.org/abs/2305.11789
- Reference count: 7
- Primary result: Discussion-based system improves NLI accuracy by up to 25 points

## Executive Summary
This paper proposes a novel human-system collaboration approach for NLP tasks through discussion, inspired by how humans solve problems by explaining and agreeing/disagreeing with each other. The authors create a dataset of human-human discussions on the natural language inference (NLI) task, focusing on difficult cases to spur discussion. They then train a system using few-shot learning to engage in discussions with humans about NLI problems, accepting or objecting to provided opinions. Experiments show that the discussion-based system significantly improves NLI accuracy compared to a system without discussion training.

## Method Summary
The authors collected human-human discussion data on NLI problems from SNLI and ANLI datasets, focusing on difficult cases where annotators had different predictions. They used GPT-3 with few-shot learning, providing prompts that included task descriptions, examples, and human discussion examples. The system generates discussion utterances and predicts NLI labels based on these interactions. Evaluation included BERTScore for automatic assessment of discussion quality and human evaluation for agreement and objection rates.

## Key Results
- Discussion-based system improves NLI accuracy by up to 25 points compared to non-discussion systems
- Few-shot discussion system generates more supportive utterances and fewer unsupportive ones than zero-shot or few-shot systems
- Pseudo-discussion data generated by the system achieves performance comparable to manually created discussion data in 2 of 4 datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Discussion-based prompting improves NLI accuracy by up to 25 points by leveraging human-system interaction
- Mechanism: The system uses discussion examples in prompts to simulate collaborative reasoning, allowing it to refine predictions through interactive exchange of arguments
- Core assumption: The system can learn to engage in meaningful dialogue and accept/refute arguments appropriately from few-shot examples
- Evidence anchors:
  - [abstract]: "experiments show that the discussion-based system improves NLI accuracy by up to 25 points compared to a system without discussion training"
  - [section]: "The results of both quantitative and human evaluation demonstrate that a system trained with few-shot learning for discussion could perform more useful discussions compared to a system that was not trained for discussion"
- Break condition: If the system fails to generate supportive/unsupportive utterances appropriately or becomes too compliant with incorrect human opinions

### Mechanism 2
- Claim: Few-shot learning with discussion examples enables the system to generate more supportive utterances and fewer unsupportive ones
- Mechanism: By including human-human discussion examples in prompts, the system learns to match the content and style of supportive arguments that lead to correct labels
- Core assumption: BERTScore similarity to human utterances correlates with the quality of generated discussion content
- Evidence anchors:
  - [section]: "BERTScore of the few-shot-discussion is generally higher than that of the zero-shot and the few-shot systems"
  - [section]: "the difference between supportive and unsupportive utterance accuracies is greater in few-shot-discussion than in zero-shot and few-shot systems"
- Break condition: If BERTScore similarity does not correlate with actual discussion quality or if the system generates irrelevant content

### Mechanism 3
- Claim: Pseudo-discussion data generated by the system can achieve performance comparable to manually created discussion data
- Mechanism: The system can simulate human-like discussions in zero-shot, reducing the cost of creating training data while maintaining effectiveness
- Core assumption: GPT-3 can generate realistic discussion examples that capture the essential characteristics of human reasoning
- Evidence anchors:
  - [section]: "using discussion data created by the system itself for few-shot learning, the system achieved results equivalent to those of the system that used discussion data created by humans"
  - [section]: "the system's performance with pseudo-discussion data outperforms that of the system with manually created data" (in 2 of 4 datasets)
- Break condition: If the pseudo-discussions fail to capture the nuances of human reasoning or contain factual errors

## Foundational Learning

- Concept: Natural Language Inference (NLI)
  - Why needed here: The paper uses NLI as the task for human-system collaboration
  - Quick check question: What are the three possible relationships between premise and hypothesis in NLI?

- Concept: Few-shot learning
  - Why needed here: The system learns to discuss from limited examples rather than extensive training
  - Quick check question: How does few-shot learning differ from traditional supervised learning?

- Concept: BERTScore evaluation
  - Why needed here: Used to automatically evaluate the quality of system-generated discussion utterances
  - Quick check question: What does BERTScore measure and how is it calculated?

## Architecture Onboarding

- Component map: GPT-3 model with discussion examples as prompts → NLI prediction → discussion generation → final label determination
- Critical path: Prompt construction → model inference → BERTScore evaluation → human evaluation
- Design tradeoffs: Cost vs. quality of discussion data (manual vs. pseudo-generated)
- Failure signatures: System becomes too compliant, generates irrelevant content, or fails to refute incorrect arguments
- First 3 experiments:
  1. Compare zero-shot, few-shot, and few-shot-discussion systems on NLI accuracy
  2. Evaluate BERTScore similarity for supportive vs. unsupportive utterances
  3. Test pseudo-discussion data generation and its impact on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are discussions in improving model performance across different NLP tasks beyond NLI?
- Basis in paper: [explicit] The paper focuses on discussions for NLI but suggests exploring discussions for other tasks.
- Why unresolved: The paper only presents results for NLI, leaving open the question of how discussions would impact other NLP tasks.
- What evidence would resolve it: Experiments applying the discussion approach to various NLP tasks and comparing performance gains.

### Open Question 2
- Question: What is the optimal number and type of discussion turns needed to improve model performance?
- Basis in paper: [inferred] The paper mentions discussions until a final label is agreed upon, but doesn't explore the impact of different numbers or types of turns.
- Why unresolved: The paper doesn't systematically investigate how varying the length or nature of discussions affects performance.
- What evidence would resolve it: Experiments varying the number and type of discussion turns and measuring the resulting performance.

### Open Question 3
- Question: How robust are the systems to adversarial or misleading human input during discussions?
- Basis in paper: [explicit] The paper notes that systems tend to be too compliant with human opinions and suggests addressing the risk of transmitting incorrect knowledge.
- Why unresolved: The paper doesn't explore the systems' performance when exposed to intentionally misleading or adversarial human input during discussions.
- What evidence would resolve it: Experiments testing the systems' performance and robustness when exposed to various forms of adversarial or misleading human input during discussions.

## Limitations
- Findings based on a single NLP task (NLI) may not generalize to other domains
- Human discussion dataset is relatively small (102 problems), potentially limiting robustness
- BERTScore evaluation may not fully capture nuances of human reasoning and argumentation

## Confidence

- **High Confidence**: The claim that discussion-based systems improve NLI accuracy compared to non-discussion systems (supported by quantitative results showing up to 25-point improvement).
- **Medium Confidence**: The effectiveness of few-shot learning with discussion examples in generating supportive utterances (supported by BERTScore comparisons but limited by dataset size).
- **Medium Confidence**: The claim that pseudo-discussion data can achieve comparable performance to manually created data (supported by results in 2 of 4 datasets but needs further validation).

## Next Checks
1. Test the discussion-based approach on additional NLP tasks beyond NLI to assess generalizability across different problem types.
2. Conduct a larger-scale human evaluation with diverse annotator pools to validate the robustness of discussion quality metrics and reduce potential biases.
3. Compare the discussion-based approach against other collaborative AI methods, such as chain-of-thought prompting or ensemble methods, to establish relative effectiveness.