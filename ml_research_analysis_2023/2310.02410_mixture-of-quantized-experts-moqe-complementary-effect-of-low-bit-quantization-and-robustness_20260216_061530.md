---
ver: rpa2
title: 'Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit Quantization
  and Robustness'
arxiv_id: '2310.02410'
source_url: https://arxiv.org/abs/2310.02410
tags:
- quantization
- dense
- layers
- expert
- bleu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mixture-of-Experts (MoE) models have become increasingly popular
  due to their ability to scale efficiently while maintaining high performance on
  various language tasks. However, MoE models face challenges related to memory consumption
  and increased memory bandwidth requirements during deployment.
---

# Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit Quantization and Robustness

## Quick Facts
- arXiv ID: 2310.02410
- Source URL: https://arxiv.org/abs/2310.02410
- Reference count: 40
- Key outcome: MoQE reduces model size by 79.6% while maintaining performance and achieving 1.24X speed-up on A100 GPUs

## Executive Summary
This paper introduces Mixture of Quantized Experts (MoQE), a weight-only quantization method that applies ultra low-bit quantization specifically to expert weights in MoE models. The key insight is that expert layers in MoE models are significantly more robust to quantization than conventional dense layers, enabling aggressive 2-bit quantization while maintaining model quality. MoQE achieves substantial memory savings and inference speed improvements without compromising translation quality on multilingual tasks.

## Method Summary
MoQE applies selective weight-only quantization to expert layers in MoE models, using channel-wise linear quantization with scaling factors. The method employs post-training quantization for 3-bit and above, while 2-bit quantization requires quantization-aware training (QAT). Expert weights are quantized while other components remain at higher precision. The approach is validated on multilingual machine translation tasks with 20 language directions, using transformer architectures with MoE layers at every other position and 32-128 experts depending on model size.

## Key Results
- Expert layers in MoE models maintain accuracy with 2-bit quantization while dense layers cannot
- MoQE reduces model size by 79.6% compared to fp16 MoE models
- Combined with optimized GPU runtime, achieves 1.24X speed-up on A100 GPUs
- 2-bit quantized MoE models outperform dense models trained on the same data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expert FFN layers are more robust to quantization than dense FFN layers due to weight distribution characteristics
- Mechanism: Expert weight matrices have smaller numerical ranges and fewer outliers compared to dense layers
- Core assumption: Weight distribution properties directly correlate with quantization robustness
- Evidence anchors: Expert layers have much smaller range than dense layers; dense layers have more outliers
- Break condition: If expert FFN layers develop outlier distributions through training or routing becomes highly skewed

### Mechanism 2
- Claim: Selective quantization of expert weights preserves model performance while reducing memory footprint
- Mechanism: Only expert weights are quantized (which are robust), other components remain at higher precision
- Core assumption: Expert weights contribute sufficient model capacity while being less sensitive to precision loss
- Evidence anchors: 79.6% model size reduction while maintaining performance; 2-bit MoE outperforms dense models
- Break condition: If routing instability causes certain experts to dominate, or non-expert components become bottlenecks

### Mechanism 3
- Claim: Low-bit quantization enables faster inference through reduced memory bandwidth requirements
- Mechanism: Quantized weights require less memory bandwidth, combined with optimized GPU runtime for speed gains
- Core assumption: Memory bandwidth is the primary bottleneck for MoE inference
- Evidence anchors: 1.24X speed-up on A100 GPUs; notable surge in memory bandwidth requirement during inference
- Break condition: If computation becomes bottleneck instead of memory bandwidth, or quantized weights cause cache inefficiencies

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: Understanding MoE is fundamental to why quantization works differently on expert vs dense layers
  - Quick check question: What is the primary advantage of MoE architecture over dense models in terms of parameter efficiency?

- Concept: Quantization methods and their sensitivity to weight distributions
  - Why needed here: Different quantization techniques behave differently based on weight statistics
  - Quick check question: How do outliers in weight distributions typically affect quantization quality?

- Concept: GPU memory bandwidth vs computation bottlenecks
  - Why needed here: Performance gains depend on understanding what limits inference speed
  - Quick check question: What factors determine whether memory bandwidth or computation is the bottleneck in transformer inference?

## Architecture Onboarding

- Component map: Transformer encoder/decoder -> MoE layers at even positions -> Expert weight quantization -> Optional QAT for 2-bit -> Optimized GPU runtime
- Critical path: Model training → Expert weight analysis → Selective quantization application → Optional QAT for 2-bit → Optimized runtime integration → Inference deployment
- Design tradeoffs: (1) Precision vs memory savings - lower bits save more memory but may need QAT, (2) Selective vs full model quantization - selective preserves accuracy better, (3) Runtime optimization complexity vs inference speedup gains
- Failure signatures: (1) Significant BLEU score drops indicate quantization is harming critical components, (2) Routing instability may suggest expert weights are being damaged, (3) No speedup despite quantization indicates memory bandwidth isn't the bottleneck
- First 3 experiments:
  1. Compare weight distributions of expert vs dense FFN layers to verify the foundational robustness claim
  2. Test 4-bit post-training quantization on expert weights only and measure accuracy impact
  3. Measure memory bandwidth usage and inference throughput with and without MoQE quantization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the robustness of expert layers to quantization in MoE models compare to dense models when using other low-bit quantization techniques beyond the two methods tested?
- Basis in paper: The paper investigates two quantization techniques (linear and log-scale) and finds that expert layers are more robust to quantization than dense layers.
- Why unresolved: The study primarily focuses on two quantization techniques and does not explore other low-bit quantization methods that might yield different results in terms of robustness.
- What evidence would resolve it: Conducting experiments with additional low-bit quantization techniques and comparing their impact on the robustness of expert layers in MoE models versus dense models.

### Open Question 2
- Question: What are the potential reasons for the skewness in weight distribution observed in dense models' FFN layers compared to MoE models' expert FFN layers?
- Basis in paper: The paper mentions that dense FFN layers tend to have more outliers and a higher skewness in weight distribution compared to MoE FFN layers.
- Why unresolved: The study identifies the phenomenon but does not delve into the underlying causes of the skewness difference between dense and MoE models.
- What evidence would resolve it: Analyzing the architectural and training differences between dense and MoE models that might lead to the observed skewness in weight distribution.

### Open Question 3
- Question: How does the performance of MoE models with 2-bit expert weights compare to dense models when trained on different types of datasets or tasks beyond machine translation?
- Basis in paper: The paper demonstrates that MoE models with 2-bit expert weights can deliver better performance than dense models on multilingual machine translation tasks.
- Why unresolved: The study focuses on machine translation tasks and does not explore the generalizability of the results to other types of datasets or tasks.
- What evidence would resolve it: Evaluating the performance of MoE models with 2-bit expert weights on a diverse range of tasks and datasets to determine if the observed benefits extend beyond machine translation.

## Limitations

- The fundamental claim about weight distribution characteristics lacks direct empirical validation through visualizations or statistical analysis
- Speed-up claims rely on an unspecified optimized GPU runtime implementation, making it unclear how much is due to quantization vs implementation
- Ablation studies are incomplete - missing intermediate bit-widths and testing across diverse model architectures

## Confidence

- **High confidence**: Model size reduction claims (79.6%) are well-supported by the quantization methodology described
- **Medium confidence**: BLEU score preservation claims are reasonable given the selective quantization approach, but depend on the robustness of expert layers
- **Low confidence**: The mechanism explanation about weight distribution characteristics lacks direct empirical validation

## Next Checks

1. Generate and compare histograms/box plots of weight values for expert vs dense FFN layers across multiple trained models to verify claimed distribution differences and outlier presence

2. Test 3-bit quantization on expert weights with both post-training and QAT methods across multiple language pairs to establish minimum bit-width threshold for quality preservation

3. Monitor expert activation patterns during inference with quantized weights to ensure routing decisions remain stable and no single expert becomes a bottleneck due to quantization-induced weight distortions