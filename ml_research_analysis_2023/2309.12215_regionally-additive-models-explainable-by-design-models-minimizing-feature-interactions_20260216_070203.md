---
ver: rpa2
title: 'Regionally Additive Models: Explainable-by-design models minimizing feature
  interactions'
arxiv_id: '2309.12215'
source_url: https://arxiv.org/abs/2309.12215
tags:
- feature
- each
- additive
- subregions
- interactions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Regionally Additive Models (RAMs), a new
  class of explainable-by-design models that improve upon Generalized Additive Models
  (GAMs) by better capturing feature interactions. RAMs identify subregions within
  the feature space where the black-box model exhibits near-local additivity, allowing
  them to fit one component per subregion instead of one per feature.
---

# Regionally Additive Models: Explainable-by-design models minimizing feature interactions

## Quick Facts
- arXiv ID: 2309.12215
- Source URL: https://arxiv.org/abs/2309.12215
- Reference count: 0
- This paper introduces Regionally Additive Models (RAMs), a new class of explainable-by-design models that improve upon Generalized Additive Models (GAMs) by better capturing feature interactions.

## Executive Summary
Regionally Additive Models (RAMs) are a new class of explainable-by-design models that extend Generalized Additive Models (GAMs) by better capturing feature interactions. RAMs identify subregions within the feature space where the black-box model exhibits near-local additivity, allowing them to fit one component per subregion instead of one per feature. This approach yields more expressive models while retaining interpretability. The RAM framework consists of three steps: training a black-box model, identifying subregions using Regional Effect Plots, and fitting a GAM component for each identified subregion. Experiments on synthetic and real-world datasets demonstrate that RAMs offer improved expressiveness compared to GAMs while maintaining interpretability.

## Method Summary
RAM is a three-step pipeline that combines the strengths of black-box models and interpretable GAMs. First, a differentiable black-box model is trained on the target dataset. Second, Regional Effect Plots (using DALE) are computed to identify subregions where feature interactions are minimized. Third, a GAM component is fitted for each identified subregion using an extended feature space created from these subregions. The method leverages a tree-based algorithm to split the feature space, minimizing the weighted sum of regional interactions. This approach allows RAM to capture complex feature interactions while maintaining interpretability through conditional terms that can be visualized as one-dimensional or two-dimensional plots.

## Key Results
- On the Bike-Sharing dataset, RAM achieves an RMSE of 0.563 compared to 0.734 for GAM
- On the California Housing dataset, RAM achieves an RMSE of 0.754 compared to 0.819 for GAM
- RAM successfully captures three-feature interactions that standard GAMs miss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Splitting the input space into subregions allows fitting additive models in each region, capturing interactions that GAMs miss.
- Mechanism: RAM identifies regions where the black-box model's behavior is close to additive, then fits a GAM component for each subregion. This decomposition allows RAM to capture interactions without explicit interaction terms.
- Core assumption: The black-box function can be accurately decomposed into additive components within specific subregions of the feature space.
- Evidence anchors:
  - [abstract] "RAMs identify subregions within the feature space where interactions are minimized."
  - [section] "if we knew the appropriate subregions, namely, R21 = {x1 > 0 and x3 = 0} and R22 = {x1 â‰¤ 0 or x3 = 1 }, we could split the impact of x2 appropriately and fit the following model"

### Mechanism 2
- Claim: Regional Effect Plots can identify subregions where feature interactions are minimized.
- Mechanism: RAM uses DALE (Differential Accumulated Local Effects) to measure feature interactions in different regions. It then uses a tree-based algorithm to split the feature space, minimizing weighted sum of regional interactions.
- Core assumption: The DALE method accurately captures feature interactions, and the tree-based algorithm effectively identifies optimal splits.
- Evidence anchors:
  - [section] "we use regional effect methods [Herbinger et al., 2023, 2022] to identify the regions where the black-box model exhibits near-local additivity."
  - [section] "we have developed a tree-based algorithm based on the approach proposed by [Herbinger et al., 2023]"

### Mechanism 3
- Claim: RAM maintains interpretability by using conditional terms that can be visualized as one-dimensional or two-dimensional plots.
- Mechanism: RAM represents the model as a sum of conditional components, each active in specific subregions. These components can be visualized independently, preserving the interpretability of GAMs.
- Core assumption: Conditional terms with at most two variables can be effectively visualized and interpreted by users.
- Evidence anchors:
  - [abstract] "RAMs identify subregions within the feature space where interactions are minimized. Within these regions, it is more accurate to express the output as a sum of univariate functions (components)."
  - [section] "RAM can accurately represent f through learning two second-degree conditional terms, one for each marital status. Furthermore, the two sets of terms can be visualized and interpreted as using two-dimensional plots."

## Foundational Learning

- Concept: Generalized Additive Models (GAMs)
  - Why needed here: RAM builds upon GAMs by extending their ability to capture feature interactions while maintaining interpretability.
  - Quick check question: What is the basic structure of a GAM model?

- Concept: Feature Interactions
  - Why needed here: Understanding feature interactions is crucial for grasping why RAMs are needed and how they improve upon GAMs.
  - Quick check question: Why do standard GAMs fail to capture feature interactions?

- Concept: Black-box models and their limitations
  - Why needed here: RAM uses a black-box model as a starting point to identify subregions where the function is approximately additive.
  - Quick check question: What are the trade-offs between using black-box models and interpretable models like GAMs?

## Architecture Onboarding

- Component map: Black-box model training -> DALE computation -> Subregion detection -> GAM component fitting -> Model evaluation
- Critical path:
  1. Train black-box model
  2. Compute Jacobian for DALE
  3. Detect subregions
  4. Fit GAM components
  5. Evaluate model performance

- Design tradeoffs:
  - Number of subregions vs. interpretability
  - Depth of tree search vs. computational cost
  - Choice of black-box model vs. accuracy of subregion detection

- Failure signatures:
  - High RMSE compared to black-box model
  - Too many subregions identified (loss of interpretability)
  - Subregions that don't capture meaningful patterns

- First 3 experiments:
  1. Implement RAM on the toy example from Section 2 to verify it captures the three-feature interaction.
  2. Compare RAM performance to GAM on the Bike-Sharing dataset, focusing on the working day vs. non-working day distinction.
  3. Experiment with different depths of tree search on the California Housing dataset to find the optimal balance between accuracy and interpretability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does RAM consistently outperform GAM and other interpretable models across diverse datasets and problem domains?
- Basis in paper: [explicit] The paper concludes that "RAMs can provide more accurate predictions compared to GAMs while maintaining the same level of interpretability" and calls for systematic evaluation on a larger set of datasets to ensure improvements are not specific to particular datasets.
- Why unresolved: The experiments were conducted on only two datasets (Bike-Sharing and California Housing), which may not be representative of all potential applications of RAM.
- What evidence would resolve it: Extensive experiments on a wide variety of datasets with different characteristics (e.g., number of features, feature types, interaction complexity) demonstrating consistent performance improvements of RAM over GAM and other interpretable models.

### Open Question 2
- Question: How does the choice of black-box model in the first step of RAM affect the quality of identified subregions and overall model performance?
- Basis in paper: [explicit] The paper mentions experimenting with various black-box models as a future direction and used a fully-connected Neural Network in their experiments.
- Why unresolved: The paper only used one type of black-box model, so the impact of different choices on subregion identification and final performance remains unknown.
- What evidence would resolve it: Comparative experiments using different black-box models (e.g., random forests, gradient boosting, different neural network architectures) and analysis of how these choices affect subregion quality and final RAM performance.

### Open Question 3
- Question: Are there alternative clustering algorithms or methods for subregion detection that could improve upon the tree-based approach proposed in RAM?
- Basis in paper: [explicit] The paper suggests exploring alternative clustering algorithms for the subregion detection step as a future direction.
- Why unresolved: The paper only evaluated their specific tree-based algorithm, leaving open the question of whether other approaches might yield better subregions.
- What evidence would resolve it: Experiments comparing the proposed tree-based subregion detection method with other clustering approaches (e.g., k-means, hierarchical clustering, density-based methods) and analysis of their impact on RAM performance and interpretability.

## Limitations
- RAM's performance is highly sensitive to the quality of the black-box model used for initial training
- The method may struggle with highly complex interactions that cannot be adequately captured by conditional terms
- While RAMs reduce dimensionality by grouping features into subregions, they may still face challenges with interpretability when too many subregions are identified

## Confidence
- High confidence: The core mechanism of splitting feature space into subregions to capture interactions (Mechanism 1)
- Medium confidence: The effectiveness of Regional Effect Plots for identifying subregions (Mechanism 2)
- Medium confidence: The claim that RAMs maintain interpretability through conditional terms (Mechanism 3)

## Next Checks
1. Test RAM on datasets with known complex interactions to verify the method can identify and capture these interactions effectively
2. Compare RAM performance across different black-box model architectures to assess sensitivity to initial model choice
3. Evaluate the impact of varying tree search depth on both model accuracy and interpretability to find optimal hyperparameters for practical use