---
ver: rpa2
title: 'PrIeD-KIE: Towards Privacy Preserved Document Key Information Extraction'
arxiv_id: '2310.03777'
source_url: https://arxiv.org/abs/2310.03777
tags:
- privacy
- document
- learning
- datasets
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces strategies for developing private Key Information\
  \ Extraction (KIE) systems by leveraging large pretrained document foundation models\
  \ in conjunction with differential privacy (DP), federated learning (FL), and Differentially\
  \ Private Federated Learning (DP-FL). Through extensive experimentation on six benchmark\
  \ datasets (FUNSD, CORD, SROIE, WildReceipts, XFUND, and DOCILE), the authors demonstrate\
  \ that large document foundation models can be effectively fine-tuned for the KIE\
  \ task under private settings to achieve adequate performance while maintaining\
  \ strong privacy guarantees (\u03F5 \u2208 {8, 20} for DP and DP-FL)."
---

# PrIeD-KIE: Towards Privacy Preserved Document Key Information Extraction

## Quick Facts
- arXiv ID: 2310.03777
- Source URL: https://arxiv.org/abs/2310.03777
- Reference count: 40
- Key outcome: Introduces DP-Adam, task-specific pretraining, and FeAm-DP algorithm for private KIE, achieving strong performance under privacy budgets (ε ∈ {8, 20}) across six datasets.

## Executive Summary
This paper addresses the challenge of privacy-preserving Key Information Extraction (KIE) from documents by leveraging large pretrained document foundation models with differential privacy (DP), federated learning (FL), and Differentially Private Federated Learning (DP-FL). Through extensive experimentation on six benchmark datasets, the authors demonstrate that large document foundation models can be effectively fine-tuned for the KIE task under private settings to achieve adequate performance while maintaining strong privacy guarantees. The proposed guidelines for optimal privacy-utility trade-off and the novel FeAm-DP algorithm enable efficient upscaling of global DP from standalone to multi-client federated environments, achieving comparable performance and privacy guarantees even with an increasing number of participating clients.

## Method Summary
The method involves fine-tuning the LayoutLMv3 model for KIE using DP-Adam with large batch sizes, high learning rates, and small clipping norms. Task-specific pretraining is applied when relevant datasets with overlapping entity types are available. For federated settings, the FeAm-DP algorithm extends global DP guarantees to multi-client environments by scaling client-level noise and averaging gradients globally. The approach is evaluated across six benchmark datasets (FUNSD, CORD, SROIE, WildReceipts, XFUND, DOCILE) under different privacy budgets (ε ∈ {8, 20}).

## Key Results
- DP-Adam with large batch sizes (B=512) and high learning rates achieves better privacy-utility tradeoffs than standard DP-SGD for KIE.
- Task-specific pretraining improves performance by 2-6.5% when pretraining datasets share entity label overlap with target datasets.
- FeAm-DP achieves comparable performance and privacy guarantees to standalone DP even with increasing numbers of clients (K ∈ {2, 4, 8, 16}).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DP-Adam with large batch sizes and high learning rates achieves better privacy-utility tradeoff for KIE.
- Mechanism: Large batch sizes reduce per-example noise impact and increase effective gradient stability; high learning rates accelerate convergence before noise dominates.
- Core assumption: Sampling rate q between 1/10 and 1/3 yields optimal gradient estimation without excessive privacy cost.
- Evidence anchors:
  - [abstract]: Demonstrates sufficient utility under strong privacy guarantees with tuned hyperparameters.
  - [section]: Figures 2-4 show larger batch sizes, higher learning rates, and smaller clipping norms improve performance under DP.
  - [corpus]: Weak—no direct citations; inferred from experimental design.
- Break condition: If batch size is too small, noise overwhelms gradient signal; if learning rate is too high, training becomes unstable.

### Mechanism 2
- Claim: Task-specific pretraining (TSP) improves DP fine-tuning by aligning entity label distributions.
- Mechanism: Pretraining on related datasets transfers entity type knowledge, reducing the amount of private data needed to adapt the model.
- Core assumption: Datasets with overlapping entity types (e.g., CORD and WildReceipts) provide relevant prior knowledge.
- Evidence anchors:
  - [abstract]: Shows TSP yields performance improvements ranging from ~2% to ~6.5%.
  - [section]: Table 2 demonstrates performance gains when pretraining datasets share entity label overlap.
  - [corpus]: Weak—no corpus citations; based on internal analysis.
- Break condition: If pretraining dataset has no entity label overlap, TSP provides negligible or negative benefit.

### Mechanism 3
- Claim: FeAm-DP enables global DP guarantees in federated settings without per-client DP overhead.
- Mechanism: Client-level noise is scaled (√m) and gradients averaged globally, preserving the same privacy budget as standalone DP while reducing per-client noise impact.
- Core assumption: Each client holds an equal partition of the global dataset, so averaging preserves the per-example sampling rate q.
- Evidence anchors:
  - [abstract]: FeAm-DP achieves comparable performance and privacy guarantees to standalone DP even with increasing clients.
  - [section]: Algorithm 1 and surrounding text show noise scaling and sampling rate adjustment logic.
  - [corpus]: Weak—no external citations; internally derived from DP theory extension.
- Break condition: If client data partitions are highly imbalanced, privacy guarantees and performance may degrade.

## Foundational Learning

- Concept: Differential Privacy (DP) and privacy accounting (RDP, Gaussian, PRV).
  - Why needed here: Ensures model training leaks minimal information about private document data.
  - Quick check question: What is the difference between (ε, δ)-DP and (α, ε)-RDP accounting?

- Concept: Federated Learning (FL) and client sampling strategies.
  - Why needed here: Enables multi-client privacy while keeping data on-device and reducing communication overhead.
  - Quick check question: How does client sampling rate C affect the total privacy cost in FeAm-DP?

- Concept: Document foundation models (LayoutLMv3) and multimodal pretraining.
  - Why needed here: Provides strong visual-text-layout representations that transfer well to KIE under private settings.
  - Quick check question: Why does adding image modality sometimes hurt performance in private fine-tuning?

## Architecture Onboarding

- Component map: LayoutLMv3 base -> DP-Adam fine-tuning -> TSP (optional) -> FeAm-DP wrapper (for FL) -> Privacy accountant -> Hyperparameter tuner -> Evaluation pipeline
- Critical path:
  1. Load pretrained LayoutLMv3 with text + image + layout inputs.
  2. Apply TSP if relevant datasets available.
  3. Fine-tune with DP-Adam using large batch size, high LR, small clipping norm.
  4. For FL, wrap with FeAm-DP to scale to multiple clients.
  5. Evaluate F1-score and privacy bounds.
- Design tradeoffs:
  - Larger batch size → better privacy-utility but higher memory.
  - Full fine-tuning → best performance but computationally expensive.
  - Dropout → helpful only on small datasets; degrades performance on large ones.
- Failure signatures:
  - Low F1 + high ε → too much noise, need larger batch or weaker privacy.
  - Training instability → learning rate too high, reduce by factor of 2.
  - Degraded performance with TSP → entity label mismatch, try different pretraining set.
- First 3 experiments:
  1. Non-private baseline on FUNSD with full fine-tuning.
  2. DP-Adam fine-tuning on FUNSD with L=128, B=64, η=5e-4, S=0.1, TSP=CORD.
  3. FeAm-DP with K=4 clients on FUNSD, reusing DP hyperparameters from (2).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the privacy-utility trade-offs of DP-Adam and DP-SGD differ across various datasets and privacy budgets, and what are the optimal hyperparameter configurations for each?
- Basis in paper: [explicit] The paper briefly mentions that DP-SGD performed comparably to DP-Adam in some experiments but suggests further investigation is warranted.
- Why unresolved: The paper primarily focuses on DP-Adam and does not provide a comprehensive comparison between DP-Adam and DP-SGD across all datasets and privacy budgets.
- What evidence would resolve it: A thorough experimental comparison of DP-Adam and DP-SGD across all datasets and privacy budgets, with detailed analysis of the privacy-utility trade-offs and optimal hyperparameter configurations for each.

### Open Question 2
- Question: What are the specific mechanisms by which the addition of image modality information impacts model performance in both non-private and private settings, and how can this information be leveraged to improve model accuracy?
- Basis in paper: [explicit] The paper observes that the impact of image modality information on model performance varies across datasets and privacy settings, but does not delve into the underlying mechanisms.
- Why unresolved: The paper does not provide a detailed analysis of the mechanisms by which image modality information affects model performance, nor does it offer specific strategies for leveraging this information to improve accuracy.
- What evidence would resolve it: A comprehensive analysis of the impact of image modality information on model performance, including visualizations and explanations of the learned representations, and the development of strategies to effectively incorporate image information for improved accuracy.

### Open Question 3
- Question: How does the performance of FeAm-DP scale with increasing numbers of clients and varying levels of data heterogeneity across clients?
- Basis in paper: [explicit] The paper evaluates FeAm-DP with a limited number of clients (K ∈ {2, 4, 8, 16}) and does not investigate the impact of data heterogeneity on performance.
- Why unresolved: The paper does not provide insights into the scalability of FeAm-DP with a larger number of clients or the effects of data heterogeneity on performance.
- What evidence would resolve it: Extensive experiments with a larger number of clients and varying levels of data heterogeneity, along with analysis of the scalability and performance of FeAm-DP under these conditions.

## Limitations
- The FeAm-DP algorithm's implementation details and client-level noise handling remain underspecified, making exact replication challenging.
- The performance improvements from task-specific pretraining are highly dataset-dependent and may not generalize to domains with minimal entity label overlap.
- The computational requirements for large batch sizes (e.g., B=512) may limit practical deployment on resource-constrained devices.

## Confidence

- **High Confidence**: The core finding that DP-Adam with large batch sizes and high learning rates improves privacy-utility tradeoffs is well-supported by systematic experimentation across six datasets.
- **Medium Confidence**: The effectiveness of task-specific pretraining depends strongly on entity label overlap between pretraining and target datasets, making its benefits conditional rather than universal.
- **Medium Confidence**: FeAm-DP's theoretical privacy guarantees are sound, but empirical validation across diverse client distributions and data imbalances remains limited.

## Next Checks

1. Implement and test FeAm-DP with imbalanced client data distributions to verify privacy guarantees hold when partitions are non-uniform.
2. Conduct ablation studies removing task-specific pretraining to quantify its contribution across datasets with varying entity label overlap.
3. Benchmark memory and computational requirements for large-batch DP-Adam training on different hardware configurations to assess practical deployment feasibility.