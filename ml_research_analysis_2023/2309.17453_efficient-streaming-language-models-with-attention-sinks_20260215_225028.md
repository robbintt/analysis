---
ver: rpa2
title: Efficient Streaming Language Models with Attention Sinks
arxiv_id: '2309.17453'
source_url: https://arxiv.org/abs/2309.17453
tags:
- attention
- tokens
- window
- token
- sink
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces StreamingLLM, an efficient framework enabling
  large language models to handle streaming inputs of unlimited length without fine-tuning.
  The key challenge addressed is the failure of window attention when the cache size
  is exceeded, which is explained by the "attention sink" phenomenon where models
  disproportionately focus on initial tokens.
---

# Efficient Streaming Language Models with Attention Sinks

## Quick Facts
- arXiv ID: 2309.17453
- Source URL: https://arxiv.org/abs/2309.17453
- Reference count: 7
- Models including Llama-2, MPT, Falcon, and Pythia can reliably model 4 million tokens using StreamingLLM

## Executive Summary
This paper introduces StreamingLLM, an efficient framework that enables large language models to handle streaming inputs of unlimited length without fine-tuning. The key challenge addressed is the failure of window attention when the cache size is exceeded, explained by the "attention sink" phenomenon where models disproportionately focus on initial tokens. StreamingLLM stabilizes attention by preserving the KV states of a few initial tokens alongside recent tokens, achieving stable performance on texts up to 4 million tokens. The method achieves up to 22.2× speedup compared to sliding window recomputation while maintaining similar memory usage.

## Method Summary
StreamingLLM addresses the challenge of handling streaming inputs longer than pre-training sequence lengths by exploiting the attention sink phenomenon. When initial tokens are evicted from the cache, their contribution to the softmax denominator disappears, causing performance collapse. StreamingLLM preserves the KV states of a few initial tokens (typically 4) alongside recent tokens, maintaining the attention score distribution close to normal. Additionally, the paper proposes pre-training with a designated sink token—a learnable placeholder at the beginning of all training samples—which allows models to offload unnecessary attention scores to a single token, further improving streaming performance.

## Key Results
- StreamingLLM achieves stable performance on texts up to 4 million tokens without fine-tuning
- Models including Llama-2, MPT, Falcon, and Pythia can reliably model 4 million tokens
- StreamingLLM achieves up to 22.2× speedup compared to sliding window recomputation baseline
- Pre-training with a designated sink token further improves streaming performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Initial tokens act as attention sinks because the softmax constraint forces unused attention mass to be allocated somewhere, and initial tokens are globally visible.
- Core assumption: The attention sink phenomenon is primarily driven by the mathematical constraint of softmax rather than semantic importance of initial tokens.
- Evidence anchors:
  - [abstract]: "We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention"
  - [section]: "We attribute the reason to the Softmax operation, which requires attention scores to sum up to one for all contextual tokens"
  - [corpus]: Weak - no direct evidence in corpus about softmax constraints

### Mechanism 2
- Claim: Preserving attention sink tokens' KV states maintains the attention score distribution closer to normal, preventing performance collapse.
- Core assumption: The attention sink tokens serve as anchors that stabilize the attention distribution across all layers and heads.
- Evidence anchors:
  - [abstract]: "StreamingLLM exploits the fact that attention sinks have high attention values, and preserving them can maintain the attention score distribution close to normal"
  - [section]: "Removing these initial tokens' KV will remove a considerable portion of the denominator in the SoftMax function"
  - [corpus]: Weak - no direct evidence about attention distribution stability

### Mechanism 3
- Claim: Pre-training with a designated sink token allows models to offload unnecessary attention scores to a single token, reducing the need for multiple initial tokens as sinks.
- Core assumption: A consistent, designated sink token during pre-training can replace the need for multiple initial tokens to serve as attention sinks.
- Evidence anchors:
  - [abstract]: "we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment"
  - [section]: "Introducing a sink token is highly effective in stabilizing the attention mechanism"
  - [corpus]: Weak - no direct evidence in corpus about pre-training with sink tokens

## Foundational Learning

- Concept: Transformer attention mechanism with softmax
  - Why needed here: Understanding how attention scores are computed and normalized is crucial to grasping why initial tokens become attention sinks
  - Quick check question: What mathematical constraint in the attention mechanism forces unused attention mass to be allocated somewhere?

- Concept: Autoregressive language modeling
  - Why needed here: The sequential nature of autoregressive generation means initial tokens are visible to all subsequent tokens, making them prime candidates for attention sinks
  - Quick check question: Why are initial tokens more visible to subsequent tokens compared to later tokens in autoregressive generation?

- Concept: KV cache mechanism
  - Why needed here: Understanding how key and value states are cached during inference is essential to grasp the window attention failure and StreamingLLM's solution
  - Quick check question: What happens to the attention computation when initial tokens' KV states are evicted from the cache?

## Architecture Onboarding

- Component map:
  Input tokens -> Positional encoding -> Multi-head attention -> Feed-forward network -> Output logits
  StreamingLLM adds: Attention sink token preservation alongside rolling KV cache
  Key modification: Modified positional encoding within cache rather than original text positions

- Critical path:
  1. Token generation with sliding window attention
  2. Detection of cache size limit reached
  3. Preservation of attention sink tokens' KV states
  4. Continued generation with stable attention distribution

- Design tradeoffs:
  - Memory usage vs. performance stability: Keeping attention sink tokens increases memory slightly but prevents performance collapse
  - Cache size vs. efficiency: Larger cache sizes don't consistently improve perplexity, suggesting models don't fully utilize extended context
  - Pre-training modifications vs. deployment flexibility: Adding sink tokens during pre-training improves streaming but requires architectural changes

- Failure signatures:
  - Performance collapse when initial tokens are evicted from cache
  - Unexpected attention concentration on semantically irrelevant initial tokens
  - Perplexity spikes when sequence length exceeds pre-training window size

- First 3 experiments:
  1. Measure attention distribution when initial tokens are present vs. evicted from cache
  2. Test different numbers of attention sink tokens (1, 2, 4, 8) to find minimum effective number
  3. Compare performance of models pre-trained with vs. without designated sink token in streaming settings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can pre-training LLMs with a designated sink token be extended to other types of models or tasks beyond language modeling?
- Basis in paper: [explicit] The paper discusses the effectiveness of using a designated sink token during pre-training for language models.
- Why unresolved: The paper primarily focuses on language models, and it's unclear whether the same approach would be effective for other types of models or tasks.
- What evidence would resolve it: Conducting experiments to pre-train different types of models (e.g., vision transformers) or for different tasks (e.g., image classification) with a designated sink token would provide evidence.

### Open Question 2
- Question: What is the optimal number of initial tokens to use as attention sinks in StreamingLLM?
- Basis in paper: [explicit] The paper suggests using four initial tokens as attention sinks but also mentions that the model might not solely use the first token as the attention sink.
- Why unresolved: The paper does not provide a thorough investigation into the optimal number of initial tokens, and the choice of four seems arbitrary.
- What evidence would resolve it: Conducting experiments with varying numbers of initial tokens as attention sinks and comparing the performance would provide evidence.

### Open Question 3
- Question: Can the attention sink phenomenon be leveraged to improve the efficiency of other attention-based models or tasks?
- Basis in paper: [explicit] The paper introduces the concept of attention sinks and shows how it can be used to improve the efficiency of language models.
- Why unresolved: The paper only explores the application of attention sinks in the context of language models, and it's unclear whether the same approach could be beneficial for other attention-based models or tasks.
- What evidence would resolve it: Conducting experiments to apply the attention sink concept to other attention-based models (e.g., vision transformers) or tasks (e.g., image captioning) would provide evidence.

## Limitations

- The "attention sink" phenomenon is observed empirically but lacks rigorous theoretical explanation
- The effectiveness of preserving initial tokens' KV states is demonstrated across multiple model families, but the optimal number of sink tokens varies without clear theoretical justification
- Pre-training with designated sink tokens introduces architectural dependencies that may limit deployment flexibility

## Confidence

**High Confidence:** The empirical observation that window attention fails when initial tokens are evicted from the cache is well-supported by experimental results across multiple model families.

**Medium Confidence:** The explanation that softmax constraints drive the attention sink phenomenon is plausible but not definitively proven.

**Low Confidence:** The claim that pre-training with a designated sink token significantly improves streaming performance is supported by limited experimental evidence.

## Next Checks

1. **Ablation study on softmax variants:** Test StreamingLLM's effectiveness when using attention mechanisms without strict softmax constraints (e.g., Softmax-off-by-one, or sparse attention variants).

2. **Cross-model sink token optimization:** Systematically investigate the relationship between model architecture parameters and the optimal number of sink tokens required.

3. **Semantic coherence evaluation:** Design experiments that specifically test whether StreamingLLM preserves long-range semantic dependencies in generated text, beyond just maintaining perplexity.