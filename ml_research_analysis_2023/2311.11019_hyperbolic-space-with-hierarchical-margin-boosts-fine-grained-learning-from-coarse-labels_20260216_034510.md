---
ver: rpa2
title: Hyperbolic Space with Hierarchical Margin Boosts Fine-Grained Learning from
  Coarse Labels
arxiv_id: '2311.11019'
source_url: https://arxiv.org/abs/2311.11019
tags:
- fine-grained
- learning
- space
- hyperbolic
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of learning fine-grained visual
  representations from coarse labels, a task with practical importance due to the
  high cost of fine-grained annotations. The authors propose a method that embeds
  features into a hyperbolic space and enhances their discriminative ability using
  hierarchical cosine margins.
---

# Hyperbolic Space with Hierarchical Margin Boosts Fine-Grained Learning from Coarse Labels

## Quick Facts
- arXiv ID: 2311.11019
- Source URL: https://arxiv.org/abs/2311.11019
- Reference count: 40
- Primary result: Achieves state-of-the-art performance on five benchmark datasets, surpassing competing methods by 4.05% and 6.36% on CIFAR-100 in 5-way and all-way settings respectively

## Executive Summary
This paper addresses the challenge of learning fine-grained visual representations from coarse labels, a task with practical importance due to the high cost of fine-grained annotations. The authors propose a method that embeds features into a hyperbolic space and enhances their discriminative ability using hierarchical cosine margins. Specifically, the hyperbolic space captures hierarchical relationships and increases expressive power, while the hierarchical cosine margins enforce large/small similarity margins between coarse/fine classes. An adaptive strategy dynamically updates target distances during training to reflect the real data distribution. Extensive experiments on five benchmark datasets show that the proposed method achieves state-of-the-art performance, surpassing competing methods by 4.05% and 6.36% on CIFAR-100 in 5-way and all-way settings, respectively.

## Method Summary
The method embeds visual features into a hyperbolic space using the Poincaré ball model, then applies hierarchical cosine margins to enforce different similarity distances between sample pairs at different levels of the hierarchy (same instance, same fine-grained class, same coarse-grained class, different coarse-grained classes). The approach uses k-means clustering to generate pseudo fine-grained labels from coarse labels, and employs an adaptive strategy to dynamically update target distances during training based on observed batch statistics. The overall loss combines cross-entropy loss for coarse classification with a hierarchical cosine margin loss that enforces the desired distance constraints.

## Key Results
- Achieves state-of-the-art performance on five benchmark datasets
- Surpasses competing methods by 4.05% and 6.36% on CIFAR-100 in 5-way and all-way settings respectively
- Demonstrates effectiveness in bridging the gap between coarse-grained and fine-grained labels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hyperbolic space captures hierarchical relationships more efficiently than Euclidean space for coarse-to-fine tasks
- Mechanism: The hyperbolic space has constant negative curvature that enables exponential growth of distances with depth, matching the exponential growth of nodes in hierarchical structures. This allows the model to represent multiple levels of granularity (coarse, fine, instance) within a finite-dimensional embedding space
- Core assumption: The label hierarchy in coarse-to-fine tasks follows a tree-like structure where fine-grained classes are naturally grouped under coarse classes
- Evidence anchors:
  - [abstract] "the hyperbolic space offers distinct advantages, including the ability to capture hierarchical relationships and increased expressive power, which favors modeling fine-grained objects"
  - [section 3] "Compared to the Euclidean space which has a constant sectional curvature of 0, the hyperbolic space has a constant negative curvature, which enables more efficient low-dimensional embedding for modeling hierarchical structure property data"
  - [corpus] Weak evidence - no direct citations supporting this specific claim about coarse-to-fine tasks
- Break condition: If the true label hierarchy is not tree-structured (e.g., multi-label or overlapping hierarchies), the hyperbolic space's representational advantages diminish

### Mechanism 2
- Claim: Hierarchical cosine margins enforce appropriate similarity gaps between different levels of granularity
- Mechanism: By assigning different target cosine distances for same instance, same fine-grained class, same coarse-grained class, and different coarse-grained classes, the model learns to create larger margins between coarser categories and smaller margins between finer categories, reflecting their semantic similarity
- Core assumption: The four-level hierarchical relationship structure (instance → fine → coarse → different) is appropriate for the task and dataset
- Evidence anchors:
  - [abstract] "we further enforce relatively large/small similarity margins between coarse/fine classes, respectively, yielding the so-called hierarchical cosine margins manner"
  - [section 3] "we set the distances between sample pairs with the same sample, the same fine-grained category, the same coarse-grained category, and different coarse-grained categories as d0, d1, d2, and d3, respectively, where d0 < d1 < d2 < d3"
  - [corpus] Weak evidence - no direct citations supporting this specific hierarchical margin approach
- Break condition: If the actual semantic similarity between classes doesn't follow this hierarchy (e.g., fine-grained classes are actually more dissimilar than coarse classes), the margin enforcement becomes counterproductive

### Mechanism 3
- Claim: Adaptive hierarchical cosine distance dynamically adjusts target margins based on data distribution
- Mechanism: The adaptive strategy updates d1 and d2 (target distances for fine-grained and coarse-grained pairs) using momentum updates based on the average observed distances in each batch, allowing the model to discover appropriate margins for the specific dataset rather than using fixed values
- Core assumption: The average pairwise distances in mini-batches provide a reasonable estimate of the true underlying distance distribution
- Evidence anchors:
  - [section 3] "we propose an Adaptive Hierarchical Cosine Distance (AHCD) strategy to update the target values during the training process according to the training data" and "We use the momentum update method to dynamically update the target semantic cosine distance"
  - [section 4] "We observe that the margin between coarse-grained categories (d2) is larger than the margin between fine-grained categories (d1), and the margin between fine-grained categories on CIFAR-100 is larger than the margin between fine-grained categories on LIVING-17, which aligns with the true category relationships in the data distribution"
  - [corpus] Weak evidence - no direct citations supporting this specific adaptive margin approach
- Break condition: If the batch statistics are unrepresentative of the full dataset distribution, the adaptive updates may converge to incorrect margin values

## Foundational Learning

- Concept: Hyperbolic geometry and Poincaré ball model
  - Why needed here: The method relies on mapping Euclidean features to hyperbolic space using exponential mapping (Eq. 1) and computing hyperbolic distances and inner products for classification
  - Quick check question: What is the key property of the Poincaré ball that makes it suitable for hierarchical data representation?

- Concept: Contrastive learning and similarity margins
  - Why needed here: The method builds on contrastive learning principles by enforcing different margins for different levels of similarity, extending traditional cosine margin approaches to hierarchical settings
  - Quick check question: How does enforcing larger margins between coarse classes versus fine classes help with fine-grained recognition?

- Concept: K-means clustering for pseudo-label generation
  - Why needed here: The method uses k-means clustering on coarse-class features to generate pseudo fine-grained labels, creating the multi-level supervision needed for hierarchical margin enforcement
  - Quick check question: Why might clustering be a reasonable approach for discovering fine-grained subcategories within coarse classes?

## Architecture Onboarding

- Component map: Encoder → Projector → Poincaré MLR (coarse classification) + HCM Module (hierarchical margin constraint) → Loss (Lcls + α·Lhcm)
- Critical path: The HCM module is the core innovation - it computes pairwise cosine distances between all features in a batch, assigns target distances based on hierarchical relationships, and enforces consistency via KL divergence
- Design tradeoffs: The method trades computational efficiency (O(N²) pairwise comparisons) for the ability to capture fine-grained distinctions without explicit fine-grained labels
- Failure signatures: Poor performance on datasets where fine-grained classes don't naturally cluster within coarse classes, or where the hierarchy is too flat or too deep for the fixed four-level structure
- First 3 experiments:
  1. Baseline: Train with only Poincaré MLR (hyperbolic space but no hierarchical margins) to isolate the benefit of the hyperbolic space
  2. HCM without adaptation: Fix d1 and d2 values instead of using adaptive updates to test if the adaptive strategy matters
  3. Euclidean HCM: Replace hyperbolic space with Euclidean space but keep hierarchical margins to test if both components are necessary

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform when class hierarchies exhibit substantial variations in scope, where some classes have much broader or narrower scope than others within the same hierarchy level?
- Basis in paper: [explicit] The authors note this is a limitation of their approach, stating "we have occasionally observed a particular scenario where our method yields comparatively lower results" when classes have varying scopes.
- Why unresolved: The paper does not provide experimental results or analysis of this specific scenario. It only mentions it as a future research direction.
- What evidence would resolve it: Experiments comparing the proposed method's performance on datasets with varying class scope variations, or modifications to handle class-specific semantic hierarchical distance constraints.

### Open Question 2
- Question: What is the optimal number of clusters for k-means clustering when generating fine-grained pseudo-labels, and how does this vary across different datasets and coarse-grained categories?
- Basis in paper: [explicit] The authors found "a recurring pattern that offers a guideline" suggesting the optimal number is roughly twice the actual number of fine-grained subclasses, but acknowledge this "may not be universally optimal."
- Why unresolved: The paper does not provide a definitive method for determining the optimal number of clusters, only a heuristic that works across tested datasets.
- What evidence would resolve it: A comprehensive study across diverse datasets to establish guidelines or an adaptive method for determining the optimal number of clusters for different coarse-grained categories.

### Open Question 3
- Question: How does the performance of the proposed method compare to other state-of-the-art methods on coarse-to-fine learning tasks that involve more than three levels of granularity (e.g., instance-level, fine-grained, coarse-grained, and meta-level)?
- Basis in paper: [inferred] The current method handles three levels of granularity (instance, fine-grained, and coarse), but the paper does not explore scenarios with more levels of granularity.
- Why unresolved: The experiments and comparisons are limited to three levels of granularity, and the method's scalability to more levels is not tested.
- What evidence would resolve it: Experiments comparing the proposed method's performance on datasets with more than three levels of granularity against other methods that can handle such scenarios.

## Limitations

- The method assumes a strict hierarchical label structure that may not generalize well to datasets with overlapping categories or non-tree-like relationships
- The computational complexity of the HCM module scales quadratically with batch size due to pairwise distance computations, limiting scalability
- The adaptive margin strategy relies on batch statistics that may be unrepresentative in early training stages or with small batch sizes

## Confidence

- **High confidence**: The core mechanism of using hyperbolic space for hierarchical data representation is well-established in the literature and the empirical improvements are substantial and consistent across multiple datasets
- **Medium confidence**: The effectiveness of the specific hierarchical margin enforcement scheme and adaptive distance updates, as these components lack direct citation support and may be sensitive to hyperparameter choices
- **Low confidence**: The generalizability to datasets with non-standard label hierarchies or where fine-grained distinctions don't align with natural clustering within coarse classes

## Next Checks

1. **Computational efficiency analysis**: Measure training time and memory usage as a function of batch size to quantify the scalability limitations of the pairwise distance computation in the HCM module
2. **Robustness to hierarchy structure**: Test the method on datasets with known non-tree label hierarchies (e.g., multi-label datasets) to evaluate performance degradation when the assumed hierarchy is violated
3. **Ablation on adaptive margins**: Compare performance using fixed vs. adaptive target distances across different dataset sizes and batch sizes to determine when the adaptive strategy provides meaningful benefits versus when it introduces instability