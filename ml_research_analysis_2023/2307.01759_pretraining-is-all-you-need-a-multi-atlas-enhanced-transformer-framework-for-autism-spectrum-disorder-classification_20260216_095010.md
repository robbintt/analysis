---
ver: rpa2
title: 'Pretraining is All You Need: A Multi-Atlas Enhanced Transformer Framework
  for Autism Spectrum Disorder Classification'
arxiv_id: '2307.01759'
source_url: https://arxiv.org/abs/2307.01759
tags:
- classification
- transformer
- data
- pretraining
- autism
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces METAFormer, a novel multi-atlas enhanced
  transformer framework for classifying Autism Spectrum Disorder (ASD) using resting-state
  functional MRI (rs-fMRI) data from the ABIDE I dataset. The framework employs a
  multi-atlas approach, using flattened connectivity matrices from the AAL, CC200,
  and DOS160 atlases as input to the transformer encoder.
---

# Pretraining is All You Need: A Multi-Atlas Enhanced Transformer Framework for Autism Spectrum Disorder Classification

## Quick Facts
- arXiv ID: 2307.01759
- Source URL: https://arxiv.org/abs/2307.01759
- Authors: 
- Reference count: 33
- Key outcome: METAFormer achieves 83.7% average accuracy and 0.832 AUC-score on ABIDE I dataset for ASD classification

## Executive Summary
This paper introduces METAFormer, a novel multi-atlas enhanced transformer framework for classifying Autism Spectrum Disorder (ASD) using resting-state functional MRI (rs-fMRI) data. The framework employs a multi-atlas approach, using flattened connectivity matrices from the AAL, CC200, and DOS160 atlases as input to the transformer encoder. The study demonstrates that self-supervised pretraining, which involves reconstructing masked values from the input, significantly enhances classification performance without the need for additional or separate training data. Evaluated through stratified cross-validation, METAFormer surpasses state-of-the-art performance on the ABIDE I dataset, achieving an average accuracy of 83.7% and an AUC-score of 0.832. The results indicate that the multi-atlas approach and pretraining contribute to improved robustness and stability of the model.

## Method Summary
The METAFormer framework uses a multi-atlas approach with three transformer encoders, each processing connectivity matrices from AAL, CC200, and DOS160 atlases. The model employs self-supervised pretraining via masked value reconstruction, followed by fine-tuning for ASD classification. The architecture uses a BERT-style transformer encoder with dmodel=256, dff=128, and h=4 attention heads. Training involves 10-fold stratified cross-validation with AdamW optimizer, early stopping, and hyperparameter optimization. The final prediction is obtained by averaging the outputs of the three transformers.

## Key Results
- METAFormer achieves 83.7% average accuracy and 0.832 AUC-score on ABIDE I dataset
- Self-supervised pretraining significantly improves performance across all models
- Multi-atlas approach outperforms single atlas models in robustness and stability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-supervised pretraining on masked imputation improves ASD classification by enabling the model to learn robust feature representations from incomplete data.
- Mechanism: The model is trained to predict masked values in the connectivity matrices, forcing it to infer missing information based on the global context. This process enhances the model's ability to capture meaningful patterns in the data.
- Core assumption: The ability to impute missing values correlates with improved classification performance because the model learns to represent underlying brain connectivity patterns more effectively.
- Evidence anchors:
  - [abstract]: "self-supervised pretraining, involving the reconstruction of masked values from the input, significantly enhances classification performance"
  - [section]: "we propose a self-supervised pretraining task for our model. Our approach involves imputing missing elements in the functional connectivity matrices"
  - [corpus]: Weak evidence - the corpus mentions related work on pretraining but does not specifically address masked imputation for ASD classification.
- Break condition: If the masked imputation task does not lead to improved classification performance, the mechanism fails.

### Mechanism 2
- Claim: Using multiple atlases as input improves classification robustness and stability by capturing different perspectives of brain connectivity.
- Mechanism: Each atlas provides a different parcellation of the brain, and the multi-atlas transformer architecture allows the model to learn from these diverse representations. The ensemble of transformers averages their outputs, leading to more stable predictions.
- Core assumption: Different atlases capture complementary information about brain connectivity, and combining them leads to better classification than any single atlas.
- Evidence anchors:
  - [abstract]: "METAFormer employs a multi-atlas approach, where flattened connectivity matrices from the AAL, CC200, and DOS160 atlases serve as input to the transformer encoder"
  - [section]: "We also evaluated the effect of self-supervised pretraining on the classification performance of our models. As Table 2 shows pretraining significantly improves the performance of all models"
  - [corpus]: Weak evidence - the corpus does not provide specific evidence for the effectiveness of multi-atlas approaches in ASD classification.
- Break condition: If combining multiple atlases does not improve classification performance compared to single atlas models, the mechanism fails.

### Mechanism 3
- Claim: The transformer architecture is well-suited for processing functional connectivity matrices due to its ability to capture long-range dependencies and complex interactions.
- Mechanism: The transformer encoder uses self-attention to weigh the importance of different brain regions and their interactions. This allows the model to learn complex patterns in the connectivity data that may be missed by traditional methods.
- Core assumption: The self-attention mechanism in transformers is effective at capturing the complex relationships in brain connectivity data.
- Evidence anchors:
  - [abstract]: "METAFormer employs a multi-atlas approach, where flattened connectivity matrices from the AAL, CC200, and DOS160 atlases serve as input to the transformer encoder"
  - [section]: "At the core of which lies the transformer encoder architecture, originally proposed by Vaswani et al. [28] for natural language processing (NLP) tasks"
  - [corpus]: Weak evidence - the corpus mentions related work on transformers but does not specifically address their application to functional connectivity data.
- Break condition: If the transformer architecture does not lead to improved classification performance compared to other architectures, the mechanism fails.

## Foundational Learning

- Concept: Functional connectivity analysis
  - Why needed here: Understanding how to compute and interpret functional connectivity matrices is crucial for working with rs-fMRI data and applying the proposed framework.
  - Quick check question: What is the Pearson correlation coefficient and how is it used to compute functional connectivity between brain regions?

- Concept: Transformer architecture and self-attention
  - Why needed here: The proposed framework is based on a transformer encoder, so understanding how transformers work and how self-attention operates is essential for implementing and modifying the model.
  - Quick check question: How does the self-attention mechanism in transformers allow the model to weigh the importance of different input features?

- Concept: Self-supervised learning
  - Why needed here: The framework uses self-supervised pretraining on a masked imputation task, so understanding the principles of self-supervised learning is important for implementing and adapting the pretraining strategy.
  - Quick check question: What is the difference between supervised and self-supervised learning, and why might self-supervised pretraining be beneficial for this task?

## Architecture Onboarding

- Component map:
  - Input layer: Flattened connectivity matrices from three atlases (AAL, CC200, DOS160)
  - Embedding layer: Linear layer with dmodel=256 to embed input into latent space
  - Transformer encoder: BERT-style encoder with N=2 layers, dff=128 feed forward units, and h=4 attention heads
  - Dropout layer: Applied after the final encoder layer
  - Classification head: Linear layer with dmodel hidden units and two output units for ASD/TC classification
  - Ensemble: Average the outputs of the three separate transformers and apply softmax to derive final class probabilities

- Critical path:
  1. Preprocess rs-fMRI data to obtain connectivity matrices from three atlases
  2. Implement the transformer encoder architecture with the specified hyperparameters
  3. Implement the self-supervised pretraining task for masked imputation
  4. Train the model using the pretraining task followed by fine-tuning on the classification task
  5. Evaluate the model using stratified cross-validation and report accuracy, precision, recall, F1-score, and AUC-score

- Design tradeoffs:
  - Using multiple atlases increases model complexity but may improve robustness and stability
  - Self-supervised pretraining requires additional training time but can improve downstream classification performance
  - The choice of hyperparameters (e.g., number of layers, attention heads, embedding dimension) can impact model performance

- Failure signatures:
  - Low classification accuracy: May indicate issues with the pretraining task, model architecture, or hyperparameters
  - High variance in cross-validation scores: May suggest overfitting or instability in the model
  - Slow convergence during training: Could be due to suboptimal learning rate or other hyperparameters

- First 3 experiments:
  1. Train a single atlas transformer model without pretraining and evaluate its performance using stratified cross-validation
  2. Implement the self-supervised pretraining task and evaluate its impact on the single atlas model's performance
  3. Train the multi-atlas transformer model with pretraining and compare its performance to the single atlas models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of brain atlas (AAL, CC200, DOS160) impact the performance of the METAFormer model?
- Basis in paper: [explicit] The paper uses three different atlases and compares their performance with the multi-atlas approach.
- Why unresolved: The paper does not provide a detailed analysis of how each individual atlas contributes to the overall performance or if one atlas is more effective than the others.
- What evidence would resolve it: A systematic comparison of the METAFormer model's performance using each atlas individually versus the multi-atlas approach, along with an analysis of the unique features each atlas contributes to the classification.

### Open Question 2
- Question: What is the impact of varying the percentage of masked features during self-supervised pretraining on the model's performance?
- Basis in paper: [explicit] The paper mentions that 10% of the features in each connectome are randomly set to 0 during pretraining.
- Why unresolved: The paper does not explore how different masking percentages affect the model's ability to learn and its subsequent classification performance.
- What evidence would resolve it: An experiment comparing the METAFormer model's performance with different percentages of masked features during pretraining, analyzing the trade-off between masking ratio and classification accuracy.

### Open Question 3
- Question: How does the METAFormer model generalize to other psychiatric disorders beyond Autism Spectrum Disorder?
- Basis in paper: [inferred] The paper focuses on ASD classification using the ABIDE I dataset but does not discuss the model's applicability to other disorders.
- Why unresolved: The study is limited to a single psychiatric condition, and there is no evidence of the model's performance on other brain disorders or its ability to generalize across different diagnostic categories.
- What evidence would resolve it: Testing the METAFormer model on datasets from other psychiatric disorders, such as schizophrenia or depression, and comparing its classification performance to existing methods for those conditions.

## Limitations

- The paper does not provide detailed implementation specifications for the self-supervised pretraining task, including the exact masking strategy and reconstruction evaluation.
- The relative contributions of individual atlases to the improved performance are not analyzed, making it unclear which atlas provides the most valuable information.
- The model's generalizability to other psychiatric disorders beyond ASD is not explored, limiting understanding of its broader applicability.

## Confidence

- High Confidence: The transformer architecture implementation and basic classification pipeline appear technically sound
- Medium Confidence: The improvement from self-supervised pretraining, though the specific masking strategy details are unclear
- Medium Confidence: The multi-atlas approach benefits, though individual atlas contributions are not analyzed

## Next Checks

1. Implement and test the self-supervised pretraining task independently to verify its impact on classification performance
2. Conduct ablation studies to isolate the contribution of each atlas to the final classification accuracy
3. Perform cross-site validation to assess model generalization across different acquisition protocols and demographic groups