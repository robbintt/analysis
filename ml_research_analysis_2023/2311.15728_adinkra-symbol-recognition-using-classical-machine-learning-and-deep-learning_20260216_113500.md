---
ver: rpa2
title: Adinkra Symbol Recognition using Classical Machine Learning and Deep Learning
arxiv_id: '2311.15728'
source_url: https://arxiv.org/abs/2311.15728
tags:
- learning
- machine
- pages
- adinkra
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed a comprehensive dataset of 174,338 Adinkra
  symbol images organized into 62 classes, addressing the challenge of cultural symbol
  recognition in AI. The researchers employed both classical machine learning (SVM,
  kNN, Random Forest, MLP) and deep learning approaches, including a custom CNN model
  and pre-trained models (VGG, ResNet) via transfer learning.
---

# Adinkra Symbol Recognition using Classical Machine Learning and Deep Learning

## Quick Facts
- arXiv ID: 2311.15728
- Source URL: https://arxiv.org/abs/2311.15728
- Reference count: 40
- Dataset: 174,338 Adinkra symbol images in 62 classes

## Executive Summary
This research addresses the challenge of recognizing Adinkra symbols, culturally significant patterns from Ghana, using both classical machine learning and deep learning approaches. The study develops a comprehensive dataset of 174,338 images across 62 classes and systematically evaluates multiple classification methods. Through experiments with custom CNN architectures, transfer learning from pre-trained models (VGG, ResNet), and classical ML algorithms (SVM, kNN, Random Forest, MLP), the research establishes benchmark performance metrics. The custom CNN achieved 97.29% accuracy while ResNet achieved the highest accuracy of 97.94%, demonstrating the effectiveness of combining traditional and deep learning methods for cultural symbol classification.

## Method Summary
The researchers developed a comprehensive dataset of 174,338 Adinkra symbol images organized into 62 classes. They employed both classical machine learning approaches (SVM, kNN, Random Forest, MLP) and deep learning methods, including a custom CNN model with six convolutional layers and three fully connected layers, as well as pre-trained models (VGG, ResNet) via transfer learning. The dataset was split into training (104,603 images), validation (34,868 images), and testing (34,867 images) sets. The custom CNN was trained for 50 epochs using Adam optimizer with a learning rate of 1e-4 and batch size of 32. Transfer learning experiments fine-tuned pre-trained models on the Adinkra dataset, while classical ML models were trained on extracted features.

## Key Results
- Custom CNN achieved 97.29% accuracy on the test set
- ResNet achieved the highest accuracy of 97.94% through transfer learning
- Dataset contains 174,338 images across 62 Adinkra symbol classes
- Both classical ML and deep learning methods demonstrated strong performance on the recognition task

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Transfer learning from pre-trained models (VGG, ResNet) improves symbol classification accuracy by leveraging learned features from large datasets.
- **Mechanism:** Pre-trained models extract hierarchical features (edges, textures, shapes) that are generalizable across visual domains, reducing the need for extensive Adinkra-specific training data.
- **Core assumption:** Features learned on ImageNet (or similar large datasets) are transferable to culturally specific symbols like Adinkra.
- **Evidence anchors:** The paper mentions using pre-trained models like VGG and ResNet, but the corpus provides weak evidence for symbol-specific transfer learning effectiveness.
- **Break Condition:** If Adinkra symbols have unique visual features not present in natural images used to train VGG/ResNet, transfer learning may provide limited benefit.

### Mechanism 2
- **Claim:** The custom CNN architecture, being simpler than VGG but still deep, balances model complexity and computational efficiency for Adinkra symbol recognition.
- **Mechanism:** Six convolutional layers with progressively increasing channels (64→128→256→512) capture hierarchical patterns while remaining computationally tractable on a single GPU.
- **Core assumption:** Adinkra symbols have sufficient visual complexity to benefit from deep architectures but not so much that VGG-level depth is necessary.
- **Evidence anchors:** The custom CNN is described as a simpler version of VGG with fewer layers and smaller channel sizes, but the corpus provides no direct evidence about its effectiveness.
- **Break Condition:** If Adinkra symbols require more complex feature extraction than the custom CNN provides, accuracy may plateau below desired levels.

### Mechanism 3
- **Claim:** Using both classical machine learning (SVM, kNN, RF) and deep learning provides complementary strengths for different aspects of the recognition task.
- **Mechanism:** Classical ML handles simpler feature representations efficiently, while deep learning captures complex spatial patterns; combined approach allows benchmarking and error analysis.
- **Core assumption:** Some Adinkra symbols may be distinguishable by simple features (color, basic shapes) while others require learned representations.
- **Evidence anchors:** The paper mentions diving into both classical ML and deep learning, but the corpus provides weak evidence for hybrid ML/DL approaches in cultural symbol recognition.
- **Break Condition:** If classical ML consistently underperforms deep learning on all symbol classes, maintaining dual approaches may add unnecessary complexity.

## Foundational Learning

- **Concept: Convolutional Neural Networks**
  - Why needed here: Adinkra symbols are spatial patterns requiring hierarchical feature extraction from pixel data.
  - Quick check question: What type of layers automatically learn spatial hierarchies in image data?

- **Concept: Transfer Learning**
  - Why needed here: Limited Adinkra training data makes it difficult to train deep networks from scratch; pre-trained models provide feature extraction capabilities.
  - Quick check question: How does fine-tuning differ from using a pre-trained model as a feature extractor?

- **Concept: Class Imbalance Handling**
  - Why needed here: The dataset shows "certain classes being underrepresented," which can bias model performance toward majority classes.
  - Quick check question: What techniques can mitigate bias when training data is imbalanced across classes?

## Architecture Onboarding

- **Component Map:** Input Layer (128x128 RGB/grayscale images) → Conv Block 1: 64 filters, 3x3 kernel → ReLU → MaxPool → Conv Block 2: 128 filters, 3x3 kernel → ReLU → MaxPool → Conv Block 3: 256 filters, 3x3 kernel → ReLU → MaxPool → Conv Block 4: 512 filters, 3x3 kernel → ReLU → MaxPool → Flatten → FC1 (4096 units) → ReLU → FC2 (4096 units) → ReLU → Output (62 classes) → Optional Dropout layers after pooling and FC layers

- **Critical Path:** Image preprocessing → CNN feature extraction → Classification → Evaluation metrics (accuracy, convergence)

- **Design Tradeoffs:**
  - Custom CNN vs. pre-trained models: Custom offers task-specific optimization but requires more data; pre-trained provides strong starting point with less data
  - Model depth vs. computational cost: Six layers balance performance and GPU memory constraints
  - Fixed kernel size (3x3) vs. variable: Simpler implementation but may miss larger patterns

- **Failure Signatures:**
  - Training accuracy high but test accuracy low → overfitting
  - Both accuracies low → underfitting or poor data quality
  - Convergence plateaus early → learning rate too high/low or insufficient model capacity
  - Class-specific failures → class imbalance or discriminative features missing

- **First 3 Experiments:**
  1. Train custom CNN on grayscale images only to establish baseline performance
  2. Fine-tune pre-trained ResNet on the full RGB dataset to compare transfer learning benefits
  3. Test classical ML classifiers (SVM, RF) on extracted CNN features to validate feature quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does class imbalance in the Adinkra dataset affect the performance of classical machine learning models compared to deep learning models?
- Basis in paper: The paper mentions data imbalance as a challenge but doesn't provide detailed analysis of how this specifically impacts different types of models.
- Why unresolved: The paper mentions data imbalance but doesn't provide a detailed analysis of how this specifically impacts different types of models or whether certain algorithms are more robust to this issue.
- What evidence would resolve it: Comparative experiments showing performance degradation for different algorithms as imbalance increases, or ablation studies where balanced and imbalanced versions of the dataset are tested.

### Open Question 2
- Question: What specific visual features do the convolutional layers learn to distinguish between different Adinkra symbols, and how do these features relate to the cultural meaning of the symbols?
- Basis in paper: The paper mentions visualizing learned features for model interpretability but doesn't provide detailed interpretation of specific visual patterns.
- Why unresolved: While the paper mentions feature visualization, it doesn't provide a detailed interpretation of what specific visual patterns the model uses to differentiate between symbols, nor does it connect these patterns to their cultural significance.
- What evidence would resolve it: Detailed analysis of feature maps showing which visual elements are most discriminative, combined with cultural experts' interpretation of how these visual elements relate to symbolic meanings.

### Open Question 3
- Question: How does the custom CNN's performance compare to other architectural choices (e.g., varying depth, kernel sizes, or attention mechanisms) for this specific cultural symbol recognition task?
- Basis in paper: The paper presents a custom CNN architecture but doesn't explore alternative architectural choices or conduct systematic ablation studies.
- Why unresolved: The authors present one custom architecture but don't explore the architectural design space to determine whether the chosen design is optimal for this specific domain.
- What evidence would resolve it: Comparative experiments testing variations in architecture (different depths, attention mechanisms, kernel sizes, skip connections) and analyzing their impact on both accuracy and interpretability for Adinkra symbol recognition.

## Limitations

- The study relies heavily on transfer learning from models trained on natural images, which may not capture the unique geometric and cultural features of Adinkra symbols
- The custom CNN architecture lacks extensive ablation studies to validate design choices
- Evaluation focuses primarily on overall accuracy without detailed analysis of class-specific performance or robustness to noise and variations

## Confidence

- **High Confidence:** The dataset creation methodology and basic experimental framework are well-documented and reproducible
- **Medium Confidence:** The reported accuracy figures (97.29% for custom CNN, 97.94% for ResNet) are plausible but lack detailed statistical validation across multiple runs
- **Low Confidence:** Claims about the superiority of transfer learning for Adinkra symbols specifically are weakly supported by the corpus evidence

## Next Checks

1. **Cross-validation stability test:** Run the same experiments with k-fold cross-validation (k=5) to assess result consistency and variance
2. **Domain adaptation experiment:** Compare transfer learning performance against models trained from scratch on Adinkra data to quantify the actual benefit
3. **Class-wise performance analysis:** Generate confusion matrices and per-class accuracy metrics to identify potential biases toward majority classes or systematic recognition errors