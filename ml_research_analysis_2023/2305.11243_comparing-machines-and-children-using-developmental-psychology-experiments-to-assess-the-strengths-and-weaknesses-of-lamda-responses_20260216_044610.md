---
ver: rpa2
title: 'Comparing Machines and Children: Using Developmental Psychology Experiments
  to Assess the Strengths and Weaknesses of LaMDA Responses'
arxiv_id: '2305.11243'
source_url: https://arxiv.org/abs/2305.11243
tags:
- lamda
- children
- causal
- developmental
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes using developmental psychology experiments to
  systematically evaluate the capabilities of large language models like LaMDA. It
  converts classic child development tasks into text prompts and scores LaMDA's responses
  against child-like reasoning using a novel LLM Response Score metric.
---

# Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses

## Quick Facts
- arXiv ID: 2305.11243
- Source URL: https://arxiv.org/abs/2305.11243
- Reference count: 4
- The paper evaluates LaMDA's capabilities using developmental psychology experiments, finding it excels at social understanding but struggles with causal reasoning and theory of mind.

## Executive Summary
This paper proposes using classic developmental psychology experiments to systematically evaluate large language models like LaMDA. By converting child development tasks into text prompts and scoring responses against child-like reasoning using a novel LLM Response Score (LRS) metric, the authors reveal distinct patterns in LaMDA's capabilities. The findings show LaMDA performs well on social understanding tasks but poorly on causal reasoning and theory of mind, suggesting its responses reflect different learning mechanisms than human children - excelling where linguistic patterns suffice but struggling where active inference from novel evidence is needed.

## Method Summary
The authors adapted classic developmental psychology experiments into text-based prompts and administered them to LaMDA 137B. They created the Large Language Model Response Score (LRS) metric to quantify how closely LaMDA's responses matched expected child developmental responses, with scores ranging from 0-10. Each task was run 10 times with systematic permutations to ensure consistency, and responses were scored based on correctness and appropriateness for each developmental domain. The study focused on four key domains: object permanence, action understanding, social understanding, and theory of mind.

## Key Results
- LaMDA excels at social understanding tasks (average LRS ~7.5) but performs poorly on causal reasoning (average LRS ~2.0)
- The model scores at chance levels on theory of mind tasks (average LRS ~5.1), suggesting limitations in understanding others' mental states
- Performance patterns indicate LaMDA's knowledge acquisition differs from human children, succeeding where information is accessible through language patterns but failing where real-world exploration is required

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LaMDA's text-only training leads to better performance on social understanding tasks than on tasks requiring physical interaction.
- Mechanism: Social understanding can be learned from language patterns because it involves tracking others' intentions and preferences, which are frequently discussed in text. Physical reasoning requires active exploration of the world, which text cannot fully capture.
- Core assumption: The information encoded in language is sufficient to represent social concepts but insufficient for physical causal reasoning.
- Evidence anchors:
  - [abstract] "LaMDA performs well in social understanding tasks, reflecting knowledge accessible through language, but poorly in causal reasoning tasks that likely require real-world exploration."
  - [section] "LaMDA generates appropriate responses that are similar to children in experiments involving social understanding, perhaps providing evidence that knowledge of these domains is discovered through language."

### Mechanism 2
- Claim: The LRS scoring metric effectively discriminates between text-based and exploration-based knowledge acquisition.
- Mechanism: By comparing LaMDA's responses to child developmental benchmarks, we can identify which cognitive domains can be learned from text patterns versus those requiring real-world interaction.
- Core assumption: Child developmental experiments provide valid benchmarks for assessing LLM capabilities in specific cognitive domains.
- Evidence anchors:
  - [section] "We created a scoring metric called the Large Language Models Response Score (LRS) and were able to assign values to quantify LaMDA's ability to respond like a child across crucial developmental domains."
  - [section] "We find that LaMDA does not seem to follow a human developmental trajectory. Instead, LaMDA's performance reflects differences in how much tasks rely on prior knowledge that may be encoded and transmitted through language."

### Mechanism 3
- Claim: LaMDA's performance on theory of mind tasks at chance levels indicates limitations in understanding others' mental states from text alone.
- Mechanism: Theory of mind requires tracking false beliefs and understanding that others may have different knowledge states, which may require more than pattern matching from text.
- Core assumption: The ability to attribute false beliefs requires understanding mental states that cannot be fully captured through linguistic patterns.
- Evidence anchors:
  - [abstract] "It scores at chance levels in early object/action understanding and theory of mind domains."
  - [section] "LaMDA seems to perform at chance on our tasks across this domain receiving an average LRS of 5.1."

## Foundational Learning

- Concept: Developmental psychology experimental design
  - Why needed here: The paper adapts classic child development experiments for LLM evaluation, requiring understanding of control conditions and experimental controls
  - Quick check question: Why are control conditions crucial in developmental psychology experiments, and how does this apply to LLM testing?

- Concept: Text-based vs. real-world knowledge acquisition
  - Why needed here: The paper argues that some knowledge can be learned from text while other knowledge requires physical interaction
  - Quick check question: What distinguishes knowledge that can be learned from text versus knowledge requiring real-world exploration?

- Concept: Scoring metrics and evaluation methodology
  - Why needed here: The LRS metric requires understanding how to quantify LLM responses against child-like reasoning
  - Quick check question: How does the LRS scoring system work, and what does a score of 0 vs. 10 represent?

## Architecture Onboarding

- Component map: Experimental prompt generation → LaMDA response generation → LRS scoring → Consistency averaging (10 trials)
- Critical path: Prompt generation → LLM response generation → LRS scoring → consistency averaging across 10 trials
- Design tradeoffs: Using text-only prompts versus multimodal inputs; maintaining experimental integrity while adapting for LLM constraints; balancing prompt complexity with model capabilities
- Failure signatures: Inconsistent responses across trials indicating randomness; poor performance on control conditions suggesting fundamental misunderstanding; inability to handle novel task permutations
- First 3 experiments:
  1. Helper/Hinderer task (Experiment 8): Tests basic social preference recognition with clear right/wrong answers
  2. Object Permanence task (Experiment 1): Tests tracking objects behind barriers, simpler than complex reasoning tasks
  3. Sally-Anne false belief task (Experiment 5): Tests theory of mind with clear control conditions and established benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do small variations in task wording systematically affect LaMDA's performance across different cognitive domains?
- Basis in paper: [explicit] The paper discusses implementing permutations in experiments and notes previous work showing small changes can lead to complete failure of tasks.
- Why unresolved: The paper implemented permutations but didn't systematically study how different types or magnitudes of variations affect performance across domains.
- What evidence would resolve it: A systematic study varying task parameters (wording, order, context) while measuring performance changes across all four cognitive domains.

### Open Question 2
- Question: What specific aspects of real-world exploration are necessary for causal reasoning that cannot be learned from language patterns alone?
- Basis in paper: [explicit] The paper suggests causal reasoning requires "real-world and self-initiated exploration" but doesn't specify which aspects are missing from text-based learning.
- Why unresolved: The paper identifies causal reasoning as a weakness but doesn't analyze which specific types of experiential learning are needed.
- What evidence would resolve it: Comparative studies of LaMDA's performance on causal tasks with varying degrees of physical world interaction requirements.

### Open Question 3
- Question: How would RLHF training affect LaMDA's performance on developmental psychology tasks?
- Basis in paper: [explicit] The paper notes that unlike other LLMs like GPT-4, LaMDA does not use RLHF, which could prune away mistaken responses that indicate failure.
- Why unresolved: The paper chose not to use RLHF but acknowledges it could significantly affect results, yet doesn't test this hypothesis.
- What evidence would resolve it: Direct comparison of LaMDA performance with and without RLHF training on the same developmental tasks.

## Limitations
- LaMDA's responses may simply reflect memorized patterns from training data rather than genuine understanding
- The LRS scoring metric relies on subjective interpretation of what constitutes appropriate developmental responses
- Adapting complex developmental tasks to text-only prompts may lose crucial contextual elements that children experience

## Confidence
- **High Confidence**: LaMDA's superior performance on social understanding tasks compared to physical reasoning tasks is well-supported by consistent experimental results across multiple trials.
- **Medium Confidence**: The claim that text-based knowledge acquisition differs fundamentally from exploration-based learning is supported by the data but requires further validation across additional cognitive domains.
- **Low Confidence**: The assertion that LaMDA's chance-level performance on theory of mind tasks definitively indicates limitations in understanding mental states, as alternative explanations (such as prompt formulation issues) remain possible.

## Next Checks
1. Test LaMDA's performance on the same developmental tasks using multimodal inputs (images, videos) to determine if visual context improves performance on physical reasoning and theory of mind domains.

2. Conduct cross-linguistic validation by translating experimental prompts into multiple languages to verify that performance differences reflect genuine understanding rather than language-specific patterns in the training data.

3. Implement ablation studies by systematically removing different types of knowledge from LaMDA's training data (e.g., social media, academic papers, fiction) to identify which information sources contribute most to performance in each cognitive domain.