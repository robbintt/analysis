---
ver: rpa2
title: Curriculum Guided Domain Adaptation in the Dark
arxiv_id: '2308.00956'
source_url: https://arxiv.org/abs/2308.00956
tags:
- domain
- adaptation
- target
- source
- noisy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses domain adaptation for black-box models, a
  scenario where source data and model parameters are unavailable during adaptation.
  The proposed method, Curriculum Adaptation for Black-Box (CABB), uses Jensen-Shannon
  divergence to separate clean and noisy samples from target data, then employs a
  curriculum learning strategy to progressively train on clean samples first and noisy
  samples later.
---

# Curriculum Guided Domain Adaptation in the Dark

## Quick Facts
- arXiv ID: 2308.00956
- Source URL: https://arxiv.org/abs/2308.00956
- Authors: [Not specified in input]
- Reference count: 40
- Primary result: Proposes CABB method achieving state-of-the-art black-box domain adaptation performance on Office-31, Office-Home, and VisDA-C datasets.

## Executive Summary
This paper addresses the challenge of domain adaptation when source data and model parameters are unavailable during adaptation. The proposed Curriculum Adaptation for Black-Box (CABB) method uses Jensen-Shannon divergence to separate clean and noisy samples from target data, then employs a curriculum learning strategy to progressively train on clean samples first and noisy samples later. The approach utilizes a dual-branch network with co-training to suppress error accumulation. CABB outperforms existing black-box domain adaptation methods on standard benchmarks while achieving comparable performance to white-box source-free methods.

## Method Summary
CABB addresses black-box domain adaptation through a three-stage approach: First, it uses Jensen-Shannon divergence to separate clean and noisy samples from target data by comparing source predictions with target features. Second, it implements a curriculum learning strategy that progressively trains the model on clean samples before introducing noisy samples, controlled by a dynamically adjusted curriculum factor. Third, it employs co-training of a dual-branch network where each branch's clean-noisy separation is used to train the other, suppressing error accumulation through parameter divergence. The method produces robust pseudolabels through ensemble-based augmentation and trains end-to-end without separate fine-tuning stages.

## Key Results
- Achieves state-of-the-art performance on Office-31, Office-Home, and VisDA-C datasets for black-box domain adaptation
- Outperforms existing black-box methods by significant margins while matching white-box source-free approaches
- Demonstrates effectiveness of Jensen-Shannon divergence over cross-entropy for clean-noisy sample separation
- Shows curriculum learning strategy effectively mitigates noisy label issues in domain adaptation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Jensen-Shannon divergence (JSD) is a better criterion than cross-entropy for separating clean and noisy samples in black-box domain adaptation.
- Mechanism: JSD produces a bimodal distribution when applied to source predictions and target features, which can be modeled by a 2-component Gaussian Mixture Model (GMM). The distribution with lower JSD contains cleaner samples, while the higher JSD distribution contains noisier samples.
- Core assumption: The noisy labels in unsupervised domain adaptation are unbounded in noise rate, making symmetric measures like JSD more robust than asymmetric measures like cross-entropy.
- Evidence anchors:
  - [abstract]: "CABB utilizes Jensen-Shannon divergence as a better criterion for clean-noisy sample separation, compared to the traditional criterion of cross entropy loss."
  - [section III-A]: "We propose Jensen-Shannon distance (JSD) [34] between the source predicted hard labels ŷi t and the target features as the criterion for clean-noisy sample separation under unbounded noise rate."
  - [corpus]: Weak evidence - only 1 related paper mentions curriculum learning, no direct comparison of JSD vs cross-entropy.
- Break condition: If the JSD distribution is not clearly bimodal or if the GMM modeling fails to separate the distributions effectively.

### Mechanism 2
- Claim: Curriculum learning strategy progressively trains the target model on clean samples first and noisy samples later.
- Mechanism: A curriculum factor γn is dynamically adjusted based on the ratio of current to previous cross-entropy loss on clean samples. This factor balances the supervised loss on clean samples and unsupervised loss on noisy samples.
- Core assumption: Deep networks tend to fit clean samples first and noisy samples later during training, making a curriculum approach effective for noisy label learning.
- Evidence anchors:
  - [abstract]: "CABB introduces a curriculum learning strategy to adaptively learn from the clean samples first, and the noisy samples later during the adaptation process."
  - [section III-C]: "Based on the success of the clean-noisy sample separation, the pseudolabels in the clean sample set Xtc are more likely to be correct, while those in the noisy sample set Xtn have a much higher noise rate."
  - [corpus]: Weak evidence - only 1 related paper mentions curriculum learning, no direct evidence of curriculum effectiveness in this specific context.
- Break condition: If the curriculum factor does not decrease appropriately or if the model performance degrades when learning from noisy samples.

### Mechanism 3
- Claim: Co-training of a dual-branch network suppresses error accumulation from confirmation bias.
- Mechanism: Two identical networks are trained simultaneously, where one network's clean-noisy sample separation is used to train the other network, and vice versa. This breaks the flow of error through the network.
- Core assumption: Due to difference in branch parameters, error introduced by noisy pseudolabels in one branch can be filtered out by the other branch.
- Evidence anchors:
  - [abstract]: "Our method utilizes co-training of a dual-branch target model to suppress error accumulation resulting from confirmation bias."
  - [section III]: "In order to mitigate early training time memorization [32] induced from noisy labels during the adaptation of deep models, we introduce a curriculum guided learning to train the target model on the clean samples first, and on the noisy samples later."
  - [corpus]: Weak evidence - only 1 related paper mentions curriculum learning, no direct evidence of co-training effectiveness in this specific context.
- Break condition: If the dual branches converge to similar parameters, reducing the effectiveness of error filtering.

## Foundational Learning

- Concept: Domain Adaptation
  - Why needed here: Understanding the domain shift between source and target domains is crucial for addressing the black-box domain adaptation problem.
  - Quick check question: What is the main difference between unsupervised domain adaptation and source-free domain adaptation?

- Concept: Noisy Label Learning
  - Why needed here: The target model predictions are noisy due to domain shift, requiring techniques from noisy label learning to handle unreliable labels.
  - Quick check question: How does confirmation bias affect the training of deep networks with noisy labels?

- Concept: Curriculum Learning
  - Why needed here: Progressive learning from easy (clean) to hard (noisy) samples helps mitigate the impact of noisy labels on model training.
  - Quick check question: What is the main advantage of curriculum learning over standard training approaches when dealing with noisy labels?

## Architecture Onboarding

- Component map: Source model -> JSD separator -> Dual-branch network (co-training) -> Curriculum learner -> Ensemble pseudolabeler

- Critical path:
  1. Source model generates pseudolabels for target data
  2. JSD-based separation identifies clean and noisy samples
  3. Dual-branch co-training progressively learns from clean to noisy samples
  4. Ensemble-based pseudolabeling produces robust final predictions

- Design tradeoffs:
  - JSD vs cross-entropy for sample separation: JSD is more robust to unbounded noise but may be computationally more expensive
  - Curriculum factor adjustment: Too aggressive decrease may lead to early exposure to noisy samples, while too conservative may slow down adaptation

- Failure signatures:
  - Poor separation of clean and noisy samples indicated by low accuracy on clean sample set
  - Degradation in model performance when learning from noisy samples
  - Convergence of dual branches to similar parameters, reducing error filtering effectiveness

- First 3 experiments:
  1. Compare JSD-based separation with cross-entropy-based separation on clean sample set accuracy
  2. Evaluate the impact of curriculum learning by comparing with non-curriculum baseline
  3. Assess the effectiveness of co-training by comparing with single-branch model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CABB change when applied to datasets with significantly different characteristics, such as medical imaging or satellite imagery, where domain shifts may be more pronounced?
- Basis in paper: [inferred] The paper evaluates CABB on standard domain adaptation datasets (Office-31, Office-Home, VisDA-C) but does not explore its performance on other types of datasets with different characteristics.
- Why unresolved: The paper focuses on common object recognition tasks and does not investigate the generalization of CABB to other domains or datasets with more complex domain shifts.
- What evidence would resolve it: Conducting experiments on diverse datasets with varying domain shift characteristics and comparing the performance of CABB against other domain adaptation methods would provide insights into its generalization capabilities.

### Open Question 2
- Question: Can the curriculum learning strategy in CABB be further optimized to dynamically adjust the curriculum factor based on the model's performance during training?
- Basis in paper: [explicit] The paper mentions that the curriculum factor γn is set according to a specific equation based on the loss ratio, but it does not explore alternative strategies for dynamically adjusting the curriculum factor.
- Why unresolved: The paper does not investigate the impact of different curriculum learning strategies on the performance of CABB or explore ways to adaptively adjust the curriculum factor based on the model's progress.
- What evidence would resolve it: Experimenting with different curriculum learning strategies and evaluating their impact on the performance of CABB would provide insights into the effectiveness of dynamic curriculum adjustment.

### Open Question 3
- Question: How does the choice of the number of augmentations M in the ensemble-based pseudolabeling affect the performance of CABB?
- Basis in paper: [inferred] The paper mentions using an ensemble of predictions generated by augmentations, but it does not explore the impact of varying the number of augmentations on the performance of CABB.
- Why unresolved: The paper does not provide insights into the optimal number of augmentations or investigate how the performance of CABB changes with different values of M.
- What evidence would resolve it: Conducting experiments with different values of M and analyzing the impact on the performance of CABB would help determine the optimal number of augmentations for ensemble-based pseudolabeling.

## Limitations

- Limited empirical comparison of JSD vs cross-entropy for sample separation, relying on theoretical arguments about robustness to unbounded noise
- No analysis of curriculum factor adaptation across diverse datasets or exploration of alternative curriculum learning strategies
- Insufficient investigation of the impact of augmentation count on ensemble-based pseudolabeling performance

## Confidence

- **High Confidence**: The overall methodology and experimental results showing CABB's superiority on standard benchmarks
- **Medium Confidence**: The specific mechanisms (JSD separation, curriculum learning, co-training) and their individual contributions to performance
- **Low Confidence**: Claims about the relative effectiveness of JSD vs cross-entropy and the robustness of curriculum factor adaptation across diverse datasets

## Next Checks

1. **Ablation Study**: Remove the co-training component and compare performance with single-branch baseline to isolate the contribution of error suppression through dual-branch learning.
2. **JSD vs Cross-Entropy**: Implement cross-entropy-based sample separation and conduct head-to-head comparison on clean sample accuracy and overall adaptation performance.
3. **Curriculum Factor Analysis**: Track curriculum factor evolution across training epochs and domains to verify the assumed monotonic decrease pattern and identify scenarios where the adaptation strategy may fail.