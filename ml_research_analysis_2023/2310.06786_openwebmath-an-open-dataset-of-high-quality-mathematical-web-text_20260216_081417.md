---
ver: rpa2
title: 'OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text'
arxiv_id: '2310.06786'
source_url: https://arxiv.org/abs/2310.06786
tags:
- dataset
- documents
- text
- common
- latex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OpenWebMath, an open dataset of 14.7B tokens
  of high-quality mathematical web text. The authors extract text and LaTeX content
  from HTML documents, apply quality filtering and deduplication, and release the
  dataset on Hugging Face.
---

# OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text

## Quick Facts
- **arXiv ID**: 2310.06786
- **Source URL**: https://arxiv.org/abs/2310.06786
- **Reference count**: 20
- **Primary result**: OpenWebMath is a 14.7B token dataset of high-quality mathematical web text that enables 1.4B parameter models to outperform those trained on 20x more general language data.

## Executive Summary
This paper introduces OpenWebMath, an open dataset containing 14.7B tokens of high-quality mathematical web text extracted from Common Crawl HTML documents. The authors developed a comprehensive pipeline to extract text and LaTeX content while filtering out boilerplate and low-quality material. They demonstrate that language models trained on this specialized mathematical dataset significantly outperform models trained on over 20 times the amount of general language data, establishing OpenWebMath as a valuable resource for improving mathematical reasoning capabilities in large language models.

## Method Summary
The authors extract text and LaTeX content from HTML documents using a four-stage pipeline: LaTeX extraction via regular expressions, text extraction with Resiliparse, DOM processing, and line processing. They apply quality filtering through language identification, a MathScore classifier, perplexity filtering, and manual inspection. The dataset is deduplicated using SimHash with a threshold of 0.7. The final dataset is released on Hugging Face and used to train 1.4B parameter language models using the same architecture and hyperparameters as Pythia 1.4B.

## Key Results
- 1.4B parameter models trained on 14.7B tokens of OpenWebMath surpass performance of models trained on over 20x more general language data
- The dataset contains high-quality Q&A forum posts, educational documents, blogs, and technical content across mathematics, physics, and computer science
- OpenWebMath is publicly available on Hugging Face, enabling reproducible research in mathematical language modeling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OpenWebMath's filtering pipeline removes low-quality, non-mathematical documents while preserving those with LaTeX content.
- Mechanism: A combination of text extraction, language identification, perplexity filtering, and manual inspection ensures that only high-quality mathematical content remains.
- Core assumption: The filters are tuned to have high recall for mathematical content, even if it means including some non-mathematical documents.
- Evidence anchors:
  - [abstract] "We describe in detail our method for extracting text and LATEX content and removing boilerplate from HTML documents, as well as our methods for quality filtering and deduplication."
  - [section] "Our text extraction pipeline consists of four stages: LATEX extraction, text extraction, DOM processing, and line processing."
- Break condition: If filters become too aggressive and remove relevant mathematical content, the dataset's utility for training mathematical reasoning models would decrease.

### Mechanism 2
- Claim: The dataset's size and diversity improve the mathematical reasoning capabilities of large language models.
- Mechanism: Training on a large corpus of high-quality mathematical text allows the model to learn patterns and relationships specific to mathematical language and notation.
- Core assumption: The mathematical content in OpenWebMath is diverse enough to cover a wide range of mathematical concepts and problem-solving techniques.
- Evidence anchors:
  - [abstract] "We train 1.4B parameter language models on OpenWebMath, showing that models trained on 14.7B tokens of our dataset surpass the performance of models trained on over 20x the amount of general language data."
  - [section] "OpenWebMath consists of high quality Q&A forum posts, educational documents, blogs, and more spread across mathematics, physics, computer science, and other technical domains."
- Break condition: If the dataset lacks diversity in mathematical topics or problem-solving approaches, the model's ability to generalize to new mathematical problems may be limited.

### Mechanism 3
- Claim: The dataset's LaTeX extraction capabilities preserve mathematical notation, enabling the model to learn from formatted mathematical expressions.
- Mechanism: The text extraction pipeline is designed to identify and extract LaTeX equations from HTML documents, preserving the mathematical notation.
- Core assumption: The LaTeX extraction process is accurate and comprehensive, capturing the majority of mathematical expressions in the dataset.
- Evidence anchors:
  - [abstract] "We extract text and LATEX content from HTML documents, apply quality filtering and deduplication, and release the dataset on Hugging Face."
  - [section] "LATEX Extraction...We use regular expressions to search for code that calls the configuration function for MathJax to extract the delimiters used for equations."
- Break condition: If the LaTeX extraction process fails to capture a significant portion of mathematical expressions, the model's ability to learn from formatted mathematical notation may be hindered.

## Foundational Learning

- **Concept**: Text extraction from HTML
  - Why needed here: The dataset is built from HTML documents, so an effective text extraction process is crucial for obtaining the mathematical content.
  - Quick check question: What are the key steps in the text extraction pipeline, and how do they ensure that mathematical content is preserved?

- **Concept**: Quality filtering for mathematical content
  - Why needed here: The dataset needs to be filtered to remove low-quality or non-mathematical content, ensuring that the model is trained on relevant material.
  - Quick check question: How does the MathScore classifier work, and what criteria does it use to determine if a document is mathematical?

- **Concept**: LaTeX equation extraction
  - Why needed here: Mathematical notation is often expressed using LaTeX, so the ability to extract and preserve these equations is essential for training mathematical reasoning models.
  - Quick check question: What are the different ways that LaTeX can be encoded in HTML documents, and how does the extraction process handle each case?

## Architecture Onboarding

- **Component map**: Common Crawl HTML documents -> Resiliparse text extraction with custom LaTeX extraction -> Language identification, MathScore classifier, perplexity filtering -> SimHash deduplication -> OpenWebMath dataset
- **Critical path**: The text extraction and quality filtering stages are the most critical, as they directly impact the dataset's quality and relevance for training mathematical reasoning models.
- **Design tradeoffs**: The dataset prioritizes high recall for mathematical content, which means that some non-mathematical documents may be included. This tradeoff is made to ensure that valuable mathematical content is not accidentally filtered out.
- **Failure signatures**: If the text extraction fails to preserve LaTeX equations, or if the quality filters are too aggressive, the dataset's utility for training mathematical reasoning models may be compromised.
- **First 3 experiments**:
  1. Train a small language model on OpenWebMath and evaluate its performance on a mathematical reasoning benchmark.
  2. Analyze the distribution of mathematical topics in the dataset to assess its diversity and coverage.
  3. Compare the performance of models trained on OpenWebMath to those trained on general language datasets to quantify the benefits of domain-specific training data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal threshold for SimHash near-duplication removal in OpenWebMath?
- Basis in paper: [explicit] The paper mentions using a threshold of 0.7 for SimHash deduplication but acknowledges this was chosen based on manual inspection rather than empirical justification.
- Why unresolved: The paper states they only ran their pipeline once due to cost constraints, preventing them from doing ablation studies on different deduplication thresholds.
- What evidence would resolve it: Running the full pipeline with multiple SimHash thresholds and comparing the resulting dataset quality, document diversity, and downstream model performance.

### Open Question 2
- Question: How much mathematical content is potentially missed by the prefiltering stage?
- Basis in paper: [explicit] The paper notes their prefilter reduces HTML documents processed to under 1% of Common Crawl and acknowledges this "may be too aggressive."
- Why unresolved: The authors opted for high recall in filtering but didn't empirically measure what percentage of relevant mathematical documents might be excluded by the prefilter.
- What evidence would resolve it: Comparing the mathematical content in documents that pass vs. fail the prefilter, or running the full extraction pipeline on a random sample of prefiltered-out documents.

### Open Question 3
- Question: How would OpenWebMath's quality change with more aggressive filtering?
- Basis in paper: [inferred] The paper mentions their dataset has "a small portion of documents... not related to mathematics, or contain bad quality content" and suggests future work could "explore filtering OpenWebMath more aggressively."
- Why unresolved: The authors chose to keep as much relevant content as possible, sacrificing some quality for quantity, without exploring the tradeoff empirically.
- What evidence would resolve it: Creating and evaluating multiple versions of OpenWebMath with varying levels of filtering aggressiveness and measuring impacts on downstream model performance.

## Limitations
- The evaluation relies heavily on perplexity as a primary metric, with limited downstream task validation
- The dataset construction pipeline lacks complete transparency regarding parameter choices and thresholds
- Limited comparison with other mathematical reasoning models of similar size to establish performance gains

## Confidence

- **High Confidence**: The dataset construction methodology is sound and follows established best practices for web-scale data curation. The claim that domain-specific pretraining improves mathematical reasoning over general pretraining is well-supported by the experimental results.
- **Medium Confidence**: The assertion that OpenWebMath outperforms models trained on 20x more general data is supported by the presented results, but the comparison is limited to a single model architecture (Pythia 1.4B) and may not generalize across different model families or scales.
- **Low Confidence**: The dataset's coverage of mathematical domains and the effectiveness of the LaTeX extraction pipeline are not thoroughly validated. The paper claims high-quality mathematical content but provides limited quantitative evidence of the dataset's mathematical diversity or the accuracy of equation preservation.

## Next Checks

1. **Downstream Task Validation**: Evaluate the trained models on a broader suite of mathematical reasoning tasks, including proof generation, symbolic manipulation, and STEM domain-specific benchmarks, to establish the practical utility of OpenWebMath pretraining beyond perplexity improvements.

2. **Dataset Quality Analysis**: Conduct a systematic analysis of the mathematical content distribution across different domains (algebra, calculus, geometry, etc.) and verify the accuracy of LaTeX equation extraction through manual sampling and automated validation techniques.

3. **Scaling Study**: Train models of varying sizes (e.g., 350M, 2.7B, 6.7B parameters) on OpenWebMath to determine whether the observed improvements scale with model capacity and to identify potential saturation points where additional mathematical data yields diminishing returns.