---
ver: rpa2
title: 'When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale'
arxiv_id: '2309.04564'
source_url: https://arxiv.org/abs/2309.04564
tags:
- pruning
- data
- training
- reference
- trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates data pruning methods for pretraining large
  language models (LLMs). The authors explore three automatic pruning metrics: perplexity,
  Error L2-Norm (EL2N), and memorization.'
---

# When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale

## Quick Facts
- arXiv ID: 2309.04564
- Source URL: https://arxiv.org/abs/2309.04564
- Authors: 
- Reference count: 23
- Key outcome: Perplexity-based data pruning outperforms more complex methods and achieves 1.5% improvement on 1.5B parameter models

## Executive Summary
This paper investigates data pruning methods for pretraining large language models, challenging the conventional wisdom that more training data always leads to better performance. The authors explore three automatic pruning metrics - perplexity, Error L2-Norm (EL2N), and memorization - and find that surprisingly, simple perplexity-based pruning consistently outperforms more computationally expensive alternatives. Their results show that removing up to 50% of training data using perplexity-based pruning can actually improve model performance, with the middle 50% of data yielding the best results.

## Method Summary
The authors train autoregressive decoder-only Transformer models (GPT-style) ranging from 124M to 1.5B parameters on pruned versions of a web-crawled corpus. They use reference models of varying sizes (124M to 52B parameters) to compute pruning scores for each data point using three metrics: perplexity, EL2N, and memorization. The datasets are then pruned to retain specific subsets (bottom 10%, middle 50%, top 10%) based on these scores. The pruned datasets are used to train new models, which are evaluated on test set perplexity and downstream GLUE tasks. The paper also explores using early checkpoints of reference models to reduce computational costs.

## Key Results
- Perplexity-based pruning consistently outperforms EL2N and memorization across all tested dataset sizes and model scales
- Models trained on 50% of the dataset pruned using perplexity achieve a 1.33% improvement over EL2N-pruned models and 1.77% over memorization-pruned models
- Perplexity-based pruning generalizes to 1.5B parameter models, achieving a 1.5% improvement over a no-pruning baseline
- Larger reference models produce more effective pruning rankings, with the 52B parameter reference model showing the best performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perplexity-based pruning consistently outperforms more complex methods like EL2N and memorization factor across varying dataset sizes and model scales.
- Mechanism: Simpler metrics like perplexity effectively capture the informativeness of training examples, allowing models to learn more efficiently from a smaller, higher-quality subset of data.
- Core assumption: The perplexity score of a reference model accurately reflects the difficulty or usefulness of a training example for a future model.
- Evidence anchors:
  - [abstract] "Surprisingly, we find that the simple technique of perplexity outperforms our more computationally expensive scoring methods."
  - [section] "In Figure 4 we present results comparing the performance of the best variant of each pruning metric... Perplexity-based pruning consistently surpasses both alternative metrics and the no pruning experiments."
  - [corpus] Weak. No direct evidence in corpus about why perplexity outperforms other metrics.
- Break condition: If reference model's perplexity distribution does not correlate with actual learning difficulty for the trained model, or if training dynamics shift such that early examples become more informative.

### Mechanism 2
- Claim: Larger reference models produce more effective perplexity rankings, leading to better pruning outcomes.
- Mechanism: Larger models have better-calibrated probability distributions, allowing them to more accurately distinguish between high- and low-quality examples.
- Core assumption: Model size directly correlates with the quality of the pruning signal derived from perplexity scores.
- Evidence anchors:
  - [abstract] "We show this scales to 1.5B parameter models, achieving 1.5% improvement in test set perplexity over a no-pruning baseline of the same size."
  - [section] "Figure 2 shows the trained model performances after pruning with perplexity calculated with reference models ranging from 124M to 52B parameters. We find that increasing reference model size improves trained model performance over the no-pruning baseline."
  - [corpus] Weak. Corpus does not provide explicit evidence about why larger models produce better rankings.
- Break condition: If the computational cost of using larger reference models outweighs the benefits, or if diminishing returns occur beyond a certain model size.

### Mechanism 3
- Claim: Early checkpoints of reference models can provide sufficient pruning signal, reducing computational cost.
- Mechanism: Initial training phases capture important learning signals that remain relevant for identifying useful training examples.
- Core assumption: The early training dynamics of a reference model contain sufficient information to rank data quality effectively.
- Evidence anchors:
  - [section] "Figure 6 showcases the results of these experiments... the 55% reference models perform in a similar manner to the fully trained models, performing best with the middle subset..."
  - [abstract] No direct mention of early checkpoints.
  - [corpus] Weak. Corpus does not contain evidence about early checkpoint effectiveness.
- Break condition: If early checkpoints miss important nuances in data quality that only emerge in later training stages, or if the reference model hasn't learned enough to provide meaningful rankings.

## Foundational Learning

- Concept: Perplexity as a measure of language model performance
  - Why needed here: Understanding perplexity is crucial for grasping how the pruning method works and why it's effective.
  - Quick check question: What does a lower perplexity score indicate about a language model's performance on a given text?

- Concept: Reference models and their role in data pruning
  - Why needed here: The entire pruning methodology relies on using a separate model to score training data before training the final model.
  - Quick check question: Why do the authors use a reference model to compute perplexity scores instead of using the model being trained?

- Concept: Data pruning and its impact on model training
  - Why needed here: The core contribution is showing that removing certain portions of training data can improve model performance.
  - Quick check question: How does the performance of a model trained on pruned data compare to one trained on the full dataset according to the results?

## Architecture Onboarding

- Component map: Data preprocessing -> Reference model training -> Pruning metric calculation -> Data selection -> Model training -> Evaluation
- Critical path: Reference model → Pruning scores → Data selection → Model training → Evaluation
- Design tradeoffs:
  - Using larger reference models improves pruning quality but increases computational cost
  - Early checkpoint reference models reduce cost but may sacrifice some quality
  - Different pruning metrics offer varying balances of complexity and effectiveness
- Failure signatures:
  - Poor performance when using bottom subsets of any pruning metric
  - Random pruning performing as well as sophisticated methods indicates issues with the metric
  - No improvement over baseline when scaling to larger models
- First 3 experiments:
  1. Train a small reference model on clean data (e.g., Wikipedia) and use it to compute perplexity scores for the entire dataset.
  2. Prune the dataset to keep only the middle 50% of examples based on perplexity scores and train a new model.
  3. Compare the performance of this pruned model to a baseline trained on the full dataset and to models pruned using other metrics (EL2N, memorization).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of perplexity-based pruning scale beyond 1.5B parameter models?
- Basis in paper: [explicit] The authors state "We take our strongest pruning variant – perplexity computed using a 52B parameter reference model while retaining the middle subset – to explore the robustness of our findings at a larger scale by validating our findings on a 1.5B model." They show improvements at 1.5B but do not test larger scales.
- Why unresolved: The experiments only tested up to 1.5B parameter models. Larger models like GPT-3 (175B) or PaLM (540B) were not evaluated.
- What evidence would resolve it: Experiments showing perplexity-based pruning effectiveness on models larger than 1.5B parameters, ideally scaling to the largest current models.

### Open Question 2
- Question: How do different types of noisy web text (e.g. machine-generated spam vs. low-quality human text) respond differently to data pruning methods?
- Basis in paper: [inferred] The authors mention "Significant portions of web-scraped data used for language model pretraining have been shown to be of low quality, machine-generated spam, pornographic content" but do not analyze different types of noise separately.
- Why unresolved: The experiments used a general web-crawled corpus without analyzing how different types of low-quality content affect pruning effectiveness.
- What evidence would resolve it: Experiments comparing pruning effectiveness on different subsets of web text classified by quality or generation method (human vs. machine-generated).

### Open Question 3
- Question: Does the optimal pruning strategy differ between pretraining objectives (e.g. causal language modeling vs. masked language modeling)?
- Basis in paper: [explicit] The authors state "We train autoregressive decoder-only Transformer models (Vaswani et al., 2023) with a standard language modeling objective" but do not explore other pretraining objectives.
- Why unresolved: Only one pretraining objective was tested, leaving open whether results generalize to other objectives like BERT's masked language modeling.
- What evidence would resolve it: Experiments comparing perplexity-based pruning effectiveness across different pretraining objectives using the same datasets and model architectures.

## Limitations

- The study focuses on a single dataset (CommonCrawl) and limited set of downstream tasks (GLUE), potentially limiting generalizability to other domains and languages
- Computational overhead of training large reference models presents practical constraints despite performance gains
- The optimal pruning ratio (50%) and effectiveness at scales beyond 1.5B parameters remain uncertain

## Confidence

- **High confidence**: The core finding that perplexity-based pruning outperforms more complex methods and improves performance at 1.5B scale is well-supported by experimental results across multiple runs and conditions.
- **Medium confidence**: The mechanism explaining why perplexity works better (capturing informativeness) is plausible but not definitively proven, as the paper lacks ablation studies on the specific features that make perplexity effective.
- **Medium confidence**: The claim that larger reference models produce better pruning signals is supported by results but lacks theoretical justification for why model size specifically improves the pruning quality.

## Next Checks

1. **Generalization Test**: Validate the perplexity pruning approach on a distinct dataset (e.g., C4 or The Pile) to confirm the method's robustness beyond CommonCrawl and assess whether the 50% pruning ratio remains optimal.

2. **Scaling Analysis**: Extend experiments to models of 10B+ parameters to determine if the 1.5% improvement observed at 1.5B parameters scales proportionally or exhibits diminishing returns, and whether larger reference models remain beneficial at this scale.

3. **Alternative Pruning Signal**: Replace the perplexity reference model with a smaller, more efficient model (e.g., a distilled version) to assess whether the computational cost of large reference models is justified, or if simpler models can provide comparable pruning signals.