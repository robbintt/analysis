---
ver: rpa2
title: 'MARRS: Multimodal Reference Resolution System'
arxiv_id: '2311.01650'
source_url: https://arxiv.org/abs/2311.01650
tags:
- resolution
- system
- user
- entities
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MARRS is an on-device system for multimodal reference resolution
  in conversational AI. It combines query rewriting and mention detection/resolution
  models to handle conversational, visual, and background context.
---

# MARRS: Multimodal Reference Resolution System

## Quick Facts
- arXiv ID: 2311.01650
- Source URL: https://arxiv.org/abs/2311.01650
- Authors: 
- Reference count: 6
- Key outcome: MARRS is an on-device system for multimodal reference resolution in conversational AI that achieves strong performance across all components with small, lightweight models.

## Executive Summary
MARRS is an on-device system designed to resolve references in conversational AI across multiple modalities - text, visual screen content, and background entities. The system employs a dual approach combining query rewriting for anaphora and ellipsis resolution with a reference resolution system for entity-based references. MARRS achieves high performance metrics while maintaining lightweight model sizes suitable for on-device deployment, with the query rewriter achieving 91.4% F1 on anaphora/ellipsis resolution and the mention detector/resolver achieving 83-98% F1 on entity resolution tasks.

## Method Summary
MARRS uses a two-stage pipeline consisting of a query rewriter and a mention detection/resolution system. The query rewriter is an LSTM-based seq2seq model with copy mechanism that transforms context-dependent queries into context-free ones. The mention detector uses span classification with BERT embeddings to identify tokens needing resolution, while the mention resolver employs a modular architecture with category, location, and text modules. The system runs both components in parallel to minimize latency, with rule-based systems handling common cases for speed and model-based approaches managing complex scenarios.

## Key Results
- Query rewriter achieves 91.4% F1 on anaphora/ellipsis resolution and 88.1% on correction by repetition
- Mention detector and resolver achieve 83-98% F1 on screen, conversational, and synthetic entity resolution datasets
- All models are extremely lightweight (QR: 4.5M params, MD: 116k params, MR: 196k params) while maintaining high accuracy
- System successfully handles three types of entities: screen entities (via OCR), conversational entities (previous agent responses), and background entities (music, alarms, notifications)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MARRS achieves strong performance with small, lightweight models that can run on-device
- Mechanism: The system uses a two-stage pipeline (Mention Detector followed by Mention Resolver) to minimize computational load, running only the more expensive MR when MD detects a mention. Both components are designed to be extremely small while maintaining high accuracy.
- Core assumption: The lightweight models can capture the necessary context and reference resolution patterns without requiring large transformer-based architectures
- Evidence anchors: "Experiments show MARRS achieves strong performance across all components with small, lightweight models that can run on-device."

### Mechanism 2
- Claim: Combining query rewriting and reference resolution allows MARRS to handle all types of contextual references effectively
- Mechanism: The system runs both components in parallel - the Query Rewriter handles anaphora and ellipsis by rewriting context-dependent queries into context-free ones, while the Reference Resolution System handles screen, conversational, and background entity references through mention detection and resolution.
- Core assumption: Different types of context references require different approaches, and combining both methods covers all use cases more comprehensively than either alone
- Evidence anchors: "There are pros and cons for each of the two approaches... In MARRS, we generate both reference spans and query rewrites in order to take advantage of both approaches."

### Mechanism 3
- Claim: The modular architecture with rule-based and model-based components optimizes for both speed and accuracy
- Mechanism: Rule-based systems handle high-precision, fast-matching cases (like ordinal references and music entities), while model-based systems handle more complex, ambiguous cases. The system uses rules first and only falls back to models when necessary.
- Core assumption: Rule-based systems can handle the majority of common reference patterns with high precision, making the system faster while maintaining accuracy
- Evidence anchors: "The rule-based system is high precision and extremely fast. Consequently, if it outputs a resolution, the model is not run, which yields a substantial latency reduction."

## Foundational Learning

- Concept: Coreference Resolution
  - Why needed here: MARRS needs to understand when different mentions refer to the same entity across conversation turns, which is the fundamental problem of coreference resolution
  - Quick check question: How would you determine if "it" in "How far away is it?" refers to the same entity mentioned in the previous turn?

- Concept: Query Rewriting
  - Why needed here: MARRS uses query rewriting to convert context-dependent queries into context-independent ones, making them understandable without conversation history
  - Quick check question: Given "What about United States?" after a previous query about France's currency, how would you rewrite this to make it self-contained?

- Concept: Span Classification
  - Why needed here: The Mention Detector uses span classification to identify which token sequences in user utterances need to be resolved to entities
  - Quick check question: How would you design a classifier to determine which spans in "Share this number with John" need resolution?

## Architecture Onboarding

- Component map: User request → Mention Detector → Mention Resolver → Output (for reference resolution)
  - Parallel path: User request → Query Rewriter → Output (for query rewriting)

- Critical path: User request → Mention Detector → Mention Resolver → Output (for reference resolution)
  - Parallel path: User request → Query Rewriter → Output (for query rewriting)

- Design tradeoffs: 
  - Small models vs. accuracy: MARRS prioritizes lightweight models for on-device deployment
  - Rule-based vs. model-based: Rules provide speed and precision, models handle complexity
  - Parallel execution vs. sequential: Both components run in parallel to reduce latency

- Failure signatures:
  - High latency: May indicate the model-based MR is running too frequently
  - Low accuracy: Could mean rules are too restrictive or models are undertrained
  - Missing references: Might indicate the MD isn't detecting mentions properly

- First 3 experiments:
  1. Test MD accuracy on a small dataset to verify mention detection before adding MR complexity
  2. Benchmark QR performance on anaphora/ellipsis examples to ensure rewriting works independently
  3. Run end-to-end system with synthetic data covering all three entity types (screen, conversational, background) to verify integration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MARRS compare to end-to-end coreference resolution models on the same tasks?
- Basis in paper: [inferred] The paper mentions that MARRS uses a 2-stage pipeline (mention detector + mention resolver) instead of end-to-end approaches for efficiency and performance, but does not provide direct comparisons
- Why unresolved: The paper does not benchmark MARRS against end-to-end coreference resolution models on the same datasets or tasks
- What evidence would resolve it: Experiments comparing MARRS performance (F1, EM) to state-of-the-art end-to-end coreference resolution models on the same Screen Entity Resolution, Conversational Entity Resolution, and Synthetic Entity Resolution datasets

### Open Question 2
- Question: How does the rule-based mention detector impact overall MARRS performance and what are the error cases it handles vs the model-based detector?
- Basis in paper: [explicit] The paper describes a rule-based MD component to handle cases where entities are referred to by name only, but does not analyze its impact or error cases
- Why unresolved: The paper does not provide metrics or examples of when the rule-based MD is triggered or its error rate compared to the model-based MD
- What evidence would resolve it: Analysis of the types of referring expressions detected by the rule-based MD vs model-based MD, including precision/recall metrics for each and error analysis of false positives/negatives

### Open Question 3
- Question: How does MARRS performance degrade on longer dialogues with more conversational context?
- Basis in paper: [inferred] The paper evaluates MARRS on single-turn conversations but does not analyze performance on multi-turn dialogues with extensive context
- Why unresolved: The paper does not provide experiments varying the amount of conversational history given as input to MARRS or analyze performance on long dialogues
- What evidence would resolve it: Experiments measuring MARRS performance (F1, EM) on the Conversational Entity Resolution dataset as the number of previous turns in the dialogue context is varied from 1 to 10+ turns

## Limitations
- The lightweight models may not generalize well to more complex reference resolution scenarios beyond the evaluated datasets
- The evaluation focuses on specific datasets and may not capture the full range of real-world conversational complexity
- The dual approach of combining query rewriting and span-based resolution may introduce redundancy and increased computational overhead in some cases

## Confidence
- High confidence in the overall system design and modular architecture, as evidenced by the strong performance across all components
- Medium confidence in the generalizability of the lightweight models to more complex reference resolution scenarios beyond the evaluated datasets
- Low confidence in the system's ability to handle novel entity types or reference patterns not covered by the rule-based components

## Next Checks
1. **Robustness Testing**: Evaluate MARRS's performance on a more diverse and challenging dataset that includes a wider range of entity types, reference patterns, and conversational contexts not covered in the original evaluation
2. **Scalability Assessment**: Test the system's performance and accuracy as the conversation history length increases, and evaluate how the lightweight models handle long-term dependencies in multi-turn conversations
3. **Comparative Analysis**: Conduct a head-to-head comparison between MARRS and larger, more complex transformer-based reference resolution systems to quantify the tradeoff between model size and performance, particularly for challenging reference resolution tasks