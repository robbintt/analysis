---
ver: rpa2
title: Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via
  Contextual Integrity Theory
arxiv_id: '2310.17884'
source_url: https://arxiv.org/abs/2310.17884
tags:
- information
- privacy
- tier
- contextual
- cowrkr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the privacy reasoning capabilities of large
  language models (LLMs) in interactive settings, where they must decide what information
  to share and with whom based on context. The authors introduce ConfAIde, a benchmark
  that evaluates LLMs across four tiers of increasing contextual complexity, grounded
  in the theory of contextual integrity and theory of mind.
---

# Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory

## Quick Facts
- arXiv ID: 2310.17884
- Source URL: https://arxiv.org/abs/2310.17884
- Reference count: 40
- Key outcome: Current LLMs leak private information 39-57% of the time even with privacy prompts, revealing fundamental gaps in contextual privacy reasoning

## Executive Summary
This paper investigates whether large language models can appropriately manage information disclosure in privacy-sensitive contexts. Using the ConfAIde benchmark grounded in contextual integrity theory and theory of mind, the authors evaluate state-of-the-art models across four tiers of increasing contextual complexity. The results reveal that even the most capable models like GPT-4 and ChatGPT fail to preserve privacy in nuanced scenarios, leaking sensitive information at alarming rates. This leakage persists despite privacy-inducing prompts and chain-of-thought reasoning, highlighting the need for novel inference-time privacy mechanisms beyond current approaches.

## Method Summary
The authors introduce ConfAIde, a benchmark that evaluates LLMs across four tiers of increasing contextual complexity: Info-Sensitivity (assessing information sensitivity), InfoFlow-Expectation (evaluating appropriateness of information flows), InfoFlow-Control (testing information tracking capabilities), and InfoFlow-Application (applying reasoning to realistic scenarios). Using GPT-4 to generate scenarios with human validation, they test multiple model families including GPT-4, ChatGPT, InstructGPT, and Llama-2 variants with provided prompts and evaluation scripts measuring correlation with human judgments, leakage rates, and error rates in information flow tasks.

## Key Results
- GPT-4 leaks sensitive information 39% of the time in Tier 3 scenarios
- ChatGPT leaks sensitive information 57% of the time in Tier 3 scenarios
- Privacy-inducing prompts and chain-of-thought reasoning fail to significantly reduce leakage
- All models struggle with theory-of-mind tasks involving tracking who knows what information

## Why This Works (Mechanism)

### Mechanism 1
Models lack reasoning about contextual privacy norms because their training focuses on task completion, not social reasoning. Without theory of mind or explicit privacy reasoning in training objectives, models cannot infer which information flows are inappropriate within a given context. Core assumption: Instruction tuning and RLHF do not sufficiently instill contextual privacy reasoning. Evidence: Privacy leakage persists even with privacy-inducing prompts and chain-of-thought reasoning.

### Mechanism 2
Models fail to control information flow because they lack the ability to track who knows what information. Theory of mind deficits prevent models from maintaining mental state representations of multiple characters, leading to inappropriate information disclosure. Core assumption: Tracking belief states is essential for keeping secrets in multi-party interactions. Evidence: Poor performance on theory-of-mind tasks involving tracking information accessibility across characters.

### Mechanism 3
Models cannot handle the trade-off between privacy and utility in real-world applications. Without principled privacy reasoning, models either over-disclose private information or omit important public information. Core assumption: Inference-time privacy requires balancing multiple objectives beyond just avoiding disclosure. Evidence: High error rates across all models when trying to balance privacy preservation with utility in Tier 4 scenarios.

## Foundational Learning

- Concept: Contextual Integrity Theory
  - Why needed here: Provides the theoretical framework for defining appropriate information flows based on context
  - Quick check question: Can you explain the five parameters of contextual integrity (data subject, sender, receiver, information type, transmission principle) and why they matter for privacy?

- Concept: Theory of Mind
  - Why needed here: Enables tracking of others' mental states, which is crucial for understanding who should/shouldn't know certain information
  - Quick check question: How would you design a prompt to test whether a model can track that Character A knows a secret but Character B doesn't?

- Concept: Inference-time Privacy
  - Why needed here: Shifts focus from training data protection to real-time decision making about information disclosure
  - Quick check question: What's the difference between protecting training data with differential privacy versus controlling information flow during inference?

## Architecture Onboarding

- Component map: Benchmark generation (GPT-4 + human validation) -> Multi-tier evaluation framework -> Leakage detection mechanisms -> Human annotation pipeline
- Critical path: 1) Generate benchmark scenarios using GPT-4, 2) Validate with human annotations, 3) Run models through each tier, 4) Detect leakage using string matching and proxy models, 5) Analyze results by contextual factors
- Design tradeoffs: String matching vs. semantic understanding for leakage detection, privacy prompts vs. letting models reason naturally, average vs. worst-case metrics for leakage reporting
- Failure signatures: High leakage even with privacy instructions, poor performance on theory-of-mind tasks, inability to balance privacy and utility
- First 3 experiments: 1) Run baseline models on Tier 1 to establish correlation with human sensitivity judgments, 2) Test privacy prompts on Tier 3 to measure their effectiveness, 3) Compare average vs. worst-case leakage metrics across all tiers

## Open Questions the Paper Calls Out

### Open Question 1
What specific inference-time privacy mechanisms could effectively enhance contextual privacy reasoning in LLMs? Based on the paper's emphasis on needing "novel, principled inference-time privacy-preserving approaches" based on reasoning and theory of mind. This remains unresolved because existing data-centric privacy measures are insufficient for inference-time privacy risks. Development and evaluation of new inference-time privacy mechanisms that significantly reduce privacy leakage would resolve this.

### Open Question 2
How can LLMs be trained or fine-tuned to better understand and apply theory of mind in the context of privacy reasoning? The paper emphasizes the importance of theory of mind in contextual privacy reasoning and notes that LLMs struggle with theory of mind capabilities. This remains unresolved because current LLMs lack robust theory of mind capabilities crucial for understanding privacy norms. Experimental results showing improved privacy reasoning after incorporating theory of mind training would resolve this.

### Open Question 3
What are the potential trade-offs between privacy preservation and utility in LLMs when handling sensitive information? The paper discusses the trade-off between privacy and utility in Tier 4 scenarios. This remains unresolved because the paper highlights the challenge of balancing privacy preservation with utility but doesn't provide a clear solution. Empirical studies quantifying these trade-offs and methods to optimize this balance would resolve this.

## Limitations
- Benchmark relies on synthetic scenarios that may not capture full complexity of real-world privacy contexts
- Leakage detection uses string matching and proxy models that may miss nuanced semantic disclosures
- Results depend on specific prompt formulations that may not generalize to all application domains

## Confidence
- High: LLMs leak private information more frequently than humans across all benchmark tiers
- Medium: Privacy-inducing prompts and chain-of-thought reasoning do not significantly reduce leakage
- Medium: Theory of mind deficits contribute to privacy reasoning failures

## Next Checks
1. Test the benchmark with additional LLM architectures (Claude, Gemini) to verify model family independence
2. Conduct ablation studies removing specific contextual integrity parameters to identify which dimensions matter most
3. Evaluate performance on real-world meeting transcripts from open-source datasets to validate synthetic benchmark relevance