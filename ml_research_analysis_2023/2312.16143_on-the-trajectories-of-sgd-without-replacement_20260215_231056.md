---
ver: rpa2
title: On the Trajectories of SGD Without Replacement
arxiv_id: '2312.16143'
source_url: https://arxiv.org/abs/2312.16143
tags:
- replacement
- regularizer
- arxiv
- hessian
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the implicit regularization of Stochastic Gradient
  Descent (SGD) without replacement, the variant typically used to optimize large-scale
  neural networks. The authors consider a realistic regime where the product of the
  learning rate and Hessian can be O(1), without specifying any model architecture,
  learning task, or loss function.
---

# On the Trajectories of SGD Without Replacement

## Quick Facts
- arXiv ID: 2312.16143
- Source URL: https://arxiv.org/abs/2312.16143
- Authors: 
- Reference count: 40
- Key outcome: SGD without replacement implicitly regularizes the loss landscape via a covariance-based regularizer, leading to faster saddle escape and avoidance of the edge of stability.

## Executive Summary
This paper analyzes the implicit regularization of Stochastic Gradient Descent (SGD) without replacement, the variant typically used to optimize large-scale neural networks. The authors show that, in a realistic regime where the product of learning rate and Hessian can be O(1), SGD without replacement differs from SGD with replacement by an additional step on a novel regularizer. This regularizer penalizes the covariance of gradients, measured in a loss-dependent norm determined by the Hessian, and encourages sparsity in the Hessian spectrum. The paper also explains why SGD without replacement escapes saddle points faster and does not reach the edge of stability, as opposed to full-batch gradient descent.

## Method Summary
The authors analyze the implicit regularization effect of SGD without replacement by considering the local trajectory deviation from full-batch gradient descent using Taylor expansions. They derive a regularizer that penalizes the covariance of gradients, measured in a Hessian-dependent norm, and analyze its effect on the Hessian spectrum, saddle escape dynamics, and edge-of-stability behavior. The theoretical analysis is complemented by experiments on synthetic regression tasks and CIFAR-10.

## Key Results
- SGD without replacement is locally equivalent to making an additional step on a novel regularizer that penalizes the covariance of gradients.
- This regularizer encourages sparsity in the Hessian spectrum and pushes the trajectory toward flatter regions of the loss landscape.
- SGD without replacement escapes saddle points significantly faster than SGD with replacement via a drift effect rather than diffusion.
- The regularizer's step size becomes larger than full-batch GD steps when the top Hessian eigenvalues exceed a threshold, preventing the algorithm from reaching the edge of stability.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SGD without replacement implicitly regularizes the loss landscape by penalizing the covariance of the gradients, measured in a Hessian-dependent norm.
- Mechanism: The algorithm adds a step on a regularizer that pushes the trajectory toward regions where the variance of mini-batch gradients is lower. This occurs because the dependency between batches (due to reshuffling without replacement) induces a bias in the expected trajectory.
- Core assumption: The product of learning rate and Hessian can be O(1), and the gradient norms are small compared to the Hessian norms during training.
- Evidence anchors:
  - [abstract] "Our core theoretical result is that optimizing with SGD without replacement is locally equivalent to making an additional step on a novel regularizer."
  - [section 3.2] "SGD without replacement differs from the same number of steps of SGD with replacement or GD, by an additional step on a regularizer."
  - [corpus] Found 25 related papers, but none directly confirm the covariance-based regularizer mechanism. FMR=0.386 suggests moderate relatedness.
- Break condition: If the Hessian eigenvalues are uniformly large and positive, or if the gradient variance is negligible, the regularizer effect diminishes.

### Mechanism 2
- Claim: SGD without replacement escapes saddle points much faster than SGD with replacement due to a drift effect rather than diffusion.
- Mechanism: The regularizer introduces a non-zero effective drift in directions where the loss gradients have non-zero overlap with negative Hessian eigenvectors. This drift pulls the trajectory away from saddle points without relying on random noise.
- Core assumption: At least one data point's gradient has a non-zero projection onto the escaping direction of the saddle.
- Evidence anchors:
  - [section 5.2] "SGD without replacement escapes saddle, i.e., the loss is at least O(1) smaller, after #epochs > 2 ln(η) + ln(u) + 2 ln(c) − ln(b) / cλ."
  - [abstract] "SGD without replacement may escape saddles significantly faster than SGD with replacement."
  - [corpus] No direct evidence in neighbors for drift-based saddle escape; FMR suggests moderate relevance.
- Break condition: If all gradients at the saddle have zero projection onto escaping directions, or if the third derivative is unbounded, the drift vanishes.

### Mechanism 3
- Claim: SGD without replacement does not reach the edge of stability because the regularizer's step size becomes larger than full-batch GD steps when the top Hessian eigenvalues exceed a threshold.
- Mechanism: When λ_max > η⁻¹ + α_EoS, the regularizer step dominates, pulling the trajectory away from the direction of progressive sharpening and preventing further Hessian growth.
- Core assumption: There exist at least two large eigenvalues and the noise covariance is non-zero along their eigendirections.
- Evidence anchors:
  - [section 6.2] "When λ_max > η⁻¹ + α_EoS, the size of a step on the regularizer of Theorem 1 becomes bigger than k steps of GD."
  - [abstract] "We also propose an explanation for why SGD does not train at the edge of stability (as opposed to GD)."
  - [corpus] No direct neighbor evidence for edge-of-stability breaking; moderate FMR.
- Break condition: If the noise covariance along large eigenvalues is zero, or if the learning rate is too small, the threshold α_EoS may not be reached.

## Foundational Learning

- Concept: Implicit regularization in optimization
  - Why needed here: The paper's central claim is that SGD without replacement adds an implicit regularizer that biases the trajectory. Understanding implicit regularization is essential to interpret the theoretical results.
  - Quick check question: What is the difference between explicit and implicit regularization in optimization algorithms?

- Concept: Hessian spectrum and its role in optimization
  - Why needed here: The regularizer penalizes directions corresponding to small and negative Hessian eigenvalues. Knowing how the Hessian spectrum shapes optimization dynamics is key.
  - Quick check question: How does the distribution of Hessian eigenvalues affect the convergence and generalization of SGD?

- Concept: Stochastic gradient descent variants (with vs. without replacement)
  - Why needed here: The paper contrasts SGD with and without replacement, showing qualitative differences in trajectories. Understanding the mechanics of each variant is foundational.
  - Quick check question: Why does SGD without replacement have dependent batches, and how does this affect the noise structure?

## Architecture Onboarding

- Component map: Taylor expansion of SGD trajectory deviation from full-batch GD -> Regularizer derivation via expectation of batch-dependent terms -> Analysis of regularizer effect on Hessian spectrum and saddle escape -> Edge-of-stability threshold determination
- Critical path:
  1. Compute trajectory deviation using Taylor expansion.
  2. Take expectation over batch sampling to identify regularizer.
  3. Analyze regularizer effect on Hessian spectrum and saddle escape.
  4. Determine conditions for edge-of-stability avoidance.
- Design tradeoffs:
  - Local vs. global analysis: The theory is local (Taylor-based) but claims global effects (escaping saddles, avoiding edge of stability).
  - Dependency assumption: Relies on batch dependency; if batches were independent, the regularizer would vanish.
- Failure signatures:
  - If the gradient norms do not shrink during training, the Taylor expansion may not converge.
  - If the Hessian has uniformly large positive eigenvalues, the regularizer may not bias the trajectory effectively.
  - If the noise covariance along large eigenvalues is zero, the edge-of-stability threshold may not be reached.
- First 3 experiments:
  1. Train a shallow network on a small dataset with SGD without replacement and measure the Hessian spectrum evolution; compare with SGD with replacement.
  2. Initialize at a strict saddle point and run SGD without replacement; measure escape time and compare with SGD with replacement.
  3. Increase learning rate until λ_max approaches η⁻¹; observe whether the trajectory deviates from GD's edge-of-stability path.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the regularizer in SGD without replacement lead to qualitatively different minima compared to SGD with replacement?
- Basis in paper: [explicit] The paper shows that SGD without replacement travels flat areas and escapes saddles faster than SGD with replacement, suggesting potential differences in the minima they converge to.
- Why unresolved: While the paper demonstrates qualitative differences in the trajectories of the two algorithms, it does not definitively prove that they converge to different minima.
- What evidence would resolve it: Experiments comparing the minima found by SGD with and without replacement on the same tasks and datasets.

### Open Question 2
- Question: How does the size of the learning rate relative to the Hessian eigenvalues affect the strength of the implicit regularization in SGD without replacement?
- Basis in paper: [explicit] The paper discusses how the regularizer's step size depends on the product of the learning rate and Hessian eigenvalues, and how this affects the algorithm's behavior in different regions of the loss landscape.
- Why unresolved: The paper provides theoretical insights but does not offer a precise characterization of how the learning rate-Hessian relationship determines the regularization strength.
- What evidence would resolve it: Empirical studies systematically varying the learning rate and Hessian eigenvalues to measure the impact on the regularizer's strength and the resulting model properties.

### Open Question 3
- Question: Does the implicit regularization in SGD without replacement improve generalization performance beyond reducing the variance of gradients?
- Basis in paper: [explicit] The paper suggests that the regularizer penalizes the trace of the Fisher matrix, which is related to generalization. It also discusses how the regularizer can lead to flatter minima, which are often associated with better generalization.
- Why unresolved: While the paper provides theoretical justifications, it does not offer conclusive empirical evidence that the implicit regularization directly improves generalization.
- What evidence would resolve it: Experiments comparing the generalization performance of models trained with SGD with and without replacement, controlling for other factors like the learning rate and batch size.

## Limitations
- The theoretical analysis relies on local Taylor expansions, which may not fully capture global optimization dynamics.
- The assumptions about gradient and Hessian norms being small enough for the Taylor expansion to hold are not verified empirically.
- The edge-of-stability analysis depends on the existence of at least two large Hessian eigenvalues, which may not always be the case in practical neural network training.

## Confidence
- High Confidence: The derivation of the regularizer as a covariance term in the Hessian-dependent norm is mathematically rigorous and well-supported by the Taylor expansion framework.
- Medium Confidence: The claim that SGD without replacement escapes saddles faster than SGD with replacement via drift rather than diffusion is plausible but lacks direct empirical evidence in the paper.
- Low Confidence: The explanation for why SGD does not reach the edge of stability relies on several assumptions about the Hessian spectrum and noise covariance that are not fully verified.

## Next Checks
1. **Empirical Hessian Spectrum Analysis**: Train a shallow network on CIFAR-10 using both SGD with and without replacement, then compute and compare the Hessian spectrum at convergence to verify the claimed sparsity effect.
2. **Saddle Escape Experiment**: Initialize a network at a known strict saddle point and measure the escape time for both SGD variants, confirming whether drift-based escape is indeed faster.
3. **Edge-of-Stability Threshold Test**: Systematically increase the learning rate and monitor the top Hessian eigenvalues; verify whether the trajectory deviates from GD's edge-of-stability path when λ_max exceeds η⁻¹ + α_EoS.