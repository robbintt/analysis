---
ver: rpa2
title: Doubly Optimal No-Regret Learning in Monotone Games
arxiv_id: '2301.13120'
source_url: https://arxiv.org/abs/2301.13120
tags:
- games
- regret
- learning
- algorithm
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes the accelerated optimistic gradient (AOG) algorithm,\
  \ which achieves optimal O(\u221AT) regret in adversarial settings and optimal O(1/T)\
  \ last-iterate convergence rate to Nash equilibrium in smooth monotone games. The\
  \ algorithm combines optimism with Halpern iteration, striking a balance between\
  \ adversarial regret and convergence rate."
---

# Doubly Optimal No-Regret Learning in Monotone Games

## Quick Facts
- arXiv ID: 2301.13120
- Source URL: https://arxiv.org/abs/2301.13120
- Reference count: 40
- Achieves optimal O(√T) regret in adversarial settings and optimal O(1/T) last-iterate convergence to Nash equilibrium in smooth monotone games

## Executive Summary
This paper introduces the Accelerated Optimistic Gradient (AOG) algorithm that simultaneously achieves optimal regret in adversarial settings and optimal last-iterate convergence to Nash equilibrium in smooth monotone games. The key innovation combines optimism with Halpern iteration, striking a precise balance between adversarial robustness and convergence speed. A novel step-size adaptation procedure automatically adjusts learning rates based on second-order gradient variation, enabling seamless switching between O(1/T) convergence and O(√T) regret guarantees depending on the environment.

## Method Summary
AOG builds on optimistic gradient methods by incorporating Halpern iteration, which adds a carefully diminishing strongly convex loss to each player's objective. The algorithm maintains two points per iteration: a midpoint played by the player and an update point used for the next iteration. Step sizes are adapted using the player's own second-order gradient variation as a proxy for environmental conditions - constant when variation is bounded (monotone games) and decreasing when variation grows (adversarial settings). This enables automatic switching between convergence and regret guarantees without prior knowledge of the game structure.

## Key Results
- Achieves optimal O(√T) regret in adversarial settings
- Achieves optimal O(1/T) last-iterate convergence rate to Nash equilibrium in smooth monotone games
- Improves individual dynamic regret from O(√T) to O(log T) in smooth monotone games

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AOG achieves doubly optimal performance by combining optimism with Halpern iteration
- Mechanism: Optimism stabilizes trajectory convergence in games while Halpern iteration provides acceleration in optimization. The algorithm adds a diminishing strongly convex loss to the player's loss function with a carefully crafted schedule that balances adversarial regret and convergence rate.
- Core assumption: The diminishing schedule of the strongly convex loss must strike a precise balance - too slow causes sub-optimal adversarial regret, too fast causes slower convergence
- Evidence anchors:
  - [abstract]: "The key of our new algorithm is combining optimism with Halpern iteration [Halpern, 1967], a mechanism used in optimization to design accelerated methods."
  - [section]: "The key of our new algorithm is combining optimism with Halpern iteration [Halpern, 1967], a mechanism used in optimization to design accelerated methods."
- Break condition: If the diminishing schedule is not carefully crafted, the algorithm will either have sub-optimal adversarial regret (if added loss diminishes too slowly) or converge at a slower rate (if added loss decreases too quickly)

### Mechanism 2
- Claim: AOG achieves optimal O(√T) regret in adversarial settings and optimal O(1/T) last-iterate convergence in smooth monotone games
- Mechanism: The step-size adaptation procedure uses second-order gradient variation as a proxy for the environment. When gradient variation is bounded (monotone game setting), constant step size is maintained for O(1/T) convergence. When variation exceeds threshold (adversarial setting), step size decreases to bound regret by the variation, which is at most O(√T).
- Core assumption: The second-order gradient variation St+1 = ∑t s=2∥gs+ 1/2 − gs− 1/2∥2 is bounded by a constant in smooth monotone games
- Evidence anchors:
  - [section]: "This procedure uses the player's own second-order gradient variation St+1 = ∑t s=2∥gs+ 1/2 − gs− 1/2∥2 as a proxy for the environment"
  - [section]: "Theorem 4: In the same setup of Theorem 2 but with η = 1/3L, for any player i and time t≥ 2, we have SiT ≤ 4500πD2L2"
- Break condition: If the game is not smooth monotone, the second-order gradient variation may grow unbounded, preventing the algorithm from achieving O(1/T) convergence

### Mechanism 3
- Claim: Each player suffers only O(log T) individual dynamic regret in smooth monotone games
- Mechanism: The O(1/T) last-iterate convergence rate implies that the total gap function TGAP(xt+1/2) = O(1/t). Summing this from t=2 to T gives O(log T) dynamic regret. This is exponentially better than the previous O(√T) bound.
- Core assumption: The last-iterate convergence rate of O(1/T) directly translates to the total gap function being O(1/t) for each t
- Evidence anchors:
  - [section]: "As an interesting byproduct of our last-iterate convergence rate, we further show that each player suffers only an O(log T) individual dynamic regret"
  - [section]: "DynamicRegi(T) = ∑t=1 ℓi(xt+1/2) − minx′∈Xi ℓi(x′, x−it+1/2) ≤ O(1) + ∑t=2T TGAP(xt+1/2) ≤ ∑t=2T O(1/t) = O(log T)"
- Break condition: If the game is not monotone or the last-iterate convergence rate is not O(1/T), the dynamic regret bound will degrade to at least O(√T)

## Foundational Learning

- Concept: Monotone games and Nash equilibria
  - Why needed here: The algorithm is designed specifically for smooth monotone games, which encompass many commonly studied games including two-player zero-sum games and convex-concave games
  - Quick check question: What is the key property of monotone games that guarantees existence of Nash equilibrium?

- Concept: Online learning and regret
  - Why needed here: The algorithm must achieve optimal regret in the adversarial setting while also converging to Nash equilibrium in the game setting
  - Quick check question: What is the difference between external regret and dynamic regret?

- Concept: Gradient-based algorithms and their limitations
  - Why needed here: Understanding why existing algorithms like optimistic gradient and extragradient have O(1/√T) convergence rate, which is not optimal for all gradient-based algorithms
  - Quick check question: Why does the well-studied family of no-regret learning algorithms - follow-the-regularized-leader fail to converge even in two-player zero-sum games?

## Architecture Onboarding

- Component map:
  - Core algorithm: Accelerated Optimistic Gradient (AOG) combining optimism with Halpern iteration
  - Step-size adaptation: Uses second-order gradient variation to automatically adjust learning rate
  - Potential function: Used to prove last-iterate convergence rate
  - Normal cone projection: Handles constrained optimization

- Critical path: 
  1. Initialize with arbitrary x1
  2. At each iteration t: compute xt+1/2 using projection with modified gradients
  3. Play xt+1/2 and receive gradient feedback
  4. Update xt+1 using projection with modified gradients
  5. Check second-order gradient variation and adjust step size if needed
  6. Repeat

- Design tradeoffs:
  - Using constant step size vs adaptive step size: Constant is better for convergence, adaptive handles adversarial settings
  - Single gradient vs double gradient: AOG uses single gradient like OG, EAG uses double gradient but requires playing both points
  - Convergence rate vs regret: O(1/T) convergence requires careful step size choice, O(√T) regret is more robust

- Failure signatures:
  - Sub-optimal regret: Second-order gradient variation grows too large, step size adaptation not working
  - Slow convergence: Step size too small, game not smooth monotone
  - Cycling behavior: Missing optimism component, algorithm behaves like FTRL

- First 3 experiments:
  1. Test AOG in adversarial setting with known Lipschitz constant and diameter to verify O(√T) regret
  2. Test AOG in smooth monotone game (e.g., two-player zero-sum game) with constant step size to verify O(1/T) convergence
  3. Test AOG with step-size adaptation in mixed environment (game with occasional adversarial players) to verify automatic switching between convergence and regret guarantees

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the doubly optimal no-regret learning framework be extended to handle noisy gradient feedback or bandit feedback scenarios?
- Basis in paper: The authors mention that extending results to noisy or bandit feedback is an interesting future direction, and note that optimal sub-linear last-iterate convergence rates are known for strongly monotone games under bandit feedback [Lin et al., 2022, Jordan et al., 2022].
- Why unresolved: The current AOG algorithm and analysis rely on exact gradient feedback. Extending to noisy or partial feedback requires new techniques to handle gradient estimation errors and their impact on regret and convergence guarantees.
- What evidence would resolve it: A modified AOG algorithm with theoretical guarantees for regret and last-iterate convergence under noisy/bandit feedback, supported by both theoretical analysis and empirical validation.

### Open Question 2
- Question: What is the optimal individual dynamic regret achievable in smooth monotone games using no-regret learning algorithms?
- Basis in paper: The authors significantly improve the state-of-the-art individual dynamic regret bound from O(√T) to O(log T) and explicitly pose this as an open question for future research.
- Why unresolved: While the O(log T) bound is a substantial improvement, it's unclear if this is the theoretical limit or if better bounds are achievable with different algorithm designs or game structures.
- What evidence would resolve it: Either a lower bound proof showing Ω(log T) is optimal for individual dynamic regret in smooth monotone games, or a new algorithm achieving better than O(log T) individual dynamic regret.

### Open Question 3
- Question: How does the performance of AOG scale with the number of players in multi-player monotone games?
- Basis in paper: The analysis assumes a fixed number of players N, but the paper doesn't explore how the convergence rates and regret bounds depend on N.
- Why unresolved: The potential function and gradient variation analysis may behave differently as N increases, potentially affecting the tightness of the bounds or requiring different step-size adaptation strategies.
- What evidence would resolve it: An extension of Theorem 2 and Theorem 4 showing explicit dependence on N, with experimental validation showing performance across different numbers of players.

## Limitations

- The algorithm's performance heavily depends on accurate estimation of the Lipschitz constant L, which may be challenging in practice
- The step-size adaptation procedure may struggle in environments where gradient variation is neither clearly bounded nor clearly unbounded
- The theoretical guarantees assume perfect gradient feedback, which may not hold in stochastic or noisy environments

## Confidence

- Confidence Level: Medium on the universal applicability of the second-order gradient variation adaptation procedure
- Confidence Level: Low on the practical implementation details, particularly around the normal cone projection computation
- Confidence Level: Medium on the O(log T) dynamic regret claim

## Next Checks

1. **Empirical robustness testing**: Evaluate AOG's performance across a spectrum of games varying from fully monotone to highly non-monotone, measuring both regret and convergence rates to identify regime boundaries.

2. **Threshold sensitivity analysis**: Systematically vary the second-order gradient variation threshold (currently 4500π) to understand its impact on both convergence rate and regret performance, potentially deriving a more principled choice.

3. **Noise tolerance evaluation**: Test the algorithm's behavior under different levels of gradient noise to assess the practical applicability of the theoretical guarantees and identify potential modifications needed for stochastic settings.