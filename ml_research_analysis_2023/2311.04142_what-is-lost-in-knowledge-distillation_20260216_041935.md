---
ver: rpa2
title: What is Lost in Knowledge Distillation?
arxiv_id: '2311.04142'
source_url: https://arxiv.org/abs/2311.04142
tags:
- layers
- teacher
- number
- student
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates knowledge distillation (KD) in NLP by analyzing
  what information is lost during the compression of a large teacher model into smaller
  student models. The authors systematically vary model configurations (layers, attention
  heads, hidden dimensions) and evaluate performance across 8 GLUE tasks.
---

# What is Lost in Knowledge Distillation?

## Quick Facts
- arXiv ID: 2311.04142
- Source URL: https://arxiv.org/abs/2311.04142
- Authors: 
- Reference count: 9
- Key outcome: Layer reduction is the most effective KD strategy with minimal performance loss

## Executive Summary
This study systematically investigates knowledge distillation in NLP by analyzing what information is lost when compressing large teacher models into smaller student models. Using RoBERTa base as the teacher and varying configurations across layers, attention heads, and hidden dimensions, the authors evaluate performance across 8 GLUE tasks. The research reveals that reducing model layers is the most effective compression strategy with minimal performance degradation, while reducing attention heads proves more detrimental than previously suggested. The study also identifies task-specific sensitivities to KD, with some tasks showing high resistance while others are particularly vulnerable to compression.

## Method Summary
The authors conduct controlled experiments using RoBERTa base (12 layers, 12 heads, 768 dimensions) as the teacher model, creating student models with various configurations including 3L, 6L, 9L, 4AH, 8AH, 384D, 516D, and 6L_384D. They implement a layer mapping strategy that connects first and last layers between teacher and student models, with uniform mapping for intermediate layers. KD training uses response-based and feature-based approaches with temperature=2, α=0.33, hard loss=0.33, KD loss=0.33, learning rate=5e-5, 10 epochs, batch size=12, and Adam optimizer. Performance is evaluated on all 8 GLUE tasks using appropriate metrics (accuracy, F1, Pearson-Spearman, Matthews correlation).

## Key Results
- Layer reduction (9L, 6L) maintains most performance with minimal loss compared to the 12-layer teacher
- Reducing attention heads from 12 to 4 causes significant performance degradation, contradicting prior work
- Some tasks (SST2, QQP) are highly resistant to KD while others (RTE, CoLA) are extremely sensitive
- Reducing hidden dimensions (384D, 516D) leads to moderate performance loss across most tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reducing the number of layers in KD maintains most of the teacher's knowledge up to a certain threshold
- Mechanism: When the number of layers is reduced from 12 to 9 or 6, the remaining layers can still capture the essential information from the teacher model because the first and last layers are preserved and internal layers are mapped uniformly to maintain structural continuity
- Core assumption: The first and last layers contain the most critical information for task performance, and uniform mapping of internal layers preserves enough semantic relationships
- Evidence anchors:
  - [abstract]: "reducing layers is the most effective compression strategy with minimal performance loss"
  - [section]: "When reducing the size of the teacher model (from 12 to fewer layers), we had to come up with a mapping strategy...we connect the first and last layers of the student and teacher models in all configurations"
  - [corpus]: "No direct corpus evidence available for this specific mechanism"
- Break condition: When the number of layers is reduced below 3, the model loses critical information and performance degrades significantly as observed in the experimental results

### Mechanism 2
- Claim: Attention heads are critical for knowledge distillation and cannot be reduced without significant performance loss
- Mechanism: Attention heads in transformer models enable parallel processing of different input subspaces. Reducing the number of heads decreases the model's ability to attend to different parts of the input simultaneously, which is crucial for capturing nuanced relationships in the data
- Core assumption: Each attention head processes a unique subspace of the input, and reducing their number eliminates important parallel processing capabilities
- Evidence anchors:
  - [abstract]: "The number of attention heads proves critical - contrary to prior work suggesting they can be reduced without impact"
  - [section]: "Reducing the number of attention heads should also help with the speed increase whereas this is not observed in our experiments...in Transformers, parallelism mainly comes from the multi-head attention mechanism"
  - [corpus]: "No direct corpus evidence available for this specific mechanism"
- Break condition: If the attention mechanism is modified to maintain parallel processing capabilities despite fewer heads, or if the task is simple enough that fewer attention heads suffice

### Mechanism 3
- Claim: Different tasks show varying sensitivity to knowledge distillation, with some tasks being more resistant than others
- Mechanism: The structure and complexity of the task, along with the size and characteristics of the training data, determine how much information can be effectively transferred during KD. Tasks with simpler decision boundaries or larger datasets are more resistant to compression
- Core assumption: The amount of training data and the inherent complexity of the task influence the model's ability to learn and retain information during KD
- Evidence anchors:
  - [abstract]: "Some tasks (SST2, QQP) are more resistant to KD while others (RTE, CoLA) are more sensitive"
  - [section]: "There is a direct relation between the size of the training set and the KD loss, i.e. the smaller the set, the higher the loss"
  - [corpus]: "No direct corpus evidence available for this specific mechanism"
- Break condition: When the training data is sufficiently large or the task complexity is reduced, the sensitivity to KD may decrease

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Understanding how transformers work is crucial for comprehending why attention heads are critical and how layer reduction affects model performance
  - Quick check question: How do attention heads in transformers enable parallel processing of input data?

- Concept: Knowledge distillation principles
  - Why needed here: The paper builds on fundamental KD concepts, so understanding how knowledge is transferred from teacher to student is essential
  - Quick check question: What is the difference between response-based and feature-based knowledge distillation?

- Concept: Model compression techniques
  - Why needed here: The paper compares KD with other compression methods, so understanding the landscape of compression techniques provides context
  - Quick check question: How does knowledge distillation differ from model pruning and quantization?

## Architecture Onboarding

- Component map: Teacher model (12-layer RoBERTa base) → Student models (various configurations: 3L, 6L, 9L, 4AH, 8AH, 384D, 516D, 6L_384D) → Evaluation on GLUE tasks
- Critical path: Layer mapping strategy → KD training with specific hyperparameters → Evaluation on development sets
- Design tradeoffs: Layer reduction vs. attention head reduction vs. hidden dimension reduction - each has different impacts on performance and speed
- Failure signatures: Performance degradation below certain thresholds (e.g., 3 layers), unexpected speed slowdown with fewer attention heads, task-specific sensitivity
- First 3 experiments:
  1. Compare performance of 12-layer teacher vs. 9-layer student on SST2 task to validate minimal loss with layer reduction
  2. Test 12-layer teacher vs. 4-head student on RTE task to demonstrate attention head sensitivity
  3. Evaluate 12-layer teacher vs. 384-dimension student on MNLI task to assess hidden dimension impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal attention head configuration for knowledge distillation across different NLP tasks?
- Basis in paper: [explicit] The paper found that reducing attention heads significantly degraded performance, contradicting prior work suggesting heads could be reduced without impact
- Why unresolved: The paper only tested 4-head and 8-head configurations against the baseline of 12 heads, leaving uncertainty about intermediate configurations and task-specific optimal values
- What evidence would resolve it: Systematic testing of attention head counts (2, 3, 4, 5, 6, 8, 10, 12) across multiple tasks to identify performance curves and task-specific optimal configurations

### Open Question 2
- Question: How does the number of training samples affect the sensitivity of tasks to knowledge distillation?
- Basis in paper: [explicit] The authors observed that smaller training sets generally showed higher KD loss, with MNLI being an exception despite having the largest dataset
- Why unresolved: The paper only tested this relationship across 8 GLUE tasks without varying training set sizes systematically or exploring why MNLI behaves differently
- What evidence would resolve it: Controlled experiments with subsampled training sets for each task to map sensitivity curves, plus analysis of MNLI's unique characteristics that affect its distillation behavior

### Open Question 3
- Question: What architectural modifications could improve knowledge distillation for tasks that show high sensitivity to compression?
- Basis in paper: [explicit] RTE and CoLA showed high sensitivity to KD across all configurations tested, suggesting these tasks require special consideration
- Why unresolved: The paper only tested standard compression techniques (layer reduction, width reduction, head reduction) without exploring architectural modifications or task-specific adaptations
- What evidence would resolve it: Testing alternative architectures (sparse attention patterns, task-specific layer arrangements, or specialized distillation objectives) for high-sensitivity tasks to identify configurations that maintain performance during compression

## Limitations

- Experiments limited to RoBERTa base architecture and English GLUE tasks, limiting generalizability
- Does not explore interactions between simultaneous compression strategies (layers, heads, dimensions)
- Theoretical speed analysis without empirical measurements across different hardware platforms

## Confidence

- High Confidence:
  - Layer reduction is the most effective compression strategy with minimal performance loss
  - Reducing attention heads leads to significant performance degradation
  - Task-specific sensitivity to KD varies systematically

- Medium Confidence:
  - The optimal layer mapping strategy preserves most information
  - Attention heads are critical for capturing nuanced relationships
  - Training data size correlates with KD effectiveness

- Low Confidence:
  - Specific threshold values for acceptable performance degradation
  - Precise quantification of speed improvements across hardware
  - Generalizability to non-English tasks and other model architectures

## Next Checks

1. **Cross-Architecture Validation**: Replicate the experiments using BERT and other transformer variants as teacher models to verify if the layer reduction advantage holds across different architectures and pretraining objectives.

2. **Combined Compression Strategy Analysis**: Systematically evaluate models with simultaneous reductions in layers, attention heads, and hidden dimensions to identify optimal configurations that balance performance and efficiency more effectively than single-dimension reductions.

3. **Hardware-Aware Performance Evaluation**: Conduct empirical measurements of inference speed and memory usage across different hardware platforms (CPU, GPU, TPU) to validate theoretical speedups and identify practical deployment considerations.