---
ver: rpa2
title: Leveraging Auxiliary Domain Parallel Data in Intermediate Task Fine-tuning
  for Low-resource Translation
arxiv_id: '2306.01382'
source_url: https://arxiv.org/abs/2306.01382
tags:
- size
- task
- train
- language
- pair
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of intermediate-task fine-tuning
  (ITFT) to improve domain-specific translation for low-resource languages using pre-trained
  multilingual sequence-to-sequence (PMSS) models. The authors experiment with several
  low-resource languages, some of which are not present in the selected PMSS model.
---

# Leveraging Auxiliary Domain Parallel Data in Intermediate Task Fine-tuning for Low-resource Translation

## Quick Facts
- **arXiv ID**: 2306.01382
- **Source URL**: https://arxiv.org/abs/2306.01382
- **Reference count**: 40
- **Primary result**: ITFT improves domain-specific translation for low-resource languages, especially when target domain data is limited and languages are missing/under-represented in PMSS models.

## Executive Summary
This paper investigates intermediate-task fine-tuning (ITFT) for improving domain-specific neural machine translation (NMT) of low-resource languages using pre-trained multilingual sequence-to-sequence models. The authors experiment with five low-resource Indic languages across three domains, finding that ITFT with auxiliary domain data significantly improves in-domain translation quality when target domain data is limited or unavailable. The study also reveals that ITFT can partially mitigate the negative impact of domain divergence between training and test data. A notable contribution is the release of a multi-way parallel Bible dataset containing 25k sentences for the selected low-resource languages.

## Method Summary
The method involves a two-stage fine-tuning process using pre-trained multilingual sequence-to-sequence models (mBART, mT5). First, the PMSS model undergoes fine-tuning on auxiliary domain parallel data (e.g., CCAligned corpus with 100k sentences). Then, the model is further fine-tuned on the target domain parallel data (e.g., PMIndia corpus with 50k sentences). The study varies the size of the intermediate task (1k, 25k, 100k sentences) and tests on in-domain and out-domain test sets. Performance is measured using SentencePiece BLEU (spBLEU) across five low-resource languages: Hindi, Gujarati, Kannada, Sinhala, and Tamil.

## Key Results
- ITFT with auxiliary domain data significantly improves domain-specific translation quality when target domain data is limited (less than 10k sentences).
- The benefits of ITFT are particularly pronounced for languages missing or under-represented in the PMSS models (e.g., Kannada).
- ITFT can partially mitigate the negative impact of domain divergence between training and test data, though the effect depends on the size of the intermediate task.
- The newly released Bible dataset (25k sentences) proves valuable for training, as these languages previously had less than 4k parallel sentences available.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ITFT with auxiliary domain data improves performance by exposing the model to more linguistic variation before final domain adaptation.
- Mechanism: The PMSS model first fine-tunes on auxiliary domain data, broadening its exposure to different language patterns and vocabulary, enabling more effective adaptation during final domain fine-tuning.
- Core assumption: Auxiliary domain data provides useful linguistic diversity that helps the model generalize better during final domain adaptation.
- Evidence anchors:
  - [abstract] "intermediate-task fine-tuning (ITFT) of PMSS models is extremely beneficial for domain-specific NMT, especially when target domain data is limited/unavailable"
  - [section 4.2] "ITFT with a different domain outperforms the baseline when the final task has less than 10k data"
  - [corpus] Weak - corpus evidence mentions related work but doesn't directly confirm linguistic diversity benefit
- Break condition: If auxiliary domain data is too dissimilar from target domain, benefits may not materialize or could degrade performance.

### Mechanism 2
- Claim: ITFT mitigates the negative impact of domain divergence between training and test data.
- Mechanism: By first training on auxiliary domain, the model develops more robust representations that are less sensitive to domain-specific shifts, making final fine-tuning on target domain more effective even with domain differences.
- Core assumption: The model's representations become more domain-invariant after exposure to multiple domains through ITFT.
- Evidence anchors:
  - [abstract] "show that ITFT can mitigate the impact of domain divergence to some extent"
  - [section 4.3] "When the size of the intermediate task is 25k, the divergence between the domain of the final task and the domain of the test task shows a lesser correlation to that of the baseline"
  - [corpus] Weak - related papers mention domain adaptation but don't directly quantify divergence mitigation
- Break condition: If intermediate task size is too small (e.g., 1k sentences), divergence mitigation effect disappears.

### Mechanism 3
- Claim: Languages missing or underrepresented in PMSS models benefit more from ITFT because auxiliary domain fine-tuning compensates for poor initial representation.
- Mechanism: For languages with limited presence in pre-trained model, initial auxiliary domain fine-tuning provides crucial exposure that the PMSS model lacks, leading to better downstream performance.
- Core assumption: Languages missing in PMSS models start with poor representations that can be improved through targeted fine-tuning on relevant data.
- Evidence anchors:
  - [abstract] "especially when target domain data is limited/unavailable and the considered languages are missing or under-represented in the PMSS model"
  - [section 4.2] "The impact of ITFT is the highest for Kannada" (a language missing from mBART)
  - [corpus] Moderate - Lee et al. (2022) mentioned in abstract also observed poor results for unseen languages
- Break condition: If target language is well-represented in PMSS model, additional benefit from ITFT may be minimal.

## Foundational Learning

- Concept: Domain divergence measurement using Jensen-Shannon divergence
  - Why needed here: To quantify how different training and test domains are, helping explain performance variations
  - Quick check question: What does a lower JS divergence value indicate about two domains?

- Concept: Multilingual pre-trained sequence-to-sequence models (PMSS)
  - Why needed here: These are base models being fine-tuned, and understanding their limitations with low-resource languages is crucial
  - Quick check question: Why do PMSS models struggle with languages missing from their pre-training data?

- Concept: Fine-tuning strategies and their impact on model performance
  - Why needed here: The paper compares single-stage vs multi-stage fine-tuning approaches
  - Quick check question: How does intermediate task fine-tuning differ from standard fine-tuning in terms of model adaptation?

## Architecture Onboarding

- Component map: Pre-trained PMSS model (mBART/mT5) -> Auxiliary domain fine-tuning -> Target domain fine-tuning -> Evaluation
- Critical path: 1. Select PMSS model and languages 2. Prepare auxiliary and target domain datasets 3. Perform intermediate task fine-tuning 4. Perform final task fine-tuning 5. Evaluate on in-domain and out-domain test sets
- Design tradeoffs:
  - Intermediate task size vs. target task size: Larger intermediate tasks provide more benefit but require more computation
  - Domain similarity: Choosing auxiliary domain too dissimilar from target may hurt performance
  - Language coverage: Models missing languages need more careful fine-tuning strategies
- Failure signatures:
  - Poor performance on out-domain tests indicates domain divergence issues
  - No improvement over baseline suggests intermediate task is too small or too dissimilar
  - Degradation in performance indicates catastrophic forgetting during fine-tuning
- First 3 experiments:
  1. Baseline: Fine-tune PMSS model directly on target domain data (0k intermediate)
  2. Simple ITFT: Fine-tune on 1k auxiliary domain data, then target domain data
  3. Enhanced ITFT: Fine-tune on 25k auxiliary domain data, then target domain data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the impact of ITFT vary across different language families beyond Indo-Aryan and Dravidian languages?
- Basis in paper: [inferred] The paper focuses on four low-resource languages from Indo-Aryan and Dravidian families but does not explore other language families
- Why unresolved: The study only tested languages from two specific families, limiting generalizability to other language families
- What evidence would resolve it: Testing ITFT with languages from Turkic, Sino-Tibetan, Niger-Congo, or other language families to compare effectiveness across different typological groups

### Open Question 2
- Question: What is the optimal intermediate task domain selection strategy when multiple domain options are available for a language pair?
- Basis in paper: [explicit] "consciously selecting data for ITFT is important" and the paper notes domain divergence impact but doesn't provide a systematic selection framework
- Why unresolved: The paper demonstrates domain divergence affects results but doesn't provide guidelines for choosing between available domains
- What evidence would resolve it: A systematic study testing different intermediate domain selection strategies across multiple language pairs and measuring correlation with final task performance

### Open Question 3
- Question: How does the effectiveness of ITFT compare to multi-task learning approaches using combined parallel data from multiple domains?
- Basis in paper: [explicit] "In future, we expect to combine parallel data from different domains in a multi-task setup to further mitigate the impact domain divergence"
- Why unresolved: The paper only tests ITFT sequentially and mentions future work on multi-task learning without empirical comparison
- What evidence would resolve it: Direct comparison experiments between sequential ITFT and parallel multi-task learning approaches using identical datasets

### Open Question 4
- Question: Does the effectiveness of ITFT vary based on the representation level of the language in the PMSS model?
- Basis in paper: [explicit] "especially for languages missing/under-represented in the PMSS model" and the paper notes different gains for different languages
- Why unresolved: The paper shows different gains for different languages but doesn't quantify the relationship between language representation in PMSS models and ITFT effectiveness
- What evidence would resolve it: Testing ITFT across languages with varying levels of representation in PMSS models (high, medium, low, missing) and measuring correlation with performance gains

## Limitations

- Domain divergence measurement relies on correlation analysis rather than direct performance prediction, potentially oversimplifying the relationship between divergence metrics and translation quality.
- The study focuses on 5 Indic languages from similar families, limiting generalizability to truly isolated languages or those with different script systems.
- The optimal intermediate task size (25k) may be specific to the experimental setup rather than universally applicable across different language pairs and domain combinations.

## Confidence

- **High Confidence**: The core finding that ITFT improves in-domain translation for low-resource languages when target data is limited, supported by consistent results across multiple languages and domains.
- **Medium Confidence**: The claim that ITFT mitigates domain divergence effects, though the effect size varies considerably and depends heavily on intermediate task size selection.
- **Low Confidence**: The generalizability of the 25k intermediate task size as optimal across different language pairs and domain combinations.

## Next Checks

1. **Ablation on Language Diversity**: Test ITFT effectiveness on languages from completely different families (e.g., Japanese, Arabic, Swahili) to validate whether benefits extend beyond closely related Indic languages.

2. **Dynamic Intermediate Task Sizing**: Implement a curriculum learning approach where intermediate task size scales based on target domain availability and domain divergence metrics, rather than using fixed sizes.

3. **Long-Tail Language Analysis**: Systematically evaluate performance on languages with fewer than 1k sentences in PMSS models to establish the lower bounds of ITFT effectiveness for truly missing languages.