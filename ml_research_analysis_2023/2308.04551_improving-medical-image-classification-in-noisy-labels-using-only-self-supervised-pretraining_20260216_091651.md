---
ver: rpa2
title: Improving Medical Image Classification in Noisy Labels Using Only Self-supervised
  Pretraining
arxiv_id: '2308.04551'
source_url: https://arxiv.org/abs/2308.04551
tags:
- noisy
- labels
- image
- learning
- self-supervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the effectiveness of self-supervised pretraining
  to improve robustness against label noise in medical image classification. The authors
  explore contrastive learning (SimCLR) and pretext task-based approaches (rotation
  prediction, jigsaw puzzle, and Jigmag puzzle) to initialize deep learning models
  before training with noisy labels on two medical datasets: NCT-CRC-HE-100K tissue
  histology images and COVID-QU-Ex chest X-ray images.'
---

# Improving Medical Image Classification in Noisy Labels Using Only Self-supervised Pretraining

## Quick Facts
- **arXiv ID**: 2308.04551
- **Source URL**: https://arxiv.org/abs/2308.04551
- **Reference count**: 40
- **Primary result**: Self-supervised pretraining significantly improves medical image classification robustness against label noise compared to random initialization.

## Executive Summary
This paper investigates how self-supervised pretraining can improve medical image classification robustness when labels are noisy. The authors explore four self-supervised methods—SimCLR, rotation prediction, jigsaw puzzle, and Jigmag puzzle—to initialize deep learning models before training with noisy labels on two medical datasets. They find that models initialized with self-supervised pretraining weights significantly outperform random initialization, particularly at high noise rates. The choice of best pretraining method depends on the dataset and noise level, with SimCLR and rotation prediction generally performing best. This work addresses a gap in noisy-label learning research by demonstrating the effectiveness of self-supervised pretraining for medical imaging applications.

## Method Summary
The authors implement a two-stage approach: first, self-supervised pretraining using four different methods (SimCLR, rotation prediction, jigsaw puzzle, and Jigmag puzzle) on medical image datasets; second, retraining with noisy labels using standard cross-entropy, Co-teaching, or DivideMix. They use ResNet18 as the backbone architecture and evaluate performance on NCT-CRC-HE-100K tissue histology images and COVID-QU-Ex chest X-ray images under various label noise rates (0.5, 0.6, 0.7, 0.8). The key innovation is using self-supervised pretraining alone to learn better feature extractors that improve robustness against noisy labels, rather than combining it with supervised pretraining.

## Key Results
- Models initialized with self-supervised pretraining weights show significant improvements in classification accuracy and robustness against noisy labels compared to random initialization.
- SimCLR and rotation prediction pretraining methods generally outperform jigsaw puzzle and Jigmag puzzle, with the best choice depending on dataset and noise level.
- Self-supervised pretraining helps overcome the warm-up obstacle in noisy label learning by learning better feature extractors before exposure to noisy labels.
- The improvements are most pronounced at high noise rates (0.6-0.8), where traditional methods struggle with memorization of incorrect labels.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-supervised pretraining acts as a warm-up that prevents memorization of noisy labels.
- Mechanism: Pretraining trains the model on tasks requiring understanding image structure without labels, creating robust feature extractors before noisy label exposure, reducing the model's tendency to fit label noise.
- Core assumption: Early learned features from self-supervised tasks are generalizable enough to benefit the downstream classification task, even with label noise.
- Evidence anchors: Abstract states models initialized with self-supervised pretraining "significantly improve classification accuracy and robustness against noisy labels compared to random initialization."

### Mechanism 2
- Claim: Different self-supervised tasks perform differently depending on dataset and noise level.
- Mechanism: Each pretext task imposes different inductive biases. SimCLR learns global structure, rotation prediction encourages orientation invariance, jigsaw puzzles focus on local patch relationships. Depending on dataset properties, one task's bias may align better.
- Core assumption: The pretraining task's bias is compatible with the dataset's visual and class structure.
- Evidence anchors: Abstract notes "The choice of the best self-supervised task varies based on the dataset and noise level."

### Mechanism 3
- Claim: Pretraining reduces the warm-up obstacle in existing noisy-label methods.
- Mechanism: LNL methods use a warm-up phase that trains on noisy data before filtering. Pretrained models start with better features, so the warm-up phase learns useful structure faster and less corrupted features.
- Core assumption: Better initial feature quality shortens the number of warm-up epochs needed for effective noise filtering.
- Evidence anchors: Abstract states self-supervised pretraining can "effectively learn better features and improve robustness against noisy labels."

## Foundational Learning

- **Concept**: Contrastive learning loss
  - Why needed here: SimCLR's contrastive loss drives the model to bring augmented views of the same image closer while pushing others apart, building global feature representations that transfer well.
  - Quick check question: Why do negative samples matter in contrastive loss?

- **Concept**: Self-supervised pretext tasks
  - Why needed here: Tasks like rotation prediction, jigsaw puzzle, and Jigmag puzzle provide supervision signals without labels, training feature extractors useful for classification.
  - Quick check question: How does rotation prediction encourage the model to learn rotation-invariant features?

- **Concept**: Warm-up phase in noisy label learning
  - Why needed here: LNL methods need an initial training period where the model learns to separate clean from noisy samples. Good initialization reduces corruption during this phase.
  - Quick check question: What happens if the warm-up phase starts from random weights on high noise?

## Architecture Onboarding

- **Component map**: ResNet18 backbone → self-supervised pretraining (rotation, jigsaw, jigmag, or SimCLR) → fully connected layer for classification → training with noisy labels (CE, Co-teaching, or DivideMix)
- **Critical path**: Pretrain → initialize → fine-tune on noisy labels → evaluate BEST/LAST
- **Design tradeoffs**: Using patch-based tasks (jigsaw, jigmag) may lose global context; rotation and SimCLR preserve it. Tradeoff between task simplicity and feature quality.
- **Failure signatures**: If pretraining doesn't improve over random init, or if performance drops at high noise rates, the task or hyperparameters may be mismatched.
- **First 3 experiments**:
  1. Pretrain ResNet18 on rotation prediction, fine-tune on noisy labels with CE, compare to random init baseline.
  2. Pretrain with SimCLR, fine-tune with Co-teaching on noisy labels, compare BEST/LAST to baseline.
  3. Pretrain with jigsaw puzzle, fine-tune with DivideMix, evaluate on high noise rates and analyze feature filter quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of self-supervised pretraining methods vary across different medical imaging modalities (e.g., CT, MRI, ultrasound) beyond the chest X-ray and histopathology datasets studied?
- Basis in paper: The study is limited to two specific medical imaging modalities, leaving uncertainty about generalizability to other types of medical images.
- Why unresolved: The study only tests on two specific medical datasets, acknowledging the need to investigate additional self-supervised baselines and how dataset nature/size influences improvements.
- What evidence would resolve it: Conducting experiments on additional medical imaging modalities with self-supervised pretraining and measuring performance against label noise would demonstrate generalizability.

### Open Question 2
- Question: What is the optimal combination of self-supervised pretraining task and LNL method for different noise rates and medical datasets?
- Basis in paper: The paper finds that "the choice of the best self-supervised task varies based on the dataset, noise rate, and evaluation criteria" and that different methods perform better at different noise levels.
- Why unresolved: The paper identifies that different combinations work better in different scenarios but doesn't provide a systematic framework for selecting the optimal combination for a given dataset and noise level.
- What evidence would resolve it: A comprehensive study mapping different self-supervised pretraining tasks to optimal LNL methods across various noise rates and medical datasets would establish clear guidelines for method selection.

### Open Question 3
- Question: How do transformer-based architectures with self-supervised pretraining compare to CNN-based approaches for medical image classification with noisy labels?
- Basis in paper: The paper states "In this work, we have limited the investigation to CNN-based architecture. It would be interesting to investigate how recent transformer-based architectures behave under various levels of label noise."
- Why unresolved: The study exclusively uses CNN architectures, while transformer-based approaches are becoming increasingly popular in medical imaging and may have different properties when dealing with label noise.
- What evidence would resolve it: Direct comparison studies using transformer architectures (ViT, Swin Transformer, etc.) with self-supervised pretraining against CNN approaches on the same medical image datasets with noisy labels would reveal relative performance characteristics.

## Limitations
- The study lacks detailed implementation specifics for the Jigmag puzzle method and exact data augmentation strategies used.
- Dataset availability is unclear, potentially limiting reproducibility of the exact experiments.
- The analysis focuses primarily on ResNet18, leaving questions about scalability to larger architectures.
- While results show improvement at high noise rates, performance degradation at lower noise rates suggests pretraining may not always be beneficial.

## Confidence

- **High confidence**: The general finding that self-supervised pretraining improves robustness against label noise in medical image classification is well-supported by experimental results across both datasets and multiple LNL methods.
- **Medium confidence**: The claim that different self-supervised tasks perform better on different datasets is supported but would benefit from more systematic ablation studies across additional datasets.
- **Medium confidence**: The mechanism explaining how pretraining reduces the warm-up obstacle in noisy label learning is plausible but lacks direct experimental validation beyond performance improvements.

## Next Checks

1. Conduct systematic ablation studies varying pretraining task complexity and duration to determine optimal pretraining strategies for different noise levels.
2. Test scalability by implementing the same methodology with larger architectures (ResNet50, EfficientNet) to verify if benefits transfer to more complex models.
3. Perform feature visualization analysis comparing first-layer filters learned through self-supervised pretraining versus random initialization to provide visual evidence of improved feature quality.