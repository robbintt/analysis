---
ver: rpa2
title: Diffusion Models With Learned Adaptive Noise
arxiv_id: '2312.13236'
source_url: https://arxiv.org/abs/2312.13236
tags:
- noise
- diffusion
- process
- schedule
- mulan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a learned adaptive noise process for diffusion
  models that achieves state-of-the-art likelihood estimation on CIFAR-10 and ImageNet.
  The key insight is that multivariate noise schedules, when conditioned on the input
  data, yield a tighter variational lower bound (ELBO) than traditional univariate
  schedules.
---

# Diffusion Models With Learned Adaptive Noise

## Quick Facts
- arXiv ID: 2312.13236
- Source URL: https://arxiv.org/abs/2312.13236
- Authors: 
- Reference count: 40
- One-line primary result: Introduces MULAN, a learned multivariate noise schedule for diffusion models achieving state-of-the-art likelihood on CIFAR-10 (2.55 BPD) and ImageNet-32 (3.67 BPD).

## Executive Summary
This paper introduces MULAN (Multivariate Learned Adaptive Noise), a diffusion model that learns a multivariate noise schedule conditioned on input data characteristics. By applying different noise rates across an image through an auxiliary latent variable, MULAN achieves tighter variational bounds and significantly improves likelihood estimation compared to classical diffusion models. The method demonstrates state-of-the-art density estimation on CIFAR-10 and ImageNet-32 while reducing training time by 50%.

## Method Summary
MULAN introduces a multivariate noise schedule γϕ(z,t) that varies across image dimensions and is conditioned on an auxiliary latent variable z. The model encodes input x0 to obtain z, which then conditions both the noise schedule and denoising network. A polynomial parameterization ensures monotonicity while allowing flexible noise trajectories. During training, the model optimizes a modified ELBO that accounts for the learned noise schedule, and at generation time, samples are produced by first sampling z from its prior and then running reverse diffusion with the conditioned noise schedule.

## Key Results
- Achieves state-of-the-art likelihood on CIFAR-10: 2.55 bits per dimension
- Achieves state-of-the-art likelihood on ImageNet-32: 3.67 bits per dimension
- Outperforms classical diffusion models with 50% reduction in training time
- Demonstrates significant improvement over VDM and DDPM baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ELBO is not invariant to the choice of diffusion process when using multivariate noise schedules.
- Mechanism: Classical diffusion models use univariate noise schedules where the ELBO is path-independent. MULAN's multivariate schedules create path-dependent integrals, enabling tighter bounds.
- Core assumption: The noise schedule ν(t) is multivariate and varies across different dimensions/pixels.
- Evidence anchors:
  - [abstract]: "we show that the ELBO is not invariant to more complex forward processes"
  - [section 3.2.1]: "Intuitively, a multivariate noise schedule injects noise at different rates for each pixel of an input image."
  - [section 3.5]: "We can then write the integral in (12) as a line integral... Thus the diffusion loss, Ldiffusion, can be interpreted as a measure of work done along the trajectory ν(z, t)"
  - [corpus]: Weak evidence - no direct citations found supporting this thermodynamic interpretation.

### Mechanism 2
- Claim: Conditioning the noise schedule on context (via auxiliary latent variables) improves likelihood estimation.
- Mechanism: By introducing auxiliary latent variables z, the noise schedule becomes a function of both time and context, allowing adaptation to high-level semantic features.
- Core assumption: The auxiliary latent variable z captures meaningful semantic information about the input data.
- Evidence anchors:
  - [abstract]: "MULAN, a learned diffusion process that applies Gaussian noise at different rates across an image. Specifically, our method relies on a multivariate noise schedule that is a function of the data"
  - [section 3.3.2]: "We define z ∈ Rm as a low-dimensional auxiliary latent variable... They can be continuous or discrete."
  - [section 3.4.1]: "log pθ(x0) ≥ Eqϕ(z|x0)[ELBO(pθ(x0:1|z), qϕ(x0:1|z))] − DKL(qϕ(z|x0)∥pθ(z))"
  - [corpus]: Weak evidence - no direct citations found supporting this specific mechanism.

### Mechanism 3
- Claim: Polynomial parameterization of the noise schedule outperforms monotonic neural networks and sigmoid functions.
- Mechanism: The polynomial form γϕ(c, t) = γmin + (γmax − γmin) fϕ(c, t)/fϕ(c, t = 1) enables flexible yet monotonic schedules with zero derivatives at endpoints.
- Core assumption: The polynomial form can capture the optimal noise schedule trajectory better than alternative parameterizations.
- Evidence anchors:
  - [section 3.2.2]: "Specifically, we parameterize the diffusion parameters αt, σt, ν(t) as... γϕ(c, t) : Rm × [0, 1] → [γmin, γmax]d is a neural network with the property that γϕ(c, t) is monotonic in t."
  - [section 3.2.2]: "We explore various parameterizations for γϕ(c, t)... The polynomial parameterization is novel to our work and yields significant performance gains."
  - [section 4.2]: "Among polynomial, sigmoid, and monotonic neural network, we find that the polynomial parameterization yields the best performance."
  - [corpus]: Weak evidence - no direct citations found supporting this specific claim.

## Foundational Learning

- Concept: Variational Inference and ELBO
  - Why needed here: The paper frames diffusion models as approximate variational posteriors, where the ELBO serves as the learning objective.
  - Quick check question: What is the gap between the ELBO and the marginal log-likelihood, and how does this relate to the choice of diffusion process?

- Concept: Stochastic Differential Equations (SDEs) and Probability Flow ODEs
  - Why needed here: The paper connects diffusion models to SDEs and their equivalent ODEs (probability flow), which enables exact likelihood computation.
  - Quick check question: How does the probability flow ODE enable exact likelihood computation, and what role does the score function play in this formulation?

- Concept: Monotonic Neural Networks and Parameterization Techniques
  - Why needed here: The paper uses monotonic neural networks and various parameterizations for the noise schedule.
  - Quick check question: What properties must a neural network parameterization satisfy to ensure the noise schedule remains monotonic in time?

## Architecture Onboarding

- Component map:
  - Encoder network (qϕ(z|x0)) -> Noise schedule network (γϕ) -> Denoising network (θ) -> Integration with VDM codebase

- Critical path:
  1. Encode input x0 to get latent distribution qϕ(z|x0)
  2. Sample z from qϕ(z|x0)
  3. Use z to condition noise schedule γϕ(z, t)
  4. Apply noise schedule during forward diffusion
  5. Train denoising network using modified ELBO
  6. Generate samples by sampling z from pθ(z) and running reverse diffusion

- Design tradeoffs:
  - Auxiliary latent dimension (m) vs. expressiveness vs. computational cost
  - Polynomial degree vs. flexibility vs. risk of non-monotonicity
  - Continuous vs. discrete latent space for z
  - Training with vs. without importance sampling for likelihood estimation

- Failure signatures:
  - NaNs during training (often due to improper noise schedule parameterization)
  - Degraded sample quality (indicates issues with auxiliary latent space or noise schedule conditioning)
  - Poor likelihood estimates (suggests problems with the ELBO computation or ODE solver)

- First 3 experiments:
  1. Implement polynomial noise schedule parameterization and verify monotonicity
  2. Add auxiliary latent space conditioning and test likelihood improvement on CIFAR-10
  3. Compare different noise schedule parameterizations (polynomial vs. sigmoid vs. monotonic NN) on a subset of CIFAR-10

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the polynomial noise schedule parameterization consistently outperform other parameterizations (sigmoid, monotonic neural network) across different datasets and model architectures?
- Basis in paper: [explicit] The paper states that among polynomial, sigmoid, and monotonic neural network parameterizations, the polynomial parameterization yields the best performance on CIFAR-10.
- Why unresolved: The ablation study only compares these parameterizations on CIFAR-10 with a reduced batch size and fewer training steps. Performance may vary with different datasets, architectures, or training regimes.
- What evidence would resolve it: Systematic experiments comparing all three parameterizations across multiple datasets (e.g., CIFAR-10, ImageNet, LSUN), model architectures (different UNet variants), and training configurations.

### Open Question 2
- Question: Are there interpretable patterns in the learned multivariate noise schedules across different image characteristics (e.g., frequency content, intensity, spatial structure)?
- Basis in paper: [inferred] The paper mentions attempting to visualize noise schedules across different dataset images and areas of the same image but found no human-interpretable patterns.
- Why unresolved: The visualizations only showed basic statistics (variance across samples) and did not reveal clear patterns. The authors hypothesize that other architectures or conditioning methods might reveal interpretable variations.
- What evidence would resolve it: Visualization techniques that reveal correlations between noise schedule variations and specific image features (e.g., high/low frequency regions, edges, textures), potentially using different conditioning approaches or architectural modifications.

### Open Question 3
- Question: How does MULAN perform when combined with other diffusion model improvements like importance-sampled training, variance minimization, or higher-order score matching?
- Basis in paper: [explicit] The paper notes that MULAN did not incorporate techniques used in i-DODE (Zheng et al., 2023) such as importance sampling, variance minimization, and higher-order score matching, which could further improve performance.
- Why unresolved: The evaluation of MULAN was done without these additional techniques, leaving open the question of whether combining them would yield further improvements.
- What evidence would resolve it: Experiments combining MULAN with these advanced training techniques and comparing the resulting performance to both the current MULAN implementation and the state-of-the-art methods that already use these techniques.

## Limitations

- The thermodynamic interpretation of ELBO as path-dependent work remains largely theoretical with weak empirical validation
- Performance gains are demonstrated primarily on CIFAR-10 and ImageNet-32, with limited testing on diverse data distributions
- The method requires careful parameterization to maintain monotonicity, adding complexity to implementation

## Confidence

- High: The mathematical formulation of the variational objective and the polynomial parameterization technique are well-specified and reproducible.
- Medium: The empirical performance gains over baselines (VDM, DDPM) are convincingly demonstrated, though the ablation studies could be more comprehensive.
- Low: The theoretical mechanism explaining why multivariate schedules yield better likelihoods (path-dependent work interpretation) lacks direct experimental validation.

## Next Checks

1. **Ablation of dimensionality**: Test MULAN with varying numbers of noise schedule dimensions (d) to quantify the relationship between schedule complexity and likelihood improvement.

2. **Synthetic data validation**: Apply MULAN to synthetic datasets with known spatial correlations to verify that the model learns to inject noise at rates that reflect the data's inherent structure.

3. **Comparison to learned SDE parameterizations**: Benchmark MULAN against continuous-time diffusion models that learn the SDE directly, isolating the benefit of multivariate vs. univariate schedules from the conditioning mechanism.