---
ver: rpa2
title: Multimodal Classification of Teaching Activities from University Lecture Recordings
arxiv_id: '2312.17262'
source_url: https://arxiv.org/abs/2312.17262
tags:
- audio
- transcription
- text
- recordings
- activities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multimodal classification algorithm to identify
  teaching activities in university lecture recordings. The approach leverages both
  audio signals and automated transcriptions to classify segments into activities
  like theory, examples, organization, interaction, and exercise.
---

# Multimodal Classification of Teaching Activities from University Lecture Recordings

## Quick Facts
- arXiv ID: 2312.17262
- Source URL: https://arxiv.org/abs/2312.17262
- Reference count: 40
- One-line primary result: Multimodal model combining audio and text achieves F-scores from 0.024 to 0.875 for classifying teaching activities in lecture recordings

## Executive Summary
This paper proposes a multimodal classification algorithm to identify teaching activities in university lecture recordings. The approach leverages both audio signals and automated transcriptions to classify segments into activities like theory, examples, organization, interaction, and exercise. A transformer-based language model (XLM-RoBERTa) is used for text embeddings, while Wav2Vec 2 provides audio embeddings. These features are processed by bidirectional LSTM layers and fed to a classifier. The model achieves F-scores ranging from 0.024 to 0.875 across different activity classes, with better performance for audio-based activities like miscellaneous sounds and interaction.

## Method Summary
The method processes one-second frames of lecture recordings, extracting audio embeddings using Wav2Vec 2 and text embeddings using XLM-RoBERTa from automated transcriptions. These features are processed by separate bidirectional LSTM layers before being concatenated and classified using a fully connected layer with Softmax activation. The model is trained with the Adam optimizer and one-cycle learning rate policy, using class weights to handle dataset imbalance. Evaluation metrics include precision, recall, and F-score for ten activity categories.

## Key Results
- Model achieves F-scores ranging from 0.024 (Digression) to 0.875 (Miscellaneous) across different activity classes
- Audio features perform better for activities like Miscellaneous sounds and Interaction (F-scores 0.791-0.875)
- Text features are more important for activities like Organization and Digression, though performance remains limited
- Joint audio-text embeddings show complementary benefits for activities requiring both modalities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Audio-based features capture speaker engagement patterns that distinguish interaction-heavy activities like exercises and Q&A from lecture-style activities.
- Mechanism: Raw audio waveforms and intensity variations in segments containing teacher-student exchanges (e.g., short silences interleaved with speech) are different from those in extended monologues, enabling discrimination via audio embeddings.
- Core assumption: Pauses and speech cadence in interactive segments are reliably distinguishable from non-interactive segments in recorded lectures.
- Evidence anchors:
  - The paper contrasts audio samples: Interaction segments show short silences interleaved with speech, while Theory/Concept segments have longer continuous speech (Figures 5 vs 7, Tables 4 and 6).
  - MÂ³AV dataset similarly combines audio and visual cues to classify lecture segments, suggesting multimodal analysis of audio-visual features aids classification.
- Break condition: If pauses are consistently too short or the lapel microphone fails to capture student responses, the audio cue becomes unreliable.

### Mechanism 2
- Claim: Text-based features capture semantic differences between teaching activities (e.g., theory explanation vs. exercise solving) through specialized vocabulary and structure.
- Mechanism: XLM-RoBERTa embeddings represent text frames such that words with similar academic meanings or contexts (e.g., formulae vs. administrative language) are clustered, allowing the classifier to differentiate activities.
- Core assumption: Automated transcriptions, despite noise, preserve enough lexical and syntactic cues to differentiate academic activities.
- Evidence anchors:
  - The model uses transformer-based language models to exploit features from automated lecture transcriptions for classification.
  - The paper notes that classes like "Organization" have distinctive vocabularies (dates, grading system) distinguishable from "Theory" or "Digression."
- Break condition: If automated transcriptions introduce too many errors or lack context markers (punctuation), the semantic distinctions may be lost.

### Mechanism 3
- Claim: Joint audio-text embeddings provide complementary signals, improving classification for activities that are ambiguous when using only one modality.
- Mechanism: BiLSTM layers process concatenated audio and text embeddings per frame, allowing the model to learn activity-specific patterns that rely on both modalities (e.g., "Exercise" has both specific vocabulary and interactive audio cues).
- Core assumption: Some teaching activities are inherently multimodal and require both audio and text features for accurate classification.
- Evidence anchors:
  - The model combines audio and text features using BiLSTM layers to classify teaching activities.
  - "Exercise" is categorized under both Audio and Transcription because it involves student engagement (audible) and specific vocabulary (text).
- Break condition: If one modality dominates (e.g., audio features are much stronger), the model may ignore the weaker modality, reducing benefits of fusion.

## Foundational Learning

- Concept: Transformer-based language models (e.g., BERT, XLM-RoBERTa)
  - Why needed here: They provide rich, context-aware text embeddings from automated transcriptions, capturing semantic meaning despite transcription noise.
  - Quick check question: What is the role of the [CLS] token in transformer-based models for classification tasks?

- Concept: Audio feature extraction with Wav2Vec 2
  - Why needed here: It converts raw audio into high-level speech representations that capture speaker engagement and activity patterns.
  - Quick check question: How does Wav2Vec 2 differ from traditional MFCC-based audio features?

- Concept: Bidirectional LSTM (BiLSTM) networks
  - Why needed here: They process sequential data (frames) in both forward and backward directions, capturing context from surrounding frames for better activity classification.
  - Quick check question: Why might a BiLSTM be preferred over a unidirectional LSTM for this classification task?

## Architecture Onboarding

- Component map: Raw audio frames -> Wav2Vec 2 -> (49, 512) embeddings; Text frames -> XLM-RoBERTa -> (1024,) embeddings; Audio embeddings -> BiLSTM -> Text embeddings -> BiLSTM -> Concatenation -> Classifier (2048 units, Gelu) -> Output (Softmax)

- Critical path: Feature extraction -> frame concatenation -> BiLSTM processing -> classification

- Design tradeoffs:
  - Audio vs. text emphasis: Model currently favors audio for certain classes; balancing attention may improve text-heavy activity detection.
  - Frame size: 1-second frames balance granularity and computational cost; shorter frames may improve precision but increase complexity.

- Failure signatures:
  - Poor performance on text-heavy classes (e.g., "Digression," "Other") suggests insufficient text feature utilization.
  - High misclassification of "Pause" as "Miscellaneous" indicates audio similarity issues.

- First 3 experiments:
  1. Test classifier with only audio features to quantify audio-only performance vs. multimodal.
  2. Test classifier with only text features to quantify text-only performance vs. multimodal.
  3. Vary frame size (e.g., 0.5s, 2s) to assess impact on classification accuracy and computational cost.

## Open Questions the Paper Calls Out

1. How would the accuracy of the classification model change if the dataset included student voices captured by multiple microphones instead of only the lecturer's voice?
2. Would fine-tuning the XLM-RoBERTa model on the automated transcriptions of the lecture repository improve the classification accuracy?
3. How would the model's performance be affected if the transcripts were manually corrected instead of using automated transcriptions?

## Limitations

- The study relies on automated transcriptions which may contain errors that affect classification accuracy, particularly for text-heavy activities
- The dataset consists of recordings from only 10 professors, limiting generalizability across different teaching styles and accents
- The paper does not specify how the dataset was split into training and evaluation sets, making it difficult to assess result robustness

## Confidence

- High Confidence: Audio features are more informative for activities like "Miscellaneous" and "Interaction" (F-scores 0.791-0.875)
- Medium Confidence: Text features are crucial for activities like "Organization," though performance remains suboptimal (F-scores 0.024-0.359)
- Low Confidence: Joint audio-text embeddings significantly improve classification for ambiguous activities lacks strong empirical support

## Next Checks

1. Conduct systematic ablation studies comparing model performance using only audio features, only text features, and multimodal features to quantify the actual contribution of each modality.

2. Test the model on lecture recordings from additional professors with different teaching styles, accents, and subject matters to evaluate generalizability and identify potential biases.

3. Systematically evaluate how transcription errors affect classification performance by introducing controlled noise or errors into the transcriptions and measuring degradation in accuracy across different activity categories.