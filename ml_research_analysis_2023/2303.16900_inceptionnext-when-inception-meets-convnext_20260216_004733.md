---
ver: rpa2
title: 'InceptionNeXt: When Inception Meets ConvNeXt'
arxiv_id: '2303.16900'
source_url: https://arxiv.org/abs/2303.16900
tags:
- kernel
- size
- convolution
- vision
- depthwise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of improving the efficiency
  of large-kernel depthwise convolution operations in CNN models without sacrificing
  performance. The authors propose a new method called Inception depthwise convolution,
  which decomposes large kernels into four parallel branches: small square kernel,
  two orthogonal band kernels, and an identity mapping.'
---

# InceptionNeXt: When Inception Meets ConvNeXt

## Quick Facts
- **arXiv ID**: 2303.16900
- **Source URL**: https://arxiv.org/abs/2303.16900
- **Reference count**: 40
- **Primary result**: InceptionNeXt achieves 1.6x training speedup over ConvNeXt-T with 0.2% higher top-1 accuracy on ImageNet-1K

## Executive Summary
InceptionNeXt addresses the computational inefficiency of large-kernel depthwise convolutions by decomposing them into four parallel branches: small square kernels, orthogonal band kernels, and identity mappings. This Inception-style decomposition maintains the large receptive field while significantly reducing memory access costs and computational complexity. The resulting architecture achieves substantial training throughput improvements without sacrificing accuracy, demonstrating effectiveness across image classification and semantic segmentation tasks.

## Method Summary
The paper proposes decomposing large-kernel depthwise convolutions into four parallel branches: 3x3 square kernels, 1x11 horizontal band kernels, 11x1 vertical band kernels, and an identity branch. This decomposition is integrated into the ConvNeXt architecture, with a convolution branch ratio parameter controlling the proportion of channels processed by convolution operations. The method is trained using standard techniques on ImageNet-1K with 300 epochs, AdamW optimizer, and large batch sizes.

## Key Results
- InceptionNeXt-T achieves 1.6x training throughput improvement over ConvNeXt-T
- 0.2% top-1 accuracy improvement on ImageNet-1K (82.3% vs 82.1%)
- Strong performance on semantic segmentation (55.4% mIoU on ADE20K)
- Computational efficiency gains through reduced FLOPs in depthwise convolutions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing large kernels into parallel branches reduces memory access costs while maintaining receptive field size
- Mechanism: Large-kernel depthwise convolution requires accessing distant memory locations, creating high memory access costs. By splitting into small square kernels (3x3) and orthogonal band kernels (1xk, kx1), memory accesses become localized and cache-friendly. The identity branch preserves information without computation.
- Core assumption: Parallel branch execution on modern GPUs is more efficient than sequential large-kernel computation due to better memory locality and cache utilization
- Evidence anchors: [abstract] "large-kernel convolutions are widely studied and adopted recently to enlarge the receptive field" and "high memory access costs" from depthwise convolution; [section 3.2] "we propose to decompose large kernel of depthwise convolution into several groups of small kernels with Inception style"

### Mechanism 2
- Claim: The 1x11 and 11x1 band kernels effectively capture long-range spatial dependencies similar to 7x7 kernel but with reduced computational cost
- Mechanism: The horizontal and vertical band kernels span the full width or height of the feature map while only covering one pixel in the orthogonal dimension. When combined, they cover the same area as a 7x7 kernel but through two separate passes, reducing the number of multiplications needed.
- Core assumption: Spatial information can be effectively captured through separate horizontal and vertical processing rather than joint spatial processing
- Evidence anchors: [section 3.2] "large kernel kh × kw is decomposed as 1 × kw and kh × 1 inspired by Inception v3"; [section 4.3] "When removing any branch of horizontal or vertical band kernel, performance significantly drops from 82.3% to 81.9%"

### Mechanism 3
- Claim: Processing only a subset of channels with convolution while leaving others unchanged maintains performance while reducing computation
- Mechanism: By setting a convolution branch ratio (default 1/8), only 1/8 of channels undergo the expensive convolution operations while 7/8 pass through identity mapping. This reduces FLOPs by approximately 87.5% for the depthwise convolution component.
- Core assumption: Not all channels in a feature map require spatial mixing at each layer, allowing some to bypass computation
- Evidence anchors: [abstract] "we propose to leave some channels unaltered and process only a portion of the channels with the depthwise convolution operation"; [section 4.3] "When the ratio increases from 1/8 to 1/4, performance improvement can not be observed"

## Foundational Learning

- **Concept**: Memory access patterns and cache hierarchy in GPU architectures
  - Why needed here: The efficiency gains come from changing memory access patterns from random (large kernels) to localized (small kernels), which is only beneficial if GPU cache hierarchy can exploit this
  - Quick check question: Why would 4 separate 3x3 convolutions potentially be faster than one 7x7 convolution on a GPU?

- **Concept**: Spatial receptive field and information propagation in convolutional networks
  - Why needed here: Understanding how spatial information flows through networks helps explain why band kernels (1xk, kx1) can replace square kernels while maintaining performance
  - Quick check question: How does a 1x11 kernel followed by an 11x1 kernel compare to a 11x11 kernel in terms of receptive field coverage?

- **Concept**: Channel-wise feature representation and redundancy in deep networks
  - Why needed here: The convolution branch ratio relies on understanding that not all channels need spatial mixing at every layer, which requires knowledge of feature redundancy
  - Quick check question: What evidence would suggest that some channels in a feature map are redundant for spatial information propagation?

## Architecture Onboarding

- **Component map**: Channel splitting -> parallel convolution operations (3x3, 1x11, 11x1, identity) -> concatenation -> normalization -> MLP -> skip connection

- **Critical path**: Channel splitting → parallel convolution operations → concatenation → normalization → MLP → skip connection

- **Design tradeoffs**:
  - Speed vs accuracy: Smaller kernel sizes increase speed but may reduce accuracy; the Inception decomposition balances this
  - Memory vs computation: More branches increase memory usage but reduce computation; ratio parameter controls this balance
  - Flexibility vs simplicity: More complex decompositions could be more efficient but harder to implement and optimize

- **Failure signatures**:
  - Accuracy drop without throughput improvement: Indicates poor parallel execution on target hardware
  - Training instability: Suggests the decomposed receptive field is insufficient for learning spatial relationships
  - Memory overflow: Happens if branch ratio is set too high, creating too many parallel operations

- **First 3 experiments**:
  1. Replace depthwise convolution with identity mapping only to establish baseline throughput and measure accuracy impact
  2. Implement 3x3 depthwise convolution (no bands, no identity) to measure performance vs standard 7x7
  3. Test different convolution branch ratios (1/4, 1/8, 1/16) to find optimal speed-accuracy tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of InceptionNeXt compare to other modern CNN architectures (e.g., ConvNeXt, RepLKNet) on large-scale datasets like JFT-300M?
- Basis in paper: [explicit] The paper focuses on ImageNet-1K and ADE20K, but doesn't evaluate on larger datasets
- Why unresolved: The paper only evaluates on relatively small datasets, so it's unclear if the performance gains would scale to larger, more diverse datasets
- What evidence would resolve it: Benchmarking InceptionNeXt on larger datasets like JFT-300M or Instagram-1B would show if the architecture's advantages generalize

### Open Question 2
- Question: Can the Inception depthwise convolution be further optimized for inference speed on different hardware (e.g., CPUs, mobile devices)?
- Basis in paper: [inferred] The paper mentions that the speed-up ratio is smaller during inference compared to training, suggesting potential for further optimization
- Why unresolved: The paper only provides inference speed results for A100 GPUs, not for other hardware
- What evidence would resolve it: Benchmarking InceptionNeXt on various hardware platforms (CPUs, mobile SoCs, TPUs) would reveal if the architecture's efficiency translates across different devices

### Open Question 3
- Question: How does the InceptionNeXt architecture perform on video recognition tasks, where temporal modeling is crucial?
- Basis in paper: [explicit] The paper focuses on image classification and semantic segmentation, with no mention of video tasks
- Why unresolved: The paper doesn't explore the architecture's capabilities beyond static images
- What evidence would resolve it: Evaluating InceptionNeXt on video datasets like Kinetics or Something-Something would demonstrate its effectiveness for temporal modeling

### Open Question 4
- Question: What is the optimal branch ratio for the Inception depthwise convolution across different model sizes and tasks?
- Basis in paper: [explicit] The paper sets the convolution branch ratio to 1/8 by default but mentions that different ratios affect performance
- Why unresolved: The paper only explores a limited range of branch ratios and doesn't provide a systematic study across different architectures
- What evidence would resolve it: Conducting a comprehensive ablation study across various model sizes (small, base, large) and tasks (classification, detection, segmentation) would identify optimal branch ratios for each scenario

## Limitations
- The computational efficiency claims lack extensive validation across different hardware configurations
- The paper doesn't provide statistical significance testing for the 1.6x training speedup claim
- Limited evaluation scope restricts understanding of generalization to other architectures and tasks

## Confidence
- **High confidence**: The computational complexity analysis showing FLOPs reduction through kernel decomposition is mathematically sound and well-supported by the equations provided
- **Medium confidence**: The ImageNet-1K accuracy improvements (0.2%) are reproducible based on the training procedure description, though statistical significance isn't established
- **Low confidence**: The claimed 1.6x training throughput improvement lacks sufficient validation across different hardware configurations and batch sizes

## Next Checks
1. **Statistical validation**: Perform statistical tests comparing ConvNeXt and InceptionNeXt across multiple training runs to establish confidence intervals for both accuracy and throughput metrics
2. **Hardware scalability testing**: Evaluate the 1.6x speedup claim across different GPU architectures (V100, A100, H100) and batch sizes to verify the parallel execution efficiency assumption
3. **Architecture generalization study**: Implement Inception depthwise convolution in alternative architectures (ResNet, EfficientNet) to test whether the performance improvements generalize beyond ConvNeXt