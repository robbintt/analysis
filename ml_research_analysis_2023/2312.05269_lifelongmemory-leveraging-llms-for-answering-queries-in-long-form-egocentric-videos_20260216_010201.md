---
ver: rpa2
title: 'LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric
  Videos'
arxiv_id: '2312.05269'
source_url: https://arxiv.org/abs/2312.05269
tags:
- video
- captions
- arxiv
- language
- ego4d
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LifelongMemory uses pre-trained LLMs and captioning models to answer
  natural language queries in long-form egocentric videos. It converts videos into
  textual captions, prompts an LLM to predict coarse temporal windows, and refines
  these predictions with a pre-trained NLQ model.
---

# LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos

## Quick Facts
- **arXiv ID**: 2312.05269
- **Source URL**: https://arxiv.org/abs/2312.05269
- **Reference count**: 40
- **Key outcome**: Uses pre-trained LLMs and captioning models to answer natural language queries in long-form egocentric videos, achieving state-of-the-art performance on EgoSchema benchmark.

## Executive Summary
LifelongMemory addresses the challenge of answering natural language queries in long-form egocentric videos by converting videos into textual captions, using an LLM to predict coarse temporal windows, and refining these predictions with a pre-trained NLQ model. The approach achieves state-of-the-art performance on the EgoSchema benchmark and is competitive on the Ego4D NLQ challenge, demonstrating the effectiveness of combining multiple pre-trained multimodal LLMs for complex vision-language tasks.

## Method Summary
The method processes long-form egocentric videos through three stages: first, it converts video frames into textual captions using pre-trained models like LaViLa or LLaVA; second, it condenses these captions through a digest process and prompts an LLM (e.g., GPT-4) to predict coarse temporal windows containing answers to natural language queries; finally, it refines these predictions using a pre-trained NLQ model to obtain fine-grained temporal windows. The approach leverages the zero-shot reasoning capabilities of LLMs and the specialized training of NLQ models to achieve high performance on video query localization tasks.

## Key Results
- Achieves state-of-the-art performance on the EgoSchema benchmark for egocentric video query localization
- Competitive results on the Ego4D NLQ challenge, outperforming many specialized models
- Demonstrates that pre-trained multimodal LLMs can effectively reason about video content when provided with quality captions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using pre-trained captioning models to convert long-form egocentric videos into concise textual descriptions enables LLMs to process the content effectively.
- Mechanism: The captioning model generates detailed narrations of the video content at regular intervals, which are then condensed through a "Caption Digest" process that removes irrelevant and redundant information.
- Core assumption: Converting video to text preserves the essential information needed to answer natural language queries about the video content.
- Evidence anchors: [abstract] "We address the unique challenge by employing a pre-trained captioning model to create detailed narratives of the videos." [section] "Raw captions, however, can be rather verbose and repetitive, and consequently hinder the downstream retrieval process. We propose to create a caption digest to condense the information."

### Mechanism 2
- Claim: LLMs can effectively reason over the condensed captions to predict coarse temporal windows containing answers to natural language queries.
- Mechanism: The LLM processes the condensed captions along with the query using a carefully crafted prompt template, outputting candidate intervals, explanations, and confidence levels.
- Core assumption: The LLM's zero-shot reasoning capabilities are sufficient to understand the video context from text descriptions and localize relevant temporal segments.
- Evidence anchors: [abstract] "These narratives are then used to prompt a frozen LLM to generate coarse-grained temporal window predictions." [section] "We leverage an LLM here for its outstanding contextual understanding and reasoning skills."

### Mechanism 3
- Claim: Pre-trained NLQ models can refine the coarse LLM predictions into fine-grained temporal windows with high precision.
- Mechanism: The system uses a pre-trained NLQ model to classify and select the optimal candidate interval from multiple LLM predictions, then extends these intervals to provide context before feeding them to a fine-grained localization model.
- Core assumption: The pre-trained NLQ model can effectively distinguish between relevant and irrelevant candidate intervals and improve localization accuracy.
- Evidence anchors: [abstract] "which are subsequently refined using a pre-trained NLQ model." [section] "we employ a pretrained NLQ model and feed in candidate intervals predicted by our previous stage."

## Foundational Learning

- **Concept**: Video captioning and understanding
  - Why needed here: The entire pipeline depends on converting video content to text that captures the essential information needed for question answering.
  - Quick check question: What information might be lost when converting video to text, and how could this affect the LLM's ability to answer queries?

- **Concept**: Zero-shot learning with LLMs
  - Why needed here: The approach relies on LLMs' ability to understand and reason about video content from text descriptions without fine-tuning.
  - Quick check question: How does the LLM's performance on this task compare to models fine-tuned on video data, and what are the trade-offs?

- **Concept**: Temporal localization in videos
  - Why needed here: The final goal is to identify precise temporal windows in videos that contain answers to queries, requiring understanding of video temporal structure.
  - Quick check question: What challenges arise when trying to localize short answer windows within long videos, and how does the two-stage coarse-to-fine approach address these challenges?

## Architecture Onboarding

- **Component map**: Input video -> Captioning model (LaViLa/LLaVA) -> Raw captions -> Caption digest -> Condensed captions -> LLM reasoning -> Coarse intervals -> NLQ refinement -> Fine-grained temporal windows

- **Critical path**: 1. Generate captions from video frames/clips 2. Condense captions through filtering and merging 3. LLM processes condensed captions + query to predict intervals 4. NLQ model refines predictions to fine-grained windows

- **Design tradeoffs**: Caption density vs. computational cost: Denser captions (every 2 seconds) provide better context but increase processing time and LLM input length; LLM confidence threshold: Higher confidence thresholds improve precision but may miss some valid answers; Caption model choice: In-domain models (Ego4D narrations) perform better than general models, but may be less flexible

- **Failure signatures**: High NA rate from LLM: Indicates captions are missing critical information or LLM struggles with the reasoning task; Poor overlap between predictions and ground truth: Suggests issues with either captioning quality or LLM reasoning; Classifier low accuracy in NLQ refinement: Indicates LLM predictions are too ambiguous for reliable classification

- **First 3 experiments**: 1. Compare different captioning models (LaViLa vs. LLaVA vs. Ego4D narrations) on the same video-query pairs to identify the best caption source 2. Test different LLM confidence thresholds to find the optimal balance between precision and recall 3. Evaluate the impact of caption density (sampling intervals) on LLM performance by testing 1s, 2s, and 4s intervals

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LifelongMemory scale with video length, and is there an optimal video length for maximizing LLM-based retrieval accuracy?
- Basis in paper: [inferred] The paper discusses the challenge of capturing long-range temporal dependencies in lengthy videos, but does not provide systematic experiments varying video length.
- Why unresolved: The authors acknowledge this as a limitation but do not conduct controlled experiments to quantify the impact of video length on performance.
- What evidence would resolve it: Experiments varying video length across a wide range and measuring retrieval accuracy at each length, identifying any optimal length and explaining the performance curve.

### Open Question 2
- Question: How does the quality of video captions impact LLM-based retrieval accuracy, and what is the relative importance of caption accuracy vs. LLM reasoning capabilities?
- Basis in paper: [explicit] The authors note that captions might not include all relevant information and that the LLM can still produce high-quality responses based on imperfect captions, suggesting the importance of both caption quality and LLM reasoning.
- Why unresolved: The paper does not provide quantitative analysis of the relationship between caption quality and retrieval accuracy, nor does it isolate the contribution of caption accuracy vs. LLM reasoning.
- What evidence would resolve it: Controlled experiments manipulating caption quality (e.g., by adding noise or removing information) and measuring the impact on retrieval accuracy, as well as ablation studies isolating the contributions of caption accuracy and LLM reasoning.

### Open Question 3
- Question: How does LifelongMemory perform on egocentric videos with diverse activities and environments, and are there specific types of activities or environments where it struggles?
- Basis in paper: [inferred] The authors evaluate LifelongMemory on the Ego4D dataset, which includes a wide variety of daily life activities, but do not provide detailed analysis of performance across different activity types or environments.
- Why unresolved: The paper does not provide granular analysis of performance across different activity categories or environments within the Ego4D dataset.
- What evidence would resolve it: Detailed analysis of performance on different activity types and environments within the Ego4D dataset, identifying any specific areas where LifelongMemory excels or struggles, and providing insights into the underlying reasons.

## Limitations

- **Caption quality dependency**: The entire pipeline's success critically depends on the captioning model's ability to capture relevant visual information, with no analysis of how caption errors propagate through the system.
- **LLM zero-shot reasoning effectiveness**: The approach assumes LLMs can effectively reason over video captions without fine-tuning, but doesn't compare against models fine-tuned on video data.
- **NLQ refinement limitations**: The refinement stage relies on a pre-trained NLQ model, but the paper doesn't analyze failure cases or the impact of candidate interval quality on final performance.

## Confidence

- **High confidence**: The system's ability to achieve state-of-the-art performance on the EgoSchema benchmark and competitive results on Ego4D NLQ challenge.
- **Medium confidence**: The effectiveness of the caption digest process in improving LLM reasoning.
- **Low confidence**: Claims about the generality of the approach beyond the Ego4D dataset.

## Next Checks

1. **Caption quality ablation study**: Systematically compare the performance of different captioning models (LaViLa, LLaVA, Ego4D narrations) on the same video-query pairs to quantify the impact of caption quality on final localization accuracy.
2. **LLM fine-tuning comparison**: Fine-tune a smaller language model on video caption-query pairs and compare its performance against the zero-shot LLM approach.
3. **Failure mode analysis**: Analyze the distribution of null predictions ("NA") and poor overlap cases to identify systematic weaknesses in the approach.