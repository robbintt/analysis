---
ver: rpa2
title: UvA-MT's Participation in the WMT23 General Translation Shared Task
arxiv_id: '2310.09946'
source_url: https://arxiv.org/abs/2310.09946
tags:
- data
- translation
- training
- system
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes UvA-MT's submission to the WMT 2023 shared
  task on general machine translation for English-Hebrew directions. The key approach
  was using a single multilingual model to handle both translation directions simultaneously,
  as a minimal setting of multilingual machine translation (MMT).
---

# UvA-MT's Participation in the WMT23 General Translation Shared Task

## Quick Facts
- arXiv ID: 2310.09946
- Source URL: https://arxiv.org/abs/2310.09946
- Reference count: 7
- Primary result: First place in both English→Hebrew (35.0 BLEU) and Hebrew→English (51.0 BLEU) directions in WMT 2023 shared task constrained track

## Executive Summary
This paper describes UvA-MT's winning submission to the WMT 2023 shared task on general machine translation for English↔Hebrew directions. The system used a single multilingual transformer model with three-stage training (pretraining with synthetic data, training without synthetic data, and fine-tuning on task-specific data) and re-parameterized embedding tables to enhance word similarity across languages. The approach achieved BLEU scores of 35.0 for English→Hebrew and 51.0 for Hebrew→English, outperforming the second-place constrained submission by +10 BLEU in both directions.

## Method Summary
The approach used a multilingual transformer model trained on English↔Hebrew directions with a single model handling both translation directions. Key innovations included re-parameterized embedding tables using graph neural networks to encode bilingual word alignments, three-stage progressive training with synthetic data generation through back-translation, and a shared vocabulary enhanced for cross-lingual representation. The system leveraged all official parallel data and monolingual data provided by WMT23 organizers.

## Key Results
- Achieved 35.0 BLEU for English→Hebrew and 51.0 BLEU for Hebrew→English
- First place in both translation directions in the constrained track
- Outperformed second-place constrained submission by +10 BLEU points
- Single MMT model achieved comparable performance to bilingual models for both directions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Re-parameterized embedding tables enhance representational word similarity between English and Hebrew
- Mechanism: Graph neural networks encode bilingual word alignments into a graph structure, re-parameterizing the embedding table to make semantically equivalent words across languages have more similar vector representations
- Core assumption: Word alignments capture semantic equivalence that can improve cross-lingual representation
- Break condition: If word alignments don't capture true semantic equivalence (e.g., due to morphological differences or polysemy), re-parameterization could hurt performance

### Mechanism 2
- Claim: Three-stage training progressively improves MMT performance by narrowing data distribution
- Mechanism: Stage 1 uses synthetic data from back-translation for data augmentation, Stage 2 removes synthetic data to focus on real distribution, Stage 3 fine-tunes on task-specific data for specialization
- Core assumption: Progressive narrowing of data distribution leads to better adaptation to target domain
- Break condition: If synthetic data quality is too low or domain mismatch is severe, including it in Stage 1 could introduce noise that harms downstream performance

### Mechanism 3
- Claim: Single MMT model can achieve comparable performance to bilingual models for both directions
- Mechanism: Single model with language tags to indicate translation direction enables knowledge transfer between directions with shared parameters
- Core assumption: Parameter sharing in MMT provides sufficient capacity for both directions without degradation
- Break condition: If model capacity is insufficient for the complexity of both directions, or if languages are too dissimilar for effective parameter sharing

## Foundational Learning

- Concept: Back-translation for synthetic data generation
  - Why needed here: Hebrew monolingual data was limited (only 1M sentences), so synthetic data generation was crucial for improving translation quality
  - Quick check question: What is the purpose of adding the "2syn" language tag during back-translation in Stage 1?

- Concept: Graph neural networks for embedding re-parameterization
  - Why needed here: Standard shared vocabularies don't account for semantic similarity between words across languages; GNNs can encode alignment information to make representations more transferable
  - Quick check question: How does the hop number in the GNN layers affect the re-parameterization of the embedding table?

- Concept: Three-stage progressive training methodology
  - Why needed here: Each stage serves a different purpose - Stage 1 for data augmentation, Stage 2 for focusing on real data distribution, Stage 3 for task specialization
  - Quick check question: Why did Stage 3 (fine-tuning on task-specific data) unexpectedly decrease performance according to the results?

## Architecture Onboarding

- Component map: Data preprocessing → MMT model with re-parameterized embeddings → Stage 1 training with synthetic data → Stage 2 training without synthetic data → Stage 3 fine-tuning → post-processing for emoji tokens
- Critical path: Data preprocessing → MMT model with re-parameterized embeddings → Stage 1 training with synthetic data → Stage 2 training without synthetic data → Stage 3 fine-tuning → post-processing for emoji tokens
- Design tradeoffs: MMT efficiency vs. potential performance loss compared to bilingual models; synthetic data augmentation vs. domain mismatch; embedding re-parameterization complexity vs. potential gains
- Failure signatures: Performance degradation when synthetic data is included in later stages; unexpected drops when fine-tuning on specific directions; emoji token issues in test sets
- First 3 experiments:
  1. Verify MMT baseline performance vs bilingual baseline on sampled data (2M sentences)
  2. Test different hop numbers (1-hop vs 2-hop) for re-parameterized embeddings on the same sampled data
  3. Run Stage 1 vs Stage 2 vs combined Stage 1+2 on full dataset to confirm progressive improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the three-stage training approach generalize to other language pairs beyond English-Hebrew?
- Basis in paper: The paper describes the effectiveness of this three-stage training approach for English-Hebrew translation, but does not explore its applicability to other language pairs.
- Why unresolved: The paper only evaluates this approach on a single language pair (English-Hebrew), limiting the generalizability of the findings.
- What evidence would resolve it: Testing the three-stage training approach on multiple language pairs with varying resource levels and linguistic properties would provide evidence for its generalizability.

### Open Question 2
- Question: Why does fine-tuning on task-specific data lead to a performance drop in the three-stage training process?
- Basis in paper: The paper observes a performance drop when applying stage 3 (fine-tuning on task-specific data) to the second-stage system, but does not provide a detailed explanation for this phenomenon.
- Why unresolved: The paper does not explore the underlying reasons for the performance drop during fine-tuning on task-specific data.
- What evidence would resolve it: Investigating the impact of fine-tuning data size, domain mismatch, and the interaction between multilingual and bilingual training could help explain the performance drop.

### Open Question 3
- Question: How does the re-parameterized embedding table approach perform when scaling up to extremely large-scale multilingual models with hundreds of languages?
- Basis in paper: The paper demonstrates the effectiveness of the re-parameterized embedding table approach for English-Hebrew translation, but does not explore its performance in extremely large-scale multilingual settings.
- Why unresolved: The paper only evaluates the approach on a single language pair and does not address its scalability to larger multilingual models.
- What evidence would resolve it: Scaling up the re-parameterized embedding table approach to models with hundreds of languages and evaluating its impact on performance and computational efficiency would provide insights into its scalability.

## Limitations

- Limited evaluation to a single language pair (English-Hebrew) restricts generalizability of findings
- Unexpected performance drop during Stage 3 fine-tuning on task-specific data lacks detailed explanation
- Implementation details of graph neural network for embedding re-parameterization are not fully specified

## Confidence

**High Confidence:** The overall methodology of using a multilingual model with back-translation and progressive training stages is well-established in the literature and results (35.0 BLEU English→Hebrew, 51.0 BLEU Hebrew→English) are clearly reported and verified through official WMT evaluation.

**Medium Confidence:** The effectiveness of the re-parameterized embedding table for enhancing word similarity between English and Hebrew, as this mechanism is described but lacks detailed implementation specifics and ablation studies.

**Low Confidence:** The exact contribution of each training stage to final performance, particularly why Stage 3 decreased performance when fine-tuning on task-specific data, and the specific implementation details of the GNN-based re-parameterization.

## Next Checks

1. **Ablation study on embedding re-parameterization:** Run the full three-stage training pipeline with and without the re-parameterized embedding table to quantify its exact contribution to the 10 BLEU point improvement over second place.

2. **Stage-by-stage performance analysis:** Conduct detailed analysis of model performance after each training stage (Stage 1, Stage 2, and Stage 3 separately) to understand the unexpected performance drop in Stage 3 and determine optimal checkpoint selection criteria for each direction.

3. **Cross-validation on synthetic data quality:** Evaluate the impact of different synthetic data generation strategies by varying the quality thresholds for back-translation outputs and measuring how this affects downstream performance.