---
ver: rpa2
title: Evaluating Gender Bias of Pre-trained Language Models in Natural Language Inference
  by Considering All Labels
arxiv_id: '2309.09697'
source_url: https://arxiv.org/abs/2309.09697
tags:
- bias
- evaluation
- tennis
- playing
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses gender bias in pre-trained language models
  (PLMs) for natural language inference (NLI) by proposing a method that considers
  all output labels. The authors create evaluation datasets in English, Japanese,
  and Chinese, categorizing them into pro-stereotypical, anti-stereotypical, and non-stereotypical
  groups.
---

# Evaluating Gender Bias of Pre-trained Language Models in Natural Language Inference by Considering All Labels

## Quick Facts
- arXiv ID: 2309.09697
- Source URL: https://arxiv.org/abs/2309.09697
- Reference count: 29
- Key outcome: A method for evaluating gender bias in PLMs for NLI by considering all three output labels (entailment, contradiction, neutral), showing improved accuracy in distinguishing biased inferences from general model errors across English, Japanese, and Chinese.

## Executive Summary
This paper addresses gender bias in pre-trained language models for natural language inference by proposing a novel evaluation method that considers all three NLI output labels rather than focusing on a single label like neutral. The authors create evaluation datasets in English, Japanese, and Chinese by extracting sentences from image caption datasets and substituting occupation and gender words. They categorize the data into pro-stereotypical, anti-stereotypical, and non-stereotypical groups, then define a bias measure based on the proportions of entailment, contradiction, and neutral predictions for each group. Meta-evaluation shows their method more accurately distinguishes biased inferences from bias-unrelated incorrect inferences compared to existing approaches.

## Method Summary
The authors propose a method that evaluates gender bias in PLMs by considering all three NLI labels (entailment, contradiction, neutral). They create evaluation datasets by extracting sentences from MSCOCO (English), YJ Captions (Japanese), and Flickr8K-CN (Chinese) image caption datasets, then substitute occupation words and gender words into sentence templates. Occupation words are categorized as female stereotypical, male stereotypical, or non-stereotypical based on pre-calculated gender and stereotype scores. The bias measure is calculated as s = (ep + ca + (1 - nn)) / 3, where ep, ca, and nn are proportions of entailment, contradiction, and neutral predictions for pro-stereotypical, anti-stereotypical, and non-stereotypical sets. Meta-evaluation is performed by creating bias-controlled training datasets with varying proportions of biased and bias-unrelated incorrect examples, then measuring rank correlation between bias rates and bias scores.

## Key Results
- The proposed method using all three NLI labels achieves higher rank correlation between bias rates and bias scores compared to baseline methods
- Results are consistent across English, Japanese, and Chinese, demonstrating cross-linguistic applicability
- The method successfully distinguishes biased inferences from bias-unrelated incorrect inferences, addressing a key limitation of existing approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using all three NLI labels allows distinguishing biased inferences from bias-unrelated incorrect inferences.
- Mechanism: The method categorizes evaluation data into pro-stereotypical, anti-stereotypical, and non-stereotypical groups. By calculating the proportion of each label in each group, it identifies which predictions are due to bias versus general model errors.
- Core assumption: Different types of biased inferences are associated with different output labels.
- Evidence anchors: Abstract states "we propose an evaluation method that considers all labels in the NLI task"; section 2.2 explains biased models predict entailment in pro-stereotypical sets, contradiction in anti-stereotypical sets, and non-neutral labels in non-stereotypical sets.
- Break condition: If spurious correlations between labels and gender stereotypes don't reflect true bias, or if NLI task itself has inherent label biases.

### Mechanism 2
- Claim: Meta-evaluation by controlling bias rates in training data provides ground truth for measuring bias evaluation accuracy.
- Mechanism: Authors create bias-controlled training datasets with varying proportions of biased examples (bias rate r from 0 to 1). Models trained on these datasets have corresponding levels of bias. Correlation between actual bias rates and measured bias scores indicates accuracy of bias measure.
- Core assumption: Varying proportion of biased examples in training data creates models with proportionally varying degrees of bias.
- Evidence anchors: Section 3 describes extending meta-evaluation method to evaluate NLI bias measures; section 4.2.2 details varying bias rate r to r = 0.0, 0.1, ..., 0.9, 1.0.
- Break condition: If biased examples in training data don't translate proportionally to model bias, or if other factors affect bias independently of training data bias rates.

### Mechanism 3
- Claim: Using human-written sentences from image captions rather than templates reduces evaluation artifacts and better reflects natural language usage.
- Mechanism: Instead of creating artificial sentence pairs using templates, authors extract sentences from MSCOCO, YJ Captions, and Flickr8K-CN datasets, then substitute occupation and gender words while keeping original context.
- Core assumption: Template-based evaluation is problematic because it utilizes artificial sentences that don't reflect natural usage and distribution of tokens.
- Evidence anchors: Section 2.3 states "Template-based evaluation is problematic because it utilizes an artificial sentence that does not reflect the natural usage and distribution of tokens"; section 4.2.2 describes using MSCOCO, YJ Captions, and Flickr8K-CN as image caption datasets.
- Break condition: If image caption datasets themselves contain gender biases or if substitution process introduces artifacts not present in original templates.

## Foundational Learning

- Concept: Natural Language Inference (NLI) task structure and label meanings
  - Why needed here: The paper's entire methodology is built around understanding how NLI models make predictions across entailment, contradiction, and neutral labels
  - Quick check question: What distinguishes entailment from contradiction in NLI, and why would gender bias manifest differently across these labels?

- Concept: Bias measurement and evaluation methodology
  - Why needed here: The paper introduces novel methods for creating evaluation datasets and defining bias scores, requiring understanding of how bias is quantified in NLP
  - Quick check question: How does the proposed bias score (ep + ca + (1 - nn)/3) capture gender bias differently from simply measuring the proportion of neutral predictions?

- Concept: Meta-evaluation techniques in machine learning
  - Why needed here: The paper uses meta-evaluation to validate their bias measure by creating ground truth through controlled training data
  - Quick check question: Why is it important to vary both bias rates and bias-unrelated incorrect examples when creating bias-controlled datasets for meta-evaluation?

## Architecture Onboarding

- Component map: Data creation pipeline (extracts sentences from image captions → substitutes occupation/gender words → categorizes into PS/AS/NS sets) → Bias measure calculator (computes proportions of entailment, contradiction, neutral for each set → calculates bias score) → Meta-evaluation system (creates bias-controlled training data → trains models → measures bias → correlates with ground truth) → Multi-language adapter (translates occupation words and applies same methodology across English, Japanese, Chinese)

- Critical path: Data creation → Bias score calculation → Meta-evaluation validation → Cross-language application

- Design tradeoffs: Human-written sentences vs. templates (naturalness vs. control), single label vs. all labels (simplicity vs. accuracy), intrinsic vs. extrinsic bias measures (independence vs. task-specificity)

- Failure signatures:
  - Low correlation between bias rates and scores indicates poor bias measure accuracy
  - High bias scores in non-stereotypical sets suggest model is making gender-related errors beyond stereotypes
  - Language-specific discrepancies may indicate methodology limitations for certain languages

- First 3 experiments:
  1. Implement the data creation pipeline for one language and verify the PS/AS/NS categorization logic
  2. Calculate bias scores for a simple model and compare with the baseline FN score to understand the difference
  3. Create a small bias-controlled dataset with r=0.5 and train a model to observe how bias manifests in predictions

## Open Questions the Paper Calls Out

- Question: How does the proposed method perform on languages with more complex gender systems (e.g., languages with grammatical gender, honorifics, or gender-neutral pronouns)?
  - Basis in paper: The paper focuses on English, Japanese, and Chinese, which have relatively simple gender systems compared to some other languages. The authors acknowledge this limitation in the ethics statement.
  - Why unresolved: The paper does not provide any data or analysis on languages with more complex gender systems.
  - What evidence would resolve it: Experiments applying the proposed method to languages with more complex gender systems, such as German, Russian, or Arabic, and comparing the results to those obtained on English, Japanese, and Chinese.

- Question: How does the proposed method perform when applied to other types of social biases beyond gender, such as racial or religious biases?
  - Basis in paper: The paper focuses specifically on gender bias in occupation words. The authors mention that extending the study to other types of biases is scoped as future work.
  - Why unresolved: The paper does not provide any data or analysis on biases other than gender.
  - What evidence would resolve it: Experiments applying the proposed method to evaluate other types of social biases, such as racial or religious biases, and comparing the results to those obtained on gender bias.

## Limitations

- The method assumes that varying bias rates in training data creates proportionally biased models, which may not hold if models learn spurious correlations or if architecture affects bias independently
- The use of image caption datasets introduces potential confounds, as these datasets themselves may contain gender biases that transfer to the evaluation
- The method's effectiveness may vary across languages or domains if the relationship between gender bias and NLI labels differs systematically

## Confidence

- High confidence: The core mechanism of using all three NLI labels (entailment, contradiction, neutral) to detect gender bias, supported by clear theoretical justification and empirical results across three languages
- Medium confidence: The meta-evaluation approach using bias-controlled training data, as it relies on the assumption that bias rates in training data translate proportionally to model bias
- Low confidence: The claim that template-based evaluation is inherently problematic, as the paper doesn't directly compare results with template-based methods

## Next Checks

1. Verify the assumption about bias rate translation by conducting ablation studies with different model architectures and measuring bias independently of training data bias rates
2. Compare the proposed method's results with template-based evaluation approaches on the same datasets to quantify the impact of using natural sentences versus templates
3. Extend the meta-evaluation to include domain-specific bias scenarios (e.g., medical or technical domains) to test whether the method generalizes beyond occupation-based gender stereotypes