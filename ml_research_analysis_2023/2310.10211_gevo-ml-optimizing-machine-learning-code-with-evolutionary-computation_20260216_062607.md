---
ver: rpa2
title: 'GEVO-ML: Optimizing Machine Learning Code with Evolutionary Computation'
arxiv_id: '2310.10211'
source_url: https://arxiv.org/abs/2310.10211
tags:
- gevo-ml
- mlir
- learning
- code
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GEVO-ML uses evolutionary computation to optimize ML workloads
  expressed in MLIR, automatically discovering performance improvements that span
  model architecture and low-level code implementation. It employs multi-objective
  search to balance execution time and model accuracy by applying mutations that copy,
  delete, or resize tensor variables in the MLIR representation.
---

# GEVO-ML: Optimizing Machine Learning Code with Evolutionary Computation

## Quick Facts
- arXiv ID: 2310.10211
- Source URL: https://arxiv.org/abs/2310.10211
- Reference count: 40
- One-line primary result: GEVO-ML achieves 90.43% runtime improvement for MobileNet prediction with only 2% accuracy drop

## Executive Summary
GEVO-ML introduces an evolutionary computation approach to optimize machine learning workloads expressed in MLIR, a unified intermediate representation. By applying mutations to tensor variables in the MLIR code, the system automatically discovers performance improvements that span both model architecture and low-level code implementation. The approach uses multi-objective search to balance execution time and model accuracy, enabling users to select optimal tradeoffs for their specific use cases.

## Method Summary
GEVO-ML operates on MLIR programs by applying three mutation operators: copy, delete, and resize tensor variables. The system uses NSGA-II evolutionary algorithm to optimize both execution time and model error simultaneously. Fitness evaluation involves running the mutated MLIR code through the IREE runtime and measuring both performance and accuracy. The approach repairs use-def chain violations caused by mutations and evolves solutions across generations to find Pareto-optimal tradeoffs between speed and accuracy.

## Key Results
- Achieved 90.43% runtime improvement for MobileNet (CIFAR10) prediction with only 2% accuracy drop
- Obtained 4.88% accuracy improvement for 2fcNet (MNIST) training without sacrificing speed
- Discovered optimizations including layer removal, bias term pruning, and gradient calculation modifications

## Why This Works (Mechanism)

### Mechanism 1
GEVO-ML finds optimizations across multiple abstraction layers by operating on MLIR, which preserves both high-level model structure and low-level execution details. This unified representation enables evolutionary mutations to modify model architecture and implementation simultaneously.

### Mechanism 2
The tensor resizing mutation operator enables model architecture optimizations by breaking fixed tensor size constraints. This allows GEVO-ML to drop or pad tensor values, effectively enabling layer pruning and parameter modifications that would otherwise be invalid.

### Mechanism 3
Multi-objective evolutionary search with NSGA-II enables finding Pareto-optimal solutions that balance execution time and model accuracy. This allows users to choose appropriate tradeoffs rather than optimizing for a single objective.

## Foundational Learning

- **Intermediate representations and compiler optimization passes**: Understanding MLIR as a unified IR spanning multiple abstraction levels is crucial for grasping why GEVO-ML can find cross-layer optimizations that traditional approaches miss.
  - Quick check: What is the key difference between traditional compiler IRs (like LLVM-IR) and MLIR that enables GEVO-ML's approach?

- **Evolutionary computation and genetic algorithms**: The core search mechanism relies on evolutionary principles like mutation, crossover, and selection to explore the optimization space.
  - Quick check: How does the patch representation used in GEVO-ML's crossover differ from traditional genetic programming approaches?

- **Multi-objective optimization and Pareto frontiers**: GEVO-ML uses NSGA-II to find solutions that balance competing objectives (speed vs accuracy).
  - Quick check: What does it mean when a solution is "Pareto optimal" in the context of GEVO-ML's optimization objectives?

## Architecture Onboarding

- **Component map**: DEAP-based evolutionary search framework (Python) -> C++ MLIR mutation engine -> IREE MLIR runtime execution environment -> Fitness evaluation pipeline (execution time + accuracy measurement) -> NSGA-II selection mechanism
- **Critical path**: MLIR program → mutation → repair → fitness evaluation → selection → next generation
- **Design tradeoffs**: Unified MLIR representation vs. specialized representations for different hardware targets; complex tensor mutation operations vs. simpler but less powerful mutations; multi-objective optimization vs. single-objective focus
- **Failure signatures**: Invalid MLIR after mutation (repair fails), fitness evaluation timeouts, convergence to local optima, Pareto frontier collapse (no meaningful tradeoffs)
- **First 3 experiments**:
  1. Run GEVO-ML on a simple 2fcNet with MNIST to verify basic functionality and understand the mutation process
  2. Compare results with and without tensor resizing mutations to quantify their impact
  3. Test different population sizes and generation limits to understand scalability and convergence behavior

## Open Questions the Paper Calls Out

### Open Question 1
How do tensor size mutations affect ML model behavior and performance in GEVO-ML, and what are the semantic implications of dropping or padding tensor values? The paper acknowledges this as a future research direction needing more extensive understanding.

### Open Question 2
What optimization opportunities would GEVO-ML uncover as the MLIR ecosystem matures and gains support for more dialects and hardware back-ends? The current implementation is limited by the immaturity of the MLIR ecosystem.

### Open Question 3
How does the performance of GEVO-ML compare to other AutoML approaches, such as those using reinforcement learning or superoptimization, in terms of optimization effectiveness and resource efficiency? The paper suggests this comparison is needed but not provided.

## Limitations

- Evaluation limited to two relatively simple models (MobileNet and 2fcNet) that may not generalize to complex architectures
- 90.43% runtime improvement claim appears exceptionally high without comparison to established optimization techniques
- Current implementation limited by MLIR ecosystem immaturity, particularly lack of support for many TensorFlow operations

## Confidence

- **High**: The fundamental approach of using MLIR as a unified representation for evolutionary optimization is technically sound
- **Medium**: The reported performance improvements are likely real but may be overstated or not directly comparable to traditional methods
- **Low**: The claim that tensor resizing mutations uniquely enable cross-layer optimizations is plausible but not definitively proven

## Next Checks

1. **Baseline comparison validation**: Run GEVO-ML against traditional ML optimization techniques (quantization, pruning, compiler optimizations) on the same models to establish whether claimed improvements are truly unique to the evolutionary approach.

2. **Tensor resizing necessity test**: Implement a variant of GEVO-ML without the tensor resizing mutation and compare results to quantify exactly how much this specific operator contributes to the reported improvements.

3. **Scaling test**: Apply GEVO-ML to a significantly larger and more complex model (e.g., ResNet-50 or EfficientNet) to assess whether the approach scales beyond the simple models tested in the paper.