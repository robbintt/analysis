---
ver: rpa2
title: Analysing the Impact of Removing Infrequent Words on Topic Quality in LDA Models
arxiv_id: '2311.14505'
source_url: https://arxiv.org/abs/2311.14505
tags:
- uni00000011
- uni00000013
- uni00000018
- uni00000003
- uni00000015
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the impact of removing infrequent words
  on topic quality in LDA models. The authors conducted a Monte Carlo simulation study
  to analyze the effects of different criteria for infrequent terms removal and various
  evaluation metrics.
---

# Analysing the Impact of Removing Infrequent Words on Topic Quality in LDA Models

## Quick Facts
- **arXiv ID**: 2311.14505
- **Source URL**: https://arxiv.org/abs/2311.14505
- **Reference count**: 9
- **Primary result**: Pruning infrequent words improves LDA topic quality, with about 30% of words removable without qualitative losses

## Executive Summary
This study investigates how removing infrequent words affects topic quality in Latent Dirichlet Allocation models. Through a Monte Carlo simulation study using synthetic corpora, the authors analyze different criteria for removing infrequent terms and their impact on various evaluation metrics. The results show that vocabulary pruning is beneficial, significantly reducing computational complexity while maintaining or improving topic quality. The study provides practical guidelines suggesting that approximately 30% of words can be removed without negative impacts on topic quality.

## Method Summary
The authors conducted a Monte Carlo simulation study using synthetic corpora generated from two different data generating processes (DGPs). DGP1 created 1,000 documents with 3,000 words each and 50 topics, while DGP2 generated 10,000 documents with 150 words each and 15 topics. For each DGP, 100 corpora were generated. The study applied different criteria for removing infrequent words (document frequency, term frequency, and TF-IDF) at various thresholds, then estimated LDA models on the preprocessed corpora. Multiple evaluation metrics including model fit, topic similarity, coherence, and recall were used to compare estimated topics against the true topics from the DGPs.

## Key Results
- Vocabulary pruning significantly reduces computational complexity by decreasing vocabulary size
- Approximately 30% of words can be removed without qualitative losses in resulting topics
- Optimal pruning thresholds vary between DGPs: 3% for DGP1 (long documents) and 2% for DGP2 (short texts)
- Pruning improves topic coherence and model fit while maintaining topic recall

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Removing infrequent terms reduces dimensionality, leading to more efficient and stable parameter estimation
- **Mechanism**: Infrequent words appear in few documents and provide limited information about general topics. Their removal shrinks vocabulary size, decreasing computational complexity of estimating topic-word and document-topic distributions, improving numerical stability of inference algorithms
- **Core assumption**: Removed words are uninformative for topic modeling and don't contribute meaningfully to semantic structure
- **Evidence anchors**: Abstract states removing infrequent words "believed to provide limited information about the corpus"; section notes removal "decreases the vocabulary size substantially and, consequently, accelerates model estimation"
- **Break condition**: If infrequent words are highly specific to niche topics or contain important domain-specific terminology, their removal could degrade topic quality

### Mechanism 2
- **Claim**: Pruning vocabulary based on document frequency improves topic coherence
- **Mechanism**: Words appearing in very few documents are often too specific and may not co-occur with other relevant terms. Removing such terms leaves vocabulary with stronger co-occurrence patterns, leading to more coherent and interpretable topics
- **Core assumption**: Topic coherence is a valid proxy for topic quality and interpretability
- **Evidence anchors**: Abstract discusses "effects of removing infrequent words for the quality of topics"; section notes words "occuring only with low frequency are believed to be too specific to contribute to the meaning of the resulting topics"
- **Break condition**: If evaluation metric (e.g., coherence) doesn't align with human interpretability or downstream task performance, improvement may not translate to better topics

### Mechanism 3
- **Claim**: Vocabulary pruning improves model fit by reducing noise and focusing estimation on informative terms
- **Mechanism**: Model fit metric compares estimated topic-word distributions to true distributions from DGP. Removing uninformative terms prevents LDA from assigning spurious topic probabilities to noise words, resulting in closer match to true topic structure
- **Core assumption**: True topic-word distributions are known and can serve as ground truth for evaluation
- **Evidence anchors**: Section states "we can analyze the impact of different settings on the model results as compared to the true DGP"; "we use different metrics to measure the similarity between true and estimated topics"
- **Break condition**: If evaluation metrics aren't robust to sampling variability or synthetic data doesn't capture real-world text complexity, improved model fit may not generalize

## Foundational Learning

- **Concept**: Latent Dirichlet Allocation (LDA) generative process
  - Why needed here: Understanding how LDA generates documents from topics is crucial for interpreting simulation study and preprocessing impact
  - Quick check question: In LDA generative process, what distribution is used to sample topic proportions for each document?

- **Concept**: Text preprocessing and vocabulary pruning
  - Why needed here: Study focuses on impact of removing infrequent words, so understanding different criteria (document frequency, term frequency, TF-IDF) and implications is essential
  - Quick check question: What is the difference between removing words based on document frequency versus term frequency?

- **Concept**: Topic model evaluation metrics
  - Why needed here: Study uses multiple metrics (coherence, topic similarity, model fit, recall) to assess topic quality, so understanding what each measures and limitations is important
  - Quick check question: Which evaluation metric in the study directly compares estimated topics to true topics from data generating process?

## Architecture Onboarding

- **Component map**: Data Generation → Preprocessing → Model Estimation → Evaluation
- **Critical path**: Data Generation → Preprocessing → Model Estimation → Evaluation. Each step must be completed before next one begins, and evaluation step depends on availability of true topics from DGP
- **Design tradeoffs**:
  - Computational efficiency vs. topic quality: More aggressive pruning speeds up estimation but may degrade topic quality if important terms removed
  - Generality vs. specificity: Removing infrequent terms improves general topic coherence but may lose niche or domain-specific topics
  - Synthetic vs. real data: Using synthetic data allows ground truth comparison but may not capture complexity of real-world text
- **Failure signatures**:
  - If recall drops sharply for higher pruning thresholds, indicates too many informative terms have been removed
  - If coherence improves but topic similarity to true topics decreases, pruning may be introducing artifacts
  - If evaluation metrics disagree on optimal pruning threshold, suggests metrics may not be aligned or true optimum is dataset-dependent
- **First 3 experiments**:
  1. Run LDA on full vocabulary (no pruning) and compute all evaluation metrics to establish baseline
  2. Apply document frequency-based pruning with low threshold (e.g., 1%) and compare results to baseline
  3. Apply document frequency-based pruning with high threshold (e.g., 5%) and observe impact on recall and model fit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different combinations of text preprocessing steps impact topic quality in LDA models beyond just removing infrequent words?
- Basis in paper: [inferred] Authors mention future research could follow Denny and Spirling (2018) to focus on different combinations of text preprocessing steps and investigate their impact in systematic manner through Monte Carlo studies
- Why unresolved: Current study only focuses on impact of removing infrequent words, while Denny and Spirling (2018) analyzed various preprocessing steps but didn't provide specific guidelines for LDA
- What evidence would resolve it: Comprehensive Monte Carlo simulation study comparing different combinations of preprocessing steps (e.g., stemming, lemmatizing, stopword removal, and vocabulary pruning) on LDA topic quality using multiple evaluation metrics

### Open Question 2
- Question: How does optimal threshold for removing infrequent words vary across different domains or types of corpora?
- Basis in paper: [explicit] Authors find optimal threshold varies between their two defined DGPs (DGP1 with long documents and DGP2 with short texts), suggesting domain-specific differences
- Why unresolved: Study only examines two specific DGPs and doesn't explore how optimal threshold might change for other types of text corpora like social media, legal documents, or scientific literature
- What evidence would resolve it: Systematic study applying same methodology across diverse real-world corpora from different domains to identify patterns in optimal vocabulary pruning thresholds

### Open Question 3
- Question: What is the computational trade-off between vocabulary size reduction and topic quality improvement across different LDA implementations?
- Basis in paper: [inferred] Authors note vocabulary pruning reduces computational burden and discuss trade-off between information loss and efficiency gains, but don't quantify computational savings
- Why unresolved: Study focuses on topic quality metrics but doesn't measure or compare actual computational time or resources required for different vocabulary sizes
- What evidence would resolve it: Empirical measurements of LDA estimation time and resource usage across different vocabulary pruning levels, coupled with topic quality assessments to determine optimal balance

## Limitations

- Confidence in results is limited by exclusive use of synthetic data, which may not capture real-world complexities like polysemy, multi-topic documents, or domain-specific terminology affected by pruning
- Study assumes all infrequent words are uninformative but doesn't validate this assumption against human judgment or downstream task performance
- Evaluation metrics (particularly coherence) may not fully align with human interpretability as no human validation is reported
- Optimal pruning threshold appears dataset-dependent (3% for DGP1 vs 2% for DGP2), suggesting generalizability may be limited

## Confidence

- **High confidence**: Computational efficiency gains from vocabulary pruning are well-established and clearly demonstrated through reduced vocabulary sizes and faster model estimation
- **Medium confidence**: Claim that ~30% of words can be removed without qualitative losses is supported within synthetic data context but may not generalize to real-world corpora
- **Medium confidence**: Improvement in topic coherence and model fit metrics is demonstrated but requires validation against human judgment and real-world task performance

## Next Checks

1. **Real-world validation**: Apply pruning methodology to diverse set of real-world corpora (news articles, scientific papers, social media) and compare topic quality using both automated metrics and human judgment
2. **Downstream task evaluation**: Test whether vocabulary pruning improves or degrades performance on downstream NLP tasks like document classification or information retrieval
3. **Sensitivity analysis**: Conduct more granular exploration of pruning thresholds (e.g., 0.5% increments) across different corpus sizes and domain types to establish more robust guidelines