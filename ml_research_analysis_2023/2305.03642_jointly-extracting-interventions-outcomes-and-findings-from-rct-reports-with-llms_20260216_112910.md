---
ver: rpa2
title: Jointly Extracting Interventions, Outcomes, and Findings from RCT Reports with
  LLMs
arxiv_id: '2305.03642'
source_url: https://arxiv.org/abs/2305.03642
tags:
- evidence
- extraction
- group
- language
- placebo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an approach for extracting structured clinical
  evidence from RCT abstracts using instruction-tuned LLMs. The method treats the
  task as a conditional generation problem, training a language model to produce linearized
  tuples containing interventions, comparators, outcomes, evidence, and inference
  labels.
---

# Jointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs

## Quick Facts
- arXiv ID: 2305.03642
- Source URL: https://arxiv.org/abs/2305.03642
- Reference count: 32
- Primary result: ~20 point absolute F1 gain over prior SOTA using instruction-tuned LLMs for end-to-end evidence extraction

## Executive Summary
This paper introduces an approach for extracting structured clinical evidence from RCT abstracts using instruction-tuned LLMs. The method treats the task as a conditional generation problem, training a language model to produce linearized tuples containing interventions, comparators, outcomes, evidence, and inference labels. Manual and automated evaluations show substantial performance gains, with the best model achieving over 70% F1 score, significantly outperforming prior methods. Ablations reveal that including evidence spans in the output improves results. The authors also release a searchable database of structured findings from over 650,000 RCT articles.

## Method Summary
The authors frame evidence extraction as a conditional generation task and fine-tune LLMs (Flan-T5-large) to generate linearized tuples containing interventions, comparators, outcomes, evidence spans, and inference labels from RCT abstracts. The model is trained with cross-entropy loss using teacher forcing, and outputs are parsed into structured tuples. The approach is evaluated on the Evidence Inference dataset and an exhaustive test set, with both automated metrics and manual expert evaluation showing significant performance improvements over prior methods.

## Key Results
- Achieved over 70% F1 score on end-to-end evidence extraction
- ~20 point absolute F1 gain over previous state-of-the-art
- Including evidence spans in the output improves model performance
- Released searchable database of structured findings from 650K+ RCT articles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction-tuned LLMs can generate structured tuples by treating extraction as a conditional generation problem.
- Mechanism: The model is fine-tuned to output linearized strings containing interventions, comparators, outcomes, evidence spans, and inference labels in a single pass, rather than using separate modules for each subtask.
- Core assumption: LLMs can learn to produce valid structured outputs by conditioning on input abstracts and teacher forcing during training.
- Evidence anchors:
  - [abstract]: "framing evidence extraction as a conditional generation task and fine-tuning LLMs for this purpose realizes considerable (∼20 point absolute F1 score) gains over the previous SOTA."
  - [section]: "we propose to train and evaluate models to conditionally generate ICO spans, findings regarding the reported comparative effectiveness... and supporting textual evidence."
  - [corpus]: Weak corpus support; most related work focuses on separate extraction stages, not end-to-end generation.
- Break condition: If the model fails to produce syntactically valid outputs (missing elements, invalid structure), the end-to-end generation approach fails.

### Mechanism 2
- Claim: Including evidence spans in the output improves model performance.
- Mechanism: Evidence spans provide grounding for the inference label, allowing the model to jointly learn extraction and inference rather than relying on fixed, noisy labels.
- Core assumption: Distant supervision labels in the training data are noisy, so conditioning on explicit evidence helps the model make better inferences.
- Evidence anchors:
  - [abstract]: "Ablations indicate the importance of jointly extracting evidence spans to support the inference task."
  - [section]: "Ablations indicate the importance of jointly extracting evidence spans to support the inference task; this may have implications for work on relation extraction via conditional generative models more broadly."
  - [corpus]: Limited corpus support; most related work does not explicitly extract evidence spans for relation extraction.
- Break condition: If evidence spans are missing or incorrect, the model's inference labels may become unreliable.

### Mechanism 3
- Claim: The fine-tuned LLM outperforms prior methods that use separate entity extraction, linking, and inference stages.
- Mechanism: By training a single model to generate all components together, the approach reduces error propagation and leverages learned correlations between elements.
- Core assumption: Joint modeling can outperform pipelines because it learns dependencies end-to-end rather than sequentially.
- Evidence anchors:
  - [abstract]: "The model we introduce yields a ∼20 point absolute gain in F1 score over the prior SOTA approach."
  - [section]: "Prior methods for joint extraction and inference pre-dated the modern LLMs which are the current dominant paradigm in NLP. Here we adopt such models..."
  - [corpus]: Weak corpus support; no direct comparisons to non-LLM joint models in the related papers.
- Break condition: If the model cannot handle variable-length outputs or invalid structures, pipeline approaches may recover better.

## Foundational Learning

- Concept: Conditional language modeling with teacher forcing
  - Why needed here: The model must learn to generate sequences that combine multiple structured elements from free text.
  - Quick check question: What does "teacher forcing" mean in the context of training this model?
- Concept: Linearized structured output formatting
  - Why needed here: The LLM needs a consistent way to represent tuples so they can be parsed after generation.
  - Quick check question: How are multiple tuples separated in the linearized string?
- Concept: Distant supervision and noisy labels
  - Why needed here: The training data uses automated label extraction, which may be incomplete or incorrect.
  - Quick check question: Why might human annotations find more tuples per abstract than the training set?

## Architecture Onboarding

- Component map:
  Input abstract -> Fine-tuned LLM (Flan-T5-large) -> Linearized tuple string -> Parsed structured tuples
- Critical path:
  1. Preprocess abstract to fit model input length
  2. Generate linearized output using LLM
  3. Validate and parse output structure
  4. Store or index tuples for retrieval
- Design tradeoffs:
  - End-to-end generation vs. modular pipeline: simpler deployment but risk of invalid outputs
  - Fixed output format vs. flexible generation: easier parsing but may miss nuanced relations
  - Larger models vs. smaller models: better accuracy but higher compute cost
- Failure signatures:
  - Invalid tuple structure (wrong number of elements)
  - Missing or swapped ICO elements
  - Evidence spans not grounded in abstract
- First 3 experiments:
  1. Train on a small subset of data and evaluate tuple parsing accuracy.
  2. Compare model outputs with and without evidence spans to measure impact on F1.
  3. Test model on abstracts with known missing tuples to assess recall.

## Open Questions the Paper Calls Out
- How do larger language models like GPT-4 perform on the end-to-end evidence extraction task compared to Flan-T5-large?
- How does the quality of evidence span extraction impact downstream clinical decision-making tasks?
- Can the conditional generation approach be extended to handle overlapping entities and relations in RCT abstracts?

## Limitations
- The ~20 point F1 gain is based on comparison with a single baseline using separate extraction modules
- No inter-annotator agreement statistics reported for human evaluation
- The approach requires careful parsing and is vulnerable to invalid output structures
- Training data uses noisy distant supervision labels

## Confidence
- **High confidence**: Including evidence spans in the output improves model performance
- **Medium confidence**: End-to-end generation outperforms pipeline approaches
- **Low confidence**: Generalizability of ~20 point F1 improvement without broader SOTA comparisons

## Next Checks
1. Conduct ablation studies removing the evidence span conditioning to quantify its contribution to overall performance, and verify that the observed improvement is statistically significant across multiple random seeds.
2. Perform inter-annotator agreement analysis on the human evaluation subset to establish reliability of the manual assessment, and compare results against additional baselines beyond the single Xu et al. (2022) method.
3. Test the model's robustness to out-of-distribution abstracts by evaluating on RCT abstracts from different medical domains or with different writing styles to assess generalizability beyond the training corpus.