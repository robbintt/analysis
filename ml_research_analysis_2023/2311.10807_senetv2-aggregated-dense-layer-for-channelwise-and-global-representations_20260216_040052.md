---
ver: rpa2
title: 'SENetV2: Aggregated dense layer for channelwise and global representations'
arxiv_id: '2311.10807'
source_url: https://arxiv.org/abs/2311.10807
tags:
- module
- network
- conv
- proposed
- squeeze
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces SENetV2, which enhances channel-wise representations\
  \ by incorporating an aggregated dense layer within the Squeeze-and-Excitation module.\
  \ The proposed method combines the strengths of squeeze-excitation networks with\
  \ multi-branch fully connected layers, improving the model\u2019s ability to capture\
  \ both channel-wise and global representations."
---

# SENetV2: Aggregated dense layer for channelwise and global representations

## Quick Facts
- arXiv ID: 2311.10807
- Source URL: https://arxiv.org/abs/2311.10807
- Reference count: 20
- Primary result: SENetV2-50 outperforms ResNet-50 by 0.88% on CIFAR-10 and 1.80% on CIFAR-100 while maintaining negligible parameter increase.

## Executive Summary
SENetV2 enhances channel-wise representations by incorporating an aggregated dense layer within the Squeeze-and-Excitation module. The proposed method combines the strengths of squeeze-excitation networks with multi-branch fully connected layers, improving the model's ability to capture both channel-wise and global representations. Extensive experiments on CIFAR-10, CIFAR-100, and a modified ImageNet dataset demonstrate that SENetV2 achieves higher classification accuracy compared to vanilla ResNet and SENet while maintaining a negligible increase in parameters.

## Method Summary
SENetV2 introduces a multi-branch dense layer within the Squeeze-and-Excitation module to enhance global feature representations. The method replaces the standard single dense layer in the squeeze operation with multiple parallel dense layers (cardinality = 4) that capture diverse feature transformations. These are concatenated and passed to the excitation layer, which scales the original feature map. The approach is integrated into ResNet architectures and trained using Adam optimizer with categorical cross-entropy loss.

## Key Results
- SENetV2-50 achieves 0.88% higher accuracy than ResNet-50 on CIFAR-10
- SENetV2-50 achieves 1.80% higher accuracy than ResNet-50 on CIFAR-100
- Parameter increase is negligible compared to standard SENet
- Competitive results on modified ImageNet dataset (250 images per class)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggregated multi-branch fully connected layers enable the network to learn richer global representations by combining diverse feature interactions.
- Mechanism: The squeeze operation extracts channel-wise statistics via global average pooling. Instead of a single dense layer, SENetV2 uses multiple parallel dense layers (cardinality = 4) to capture different transformations of the reduced representation. These are concatenated and passed to the excitation layer, which scales the original feature map.
- Core assumption: Parallel dense layers with the same input can learn complementary global patterns that a single dense layer cannot.
- Evidence anchors:
  - [abstract] "We introduce a novel aggregated multilayer perceptron, a multi-branch dense layer, within the Squeeze excitation residual module..."
  - [section] "This proposed model has a negligible increase in parameters when compared to SENet."

### Mechanism 2
- Claim: The aggregated dense layer improves channel-wise recalibration by leveraging richer global context.
- Mechanism: The excitation layer in SENetV2 receives concatenated outputs from multiple dense branches, which encode different global feature transformations. This richer context allows more precise channel-wise scaling when the result is multiplied back with the feature map.
- Core assumption: More diverse global representations lead to better channel-wise importance weighting.
- Evidence anchors:
  - [section] "This fusion enhances the network's ability to capture channel-wise patterns and have global knowledge..."
  - [corpus] Weak. No direct comparison of channel recalibration quality in related works.

### Mechanism 3
- Claim: Integrating aggregated dense layers into the squeeze-excitation module adds representational power without significant parameter increase.
- Mechanism: By using dense layers with reduced size and controlled cardinality, SENetV2 maintains parameter efficiency while expanding the representational capacity compared to standard SE modules.
- Core assumption: Dense layers with reduced size and moderate cardinality can increase representational power without blowing up parameters.
- Evidence anchors:
  - [abstract] "...maintaining a negligible increase in parameters..."
  - [section] "This proposed model has a negligible increase in parameters when compared to SENet."

## Foundational Learning

- Concept: Squeeze operation via global average pooling
  - Why needed here: Extracts channel-wise statistics to summarize spatial information, enabling channel-wise recalibration.
  - Quick check question: What is the shape of the tensor after global average pooling on a feature map of shape (batch, channels, height, width)?

- Concept: Multi-branch architecture (cardinality)
  - Why needed here: Allows parallel processing of global representations, capturing diverse feature interactions similar to Inception or ResNeXt.
  - Quick check question: How does increasing cardinality affect model capacity and parameter count?

- Concept: Residual connections with shortcut paths
  - Why needed here: Enables stable gradient flow and identity mapping, essential for deep networks and for integrating new modules without disrupting training.
  - Quick check question: What is the mathematical form of a residual block output in terms of input x and transformation F(x)?

## Architecture Onboarding

- Component map: Input feature map → Global Average Pooling → Squeeze (multi-branch dense layers) → Concatenation → Excitation (dense + sigmoid) → Scaling with input → Output feature map
- Critical path: Squeeze → Multi-branch dense layers → Concatenation → Excitation → Scaling
- Design tradeoffs:
  - Parameter efficiency vs. representational power (controlled by cardinality and reduction size)
  - Computational overhead of additional dense layers
  - Model depth vs. width in capturing global patterns
- Failure signatures:
  - Performance plateaus or degrades if cardinality is too low (insufficient diversity) or too high (overfitting/noise)
  - Training instability if excitation weights are not properly normalized
  - Increased memory usage with larger cardinality
- First 3 experiments:
  1. Replace single dense layer in SE module with 2-branch dense layers; compare CIFAR-10 accuracy and parameter count.
  2. Vary cardinality (2, 4, 8) in the multi-branch dense layers; measure impact on CIFAR-100 performance.
  3. Test different reduction sizes (8, 16, 32) to find optimal balance between parameter efficiency and accuracy on CIFAR-10.

## Open Questions the Paper Calls Out

- How does the performance of SENetV2 compare to other state-of-the-art architectures like EfficientNet or Vision Transformers on the same datasets?
- What is the impact of increasing the cardinality value beyond 4 in the aggregated dense layer on the model's performance and computational complexity?
- How does SENetV2 perform on other vision tasks beyond image classification, such as object detection or semantic segmentation?

## Limitations
- Lack of detailed architectural specifications for the multi-branch dense layer
- Absence of ablation studies comparing single vs multi-branch dense layers
- Modified ImageNet dataset with only 250 training images per class limits generalizability

## Confidence
- High Confidence: SENetV2 improves classification accuracy on CIFAR-10 and CIFAR-100
- Medium Confidence: Parameter increase is negligible compared to standard SENet
- Low Confidence: Competitive performance on modified ImageNet dataset

## Next Checks
1. Implement SENetV2 with varying cardinality (2, 4, 8) and reduction sizes (8, 16, 32) to empirically determine optimal configuration.
2. Conduct ablation study comparing single dense layer vs. multi-branch dense layers within SE module.
3. Train SENetV2 on standard ImageNet-1k dataset and compare against EfficientNet and Vision Transformer models.