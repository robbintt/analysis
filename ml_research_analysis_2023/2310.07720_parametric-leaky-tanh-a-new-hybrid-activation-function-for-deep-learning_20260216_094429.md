---
ver: rpa2
title: 'Parametric Leaky Tanh: A New Hybrid Activation Function for Deep Learning'
arxiv_id: '2310.07720'
source_url: https://arxiv.org/abs/2310.07720
tags:
- activation
- function
- layer
- tanh
- pltanh
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Parametric Leaky Tanh (PLTanh), a novel
  hybrid activation function that combines the strengths of Tanh and Leaky ReLU to
  improve deep neural network performance. PLTanh addresses the 'dying ReLU' problem
  by ensuring non-zero gradients for negative inputs while maintaining the smooth,
  bounded output of Tanh.
---

# Parametric Leaky Tanh: A New Hybrid Activation Function for Deep Learning

## Quick Facts
- arXiv ID: 2310.07720
- Source URL: https://arxiv.org/abs/2310.07720
- Reference count: 0
- Introduces Parametric Leaky Tanh (PLTanh), a hybrid activation function combining Tanh and Leaky ReLU

## Executive Summary
This paper introduces the Parametric Leaky Tanh (PLTanh), a novel hybrid activation function that combines the strengths of Tanh and Leaky ReLU to improve deep neural network performance. PLTanh addresses the 'dying ReLU' problem by ensuring non-zero gradients for negative inputs while maintaining the smooth, bounded output of Tanh. The function is differentiable at all points and introduces a tunable parameter α for flexibility across datasets. Experiments on five diverse datasets using 5-fold cross-validation and Bayesian optimization for parameter tuning demonstrate PLTanh's superior performance over ReLU, LReLU, and ALReLU.

## Method Summary
The paper evaluates PLTanh against ReLU, LReLU, and ALReLU on image classification tasks across five datasets (MNIST, Fashion MNIST, TensorFlow Flowers, CIFAR-10, and Histopathologic Cancer Detection). The method involves training CNNs with architectures specified per dataset, using Adam optimizer and Bayesian optimization to find optimal α parameter values. Performance is measured through 5-fold cross-validation with metrics including accuracy, macro precision, macro recall, macro F1-score, and AUC. The PLTanh activation function is implemented as a custom Keras layer.

## Key Results
- PLTanh achieved accuracy improvements ranging from 0.1% to 2.8% across datasets
- Consistently outperformed competitors in precision, recall, and F1-score metrics
- Superior performance validated across diverse image classification tasks including histopathologic cancer detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PLTanh avoids the dying ReLU problem by maintaining non-zero gradients for negative inputs.
- Mechanism: The PLTanh function combines Tanh and Leaky ReLU by taking max(tanh(x), α * abs(x)), ensuring that for negative inputs, either tanh(x) or α * abs(x) remains positive, thus providing a non-zero gradient.
- Core assumption: The Leaky ReLU component (α * abs(x)) provides a sufficiently small positive slope to prevent neuron death while not overwhelming the Tanh component.

### Mechanism 2
- Claim: PLTanh improves gradient flow by combining the smooth nonlinearity of Tanh with the computational efficiency of Leaky ReLU.
- Mechanism: The PLTanh function leverages Tanh's smooth, bounded output for positive inputs while using Leaky ReLU's linear behavior for negative inputs, creating a hybrid that maintains smooth gradients throughout the input space.
- Core assumption: The combination of smooth Tanh and linear Leaky ReLU components creates a more stable gradient landscape than either function alone.

### Mechanism 3
- Claim: PLTanh's tunable parameter α allows adaptation to different data distributions and network architectures.
- Mechanism: The α parameter controls the slope of the Leaky ReLU component, allowing the activation function to be tuned for specific datasets or tasks through Bayesian optimization.
- Core assumption: Different datasets benefit from different balances between Tanh and Leaky ReLU components, and the α parameter provides this flexibility.

## Foundational Learning

- Concept: Activation functions in neural networks
  - Why needed here: Understanding how activation functions transform inputs and introduce nonlinearity is crucial for grasping PLTanh's design and benefits
  - Quick check question: What is the primary purpose of activation functions in deep neural networks?

- Concept: Gradient flow and backpropagation
  - Why needed here: The paper's claims about addressing dying ReLU and vanishing gradients require understanding how gradients propagate through activation functions
  - Quick check question: How does the derivative of an activation function affect gradient flow during backpropagation?

- Concept: Convolutional neural networks (CNNs)
  - Why needed here: The experimental evaluation uses CNN architectures, so understanding their structure and training process is essential
  - Quick check question: What are the key components of a typical CNN architecture used for image classification?

## Architecture Onboarding

- Component map: Input -> CNN Layers -> PLTanh Activation -> Output Layer
- Critical path: During forward propagation, PLTanh computes max(tanh(x), α * abs(x)); during backpropagation, it computes the derivative based on the piecewise conditions outlined in the paper
- Design tradeoffs: PLTanh offers improved gradient flow and flexibility but adds computational overhead compared to simple ReLU; the optimal α value varies by dataset and requires additional hyperparameter tuning
- Failure signatures: Poor performance on datasets where standard activation functions already perform well; instability during training if α is set too high; dead neurons if α is set too low
- First 3 experiments:
  1. Replace ReLU with PLTanh (using default α=0.01) in a simple CNN trained on MNIST to verify basic functionality
  2. Perform a grid search over α values (0.001 to 0.1) on Fashion MNIST to find the optimal parameter for this dataset
  3. Compare PLTanh against ReLU and Leaky ReLU on CIFAR-10 with Bayesian optimization to find optimal α and verify performance improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PLTanh compare to other hybrid activation functions that combine different properties of existing activation functions (e.g., combinations of ReLU, Tanh, Sigmoid)?
- Basis in paper: [explicit] The paper mentions that despite significant advancements in the development of activation functions, traditional functions like Sigmoid and Tanh are still plagued by the vanishing gradient problem. It also notes that ReLU variants like LReLU, PReLU, RReLU, and CReLU were developed to address these challenges.
- Why unresolved: The paper only compares PLTanh to ReLU, LReLU, and ALReLU, but does not explore its performance relative to other hybrid activation functions.
- What evidence would resolve it: Experimental results comparing PLTanh to other hybrid activation functions across the same datasets and evaluation metrics used in the paper.

### Open Question 2
- Question: How does the choice of the alpha parameter (α) in PLTanh affect its performance across different types of datasets and tasks?
- Basis in paper: [explicit] The paper states that the inclusion of the alpha parameter offers adaptability, allowing the function to be attuned to various data distributions. It also mentions that Bayesian Optimization was used to pinpoint the 'α' parameter for the PLTanh AF in each dataset.
- Why unresolved: While the paper provides optimal alpha values for each dataset, it does not explore how different alpha values affect performance across a range of datasets or tasks.
- What evidence would resolve it: A systematic study varying the alpha parameter across a diverse set of datasets and tasks, analyzing the resulting performance changes.

### Open Question 3
- Question: How does the computational efficiency of PLTanh compare to other activation functions, especially in very deep networks?
- Basis in paper: [explicit] The paper mentions that LReLU is favored in DNNs for its computational efficiency. It also notes that PLTanh introduces a tunable parameter α for flexibility across datasets.
- Why unresolved: The paper focuses on accuracy and other performance metrics but does not discuss the computational cost of using PLTanh compared to other activation functions.
- What evidence would resolve it: Benchmarking the training and inference time of networks using PLTanh versus other activation functions, especially as network depth increases.

## Limitations

- Experiments focus exclusively on image classification tasks, limiting generalizability to other domains
- Bayesian optimization for α parameter tuning is computationally intensive and may not scale efficiently
- 5-fold cross-validation may not fully capture performance variance across different random initializations

## Confidence

- **High Confidence**: PLTanh's mathematical differentiability and avoidance of dying ReLU (core mathematical claims)
- **Medium Confidence**: Performance improvements on benchmark datasets (dependent on experimental setup)
- **Low Confidence**: Generalization to non-image domains and larger-scale applications

## Next Checks

1. **Ablation Study**: Test PLTanh with fixed α values across all datasets to determine if Bayesian optimization is essential for performance gains or if a universal α value works reasonably well.

2. **Domain Generalization**: Evaluate PLTanh on non-image datasets (e.g., UCI repository classification tasks) to assess cross-domain applicability beyond the image classification focus.

3. **Computational Overhead Analysis**: Measure training time and memory usage differences between PLTanh and baseline activation functions to quantify practical deployment costs.