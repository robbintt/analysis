---
ver: rpa2
title: Self-Supervised Learning for Anomalous Sound Detection
arxiv_id: '2312.09578'
source_url: https://arxiv.org/abs/2312.09578
tags:
- system
- anomalous
- dcase2023
- sound
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates self-supervised learning (SSL) for anomalous
  sound detection (ASD). It reviews two existing SSL approaches (mixup and StatEx)
  and proposes a novel SSL approach called feature exchange (FeatEx).
---

# Self-Supervised Learning for Anomalous Sound Detection

## Quick Facts
- arXiv ID: 2312.09578
- Source URL: https://arxiv.org/abs/2312.09578
- Reference count: 0
- Primary result: Proposed FeatEx SSL method achieves 72.6% harmonic mean of AUCs and pAUCs on DCASE2023, setting new state-of-the-art

## Executive Summary
This work investigates self-supervised learning (SSL) for anomalous sound detection (ASD) in machine condition monitoring. The authors review two existing SSL approaches (mixup and StatEx) and propose a novel SSL approach called feature exchange (FeatEx). They combine these approaches into a single loss function for training an ASD system. Experiments on the DCASE2022 and D2023 ASD datasets demonstrate that FeatEx outperforms existing SSL approaches, and that applying SSL to ASD is highly beneficial. The proposed system achieves state-of-the-art performance on the DCASE2023 ASD dataset, significantly outperforming all other published results.

## Method Summary
The proposed method uses a two-branch CNN architecture with one sub-network processing full magnitude spectrum and another processing spectrogram with temporal mean normalization. The system is trained using angular margin loss (AdaCos) with sub-clusters, combined with three SSL losses: mixup (50% probability), StatEx variant (50% probability, temporal axis only), and FeatEx (exchange features between sub-networks, 50% probability). SSL-introduced classes use trainable cluster centers. The combined loss includes supervised and SSL components with equal weighting. During inference, k-means clustering is applied to normal samples from the source domain, and cosine distance to means and target samples is used for anomaly scoring.

## Key Results
- FeatEx SSL approach outperforms existing StatEx and mixup approaches on DCASE2022 and D2023 datasets
- Applying SSL to ASD is highly beneficial, significantly improving performance over supervised-only approaches
- The proposed system achieves 72.6% harmonic mean of AUCs and pAUCs on DCASE2023, setting new state-of-the-art performance
- FeatEx with trainable cluster centers shows better performance than with non-trainable centers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining supervised classification with SSL losses improves ASD performance by capturing richer embeddings
- Mechanism: Supervised loss focuses on target machine sounds using class labels, while SSL approaches like FeatEx and StatEx introduce additional variability that forces the model to learn more general representations invariant to domain shifts
- Core assumption: Class labels are highly beneficial for noisy audio, but embeddings become less informative as classes become too similar; SSL compensates by adding structural learning without labels
- Evidence anchors: [abstract] "the less difficult the classification task becomes, the less informative are the embeddings"; [section] "it was shown that class labels alone are very beneficial to detect anomalous sounds in noisy conditions"

### Mechanism 2
- Claim: FeatEx improves over StatEx by operating on embeddings rather than raw statistics, encouraging cross-branch consistency
- Mechanism: FeatEx exchanges features between the spectrogram and magnitude spectrum sub-networks, forcing the model to learn complementary and consistent representations; this is more informative than exchanging statistics within one branch
- Core assumption: Different input feature representations capture different aspects of the signal; exchanging embeddings aligns these representations more effectively than exchanging statistics
- Evidence anchors: [section] "Let e1 = (e1_1, e2_1), e2 = (e1_2, e2_2) ∈ R^{2D}... define a new embedding and its label by setting enew = (e1_1, e2_2)"

### Mechanism 3
- Claim: Using trainable cluster centers for SSL-introduced classes improves performance by allowing the model to refine pseudo-class boundaries
- Mechanism: SSL approaches create new pseudo-classes; trainable centers let the model adapt these boundaries during training, whereas non-trainable centers might enforce suboptimal partitions
- Core assumption: SSL pseudo-classes are not perfect and can be improved with training; rigid centers prevent this refinement
- Evidence anchors: [section] "Experimentally, we found that the SSL approaches performed better with trainable cluster centers"

## Foundational Learning

- Concept: Angular margin loss (AdaCos)
  - Why needed here: Encourages larger inter-class margins in embedding space, improving separation between normal and anomalous sounds
  - Quick check question: How does AdaCos adapt the margin during training compared to a fixed margin?

- Concept: Self-supervised learning via data augmentation
  - Why needed here: Creates artificial classes without labels to teach the model about data structure, useful when class labels are insufficient or too easy
  - Quick check question: What is the difference between using mixup as data augmentation vs. as an SSL approach in this context?

- Concept: Domain generalization
  - Why needed here: The system must detect anomalies across different domains (source vs. target) with limited target domain samples; SSL helps learn domain-invariant features
  - Quick check question: How does the target domain differ from the source domain in the DCASE datasets?

## Architecture Onboarding

- Component map: Raw waveform -> feature extraction -> dual sub-networks (magnitude spectrum and mean-normalized spectrogram) -> concatenated embeddings -> anomaly scoring via cosine distance
- Critical path: Raw waveform → feature extraction → dual sub-networks → concatenated embeddings → anomaly scoring via cosine distance to k-means cluster means
- Design tradeoffs: Trainable vs. non-trainable cluster centers; SSL loss weighting; feature exchange vs. statistics exchange; temporal vs. frequency axis for StatEx
- Failure signatures: Performance drop on target domain; overfitting to SSL pseudo-classes; embeddings not capturing anomalies; high variance across runs
- First 3 experiments:
  1. Baseline vs. FeatEx alone on DCASE2023 dev source split
  2. Ablation: FeatEx with non-trainable vs. trainable cluster centers
  3. Ablation: FeatEx vs. StatEx vs. mixup on DCASE2023 eval target split

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed FeatEx method compare to other SSL approaches in scenarios with different levels of domain shift between source and target data?
- Basis in paper: [inferred] The paper shows FeatEx outperforms StatEx and mixup, but doesn't systematically explore performance across varying degrees of domain shift
- Why unresolved: The experiments used fixed domain shift scenarios in DCASE datasets, not varying the magnitude of shift
- What evidence would resolve it: Comparative experiments on datasets with controlled, varying levels of domain shift between source and target domains

### Open Question 2
- Question: What is the impact of the number of classes (N_classes) on the performance of SSL approaches for ASD?
- Basis in paper: [inferred] The paper uses different numbers of classes for different datasets but doesn't systematically study how class count affects performance
- Why unresolved: Experiments used fixed class configurations based on available meta-information rather than exploring a range of class counts
- What evidence would resolve it: Systematic experiments varying the number of classes while keeping other factors constant

### Open Question 3
- Question: How does the performance of SSL-based ASD systems scale with the amount of unlabeled data available for training?
- Basis in paper: [inferred] The paper uses fixed dataset sizes and doesn't investigate how performance changes with more unlabeled training data
- Why unresolved: Experiments used standard DCASE dataset splits without varying the amount of available training data
- What evidence would resolve it: Performance comparison across datasets with varying amounts of unlabeled training data

### Open Question 4
- Question: What is the optimal probability for applying each SSL technique during training?
- Basis in paper: [explicit] The paper uses 50% probability for StatEx and FeatEx, 100% for mixup, but notes this was determined empirically without systematic optimization
- Why unresolved: The paper doesn't explore how different application probabilities affect performance
- What evidence would resolve it: Systematic experiments varying the application probabilities for each SSL technique

### Open Question 5
- Question: How do the proposed SSL approaches perform on real-world industrial datasets with naturally occurring anomalies?
- Basis in paper: [inferred] The experiments use synthetic anomalies in controlled datasets, not real-world industrial scenarios
- Why unresolved: All experiments were conducted on DCASE datasets with artificially generated anomalies
- What evidence would resolve it: Experiments on real industrial datasets with naturally occurring machine faults

## Limitations
- Performance evaluation relies on ablation studies within the same dataset without external validation
- Proposed FeatEx mechanism lacks comparative analysis against other feature-level SSL methods
- Hyperparameter sensitivity, particularly regarding SSL loss weighting and trainable cluster center configuration, is not extensively explored

## Confidence
- High Confidence: The core mechanism of combining supervised classification with SSL losses for ASD shows consistent improvement across multiple datasets and splits
- Medium Confidence: The specific advantage of FeatEx over StatEx is demonstrated through ablation studies, but the underlying reasons for improved performance could benefit from additional analysis
- Medium Confidence: The effectiveness of trainable cluster centers for SSL approaches is supported by experimental results but lacks theoretical justification

## Next Checks
1. Evaluate the proposed method on an independent anomalous sound detection dataset not used in training or development to assess true generalization capabilities
2. Conduct systematic ablation studies varying the probability and weighting of each SSL component (mixup, StatEx, FeatEx) to identify optimal configurations and interactions
3. Visualize and analyze the learned embeddings using t-SNE or UMAP to verify that FeatEx produces more discriminative clusters between normal and anomalous sounds compared to baseline and StatEx approaches