---
ver: rpa2
title: Modality-aware Transformer for Financial Time series Forecasting
arxiv_id: '2310.01232'
source_url: https://arxiv.org/abs/2310.01232
tags:
- time
- series
- data
- forecasting
- modality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of multi-modal time series forecasting,
  specifically in the financial domain, where accurate predictions rely on external
  data sources beyond historical values. The authors propose a novel Modality-aware
  Transformer (MAT) model that effectively combines textual and numerical time series
  data to forecast target time series.
---

# Modality-aware Transformer for Financial Time series Forecasting

## Quick Facts
- arXiv ID: 2310.01232
- Source URL: https://arxiv.org/abs/2310.01232
- Reference count: 34
- The paper proposes a novel Modality-aware Transformer (MAT) model that combines textual and numerical time series data to improve financial forecasting accuracy.

## Executive Summary
This paper addresses the challenge of multi-modal time series forecasting in the financial domain, where accurate predictions rely on external data sources beyond historical values. The authors propose a novel Modality-aware Transformer (MAT) model that effectively combines textual and numerical time series data to forecast target time series. MAT introduces feature-level attention layers to focus on the most relevant features within each modality, and develops novel Intra-modal, Inter-modal, and Modality-target Multi-Head Attention mechanisms to capture both feature and temporal attentions. The model demonstrates significant improvements in forecasting accuracy compared to existing state-of-the-art methods, as evidenced by lower MSE and MAE values across various interest rate maturities and prediction horizons on real-world financial datasets.

## Method Summary
The Modality-aware Transformer (MAT) is designed to handle multi-modal time series forecasting by incorporating feature-level attention and novel attention mechanisms. The model processes two input modalities: numerical time series data (FRED indices) and textual reports (Federal Reserve statements). A textual module extracts topic-level sentiment scores from the text using financial BERT. The feature-level attention layer computes attention weights over input features of each modality, emphasizing informative features while suppressing less relevant ones. The encoder maintains separate modality streams with intra-modal MHAs for within-modality attention and inter-modal MHAs for cross-modality attention. The decoder includes masked MHA and modality-target MHA with feature-level attention. The model is trained using Adam optimizer with learning rate 1e-4, batch size 16, and early stopping after 10 epochs.

## Key Results
- MAT achieves lower MSE and MAE values compared to Transformer, Reformer, Autoformer, Informer, and ElasticNet baselines across various interest rate maturities and prediction horizons.
- The model demonstrates improved performance with both FRED and FED reports as inputs compared to using a single modality.
- Feature-level attention and the modality-aware structure contribute to the model's ability to effectively exploit information within each modality and foster cross-modal understanding.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature-level attention enables the model to selectively weight different features within each modality based on their relevance to the target time series.
- Mechanism: The feature-level attention layer computes attention weights over the input features of each modality and applies element-wise multiplication to create weighted inputs. This process emphasizes informative features while suppressing less relevant ones before they enter the transformer blocks.
- Core assumption: Different features within a modality have varying degrees of relevance to the target forecasting task, and this relevance can be learned from data.
- Evidence anchors:
  - [abstract] "develop feature-level attention layers that encourage the model to focus on the most relevant features within each data modality"
  - [section] "X att modi = softmax (conv1(Xmodi)) ◦ Xmodi"
- Break condition: If the feature-level attention fails to learn meaningful patterns, the model would perform similarly to baseline transformers that treat all features equally.

### Mechanism 2
- Claim: The novel Intra-modal MHA incorporates both feature and temporal attention, allowing the model to focus on important time steps while considering feature importance.
- Mechanism: By using feature-level attention weights as part of the query, key, and value computations in the MHA, the attention scores are modulated by feature importance, creating more informative embeddings that consider both temporal and feature-level relevance.
- Core assumption: The combination of feature and temporal attention provides richer representations than temporal attention alone.
- Evidence anchors:
  - [abstract] "develop a novel Intra-modal multi-head attention (MHA), Inter-modal MHA and Modality-target MHA in a way that both feature and temporal attentions are incorporated in MHAs"
  - [section] "By incorporating the proposed feature-level attention, we develop a novel Intra-modal Multi-Head Attention (MHA) and Inter-modal MHA in our modality-aware structure in a way that both feature and temporal attentions are incorporated in MHAs"
- Break condition: If the feature-level attention doesn't provide meaningful modulation, the MHA would effectively reduce to standard temporal attention.

### Mechanism 3
- Claim: The modality-aware structure with separate streams for each modality enables effective cross-modal understanding while preserving modality-specific information.
- Mechanism: The encoder maintains separate modality streams with intra-modal MHAs for within-modality attention and inter-modal MHAs for cross-modality attention, allowing the model to discover correlations between modalities at different time steps.
- Core assumption: Different modalities may have different sampling rates and temporal alignments, requiring separate processing streams before cross-modal integration.
- Evidence anchors:
  - [abstract] "The proposed modality-aware structure enables the model to effectively exploit information within each modality as well as foster cross-modal understanding"
  - [section] "MAT has two separate modality streams in the encoder for extracting information embedded in different modalities"
- Break condition: If the inter-modal MHAs fail to discover meaningful cross-modal correlations, the model would perform similarly to models that concatenate modalities before processing.

## Foundational Learning

- Concept: Multi-head attention mechanism
  - Why needed here: The model extends the standard multi-head attention to incorporate feature-level attention, requiring understanding of how attention mechanisms work in transformers
  - Quick check question: How does the scaled dot-product attention compute attention scores between queries and keys?

- Concept: Time series forecasting with external data sources
  - Why needed here: The model specifically addresses financial time series forecasting that relies on external textual and economic data, requiring understanding of how external factors influence time series
  - Quick check question: Why might future interest rates be more influenced by textual reports than by their own historical values?

- Concept: Feature importance weighting
  - Why needed here: The feature-level attention mechanism requires understanding how to compute and apply feature importance weights to inputs
  - Quick check question: How does the softmax operation ensure that attention weights sum to 1 across features?

## Architecture Onboarding

- Component map: Input → Feature-level attention → Encoder (intra-modal → inter-modal) → Decoder (masked MHA → modality-target MHA) → Output
- Critical path: Input → Feature-level attention → Encoder (intra-modal → inter-modal) → Decoder (masked MHA → modality-target MHA) → Output
- Design tradeoffs:
  - Separate modality streams provide flexibility but increase model complexity
  - Feature-level attention adds interpretability but requires additional parameters
  - The modality-aware structure supports different sampling rates but may require careful alignment handling
- Failure signatures:
  - Poor performance with single modality suggests feature-level attention or cross-modal mechanisms aren't working
  - High variance in predictions might indicate insufficient feature-level attention learning
  - Computational bottlenecks could arise from maintaining separate modality streams
- First 3 experiments:
  1. Ablation test: Remove feature-level attention and compare performance to full model
  2. Single modality test: Train with only text or only time series data to verify cross-modal learning
  3. Prediction horizon test: Evaluate performance across different forecast horizons (1-month, 3-months, 6-months) to identify temporal generalization limits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MAT model perform on other financial time series datasets beyond the FED dataset used in this study?
- Basis in paper: [explicit] The authors state that they evaluated their model on a real-world dataset consisting of U.S. interest rates and FED reports, but they do not explore its performance on other financial datasets.
- Why unresolved: The paper focuses on a single dataset, limiting the generalizability of the results to other financial time series.
- What evidence would resolve it: Testing the MAT model on diverse financial datasets, such as stock prices, commodity prices, or exchange rates, would provide insights into its broader applicability and robustness.

### Open Question 2
- Question: What is the impact of using different language models for sentiment analysis in the textual module of the MAT model?
- Basis in paper: [explicit] The authors use a domain-adapted financial BERT model for sentiment analysis, but they do not explore the effects of using other language models.
- Why unresolved: Different language models may capture different aspects of sentiment, potentially affecting the model's performance.
- What evidence would resolve it: Comparing the performance of MAT using various language models, such as RoBERTa, XLNet, or GPT, would reveal the optimal choice for sentiment analysis in financial text data.

### Open Question 3
- Question: How does the MAT model handle missing data in the input time series or text modalities?
- Basis in paper: [inferred] The paper does not explicitly address the issue of missing data, which is a common challenge in real-world time series forecasting.
- Why unresolved: Missing data can significantly impact the performance of time series forecasting models, and the MAT model's ability to handle such scenarios is unclear.
- What evidence would resolve it: Conducting experiments with artificially introduced missing data in the input modalities and evaluating the MAT model's performance would provide insights into its robustness to missing data.

## Limitations

- The model's effectiveness heavily depends on the quality and alignment of external data sources, particularly textual reports from the Federal Reserve.
- The specific preprocessing pipeline for text data (topic modeling, sentiment scoring) may not generalize well to other domains or data sources.
- The computational complexity of maintaining separate modality streams and multiple attention mechanisms may limit scalability to very large datasets or real-time applications.

## Confidence

**High Confidence**: The core architectural contributions (feature-level attention, modality-aware transformer structure) are well-defined and supported by mathematical formulations. The model's ability to outperform baseline methods on the specific financial forecasting task is demonstrated with clear metrics (MSE and MAE).

**Medium Confidence**: The generalizability of the approach to other domains or data types remains to be fully established. While the financial forecasting results are compelling, the specific mechanisms may not translate directly to other time series prediction problems without modification.

**Low Confidence**: The exact contribution of individual components to the overall performance gain is not definitively established. The ablation studies show performance differences but don't isolate the impact of feature-level attention versus modality-aware structure versus attention mechanisms.

## Next Checks

1. **Ablation Study**: Systematically remove each component (feature-level attention, inter-modal MHA, modality-target MHA) to quantify their individual contributions to performance gains. This would clarify which mechanisms are essential versus beneficial.

2. **Cross-Domain Testing**: Apply MAT to a non-financial time series forecasting problem (e.g., energy consumption or weather prediction) with different types of external data to validate generalizability. This would test whether the modality-aware approach transfers beyond financial applications.

3. **Attention Pattern Analysis**: Visualize and analyze the learned attention weights across features, modalities, and time steps to verify that the model is focusing on meaningful patterns. This would confirm whether the attention mechanisms are learning interpretable and useful relationships between modalities and the target series.