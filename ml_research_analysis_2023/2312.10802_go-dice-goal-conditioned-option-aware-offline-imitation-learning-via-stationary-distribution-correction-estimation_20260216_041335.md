---
ver: rpa2
title: 'GO-DICE: Goal-Conditioned Option-Aware Offline Imitation Learning via Stationary
  Distribution Correction Estimation'
arxiv_id: '2312.10802'
source_url: https://arxiv.org/abs/2312.10802
tags:
- learning
- go-dice
- task
- tasks
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GO-DICE, a goal-conditioned option-aware
  offline imitation learning approach that learns hierarchical policies from expert
  demonstrations. The key idea is to segment demonstrations into sub-tasks, learn
  separate policies for sub-task transitions and action execution, and use stationary
  distribution correction estimation for policy learning.
---

# GO-DICE: Goal-Conditioned Option-Aware Offline Imitation Learning via Stationary Distribution Correction Estimation

## Quick Facts
- arXiv ID: 2312.10802
- Source URL: https://arxiv.org/abs/2312.10802
- Authors: Not specified
- Reference count: 22
- Key outcome: Outperforms recent baselines in task completion rate for robotic manipulation tasks, particularly for long-horizon tasks.

## Executive Summary
GO-DICE introduces a goal-conditioned option-aware offline imitation learning approach that learns hierarchical policies from expert demonstrations. The method segments demonstrations into sub-tasks, learns separate policies for sub-task transitions and action execution, and uses stationary distribution correction estimation for policy learning. Experiments on robotic manipulation tasks show GO-DICE achieves superior performance, especially for long-horizon tasks with multiple objects.

## Method Summary
GO-DICE learns hierarchical policies by first segmenting demonstrations into K discrete options (sub-tasks) and then learning separate high-level and low-level policies for transitioning between sub-tasks and executing actions within sub-tasks. The approach uses stationary distribution correction estimation (DICE) to align the learned policy distribution with the expert policy distribution while incorporating goal conditioning. This enables zero-shot transfer to tasks with similar sub-task structures but different goals.

## Key Results
- Outperforms BC, GoFAR, and g-DemoDICE baselines on Pick-and-Place tasks with 1-3 objects
- Achieves 90% task completion rate on single-object tasks
- Shows effective transfer of low-level policies from simpler to more complex tasks without additional training

## Why This Works (Mechanism)

### Mechanism 1
- Hierarchical segmentation into sub-tasks allows efficient long-horizon reasoning
- Core assumption: Sub-tasks can be identified and labeled consistently across demonstrations
- Evidence: Algorithm iteratively segments demonstrations and learns separate policies
- Break condition: Poor segmentation or poorly chosen K degrades performance

### Mechanism 2
- Goal conditioning enables zero-shot transfer to tasks with similar sub-task structures
- Core assumption: Sub-task structure remains consistent across different goals
- Evidence: Both policies are learned with goal conditioning to minimize retraining
- Break condition: Goals significantly change sub-task structure or goal space is too diverse

### Mechanism 3
- Stationary distribution correction estimation enables stable learning from imperfect demonstrations
- Core assumption: Stationary distribution ratio between expert and imperfect demonstrations can be estimated reliably
- Evidence: DICE framework minimizes KL divergence with regularization
- Break condition: Poor demonstrations or mis-specified regularization coefficient α causes instability

## Foundational Learning

- **Options framework (hierarchical RL)**: Provides theoretical foundation for decomposing long-horizon tasks into sub-tasks with separate policies
  - Quick check: What is the key difference between primitive actions and options in the options framework?

- **Stationary distributions in MDPs**: DICE methods optimize over stationary distributions to align learned policies with expert behavior without environment interaction
  - Quick check: How does the stationary distribution of a policy relate to its state-action occupancy?

- **Goal-conditioned policies**: Enables learning unified policies that can handle varying task goals without retraining
  - Quick check: What is the advantage of representing goals as part of the policy input rather than training separate policies per goal?

## Architecture Onboarding

- **Component map**: Demonstrations → Segmentation → Policy Learning → Evaluation
- **Critical path**: Segment demonstrations into K options → Learn high-level policy πH(c|s, c', g) and low-level policy πL(a|s, c, g) → Estimate discriminator Ψ(c', s, c, a; g) → Optimize with Lagrange multipliers ν
- **Design tradeoffs**: Option count K vs. expressiveness vs. learning complexity; imperfect demonstration weighting α vs. stability vs. bias; target network update frequency M vs. convergence speed vs. stability
- **Failure signatures**: Poor segmentation (uniform option activation, no learning improvement); goal conditioning failure (poor performance on novel goals); DICE optimization failure (exploding gradients, unstable training)
- **First 3 experiments**: 1) Single-object PnP task: baseline comparison with BC and g-DemoDICE; 2) Two-object PnP task: evaluate hierarchical reasoning with K=3 options; 3) Semi-supervised variant: test performance boost from expert sub-task annotations

## Open Questions the Paper Calls Out

- **Performance degradation with multiple objects**: The paper acknowledges that performance degrades in three-object tasks and suggests potential reasons like close object proximity and unintended collisions, but doesn't systematically investigate the underlying causes.

- **Generalization to other domains**: The paper acknowledges that evaluations are limited to the robotics domain and encourages replication studies in other domains to demonstrate generality.

- **Offline pre-training for online methods**: The paper suggests GO-DICE could pre-train low-level policies that online methods could use, but this potential application is mentioned as future work without experimental validation.

## Limitations
- Performance degrades as the number of objects increases, particularly in three-object tasks
- Method assumes sub-tasks can be reliably identified across demonstrations
- Stationary distribution estimation relies on accurate state visitation patterns from limited offline data

## Confidence
- **High Confidence**: Hierarchical decomposition approach and empirical improvements over baselines on robotic manipulation tasks
- **Medium Confidence**: Zero-shot transfer capability of goal-conditioned policies to new task configurations
- **Low Confidence**: Robustness to severely degraded demonstrations and sensitivity to hyperparameter choices (K and α)

## Next Checks
1. **Cross-task transfer validation**: Test GO-DICE on a new robotic task (e.g., bin-packing) to assess whether learned low-level policies generalize to substantially different goal configurations without retraining.

2. **Robustness to segmentation errors**: Intentionally corrupt demonstration segmentations and measure the degradation in performance to establish failure bounds for the Viterbi-style segmentation approach.

3. **Hyperparameter sensitivity analysis**: Systematically vary K and α across multiple orders of magnitude to identify stable operating regions and quantify the impact on task completion rates.