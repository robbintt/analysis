---
ver: rpa2
title: 'CORECODE: A Common Sense Annotated Dialogue Dataset with Benchmark Tasks for
  Chinese Large Language Models'
arxiv_id: '2312.12853'
source_url: https://arxiv.org/abs/2312.12853
tags:
- commonsense
- knowledge
- event
- llms
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces CORECODE, a Chinese dialogue dataset with\
  \ 76,787 manually annotated commonsense knowledge instances spanning three dimensions:\
  \ entity, event, and social interaction. A two-level taxonomy with 9 domains and\
  \ 37 slots standardizes annotations in the form \"domain: slot = value.\" Six benchmark\
  \ tasks\u2014commonsense knowledge filling, generation, conflict phrase detection,\
  \ domain identification, slot identification, and event causal inference\u2014evaluate\
  \ Chinese LLMs\u2019 commonsense reasoning and conflict detection."
---

# CORECODE: A Common Sense Annotated Dialogue Dataset with Benchmark Tasks for Chinese Large Language Models

## Quick Facts
- **arXiv ID**: 2312.12853
- **Source URL**: https://arxiv.org/abs/2312.12853
- **Reference count**: 20
- **Key outcome**: Introduces CORECODE, a Chinese dialogue dataset with 76,787 annotated commonsense knowledge instances across 9 domains and 37 slots, evaluating 14 Chinese LLMs on 6 benchmark tasks reveals poor performance with zero-shot accuracies of 0.275 and 0.084 for domain and slot identification respectively.

## Executive Summary
This paper introduces CORECODE, a Chinese dialogue dataset with 76,787 manually annotated commonsense knowledge instances spanning three dimensions: entity, event, and social interaction. A two-level taxonomy with 9 domains and 37 slots standardizes annotations in the form "domain: slot = value." Six benchmark tasks—commonsense knowledge filling, generation, conflict phrase detection, domain identification, slot identification, and event causal inference—evaluate Chinese LLMs' commonsense reasoning and conflict detection. Experiments with 14 models show all perform poorly: zero-shot accuracies on domain and slot identification are only 0.275 and 0.084 for ChatGPT, and even fine-tuned models degrade significantly under perturbations, revealing robustness issues. The dataset and code are publicly released to advance commonsense reasoning research in dialogue contexts.

## Method Summary
The paper introduces CORECODE, a dataset for evaluating commonsense reasoning in Chinese large language models (LLMs). It includes six benchmark tasks: commonsense knowledge filling, generation, conflict phrase detection, domain identification, slot identification, and event causal inference. The dataset consists of 76,787 annotated instances from 19,700 dialogues, each containing original dialogue, an entity/event/social interaction instance, a commonsense knowledge represented by a domain-slot-value triplet, the involved phrase, and two commonsense conflict phrases. The paper evaluates the performance of various Chinese LLMs on these tasks using metrics such as accuracy, F1 score, and generation metrics (BLEU, METEOR, ROUGE, CIDEr). Both pre-trained and fine-tuned models are tested, with fine-tuning performed using LoRA. Robustness is tested by introducing perturbations to task inputs.

## Key Results
- All evaluated models show poor performance on CORECODE tasks, with ChatGPT achieving only 0.275 and 0.084 accuracy for domain and slot identification in zero-shot settings
- Fine-tuned models show better performance than pre-trained models but still struggle, particularly on conflict phrase detection tasks
- Performance degrades significantly under perturbations, revealing robustness issues in commonsense reasoning
- Instruction-tuned models generally outperform pre-trained models, but task-specific variations exist across the six benchmark tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The two-level taxonomy (domain:slot=value) standardizes commonsense annotations and ensures consistency across crowdsourced workers.
- **Mechanism**: By providing a predefined ontology with 9 domains and 37 slots, annotators can map diverse commonsense knowledge into a uniform format, reducing ambiguity and improving inter-annotator agreement.
- **Core assumption**: The taxonomy comprehensively covers all commonsense knowledge types in dialogues without overlap or gaps.
- **Evidence anchors**:
  - [abstract]: "A total of 9 domains and 37 slots are defined to capture diverse commonsense knowledge."
  - [section]: "We categorize commonsense knowledge in everyday conversations into three dimensions: entity, event, and social interaction... We define different slots for each domain."
  - [corpus]: Weak - the related papers focus on benchmark surveys rather than annotation methodology validation.
- **Break condition**: If the taxonomy misses relevant commonsense categories, annotators will need to use ad-hoc representations, breaking consistency.

### Mechanism 2
- **Claim**: Providing commonsense conflict phrases (minimal and maximum modification) enables targeted evaluation of models' ability to detect contextually inconsistent knowledge.
- **Mechanism**: By replacing original dialogue phrases with annotated conflict phrases and asking models to identify them, the evaluation directly tests models' commonsense reasoning rather than just knowledge retrieval.
- **Core assumption**: The conflict phrases are sufficiently similar to original phrases to require genuine commonsense reasoning for detection.
- **Evidence anchors**:
  - [abstract]: "We manually provide phrases corresponding to the phrases in an original dialogue, which are against common sense in that context."
  - [section]: "For each set of phrases...annotators are required to choose one phrase and provide it with the following two commonsense conflict phrases: Commonsense Conflict Phrase 1...Commonsense Conflict Phrase 2..."
  - [corpus]: Missing - no direct evidence that this approach improves evaluation quality over standard detection tasks.
- **Break condition**: If conflict phrases are too obviously incorrect or too subtly modified, the task becomes either trivial or unfairly difficult.

### Mechanism 3
- **Claim**: The HARD vs EASY dataset distinction (using "x"/"y" vs "A"/"B" for event subjects) creates graduated difficulty levels that stress-test models' reasoning capabilities.
- **Mechanism**: By removing explicit speaker references in the HARD set, models must first deduce event initiators before applying commonsense reasoning, adding a reasoning layer beyond simple knowledge lookup.
- **Core assumption**: This abstraction forces models to demonstrate true reasoning rather than pattern matching on speaker identifiers.
- **Evidence anchors**:
  - [section]: "We propose two distinct annotated subsets with varying difficulty levels...the HARD set...requires LLMs to first deduce and locate the event initiator before reasoning."
  - [abstract]: No direct mention of difficulty distinction in evaluation strategy.
  - [corpus]: No evidence that this distinction meaningfully impacts model performance or evaluation validity.
- **Break condition**: If models simply learn to map "x" and "y" to speakers without actual reasoning, the difficulty distinction becomes superficial.

## Foundational Learning

- **Concept**: Two-level hierarchical taxonomy design
  - **Why needed here**: Provides structured framework for consistent commonsense annotation across diverse dialogue content
  - **Quick check question**: Can you explain why having both domains and slots (rather than just one level) improves annotation consistency?

- **Concept**: Commonsense conflict phrase generation principles
  - **Why needed here**: Enables creation of evaluation data that tests detection rather than just knowledge retrieval
  - **Quick check question**: What's the difference between minimal modification and maximum modification principles for conflict phrases?

- **Concept**: Event causal inference task formulation
  - **Why needed here**: Tests deeper reasoning ability beyond simple knowledge lookup by requiring models to generate causes and consequences
  - **Quick check question**: Why are event causal inference tasks formulated as generation tasks rather than multiple choice?

## Architecture Onboarding

- **Component map**: Data annotation pipeline → Dataset curation → Task formulation → Model evaluation framework → Results analysis
- **Critical path**: Annotation quality control → Task definition → Model evaluation setup → Performance analysis
- **Design tradeoffs**: Comprehensive ontology coverage vs. annotation complexity; multiple task types vs. evaluation consistency; automatic vs. manual evaluation methods
- **Failure signatures**: Low inter-annotator agreement suggests taxonomy issues; poor model performance could indicate task formulation problems or genuine reasoning difficulty; inconsistent evaluation metrics across tasks
- **First 3 experiments**:
  1. Evaluate inter-annotator agreement on sample annotations to validate taxonomy design
  2. Test automatic evaluation filtering measures on small sample to ensure they don't underestimate performance
  3. Run baseline models on EASY vs HARD sets to confirm difficulty distinction is meaningful

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the representation of subjects and predicates (using "A" and "B" vs "x" and "y") affect LLM performance on commonsense reasoning tasks?
- Basis in paper: [explicit] The paper compares performance on EASY set (using "A" and "B") vs HARD set (using "x" and "y") and finds LLMs perform better on the EASY set, especially for event-related tasks.
- Why unresolved: While the paper demonstrates a performance difference, it does not investigate why this representation impacts reasoning ability or explore methods to mitigate this difficulty.
- What evidence would resolve it: Experiments comparing various representations (e.g., "A" and "B", "x" and "y", named entities, pronouns) and analyzing model attention patterns when processing different representations.

### Open Question 2
- Question: What is the relationship between model size, instruction-tuning, and commonsense reasoning performance across different task types?
- Basis in paper: [explicit] The paper compares various models (pre-trained only vs instruction-tuned) and notes that instruction-tuned models generally outperform pre-trained ones, but also observes task-specific variations.
- Why unresolved: The paper doesn't analyze whether performance differences stem from model capacity, instruction-following ability, or task-specific knowledge, nor does it examine scaling trends.
- What evidence would resolve it: Systematic evaluation of models across different sizes and training regimes on all task types, combined with analysis of model internal representations and attention patterns.

### Open Question 3
- Question: How robust are LLM commonsense reasoning abilities to perturbations beyond the option re-indicating and shuffling tested in the paper?
- Basis in paper: [explicit] The paper demonstrates significant performance drops under option re-indicating and shuffling perturbations but suggests these may be just one aspect of robustness.
- Why unresolved: The paper only tests two specific perturbation types and doesn't explore other potential vulnerabilities like paraphrasing, negation, or contextual modifications.
- What evidence would resolve it: Comprehensive robustness testing using diverse perturbation strategies, including natural language variations and adversarial examples, across all task types.

## Limitations

- The taxonomy's comprehensiveness remains uncertain with no validation that it covers all relevant commonsense knowledge types in dialogue contexts
- The difficulty distinction between HARD and EASY sets needs empirical validation to confirm it meaningfully stresses models' reasoning capabilities
- The paper lacks detailed ablation studies showing how individual taxonomy components contribute to annotation quality and model performance
- Limited exploration of alternative perturbation strategies beyond option re-indicating and shuffling

## Confidence

- **High confidence**: The dataset construction methodology and annotation format are clearly specified and reproducible
- **Medium confidence**: The taxonomy design effectively captures diverse commonsense knowledge types, though completeness needs validation
- **Medium confidence**: The conflict phrase generation approach enables targeted evaluation of commonsense reasoning
- **Low confidence**: The HARD vs EASY difficulty distinction meaningfully tests different reasoning capabilities
- **Medium confidence**: Performance degradation under perturbations reveals genuine robustness issues rather than task formulation problems

## Next Checks

1. **Inter-annotator agreement validation**: Measure consistency scores across multiple annotators on sample annotations to empirically validate that the two-level taxonomy reduces ambiguity and improves annotation quality
2. **Taxonomy coverage analysis**: Conduct expert review to identify potential commonsense knowledge types missing from the 9 domains and 37 slots, and test whether models can handle knowledge outside this structure
3. **Difficulty distinction verification**: Run controlled experiments comparing model performance on HARD vs EASY sets with controlled variables to confirm the difficulty difference stems from reasoning complexity rather than other factors