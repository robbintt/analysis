---
ver: rpa2
title: 'ChatGPT as Data Augmentation for Compositional Generalization: A Case Study
  in Open Intent Detection'
arxiv_id: '2308.13517'
source_url: https://arxiv.org/abs/2308.13517
tags:
- card
- intent
- instance
- chatgpt
- change
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of compositional generalization
  in open intent detection by leveraging ChatGPT for data augmentation. The authors
  construct compositionally diverse subsets from existing benchmarks and generate
  synthetic paraphrases using ChatGPT, which are then incorporated into the training
  process through three strategies: synthesizing 10 paraphrases per instance, 4 paraphrases
  per instance, and 10 paraphrases per incorrectly predicted instance.'
---

# ChatGPT as Data Augmentation for Compositional Generalization: A Case Study in Open Intent Detection

## Quick Facts
- arXiv ID: 2308.13517
- Source URL: https://arxiv.org/abs/2308.13517
- Reference count: 40
- Primary result: F1 score improvements of 2-5% on compositionally diverse subsets compared to baseline methods

## Executive Summary
This paper addresses compositional generalization in open intent detection by leveraging ChatGPT for data augmentation. The authors construct compositionally diverse subsets from existing benchmarks and generate synthetic paraphrases using ChatGPT, which are then incorporated into training through three strategies: synthesizing 10 paraphrases per instance, 4 paraphrases per instance, and 10 paraphrases per incorrectly predicted instance. Experiments on Banking_CG, OOS_CG, and StackOverflow_CG show that ADB with ChatGPT data augmentation significantly outperforms baseline methods, demonstrating that synthetically generated paraphrases can effectively improve model performance on compositional generalization tasks.

## Method Summary
The approach involves constructing compositionally diverse datasets by pruning similar utterance pairs using Rouge-L scores, then generating paraphrases with ChatGPT using three strategies: GPTAUG-F4 (4 paraphrases per instance), GPTAUG-F10 (10 paraphrases per instance), and GPTAUG-WP10 (10 paraphrases per wrongly predicted instance). These augmented datasets are used to train BERT models with Adaptive Decision Boundary (ADB) and Distance-Aware ADB (DA-ADB) frameworks. The training process iterates until evaluation accuracy no longer improves, with final performance measured on test sets.

## Key Results
- GPTAUG-F10 achieves highest F1 score improvements of 2-5% across all three datasets (Banking_CG, OOS_CG, StackOverflow_CG)
- DA-ADB consistently outperforms ADB across all evaluation metrics and datasets
- GPTAUG-WP10 sometimes underperforms GPTAUG-F10 despite focusing on error instances

## Why This Works (Mechanism)

### Mechanism 1
ChatGPT-generated paraphrases provide compositional diversity that bridges training-test gaps by exposing the model to alternative phrasings for the same semantic intent. This introduces novel combinations of language components during training that may appear in test data. Break condition: If paraphrases change semantic meaning or fail to introduce compositional variety.

### Mechanism 2
Iterative training with synthetic data improves model robustness to unseen compositions by refining the model's understanding through synthetic examples of misclassified instances. This targets specific weaknesses in the decision boundary. Break condition: If iterative training causes overfitting or fails to improve generalization.

### Mechanism 3
Compositionally diverse datasets enable proper evaluation of generalization capabilities by forcing models to handle truly novel combinations rather than memorized patterns. Rouge-L pruning ensures minimal overlap between training and test instances. Break condition: If Rouge-L threshold selection is too conservative or too lenient.

## Foundational Learning

- **Compositional generalization**: Why needed: The paper's core problem is that models fail when encountering new combinations of known language components. Quick check: Can a model that learns "book flight to Paris" and "book hotel in London" correctly handle "book flight to London"?

- **Data augmentation strategies**: Why needed: Different augmentation approaches (fixed number vs. error-focused) have varying impacts on model performance. Quick check: Why might generating paraphrases only for misclassified instances sometimes underperform generating for all instances?

- **Iterative model refinement**: Why needed: The training process continues until convergence, requiring understanding of when to stop. Quick check: What metric should be monitored to determine when iterative training has converged?

## Architecture Onboarding

- **Component map**: Dataset construction (Rouge-L pruning) -> ChatGPT paraphrase generation -> BERT training with ADB/DA-ADB -> Evaluation pipeline
- **Critical path**: Dataset construction → ChatGPT augmentation → Model training → Evaluation
- **Design tradeoffs**: More paraphrases per instance increases diversity but computational cost; error-focused augmentation targets weaknesses but may miss general patterns; dataset pruning increases diversity but reduces training data size
- **Failure signatures**: No improvement in F1 scores despite augmentation; model overfitting to synthetic examples; evaluation scores plateauing too early
- **First 3 experiments**: 1) Baseline comparison: ADB vs DA-ADB on original datasets; 2) Ablation study: GPTAUG-F4 vs GPTAUG-F10 vs GPTAUG-WP10; 3) Threshold sensitivity: Test different Rouge-L thresholds for dataset construction

## Open Questions the Paper Calls Out

### Open Question 1
How do different prompt engineering strategies for ChatGPT affect the quality and diversity of generated paraphrases for compositional generalization? Basis: The paper mentions future research should focus on "designing better-instructed prompts for ChatGPT to encourage more diverse paraphrases." Why unresolved: The paper does not explore different prompt engineering strategies or compare their effectiveness. What evidence would resolve it: Experiments comparing different prompt engineering strategies and their impact on diversity and quality, measured through automated metrics and human evaluation.

### Open Question 2
What is the optimal strategy for incorporating ChatGPT-generated paraphrases into the training process of open intent detection models? Basis: The paper evaluates three strategies but suggests exploring alternative strategies may lead to further performance improvements. Why unresolved: The paper only explores three basic strategies without investigating more sophisticated approaches. What evidence would resolve it: Experiments comparing different strategies including curriculum learning, adaptive sampling, or active learning.

### Open Question 3
How does the size and diversity of the ChatGPT-generated paraphrases affect the model's ability to handle compositional generalization in open intent detection? Basis: The paper uses fixed numbers of paraphrases but does not explore the impact of varying the number or diversity of paraphrases. Why unresolved: The paper does not investigate the relationship between paraphrase size/diversity and model performance. What evidence would resolve it: Experiments varying the number and diversity of ChatGPT-generated paraphrases and measuring their impact on model performance.

## Limitations

- Unknown implementation details for ChatGPT prompt templates and ADB/DA-ADB frameworks prevent faithful reproduction
- Dataset construction methodology lacks systematic evaluation of Rouge-L threshold sensitivity
- Results demonstrated on three specific datasets may not generalize to other domains or languages

## Confidence

- **High Confidence**: Core claim that ChatGPT data augmentation improves compositional generalization has strong experimental support with consistent F1 score improvements across all datasets
- **Medium Confidence**: Effectiveness of three augmentation strategies demonstrated, though relative performance differences lack consistent explanation
- **Low Confidence**: Claims about specific mechanisms lack direct experimental validation; no ablation studies isolating individual component contributions

## Next Checks

1. **Ablation Study on Prompt Engineering**: Conduct experiments varying ChatGPT prompt templates systematically to determine their impact on paraphrase quality and compositional diversity. Measure Rouge-L scores between original and generated paraphrases to quantify diversity introduced.

2. **Threshold Sensitivity Analysis**: Evaluate model performance across a range of Rouge-L thresholds (0.1, 0.2, 0.3, 0.4) for dataset construction to determine optimal balance between compositional diversity and dataset size.

3. **Cross-Domain Generalization Test**: Apply the approach to datasets from different domains (healthcare, e-commerce) and languages to assess generalizability. Compare performance on domain-specific compositional patterns versus general language constructions.