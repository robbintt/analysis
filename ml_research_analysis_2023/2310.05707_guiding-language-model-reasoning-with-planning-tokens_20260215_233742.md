---
ver: rpa2
title: Guiding Language Model Reasoning with Planning Tokens
arxiv_id: '2310.05707'
source_url: https://arxiv.org/abs/2310.05707
tags:
- planning
- reasoning
- tokens
- arxiv
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving the reasoning abilities
  of large language models (LLMs) in complex tasks like math problems. The core idea
  is to introduce planning tokens at the start of each reasoning step, which serve
  as high-level plans for the next step.
---

# Guiding Language Model Reasoning with Planning Tokens

## Quick Facts
- arXiv ID: 2310.05707
- Source URL: https://arxiv.org/abs/2310.05707
- Reference count: 29
- Key outcome: Introduces planning tokens to improve LLM reasoning on math problems, achieving 3.3% average accuracy improvement across three datasets

## Executive Summary
This paper addresses the challenge of improving reasoning abilities in large language models for complex tasks like math word problems. The authors propose a novel approach that introduces planning tokens at the start of each reasoning step, serving as high-level plans that guide the model's step-by-step reasoning process. By fine-tuning LLMs with these planning tokens, the method achieves notable accuracy improvements compared to standard fine-tuning baselines across three different model sizes (Phi 1.5, 7B Llama 2, and 13B Llama 2) and three math word problem datasets.

## Method Summary
The approach works by first splitting ground-truth reasoning steps using a delimiter token, then inferring planning tokens for each step using arithmetic operators, K-Means clustering, or Soft Q-VAE clustering. These planning tokens are added as new vocabulary entries to the LLM, which is then fine-tuned on the augmented dataset to predict both planning tokens and reasoning steps. The method can use either full fine-tuning or LoRA parameter-efficient tuning. During inference, the model generates planning tokens first, then uses them to guide the generation of corresponding reasoning steps, creating a hierarchical generation scheme that mirrors human reasoning processes.

## Key Results
- Average accuracy improvement of 3.3% across three math word problem datasets (GSM8K, MATH, AQUA-RAT)
- Outperformed standard fine-tuning baselines on all three evaluated LLMs (Phi 1.5, Llama2-7B, Llama2-13B)
- Planning tokens helped reduce error rates in math reasoning by providing structured guidance
- The method showed consistent improvements across different clustering approaches (K-Means and Soft Q-VAE)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Planning tokens act as discrete latent variables that guide the LLM's reasoning steps toward higher-level structural coherence
- Mechanism: By prepending a planning token to each reasoning step, the model first commits to a high-level plan (e.g., the type of arithmetic operation) before generating the detailed reasoning. This hierarchical generation enforces a structure that mirrors human reasoning processes.
- Core assumption: The discrete latent variable captures meaningful structure in the reasoning steps that is predictive of correct step generation
- Evidence anchors: [abstract] mentions hierarchical generation scheme; [section] describes planning tokens as verbalization of discrete latent variables
- Break condition: If planning token inference is poor, the guidance becomes noise rather than signal, leading to degraded performance

### Mechanism 2
- Claim: Planning tokens increase the computational space available to the model, allowing more deliberation before generating each reasoning step
- Mechanism: By inserting additional tokens at the start of each reasoning step, the model can use more of its attention capacity to plan the step, reducing the likelihood of compounding errors
- Core assumption: The model can effectively utilize the additional tokens as "scratch space" for reasoning
- Evidence anchors: [section] describes fine-tuning on augmented dataset to predict planning tokens first
- Break condition: If additional tokens do not meaningfully contribute to step planning, performance gain will be minimal or negative

### Mechanism 3
- Claim: Clustering-based planning token inference captures semantic similarity between reasoning steps better than fixed heuristics
- Mechanism: By clustering reasoning step embeddings, the model learns to group semantically similar steps, leading to more specialized planning tokens that guide reasoning more effectively
- Core assumption: The embedding space captures meaningful semantic differences between reasoning steps that are predictive of correct reasoning flow
- Evidence anchors: [section] experiments with K-Means and learnt non-linear clustering methods
- Break condition: If clustering does not capture meaningful semantic structure, planning tokens will not provide effective guidance

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: The paper builds on CoT as the baseline reasoning method and aims to improve its consistency and accuracy
  - Quick check question: What is the key difference between standard fine-tuning and CoT fine-tuning in the context of LLMs?

- Concept: Variational Autoencoders (VAEs)
  - Why needed here: The soft Q-VAE clustering method uses VAE principles to learn non-linear transformations of reasoning step embeddings
  - Quick check question: How does a VAE differ from a standard autoencoder in terms of latent space modeling?

- Concept: Expectation-Maximization (EM) algorithm
  - Why needed here: The paper frames the planning token inference as an EM-like process, first imputing latent variables then learning model parameters
  - Quick check question: What are the two main steps of the EM algorithm and how do they apply to the planning token framework?

## Architecture Onboarding

- Component map: Base LLM (Phi 1.5, Llama2 variants) -> Planning token embeddings (new vocabulary entries) -> Clustering model (K-Means or SQ-VAE) -> Fine-tuning pipeline (full or LoRA-based)

- Critical path: 1) Preprocess dataset: split reasoning into steps, extract planning tokens 2) Cluster reasoning steps to obtain planning token assignments 3) Extend LLM vocabulary with planning tokens 4) Fine-tune LLM on augmented dataset 5) Evaluate on held-out math word problems

- Design tradeoffs: Number of clusters (P) vs. granularity of planning guidance; Number of planning tokens per step (n prefix + n special) vs. computational overhead; Clustering method (K-Means vs. SQ-VAE) vs. inference quality

- Failure signatures: Planning tokens not improving accuracy over baseline; Model generating planning tokens that don't match actual reasoning step; Planning token assignments showing no semantic coherence when clustered

- First 3 experiments: 1) Implement arithmetic operator-based planning tokens and evaluate on GSM8K 2) Compare K-Means clustering with varying numbers of clusters on GSM8K 3) Implement SQ-VAE clustering and evaluate against K-Means baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the planning tokens approach perform on non-mathematical reasoning tasks like logical inference or multi-step decision making?
- Basis in paper: [inferred] The paper focuses on math word problems and does not explore other domains of reasoning
- Why unresolved: The paper's experiments are limited to mathematical reasoning datasets (GSM8K, MATH, AQUA-RAT)
- What evidence would resolve it: Empirical results showing performance of planning tokens on diverse reasoning tasks beyond mathematics

### Open Question 2
- Question: What is the optimal number of planning tokens to use for different types of reasoning tasks and model scales?
- Basis in paper: [explicit] The paper mentions that "it is not necessary to assign too many tokens to a planning type"
- Why unresolved: The paper does not provide systematic analysis of how number of planning tokens affects performance across different tasks and model sizes
- What evidence would resolve it: A comprehensive ablation study varying the number of planning tokens across different reasoning tasks and model scales

### Open Question 3
- Question: Can the planning tokens be learned end-to-end with the language model parameters, rather than being inferred through clustering or heuristics?
- Basis in paper: [explicit] The paper mentions that ideally they would use variational EM but considers it difficult due to discrete nature of latent variables
- Why unresolved: The paper relies on heuristic or clustering-based methods to infer the planning tokens
- What evidence would resolve it: Implementation and evaluation of an end-to-end learning approach for the planning tokens

## Limitations

- Limited generalizability beyond mathematical reasoning tasks to other domains requiring complex reasoning
- Reliance on quality of clustering algorithms without thorough justification of method choice
- Use of relatively small LLMs (up to 13B parameters) raises questions about scalability to larger models
- Computational overhead of adding planning tokens to each reasoning step not quantified
- Lack of ablation studies determining whether improvements come from planning guidance itself or increased token budget

## Confidence

**High confidence:** The experimental methodology is sound, with proper train-test splits and multiple baseline comparisons. The observed accuracy improvements (3.3% average across datasets) are statistically significant and reproducible.

**Medium confidence:** The claimed mechanisms for why planning tokens work (hierarchical generation, discrete latent variables, semantic clustering) are plausible but not definitively proven. The evidence supports correlation but not necessarily causation.

**Low confidence:** The generalizability of results to other reasoning domains beyond math word problems, and to larger language models beyond the 1.3B-13B parameter range tested.

## Next Checks

1. **Clustering quality analysis**: Conduct detailed analysis of semantic coherence of planning token clusters by sampling reasoning steps assigned to same token and evaluating whether humans would judge them as semantically similar.

2. **Ablation on token budget**: Create control experiment that adds same number of random tokens (instead of planning tokens) to each reasoning step and compare performance to determine whether improvements come from planning guidance itself or simply increased computational capacity per step.

3. **Cross-domain transfer**: Test planning token framework on non-mathematical reasoning tasks such as commonsense reasoning (StrategyQA), logical deduction (LogiQA), or multi-step decision making to validate whether approach generalizes beyond mathematical domain.