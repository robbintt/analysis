---
ver: rpa2
title: 'LegendreTron: Uprising Proper Multiclass Loss Learning'
arxiv_id: '2301.11695'
source_url: https://arxiv.org/abs/2301.11695
tags:
- function
- convex
- losses
- proper
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper extends proper loss learning from binary to multiclass
  problems by using gradients of convex functions to model the inverse canonical link.
  It provides necessary and sufficient conditions for composite functions to be gradients
  of convex functions and derives sufficient conditions for composite functions to
  be gradients of Legendre functions.
---

# LegendreTron: Uprising Proper Multiclass Loss Learning

## Quick Facts
- arXiv ID: 2301.11695
- Source URL: https://arxiv.org/abs/2301.11695
- Reference count: 35
- The paper extends proper loss learning from binary to multiclass problems using gradients of convex functions and FICNNs, achieving superior performance especially under label noise.

## Executive Summary
This paper introduces LegendreTron, a novel algorithm for learning proper canonical losses in multiclass classification by leveraging gradients of convex functions. The method extends the theory of proper losses beyond binary classification to the multiclass setting, addressing a significant gap in the literature. By using Fully Input Convex Neural Networks (FICNNs) to parameterize the inverse canonical link function, LegendreTron ensures that the learned loss is strictly proper, meaning the Bayes optimal decision rule aligns with the true posterior probabilities. Experimental results demonstrate that LegendreTron consistently outperforms multinomial logistic regression, particularly on datasets with more than 10 classes and under label noise conditions.

## Method Summary
LegendreTron learns proper canonical losses by parameterizing the inverse canonical link function as a composition of gradients of convex functions, specifically using FICNNs. The algorithm optimizes a composite function u∘v⁻¹, where u is a fixed squashing function (softmax+) and v⁻¹ is composed of FICNN layers that guarantee positive definite Hessians and invertibility. This design ensures the learned loss is strictly proper and can adapt to label noise by maintaining calibrated probability estimates. The method is trained jointly with the model parameters using standard optimization techniques like Adam with learning rate decay.

## Key Results
- LegendreTron consistently outperforms multinomial logistic regression on multiclass datasets with more than 10 classes.
- The method shows significant improvements under label noise conditions, especially at 20% and 50% noise levels.
- Experimental results on MNIST, FMNIST, KMNIST, aloi, and 15 LIBSVM/UCI/Statlog datasets validate the approach's effectiveness.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LegendreTron achieves superior performance by learning proper canonical losses that respect the true multiclass probability structure.
- Mechanism: The algorithm learns a composite inverse canonical link function u∘v⁻¹ where u is a fixed squashing function and v⁻¹ is a composition of gradients of convex functions (FICNNs). This ensures the learned loss is strictly proper, meaning the Bayes optimal decision rule aligns with the true posterior probabilities.
- Core assumption: The composite function u∘v⁻¹ maps to the projected probability simplex and is the gradient of a twice-differentiable and strictly convex function.
- Evidence anchors:
  - [abstract]: "we extend monotonicity to maps between RC−1 and the projected probability simplex ˜∆C−1 by using monotonicity of gradients of convex functions"
  - [section 4]: "If one can prove that u◦ v−1 maps to ˜∆C−1 and is the gradient of a twice-diﬀerentiable and strictly convex function, then one can set ( −˜L)∗ = f"
  - [corpus]: Weak evidence; related works focus on margin-based or surrogate losses but not on Legendre-based canonical link learning.
- Break condition: If the Jacobian of the composite function is not positive definite, the function cannot be a gradient of a convex function, breaking the properness guarantee.

### Mechanism 2
- Claim: The use of Fully Input Convex Neural Networks (FICNNs) ensures the learnable component v⁻¹ is invertible and has a positive definite Hessian, satisfying the conditions for being a gradient of a Legendre function.
- Mechanism: FICNNs are designed with positive weight layers and a quadratic term, guaranteeing strong convexity and invertibility of the gradient. Composing multiple FICNNs preserves these properties.
- Core assumption: Each FICNN layer maintains positive definiteness of the Hessian and invertibility of the gradient.
- Evidence anchors:
  - [section 5]: "we use the same architecture as Huang et al. which is described as ... each gi is strongly convex (and therefore strictly convex) with an invertible gradient and positive definite Hessian for all x∈ RC−1"
  - [section 4]: "Let f : RC−1→ S and g : RC−1→ RC−1 be diﬀerentiable where S⊆ RC−1, and Jf(x) and Jg(x) are symmetric and positive definite for all x∈ RC−1"
  - [corpus]: No direct evidence; related works do not use FICNNs for loss learning.
- Break condition: If the FICNN architecture is modified such that the Hessian becomes indefinite or the gradient non-invertible, the properness property fails.

### Mechanism 3
- Claim: LegendreTron is robust to label noise because the learned proper canonical loss adapts to the corrupted label distribution, maintaining calibrated probability estimates.
- Mechanism: By jointly learning the loss and the model, LegendreTron can adjust the decision boundary to account for label noise, unlike fixed losses like multinomial logistic regression.
- Core assumption: The properness of the learned loss allows it to remain calibrated even when the training labels are corrupted.
- Evidence anchors:
  - [abstract]: "Experimental results show that LegendreTron consistently outperforms multinomial logistic regression on multiclass datasets with more than 10 classes, especially under label noise"
  - [section 6]: "Our results in Table 2 show that LegendreTron outperforms multinomial logistic regression under a t-test at 99% significance for most datasets and label noise settings"
  - [corpus]: Weak evidence; no direct mention of noise robustness in related works.
- Break condition: If the label noise is too severe (e.g., η > 0.5), the learned loss may overfit to the noise, degrading performance.

## Foundational Learning

- Concept: Proper losses and their connection to Bregman divergences.
  - Why needed here: The paper builds on the theory that proper losses are uniquely characterized by their conditional Bayes risk being a concave function, and the connection to Bregman divergences ensures the loss is calibrated.
  - Quick check question: What is the Bregman divergence representation of a proper loss, and how does it relate to the conditional Bayes risk?

- Concept: Legendre functions and the (u,v)-geometric structure.
  - Why needed here: The algorithm uses Legendre functions to design canonical link functions, and the (u,v)-geometric structure allows flexible parameterization of the inverse canonical link as a composite function.
  - Quick check question: How does the (u,v)-geometric structure enable the design of proper canonical losses, and what role do Legendre functions play?

- Concept: Maximal cyclical monotonicity and its characterization via positive semi-definite Jacobians.
  - Why needed here: The paper provides conditions under which a composite function is the gradient of a convex function, relying on the equivalence between maximal cyclical monotonicity and positive semi-definite Jacobians.
  - Quick check question: What are the equivalent conditions for a composite function to be the gradient of a convex function, and how do they relate to monotonicity?

## Architecture Onboarding

- Component map:
  - Input: Data matrix X (N×p), labels y (N×1)
  - Linear predictor: W (C-1×p), b (C-1×1)
  - FICNNs: B networks, each with M layers of dimension H
  - Squashing function: u (softmax+)
  - Composite inverse link: v⁻¹ = ∇g₁ ∘ ∇g₂ ∘ ... ∘ ∇g_B
  - Output: Probability estimates ˆp = u(v⁻¹(WX + b))

- Critical path:
  1. Initialize W, b, and FICNN parameters θ
  2. For each iteration:
     - Compute v⁻¹ using current FICNN parameters
     - For each sample, compute z = WX + b and ˆp = u(v⁻¹(z))
     - Compute log-likelihood loss
     - Backpropagate to update W, b, and θ

- Design tradeoffs:
  - Using FICNNs vs. other invertible architectures: FICNNs guarantee positive definite Hessians but may be less expressive than normalizing flows.
  - Fixed squashing function vs. learned u: A fixed u (softmax+) simplifies the problem but may limit flexibility.

- Failure signatures:
  - NaN or Inf in probabilities: Likely due to numerical instability in the FICNN or softmax+ computation.
  - Poor convergence: Could indicate inappropriate learning rate or initialization.
  - Overfitting: May occur if the model is too complex relative to the dataset size.

- First 3 experiments:
  1. Verify the composite function u∘v⁻¹ maps to the probability simplex by checking the output range.
  2. Test the invertibility of v⁻¹ by ensuring the Jacobian determinant is non-zero.
  3. Compare the learned loss to multinomial logistic regression on a small multiclass dataset to confirm improved calibration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical limitations of using Legendre functions for multiclass proper loss learning?
- Basis in paper: [explicit] The paper derives necessary and sufficient conditions for composite functions to be gradients of convex functions and sufficient conditions for them to be gradients of Legendre functions.
- Why unresolved: The paper provides conditions but doesn't explore the limitations of Legendre functions in this context.
- What evidence would resolve it: A detailed analysis of cases where Legendre functions fail to capture the proper loss structure or where their use leads to computational intractability.

### Open Question 2
- Question: How does the choice of squashing function u impact the performance of LegendreTron?
- Basis in paper: [explicit] The paper mentions that the specification of u remains a modeling choice and provides softmax+ as a natural choice.
- Why unresolved: The paper doesn't explore the impact of different squashing functions on the algorithm's performance.
- What evidence would resolve it: Experiments comparing LegendreTron's performance with different squashing functions across various datasets and noise levels.

### Open Question 3
- Question: Can LegendreTron be extended to handle structured output spaces beyond the probability simplex?
- Basis in paper: [inferred] The paper focuses on the projected probability simplex but doesn't discuss extensions to other structured output spaces.
- Why unresolved: The paper's theoretical framework is built around the probability simplex, and it's unclear how it generalizes to other structures.
- What evidence would resolve it: A theoretical extension of LegendreTron's framework to handle structured output spaces like trees, sequences, or graphs, along with empirical validation.

### Open Question 4
- Question: What are the computational trade-offs of using fully input convex neural networks (FICNNs) in LegendreTron?
- Basis in paper: [explicit] The paper uses FICNNs to parameterize the inverse canonical link but doesn't discuss their computational implications.
- Why unresolved: The paper doesn't provide a detailed analysis of the computational complexity or training dynamics of FICNNs in this context.
- What evidence would resolve it: A comparative study of LegendreTron's computational efficiency against other methods using different network architectures, including training time, memory usage, and scalability to large datasets.

## Limitations

- The exact architecture details of the FICNN layers beyond the specified dimensions are not provided, which could affect reproducibility.
- The theoretical conditions for the composite function to be a gradient of a Legendre function are necessary but not sufficient in practice, as they depend on the specific implementation of the FICNN layers.
- The experimental results under label noise are promising but limited to specific noise levels and datasets, leaving open questions about performance under extreme noise.

## Confidence

- **High**: FICNN architecture guarantees positive definite Hessians and invertibility.
- **Medium**: Theoretical conditions for the composite function being a gradient of a Legendre function are provided.
- **Low**: Robustness to label noise is demonstrated but not theoretically justified.

## Next Checks

1. **Reproduce FICNN Implementation**: Implement the FICNN architecture as specified and verify that the Hessian remains positive definite and the gradient invertible for various input dimensions.
2. **Test Extreme Noise Levels**: Evaluate LegendreTron on datasets with label noise levels beyond 50% to assess the limits of its robustness.
3. **Compare with Alternative Architectures**: Compare LegendreTron's performance with other invertible architectures (e.g., normalizing flows) to determine if the FICNN-specific properties are essential for the observed improvements.