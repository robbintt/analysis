---
ver: rpa2
title: When Layers Play the Lottery, all Tickets Win at Initialization
arxiv_id: '2301.10835'
source_url: https://arxiv.org/abs/2301.10835
tags:
- pruning
- tickets
- winning
- network
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the Lottery Ticket Hypothesis (LTH) and
  pruning at initialization from the perspective of layer pruning. The authors first
  confirm that winning tickets exist when the pruning process removes entire layers
  from a trained dense network.
---

# When Layers Play the Lottery, all Tickets Win at Initialization

## Quick Facts
- arXiv ID: 2301.10835
- Source URL: https://arxiv.org/abs/2301.10835
- Authors: 
- Reference count: 9
- One-line primary result: Winning tickets can be found at initialization when pruning entire layers, achieving 2× faster training and 51% carbon reduction while maintaining accuracy and robustness.

## Executive Summary
This paper investigates the Lottery Ticket Hypothesis from the perspective of layer pruning, demonstrating that winning tickets exist when entire layers are removed from trained dense networks. The authors propose a method to discover these winning tickets at initialization, eliminating the need to train the initial dense network. Their approach significantly speeds up training and reduces carbon emissions while maintaining accuracy and improving robustness against adversarial and out-of-distribution examples.

## Method Summary
The authors develop a layer pruning approach that removes entire residual blocks from neural networks instead of individual weights or filters. They apply gradient-based importance criteria (SNIP, GraSP) at initialization to identify unimportant layers, then create sparse networks by removing these layers while maintaining architectural constraints. The method includes weight rewinding to early training epochs or random initialization, and evaluates performance across accuracy, training speed, carbon emissions, and robustness metrics.

## Key Results
- Layer pruning tickets achieve up to 2× faster training compared to dense networks
- Carbon emissions reduced by up to 51% while maintaining accuracy
- Subnetworks exhibit improved robustness against adversarial and out-of-distribution examples
- Winning tickets found at initialization without requiring dense network training

## Why This Works (Mechanism)

### Mechanism 1
Removing entire layers yields more efficient sparse subnetworks than removing individual filters by eliminating all filters within a layer, drastically reducing sequential computation and memory usage compared to filter pruning which only removes subsets of filters. This works in residual architectures due to identity mappings, but would cause accuracy collapse in architectures without residual connections.

### Mechanism 2
Winning tickets can be found at initialization when pruning removes layers because layer pruning at initialization identifies unimportant layers using gradient-based criteria (SNIP, GraSP) that remain effective even before training. The importance criteria designed for initialization can identify layers whose removal doesn't harm representational capacity, though they may fail to measure importance without training updates.

### Mechanism 3
Layer pruning tickets are more robust than dense networks to adversarial and out-of-distribution examples because the structural sparsity from layer removal creates inherent regularization that improves robustness without requiring adversarial training. The structural changes from removing layers alter internal representations in ways that reduce sensitivity to perturbations, though these robustness gains may not transfer to different attack types or distributions.

## Foundational Learning

- Concept: Residual connections and identity mappings
  - Why needed here: Understanding why layer pruning works in ResNet architectures but not plain networks
  - Quick check question: What happens to a ResNet's output if you remove a layer with identity mapping?

- Concept: Lottery Ticket Hypothesis and weight rewinding
  - Why needed here: Understanding how winning tickets maintain performance when rewound to different training epochs
  - Quick check question: Why does rewinding to early epochs often produce better winning tickets?

- Concept: Pruning criteria (SNIP, GraSP) for initialization
  - Why needed here: Knowing how to evaluate layer importance before training begins
  - Quick check question: How does SNIP calculate importance scores using gradients at initialization?

## Architecture Onboarding

- Component map:
  - Core pruning engine -> Weight rewinding module -> Performance tracker -> Robustness evaluator

- Critical path:
  1. Compute importance scores for all layers
  2. Select layers to remove based on pruning density
  3. Create new network architecture without removed layers
  4. Transfer weights from kept layers
  5. Train the sparse network
  6. Evaluate performance metrics

- Design tradeoffs:
  - Layer pruning vs filter pruning: Better efficiency but limited to residual architectures
  - Random initialization vs weight rewinding: Simpler but may lose performance benefits
  - Pruning density: Higher density = more efficiency but risk of accuracy loss

- Failure signatures:
  - Accuracy drops >1% from dense network baseline
  - Training speed doesn't improve as expected
  - Robustness metrics worsen compared to dense network
  - Layer removal violates architectural constraints (before/after downsampling)

- First 3 experiments:
  1. Run layer pruning on ResNet32 with GraSP criterion at p=1, measure accuracy and FLOPs
  2. Compare weight rewinding to random initialization for layer pruning tickets
  3. Test robustness of layer pruning tickets against CIFAR-C adversarial examples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of pruned structure (weights, filters, or layers) influence the existence and quality of winning tickets in the Lottery Ticket Hypothesis?
- Basis in paper: The authors state that "we believe the findings above open a new direction for research in LTH: the influence of the structure taken into account during the pruning process."
- Why unresolved: The paper focuses on layer pruning and demonstrates its advantages over filter pruning, but does not provide a comprehensive comparison of all three structure types.
- What evidence would resolve it: Experiments comparing winning ticket quality and existence across weight, filter, and layer pruning methods on multiple architectures and datasets.

### Open Question 2
- Question: What are the limitations of using SNIP and GraSP criteria for pruning at initialization when applied to layer pruning, and how can these limitations be addressed?
- Basis in paper: The authors note that "most criteria fail to measure importance without any training as the weights change drastically" and discuss the limitations of SNIP and GraSP for layer pruning.
- Why unresolved: The paper uses SNIP and GraSP as initial approaches but does not explore alternative criteria or modifications to these methods for layer pruning.
- What evidence would resolve it: Development and evaluation of new or modified criteria specifically designed for layer pruning at initialization, comparing their effectiveness to SNIP and GraSP.

### Open Question 3
- Question: How does layer pruning affect the robustness and generalization of neural networks to out-of-distribution examples, particularly in shallow architectures?
- Basis in paper: The authors observe that "on the ResNet32, however, we observe that the results become negative as a function of the pruning severity" regarding out-of-distribution generalization.
- Why unresolved: The paper demonstrates improved robustness for deep architectures but notes potential issues with shallow networks, leaving the underlying causes and solutions unclear.
- What evidence would resolve it: Detailed analysis of the relationship between layer pruning, architecture depth, and out-of-distribution robustness, including potential modifications to pruning strategies for shallow networks.

## Limitations
- Layer pruning approach is limited to residual architectures and cannot be directly applied to plain networks without identity mappings
- Performance degradation occurs in shallow architectures like ResNet32 when pruning at high severity
- The robustness improvements may not generalize to all types of adversarial attacks or out-of-distribution datasets

## Confidence
- Confidence in layer pruning approach: Medium-High for residual architectures, Low-Medium for non-residual architectures
- Confidence in winning tickets at initialization: Medium
- Confidence in robustness claims: Medium

## Next Checks
1. **Architecture Transferability**: Test layer pruning on non-residual architectures (e.g., VGG, DenseNet) to validate the claim that identity mappings are essential for this approach.

2. **Dataset Generalization**: Apply the layer pruning method to larger datasets like ImageNet to verify if the 2× speedup and 51% carbon reduction claims scale proportionally.

3. **Robustness Breadth**: Test robustness against a wider variety of adversarial attacks (PGD, DeepFool) and out-of-distribution datasets (ImageNet-C, corrupted versions of other datasets) to strengthen the robustness claims.