---
ver: rpa2
title: Improving the Robustness of Transformer-based Large Language Models with Dynamic
  Attention
arxiv_id: '2311.17400'
source_url: https://arxiv.org/abs/2311.17400
tags:
- attention
- adversarial
- dynamic
- uni00000048
- uni00000006
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces dynamic attention, a method to enhance the
  robustness of transformer-based language models against textual adversarial attacks.
  The approach dynamically masks or weakens the attention values of highly attended
  tokens, preventing the model from being misled by adversarial examples.
---

# Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention

## Quick Facts
- arXiv ID: 2311.17400
- Source URL: https://arxiv.org/abs/2311.17400
- Reference count: 40
- Primary result: Dynamic attention improves robustness against adversarial attacks by up to 33% in attack success rate

## Executive Summary
This paper introduces dynamic attention, a method to enhance the robustness of transformer-based language models against textual adversarial attacks. The approach dynamically masks or weakens the attention values of highly attended tokens, preventing the model from being misled by adversarial examples. Extensive experiments demonstrate that dynamic attention significantly improves robustness across various tasks (text classification and generation) and attack scenarios, while preserving the model's original performance and stability.

## Method Summary
Dynamic attention operates by accumulating attention maps across all heads in each transformer layer, ranking key tokens by total attention, and masking or weakening attention to top-ranked tokens. The method introduces randomness through dynamic modeling that varies the number of masked tokens across layers and runs. This creates a defense that prevents adversarial examples from manipulating attention values to mislead the model. The approach can be combined with other defense methods like dropout, creating a multi-layered defense system.

## Key Results
- Dynamic attention outperforms previous methods by up to 33% in terms of attack success rate
- The method maintains original model performance on clean data while improving robustness
- Dynamic attention is effective across multiple tasks (text classification and generation) and attack types
- Combining dynamic attention with dropout provides additive robustness benefits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic attention mitigates adversarial attacks by masking or weakening attention to highly attended tokens that differ between clean and adversarial inputs.
- Mechanism: The method accumulates attention maps across all heads in each layer, ranks key tokens by total attention, and masks or weakens attention to top-ranked tokens. This prevents the model from being misled by task-irrelevant tokens that receive high attention in adversarial examples.
- Core assumption: Adversarial examples manipulate the attention mechanism by causing task-irrelevant tokens to receive higher attention values than in clean inputs.
- Evidence anchors:
  - [abstract] states "dynamic attention consists of two modules: (i) attention rectification, which masks or weakens the attention value of the chosen tokens, and (ii) dynamic modeling, which dynamically builds the set of candidate tokens"
  - [section III-A] provides empirical evidence showing "tokens with high attention value in adversarial texts are different from those in their original texts" and demonstrates that replacing adversarial attention with clean attention helps the model classify correctly
  - [corpus] shows related work on attention-based defenses for transformers
- Break condition: If adversarial examples do not manipulate attention values differently between clean and adversarial inputs, or if masking critical tokens harms model performance.

### Mechanism 2
- Claim: Dynamic modeling increases attack difficulty by varying the number of masked tokens across layers and runs.
- Mechanism: Instead of using a fixed number of masked tokens, the method uses a random variable m that changes for each layer and each run. For text classification, m is drawn from a range proportional to text length; for text generation, ma is fixed while mb varies.
- Core assumption: Adversarial examples are inherently unstable and have low transferability between models, so dynamic variation makes them harder to re-apply.
- Evidence anchors:
  - [abstract] mentions "dynamic modeling, which dynamically builds the set of candidate tokens"
  - [section III-A] states "adversarial examples are inherently unstable" and "a fixed number of weakened or masked tokens cannot reduce the attack success rate"
  - [section V-A] shows dynamic attention increases attack queries significantly (e.g., from 234.44 to 320.66 for TextBugger on fine-tuned models)
- Break condition: If attackers can adapt to the dynamic pattern or if the randomness introduces too much instability in clean predictions.

### Mechanism 3
- Claim: Combining dynamic attention with dropout provides additive robustness benefits.
- Mechanism: The fusion model integrates both dynamic attention (masking based on attention values) and dropout (random neuron masking), creating multiple layers of defense that work synergistically.
- Core assumption: Dropout and dynamic attention defend against attacks through different mechanisms - dropout through gradient obfuscation and dynamic attention through attention manipulation.
- Evidence anchors:
  - [abstract] states "The model-level design of dynamic attention enables it to be easily combined with other defense methods (e.g., adversarial training) to further enhance the model's robustness"
  - [section V-A] shows fusion model achieves up to 33% better performance than defensive dropout alone
  - [section V-F] demonstrates fusion model maintains better stability than dropout alone (e.g., ASR increases from 33.48% to 55.66% vs 93.21% for dropout under multiple uploads)
- Break condition: If the combination creates interference between the two methods or if one method dominates and nullifies the other's benefits.

## Foundational Learning

- Concept: Transformer attention mechanism and multi-head attention
  - Why needed here: Dynamic attention operates directly on the attention mechanism by modifying attention values, so understanding how attention works in transformers is essential
  - Quick check question: How does multi-head attention in transformers compute attention scores, and what role does the softmax function play?

- Concept: Adversarial attacks in NLP (word-level vs character-level)
  - Why needed here: The method specifically targets word-level adversarial attacks that manipulate attention by replacing words with synonyms
  - Quick check question: What distinguishes word-level adversarial attacks like TextFooler from character-level attacks, and why are word-level attacks more challenging to detect?

- Concept: Dynamic neural networks and stochastic methods
  - Why needed here: Dynamic attention introduces randomness through variable masking, requiring understanding of how dynamic models differ from static ones
  - Quick check question: How does dynamic modeling in neural networks differ from traditional static architectures, and what are the trade-offs in terms of robustness vs performance?

## Architecture Onboarding

- Component map:
  Input layer → Token embeddings → Multi-head attention with dynamic masking → Feed-forward network → Output layer
  Dynamic attention module sits within each transformer layer, modifying attention computation before the feed-forward network
  Two key components: attention rectification (masking logic) and dynamic modeling (random variable generation)

- Critical path:
  1. Compute standard attention matrix for each head
  2. Sum attention maps across all heads to get global attention
  3. Rank key tokens by total attention received
  4. Select tokens to mask/weaken based on task type (classification vs generation)
  5. Apply reduction factor β to selected tokens
  6. Use modified attention for value matrix multiplication
  7. Repeat for each layer with new random m value

- Design tradeoffs:
  - Masking too many tokens vs not enough masking (balance between robustness and performance)
  - Static vs dynamic masking (stability vs adaptive defense)
  - Attention-based vs random masking (targeted defense vs gradient obfuscation)
  - Integration with existing components (dropout, adversarial training) vs standalone solution

- Failure signatures:
  - Performance degradation on clean data (masking too many important tokens)
  - Increased variance in predictions (too much randomness in dynamic modeling)
  - Ineffective against certain attack types (attacks that don't manipulate attention values)
  - Computational overhead (complex attention calculations in each layer)

- First 3 experiments:
  1. Baseline test: Run original model vs dynamic attention model on clean data to verify minimal performance impact
  2. Attack resistance test: Generate adversarial examples using TextFooler and measure ASR reduction with dynamic attention
  3. Dynamic behavior test: Verify that masking patterns change across layers and runs by logging attention modifications

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does dynamic attention perform against adaptive attacks that specifically target the attention mechanism?
- Basis in paper: [explicit] The paper mentions investigating two adaptive attack strategies toward the text classification task, but does not provide detailed results or analysis of the attacks' effectiveness.
- Why unresolved: The paper only briefly mentions the adaptive attacks and their results, without providing a comprehensive analysis of the attacks' effectiveness and potential countermeasures.
- What evidence would resolve it: Detailed results and analysis of the adaptive attacks' success rates, as well as potential countermeasures or improvements to dynamic attention to mitigate these attacks.

### Open Question 2
- Question: How does dynamic attention compare to other robustness enhancement methods, such as adversarial training or information bottleneck, in terms of computational efficiency and resource consumption?
- Basis in paper: [explicit] The paper mentions that dynamic attention is more efficient than adversarial training and does not require downstream task knowledge, but does not provide a detailed comparison of computational efficiency and resource consumption with other methods.
- Why unresolved: The paper does not provide a comprehensive comparison of dynamic attention with other robustness enhancement methods in terms of computational efficiency and resource consumption.
- What evidence would resolve it: A detailed comparison of dynamic attention with other robustness enhancement methods in terms of computational efficiency and resource consumption, including runtime analysis and memory usage.

### Open Question 3
- Question: How does dynamic attention perform on other types of adversarial attacks, such as character-level or sentence-level attacks, beyond the word-level attacks evaluated in the paper?
- Basis in paper: [explicit] The paper only evaluates dynamic attention against word-level attacks, but mentions that character-level and sentence-level attacks exist in the literature.
- Why unresolved: The paper does not provide any evaluation of dynamic attention against character-level or sentence-level attacks, which are also important types of adversarial attacks in NLP.
- What evidence would resolve it: Experimental results and analysis of dynamic attention's performance against character-level and sentence-level attacks, including success rates and potential countermeasures.

## Limitations

- The paper does not provide detailed implementation specifics for the dynamic attention mechanism, making faithful reproduction challenging
- Computational overhead for clean inference and training is not quantified, which could be significant for large-scale applications
- The evaluation is limited to specific datasets and tasks, requiring further validation for generalizability to other NLP domains

## Confidence

**High confidence**: The core claim that dynamic attention improves robustness against textual adversarial attacks is well-supported by extensive experiments across multiple datasets, attack methods, and threat models. The empirical evidence showing up to 33% improvement in ASR reduction is robust and reproducible.

**Medium confidence**: The claim that dynamic attention preserves original model performance while enhancing robustness is supported by experimental results, but the evaluation is limited to specific datasets and tasks. The generalizability to other NLP tasks and domains requires further validation.

**Low confidence**: The assertion that combining dynamic attention with dropout provides additive benefits relies on comparison with defensive dropout alone. The paper does not conduct ablation studies to isolate the individual contributions of each component, making it difficult to assess whether the combination is truly synergistic or if one method dominates.

## Next Checks

1. **Ablation study**: Implement and test dynamic attention with static masking (fixed m) versus dynamic masking to quantify the exact contribution of the dynamic modeling component. This would isolate whether the randomness is essential for robustness or if the attention rectification alone suffices.

2. **Adaptive attack evaluation**: Design an adaptive attack that specifically targets the dynamic attention mechanism by learning the pattern of token masking over multiple queries. This would test whether the dynamic approach provides robustness against sophisticated attackers who can adapt to the defense.

3. **Computational overhead measurement**: Benchmark the runtime performance of dynamic attention during clean inference and training compared to baseline models. This would provide concrete data on the practical trade-offs between robustness gains and computational costs, particularly important for deployment in resource-constrained environments.