---
ver: rpa2
title: 'Katakomba: Tools and Benchmarks for Data-Driven NetHack'
arxiv_id: '2306.08772'
source_url: https://arxiv.org/abs/2306.08772
tags:
- learning
- dataset
- nethack
- offline
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Katakomba provides open-source tools and benchmarks for data-driven
  reinforcement learning research on NetHack, addressing key obstacles to adopting
  existing large-scale datasets. The library offers pre-defined D4RL-style tasks,
  clean recurrent offline RL baseline implementations, reliable evaluation tools with
  cloud-synced configs and logs, and optimized data loaders balancing speed and memory
  usage.
---

# Katakomba: Tools and Benchmarks for Data-Driven NetHack

## Quick Facts
- arXiv ID: 2306.08772
- Source URL: https://arxiv.org/abs/2306.08772
- Reference count: 40
- Key outcome: Katakomba provides open-source tools and benchmarks for data-driven reinforcement learning research on NetHack, addressing key obstacles to adopting existing large-scale datasets

## Executive Summary
Katakomba is a comprehensive library that enables research on data-driven reinforcement learning for NetHack, one of the most challenging video game benchmarks. The library addresses key obstacles to using existing large-scale datasets by providing pre-defined D4RL-style tasks, clean implementations of recurrent offline RL baselines, reliable evaluation tools with cloud-synced configs and logs, and optimized data loaders that balance speed and memory usage. The system decomposes the original AutoAscend dataset into 38 smaller tasks based on character configurations, making experimentation more manageable.

## Method Summary
The Katakomba library provides three main components: pre-defined D4RL-style tasks, clean recurrent offline RL baseline implementations, and reliable evaluation tools with cloud-synced configurations and logs. The library offers three different data loaders that trade off memory and speed: in-memory (RAM), memory-mapped (disk), and compressed on-disk. The original AutoAscend dataset is decomposed into 38 smaller HDF5 files based on character configurations (role, race, and alignment), with a minimum of 100 trajectories per configuration. The evaluation uses RLiable tools with bootstrapped confidence intervals and performance profiles rather than simple mean/median statistics.

## Key Results
- Current offline RL algorithms achieve normalized scores below 6.0 on 38 decomposed NetHack datasets
- Behavioral cloning often matches or exceeds the performance of more sophisticated offline RL methods
- Most algorithms fail to progress beyond the first dungeon level in NetHack

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The modular data loader design enables controlled trade-offs between speed and memory usage.
- **Mechanism:** Katakomba offers three data loading modes: in-memory (RAM), memory-mapped (disk), and compressed on-disk. This lets users choose based on available resources, avoiding the slow on-the-fly decompression of the original TTYRec loader.
- **Core assumption:** Different users have different hardware constraints and speed requirements.
- **Evidence anchors:**
  - [abstract]: "optimized data loaders balancing speed and memory usage"
  - [section]: "three different loaders trading-off memory and speed"
  - [corpus]: Weak – no corpus mentions loader design.
- **Break condition:** If the decompressed dataset size exceeds available memory, the in-memory loader fails; if disk I/O is slow, memory-mapped loader performance degrades.

### Mechanism 2
- **Claim:** Decomposition of the large-scale dataset into smaller, role-based tasks improves focus and reduces resource requirements.
- **Mechanism:** By filtering and subsampling the original AutoAscend dataset into 38 smaller HDF5 files based on character configurations (role, race, alignment), Katakomba enables faster iteration and targeted experimentation.
- **Core assumption:** Role has the largest impact on gameplay, so focusing on role-centric tasks captures most learning challenges.
- **Evidence anchors:**
  - [section]: "we divide the tasks based on the character configurations: role, race, and alignment"
  - [abstract]: "38 decomposed datasets"
  - [corpus]: Weak – no corpus mentions dataset decomposition.
- **Break condition:** If a user needs data spanning multiple roles or configurations, they must download multiple datasets, increasing total storage needs.

### Mechanism 3
- **Claim:** Use of robust evaluation tools (RLiable) addresses high variability in NetHack returns.
- **Mechanism:** Instead of reporting mean/median returns, Katakomba uses bootstrapped estimates, performance profiles, and optimality gaps to compare algorithms fairly despite stochastic episode outcomes.
- **Core assumption:** RL algorithms exhibit high variability, especially in complex environments like NetHack, making simple averages misleading.
- **Evidence anchors:**
  - [section]: "we argue that the evaluation toolbox from Agarwal et al. (2021) is more appropriate"
  - [abstract]: "reliable evaluation tools with accompanying configs and logs synced to the cloud"
  - [corpus]: Weak – no corpus mentions evaluation methodology.
- **Break condition:** If the number of evaluation episodes is too small, bootstrapping may not stabilize estimates.

## Foundational Learning

- **Concept:** Reinforcement learning with recurrent policies
  - **Why needed here:** NetHack episodes are long (up to 100k steps) and require memory of past events; standard feed-forward policies struggle with this.
  - **Quick check question:** Why might a recurrent network (e.g., LSTM) be more suitable than a feed-forward network for NetHack?
- **Concept:** Offline reinforcement learning and behavioral cloning
  - **Why needed here:** The benchmark focuses on learning from pre-collected datasets without environment interaction; BC is a simple baseline and a core component of many ORL algorithms.
  - **Quick check question:** What is the main difference between online and offline RL in terms of data collection?
- **Concept:** Dataset decomposition and stratified sampling
  - **Why needed here:** Breaking a large dataset into smaller, focused tasks allows faster experimentation and clearer attribution of performance to specific character configurations.
  - **Quick check question:** Why is stratified sampling by game score useful when creating smaller datasets?

## Architecture Onboarding

- **Component map:** NetHackChallenge -> OfflineNetHackChallengeWrapper -> Data loaders (in-memory, memmap, compressed) -> RL algorithm modules (BC, CQL, AWAC, IQL, REM) -> Evaluation tools
- **Critical path:**
  1. Initialize environment with character configuration
  2. Wrap with OfflineNetHackChallengeWrapper
  3. Load dataset via get_dataset(mode=...)
  4. Train selected RL algorithm
  5. Evaluate using RLiable tools
- **Design tradeoffs:**
  - Speed vs. memory: in-memory fastest but requires large RAM; memmap slower but disk-based; compressed slowest but minimal disk usage
  - Dataset size vs. coverage: smaller datasets faster to load but may miss rare scenarios
  - Algorithm complexity vs. performance: simple BC is stable but may underperform more complex ORL methods
- **Failure signatures:**
  - Out-of-memory errors → switch to memmap or compressed loader
  - Slow iteration → switch to in-memory loader if RAM allows
  - Poor algorithm performance → check reward shaping, hyperparameters, or try BC as baseline
- **First 3 experiments:**
  1. Train BC on arc-hum-neu with in-memory loader; verify score normalization
  2. Compare BC vs. CQL on bar-hum-neu using memmap loader; evaluate with RLiable
  3. Test IQL on mon-hum-neu with compressed loader; check for reward shaping issues

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do offline RL algorithms fail to outperform behavioral cloning on NetHack despite using more sophisticated techniques?
- Basis in paper: [explicit] "Experiments on 38 decomposed datasets show that current offline RL algorithms struggle to outperform simple behavioral cloning, achieving normalized scores below 6.0"
- Why unresolved: The paper shows this empirically but doesn't deeply investigate the underlying reasons. The algorithms use various techniques (conservative Q-learning, advantage weighting, etc.) that should theoretically improve upon BC.
- What evidence would resolve it: Comparative analysis of learned policies' behavior distributions versus the behavioral policy, ablation studies on specific algorithmic components, or theoretical analysis of the value gap between learned policies and BC.

### Open Question 2
- Question: What is the optimal balance between dataset size and task decomposition for effective offline RL on NetHack?
- Basis in paper: [inferred] The authors decompose the large dataset into 38 smaller tasks and note that this allows "more focus on the characters' gameplay and the size reduction one needs to download"
- Why unresolved: The paper doesn't empirically test whether training on individual decomposed tasks or the full dataset yields better performance, nor does it explore intermediate levels of task granularity.
- What evidence would resolve it: Systematic experiments varying dataset composition strategies (individual tasks vs. combinations vs. full dataset) and measuring learning efficiency and final performance.

### Open Question 3
- Question: How does the choice of evaluation metric affect perceived algorithm performance in highly variable environments like NetHack?
- Basis in paper: [explicit] The authors argue that "mean and median statistics of average episode returns over training seeds" are unreliable and instead use RLiable tools with bootstrapped confidence intervals
- Why unresolved: While the paper advocates for better evaluation methods, it doesn't quantify how different evaluation approaches would rank the same algorithms differently.
- What evidence would resolve it: Side-by-side comparison of algorithm rankings using traditional metrics versus RLiable metrics across multiple seeds and environments to measure correlation and identify cases where rankings diverge.

## Limitations

- The benchmark only tests five offline RL algorithms, which may not capture the full space of possible approaches
- The decomposition strategy focuses on role-based tasks, potentially missing cross-role generalization challenges
- The performance gap between offline RL and behavioral cloning may be due to dataset quality rather than algorithmic limitations

## Confidence

- Claim that offline RL algorithms cannot solve NetHack: Medium confidence - experimental results are clear but limited to five algorithms
- Data loader speed-memory tradeoff: Medium confidence - assumes users can select appropriate loaders based on hardware
- RLiable evaluation effectiveness: High confidence - based on established practice in the field

## Next Checks

1. Test additional offline RL algorithms (e.g., BRAC, TD3+BC) on the decomposed datasets to confirm the observed performance ceiling
2. Evaluate cross-role transfer learning by training on one role configuration and testing on others
3. Benchmark data loader performance across different hardware configurations to validate the speed-memory tradeoff claims