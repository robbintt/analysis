---
ver: rpa2
title: Leveraging Structured Information for Explainable Multi-hop Question Answering
  and Reasoning
arxiv_id: '2311.03734'
source_url: https://arxiv.org/abs/2311.03734
tags:
- reasoning
- graph
- question
- graphs
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles multi-hop question answering, where the answer
  requires reasoning over multiple pieces of information across documents. The authors
  propose using semantic graphs extracted via information extraction to enhance the
  reasoning capabilities of large language models (LLMs) in this task.
---

# Leveraging Structured Information for Explainable Multi-hop Question Answering and Reasoning

## Quick Facts
- arXiv ID: 2311.03734
- Source URL: https://arxiv.org/abs/2311.03734
- Authors: 
- Reference count: 11
- Primary result: Semantic graphs extracted via information extraction improve multi-hop QA performance and reasoning chain faithfulness when used to guide LLM reasoning.

## Executive Summary
This paper addresses the challenge of multi-hop question answering by proposing a framework that leverages semantic graphs extracted from documents to enhance large language model reasoning. The approach uses information extraction to identify entities and relations, constructing semantic graphs that guide the LLM through the reasoning process via prompting. Experiments on HotpotQA and 2WikiMultiHopQA datasets demonstrate that this method generates more faithful reasoning chains and improves QA performance, with recall gains of up to 4% compared to baselines. Human evaluations also indicate that the extracted semantic graphs provide more grounded and preferred explanations than generated reasoning chains or saliency-based methods.

## Method Summary
The method involves extracting entities and relations from documents using prompt-based information extraction (either multi-step or joint extraction) to build semantic graphs. These graphs are then sequentialized into text prompts combined with the question and context, optionally including chain-of-thought reasoning instructions. The LLM generates reasoning chains and final answers, which are evaluated using exact match, F1, precision, and recall metrics. The framework compares semantic graph-augmented prompts against standard base prompts and ablation baselines.

## Key Results
- Semantic graph augmentation improves QA recall by up to 4% compared to baselines on HotpotQA and 2WikiMultiHopQA datasets
- Generated reasoning chains using semantic graphs show higher faithfulness in human evaluations compared to base prompt approaches
- Human evaluators prefer explanations derived from extracted semantic graphs over those from generated reasoning chains or saliency-based methods

## Why This Works (Mechanism)

### Mechanism 1
Semantic graphs extracted from documents improve multi-hop reasoning by providing explicit relational structure that guides LLMs. Information extraction identifies entities and relations, forming semantic graphs that represent factual dependencies. These graphs are sequentialized into text prompts, giving LLMs structured context for reasoning steps. The core assumption is that extracted semantic relations accurately reflect document content and provide sufficient guidance for correct reasoning paths. Evidence shows the framework generates more faithful reasoning chains and improves QA performance, though the evidence base is limited to qualitative examples without direct quantitative correlation analysis. If extracted relations contain errors or hallucinations, the guided reasoning will be compromised.

### Mechanism 2
Sequentialized semantic graphs improve reasoning chain faithfulness compared to end-to-end generation by providing explicit relational triples in the prompt, allowing LLMs to "hop through" structured relationships rather than inferring them from raw text, reducing hallucinations and improving faithfulness. The core assumption is that LLMs can effectively parse and utilize sequentialized graph structures when incorporated into prompts. Evidence indicates extracted structures provide more grounded explanations preferred by humans, but the corpus lacks direct evidence for this specific mechanism. If graph sequentialization doesn't preserve relational clarity or LLMs struggle to process the structured format, reasoning quality won't improve.

### Mechanism 3
Recall is a more appropriate metric than precision or F1 for evaluating generated multi-hop QA answers because generated answers vary in length and may include additional context beyond gold answers. Recall better captures correctness when answers are correct but include extra information, while precision unfairly penalizes this. The core assumption is that longer generated answers containing the gold answer plus additional context should be considered correct. Evidence shows recall corresponds more to human evaluation through manual studies showing Spearman and Kendall-Tau correlations, though the evidence base is relatively limited. If generated answers frequently include incorrect information beyond the gold answer, precision becomes more important despite the additional context.

## Foundational Learning

- **Information Extraction (IE) for semantic graphs**: The framework relies on extracting entities and relations from text to form semantic graphs that guide reasoning. Quick check: What distinguishes semantic relations from named entities in this framework, and why is this distinction important?

- **Chain-of-Thought (CoT) prompting**: The framework builds on CoT approaches but augments them with structured semantic information to improve reasoning. Quick check: How does the semantic graph integration modify the standard CoT approach, and what specific problems does it address?

- **Multi-hop question answering evaluation metrics**: Understanding why recall is preferred over precision/F1 for generated answers is crucial for proper evaluation. Quick check: Why might precision unfairly penalize correct answers in the generative QA setting, and how does recall address this issue?

## Architecture Onboarding

- **Component map**: Document preprocessing → Information Extraction (entity and relation extraction) → Semantic Graph Construction → Graph Sequentialization → Prompt Construction → LLM Inference → Answer Generation → Post-processing. Key data flows: Documents → semantic graphs → prompts → LLM → answers. External dependencies: Information extraction model (GPT-3.5), QA model (GPT-3.5), evaluation scripts.

- **Critical path**: Document → Information Extraction → Semantic Graph → Prompt → LLM → Answer. Bottleneck: Information extraction quality directly impacts downstream reasoning performance. Time-sensitive: Each LLM call (extraction and QA) contributes to total inference time.

- **Design tradeoffs**: Single-step vs multi-step extraction (joint extraction is faster but may sacrifice some quality); graph completeness vs prompt length (more complete graphs provide better guidance but may exceed LLM input limits); generation temperature (set to 0 for deterministic outputs but may miss alternative valid reasoning paths).

- **Failure signatures**: Poor answer quality (check information extraction quality and graph construction); hallucinations in reasoning chains (indicates extraction errors or LLM struggling with graph format); long inference times (may indicate inefficient graph construction or excessive prompt length).

- **First 3 experiments**: 1) Compare SG-Multi vs SG-One vs G-Full vs base prompt on a small sample of questions from HotpotQA to validate recall improvements; 2) Manual evaluation of reasoning chain faithfulness comparing base prompt vs semantic graph-augmented prompts on 50 questions; 3) A/B test on human preference for explanations: semantic graph explanations vs generated reasoning chains vs saliency-based explanations on 20 questions.

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality of the extracted semantic graphs correlate with the quality of the generated reasoning chains and final answers in multi-hop QA? The paper discusses that reasoning chain quality relies on extracted semantic graph quality, with better-quality graphs leading to more accurate chains and answers. This remains unresolved because while qualitative examples show the impact, there's no quantitative analysis measuring this correlation directly. Conducting a large-scale study where semantic graphs are systematically varied in quality and measuring corresponding changes in reasoning chain and answer quality would provide empirical evidence.

### Open Question 2
Can the proposed framework handle multi-hop QA tasks that require external knowledge beyond the provided documents? The paper mentions errors caused by lack of external or commonsense knowledge, indicating a potential limitation. This remains unresolved because the paper doesn't explore or test the framework's ability to integrate external knowledge sources. Experiments testing the framework on multi-hop QA tasks explicitly requiring external knowledge, possibly by integrating external knowledge bases or using pre-trained models with broader knowledge, would show whether the framework can be extended to handle such tasks.

### Open Question 3
What are the trade-offs between the multi-step and joint extraction methods for constructing semantic graphs in terms of accuracy, efficiency, and scalability? The paper introduces both methods and notes differences in application and outcomes. This remains unresolved because while the paper compares method quality and QA performance impact, it doesn't delve into computational efficiency or scalability trade-offs for larger datasets. A detailed analysis comparing computational costs, processing times, and scalability across various dataset sizes and complexities would clarify the trade-offs.

## Limitations
- Evaluation relies heavily on recall as the primary metric without sufficient evidence that it consistently captures correctness better than precision across different question types
- Information extraction quality is a critical bottleneck with limited empirical validation of extraction accuracy or error rates
- Framework's scalability to longer documents or more complex reasoning chains remains unclear, with no exploration of context window limitations

## Confidence
- **High Confidence**: The core claim that semantic graphs can provide structured guidance for LLM reasoning is well-supported by general literature on knowledge graphs and information extraction
- **Medium Confidence**: The claim that recall is a better metric than precision for evaluating generated multi-hop QA answers is supported by manual correlation studies but has a relatively limited evidence base
- **Low Confidence**: The specific claim that sequentialized semantic graphs substantially improve reasoning chain faithfulness over end-to-end generation lacks direct comparative evidence with rigorous factuality metrics

## Next Checks
1. **Extraction Quality Audit**: Perform systematic evaluation of entity and relation extraction accuracy on a sample of documents, measuring precision, recall, and F1 for the information extraction component. Compare extracted triples against human-annotated gold relations to establish a baseline for how extraction errors propagate through the reasoning pipeline.

2. **Metric Robustness Test**: Design experiments that specifically test the claim about recall being superior to precision for generated answers. Create controlled scenarios where generated answers contain varying amounts of correct context plus incorrect information, then measure how different metrics (EM, F1, Precision, Recall) rank the quality of these responses.

3. **Prompt Format Ablation**: Systematically vary the format and presentation of semantic graph information in prompts (e.g., different sequentialization approaches, graph structure vs flat triples, varying levels of detail) to determine which aspects of the graph representation most strongly contribute to improved reasoning performance.