---
ver: rpa2
title: 'Tram-FL: Routing-based Model Training for Decentralized Federated Learning'
arxiv_id: '2308.04762'
source_url: https://arxiv.org/abs/2308.04762
tags:
- data
- node
- nodes
- routing
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of achieving high-accuracy model
  training in decentralized federated learning (DFL) under non-independent and identically
  distributed (non-IID) data conditions and high communication costs. The core method,
  Tram-FL, introduces a novel DFL approach that sequentially transfers a single global
  model among nodes, rather than exchanging and aggregating local models.
---

# Tram-FL: Routing-based Model Training for Decentralized Federated Learning

## Quick Facts
- arXiv ID: 2308.04762
- Source URL: https://arxiv.org/abs/2308.04762
- Reference count: 16
- One-line primary result: Tram-FL achieves high model accuracy under non-IID conditions while reducing communication costs through sequential model transfer and dynamic routing.

## Executive Summary
Tram-FL addresses the challenge of decentralized federated learning (DFL) under non-IID data conditions and high communication costs. The method introduces a novel approach that sequentially transfers a single global model among nodes, complemented by a dynamic model routing algorithm that selects optimal routes to enhance model precision while minimizing forwarding. Experimental results on MNIST, CIFAR-10, and IMDb datasets demonstrate that Tram-FL achieves high model accuracy under non-IID conditions, outperforming baseline methods while significantly reducing communication costs.

## Method Summary
Tram-FL operates without a central server, relying on node-to-node model passing in a fully meshed network topology. The method initializes a global model at a starting node, which then updates the model using its local mini-batches. A dynamic routing module selects the next node based on minimizing variance in label distribution usage, aiming for uniform label coverage over time. The model is transmitted to the selected node, which updates it with its own data before passing it on. This process continues until convergence or a maximum number of rounds is reached. The approach uses specific DNN architectures for each dataset, with hyperparameters detailed in the paper.

## Key Results
- Tram-FL achieves high model accuracy under non-IID conditions, outperforming baseline methods
- The dynamic routing algorithm reduces model transmissions by 16.25% compared to static routing and 20.22% compared to random routing
- Sequential model transfer reduces communication overhead compared to exchanging local models in parallel

## Why This Works (Mechanism)

### Mechanism 1
- Sequential model transfer reduces communication overhead compared to exchanging local models in Gossip-SGD or PDMM-SGD
- Only one global model is transmitted sequentially between nodes instead of exchanging multiple local models in parallel
- Break condition: If node data is highly biased and sequential updates cannot approximate IID behavior, model convergence may fail

### Mechanism 2
- Dynamic routing based on label distribution bias minimizes the variance of data samples used for training
- The routing algorithm selects the next node to visit by minimizing the variance of the cumulative label distribution used so far
- Break condition: If the data label distribution is too skewed or node capacities differ greatly, uniform coverage may be impossible and variance minimization ineffective

### Mechanism 3
- Sequential model updates approximate training on aggregated IID data over medium-to-long-term periods
- By updating the same global model across diverse nodes, the model gradually sees a broader distribution of data
- Break condition: If the number of nodes or rounds is too small, non-IID bias may persist and degrade accuracy

## Foundational Learning

- Concept: Decentralized Federated Learning (DFL)
  - Why needed here: Tram-FL operates without a central server, relying on node-to-node model passing
  - Quick check question: In DFL, who coordinates model aggregation? (Answer: No central coordinator; nodes coordinate directly.)

- Concept: Non-IID data distribution
  - Why needed here: The core challenge Tram-FL addresses is training accuracy under data heterogeneity across nodes
  - Quick check question: What is the main challenge when data across nodes is non-IID in FL? (Answer: Difficulty in achieving a consensus on a global model due to varying gradients.)

- Concept: Model routing algorithms
  - Why needed here: Dynamic routing is key to Tram-FL's efficiency; selecting optimal nodes to visit minimizes transmissions and maximizes accuracy
  - Quick check question: What does the dynamic routing algorithm in Tram-FL aim to minimize? (Answer: The variance of the cumulative label distribution used for training.)

## Architecture Onboarding

- Component map: Nodes -> Global model -> Dynamic routing module -> Communication layer -> Training loop
- Critical path: 1) Initialize global model at starting node, 2) Node updates model with local mini-batches, 3) Routing module selects next node, 4) Model transmitted to next node, 5) Repeat until convergence or max rounds
- Design tradeoffs: Sequential updates reduce communication but increase rounds; dynamic routing improves convergence but adds routing computation overhead; smaller batch size and T=1 improve IID approximation but increase transmissions
- Failure signatures: Model accuracy plateaus or degrades despite many rounds; communication rounds increase without corresponding accuracy gains; routing frequently selects the same node, indicating poor label diversity coverage
- First 3 experiments: 1) Run Tram-FL with static routing and compare convergence vs dynamic routing, 2) Test Tram-FL vs Gossip-SGD and PDMM-SGD under varying non-IID data splits, 3) Vary batch size and T parameter to find optimal balance between communication cost and convergence speed

## Open Questions the Paper Calls Out

- The paper does not explicitly call out open questions in the provided text.

## Limitations
- The dynamic routing algorithm's implementation details, particularly variance calculation, are not fully specified
- Some hyperparameters for the IMDb dataset are referenced externally without direct specification
- Experimental results are limited to small-scale networks (up to 10 nodes), scalability to larger networks is unexplored

## Confidence

- High: Sequential model transfer reduces communication overhead compared to parallel local model exchanges
- Medium: Dynamic routing based on label distribution variance minimization improves convergence in non-IID settings
- Low: Sequential updates approximate IID training over medium-to-long-term periods without rigorous empirical validation

## Next Checks

1. Implement Tram-FL with static routing to quantify the impact of dynamic routing on convergence speed and communication efficiency
2. Test Tram-FL against Gossip-SGD and PDMM-SGD under varying non-IID splits to validate accuracy gains and transmission reductions
3. Experiment with different batch sizes and T values to identify the optimal balance between communication cost and convergence speed