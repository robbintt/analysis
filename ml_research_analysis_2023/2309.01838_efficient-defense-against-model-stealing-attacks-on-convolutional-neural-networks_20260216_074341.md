---
ver: rpa2
title: Efficient Defense Against Model Stealing Attacks on Convolutional Neural Networks
arxiv_id: '2309.01838'
source_url: https://arxiv.org/abs/2309.01838
tags:
- defense
- attacks
- error
- stealing
- adversary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of model stealing attacks against
  deep learning models, particularly focusing on convolutional neural networks (CNNs)
  used in computer vision tasks. The proposed method, Deception (DCP), introduces
  a heuristic approach to perturb output probabilities with the aim of deceiving attackers
  while maintaining model transparency and performance.
---

# Efficient Defense Against Model Stealing Attacks on Convolutional Neural Networks

## Quick Facts
- arXiv ID: 2309.01838
- Source URL: https://arxiv.org/abs/2309.01838
- Reference count: 29
- Primary result: DCP defense achieves up to 37× faster inference latency compared to previous defenses while effectively protecting against model stealing attacks

## Executive Summary
This paper presents Deception (DCP), a novel defense against model stealing attacks on convolutional neural networks that perturbs output probabilities to deceive attackers while maintaining model transparency and performance. The approach selectively modifies prediction probabilities using a noise function and detector mechanism targeting uncertain predictions, increasing the adversary's classification error without significantly impacting the defender's accuracy. Experimental results demonstrate that DCP effectively defends against state-of-the-art model stealing attacks across multiple datasets while achieving substantially faster inference latency and lower energy consumption compared to existing defenses.

## Method Summary
DCP is a heuristic defense that perturbs output probabilities of CNN models to defend against model stealing attacks. The method uses a detector function that activates when the highest prediction probability falls below a threshold, at which point it injects noise derived from a reverse sigmoid transformation of the clean posteriors. This noise is selectively applied only to uncertain predictions, bounded by an ℓ1 budget, and normalized to maintain valid probability distributions. The defense leverages the victim model's own predictions to generate perturbations, eliminating the need for auxiliary models while maintaining effectiveness across multiple datasets and quantized CNN implementations.

## Key Results
- DCP achieves up to 37× faster inference latency compared to previous defenses while maintaining effectiveness against model stealing attacks
- The defense successfully protects against state-of-the-art attacks (KnockoffNets, MAZE, DFME) across CIFAR-10, CIFAR-100, SVHN, GTSRB, and CUB200 datasets
- DCP maintains comparable or superior performance to baseline defenses when applied to quantized CNNs for edge devices while using lower perturbation budgets
- The approach demonstrates effective trade-offs between defense strength, model utility preservation, and computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DCP increases the adversary's classification error by selectively perturbing low-confidence posterior probabilities while preserving high-confidence predictions.
- Mechanism: The defense uses a detector function α that activates only when the highest probability in the prediction vector is below a threshold τ. When α > 0, it injects noise derived from a reverse sigmoid transformation of the clean posteriors. This noise is scaled by β and combined with the clean prediction using y' = N(FV(x;θ) - β·α·r(y)), where r(y) is the perturbation and N normalizes to sum-to-one probabilities.
- Core assumption: Low-confidence predictions are less likely to be true positives, so perturbing them minimally impacts the defender's accuracy while confusing the adversary.
- Evidence anchors:
  - [abstract] "Our heuristic approach aims to deceive the attacker and decrease the accuracy of his stolen model by adding an adaptive noise that deviates the probability scores without increasing the inference time."
  - [section] "Our rationale is that probabilities with low values are of less importance...if a well-trained model is uncertain about the prediction of an input, it is highly probable that the model made a wrong prediction (false positive)."
  - [corpus] Weak corpus coverage for this specific adaptive noise mechanism; most related work focuses on model extraction defenses rather than this selective perturbation strategy.
- Break condition: If the adversary's training process becomes robust to noisy labels (e.g., through data augmentation or noise-tolerant training), the effectiveness of this selective perturbation may diminish.

### Mechanism 2
- Claim: DCP avoids the computational overhead of auxiliary models by using the victim model's own predictions as the basis for noise generation.
- Mechanism: Instead of training a separate misinformation model as in AM, DCP applies the reverse sigmoid function to the victim's own predictions y to compute perturbations r(y) = S(γS⁻¹(y)) - 1/2. This eliminates the need for extra training while still producing effective noise.
- Core assumption: The victim model's predictions contain sufficient information to generate perturbations that mislead the adversary without requiring a separate model.
- Evidence anchors:
  - [section] "Instead of using an extra model to inject the noise, our defense leverages a noise function r(y) inspired from RS [8]...we use the victim's predictions y = FV(x;θ) as starting point to compute the perturbations r(y)."
  - [abstract] "Our technique outperforms the state-of-the-art defenses...without requiring any additional model"
  - [corpus] The corpus contains some work on model extraction defenses but limited evidence of approaches that avoid auxiliary models entirely.
- Break condition: If the victim model's predictions become too smooth or uniform (e.g., due to over-regularization), the reverse sigmoid transformation may produce ineffective noise patterns.

### Mechanism 3
- Claim: DCP maintains model utility by constraining perturbation magnitude through an ℓ1 budget and selective application based on prediction confidence.
- Mechanism: The defense applies the perturbation only when the maximum probability is below threshold τ, and the overall perturbation is bounded by ℓ1 distance constraint. This ensures that high-confidence predictions remain largely unchanged, preserving transparency and usability for critical systems.
- Core assumption: Users and systems rely on confident predictions, so maintaining high-confidence outputs is crucial for practical utility.
- Evidence anchors:
  - [abstract] "We aim to decrease the attack performance with a low amount of perturbation...In critical systems, inference latency is very important which makes state-of-the-art defenses infeasible."
  - [section] "However, the prediction probabilities y have to keep a certain level of confidence score to maintain the model's utility and transparency...Hence, we constrain the perturbation magnitude on the posteriors y by budget ϵ"
  - [corpus] Limited corpus evidence specifically addressing the ℓ1 budget constraint as a utility-preserving mechanism in model extraction defenses.
- Break condition: If the adversary can detect and exploit the pattern of when perturbations are applied (e.g., always when confidence is low), they may adapt their attack strategy to avoid these cases.

## Foundational Learning

- Concept: Model stealing attacks via black-box API access
  - Why needed here: Understanding how attackers query the model and use output probabilities to train surrogate models is fundamental to designing effective defenses.
  - Quick check question: What information does an attacker need from the victim model to successfully perform a model stealing attack?

- Concept: Perturbation-based defense strategies
  - Why needed here: The paper builds on existing perturbation-based defenses but improves them by making them more efficient and less intrusive.
  - Quick check question: How do perturbation-based defenses differ from reactive defenses that detect ongoing attacks?

- Concept: Quantized neural networks and their vulnerabilities
  - Why needed here: The paper demonstrates that quantized models are as vulnerable to model stealing as full-precision models, and the defense works for both.
  - Quick check question: Why might quantized models be particularly attractive targets for model stealing attacks in edge device scenarios?

## Architecture Onboarding

- Component map:
  Detector function α -> Noise function r(y) -> Normalizer N -> Output prediction

- Critical path:
  1. Model receives input query
  2. Compute clean prediction probabilities FV(x;θ)
  3. Calculate ymax and detector function α
  4. If α > 0, compute noise r(y) and apply perturbation
  5. Normalize perturbed output to sum-to-one probabilities
  6. Return perturbed or clean prediction based on α

- Design tradeoffs:
  - Higher β increases defense effectiveness but may impact defender's accuracy
  - Lower τ makes defense more aggressive but risks more utility loss
  - ℓ1 budget constrains perturbation magnitude but may reduce defense strength
  - No auxiliary model saves computation but may be less adaptive than model-based approaches

- Failure signatures:
  - Defender's accuracy drops significantly while adversary's accuracy remains high
  - Perturbation budget is consistently maxed out across all queries
  - Detector function α is always near 0 or always near 1 (no selective application)
  - Energy consumption increases dramatically despite claims of efficiency

- First 3 experiments:
  1. Test defense against KnockoffNets attack on CIFAR-10→CIFAR-100 transfer to verify basic functionality
  2. Measure defender's classification error and ℓ1 distance across different β values to find optimal tradeoff
  3. Compare inference latency and energy consumption against RS baseline to validate efficiency claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the DCP defense scale with different perturbation budgets and what is the optimal budget for balancing defender's accuracy and adversary's classification error?
- Basis in paper: [explicit] The paper discusses the use of a perturbation budget ϵ to constrain the magnitude of perturbations on prediction probabilities, but does not provide an optimal budget for balancing defender's accuracy and adversary's classification error.
- Why unresolved: The paper does not provide a systematic study on how the performance of the DCP defense scales with different perturbation budgets, nor does it identify an optimal budget for balancing defender's accuracy and adversary's classification error.
- What evidence would resolve it: A comprehensive evaluation of the DCP defense's performance across a range of perturbation budgets, identifying the budget that maximizes the adversary's classification error while minimizing the defender's classification error.

### Open Question 2
- Question: How does the DCP defense perform against model stealing attacks that use different query distributions and strategies (e.g., random vs. adaptive querying)?
- Basis in paper: [explicit] The paper mentions that the defense aims to defend against diverse strategies for model stealing, but does not provide an extensive evaluation against attacks using different query distributions and strategies.
- Why unresolved: The paper does not provide a comprehensive evaluation of the DCP defense against model stealing attacks that use different query distributions and strategies, such as random vs. adaptive querying.
- What evidence would resolve it: A thorough evaluation of the DCP defense against model stealing attacks that use various query distributions and strategies, demonstrating its effectiveness across a wide range of attack scenarios.

### Open Question 3
- Question: How does the DCP defense perform when applied to quantized CNNs with different quantization techniques and bit-widths?
- Basis in paper: [explicit] The paper mentions that the DCP defense is validated on quantized CNNs, but does not provide an extensive evaluation of its performance across different quantization techniques and bit-widths.
- Why unresolved: The paper does not provide a comprehensive evaluation of the DCP defense's performance on quantized CNNs with different quantization techniques and bit-widths, such as post-training quantization (PTQ) and quantization-aware training (QAT) with various bit-widths.
- What evidence would resolve it: A thorough evaluation of the DCP defense's performance on quantized CNNs with different quantization techniques and bit-widths, demonstrating its effectiveness and robustness across a wide range of quantization settings.

## Limitations
- The defense's effectiveness against adaptive adversaries who can detect and circumvent the selective perturbation strategy remains unproven
- The ℓ1 budget constraint may limit defense strength in scenarios requiring stronger protection
- The assumption that low-confidence predictions are less important may not hold for all applications or datasets

## Confidence
- High confidence: Core experimental results showing DCP's effectiveness against multiple attack types across various datasets
- Medium confidence: Generalization of results to real-world scenarios and long-term robustness against adaptive attacks
- Low confidence: Performance on non-vision tasks and more complex model architectures beyond ResNet

## Next Checks
1. Test DCP against adaptive attacks where adversaries specifically target the pattern of when perturbations are applied (i.e., when confidence is low)
2. Evaluate the defense's performance when the adversary uses noise-tolerant training methods that can handle perturbed labels
3. Assess the robustness of DCP on more diverse datasets and architectures, including those with different confidence distributions or task complexities