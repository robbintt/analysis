---
ver: rpa2
title: 'RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting'
arxiv_id: '2305.15685'
source_url: https://arxiv.org/abs/2305.15685
tags:
- text
- source
- instruction
- language
- edit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces RewriteLM, a large language model specifically
  fine-tuned for text rewriting tasks. The authors address the challenge of LLMs being
  primarily trained on final text outputs rather than intermediate revisions, which
  limits their performance in rewriting tasks.
---

# RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting

## Quick Facts
- arXiv ID: 2305.15685
- Source URL: https://arxiv.org/abs/2305.15685
- Reference count: 34
- Key outcome: RewriteLM significantly outperforms baselines in text rewriting tasks by fine-tuning on diverse instructions and reinforcement learning with synthetic preference data

## Executive Summary
This paper introduces RewriteLM, a large language model specifically fine-tuned for text rewriting tasks. The authors address the challenge that LLMs are primarily trained on final text outputs rather than intermediate revisions, which limits their performance in rewriting tasks. To overcome this, they develop strategies for instruction tuning and reinforcement learning to align LLMs with cross-sentence rewriting tasks using diverse wording and structures expressed in natural language. They introduce OpenRewriteEval, a novel benchmark covering a wide variety of rewriting types, and generate synthetic instruction and preference data with minimal human intervention. The results show that RewriteLM significantly outperforms various baselines, including pretrained LLMs and other instruction-tuned models, in preserving the essential content and meaning of the source text while generating rewrites with diverse wording and structures.

## Method Summary
The paper develops a framework for instruction tuning and reinforcement learning to align LLMs with cross-sentence rewriting tasks. The method involves three main stages: (1) Supervised fine-tuning on a dataset of rewriting instructions generated from Wiki revisions and synthetic instructions created using chain-of-thought prompting; (2) Reward model training on synthetic preference data generated by ranking LLM outputs using heuristic metrics like NLI scores and edit distance; and (3) Reinforcement learning to optimize the policy against the reward model. The approach minimizes human intervention by leveraging Wiki revision history as a natural source of source-target pairs and using LLMs to generate synthetic instructions and preferences.

## Key Results
- RewriteLM outperforms various baselines including pretrained LLMs, GPT-3.5, and other instruction-tuned models on OpenRewriteEval benchmark
- Reinforcement learning with synthetic preference data significantly improves content preservation and rewriting diversity compared to supervised fine-tuning alone
- The model successfully preserves essential content while generating rewrites with diverse wording and structures across multiple rewriting types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction tuning with diverse rewriting instructions enables LLMs to follow open-ended rewriting tasks more effectively than standard pre-training
- Mechanism: By generating rewriting instructions from Wiki edits and augmenting with chain-of-thought prompting, the model learns to interpret and execute varied rewriting intents, preserving content while varying form
- Core assumption: The instruction dataset captures the semantic essence of rewriting tasks and is sufficiently diverse to generalize across unseen instructions
- Evidence anchors:
  - [abstract]: "develop new strategies for instruction tuning and reinforcement learning to better align LLMs for cross-sentence rewriting tasks using diverse wording and structures expressed through natural languages"
  - [section]: "generate rewriting instruction data from Wiki edits and public corpus through instruction generation and chain-of-thought prompting"
  - [corpus]: Found 25 related papers; top related title: "Dr Genre: Reinforcement Learning from Decoupled LLM Feedback for Generic Text Rewriting" — weak evidence as cited count=0
- Break condition: If instruction generation quality drops or dataset bias is high, model will hallucinate or fail to follow instructions

### Mechanism 2
- Claim: Reinforcement learning guided by a reward model trained on synthetic preference data improves content preservation and rewriting diversity beyond supervised fine-tuning
- Mechanism: Reward model ranks model outputs based on NLI-based content preservation, edit distance, and length ratio heuristics; RL optimizes policy against this reward to balance faithfulness and creativity
- Core assumption: The heuristic ranker captures human preferences well enough for the reward model to learn effective reward signals
- Evidence anchors:
  - [abstract]: "we propose a strong baseline model — RewriteLM... perform supervised fine-tuning, reward model training, and reinforcement learning"
  - [section]: "we employ the methodology outlined in Section 3.1.3... We utilize NLI scores to assess content preservation and the degree of hallucination in the generated text"
  - [corpus]: Weak support; no direct RLHF text rewriting corpus hits
- Break condition: If reward model overfits synthetic data or ignores key quality dimensions, RL may diverge from human preferences

### Mechanism 3
- Claim: Minimizing human intervention in data collection via synthetic data generation and heuristic filtering enables scalable model development without sacrificing quality
- Mechanism: Wiki revisions provide natural source-target pairs; chain-of-thought prompting and LLM generation expand instruction variety; heuristic post-processing removes hallucinations and low-edit examples
- Core assumption: Synthetic and heuristic-augmented data maintain sufficient quality to train competent models
- Evidence anchors:
  - [abstract]: "develop new strategies that facilitate the generation of diverse instructions and preference data with minimal human intervention"
  - [section]: "For instruction data generation, we extract long-form, high quality edits with substantial changes from Wiki... To generate more preference data, we sample multiple LLM model outputs and rank them using a human-designed heuristic ranker"
  - [corpus]: Weak; only indirect support from related rewriting instruction-tuning works
- Break condition: If synthetic data introduces noise or biases not caught by heuristics, model performance degrades

## Foundational Learning

- Concept: Natural Language Inference (NLI)
  - Why needed here: Used to detect hallucinations and ensure content preservation between source and rewritten text
  - Quick check question: Given a premise and a hypothesis, can you decide if the hypothesis is entailed, neutral, or contradictory?

- Concept: Edit distance and length ratio metrics
  - Why needed here: Quantify textual change magnitude and appropriateness of rewriting (e.g., expansion vs. compression)
  - Quick check question: How would you interpret an edit ratio near 0 versus near 1 for a rewrite task?

- Concept: Reinforcement learning from human feedback (RLHF)
  - Why needed here: Fine-tunes the policy to maximize human-aligned rewards beyond supervised learning
  - Quick check question: What is the difference between training a reward model and training the policy in RLHF?

## Architecture Onboarding

- Component map: Pretrained LLM → Supervised fine-tuning on Wiki/synthetic instructions → Reward model training on synthetic preferences → Reinforcement learning → Final RewriteLM
- Critical path: Data generation → SFT → Reward model → RL fine-tuning
- Design tradeoffs: More synthetic data reduces human cost but risks noise; heuristic filtering trades recall for precision; RL vs. pure SFT balances creativity and safety
- Failure signatures: Low NLI scores → hallucinations; high edit ratio with low NLI → excessive hallucination; RL reward collapse → degenerate outputs
- First 3 experiments:
  1. Run SFT on small Wiki subset, evaluate NLI/edit ratio on held-out instructions
  2. Generate synthetic preference data, train reward model, check ranking consistency with heuristics
  3. Apply RL fine-tuning, compare automatic metrics against SFT-only baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different RL reward models (with vs. without synthetic preference data) affect long-term performance stability and generalizability across unseen rewriting tasks?
- Basis in paper: The paper explicitly compares Rewrite-RL-PaLM 2 (without synthetic preference data) and Rewrite-RL r/w-PaLM 2 (with synthetic preference data), showing improved performance for the latter across nearly all metrics
- Why unresolved: The paper only presents snapshot comparisons. Long-term stability, potential reward hacking, and performance on truly out-of-distribution rewriting tasks remain untested
- What evidence would resolve it: A longitudinal study tracking model performance over extended training and evaluation periods, plus testing on diverse, unseen rewriting tasks not represented in the training data

### Open Question 2
- Question: What is the impact of instruction diversity in the synthetic instruction dataset on the model's ability to handle truly open-ended rewriting instructions?
- Basis in paper: The paper discusses generating synthetic instructions using chain-of-thought prompting and LLMs, but does not analyze the diversity or coverage of these instructions relative to human-written ones
- Why unresolved: The paper doesn't quantify instruction diversity or evaluate model performance on instructions that differ significantly from the training distribution
- What evidence would resolve it: A detailed analysis of instruction diversity metrics (e.g., semantic clustering, instruction complexity measures) and model performance degradation as instruction similarity to training data decreases

### Open Question 3
- Question: How do human evaluators perceive the quality of model-generated rewrites compared to the automatic metrics used in the paper?
- Basis in paper: The paper uses automatic metrics (NLI, edit distance, SARI, GLEU) and AutoSxS with PaLM 2, but does not report direct human evaluation results
- Why unresolved: Automatic metrics and AI judges may not align with human preferences for rewrite quality, especially regarding naturalness and task completion
- What evidence would resolve it: Comprehensive human evaluation studies comparing model outputs to human rewrites across multiple dimensions (content preservation, fluency, task completion, naturalness)

## Limitations

- Heavy reliance on synthetic data generation may introduce quality concerns and biases not fully captured by heuristic filtering
- Automated evaluation methodology using NLI scores and PaLM 2-based side-by-side evaluation may not fully align with human judgment
- The effectiveness of Wiki revision history as a source of high-quality instruction data depends on the quality and diversity of edit summaries

## Confidence

- High confidence: The overall framework of instruction tuning followed by reward modeling and reinforcement learning is well-established and technically sound
- Medium confidence: The effectiveness of Wiki revision history as a source of high-quality instruction data, given potential noise and bias in edit summaries
- Medium confidence: The synthetic data generation process, particularly the quality and diversity of chain-of-thought prompting and LLM-based instruction generation
- Medium confidence: The automated evaluation methodology, particularly the use of NLI scores for content preservation and the PaLM 2-based side-by-side evaluation

## Next Checks

1. Conduct human evaluation studies on a subset of rewriting tasks to validate the automatic metrics (NLI, SARI, GLEU) and side-by-side evaluation results
2. Systematically remove or modify components of the synthetic data generation pipeline to assess the impact of instruction quality and diversity on final model performance
3. Evaluate RewriteLM's performance across diverse domains and writing styles not represented in the Wiki training data to assess generalization capability