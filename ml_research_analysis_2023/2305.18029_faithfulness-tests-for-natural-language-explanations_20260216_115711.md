---
ver: rpa2
title: Faithfulness Tests for Natural Language Explanations
arxiv_id: '2305.18029'
source_url: https://arxiv.org/abs/2305.18029
tags:
- prediction
- nles
- test
- explanations
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two tests for evaluating the faithfulness
  of natural language explanations (NLEs) generated by neural models. The first test,
  counterfactual input editing, inserts tokens into inputs to create counterfactual
  instances that change model predictions, then checks if the NLEs reflect these changes.
---

# Faithfulness Tests for Natural Language Explanations

## Quick Facts
- arXiv ID: 2305.18029
- Source URL: https://arxiv.org/abs/2305.18029
- Reference count: 24
- This paper introduces two tests for evaluating the faithfulness of natural language explanations (NLEs) generated by neural models.

## Executive Summary
This paper addresses the critical issue of evaluating the faithfulness of natural language explanations (NLEs) generated by neural models. The authors introduce two novel tests - counterfactual input editing and input reconstruction - to systematically assess whether NLEs accurately reflect the model's decision-making process. Through experiments on three datasets with four different NLE model configurations, they demonstrate that existing NLE models frequently produce explanations that do not faithfully represent the model's reasoning, with up to 44% of NLEs failing faithfulness tests. The results highlight the urgent need for improved evaluation methods and more faithful explanation generation approaches in machine learning.

## Method Summary
The authors develop two complementary tests for evaluating NLE faithfulness. The counterfactual input editing test inserts rare words into inputs to create counterfactual instances that change model predictions, then checks if the NLEs reflect these changes. The input reconstruction test extracts reasons from NLEs, reconstructs new inputs, and verifies if they lead to the same predictions as original inputs. Four NLE model configurations are evaluated: multi-task rationalizing (MT-Ra), multi-task reasoning (MT-Re), single-task rationalizing (ST-Ra), and single-task reasoning (ST-Re), all based on T5-base architecture. Models are trained for 20 epochs with learning rate 1e-4 using Adam optimizer.

## Key Results
- All four NLE model configurations exhibited significant unfaithfulness, with up to 44% failing the counterfactual test and up to 40% failing the input reconstruction test
- Multi-task setups (MT-Ra and MT-Re) showed slightly higher faithfulness than single-task setups in the input reconstruction test
- The CoS-E dataset showed the highest failure rates across both tests, indicating particular challenges in the commonsense reasoning domain
- No significant differences were observed between rationalizing and reasoning setups within the same task structure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Inserting rare words into inputs changes model predictions while NLEs remain unchanged, exposing unfaithfulness.
- Mechanism: The counterfactual input editor introduces rare words into the input text. If these insertions change the model's prediction but the generated NLE does not mention the inserted words, this indicates the NLE is not faithful to the model's reasoning.
- Core assumption: Model predictions are sensitive to rare word insertions, and NLEs should reflect these changes if they are faithful.
- Evidence anchors:
  - [section]: "During inference, we provide as target labels yC_i ∈ Y, yC_i ̸= byi, and we search over n2 different positions to insert n3 candidate tokens at each position at a time."
  - [corpus]: "Explanation-Driven Counterfactual Testing for Faithfulness in Vision-Language Model Explanations" - This work also uses counterfactual testing for evaluating faithfulness.

### Mechanism 2
- Claim: Reconstructing inputs from reasons stated in NLEs should lead to the same predictions as the original inputs if the NLEs are faithful.
- Mechanism: Automated agents Rs are used to extract reasons from NLEs and reconstruct new inputs. If the reconstructed inputs lead to different predictions than the original inputs, this indicates the NLEs are not faithful.
- Core assumption: The reasons stated in the NLEs are sufficient to reproduce the model's prediction if the NLEs are faithful.
- Evidence anchors:
  - [section]: "Given two sentences, the ComVe task is to pick the one that contradicts common sense. If the generated NLE is faithful, replacing the correct sentence with the NLE should lead the model to the same prediction."
  - [corpus]: "Benchmarking Faithfulness: Towards Accurate Natural Language Explanations in Vision-Language Tasks" - This work also explores evaluating faithfulness of NLEs.

### Mechanism 3
- Claim: Comparing the faithfulness of NLE models trained with different objectives (multi-task vs single-task, rationalizing vs reasoning) can reveal which setups are more prone to generating unfaithful explanations.
- Mechanism: Four NLE model setups are evaluated: MT-Re, MT-Ra, ST-Re, and ST-Ra. The faithfulness of these models is compared using the counterfactual test and input reconstruction test.
- Core assumption: Different training objectives and model architectures can lead to different levels of faithfulness in generated NLEs.
- Evidence anchors:
  - [section]: "Following Hase et al. (2020), we experiment with four setups for NLE models, which can be grouped by whether the prediction and NLE generation are trained with a multi-task objective using a joint model (MT) or with single-task objectives using separate models (ST)."

## Foundational Learning

- Concept: Natural Language Explanations (NLEs)
  - Why needed here: NLEs are the primary focus of this paper, and understanding what they are and how they are generated is crucial for evaluating their faithfulness.
  - Quick check question: What are natural language explanations and why are they important in machine learning?

- Concept: Counterfactual explanations
  - Why needed here: The counterfactual test relies on generating counterfactual instances by inserting rare words into inputs. Understanding counterfactual explanations is essential for designing and interpreting this test.
  - Quick check question: What are counterfactual explanations and how are they used to evaluate the faithfulness of model explanations?

- Concept: Faithfulness evaluation
  - Why needed here: The paper introduces two tests for evaluating the faithfulness of NLEs. Understanding the concept of faithfulness and how it is evaluated is crucial for interpreting the results of these tests.
  - Quick check question: What is faithfulness in the context of model explanations and why is it important to evaluate?

## Architecture Onboarding

- Component map: Input → Counterfactual input editor → Counterfactual instances → NLE models → NLEs → Automated agents Rs → Reconstructed inputs → Evaluation of faithfulness

- Critical path: The core evaluation pipeline involves generating counterfactual instances, producing NLEs, extracting reasons, reconstructing inputs, and comparing predictions to assess faithfulness.

- Design tradeoffs:
  - Computational cost: Generating counterfactual instances and reconstructing inputs can be computationally expensive.
  - Accuracy of automated extraction: The faithfulness of the tests depends on the accuracy of the automated extraction of reasons from NLEs.

- Failure signatures:
  - If the counterfactual input editor fails to generate counterfactual instances that change model predictions, the test may not be effective.
  - If the automated agents Rs fail to accurately extract reasons from NLEs, the input reconstruction test may not be effective.

- First 3 experiments:
  1. Evaluate the counterfactual test on a simple dataset with a known unfaithful NLE model.
  2. Compare the faithfulness of NLE models trained with different objectives on a benchmark dataset.
  3. Investigate the impact of rare word insertions on model predictions and NLE generation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop automated input reconstruction functions that can be applied to any task, not just task-dependent ones like e-SNLI and ComVE?
- Basis in paper: [inferred] The paper mentions that the input reconstruction test uses task-dependent heuristics and could not be constructed for the CoS-E dataset. It suggests investigating automated reconstruction functions using machine learning models trained on a small number of annotations.
- Why unresolved: The paper only provides examples for e-SNLI and ComVE, and the approach is not generalized to other tasks. The challenge lies in creating a universal method that can extract reasons from NLEs across diverse datasets and tasks.
- What evidence would resolve it: A proposed solution would involve designing a machine learning model that can automatically generate reconstructed inputs based on the generated NLEs, validated on multiple datasets and tasks beyond the ones used in the paper.

### Open Question 2
- Question: What are the confounding factors that might cause changes in model predictions when counterfactual interventions are inserted, apart from the intervention itself?
- Basis in paper: [explicit] The paper acknowledges that confounding factors, such as the model changing its focus towards other parts of the input, might cause the change in prediction. However, it states that such factors are presumed to be rare, especially when insertions rather than deletions are performed.
- Why unresolved: The paper does not provide a detailed investigation into these confounding factors. Understanding these factors is crucial for assessing the true impact of counterfactual interventions on model predictions.
- What evidence would resolve it: Empirical studies that identify and quantify the impact of confounding factors on model predictions when counterfactual interventions are inserted would provide clarity. This could involve analyzing cases where the model's prediction changes due to factors other than the intervention itself.

### Open Question 3
- Question: How can we efficiently search the space of insertion candidates to find counterfactual interventions that reveal unfaithful NLEs without incurring high computational demands?
- Basis in paper: [explicit] The paper notes that increasing the number of positions (n2) and candidate words (n3) for insertion leads to higher computational demands. It suggests exploring strategies for efficient searching of the insertion candidates' space.
- Why unresolved: The paper does not propose specific strategies for efficient searching. Balancing the thoroughness of the search with computational efficiency remains a challenge.
- What evidence would resolve it: Developing and validating an efficient search algorithm that can effectively identify counterfactual interventions revealing unfaithful NLEs, while keeping computational costs manageable, would address this question. This could involve techniques like prioritization, pruning, or approximation methods.

## Limitations

- The effectiveness of the counterfactual test depends heavily on the model's sensitivity to rare word insertions, which may vary across domains and model architectures
- The input reconstruction test relies on automated extraction of reasons from NLEs, but the paper lacks detailed validation of extraction accuracy, particularly for the CoS-E dataset
- The paper does not establish whether the selected rare words systematically influence predictions across all datasets

## Confidence

- High confidence: The observation that existing NLE models produce unfaithful explanations (supported by consistent results across multiple datasets and model configurations)
- Medium confidence: The comparative effectiveness of different NLE model setups (MT vs ST, Re vs Ra), as the differences, while present, show complex interactions that require further investigation
- Medium confidence: The specific failure rates reported, as they depend on the chosen thresholds and evaluation procedures which could affect the quantitative results

## Next Checks

1. Test the sensitivity of counterfactual testing by systematically varying the rarity and semantic properties of inserted words across different datasets to determine if results generalize beyond the current selection method.

2. Conduct ablation studies on the input reconstruction test by varying the complexity of extraction heuristics and measuring their impact on faithfulness detection accuracy.

3. Implement cross-validation between the two tests by identifying cases where one test flags an NLE as unfaithful while the other does not, then conduct manual analysis to understand the discrepancy and improve test reliability.