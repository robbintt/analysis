---
ver: rpa2
title: Implicit Bias of Gradient Descent for Two-layer ReLU and Leaky ReLU Networks
  on Nearly-orthogonal Data
arxiv_id: '2310.18935'
source_url: https://arxiv.org/abs/2310.18935
tags:
- lemma
- relu
- have
- inequality
- minpn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the implicit bias of gradient descent for training
  two-layer ReLU and leaky ReLU networks. The authors analyze the dynamics of the
  weight matrix during training and prove that gradient descent finds networks with
  low stable rank.
---

# Implicit Bias of Gradient Descent for Two-layer ReLU and Leaky ReLU Networks on Nearly-orthogonal Data

## Quick Facts
- arXiv ID: 2310.18935
- Source URL: https://arxiv.org/abs/2310.18935
- Reference count: 40
- Primary result: Gradient descent on two-layer ReLU and leaky ReLU networks converges to solutions with low stable rank, with leaky ReLU converging to rank 1 asymptotically

## Executive Summary
This paper analyzes the implicit bias of gradient descent when training two-layer neural networks with ReLU and leaky ReLU activations on nearly-orthogonal data. The authors develop a data-correlated decomposition technique that enables analysis of infinite-time training dynamics by replacing early-stopping thresholds with logarithmic coefficient bounds. They prove that gradient descent finds networks with low stable rank - specifically converging to rank 1 for leaky ReLU and bounded by a constant for ReLU networks. The paper also shows that all training data points achieve the same normalized margin asymptotically, leading to O(t⁻¹) loss convergence rates.

## Method Summary
The authors use a data-correlated decomposition technique to analyze gradient descent dynamics for two-layer fully connected neural networks. They decompose weight vectors as linear combinations of training data and track coefficient evolution over infinite training time. For leaky ReLU networks, they prove automatic balance of coefficients enables rank-1 convergence, while for ReLU networks they show bounded rank through activation pattern stabilization. The analysis relies on nearly-orthogonal data assumptions and careful tracking of margin dynamics to establish convergence properties.

## Key Results
- Leaky ReLU networks converge to stable rank 1 asymptotically
- ReLU networks achieve stable rank bounded by a constant
- All training data points converge to the same normalized margin
- Training loss converges at O(t⁻¹) rate
- Activation patterns stabilize after finite time for both activation functions

## Why This Works (Mechanism)

### Mechanism 1: Data-correlated decomposition
- Claim: Data-correlated decomposition enables stable rank analysis for infinite-time gradient descent by replacing early-stopping thresholds with logarithmic coefficient bounds
- Mechanism: Instead of bounding coefficients ρ(t) by a fixed value dependent on early stopping time T*, the refined analysis expresses |ρ(t)| growth via logarithmic rates using Lemma H.1, which relates exponential-decay inequalities to log(t) bounds
- Core assumption: The ratio between coefficients for different data points remains bounded throughout training (automatic balance)
- Evidence anchors:
  - [abstract] "we demonstrate that the neuron activation pattern reaches a stable state beyond a specific time threshold"
  - [section] "we propose a refined analysis of decomposition coefficients which generalizes Cao et al. (2022)'s technique"

### Mechanism 2: Leaky ReLU rank convergence
- Claim: Leaky ReLU networks converge to stable rank 1 because activation patterns stabilize and coefficients between neurons become equal beyond threshold T
- Mechanism: After time T, all neurons in the same class (positive/negative) have identical coefficient increments (Lemma D.3), making weight vectors nearly identical
- Core assumption: The activation pattern becomes fixed after time T (Lemma C.5), ensuring consistent gradient updates across neurons
- Evidence anchors:
  - [abstract] "for leaky ReLU activation function, gradient descent will find a network with a stable rank that converges to 1"
  - [section] "we can demonstrate that ∥w(t)j,r − w(t)j,r′∥2(r ̸= r′) can be upper bounded by a constant"

### Mechanism 3: Margin uniformity and loss convergence
- Claim: Both ReLU and leaky ReLU networks achieve O(t⁻¹) loss convergence by maintaining bounded margin differences across all training points
- Mechanism: The nearly orthogonal data property ensures that gradient updates affect all training points similarly
- Core assumption: Nearly orthogonal data ensures consistent gradient effects across all training points
- Evidence anchors:
  - [abstract] "we show that gradient descent will find a neural network such that all the training data points have the same normalized margin asymptotically"
  - [section] "η/5nm · |ℓ′(t)i | · ∥xi∥2 2 ≤ yif(W(t+1), xi) − yif(W(t), xi) ≤ 3η/nm · |ℓ′(t)i |∥xi∥2 2"

## Foundational Learning

- Concept: Data-correlated decomposition technique
  - Why needed here: Enables analysis of infinite-time training dynamics by expressing weight vectors as linear combinations of training data, replacing early stopping with asymptotic bounds
  - Quick check question: How does data-correlated decomposition differ from signal-noise decomposition in terms of normalization factors?

- Concept: Automatic balance in coefficient ratios
  - Why needed here: Ensures coefficients for different data points maintain bounded ratios, enabling approximation of neural network outputs using only coefficients for the current data point
  - Quick check question: What property of leaky ReLU activation enables the automatic balance lemma to hold?

- Concept: Stable rank convergence analysis
  - Why needed here: Provides the main theoretical result about network generalization - low stable rank implies simpler, more generalizable solutions
  - Quick check question: How does the stable rank of Wj relate to the rank of the matrix formed by its rows?

## Architecture Onboarding

- Component map: Data generation → Initialization → Gradient descent → Coefficient updates → Activation pattern stabilization → Stable rank convergence → Margin uniformity → Loss convergence

- Critical path: Data generation → Initialization → Gradient descent → Coefficient updates → Activation pattern stabilization → Stable rank convergence → Margin uniformity → Loss convergence

- Design tradeoffs:
  - Nearly orthogonal data assumption vs. real-world data: The theory requires R²min ≥ CR²np, which may not hold for correlated datasets
  - Width m requirements: Must be large enough for concentration bounds but not so large as to prevent rank convergence
  - Learning rate η constraints: Must be small enough for stability but large enough for meaningful progress

- Failure signatures:
  - If stable rank doesn't decrease: Check if activation patterns stabilized (violation of Lemma C.5/E.1)
  - If loss doesn't converge at O(t⁻¹): Verify nearly orthogonal data property holds
  - If coefficients grow unbounded: Check automatic balance lemma conditions

- First 3 experiments:
  1. Verify activation pattern stabilization: Plot sign(⟨w(t)j,r, xi⟩) over time for leaky ReLU network
  2. Test stable rank convergence: Compute ∥Wj∥²F/∥Wj∥²2 over training epochs
  3. Validate loss rate: Plot log(loss) vs log(t) to confirm O(t⁻¹) slope

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the weight matrix converge in direction for gradient descent on two-layer ReLU networks?
- Basis in paper: [explicit] The paper states "An important future work is to investigate the directional convergence of the weight matrix in neural networks trained via gradient descent, which is essential to prove the convergence to a KKT point of the max-margin problem."
- Why unresolved: The paper does not prove directional convergence for ReLU networks, only for leaky ReLU networks under certain assumptions.
- What evidence would resolve it: A rigorous proof showing that the weight matrix W(t)/||W(t)||_F converges to a limit as t→∞ for ReLU networks.

### Open Question 2
- Question: Do neurons in ReLU networks ever change their activation patterns infinitely often during training?
- Basis in paper: [explicit] "Furthermore, it is important to extend our analysis to fully understand the neuron activation patterns in ReLU networks. Specifically, we will explore whether certain neurons will switch their activation patterns by an infinite number of times throughout the training or if the activation patterns stabilize after a certain number of gradient descent iterations."
- Why unresolved: The paper only proves that once a neuron is activated, it remains activated, but does not address whether neurons can switch activation patterns infinitely often.
- What evidence would resolve it: A proof showing either that neuron activation patterns stabilize after finite time, or that some neurons can change activation infinitely often.

### Open Question 3
- Question: Can the stable rank of ReLU networks trained on orthogonal data converge to values other than 2?
- Basis in paper: [explicit] "For ReLU networks, we provide an example in the appendix concerning fully orthogonal training data and prove that the stable rank of the network's weight matrix Wj converges to approximately 2."
- Why unresolved: The paper only considers one specific case of orthogonal data and shows convergence to 2, but does not explore other data configurations.
- What evidence would resolve it: Analysis of ReLU network training on different types of orthogonal data to determine if stable rank always converges to 2 or can take other values.

## Limitations

- The nearly orthogonal data assumption may not hold for real-world datasets, limiting practical applicability
- The analysis relies on infinite-width approximations that may not capture finite-width effects
- The automatic balance property requires specific geometric conditions that may fail in practice

## Confidence

- Stable rank convergence results: Medium-High (rigorous proofs but depend on nearly orthogonal data condition)
- O(t⁻¹) loss convergence: Medium (follows from margin analysis but assumes data property throughout training)
- Asymptotic margin uniformity: Low-Medium (requires both stable rank convergence and data assumptions to hold simultaneously)

## Next Checks

1. Test the data-correlated decomposition technique on non-nearly-orthogonal datasets to identify breaking conditions for the coefficient analysis
2. Implement finite-width corrections to the stable rank analysis to quantify the gap between asymptotic predictions and practical observations
3. Measure the correlation between data point gradients during training to empirically verify when the "automatic balance" property breaks down