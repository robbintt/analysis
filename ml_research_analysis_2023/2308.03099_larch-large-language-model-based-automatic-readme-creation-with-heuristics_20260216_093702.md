---
ver: rpa2
title: 'LARCH: Large Language Model-based Automatic Readme Creation with Heuristics'
arxiv_id: '2308.03099'
source_url: https://arxiv.org/abs/2308.03099
tags:
- code
- larch
- readme
- https
- representative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LARCH, a system that automatically generates
  high-quality readme files for software repositories using large language models
  (LLMs). The key innovation is identifying a representative code fragment from the
  repository, which significantly improves the factual accuracy of the generated readme.
---

# LARCH: Large Language Model-based Automatic Readme Creation with Heuristics

## Quick Facts
- arXiv ID: 2308.03099
- Source URL: https://arxiv.org/abs/2308.03099
- Reference count: 21
- Key outcome: LARCH automatically generates high-quality readme files using LLMs with 65% usefulness rate, significantly outperforming random file baseline (40%)

## Executive Summary
LARCH is an automated system for generating high-quality readme files for software repositories using large language models. The key innovation is identifying a representative code fragment from the repository, which dramatically improves the factual accuracy of the generated readme. By combining heuristics and weak supervision techniques, LARCH selects the most representative code file, constructs an optimized prompt, and leverages GPT-3 to generate comprehensive documentation. Human evaluation shows LARCH produces useful readmes in 65% of cases with 65% factual correctness, compared to 40% usefulness and 30% factual correctness for the random file baseline.

## Method Summary
LARCH uses a multi-stage approach: first identifying representative code files through handcrafted labeling functions and weak supervision (Snorkel), then training a gradient boosting model to rank files by representativeness, and finally constructing optimized prompts for GPT-3 generation. The system includes a VSCode plugin and command-line interface that coordinate with an API server to analyze repositories, identify representative code, construct prompts, and format results. The approach addresses the challenge of LLMs struggling with repository-wide context by focusing on a single representative file that captures the project's essence.

## Key Results
- Human evaluation shows LARCH generates useful readmes in 65% of cases vs 40% for random file baseline
- Factual correctness reaches 65% for LARCH vs 30% for random baseline
- Automated ROUGE evaluation demonstrates improved performance over baseline methods
- The system successfully handles diverse repository types including applications, libraries, and scripts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Representative code identification dramatically improves factual correctness of generated readmes.
- Mechanism: LLMs generate better abstract summaries when provided with a single representative code file rather than the entire repository. This is because representative code captures the essence and purpose of the project in a digestible form.
- Core assumption: A single representative code file can accurately represent the overall functionality and purpose of an entire repository.
- Evidence anchors:
  - [abstract]: "LLMs are capable of generating a coherent and factually correct readmes if we can identify a code fragment that is representative of the repository"
  - [section]: "Through pilot studies, we found that identifying representative code is the key to readme generation"
  - [corpus]: Weak - corpus contains papers about LLMs and readmes but none specifically address representative code identification as a mechanism.

### Mechanism 2
- Claim: Weak supervision with heuristics can effectively identify representative code files.
- Mechanism: Instead of manual annotation, LARCH uses handcrafted labeling functions that capture various properties of code files (entry points, imports, file names, etc.) to create noisy labels. These labels train a gradient boosting model to rank files by their representativeness.
- Core assumption: The handcrafted labeling functions capture sufficient information to distinguish representative from non-representative code.
- Evidence anchors:
  - [section]: "we implemented labeling functions where ð‘—-th function takes ð‘–-th file and returns a noisy label... We developed 14 labeling functions"
  - [section]: "we train gradient boosting trees to identify representative code files. We use 14 features that are similar to our labeling functions"
  - [corpus]: Weak - corpus papers mention heuristics and weak supervision but not specifically in the context of code file selection for readme generation.

### Mechanism 3
- Claim: Prompt engineering with structured templates significantly improves LLM output quality.
- Mechanism: LARCH constructs prompts that include project name, representative code (truncated), and file names. This structure provides context while fitting within LLM context length constraints.
- Core assumption: LLMs respond well to structured prompts that provide specific contextual information about the code repository.
- Evidence anchors:
  - [section]: "Previous studies have shown that the design of prompt has a significant effect on downstream tasks"
  - [section]: "we carefully designed a prompt template that utilizes extracted representative code"
  - [corpus]: Weak - corpus contains papers about LLMs and prompts but none specifically address readme generation prompt design.

## Foundational Learning

- Concept: Weak supervision and data programming
  - Why needed here: Manual annotation of representative code is expensive and impractical for large-scale training.
  - Quick check question: How does weak supervision differ from traditional supervised learning in terms of labeling requirements?

- Concept: Gradient boosting trees for ranking
  - Why needed here: The task is to select the single most representative file from a repository, which is a ranking problem rather than classification.
  - Quick check question: Why would you choose a ranking formulation over binary classification for selecting representative code?

- Concept: Prompt engineering for LLMs
  - Why needed here: LLMs have context length limitations and respond differently to various prompt structures.
  - Quick check question: What are the key considerations when designing prompts for code summarization tasks?

## Architecture Onboarding

- Component map:
  - VSCode plugin/CLI (user interface) -> API server (coordinator) -> Repository analyzer (aggregates code) -> Representative code identifier (ML model) -> Prompt constructor -> LLM interface (GPT-3) -> Result formatter

- Critical path: User input -> Repository analyzer -> Representative code identifier -> Prompt constructor -> LLM -> Result formatter -> User output

- Design tradeoffs:
  - Single file vs. multiple files: Using one representative file simplifies context but may miss broader functionality
  - Prompt length vs. information completeness: Longer prompts provide more context but risk truncation
  - Local vs. external LLM: Local models offer privacy but may lack quality; external models provide quality but raise privacy concerns

- Failure signatures:
  - Factual errors in generated readme: likely representative code identification failure
  - Poor structure or coherence: likely prompt engineering or LLM issues
  - Slow response times: likely API server or LLM API latency
  - Incorrect file selection: likely weak supervision model or labeling functions

- First 3 experiments:
  1. Test with a simple repository where the entry point is clearly identifiable - verify the system selects the correct file and generates a reasonable readme
  2. Test with a library repository (multiple classes) - verify the system identifies a representative facade or base class
  3. Test with a repository containing both application code and tests - verify the system ignores test files and selects actual implementation code

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of labeling functions impact the accuracy of representative code identification across different programming languages?
- Basis in paper: [explicit] The paper mentions that they used 14 labeling functions for Python projects and that "our framework can be extended to other languages as well."
- Why unresolved: The paper only evaluated on Python projects and did not explore how different labeling functions might perform for other programming languages with different idioms and structures.
- What evidence would resolve it: Comparative studies of LARCH's performance using different labeling function sets across multiple programming languages, with systematic evaluation of which functions are most predictive for each language.

### Open Question 2
- Question: What is the optimal context length for readme generation across different repository sizes and complexities?
- Basis in paper: [inferred] The paper mentions that LLMs have quadratic computational complexity with sequence length and that they use a fixed 3,000 token prompt, but don't explore whether this is optimal.
- Why unresolved: The paper used a fixed prompt length without exploring whether shorter or longer contexts might improve generation quality, particularly for very small or very large repositories.
- What evidence would resolve it: Systematic experiments varying context lengths across repositories of different sizes, measuring the impact on factual accuracy and coherence of generated readmes.

### Open Question 3
- Question: How can LARCH be extended to generate readmes for repositories that contain multiple distinct projects or modules?
- Basis in paper: [inferred] The paper focuses on single-project repositories and uses a single representative code file, but doesn't address repositories with multiple distinct components.
- Why unresolved: The current approach identifies one representative file, which may not capture the full scope of multi-module repositories, potentially leading to incomplete or misleading readmes.
- What evidence would resolve it: Evaluation of LARCH on multi-module repositories with ground truth indicating which components should be documented, and development of approaches to identify and generate separate readmes for each module.

### Open Question 4
- Question: What is the impact of different LLM architectures and prompting strategies on the quality of generated readmes?
- Basis in paper: [explicit] The paper used GPT-3 with a specific prompt design but acknowledges that "the design of prompt has a significant effect on downstream tasks."
- Why unresolved: The paper only evaluated one LLM (GPT-3) with one prompting strategy, leaving open questions about whether other models or prompt designs might yield better results.
- What evidence would resolve it: Comparative evaluation of different LLM architectures (including open-source alternatives) and various prompting strategies (few-shot, chain-of-thought, etc.) on the same set of repositories.

## Limitations
- The approach relies heavily on the assumption that a single representative code file can capture the essence of an entire repository, which may fail for complex projects with multiple independent modules or microservices.
- The effectiveness of the 14 handcrafted labeling functions for representative code identification has not been validated across diverse repository types and programming languages beyond Python.
- The human evaluation methodology used a relatively small sample size (20 repositories) and may not capture edge cases or rare failure modes.

## Confidence

**High Confidence**: The core claim that representative code identification improves factual correctness is well-supported by the experimental results showing 65% vs 30% factual correctness for LARCH vs random baseline. The mechanism of using weak supervision with heuristics to avoid manual annotation is also well-established.

**Medium Confidence**: The claim that the specific 14 labeling functions capture sufficient information for representative code identification is reasonable but not thoroughly validated. The prompt engineering approach is supported by general LLM literature but lacks specific validation for readme generation tasks.

**Low Confidence**: The generalizability of LARCH to non-Python repositories and projects without clear entry points remains untested. The long-term maintenance implications of automatically generated readmes (e.g., handling breaking changes) are not addressed.

## Next Checks

1. Test LARCH on a diverse corpus of 100+ repositories spanning different programming languages, project sizes, and architectural patterns to assess generalizability.
2. Conduct a longitudinal study tracking the accuracy of automatically generated readmes over time as repositories evolve and change.
3. Perform ablation studies to identify which labeling functions contribute most to representative code identification performance and whether the approach works with fewer functions.