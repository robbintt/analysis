---
ver: rpa2
title: A Doubly Robust Approach to Sparse Reinforcement Learning
arxiv_id: '2310.15286'
source_url: https://arxiv.org/abs/2310.15286
tags:
- regret
- bound
- learning
- function
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles sparse reinforcement learning in episodic Markov
  decision processes where state transitions are linear in a feature map. Prior online
  algorithms required knowledge of the sparsity level and oracle access to an exploratory
  policy, making them impractical.
---

# A Doubly Robust Approach to Sparse Reinforcement Learning

## Quick Facts
- arXiv ID: 2310.15286
- Source URL: https://arxiv.org/abs/2310.15286
- Reference count: 40
- Primary result: Introduces RDRLVI algorithm achieving O(σmin^(-1) s⋆ H √N) regret in sparse RL with linear MDPs

## Executive Summary
This paper addresses sparse reinforcement learning in episodic Markov decision processes where the optimal Q-function is a sparse linear combination of features. The authors develop a doubly robust estimator that uses feature vectors from all actions rather than just the selected one, and employ a novel analysis technique that uses data from all periods across all episodes. Their Randomized Doubly Robust Lasso Value Iteration (RDRLVI) algorithm overcomes limitations of prior work that required knowledge of sparsity levels and oracle access to exploratory policies. The algorithm achieves a regret bound of O(σmin^(-1) s⋆ H √N) and provides matching lower bounds for certain problem subclasses.

## Method Summary
The method introduces RDRLVI, a doubly robust estimator for sparse RL that leverages feature vectors from all actions and uses data from all periods across all episodes. The algorithm constructs pseudo-rewards that are unbiased for all actions using a random action and imputation, allowing it to use information from unselected actions. It employs an ϵ-greedy policy with resampling to ensure the pseudo-actions match actual actions, enabling unbiased estimation. The algorithm uses Lasso regression for both imputation and estimation, with a critical dependence on the restrictive minimum eigenvalue (RME) σmin of the feature Gram matrix.

## Key Results
- RDRLVI achieves O(σmin^(-1) s⋆ H √N) regret in sparse linear MDPs
- Regret bound depends on RME σmin, showing different scaling when σmin ≥ √(s⋆/d) vs σmin < √(s⋆/d)
- Provides matching lower bounds for a subclass of problems
- Numerical experiments validate performance improvements over previous methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The doubly robust estimator allows using feature vectors from all actions, not just the selected one, which improves estimation accuracy.
- Mechanism: By constructing a pseudo-reward that is unbiased for all actions using a random action and imputation, the algorithm can use all available feature information to estimate Q-values, rather than discarding information from unselected actions.
- Core assumption: The feature vectors from all actions contain useful information for estimating the optimal Q-value function, and the pseudo-reward construction is indeed unbiased.
- Evidence anchors:
  - [abstract] "We overcome these limitations by combining the doubly robust method that allows one to use feature vectors of all actions"
  - [section 4.1] "we consider the estimated Q-value function for unselected actions a ≠ a(τ)k as missing data and apply the DR method"
- Break condition: If the pseudo-reward construction fails to be unbiased, or if the feature vectors from unselected actions are not informative, the benefit disappears.

### Mechanism 2
- Claim: Using data from all periods in all episodes (rather than partitioning episodes) increases effective sample size and improves estimation.
- Mechanism: By carefully accounting for dependencies between Q-value estimates across periods, the algorithm can use all n episodes for each period's estimation, rather than splitting into H partitions, effectively increasing sample size from n/H to n.
- Core assumption: The dependencies between Q-value estimates across periods can be properly accounted for in the analysis without introducing bias.
- Evidence anchors:
  - [abstract] "a novel analysis technique that enables the algorithm to use data from all periods in all episodes, rather than partitioning episodes"
  - [section 4.1] "our estimator utilizes data that are not used by previous works in that (i) we use unbiased pseudo-rewards and feature vectors of all arms in A and (ii) we use all data points in τ ∈ [n]"
- Break condition: If the dependency analysis is flawed or if the increased sample size doesn't translate to better estimates due to correlation, the benefit disappears.

### Mechanism 3
- Claim: The restrictive minimum eigenvalue (RME) σmin determines the difficulty of sparse feature estimation and thus affects regret bounds.
- Mechanism: When σmin is large (σmin ≥ s⋆/d), the algorithm can identify the s⋆ non-zero features more easily, leading to better regret bounds that don't depend on ambient dimension d. When σmin is small, the algorithm must estimate all d features.
- Core assumption: The RME is a meaningful measure of feature identifiability in the sparse setting.
- Evidence anchors:
  - [abstract] "The regret of the proposed algorithm is O(σ^{-1}_min s_⋆ H √N)"
  - [section 3.2] "The bound (4) generalizes Ω(√s_* dN) lower bound for sparse linear bandits... when RME σ_min(ΣU, s⋆) ≤ √(s_⋆/d)"
- Break condition: If the RME doesn't capture the true difficulty of feature estimation, or if other factors dominate the regret.

## Foundational Learning

- Concept: Sparse linear function approximation
  - Why needed here: The problem assumes the optimal Q-function is a sparse linear combination of features, which allows for more efficient learning than general function approximation.
  - Quick check question: Why is sparse linear approximation more efficient than general linear approximation in high dimensions?

- Concept: Doubly robust estimation
  - Why needed here: DR estimation combines an imputation estimator with inverse propensity weighting to create an unbiased estimator that can use information from all actions.
  - Quick check question: What makes the pseudo-reward in equation (8) unbiased for all actions?

- Concept: Restrictive minimum eigenvalue
  - Why needed here: RME measures the identifiability of sparse features and determines whether regret bounds depend on ambient dimension d.
  - Quick check question: How does RME differ from standard minimum eigenvalue in the context of sparse estimation?

## Architecture Onboarding

- Component map: State and action observation -> Resampling mechanism -> Pseudo-reward generation -> Imputation estimator update -> Q-value estimator update -> Policy selection

- Critical path:
  1. Observe state and take action
  2. Resample until pseudo-action matches actual action
  3. Compute pseudo-rewards for all actions
  4. Update imputation estimator
  5. Update Q-value estimator using all actions
  6. Select next action using updated Q-values

- Design tradeoffs:
  - Computational cost vs. sample efficiency: Using all actions increases computation but improves sample efficiency
  - Exploration vs. exploitation: ϵ-greedy balances finding good policies vs. exploiting known good actions
  - Regularization vs. bias: Lasso regularization helps with sparsity but may introduce bias

- Failure signatures:
  - High regret despite many episodes: May indicate poor RME or incorrect feature mapping
  - Algorithm gets stuck in resampling: May indicate exploration parameter too low
  - Estimates don't converge: May indicate insufficient regularization or poor feature selection

- First 3 experiments:
  1. Verify the pseudo-reward is unbiased by checking E[˜Y] = Y for a simple case
  2. Test estimation accuracy with synthetic data where ground truth is known
  3. Compare regret on a simple MDP with known optimal policy to verify convergence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the restrictive minimum eigenvalue (RME) σmin(ΣU, s⋆) behave for different sparse MDP instances and feature distributions?
- Basis in paper: [explicit] The paper establishes a lower bound on regret that critically depends on σmin, showing different scaling for SMDPs in SE (σ2min ≥ s⋆/d) vs SH (σ2min < s⋆/d).
- Why unresolved: The paper does not provide a comprehensive characterization of when RME is large or small for various problem classes.
- What evidence would resolve it: Empirical studies across diverse MDP structures and feature distributions, or theoretical bounds relating RME to problem parameters.

### Open Question 2
- Question: Can the gap in the regret bound for SMDPs in SH (when σ2min ≤ s⋆/d) be closed to match the upper bound?
- Basis in paper: [explicit] The lower bound for SMDPs in SH includes a factor of √d, while the upper bound does not. The paper notes this gap needs to be addressed.
- Why unresolved: The current analysis relies on a restricted eigenvalue condition that fails when σ2min is too small, preventing matching upper and lower bounds.
- What evidence would resolve it: New algorithmic techniques that do not depend on the restricted eigenvalue, or refined lower bound proofs that exploit problem structure.

### Open Question 3
- Question: How does the dimension d affect the regret bound when the RME is sufficiently large (σ2min ≥ s⋆/d)?
- Basis in paper: [explicit] The paper shows that when RME is large, the regret bound is nearly independent of d, but the exact scaling with d is not fully characterized.
- Why unresolved: The analysis focuses on the RME as the key parameter, but does not provide a detailed characterization of the d-dependence in the logarithmic terms.
- What evidence would resolve it: Numerical experiments varying d for different RME values, or theoretical bounds on the logarithmic terms involving d.

## Limitations

- Theoretical analysis relies on strong assumptions including known feature maps, known reward functions, and restrictive minimum eigenvalue conditions that may not hold in practice
- Computational complexity of the resampling mechanism is not thoroughly analyzed, potentially limiting scalability to large action spaces
- Does not address model misspecification or non-sparse true Q-functions, which could significantly degrade performance

## Confidence

**High Confidence**: The doubly robust estimator construction and its unbiasedness (equations 7-8) - this follows standard DR methodology and the mathematical derivation appears sound. The regret upper bound O(σmin^(-1) s⋆ H √N) when σmin ≥ √(s⋆/d) is well-supported by the analysis, though dependent on strong assumptions.

**Medium Confidence**: The matching lower bound argument - while the paper claims a matching lower bound for a subclass of problems, the construction and proof details are not fully elaborated in the main text. The practical significance of the RME condition in real applications remains somewhat unclear.

**Low Confidence**: The empirical validation - the numerical experiments are presented but lack details on hyperparameter tuning, sensitivity analysis, and comparison with more baselines. The synthetic setup may not capture real-world challenges like feature misspecification or non-stationarity.

## Next Checks

1. **Robustness to RME violations**: Systematically test RDRLVI's performance when σmin < √(s⋆/d) across a range of problem instances to verify the predicted transition from dimension-dependent to dimension-free regret bounds.

2. **Computational scalability analysis**: Measure the actual computational cost of the resampling mechanism as a function of action space size |A| and feature dimension d, comparing against theoretical complexity predictions.

3. **Sensitivity to hyperparameter choices**: Conduct ablation studies varying the regularization parameters λ_Im and λ_Est, exploration rate ϵ, and sparsity assumptions s⋆ to identify which parameters most critically affect performance and whether automatic tuning methods could be developed.