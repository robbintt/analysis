---
ver: rpa2
title: Replay Buffer with Local Forgetting for Adapting to Local Environment Changes
  in Deep Model-Based Reinforcement Learning
arxiv_id: '2303.08690'
source_url: https://arxiv.org/abs/2303.08690
tags:
- buffer
- replay
- phase
- learning
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel replay buffer technique, called Local
  Forgetting (LOFO), designed to improve adaptation in deep model-based reinforcement
  learning. The key challenge addressed is the inability of current deep MBRL methods
  to adapt to local changes in the environment due to catastrophic forgetting and
  interference from stale data in traditional replay buffers.
---

# Replay Buffer with Local Forgetting for Adapting to Local Environment Changes in Deep Model-Based Reinforcement Learning

## Quick Facts
- arXiv ID: 2303.08690
- Source URL: https://arxiv.org/abs/2303.08690
- Reference count: 40
- The paper introduces Local Forgetting (LOFO), a replay buffer technique that removes only the oldest samples from the local neighborhood of newly observed samples to improve adaptation in deep MBRL.

## Executive Summary
This paper addresses a critical challenge in deep model-based reinforcement learning: adapting to local environmental changes while avoiding catastrophic forgetting. The proposed solution, Local Forgetting (LOFO), is a replay buffer technique that removes only the oldest samples from the local neighborhood of newly observed samples, rather than from the entire buffer. This approach maintains diversity across the state-space while allowing the agent to adapt to local changes in the reward function. Experiments across multiple domains demonstrate that LOFO significantly outperforms traditional FIFO replay buffers in adaptation scenarios.

## Method Summary
The LOFO replay buffer works by maintaining a state locality function learned through contrastive learning, which maps temporally close states to closer embeddings. When new samples are observed, the buffer identifies the local neighborhood using this locality function and removes only the oldest sample from that neighborhood. This preserves diversity across the state-space while removing potentially stale or incorrect data near the observed change. The method is evaluated with deep MBRL algorithms like Dyna-Q, PlaNet, and DreamerV2 on domains with local reward function changes.

## Key Results
- LOFO significantly outperforms traditional FIFO replay buffers in adapting to local changes in reward functions
- The method maintains world model accuracy across the state-space while enabling local adaptation
- LOFO demonstrates robust performance across multiple domains including MountainCar, MiniGrid, Reacher, and RandomizedReacher

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Removing only the oldest samples in the local neighborhood of newly observed samples prevents interference from stale data while maintaining coverage across the state-space.
- **Mechanism**: The LOFO replay buffer maintains diversity by removing outdated local samples, ensuring the buffer contains recent and relevant data for both adaptation and general accuracy.
- **Core assumption**: A locality function can effectively identify samples within the local neighborhood of newly observed samples.
- **Evidence anchors**:
  - [abstract] "By removing only samples from the buffer from the local neighbourhood of the newly observed samples, deep world models can be built that maintain their accuracy across the state-space..."
  - [section] "When a change occurs in the environment and the agent observes that change, adding the new samples to the replay buffer and removing the oldest samples from only the local neighbourhood of the new data instead of the oldest samples from the entire replay buffer facilitates removal of the potentially incorrect and stale data sooner."
  - [corpus] Weak evidence; no direct comparison of LOFO to locality-based removal strategies in the corpus.
- **Break condition**: The locality function fails to accurately identify the local neighborhood, leading to either excessive data removal or retention of stale samples.

### Mechanism 2
- **Claim**: Contrastive learning effectively learns a state locality function that maps temporally close states to closer embeddings.
- **Mechanism**: Training an embedding function using contrastive learning ensures that states reachable with fewer actions have smaller distances in the embedding space, providing a proxy for state locality.
- **Core assumption**: Temporally close states are good proxies for spatially close states in the state-space.
- **Evidence anchors**:
  - [abstract] "We train this locality function using contrastive learning (Hadsell et al., 2006; Dosovitskiy et al., 2014; Wu et al., 2018) during the initial stages of learning..."
  - [section] "Using contrastive learning (Hadsell et al., 2006; Dosovitskiy et al., 2014; Wu et al., 2018) we learn neural embedding representation of states such that states that are temporally closer, i.e., reachable with fewer actions are also closer in their neural embedding representation."
  - [corpus] No direct evidence; the corpus does not mention contrastive learning for locality functions.
- **Break condition**: The embedding function fails to generalize beyond the initial training data, leading to inaccurate locality estimates.

### Mechanism 3
- **Claim**: The LOFO replay buffer enables adaptation to local changes in the reward function by maintaining accurate world models across the state-space.
- **Mechanism**: By preventing catastrophic forgetting and interference, the LOFO replay buffer allows deep MBRL methods to maintain accurate predictions for states outside the T1-zone, enabling effective adaptation when the reward function changes locally.
- **Core assumption**: Maintaining accurate predictions across the state-space is necessary for effective adaptation to local changes.
- **Evidence anchors**:
  - [abstract] "Consequently, updating the deep world model with samples drawn randomly from this replay buffer results in a world model that is approximately accurate across the state-space at each moment in time."
  - [section] "Since only the samples in a local neighbourhood of the new samples are removed, old samples in other parts of the state-space including those that have not been visited recently remain in the replay buffer."
  - [corpus] Weak evidence; the corpus does not directly compare adaptation performance with and without LOFO.
- **Break condition**: The buffer size becomes too small to maintain sufficient diversity, or the locality function becomes too restrictive, leading to loss of coverage.

## Foundational Learning

- **Concept: Catastrophic forgetting**
  - Why needed here: Understanding catastrophic forgetting is crucial for grasping why traditional FIFO replay buffers fail to adapt to local changes.
  - Quick check question: What happens to the accuracy of a deep neural network when it is trained on new data without retaining old data?

- **Concept: Replay buffers**
  - Why needed here: Replay buffers are essential for mitigating catastrophic forgetting in deep MBRL methods, and understanding their limitations is key to appreciating the LOFO approach.
  - Quick check question: How do replay buffers help mitigate the effects of catastrophic forgetting in deep learning?

- **Concept: Contrastive learning**
  - Why needed here: Contrastive learning is used to learn the state locality function, which is a core component of the LOFO replay buffer.
  - Quick check question: How does contrastive learning learn representations that bring similar samples closer and dissimilar samples farther apart?

## Architecture Onboarding

- **Component map**: Agent -> Locality Function -> LOFO Replay Buffer -> Deep MBRL Method -> World Model

- **Critical path**:
  1. Agent observes new sample (s, a, r, s')
  2. Locality function identifies samples in the local neighborhood of s
  3. If the neighborhood has enough samples, the oldest one is removed
  4. New sample is added to the replay buffer
  5. Deep MBRL method samples from the replay buffer to train the world model

- **Design tradeoffs**:
  - Buffer size vs. locality function accuracy: A smaller buffer requires a more accurate locality function
  - Locality function complexity vs. generalization: A more complex function may overfit to initial data
  - Removal threshold vs. coverage: A higher threshold maintains more samples but risks stale data

- **Failure signatures**:
  - Poor adaptation: Indicates the locality function is not accurately identifying local neighborhoods
  - Catastrophic forgetting: Suggests the buffer is removing too many relevant samples
  - Interference from stale data: Implies the locality function is too lenient in removing samples

- **First 3 experiments**:
  1. Test LOFO with a simple locality function (e.g., Euclidean distance) on a low-dimensional domain
  2. Compare adaptation performance of LOFO vs. FIFO on a domain with known local changes
  3. Evaluate the impact of locality function accuracy on adaptation performance

## Open Questions the Paper Calls Out

- How does LOFO perform when the environment exhibits non-stationary transition dynamics, not just non-stationary rewards?
- What is the optimal strategy for learning the state locality function in complex environments with significant exploration challenges?
- How does the performance of LOFO scale to larger and more complex domains with longer decision horizons?

## Limitations
- The effectiveness of the locality function depends heavily on the quality of the contrastive learning embedding
- Performance in high-dimensional state spaces may be compromised if the locality function cannot accurately identify relevant neighborhoods
- Limited empirical validation of diversity maintenance across the state-space

## Confidence
- High confidence: The fundamental insight that local forgetting can improve adaptation while maintaining general accuracy across the state-space
- Medium confidence: The contrastive learning approach for learning locality functions, as implementation details are sparse
- Low confidence: The scalability of LOFO to very high-dimensional domains and complex environment dynamics

## Next Checks
1. **Locality Function Ablation**: Compare LOFO performance using different locality functions (Euclidean distance vs. contrastive learning embedding) on the same domain to isolate the impact of the embedding quality
2. **Buffer Size Sensitivity**: Systematically vary the replay buffer size and measure how adaptation performance scales, particularly focusing on the tradeoff between coverage and stale data removal
3. **Cross-Domain Generalization**: Test LOFO on a domain where the state distribution changes significantly between phases (not just the reward function) to evaluate whether the locality function can adapt to broader environmental changes