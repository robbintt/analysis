---
ver: rpa2
title: 'Enhancing Recipe Retrieval with Foundation Models: A Data Augmentation Perspective'
arxiv_id: '2312.04763'
source_url: https://arxiv.org/abs/2312.04763
tags:
- recipe
- image
- visual
- retrieval
- food
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses cross-modal recipe retrieval, where the goal
  is to search for recipes given a food image and vice versa. The key challenge is
  that recipes and images contain information that is not directly alignable with
  each other.
---

# Enhancing Recipe Retrieval with Foundation Models: A Data Augmentation Perspective

## Quick Facts
- **arXiv ID**: 2312.04763
- **Source URL**: https://arxiv.org/abs/2312.04763
- **Reference count**: 40
- **Primary result**: Achieves state-of-the-art performance on Recipe1M with R@1=40.1 for image-to-recipe retrieval using foundation model-based data augmentation.

## Executive Summary
This paper addresses cross-modal recipe retrieval, where the goal is to search for recipes given food images and vice versa. The key challenge is that recipes and images contain information that is not directly alignable with each other. To address this, the authors propose a novel framework called CAR that uses foundation models for data augmentation. Specifically, they employ adapter layers to consolidate the pre-trained CLIP model with less computation cost, use SAM to segment food images into key ingredients, and leverage LLM to generate visual imagination descriptions from recipes. They also introduce a multi-level circle loss to align the original and augmented data pairs. On the Recipe1M dataset, CAR outperforms existing methods by a large margin, achieving state-of-the-art performance with a significant reduction in trainable parameters.

## Method Summary
CAR enhances cross-modal recipe retrieval by integrating foundation models for data augmentation with adapter-based transfer learning. The framework uses adapter layers to efficiently fine-tune CLIP for the recipe domain, SAM to segment food images into ingredient regions, and Llama2 to generate visual imagination descriptions from recipes. These augmentations are aligned using a multi-level circle loss that applies asymmetric penalties for positive and negative pairs. The model achieves significant performance gains while reducing trainable parameters from 86.94M to 0.05M compared to full fine-tuning.

## Key Results
- Achieves state-of-the-art R@1=40.1 for image-to-recipe retrieval on Recipe1M
- Reduces trainable parameters from 86.94M to 0.05M using adapter layers
- Outperforms existing methods by a large margin across all retrieval metrics

## Why This Works (Mechanism)

### Mechanism 1
Adapter layers consolidate CLIP without costly full fine-tuning while improving retrieval accuracy. By injecting lightweight downsampling/upsampling projections into CLIP's transformer layers, the model adapts pre-trained representations for food domain without altering frozen CLIP weights. Core assumption: Pre-trained CLIP has general vision-language alignment capability, and small adapter modules can specialize it to the recipe domain.

### Mechanism 2
Foundation models (SAM and Llama2) provide semantically aligned data augmentation for training and evaluation. SAM segments food images into ingredient regions; Llama2 generates visual imagination descriptions from recipes; both augmentations are aligned with cross-modal targets. Core assumption: SAM's segmentations correspond to recipe ingredients; Llama2's generated descriptions reflect visual cues of the dish.

### Mechanism 3
Multi-level circle loss with asymmetric penalties improves alignment over triplet loss. Circle loss applies different penalties for positive/negative pairs in cross-modal and within-recipe alignments; symmetric bidirectional form ensures consistency. Core assumption: Assigning varying penalties to positive/negative pairs during optimization is more flexible and effective than fixed-margin triplet loss.

## Foundational Learning

- **Concept: Cross-modal embedding alignment**
  - Why needed here: Recipe retrieval requires joint visual-text representations in a shared space; misalignment causes retrieval failures.
  - Quick check question: If image and recipe embeddings are not aligned, what retrieval metric is most affected?

- **Concept: Adapter-based transfer learning**
  - Why needed here: Full fine-tuning CLIP is computationally prohibitive; adapters allow efficient domain adaptation.
  - Quick check question: What is the parameter reduction ratio when using adapters instead of full fine-tuning CLIP?

- **Concept: Data augmentation via foundation models**
  - Why needed here: Recipe and image modalities contain non-alignable content; augmentation extracts shared semantic cues.
  - Quick check question: How does SAM's segmentation quality affect retrieval performance if segments include background noise?

## Architecture Onboarding

- **Component map**: Image → EV → circle loss → retrieval; Recipe → ER → circle loss → retrieval
- **Critical path**: Image → EV → circle loss → retrieval; Recipe → ER → circle loss → retrieval
- **Design tradeoffs**:
  - Adapter dimension (64) vs. fine-tuning: fewer parameters but potential underfitting
  - Segment count (4) vs. noise: more segments capture more info but risk background noise
  - Visual imagination description length (30 words) vs. detail: short enough for CLIP input but may miss visual nuance
- **Failure signatures**:
  - Low R@1 but high R@5: embeddings close but not precise enough
  - High training loss, low validation loss: overfitting to augmented data
  - Degraded performance with segments: SAM over-segmentation or background noise
- **First 3 experiments**:
  1. Verify adapter insertion does not break forward pass by checking EV shape matches expectations.
  2. Validate SAM segments correspond to recipe ingredients by manual inspection of sample pairs.
  3. Compare circle loss vs. triplet loss on a small subset to confirm performance gain.

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of adapter architecture (e.g., down/up sampling dimensions, bottleneck size) affect the trade-off between model performance and parameter efficiency in the CAR framework? The paper uses a fixed adapter architecture without exploring how varying the dimensions or structure would impact performance and efficiency.

### Open Question 2
What is the optimal number of image segments to use for data augmentation, and how does this vary based on recipe complexity or dish type? The paper uses a fixed number of segments (4) without exploring how the optimal number might vary based on recipe characteristics.

### Open Question 3
How can the visual imagination descriptions generated by LLM be made more robust to hallucinations and irrelevant information? The paper acknowledges that LLM can generate hallucinated information but does not implement or evaluate concrete solutions to mitigate it.

### Open Question 4
What is the relative contribution of each component of the multi-level circle loss (L(EV,ER), L(ES,ER), L(EV,ED), Lrec) to overall performance? The paper combines all four loss components with equal weights but does not provide ablation studies on individual loss contributions.

### Open Question 5
How does the CAR framework perform on other cross-modal retrieval datasets beyond Recipe1M, particularly those with different domain characteristics? The paper only evaluates on the Recipe1M dataset and does not test generalizability to other cross-modal retrieval tasks.

## Limitations
- Limited ablation of foundation model components without quantifying individual contributions
- Evaluation restricted to Recipe1M dataset without testing cross-dataset generalization
- Computational overhead of inference-time SAM segmentation not addressed for real-time applications

## Confidence
- **High confidence**: Adapter layers significantly reduce trainable parameters while maintaining performance (0.05M vs 86.94M)
- **Medium confidence**: Multi-level circle loss outperforms triplet loss (lacks direct comparison or ablation)
- **Medium confidence**: Foundation model-based augmentation effectiveness demonstrated but individual contributions unclear

## Next Checks
1. **Ablation study of foundation model components**: Evaluate CAR performance with individual foundation model augmentations disabled to quantify their relative contributions.

2. **Cross-dataset generalization test**: Evaluate the trained CAR model on alternative recipe datasets to assess whether foundation model augmentations improve generalization beyond Recipe1M.

3. **Inference efficiency measurement**: Profile end-to-end inference time including SAM segmentation to determine practical computational overhead for real-time deployment.