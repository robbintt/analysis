---
ver: rpa2
title: Feasible Policy Iteration for Safe Reinforcement Learning
arxiv_id: '2304.08845'
source_url: https://arxiv.org/abs/2304.08845
tags:
- feasible
- policy
- function
- region
- constraint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces feasible policy iteration (FPI), the first
  dynamic programming algorithm for safe reinforcement learning. FPI alternates between
  policy evaluation, feasible region identification, and feasible policy improvement.
---

# Feasible Policy Iteration for Safe Reinforcement Learning

## Quick Facts
- arXiv ID: 2304.08845
- Source URL: https://arxiv.org/abs/2304.08845
- Reference count: 40
- Primary result: First dynamic programming algorithm for safe RL achieving zero constraint violation and monotonic expansion of feasible region

## Executive Summary
This paper introduces Feasible Policy Iteration (FPI), a dynamic programming algorithm for safe reinforcement learning that guarantees zero constraint violation while converging to the optimal safe policy. FPI alternates between policy evaluation, feasible region identification, and feasible policy improvement using a region-wise update rule that maximizes value inside the feasible region and minimizes a feasibility function outside it. The method proves monotonic expansion of the feasible region and convergence to the optimal state-value function through the feasible Bellman equation.

## Method Summary
FPI solves constrained MDPs by iteratively computing the constraint decay function (CDF) representing the feasible region, evaluating the state-value function, and improving the policy using region-wise updates. Inside the feasible region, the policy maximizes the state-value function while ensuring the next state remains feasible; outside the region, it minimizes the feasibility function. The algorithm combines with SAC and PPO as FPI-SAC and FPI-PPO respectively, using cross-entropy loss for CDF learning and interior point methods for constrained optimization.

## Key Results
- Achieves zero constraint violation on classic control tasks (ACC, LK, Pendulum, Quadrotor)
- Demonstrates monotonic expansion of feasible region through visualizations
- Outperforms existing methods on Safety Gym tasks in constraint adherence while maintaining comparable or higher reward performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FPI achieves monotonic expansion of the feasible region and monotonic improvement of the state-value function
- Mechanism: Alternates between policy evaluation, feasible region identification, and feasible policy improvement using different update rules inside and outside the feasible region
- Core assumption: CDF accurately represents feasible region and can be computed iteratively
- Evidence anchors: Abstract mentions region-wise update rule; Theorem 3 proves monotonic expansion; limited corpus evidence for this specific approach
- Break condition: CDF approximation failure or infeasible constrained optimization

### Mechanism 2
- Claim: FPI converges to maximum feasible region and optimal state-value function
- Mechanism: Uses feasible Bellman equation as optimality condition with contraction mapping property
- Core assumption: Feasible Bellman operator is a contraction mapping ensuring unique fixed point
- Evidence anchors: Abstract states convergence using feasible Bellman equation; Theorem 5 and 6 establish optimality and convergence; limited corpus evidence for convergence proofs in safe RL
- Break condition: Non-unique solution to feasible Bellman equation or broken contraction property due to approximation errors

### Mechanism 3
- Claim: FPI maintains strict safety (zero constraint violations) throughout learning
- Mechanism: Iteratively uses feasible region of last policy to constrain current policy, ensuring each intermediate policy is feasible
- Core assumption: Initial policy is feasible and policy improvement never generates constraint-violating actions
- Evidence anchors: Abstract reports zero constraint violation on low-dimensional tasks; section states monotonic expansion and feasibility in each iteration; limited corpus evidence for safe RL methods maintaining safety during training
- Break condition: Non-feasible initial policy or incorrect feasible region identification due to CDF approximation errors

## Foundational Learning

- Concept: Constrained Markov Decision Processes (CMDPs)
  - Why needed here: Problem formulation requires maximizing rewards while satisfying state-wise constraints
  - Quick check question: What is the difference between a standard MDP and a CMDP?

- Concept: Feasibility functions and their relationship to constraint satisfaction
  - Why needed here: CDF aggregates infinitely many state-wise constraints into single function for tractable optimization
  - Quick check question: How does CDF value relate to time steps until constraint violation?

- Concept: Contraction mappings and fixed-point theorems
  - Why needed here: Convergence proofs rely on showing feasible Bellman operator is contraction mapping
  - Quick check question: What are conditions for operator to be contraction mapping under infinity norm?

## Architecture Onboarding

- Component map: Policy Evaluation -> Feasible Region Identification (CDF) -> Feasible Policy Improvement
- Critical path: Feasible region identification and policy improvement steps ensure monotonic expansion and value improvement
- Design tradeoffs: Trades computational complexity (solving constrained optimization) for guaranteed safety and convergence
- Failure signatures: CDF approximation failure, infeasible constrained optimization, or incorrect feasible region identification
- First 3 experiments:
  1. Implement feasible region identification step and verify CDF accurately represents feasible region on simple 1D task
  2. Implement feasible policy improvement step and verify it expands feasible region and improves state-value on simple 1D task
  3. Combine all three steps and verify convergence to maximum feasible region and optimal state-value on simple 1D task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can FPI guarantee monotonic expansion and improvement for all initial policies?
- Basis in paper: Paper states guarantees but doesn't address whether they hold for all initial policies
- Why unresolved: No rigorous proof that FPI always expands feasible region and improves state-value regardless of initial policy
- What evidence would resolve it: Formal proof showing FPI guarantees monotonic expansion and improvement for any initial policy

### Open Question 2
- Question: How does FPI performance compare to other safe RL methods on wider range of tasks?
- Basis in paper: Limited experimental scope comparing only to few baseline methods
- Why unresolved: More comprehensive evaluation needed across diverse tasks and algorithms
- What evidence would resolve it: Extensive experiments comparing FPI to wide range of safe RL algorithms on various benchmark tasks

### Open Question 3
- Question: How sensitive is FPI to feasibility threshold and interior point method parameters?
- Basis in paper: Mentions use of feasibility threshold and interior point method but no sensitivity analysis
- Why unresolved: No analysis on impact of these parameters on FPI performance
- What evidence would resolve it: Sensitivity analysis exploring effects of different feasibility threshold values and interior point method parameters

## Limitations
- Experimental validation limited to relatively simple benchmark tasks; scalability to high-dimensional problems unclear
- Requires accurate dynamics models for CDF computation, limiting real-world applicability with significant model uncertainty
- Computational complexity of constrained optimization steps not discussed, potentially prohibitive in high-dimensional spaces

## Confidence
- Monotonic expansion and convergence (High confidence): Rigorous theoretical framework with formal proofs and sound mathematical arguments
- Zero constraint violation in practice (Medium confidence): Reported on benchmark tasks but needs broader testing for robustness
- Superior performance compared to baselines (Medium confidence): Competitive results on Safety Gym but sample complexity and computational requirements need more detailed analysis

## Next Checks
1. Test FPI with multiple random initial policies (including those near constraint boundaries) to verify zero constraint violation maintenance and convergence to optimal safe policies
2. Implement FPI on higher-dimensional control tasks to empirically evaluate computational complexity scaling with problem size
3. Evaluate FPI in simulated robotics environment with significant model uncertainty to assess impact of CDF computation accuracy on safety guarantees and performance