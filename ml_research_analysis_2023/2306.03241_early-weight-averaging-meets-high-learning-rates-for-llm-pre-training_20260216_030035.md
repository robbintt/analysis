---
ver: rpa2
title: Early Weight Averaging meets High Learning Rates for LLM Pre-training
arxiv_id: '2306.03241'
source_url: https://arxiv.org/abs/2306.03241
tags:
- training
- lawa
- checkpoints
- learning
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates early weight averaging (LAWA) as a technique
  to accelerate convergence and improve generalization for large language model (LLM)
  training. The method involves averaging intermediate model checkpoints saved during
  training, with a focus on early-mid stages.
---

# Early Weight Averaging meets High Learning Rates for LLM Pre-training

## Quick Facts
- arXiv ID: 2306.03241
- Source URL: https://arxiv.org/abs/2306.03241
- Reference count: 40
- Key outcome: LAWA averaging intermediate checkpoints early in LLM training boosts perplexity and zero-shot performance, saving GPU hours.

## Executive Summary
This paper introduces Early Weight Averaging (LAWA), a technique that averages intermediate model checkpoints during LLM pretraining to accelerate convergence and improve generalization. By averaging checkpoints from early-mid training stages in a sliding window fashion, LAWA smooths the optimization trajectory, acting as implicit regularization and approximating lower learning rates. Experiments on Pythia LLMs (1B-12B) trained on PILE show significant perplexity and zero-shot task gains, with larger benefits for moderate-sized models trained with higher learning rates. LAWA also mitigates loss spikes during training. Preliminary results on diffusion models indicate similar improvements.

## Method Summary
LAWA averages the last k checkpoints within a moving window during training, sliding at fixed intervals. For LLMs, k=5 and interval=3K steps were used; checkpoints were saved every 1K steps. Derived checkpoints were evaluated on held-out PILE subsets for perplexity and zero-shot academic QA tasks. Ablations varied k and intervals; small-scale experiments with PreAct ResNet-18 on CIFAR-10/100 and a toy linear model validated the approach across architectures.

## Key Results
- LAWA-derived checkpoints achieve lower perplexity and better zero-shot performance with fewer training steps, saving GPU hours.
- Gains are more pronounced for moderate-sized models (Pythia-1B, 2.8B) trained with higher learning rates.
- LAWA mitigates loss spikes during training by smoothing outliers in the training trajectory.
- Preliminary results on diffusion models show similar improvements.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Averaging early checkpoints reduces variance in weight updates, providing implicit regularization.
- Mechanism: Smooths optimization trajectory, preventing sharp minima and encouraging flatter, more generalizable regions.
- Core assumption: Checkpoints lie within a basin of attraction of a single generalizable minimum.
- Evidence anchors: Abstract states averaging boosts test performance; section notes LAWA reduces variance in weight updates; corpus cites related work on bias-variance tradeoff.
- Break condition: If checkpoints are from different basins or exhibit high mode connectivity error, averaging may degrade performance.

### Mechanism 2
- Claim: LAWA approximates lower learning rates, leading to more stable convergence.
- Mechanism: Averaging weights from different steps simulates a lower learning rate, smoothing rapid changes and preventing overshooting.
- Core assumption: Aggressive early learning rate decays later; averaging early checkpoints captures this phase.
- Evidence anchors: Abstract notes higher gains for models trained with high learning rates; section states LAWA approximates tenfold lower learning rate; corpus includes related work on learning rate schedules.
- Break condition: If learning rates are already low or constant, benefit diminishes.

### Mechanism 3
- Claim: LAWA mitigates loss spikes by smoothing out outliers.
- Mechanism: Averaging weights over steps reduces impact of transient degradations, leading to smoother loss landscape.
- Core assumption: Loss spikes are temporary and not indicative of true generalization.
- Evidence anchors: Abstract mentions spikes were mitigated by averaging; section explains LAWA mitigates spikes via smoothing effect; corpus cites work on instability in downstream task performance.
- Break condition: If loss spikes are persistent or indicative of deeper issues, averaging may not resolve the problem.

## Foundational Learning

- **Stochastic Gradient Descent (SGD) dynamics and learning rate schedules**: Understanding how learning rates affect convergence and generalization is crucial for interpreting LAWA's effectiveness. Quick check: What happens to the optimization trajectory when the learning rate is high versus low?
- **Weight averaging techniques (e.g., SWA, EMA)**: Knowing how and why weight averaging works helps understand LAWA's mechanisms. Quick check: How does averaging weights from different checkpoints affect the model's loss landscape?
- **Linear mode connectivity**: The effectiveness of LAWA depends on whether checkpoints are in the same basin; linear mode connectivity indicates this. Quick check: What does it mean for two model checkpoints to be linearly mode connected?

## Architecture Onboarding

- **Component map**: Training loop with checkpoint saving every 1K steps -> LAWA module averaging last k checkpoints -> Evaluation pipeline for perplexity and zero-shot tasks -> Ablation studies varying k and intervals.
- **Critical path**: 1) Save intermediate checkpoints during training. 2) Apply LAWA to derive new checkpoints by averaging selected ones. 3) Evaluate derived checkpoints on validation/test sets and downstream tasks. 4) Compare performance against original checkpoints.
- **Design tradeoffs**: Disk space vs. number of checkpoints averaged (larger k requires more storage); frequency of averaging vs. smoothness of trajectory (more frequent may over-smooth); model size vs. effectiveness of LAWA (moderate models benefit more).
- **Failure signatures**: Performance degradation if checkpoints are from different basins; minimal gains if learning rates are already optimal or low; over-smoothing leading to loss of important model characteristics.
- **First 3 experiments**: 1) Implement LAWA with k=5 and interval=3K on a small Pythia model and compare perplexity. 2) Vary k (2, 5, 10) while keeping interval fixed to find optimal checkpoint count. 3) Test LAWA on a non-LLM model (e.g., PreAct ResNet-18) to validate generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Why does LAWA provide more significant improvements for moderate-sized LLMs (Pythia-1B, 2.8B) compared to larger models (Pythia-6.9B, 12B)?
- **Basis in paper**: The paper notes gains are more pronounced for moderate-sized models and attributes this to differences in learning rates used during training, but does not provide definitive proof of the mechanism.
- **Why unresolved**: The exact relationship between model size, learning rate, and LAWA effectiveness is not fully explained; the paper conjectures but does not provide proof.
- **What evidence would resolve it**: Experiments varying learning rates independently for different model sizes, or ablation studies isolating the effect of batch size vs. learning rate on LAWA performance.

### Open Question 2
- **Question**: What is the optimal number of checkpoints (k) and averaging interval (ν) for LAWA across different model architectures and training regimes?
- **Basis in paper**: The paper performs ablations with k={2,5,10} and ν={1K,2K,5K,10K}, finding k=5 and ν=1K works best, but acknowledges this may not generalize.
- **Why unresolved**: Optimal parameters likely depend on factors like model size, dataset, learning rate schedule, and optimization dynamics; the study only tests a limited range on Pythia models.
- **What evidence would resolve it**: Systematic hyperparameter sweeps across diverse model architectures, datasets, and training configurations to establish general guidelines for k and ν selection.

### Open Question 3
- **Question**: How does LAWA interact with learning rate schedules, and can it provide consistent benefits throughout training if constant learning rates are used?
- **Basis in paper**: The paper observes LAWA provides consistent gains when training with a constant learning rate, suggesting lower gains at later stages with decayed learning rates might be due to the schedule itself.
- **Why unresolved**: The paper does not explore different learning rate schedules or their interaction with LAWA in depth.
- **What evidence would resolve it**: Experiments comparing LAWA performance across various learning rate schedules (constant, step decay, cosine, etc.) to determine optimal pairing strategies.

### Open Question 4
- **Question**: Can LAWA be effectively applied to other generative model architectures beyond LLMs and diffusion models, such as GANs or VAEs?
- **Basis in paper**: The paper only demonstrates LAWA on LLMs and diffusion models, leaving open the question of its applicability to other generative architectures.
- **Why unresolved**: Different generative model architectures have distinct training dynamics and loss landscapes, which may affect how checkpoint averaging influences convergence and sample quality.
- **What evidence would resolve it**: Applying LAWA to GANs, VAEs, and other generative models, evaluating its impact on training stability, sample quality metrics, and mode coverage.

## Limitations
- LAWA's effectiveness is tied to moderate-sized models and higher learning rates, with less benefit for smaller models or when learning rates are already low.
- The exact mechanisms through which LAWA improves generalization—implicit regularization, learning rate approximation, or loss smoothing—are not fully isolated or characterized.
- Ablation studies suggest optimal k=5 and interval=3K for LLMs, but do not explore the full hyperparameter space, leaving questions about robustness across architectures and datasets.

## Confidence
- **High confidence**: LAWA reduces loss spikes and provides consistent perplexity improvements when applied to Pythia LLMs trained with higher learning rates.
- **Medium confidence**: LAWA approximates lower learning rates and offers generalization gains via implicit regularization, though exact mechanisms are not fully isolated.
- **Low confidence**: The benefit of LAWA generalizes robustly to diffusion models and other non-LLM architectures; preliminary results are mentioned but not detailed.

## Next Checks
1. **Hyperparameter sensitivity**: Systematically vary k (2, 5, 10) and averaging intervals (1K, 2K, 5K steps) on a held-out Pythia LLM validation set to map robustness of LAWA gains and identify failure modes.
2. **Architecture generalization**: Apply LAWA to a non-LLM architecture (e.g., PreAct ResNet-18) with multiple learning rates and fixed LR schedule, comparing baseline vs. LAWA-derived checkpoints for test accuracy to test mechanism claims.
3. **Mechanism isolation**: Train a small linear model with two learning rates (high and low), apply LAWA with varying k, and measure changes in final loss and generalization to empirically test whether LAWA approximates lower learning rates or provides regularization.