---
ver: rpa2
title: 'RobustFair: Adversarial Evaluation through Fairness Confusion Directed Gradient
  Search'
arxiv_id: '2305.10906'
source_url: https://arxiv.org/abs/2305.10906
tags:
- fairness
- biased
- instances
- robustfair
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RobustFair, a novel approach for evaluating
  the accurate fairness of deep neural networks (DNNs) under adversarial perturbations.
  RobustFair employs a fairness confusion matrix to guide differentiated gradient
  searches, identifying adversarial instances that undermine prediction accuracy (robustness)
  or cause biased predictions (individual fairness).
---

# RobustFair: Adversarial Evaluation through Fairness Confusion Directed Gradient Search

## Quick Facts
- arXiv ID: 2305.10906
- Source URL: https://arxiv.org/abs/2305.10906
- Reference count: 31
- Key outcome: RobustFair detects 1.77-11.87x more adversarial perturbations than state-of-the-art techniques, improving accurate fairness by 21% and individual fairness by 19% without sacrificing accuracy.

## Executive Summary
This paper introduces RobustFair, a novel approach for evaluating the accurate fairness of deep neural networks (DNNs) under adversarial perturbations. RobustFair employs a fairness confusion matrix to guide differentiated gradient searches, identifying adversarial instances that undermine prediction accuracy (robustness) or cause biased predictions (individual fairness). The method uses Taylor expansions to approximate ground truths of perturbed instances, enabling detection of subtle defects often overlooked in standard evaluations. Experimental results on benchmark datasets demonstrate that RobustFair significantly outperforms existing techniques in detecting fairness-related adversarial instances and improves model fairness through retraining.

## Method Summary
RobustFair is a two-phase adversarial evaluation framework that generates fairness-aware adversarial instances through differentiated gradient searches guided by a fairness confusion matrix. The method first performs global generation to create diverse adversarial instances by perturbing seeds in fairness confusion directions (false fair, true biased, false biased). It then refines these instances locally by perturbing attributes with minimal gradient magnitudes. For each generated instance, RobustFair approximates the ground truth using first-order Taylor expansion of the loss function. The approach is evaluated on four benchmark datasets, showing significant improvements in detecting fairness-related defects and enhancing model fairness through retraining.

## Key Results
- RobustFair detects 1.77-11.87x more adversarial perturbations than state-of-the-art techniques
- Retraining with RobustFair-generated instances improves accurate fairness by 21% and individual fairness by 19%
- The method achieves these improvements without sacrificing prediction accuracy
- RobustFair successfully identifies subtle fairness defects that traditional adversarial attacks miss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RobustFair detects adversarial instances that cause both accuracy loss and biased predictions by using fairness confusion matrix-guided gradient search.
- Mechanism: The fairness confusion matrix categorizes predictions into four types: true fair, true biased, false fair, and false biased. RobustFair then performs gradient-based perturbations in different directions for each category. For false fair defects, it perturbs non-sensitive attributes with matching gradient signs to increase loss in both the instance and its similar counterpart. For true biased defects, it perturbs attributes with opposite gradient signs so that the instance's prediction gets closer to ground truth while the similar counterpart's prediction deviates. For false biased defects, it does the opposite: moves the instance's prediction away from ground truth while pulling the similar counterpart's prediction closer.
- Core assumption: Gradient sign alignment/disalignment between an instance and its maximally dissimilar similar counterpart reliably indicates which non-sensitive attributes to perturb to generate the targeted fairness confusion defect.
- Evidence anchors:
  - [abstract]: "RobustFair employs the notion of the fairness confusion matrix to guide differentiated gradient searches, identifying adversarial instances that undermine prediction accuracy (robustness) or cause biased predictions (individual fairness)."
  - [section]: "We propose Algorithms 3, 4, and 5 that are devised for FF, TB, and FB perturbations... In Algorithm 3 for a true biased perturbation, the prediction for the instance should get close to the ground truth, while the one for its similar counterpart should deviate from the ground truth."
  - [corpus]: Weak. Corpus lacks direct citations of fairness confusion matrix in adversarial testing context; this appears to be a novel contribution.
- Break condition: If the gradient signs do not meaningfully correlate with changes in loss for the instance vs. its similar counterpart, the perturbation direction will fail to generate the desired fairness confusion defect.

### Mechanism 2
- Claim: RobustFair approximates ground truths for generated adversarial instances using first-order Taylor expansion of the loss function.
- Mechanism: Given a generated perturbed instance vp, RobustFair uses the first-order Taylor expansion: loss(yp, f(vp)) = loss(y, f(v)) + g(vp - v), where g is the gradient of the loss at the original instance v. It then solves for yp such that the approximated loss matches the actual loss. For MSE loss, this yields two candidate ground truths y+ and y-, and the one closer to the original ground truth y is chosen as the approximation.
- Core assumption: The first-order Taylor expansion provides a sufficiently accurate local approximation of the loss function for determining the ground truth of the perturbed instance.
- Evidence anchors:
  - [abstract]: "RobustFair then infers the ground truth of these generated adversarial instances based on their loss function values approximated by the total derivative."
  - [section]: "RobustFair approximates the loss of the generated instance vp through the first-order Taylor expansion, as depicted below: loss(yp, f(vp)) = loss(y, f(v)) + g(vp - v) (1)"
  - [corpus]: Missing. No corpus evidence found for Taylor expansion application in adversarial instance ground truth approximation.
- Break condition: If the loss function is highly nonlinear in the region of the perturbation, the first-order Taylor expansion will be inaccurate, leading to wrong ground truth approximation and incorrect fairness evaluation.

### Mechanism 3
- Claim: RobustFair's two-phase generation (global then local) effectively balances diversity and precision in adversarial instance generation.
- Mechanism: The global generation phase samples seeds from clustered data and generates diverse adversarial instances by perturbing each seed in fairness confusion directions. The local generation phase refines each globally generated instance by perturbing only the attributes with the smallest gradient magnitudes, thus making minimal changes while still crossing the decision boundary. This two-phase approach ensures broad coverage while also producing fine-grained adversarial examples.
- Core assumption: Attributes with smaller gradient magnitudes are more effective for making minimal perturbations that still change the prediction outcome.
- Evidence anchors:
  - [section]: "RobustFair then sorts each perturbation direction vector dir in the ascending order of the absolute value of the derivative of each non-sensitive attribute... This allows RobustFair to select the attributes with the less absolute derivative values to perturb, which presumably results in the less deviations between the prediction outputs."
  - [section]: "The global generation phase searches for diverse instances to cover a broad range of an instance space... In the local generation phase, RobustFair generates instances in the vicinity of each instance (v, y) ∈ RG generated in the global generation phase."
  - [corpus]: Weak. No corpus evidence found for two-phase adversarial generation specifically combining global diversity with local precision.
- Break condition: If the gradient magnitudes do not correlate with the sensitivity of the prediction to attribute changes, the local refinement will fail to produce meaningful adversarial instances.

## Foundational Learning

- Concept: Fairness confusion matrix
  - Why needed here: It provides a principled framework for categorizing adversarial defects that simultaneously affect accuracy and individual fairness, which is essential for RobustFair's differentiated gradient search.
  - Quick check question: What are the four categories in the fairness confusion matrix, and how do they relate to accuracy and individual fairness?

- Concept: First-order Taylor expansion for loss approximation
  - Why needed here: It enables RobustFair to estimate the ground truth of perturbed instances without requiring access to true labels, which is critical for evaluating accurate fairness.
  - Quick check question: How does the first-order Taylor expansion approximate the loss of a perturbed instance, and what is the formula for selecting the approximated ground truth when using MSE loss?

- Concept: Gradient-based adversarial attack strategies (FGSM, PGD, etc.)
  - Why needed here: Understanding these provides context for how RobustFair's fairness confusion-directed approach differs from traditional robustness evaluation methods.
  - Quick check question: How does RobustFair's gradient search differ from FGSM or PGD in terms of the perturbation direction and objective?

## Architecture Onboarding

- Component map: Data preprocessing -> Clustering and seed sampling -> Global generation module -> Ground truth approximation module -> Local generation module -> Fairness evaluation -> Retraining
- Critical path: Seed → Global perturbation → Ground truth approximation → Local refinement → Fairness evaluation → Retraining
- Design tradeoffs:
  - Diversity vs. precision: Global generation maximizes diversity but may miss fine-grained defects; local generation ensures precision but may reduce diversity.
  - Computational cost vs. coverage: More seeds and iterations increase coverage but also increase computation time.
  - Approximation accuracy vs. practicality: Taylor expansion is efficient but may be inaccurate for highly nonlinear loss surfaces.
- Failure signatures:
  - Low diversity in generated instances: Likely cause is insufficient seeds or iterations in global generation.
  - Inaccurate fairness evaluation: Likely cause is poor ground truth approximation, possibly due to high nonlinearity or insufficient Taylor expansion order.
  - Minimal improvement in retraining: Likely cause is suboptimal selection of adversarial instances for data augmentation (e.g., not prioritizing instances similar to training data).
- First 3 experiments:
  1. Run RobustFair with default parameters on a small dataset (e.g., German Credit) and verify that it generates instances across all four fairness confusion categories.
  2. Compare the number and diversity of adversarial instances generated by RobustFair vs. FGSM/PGD on the same dataset to validate the two-phase generation advantage.
  3. Perform retraining with instances generated by RobustFair and measure improvements in both accuracy and individual fairness to confirm the practical benefit.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RobustFair's performance scale with larger datasets and more complex models?
- Basis in paper: [explicit] The paper mentions potential future work to extend the approach to black-box scenarios and various datasets, models, and tasks in practical applications.
- Why unresolved: The paper only evaluates RobustFair on four specific benchmark datasets with fully connected neural networks. It does not explore performance on larger datasets or more complex model architectures like CNNs or transformers.
- What evidence would resolve it: Systematic experiments testing RobustFair on progressively larger datasets and more complex model architectures, comparing detection rates and computational efficiency across different scales.

### Open Question 2
- Question: What is the theoretical limit of Taylor expansion approximation accuracy for ground truth estimation in RobustFair?
- Basis in paper: [explicit] The paper uses first-order Taylor expansions to approximate ground truths of perturbed instances, but does not analyze the approximation error bounds or convergence properties.
- Why unresolved: The paper implements Taylor expansion approximation but doesn't provide mathematical analysis of its accuracy or conditions under which it might fail, especially for highly non-linear decision boundaries.
- What evidence would resolve it: Formal error bounds on Taylor expansion approximation accuracy, empirical analysis of approximation error across different types of decision boundaries, and comparison with alternative approximation methods.

### Open Question 3
- Question: How does RobustFair perform in real-world deployment scenarios with noisy, incomplete, or streaming data?
- Basis in paper: [inferred] The paper focuses on benchmark datasets and doesn't address practical deployment challenges like data drift, concept drift, or noisy real-world data.
- Why unresolved: The evaluation is limited to static, clean benchmark datasets, and the paper doesn't discuss how the approach would handle dynamic data distributions or noisy inputs common in real-world applications.
- What evidence would resolve it: Deployment studies in real-world applications, experiments with synthetic noise injection, and analysis of performance degradation under data distribution shifts.

### Open Question 4
- Question: What is the relationship between RobustFair's perturbation detection rate and actual fairness improvements in downstream tasks?
- Basis in paper: [explicit] The paper shows that RobustFair detects more adversarial instances but doesn't establish a direct causal link between detection rate and fairness improvement in real applications.
- Why unresolved: While the paper demonstrates improved fairness metrics after retraining with detected instances, it doesn't investigate whether higher detection rates necessarily lead to better fairness outcomes in practice, or if there's a point of diminishing returns.
- What evidence would resolve it: Correlation studies between detection rates and actual fairness improvements across multiple applications, analysis of optimal detection thresholds for different fairness metrics, and investigation of the relationship between perturbation diversity and fairness enhancement.

## Limitations

- The effectiveness of Taylor expansion for ground truth approximation may be limited for highly nonlinear loss surfaces
- The gradient magnitude-based attribute selection assumes correlation between gradient magnitude and perturbation effectiveness, which may not hold for all models
- The approach requires white-box access to model gradients, limiting applicability to black-box scenarios

## Confidence

- **High confidence**: The overall experimental results showing improved fairness metrics after retraining with detected instances
- **Medium confidence**: The fairness confusion matrix framework for categorizing adversarial defects
- **Low confidence**: The effectiveness of Taylor expansion for ground truth approximation and the gradient magnitude-based attribute selection in local refinement

## Next Checks

1. **Ground truth validation**: Compare the approximated ground truths from Taylor expansion against actual ground truths on a small subset where true labels can be obtained, measuring the approximation error rate.
2. **Gradient correlation analysis**: Quantify the correlation between gradient sign alignment and actual loss changes for instances and their similar counterparts to validate the fairness confusion direction assumption.
3. **Alternative approximation methods**: Implement and compare RobustFair's Taylor expansion approach against higher-order approximations or numerical methods to assess the impact on detection accuracy and fairness evaluation.