---
ver: rpa2
title: 'AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation'
arxiv_id: '2311.07397'
source_url: https://arxiv.org/abs/2311.07397
tags:
- mllms
- hallucination
- evaluation
- task
- amber
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AMBER, an LLM-free multi-dimensional benchmark
  for evaluating hallucinations in Multi-modal Large Language Models (MLLMs). The
  authors address the challenge of high evaluation costs and insufficient evaluation
  dimensions in existing methods.
---

# AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation

## Quick Facts
- arXiv ID: 2311.07397
- Source URL: https://arxiv.org/abs/2311.07397
- Reference count: 4
- This paper introduces AMBER, an LLM-free multi-dimensional benchmark for evaluating hallucinations in Multi-modal Large Language Models (MLLMs).

## Executive Summary
This paper introduces AMBER, an LLM-free multi-dimensional benchmark for evaluating hallucinations in Multi-modal Large Language Models (MLLMs). The authors address the challenge of high evaluation costs and insufficient evaluation dimensions in existing methods. AMBER covers both generative and discriminative tasks, including existence, attribute, and relation hallucinations. The authors design a low-cost and efficient evaluation pipeline without relying on external models. The comprehensive evaluation of mainstream MLLMs, including GPT-4V, reveals that even the most advanced models still suffer from hallucination problems. The authors provide guideline suggestions for mitigating hallucinations, emphasizing the importance of reinforcing data from attribute and relation perspectives.

## Method Summary
AMBER introduces an LLM-free evaluation pipeline that processes MLLM responses directly without routing through another model for judgment. The method extracts nouns from generative outputs and compares them to pre-annotated object lists using set intersection. For discriminative tasks, it uses counterfactual prompts to test hallucination likelihood. The benchmark covers existence, attribute, and relation hallucinations through a multi-dimensional annotation schema. Evaluation metrics include CHAIR (hallucination rate), Cover (response coverage), and AMBER Score (combined metric balancing hallucination detection with response quality).

## Key Results
- AMBER reveals significant hallucination problems in state-of-the-art MLLMs including GPT-4V
- The benchmark shows that GPT-4V performs best but still suffers from hallucination issues
- AMBER achieves comparable performance to existing benchmarks (MMMU, M3W) while being more efficient and covering more hallucination types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AMBER eliminates LLM dependency for hallucination evaluation by using direct string comparison between model outputs and ground truth annotations.
- Mechanism: Instead of routing outputs through another LLM for judgment, AMBER processes MLLM responses directly—extracting nouns from generative outputs and comparing them to pre-annotated object lists using set intersection.
- Core assumption: Extracted nouns from responses can reliably represent hallucinatory content without semantic understanding.
- Evidence anchors:
  - [abstract] "we design a low-cost and efficient evaluation pipeline without relying on external models"
  - [section] "We obtain an initial response R by fitting Input into a specific MLLM respondent and extract key elements for compute metrics"
- Break condition: If MLLM responses contain paraphrases or synonyms that are semantically equivalent but lexically different from annotations, the evaluation would incorrectly flag them as hallucinations.

### Mechanism 2
- Claim: Multi-dimensional annotation schema enables fine-grained detection of different hallucination types (existence, attribute, relation).
- Mechanism: The annotation system captures objects, their attributes (state, number, action), relations, and hallucinatory target objects. This structure allows targeted evaluation prompts and metrics for each dimension.
- Core assumption: Detailed annotations can be consistently applied across diverse images without introducing bias.
- Evidence anchors:
  - [section] "We meticulously annotate 4 types of content for each image" and lists "Existence," "Attribute," "Relation," and "Hallucinatory target objects"
  - [section] "We design corresponding prompts based on different types of hallucination"
- Break condition: If annotation quality varies significantly across raters or images, the evaluation would produce inconsistent results across hallucination types.

### Mechanism 3
- Claim: Coverage metric balances hallucination detection with response quality preservation.
- Mechanism: While CHAIR measures hallucination frequency, the Cover metric ensures that reducing hallucinations doesn't excessively truncate valid image content in responses. The AMBER Score combines both to find optimal tradeoff.
- Core assumption: A good evaluation should penalize hallucinations without overly restricting legitimate descriptive content.
- Evidence anchors:
  - [section] "We believe that an ideal response should maintain a low hallucination level without sacrificing response quality too much"
  - [section] "The AMBER Score is represented by the following formula: AMBER Score = Avg(1 − CHAIR, F1)"
- Break condition: If Cover metric doesn't properly account for semantic relevance, models might achieve high scores by mentioning many objects without meaningful image description.

## Foundational Learning

- Concept: Set theory and intersection operations
  - Why needed here: AMBER uses set intersection (R'_obj ∩ Aobj) to determine hallucination presence and coverage
  - Quick check question: If R'_obj = {"dog", "ball", "tree"} and Aobj = {"dog", "tree", "car"}, what is the hallucination rate using CHAIR?

- Concept: Precision, recall, and F1-score in classification tasks
  - Why needed here: AMBER uses these metrics for discriminative task evaluation, specifically calculating precision and recall for hallucinatory questions where ground truth is "no"
  - Quick check question: If a model answers 80 questions with 60 correct "no" responses out of 70 actual "no" cases, what are the precision and recall?

- Concept: Counterfactual prompting and prompt engineering
  - Why needed here: AMBER uses counterfactual prompts like "Is there a {hal object} in this image?" to test hallucination likelihood
  - Quick check question: How would you construct a prompt to test whether a model hallucinates about a "purple elephant" in a beach scene?

## Architecture Onboarding

- Component map: Data collection → Annotation pipeline → Prompt template design → MLLM response collection → Response processing → Metric computation → Analysis
- Critical path: Annotation quality → Prompt effectiveness → Response processing accuracy → Metric calculation reliability
- Design tradeoffs: Annotation comprehensiveness vs. annotation cost; evaluation precision vs. generalization to new object categories
- Failure signatures: High variance in inter-annotator agreement; inconsistent object extraction from responses; metric values that don't align with human judgment
- First 3 experiments:
  1. Validate annotation consistency by having multiple annotators label the same 50 images and computing agreement scores
  2. Test object extraction pipeline by running responses from known models through the extraction process and verifying output quality
  3. Benchmark AMBER metrics against human evaluation on a sample set to validate metric relevance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively mitigate hallucinations in MLLMs without significantly compromising their performance on other tasks?
- Basis in paper: [explicit] The authors suggest that data creators consider reinforcing existing data from attribute and relation perspectives to mitigate hallucinations, but acknowledge that this is a guideline suggestion rather than a tested solution.
- Why unresolved: While the authors provide some guidance on mitigating hallucinations, they do not offer a comprehensive solution or evaluate the effectiveness of their suggestions. Additionally, it is unclear how to balance hallucination mitigation with maintaining performance on other tasks.
- What evidence would resolve it: A systematic evaluation of different mitigation strategies, including data augmentation and fine-tuning techniques, with a focus on both hallucination reduction and overall task performance.

### Open Question 2
- Question: Can we develop a more efficient and scalable evaluation pipeline for MLLMs' hallucinations that does not rely on external models or human annotation?
- Basis in paper: [explicit] The authors introduce an LLM-free evaluation pipeline for MLLMs' hallucinations, but acknowledge that their approach still has some limitations, such as the possibility of mistakes in extracting objects using language toolkits.
- Why unresolved: While the authors' evaluation pipeline is a step towards more efficient and scalable hallucination evaluation, it still has some limitations and potential sources of error. Additionally, it is unclear how well this approach generalizes to different types of MLLMs and hallucination scenarios.
- What evidence would resolve it: A comprehensive evaluation of the proposed evaluation pipeline across different MLLMs, hallucination types, and scenarios, as well as a comparison with other evaluation methods in terms of efficiency, scalability, and accuracy.

### Open Question 3
- Question: How do different MLLM architectures and training strategies impact their susceptibility to hallucinations, and can we use this knowledge to design more robust models?
- Basis in paper: [inferred] The authors compare the performance of different MLLMs on their proposed benchmark, noting that some models perform better than others in terms of hallucination mitigation. However, they do not provide a detailed analysis of the relationship between model architecture, training strategy, and hallucination susceptibility.
- Why unresolved: While the authors provide some insights into the performance of different MLLMs on their benchmark, they do not explore the underlying reasons for these differences or provide guidance on how to design more robust models.
- What evidence would resolve it: A systematic analysis of the relationship between MLLM architecture, training strategy, and hallucination susceptibility, including ablation studies and controlled experiments to isolate the impact of different factors.

## Limitations

- The evaluation pipeline's reliance on exact string matching may incorrectly flag semantically equivalent but lexically different responses as hallucinations
- Annotation process scalability and consistency are questionable without quantified inter-annotator agreement metrics
- Comparison against existing benchmarks is limited to CHAIR scores only, without comprehensive evaluation across other relevant metrics

## Confidence

**High Confidence**: The core claim that AMBER provides an LLM-free evaluation pipeline is well-supported by the methodology description and implementation details. The mechanism for extracting objects from responses and comparing against ground truth annotations is clearly specified.

**Medium Confidence**: The claim that AMBER comprehensively covers existence, attribute, and relation hallucinations is supported by the annotation schema but lacks validation through inter-annotator agreement metrics or coverage analysis of hallucination types in the dataset.

**Low Confidence**: The assertion that AMBER reveals significant hallucination problems in state-of-the-art MLLMs (like GPT-4V) is based on CHAIR scores alone without considering whether these scores translate to meaningful quality degradation in real-world applications.

## Next Checks

1. **Inter-annotator Agreement Validation**: Conduct a formal agreement study with 3-5 annotators on 100 randomly selected images to establish Fleiss' Kappa scores for each hallucination type. This would validate the consistency of the annotation schema and identify potential sources of measurement error.

2. **Semantic Equivalence Error Analysis**: Create a test set of 200 MLLM responses containing paraphrases, synonyms, and semantically equivalent descriptions. Measure the false positive rate of the CHAIR metric and determine whether the 0.1 threshold appropriately balances sensitivity and specificity.

3. **Real-world Impact Assessment**: Design a user study comparing responses from high-CHAIR and low-CHAIR models on practical visual question answering tasks. Measure whether the CHAIR metric correlates with user trust and task completion rates, validating whether hallucination reduction translates to meaningful quality improvements.