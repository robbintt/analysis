---
ver: rpa2
title: 'C-STS: Conditional Semantic Textual Similarity'
arxiv_id: '2305.15093'
source_url: https://arxiv.org/abs/2305.15093
tags:
- similarity
- sentence
- c-sts
- sentences
- condition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces C-STS (Conditional Semantic Textual Similarity),
  a novel task that addresses ambiguity in traditional STS by measuring sentence similarity
  conditioned on a natural language aspect. The authors create C-STS-2023, a dataset
  of nearly 20,000 instances using image-caption pairs from COCO and Flickr30K, with
  human annotations providing two conditions per pair (one indicating high similarity,
  one low).
---

# C-STS: Conditional Semantic Textual Similarity

## Quick Facts
- arXiv ID: 2305.15093
- Source URL: https://arxiv.org/abs/2305.15093
- Reference count: 11
- Primary result: State-of-the-art models achieve Spearman correlation below 50 on C-STS, significantly worse than on standard STS benchmarks

## Executive Summary
C-STS introduces a novel task that measures semantic textual similarity conditioned on natural language aspects, addressing ambiguity in traditional STS. The authors create C-STS-2023, a dataset of nearly 20,000 instances using image-caption pairs from COCO and Flickr30K with human annotations providing two conditions per pair. Even advanced models like SimCSE, Flan-T5, and GPT-4 struggle on C-STS, revealing systematic weaknesses in current approaches to semantic understanding. The paper proposes tri-encoder models and quadruplet contrastive training methods, finding that performance scales linearly with dataset size, suggesting the need for improved modeling strategies.

## Method Summary
The method uses image-caption pairs from COCO and Flickr30K, filtering them through CLIP embeddings to create sentence pairs with moderate unconditional similarity. Human annotators then provide two conditions per pair (one indicating high similarity, one low) using free-form natural language. The authors propose tri-encoder models that separately encode each sentence and condition, trained with quadruplet contrastive loss to distinguish high-similarity from low-similarity pairs under the same condition. Evaluation uses Spearman correlation between model predictions and human similarity judgments on a Likert scale.

## Key Results
- State-of-the-art models achieve Spearman correlation below 50 on C-STS
- Performance scales linearly with dataset size up to 7000 examples
- Tri-encoder models with quadruplet loss outperform bi-encoder baselines
- Even instruction-tuned models like GPT-4 struggle with conditional similarity judgments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using image-caption pairs with CLIP embeddings produces sentence pairs that are lexically and semantically ambiguous
- Mechanism: Image retrieval retrieves visually similar images, whose captions may describe different but related content. These captions form sentence pairs that are neither too similar (high IOU) nor too dissimilar (SimCSE distance), creating natural ambiguity
- Core assumption: Visually similar images often have captions that differ in meaningful ways but are contextually related, producing pairs suitable for conditional similarity
- Evidence anchors: [abstract] "Image-captioning datasets provide a compelling data source because image pair similarity and caption (text) pair similarity encode different semantics"

### Mechanism 2
- Claim: Free-form natural language conditions reduce ambiguity and enable fine-grained similarity evaluation
- Mechanism: By forcing annotators to specify the aspect of similarity (e.g., "color" vs "size"), conditions explicitly resolve which attribute to compare, making similarity judgments more precise and grounded
- Core assumption: Annotators can identify and articulate relevant aspects that disambiguate similarity judgments without being constrained to predefined categories
- Evidence anchors: [abstract] "C-STSâ€™s advantages are two-fold: (1) it reduces the subjectivity and ambiguity of STS, and (2) enables fine-grained similarity evaluation using diverse conditions"

### Mechanism 3
- Claim: Tri-encoder models with quadruplet contrastive loss learn better conditional similarity representations than bi-encoders
- Mechanism: The tri-encoder encodes each sentence and condition separately, allowing the model to learn condition-specific transformations. The quadruplet loss contrasts high-similarity vs low-similarity pairs under the same condition, encouraging fine-grained distinctions
- Core assumption: Separating condition encoding from sentence encoding allows the model to learn condition-aware transformations that capture subtle semantic differences
- Evidence anchors: [abstract] "We also propose new tri-encoder model and a quadruplet training loss which allows us to perform contrastive learning based on different conditions for the same sentence pair"

## Foundational Learning

- Concept: Semantic similarity vs relatedness
  - Why needed here: C-STS requires understanding that similarity is about shared meaning under specific aspects, not just topical relatedness
  - Quick check question: Given "The cat sleeps on the mat" and "The dog sleeps on the rug", are they similar because they share the activity "sleeping", or dissimilar because of different subjects and objects?

- Concept: Image-text embeddings and cross-modal retrieval
  - Why needed here: The paper uses CLIP embeddings to retrieve sentence pairs from image captions, so understanding how visual and textual representations align is crucial
  - Quick check question: If two images have similar CLIP embeddings, what does that imply about their captions' potential for forming ambiguous sentence pairs?

- Concept: Contrastive learning and quadruplet losses
  - Why needed here: The proposed training objective uses quadruplet loss to contrast high vs low similarity pairs under the same condition, requiring understanding of advanced contrastive objectives
  - Quick check question: How does a quadruplet loss differ from a triplet loss in terms of what it tries to achieve during training?

## Architecture Onboarding

- Component map: COCO/Flickr30K image-caption pairs -> CLIP embeddings -> sentence pair retrieval -> filtering -> human annotation -> tri-encoder model with quadruplet loss -> evaluation

- Critical path: Retrieve sentence pairs -> filter for ambiguity -> annotate with conditions -> train tri-encoder with quadruplet loss -> evaluate on C-STS test set

- Design tradeoffs:
  - Image retrieval vs text retrieval: Image retrieval creates more natural ambiguity but adds computational overhead
  - Bi-encoder vs tri-encoder: Tri-encoder allows condition-specific transformations but increases parameter count and complexity
  - Free-form vs structured conditions: Free-form enables fine-grained evaluation but may introduce noise and annotation complexity

- Failure signatures:
  - Low Spearman correlation despite high STS-B performance: Model isn't learning to incorporate conditions
  - High invalid prediction rate in few-shot setting: Instructions or examples are unclear
  - Minimal performance gain from increasing dataset size: Model architecture or training objective needs improvement

- First 3 experiments:
  1. Compare bi-encoder vs tri-encoder performance on a small C-STS subset to verify condition incorporation matters
  2. Test different transformation functions h (MLP vs Hadamard) in tri-encoder to find optimal architecture
  3. Evaluate quadruplet loss vs MSE alone on validation set to confirm its effectiveness for this task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of C-STS models scale with dataset size beyond 7000 examples?
- Basis in paper: [explicit] The paper shows linear performance scaling up to 7000 examples and states "we require better modeling strategies to improve performance"
- Why unresolved: The experiments only test up to 7000 examples, leaving the potential for further scaling unexplored
- What evidence would resolve it: Training and evaluating models on datasets with 10,000+ examples to determine if the linear trend continues or plateaus

### Open Question 2
- Question: What specific modeling strategies could improve C-STS performance beyond simple dataset scaling?
- Basis in paper: [explicit] The authors state "we require better modeling strategies to improve performance" and note that even instruction-tuned models struggle
- Why unresolved: The paper only evaluates existing architectures without proposing novel approaches for C-STS
- What evidence would resolve it: Testing specialized architectures that explicitly model the relationship between conditions and sentence pairs, or novel training objectives that better capture conditional similarity

### Open Question 3
- Question: How do models trained on C-STS transfer to other conditional understanding tasks?
- Basis in paper: [inferred] C-STS is presented as a task that requires nuanced conditional understanding, which is fundamental to many NLP applications
- Why unresolved: The paper only evaluates C-STS performance in isolation without testing transfer to related tasks
- What evidence would resolve it: Evaluating C-STS-trained models on tasks like visual question answering, context-dependent sentiment analysis, or instruction following to measure generalization

### Open Question 4
- Question: What linguistic phenomena make certain conditions more challenging for models?
- Basis in paper: [inferred] The qualitative analysis shows systematic model failures, suggesting some conditions are harder than others
- Why unresolved: The paper doesn't analyze which types of conditions (e.g., abstract vs. concrete, lexical vs. compositional) pose the greatest challenges
- What evidence would resolve it: Detailed error analysis categorizing failed conditions by linguistic properties to identify patterns in model weaknesses

## Limitations
- The dataset construction relies heavily on CLIP embeddings, which may not always capture nuanced differences needed for effective conditional similarity judgments
- Filtering thresholds for selecting candidate sentence pairs are not fully specified, affecting reproducibility and dataset quality
- The claim that free-form conditions enable fine-grained evaluation is compelling but under-examined for potential noise or inconsistency

## Confidence
- High confidence: Current state-of-the-art models struggle with C-STS compared to traditional STS benchmarks
- Medium confidence: Tri-encoder architecture and quadruplet contrastive loss are theoretically sound but may not be sufficient alone
- Low confidence: Free-form natural language conditions may introduce noise or ambiguity that hasn't been thoroughly investigated

## Next Checks
1. Conduct systematic study of condition quality by having annotators rate conditions for relevance, clarity, and consistency
2. Perform ablation study on filtering thresholds to identify optimal parameters and understand their impact on task difficulty
3. Evaluate whether C-STS-trained models generalize to other conditional similarity tasks across different domains