---
ver: rpa2
title: 'Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity'
arxiv_id: '2310.07521'
source_url: https://arxiv.org/abs/2310.07521
tags:
- language
- llms
- knowledge
- arxiv
- factuality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively examines the factuality of large language
  models (LLMs), addressing the problem of LLMs producing content inconsistent with
  established facts. It defines the factuality issue, analyzes its causes, and reviews
  evaluation methods and enhancement techniques.
---

# Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity

## Quick Facts
- arXiv ID: 2310.07521
- Source URL: https://arxiv.org/abs/2310.07521
- Reference count: 40
- Key outcome: This survey comprehensively examines factuality in large language models, analyzing evaluation methods, enhancement techniques, and domain-specific challenges.

## Executive Summary
This survey provides a comprehensive examination of factuality in large language models (LLMs), addressing the critical problem of LLMs producing content inconsistent with established facts. It defines the factuality issue, analyzes its underlying causes, and reviews various evaluation methods and enhancement techniques. The survey covers both standalone LLMs and retrieval-augmented LLMs, providing insights into their unique challenges and potential solutions for improving factual reliability.

## Method Summary
The survey synthesizes existing literature on LLM factuality through systematic analysis of evaluation metrics, benchmarks, and enhancement techniques. It categorizes factuality evaluation into rule-based, neural, hybrid, and human evaluation metrics, while examining key benchmarks like MMLU and TruthfulQA. The methodology involves reviewing mechanisms of factuality in LLMs at model, retrieval, and inference levels, and discussing enhancement methods including pretraining-based approaches, supervised fine-tuning, multi-agent systems, and prompt techniques for standalone models, as well as interactive retrieval and adaptation methods for retrieval-augmented systems.

## Key Results
- Factuality evaluation metrics are categorized into rule-based, neural, hybrid, and human evaluation approaches
- Retrieval-augmented generation significantly improves factuality by integrating external knowledge at inference time
- Supervised fine-tuning on domain-specific data enhances factuality by aligning model outputs with domain knowledge
- Chain-of-thought reasoning can improve factuality by enabling step-by-step logical inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented generation improves factuality by integrating external knowledge at inference time.
- Mechanism: When the LLM is asked a question, a retrieval module searches for relevant documents, and the LLM generates an answer conditioned on both its internal knowledge and the retrieved context.
- Core assumption: The retrieved context contains accurate, relevant information that the LLM can correctly interpret and incorporate.
- Evidence anchors:
  - [abstract]: "We focus two primary LLM configurations standalone LLMs and Retrieval-Augmented LLMs that utilizes external data, we detail their unique challenges and potential enhancements."
  - [section]: "Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach to address certain limitations inherent to standalone LLMs, such as outdated information and the inability to memorize (Chase, 2022; Liu, 2022)."
  - [corpus]: Weak corpus evidence; only mentions "RARE" and "Mitigating Hallucinations in Large Language Models via Self-Refinement-Enhanced Knowledge Retrieval" but no specific mechanisms detailed.
- Break condition: If retrieved documents are irrelevant, contradictory, or misinterpreted, the LLM may generate incorrect answers or default to its parametric knowledge.

### Mechanism 2
- Claim: Supervised fine-tuning on domain-specific data improves factuality by aligning the model's outputs with factual domain knowledge.
- Mechanism: The LLM is fine-tuned using labeled datasets from the target domain, teaching it to generate responses consistent with domain-specific facts and reducing hallucinations.
- Core assumption: Labeled domain data is accurate and comprehensive enough to guide the model toward factual correctness.
- Evidence anchors:
  - [abstract]: "Enhancement methods are discussed, including pretraining-based, supervised finetuning, multi-agent, and novel prompt techniques for standalone LLMs."
  - [section]: "Continual SFT: A cyclic fine-tuning approach, where the model undergoes consistent refinement using sequential sets of labeled data."
  - [corpus]: No specific studies cited; assumption based on general fine-tuning literature.
- Break condition: If the fine-tuning data contains errors or biases, the model may learn and propagate those inaccuracies.

### Mechanism 3
- Claim: Chain-of-thought reasoning improves factuality by enabling step-by-step logical inference, reducing reasoning failures.
- Mechanism: The LLM is prompted to generate intermediate reasoning steps before arriving at a final answer, allowing it to catch and correct errors during the reasoning process.
- Core assumption: The LLM's parametric knowledge is sufficient to support accurate reasoning when guided through explicit intermediate steps.
- Evidence anchors:
  - [abstract]: "Our discussion then transitions to methodologies for evaluating LLM factuality, emphasizing key metrics, benchmarks, and studies."
  - [section]: "Interactive Retrieval: While retrieval systems are designed to source relevant information, they may occasionally fail to retrieve accurate or comprehensive data."
  - [corpus]: Weak evidence; only general mention of "Chain-of-thought reasoning" without specific studies.
- Break condition: If the intermediate reasoning steps are flawed or the model lacks necessary knowledge, errors may propagate through the reasoning chain.

## Foundational Learning

- Concept: Knowledge storage in LLMs
  - Why needed here: Understanding how LLMs store and retrieve factual knowledge is essential for diagnosing and addressing factuality issues.
  - Quick check question: How do LLMs represent and access factual knowledge within their parameters?

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: RAG is a key technique for enhancing factuality by integrating external knowledge at inference time.
  - Quick check question: What are the main components of a RAG system and how do they interact?

- Concept: Supervised fine-tuning
  - Why needed here: Fine-tuning is a common method for adapting LLMs to specific domains and improving their factuality.
  - Quick check question: What are the key considerations when fine-tuning an LLM for a specific domain?

## Architecture Onboarding

- Component map: Pre-trained LLM -> Retrieval Module -> Generation Module -> Answer
- Critical path: Retrieve relevant documents → Interpret and incorporate context → Generate factual answer
- Design tradeoffs: Parametric knowledge vs. retrieved knowledge, hallucination risk vs. creativity, computational cost vs. factuality improvement
- Failure signatures: Incorrect retrieval, misinterpretation of context, propagation of reasoning errors, overreliance on parametric knowledge
- First 3 experiments:
  1. Evaluate factuality of standalone LLM on TruthfulQA or MMLU benchmarks
  2. Implement basic RAG system and compare factuality improvement
  3. Fine-tune LLM on domain-specific dataset and assess domain-related factuality enhancement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop a universal metric that accurately captures the factuality of LLM outputs across diverse domains and contexts?
- Basis in paper: [inferred] The survey highlights the challenges in evaluating factuality, noting the variability and nuances of natural languages, and the need for more robust evaluation metrics.
- Why unresolved: Current evaluation metrics are either domain-specific or rely heavily on human evaluation, which is time-consuming and expensive. A universal metric would need to account for the complexity of natural language, the diversity of domains, and the potential for nuanced interpretations.
- What evidence would resolve it: A comprehensive evaluation framework that demonstrates consistent and accurate assessment of LLM factuality across multiple domains, contexts, and languages, using a single, unified metric.

### Open Question 2
- Question: What are the underlying mechanisms that govern how LLMs store, update, and retrieve factual knowledge, and how can we leverage this understanding to improve factuality?
- Basis in paper: [explicit] The survey discusses the mechanisms through which LLMs process, interpret, and produce factual content, but notes that the organization of knowledge within LLMs remains largely mysterious.
- Why unresolved: While some studies have explored the storage of factual knowledge in LLMs, the overall understanding of how LLMs organize and retrieve this knowledge is still limited. This lack of understanding hinders the development of effective strategies for improving factuality.
- What evidence would resolve it: Detailed insights into the neural architectures and mechanisms that govern knowledge storage, updating, and retrieval in LLMs, potentially through advanced neuroimaging techniques or novel computational models.

### Open Question 3
- Question: How can we effectively mitigate the issue of "hallucinations" in LLMs, where the model generates content that contradicts established facts, while maintaining its ability to generate creative and diverse outputs?
- Basis in paper: [explicit] The survey defines the "factuality issue" as the probability of LLMs to produce content inconsistent with established facts, and discusses the challenges of hallucinations in LLMs.
- Why unresolved: Hallucinations in LLMs are a significant challenge, as they can lead to the generation of misleading or inaccurate information. However, completely eliminating hallucinations may also restrict the model's ability to generate creative and diverse outputs.
- What evidence would resolve it: A comprehensive approach that effectively balances the need for factual accuracy with the desire for creative and diverse outputs, potentially through a combination of techniques such as retrieval-augmented generation, model editing, and novel decoding strategies.

## Limitations

- The survey lacks specific implementation details for human evaluation protocols and domain-specific metrics, making exact reproduction challenging
- Many claims about enhancement techniques are supported primarily by general literature references rather than direct empirical evidence from surveyed studies
- Corpus analysis reveals limited citations and modest author influence, suggesting potential gaps in coverage of cutting-edge research

## Confidence

- **High Confidence**: Claims about existence of factuality issues and general categorization of evaluation metrics are well-supported by established benchmarks
- **Medium Confidence**: Assertions about specific enhancement techniques like RAG and supervised fine-tuning are supported by theoretical frameworks but lack direct empirical validation
- **Low Confidence**: Detailed mechanisms of how specific techniques improve factuality are mentioned but not empirically demonstrated within the survey

## Next Checks

1. **Benchmark Validation**: Replicate factuality evaluation using MMLU and TruthfulQA benchmarks with multiple LLMs to verify reported performance trends and compare automated vs. human evaluation results

2. **RAG Implementation Test**: Implement basic RAG system and evaluate its impact on factuality compared to standalone LLMs across multiple question types, focusing on cases where retrieval succeeds vs. fails

3. **Domain-Specific Evaluation Protocol**: Develop and test domain-specific evaluation framework for legal or medical factuality, comparing automated metrics against human expert evaluation to validate claims about domain-specific challenges and solutions