---
ver: rpa2
title: How does training shape the Riemannian geometry of neural network representations?
arxiv_id: '2301.11375'
source_url: https://arxiv.org/abs/2301.11375
tags:
- networks
- neural
- volume
- decision
- element
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how training shapes the Riemannian geometry
  of neural network representations. The authors analyze how the geometry induced
  by neural network feature maps evolves during training, focusing on the magnification
  of local areas near decision boundaries.
---

# How does training shape the Riemannian geometry of neural network representations?

## Quick Facts
- arXiv ID: 2301.11375
- Source URL: https://arxiv.org/abs/2301.11375
- Reference count: 40
- Primary result: Training on classification tasks causes neural networks to expand local volume elements near decision boundaries, improving input discriminability.

## Executive Summary
This paper investigates how training shapes the Riemannian geometry of neural network representations by analyzing the geometry induced by feature maps. The authors demonstrate that while infinite-width networks with random parameters exhibit spherical symmetry in their induced metrics, training on classification tasks leads to expansion of volume elements along decision boundaries. This geometric effect is observed consistently across different network architectures and datasets, suggesting a fundamental mechanism by which feature learning improves local input discriminability.

## Method Summary
The authors analyze neural network representations through the lens of Riemannian geometry, computing pullback metrics induced by feature maps and tracking geometric quantities like volume elements and Ricci curvature during training. For shallow networks, they use explicit formulas based on Gaussian process theory, while for deep networks they employ Neural Tangent Kernel approximations. Experiments include training on synthetic 2D tasks (XOR and sinusoidal boundaries), MNIST, and CIFAR-10, with geometric analysis performed through both direct computation and visualization along interpolated paths between examples.

## Key Results
- At infinite width with random Gaussian weights, neural networks induce spherically symmetric metrics on input space.
- Training on classification tasks causes volume element expansion near decision boundaries across shallow networks, deep fully-connected networks, and residual networks.
- The magnification effect is observable through peaks in the volume element along interpolated paths crossing decision regions, consistent with improved local discriminability.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training on classification tasks causes neural networks to expand local volume elements near decision boundaries, improving input discriminability.
- Mechanism: During training, gradients push the network to increase the Jacobian determinant near regions where small input changes flip the class label, effectively magnifying these boundary regions.
- Core assumption: The network's feature map is smooth and differentiable, allowing the induced Riemannian metric to be well-defined and responsive to training.
- Evidence anchors:
  - [abstract] "networks trained to perform classification tasks learn to magnify local areas along decision boundaries."
  - [section] "As the network's decision boundary is gradually molded to conform to the true boundary, the volume element develops peaks in the same vicinity."
  - [corpus] "RNNs perform task computations by dynamically warping neural representations" - weak signal, but relevant.
- Break condition: If the activation function is non-differentiable (e.g., ReLU) or the metric computation fails numerically due to eigenvalue collapse, this mechanism may not hold.

### Mechanism 2
- Claim: At infinite width with random Gaussian weights, the Riemannian geometry of the network's feature map is spherically symmetric.
- Mechanism: The independence of Gaussian weights causes the pullback metric to depend only on the input norm, not its direction, leading to spherical symmetry.
- Core assumption: The activation function is smooth and satisfies integrability conditions for the neural network Gaussian process (NNGP) kernel to exist.
- Evidence anchors:
  - [abstract] "at infinite width, neural networks with random parameters induce highly symmetric metrics on input space."
  - [section] "Thanks to the assumption of independent Gaussian weights, the geometric quantities associated to the shallow Neural Tangent Kernel and to the deep NNGP will share this spherical symmetry."
  - [corpus] "Emergent Riemannian geometry over learning discrete computations on continuous manifolds" - weak signal, but relevant.
- Break condition: If the weights are not Gaussian or the activation function is not smooth, the spherical symmetry will break.

### Mechanism 3
- Claim: Feature learning in finite-width networks introduces perturbative corrections to the infinite-width geometry, breaking spherical symmetry in task-adapted ways.
- Mechanism: Training induces correlations between weights that were independent at initialization, leading to deviations from the NNGP metric that reflect the task structure.
- Core assumption: The width is large but finite, allowing perturbative expansion in 1/n to capture the leading corrections.
- Evidence anchors:
  - [abstract] "Feature learning in networks trained to perform classification tasks magnifies local areas along decision boundaries."
  - [section] "In Appendix E, we use recent results on perturbative feature-learning corrections to the NNGP kernel...to compute corrections to the posterior mean of the volume element."
  - [corpus] "A singular Riemannian Geometry Approach to Deep Neural Networks III. Piecewise Differentiable Layers and Random Walks on n-dimensional Classes" - weak signal, but relevant.
- Break condition: If the network is too small or the training signal is too weak, perturbative corrections may not be observable.

## Foundational Learning

- Concept: Riemannian geometry and pullback metrics
  - Why needed here: The paper's central claim is about how training shapes the Riemannian geometry of neural network representations, so understanding the mathematical framework is essential.
  - Quick check question: What is the pullback metric induced by a feature map Φ: R^d → H?

- Concept: Neural Tangent Kernel (NTK) and Neural Network Gaussian Process (NNGP)
  - Why needed here: These kernels describe the geometry of infinitely wide networks, providing a baseline for understanding how finite-width training deviates from this limit.
  - Quick check question: How does the NTK differ from the NNGP kernel, and what does this imply for the induced geometry?

- Concept: Volume element and curvature in differential geometry
  - Why needed here: The paper focuses on how training affects the volume element and curvature of the feature map's manifold, which are key geometric quantities.
  - Quick check question: What does the volume element √det(g) measure in the context of neural network representations?

## Architecture Onboarding

- Component map:
  Input space: d-dimensional data manifold -> Feature map: Φ: R^d → H (Hilbert space) -> Metric: g_μν = ∂_μφ_i ∂_νφ_i (pullback of flat metric on H) -> Geometric quantities: volume element √det(g), Ricci scalar R -> Network types: shallow FC, deep FC, ResNet with GELU

- Critical path:
  1. Define feature map and compute metric
  2. Compute geometric quantities (volume element, curvature)
  3. Visualize changes over training (2D slices, interpolated images)
  4. Compare to infinite-width limit (NNGP)

- Design tradeoffs:
  - High-dimensional tasks require dimensionality reduction for visualization, which may obscure fine-grained structure.
  - Numerical stability issues arise when computing metrics for deep networks with many parameters.
  - Linear interpolation in pixel space does not respect the true data manifold structure.

- Failure signatures:
  - Singular metrics (zero eigenvalues) indicate degenerate representations.
  - Numerical overflow/underflow in log(√det(g)) for high-dimensional inputs.
  - Lack of correlation between volume element and decision boundary indicates failed magnification.

- First 3 experiments:
  1. Train a shallow network on 2D XOR task, plot volume element and curvature over training.
  2. Train a shallow network on MNIST, compute log(√det(g)) for interpolated images, check if peaks align with decision boundaries.
  3. Train a ResNet on CIFAR-10, compute log(√det(g)) for 1D and 2D interpolated slices, compare to shallow network behavior.

## Open Questions the Paper Calls Out
- Does expanding areas near decision boundaries in neural networks generally improve generalization performance, or can it lead to overfitting?
- How does the Riemannian geometry induced by ReLU networks differ from that of networks with smooth activation functions?
- Can the method of Radhakrishnan et al. (2022) for iterative kernel learning be extended to induce non-flat metrics that improve performance on complex tasks?

## Limitations
- The analysis of finite-width corrections relies on perturbative expansions that are only valid in the large-width limit.
- The paper does not provide quantitative error bounds for the Ricci scalar computations in high dimensions.
- Linear interpolation in pixel space rather than manifold-respecting paths introduces unknown bias in sampling the volume element.

## Confidence
- **High confidence**: The spherical symmetry of infinite-width random networks (Mechanism 2) - this follows directly from Gaussian weight independence and is mathematically rigorous.
- **Medium confidence**: The boundary magnification phenomenon (Mechanism 1) - well-supported by experiments but relies on assumptions about smoothness and numerical stability of the metric computation.
- **Medium confidence**: The perturbative correction framework (Mechanism 3) - theoretically sound but depends on uncontrolled approximations in the finite-width regime.

## Next Checks
1. Compute the eigenvalue spectrum of the metric during training to verify that the magnification is not caused by numerical instabilities or degeneracy in the representation.
2. Replicate the boundary magnification experiments using manifold-respecting interpolation paths (e.g., via generative models) rather than linear pixel interpolation to test the robustness of the observed effects.
3. Systematically vary network width and depth to map out the regime where perturbative corrections to the NNGP geometry become significant, and compare with theoretical predictions.