---
ver: rpa2
title: 'Exploring the Use of Large Language Models for Reference-Free Text Quality
  Evaluation: An Empirical Study'
arxiv_id: '2304.00723'
source_url: https://arxiv.org/abs/2304.00723
tags:
- score
- chatgpt
- prompt
- text
- story
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the use of large language models (LLMs),
  particularly ChatGPT, for reference-free text quality evaluation. The authors compare
  three methods: Explicit Score (direct numeric scoring), Implicit Score (based on
  model confidence), and Pairwise Comparison (direct text comparison).'
---

# Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study

## Quick Facts
- arXiv ID: 2304.00723
- Source URL: https://arxiv.org/abs/2304.00723
- Reference count: 12
- ChatGPT's Explicit Score method outperforms most existing automatic metrics for reference-free text quality evaluation

## Executive Summary
This paper investigates the use of large language models, particularly ChatGPT, for reference-free text quality evaluation across multiple text generation tasks. The authors compare three evaluation methods: Explicit Score (direct numeric scoring), Implicit Score (based on model confidence), and Pairwise Comparison (direct text comparison). ChatGPT's Explicit Score method demonstrates superior performance compared to most existing automatic metrics, while pairwise comparison yields suboptimal results due to ChatGPT's high quality standards. The study provides valuable insights into the strengths and limitations of using LLMs for automated text quality assessment.

## Method Summary
The study evaluates text quality using three distinct approaches: Explicit Score (ChatGPT generates direct numeric scores), Implicit Score (based on text-davinci model confidence), and Pairwise Comparison (ChatGPT directly compares two texts). The evaluation was conducted across four text generation tasks - summarization, dialogue response, story generation, and paraphrase generation - using datasets including SummEval, FED, OpenMEV A-ROC, and Twitter-Para. Performance was measured through correlation with human judgments using Spearman and Pearson coefficients, along with Kendall's tau-b for pairwise comparisons.

## Key Results
- ChatGPT's Explicit Score method outperforms most existing automatic metrics for reference-free evaluation
- Explicit Score using greedy decoding shows improved performance over sampling methods
- Direct pairwise comparison using ChatGPT yields suboptimal results due to the model's high quality standards
- Implicit Score is generally less effective than Explicit Score, likely due to its limited range and peaked distribution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit Score using ChatGPT generates more reliable quality evaluations than Implicit Score.
- Mechanism: Direct numeric generation by ChatGPT allows finer-grained discrimination across quality levels, while Implicit Score based on token probabilities is limited by peaked distribution and binary framing.
- Core assumption: ChatGPT's generation process can produce more nuanced scores than binary confidence-based Implicit Score.
- Evidence anchors:
  - [section]: "Compared to directly generating an Explicit Score with ChatGPT to measure text quality, using the confidence of text-davinci series models to determine whether a text meets certain evaluation criteria (Implicit Score) is less effective."
  - [abstract]: "The Explicit Score, which utilizes ChatGPT to generate a numeric score measuring text quality, is the most effective and reliable method among the three exploited approaches."

### Mechanism 2
- Claim: Greedy decoding strategy produces more reliable Explicit Scores than sampling methods.
- Mechanism: Greedy decoding reduces variability in generated scores, leading to more consistent evaluations across similar texts.
- Core assumption: Sampling introduces unnecessary noise in score generation that doesn't improve evaluation quality.
- Evidence anchors:
  - [section]: "Additionally, the performance of the Explicit Score further improves when we use greedy search instead of Top-P sampling for decoding."
  - [abstract]: "The Explicit Score, which utilizes ChatGPT to generate a numeric score measuring text quality, is the most effective and reliable method among the three exploited approaches."

### Mechanism 3
- Claim: Direct pairwise comparison using ChatGPT yields suboptimal results due to ChatGPT's high quality standards.
- Mechanism: ChatGPT's stringent evaluation criteria make it difficult to distinguish between two texts it considers both subpar, leading to inconsistent or tie decisions.
- Core assumption: ChatGPT evaluates text quality against very high standards, making most generated texts appear equally poor.
- Evidence anchors:
  - [abstract]: "However, directly comparing the quality of two texts using ChatGPT may lead to suboptimal results."
  - [section]: "We speculate that the poor quality of the candidate texts used in our experiments is the main reason why comparing pairs directly with ChatGPT did not yield good results."

## Foundational Learning

- Concept: Reference-free evaluation methods
  - Why needed here: The study focuses on evaluating text quality without ground truth references, which is crucial for real-world applications where references may not exist.
  - Quick check question: What are the key differences between reference-based and reference-free evaluation methods?

- Concept: Spearman and Pearson correlation coefficients
  - Why needed here: These statistical measures are used to evaluate the reliability of different evaluation methods by comparing their scores with human judgments.
  - Quick check question: When would you use Spearman correlation versus Pearson correlation for evaluating evaluation methods?

- Concept: Token probability and confidence-based scoring
  - Why needed here: Implicit Score relies on token probabilities to determine text quality, requiring understanding of how language models generate probabilities.
  - Quick check question: How does token probability relate to model confidence in evaluating text quality?

## Architecture Onboarding

- Component map: Datasets -> Text Generation -> Evaluation Methods (Explicit Score, Implicit Score, Pairwise Comparison) -> Correlation with Human Judgments
- Critical path: Text generation -> Evaluation method application -> Correlation calculation with human judgments
- Design tradeoffs: Explicit Score offers better discrimination but requires direct generation, while Implicit Score is more efficient but has limited range. Pairwise Comparison is intuitive but suffers from ChatGPT's high standards.
- Failure signatures: Poor correlation with human judgments, inconsistent scores across similar texts, excessive tie decisions in pairwise comparisons, peaked distributions in Implicit Score.
- First 3 experiments:
  1. Compare Explicit Score performance using greedy vs sampling decoding strategies on a small dataset
  2. Test Implicit Score effectiveness using different text-davinci model versions (001 vs 003)
  3. Evaluate pairwise comparison reliability with varying prompt designs on texts of different quality levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on evaluating extremely high-quality or extremely low-quality text compared to human raters?
- Basis in paper: The paper mentions that their evaluation has been limited to short texts and may have introduced bias due to non-uniform quality distribution, lacking extremely high-quality texts.
- Why unresolved: The study focused on a subset of data with moderate quality texts and didn't specifically test the extremes of text quality.
- What evidence would resolve it: Experiments testing LLMs on datasets with deliberately crafted extremely high and low quality texts across various text generation tasks.

### Open Question 2
- Question: How would prompt design with few-shot demonstrations or multi-turn conversations affect the performance of LLMs in text quality evaluation?
- Basis in paper: The paper mentions that their exploration of prompts was limited to a few basic variations and suggests that future research could benefit from more sophisticated prompt designs.
- Why unresolved: The study only tested a limited set of prompt variations and didn't explore more advanced prompting techniques.
- What evidence would resolve it: Systematic comparison of different prompt designs including few-shot examples, multi-turn conversations, and more precise annotation guidelines across various text generation tasks.

### Open Question 3
- Question: How do different decoding strategies (e.g., beam search, sampling with temperature) affect the reliability of Implicit Scores compared to Explicit Scores?
- Basis in paper: The paper used greedy decoding for some experiments and sampling for others, noting differences in performance, but didn't systematically compare different decoding strategies.
- Why unresolved: The study primarily compared greedy vs sampling decoding but didn't explore other decoding strategies or their effects on Implicit Scores specifically.
- What evidence would resolve it: Comparative analysis of Implicit Scores using various decoding strategies (beam search, nucleus sampling, temperature-based sampling) across multiple LLMs and text generation tasks.

## Limitations

- Limited exploration of prompt variations, focusing only on basic prompt designs
- Evaluation restricted to short texts, potentially introducing bias due to non-uniform quality distribution
- Lack of extremely high-quality texts in the evaluation datasets

## Confidence

- **High Confidence**: The superiority of Explicit Score over most existing metrics for reference-free evaluation is well-supported by correlation analysis with human judgments across multiple tasks.
- **Medium Confidence**: The claim about Greedy decoding producing more reliable Explicit Scores than sampling methods is supported but could benefit from more extensive testing across different text qualities.
- **Medium Confidence**: The explanation for pairwise comparison failures due to ChatGPT's high standards is plausible but not definitively proven, as the study does not test with higher-quality candidate texts.

## Next Checks

1. **Quality Distribution Test**: Evaluate the pairwise comparison method using texts of varying quality levels, including some that approach ChatGPT's quality standards, to verify whether the failure is indeed due to text quality differences.

2. **Prompt Variation Analysis**: Systematically test different prompt formulations for the Explicit Score method to determine if certain phrasings improve discrimination between text qualities or reduce bias in scoring.

3. **Cross-Domain Generalization**: Apply the three evaluation methods to a new domain (e.g., code generation or technical writing) to assess whether the Explicit Score's superiority holds across different types of generated content.