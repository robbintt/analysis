---
ver: rpa2
title: 'Classic algorithms are fair learners: Classification Analysis of natural weather
  and wildfire occurrences'
arxiv_id: '2309.01381'
source_url: https://arxiv.org/abs/2309.01381
tags:
- accuracy
- algorithms
- data
- training
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates classic machine learning algorithms\u2014\
  Decision Trees, Boosting, k-Nearest Neighbors, Neural Networks, and Support Vector\
  \ Machines\u2014on sparse, imbalanced tabular datasets for classification tasks.\
  \ The experiments involved synthetic noise perturbations and hyperparameter tuning\
  \ to assess generalization capabilities."
---

# Classic algorithms are fair learners: Classification Analysis of natural weather and wildfire occurrences

## Quick Facts
- arXiv ID: 2309.01381
- Source URL: https://arxiv.org/abs/2309.01381
- Reference count: 20
- Classic ML algorithms show strong performance on sparse, imbalanced tabular datasets with hyperparameter tuning

## Executive Summary
This study evaluates classic machine learning algorithms—Decision Trees, Boosting, k-Nearest Neighbors, Neural Networks, and Support Vector Machines—on sparse, imbalanced tabular datasets for classification tasks. The experiments involved synthetic noise perturbations and hyperparameter tuning to assess generalization capabilities. Decision Trees and Boosting showed strong performance on the Wildfire dataset (accuracy up to 0.59), while Neural Networks and SVMs excelled on the higher-dimensional Rattle dataset (accuracy up to 0.86). k-Nearest Neighbors demonstrated robustness with low bias and variance. Hyperparameter tuning significantly improved accuracy over default settings, confirming that classic algorithms remain effective and fair learners even for limited and noisy data.

## Method Summary
The study evaluated five classic ML algorithms on two datasets: the Rattle dataset (56k samples, 65 features, binary classification) and the Wildfire dataset (11k samples, 7 features, multi-class classification). Data preprocessing included feature filtering, label encoding, and scaling. Baseline models were trained with default hyperparameters, followed by grid search for hyperparameter tuning. Learning curves were generated to diagnose bias-variance patterns. Models were evaluated using accuracy, precision, recall, and F1-score metrics on holdout test sets.

## Key Results
- Decision Trees and Boosting achieved highest accuracy (up to 0.59) on the sparse Wildfire dataset
- Neural Networks and SVMs performed best (up to 0.86) on the high-dimensional Rattle dataset
- k-Nearest Neighbors showed consistent robustness with low bias and variance across both datasets
- Hyperparameter tuning improved accuracy significantly over default settings for all algorithms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Classic algorithms maintain reasonable accuracy on sparse, imbalanced datasets due to their inherent inductive biases.
- Mechanism: Decision Trees split on features that maximize information gain, creating interpretable boundaries even with limited samples. SVMs seek maximal margin hyperplanes that generalize well despite few support vectors. kNN leverages local neighborhoods, avoiding assumptions about global structure.
- Core assumption: The datasets contain sufficient discriminative signal in the chosen features, even if distribution is sparse.
- Evidence anchors:
  - [abstract] "classic algorithms are fair learners even for such limited data due to their inherent properties even for noisy and sparse datasets."
  - [section] "Due to its algorithmic nature, decision trees are susceptible to overfitting leading to high variance with deeper trees... around values 5-7... the training and validation curves start to diverge indicating overfitting and high variance at higher values of max_depth."
  - [corpus] No direct evidence found; assumption must be validated empirically.
- Break condition: If features are too few or noisy, all algorithms will fail regardless of tuning.

### Mechanism 2
- Claim: Hyperparameter tuning substantially improves generalization by controlling model complexity and bias-variance trade-off.
- Mechanism: Tuning max_depth, min_samples_split, learning_rate, and C/γ directly adjusts the capacity of each model. Pruning reduces tree size to lower variance; boosting reduces bias by aggregating weak learners; SVM regularization balances margin size against misclassification penalty.
- Core assumption: There exists a configuration within the search grid that balances bias and variance for the given data.
- Evidence anchors:
  - [abstract] "Hyperparameter tuning significantly improved accuracy over default settings, confirming that classic algorithms remain effective and fair learners even for limited and noisy data."
  - [section] "The effects of various values for both these parameters are illustrated in Figure 12" (kNN), "Plotting the learning curve using the optimal hyper parameter values... Boosting has not completely addressed the high bias..." (Boosting).
  - [corpus] Weak evidence; claims rely on internal paper experiments.
- Break condition: If search space is too narrow or evaluation metric is misaligned, tuning may miss the optimal region.

### Mechanism 3
- Claim: Sparse tabular data benefits from algorithms that do not rely on high-dimensional embedding or massive data volume.
- Mechanism: Decision Trees and kNN work directly on the raw feature space; SVMs with linear kernels avoid overfitting in low-dimensional settings; shallow ANNs avoid vanishing gradients when features are categorical and few.
- Core assumption: High dimensionality or massive data volume is not required for these datasets to contain predictive patterns.
- Evidence anchors:
  - [abstract] "The paper intends to show that these classic algorithms are fair learners even for such limited data due to their inherent properties even for noisy and sparse datasets."
  - [section] "Due to the richness of data and feature set in Rattle, the data was linearly separable allowing the linear kernel to function with slightly higher accuracy than the RBF kernel."
  - [corpus] No corpus evidence; must infer from dataset descriptions.
- Break condition: If underlying signal is truly absent, no algorithm will recover it regardless of data sparsity.

## Foundational Learning

- Concept: Bias-Variance Trade-off
  - Why needed here: All experiments revolve around adjusting model capacity to balance overfitting (high variance) and underfitting (high bias).
  - Quick check question: If training accuracy is much higher than validation accuracy, which error component dominates?
- Concept: Information Gain and Entropy in Decision Trees
  - Why needed here: Used to decide split criteria; understanding it clarifies why categorical features may favor entropy over Gini.
  - Quick check question: In a binary split, if one child node is pure, what is the entropy of that child?
- Concept: Kernel Methods in SVMs
  - Why needed here: Determines how SVMs handle linear vs. non-linear separability; impacts choice of C and gamma.
  - Quick check question: What kernel would you choose for a dataset with clear linear boundaries and why?

## Architecture Onboarding

- Component map: Data preprocessing -> Baseline training -> Grid search -> Learning curve analysis -> Holdout evaluation -> Pruning/analysis
- Critical path: Data preprocessing (label encoding, scaling) -> Train/validation split -> Grid search for optimal hyperparameters -> Learning curve to diagnose bias/variance -> Holdout evaluation
- Design tradeoffs: Depth vs. accuracy in trees (deep trees overfit); learning_rate vs. n_estimators in boosting (slow learning needs more estimators); C vs. gamma in SVM (tight boundary vs. margin softness); neighbors count in kNN (locality vs. stability)
- Failure signatures: Training score much higher than validation score → high variance; training score low across sizes → high bias; no improvement after tuning → insufficient signal or bad metric choice
- First 3 experiments:
  1. Run Decision Tree with default params on both datasets; record training/validation accuracy and depth
  2. Perform grid search over max_depth and min_samples_split; plot validation accuracy heatmap
  3. Generate learning curves for best Decision Tree and for kNN; compare bias-variance patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific hyperparameters affect model performance on sparse and imbalanced datasets?
- Basis in paper: [explicit] The paper explicitly states that hyperparameters significantly impact accuracy on sparse and imbalanced data.
- Why unresolved: While the paper provides empirical results showing improved accuracy with tuned hyperparameters, it doesn't systematically isolate the effect of individual hyperparameters or provide a comprehensive analysis of their interactions.
- What evidence would resolve it: Controlled experiments varying one hyperparameter at a time while holding others constant, combined with statistical analysis of the impact on accuracy metrics.

### Open Question 2
- Question: What is the optimal hyperparameter configuration for each algorithm on sparse and imbalanced datasets?
- Basis in paper: [explicit] The paper mentions that hyperparameter tuning significantly improved accuracy over default settings.
- Why unresolved: The paper identifies optimal configurations for the specific datasets used, but doesn't generalize these findings to other sparse and imbalanced datasets or provide a framework for determining optimal configurations.
- What evidence would resolve it: A comprehensive study across multiple sparse and imbalanced datasets, identifying common patterns in optimal hyperparameter configurations and developing a generalized framework for hyperparameter selection.

### Open Question 3
- Question: How do classic algorithms compare to more advanced techniques (e.g., ensemble methods, deep learning) on sparse and imbalanced datasets?
- Basis in paper: [inferred] The paper focuses on classic algorithms and doesn't directly compare them to more advanced techniques.
- Why unresolved: The paper demonstrates the effectiveness of classic algorithms but doesn't provide a direct comparison with more advanced techniques, leaving open the question of whether classic algorithms are truly optimal for these types of datasets.
- What evidence would resolve it: A comparative study evaluating the performance of classic algorithms against advanced techniques (e.g., ensemble methods, deep learning) on a range of sparse and imbalanced datasets, using consistent evaluation metrics and hyperparameter tuning strategies.

## Limitations
- Evaluation focuses primarily on accuracy metrics without addressing algorithmic fairness criteria
- Hyperparameter search spaces are not fully specified, making exact reproduction challenging
- Study uses only two datasets, constraining generalizability of conclusions across different domains

## Confidence
- Claim: Hyperparameter tuning improves performance on sparse datasets - High
- Claim: Classic algorithms are "fair learners" - Low (lack of fairness-specific analysis)
- Claim: Inherent properties make algorithms suitable for sparse data - Medium (supported by learning curves but requires broader validation)

## Next Checks
1. Conduct fairness analysis using standard metrics (demographic parity, equal opportunity, disparate impact) across protected subgroups in both datasets
2. Expand experiments to include additional tabular datasets with varying sparsity levels and class imbalance ratios to test generalizability
3. Perform ablation studies comparing classic algorithms against modern sparse learning methods (e.g., LightGBM, CatBoost) on identical tasks and metrics