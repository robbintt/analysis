---
ver: rpa2
title: 'Technical Note: Defining and Quantifying AND-OR Interactions for Faithful
  and Concise Explanation of DNNs'
arxiv_id: '2304.13312'
source_url: https://arxiv.org/abs/2304.13312
tags:
- interactions
- interaction
- variables
- input
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces a new approach to explain DNNs by quantifying
  interactions between input variables, focusing on AND-OR interactions. The authors
  define two types of interactions: AND interaction, where the co-appearance of variables
  contributes to the model output, and OR interaction, where the appearance of any
  variable contributes.'
---

# Technical Note: Defining and Quantifying AND-OR Interactions for Faithful and Concise Explanation of DNNs

## Quick Facts
- arXiv ID: 2304.13312
- Source URL: https://arxiv.org/abs/2304.13312
- Reference count: 8
- Key outcome: Introduces AND-OR interactions to explain DNNs by quantifying AND/OR relationships between input variables, achieving both faithfulness and conciseness

## Executive Summary
This paper presents a novel approach to explain deep neural networks by quantifying AND-OR interactions between input variables. The method defines AND interactions (where co-appearance of variables contributes to output) and OR interactions (where appearance of any variable contributes), proving their uniqueness in capturing these logical relationships. By combining game-theoretic foundations with Lasso regularization, the approach achieves faithful explanations that are also concise through sparsity. The authors demonstrate that DNN inference logic can be explained using a small set of symbolic AND-OR interaction concepts.

## Method Summary
The method decomposes DNN outputs into AND and OR interaction effects using Harsanyi dividends and a modified formula, respectively. It optimizes the selection of these interactions using Lasso loss to boost sparsity while maintaining faithfulness constraints. The approach jointly learns both interaction types and allows small approximation errors to further improve conciseness. The extracted salient symbolic concepts from non-negligible interactions provide explanations of the DNN's inference logic.

## Key Results
- Proves uniqueness of AND and OR interactions for quantifying AND/OR relationships between variables
- Demonstrates that faithfulness and conciseness can be achieved simultaneously through sparsity optimization
- Shows that DNN inference logic can be explained by a small set of symbolic AND-OR interaction concepts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AND-OR interactions uniquely quantify the effect of AND (OR) relationships between input variables in DNNs
- Mechanism: Uses Harsanyi dividends for AND interactions and a complementary formulation for OR interactions, both satisfying faithfulness axioms
- Core assumption: DNN's output can be faithfully decomposed into interaction effects from AND and OR relationships
- Evidence anchors:
  - [abstract] "For faithfulness, we prove the uniqueness of the AND (OR) interaction in quantifying the effect of the AND (OR) relationship between input variables."
  - [section 2.1] "Ren et al. have proved that the Harsanyi dividend is the unique metric for AND interactions that fully satisfies the faithfulness requirement"
- Break condition: If DNN's decision boundary cannot be expressed as combinations of AND/OR relationships

### Mechanism 2
- Claim: Sparsity of interaction effects can be boosted without hurting faithfulness
- Mechanism: Uses Lasso regularization to optimize interaction selection while maintaining faithfulness constraints
- Core assumption: Most interaction effects are negligible and can be eliminated while preserving faithfulness
- Evidence anchors:
  - [abstract] "we design techniques to boost the conciseness of the explanation, while not hurting the faithfulness"
  - [section 2.3] "we optimize the AND interactions { ˆI AND(S)}S⊆N and the OR interactions { ˆI OR(S)}S⊆N using a Lasso loss, while forcing the faithfulness constraint in Eq. (6) to hold"
- Break condition: If DNN requires dense interaction patterns for faithful representation

### Mechanism 3
- Claim: Model output can be partitioned into AND and OR components for separate explanation
- Mechanism: Optimizes decomposition v(S) = vAND(S) + vOR(S) where each component uses its respective interaction type
- Core assumption: Any model output can be decomposed into components explainable by AND and OR interactions separately
- Evidence anchors:
  - [section 2.3] "we believe that the model output is a mixture model of both AND interactions and OR interactions"
- Break condition: If certain interaction patterns cannot be classified as purely AND or OR type

## Foundational Learning

- Concept: Game-theoretic interactions and Shapley values
  - Why needed here: Method builds on game theory foundations to define interaction effects
  - Quick check question: What is the relationship between Shapley values and Harsanyi dividends for AND interactions?

- Concept: Faithfulness and conciseness requirements for explanations
  - Why needed here: These are key criteria that method aims to satisfy simultaneously
  - Quick check question: How does the faithfulness constraint ensure explanation accurately reflects model's behavior?

- Concept: Lasso regularization and sparsity
  - Why needed here: Used to boost conciseness by eliminating negligible interaction effects
  - Quick check question: What role does Lasso loss play in optimization problem for interaction selection?

## Architecture Onboarding

- Component map:
  - Input preprocessing -> Mask generation for all 2^n variable subsets
  -> Interaction computation -> Harsanyi dividend calculation for AND interactions, OR interaction formula
  -> Optimization engine -> Lasso-based selection of sparse interaction set
  -> Explanation generation -> Symbolic representation of selected AND-OR interactions

- Critical path:
  1. Generate all masked input samples (2^n total)
  2. Compute model outputs for all masked samples
  3. Calculate AND-OR interactions using Eqs. (3) and (5)
  4. Optimize interaction selection with Lasso regularization
  5. Extract salient symbolic concepts from sparse interactions

- Design tradeoffs:
  - Computational complexity vs. explanation fidelity: Exponential in number of variables but can be approximated
  - Sparsity vs. faithfulness: Allowing small approximation errors can improve conciseness
  - AND vs. OR interactions: Different types capture different logical relationships

- Failure signatures:
  - Dense interaction patterns: Many non-zero interaction effects suggest model doesn't use simple AND-OR logic
  - Poor reconstruction accuracy: If model outputs cannot be accurately reconstructed from selected interactions
  - Instability: Different runs producing different interaction sets

- First 3 experiments:
  1. Simple AND gate verification: Test on 2-input AND gate to verify basic functionality
  2. Mixed AND-OR circuit: Create small circuit with both AND and OR gates to test interaction type discrimination
  3. Sentiment analysis benchmark: Apply to simple sentiment analysis task with clear AND-OR patterns in text

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we extend the AND-OR interaction framework to handle continuous input variables rather than just discrete binary variables?
- Basis in paper: [explicit] Paper focuses on interactions between discrete input variables using masking with baseline values
- Why unresolved: Current formulation relies on discrete masking operations not directly applicable to continuous inputs
- What evidence would resolve it: Theoretical extension of AND-OR interaction definitions to continuous variables with experimental validation

### Open Question 2
- Question: What is the computational complexity of computing AND-OR interactions for large-scale DNNs with thousands of input variables?
- Basis in paper: [inferred] Paper mentions potentially 2^n interactions but provides no complexity analysis
- Why unresolved: Exponential growth in possible interactions makes approach potentially intractable for large-scale models
- What evidence would resolve it: Detailed computational complexity analysis showing scalability with input dimensionality and empirical performance results

### Open Question 3
- Question: How do AND-OR interactions relate to other established interpretability methods like feature importance scores or counterfactual explanations?
- Basis in paper: [explicit] Mentions connections to Shapley values and Shapley interaction indices but lacks comprehensive comparison
- Why unresolved: Relationship between AND-OR interactions and other interpretability frameworks remains unclear
- What evidence would resolve it: Systematic comparison showing how AND-OR interactions relate to other methods in terms of faithfulness, conciseness, and explanatory power

## Limitations
- Computational scalability: Method requires evaluating 2^n masked input combinations, impractical for high-dimensional inputs
- Approximation trade-offs: Sparsity-boosting relaxation introduces potential faithfulness violations that are difficult to quantify
- Interaction classification: Separation of outputs into AND and OR components may not always be clear-cut or optimal

## Confidence
- **High confidence** in theoretical foundations regarding Harsanyi dividends and their uniqueness for AND interactions
- **Medium confidence** in sparsity optimization approach and its ability to maintain faithfulness while boosting conciseness
- **Low confidence** in decomposition assumption that all model outputs can be expressed as combinations of AND-OR interactions

## Next Checks
1. Controlled logic circuit tests: Validate method on synthetic circuits with known AND-OR structures to verify correct interaction identification
2. Faithfulness verification: Systematically measure reconstruction error as function of sparsity penalty to establish faithfulness-sparsity trade-off curve
3. Cross-model consistency: Apply method to multiple models trained on same task to assess stability and consistency of extracted interaction patterns