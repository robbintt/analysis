---
ver: rpa2
title: 'Exploring Large Language Models for Communication Games: An Empirical Study
  on Werewolf'
arxiv_id: '2309.04658'
source_url: https://arxiv.org/abs/2309.04658
tags:
- player
- your
- players
- werewolf
- moderator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a tuning-free framework to engage large language
  models (LLMs) in communication games using the example of Werewolf. The approach
  keeps LLMs frozen and improves performance through retrieval and reflection on past
  communications and experiences.
---

# Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf

## Quick Facts
- arXiv ID: 2309.04658
- Source URL: https://arxiv.org/abs/2309.04658
- Reference count: 38
- Primary result: Tuning-free framework enables LLMs to play Werewolf effectively using retrieval and reflection, with emergent strategic behaviors like trust, confrontation, camouflage, and leadership

## Executive Summary
This paper introduces a novel tuning-free framework that enables large language models (LLMs) to participate effectively in communication games, specifically demonstrating the approach on the popular social deduction game Werewolf. The framework addresses two key challenges: limited context length and the need for learning from experience without parameter tuning. By combining strategic information retrieval, reflection mechanisms, and experience-based suggestions, the system allows LLMs to maintain coherent gameplay and develop strategic behaviors while keeping the models frozen. Experiments demonstrate that the framework can achieve competitive performance in Werewolf without requiring any fine-tuning of LLM parameters.

## Method Summary
The framework engages frozen LLMs in Werewolf by maintaining an experience pool of past game rounds, each scored based on outcomes (winners vs losers). During gameplay, the system retrieves relevant historical experiences using similarity matching with current reflections and extracts actionable suggestions. To address context limitations, it selectively includes recent messages for freshness, uses rule matching to identify informative messages, and employs reflection questions answered through historical retrieval. The approach uses chain-of-thought prompting to enable complex strategic reasoning, breaking down decision-making into step-by-step processes. The entire system operates without any parameter tuning of the underlying LLMs.

## Key Results
- The framework effectively plays Werewolf without tuning LLM parameters
- Strategic behaviors including trust, confrontation, camouflage, and leadership emerge during gameplay
- The villager side's winning rate increases in most cases when using experience pools, though the trend is not clear as experience amount increases
- Performance becomes unstable when the experience pool size exceeds 20 rounds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Historical experience can improve LLM performance in communication games without tuning parameters.
- Mechanism: The framework collects experiences from past game rounds, scores them based on outcome (winners vs losers), and retrieves relevant experiences using similarity matching with current reflections. Suggestions extracted from these experiences guide decision-making.
- Core assumption: Experiences from winning rounds contain useful strategies that can be generalized to similar situations in new rounds.
- Evidence anchors:
  - [abstract]: "Experiments on Werewolf demonstrate that our framework can effectively play Werewolf game without tuning the parameters of the LLMs."
  - [section 3.4]: "Experience Pool. The experience pool is a collection of response, reflection and score tuples. For each agent i, we define the experience Er_i collected from it in round r as..."
  - [corpus]: Weak evidence - the corpus shows similar work exists but doesn't directly support the specific scoring/retrieval mechanism described here.
- Break condition: If the scoring heuristic fails to distinguish useful from useless experiences, or if retrieved experiences are not contextually relevant to current situations.

### Mechanism 2
- Claim: Limited context length can be addressed through strategic information selection and reflection.
- Mechanism: The framework maintains freshness by including recent messages, informativeness by selecting key messages through rule matching, and completeness through reflection on questions answered using retrieved historical context.
- Core assumption: LLMs can effectively reason about game state when provided with a compact representation containing recent, informative, and reflected information rather than raw full history.
- Evidence anchors:
  - [section 3.3]: "For efficiency, we collect the easy-to-identify informative messages using rule matching and fill the top N of them ranked by a heuristic metric into the prompt..."
  - [abstract]: "The framework addresses the context-length limitation by retrieving and reflecting on necessary historical information..."
  - [corpus]: Moderate evidence - related work exists on LLM context management but not specifically for Werewolf communication games.
- Break condition: If the heuristic for selecting informative messages fails to capture critical game information, or if reflection questions don't retrieve relevant context.

### Mechanism 3
- Claim: Chain-of-thought prompting enables complex reasoning in multi-agent communication games.
- Mechanism: The framework includes chain-of-thought reasoning as part of the prompt, breaking down decision-making into step-by-step processes that help LLMs handle complex strategic reasoning.
- Core assumption: LLMs can perform better reasoning when explicitly prompted to think through problems step-by-step rather than generating responses directly.
- Evidence anchors:
  - [section 3.2]: "Chain-of-thought prompt to elicit reasoning (part 4)" and the prompt example shows step-by-step thinking.
  - [abstract]: "strategic behaviors such as trust, confrontation, camouflage, and leadership begin to emerge, indicating the potential of LLMs in communication games."
  - [corpus]: Strong evidence - chain-of-thought prompting is well-established in LLM literature for complex reasoning tasks.
- Break condition: If the step-by-step reasoning becomes too verbose and exceeds context limits, or if the reasoning steps don't actually improve decision quality.

## Foundational Learning

- Concept: Prompt engineering for role-based behavior
  - Why needed here: Different roles (werewolf, villager, seer, etc.) have distinct objectives and abilities that require specific prompting strategies
  - Quick check question: How would you modify the prompt for a werewolf versus a villager to align with their different win conditions?

- Concept: Similarity-based retrieval for experience matching
  - Why needed here: The framework needs to find relevant past experiences without fine-tuning, requiring effective similarity search
  - Quick check question: What distance metric would you use to compare current reflections with past experiences for retrieval?

- Concept: Scoring functions for experience prioritization
  - Why needed here: The framework needs to prioritize which past experiences are most valuable for learning, using game outcomes as feedback
  - Quick check question: How would you modify the scoring function if you wanted to penalize slow victories more heavily?

## Architecture Onboarding

- Component map:
  - Game engine -> LLM backend -> Experience pool -> Retrieval system -> Prompt builder -> Scoring system

- Critical path:
  1. Game state → Prompt builder → LLM → Response
  2. Response + Reflection → Experience pool (end of round)
  3. Current Reflection → Retrieval → Suggestion extraction → Prompt builder

- Design tradeoffs:
  - Context length vs. completeness: Including more recent messages improves freshness but reduces space for other components
  - Retrieval threshold vs. suggestion quality: Higher thresholds reduce noise but may limit useful experiences
  - Scoring simplicity vs. learning effectiveness: Simple scoring is easier to implement but may miss nuanced learning opportunities

- Failure signatures:
  - Agent generates responses that contradict its stated role
  - Agent forgets key game state information between turns
  - Agent fails to adapt strategy despite accumulating experience
  - Agent generates overly long or short responses that break game flow

- First 3 experiments:
  1. Baseline test: Run game without experience pool to establish performance without learning
  2. Experience pool test: Run with experience pool but disable retrieval to test storage mechanism
  3. Retrieval test: Run with full system but use synthetic experiences to test retrieval quality independently

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do emergent strategic behaviors like trust, confrontation, camouflage, and leadership evolve as LLMs gain more experience through multiple rounds of gameplay?
- Basis in paper: [explicit] The paper observes these behaviors emerging in experiments and notes that the winning rate of the villager side increases in most cases when using experience pools, but the trend is not clear as the amount of experience increases.
- Why unresolved: The paper only conducts experiments with experience pools of 10, 20, 30, and 40 rounds, which may not be sufficient to fully understand the long-term evolution of these behaviors.
- What evidence would resolve it: Conducting experiments with larger experience pools and tracking the emergence and evolution of strategic behaviors over a greater number of rounds would provide more insights into how these behaviors develop and stabilize.

### Open Question 2
- Question: What is the impact of different score functions for evaluating experiences on the learning process and strategic behaviors of LLMs?
- Basis in paper: [explicit] The paper mentions that the current score function is simple and that more sophisticated score functions could be explored in the future.
- Why unresolved: The paper only uses a basic score function and does not investigate how different scoring mechanisms might influence the learning and behavior of LLMs.
- What evidence would resolve it: Experimenting with various score functions and analyzing their effects on the learning process and strategic behaviors of LLMs would provide insights into the importance of scoring mechanisms.

### Open Question 3
- Question: How do LLMs adapt their strategies in response to changes in the strategies of other LLMs in multi-player communication games?
- Basis in paper: [explicit] The paper notes that the capability of the LLMs might change in response to variations of the capability of other LLMs, and that this could explain the trends observed in the winning rate and game duration.
- Why unresolved: The paper does not delve into the mechanisms behind this adaptive behavior or how it influences the overall gameplay.
- What evidence would resolve it: Analyzing the interactions between LLMs and how their strategies evolve in response to each other would provide insights into the dynamics of multi-player communication games.

## Limitations
- The scoring function and similarity metrics are not fully specified, making it difficult to reproduce the exact behavior
- The framework shows performance instability when experience pool size exceeds 20 rounds, suggesting scalability issues
- The emergence of strategic behaviors is observational rather than quantitatively measured

## Confidence

- **High confidence**: The basic architecture of using frozen LLMs with retrieval and reflection is technically sound and aligns with established LLM prompting techniques. The claim that chain-of-thought prompting enables complex reasoning is well-supported by existing literature.
- **Medium confidence**: The framework's ability to address context-length limitations through strategic information selection is plausible but requires empirical validation. The scoring and retrieval mechanisms show promise but their effectiveness depends on implementation details not fully specified.
- **Low confidence**: The emergence of strategic behaviors (trust, confrontation, camouflage, leadership) is an observational claim that requires more rigorous analysis. The paper provides qualitative observations but lacks quantitative measures of these emergent strategies.

## Next Checks

1. **Ablation study on experience pool size**: Systematically vary the number of stored experiences (e.g., 5, 10, 20, 50 rounds) and measure winning rates and game duration to identify the optimal experience pool size and test the claim about instability beyond 20 rounds.

2. **Cross-validation of scoring function**: Create synthetic game outcomes with known strategic value and test whether the scoring function correctly prioritizes more valuable experiences over less valuable ones, validating the learning mechanism.

3. **Behavioral analysis of strategic emergence**: Track specific in-game actions (e.g., number of trust statements, confrontation frequency, camouflage attempts) across multiple games and correlate these with game outcomes to quantitatively measure the emergence of claimed strategic behaviors.