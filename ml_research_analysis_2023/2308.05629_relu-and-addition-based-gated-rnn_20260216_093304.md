---
ver: rpa2
title: ReLU and Addition-based Gated RNN
arxiv_id: '2308.05629'
source_url: https://arxiv.org/abs/2308.05629
tags:
- state
- gate
- multiplication
- which
- addition-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel gating mechanism for recurrent neural
  networks (RNNs) that replaces the conventional multiplication and sigmoid function
  with addition and ReLU activation. The motivation is to reduce computational costs,
  particularly for hardware architectures or arithmetic systems where multiplication
  is expensive, such as homomorphic encryption.
---

# ReLU and Addition-based Gated RNN

## Quick Facts
- arXiv ID: 2308.05629
- Source URL: https://arxiv.org/abs/2308.05629
- Reference count: 31
- Primary result: Addition-based gated RNN achieves comparable accuracy to GRU/LSTM while reducing execution time by 50% on CPU and 33% under homomorphic encryption

## Executive Summary
This paper introduces a novel gating mechanism for recurrent neural networks that replaces conventional multiplication and sigmoid functions with addition and ReLU activation. The primary motivation is to reduce computational costs, particularly for hardware architectures where multiplication is expensive, such as homomorphic encryption systems. The authors demonstrate that their addition-based gated RNN can solve standard synthetic sequence learning tasks while significantly reducing execution time compared to conventional gated RNNs. Experimental results on handwriting recognition tasks show comparable accuracy to conventional GRU and LSTM baselines.

## Method Summary
The paper proposes a new gating mechanism for RNNs that uses addition and ReLU activation instead of multiplication and sigmoid. The core formulation replaces conventional gates with equations like ht = (ht-1 + u_t-)+ + (ht - u_t+)+, where the gate computation uses only addition and ReLU operations. This approach enables computational efficiency gains while maintaining similar gating behavior for extreme values. The method is evaluated on a synthetic addition task using handcrafted weights and on real-world handwriting recognition tasks (MNIST and IAM Words) using trained models with Adam optimizer (batch size 64) for 50 epochs.

## Key Results
- Addition-based gated RNN reduces execution time by half on CPU compared to conventional GRU
- Under homomorphic encryption, the method reduces execution time by one-third compared to multiplication-based formulations
- Achieves comparable accuracy to GRU and LSTM baselines on MNIST and IAM Words datasets, though slightly lower
- The addition-based formulation requires 4 Programmable Bootstrap operations in TFHE versus 6 for multiplication-based formulations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing multiplication with addition and ReLU activation reduces computational cost while preserving gating function
- Mechanism: The addition-based gate uses ht = (ht-1 + u_t-)+ + (ht - u_t+)+ to replace sigmoid and element-wise multiplication, achieving similar thresholding behavior without expensive multiplications
- Core assumption: ReLU activation with addition can replicate sigmoid's gating behavior for extreme values (near 0 and 1) while being computationally cheaper
- Evidence anchors: [abstract], [section 3.1], weak evidence from related works on FHE applications

### Mechanism 2
- Claim: Avoiding multiplication between variables enables better performance under homomorphic encryption
- Mechanism: By eliminating element-wise multiplication between encrypted variables, the architecture reduces expensive Programmable Bootstrap operations required in TFHE
- Core assumption: Multiplication between encrypted variables is the dominant cost in homomorphic evaluation of RNNs
- Evidence anchors: [abstract], [section 4.2], strong evidence from multiple related papers targeting FHE efficiency

### Mechanism 3
- Claim: Non-negative state constraint enables the addition-based gate to preserve long-term memory
- Mechanism: By requiring ht ≥ 0, the gate ensures ht ≈ ht-1 when u_t is large positive, naturally preserving information across time steps
- Core assumption: Non-negative activation functions combined with the proposed update rule create memory retention properties similar to conventional gates
- Evidence anchors: [section 3.1], weak evidence from lack of direct validation of non-negativity requirement

## Foundational Learning

- Concept: Gating mechanisms in RNNs (LSTM, GRU)
  - Why needed here: The paper builds directly on understanding how gates control information flow through multiplication and sigmoid functions
  - Quick check question: What are the two roles multiplication plays in conventional gated RNNs?

- Concept: Homomorphic encryption and TFHE scheme
  - Why needed here: The computational benefits are specifically demonstrated under encrypted evaluation using TFHE
  - Quick check question: What TFHE operation is required for multiplication between encrypted variables and why is it expensive?

- Concept: Activation functions and their computational properties
  - Why needed here: The switch from sigmoid to ReLU is central to the proposed mechanism's efficiency
  - Quick check question: How does ReLU's computational complexity compare to sigmoid on typical hardware?

## Architecture Onboarding

- Component map: Input → Gate computation → Proposal computation → State update → Output
- Critical path: Each step must maintain non-negative values through ReLU activation
- Design tradeoffs:
  - Addition-based gate: Lower computational cost, natural quantization support, but potentially less precise interpolation than sigmoid
  - Non-negative state constraint: Enables memory preservation but may limit representational capacity
  - FHE compatibility: Excellent for privacy applications but requires careful handling of precision
- Failure signatures:
  - Training instability or divergence (likely from improper handling of non-negative constraints)
  - Performance degradation on tasks requiring fine-grained gating (middle range of sigmoid values)
  - Precision loss under encryption larger than expected (indicates numerical stability issues)
- First 3 experiments:
  1. Implement the synthetic addition task with handcrafted weights to verify memory preservation capability
  2. Compare execution time on CPU for addition-based vs multiplication-based GRU with identical architecture
  3. Test training convergence on MNIST with both gate types using identical hyperparameters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the computational advantages of the addition-based gated RNN translate into actual energy efficiency gains across different hardware architectures?
- Basis in paper: [explicit] The paper mentions that addition-based formulations can avoid expansion to double precision required for multiplication, and that different operations have different power and energy requirements, but does not explicitly measure energy efficiency
- Why unresolved: The paper only examined execution time on CPU for single-thread execution and did not measure energy consumption across different hardware platforms
- What evidence would resolve it: Direct energy measurements comparing addition-based and multiplication-based RNNs across multiple hardware architectures (CPU, GPU, specialized accelerators) under various workloads and precision settings

### Open Question 2
- Question: How does the quantization performance of the addition-based gated RNN compare to conventional gated RNNs in both plaintext and homomorphic encryption settings?
- Basis in paper: [explicit] The paper states that the addition-based formulation "works naturally with integers" and "may therefore be realized under quantization," while noting that quantization is "hard for recurrent neural networks" in both plaintext and homomorphic encryption contexts
- Why unresolved: The paper did not include experiments testing quantized implementations of the addition-based gate, despite suggesting potential performance gains
- What evidence would resolve it: Comparative experiments testing quantized versions of addition-based and conventional gated RNNs across multiple precision levels in both unencrypted and encrypted settings, measuring accuracy, speed, and energy consumption

### Open Question 3
- Question: What is the impact of state range constraints on the learning capacity of the addition-based gated RNN, and how can this be optimized?
- Basis in paper: [explicit] The paper notes that requiring non-negative state (using ReLU activation) was "less restrictive than it may first appear" through state shifting, and shows that shifted models achieved better accuracy than non-negative versions, but suggests "the importance of this could be examined further"
- Why unresolved: While the paper demonstrates that state shifting improves performance, it did not systematically investigate optimal state range configurations or their impact on learning dynamics
- What evidence would resolve it: Systematic experiments varying state range constraints and initialization strategies across multiple tasks, measuring both training dynamics and final performance to identify optimal configurations

## Limitations

- The mechanism's performance on tasks requiring fine-grained gating interpolation (sigmoid values near 0.5) remains unproven
- The necessity of the non-negative state constraint for proper gate function lacks direct experimental validation
- Computational efficiency gains demonstrated primarily on CPU and under encryption, with limited exploration of other hardware architectures

## Confidence

- **High confidence**: Computational efficiency improvements on CPU and under homomorphic encryption are well-supported by empirical measurements
- **Medium confidence**: The addition-based gate's ability to preserve long-term memory through the proposed mechanism is theoretically sound but requires more rigorous validation
- **Low confidence**: The claim that the non-negative state constraint is necessary for the gate to function properly lacks direct experimental validation

## Next Checks

1. Conduct ablation studies removing the non-negative state constraint to test its necessity for proper gate function
2. Evaluate performance on tasks requiring precise gating interpolation (not just extreme values) to identify potential limitations
3. Test quantized implementations of the addition-based gate across different bit-widths to quantify precision trade-offs