---
ver: rpa2
title: 'LASER: LLM Agent with State-Space Exploration for Web Navigation'
arxiv_id: '2309.08172'
source_url: https://arxiv.org/abs/2309.08172
tags:
- agent
- item
- button
- state
- laser
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LASER is an LLM agent that tackles web navigation as state-space
  exploration. It defines a set of high-level states with unique action spaces, enabling
  the agent to flexibly backtrack from errors.
---

# LASER: LLM Agent with State-Space Exploration for Web Navigation

## Quick Facts
- arXiv ID: 2309.08172
- Source URL: https://arxiv.org/abs/2309.08172
- Reference count: 8
- Primary result: LASER achieves 50.0% success rate on WebShop task, closing gap with human performance

## Executive Summary
LASER introduces a novel approach to web navigation using state-space exploration with an LLM agent. The agent transitions among pre-defined high-level states, each with its own action space, enabling flexible backtracking and error recovery. Without relying on in-context examples, LASER significantly outperforms previous methods on the WebShop task, achieving a 50.0% success rate and 75.6 reward score.

## Method Summary
LASER tackles web navigation as state-space exploration by defining a set of high-level states with unique action spaces. The agent uses state-specific instructions instead of in-context examples to guide behavior in each state. It employs function-calling functionality to enforce valid actions by selecting from pre-defined permissible actions. The agent is tested on the WebShop simulated e-commerce environment using GPT-4-0613 model with up to 15 steps per episode.

## Key Results
- LASER achieves 50.0% success rate on WebShop task
- Performance significantly exceeds previous methods (ReAct: 31.2%, ASH: 30.3%, WebGUM: 22.6%)
- LASER achieves 75.6 reward score, closing gap with human performance (74.2%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: State-space exploration allows the agent to recover from errors by enabling backtracking.
- Mechanism: The agent transitions among a pre-defined set of high-level states, each with its own action space. When an error occurs, the agent can take another action that transitions it back to a previous state.
- Core assumption: The pre-defined states accurately capture the possible scenarios the agent might encounter during task execution.
- Evidence anchors:
  - [abstract]: "This formulation enables flexible backtracking, allowing the model to recover from errors easily."
  - [section]: "This formulation effectively converts the LLM agent's exploration in the interactive task as state transitions, where each action takes the agent from one state to another. Naturally, this allows the agent to easily recover from one wrong action: taking another action that would transition it back to the previous state."
  - [corpus]: Weak evidence - no direct mention of error recovery in corpus neighbors.

### Mechanism 2
- Claim: State-specific instructions guide the agent more effectively than in-context examples.
- Mechanism: For each state, the agent receives a generic instruction that describes the state in detail, including a sample layout and specific actions to take. This replaces the need for in-context examples.
- Core assumption: The state-specific instructions are detailed and informative enough to guide the agent through all possible scenarios in that state.
- Evidence anchors:
  - [abstract]: "This formulation clearly associates the action space with each individual state, which reduces the difficulty of the task and allows the agent to always select the valid action at any step."
  - [section]: "For each state, we write a generic instruction that describes the state in detail. Specifically, we provide a sample layout of the observation the agent would receive in that state and replace all specifications in the layout with placeholders. We also provide a high-level goal and detailed instructions to act in that state."
  - [corpus]: Weak evidence - no direct mention of state-specific instructions in corpus neighbors.

### Mechanism 3
- Claim: Function-calling functionality improves the agent's performance by enforcing valid actions.
- Mechanism: The agent uses the function-calling functionality to select actions from a predefined list of permissible actions for each state. This ensures that the agent always performs valid actions.
- Core assumption: The function-calling functionality is robust and can handle the predefined list of actions effectively.
- Evidence anchors:
  - [abstract]: "Moreover, our proposed formulation clearly associates the action space with each individual state, which reduces the difficulty of the task and allows the agent to always select the valid action at any step."
  - [section]: "To address the aforementioned issues, we propose an LLM agent that tackles the web navigation task as state-space exploration and the agent's exploration is guided by state-specific instructions."
  - [corpus]: Weak evidence - no direct mention of function-calling in corpus neighbors.

## Foundational Learning

- Concept: State-space exploration
  - Why needed here: It allows the agent to recover from errors by enabling backtracking and ensures that the agent always performs valid actions.
  - Quick check question: What is the main advantage of using state-space exploration in web navigation tasks?

- Concept: State-specific instructions
  - Why needed here: They guide the agent more effectively than in-context examples by providing detailed information about each state and the actions to take.
  - Quick check question: How do state-specific instructions differ from in-context examples in guiding the agent?

- Concept: Function-calling functionality
  - Why needed here: It enforces valid actions by allowing the agent to select actions from a predefined list of permissible actions for each state.
  - Quick check question: What is the role of function-calling functionality in ensuring that the agent always performs valid actions?

## Architecture Onboarding

- Component map: State-space exploration -> State-specific instructions -> Function-calling functionality
- Critical path:
  1. Define the set of high-level states and their corresponding action spaces.
  2. Write state-specific instructions for each state, including a sample layout and specific actions to take.
  3. Implement the function-calling functionality to enforce valid actions.
  4. Test the agent on the WebShop task and evaluate its performance.
- Design tradeoffs:
  - Using state-space exploration instead of in-context examples: Provides more flexibility and reduces the need for a large number of examples, but requires manual annotation of possible states.
  - Using state-specific instructions instead of in-context examples: Provides more detailed guidance for each state, but requires writing detailed instructions for each state.
  - Using function-calling functionality instead of regular text generation: Enforces valid actions, but requires the availability of function-calling functionality.
- Failure signatures:
  - The agent makes invalid actions or gets stuck: This could be due to incomplete or inaccurate state-specific instructions or a lack of function-calling functionality.
  - The agent fails to recover from errors: This could be due to an incomplete set of pre-defined states or a lack of backtracking capability.
  - The agent performs poorly on the WebShop task: This could be due to a combination of the above issues or a lack of sufficient training data.
- First 3 experiments:
  1. Test the agent with a simplified version of the WebShop task to ensure that the basic functionality works as expected.
  2. Evaluate the agent's performance on the full WebShop task and compare it to the performance of previous methods.
  3. Conduct ablation studies to understand the importance of each component (state-space exploration, state-specific instructions, and function-calling functionality) in the agent's performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LASER's performance scale with increasing complexity of user instructions, beyond the WebShop task?
- Basis in paper: [inferred] The paper mentions that LASER is tested on the WebShop task and shows promising results, but does not explore its performance on more complex or varied instructions.
- Why unresolved: The paper focuses on a specific task and does not provide evidence of LASER's ability to handle more diverse or complex instructions.
- What evidence would resolve it: Testing LASER on a wider range of tasks with varying levels of instruction complexity, including real-world e-commerce websites, would provide evidence of its scalability and robustness.

### Open Question 2
- Question: What is the impact of the number and specificity of predefined states on LASER's performance in different domains?
- Basis in paper: [explicit] The paper mentions that LASER requires manual annotation of possible states in the environment, which might limit its applicability to specific domains.
- Why unresolved: The paper does not explore how varying the number and specificity of predefined states affects LASER's performance across different domains.
- What evidence would resolve it: Conducting experiments with LASER in multiple domains, using different numbers and specificities of predefined states, would help determine the optimal configuration for various applications.

### Open Question 3
- Question: How does LASER's performance compare to other state-of-the-art methods when applied to open-world web navigation tasks?
- Basis in paper: [inferred] The paper focuses on the WebShop task and does not compare LASER's performance to other methods on open-world web navigation tasks.
- Why unresolved: The paper does not provide evidence of LASER's effectiveness in more general, open-world web navigation scenarios.
- What evidence would resolve it: Evaluating LASER against other state-of-the-art methods on open-world web navigation benchmarks, such as WebArena, would provide a clearer comparison of its capabilities.

## Limitations
- Unknown generalizability beyond WebShop environment due to manual state annotation
- 24.2 percentage point gap remains between LASER performance and human performance
- Heavy dependency on function-calling functionality that may not be universally available

## Confidence

High confidence: The basic mechanism of using state-specific instructions to guide agent behavior is well-supported by the evidence. The improvement over baselines (50.0% vs 31.2% for ReAct) is substantial and clearly demonstrated.

Medium confidence: The claim that state-space exploration enables better error recovery is plausible but not thoroughly validated. The paper asserts this capability without providing systematic error recovery analysis.

Low confidence: The claim about scalability and generalizability beyond WebShop is unsupported. The manual state annotation process suggests limited applicability to other domains.

## Next Checks

1. Design experiments that intentionally introduce errors at various stages of task execution and measure LASER's ability to recover compared to baseline methods. Track the number of steps needed for recovery and success rates in error scenarios.

2. Systematically test LASER on edge cases and rare scenarios not covered by the pre-defined states. Measure performance degradation and identify gaps in state definitions that could be automated.

3. Implement LASER on a different web navigation task (e.g., flight booking or news article retrieval) using the same state-space approach. Compare performance to baseline methods and analyze how much state annotation is required for new domains.