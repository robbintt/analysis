---
ver: rpa2
title: Initialization-Dependent Sample Complexity of Linear Predictors and Neural
  Networks
arxiv_id: '2305.16475'
source_url: https://arxiv.org/abs/2305.16475
tags:
- complexity
- function
- some
- class
- norm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the sample complexity of learning linear predictors
  and neural networks when parameters are constrained by their distance from a reference
  point (initialization). It shows that for vector-valued linear predictors, the sample
  complexity is unexpectedly exponential in the norm and accuracy parameters, unlike
  scalar-valued predictors where it scales polynomially.
---

# Initialization-Dependent Sample Complexity of Linear Predictors and Neural Networks

## Quick Facts
- **arXiv ID:** 2305.16475
- **Source URL:** https://arxiv.org/abs/2305.16475
- **Reference count:** 40
- **Primary result:** Shows exponential sample complexity for vector-valued linear predictors with Frobenius norm constraints, unlike scalar-valued predictors.

## Executive Summary
This paper investigates how initialization affects the sample complexity of learning linear predictors and neural networks. The key finding is that vector-valued linear predictors exhibit unexpectedly exponential sample complexity when parameters are constrained by their distance from a reference point, contrasting with the polynomial scaling of scalar-valued predictors. The work reveals that with zero initialization, sample complexity is exp(Θ(L²B²/ε²)), and this bound holds even when the activation is not fixed but also Lipschitz. Crucially, with non-zero initialization, sample complexity becomes infinite even for very small reference norms, unlike the scalar case. These results lead to a new convex learning problem that is learnable without uniform convergence.

## Method Summary
The paper employs probabilistic methods and covering number techniques to establish lower bounds on sample complexity, particularly for the zero initialization case. For upper bounds, it uses Rademacher complexity analysis combined with Dudley integral covering number bounds. The key technical approach involves approximating weight matrices by low-rank matrices and bounding the complexity of Lipschitz functions composed with these linear maps. The analysis extends to neural networks of various depths, with smooth activations enabling polynomial bounds for one-hidden-layer networks, and Lipschitz activations with zero initialization yielding depth-dependent but width-independent bounds for deep networks.

## Key Results
- Vector-valued linear predictors have sample complexity exp(Θ(L²B²/ε²)) when constrained by Frobenius norm from zero initialization
- Non-zero initialization leads to infinite sample complexity even with very small reference norms
- Creates a new convex learning problem that is learnable without uniform convergence
- Establishes depth-dependent but width-independent sample complexity bounds for deep neural networks with Lipschitz activations and zero initialization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Vector-valued linear predictors have sample complexity exponential in B²/ε² with Frobenius norm constraints from zero initialization.
- **Mechanism:** The function class Ff,W0=0B,n,d contains functions x → f(Wx) where ||W||F ≤ B. The paper proves upper and lower bounds on Rademacher complexity showing it scales as exp(Θ(L²B²/ε²)). This is achieved by approximating any function in the class by a low-rank matrix W̃ε with rank at most B²/ε², then bounding the complexity of Lipschitz functions composed with linear maps to this low-dimensional space.
- **Core assumption:** Activation function f is L-Lipschitz and input domain is bounded (||x|| ≤ 1).
- **Evidence anchors:** Abstract states "sample complexity is unexpectedly exponential in the norm and accuracy parameters"; Theorem 2 proves Rademacher complexity bounds; related work lacks direct evidence of this exponential dependence.

### Mechanism 2
- **Claim:** With non-zero initialization (W0 ≠ 0), even very small spectral norms of W0 lead to infinite sample complexity.
- **Mechanism:** The paper constructs a function class Ff,W0B,n,d where W0 has spectral norm 2√2·ε, and proves it can shatter arbitrarily many points. The construction encodes both the index and target values into the matrix product Wyx, requiring exponentially many rows in the matrix to represent all possible labelings.
- **Core assumption:** The Lipschitz function f can be chosen adversarially to shatter points, and matrix dimensions n,d can be made sufficiently large.
- **Evidence anchors:** Abstract states "sample complexity becomes infinite even for very small reference norms"; Theorem 3 with construction showing shattering of m points with margin ε; no direct evidence of this phenomenon in other works.

### Mechanism 3
- **Claim:** Vector-valued linear predictors are learnable without uniform convergence, creating a new convex problem where SGD works but uniform convergence fails.
- **Mechanism:** The class Gℓ,W0B,n,d of matrix-valued linear predictors with convex Lipschitz losses is shown to be learnable via SGD, but the fat-shattering dimension is unbounded, proving uniform convergence does not hold. This is achieved by showing the same lower bound construction from Theorem 3 applies when f is convex.
- **Core assumption:** Loss function ℓ(.,y) is convex and L-Lipschitz for each y, and inputs have bounded norm.
- **Evidence anchors:** Abstract states "This leads to new sample complexity bounds... establishing a new convex linear prediction problem that is provably learnable without uniform convergence"; Theorem 5 shows construction works with convex functions; no direct evidence of similar convex problems without uniform convergence.

## Foundational Learning

- **Concept: Fat-shattering dimension**
  - Why needed here: Provides lower bounds on sample complexity and is used to prove that uniform convergence fails for the vector-valued predictor class
  - Quick check question: If a function class can shatter m points with margin ε, what is the minimum value of its fat-shattering dimension at scale ε?

- **Concept: Rademacher complexity**
  - Why needed here: Provides upper bounds on sample complexity and is used throughout the paper to prove size-independent bounds
  - Quick check question: How does the Rademacher complexity of a function class relate to the sample complexity required to achieve ε-accurate learning?

- **Concept: Covering numbers**
  - Why needed here: Used as a technical tool to bound the Rademacher complexity, particularly in the proof of Lemma 1 which bounds covering numbers of composed Lipschitz function classes
  - Quick check question: What is the relationship between covering numbers with respect to the empirical L² metric and the Rademacher complexity?

## Architecture Onboarding

- **Component map:** Input x → Parameter matrix W (||W - W₀||F ≤ B) → Reference matrix W₀ (||W₀|| ≤ B₀) → L-Lipschitz activation f → Output f(Wx)

- **Critical path:** 1) Initialize W near W₀ (often W₀ = 0) → 2) Apply matrix multiplication Wx → 3) Apply Lipschitz activation f → 4) Compute loss and gradients → 5) Update W via projected SGD

- **Design tradeoffs:**
  - Frobenius norm constraint vs. spectral norm constraint: Frobenius allows size-independent bounds but requires careful initialization analysis
  - Zero vs. non-zero initialization: Zero initialization permits finite sample complexity; non-zero initialization can lead to infinite complexity
  - Lipschitz vs. smooth activations: Smooth activations enable polynomial sample complexity bounds for one-hidden-layer networks

- **Failure signatures:**
  - Exponential sample complexity growth when B or 1/ε increase
  - Shattering of arbitrarily many points with non-zero W₀
  - SGD convergence without uniform convergence guarantees

- **First 3 experiments:**
  1. Verify the exponential sample complexity bound by computing Rademacher complexity for varying B and ε values
  2. Test the non-zero initialization lower bound by constructing the adversarial W₀ and verifying shattering
  3. Implement SGD on the convex loss problem and confirm learning occurs without uniform convergence

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can we prove a lower bound on the sample complexity of matrix-valued linear predictors that matches the upper bound, particularly in the case of non-zero initialization (W0 ≠ 0)?
- **Basis in paper:** [explicit] The paper establishes upper bounds on sample complexity but leaves open the question of matching lower bounds, especially for non-zero initialization.
- **Why unresolved:** The current lower bounds only apply to the zero initialization case (W0 = 0), and the paper explicitly states that non-zero initialization leads to infinite sample complexity even for very small reference norms.
- **What evidence would resolve it:** A proof showing that the exponential dependence on problem parameters (B, ε) is indeed necessary for non-zero initialization, or demonstrating that some weaker dependence exists.

### Open Question 2
- **Question:** Is the smoothness assumption on the activation function necessary for the size-independent Rademacher complexity bound of one-hidden-layer neural networks with element-wise activations?
- **Basis in paper:** [explicit] The paper proves a bound for smooth activations but leaves open whether this assumption is necessary.
- **Why unresolved:** The authors state they do not know if smoothness is necessary for the bound, suggesting the possibility of a more general result.
- **What evidence would resolve it:** A proof showing that the bound holds without the smoothness assumption, or a counterexample demonstrating that smoothness is indeed required.

### Open Question 3
- **Question:** Can we establish a fully size-independent sample complexity bound for deep neural networks with non-zero initialization and general Lipschitz activations?
- **Basis in paper:** [explicit] The paper only provides bounds for deep networks with zero initialization, leaving the non-zero initialization case as an open problem.
- **Why unresolved:** The authors explicitly state that extending the results to non-zero initialization for deep networks is an open question.
- **What evidence would resolve it:** A proof showing that a fully size-independent bound can be achieved with non-zero initialization, or demonstrating that such a bound is impossible due to inherent limitations.

## Limitations

- The exponential dependence on B²/ε² for vector-valued linear predictors may be an artifact of the Frobenius norm constraint rather than an intrinsic property of the learning problem
- Non-zero initialization results rely on constructing adversarial W₀ matrices that may not reflect practical initialization schemes used in deep learning
- The paper only establishes upper bounds for zero initialization cases, leaving open the question of matching lower bounds

## Confidence

- **High confidence:** Upper bounds (Theorems 2 and 4) for zero initialization cases
- **Medium confidence:** Lower bounds (Theorems 1 and 3) due to reliance on adversarial constructions
- **Medium confidence:** Uniform convergence failure result (Theorem 5) as it builds directly on the lower bound construction

## Next Checks

1. Verify the tightness of the exponential sample complexity bound by testing whether alternative norm constraints (e.g., spectral norm) yield polynomial bounds for the same problem class
2. Implement numerical experiments comparing SGD convergence rates with and without the non-zero initialization construction to empirically validate the shattering phenomenon
3. Extend the analysis to random initialization schemes commonly used in practice to determine if the pathological behavior persists or if practical initializations avoid these worst-case scenarios