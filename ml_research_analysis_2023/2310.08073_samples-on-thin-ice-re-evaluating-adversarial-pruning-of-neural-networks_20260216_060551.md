---
ver: rpa2
title: 'Samples on Thin Ice: Re-Evaluating Adversarial Pruning of Neural Networks'
arxiv_id: '2310.08073'
source_url: https://arxiv.org/abs/2310.08073
tags:
- samples
- robustness
- pruning
- adversarial
- pruned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Adversarial pruning aims to maintain model robustness after network
  compression, but recent methods may overestimate this robustness. This work re-evaluates
  three state-of-the-art adversarial pruning methods (ATMC, HYDRA, ADMM) using AutoAttack,
  revealing significant drops in robustness (up to 7.05%) compared to reported values,
  especially at high sparsity levels.
---

# Samples on Thin Ice: Re-Evaluating Adversarial Pruning of Neural Networks

## Quick Facts
- **arXiv ID:** 2310.08073
- **Source URL:** https://arxiv.org/abs/2310.08073
- **Reference count:** 23
- **Key outcome:** Adversarial pruning methods overestimate robustness, with up to 7.05% drops when evaluated with AutoAttack

## Executive Summary
Recent work has claimed that adversarial pruning methods can produce sparse networks while preserving robustness to adversarial examples. This work re-evaluates three state-of-the-art adversarial pruning methods (ATMC, HYDRA, ADMM) using AutoAttack, revealing significant drops in robustness compared to reported values, especially at high sparsity levels. A novel analysis identifies that samples misclassified after pruning—called "samples on thin ice"—are consistently located near the decision boundary of the unpruned model. Statistical tests confirm these samples are significantly closer to the boundary than correctly classified ones (AUC > 0.89, p < 1e-147). This insight suggests that future adversarial pruning methods could benefit from incorporating boundary-aware training strategies to improve robustness preservation.

## Method Summary
The authors re-evaluate three adversarial pruning methods (ATMC, HYDRA, ADMM) by applying pruning to achieve 90% and 95% sparsity on CIFAR-10 models. They evaluate robustness using AutoAttack (a comprehensive ensemble of attacks) rather than the weaker attacks used in original papers. The analysis examines sample-wise changes by computing distances to the decision boundary using the Fast-Minimum-Norm attack, comparing misclassified samples (S1,0) with correctly classified ones (S1,1). Statistical tests assess whether samples misclassified after pruning are significantly closer to the decision boundary than those correctly classified.

## Key Results
- AutoAttack evaluation reveals robustness drops of up to 7.05% compared to values reported in original papers
- Samples misclassified after pruning (S1,0) are consistently located closer to the decision boundary than correctly classified samples
- Statistical tests show high significance (AUC > 0.89, p < 1e-147) in the correlation between boundary proximity and misclassification
- The effect is most pronounced at higher sparsity levels (95% sparsity shows greater robustness degradation)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adversarial pruning methods overestimate robustness because they rely on weaker attacks for evaluation.
- **Mechanism:** The original papers used less comprehensive attack methods (e.g., PGD), which fail to expose vulnerabilities that stronger, ensemble-based attacks (AutoAttack) can reveal. This leads to inflated robustness metrics.
- **Core assumption:** AutoAttack provides a more rigorous and reliable evaluation of adversarial robustness compared to individual attack methods.
- **Evidence anchors:** [abstract] "Recent work has claimed that adversarial pruning methods can produce sparse networks while also preserving robustness to adversarial examples. In this work, we first re-evaluate three state-of-the-art adversarial pruning methods, showing that their robustness was indeed overestimated." [section] "The authors of [8] proposed a framework called AutoAttack, which ensemble various parameter-free attacks. This framework has become the minimal and adequate robustness test to foster a more reliable analysis."
- **Break condition:** If future evaluations consistently show no significant robustness drop when using AutoAttack, suggesting the overestimation claim was context-specific.

### Mechanism 2
- **Claim:** Samples misclassified after pruning ("samples on thin ice") are consistently located near the decision boundary of the unpruned model.
- **Mechanism:** Pruning alters the decision boundary, causing samples near the original boundary to shift across it, resulting in misclassification. The proximity to the boundary makes these samples more sensitive to structural changes introduced by pruning.
- **Core assumption:** The decision boundary shift caused by pruning is significant enough to affect samples near the original boundary.
- **Evidence anchors:** [abstract] "We then compare pruned and dense versions of the same models, discovering that samples on thin ice, i.e., closer to the unpruned model's decision boundary, are typically misclassified after pruning." [section] "Interestingly, as Figure 1 shows, we observe that samples lying in the proximity of the decision boundary (samples on thin ice) are more likely to be missed (or occasionally corrected) by the pruned model with respect to the dense, hence more likely to be S1,0 (or occasionally S0,1) samples."
- **Break condition:** If statistical analysis fails to show a significant correlation between boundary proximity and misclassification after pruning.

### Mechanism 3
- **Claim:** Statistical tests confirm that samples misclassified after pruning are significantly closer to the decision boundary than correctly classified ones.
- **Mechanism:** By measuring the distance to the decision boundary (epsilon) for each sample, the analysis demonstrates that misclassified samples (S1,0) have significantly lower epsilon values compared to correctly classified samples (S1,1), with high statistical significance (AUC > 0.89, p < 1e-147).
- **Core assumption:** The epsilon metric accurately captures the distance to the decision boundary and correlates with classification stability.
- **Evidence anchors:** [abstract] "Statistical tests confirm these samples are significantly closer to the boundary than correctly classified ones (AUC > 0.89, p < 1e-147)." [section] "Table 2 provides a meticulous empirical display of the statistics related to the samples and their ϵ value... The results for both AUC and p-value suggest a very high statistical significance of the distance to the boundary ϵ with respect to the two S1,p populations."
- **Break condition:** If the statistical correlation between epsilon and misclassification disappears under different pruning methods or datasets.

## Foundational Learning

- **Concept:** Adversarial examples and their impact on neural network robustness.
  - **Why needed here:** Understanding how adversarial examples work is crucial for grasping why pruning affects robustness and how to evaluate it properly.
  - **Quick check question:** What is the difference between clean accuracy and robust accuracy in the context of adversarial attacks?

- **Concept:** Neural network pruning techniques and their effects on model performance.
  - **Why needed here:** Knowledge of pruning methods helps understand how structural changes can impact both accuracy and robustness.
  - **Quick check question:** How does weight pruning differ from filter pruning in terms of impact on network structure?

- **Concept:** Statistical significance testing and its application in machine learning research.
  - **Why needed here:** Understanding statistical tests is essential for interpreting the results showing the correlation between boundary proximity and misclassification.
  - **Quick check question:** What does an AUC value of 0.89 indicate about the separability of two distributions?

## Architecture Onboarding

- **Component map:** CIFAR-10 dataset -> Dense models (VGG, ResNet, WideResNet) -> Adversarial training -> Pruning methods (ATMC, HYDRA, ADMM) -> AutoAttack evaluation -> Fast-Minimum-Norm boundary analysis -> Statistical testing

- **Critical path:**
  1. Load and preprocess CIFAR-10 dataset
  2. Initialize dense models and apply adversarial training
  3. Prune models using selected methods to target sparsity
  4. Fine-tune pruned models
  5. Evaluate robustness using AutoAttack
  6. Analyze sample-wise changes using Fast-Minimum-Norm attack
  7. Perform statistical tests on boundary proximity

- **Design tradeoffs:**
  - Computational cost vs. evaluation rigor (AutoAttack vs. single attacks)
  - Sample size for analysis (1k vs. full test set)
  - Model complexity vs. pruning effectiveness
  - Training time vs. robustness preservation

- **Failure signatures:**
  - Robustness drop exceeding reported values
  - Statistical tests failing to show significant correlation between boundary proximity and misclassification
  - Inconsistency between clean accuracy and robust accuracy trends
  - High variance in results across different sparsity levels

- **First 3 experiments:**
  1. Replicate robustness evaluation using AutoAttack on a single model-architecture-method combination
  2. Visualize decision boundary changes for a small subset of samples
  3. Perform initial statistical analysis on boundary proximity for misclassified vs. correctly classified samples

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can adversarial pruning methods be designed to preserve robustness while achieving high sparsity levels?
- **Basis in paper:** [explicit] The paper identifies that samples misclassified after pruning are consistently located near the decision boundary of the unpruned model, suggesting a need for boundary-aware training strategies.
- **Why unresolved:** The paper discusses the correlation between misclassified samples and their proximity to the decision boundary but does not provide a concrete method to incorporate this insight into adversarial pruning.
- **What evidence would resolve it:** Development and empirical validation of a pruning method that explicitly considers the distance to the decision boundary during training to maintain robustness at high sparsity levels.

### Open Question 2
- **Question:** What are the limitations of current adversarial pruning methods in maintaining model robustness, and how can these be quantified more accurately?
- **Basis in paper:** [explicit] The paper re-evaluates state-of-the-art adversarial pruning methods using AutoAttack, revealing significant drops in robustness compared to reported values, especially at high sparsity levels.
- **Why unresolved:** The paper highlights the overestimation of robustness in current methods but does not explore the underlying reasons for these limitations or propose a framework for more accurate quantification.
- **What evidence would resolve it:** A comprehensive analysis identifying the specific factors contributing to robustness degradation and a new evaluation framework that accurately measures these factors.

### Open Question 3
- **Question:** Can the insights from the sample-wise analysis of pruned models lead to more effective training procedures for adversarial pruning?
- **Basis in paper:** [explicit] The paper suggests that a training procedure weighting samples based on their distance to the boundary might uplift the performance of pruned models.
- **Why unresolved:** While the paper proposes the idea of boundary-aware training, it does not provide a detailed methodology or experimental results to support this approach.
- **What evidence would resolve it:** Implementation and testing of a training procedure that incorporates sample distances to the decision boundary, along with empirical results demonstrating improved robustness in pruned models.

## Limitations
- Analysis relies on a subset of 1,000 samples for statistical testing, which may not fully capture population-level trends
- Decision boundary proximity analysis assumes the Fast-Minimum-Norm attack accurately captures boundary distances, though this metric's reliability for pruned networks remains untested
- Study focuses on three specific pruning methods and CIFAR-10, limiting generalizability to other architectures or datasets

## Confidence

- **High:** The claim that original papers overestimated robustness (supported by direct comparison with AutoAttack)
- **Medium:** The statistical significance of boundary proximity correlation (based on limited sample size)
- **Medium:** The mechanism linking boundary proximity to misclassification (inferred from correlation rather than causal proof)

## Next Checks
1. Replicate findings using the full test set to verify statistical conclusions are not artifacts of the 1,000-sample subset
2. Test additional pruning methods and architectures to assess generalizability of the boundary proximity mechanism
3. Implement and evaluate the proposed boundary-aware pruning strategy to determine if it improves robustness preservation