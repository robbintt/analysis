---
ver: rpa2
title: 'CL-MASR: A Continual Learning Benchmark for Multilingual ASR'
arxiv_id: '2310.16931'
source_url: https://arxiv.org/abs/2310.16931
tags:
- learning
- languages
- base
- language
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CL-MASR is the first continual learning benchmark for multilingual
  ASR, designed to study how large-scale multilingual ASR models like Whisper can
  be incrementally trained on new languages without catastrophic forgetting. It provides
  a curated dataset of medium/low-resource languages, implementations of diverse continual
  learning methods (rehearsal-based, architecture-based, and regularization-based),
  and standard evaluation metrics including average word error rate, backward transfer,
  intransigence measure, and forward transfer.
---

# CL-MASR: A Continual Learning Benchmark for Multilingual ASR

## Quick Facts
- arXiv ID: 2310.16931
- Source URL: https://arxiv.org/abs/2310.16931
- Authors: 
- Reference count: 40
- Primary result: First continual learning benchmark for multilingual ASR showing experience replay effectively mitigates catastrophic forgetting

## Executive Summary
CL-MASR introduces the first comprehensive benchmark for continual learning in multilingual automatic speech recognition. The benchmark addresses catastrophic forgetting when incrementally training large-scale multilingual ASR models like Whisper on new languages. It provides a curated dataset of medium/low-resource languages from Common Voice 13, implementations of diverse continual learning methods (rehearsal-based, architecture-based, and regularization-based), and standard evaluation metrics including average word error rate, backward transfer, intransigence measure, and forward transfer. Experiments demonstrate that experience replay is among the most effective strategies for balancing stability (retaining old knowledge) and plasticity (learning new languages).

## Method Summary
CL-MASR uses Common Voice 13 with 10 base languages and 10 new languages, each providing up to 10 hours of training data. The benchmark employs large pretrained models (Whisper large-v2 and WavLM large) as base models and implements 10 different continual learning methods including fine-tuning, experience replay, A-GEM, DER, progressive neural networks, parallel branches, L2P, EWC, LwF, and MAS. Training uses AdamW optimizer with learning rate 0.0001, gradient clipping, and automatic mixed precision. The framework is built on SpeechBrain for modularity and reproducibility.

## Key Results
- Experience replay is among the most effective continual learning strategies for multilingual ASR
- Architecture-based methods like progressive neural networks provide stability but poor plasticity for new languages
- Catastrophic forgetting is a significant issue for large-scale multilingual ASR models like Whisper
- Task-agnostic methods are more robust but less parameter-efficient than task-specific approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Experience replay mitigates catastrophic forgetting by reintroducing samples from previous tasks during training on new languages
- Mechanism: ER maintains a replay buffer containing 10% of data from each previously learned language. When training on a new language, ER mixes current task data with randomly sampled data from the buffer, ensuring the model adjusts for new languages while maintaining performance on older ones
- Core assumption: The replay buffer retains a representative sample of previous task distributions, allowing the model to preserve relevant feature representations
- Evidence anchors:
  - [abstract]: "Experiments show that experience replay is among the most effective continual learning strategies for this task, effectively mitigating forgetting while maintaining adaptability to new languages"
  - [section]: "ER can be implemented at either the batch level or the dataset level. In our experiments, we found the latter approach to be more effective"
  - [corpus]: Found 25 related papers; several mention experience replay as a baseline in multilingual ASR continual learning, supporting its effectiveness

### Mechanism 2
- Claim: Progressive neural networks avoid catastrophic forgetting by allocating task-specific sub-networks while allowing knowledge transfer through lateral connections
- Mechanism: PNN adds a new sub-network for each new language, freezing parameters of previous sub-networks. Lateral adaptor connections enable the new sub-network to leverage knowledge from earlier ones, facilitating positive backward transfer without interfering with previous tasks
- Core assumption: The base model's shared representations are sufficiently rich to support task-specific adaptations without requiring updates to earlier sub-networks
- Evidence anchors:
  - [abstract]: "CL-MASR provides a diverse set of continual learning methods implemented on top of large-scale pretrained ASR models"
  - [section]: "PNN requires the task identity to be provided at inference time. In our experiments, we extend Whisper by adding a final task-specific transformer decoder layer and a corresponding embedding layer for each language"
  - [corpus]: Some neighbor papers discuss adapter-based approaches for language expansion, conceptually similar to PNN's lateral connections

### Mechanism 3
- Claim: Regularization-based methods like elastic weight consolidation prevent forgetting by penalizing changes to parameters important for previous tasks
- Mechanism: EWC computes the Fisher information matrix to estimate parameter importance for past tasks. During training on new languages, a penalty term is added to the loss, discouraging updates to crucial parameters to maintain performance on previous languages while allowing adaptation to new ones
- Core assumption: The FIM accurately reflects parameter importance, and the penalty strength is properly tuned to balance learning new tasks and preserving old ones
- Evidence anchors:
  - [abstract]: "CL-MASR provides a diverse set of continual learning methods implemented on top of large-scale pretrained ASR models, along with common metrics to assess the effectiveness of learning new languages while addressing the issue of catastrophic forgetting"
  - [section]: "EWC estimates the importance of each parameter based on its contribution to the performance on previous tasks... It does so by computing the Fisher information matrix (FIM)"
  - [corpus]: Neighbor papers mention regularization-based methods for continual learning in ASR, indicating relevance but limited specific evidence for EWC's effectiveness in multilingual settings

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: Understanding why models lose performance on previous tasks when trained on new ones is essential to grasp the problem CL-MASR addresses
  - Quick check question: What happens to a model's performance on task A when it is fine-tuned on task B without any CL intervention?

- Concept: Continual learning (CL) paradigms
  - Why needed here: CL-MASR evaluates different CL strategies (rehearsal, architecture-based, regularization-based). Knowing these categories helps understand the experimental setup
  - Quick check question: How do rehearsal-based methods differ from architecture-based methods in their approach to mitigating forgetting?

- Concept: Multilingual ASR and token spaces
  - Why needed here: The benchmark uses models like Whisper with shared token spaces across languages. Understanding how tokenization affects learning is crucial for interpreting results
  - Quick check question: Why might a shared byte-level BPE token space in Whisper lead to more susceptibility to catastrophic forgetting compared to character-level tokenization?

## Architecture Onboarding

- Component map: Base model (Whisper/WavLM) -> Continual learning method -> Evaluation metrics (AWER, BWT, IM, FWT) -> CL-MASR platform
- Critical path: 1) Pretrain base model on base languages. 2) Incrementally fine-tune on new languages using CL methods. 3) Evaluate using AWER, BWT, IM, FWT after each incremental step
- Design tradeoffs: Using large pretrained models provides strong initial performance but increases computational cost. Task-agnostic methods are more robust but may be less parameter-efficient than task-specific ones
- Failure signatures: AWER remaining high (>100%) after incremental training indicates severe forgetting. Large negative BWT values show forgetting of base languages. High IM values suggest poor adaptability to new languages
- First 3 experiments:
  1. Run naive fine-tuning (FT) on Whisper to establish baseline AWER and observe catastrophic forgetting
  2. Implement experience replay (ER) with Whisper to compare AWER and BWT against FT
  3. Test progressive neural networks (PNN) with Whisper to evaluate parameter efficiency and task-specific performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific reasons why large-scale pretrained multilingual ASR models like Whisper are more susceptible to catastrophic forgetting compared to other types of models in continual learning settings?
- Basis in paper: [explicit] The paper mentions that Whisper is more susceptible to catastrophic forgetting due to its token space being shared among all languages and its multi-task training format requiring new token embeddings for each new language
- Why unresolved: While the paper identifies potential factors contributing to Whisper's susceptibility to forgetting, it does not provide a detailed analysis or experimental evidence to quantify the relative impact of these factors or compare them with other models
- What evidence would resolve it: Systematic ablation studies varying model architectures, tokenization schemes, and training strategies while measuring forgetting across different CL methods would provide insights into the specific reasons for Whisper's increased susceptibility

### Open Question 2
- Question: How does the ordering of languages in the continual learning sequence affect the overall performance and stability of multilingual ASR models, and what are the underlying mechanisms driving these effects?
- Basis in paper: [explicit] The paper discusses the impact of language ordering on performance, showing that experience replay can mitigate forgetting and reduce ordering sensitivity, but also notes that ordering still introduces substantial variance in forward transfer and intransigence measures
- Why unresolved: The paper provides empirical observations of ordering effects but does not delve into the theoretical reasons why certain orderings might be more beneficial or detrimental, nor does it propose methods to optimize language ordering for better CL performance
- What evidence would resolve it: Controlled experiments varying language ordering while analyzing the relationship between ordering, model performance, and underlying linguistic similarities would help understand the mechanisms. Additionally, developing methods to predict optimal orderings based on language characteristics would be valuable

### Open Question 3
- Question: What are the trade-offs between stability (preventing forgetting) and plasticity (learning new tasks) in multilingual ASR continual learning, and how can these trade-offs be optimally balanced for different use cases?
- Basis in paper: [explicit] The paper discusses the stability-plasticity dilemma and shows that different CL methods achieve different balances, with architecture-based methods being stable but having poor plasticity, while FT shows good plasticity but poor stability
- Why unresolved: While the paper demonstrates the existence of this trade-off and provides examples of different methods' behaviors, it does not offer a framework for quantifying or optimizing this balance based on specific application requirements or propose methods to dynamically adjust the balance during training
- What evidence would resolve it: Developing metrics that capture both stability and plasticity in a unified framework, along with methods to tune CL algorithms based on these metrics, would help address this question. Additionally, user studies evaluating the practical impact of different stability-plasticity balances in real-world applications would provide valuable insights

## Limitations
- Benchmark evaluation is limited to 20 languages (10 base + 10 new), restricting generalizability to other language families
- Maximum 10-hour training data per language may not capture full complexity of medium-resource languages
- Replay buffer size (10%) is a hyperparameter that wasn't systematically optimized
- Focus on two large pretrained models (Whisper and WavLM) may not transfer to smaller or differently-architected ASR systems

## Confidence
- **High Confidence**: Benchmark implementation details are well-specified, including dataset preprocessing, model architectures, training procedures, and evaluation metrics
- **Medium Confidence**: Claims about experience replay being "among the most effective" CL strategies are supported by reported results but lack extensive comparison with all possible CL methods
- **Low Confidence**: Claims about broader applicability to other multilingual ASR scenarios or larger-scale language expansion tasks are not empirically validated beyond the 20-language scope

## Next Checks
1. **Language Family Generalization Test**: Validate whether reported CL method effectiveness transfers to languages from different language families (e.g., adding African or Southeast Asian languages to existing European and Asian base languages) to assess robustness across linguistic diversity
2. **Data Volume Sensitivity Analysis**: Systematically vary training data per language (from 5 to 50 hours) to determine minimum effective data requirement for each CL method, particularly experience replay, and identify when methods fail or become unnecessary
3. **Replay Buffer Optimization Study**: Conduct ablation study on replay buffer size (testing 1%, 5%, 10%, 20% of previous data) and sampling strategies (random vs. importance-based sampling) to determine optimal configuration for minimizing forgetting while maintaining efficiency