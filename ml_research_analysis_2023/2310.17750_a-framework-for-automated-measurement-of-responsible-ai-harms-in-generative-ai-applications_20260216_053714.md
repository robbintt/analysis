---
ver: rpa2
title: A Framework for Automated Measurement of Responsible AI Harms in Generative
  AI Applications
arxiv_id: '2310.17750'
source_url: https://arxiv.org/abs/2310.17750
tags:
- measurement
- framework
- harm
- llms
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an automated framework for measuring responsible
  AI harms in generative AI applications, specifically focusing on large language
  models (LLMs). The core idea is to use LLMs to simulate user interactions and evaluate
  the outputs of target LLMs against harm definitions crafted by domain experts.
---

# A Framework for Automated Measurement of Responsible AI Harms in Generative AI Applications

## Quick Facts
- arXiv ID: 2310.17750
- Source URL: https://arxiv.org/abs/2310.17750
- Reference count: 4
- Primary result: 60% exact agreement between human and model annotations for groundedness, with relative defect rates identified across three LLMs

## Executive Summary
This paper introduces an automated framework for measuring responsible AI harms in generative AI applications, specifically focusing on large language models (LLMs). The core idea is to use LLMs to simulate user interactions and evaluate the outputs of target LLMs against harm definitions crafted by domain experts. The framework comprises two main components: data generation using templates and parameters to simulate real-world scenarios, and evaluation using GPT-4 to assess outputs based on annotation guidelines. The authors demonstrate the framework's utility through case studies on groundedness and comparisons across three LLMs in areas like harmful content generation, IP leakage, and jailbreak susceptibility. They report a 60% exact agreement between human and model annotations for groundedness, and identify relative defect rates across models, with Model 3 showing the lowest rates for harmful content generation. The work highlights both the potential and limitations of automated harm measurement, including concerns about using LLMs to evaluate other LLMs and the need for carefully constructed measurement resources.

## Method Summary
The framework uses a two-component architecture for automated measurement of responsible AI harms. First, data generation simulates user interactions with the target LLM using parameterized templates that create realistic conversational scenarios. Second, evaluation uses GPT-4 to annotate LLM outputs based on expert-crafted harm definitions and annotation guidelines. The framework produces relative defect rates across different LLMs for various harm categories. The authors validate their approach through case studies on groundedness (measuring 60% human-LLM agreement) and comparative analysis of three LLMs across harmful content generation, IP leakage, and jailbreak susceptibility.

## Key Results
- 60% exact agreement between human and GPT-4 annotations for groundedness evaluation
- Model 3 showed the lowest rates for harmful content generation among three tested LLMs
- Models 2 and 3 displayed lower rates of generating IP-protected content and exposing underlying guidelines than Model 1

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can be used to simulate user interactions with target LLMs in a scalable, automated way.
- Mechanism: The framework employs a user-simulating LLM (LLM_user) that follows parameterized templates to generate realistic conversational inputs for the target LLM (LLM_test). This creates a scalable way to generate diverse test cases without manual effort.
- Core assumption: LLM_user can generate sufficiently realistic and diverse user inputs that reflect real-world usage patterns.
- Evidence anchors:
  - [abstract] "The data generation component uses templates and parameters to simulate interactions with the LLM under test"
  - [section] "We provide a set of templates, referred to as persona templates, which provide guidelines for the LLM_user regarding how to behave and which topics or goals to introduce"
  - [corpus] No direct corpus evidence on template realism
- Break condition: If LLM_user fails to generate realistic interactions, the ecological validity of generated data decreases significantly.

### Mechanism 2
- Claim: GPT-4 can reliably annotate LLM outputs for responsible AI harms using expert-crafted guidelines.
- Mechanism: The evaluation component uses GPT-4 with annotation guidelines (harm definitions, examples, defect definitions) to score LLM outputs on harm dimensions. The LLM annotator produces both numerical scores and reasoning text.
- Core assumption: GPT-4 can consistently interpret and apply complex harm definitions to produce reliable annotations.
- Evidence anchors:
  - [abstract] "evaluation using GPT-4 to assess outputs based on annotation guidelines"
  - [section] "The LLM annotator (GPT-4) is then provided with the original question posed by LLM_user, the response from LLM_test, and the context given to LLM_test"
  - [section] "Our preliminary analysis revealed an exact agreement ratio of 60% and a relaxed agreement ratio of 80.5% between human and GPT-4 annotations"
- Break condition: If GPT-4's annotations diverge significantly from human judgments, the reliability of automated harm measurement decreases.

### Mechanism 3
- Claim: The framework enables relative performance comparison across different LLMs for specific harm categories.
- Mechanism: By applying identical measurement resources (templates, parameters, annotation guidelines) to multiple LLMs, the framework produces comparable defect rates that reveal relative strengths and weaknesses.
- Core assumption: Identical measurement resources applied to different LLMs yield comparable and meaningful results.
- Evidence anchors:
  - [abstract] "identify relative defect rates across models, with Model 3 showing the lowest rates for harmful content generation"
  - [section] "When using identical measurement sets to test two AI systems, the resulting measurements can be used to compare the relative performance"
  - [section] "Models 2 and 3 display lower rates of generating IP-protected content and exposing underlying guidelines than Model 1"
- Break condition: If measurement resources are not truly comparable or valid across LLMs, relative comparisons become misleading.

## Foundational Learning

- Concept: Harm definition and sociotechnical measurement design
  - Why needed here: The framework relies on expert-crafted measurement resources that define what constitutes harm and how to detect it. Understanding how to construct valid, reliable harm definitions is critical for meaningful measurements.
  - Quick check question: What are the key sociotechnical considerations when defining a harm for automated measurement?

- Concept: LLM evaluation methodology and annotation guidelines
  - Why needed here: The framework uses LLMs to evaluate other LLMs, requiring understanding of how to construct effective annotation guidelines and interpret LLM annotations.
  - Quick check question: How do you design annotation guidelines that balance specificity with generalizability for LLM evaluation?

- Concept: Statistical interpretation of relative measurements
  - Why needed here: The framework produces relative defect rates, not absolute harm probabilities. Understanding the difference is crucial for correct interpretation.
  - Quick check question: Why can't a 0% defect rate be interpreted as "no harm possible" in real-world deployment?

## Architecture Onboarding

- Component map: Templates + Parameters → LLM_user → Simulated Conversations → Dataset → GPT-4 → Defect Scores → Defect Rates

- Critical path:
  1. Domain experts create measurement resources (harm definitions, templates, parameters, annotation guidelines)
  2. Data generation pipeline creates simulated user-LLM interactions
  3. Evaluation pipeline uses GPT-4 to annotate outputs
  4. Aggregate annotations to calculate defect rates
  5. Compare defect rates across LLMs

- Design tradeoffs:
  - Automation vs. accuracy: Using LLMs for evaluation is scalable but may sacrifice some accuracy compared to human annotation
  - Template specificity vs. coverage: More specific templates may yield better measurements but cover fewer scenarios
  - Computation cost vs. stability: Running multiple annotation passes improves stability but increases cost

- Failure signatures:
  - Low human-LLM agreement on annotations suggests annotation guidelines need refinement
  - Similar defect rates across very different LLMs may indicate measurement resources are not discriminating enough
  - Extremely high or low defect rates may indicate template or parameter issues

- First 3 experiments:
  1. Implement the groundedness case study to verify the basic framework works end-to-end
  2. Test human-LLM agreement on a small dataset to validate annotation quality
  3. Run the same measurement resources on two different LLMs to verify relative comparison capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively mitigate the risks of using LLMs to evaluate other LLMs, especially considering the potential for bias and under-annotation of specific harm types?
- Basis in paper: [explicit] The paper discusses the limitations of using LLMs to measure harms from other LLMs, highlighting concerns about ecological validity, representation of marginalized groups, and potential for under-annotation due to LLM tendencies.
- Why unresolved: The paper acknowledges this as an open research problem and suggests that the evaluation component of the framework should be flexible enough to plug in other evaluation methods. However, it does not provide specific solutions or strategies to address these risks.
- What evidence would resolve it: Research demonstrating effective methods for mitigating LLM bias in evaluation, or empirical studies showing improved agreement and reliability when using alternative evaluation approaches.

### Open Question 2
- Question: What are the best practices for creating reliable and valid measurement resources for different harm areas, and how can we ensure their quality and consistency across diverse sociotechnical contexts?
- Basis in paper: [explicit] The paper emphasizes the importance of carefully-constructed measurement resources but acknowledges that the process of creating them is a complex sociotechnical problem fraught with pitfalls. It mentions engaging with domain experts but does not delve into specific methodologies or validation techniques.
- Why unresolved: The paper recognizes the significance of this challenge but leaves it as future work to examine the practices involved in creating reliable and valid measurement resources. It does not provide concrete guidelines or frameworks for this process.
- What evidence would resolve it: Studies comparing different approaches to creating measurement resources, or frameworks that establish best practices and quality metrics for sociotechnical harm measurement.

### Open Question 3
- Question: How can we address the computational and environmental costs associated with the automated harm measurement framework, and what are the trade-offs between accuracy, efficiency, and sustainability?
- Basis in paper: [explicit] The paper acknowledges the high computational resources required for the framework, mentioning the need for multiple invocations of large models and the associated costs. It also raises concerns about the environmental effects of widespread adoption.
- Why unresolved: While the paper recognizes these limitations, it does not propose specific strategies for optimizing the framework's resource usage or mitigating its environmental impact. It leaves this as an open question for future exploration.
- What evidence would resolve it: Research on optimizing LLM inference for harm measurement tasks, or studies quantifying the environmental impact of automated harm measurement frameworks and proposing sustainable alternatives.

## Limitations

- 60% exact agreement between human and GPT-4 annotations for groundedness represents a substantial gap that could propagate through all measurements
- Framework's reliance on LLMs to evaluate other LLMs creates potential circular reasoning concerns
- Framework currently requires substantial domain expertise to craft measurement resources, limiting accessibility

## Confidence

- **High confidence**: The framework's two-component architecture (data generation + evaluation) is technically sound and the basic implementation approach is clear and reproducible.
- **Medium confidence**: The relative comparison results across LLMs are meaningful, though the absolute interpretation of defect rates remains challenging.
- **Low confidence**: The claim that automated measurement can fully replace human evaluation for responsible AI harms - the 60% agreement rate suggests significant limitations in current LLM evaluation capabilities.

## Next Checks

1. **External validity test**: Apply the framework to a held-out LLM not used in initial development to verify measurement resources generalize beyond training scenarios.

2. **Human evaluation scaling**: Conduct human evaluations on a larger sample (minimum 500 examples across all harm categories) to establish whether GPT-4's 60% agreement for groundedness is representative or an outlier.

3. **Cross-cultural harm detection**: Test the framework's measurement resources across culturally diverse prompt templates to identify potential Western-centric bias in harm definitions and evaluation criteria.