---
ver: rpa2
title: Open Sesame! Universal Black Box Jailbreaking of Large Language Models
arxiv_id: '2309.01446'
source_url: https://arxiv.org/abs/2309.01446
tags:
- adversarial
- attack
- arxiv
- output
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel automated approach for black-box
  jailbreaking of large language models (LLMs) using a genetic algorithm (GA) to generate
  universal adversarial prompts. The method optimizes a suffix of tokens that, when
  appended to user queries, consistently disrupts the model's alignment, causing it
  to produce unintended and potentially harmful outputs.
---

# Open Sesame! Universal Black Box Jailbreaking of Large Language Models

## Quick Facts
- arXiv ID: 2309.01446
- Source URL: https://arxiv.org/abs/2309.01446
- Reference count: 0
- Primary result: Novel automated approach for black-box jailbreaking of LLMs using genetic algorithm to generate universal adversarial prompts with up to 98% attack success rate

## Executive Summary
This paper introduces a novel automated approach for black-box jailbreaking of large language models (LLMs) using a genetic algorithm (GA) to generate universal adversarial prompts. The method optimizes a suffix of tokens that, when appended to user queries, consistently disrupts the model's alignment, causing it to produce unintended and potentially harmful outputs. Experiments on models like LLaMA2-7b and Falcon-7b demonstrate high success rates, with attack success rates (ASR) reaching up to 98% for certain configurations. The approach does not require access to model internals, making it practical for real-world scenarios. The findings highlight vulnerabilities in LLMs and underscore the need for improved alignment techniques and robust defenses against adversarial attacks.

## Method Summary
The approach employs a genetic algorithm to evolve token suffixes that, when appended to user queries, maximize attack success rates. The GA optimizes these suffixes by maximizing cosine similarity between model outputs and a target phrase ("Sure, here is a...") while minimizing similarity to refusal responses. The method uses tournament selection with elitism, random subset sampling for fitness approximation, and evaluates effectiveness on a dataset of harmful behaviors. The attack is tested on LLaMA2-7b and Falcon-7b models, demonstrating high success rates without requiring access to model internals.

## Key Results
- Attack success rates (ASR) reaching up to 98% for certain configurations
- Universal adversarial prompts discovered that work across different user queries
- Black-box approach effective without requiring model gradients or internals
- GA approach successfully generates prompts that disrupt model alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Genetic algorithm optimization of a universal suffix causes semantic drift in LLM responses.
- Mechanism: The GA evolves a token suffix that, when appended to queries, maximizes cosine similarity between the model's output and a target phrase ("Sure, here is a...") while minimizing similarity to refusal responses.
- Core assumption: The LLM's output embeddings are sensitive to suffix-induced perturbations and that semantic proximity in embedding space correlates with output content.
- Evidence anchors: [abstract] "The GA attack works by optimizing a universal adversarial prompt that‚Äîwhen combined with a user's query‚Äîdisrupts the attacked model's alignment"; [section] "we contextualized the LLM to answer with 'Sure, here is a...' if it knows the answer and 'No, I can't...' otherwise"

### Mechanism 2
- Claim: Tournament selection with elitism preserves high-fitness suffixes across generations.
- Mechanism: The GA selects fitter individuals via tournament selection and retains the top Œª% (Œª = N/5) each generation, ensuring the population does not lose high-performing suffixes.
- Core assumption: The fitness landscape is smooth enough that small genetic operators (crossover, mutation) can refine high-fitness suffixes without catastrophic degradation.
- Evidence anchors: [section] "We used tournament selection [38] with K = 2... Herein we chose the elitism value as a function of population size N: Œª = N/5"; [section] "Elitism is a strategy commonly used in GAs... to preserve the best-performing individuals throughout the generations"

### Mechanism 3
- Claim: Random subset sampling of the dataset approximates full-dataset fitness without incurring high computational cost.
- Mechanism: In each GA iteration, fitness is evaluated on a random subset of size F from the dataset, approximating the expected fitness over the full dataset.
- Core assumption: The dataset is sufficiently homogeneous that random subsets are representative of full-dataset behavior.
- Evidence anchors: [section] "we perform fitness approximation [36, 37]: random subset sampling from the dataset, which allows us to estimate the effectiveness of the universal attack without the need for exhaustive evaluation"; [section] "random subset sampling is performed anew in each iteration, ensuring that the optimization process is guided by diverse and representative inputs"

## Foundational Learning

- Concept: Genetic algorithms and evolutionary computation
  - Why needed here: The attack relies on evolving token suffixes via GA operators (selection, crossover, mutation) to find adversarial prompts.
  - Quick check question: How does tournament selection differ from roulette wheel selection, and why might it be preferable for this application?

- Concept: Cosine similarity and embedding spaces
  - Why needed here: Fitness is computed as the negative cosine similarity between model outputs and target embeddings; understanding this metric is critical to interpreting results.
  - Quick check question: What does a cosine similarity of 0.9 versus 0.1 imply about semantic alignment between two embeddings?

- Concept: Black-box adversarial attack methodology
  - Why needed here: The attack does not use gradients or model internals; instead it relies on query outputs, making it distinct from white-box approaches.
  - Quick check question: Why is a black-box approach more practical in real-world deployments compared to white-box attacks?

## Architecture Onboarding

- Component map: Population (token suffixes) -> Fitness evaluator (cosine similarity over LLM outputs) -> Dataset split (train/test for harmful behaviors) -> Embedder (bge-large-en) -> Tournament selector -> Crossover/mutation operators -> Elitism preservation
- Critical path: Initialize population ‚Üí evaluate fitness on random subset ‚Üí select parents ‚Üí crossover/mutation ‚Üí elitism ‚Üí repeat until convergence
- Design tradeoffs: Longer prompt suffixes yield higher attack success but increase query cost; smaller population sizes reduce runtime but may hurt exploration; random subset sampling speeds up evaluation but risks bias
- Failure signatures: Stagnant fitness across generations, low ASR on test set despite high train set fitness, embeddings failing to distinguish target from refusal outputs
- First 3 experiments:
  1. Run GA with N=10, M=20 on LLaMA2-7b; record fitness and ASR per generation
  2. Repeat with N=20, M=40; compare convergence speed and final ASR
  3. Vary random subset size F (e.g., 10, 20, 30) and measure impact on runtime vs. ASR

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the success rate of the black-box jailbreaking attack scale with different population sizes and prompt lengths?
- Basis in paper: [explicit] Table 1 presents results showing varying attack success rates (ASR) for different population sizes (ùí©) and prompt lengths (‚Ñ≥).
- Why unresolved: The paper only tested a limited range of population sizes (10 and 20) and prompt lengths (20, 40, and 60), leaving uncertainty about optimal parameter ranges for maximizing attack success.
- What evidence would resolve it: Systematic experiments testing a wider range of population sizes (e.g., 5, 15, 50) and prompt lengths (e.g., 10, 30, 50, 70) to determine the relationship between these parameters and attack success rates.

### Open Question 2
- Question: How generalizable is the black-box jailbreaking approach across different LLM architectures and alignment techniques?
- Basis in paper: [explicit] The paper tested the approach on LLaMA2-7b and Falcon-7b, but notes limitations in testing on additional LLMs like Vicuna and Guanaco.
- Why unresolved: The experiments were limited to two models with specific alignment methods (RLHF for LLaMA2 and RefinedWeb Dataset for Falcon), making it unclear how effective the attack would be on models with different architectures or alignment techniques.
- What evidence would resolve it: Testing the approach on a diverse set of LLMs with varying architectures, sizes, and alignment methods (e.g., constitutional AI, different fine-tuning datasets) to assess the attack's effectiveness across different model configurations.

### Open Question 3
- Question: What are the most effective defense mechanisms against black-box jailbreaking attacks on LLMs?
- Basis in paper: [inferred] The paper discusses potential countermeasures such as dynamically adjusting model sensitivity to longer prompts and detecting "garbage" tokens, but does not evaluate their effectiveness.
- Why unresolved: The paper proposes potential defense strategies but does not empirically test their efficacy against the proposed attack or other black-box jailbreaking techniques.
- What evidence would resolve it: Implementing and testing various defense mechanisms (e.g., input filtering, model sensitivity adjustment, anomaly detection) against the proposed attack and other black-box jailbreaking methods to determine their effectiveness in mitigating such attacks.

## Limitations
- The universality claim is tested on only two models, insufficient to establish cross-architecture effectiveness
- Random subset sampling may introduce bias if the dataset contains distinct subpopulations
- No evaluation of defense mechanisms against discovered adversarial suffixes
- The fitness landscape smoothness assumption may not hold for all LLMs

## Confidence
- High Confidence: The basic GA methodology and its implementation details are clearly described and follow established evolutionary computation principles. The use of cosine similarity for alignment measurement is standard practice in NLP.
- Medium Confidence: The reported ASR values (up to 98%) are likely reproducible given the described methodology, though exact numbers may vary with implementation details and random seeds. The observation that suffix length affects attack success is plausible and consistent with prompt injection literature.
- Low Confidence: The universality claim across different LLM architectures and the robustness of discovered suffixes to model updates or defensive fine-tuning require significantly more empirical validation than provided.

## Next Checks
1. **Fitness Landscape Analysis**: Visualize the fitness landscape by evaluating a grid of token suffixes around discovered high-fitness solutions to determine if the landscape is smooth (supporting GA effectiveness) or rugged (potentially explaining convergence failures).

2. **Cross-Architecture Transfer**: Test the discovered universal suffixes on additional LLM architectures (e.g., GPT-3.5, Claude) and different sizes of the same architecture to quantify how "universal" the suffixes truly are across model families.

3. **Defensive Robustness Test**: Apply the discovered suffixes to models that have been fine-tuned with adversarial examples or defensive alignment techniques to assess whether the attack remains effective against basic mitigation strategies.