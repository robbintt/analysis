---
ver: rpa2
title: Splitting the Difference on Adversarial Training
arxiv_id: '2310.02480'
source_url: https://arxiv.org/abs/2310.02480
tags:
- adversarial
- natural
- training
- dbat
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Double Boundary Adversarial Training (DBAT),
  a novel approach to adversarial training that treats adversarial examples of each
  class as a separate class to be learned. This effectively doubles the number of
  classes but simplifies the decision boundaries.
---

# Splitting the Difference on Adversarial Training

## Quick Facts
- arXiv ID: 2310.02480
- Source URL: https://arxiv.org/abs/2310.02480
- Authors: 
- Reference count: 40
- One-line primary result: DBAT achieves near-optimal natural accuracy of 95.01% on CIFAR-10 while maintaining significant robustness across multiple tasks

## Executive Summary
The paper proposes Double Boundary Adversarial Training (DBAT), a novel approach to adversarial training that treats adversarial examples of each class as a separate class to be learned. This effectively doubles the number of classes but simplifies the decision boundaries. The authors provide theoretical and empirical evidence for the effectiveness of this approach. DBAT achieves near-optimal natural accuracy of 95.01% on CIFAR-10 while maintaining significant robustness across multiple tasks, including white-box and black-box attacks, natural corruptions, and unforeseen adversaries. The method is applicable to real-world applications where natural accuracy is crucial, such as autonomous vehicles, face recognition systems, and healthcare monitoring.

## Method Summary
DBAT treats adversarial examples as additional classes in the dataset, effectively doubling the number of classes from C to 2C. The method generates adversarial examples using targeted-PGD with random targets, assigns them to new adversarial classes (original class label + C), and trains the model on both natural and adversarial examples. Inference uses a projection function (e.g., max) to map the doubled class space back to the original class space. This approach simplifies decision boundaries by learning separate concepts for clean and adversarial examples rather than a single complex boundary.

## Key Results
- Achieves near-optimal natural accuracy of 95.01% on CIFAR-10
- Maintains significant robustness against white-box and black-box attacks
- Demonstrates effectiveness on CIFAR-10, CIFAR-100, and SVHN datasets
- Provides theoretical justification through the "DBAT advantage" showing reduced sample complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Splitting each class into "clean" and "adversarial" classes reduces decision boundary complexity
- Mechanism: Adversarial examples are treated as a separate class distribution rather than perturbations of the original class, leading to simpler decision boundaries
- Core assumption: Adversarial examples induce distinct distributions that can be learned as separate classes
- Evidence anchors:
  - [abstract]: "we take a fundamentally different approach by treating the perturbed examples of each class as a separate class to be learned, effectively splitting each class into two classes: 'clean' and 'adversarial'"
  - [section]: "Rather, we claim that for each class, the adversarial perturbations induce a distinct distribution on the examples, so much so that it makes more sense to learn it as a separate label"
- Break condition: If adversarial examples don't form distinct distributions from their source class, the complexity reduction advantage disappears

### Mechanism 2
- Claim: DBAT achieves near-optimal natural accuracy while maintaining robustness
- Mechanism: By keeping original class boundaries smooth and learning new adversarial class boundaries separately, natural accuracy is preserved while robustness is added
- Core assumption: Natural examples and their adversarial counterparts should not necessarily be assigned to the same class
- Evidence anchors:
  - [abstract]: "our method learns robust models while attaining optimal or near-optimal natural accuracy, e.g., on CIFAR-10 we obtain near-optimal natural accuracy of 95.01% alongside significant robustness"
  - [section]: "we suggest treating adversarial examples as additional classes in the dataset... This behavior creates a trade-off, where on one hand, DBAT does not induce significant changes to existing boundaries for natural classes in terms of complexity, and keeps them smoother"
- Break condition: If the adversarial class boundaries become too complex or interfere with natural class boundaries, the natural accuracy advantage may be lost

### Mechanism 3
- Claim: DBAT reduces sample complexity through the "DBAT advantage"
- Mechanism: Learning ℓ-fold unions of simpler concepts (clean and adversarial classes) can reduce overall sample complexity compared to learning complex single boundaries
- Core assumption: The VC-dimension increase from doubling classes is offset by the reduction in complexity of individual class boundaries
- Evidence anchors:
  - [section]: "We identify a phenomenon, which we term the DBAT advantage, which, when applicable, justifies the use of our technique... the thrust of our point continues to hold for the Rademacher complexity as well"
  - [section]: "we can formulate the learning problem as a 2k-multiclass classification problem... the corresponding bound in (4) will now behave as: √((V/2 + log(2k/δ))/n) — which, for constant δ and large V, constitutes considerable savings in sample complexity"
- Break condition: If the increase in number of classes overwhelms the complexity reduction benefit, the sample complexity advantage disappears

## Foundational Learning

- Concept: Adversarial training basics
  - Why needed here: Understanding how standard adversarial training works is crucial to grasp why DBAT is different
  - Quick check question: What is the key difference between standard adversarial training and DBAT in how they treat adversarial examples?

- Concept: Decision boundary complexity
  - Why needed here: The paper's core argument relies on the complexity of decision boundaries being different between methods
  - Quick check question: How does treating adversarial examples as separate classes affect the complexity of decision boundaries?

- Concept: VC-dimension and Rademacher complexity
  - Why needed here: The theoretical analysis in section 4 relies on understanding these concepts to explain the DBAT advantage
  - Quick check question: How does increasing the number of classes while decreasing hypothesis complexity affect sample complexity bounds?

## Architecture Onboarding

- Component map:
  - Input -> Standard image input (e.g., CIFAR-10 images)
  - Model architecture -> Standard CNN (e.g., WRN-34-10) with output layer doubled in size
  - Output layer -> 2C neurons (C original classes + C adversarial classes)
  - Inference -> Aggregation function (max) to combine clean and adversarial class probabilities

- Critical path:
  1. Generate adversarial examples using targeted-PGD with random targets
  2. Assign each adversarial example to its corresponding "adversarial class" (original class + C)
  3. Train model to classify both clean and adversarial examples
  4. At inference, apply aggregation function to map 2C outputs back to C classes

- Design tradeoffs:
  - Pros: Near-optimal natural accuracy while maintaining robustness, simpler decision boundaries
  - Cons: Doubled output layer size, requires modification to inference procedure, potential for increased training complexity

- Failure signatures:
  - Natural accuracy drops significantly: Likely the adversarial class boundaries are interfering with natural class boundaries
  - Robustness doesn't improve: Adversarial examples may not form distinct distributions from source classes
  - Training instability: Possible numerical issues with the additional log operation in the loss function

- First 3 experiments:
  1. Implement DBAT on a simple 2D synthetic dataset and visualize decision boundaries compared to standard AT
  2. Train DBAT on CIFAR-10 with WRN-34-10 and measure natural accuracy, PGD robustness, and Auto-Attack robustness
  3. Compare feature space separation between clean and adversarial examples using t-SNE visualization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DBAT perform on datasets with significantly different class distributions or imbalanced classes?
- Basis in paper: [inferred] The paper evaluates DBAT on CIFAR-10, CIFAR-100, and SVHN, which have relatively balanced class distributions. However, the paper does not explore performance on datasets with imbalanced classes or significantly different class distributions.
- Why unresolved: The paper does not provide any analysis or results on datasets with imbalanced classes or significantly different class distributions.
- What evidence would resolve it: Experiments on datasets with imbalanced classes or significantly different class distributions, comparing DBAT's performance to other adversarial training methods.

### Open Question 2
- Question: What is the impact of using different projection functions (e.g., mean, median, softmax) on DBAT's robustness against various attack types?
- Basis in paper: [explicit] The paper mentions that the defender can switch the projection function at inference time, but does not explore the impact of different projection functions on robustness.
- Why unresolved: The paper does not provide any analysis or results on the impact of different projection functions on DBAT's robustness.
- What evidence would resolve it: Experiments comparing DBAT's robustness against various attack types when using different projection functions (e.g., mean, median, softmax).

### Open Question 3
- Question: How does DBAT's performance scale with larger models and datasets?
- Basis in paper: [inferred] The paper evaluates DBAT on relatively small datasets (CIFAR-10, CIFAR-100, SVHN) and models (WRN-34-10, PreAct ResNet-18). However, it does not explore performance on larger models and datasets.
- Why unresolved: The paper does not provide any analysis or results on DBAT's performance with larger models and datasets.
- What evidence would resolve it: Experiments evaluating DBAT's performance on larger models and datasets, comparing it to other adversarial training methods.

## Limitations
- Theoretical claims about sample complexity advantages rely on assumptions that may not hold for deep networks
- Performance on real-world applications (autonomous vehicles, face recognition, healthcare monitoring) remains unverified
- Limited evaluation to relatively small datasets and standard architectures

## Confidence
- High Confidence: Near-optimal natural accuracy preservation on CIFAR-10 (95.01%) and effectiveness of the aggregation function for inference
- Medium Confidence: Theoretical sample complexity advantages, distinct distribution claims for adversarial examples, generalizability to other datasets and architectures
- Low Confidence: Performance on real-world applications mentioned (autonomous vehicles, face recognition, healthcare monitoring) due to lack of empirical validation

## Next Checks
1. **Distributional Analysis**: Conduct t-SNE or other visualization analyses to empirically verify that adversarial examples form distinct distributions from their source classes across multiple layers of the network.

2. **Dataset Generalization**: Implement DBAT on ImageNet or a similarly large-scale dataset to test whether the near-optimal natural accuracy benefit extends beyond CIFAR-10.

3. **Attack Diversity Testing**: Evaluate DBAT against a broader range of adaptive attacks (including transfer-based and decision-based attacks) to verify robustness claims beyond the standard PGD and Auto-Attack benchmarks.