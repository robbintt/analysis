---
ver: rpa2
title: Nonlinearity, Feedback and Uniform Consistency in Causal Structural Learning
arxiv_id: '2308.07520'
source_url: https://arxiv.org/abs/2308.07520
tags:
- causal
- latent
- variables
- rank
- constraint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This thesis investigates two fundamental challenges in causal
  discovery: (1) providing a more general and weaker faithfulness assumption that
  allows uniform consistency in causal structure learning, and (2) relaxing linearity
  and acyclicity assumptions to learn latent causal structures with cycles and nonlinearity.
  The author introduces the Generalized k-Triangle Faithfulness assumption, which
  is applicable to nonparametric distributions and enables uniformly consistent estimators
  of causal effects.'
---

# Nonlinearity, Feedback and Uniform Consistency in Causal Structural Learning

## Quick Facts
- arXiv ID: 2308.07520
- Source URL: https://arxiv.org/abs/2308.07520
- Reference count: 0
- Primary result: Introduces Generalized k-Triangle Faithfulness assumption for uniform consistency in nonparametric causal structure learning, and develops methods using GIN and rank constraints to identify latent causal structures with cycles and nonlinearity.

## Executive Summary
This thesis addresses two fundamental challenges in causal discovery: achieving uniform consistency under weaker faithfulness assumptions, and learning causal structures with latent variables, cycles, and nonlinearity. The author introduces the Generalized k-Triangle Faithfulness assumption, which is strictly weaker than Strong Faithfulness and applicable to nonparametric distributions, enabling uniformly consistent causal effect estimation. Additionally, the thesis develops methods using Generalized Independence Noise (GIN) and rank constraints to identify latent causal structures, demonstrating that these methods can handle cycles and nonlinearity under weaker assumptions than previously thought. The work bridges theoretical guarantees with practical algorithmic approaches for complex causal discovery scenarios.

## Method Summary
The thesis proposes algorithms combining GIN and rank constraints to identify causal clusters and detect cycles in latent variable models. The CGIN algorithm uses Kernel-based Conditional Independence tests (KCI) for independence testing on synthetic data generated from Linear Latent Hierarchical Cyclic Models (L2HCM) and Linear Non-Gaussian Latent Variable Models (LiN GLaM) with sample sizes of 500, 1000, and 2000. The method evaluates performance using metrics including cluster accuracy (ClusterRecall, ClusterPrecision), latent order learning (LatentOrderRecall, LatentOrderPrecision), and cycle identification (CyclicRecall, CyclicPrecision). The approach relaxes linearity and acyclicity assumptions by leveraging rank constraints under choke sets and non-Gaussianity properties through GIN.

## Key Results
- The Generalized k-Triangle Faithfulness assumption is strictly weaker than Strong Faithfulness, allowing uniform consistency in causal structure learning for nonparametric distributions.
- Rank constraints and GIN can identify latent causal structures with cycles and nonlinearity when the model is linear under choke sets and non-Gaussian.
- Tensor constraints generalize rank constraints to non-Gaussian distributions and remain valid under k-choke set linearity assumptions, even with cycles.
- Simulation results demonstrate effective identification of causal structures with latent variables using the combined GIN and rank constraint approach.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Generalized k-Triangle Faithfulness allows uniform consistency in causal structure learning for nonparametric distributions.
- **Mechanism**: By relaxing the Strong Faithfulness assumption to bound only conditional correlations in triangle structures by functions of edge strengths, the assumption becomes weaker and avoids implausible requirements on weak edge strengths. This weaker assumption permits uniformly consistent estimators of Markov equivalence classes.
- **Core assumption**: The modified k-Triangle Faithfulness holds and there exist uniformly consistent tests of conditional independence under smoothness assumptions (TV smoothness and non-zero density bounds).
- **Evidence anchors**:
  - [abstract] "providing an alternative definition of k-Triangle Faithfulness that (i) is weaker than strong faithfulness when applied to the Gaussian family of distributions, (ii) can be applied to non-Gaussian families of distributions, and (iii) under the assumption that the modified version of Strong Faithfulness holds, can be used to show the uniform consistency of a modified causal discovery algorithm"
  - [section] "the k-Triangle-Faithfulness Assumption is strictly weaker than the Strong Faithfulness Assumption in several respects: the Strong faithfulness Assumption does not allow edges to be weak any where in a graph, while the k-Triangle-Faithfulness Assumption only excludes conditional correlations ρ(X, Z|W) from being too small if X and Z are in some triangle structures ⟨X, Y, Z⟩ and X − Z is not a weak edge"
  - [corpus] Weak evidence; corpus lacks direct support for nonparametric consistency claims.
- **Break condition**: If conditional independence tests are not uniformly consistent, or if the generalized k-Triangle Faithfulness is violated due to strong dependence in triangle structures.

### Mechanism 2
- **Claim**: Rank constraints and GIN (Generalized Independence Noise) can identify latent causal structures with cycles and nonlinearity.
- **Mechanism**: Rank constraints apply when the model is linear below choke sets, even with cycles, by bounding the rank of cross-covariance matrices. GIN leverages non-Gaussianity to detect causal clusters and orderings among latent variables, and can identify cycles when linear combinations of ancestors fail to separate non-Gaussian noise from descendants.
- **Core assumption**: Linearity under choke sets for rank constraints; non-Gaussianity for GIN; faithfulness holds for the causal structure.
- **Evidence anchors**:
  - [abstract] "relaxing the sufficiency assumption to learn causal structures with latent variables"
  - [section] "we will show that rank constraint and GIN can be used to learn latent causal structures while relaxing the assumptions of linearity and acyclicity"
  - [corpus] Weak evidence; corpus does not directly address cycles and nonlinearity relaxation.
- **Break condition**: If the model violates linearity below choke sets or non-Gaussianity assumptions, or if the rank and GIN conditions are not satisfied due to insufficient statistical signal.

### Mechanism 3
- **Claim**: Tensor constraints generalize rank constraints to non-Gaussian distributions and can identify latent structures with cycles when the model is linear under k-choke sets.
- **Mechanism**: Tensor constraints use higher-order cumulants to detect k-treks between variable sets, generalizing rank constraints. They remain valid when only linearity under k-choke sets is assumed, even with cycles, by exploiting the recursively additive model structure.
- **Core assumption**: The model follows a recursively additive structure and is linear under k-choke sets; cumulant tensors have the required properties.
- **Evidence anchors**:
  - [abstract] "relaxing various simplification assumptions is expected to extend the causal discovery method to be applicable in a wider range with diversified causal mechanism and statistical phenomena"
  - [section] "we will show that tensor constraint, a generalization of rank constraint under non-Gaussian distribution[23], is preserved with only linearity between the observed variables and their latent common causes"
  - [corpus] Weak evidence; corpus does not directly support tensor constraint claims under k-choke sets.
- **Break condition**: If the model is not recursively additive or not linear under k-choke sets, or if tensor constraints are computationally infeasible or inconsistent for odd-dimensional tensors.

## Foundational Learning

- **Concept**: Directed Acyclic Graph (DAG) and causal interpretation
  - Why needed here: Understanding DAGs is essential to grasp the causal assumptions (Markov, Faithfulness) and the structure learning algorithms discussed.
  - Quick check question: What is the difference between a collider and a non-collider in a DAG, and how does this affect conditional independence?

- **Concept**: Causal Markov and Faithfulness assumptions
  - Why needed here: These assumptions underpin the theoretical guarantees of the causal discovery algorithms and the conditions under which they are consistent.
  - Quick check question: How do the Causal Markov and Faithfulness assumptions differ, and what are the implications of violating each?

- **Concept**: Structural Equation Models (SEM) and linear Gaussian models
  - Why needed here: SEMs provide the framework for representing causal mechanisms, and linear Gaussian models are used to derive theoretical results and implement algorithms.
  - Quick check question: How does the linear Gaussian model relate to DAGs, and what are the assumptions about the error terms?

## Architecture Onboarding

- **Component map**:
  - Theoretical framework: DAGs, causal assumptions, SEMs
  - Algorithmic components: VCSGS (structure learning), Edge Estimation Algorithm (causal effect estimation), rank constraint and GIN-based algorithms (latent structure learning), tensor constraint algorithms
  - Statistical tests: Conditional independence tests, higher-order cumulant calculations

- **Critical path**:
  1. Define the causal model and assumptions
  2. Apply VCSGS or similar algorithm to learn DAG structure
  3. Estimate causal effects using Edge Estimation Algorithm
  4. If latent variables are present, apply rank constraint and/or GIN algorithms to identify clusters and cycles
  5. If non-Gaussianity is present, apply tensor constraint algorithms for further structure identification

- **Design tradeoffs**:
  - Strength of faithfulness assumption vs. consistency guarantees
  - Linearity vs. nonlinearity assumptions for different algorithms
  - Gaussian vs. non-Gaussian assumptions for tensor constraints
  - Computational complexity of tensor constraints vs. rank constraints

- **Failure signatures**:
  - Violation of faithfulness assumptions leading to incorrect structure learning
  - Inconsistent conditional independence tests due to insufficient sample size
  - Breakdown of rank or tensor constraints when linearity or non-Gaussianity assumptions are violated
  - Computational intractability of tensor constraints for high-dimensional data

- **First 3 experiments**:
  1. Simulate linear Gaussian data from a known DAG and test VCSGS for structure learning and Edge Estimation for causal effect estimation
  2. Simulate data with latent variables and cycles, apply rank constraint and GIN algorithms to identify clusters and cycles
  3. Simulate non-Gaussian data with latent variables, apply tensor constraint algorithms to identify latent structures and compare with rank constraint results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the tensor constraint be effectively applied to identify latent variables when the cumulant tensor has an odd number of dimensions?
- Basis in paper: [explicit] The paper discusses the limitations of the tensor constraint with odd-dimensional cumulant tensors and mentions that the hyperdeterminant is not zero even when the tensor constraint conditions are met.
- Why unresolved: The paper identifies the problem but does not provide a solution or method to handle odd-dimensional tensors effectively.
- What evidence would resolve it: A method or algorithm that can reliably use the tensor constraint with odd-dimensional tensors, or a proof that such a method is not possible.

### Open Question 2
- Question: Can the algorithms for identifying cyclic latent structures be extended to handle hierarchical latent structures with cycles between parents and children that are all latent?
- Basis in paper: [explicit] The paper mentions that the CGIN algorithm can identify causal clusters with cycles between measured children and latent parents, and suggests that extending this to hierarchical structures is a future research topic.
- Why unresolved: The paper only provides initial ideas and conjectures but does not develop or test algorithms for hierarchical structures.
- What evidence would resolve it: An algorithm that successfully identifies cyclic structures in hierarchical latent models, validated with simulation data.

### Open Question 3
- Question: What methods can be used to accurately identify individual causal clusters where the connection between latent blocks is cyclic?
- Basis in paper: [explicit] The paper notes that it is difficult for both GIN and rank constraints to accurately identify individual causal clusters with cyclic connections between latent blocks.
- Why unresolved: The paper acknowledges the difficulty but does not propose or test new methods to address this specific challenge.
- What evidence would resolve it: A new method or test that can accurately identify clusters with cyclic connections between latent blocks, demonstrated through simulations or real data examples.

## Limitations
- Limited empirical validation of the proposed methods, with weak evidence supporting nonparametric consistency claims.
- Computational feasibility of tensor constraint methods for high-dimensional data remains unclear.
- Performance under finite sample sizes and model misspecification requires further investigation.

## Confidence
- **High**: Theoretical proofs for weaker k-Triangle Faithfulness assumption and rank/tensor constraint preservation
- **Medium**: Practical applicability and performance of algorithms due to limited experimental results
- **Low**: Computational feasibility of tensor constraints for high-dimensional data and odd-dimensional tensors

## Next Checks
1. Implement the CGIN algorithm on simulated L2HCM and LiN GLaM datasets with varying sample sizes (500, 1000, 2000) and evaluate cluster accuracy, latent order learning, and cycle identification performance.

2. Compare the performance of tensor constraint methods against rank constraint methods on non-Gaussian datasets with latent variables to assess the practical benefits of the generalization.

3. Conduct sensitivity analysis on the k-Triangle Faithfulness parameter k to determine its impact on the consistency guarantees and the practical identifiability of causal structures.