---
ver: rpa2
title: Exploring the Impact of Training Data Distribution and Subword Tokenization
  on Gender Bias in Machine Translation
arxiv_id: '2309.12491'
source_url: https://arxiv.org/abs/2309.12491
tags:
- gender
- tokens
- forms
- translation
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of tokenization on gender bias
  in machine translation. The authors analyze the interactions between the frequency
  of gendered profession names in training data, their representation in the subword
  tokenizer's vocabulary, and gender bias.
---

# Exploring the Impact of Training Data Distribution and Subword Tokenization on Gender Bias in Machine Translation

## Quick Facts
- **arXiv ID**: 2309.12491
- **Source URL**: https://arxiv.org/abs/2309.12491
- **Reference count**: 22
- **Primary result**: Training data imbalance is the primary driver of gender bias in machine translation, not subword tokenization.

## Executive Summary
This paper investigates the impact of subword tokenization on gender bias in machine translation systems. Through analysis of German, Spanish, and Hebrew translation models, the authors demonstrate that the frequency of gender forms in training data is the primary factor affecting gender bias, rather than how words are split into subword tokens. They show that female and non-stereotypical gender inflections tend to be split into more subword tokens due to their lower frequency in training corpora. The study also introduces a method to estimate gender imbalance in training data by analyzing tokenizer vocabulary, and demonstrates that fine-tuning only token embedding layers can reduce gender bias without compromising translation quality.

## Method Summary
The authors analyze pre-trained machine translation models (OpusMT and MBART50) for English-to-German, English-to-Spanish, and English-to-Hebrew translation. They use the WinoMT dataset for evaluating gender bias and compute metrics including Gender Translation Accuracy (F1), ∆G, ∆S, and ∆T. The study involves analyzing the correlation between subword tokenization patterns and translation accuracy, examining training corpus statistics (particularly from OPUS-100), and conducting fine-tuning experiments on a gender-balanced dataset. The fine-tuning process focuses solely on token embedding layers to assess whether this can reduce gender bias while maintaining translation quality.

## Key Results
- Training data imbalance, not subword tokenization, is the primary driver of gender bias in machine translation.
- Female and non-stereotypical gender forms are more frequently split into multiple subword tokens due to lower frequency in training data.
- Fine-tuning only token embedding layers on gender-balanced data can reduce gender bias (∆G) without significantly affecting BLEU scores.
- Subword tokenization patterns can serve as indicators of gender-form imbalance in training data, even when the corpus is not publicly available.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training data frequency is the primary driver of gender bias, not subword tokenization.
- Mechanism: Gender forms appearing less frequently in training data are less likely to be represented as single tokens, leading to multi-token splits. This creates apparent correlation between tokenization and bias, but the underlying cause is frequency imbalance. The model learns stronger associations for more frequent (typically male) forms, resulting in higher translation accuracy for those forms.
- Core assumption: Training corpus is the source of frequency imbalance.
- Evidence anchors: Analysis of OPUS-100 showing male forms are more frequent; conditional independence analysis showing frequency mediates the tokenization-bias relationship.

### Mechanism 2
- Claim: Subword splits can estimate gender-form imbalance in training data.
- Mechanism: Since tokenizers are typically trained on the same data as translation models, tokenization patterns reflect word frequency distributions. More subword tokens for female/anti-stereotypical forms indicates lower frequency in training data, allowing researchers to infer gender distribution without direct corpus access.
- Core assumption: Tokenizer trained on same corpus as translation model.
- Evidence anchors: Authors demonstrate this estimation method works; analysis of OPUS-100 supports the frequency-split relationship.

### Mechanism 3
- Claim: Fine-tuning token embeddings on balanced data reduces gender bias without quality loss.
- Mechanism: Token embeddings encode learned word associations. Fine-tuning these embeddings on gender-balanced data allows the model to learn equal associations for male and female forms, reducing bias while preserving overall translation quality since other model components remain unchanged.
- Core assumption: Bias is primarily encoded in token embeddings.
- Evidence anchors: Table 1 shows fine-tuning improves gender accuracy and reduces male preference while maintaining BLEU scores.

## Foundational Learning

- **Concept**: Causal inference and confounding variables
  - Why needed here: To understand the relationship between tokenization, training data frequency, and bias, and identify frequency as the confounding variable.
  - Quick check question: If two variables are correlated, does it always mean one causes the other? What is a confounding variable?

- **Concept**: Subword tokenization methods (BPE, Unigram, SentencePiece)
  - Why needed here: To understand how tokenizers split words and how this process is influenced by word frequency.
  - Quick check question: What is the difference between word-level and subword tokenization? How does word frequency affect tokenization?

- **Concept**: Gender bias metrics (∆G, ∆S, F1)
  - Why needed here: To measure and quantify gender bias in translation models and evaluate debiasing methods.
  - Quick check question: What is the difference between precision and recall? How is F1 score calculated?

## Architecture Onboarding

- **Component map**: Input sentences with professions and pronouns -> SentencePiece tokenizer -> Translation model (Marian NMT/encoder-decoder) -> Translated sentences -> Evaluation metrics

- **Critical path**: 
  1. Tokenize input using SentencePiece
  2. Encode tokenized input with translation model
  3. Decode to generate translation
  4. Evaluate using gender bias metrics
  5. Optionally fine-tune embeddings on balanced data

- **Design tradeoffs**: 
  - Subword tokenization handles OOV words and reduces vocabulary size but may split infrequent words, affecting accuracy
  - Fine-tuning only embeddings is lightweight but may not address all bias forms
  - Gender-balanced datasets reduce bias but require creation effort

- **Failure signatures**: 
  - High ∆G/∆S values indicate gender preference
  - Low F1 scores for female/anti-stereotypical professions indicate poor accuracy
  - Fine-tuning fails if ∆G/∆S don't decrease or BLEU drops significantly

- **First 3 experiments**: 
  1. Analyze subword token counts vs translation accuracy for different gender forms
  2. Measure correlation between training frequency and tokenization patterns
  3. Fine-tune embeddings on balanced data and evaluate bias/quality changes

## Open Questions the Paper Calls Out
- How does the choice of subword tokenization algorithm affect gender bias?
- How does including neutral gender forms affect observed bias?
- How does target language choice affect observed gender bias?

## Limitations
- Correlation vs causation uncertainty: The exact magnitude of tokenization's independent contribution to bias remains unclear
- Language scope: Results may not generalize beyond German, Spanish, and Hebrew
- Reproducibility: The gender-balanced dataset for fine-tuning is not publicly available

## Confidence
- **High Confidence**: Training data imbalance is the primary driver of gender bias
- **Medium Confidence**: Subword tokenization patterns can estimate training data imbalance
- **Low Confidence**: Fine-tuning only token embeddings is universally effective for debiasing

## Next Checks
1. Conduct intervention studies with artificially balanced training data frequencies to validate causal relationship
2. Apply subword analysis method to broader range of languages and domains
3. Extend embedding fine-tuning to larger models and assess long-term stability and side effects