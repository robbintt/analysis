---
ver: rpa2
title: 'Defenses in Adversarial Machine Learning: A Survey'
arxiv_id: '2312.08890'
source_url: https://arxiv.org/abs/2312.08890
tags:
- adversarial
- backdoor
- conference
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey paper provides a unified perspective on adversarial
  machine learning (AML) defenses by categorizing them based on the life-cycle stages
  of machine learning systems: pre-training, training, post-training, deployment,
  and inference. It systematically reviews defense methods against three primary attack
  paradigms: backdoor attacks, weight attacks, and adversarial examples.'
---

# Defenses in Adversarial Machine Learning: A Survey

## Quick Facts
- arXiv ID: 2312.08890
- Source URL: https://arxiv.org/abs/2312.08890
- Reference count: 40
- One-line primary result: This survey paper categorizes and reviews defense methods in adversarial machine learning based on the life-cycle stages of machine learning systems.

## Executive Summary
This survey paper provides a unified perspective on adversarial machine learning (AML) defenses by categorizing them based on the life-cycle stages of machine learning systems: pre-training, training, post-training, deployment, and inference. It systematically reviews defense methods against three primary attack paradigms: backdoor attacks, weight attacks, and adversarial examples. The paper highlights that while numerous defense strategies exist for individual attack types, a comprehensive understanding of overall system robustness remains challenging due to the diversity and independence of these defenses. It proposes taxonomies for categorizing defense methods at each stage, facilitating analysis of their mechanisms and interconnections.

## Method Summary
The paper provides a comprehensive review of existing defense methods at each stage of the machine learning life-cycle without specifying a particular dataset, model architecture, or training procedure. It discusses various defense methods applicable to image classification tasks in general, covering defenses against backdoor attacks, weight attacks, and adversarial examples. The objective is to improve the robustness of machine learning models against various attacks, with metrics including attack success rate, model accuracy, and defense effectiveness rating.

## Key Results
- Defense categorization by lifecycle stage enables systematic comparison of defenses across attack types
- Training-stage adversarial training improves robustness by exposing models to crafted adversarial examples
- Post-training backdoor defense methods can effectively remove backdoors without access to original training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Defense categorization by lifecycle stage enables systematic comparison of defenses across attack types
- Mechanism: By organizing defenses according to the five stages of machine learning system development, the survey creates a unified framework that reveals how different defense strategies address the same attack at different points in the system's evolution
- Core assumption: Defenses at different lifecycle stages address fundamentally different aspects of attack vulnerability and can be meaningfully compared within each stage
- Evidence anchors:
  - [abstract] "We factorize a complete machine learning system into five stages... we present a clear taxonomy to categorize and review representative defense methods at each individual stage"
  - [section II] "we first categorize them by the distinct phases of the machine learning life-cycle, i.e., pre-training stage, training stage, post-training, deployment stage, and inference stage"
- Break condition: If attack strategies evolve to target multiple lifecycle stages simultaneously, the single-stage categorization may miss critical interdependencies between defenses

### Mechanism 2
- Claim: Training-stage adversarial training improves robustness by exposing models to crafted adversarial examples during the learning process
- Mechanism: The general formulation of adversarial training introduces an inner maximization problem that generates adversarial examples within a bounded perturbation space, forcing the model to learn robust features that generalize to unseen attacks
- Core assumption: Generating adversarial examples during training transfers robustness to similar but unseen attacks during inference
- Evidence anchors:
  - [section IV-A] "Adversarial training (AT) is the mainstream training-stage defense against adversarial examples by improving the model's robustness to adversarial examples"
  - [section IV-A] "The general formulation of AT is as follows: minθ E(x,y)∈D[L1(fθ(x),y) + λ maxϵ∈S L2(fθ(x+ϵ),y)]"
- Break condition: If the attack space expands beyond the perturbation constraints used during training, or if new attack types exploit vulnerabilities not addressed by the training perturbations

### Mechanism 3
- Claim: Post-training backdoor defense methods can effectively remove backdoors without access to the original training data
- Mechanism: These methods employ techniques like structure modification (pruning backdoor-related neurons) or tuning-based approaches (fine-tuning with clean samples) to eliminate backdoor functionality while preserving clean accuracy
- Core assumption: Backdoor effects manifest in specific, identifiable patterns in model weights or activations that can be isolated and removed
- Evidence anchors:
  - [section V-A] "given an untrustworthy model that may contain backdoor, the defender aims to obtain a secure model by removing the potential backdoor while preserving the clean performance"
  - [section V-A-c] "Structure Modification approaches can be divided into pruning-based methods, which aim to identify which neurons contribute to backdoor and prune them"
- Break condition: If backdoors are embedded in ways that create subtle, distributed effects throughout the model that cannot be isolated through pruning or tuning

## Foundational Learning

- Concept: Adversarial machine learning threat model
  - Why needed here: Understanding the three primary attack paradigms (backdoor attacks, weight attacks, adversarial examples) and their target stages is essential for contextualizing the defense strategies
  - Quick check question: What are the three main attack paradigms discussed in the survey and which stage of the ML lifecycle does each primarily target?

- Concept: Lifecycle-based system analysis
  - Why needed here: The survey's novel contribution is organizing defenses by system lifecycle stages rather than by attack type, requiring understanding of how different stages have different defensive capabilities and constraints
  - Quick check question: How does categorizing defenses by lifecycle stage rather than attack type provide advantages for understanding system robustness?

- Concept: Threat model evolution and adaptation
  - Why needed here: The survey identifies challenges including defending against different types of attacks and intersections between stages, requiring understanding of how threat models evolve over time
  - Quick check question: Why might a defense effective against one type of attack become less effective as threat models evolve?

## Architecture Onboarding

- Component map:
  - Survey structure: Five lifecycle stages (pre-training, training, post-training, deployment, inference)
  - Defense categories: Each stage contains specific defense types (architecture design, data preprocessing, model training, backdoor detection, fingerprint verification, input transformation, etc.)
  - Attack-Stage mapping: Backdoor attacks → training/inference, Weight attacks → deployment, Adversarial examples → inference
  - Taxonomy relationships: Stage → Defense Type → Specific Methods → Evidence/Results

- Critical path:
  1. Understand the three attack paradigms and their target stages
  2. Follow the lifecycle framework from pre-training through inference
  3. For each stage, understand the available defense mechanisms
  4. Recognize the limitations and interdependencies between stage-specific defenses

- Design tradeoffs:
  - Stage-specific vs. cross-stage defenses: Stage-specific defenses are more targeted but may miss multi-stage attack strategies
  - Black-box vs. white-box assumptions: Many defenses assume access to model internals, limiting practical applicability
  - Computational cost vs. effectiveness: More comprehensive defenses often require significantly more resources
  - Generalization vs. specialization: Defenses targeting specific attacks may be more effective but less adaptable to new threats

- Failure signatures:
  - Overfitting to specific attack types while missing others
  - Defenses that work in theory but fail under practical constraints (data limitations, black-box access)
  - Performance degradation when multiple defenses are combined
  - Inability to handle multi-stage or adaptive attacks that evolve over time

- First 3 experiments:
  1. Map existing defense papers to the lifecycle framework to identify gaps in stage coverage
  2. Test cross-stage defense combinations to evaluate whether combining defenses from different stages improves overall robustness
  3. Evaluate black-box versions of white-box defenses to assess practical applicability under realistic constraints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adversarial training effectively defend against both adversarial examples and backdoor attacks simultaneously, or is there an inherent trade-off between robustness to these two types of attacks?
- Basis in paper: [explicit] The paper discusses the relationship between adversarial robustness and backdoor robustness, citing prior works that have analyzed this trade-off and found that while there may be some trade-off, composite defenses combining both types of robustness are possible.
- Why unresolved: While some prior works have explored this relationship, a comprehensive understanding of the complex interplay between adversarial robustness and backdoor robustness remains an open challenge. The paper highlights the need for further investigation into this trade-off and the development of models that can effectively combine both types of robustness.
- What evidence would resolve it: Empirical studies comparing the effectiveness of adversarial training in defending against both adversarial examples and backdoor attacks simultaneously, as well as theoretical analysis of the underlying mechanisms and trade-offs involved, would help resolve this question.

### Open Question 2
- Question: How can defense strategies be designed to account for the interconnections between different stages of the machine learning life-cycle, rather than focusing solely on individual stages?
- Basis in paper: [explicit] The paper discusses the multi-stage nature of some attacks, such as data poisoning-based backdoor attacks that implant a backdoor during training and activate it during inference. It highlights the need for multi-stage defense strategies that can address the interconnections between different stages of the machine learning life-cycle.
- Why unresolved: Most existing backdoor defenses focus on defending against attacks at specific stages, compromising the overall effectiveness of these defenses. Addressing this challenge requires a paradigm shift towards the adoption of multi-stage defense strategies that can account for the interconnections between different stages.
- What evidence would resolve it: The development and evaluation of multi-stage defense strategies that demonstrate improved robustness against attacks that span multiple stages of the machine learning life-cycle would help resolve this question.

### Open Question 3
- Question: How can defense methods be developed that operate effectively under practical threat models, including considerations such as data resources, model accessibility, and structural requirements?
- Basis in paper: [explicit] The paper discusses the limitations of defense methods that operate under idealized assumptions, such as abundant clean data, specific model architectures, or white-box accessibility. It highlights the need for defenses that can operate effectively under practical threat models, where these assumptions may not hold.
- Why unresolved: Defense strategies often rely on assumptions that constrain their practical utility in dynamic real-world scenarios. Developing defenses that can operate effectively under practical threat models is essential for enhancing the applicability and robustness of defense mechanisms in the face of evolving and sophisticated adversarial threats.
- What evidence would resolve it: The development and evaluation of defense methods that demonstrate effectiveness under practical threat models, such as limited data resources, partial model accessibility, or diverse model architectures, would help resolve this question.

## Limitations

- The survey cannot fully address the effectiveness of combined, multi-stage defense strategies due to the inherent independence of most defense methods
- The paper focuses primarily on image classification tasks, limiting generalizability to other domains like NLP or reinforcement learning
- Most defenses are designed for specific attack types and lifecycle stages, creating gaps in understanding how defenses interact when deployed together

## Confidence

- High confidence in the taxonomy framework: The lifecycle-based categorization is well-supported by the literature and provides a clear organizational structure for understanding AML defenses
- Medium confidence in cross-stage effectiveness claims: While the paper identifies the need for attack-agnostic defenses, empirical validation of combined defenses across multiple stages remains limited in the literature
- Medium confidence in open challenge identification: The survey accurately identifies key challenges, but the proposed solutions require further research validation

## Next Checks

1. Implement a unified defense system combining at least one defense method from each lifecycle stage and evaluate its performance against multi-stage adaptive attacks
2. Conduct a meta-analysis comparing the effectiveness of black-box versus white-box versions of defenses across different attack types
3. Test the generalizability of the lifecycle framework by applying it to defense strategies in non-image domains (e.g., NLP or graph neural networks) and identifying stage-specific adaptations required for different data types