---
ver: rpa2
title: Human and AI Perceptual Differences in Image Classification Errors
arxiv_id: '2304.08733'
source_url: https://arxiv.org/abs/2304.08733
tags:
- human
- machine
- classi
- learning
- machines
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores perceptual differences between human and AI
  image classifiers. While AI models outperform humans in overall accuracy, they make
  significantly different types of errors.
---

# Human and AI Perceptual Differences in Image Classification Errors

## Quick Facts
- **arXiv ID:** 2304.08733
- **Source URL:** https://arxiv.org/abs/2304.08733
- **Reference count:** 40
- **Primary result:** Human-AI teaming outperforms standalone AI or human classification in image tasks

## Executive Summary
This paper investigates the fundamental differences between human and AI perceptual capabilities in image classification. While AI models achieve higher overall accuracy than humans on CIFAR-10 (81.9-82.8% vs 86.1%), the study reveals that machines make systematic, similar errors while humans make diverse mistakes. The research demonstrates that humans can outperform AI in cases where machines have low confidence or agreement, and that combining human and machine judgments through a simple threshold-based collaboration method significantly improves accuracy beyond what either can achieve alone.

## Method Summary
The study compares 13 robust machine learning models trained on noisy human annotations from the CIFAR-N dataset against human annotations on the CIFAR-H dataset. Researchers analyze error patterns using confusion matrices, partition the data by difficulty using four metrics (machine confidence, machine agreement, human annotation time, and human agreement), and implement a threshold-based human-AI teaming algorithm. The collaboration system replaces low-confidence machine predictions with human judgments to achieve improved accuracy.

## Key Results
- Machines make systematic, similar mistakes to each other while humans make diverse errors
- Humans outperform AI in cases where AI has low confidence or agreement
- Simple threshold-based human-AI collaboration achieves 87.8% accuracy, surpassing both standalone AI (82.8%) and AI-AI teaming (86.2%)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Machines make similar mistakes to other machines, while humans make more diverse errors
- **Mechanism:** Training data biases and model architecture create systematic blind spots shared across models, while human perception varies based on individual experience
- **Core assumption:** Training data contains inherent biases that models learn to replicate
- **Evidence anchors:**
  - "AI models tend to make similar mistakes to each other, while humans make more diverse errors"
  - "machines have a strong consensus and tend to make similar mistakes" vs "human judgments sometimes diverge"
- **Break Condition:** If training data is perfectly balanced or models avoid common pitfalls

### Mechanism 2
- **Claim:** Machine confidence strongly correlates with accuracy, while human performance shows little correlation to machine-derived difficulty metrics
- **Mechanism:** Machine confidence reflects learned probability distributions that correlate with accuracy when models are well-calibrated, while human perception operates on different criteria
- **Core assumption:** Machine confidence scores are meaningful indicators of model performance
- **Evidence anchors:**
  - "AI accuracy strongly correlates with difficulty metrics, but human performance shows little correlation"
  - "machine performance is strongly correlated to the machine confidence score" vs "human performance is only mildly correlated with machine confidence"
- **Break Condition:** If machine confidence scores are poorly calibrated

### Mechanism 3
- **Claim:** Humans outperform machines in cases where AI has low confidence or agreement
- **Mechanism:** Machine uncertainty indicates potential error likelihood, while human perception can provide correct answers in uncertain cases where machine reasoning fails
- **Core assumption:** Machine uncertainty is a reliable indicator of potential error
- **Evidence anchors:**
  - "humans outperform AI in cases where AI has low confidence or agreement"
  - "humans are more accurate than machines when machines have low confidence"
- **Break Condition:** If human performance is also poor in uncertain cases

## Foundational Learning

- **Concept:** Confusion matrices for error analysis
  - Why needed here: To quantify and visualize the pattern of mistakes made by humans vs machines, revealing whether errors are systematic or diverse
  - Quick check question: If a confusion matrix shows strong diagonal patterns in machine errors but scattered patterns in human errors, what does this tell you about their perceptual differences?

- **Concept:** Machine confidence scores and agreement metrics
  - Why needed here: To create difficulty rankings that partition the dataset into subsets where performance differences between humans and machines can be measured
  - Quick check question: If machine confidence scores perfectly correlated with accuracy, what would you expect to see in a plot of accuracy vs confidence?

- **Concept:** Post-hoc human-machine collaboration algorithms
  - Why needed here: To demonstrate the practical value of leveraging perceptual differences by combining human and machine judgments in a complementary way
  - Quick check question: In a threshold-based collaboration system, what happens to accuracy when the confidence threshold is set too high vs too low?

## Architecture Onboarding

- **Component map:** Data preprocessing -> Model training -> Error analysis -> Difficulty ranking -> Collaboration system -> Evaluation
- **Critical path:** Data → Models → Error Analysis → Difficulty Ranking → Collaboration → Evaluation
- **Design tradeoffs:**
  - Using multiple robust training methods vs focusing on a single best-performing model
  - Choosing simple threshold-based collaboration vs more complex algorithms
  - Using noisy human labels for training vs clean labels
- **Failure signatures:**
  - No accuracy improvement from collaboration (threshold too high or too low)
  - Machine errors not systematic (confusion matrix shows no patterns)
  - Human performance correlates with machine confidence (no complementary expertise)
- **First 3 experiments:**
  1. Compare confusion matrices of all machine models vs human annotations to verify systematic vs diverse error patterns
  2. Plot accuracy vs machine confidence/agreement to confirm correlation differences between humans and machines
  3. Implement and test threshold-based collaboration with different confidence thresholds to find optimal value

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can better teaming algorithms be developed that achieve closer to the oracle upper bound in human-AI collaboration?
- Basis in paper: The paper notes that the realistic teaming algorithm achieves only a fraction of the oracle upper bound, suggesting room for improvement.
- Why unresolved: The paper only demonstrates a simple threshold-based collaboration method and doesn't explore more sophisticated teaming strategies.
- What evidence would resolve it: Developing and testing more advanced teaming algorithms that approach the oracle performance would provide an answer.

### Open Question 2
- Question: What are the perceptual differences between human and machine classifiers when using different vision backbones?
- Basis in paper: The paper shows that machines trained with different vision backbones (ResNet34, VGG, Inception) still make similar mistakes to each other, while differing from humans.
- Why unresolved: The paper only tests a limited set of backbones and doesn't fully characterize how backbone choice affects human-machine perceptual differences.
- What evidence would resolve it: Testing a wider range of vision backbones and analyzing the resulting perceptual differences would provide more insight.

### Open Question 3
- Question: Can human-AI teaming performance be further improved by incorporating more than one human classifier?
- Basis in paper: The paper shows that teaming with an aggregated human classifier (majority vote of three humans) provides a greater accuracy boost than teaming with a single human.
- Why unresolved: The paper only tests aggregation of three human classifiers and doesn't explore the impact of using more or fewer human classifiers.
- What evidence would resolve it: Testing teaming performance with different numbers of human classifiers would provide an answer.

## Limitations

- Limited to CIFAR-10 dataset, may not generalize to more complex image classification tasks
- Simple threshold-based collaboration method may not scale to real-world applications requiring more nuanced human-AI interaction
- Study uses a specific set of robust learning methods, may not capture full spectrum of human-AI perceptual differences

## Confidence

- **High:** Systematic machine errors vs diverse human errors, human-outperformance in low-confidence cases, collaboration benefits
- **Medium:** Correlation patterns between difficulty metrics and performance, generalizability to other datasets
- **Low:** Scalability of collaboration method to real-world applications, robustness across different model architectures

## Next Checks

1. Replicate the study on larger, more complex datasets (e.g., ImageNet) to assess generalizability of perceptual differences
2. Test the collaboration method with different threshold strategies and more sophisticated human-AI interaction models
3. Conduct ablation studies to isolate the contribution of individual robust learning methods to the observed performance patterns