---
ver: rpa2
title: Adversarial Conversational Shaping for Intelligent Agents
arxiv_id: '2307.11785'
source_url: https://arxiv.org/abs/2307.11785
tags:
- learning
- adversarial
- language
- text
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper evaluates two GAN-based methods for improving dialogue
  generation in chatbots: GAN with Policy Gradient (GANPG) and GAN with Reward for
  Every Generation Step (REGS). Both use reinforcement learning with a discriminator
  providing rewards.'
---

# Adversarial Conversational Shaping for Intelligent Agents

## Quick Facts
- arXiv ID: 2307.11785
- Source URL: https://arxiv.org/abs/2307.11785
- Authors: 
- Reference count: 40
- Key outcome: Pre-trained T5 outperforms seq2seq with attention in dialogue generation, with lowest adversarial accuracy (68%) indicating better human-likeness

## Executive Summary
This paper evaluates two GAN-based methods for improving dialogue generation in chatbots: GAN with Policy Gradient (GANPG) and GAN with Reward for Every Generation Step (REGS). Both methods use reinforcement learning with a discriminator providing rewards to shape generator outputs. The study compares these approaches against a pre-trained baseline using both seq2seq with attention and T5 transformers as generators. Results demonstrate that pre-trained T5 significantly outperforms all other configurations, with the lowest adversarial accuracy indicating better human-likeness.

## Method Summary
The paper implements two GAN-based approaches for dialogue generation: GANPG uses policy gradients to optimize generator parameters based on discriminator rewards, while REGS assigns rewards to both partially and fully generated sequences. Both methods start with pre-training generators using maximum likelihood estimation, then fine-tune using reinforcement learning. The study compares seq2seq with attention and T5 transformers as generator architectures, evaluating performance using adversarial accuracy, BLEU scores, and diversity metrics across the DailyDialog dataset.

## Key Results
- Pre-trained T5 outperforms all other configurations with adversarial accuracy of 68%
- All T5-based models outperform seq2seq variants in both quality and human-likeness
- GAN training improves seq2seq more than T5, suggesting T5 is already powerful enough for the task
- Lower adversarial accuracy correlates with better human-likeness across all model configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained T5 outperforms seq2seq because it encodes rich linguistic priors, reducing the need for adversarial shaping
- Mechanism: Pre-training on diverse text tasks compresses broad language patterns into encoder-decoder weights, making it harder for discriminators to find exploitable weaknesses
- Core assumption: Pre-training provides generalizable linguistic representations that adversarial training cannot easily distort
- Evidence anchors:
  - Abstract states "pre-trained T5 outperforms all other configurations, with the lowest adversarial accuracy (68%) indicating better human-likeness"
  - T5 is described as a pre-trained encoder-decoder model converting tasks into human-language-like formats
  - No directly related paper cites this specific pre-training advantage

### Mechanism 2
- Claim: REGS improves seq2seq more than T5 because seq2seq lacks transformer-level context modeling
- Mechanism: REGS assigns rewards to partially generated sequences, guiding token-by-token generation; seq2seq benefits more from this stepwise feedback due to weaker context modeling
- Core assumption: Seq2seq models have weaker context modeling compared to transformers
- Evidence anchors:
  - Abstract notes "GAN training improves seq2seq more than T5, suggesting T5 is already powerful enough for the task"
  - REGS described as assigning rewards to partially and fully generated text sequences
  - No explicit comparisons of stepwise reward impact on seq2seq vs transformer models

### Mechanism 3
- Claim: Teacher forcing accelerates convergence but causes exposure bias, harming real-world generation quality especially in seq2seq
- Mechanism: Teacher forcing reduces hidden state error accumulation during training but creates mismatch at inference when model must rely on its own predictions
- Core assumption: Exposure bias is more pronounced in seq2seq due to reliance on sequential decoding without transformer-level self-attention
- Evidence anchors:
  - Section states "Teacher Forcing is widely used... This method will enforce a faster converging training as the hidden states of the model are not updated with wrong prediction sequences anymore, nonetheless, the issue of Exposure Bias occurs"
  - Abstract implies seq2seq variants lag behind T5, possibly due to exposure bias effects
  - No explicit mention of exposure bias in related works

## Foundational Learning

- Concept: Reinforcement learning with policy gradients
  - Why needed here: Discriminator provides reward signal requiring generator to optimize expected reward via gradient ascent
  - Quick check question: What equation defines the policy gradient update in GANPG? (Answer: ∇J(θ) ≈ [Q+({x, y}) − b({x, y})]∇ log π(y | x))

- Concept: Attention mechanisms in seq2seq
  - Why needed here: Seq2seq models use attention to focus on relevant input tokens during decoding; understanding limitations explains T5's superiority
  - Quick check question: How does the attention vector at decoding step i depend on encoder states? (Answer: It is a weighted sum of encoder hidden states, with weights computed from current decoder state)

- Concept: Text-to-text transfer learning
  - Why needed here: T5 reformulates all tasks as text-to-text, enabling unified fine-tuning and consistent loss computation
  - Quick check question: What is the advantage of converting every task into text-to-text format in T5? (Answer: It allows the same model, loss, and training procedure to be used across tasks, simplifying transfer)

## Architecture Onboarding

- Component map: Generator (seq2seq or T5) -> Discriminator (binary classifier or sequence scorer) -> Reward module -> Generator update via policy gradient

- Critical path:
  1. Pre-train generator on MLE
  2. Initialize discriminator (T5 or binary classifier)
  3. Generate responses, compute rewards
  4. Update generator via policy gradient
  5. Update discriminator on human vs. machine pairs
  6. Repeat until convergence or performance plateau

- Design tradeoffs:
  - Seq2seq + REGS: Lower baseline performance but larger gains from stepwise rewards; more sensitive to discriminator noise
  - T5 + GANPG: High baseline, smaller gains; less prone to overfitting to reward hacks
  - Teacher forcing: Faster convergence but risk of exposure bias
  - Discriminator architecture: Binary classifier simpler but may lack nuanced reward signals vs. sequence scorer

- Failure signatures:
  - Discriminator collapse: If discriminator becomes too strong, generator gradients vanish
  - Mode collapse: Generator produces repetitive outputs; check DIST-1/DIST-2 diversity metrics
  - Exposure bias: Free-running generation quality drops sharply vs. teacher-forced; measure BLEU on held-out seq2seq outputs

- First 3 experiments:
  1. Train seq2seq with attention on DailyDialog using MLE only; measure BLEU, DIST-1, adversarial accuracy
  2. Replace discriminator with binary classifier; run GANPG for 10 epochs; compare metrics to experiment 1
  3. Switch generator to T5; repeat experiment 2; verify lower adversarial accuracy and higher human-likeness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of T5-based GAN models compare to other state-of-the-art text generation models beyond seq2seq and pretrained baselines?
- Basis in paper: The paper mentions T5 outperforms seq2seq variants but doesn't compare against other state-of-the-art models like GPT or BERT-based approaches
- Why unresolved: The experimental comparison is limited to seq2seq and T5 models only
- What evidence would resolve it: Direct comparison experiments between T5-based GAN models and other state-of-the-art models like GPT, BERT, or more recent transformer architectures on the same datasets

### Open Question 2
- Question: What specific mechanisms in T5 make it more resistant to discriminator exploitation compared to seq2seq models in GAN training?
- Basis in paper: The paper notes that T5 performs better than seq2seq and that "the discriminator might easily cheat by finding hacks" with T5
- Why unresolved: The paper doesn't investigate the architectural or training differences that contribute to this behavior
- What evidence would resolve it: Detailed analysis of discriminator behavior patterns across different architectures, including error analysis of what specific features the discriminator learns to exploit in each case

### Open Question 3
- Question: How does the diversity of generated responses change across different training epochs and what is the optimal stopping point for maximum human-likeness?
- Basis in paper: The paper mentions diversity metrics (DIST-1, DIST-2) but doesn't analyze their evolution during training or their relationship to human-likeness
- Why unresolved: The paper only reports final diversity scores without temporal analysis or optimization of training duration
- What evidence would resolve it: Longitudinal study tracking diversity metrics and human evaluation scores across training epochs to identify optimal stopping points

### Open Question 4
- Question: How do different reward functions (beyond binary classification) affect the quality and diversity of generated responses in the REGS framework?
- Basis in paper: The paper mentions that the discriminator assigns rewards to partially and fully generated sequences but doesn't explore alternative reward formulations
- Why unresolved: The experimental setup uses a standard discriminator-based reward without exploring other reward design possibilities
- What evidence would resolve it: Comparative experiments using different reward functions such as mutual information-based rewards, semantic similarity rewards, or multi-objective reward combinations

## Limitations
- Evaluation framework relies heavily on adversarial accuracy as proxy for human-likeness without direct human evaluation studies
- Study uses single dataset (DailyDialog) without examining performance across diverse conversational domains or languages
- Computational costs and training stability of GAN-based methods versus simpler fine-tuning approaches are not thoroughly discussed

## Confidence

**High Confidence**: T5's superior performance over seq2seq configurations is well-supported by quantitative metrics across multiple evaluation criteria

**Medium Confidence**: The claim that pre-training provides sufficient linguistic priors to reduce the need for adversarial shaping is plausible but relies on indirect evidence about representation quality

**Medium Confidence**: The assertion that REGS improves seq2seq more than T5 due to transformer's superior context modeling is reasonable but lacks direct comparative analysis of stepwise reward impacts

## Next Checks

1. Conduct human evaluation studies comparing T5-generated responses against seq2seq outputs to validate that lower adversarial accuracy correlates with higher human-likeness ratings across diverse conversation topics

2. Perform ablation studies testing T5 performance with and without adversarial shaping to quantify the marginal benefit of GAN training on an already powerful pre-trained model

3. Test both methods on multiple conversational datasets (e.g., Cornell Movie Dialogs, PersonaChat) to verify whether observed performance patterns generalize beyond DailyDialog