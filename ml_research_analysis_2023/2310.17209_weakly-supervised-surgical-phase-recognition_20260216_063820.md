---
ver: rpa2
title: Weakly-Supervised Surgical Phase Recognition
arxiv_id: '2310.17209'
source_url: https://arxiv.org/abs/2310.17209
tags:
- phase
- recognition
- videos
- surgical
- supervision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses surgical phase recognition in laparoscopic
  videos under weakly-supervised settings where full frame-wise annotations are unavailable.
  The core method reformulates phase recognition as a graph segmentation problem solved
  via random walk with a graph Laplacian.
---

# Weakly-Supervised Surgical Phase Recognition

## Quick Facts
- arXiv ID: 2310.17209
- Source URL: https://arxiv.org/abs/2310.17209
- Reference count: 13
- One-line primary result: Graph-based random walk with sparse supervision achieves strong surgical phase recognition on Cholec80

## Executive Summary
This work introduces a graph-based method for surgical phase recognition using weak supervision. The approach reformulates phase recognition as a graph segmentation problem solved via random walk with graph Laplacian, enabling accurate phase segmentation without dense frame-level annotations. The method demonstrates strong performance on the Cholec80 dataset, achieving up to 90% accuracy with only 5 timestamps per phase or 75% accuracy with just 15 labeled training videos.

## Method Summary
The method reformulates surgical phase recognition as graph segmentation solved via random walk with graph Laplacian. Two forms of weak supervision are incorporated: sparse timestamps where only a few labeled frames per phase are provided at test time, and few-shot learning where a small set of fully labeled training videos is used to build spatial-temporal priors. Features from a self-supervised vision transformer are used to construct the graph, with edge weights based on feature similarity. The solution is obtained by solving a sparse linear system per phase, making the method efficient and interpretable.

## Key Results
- With 1 timestamp per phase: 72.6% accuracy and 97.24 F1@10
- With 5 timestamps per phase: 90% accuracy and 100 F1@10
- With 15 labeled training videos: 75% accuracy and 65 F1@10

## Why This Works (Mechanism)

### Mechanism 1
Graph segmentation with random walk enforces smoothness of phase predictions over time. The graph Laplacian matrix encodes temporal similarity between frames, and solving Lxs + γ(xs - zs) = 0 yields a smooth probability distribution over phases. Edge weights wij are large for similar frames and small for dissimilar frames, so the random walk naturally diffuses phase probabilities.

### Mechanism 2
Weak supervision via sparse timestamps or few-shot priors regularizes the graph solution without requiring dense annotations. In sparse timestamps, labeled frames zs act as hard constraints that the random walk smooths across the graph. In few-shot, spatial-temporal priors from a small labeled dataset guide the solution. Even a handful of labeled frames or feature statistics from a small dataset provide enough signal to bootstrap the segmentation.

### Mechanism 3
Self-supervised vision transformer features capture rich temporal structure needed for phase recognition without manual annotation. Features F from a DINO-trained ViT encode visual and temporal patterns across surgical phases, enabling meaningful edge weights wij. DINO self-supervision on ImageNet transfers to laparoscopic video domain after fine-tuning, providing strong representations for graph construction.

## Foundational Learning

- **Graph Laplacian and random walk segmentation**
  - Why needed here: Provides a principled way to propagate sparse labels across frames while preserving temporal smoothness
  - Quick check question: What does the graph Laplacian encode in this setting, and why is solving Lxs = -γ(xs - zs) equivalent to a random walk?

- **Weak supervision strategies**
  - Why needed here: Avoids expensive frame-level annotations while still guiding the model
  - Quick check question: How do sparse timestamps differ from few-shot spatial-temporal priors in terms of information provided to the model?

- **Self-supervised vision transformer features**
  - Why needed here: Supplies rich, transferable representations without manual labels, critical for low-data regimes
  - Quick check question: Why might DINO-pretrained ViT features transfer better to surgical videos than ImageNet-supervised features?

## Architecture Onboarding

- **Component map:**
  Feature extractor (DINO-pretrained ViT-S → fine-tuned on Cholec80) → Graph builder (frame nodes, edges with weights wij) → Random walk solver (solves sparse linear systems per phase) → Supervision module (sparse timestamp vector zs or few-shot spatial-temporal prior) → Prediction layer (argmax over phases for each frame)

- **Critical path:**
  1. Extract features F from test video
  2. Build graph G with edges and Laplacian L
  3. Construct prior vector zs (sparse or few-shot)
  4. Solve (L + γI)xs = γzs per phase
  5. Take argmax to get frame labels

- **Design tradeoffs:**
  - Dense vs sparse graph: denser graphs capture more transitions but increase computation
  - Feature extractor choice: stronger features yield better edge weights but may need more compute
  - Prior strength γ: higher γ enforces prior more strongly but may reduce smoothness

- **Failure signatures:**
  - Uniform or noisy predictions: likely feature extractor or edge weight issue
  - Predictions ignore timestamps: γ too low or timestamps placed incorrectly
  - Slow convergence: Laplacian poorly conditioned or graph too dense

- **First 3 experiments:**
  1. Run with γ=0 (no prior) to confirm random walk alone is weak
  2. Test with synthetic sparse timestamps on a small labeled subset to validate prior injection
  3. Compare DINO features vs random features to show importance of good representations

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the method perform with different self-supervised feature extractors (e.g., other ViT variants or architectures like MAE)?
- **Open Question 2:** What is the impact of using more than 10 timestamps per phase in the sparse supervision setup?
- **Open Question 3:** How does the method generalize to other surgical procedures beyond laparoscopic cholecystectomy?
- **Open Question 4:** What is the computational complexity and inference time compared to end-to-end deep learning approaches?

## Limitations

- Exact graph Laplacian construction details and hyperparameter choices (β, γ) are not fully specified
- Performance in cross-dataset settings is untested; generalization to other surgical procedures unknown
- Computational cost and scalability for long videos or large datasets are not discussed

## Confidence

- High confidence in the core mechanism: Graph-based random walk segmentation with Laplacian regularization is a well-established method in computer vision
- Medium confidence in the weak supervision approach: Novel combination of sparse timestamps and few-shot learning with graph segmentation requires empirical validation
- Medium confidence in feature extraction: DINO-pretrained ViT features show promise but effectiveness for surgical video understanding needs further verification

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Systematically vary β and γ to understand their impact on graph construction and phase prediction accuracy
2. **Cross-Dataset Generalization:** Evaluate the method on a different surgical video dataset (e.g., M2CAI or EndoVis) to assess its ability to generalize beyond Cholec80
3. **Feature Ablation Study:** Compare DINO-pretrained ViT features against other feature extractors (e.g., ResNet, supervised ViT) to quantify the contribution of self-supervised learning to the overall performance