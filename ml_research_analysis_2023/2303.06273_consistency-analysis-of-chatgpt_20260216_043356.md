---
ver: rpa2
title: Consistency Analysis of ChatGPT
arxiv_id: '2303.06273'
source_url: https://arxiv.org/abs/2303.06273
tags:
- consistency
- chatgpt
- language
- negation
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the logical consistency of ChatGPT and
  GPT-4 across semantic, negation, and symmetric consistency types. Using the BECEL
  dataset, the study evaluates model behavior on SNLI, RTE, and MRPC tasks.
---

# Consistency Analysis of ChatGPT

## Quick Facts
- arXiv ID: 2303.06273
- Source URL: https://arxiv.org/abs/2303.06273
- Reference count: 16
- Key outcome: ChatGPT exhibits significant logical inconsistency across semantic, negation, and symmetric consistency types, producing different outputs for paraphrased inputs and showing high sensitivity to input order.

## Executive Summary
This paper investigates the logical consistency of ChatGPT and GPT-4 across three types: semantic, negation, and symmetric consistency. Using the BECEL dataset, the study evaluates model behavior on SNLI, RTE, and MRPC tasks. Results demonstrate that ChatGPT frequently produces inconsistent outputs for paraphrased inputs (including self-generated paraphrases), is highly sensitive to input order for symmetric tasks, and violates negation consistency despite improved handling of negation expressions compared to earlier PLMs. The study concludes that while ChatGPT shows advancements in certain areas, it cannot be fully trusted without human oversight, particularly in high-risk applications.

## Method Summary
The study employs zero-shot evaluation using the BECEL dataset, which contains test sets for semantic, negation, and symmetric consistency evaluation across SNLI, RTE, and MRPC tasks. The researchers sample 200 data points per task and generate predictions for original test sets and perturbed versions (paraphrased, negated, or reordered inputs) using ChatGPT with prompts from Eleuther AI. Inconsistency rates are calculated by comparing predictions between original and perturbed inputs for each consistency type, with a low threshold used to judge predictions due to ChatGPT's relatively low zero-shot accuracy compared to fine-tuned models.

## Key Results
- ChatGPT frequently produces different outputs for paraphrased inputs, including self-generated paraphrases, violating semantic consistency
- The model shows high sensitivity to input order for symmetric tasks like semantic textual similarity, violating symmetric consistency
- While ChatGPT demonstrates improved handling of negation expressions compared to earlier PLMs, it still violates negation consistency by sometimes producing the same output for negated and non-negated inputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT's inconsistency is partly due to its inability to reliably understand and preserve semantic equivalence across paraphrased inputs.
- Mechanism: The model generates different predictions for inputs that are semantically equivalent, including paraphrases it generates itself, indicating a failure in semantic consistency.
- Core assumption: Semantic consistency should hold unconditionally for any text input conveying the same meaning.
- Evidence anchors:
  - [abstract] "it frequently produces different outputs for paraphrased inputs (including self-generated paraphrases)"
  - [section] "ChatGPT is self-contradictory, meaning that it violates semantic consistency for paraphrased inputs generated by ChatGPT itself."
- Break condition: The mechanism breaks if the model can reliably maintain consistent predictions across all paraphrased versions of the same input, including self-generated paraphrases.

### Mechanism 2
- Claim: ChatGPT shows improved handling of negation expressions compared to earlier PLMs, but still violates negation consistency.
- Mechanism: While ChatGPT better understands negation and antonyms, it sometimes produces the same output for negated and non-negated inputs, violating logical negation consistency.
- Core assumption: Negation consistency should hold when the gold label is "Entailment" (for NLI) or "Equivalent" (for STS).
- Evidence anchors:
  - [abstract] "While ChatGPT demonstrates improved handling of negation expressions compared to earlier PLMs, its overall consistency falls short of expectations."
  - [section] "ChatGPT can better understand negation expressions and antonyms, which has been a critical issue for PLMs trained in a self-supervised fashion"
- Break condition: The mechanism breaks if ChatGPT consistently produces different predictions for negated vs. non-negated inputs across all test cases.

### Mechanism 3
- Claim: ChatGPT is highly sensitive to input order for tasks where order should not matter, violating symmetric consistency.
- Mechanism: For symmetric tasks like semantic textual similarity, ChatGPT generates different predictions when the order of input sentences is switched.
- Core assumption: Symmetric consistency should hold unconditionally for tasks where the order of inputs does not affect the semantic meaning.
- Evidence anchors:
  - [abstract] "it frequently produces different outputs for paraphrased inputs... and is highly sensitive to input order"
  - [section] "ChatGPT is extremely sensitive to the input sentence order for order-invariant tasks, e.g., semantic textual similarity (STS)"
- Break condition: The mechanism breaks if ChatGPT consistently produces the same predictions regardless of input order for all symmetric tasks.

## Foundational Learning

- Concept: Semantic consistency
  - Why needed here: Understanding semantic consistency is crucial for evaluating ChatGPT's reliability across paraphrased inputs and ensuring consistent behavior in NLP applications.
  - Quick check question: What is the difference between semantic consistency and negation consistency in the context of language models?

- Concept: Negation expressions and antonyms
  - Why needed here: Grasping how negation and antonyms work is essential for understanding ChatGPT's improved but still imperfect handling of negated inputs compared to earlier PLMs.
  - Quick check question: How does a language model's understanding of negation expressions affect its performance on tasks involving logical consistency?

- Concept: Symmetric consistency
  - Why needed here: Symmetric consistency is important for tasks where input order should not affect the output, and its violation by ChatGPT impacts its reliability in certain applications.
  - Quick check question: Why is symmetric consistency particularly important for tasks like semantic textual similarity (STS)?

## Architecture Onboarding

- Component map: Input text → Semantic analysis → Consistency check (semantic, negation, symmetric) → Prediction generation
- Critical path: Input text → Semantic understanding → Consistency checking → Output generation
- Design tradeoffs:
  - Balancing between maintaining consistency and allowing for nuanced responses
  - Tradeoff between model size/complexity and consistency performance
  - Handling of rare or complex linguistic constructs vs. overall consistency
- Failure signatures:
  - Inconsistent predictions for semantically equivalent inputs
  - Same predictions for negated vs. non-negated inputs when they should differ
  - Different predictions based on input order for symmetric tasks
- First 3 experiments:
  1. Test semantic consistency by inputting paraphrased versions of the same sentence and comparing predictions
  2. Evaluate negation consistency by providing negated and non-negated versions of the same input and checking for consistent differences in output
  3. Assess symmetric consistency by switching the order of inputs in symmetric tasks and comparing the resulting predictions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can prompt design fundamentally resolve consistency issues in large language models?
- Basis in paper: [explicit] The paper states "We are sceptical of this claim" regarding prompt design as a solution, noting that "prompt design cannot go beyond inductive reasoning" and "cannot fundamentally resolve the inconsistency problem."
- Why unresolved: The paper presents arguments against prompt design as a solution but does not conduct experiments to test this hypothesis.
- What evidence would resolve it: Empirical comparison of consistency performance between models using various prompt designs versus baseline models, with statistical significance testing.

### Open Question 2
- Question: What is the specific impact of human feedback incorporation on negation understanding in large language models?
- Basis in paper: [explicit] The paper notes that ChatGPT "can better understand negation expressions and antonyms" compared to previous PLMs and suggests "incorporating human feedback into ChatGPT training plays a crucial role in learning the meaning of negation expressions."
- Why unresolved: While the paper suggests human feedback as a contributing factor, it does not isolate or quantify this effect through controlled experiments.
- What evidence would resolve it: Controlled experiments comparing negation consistency between models trained with and without human feedback, using identical architectures and training data otherwise.

### Open Question 3
- Question: What is the relationship between model size and consistency performance in large language models?
- Basis in paper: [inferred] The paper notes that "training an LLM entails tremendous financial and environmental costs" and mentions that "ChatGPT is 1590 times larger than BERT-base," yet doesn't directly investigate the correlation between size and consistency.
- Why unresolved: The paper observes consistency issues in ChatGPT but doesn't systematically examine how these issues scale with model size across different model families.
- What evidence would resolve it: Comparative analysis of consistency metrics across multiple model sizes within the same architecture family, controlling for other variables.

## Limitations
- The study relies on zero-shot evaluation without fine-tuning, which may overestimate inconsistency rates when models make incorrect predictions on original inputs
- The paper lacks comprehensive benchmarking against GPT-4 and other PLMs, limiting quantitative comparisons
- Findings focus on English-language tasks and may not generalize to other languages or specialized domains

## Confidence
- Overall claim about ChatGPT's inconsistency: Medium
- Specific quantitative comparisons with GPT-4 and other PLMs: Low

## Next Checks
1. Conduct a comparative study using fine-tuned models on the same tasks to establish whether zero-shot inconsistency is indeed an artifact of the evaluation method
2. Perform ablation studies to identify which architectural components (attention mechanisms, tokenization, etc.) contribute most to consistency violations
3. Test the model's consistency across multiple domains and languages to assess generalizability of the findings