---
ver: rpa2
title: Distance Preserving Machine Learning for Uncertainty Aware Accelerator Capacitance
  Predictions
arxiv_id: '2307.02367'
source_url: https://arxiv.org/abs/2307.02367
tags: []
core_contribution: This paper introduces a distance-preserving deep learning approach
  for uncertainty-aware capacitance predictions in accelerator systems, specifically
  for the High Voltage Converter Modulators at the Oak Ridge Spallation Neutron Source.
  The authors address the challenge of maintaining distance information when performing
  dimensionality reduction for Gaussian process models, which is critical for accurate
  uncertainty estimation in safety-critical applications.
---

# Distance Preserving Machine Learning for Uncertainty Aware Accelerator Capacitance Predictions

## Quick Facts
- arXiv ID: 2307.02367
- Source URL: https://arxiv.org/abs/2307.02367
- Reference count: 11
- Primary result: SVD-based DNGPA achieves <1% error on in-distribution capacitance predictions with superior distance preservation compared to standard methods

## Executive Summary
This paper addresses the challenge of uncertainty-aware capacitance prediction in accelerator systems by introducing a distance-preserving deep learning approach. The method combines singular value decomposition (SVD) with deep neural Gaussian process approximation (DNGPA) to maintain pairwise distances between input samples during dimensionality reduction, which is critical for accurate uncertainty estimation in safety-critical applications. The approach is evaluated on HVCM capacitance prediction at the Oak Ridge Spallation Neutron Source, showing improved performance over standard DNGPA, Bayesian neural networks, and deep quantile regression models, particularly for out-of-distribution predictions.

## Method Summary
The proposed method uses SVD-based feature extraction to preserve distances between input samples and latent representations, which is essential for Gaussian process models that rely on distance metrics in their kernel functions. The SVD layer is followed by a 5-layer residual network with dropout, a random Fourier features layer, and a dense output layer that feeds into Gaussian process approximation for uncertainty quantification. This architecture is compared against three alternative approaches: standard DNGPA with spectral-normalized dense layers, Bayesian neural networks with MC-dropout, and deep quantile regression models. All models are trained and tested on simulated HVCM waveforms with capacitance values in two ranges (2500-2800 pF for out-of-distribution testing and 2900-4000 pF for in-distribution training and testing).

## Key Results
- In-distribution predictions achieve R² > 0.8 and RMSE < 10 pF, well within 10% manufacturer tolerance
- SVD-DNGPA maintains superior distance preservation between input and latent spaces compared to standard spectral-normalized dense layers
- For out-of-distribution testing, SVD-DNGPA shows RMSE values 2-4 times smaller than competing approaches
- Distance preservation in SVD-DNGPA enables better uncertainty quantification, as evidenced by improved RMSCE and MACE metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distance preservation between input space and latent space improves uncertainty quantification accuracy in Gaussian process models.
- Mechanism: When performing dimensionality reduction for Gaussian process models, maintaining the Euclidean distances between samples ensures that the covariance kernel correctly reflects the true similarity between data points. The singular value decomposition (SVD) provides orthonormal weight vectors that preserve distances more effectively than spectral-normalized dense layers.
- Core assumption: The Euclidean distance in the original high-dimensional space meaningfully captures the relationship between samples that should be preserved in the reduced dimensional space.
- Evidence anchors:
  - [abstract] "Combining deep neural networks with Gaussian process approximation techniques have shown promising results, but dimensionality reduction through standard deep neural network layers is not guaranteed to maintain the distance information necessary for Gaussian process models."
  - [section 3.4.1] "GP rely on the distance between inputs to accurately represent uncertainties due to their inclusion in the kernel function"
- Break condition: When the underlying data manifold is highly non-linear or when Euclidean distance is not the appropriate metric for the problem domain.

### Mechanism 2
- Claim: SVD provides stronger distance preservation guarantees than spectral normalization when reducing dimensionality.
- Mechanism: The SVD produces orthonormal weight vectors where both the spectral norm and the smallest singular value equal 1, creating a tighter bound on distance preservation compared to spectral normalization which only constrains the largest singular value.
- Core assumption: The data matrix is approximately low-rank, making the truncated singular values small enough to provide meaningful distance preservation.
- Evidence anchors:
  - [section 3.4.2] "Unlike a spectral normalized dense layer, these methods produce orthonormal weight vectors from the columns of V which guarantees both the spectral norm and the smallest singular value of the weight matrix are 1."
  - [section 3.4.2] "Given a sample x (i.e. row of X), we have ∥x∥ − √(Σ(n-j=k+1) σ²j) ≤ ∥xW∥ ≤ ∥x∥"
- Break condition: When the data matrix is not approximately low-rank or when the dimensionality reduction ratio is too extreme.

### Mechanism 3
- Claim: Distance-aware deep neural Gaussian process approximation improves out-of-distribution generalization.
- Mechanism: By maintaining distance relationships from input to latent space, the model can better recognize when test samples are far from the training distribution, leading to more calibrated uncertainty estimates for out-of-distribution data.
- Core assumption: The distance in the latent space appropriately reflects the true distance from the training distribution.
- Evidence anchors:
  - [abstract] "Our model shows improved distance preservation and predicts in-distribution capacitance values with less than 1% error."
  - [section 4.3] "SVD-DNGPA still provides reasonable results with an R² value greater than 0.8 and an RMSE that was two to four times smaller than all of the other models we tested."
- Break condition: When the input-to-latent distance relationship becomes distorted due to model architecture choices or when the training data is insufficient to capture the true data distribution.

## Foundational Learning

- Concept: Gaussian Process Uncertainty Quantification
  - Why needed here: Gaussian processes are the gold standard for uncertainty estimation in safety-critical applications like accelerator systems, but they struggle with large, high-dimensional datasets that are common in accelerator systems.
  - Quick check question: Why can't we use standard Gaussian processes directly on the 7 waveforms with 5261 timesteps each?

- Concept: Dimensionality Reduction Techniques
  - Why needed here: The high-dimensional input data (7 waveforms × 5261 timesteps = 36,827 features) must be reduced to a manageable latent space size for Gaussian process approximation, but standard reduction methods may distort distance relationships critical for uncertainty estimation.
  - Quick check question: What mathematical property of the SVD ensures that it preserves distances better than standard dense layers?

- Concept: Distance Preservation in Neural Networks
  - Why needed here: The neural network must maintain bi-Lipschitz constraints (L1∥x1 − x2∥ ≤ ∥h(x1) − h(x2)∥ ≤ L2∥x1 − x2∥) between input and latent spaces to ensure that the Gaussian process uncertainty estimates remain well-calibrated.
  - Quick check question: How does spectral normalization in residual networks help maintain distance preservation compared to standard dense layers?

## Architecture Onboarding

- Component map: Input layer → SVD feature extraction (64 dimensions) → 5-layer residual network with dropout → random Fourier features layer (128 features) → dense output layer → Gaussian process approximation for uncertainty
- Critical path: SVD → ResNet → RFF → Dense → GP approximation. The SVD layer is critical because it provides the distance preservation that enables accurate uncertainty quantification.
- Design tradeoffs: SVD provides stronger distance preservation guarantees but requires computing the SVD on the entire training dataset upfront, which can be computationally expensive for very large datasets. Spectral normalization is cheaper but provides weaker guarantees.
- Failure signatures: Poor uncertainty calibration (large RMSCE/MACE values), degraded performance on out-of-distribution data, or correlations between input and latent distances that deviate significantly from the ideal line in Figure 5.
- First 3 experiments:
  1. Compare distance preservation by plotting input vs. latent distances for SVD vs. spectral-normalized dense layer on a small subset of training data.
  2. Measure uncertainty calibration on in-distribution test set by computing RMSCE and MACE for both SVD-DNGPA and standard DNGPA.
  3. Test OOD generalization by evaluating RMSE and uncertainty calibration on the 2800pF and below test set and comparing to in-distribution performance.

## Open Questions the Paper Calls Out

- Question: How would the SVD-DNGPA model perform on real HVCM data compared to the simulated data used in this study?
  - Basis in paper: [explicit] The authors mention they hope to obtain real labeled data from a single-phase low voltage system similar to the three-phase high voltage SNS HVCM for future validation.
  - Why unresolved: The current results are based on synthetic data from LTSpice simulations, and the authors acknowledge that real data does not always map directly to simulated data.
  - What evidence would resolve it: Obtaining and testing the model on real HVCM data from the SNS would provide concrete evidence of its performance in practical applications.

- Question: Would alternative kernel functions beyond the RBF kernel improve the performance of the DNGPA models?
  - Basis in paper: [explicit] The authors suggest investigating alternative kernel options beyond RBF and whether learned RBF parameters are similar to those learned through exact GP regression techniques.
  - Why unresolved: The current study uses the RBF kernel, and the authors express interest in exploring other kernel options for potential improvements.
  - What evidence would resolve it: Testing the DNGPA models with different kernel functions and comparing their performance metrics (e.g., R², RMSE) would provide insights into the impact of kernel choice.

- Question: How would the SVD-DNGPA model handle high-dimensional data in non-linear spaces with low rank?
  - Basis in paper: [explicit] The authors mention that many problems lie in non-linear spaces with low rank and express interest in investigating how these problems could be solved with other approximate isometric maps like Isomap or other algorithms.
  - Why unresolved: The current study focuses on linear dimensionality reduction using SVD, and the authors acknowledge the need to explore non-linear methods for more complex data structures.
  - What evidence would resolve it: Applying the SVD-DNGPA model to high-dimensional data in non-linear spaces and comparing its performance with other dimensionality reduction techniques would provide insights into its effectiveness in such scenarios.

## Limitations

- The computational cost of computing SVD on large datasets may limit scalability for real-time applications
- The specific tuning parameters for the LULU filter and waveform cleaning are not fully specified, which could affect reproducibility
- The distance preservation mechanism's effectiveness depends heavily on the data matrix being approximately low-rank, which may not hold for all accelerator waveform datasets

## Confidence

- **High confidence**: In-distribution prediction accuracy and the fundamental distance preservation mechanism of SVD
- **Medium confidence**: Out-of-distribution generalization performance, as the test set was relatively small (64 samples)
- **Low confidence**: Scalability claims for larger, more complex accelerator systems without further validation

## Next Checks

1. Test distance preservation across different capacitor value ranges and waveform types to verify the low-rank assumption holds broadly
2. Measure computational overhead of SVD computation during training and inference to assess real-time applicability
3. Validate uncertainty calibration on additional OOD test sets with systematically varied parameters (different IGBT timing, voltage settings) to test robustness of the distance-aware approach