---
ver: rpa2
title: Investigating the Effects of Fairness Interventions Using Pointwise Representational
  Similarity
arxiv_id: '2305.19294'
source_url: https://arxiv.org/abs/2305.19294
tags:
- pnka
- points
- similarity
- average
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PNKA, a pointwise representation similarity
  measure that addresses the limitations of existing global similarity metrics by
  providing fine-grained, per-input similarity scores. PNKA measures how similarly
  individual inputs are represented across different models by comparing their neighborhoods
  in representation space, making it invariant to orthogonal transformations and isotropic
  scaling.
---

# Investigating the Effects of Fairness Interventions Using Pointwise Representational Similarity

## Quick Facts
- arXiv ID: 2305.19294
- Source URL: https://arxiv.org/abs/2305.19294
- Reference count: 40
- Primary result: PNKA reveals that debiasing methods amplify gender information in definitional words rather than reducing bias in stereotypical words

## Executive Summary
This paper introduces PNKA, a pointwise representation similarity measure that provides fine-grained, per-input similarity scores by comparing neighborhood structures in representation space. Unlike global measures like CKA, PNKA reveals that while aggregate similarity appears high between models, individual points show varying similarity scores, with low-similarity points more likely to be misclassified. The measure demonstrates particular utility in analyzing fairness interventions on word embeddings, uncovering that debiasing methods paradoxically amplify gender information in definitional words rather than reducing stereotypical bias, contrary to what aggregate measures suggest.

## Method Summary
PNKA computes pointwise similarity by measuring how similarly a point's neighborhood is positioned in two representation spaces using kernel matrices. For representations Y and Z, PNKA calculates the cosine similarity between rows of the kernel matrices K(Y) and K(Z), where each row represents the neighborhood of a point. This approach is invariant to orthogonal transformations and isotropic scaling, addressing key limitations of existing global similarity measures. The method aggregates these pointwise scores to produce both individual similarity measures and global similarity metrics.

## Key Results
- PNKA reveals high variance in per-point similarity between randomly initialized models, with low-similarity points more likely to be misclassified
- Debiasing interventions on word embeddings actually amplify gender information in definitional words rather than reducing stereotypical bias
- PNKA serves as an effective interpretability tool, revealing neuron specialization by measuring how individual neuron removal affects representation of different inputs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** PNKA measures pointwise similarity by comparing neighborhood overlap rather than direct representation comparison
- **Mechanism:** PNKA computes similarity by measuring how similarly a point's neighborhood is positioned in two representation spaces, using the cosine similarity of kernel matrices
- **Core assumption:** Points represented similarly across two spaces should have similar nearest neighbors in both spaces
- **Evidence anchors:** Abstract states PNKA measures similarity "by comparing their neighborhoods in representation space"; paper explains that "a point represented similarly across representations should have similar nearest neighbors in both representations"
- **Break condition:** If orthogonal transformations or isotropic scaling significantly affect neighborhood structure in ways that don't preserve semantic similarity

### Mechanism 2
- **Claim:** PNKA reveals hidden biases in debiasing interventions that global measures miss
- **Mechanism:** By providing per-input similarity scores, PNKA can detect that debiasing methods amplify gender information in definitional words rather than reducing bias in stereotypical words
- **Core assumption:** Local representation changes can reveal intervention effects that aggregate measures obscure
- **Evidence anchors:** Abstract states debiasing methods "amplify gender information in definitional words rather than reducing bias in stereotypical words, contrary to what aggregate measures suggest"; Figure 4 shows words with most changed embeddings are gender-definition words
- **Break condition:** If debiasing interventions affect all word types uniformly or if the intervention's effects are too subtle for PNKA to detect

### Mechanism 3
- **Claim:** PNKA can identify misclassified instances by correlating low similarity scores with prediction disagreement
- **Mechanism:** Points with low PNKA scores between models that differ only in random initialization are more likely to be misclassified by at least one model
- **Core assumption:** Dissimilar representations lead to divergent predictions, which increases misclassification probability
- **Evidence anchors:** Abstract states "low-similarity points more likely to be misclassified"; Figure 2b shows most dissimilarly represented points are those where models disagree on predictions
- **Break condition:** If models achieve perfect agreement on all test points or if other factors (like data distribution) dominate misclassification patterns

## Foundational Learning

- **Concept: Representation similarity measures**
  - Why needed here: Understanding how PNKA relates to existing measures like CKA is crucial for grasping its novelty and advantages
  - Quick check question: What key limitation of CKA does PNKA address that makes it more suitable for analyzing individual input effects?

- **Concept: Kernel methods and similarity matrices**
  - Why needed here: PNKA's core mechanism relies on computing and comparing kernel matrices to assess neighborhood similarity
  - Quick check question: How does the choice of kernel (linear vs RBF) potentially affect PNKA's ability to capture neighborhood relationships?

- **Concept: Orthogonal invariance in neural representations**
  - Why needed here: PNKA's design specifically maintains invariance to orthogonal transformations, which is critical for meaningful representation comparison
  - Quick check question: Why would direct comparison of representations (like cosine similarity) fail when representations differ by orthogonal transformations?

## Architecture Onboarding

- **Component map:** Kernel computation K(Y) and K(Z) -> pointwise similarity calculation PNKA(Y,Z,i) using cosine similarity between kernel rows -> aggregation function for global similarity score
- **Critical path:** The most performance-critical component is kernel computation, as it requires O(N²) operations for N points; this should be optimized first
- **Design tradeoffs:** PNKA trades computational efficiency (O(N²) complexity) for fine-grained insight; alternative designs could use approximate nearest neighbors to reduce complexity
- **Failure signatures:** PNKA may fail when: (1) representation spaces have fundamentally different structures, (2) neighborhood density varies dramatically across spaces, or (3) the kernel choice poorly captures semantic similarity
- **First 3 experiments:**
  1. Reproduce Figure 1 by training two ResNet-18 models with different random initializations on CIFAR-10 and computing PNKA scores vs neighbor overlap
  2. Test PNKA's invariance properties by applying random orthogonal transformations to representations and verifying score preservation
  3. Analyze debiasing effects by computing PNKA between GloVe and debiased embeddings across different word categories (definition vs stereotype)

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- PNKA's invariance properties need more rigorous testing across diverse transformation types beyond orthogonal transformations and isotropic scaling
- Debiasing analysis conclusions are based on a single dataset (SemBias) and may not generalize to other bias detection frameworks
- The relationship between PNKA scores and misclassification rates lacks statistical significance testing and could be influenced by confounding factors

## Confidence

- **High**: PNKA's mathematical formulation and basic invariance properties
- **Medium**: PNKA's effectiveness as a debiasing intervention analysis tool
- **Medium**: The correlation between low PNKA scores and misclassification

## Next Checks

1. Test PNKA's invariance properties across a broader range of transformations (affine, non-linear) and verify robustness to representation density variations

2. Validate debiasing analysis findings using multiple bias detection datasets (e.g., WEAT, StereoSet) and different embedding models (Word2Vec, FastText)

3. Conduct statistical analysis of the relationship between PNKA scores and misclassification, including controlled experiments varying model architectures and training procedures