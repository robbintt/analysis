---
ver: rpa2
title: Beyond XAI:Obstacles Towards Responsible AI
arxiv_id: '2309.03638'
source_url: https://arxiv.org/abs/2309.03638
tags:
- explanations
- visited
- explainability
- arxiv
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper discusses the limitations of Explainable Artificial Intelligence
  (XAI) and its implications for responsible AI. It highlights the challenges in developing
  faithful and reliable explanations for various machine learning models and the need
  to address broader dimensions such as privacy, fairness, and contestability.
---

# Beyond XAI:Obstacles Towards Responsible AI

## Quick Facts
- arXiv ID: 2309.03638
- Source URL: https://arxiv.org/abs/2309.03638
- Reference count: 40
- One-line primary result: The paper identifies key limitations of Explainable AI (XAI) and calls for a holistic approach to responsible AI that integrates explainability, fairness, privacy, and contestability.

## Executive Summary
The paper critically examines the limitations of Explainable Artificial Intelligence (XAI) and its role in achieving Responsible AI. It highlights that post-hoc XAI methods can be manipulated and may not faithfully represent model logic, that fairness evaluations via feature attribution often miss deeper sources of bias, and that user perceptions of explanation quality vary significantly by context. The authors argue that responsible AI must integrate multiple dimensions—privacy, fairness, and contestability—beyond mere explainability, and that technical, human, and contextual factors must be considered in tandem. The paper calls for more comprehensive research to understand and address the intricate relationships between explainability and other aspects of responsible AI.

## Method Summary
The paper employs a literature review and critical analysis approach, synthesizing existing research on XAI methods and their limitations. It examines the interactions between explainability and broader responsible AI dimensions such as privacy, fairness, and contestability, and identifies obstacles to achieving truly responsible AI. The analysis is based on theoretical discussions and critical evaluation of current XAI evaluation strategies, though specific experimental results or datasets are not provided.

## Key Results
- Post-hoc XAI methods can be manipulated via adversarial perturbations, leading to misleading explanations that do not reflect true model logic.
- Feature attribution-based fairness evaluations may miss systemic bias embedded in data and model selection, mirroring the "fairness through unawareness" approach.
- User perceptions of explanation quality vary by context, with different groups prioritizing different explanation features (e.g., faithfulness vs. compactness).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Post-hoc XAI methods can introduce manipulable explanations that appear faithful but do not reflect true model logic.
- Mechanism: Adversarial perturbations can be applied to inputs to force misleading feature attributions from methods like LIME and SHAP, causing users to trust inaccurate explanations.
- Core assumption: Post-hoc explanations approximate model behavior and can be influenced by input modifications without changing predictions.
- Evidence anchors:
  - [section] "recent research discovered that explanations can be susceptible to manipulation. By Applying visually imperceptible pertubations to the input image that keep the network's output approximately constant,one can manipulate generated explanations arbitrarily[14]."
  - [section] "Similarly, [39] demonstrate the possibility of modifying biased classiﬁers can easily fool popular explanation techniques such as LIME and SHAP into generating innocuous explanations which do not reﬂect the underlying biases."
  - [corpus] Weak match on related papers about XAI manipulation; no direct citations found in neighbors.
- Break condition: If explanations are verified with multiple independent methods and adversarial robustness testing, or if model architecture is inherently interpretable.

### Mechanism 2
- Claim: XAI fairness evaluation via feature attribution can miss systemic bias embedded in data and model selection.
- Mechanism: Feature attribution methods like SHAP compute demographic parity differences but cannot detect biases that arise from imbalanced training data, feature engineering, or model choice.
- Core assumption: Fairness is more than model output disparities; it requires tracing bias through data collection, preprocessing, and model optimization.
- Evidence anchors:
  - [section] "Feature-based XAI techniques such as SHAP, decompose the model output into feature attributions. This breakdown can be used to compute the quantitative fairness metric such as demographic parity difference for each input feature using the SHAP value. Such an examination via XAI can help detect implicit connections between protected and unprotected features[28]."
  - [section] "XAI techniques often check whether models recognize features tied to protected attributes instead of ensuring that the input data is devoid of biases concerning those attributes. This mirrors the 'fairness through unawareness' strategy[10]."
  - [corpus] No direct evidence in neighbors; related papers discuss fairness metrics but not this specific critique.
- Break condition: If bias mitigation strategies (preprocessing, in-processing, post-processing) are explicitly integrated and evaluated alongside explanations.

### Mechanism 3
- Claim: User perceptions of explanation quality vary significantly across contexts, making one-size-fits-all XAI ineffective.
- Mechanism: Different user groups (e.g., ML engineers vs. end users) prioritize different explanation qualities (faithfulness vs. compactness), leading to divergent needs and expectations.
- Core assumption: Explanation utility is context-dependent, shaped by user expertise, domain, and task goals.
- Evidence anchors:
  - [section] "Take for instance the 'faithfulness' feature, which assesses if the explanation accurately reflects the inner workings of the complex model, versus the 'compactness' feature, ensuring the explanation remains concise and not overwhelming[25]."
  - [section] "An ML engineer may prioritize faithful explanations for model debugging and development, while an end user might prefer concise explanations, even if they aren't as detailed[34]."
  - [section] "However, these human and contextual factors have not been fully understood yet, with different studies presenting varied findings."
  - [corpus] Weak evidence; neighbors discuss XAI but not specific user-centered quality trade-offs.
- Break condition: If explanations are tailored to user segments and evaluated in real-world application settings.

## Foundational Learning

- Concept: Post-hoc vs. interpretable model approaches in XAI
  - Why needed here: The paper distinguishes these as fundamental design choices with different trade-offs in fidelity, performance, and vulnerability.
  - Quick check question: Which XAI approach—post-hoc or interpretable models—generally retains model accuracy but may sacrifice explanation fidelity?
- Concept: Fairness metrics and their limitations
  - Why needed here: Understanding how XAI methods compute fairness metrics reveals why they may miss deeper sources of bias.
  - Quick check question: What is the limitation of feature attribution methods when evaluating fairness, as noted in the paper?
- Concept: Human-centered evaluation in HCI research
  - Why needed here: The paper highlights the gap between algorithmic fidelity and real-world user needs in explanation design.
  - Quick check question: According to the paper, what type of evaluation is needed to bridge the gap between technical XAI and user needs?

## Architecture Onboarding

- Component map:
  - Data preprocessing → Model training → Explanation generation (post-hoc or interpretable) → Explanation presentation → User interaction/evaluation
  - Fairness and privacy checks embedded at multiple stages
  - Contestability mechanisms linked to explanation quality and decision context
- Critical path:
  1. Data collection and bias detection
  2. Model training with privacy-preserving techniques if needed
  3. Explanation generation and adversarial robustness testing
  4. User-centered evaluation in real-world contexts
  5. Integration of contestability features
- Design tradeoffs:
  - Post-hoc explanations offer flexibility but are manipulable; interpretable models are stable but may underperform.
  - Detailed explanations aid debugging but overwhelm end users; compact explanations aid usability but may miss nuance.
  - Privacy-preserving methods may degrade explanation quality; full transparency may expose sensitive data.
- Failure signatures:
  - Explanations that fail adversarial tests or vary significantly with minor input changes.
  - User feedback indicating confusion or distrust despite technically accurate explanations.
  - Inability to contest or challenge decisions due to missing context or opaque reasoning.
- First 3 experiments:
  1. Test explanation fidelity under adversarial input perturbations using LIME/SHAP and compare with model internals.
  2. Evaluate user satisfaction and understanding with explanations tailored for ML engineers vs. end users in a simulated high-stakes decision task.
  3. Assess the impact of differential privacy on explanation quality and user trust in a biomedical image classification scenario.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can XAI methods be designed to provide reliable explanations that are resistant to manipulation and adversarial attacks?
- Basis in paper: [explicit] The paper discusses the vulnerability of XAI methods to manipulation, citing studies that show how explanations can be altered to mislead users about a model's behavior.
- Why unresolved: Despite the identification of this issue, the paper does not provide specific solutions or techniques to address the vulnerability of XAI methods to adversarial attacks.
- What evidence would resolve it: Research demonstrating new XAI methods that are robust to manipulation and adversarial attacks, validated through rigorous testing and real-world applications.

### Open Question 2
- Question: How can XAI be effectively integrated with other dimensions of responsible AI, such as fairness and privacy, to create a holistic approach?
- Basis in paper: [explicit] The paper emphasizes the need for a comprehensive approach to responsible AI that goes beyond explainability and considers other important aspects like fairness and privacy.
- Why unresolved: The paper acknowledges the complex interactions between explainability and other facets of responsible AI but does not provide clear strategies for integrating these dimensions effectively.
- What evidence would resolve it: Studies showcasing successful integration of XAI with fairness and privacy measures, along with frameworks or guidelines for implementing a holistic responsible AI approach.

### Open Question 3
- Question: What are the key human and contextual factors that influence the effectiveness of XAI methods in real-world applications?
- Basis in paper: [explicit] The paper highlights the importance of human factors, such as knowledge, needs, and expectations of real users, in the design and evaluation of XAI methods.
- Why unresolved: While the paper recognizes the significance of human and contextual factors, it does not provide a comprehensive understanding of these factors or how they impact the effectiveness of XAI methods.
- What evidence would resolve it: Research that identifies and analyzes the key human and contextual factors influencing XAI effectiveness, supported by empirical studies and real-world evaluations.

## Limitations
- The evidence for XAI manipulation mechanisms relies heavily on citations not accessible in corpus; replication of these findings is critical.
- User-centered evaluation claims are supported by general citations but lack specific experimental data or user study results in this paper.
- Privacy and fairness interactions with explainability are theorized but not empirically tested in real-world scenarios.

## Confidence
- **High**: The general argument that post-hoc XAI methods can be manipulated and may not reflect true model logic is well-established in literature.
- **Medium**: The claim that XAI fairness evaluation via feature attribution can miss systemic bias is theoretically sound but under-tested in practice.
- **Medium**: User perception variability is plausible but under-supported by direct evidence in this paper.

## Next Checks
1. Replicate adversarial perturbation experiments on LIME/SHAP explanations to confirm manipulability under controlled conditions.
2. Conduct user studies comparing explanation satisfaction and comprehension between ML engineers and end users in high-stakes decision tasks.
3. Evaluate fairness metrics from XAI methods on datasets known to have systemic bias to identify blind spots in current approaches.