---
ver: rpa2
title: 'Beyond Confidence: Reliable Models Should Also Consider Atypicality'
arxiv_id: '2305.18262'
source_url: https://arxiv.org/abs/2305.18262
tags:
- atypicality
- confidence
- calibration
- quantile
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how atypicality (rarity) of samples or
  classes relates to the reliability of machine learning model predictions. It demonstrates
  that atypical inputs or classes lead to more overconfident and less accurate predictions.
---

# Beyond Confidence: Reliable Models Should Also Consider Atypicality

## Quick Facts
- arXiv ID: 2305.18262
- Source URL: https://arxiv.org/abs/2305.18262
- Reference count: 40
- Key outcome: Incorporating input and class atypicality into post-hoc recalibration improves uncertainty quantification and model performance across discriminative neural networks and large language models.

## Executive Summary
This paper investigates how atypicality (rarity) of samples or classes relates to the reliability of machine learning model predictions. It demonstrates that atypical inputs or classes lead to more overconfident and less accurate predictions. By incorporating input and class atypicality into post-hoc recalibration, the paper shows improved uncertainty quantification and model performance for both discriminative neural networks and large language models. A case study on skin lesion classification shows atypicality improves performance across different skin tone groups without access to group attributes. Overall, the results suggest models should consider both confidence and atypicality to improve uncertainty quantification and performance, and simple post-hoc atypicality estimators can provide significant value.

## Method Summary
The paper proposes incorporating input and class atypicality into post-hoc recalibration and conformal prediction. Input atypicality is estimated using density estimation (Gaussian Mixture Models or k-NN) in the model's embedding space, while class atypicality is computed as the inverse frequency of class labels. The Atypicality-Aware Recalibration (AAR) method adjusts temperature scaling based on both input and class atypicality, applying different corrections to different atypicality groups. For conformal prediction, Atypicality-Aware Conformal Prediction (AA-APS, AA-RAPS) groups points by confidence and atypicality, fitting separate thresholds for each group. The methods are evaluated on balanced and imbalanced image classification datasets, natural language inference tasks, and text classification with LLMs.

## Key Results
- Atypicality is strongly related to miscalibration and accuracy, with predictions for atypical inputs or classes being more overconfident and less accurate
- Atypicality-Aware Recalibration improves calibration error and accuracy compared to temperature scaling across multiple datasets and model architectures
- Atypicality-Aware Conformal Prediction improves coverage for otherwise underperforming groups without increasing set size
- A case study on skin lesion classification shows atypicality improves performance across different skin tone groups without access to group attributes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Atypicality provides additional information about prediction reliability beyond confidence.
- Mechanism: Atypicality quantifies how representative a sample is of the training distribution. Predictions for atypical inputs or classes are more miscalibrated and less accurate than typical ones. Incorporating atypicality into recalibration adjusts predictions differently based on how atypical the input or class is.
- Core assumption: Atypicality can be estimated post-hoc using simple density estimation in the model's embedding space.
- Evidence anchors:
  - [abstract] "We first demonstrate that atypicality is strongly related to miscalibration and accuracy. In particular, we empirically show that predictions for atypical inputs or atypical classes are more overconfident and have lower accuracy."
  - [section 2] "Predictions in the Extrapolation or Untrustworthy regions are more miscalibrated than the ones in the typical regions."
  - [corpus] Weak. No direct corpus evidence for this specific recalibration mechanism.
- Break condition: If atypicality estimation is poor (e.g., due to model not fitting data well), it could lead to worse performance.

### Mechanism 2
- Claim: Different groups of atypicality require different recalibration adjustments.
- Mechanism: Temperature scaling performs differently for different atypicality groups. More atypical groups need larger temperature adjustments. Atypicality-Aware Recalibration (AAR) applies different adjustments based on input and class atypicality.
- Core assumption: Temperature scaling is a good base method to build upon.
- Evidence anchors:
  - [section 4.1] "Different atypicality groups need different adjustments, and more atypical groups need larger temperatures."
  - [section 4.2] "Our algorithm not only provides better calibration rates across all classes but also improves overall accuracy."
  - [corpus] Weak. No direct corpus evidence for this specific temperature adjustment mechanism.
- Break condition: If temperature scaling is not suitable for the model or data, this mechanism may not work.

### Mechanism 3
- Claim: Atypicality-awareness improves conformal prediction.
- Mechanism: Conformal prediction methods do not satisfy conditional coverage for atypical inputs or low-confidence predictions. Atypicality-Aware Conformal Prediction (AA-APS, AA-RAPS) groups points by confidence and atypicality, fitting separate thresholds for each group.
- Core assumption: Conformal prediction methods can be modified to use atypicality information.
- Evidence anchors:
  - [section 5] "We observe that APS and RAPS do not satisfy conditional coverage for atypical inputs or low-confidence predictions. We observe that being Atypicality-Aware improves coverage across otherwise underperforming groups."
  - [corpus] Weak. No direct corpus evidence for this specific conformal prediction mechanism.
- Break condition: If conformal prediction methods are not suitable for the model or data, this mechanism may not work.

## Foundational Learning

- Concept: Calibration
  - Why needed here: Understanding calibration is crucial for interpreting the relationship between atypicality and prediction reliability.
  - Quick check question: What is the difference between calibration error and expected calibration error?
- Concept: Density estimation
  - Why needed here: Density estimation is used to estimate atypicality from the model's embedding space.
  - Quick check question: How does tying the covariance matrix in Gaussian Mixture Models help with density estimation?
- Concept: Conformal prediction
  - Why needed here: Understanding conformal prediction is necessary for interpreting the improvements from atypicality-awareness.
  - Quick check question: What is the difference between marginal and conditional coverage in conformal prediction?

## Architecture Onboarding

- Component map: Input atypicality estimation -> Class atypicality estimation -> Atypicality-Aware Recalibration -> Atypicality-Aware Conformal Prediction
- Critical path: Input atypicality estimation -> Recalibration -> Conformal prediction
- Design tradeoffs: Simple atypicality estimators vs. more complex ones; temperature scaling vs. other recalibration methods
- Failure signatures: Poor atypicality estimates leading to worse performance; conformal prediction not improving
- First 3 experiments:
  1. Estimate input atypicality using density estimation in embedding space
  2. Implement Atypicality-Aware Recalibration and compare to temperature scaling
  3. Implement Atypicality-Aware Conformal Prediction and compare to standard methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the relationship between atypicality and other notions of uncertainty such as epistemic and aleatoric uncertainty?
- Basis in paper: [inferred] The paper mentions that atypicality is distinct from notions like "out-of-distribution" and suggests it captures a different aspect of uncertainty, but does not fully explore the relationship with epistemic/aleatoric uncertainty.
- Why unresolved: The paper does not provide a detailed analysis of how atypicality relates to these other types of uncertainty, leaving open the question of whether they are complementary or overlapping concepts.
- What evidence would resolve it: Empirical studies comparing atypicality with epistemic and aleatoric uncertainty measures across different datasets and model architectures would help clarify their relationship.

### Open Question 2
- Question: How can atypicality be incorporated into non-classification tasks like regression and generation?
- Basis in paper: [explicit] The authors explicitly state that extending atypicality to regression and generation settings is a promising direction for future work.
- Why unresolved: The paper focuses solely on classification tasks and does not provide any insights or methods for applying atypicality to other types of machine learning problems.
- What evidence would resolve it: Developing and testing atypicality estimators for regression and generation tasks, and evaluating their impact on uncertainty quantification and model performance in those domains.

### Open Question 3
- Question: What is the optimal way to estimate atypicality in high-dimensional spaces or with unstructured data?
- Basis in paper: [explicit] The authors use Gaussian Mixture Models and k-Nearest Neighbors as simple estimators, but acknowledge that more sophisticated methods could be explored in future work.
- Why unresolved: The paper does not explore the limitations or performance of these simple estimators in high-dimensional or unstructured data settings, leaving open the question of how to best estimate atypicality in those cases.
- What evidence would resolve it: Comparative studies of different atypicality estimation methods across a range of high-dimensional and unstructured data scenarios would help identify the most effective approaches.

## Limitations

- Atypicality estimation quality critically impacts performance; poor density estimates could lead to worse results than standard methods
- The approach relies on post-hoc modifications that don't alter original model training, potentially limiting maximum achievable improvements
- Results are demonstrated primarily on image and text classification tasks; generalizability to other domains remains untested

## Confidence

- **Input and Class Atypicality Improves Calibration**: High confidence
- **Atypicality-Aware Methods Outperform Standard Approaches**: Medium confidence
- **Atypicality Improves Fairness Across Groups**: Medium confidence

## Next Checks

1. **Robustness to Atypicality Estimation Quality**: Systematically vary the quality of atypicality estimates (using different k values for k-NN, different numbers of components for GMM) and measure the degradation in AAR and AA-APS performance. This would quantify how sensitive the improvements are to estimation quality.

2. **Cross-Domain Generalization**: Apply the proposed methods to fundamentally different data types (medical imaging with clinical metadata, tabular insurance claims, multivariate time series from sensors) to test whether the atypicality-uncertainty relationship holds across diverse data geometries and distributions.

3. **Ablation of Temperature Scaling**: Replace the temperature scaling base method in AAR with other recalibration approaches (vector scaling, histogram binning, Bayesian binning) to determine whether the improvements are specific to temperature scaling or generalize to other recalibration frameworks.