---
ver: rpa2
title: Are Your Explanations Reliable? Investigating the Stability of LIME in Explaining
  Text Classifiers by Marrying XAI and Adversarial Attack
arxiv_id: '2305.12351'
source_url: https://arxiv.org/abs/2305.12351
tags:
- explanation
- explanations
- lime
- similarity
- stability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the stability of LIME explanations for
  text classifiers by framing the problem as an adversarial attack. The authors propose
  a novel algorithm called XAIFooler that perturbs text inputs to manipulate LIME's
  explanations while preserving text semantics and original predictions.
---

# Are Your Explanations Reliable? Investigating the Stability of LIME in Explaining Text Classifiers by Marrying XAI and Adversarial Attack

## Quick Facts
- arXiv ID: 2305.12351
- Source URL: https://arxiv.org/abs/2305.12351
- Reference count: 5
- Key outcome: LIME explanations for text classifiers are unstable under semantic-preserving perturbations, with attack success rates varying by document length

## Executive Summary
This paper investigates the stability of LIME explanations for text classifiers by framing the problem as an adversarial attack. The authors propose XAIFooler, a novel algorithm that perturbs text inputs to manipulate LIME's explanations while preserving text semantics and original predictions. Using Rank-biased Overlap (RBO) to measure explanation similarity, experiments on IMDB and Twitter datasets demonstrate that XAIFooler can successfully manipulate explanations with high semantic preservability. The study reveals that explanation stability is highly dependent on document length, with longer documents showing higher attack success rates.

## Method Summary
The paper introduces XAIFooler, an algorithm that perturbs text inputs by replacing words with semantically similar alternatives to manipulate LIME explanations. The method uses TextFooler-inspired perturbation techniques with semantic constraints (part-of-speech consistency and minimum semantic similarity thresholds). Explanations are compared using Rank-biased Overlap (RBO) with parameter p=0.80. The attack targets words not in the top k most important features, replacing them with synonyms while ensuring the base model's prediction remains unchanged. Success is measured by whether explanation similarity falls below a threshold after perturbation.

## Key Results
- On IMDB dataset: 37.06% attack success rate with average RBO of 0.417
- Attack effectiveness decreases with document length: success rates below 10% for short tweets
- Computational costs vary significantly between models: FNN (fast) vs BERT/RoBERTa (slow)

## Why This Works (Mechanism)

### Mechanism 1
LIME explanations are unstable when input text is perturbed even slightly while preserving semantics. The paper shows that replacing semantically similar words can cause significant changes in feature rankings while maintaining the base model's prediction. This reveals inherent instability in LIME's explanation generation process.

### Mechanism 2
RBO with p=0.80 effectively measures explanation stability by balancing sensitivity to changes in important features while being robust to changes in less important ones. The measure accounts for both presence and order of features, making it suitable for comparing explanations where only top features are considered important.

### Mechanism 3
TextFooler-inspired perturbation function generates semantically similar text replacements while targeting explanation instability. By replacing words with nearest semantic neighbors and enforcing part-of-speech consistency and minimum semantic similarity thresholds, the function creates meaningful text variations that challenge LIME's stability.

## Foundational Learning

- Concept: Local surrogate models and LIME algorithm
  - Why needed here: Understanding LIME's explanation generation process is fundamental to developing the XAIFooler attack
  - Quick check question: What are the four main steps in LIME's explanation generation process for text data?

- Concept: Adversarial attacks in NLP
  - Why needed here: The paper frames stability investigation as an adversarial attack problem, using techniques from this field to generate perturbations
  - Quick check question: How does the TextFooler method inspire the perturbation function in XAIFooler?

- Concept: Similarity measures for ranked lists
  - Why needed here: RBO is introduced as a suitable measure for comparing explanations, requiring understanding its properties
  - Quick check question: What are the key properties of RBO that make it suitable for comparing explanations?

## Architecture Onboarding

- Component map: Input document -> LIME explanation -> Word selection (non-top-k features) -> Semantic replacement -> RBO comparison -> Output (accept/reject perturbation)

- Critical path: 1) Generate base explanation using LIME 2) Identify candidate words for perturbation 3) Generate semantic replacements 4) Enforce constraints 5) Generate new explanation 6) Compare using RBO 7) Accept if similarity threshold crossed

- Design tradeoffs: Computational cost vs. perturbation quality, semantic preservation vs. explanation manipulation, RBO parameter p vs. sensitivity

- Failure signatures: High explanation similarity despite perturbations, base model misclassification, extremely long runtime

- First 3 experiments: 1) Test inherent stability by varying LIME's sampling rate 2) Apply XAIFooler to single document with relaxed constraints 3) Compare attack success rates across document lengths

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal similarity measure for comparing explanations that balances sensitivity to meaningful changes while avoiding amplification of minor differences? The paper uses RBO but acknowledges room for exploration of more effective measures across various explanation types and domains.

### Open Question 2
How does explanation stability vary across different local surrogate models like SHAP compared to LIME? The paper suggests this as a profitable line of future inquiry but computational constraints limited testing to LIME only.

### Open Question 3
What is the relationship between base model complexity and explanation stability under adversarial perturbations? The paper observed significant computational differences between model types but couldn't extensively test across a wide range of model complexities.

## Limitations
- Semantic similarity measures (Counter-fitted PARAGRAM-SL999, Universal Sentence Encoder) may not perform consistently across all text domains
- RBO parameter p=0.80 appears arbitrary without extensive sensitivity analysis
- High computational cost of generating 200 samples per explanation limits scalability

## Confidence

- High confidence: LIME explanations are unstable under semantic-preserving perturbations
- Medium confidence: RBO is an effective similarity measure for explanations
- Low confidence: Generalizability of attack success rates to other text classification tasks

## Next Checks

1. Conduct sensitivity analysis on the RBO parameter p across different text domains to determine optimal settings
2. Validate semantic preservation constraints using human evaluation on a sample of perturbed texts
3. Test the attack on additional text classification tasks to assess generalizability