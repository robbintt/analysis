---
ver: rpa2
title: 'A blind spot for large language models: Supradiegetic linguistic information'
arxiv_id: '2306.06794'
source_url: https://arxiv.org/abs/2306.06794
tags:
- chatgpt
- what
- like
- language
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose that large language models (LLMs) like ChatGPT
  have access to only diegetic linguistic information, not the full range of supradiegetic
  information that humans experience. This reframing explains why LLMs struggle with
  tasks like palindromes, translating cuneiform, and continuing integer sequences.
---

# A blind spot for large language models: Supradiegetic linguistic information

## Quick Facts
- arXiv ID: 2306.06794
- Source URL: https://arxiv.org/abs/2306.06794
- Reference count: 0
- The authors propose that LLMs have access to only diegetic linguistic information, not the full range of supradiegetic information that humans experience.

## Executive Summary
This paper introduces the concept of diegetic versus supradiegetic linguistic information to explain why large language models (LLMs) like ChatGPT struggle with certain tasks. Diegetic information refers to the semantic meaning accessible from within the linguistic world, while supradiegetic information includes form-level features like visual or phonetic properties. The authors argue that LLMs' tokenization and embedding processes strip away this supradiegetic information, leading to failures on tasks requiring symmetry recognition, mathematical reasoning, or translation of symbolic languages like cuneiform. They suggest that providing LLMs with more diegetic approximations of supradiegetic information could improve their performance.

## Method Summary
The authors interact with ChatGPT and GPT-4 models to test their performance on tasks involving palindromes, symmetry analysis, cuneiform translation, and integer sequence continuation. They use examples like Sumerian cuneiform symbols, words such as HOIOH and OIHIO for palindrome detection, and mathematical concepts like the holey sequence. The approach involves qualitative analysis of model responses rather than quantitative metrics, comparing LLM outputs to expected results and expert knowledge.

## Key Results
- LLMs struggle with tasks requiring visual or form-level understanding due to tokenization stripping supradiegetic information
- ChatGPT cannot reliably translate Sumerian cuneiform or detect palindromes in words
- Mathematical reasoning fails when symbols overload adjacency rules beyond what token embeddings capture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs only access diegetic linguistic information, not supradiegetic
- Mechanism: Tokenization and vector embeddings preserve semantic context but strip form-level features like visual or phonetic properties
- Core assumption: The architecture treats language as a sequence of context-dependent tokens, discarding sensory form
- Evidence anchors:
  - [abstract] "exposure to the diegetic information encoded in language...ignorance of extradiegetic information, including supradiegetic linguistic information"
  - [section III C] "diegetic: Information accessible from within the world...roughly the inside of the word/symbol, its function, the meaning"
  - [corpus] weak/no direct corpus evidence linking tokenization to sensory loss
- Break condition: If multimodal training or vision-augmented LLMs successfully integrate form-level features, this mechanism fails

### Mechanism 2
- Claim: Tasks requiring symmetry or visual pattern recognition fail because the model cannot perceive form
- Mechanism: Without access to the visual representation of symbols, LLMs cannot reason about palindromes or symmetry tasks that depend on spatial structure
- Core assumption: Human ability to solve these tasks relies on visual processing, which LLMs lack
- Evidence anchors:
  - [section III C] "ChatGPT, given words, thinks in vectors...the skin of the word is changed"
  - [section IV] "ChatGPT cannot, given a letter, successfully imagine folding it"
  - [corpus] no corpus evidence for symmetry processing failure
- Break condition: If an LLM demonstrates reliable palindrome detection after multimodal training, this mechanism breaks

### Mechanism 3
- Claim: Mathematical reasoning fails because mathematical symbols overload adjacency rules beyond what token embeddings capture
- Mechanism: Mathematics requires extradiegetic structure that cannot be derived from 1D token adjacency alone
- Core assumption: Mathematical syntax and semantics are not derivable from linguistic training data alone
- Evidence anchors:
  - [section IX A] "the dimensions involved have exploded! The rules...governing how these symbols combine to create meaning are very different"
  - [section IX A] "we overload our symbols, and we overload the relationship of proximity"
  - [corpus] no corpus evidence for mathematical reasoning limitations
- Break condition: If mathematical reasoning improves significantly with additional training data or architectural changes, this mechanism fails

## Foundational Learning

- Concept: Diegetic vs. supradiegetic linguistic information
  - Why needed here: Central to understanding why LLMs struggle with certain tasks
  - Quick check question: Can you explain in your own words what diegetic and supradiegetic information are and how they differ?

- Concept: Word embeddings and tokenization
  - Why needed here: Fundamental to understanding how LLMs process language
  - Quick check question: How do word embeddings capture semantic information, and what information do they lose compared to raw text?

- Concept: Multimodal processing in human cognition
  - Why needed here: Important for understanding why certain tasks are harder for LLMs
  - Quick check question: How do vision and language interact in human cognition, and why might this be important for tasks like palindrome detection?

## Architecture Onboarding

- Component map: Tokenizer -> Embedding layer -> Transformer layers -> Output layer
- Critical path: Tokenization → Embedding → Context processing → Output generation
- Design tradeoffs:
  - More parameters vs. computational cost
  - Longer context vs. memory usage
  - Pretraining data vs. task-specific fine-tuning
- Failure signatures:
  - Inability to recognize visual patterns (palindromes, symmetry)
  - Struggles with mathematical reasoning
  - Inconsistent performance on tasks requiring form-level understanding
- First 3 experiments:
  1. Test palindrome detection on various strings with and without visual cues
  2. Evaluate mathematical reasoning on problems of increasing complexity
  3. Compare performance on tasks requiring form-level understanding vs. purely semantic tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum amount of training data required for an LLM like ChatGPT to achieve linguistic fluency in a given language?
- Basis in paper: [explicit] "We note that there is some minimum information required for it to be true that we know what a 'chair' is, but that does not mean our understanding of it could never evolve, given more information [13, 14]. Similarly, ChatGPT needs some amount of training data to speak English fluently, but it does not need to know every word—and with more data, it could expand its diegetic world. How long (equivalently, how much training data) is needed to have what information diegetically, and what the full extent of the diegetic world of an LLM could be given as much text as it wants are open questions we will return to (but definitely not answer)."
- Why unresolved: The authors explicitly state this is an open question they do not answer
- What evidence would resolve it: Experiments varying the amount of training data provided to LLMs and measuring the resulting linguistic fluency

### Open Question 2
- Question: Can ChatGPT reliably translate Sumerian cuneiform?
- Basis in paper: [explicit] "ChatGPT is not, right now, a reliable translator. Translation is a highly anticipated task for LLMs [20], and in some cases, an area in which LLM output is already useful and impressive. However, we think it is worth pointing out that it fails (in a way that could be very misleading) for some languages and symbols, for example, Sumerian cuneiform."
- Why unresolved: The authors note ChatGPT's failure at this task but do not provide a solution or definitive answer on whether it can be made reliable
- What evidence would resolve it: Testing ChatGPT's translations of Sumerian cuneiform against expert human translations and measuring accuracy

### Open Question 3
- Question: How does the lack of access to supradiegetic linguistic information affect ChatGPT's performance on tasks that humans typically find easy?
- Basis in paper: [explicit] "We think it is worth noting explicitly that ChatGPT is not a human, and cannot do exactly what a human can do. That does not mean that ChatGPT cannot do more or less what a human can do. We can be certain that, when exposed to the same textual input, ChatGPT and a person are not granted access to equivalent supradiegetic linguistic information. But it is not obvious how well that missing information can be approximated diegetically—it seems plausible to think that, with enough diegetic approximations of extradiegetic information, ChatGPT could reach a downstream universe of conclusions and thoughts functionally indistinguishable from those a person could reach from the same text [8]."
- Why unresolved: The authors suggest this is an area for further investigation but do not provide definitive conclusions
- What evidence would resolve it: Comparing ChatGPT's performance on various tasks to human performance, focusing on tasks that rely on supradiegetic information like visual recognition or auditory processing

## Limitations

- The theoretical framework remains largely conceptual without systematic empirical validation
- The paper lacks quantitative metrics and controlled experiments to definitively prove the proposed mechanisms
- It doesn't adequately address whether improvements in LLM performance over time reflect actual access to supradiegetic information or simply better pattern matching

## Confidence

- High confidence: The distinction between diegetic and supradiegetic information is a useful conceptual framework for understanding LLM limitations
- Medium confidence: Tokenization and embedding processes do strip away form-level information that humans use for certain tasks
- Low confidence: The specific mechanisms proposed for why each task fails are definitively correct

## Next Checks

1. Design controlled experiments comparing LLM performance on tasks with and without visual/phonetic cues, isolating whether access to supradiegetic information improves outcomes
2. Test whether multimodal LLMs (with vision capabilities) show improved performance on the cited tasks, validating whether the issue is truly about missing form-level information
3. Conduct ablation studies on tokenization processes to empirically measure what information is lost at each stage and how this correlates with task performance