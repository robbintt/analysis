---
ver: rpa2
title: Wasserstein Distributionally Robust Policy Evaluation and Learning for Contextual
  Bandits
arxiv_id: '2309.08748'
source_url: https://arxiv.org/abs/2309.08748
tags:
- policy
- problem
- wasserstein
- distribution
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies off-policy evaluation (OPE) and learning (OPL)
  for contextual bandits under distribution shifts using Wasserstein distributionally
  robust optimization (DRO). The authors propose a Wasserstein DRO approach to bound
  policy values conservatively when training and testing environments differ.
---

# Wasserstein Distributionally Robust Policy Evaluation and Learning for Contextual Bandits

## Quick Facts
- **arXiv ID**: 2309.08748
- **Source URL**: https://arxiv.org/abs/2309.08748
- **Reference count**: 9
- **Primary result**: Wasserstein DRO achieves O(n^{-1/2}) OPE error rate and improves query complexity for OPL compared to KL-based methods

## Executive Summary
This paper addresses the challenge of off-policy evaluation (OPE) and learning (OPL) in contextual bandits when training and testing environments differ. The authors propose a Wasserstein distributionally robust optimization (DRO) approach that bounds policy values conservatively under distribution shifts. Unlike KL divergence-based methods, Wasserstein distance captures the geometric differences between distributions, enabling more accurate characterization of practical environment mismatches. The method provides finite sample analysis showing OPE error decreases at rate O(n^{-1/2}), improving on KL-based methods. For OPL, the authors present both a regularized method improving query complexity and a biased stochastic gradient descent method with complexity independent of support size.

## Method Summary
The method uses Wasserstein distance to define uncertainty sets for contexts and costs, enabling robust policy evaluation under distribution shifts. For OPE, the approach solves a regularized Wasserstein DRO problem using stochastic gradient descent on a smoothed dual formulation. For OPL, two methods are proposed: a regularized approach that improves query complexity and a biased stochastic gradient descent method that scales independently of context support size. The approach is validated on the International Stroke Trial dataset, demonstrating robust policy evaluation and learning under various distribution shift scenarios.

## Key Results
- Wasserstein DRO achieves OPE error rate of O(n^{-1/2}), improving on KL-based methods
- Regularized Wasserstein DRO improves query complexity for OPL from O(|X|²) to O(|X|δ⁻¹ log δ⁻¹)
- Biased SGD for OPL achieves query complexity independent of context support size
- Experiments on IST dataset show robust performance under distribution shifts compared to KL DRO baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Wasserstein distance improves distribution shift robustness compared to KL divergence by incorporating geometry of supports.
- Mechanism: The Wasserstein metric measures transportation cost between distributions, allowing it to handle different supports and capture geometric differences. This enables more accurate characterization of practical environment mismatches than KL divergence which fails for distributions with different supports.
- Core assumption: The geometry of the distribution support contains meaningful information about environmental differences relevant to policy performance.
- Evidence anchors:
  - [abstract]: "the KL uncertainty set fails to encompass distributions with varying support and lacks awareness of the geometry of the distribution support"
  - [section 2]: "it is important to note that KL divergence cannot capture the geometric differences between distributions"
  - [corpus]: Weak - neighbors discuss related Wasserstein DRO approaches but don't directly compare geometry awareness to KL.
- Break condition: When the geometry of support differences is irrelevant to policy performance, or when the transportation cost computation becomes prohibitively expensive for high-dimensional spaces.

### Mechanism 2
- Claim: Regularization with entropy smoothing improves computational tractability while maintaining theoretical guarantees.
- Mechanism: Replacing the inner maximization with a smooth "softmax" function creates a smoothed dual problem that is computationally more tractable and enables efficient optimization through first-order methods while preserving convergence properties.
- Core assumption: The smoothing parameter can be chosen to balance approximation accuracy with computational efficiency.
- Evidence anchors:
  - [section 4]: "we simplify the dual problem...by replacing the maximization operation with a smooth 'softmax' function"
  - [section 5]: "if we further assume that each f_ζ(θ, λ) is convex...it only takes Õ(δ⁻¹) iterations to find a δ-accurate optimal solution"
  - [corpus]: Weak - neighbors discuss optimization techniques but don't specifically address entropy regularization in DRO context.
- Break condition: When the smoothing approximation becomes too coarse, losing the benefits of the original Wasserstein formulation, or when the smoothing parameter selection becomes difficult in practice.

### Mechanism 3
- Claim: Biased stochastic gradient descent enables scalable policy learning with query complexity independent of support size.
- Mechanism: By sampling contexts and constructing biased gradient estimates, the method avoids the O(|X|) query complexity of full gradient methods while maintaining convergence to optimal policies.
- Core assumption: The biased gradient estimator converges sufficiently fast despite the nonlinear log function in the objective.
- Evidence anchors:
  - [section 5]: "we propose a (biased) SGD method that can find the optimal policy with query complexity independent of the distribution support size"
  - [section 5]: "The regularized DRO problem is a special case of so-called conditional stochastic optimization (CSO) problems"
  - [corpus]: Weak - neighbors discuss optimization methods but don't specifically address SGD for Wasserstein DRO policy learning.
- Break condition: When the bias in the gradient estimator becomes too large relative to the true gradient, causing convergence to suboptimal policies.

## Foundational Learning

- **Concept**: Distributionally Robust Optimization (DRO)
  - Why needed here: Provides the theoretical framework for handling distribution shifts between training and testing environments
  - Quick check question: How does DRO differ from standard optimization when the testing distribution is unknown?

- **Concept**: Wasserstein Distance vs KL Divergence
  - Why needed here: The choice of distance metric fundamentally affects the uncertainty set definition and resulting policy performance
  - Quick check question: Under what conditions would KL divergence fail to capture meaningful distribution differences that Wasserstein distance would capture?

- **Concept**: Dual Problem Formulation
  - Why needed here: Enables tractable computation of the DRO problem by converting measure optimization to scalar optimization
  - Quick check question: What conditions are required for strong duality to hold in the Wasserstein DRO formulation?

## Architecture Onboarding

- **Component map**: Data → Empirical estimates → Dual problem formulation → Optimization algorithm → Policy evaluation/learning → Performance bounds
- **Critical path**: Data → Empirical estimates → Dual problem formulation → Optimization (LP or regularized) → Policy learning → Parameter optimization → Evaluation
- **Design tradeoffs**: Wasserstein vs KL divergence (computational cost vs geometric awareness), regularized vs unregularized (tractability vs accuracy), exact vs approximate optimization (convergence guarantees vs scalability)
- **Failure signatures**: Poor policy performance on testing data, slow convergence in optimization, unstable dual variables, large gaps between empirical and true policy values
- **First 3 experiments**:
  1. Compare Wasserstein DRO policy evaluation with KL DRO on a simple contextual bandit problem with known distribution shift
  2. Evaluate the impact of smoothing parameter η on regularized Wasserstein DRO performance and convergence
  3. Test biased SGD policy learning on a problem with large context support to verify query complexity independence from support size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Wasserstein DRO method be extended to handle continuous contexts without discretization?
- Basis in paper: The authors discretize contexts for the stroke trial experiment due to limited data points, but note that the theoretical analysis assumes discrete contexts.
- Why unresolved: The paper does not provide a method for handling continuous contexts directly, and discretizing may lead to information loss or computational challenges.
- What evidence would resolve it: A modified algorithm or theoretical analysis that explicitly handles continuous contexts without discretization would resolve this question.

### Open Question 2
- Question: What is the optimal way to select the uncertainty set radius ϵ for Wasserstein DRO in practice?
- Basis in paper: The authors note that selecting the uncertainty set radius is a main challenge in DRO and propose using the Wasserstein distance between training set splits, but this may not capture all distribution shifts.
- Why unresolved: The paper does not provide a systematic method for selecting ϵ and acknowledges that the initial estimate may not be sufficient.
- What evidence would resolve it: Empirical studies comparing different methods for selecting ϵ or a theoretical analysis of the impact of ϵ on performance would help resolve this question.

### Open Question 3
- Question: How does the performance of Wasserstein DRO compare to other distributionally robust optimization methods in more complex bandit problems?
- Basis in paper: The authors compare Wasserstein DRO to KL DRO on a stroke trial dataset but do not explore more complex bandit problems.
- Why unresolved: The paper focuses on a specific dataset and does not explore the performance of Wasserstein DRO in other bandit settings.
- What evidence would resolve it: Experiments on diverse bandit problems, such as those with high-dimensional contexts or complex cost functions, would help compare the performance of Wasserstein DRO to other methods.

## Limitations

- **Implementation complexity**: The paper lacks specific implementation details for the biased SGD algorithm, particularly the exact formulas for gradient estimates and update rules in Algorithm 1.
- **Hyperparameter selection**: The approach requires careful selection of smoothing parameter η and uncertainty radii ϵx, ϵc without clear guidance on selection strategies or sensitivity analysis.
- **Computational scalability**: While theoretically scalable, the computational complexity of Wasserstein distance computation and optimization may become prohibitive for very large context spaces or complex cost distributions.

## Confidence

**High Confidence Claims**:
- The theoretical framework connecting Wasserstein DRO to distributionally robust contextual bandits is well-established and mathematically sound
- The finite sample analysis showing OPE error decreases at rate O(n^{-1/2}) follows standard statistical learning theory arguments
- The advantage of Wasserstein distance over KL divergence for handling different supports is well-supported by the geometric interpretation

**Medium Confidence Claims**:
- The practical performance improvements demonstrated on the IST dataset are likely valid but may depend on specific dataset characteristics
- The convergence rates for regularized optimization methods follow standard analysis but may be conservative in practice
- The query complexity independence from support size for biased SGD is theoretically justified but may require careful tuning in practice

**Low Confidence Claims**:
- The generalizability of results across diverse contextual bandit problems beyond the specific IST dataset
- The practical effectiveness of the approach in very high-dimensional settings where Wasserstein distance computation becomes expensive
- The robustness of the method to severe distribution shifts that may violate the bounded transport cost assumption

## Next Checks

1. **Algorithm Implementation Validation**: Implement and test the biased SGD algorithm on a synthetic contextual bandit problem with known ground truth. Verify that the query complexity remains independent of support size as claimed, and that the bias in gradient estimates doesn't lead to convergence to suboptimal policies.

2. **Hyperparameter Sensitivity Analysis**: Conduct systematic experiments varying the smoothing parameter η and uncertainty radii ϵx, ϵc on the IST dataset. Analyze how these parameters affect policy evaluation accuracy, learning performance, and computational efficiency to develop practical selection guidelines.

3. **Scalability Testing**: Evaluate the computational performance of the Wasserstein DRO approach on increasingly high-dimensional contextual bandit problems. Measure the growth in computation time and memory usage as context dimension increases, and identify practical limits for real-world applications.