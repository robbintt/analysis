---
ver: rpa2
title: Evaluating the Zero-shot Robustness of Instruction-tuned Language Models
arxiv_id: '2306.11270'
source_url: https://arxiv.org/abs/2306.11270
tags:
- question
- options
- template
- input
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Instruction-tuned LLMs show poor robustness to novel (unseen in
  training) but semantically equivalent instruction phrasings, leading to significant
  performance drops. The authors propose a simple method that introduces soft prompt
  embedding parameters and optimizes them to maximize similarity between representations
  of semantically equivalent instructions.
---

# Evaluating the Zero-shot Robustness of Instruction-tuned Language Models

## Quick Facts
- arXiv ID: 2306.11270
- Source URL: https://arxiv.org/abs/2306.11270
- Reference count: 40
- Models show significant performance drops when using novel but semantically equivalent instruction phrasings

## Executive Summary
This paper evaluates how well instruction-tuned language models handle novel instruction phrasings that are semantically equivalent to those seen during training. The authors find that these models are surprisingly sensitive to exact instruction phrasing, showing substantial performance degradation when presented with semantically equivalent but novel instructions. To address this robustness issue, they propose a soft prompt alignment method that introduces learnable soft prompt parameters optimized to maximize similarity between representations of semantically equivalent instructions. Experiments across multiple model families (T5, LLaMA, GPT-3.5) and benchmarks (MMLU, BIG-Bench) demonstrate consistent improvements in robustness, particularly for unobserved instructions.

## Method Summary
The authors propose a soft prompt alignment method that introduces learnable soft prompt parameters to the model and optimizes them using a KL-divergence loss term. This loss penalizes the model for producing dissimilar output distributions when given semantically equivalent instructions (a paraphrased instruction vs. a reference instruction). The method is applied as an additional fine-tuning step after standard instruction fine-tuning, requiring no changes to the model architecture or full parameter updates. The approach aims to encourage the model to map different instruction phrasings to similar internal representations, leading to more consistent predictions across semantically equivalent inputs.

## Key Results
- Instruction-tuned models show 10-30% performance drops when using novel but semantically equivalent instruction phrasings
- The soft prompt alignment method consistently improves robustness, with larger gains on unobserved instructions (up to 15% improvement)
- Representational distance analysis shows that aligned models produce more similar embeddings for semantically equivalent instructions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction-tuned LLMs rely heavily on exact instruction phrasing seen during training
- Mechanism: During fine-tuning, the model learns to associate specific input-output patterns with exact instruction templates. When presented with semantically equivalent but superficially different instructions at test time, the model fails to activate the same internal representations and thus produces incorrect outputs.
- Core assumption: The model's internal representations are highly sensitive to surface-level instruction variations rather than semantic content
- Evidence anchors:
  - [abstract] "using novel (unobserved) but appropriate instruction phrasings consistently degrades model performance, sometimes substantially so"
  - [section 4.2] "using instructions unobserved in training—but manually composed for the task at hand and so semantically appropriate—leads to considerable degradation in performance"
  - [corpus] Weak evidence - no direct studies on representation invariance in instruction-tuned models
- Break condition: The model would need to learn instruction-invariant representations, which current instruction-tuning approaches do not explicitly optimize for

### Mechanism 2
- Claim: The proposed soft prompt alignment method improves robustness by encouraging similar representations for semantically equivalent instructions
- Mechanism: By introducing learnable soft prompt parameters and optimizing them to minimize KL-divergence between output distributions for semantically equivalent instructions, the model learns to map different instruction phrasings to similar internal representations, leading to more consistent predictions
- Core assumption: Adjusting soft prompt parameters can effectively align instruction representations without requiring full model fine-tuning
- Evidence anchors:
  - [section 6] "This approach entails introducing an additional loss term that penalizes the model for inducing dissimilar distributions over output tokens when using (a) paraphrased instructions as opposed to (b) a reference instruction for the same task"
  - [section 6] "Using all of these components together yields the best performance, especially on unobserved instructions"
  - [corpus] Weak evidence - soft prompt methods are relatively new and under-studied in instruction-tuning context
- Break condition: If the soft prompt parameters cannot effectively capture instruction semantics, or if the KL-divergence objective creates conflicts with the primary task objective

### Mechanism 3
- Claim: Performance degradation from unobserved instructions correlates with representational distance between instruction embeddings
- Mechanism: When instruction embeddings (at the penultimate layer) are far apart in representation space, the model treats them as different tasks, leading to degraded performance. Closer embeddings indicate the model recognizes them as semantically equivalent.
- Core assumption: Euclidean distance in the model's representation space correlates with semantic similarity for instructions
- Evidence anchors:
  - [section 4.5] "We use tSNE to visualize these representations of observed and unobserved instructions... MMLU unobserved instructions are quite similar to the observed, while there is a greater separation between unobserved and observed instructions in BBL"
  - [section 4.5] "average ℓ2 distance between representations of unobserved instructions and those of their nearest observed counterparts"
  - [corpus] Weak evidence - limited prior work on instruction representation similarity in LLMs
- Break condition: If the representation space is not semantically meaningful, or if distance metrics don't capture the relevant aspects of instruction similarity

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The paper evaluates model performance when given new tasks without any examples, relying solely on the instruction to guide the model
  - Quick check question: What is the key difference between zero-shot and few-shot learning in the context of instruction-tuned models?

- Concept: Fine-tuning vs. prompting
  - Why needed here: The paper compares models trained with instruction fine-tuning against standard prompting approaches, and introduces a new fine-tuning objective
  - Quick check question: How does instruction fine-tuning differ from standard pre-training or from simply using better prompts?

- Concept: Soft prompts and prefix tuning
  - Why needed here: The proposed method introduces learnable soft prompt parameters as an alternative to full model fine-tuning
  - Quick check question: What are the advantages of using soft prompts over fine-tuning all model parameters?

## Architecture Onboarding

- Component map: Pre-trained language model -> Instruction fine-tuning dataset -> Soft prompt alignment components (soft prompt parameters + KL-divergence loss)
- Critical path: Data preprocessing → instruction fine-tuning with standard CE loss → soft prompt alignment fine-tuning → evaluation with observed vs. unobserved instructions
- Design tradeoffs: Full model fine-tuning provides more flexibility but is computationally expensive and risks catastrophic forgetting; soft prompts are parameter-efficient but may have limited representational capacity
- Failure signatures: Performance degradation on unobserved instructions indicates the model is overly sensitive to instruction phrasing; poor alignment results suggest the soft prompt approach isn't effectively learning instruction-invariant representations
- First 3 experiments:
  1. Reproduce the baseline instruction-tuned model performance on observed vs. unobserved instructions to establish the sensitivity issue
  2. Implement and evaluate the soft prompt alignment method to verify it improves robustness
  3. Analyze representation distances before and after alignment to confirm the mechanism of improvement

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the proposed soft prompt alignment method generalize to other types of NLP tasks beyond QA and classification (e.g., generation, summarization, or structured prediction)?
- **Basis in paper:** The paper evaluates the method on QA and classification tasks (MMLU, BBL) but does not test it on other task types.
- **Why unresolved:** The soft prompt alignment method may rely on the specific output formats of QA and classification tasks (e.g., selecting from predefined options), which may not apply to tasks requiring open-ended generation or structured outputs.
- **What evidence would resolve it:** Experiments showing consistent improvements across diverse task types (e.g., text generation, summarization, or structured prediction) would demonstrate the method's broader applicability.

### Open Question 2
- **Question:** How does the soft prompt alignment method scale with model size, particularly for extremely large models (e.g., 175B+ parameters)?
- **Basis in paper:** The paper evaluates the method on models up to 13B parameters (Alpaca-13B) and observes consistent improvements, but does not test it on larger models.
- **Why unresolved:** Larger models may have different representations of semantically equivalent instructions, potentially reducing the effectiveness of the alignment method.
- **What evidence would resolve it:** Testing the method on extremely large models (e.g., 175B+ parameters) and comparing performance improvements would clarify its scalability.

### Open Question 3
- **Question:** How sensitive is the soft prompt alignment method to the quality of automatically generated paraphrased instructions?
- **Basis in paper:** The method relies on GPT-4 to generate paraphrased instructions, but the paper does not analyze the quality or diversity of these paraphrases.
- **Why unresolved:** Poorly generated paraphrases could lead to ineffective alignment, while high-quality paraphrases could enhance robustness. The method's success may depend on the fidelity of the paraphrasing process.
- **What evidence would resolve it:** Ablation studies comparing performance with human-written versus automatically generated paraphrases would reveal the method's sensitivity to paraphrase quality.

## Limitations
- The study relies on a relatively small set of manually created paraphrased instructions (319 total), limiting generalizability to all possible instruction variations
- The soft prompt alignment method introduces additional parameters that could potentially memorize specific paraphrased instruction pairs rather than learning true semantic invariance
- The study focuses primarily on English instructions and may not capture cross-linguistic robustness challenges

## Confidence
- High confidence in the core observation that instruction-tuned models show significant performance drops with semantically equivalent but unobserved instruction phrasings
- Medium confidence in the proposed soft prompt alignment method's effectiveness, as the improvements are demonstrated but long-term stability and generalization potential require further validation
- Medium confidence in the representational distance analysis, as the correlation between embedding similarity and performance is observed but the causal mechanism is not fully established

## Next Checks
1. Evaluate the soft prompt alignment method on a larger, more diverse set of paraphrased instructions (at least 1000+) across multiple languages to assess whether the robustness improvements generalize beyond the specific instruction pairs used during training

2. Conduct systematic ablations to determine whether the KL divergence loss, soft prompt parameters, or their combination drives the performance improvements, and whether the soft prompt parameters can be reduced in size without sacrificing robustness

3. Monitor model performance on both observed and unobserved instructions over extended fine-tuning periods to check for catastrophic forgetting of the original instruction patterns and evaluate whether the soft prompt alignment creates new vulnerabilities to instruction phrasing variations