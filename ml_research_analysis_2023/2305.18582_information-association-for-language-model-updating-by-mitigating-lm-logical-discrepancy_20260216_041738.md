---
ver: rpa2
title: Information Association for Language Model Updating by Mitigating LM-Logical
  Discrepancy
arxiv_id: '2305.18582'
source_url: https://arxiv.org/abs/2305.18582
tags:
- information
- instruction
- language
- news
- update
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of updating large language models
  (LLMs) with new information while avoiding exposure bias. The authors propose a
  novel task, Self Information Update (SIU), which requires updating LLMs using only
  unstructured text corpora, without additional human intervention.
---

# Information Association for Language Model Updating by Mitigating LM-Logical Discrepancy

## Quick Facts
- arXiv ID: 2305.18582
- Source URL: https://arxiv.org/abs/2305.18582
- Reference count: 29
- Primary result: Improves factual consistency scores by up to 0.16 using context-aware distillation

## Executive Summary
This paper addresses the challenge of updating large language models (LLMs) with new information while avoiding exposure bias. The authors propose a novel task, Self Information Update (SIU), which requires updating LLMs using only unstructured text corpora, without additional human intervention. They identify exposure bias as a core problem, where LLMs tend to prioritize existing information over new information during fine-tuning. To mitigate this issue, the authors introduce a context-aware distillation method that incorporates relevant facts from the new information source during training. They evaluate their approach on two datasets: news articles published after March 2023 and the Natural Questions benchmark.

## Method Summary
The method involves generating instruction-response pairs from the update corpus using self instruction generation, then fine-tuning the pretrained model using context-aware distillation that incorporates source articles as context. The training dataset combines these pairs with unrelated instruction-response pairs to prevent over-adjustment. The approach uses a compact replay buffer containing only 2.3% of the training tokens to effectively mitigate forgetting while maintaining model performance on existing knowledge.

## Key Results
- Improves factual consistency scores by up to 0.16 on a scale of 0 to 1
- Effectively mitigates forgetting using a compact replay buffer (2.3% of training tokens)
- Demonstrates significant improvement over baseline approaches on both news articles and Natural Questions benchmark

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Exposure bias causes language models to prioritize existing information over new information during fine-tuning.
- Mechanism: When fine-tuning a model pretrained on corpus C with new information from corpus T, the model's gradients are dominated by tokens from C because P(IS(C)|i,A) > P(IS(T)|i,A). This leads to responses that rely on outdated knowledge.
- Core assumption: The pretraining corpus C is significantly larger than the update corpus T, and the model's learned representations from C are more strongly weighted.
- Evidence anchors:
  - [abstract]: "We observe that the naïve method of continual fine-tuning can be problematic due to LLMs' exposure bias, which prioritizes existing information over new information we aim to integrate."
  - [section 2.3]: "In other words, the fine-tuned model will predict the response referring to the information in both the new corpus T and the pretraining corpus C. However, since we perform the continual fine-tuning of A′ from A pretrained on C, we would expect P(IS(C)|i,A) > P(IS(T)|i,A)."
  - [corpus]: Weak evidence. The corpus analysis shows related work on "Implicit Bias in LLMs" which supports the general concept of bias in language models, but doesn't specifically address exposure bias in the context of information updating.

### Mechanism 2
- Claim: Context-aware distillation mitigates exposure bias by incorporating relevant facts from the new information source during training.
- Mechanism: By including the source article as context when generating instruction-response pairs, the model learns to associate new information with specific contexts, reducing reliance on existing knowledge.
- Core assumption: The model can effectively learn from the context-aware training samples and apply this learning to generate responses that incorporate new information.
- Evidence anchors:
  - [abstract]: "Based on our theoretical analysis, we propose a straightforward yet effective method to mitigate exposure bias by incorporating the selection of relevant facts into training losses."
  - [section 2.4]: "For the implementation, we utilize IS(T) as the reference article that guides the base model A in generating instruction-response pairs (i, s). When presented with an input instruction i, the model undergoes fine-tuning to generate the corresponding news article first, followed by appending the response."
  - [corpus]: Weak evidence. The corpus analysis doesn't provide direct evidence for context-aware distillation, but related work on "Adversarial Collaborative Filtering for Free" suggests that incorporating additional context can improve model performance.

### Mechanism 3
- Claim: Including unrelated instruction-response pairs in the fine-tuning dataset prevents the model from becoming biased towards new information.
- Mechanism: By sampling random instructions from the full space X, the model maintains its ability to follow instructions not related to the new information, preventing over-adjustment.
- Core assumption: The space of unrelated instructions is large enough that random sampling will provide sufficient diversity to maintain model performance on existing knowledge.
- Evidence anchors:
  - [section 2.3]: "To sample unrelated instructions from X \XT, we leverage the sparsity of XT within X and simply select random instructions from X, since the likelihood of a random sample belonging to XT is minimal."
  - [section 3.7]: "We also collect another subset of 299 instruction-response pairs that are not directly related to the new information from Dolly. We will refer this subset as UNRELATED for the rest of this paper."
  - [corpus]: No direct evidence. The corpus analysis doesn't provide evidence for the importance of unrelated instruction-response pairs in fine-tuning.

## Foundational Learning

- Concept: Language Modeling Probabilities
  - Why needed here: Understanding how language models generate text based on probabilities is crucial for grasping the exposure bias problem and the context-aware distillation approach.
  - Quick check question: How does a language model decide which word to generate next when producing a response?

- Concept: Fine-tuning and Continual Learning
  - Why needed here: The paper discusses fine-tuning a pretrained language model with new information, which requires understanding how fine-tuning works and the challenges of continual learning.
  - Quick check question: What are the potential issues when fine-tuning a model on new data without considering the existing knowledge?

- Concept: Knowledge Distillation
  - Why needed here: The context-aware distillation approach is a form of knowledge distillation, where the model learns to mimic the behavior of another model (in this case, itself with additional context).
  - Quick check question: How does knowledge distillation differ from regular supervised learning, and what are its advantages?

## Architecture Onboarding

- Component map:
  Pretrained Language Model (MixInst) -> Self Instruction Generation -> Context-aware Distillation -> Updated Model

- Critical path:
  1. Generate instruction-response pairs from the update corpus using self instruction generation.
  2. Combine these pairs with unrelated instruction-response pairs to create the fine-tuning dataset.
  3. Fine-tune the pretrained model using context-aware distillation, incorporating source articles as context.
  4. Evaluate the updated model on both related and unrelated instructions to assess performance.

- Design tradeoffs:
  - Including source articles as context improves factual consistency but increases the length of training samples, potentially leading to truncation.
  - Using a large number of unrelated instruction-response pairs prevents over-adjustment but may slow down learning of new information.
  - Generating instruction-response pairs using the model itself requires no human intervention but may introduce errors or biases.

- Failure signatures:
  - If the model consistently generates responses based on outdated knowledge, it may indicate that exposure bias is not being effectively mitigated.
  - If the model performs poorly on unrelated instructions, it may suggest that the unrelated instruction-response pairs are not sufficiently diverse or numerous.
  - If the model's responses are inconsistent with the source articles, it may indicate issues with the context-aware distillation approach or the quality of the generated instruction-response pairs.

- First 3 experiments:
  1. Compare the factual consistency scores of the updated model on related instructions with and without context-aware distillation to assess its effectiveness in mitigating exposure bias.
  2. Evaluate the model's performance on unrelated instructions to ensure that learning new information does not negatively impact its ability to follow existing instructions.
  3. Analyze the generated instruction-response pairs to identify any patterns or biases introduced by the self instruction generation process.

## Open Questions the Paper Calls Out

1. **Theoretical Foundations**: What is the minimal sufficient statistic for information in text corpora with respect to instruction-response pairs? The paper defines information in text corpus but acknowledges it's non-rigorous without proof of existence.

2. **Scaling to Larger Models**: Does the exposure bias problem exist in larger language models with hundreds of billions of parameters? The authors explicitly state they leave exploration on larger models for future work.

3. **Generalization Across Domains**: How effective is the context-aware distillation approach on domains other than news articles? The paper only uses news text corpus and states additional experiments are required to validate effectiveness on other domains.

4. **Pretraining Stage Analysis**: Does the exposure bias problem exist during the pretraining stage due to the order in which textual data is provided? The authors suggest the exposure bias may also exist during pretraining due to data order.

5. **Integration with Advanced Model Editing**: How can the exposure bias analysis be combined with advanced model editing approaches to further enhance Self Information Updates? The authors envision this as a potential extension of their work.

6. **Continual Self Information Update**: How can Continual Self Information Update be implemented with a stream of information update corpora? The authors mention maintaining an experience-replay buffer for previous updates as future work.

7. **Interaction with Retrieval-Augmented Models**: How does the context-aware distillation approach compare to retrieval-augmented language models for information updates? The paper discusses RALMs as an alternative but notes they are temporary solutions.

## Limitations

- The quantitative impact of exposure bias requires more empirical validation beyond theoretical reasoning
- The context-aware distillation approach's effectiveness may vary depending on the quality and relevance of source articles
- The optimal ratio and sampling strategy for unrelated instruction-response pairs lacks empirical validation

## Confidence

- Exposure bias analysis: Medium - The concept is well-established but the specific quantitative claims need more empirical support
- Context-aware distillation: Medium - Shows positive results but implementation details are sparse
- Unrelated instruction sampling: Low - Theoretical justification exists but lacks empirical validation

## Next Checks

1. **Gradient analysis validation**: Measure the actual gradient contributions from pretraining vs. update corpus tokens during fine-tuning to empirically verify the exposure bias claims.

2. **Context length impact study**: Systematically evaluate how different levels of context truncation affect factual consistency scores to determine the practical limits of the context-aware approach.

3. **Unrelated instruction ratio optimization**: Conduct ablation studies with varying ratios of unrelated to related instruction-response pairs to find the optimal balance for preventing over-adjustment while maintaining learning efficiency.