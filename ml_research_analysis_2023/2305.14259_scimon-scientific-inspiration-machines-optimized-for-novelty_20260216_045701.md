---
ver: rpa2
title: 'SciMON: Scientific Inspiration Machines Optimized for Novelty'
arxiv_id: '2305.14259'
source_url: https://arxiv.org/abs/2305.14259
tags:
- context
- language
- neighbors
- generation
- background
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel task of contextualized literature-based
  discovery (C-LBD), where the goal is to generate novel scientific ideas grounded
  in natural language contexts. The authors develop a framework that retrieves heterogeneous
  inspirations from knowledge graphs, citation networks, and semantic neighbors to
  enrich the input context.
---

# SciMON: Scientific Inspiration Machines Optimized for Novelty

## Quick Facts
- **arXiv ID:** 2305.14259
- **Source URL:** https://arxiv.org/abs/2305.14259
- **Reference count:** 40
- **Primary result:** Retrieval-augmented models improve hypothesis generation but struggle with novelty even with GPT-3.5 and GPT-4

## Executive Summary
This paper introduces a novel task of contextualized literature-based discovery (C-LBD) that aims to generate novel scientific ideas grounded in natural language contexts. The authors develop a framework that retrieves heterogeneous inspirations from knowledge graphs, citation networks, and semantic neighbors to enrich the input context. Using these inspirations, the model generates idea sentences or predicts new nodes through retrieval-augmented models including T5, GPT-3.5, and specialized encoders. Experiments on a dataset of 67,408 AI papers show that retrieval augmentation improves over baselines, but the task remains challenging even for large language models. Human evaluation reveals that generated ideas tend to be incremental and lack sufficient technical detail.

## Method Summary
The framework operates by first extracting background context from scientific papers using information extraction techniques. For each instance, the model retrieves semantic neighbors, knowledge graph neighbors, and citation neighbors to provide heterogeneous inspirations. These are combined with the input context and fed into generation models (T5, GPT-3.5) or prediction models (dual-encoder architectures). The generation models use an in-context contrastive objective to reduce copying from the input and encourage novelty. For node prediction, dual-encoder models encode seed entities and context separately from candidate nodes, then compute cosine similarity for ranking. The system is evaluated on both idea sentence generation and idea node prediction tasks using automated metrics and human evaluation.

## Key Results
- Retrieval-augmented models show measurable improvement over non-retrieval baselines on both generation and prediction tasks
- GPT-4 and GPT-3.5 generate ideas with low technical depth and novelty despite their capabilities
- Citation neighbors provide the most relevant context for idea generation compared to semantic or KG neighbors
- Human evaluation confirms generated ideas are often perceived as incremental and lacking sufficient detail

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval augmentation from heterogeneous sources improves hypothesis generation by providing relevant context beyond input text
- Mechanism: The model retrieves semantic neighbors, knowledge graph neighbors, and citation neighbors to enrich the background context before generation
- Core assumption: Relevant scientific ideas can be found by linking to related concepts, papers, and methods in existing literature
- Evidence anchors:
  - [abstract]: "Our models improve over baselines, including powerful large language models (LLMs), but also reveal challenges on the road to building machines that generate new scientific knowledge"
  - [section]: "Comprehensive evaluations reveal that GPT-4 tends to generate ideas with overall low technical depth and novelty, while our methods partially mitigate this issue"
- Break condition: If the retrieval sources don't contain relevant information for the given context, or if the retrieval quality is poor, the augmentation won't help and may even harm performance by introducing noise

### Mechanism 2
- Claim: In-context contrastive objective reduces copying from background context and encourages novelty in generated ideas
- Mechanism: The model adds sentences from the background context as negative examples in the contrastive loss, making the model less likely to copy phrases directly from input
- Core assumption: Excessive copying from background context reduces novelty and that distinguishing between input context and generated output is learnable
- Evidence anchors:
  - [abstract]: "we use retrieval of 'inspirations' from past scientific papers, and explicitly optimizes for novelty by iteratively comparing to prior papers and updating idea suggestions until sufficient novelty is achieved"
  - [section]: "We observe that the generation models tend to copy phrases from the background context... We introduce a new in-context contrastive objective, where negative examples are taken from the text in the input context"
- Break condition: If the background context contains phrases that are essential to the generated idea, or if the contrastive learning doesn't properly distinguish between relevant and irrelevant context, the objective may harm generation quality

### Mechanism 3
- Claim: Dual-encoder architecture with contrastive learning enables efficient link prediction in non-canonicalized knowledge graphs
- Mechanism: The model encodes seed entity and background context together, and candidate nodes separately, then computes cosine similarity between representations for ranking
- Core assumption: Scientific entities and their relationships can be effectively represented in dense vector space and that cosine similarity is a good proxy for semantic relatedness
- Evidence anchors:
  - [abstract]: "We present a new modeling framework using retrieval of 'inspirations' from a heterogeneous network of citations and knowledge graph relations"
  - [section]: "For node prediction, we explore dual-encoder (Wang et al., 2021), which relies on encoder language models such as BERT (Devlin et al., 2019) and has recently been used in link prediction (Safavi et al., 2022)"
- Break condition: If the vector representations don't capture the relevant semantic relationships, or if the knowledge graph contains too much noise, the ranking will be ineffective

## Foundational Learning

- Concept: Information extraction from scientific text
  - Why needed here: The entire dataset and task relies on extracting scientific entities, relations, and background context from paper abstracts
  - Quick check question: What are the main challenges in extracting scientific entities from natural language text compared to general text?

- Concept: Knowledge graph construction and link prediction
  - Why needed here: The model needs to predict new links between scientific concepts and generate new concepts, which requires understanding knowledge graph structure
  - Quick check question: How does link prediction in non-canonicalized knowledge graphs differ from canonicalized ones?

- Concept: Contrastive learning and negative sampling
  - Why needed here: The in-context contrastive objective and the dual-encoder model both rely on contrastive learning principles
  - Quick check question: What are the trade-offs between different types of negative samples (in-batch, pre-batch, self-negatives) in contrastive learning?

## Architecture Onboarding

- Component map: Background context + Seed term → Retrieval Module (Semantic Neighbors + KG Neighbors + Citation Neighbors) → Generation Module (T5/Dual-Encoder) → Output
- Critical path: Background context + Seed term → Retrieval → Generation → Evaluation
- Design tradeoffs: T5 provides flexibility for generation but may overfit; Dual-encoder is more efficient for link prediction but less expressive; Retrieval adds computational overhead but improves context
- Failure signatures: Low novelty (excessive copying from context); Poor relevance (retrieved neighbors don't match ground truth); Low diversity (beam search produces similar outputs)
- First 3 experiments:
  1. Test retrieval quality by checking cosine similarity between retrieved neighbors and ground truth
  2. Compare generation quality with and without in-context contrastive objective
  3. Evaluate different retrieval sources (semantic vs KG vs citation) independently to determine their individual contributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop more effective evaluation metrics for novel scientific ideas generated by language models?
- Basis in paper: [explicit] The paper highlights that evaluating generated scientific hypotheses is highly problematic due to the vast space of potentially plausible hypotheses formulated in natural language. The authors introduce new similarity-based metrics for idea node prediction but acknowledge their limitations.
- Why unresolved: Current evaluation metrics like ROUGE, BERTScore, and BARTScore show low variation across models and may not capture the true novelty and quality of generated ideas. The paper demonstrates that even human evaluation is challenging due to the subjective nature of scientific idea assessment.
- What evidence would resolve it: Developing a comprehensive evaluation framework that combines automated metrics, human evaluation, and potentially expert peer review to assess the novelty, feasibility, and potential impact of generated scientific ideas.

### Open Question 2
- Question: How can we improve the retrieval of heterogeneous inspirations to enhance the generation of novel scientific ideas?
- Basis in paper: [explicit] The paper explores retrieving inspirations from knowledge graphs, citation networks, and semantic neighbors to enrich the input context. However, the quality of the retrieval graph significantly impacts the final performance, and many test instances do not have any KG neighbors due to the sparsity of the non-canonicalized KG.
- Why unresolved: The current retrieval methods may not capture all relevant information needed for generating novel ideas. The paper shows that citation neighbors provide the most relevant context, but there may be other valuable sources of information not explored.
- What evidence would resolve it: Conducting experiments with additional retrieval sources such as related papers, patents, or even multimodal data (e.g., figures, tables) to determine their impact on the quality and novelty of generated ideas.

### Open Question 3
- Question: How can we reduce the tendency of language models to copy from the input context while generating novel scientific ideas?
- Basis in paper: [explicit] The paper introduces an in-context contrastive objective to mitigate excessive copying from the input and encourage novelty. While this approach shows some improvement, the generated ideas still tend to be incremental and lack sufficient detail.
- Why unresolved: The current in-context contrastive objective may not be sufficient to fully address the copying issue. The paper demonstrates that even with advanced large language models, generating truly novel and meaningful scientific concepts remains challenging.
- What evidence would resolve it: Experimenting with different strategies to encourage novelty, such as adversarial training, reinforcement learning with rewards for novelty, or incorporating external knowledge bases to provide additional context and constraints during generation.

## Limitations

- Generated ideas tend to be incremental and lack technical depth even with state-of-the-art models like GPT-4
- Evaluation metrics show low variation across models and may not capture true novelty and quality
- Many test instances lack knowledge graph neighbors due to the sparsity of the non-canonicalized knowledge graph

## Confidence

**High Confidence:**
- The task definition of contextualized literature-based discovery is well-defined and practically implementable
- Retrieval-augmented models show measurable improvement over non-retrieval baselines
- The evaluation methodology using multiple metrics (ROUGE-L, BARTScore, BERTScore, MRR, Hits) is appropriate for the task

**Medium Confidence:**
- The in-context contrastive objective effectively reduces copying from background context
- Heterogeneous retrieval sources (semantic neighbors, KG neighbors, citation neighbors) provide complementary benefits
- The dual-encoder architecture with contrastive learning is superior for link prediction in non-canonicalized knowledge graphs

**Low Confidence:**
- The generated ideas achieve sufficient novelty to be considered truly innovative scientific concepts
- The proposed framework can scale to generate ideas across diverse scientific domains beyond AI papers
- The human evaluation results are representative of broader scientific community judgment

## Next Checks

1. **Ablation study on retrieval sources**: Systematically disable each retrieval source (semantic neighbors, knowledge graph neighbors, citation neighbors) individually and measure the impact on novelty scores and generation quality to determine which sources contribute most to idea generation.

2. **Diversity analysis of generated outputs**: Compute pairwise similarity scores between multiple outputs generated for the same input context to quantify the diversity of the model's responses and identify whether the contrastive learning objective effectively encourages varied outputs.

3. **Long-term novelty assessment**: Track the citations and adoption of generated ideas over time in the scientific community to evaluate whether the proposed framework can generate ideas that achieve lasting impact beyond immediate evaluation metrics.