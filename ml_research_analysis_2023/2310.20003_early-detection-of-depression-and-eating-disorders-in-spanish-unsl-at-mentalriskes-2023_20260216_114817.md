---
ver: rpa2
title: 'Early Detection of Depression and Eating Disorders in Spanish: UNSL at MentalRiskES
  2023'
arxiv_id: '2310.20003'
source_url: https://arxiv.org/abs/2310.20003
tags:
- unsl
- task
- were
- tasks
- early
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper describes the participation of a research group in the
  MentalRiskES 2023 challenge, which aims to detect early signs of mental disorders
  in Spanish-language Telegram users. The group focused on Tasks 1 (eating disorders)
  and 2 (depression) using binary classification (subtask A).
---

# Early Detection of Depression and Eating Disorders in Spanish: UNSL at MentalRiskES 2023

## Quick Facts
- arXiv ID: 2310.20003
- Source URL: https://arxiv.org/abs/2310.20003
- Reference count: 19
- Second-place ranking in both classification and latency-based evaluations for early detection of mental disorders in Spanish

## Executive Summary
This paper presents the participation of the UNSL research group in the MentalRiskES 2023 challenge, focusing on early detection of depression and eating disorders in Spanish-language Telegram users. The approach combines BERT-based models (BETO) with extended vocabulary and a decision policy based on historic predictions. The system achieved second-place rankings in both classification-based and latency-based evaluations, demonstrating the effectiveness of their approach for mental health monitoring in Spanish.

## Method Summary
The approach involves fine-tuning BERT-based models with extended vocabulary of task-specific important words, combined with a decision policy based on historic rule. The extended vocabulary was created by selecting words with high confidence for the positive class from an external SS3 model. The decision policy triggers classification only after consecutive predictions exceed a threshold, balancing early detection with accuracy. Data augmentation was performed by splitting user posts into thirds to increase training samples.

## Key Results
- Second-place ranking in classification-based evaluation (macro-F1, ERDE-5, ERDE-30, latency-weighted F1)
- Second-place ranking in latency-based evaluation (macro-F1, ERDE-5, ERDE-30)
- Best results achieved with 25-50 extended vocabulary words for both tasks
- Decision policy parameters: 0.7 threshold, 10 tolerance, 5 min_delay, and 0.8 threshold, 5 tolerance, 5 min_delay for tasks 1 and 2 respectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extended vocabulary improves task-specific classification by adding domain-relevant terms.
- Mechanism: The model fine-tunes BERT with additional task-specific words extracted from SS3, which boosts the model's ability to recognize important cues for each mental disorder.
- Core assumption: Words selected from SS3 with high confidence for the positive class are genuinely discriminative for the target tasks.
- Evidence anchors:
  - [abstract]: "One of the models presented an extended vocabulary with important words for each task to be solved."
  - [section]: "We selected the best words according to the confidence values on the positive class. For Task 1, ayuno (fasting), cals (calories acronym), atracones (binge eating), and for Task 2, decepcionada (disappointed), suicidarme (to commit suicide), and daÃ±o (damage) are some examples of important words."
  - [corpus]: Weak/no direct evidence; assumed from external SS3 model.
- Break condition: If the added words are not truly discriminative or if the SS3 confidence measure is unreliable, the vocabulary extension could hurt rather than help performance.

### Mechanism 2
- Claim: The historic rule-based decision policy balances early detection with classification accuracy.
- Mechanism: The policy triggers a classification decision only after a certain number of consecutive predictions exceed a threshold, reducing false positives and ensuring confidence before labeling a user as positive.
- Core assumption: The threshold probability and tolerance parameters can be tuned to optimize both detection speed and accuracy for each task.
- Evidence anchors:
  - [abstract]: "we applied a decision policy based on the history of predictions that the model performs during user evaluation."
  - [section]: "If the current prediction and last M predictions exceed T times a Threshold, the client application must issue a risky user alarm; otherwise, it is necessary to continue the user evaluation."
  - [corpus]: Weak/no direct evidence; assumed from mock server evaluation.
- Break condition: If the threshold is set too high, detection will be delayed; too low, and false positives will increase.

### Mechanism 3
- Claim: Data augmentation by splitting user posts into thirds increases effective training samples and helps the model generalize.
- Mechanism: Each user's post sequence is divided into three equal parts, each labeled with the user's overall label, expanding the training set and allowing the model to learn from different segments of user history.
- Core assumption: Shorter post sequences preserve enough context for the model to learn relevant patterns and that the label is consistent across segments.
- Evidence anchors:
  - [section]: "For each user, we divided the list of posts into three equal parts according to the list length. Each portion was labeled using the user's label and added to the training set."
  - [corpus]: No direct evidence; assumption based on method description.
- Break condition: If context is lost in shorter segments or if the label is not consistent, augmentation may introduce noise and degrade performance.

## Foundational Learning

- Concept: BERT fine-tuning for classification
  - Why needed here: The core model is BERT-based; understanding how fine-tuning adapts pre-trained weights for the specific binary classification task is essential.
  - Quick check question: What loss function and optimizer are typically used when fine-tuning BERT for binary classification?

- Concept: Early risk detection frameworks
  - Why needed here: The system uses a two-component approach (classification + decision policy) to decide when to stop evaluating a user; understanding this framework is key to replicating or extending the approach.
  - Quick check question: What are the two main components of the early detection framework described, and how do they interact?

- Concept: Evaluation metrics for early detection
  - Why needed here: The system is evaluated using both classification accuracy and latency-based metrics (ERDE); knowing how these metrics are calculated and interpreted is necessary to assess performance.
  - Quick check question: What is the difference between ERDE-5 and ERDE-30, and what do they measure?

## Architecture Onboarding

- Component map: Data Preprocessing -> BETO Model (with optional extended vocabulary) -> Decision Policy (historic rule) -> Evaluation (mock server)
- Critical path:
  1. Load and preprocess training data.
  2. Augment data by splitting user posts into thirds.
  3. Fine-tune BETO model (with or without extended vocabulary).
  4. Use mock server to tune decision policy parameters.
  5. Evaluate on test set using classification and latency metrics.
- Design tradeoffs:
  - Vocabulary extension vs. model complexity: Adding words increases model size and may risk overfitting if words are not truly discriminative.
  - Decision policy strictness vs. detection speed: Stricter thresholds delay detection but may improve accuracy; looser thresholds speed detection but may increase false positives.
  - Data augmentation vs. context loss: Splitting posts increases samples but may lose contextual information.
- Failure signatures:
  - Low classification accuracy: Possible overfitting, poor vocabulary extension, or inadequate fine-tuning.
  - High latency metrics (ERDE): Decision policy too conservative, model uncertain in predictions.
  - Model overfitting: Small dataset with vocabulary extension without regularization.
- First 3 experiments:
  1. Train baseline BETO model (no vocabulary extension) and evaluate on validation set.
  2. Add 25 task-specific words to vocabulary and retrain; compare validation performance.
  3. Tune decision policy parameters (threshold, tolerance, min_delay) using mock server on validation set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance of the models be improved for ERDE-5 without compromising the other metrics?
- Basis in paper: [explicit] The paper states that "the results for ERDE-5 were less favorable" and suggests "it would be interesting to explore potential strategies to enhance the performance of the models for ERDE-5 without compromising the other metrics."
- Why unresolved: The paper identifies this as an area for future work but does not provide specific strategies or solutions.
- What evidence would resolve it: Empirical results demonstrating improved ERDE-5 performance while maintaining or improving other metrics, along with a detailed explanation of the strategies used.

### Open Question 2
- Question: What new decision policies could be evaluated to improve the performance of the models in terms of latency?
- Basis in paper: [explicit] The paper suggests "it would be worth analyzing new decision policies prioritizing speed and efficiency" as a future work direction.
- Why unresolved: The paper only evaluates one decision policy (historic rule) and identifies the need for exploring alternatives without specifying which ones.
- What evidence would resolve it: Comparative results showing the performance of different decision policies on latency metrics, along with analysis of their trade-offs and effectiveness.

### Open Question 3
- Question: How can the classification models be refined to better handle the subjectivity in user expressions across different domains?
- Basis in paper: [explicit] The paper notes that "Task 2 was more challenging than Task 1" due to "the subjectivity level with which users expressed themselves in each domain" and suggests this "may have impacted the performance of the models."
- Why unresolved: The paper identifies the problem but does not propose specific methods for addressing the subjectivity in user expressions.
- What evidence would resolve it: Improved model performance on both tasks, along with detailed analysis of how the refinements addressed the subjectivity issue and improved generalization across domains.

## Limitations

- The vocabulary extension mechanism depends entirely on an external SS3 model's ability to identify truly discriminative words, with no validation provided that these words are genuinely task-specific.
- The decision policy parameters appear to be tuned on the trial set, raising concerns about potential overfitting to the specific evaluation environment.
- The data augmentation strategy assumes label consistency across post segments, which may not hold if user behavior changes over time.

## Confidence

**High Confidence**: The overall framework of using BERT-based models with decision policies for early detection is sound and well-established in the literature. The preprocessing pipeline and basic fine-tuning approach are standard and likely to work as described.

**Medium Confidence**: The specific vocabulary extension method using SS3-extracted words is plausible but unverified. While the examples provided appear relevant, we cannot confirm they are optimal or even beneficial without seeing the full vocabulary and ablation studies.

**Low Confidence**: The effectiveness of the decision policy parameters and data augmentation strategy is questionable. The paper provides limited justification for the specific parameter choices, and the augmentation method's impact on model performance is not rigorously evaluated through controlled experiments.

## Next Checks

1. **Vocabulary Ablation Study**: Systematically test models with different numbers of extended vocabulary words (0, 5, 10, 25, 50) and measure the impact on both classification accuracy and latency metrics to determine if the vocabulary extension provides genuine benefit or potential overfitting.

2. **Decision Policy Sensitivity Analysis**: Vary the decision policy parameters (threshold values, tolerance levels, minimum delay) across a wide range and plot the tradeoff between early detection speed (ERDE metrics) and classification accuracy (F1 scores) to identify optimal operating points and potential brittleness.

3. **Label Consistency Verification**: Analyze the distribution of labels across the augmented post segments to confirm that splitting posts into thirds preserves label consistency. If significant label variation exists within user post sequences, this would invalidate the augmentation approach and suggest the need for alternative data expansion strategies.