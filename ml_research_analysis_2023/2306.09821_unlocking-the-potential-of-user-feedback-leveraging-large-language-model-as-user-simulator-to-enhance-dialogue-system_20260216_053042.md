---
ver: rpa2
title: 'Unlocking the Potential of User Feedback: Leveraging Large Language Model
  as User Simulator to Enhance Dialogue System'
arxiv_id: '2306.09821'
source_url: https://arxiv.org/abs/2306.09821
tags:
- user
- dialogue
- satisfaction
- system
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes leveraging large language models (LLMs) as
  annotation-free user simulators to provide satisfaction feedback for optimizing
  task-oriented dialogue (TOD) systems. The approach, User-Guided Response Optimization
  (UGRO), uses an LLM to predict user satisfaction scores for generated responses,
  which are then used as rewards in reinforcement learning to fine-tune a smaller
  TOD model.
---

# Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulator to Enhance Dialogue System

## Quick Facts
- arXiv ID: 2306.09821
- Source URL: https://arxiv.org/abs/2306.09821
- Authors: 
- Reference count: 29
- Primary result: UGRO achieves new state-of-the-art results with 34.3 BLEU and 31.7 ROUUGE on MultiWoZ 2.1

## Executive Summary
This paper proposes leveraging large language models (LLMs) as annotation-free user simulators to provide satisfaction feedback for optimizing task-oriented dialogue (TOD) systems. The approach, User-Guided Response Optimization (UGRO), uses an LLM to predict user satisfaction scores for generated responses, which are then used as rewards in reinforcement learning to fine-tune a smaller TOD model. Experiments on MultiWoZ 2.1 and Schema-Guided Dialogue datasets show that UGRO significantly improves response quality, achieving new state-of-the-art results with 34.3 BLEU and 31.7 ROUGE on MultiWoZ 2.1, outperforming previous methods.

## Method Summary
The UGRO approach trains a smaller TOD model through supervised learning, then uses an LLM (ChatGPT) as an annotation-free user simulator to predict satisfaction scores for generated responses. These scores serve as rewards in a Proximal Policy Optimization (PPO) framework to further optimize the TOD model. The LLM provides satisfaction scores with explanations through carefully designed prompts incorporating Chain-of-Thought reasoning. This creates a separation of roles where the LLM acts as a deliberative feedback provider while the TOD model serves as an intuitive response generator, following a System 1/System 2 cognitive framework.

## Key Results
- Achieved new state-of-the-art results on MultiWoZ 2.1 with 34.3 BLEU and 31.7 ROUUGE
- Demonstrated that LLM-based user simulation provides comparable performance to fine-tuned models without requiring extensive annotation
- Human evaluation confirmed improvements in user satisfaction and semantic quality
- Significant performance gains observed when increasing few-shot examples from 0 to 6 for LLM satisfaction prediction

## Why This Works (Mechanism)

### Mechanism 1
Using an LLM as an annotation-free user simulator provides satisfaction feedback that improves the smaller TOD model through reinforcement learning. The LLM predicts satisfaction scores for generated responses based on dialogue history, serving as rewards in a PPO-based optimization loop. Core assumption: The LLM's satisfaction prediction is sufficiently accurate and aligned with real user preferences. Evidence anchors: abstract and section descriptions of the UGRO approach. Break condition: If LLM predictions become misaligned with actual user preferences.

### Mechanism 2
The zero-shot Chain-of-Thought prompting approach enables the LLM to provide reliable satisfaction scores with explanations. Including an "explanation reason" slot in the prompt encourages the LLM to provide both a satisfaction score and its reasoning. Core assumption: LLMs can effectively reason about dialogue satisfaction with appropriate context and prompting. Evidence anchors: abstract and section descriptions of the prompting approach. Break condition: If LLM reasoning becomes inconsistent or explanations don't align with scores.

### Mechanism 3
The separation of roles between the LLM (reasoning/knowledge) and the smaller TOD model (intuitive response generation) creates an effective system architecture. The LLM acts as a deliberative System 2 component providing feedback, while the smaller TOD model acts as a System 1 component generating responses. Core assumption: LLM's reasoning capabilities can effectively guide the smaller model's intuitive response generation. Evidence anchors: abstract and section descriptions of the psychological framework. Break condition: If feedback becomes too complex for the smaller model to incorporate.

## Foundational Learning

- **Concept: Reinforcement Learning with Proximal Policy Optimization (PPO)**
  - Why needed here: UGRO uses PPO to optimize the TOD model based on satisfaction rewards from the LLM simulator
  - Quick check question: How does PPO address the challenge of policy updates being too large in reinforcement learning?

- **Concept: Task-Oriented Dialogue Systems**
  - Why needed here: The paper builds on existing TOD architectures and datasets (MultiWoZ 2.1, Schema-Guided Dialogue)
  - Quick check question: What are the key differences between task-oriented dialogue systems and open-domain conversational agents?

- **Concept: Large Language Model Prompting and In-Context Learning**
  - Why needed here: The LLM is used as a user simulator through carefully designed prompts
  - Quick check question: How does the inclusion of Chain-of-Thought reasoning in prompts affect LLM output quality for reasoning tasks?

## Architecture Onboarding

- **Component map:** Dialogue history and ground truth responses -> TOD Model (Flan-T5 large) -> LLM User Simulator (ChatGPT) -> PPO Optimizer -> Optimized TOD model generating higher-satisfaction responses

- **Critical path:** 1. TOD model generates response given dialogue history 2. LLM user simulator evaluates response and predicts satisfaction score 3. PPO optimizer uses satisfaction score as reward to update TOD model 4. Process repeats for multiple training iterations

- **Design tradeoffs:** Using LLM as simulator vs. fine-tuning (annotation-free but potentially less accurate), PPO vs. supervised fine-tuning (leverages satisfaction feedback but requires careful reward shaping), Chain-of-Thought prompting (improves reasoning but adds complexity)

- **Failure signatures:** Satisfaction scores become consistently high or low regardless of response quality, PPO training becomes unstable or fails to converge, generated responses become overly generic to maximize satisfaction scores

- **First 3 experiments:** 1. Validate LLM user simulator accuracy on satisfaction prediction task with different prompt configurations 2. Test UGRO optimization on a small subset of MultiWoZ to verify satisfaction rewards lead to improved BLEU/ROUGE scores 3. Compare UGRO performance against supervised-only baseline to measure impact of user feedback optimization

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the performance of LLMs as user simulators vary across different dialogue domains beyond task-oriented dialogues? Basis: The paper mentions extending the method to new domains with minimal or zero examples. Unresolved because experiments only evaluated on MultiWoZ 2.1 and Schema-Guided Dialogue datasets.

- **Open Question 2:** What is the optimal number of examples to include in few-shot prompting for user satisfaction prediction with LLMs? Basis: The paper tested performance with 0 and 6 examples, finding significant improvement with 6 examples. Unresolved because the paper did not explore intermediate values or test for diminishing returns.

- **Open Question 3:** How does the LLM user simulator performance degrade when the training data has different distributions or characteristics compared to the test data? Basis: The paper notes that dataset imbalance affects satisfaction prediction in ChatGPT. Unresolved because the paper only evaluated on standard test sets without systematically manipulating dataset characteristics.

## Limitations

- The effectiveness of LLM-based satisfaction prediction as a reward signal remains uncertain, particularly regarding alignment between LLM judgments and actual human preferences
- The psychological framework (System 1 vs System 2) is conceptually interesting but not empirically validated for this specific application
- The approach requires access to powerful LLMs (like ChatGPT), which may limit practical deployment due to cost and API dependency

## Confidence

- **High Confidence**: The technical implementation of UGRO using PPO and supervised fine-tuning is well-established and reproducible
- **Medium Confidence**: The experimental results showing improved BLEU/ROUGE scores are supported by the data, but long-term generalization and robustness remain untested
- **Low Confidence**: The assumption that LLM satisfaction predictions reliably capture user preferences across diverse dialogue scenarios requires further validation

## Next Checks

1. Conduct ablation studies removing the LLM feedback component to quantify its specific contribution to performance improvements
2. Perform extensive human evaluation across multiple domains to verify that LLM satisfaction scores align with actual user preferences
3. Test the approach on out-of-domain dialogues to assess robustness and generalization beyond the training datasets