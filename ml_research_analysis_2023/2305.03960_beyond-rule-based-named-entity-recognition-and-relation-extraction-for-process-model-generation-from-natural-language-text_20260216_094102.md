---
ver: rpa2
title: Beyond Rule-based Named Entity Recognition and Relation Extraction for Process
  Model Generation from Natural Language Text
arxiv_id: '2305.03960'
source_url: https://arxiv.org/abs/2305.03960
tags:
- uni00000057
- uni00000013
- process
- uni0000004c
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of automated business process
  model generation from natural language text by extending a prominent dataset (PET)
  with entity resolution annotations and developing new machine learning-based extraction
  methods. The authors enhance the baseline approach by replacing rule-based relation
  extraction with a CatBoost-based model and adding a neural entity resolution module.
---

# Beyond Rule-based Named Entity Recognition and Relation Extraction for Process Model Generation from Natural Language Text

## Quick Facts
- arXiv ID: 2305.03960
- Source URL: https://arxiv.org/abs/2305.03960
- Reference count: 20
- Primary result: ML-based extraction pipeline achieves comparable performance to rule-based baseline while offering better adaptability to new domains

## Executive Summary
This paper addresses automated business process model generation from natural language text by extending the PET dataset with entity resolution annotations and developing new machine learning-based extraction methods. The authors replace rule-based relation extraction with a CatBoost-based model and add a neural entity resolution module to the existing CRF-based mention extraction pipeline. They also evaluate a holistic deep learning approach (Jerex) for joint extraction. Experiments show that the proposed pipeline achieves comparable performance to the rule-based baseline while offering better adaptability to new domains and data. The neural entity resolution component outperforms naive surface-form matching, and the relation extraction method performs equally well or better than the baseline when given higher-quality inputs.

## Method Summary
The authors extend the PET dataset with entity resolution annotations and develop a three-stage pipeline for process model extraction: mention extraction using CRF, entity resolution using neural co-reference resolution with filtering, and relation extraction using CatBoost with negative sampling. The neural entity resolution predicts co-referent spans without relying solely on surface similarity, aligning spans with process elements and filtering predictions using threshold criteria. The CatBoost model is trained with negative sampling (rn=40) on features including tags, token distance, sentence distance, and neighboring context. The paper also evaluates Jerex, a holistic deep learning approach for joint extraction, as an alternative to the pipeline architecture.

## Key Results
- Neural entity resolution improves precision over naive surface-form matching
- ML-based relation extraction achieves performance comparable to rule-based methods
- Error propagation degrades pipeline performance, motivating joint models
- Pipeline achieves comparable performance to rule-based baseline while offering better adaptability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural entity resolution improves precision over naive surface-form matching.
- Mechanism: The neural model predicts co-referent spans without relying solely on surface similarity, aligning spans with process elements and filtering predictions using threshold criteria (αc=0.5, αm=0.8).
- Core assumption: Mentions of the same entity may have low surface similarity (e.g., "a claim" vs. "it") and thus require semantic understanding beyond overlap.
- Evidence anchors:
  - [abstract] "The neural entity resolution component outperforms naive surface-form matching"
  - [section] "We then align these predictions with mentions predicted in the mention extraction module to extract entities."
- Break condition: If the dataset contains mostly unambiguous surface-form mentions (high type-token ratio similarity), the neural module may add no benefit.

### Mechanism 2
- Claim: Machine learning-based relation extraction achieves performance comparable to rule-based methods.
- Mechanism: CatBoost model trained with negative sampling (rn=40) on features including tags, token distance, sentence distance, and neighboring context to predict relation types or "nothing".
- Core assumption: Relation extraction benefits from learning statistical patterns from annotated examples rather than hand-crafted rules, especially when rules are order-dependent and brittle.
- Evidence anchors:
  - [abstract] "replacing the rule-based relation extraction with a CatBoost-based model"
  - [section] "For each combination of head and tail mention of a relation we build features..."
- Break condition: If the dataset is too small or too noisy, the ML model may overfit or underperform compared to carefully engineered rules.

### Mechanism 3
- Claim: Error propagation degrades pipeline performance, motivating joint models.
- Mechanism: Errors in early steps (mention extraction) cascade into later steps (entity resolution, relation extraction), as strict evaluation counts any error in a mention as an error in all derived entities and relations.
- Core assumption: Downstream modules are evaluated on the outputs of upstream modules, so upstream errors cannot be corrected later.
- Evidence anchors:
  - [section] "Errors modules make during prediction are propagated further down the pipeline, potentially even amplifying in severity"
  - [section] "This cascade illustrates, why we opted for several scenarios that evaluate modules in isolation, or with some degree of ground-truth input."
- Break condition: If evaluation allows partial credit or if a joint model is trained end-to-end, error amplification may be mitigated.

## Foundational Learning

- Concept: Conditional Random Fields (CRF) for sequence tagging
  - Why needed here: CRF is used in the baseline for mention extraction because it models dependencies between neighboring tags, improving accuracy over independent token classification.
  - Quick check question: Why is a CRF preferred over a simple BIO tagger for named entity recognition in process text?

- Concept: Negative sampling in relation extraction
  - Why needed here: There are far more non-relation mention pairs (44,708) than relation pairs (1,916); negative sampling balances training and prevents the model from predicting "nothing" for all pairs.
  - Quick check question: What happens to precision if negative sampling is omitted in the CatBoost relation extraction module?

- Concept: Type-token ratio as a linguistic variability metric
  - Why needed here: High type-token ratio in mentions or entity arguments indicates surface form diversity, which increases difficulty for entity resolution and relation extraction.
  - Quick check question: How does a high type-token ratio in actor mentions affect the neural entity resolution performance?

## Architecture Onboarding

- Component map: Raw text → CRF-based mention extraction → Neural entity resolution → CatBoost relation extraction → Structured process model
- Critical path: Mention Extraction → Entity Resolution → Relation Extraction
  Each step's output is the next step's input; errors amplify downstream.
- Design tradeoffs:
  - Strict vs. lenient evaluation: Strict evaluation propagates errors but may overestimate difficulty; lenient evaluation is more realistic but harder to automate.
  - Pipeline vs. joint model: Pipeline allows modular debugging but suffers error propagation; joint model reduces propagation but is harder to train and debug.
  - Negative sampling rate: Higher rn improves precision but increases training time and may reduce recall.
- Failure signatures:
  - Low mention F1 → All downstream modules fail; check CRF features and tokenization.
  - Low entity resolution F1 → Surface form diversity or long intra-entity distances; inspect neural model's span predictions and filtering thresholds.
  - Low relation F1 → Class imbalance or long-range relations; check negative sampling and context window size.
- First 3 experiments:
  1. Run mention extraction alone with ground truth input to baseline F1; identify mention-level errors.
  2. Replace CRF with a simple BIO tagger; compare mention F1 to baseline; observe effect on entity resolution.
  3. Vary negative sampling rate (rn=10, 20, 40, 80) while keeping context size fixed; plot precision/recall curves for relation extraction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic features or patterns make certain relation types (like Same Gateway) particularly difficult to predict compared to others?
- Basis in paper: [explicit] The paper notes that Same Gateway relations are "problematic from a data balance perspective" and that the rule-based method "defines several rules, which predict only relations of one or two types at a time" applied in a fixed order.
- Why unresolved: The paper mentions the challenge but doesn't deeply analyze which linguistic features or contextual patterns specifically cause the difficulty for Same Gateway relations versus other relation types.
- What evidence would resolve it: Detailed linguistic analysis comparing successful versus unsuccessful Same Gateway predictions, identifying specific syntactic or semantic patterns that distinguish these relations.

### Open Question 2
- Question: How would less strict evaluation regimes affect the comparative assessment of different extraction approaches?
- Basis in paper: [explicit] The paper states "Future work could analyze, how less strict evaluation regimes correlate with user's expectations of predictions" and notes their "very strict evaluation regime leads to error propagation."
- Why unresolved: The current evaluation is described as "very strict," but the paper doesn't explore how more lenient evaluation criteria might change the relative performance rankings of different approaches.
- What evidence would resolve it: Comparative experiments using multiple evaluation regimes (strict, lenient, partial credit) to see how performance rankings change and which approach benefits most from relaxed criteria.

### Open Question 3
- Question: What is the optimal balance between negative sampling rate and context size for the CatBoost relation extraction model?
- Basis in paper: [explicit] The paper mentions "we train the module for i = 1000 iterations... using a negative sampling rate of rn = 40 and context size of c = 2" and notes that "A sampling rate rn≥ 40 improves the result quality significantly."
- Why unresolved: While the paper establishes that rn≥ 40 is beneficial, it doesn't explore the full parameter space to find the optimal balance between negative sampling rate and context window size.
- What evidence would resolve it: Systematic grid search experiments varying both rn and c parameters to identify the combination that maximizes F1 score while considering computational efficiency.

## Limitations
- Evaluation relies on a single dataset (PET) with 51 process descriptions, limiting generalizability
- Neural entity resolution performance may vary across domains and languages
- CatBoost relation extraction requires careful tuning of negative sampling rates
- Jerex holistic approach is presented as proof-of-concept rather than fully optimized solution

## Confidence
- Confidence in neural entity resolution improvement: **High**
- Confidence in CatBoost relation extraction performance: **Medium**
- Confidence in cross-domain generalizability: **Low**

## Next Checks
1. Evaluate the neural entity resolution component on a different domain (e.g., legal or medical process descriptions) to test cross-domain performance
2. Systematically vary the negative sampling rate (rn=10, 20, 40, 80) and context window size to identify optimal settings for different relation types
3. Compare error propagation patterns between the pipeline approach and the Jerex joint model using identical evaluation metrics and ground truth inputs