---
ver: rpa2
title: 'ChatGPT to Replace Crowdsourcing of Paraphrases for Intent Classification:
  Higher Diversity and Comparable Model Robustness'
arxiv_id: '2305.12947'
source_url: https://arxiv.org/abs/2305.12947
tags:
- chatgpt
- data
- taboo
- paraphrases
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares crowdsourced versus ChatGPT-generated paraphrases
  for intent classification. The authors collected paraphrases using ChatGPT following
  the same protocol as a prior crowdsourcing study, employing prompt and taboo modes.
---

# ChatGPT to Replace Crowdsourcing of Paraphrases for Intent Classification: Higher Diversity and Comparable Model Robustness

## Quick Facts
- arXiv ID: 2305.12947
- Source URL: https://arxiv.org/abs/2305.12947
- Reference count: 8
- ChatGPT-generated paraphrases show higher diversity and comparable model robustness to crowdsourced data

## Executive Summary
This paper demonstrates that ChatGPT can effectively replace crowdsourcing for generating paraphrases used in intent classification tasks. The authors collected paraphrases using ChatGPT following the same protocol as a prior crowdsourcing study, employing prompt and taboo modes. ChatGPT-generated paraphrases showed higher lexical and syntactic diversity than human-generated ones, with increased vocabulary size and tree edit distance. Models trained on ChatGPT data achieved comparable or slightly better robustness than those trained on crowdsourced data, with in-distribution accuracy around 99% and out-of-distribution accuracy between 76-99%. ChatGPT proved to be approximately 1000x cheaper than crowdsourcing while producing more diverse data.

## Method Summary
The authors collected paraphrases using ChatGPT with the same protocol as Larson et al. (2020), using 10 intent classes with 3 seed sentences each, generating 5 paraphrases per seed. They employed two modes: Prompt (no taboo words) and Taboo (3 or 6 taboo words). The collected data was evaluated for lexical diversity (vocabulary size), syntactic diversity (tree edit distance), and used to train intent classification models (BERT and SVM with TF-IDF). Model robustness was assessed through in-distribution and out-of-distribution accuracy comparisons between ChatGPT-generated and crowdsourced datasets.

## Key Results
- ChatGPT-generated paraphrases achieved higher lexical diversity (larger vocabulary) and syntactic diversity (higher tree edit distance) than crowdsourced paraphrases
- Models trained on ChatGPT data achieved in-distribution accuracy around 99% and out-of-distribution accuracy between 76-99%, comparable to crowdsourced models
- ChatGPT was approximately 1000x cheaper than crowdsourcing for generating paraphrases

## Why This Works (Mechanism)

### Mechanism 1
ChatGPT reliably generates valid paraphrases when given clear instructions, following natural language prompts effectively through its zero-shot prompting capabilities. This works because ChatGPT's instruction-following capabilities are well-developed for straightforward NLP tasks. The assumption that human worker protocols can be directly translated to ChatGPT may break if instructions are ambiguous or require complex reasoning.

### Mechanism 2
ChatGPT produces more diverse paraphrases due to its broader vocabulary access and generative capabilities compared to human workers constrained by personal language patterns. The assumption that diversity metrics accurately capture true diversity may be challenged if semantic equivalence isn't properly measured or if human workers were explicitly incentivized for creativity.

### Mechanism 3
Higher diversity in ChatGPT-generated paraphrases leads to better model generalization and robustness in intent classification. This assumes that model robustness measured by in-distribution and quasi out-of-distribution accuracy reflects true generalization ability. The comparison may be misleading if test data isn't truly out-of-distribution or if there's a ceiling effect due to limited seed diversity.

## Foundational Learning

- Concept: Intent classification task fundamentals
  - Why needed here: Understanding how intent classification works is crucial for interpreting results and implications of using different training data sources
  - Quick check question: What are the key challenges in intent classification that make diverse training data important?

- Concept: Paraphrase generation techniques
  - Why needed here: Knowing different approaches to paraphrase generation helps understand why ChatGPT might outperform human workers
  - Quick check question: How do taboo words and prompt variations affect the diversity of generated paraphrases?

- Concept: Evaluation metrics for NLP tasks
  - Why needed here: Understanding metrics like accuracy, vocabulary size, and tree edit distance is essential for interpreting results
  - Quick check question: What are the limitations of using accuracy as the sole measure of model robustness?

## Architecture Onboarding

- Component map: ChatGPT API integration -> Prompt engineering module -> Data collection pipeline -> Diversity analysis tools -> Model training/evaluation framework
- Critical path: Prompt generation → ChatGPT API call → Data validation → Diversity analysis → Model training → Robustness evaluation
- Design tradeoffs: Cost vs. diversity (ChatGPT is cheaper but may have taboo word compliance issues), control vs. automation (human workers can be directed more precisely but are more expensive)
- Failure signatures: Low diversity metrics might indicate insufficient prompt variation, poor model performance might suggest inadequate taboo word handling, high costs might indicate inefficient API usage
- First 3 experiments:
  1. Test different temperature settings to find optimal balance between diversity and coherence
  2. Experiment with alternative prompt structures to improve taboo word compliance
  3. Compare different diversity metrics to ensure comprehensive evaluation

## Open Questions the Paper Calls Out

- Open Question 1: At what point does ChatGPT begin producing too many duplicate paraphrases when repeatedly paraphrasing the same seed sentences? The paper acknowledges this as a limitation but doesn't investigate when ChatGPT starts producing excessive duplicates or explore methods to mitigate this issue.

- Open Question 2: How do ChatGPT-generated paraphrases perform on out-of-distribution test data from completely different domains compared to crowdsourced paraphrases? The authors acknowledge their evaluation suffers from a ceiling effect due to limited semantic diversity in seed samples.

- Open Question 3: Does the inclusion of tabooed samples in ChatGPT-generated data consistently improve model robustness across different intent classification tasks and domains? The study only tested this effect on financial intent classification data, and the mechanism behind why tabooed samples improve robustness is not fully understood.

## Limitations

- The study lacks direct comparison with human-generated paraphrases using identical evaluation metrics and model architectures
- The dataset size is relatively small (10 intent classes with 3 seed sentences each), limiting generalizability
- Robustness evaluation focuses on a specific set of out-of-distribution tests that may not generalize to other domains

## Confidence

- High confidence: ChatGPT's cost advantage (approximately 1000x cheaper than crowdsourcing) - straightforward calculation based on API costs and known crowdsourcing rates
- Medium confidence: ChatGPT produces more diverse paraphrases - supported by lexical and syntactic diversity metrics, but comparison methodology has limitations
- Medium confidence: Models trained on ChatGPT data achieve comparable robustness - supported by empirical results, but evaluation framework has potential ceiling effects

## Next Checks

1. Conduct a direct head-to-head comparison using identical evaluation metrics and model architectures on a subset of paraphrases from both sources to isolate true performance differences

2. Expand the robustness evaluation to include more challenging out-of-distribution scenarios and additional domains beyond the current intent classification task

3. Test taboo word compliance systematically by implementing automated taboo detection in generated paraphrases and measuring violation rates across different prompt variations