---
ver: rpa2
title: Levenshtein Distance Embedding with Poisson Regression for DNA Storage
arxiv_id: '2312.07931'
source_url: https://arxiv.org/abs/2312.07931
tags:
- embedding
- distance
- dimension
- levenshtein
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a neural network-based Levenshtein distance
  embedding technique using Poisson regression. The method first analyzes the impact
  of embedding dimension on model performance and selects an appropriate dimension
  (ESD) based on eigenvalue decay in the covariance matrix.
---

# Levenshtein Distance Embedding with Poisson Regression for DNA Storage

## Quick Facts
- **arXiv ID**: 2312.07931
- **Source URL**: https://arxiv.org/abs/2312.07931
- **Reference count**: 24
- **Primary result**: Proposes Poisson regression for Levenshtein distance embedding that outperforms state-of-the-art methods on DNA storage data

## Executive Summary
This paper introduces a neural network-based approach for Levenshtein distance embedding using Poisson regression. The method theoretically analyzes how embedding dimension affects approximation precision and proposes an early-stopping dimension criterion based on eigenvalue decay. Poisson regression is employed as the loss function, which naturally aligns with the Levenshtein distance definition and provides advantages in handling skewed distributions compared to traditional regression methods. The approach is evaluated on real DNA storage data and demonstrates superior performance compared to existing methods, with the CNN-10-w architecture achieving the best results.

## Method Summary
The method uses a Siamese neural network architecture to embed DNA sequences into fixed-dimensional vectors, where the squared Euclidean distance between vectors approximates the Levenshtein distance. A learnable scaling factor adjusts the embedding based on the average Levenshtein distance. Poisson regression is used as the loss function, assuming the Levenshtein distance follows a Poisson distribution for small distances. The embedding dimension is selected using an early-stopping criterion (ESD) determined by eigenvalue analysis of the covariance matrix of embedding differences, preventing overfitting while maintaining approximation precision.

## Key Results
- Poisson regression outperforms MSE, MAE, and chi-squared regression across all tested embedding dimensions
- CNN-10-w architecture with embedding dimension 140 achieves the highest performance
- Embedding dimension significantly impacts approximation precision, with optimal performance at the ESD of 120 for CNN-10
- The method shows superior performance on homologous sequence pairs (small Levenshtein distances)

## Why This Works (Mechanism)

### Mechanism 1
Embedding dimension influences approximation precision by controlling variance in the chi-squared distribution of approximated distances. The approximated distance between sequences follows a chi-squared distribution whose variance is inversely proportional to the embedding dimension n. Larger n reduces variance and improves precision.

Core assumption: Embedding elements are independent standard normal variables (A2), and the embedding vector has no information redundancy.

Evidence anchors:
- [abstract]: "We first provide a theoretical analysis of the impact of embedding dimension on model performance and present a criterion for selecting an appropriate embedding dimension."
- [section 3]: "it can be inferred that ui − vi follows N(0, 2), and 1/2(ui − vi)2 follows χ2(1)...the variance of approximations can be calculated as: Var[dl2 2(˜u, ˜v)] = 2d/k = 2dM/n."
- [corpus]: Weak; no direct comparison with other embedding dimension-based methods.

Break condition: Assumption A2 is violated (embedding elements are not independent), leading to non-chi-squared distributions and invalid variance formulas.

### Mechanism 2
Poisson regression naturally aligns with the definition of Levenshtein distance and provides asymmetric penalization that improves approximation precision. Levenshtein distance can be approximated as a Poisson-distributed random variable when the distance is small. Poisson regression uses the negative log-likelihood loss that is minimized at the ground truth distance, unlike MSE or MAE.

Core assumption: When Levenshtein distance is small, it can be interpreted as counting edit operations within a fixed interval (Poisson process).

Evidence anchors:
- [abstract]: "Poisson regression is introduced by assuming the Levenshtein distance between sequences of fixed length following a Poisson distribution, which naturally aligns with the definition of Levenshtein distance."
- [section 4]: "The Poisson distribution represents the distribution of events occurring within a fixed interval...we can approximate the Levenshtein distance as a Poisson-distributed random variable when the distance is significantly smaller than the length of the sequences."
- [corpus]: Weak; no direct comparison with other regression-based methods.

Break condition: When Levenshtein distances are large relative to sequence length, the Poisson approximation becomes inaccurate.

### Mechanism 3
Poisson regression approximates the negative log-likelihood of the chi-squared distribution when embedding dimension is large, preserving advantages of chi-squared regression while removing skewness. The negative log-likelihood of the chi-squared distribution converges to the Poisson negative log-likelihood as embedding dimension (k) approaches infinity, providing asymptotic equivalence.

Core assumption: The distribution of approximated distances follows a chi-squared distribution with parameter k that is proportional to embedding dimension.

Evidence anchors:
- [abstract]: "Poisson regression approximates the negative log likelihood of the chi-squared distribution and offers advancements in removing the skewness."
- [section 4]: "The PNLL in Equation (17) can be viewed as an approximation of the negative log-likelihood of distribution in Equation (10) when k is sufficiently large...when k → +∞, the PNLL in Equation (17) is the limit of Equation (21)."
- [corpus]: Weak; no direct comparison with chi-squared regression implementations.

Break condition: When embedding dimension is not sufficiently large, the Poisson approximation deviates significantly from the chi-squared negative log-likelihood.

## Foundational Learning

- **Chi-squared distribution and squared Euclidean distance**: The approximated Levenshtein distance theoretically follows a chi-squared distribution, and understanding this relationship is crucial for analyzing approximation precision and designing loss functions.
  - Quick check: If ui − vi follows N(0, 2), what distribution does 1/2(ui − vi)2 follow?

- **Poisson distribution and Poisson regression**: Poisson regression is used as the loss function, assuming Levenshtein distance follows a Poisson distribution for small distances, providing natural alignment and asymmetric penalization.
  - Quick check: What is the probability mass function of a Poisson distribution with expectation λ?

- **Spectral decomposition and eigenvalue analysis**: The early-stopping dimension (ESD) is determined by analyzing the eigenvalues of the covariance matrix of embedding differences, identifying when eigenvalues drop to zero.
  - Quick check: What happens to the sorted eigenvalues of a covariance matrix when the data lies in a lower-dimensional subspace?

## Architecture Onboarding

- **Component map**: Input layer (DNA sequences) -> Embedding network (CNN-5, CNN-10, CNN-5-w, CNN-10-w, or GRU) -> Output layer (embedding vectors) -> Loss function (Poisson negative log-likelihood) -> Training (Siamese network framework with learnable scaling factor)

- **Critical path**: 
  1. Load DNA sequence pairs and ground truth Levenshtein distances
  2. Pad sequences to fixed length (160)
  3. Pass through embedding network to get vectors u and v
  4. Compute squared Euclidean distance between u and v
  5. Apply learnable scaling factor
  6. Calculate PNLL loss
  7. Backpropagate and update network parameters

- **Design tradeoffs**: 
  - Embedding dimension vs. computational cost: Larger dimensions improve precision but increase computation
  - Network width vs. depth: Wider networks (more channels) improve performance more than deeper networks
  - Loss function choice: PNLL provides better asymmetric penalization than MSE/MAE but requires Poisson assumption validity

- **Failure signatures**:
  - Poor approximation on sequences with small Levenshtein distances (Poisson assumption violation)
  - Unstable training when embedding dimension exceeds ESD (eigenvalue decay)
  - Biased predictions for sequences with extended homopolymer runs (pattern observed in failed cases)

- **First 3 experiments**:
  1. Train CNN-5 with embedding dimension 40 and MSE loss to establish baseline performance
  2. Train CNN-5 with embedding dimension 80 and PNLL loss to test Poisson regression effectiveness
  3. Train CNN-5 with embedding dimension ESD (120) and PNLL loss to verify optimal dimension selection

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the width of the neural network architecture impact the upper bound on embedding dimension (ESD) compared to the depth of the network?
  - Basis: The paper states that "increasing the number of layers in the embedding network has little impact on enlarging the ESD, while enlarging the convolutional channels lead to a larger ESD."
  - Why unresolved: The paper provides observations on the impact of network width and depth on ESD, but does not provide a detailed theoretical explanation for why width plays a more significant role than depth.
  - What evidence would resolve it: Additional experiments comparing the ESD of networks with varying widths and depths, along with a theoretical analysis explaining the relationship between network architecture and ESD.

- **Open Question 2**: Can the proposed Poisson regression method be effectively applied to other sequence similarity metrics beyond Levenshtein distance?
  - Basis: The paper focuses on Levenshtein distance, but the Poisson regression method is proposed as a general approach to embedding sequence distances.
  - Why unresolved: The paper does not provide experiments or theoretical analysis on applying the Poisson regression method to other sequence similarity metrics.
  - What evidence would resolve it: Experiments applying the Poisson regression method to other sequence similarity metrics, along with a theoretical analysis of the conditions under which the method would be effective.

- **Open Question 3**: How does the choice of embedding dimension impact the performance of the proposed method on sequences with varying lengths?
  - Basis: The paper states that "sequences with smaller Levenshtein distance exhibit higher approximation precision" and that "the variance of approximations can be calculated as: Var[dl2 2 (˜u, ˜v)] = 2dM/n."
  - Why unresolved: The paper does not provide experiments or analysis on the impact of embedding dimension on sequences with varying lengths.
  - What evidence would resolve it: Experiments applying the proposed method to sequences with varying lengths, along with a theoretical analysis of the relationship between sequence length, embedding dimension, and approximation precision.

## Limitations

- **Assumption Sensitivity**: The method relies heavily on assumption A2 (embedding elements follow N(0,1) and are independent), which if violated would invalidate the chi-squared distribution of approximated distances and the variance formula.

- **Dataset Specificity**: Evaluation is conducted on a single DNA storage dataset with specific characteristics (sequences of length ~152, specific distribution of Levenshtein distances), limiting generalizability.

- **Embedding Dimension Selection**: While the ESD criterion is theoretically justified, the practical selection of ESD = 120 for CNN-10 architecture is based on eigenvalue decay analysis without providing sensitivity analysis for different architectures or datasets.

## Confidence

**High Confidence**:
- Poisson regression provides better asymmetric penalization than MSE/MAE for small Levenshtein distances
- Larger embedding dimensions reduce variance in approximated distances following chi-squared distribution
- CNN-10-w architecture outperforms other tested architectures on the evaluated dataset

**Medium Confidence**:
- Poisson regression approximates chi-squared negative log-likelihood for large embedding dimensions
- Early-stopping dimension (ESD) effectively prevents overfitting while maintaining precision
- The method outperforms all baseline approaches across different evaluation metrics

**Low Confidence**:
- Generalization of ESD selection criterion to different network architectures and datasets
- Robustness of Poisson assumption for diverse sequence datasets beyond the evaluated DNA storage data
- Performance stability across different random initializations (only 5 runs reported)

## Next Checks

1. Test the independence assumption A2 by computing autocorrelation between embedding elements across different sequence positions and validating normality using statistical tests (e.g., Shapiro-Wilk test) on embedding outputs.

2. Evaluate model performance on synthetic DNA datasets with controlled properties (varying sequence lengths, homopolymer content, GC content) to assess generalization beyond the specific dataset used in the paper.

3. Conduct ablation studies comparing Poisson regression loss with chi-squared regression implementation to empirically validate the claimed asymptotic equivalence when embedding dimension is large.