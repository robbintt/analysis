---
ver: rpa2
title: Few-Shot Panoptic Segmentation With Foundation Models
arxiv_id: '2309.10726'
source_url: https://arxiv.org/abs/2309.10726
tags:
- segmentation
- panoptic
- semantic
- training
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of panoptic segmentation, which
  requires significant annotated training data that is both time-consuming and expensive
  to obtain. The authors propose SPINO, a method that leverages unsupervised foundation
  models, specifically DINOv2, to enable few-shot panoptic segmentation.
---

# Few-Shot Panoptic Segmentation With Foundation Models

## Quick Facts
- arXiv ID: 2309.10726
- Source URL: https://arxiv.org/abs/2309.10726
- Reference count: 39
- Primary result: Enables few-shot panoptic segmentation with as few as 10 annotated images using DINOv2 foundation model

## Executive Summary
This paper introduces SPINO, a method that leverages unsupervised foundation models to enable few-shot panoptic segmentation. The approach uses DINOv2 features to generate high-quality panoptic pseudo-labels from minimal annotations, which can then train any existing panoptic segmentation model. SPINO achieves competitive results compared to fully supervised baselines while using less than 0.3% of the ground truth labels.

## Method Summary
SPINO combines a frozen DINOv2 backbone with lightweight network heads for semantic segmentation and boundary estimation. The method generates panoptic pseudo-labels from as few as 10 annotated images, which are then used to train panoptic segmentation models. A mixed-batch training strategy incorporates both pseudo-labels and ground truth samples to stabilize learning and prevent error propagation from noisy pseudo-labels.

## Key Results
- Achieves competitive panoptic segmentation performance using only 10 annotated images
- PQ/SQ/RQ metrics improve significantly over supervised baseline when using 100 annotated images
- Demonstrates effectiveness across multiple public (Cityscapes, KITTI-360) and in-house datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot panoptic segmentation is enabled by leveraging high-quality, semantically meaningful features from the DINOv2 foundation model, which requires no labeled data during its own training.
- Mechanism: The DINOv2 backbone provides frozen, task-agnostic visual features that already encode strong semantic and spatial information. By training lightweight task-specific heads (semantic segmentation and boundary estimation) on only ~10 annotated images, the system learns to generate high-quality pseudo-labels. These pseudo-labels are then used to train a full panoptic segmentation model, bypassing the need for large-scale manual annotation.
- Core assumption: The frozen DINOv2 features are sufficiently descriptive and semantically consistent to enable accurate segmentation and instance boundary detection even with minimal fine-tuning data.
- Evidence anchors:
  - [abstract] "our method combines a DINOv2 backbone with lightweight network heads for semantic segmentation and boundary estimation. We show that our approach, albeit being trained with only ten annotated images, predicts high-quality pseudo-labels..."
  - [section] "We leverage the recent foundation model DINOv2 [11] to extract descriptive image features for both semantic segmentation and boundary estimation."
  - [corpus] Weak: No corpus neighbor directly validates the DINOv2 + few-shot pseudo-label generation mechanism.
- Break condition: If DINOv2 features are not semantically discriminative enough for the target domain, or if the 10 annotated images are not representative, pseudo-label quality will degrade, breaking the bootstrapping loop.

### Mechanism 2
- Claim: The pseudo-label generation pipeline improves panoptic segmentation performance by explicitly modeling both semantic classes and object boundaries, enabling instance separation without requiring instance masks during training.
- Mechanism: The semantic segmentation head predicts per-pixel class labels, while the boundary estimation head predicts binary boundaries around object instances. During pseudo-label generation, connected component analysis (CCA) is applied within each semantic class, using the boundary predictions to split blobs into separate instances. This approach allows instance segmentation without requiring ground truth instance masks for the few-shot training.
- Core assumption: Object boundaries can be reliably estimated from DINOv2 features with minimal supervision, and CCA combined with boundary subtraction effectively separates touching objects.
- Evidence anchors:
  - [section] "Our label generator consists of three main building blocks... learnable modules for semantic segmentation and boundary estimation as well as a static component to fuse their predictions."
  - [section] "we perform connected component analysis (CCA) yielding disconnected blobs. If a blob consists of fewer pixels than a threshold, we assign the semantic void class to its pixels. Otherwise, we subtract the predicted border for this blob from the semantic map followed by CCA to detect separate instances within a blob."
  - [corpus] Weak: No corpus neighbor explicitly describes boundary-based instance separation in few-shot settings.
- Break condition: If boundary estimation fails on complex object configurations or if CCA incorrectly merges or splits instances, the pseudo-labels will contain significant errors, harming downstream model training.

### Mechanism 3
- Claim: Mixed-batch training (combining pseudo-labels and a small number of ground truth samples) improves the final panoptic segmentation model by stabilizing learning and reducing domain shift between pseudo-labels and true annotations.
- Mechanism: During panoptic segmentation model training, each batch contains pseudo-labeled images plus one ground truth sample from the original 10 annotated images. This exposes the model to true annotations throughout training, preventing it from overfitting to potentially noisy pseudo-labels and helping correct systematic errors in the pseudo-label generation process.
- Core assumption: Including even a small proportion of ground truth data in each batch is sufficient to regularize the model and prevent degradation from pseudo-label noise.
- Evidence anchors:
  - [section] "we propose to further exploit the k annotated images... We construct batches that contain both pseudo-labels and one ground truth sample."
  - [section] "We further apply data augmentation via color jitter and horizontal flipping."
  - [corpus] Weak: No corpus neighbor explicitly discusses mixed-batch strategies in few-shot segmentation.
- Break condition: If pseudo-label noise is too high or systematic, even mixed-batch training may not prevent error propagation, leading to degraded final model performance.

## Foundational Learning

- Concept: Foundation models (e.g., DINOv2) and their role in visual representation learning
  - Why needed here: Understanding how DINOv2 is trained (self-supervised, no labels) and what kind of features it produces (semantically meaningful, spatially consistent) is critical to grasp why it can be fine-tuned on very few samples for segmentation tasks.
  - Quick check question: What is the key difference between DINOv2 and supervised backbones like ResNet in terms of training data requirements?

- Concept: Connected component analysis (CCA) and its use in instance segmentation
  - Why needed here: The pseudo-label generation process relies on CCA to group pixels into instances after boundary estimation. Knowing how CCA works and its limitations (e.g., sensitivity to noise, handling of touching objects) helps understand potential failure modes.
  - Quick check question: How does CCA differentiate between separate instances versus parts of the same object, and what role do boundaries play in this?

- Concept: Bootstrapped cross-entropy loss and its use in few-shot learning
  - Why needed here: The semantic segmentation head uses bootstrapped cross-entropy with hard pixel mining to focus training on difficult examples, which is important when data is scarce. Understanding this loss function helps explain how the model learns effectively from limited annotations.
  - Quick check question: Why might hard pixel mining be particularly useful when training on only 10 annotated images?

## Architecture Onboarding

- Component map:
  - Input: RGB image
  - Backbone: Frozen DINOv2 (ViT-B/14)
  - Semantic head: 4-layer MLP + upsampling (produces n-class semantic map)
  - Boundary head: 2-class MLP + upsampling (produces binary boundary map)
  - Fusion module: Multi-scale test-time augmentation + CCA + instance filtering (generates pseudo-labels)
  - Panoptic model: Frozen DINOv2 + adapter + 3 task heads (semantic, center, offset) + fusion (final panoptic output)
  - Training data flow: 10 annotated images → pseudo-label generator → large set of pseudo-labels → panoptic model (with mixed batches)

- Critical path:
  1. Generate high-quality pseudo-labels using the label generator (semantic + boundary heads + fusion)
  2. Train panoptic segmentation model on pseudo-labels with mixed batches (pseudo-labels + 1 ground truth per batch)
  3. Deploy panoptic model for online inference

- Design tradeoffs:
  - Frozen DINOv2 vs. fine-tuning: Freezing the backbone speeds up training and reduces overfitting risk but may limit adaptation to domain-specific features.
  - Boundary-based instance separation vs. direct instance mask prediction: Boundary estimation requires less supervision but may be less accurate on complex object layouts.
  - Mixed-batch training vs. pure pseudo-label training: Mixed batches improve robustness but require holding back some ground truth data.

- Failure signatures:
  - Semantic head: Poor class accuracy, confusion between similar classes, missing rare classes
  - Boundary head: Thick/thin/wiggly boundaries, missing object boundaries, false positives on texture edges
  - Fusion module: Over-segmentation (splitting single objects), under-segmentation (merging separate objects), incorrect class assignment
  - Panoptic model: Low PQ/SQ/RQ metrics, artifacts at instance boundaries, class confusion

- First 3 experiments:
  1. Train semantic and boundary heads on 10 annotated images and evaluate pseudo-label quality (pixel accuracy, mIoU, visual inspection) on a held-out validation set.
  2. Generate pseudo-labels for a larger unlabeled set and train a simple panoptic model (e.g., ResNet-50 + heads) on these labels; evaluate PQ/SQ/RQ on validation set.
  3. Implement and test mixed-batch training (pseudo-labels + 1 ground truth per batch) and compare performance to pure pseudo-label training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of annotated images (k) needed to achieve results on par with fully supervised panoptic segmentation methods?
- Basis in paper: [explicit] The paper shows that SPINO achieves competitive results with as few as ten annotated images, and the authors note a continuous improvement for greater k, with k=100 yielding results almost on par with Panoptic-DeepLab.
- Why unresolved: While the paper demonstrates significant improvements with increasing k, it does not establish the exact threshold where SPINO's performance matches fully supervised methods.
- What evidence would resolve it: A comprehensive study varying k from 1 to 100+ and comparing SPINO's performance against fully supervised baselines at each increment would identify the critical threshold.

### Open Question 2
- Question: How does SPINO's performance generalize to datasets with different characteristics, such as those with fewer or more classes than Cityscapes or KITTI-360?
- Basis in paper: [explicit] The authors evaluate SPINO on Cityscapes, KITTI-360, and two in-house datasets, but do not explore datasets with significantly different class distributions or numbers.
- Why unresolved: The paper focuses on datasets with 14-19 classes, which may not represent the full spectrum of real-world applications where the number of classes can vary widely.
- What evidence would resolve it: Testing SPINO on datasets with diverse class counts and distributions, including those with significantly more or fewer classes than the evaluated datasets, would demonstrate its generalizability.

### Open Question 3
- Question: What is the impact of using different backbone architectures for the frozen DINOv2 component on SPINO's performance?
- Basis in paper: [inferred] The paper uses the DINOv2 ViT-B/14 variant but does not explore other backbone options or variations within the DINOv2 family.
- Why unresolved: While the paper demonstrates the effectiveness of DINOv2, it does not investigate whether different backbone architectures or variations within DINOv2 could further enhance SPINO's performance.
- What evidence would resolve it: Comparative experiments using different backbone architectures, including other DINOv2 variants and potentially other foundation models, would reveal the optimal choice for SPINO.

## Limitations

- The approach relies heavily on the quality of DINOv2 features, which may not generalize well to domains significantly different from the model's training data.
- The effectiveness of boundary-based instance separation in complex scenes with occlusions or highly overlapping objects remains unproven.
- The scalability of pseudo-label generation to datasets with many fine-grained classes or long-tail distributions is unclear.

## Confidence

- High confidence in the overall framework design and its potential to reduce annotation costs for panoptic segmentation.
- Medium confidence in the specific architectural choices (frozen DINOv2, boundary estimation, CCA) due to limited ablation studies.
- Low confidence in the robustness of pseudo-label quality across diverse real-world datasets beyond the reported benchmarks.

## Next Checks

1. Test SPINO on a domain with significantly different visual characteristics (e.g., medical imaging or satellite imagery) to assess DINOv2 feature generalization.
2. Conduct a systematic ablation study varying the number of annotated images (1, 5, 10, 20) to quantify the minimum annotation requirement for acceptable performance.
3. Evaluate the impact of mixed-batch training by comparing models trained with and without ground truth samples in each batch, and analyze error propagation from pseudo-labels.