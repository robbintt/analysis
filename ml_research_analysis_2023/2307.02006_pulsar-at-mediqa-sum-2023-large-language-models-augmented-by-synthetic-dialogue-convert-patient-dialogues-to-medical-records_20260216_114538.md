---
ver: rpa2
title: 'PULSAR at MEDIQA-Sum 2023: Large Language Models Augmented by Synthetic Dialogue
  Convert Patient Dialogues to Medical Records'
arxiv_id: '2307.02006'
source_url: https://arxiv.org/abs/2307.02006
tags:
- task
- data
- medical
- language
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PULSAR's framework uses domain-specific pre-training with medical
  text from MIMIC-III, followed by fine-tuning with task-specific data augmented by
  synthetic dialogues generated using ChatGPT. The pre-training objective reconstructs
  pseudo-summaries from extracted medical terms.
---

# PULSAR at MEDIQA-Sum 2023: Large Language Models Augmented by Synthetic Dialogue Convert Patient Dialogues to Medical Records

## Quick Facts
- arXiv ID: 2307.02006
- Source URL: https://arxiv.org/abs/2307.02006
- Reference count: 39
- Primary result: PULSAR-11B achieves ROUGE-1 49.20, ROUGE-2 22.25, ROUGE-L 41.36 on validation set for dialogue snippet summarization

## Executive Summary
PULSAR presents a framework for converting patient-doctor dialogues into medical records using large language models enhanced by synthetic data generation. The approach combines domain-specific pre-training on medical text from MIMIC-III with task-specific fine-tuning on the MediQA-Sum challenge data, augmented by synthetic dialogues generated using ChatGPT. The framework is evaluated on two summarization tasks: summarizing dialogue snippets into medical note sections and summarizing full conversations into comprehensive medical records.

## Method Summary
The PULSAR framework uses domain-specific pre-training with medical text from MIMIC-III, followed by fine-tuning with task-specific data augmented by synthetic dialogues generated using ChatGPT. The pre-training objective reconstructs pseudo-summaries from extracted medical terms. The study evaluated this approach on two subtasks of the MediQA-Sum challenge: summarizing dialogue snippets (Task B) and full conversations (Task C). For Task B, the best PULSAR model (11B) achieved ROUGE-1 of 49.20, ROUGE-2 of 22.25, and ROUGE-L of 41.36 on the validation set. For Task C, the Flan-T5-3B model with data augmentation achieved ROUGE-1 of 29.41, ROUGE-2 of 11.60, and ROUGE-L of 19.18 on the test set.

## Key Results
- Scaling up language model size (3B to 11B) yielded the biggest performance improvements across all metrics
- Domain-specific pre-training on MIMIC-III showed limited benefit, possibly due to domain mismatch with target tasks
- Data augmentation with synthetic dialogues improved performance when training data was scarce (67 examples for Task C)
- The approach generalized well to summarization tasks without task-specific adaptation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data augmentation with synthetic dialogues improves model performance when training data is scarce.
- Mechanism: When training data is limited (e.g., only 67 examples for Task C), generating additional synthetic conversations using a black-box LLM and using them for training provides the model with more diverse examples, helping it learn better representations.
- Core assumption: The synthetic dialogues generated by the black-box LLM are of sufficient quality and relevance to the task.
- Evidence anchors:
  - [abstract] "data augmentation helps when training data was scarce"
  - [section] "Data Augmentation can be helpful if training data is extremely scarce... data augmentation improves performance across all metrics (27.64 vs 29.41 R1, 9.79 vs 11.60 R2, 16.24 vs 19.18 RL and 23.63 vs 26.08 RLSum without and with DA, respectively)"
- Break condition: If the synthetic dialogues generated are of poor quality or irrelevant to the task, the data augmentation would not improve performance and might even harm it.

### Mechanism 2
- Claim: Larger model scale yields the biggest performance improvements.
- Mechanism: Increasing the number of parameters in the language model allows it to learn more complex patterns and representations from the data, leading to better performance on the task.
- Core assumption: The computational resources are available to train and run larger models.
- Evidence anchors:
  - [abstract] "scaling up the language model yields the best performance gains"
  - [section] "Comparing L*, 3B* and 11B* results in Table 1, we can see a clear trend where larger models of the same family consistently perform better. The biggest hike in performance is observed between the 3B and 11B models."
- Break condition: If the model is already large enough to capture the necessary patterns, or if the computational resources are insufficient, increasing the model size further may not yield significant improvements.

### Mechanism 3
- Claim: Domain-specific pre-training has limited benefit for this task.
- Mechanism: Pre-training the model on medical text from MIMIC-III using a custom objective (reconstructing pseudo-summaries from extracted medical terms) does not significantly improve the model's performance on the downstream task of converting patient dialogues to medical records.
- Core assumption: The domain mismatch between the pre-training data (inpatient progress notes) and the target task (well-written admission notes or outpatient encounters) is the reason for the limited benefit.
- Evidence anchors:
  - [abstract] "We find limited evidence towards the efficacy of domain-specific pre-training"
  - [section] "There is no conclusive evidence that domain-specific pre-training is beneficial. Comparing 11B1 and 11B2, and 3B1 and 3B2 in Table 1, respectively, we observe that domain-specific pre-training by learning to predict missing medical terms in MIMIC-III notes appears not beneficial, with the gap being smaller for bigger models."
- Break condition: If the domain mismatch is reduced (e.g., by using more similar pre-training data), or if the pre-training objective is modified to better suit the target task, the domain-specific pre-training might yield better results.

## Foundational Learning

- Concept: Abstractive summarization
  - Why needed here: The task involves converting patient-doctor dialogues into medical records, which requires the model to understand the key information from the dialogue and generate a coherent, concise summary in the form of a medical record.
  - Quick check question: What is the difference between extractive and abstractive summarization, and which approach is more suitable for this task?

- Concept: Sequence-to-sequence modeling
  - Why needed here: The task can be formulated as a sequence-to-sequence problem, where the input is a patient-doctor dialogue (sequence of turns) and the output is a medical record (sequence of sentences). The model needs to learn to map the input sequence to the output sequence.
  - Quick check question: What are the key components of a sequence-to-sequence model, and how do they work together to solve this task?

- Concept: Data augmentation
  - Why needed here: The training data for this task is limited, especially for Task C (only 67 examples). Data augmentation techniques, such as generating synthetic dialogues using a black-box LLM, can help increase the diversity and quantity of the training data, leading to better model performance.
  - Quick check question: What are the potential risks and benefits of using data augmentation in this task, and how can the quality of the augmented data be ensured?

## Architecture Onboarding

- Component map: MIMIC-III pre-training with MLM + GSG objectives -> Fine-tuning on task data with synthetic augmentation -> Evaluation using ROUGE metrics
- Critical path: Pre-training on MIMIC-III corpus → Fine-tuning on task data → Data augmentation generation → Final evaluation
- Design tradeoffs: Model size vs. computational resources; domain-specific pre-training vs. general pre-training; data augmentation vs. limited training data
- Failure signatures: Poor performance on full conversations (Task C) due to domain mismatch; ineffective pre-training if medical term extraction fails; hallucinations or input copying in generated summaries
- First 3 experiments:
  1. Compare performance of different model sizes (Flan-T5-3B, Flan-T5-11B) on validation set
  2. Compare models with and without domain-specific pre-training on validation set
  3. Compare models trained on natural data only versus augmented data on validation set

## Open Questions the Paper Calls Out

- What is the optimal amount of synthetic data to generate for data augmentation in medical dialogue summarization tasks?
- How does the choice of pre-training objective affect performance on medical dialogue summarization?
- Would task-specific fine-tuning strategies outperform the simple end-to-end approach used in this paper?
- How does PULSAR performance scale with even larger model sizes beyond 11B parameters?
- What is the impact of different synthetic data generation strategies on model performance and hallucination rates?

## Limitations
- Limited benefit from domain-specific pre-training due to potential domain mismatch between MIMIC-III inpatient notes and target tasks
- Reliance on black-box ChatGPT for synthetic data generation without validation of clinical accuracy
- No systematic exploration of optimal synthetic data quantity or generation strategies

## Confidence
- High confidence in scaling hypothesis (larger models perform better)
- Medium confidence in data augmentation benefits
- Low confidence in domain-specific pre-training value

## Next Checks
1. Conduct ablation study comparing models trained on natural data only versus models with varying proportions of synthetic data to determine optimal augmentation ratio and assess quality decay from excessive augmentation.
2. Perform qualitative analysis of synthetic dialogues generated by ChatGPT to verify clinical accuracy, coherence, and relevance to the target medical note formats, including physician review of a sample subset.
3. Test alternative domain adaptation strategies beyond MIMIC-III pre-training, such as fine-tuning on more closely matched outpatient encounter notes or using retrieval-augmented generation to incorporate relevant medical knowledge during inference.