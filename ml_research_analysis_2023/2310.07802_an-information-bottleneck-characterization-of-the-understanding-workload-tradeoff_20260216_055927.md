---
ver: rpa2
title: An Information Bottleneck Characterization of the Understanding-Workload Tradeoff
arxiv_id: '2310.07802'
source_url: https://arxiv.org/abs/2310.07802
tags:
- reward
- human
- abstractions
- complexity
- distortion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work connects Information Bottleneck (IB) methods to human
  factors in XAI, establishing empirical links between information-theoretic concepts
  and human factors constructs of workload and understanding. Using IB to generate
  abstract explanations of reward functions, the authors found that increased explanation
  complexity correlates with higher human workload, while increased reward distortion
  correlates with lower human understanding (measured via feature ranking and best
  demonstration assessments).
---

# An Information Bottleneck Characterization of the Understanding-Workload Tradeoff

## Quick Facts
- arXiv ID: 2310.07802
- Source URL: https://arxiv.org/abs/2310.07802
- Reference count: 40
- This work connects Information Bottleneck (IB) methods to human factors in XAI, establishing empirical links between information-theoretic concepts and human factors constructs of workload and understanding.

## Executive Summary
This study establishes empirical connections between information-theoretic concepts and human factors constructs in explainable AI (XAI) by applying the Information Bottleneck (IB) method to generate abstract explanations of reward functions. The research demonstrates that increased explanation complexity correlates with higher human workload, while increased reward distortion correlates with lower human understanding. These findings enable mathematical characterization of the tradeoff between understanding and workload in XAI design, providing tools for automatically generating user-tailored explanations that account for varying cognitive capacities.

## Method Summary
The study employs the Information Bottleneck method to generate abstractions of reward functions at varying levels of complexity and distortion. Human subject experiments were conducted with 47-51 participants across grid-navigation and color domains, using both continuous and discontinuous reward functions. Participants completed feature ranking tasks to assess understanding and answered NASA TLX questions to measure workload. Statistical analysis used Spearman correlations and linear mixed effects models to examine relationships between information-theoretic metrics and human factors constructs.

## Key Results
- Increased explanation complexity correlates with higher human workload (NASA TLX)
- Increased reward distortion correlates with lower human understanding (feature ranking and best demonstration assessments)
- Feature-based understanding (FR) is more reliably predicted by distortion than policy-based understanding (BD)
- Continuous reward functions are more amenable to abstraction than discontinuous ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: IB abstractions directly trade off understanding (distortion) and workload (complexity) in XAI design
- Mechanism: The IB method generates abstractions that maximize reward prediction accuracy (low distortion) while minimizing the number of bits used to represent features (low complexity). This creates a natural tradeoff surface where explanations can be tuned to balance user comprehension against cognitive load.
- Core assumption: Human cognitive workload scales with explanation complexity, and understanding scales inversely with reward distortion
- Evidence anchors:
  - [abstract] "We establish empirical connections between workload and complexity and between understanding and informativeness"
  - [section] "complexity-distortion balance in IB can be effectively applied to model the workload-understanding tradeoff"
  - [corpus] Weak - no directly comparable studies found
- Break condition: If human cognitive processing doesn't follow information-theoretic bounds, or if users can handle high complexity with low distortion without increased workload

### Mechanism 2
- Claim: Feature-based understanding (FR) is more reliably predicted by distortion than policy-based understanding (BD)
- Mechanism: Feature ranking requires direct comparison of abstract representation values, which is more sensitive to reward prediction accuracy (distortion). Policy understanding requires additional translation from abstract representation to action selection, introducing more noise and individual variation.
- Core assumption: The additional cognitive step of translating reward understanding to policy selection introduces more variance than direct feature comparison
- Evidence anchors:
  - [section] "correlations between FR (feature-based reward understanding) and distortion were stronger... than for BD"
  - [section] "factor loadings for policy-based assessments... were weaker than those for feature-based assessments"
  - [corpus] Weak - limited prior work comparing these specific understanding metrics
- Break condition: If task design or visualization changes reduce the translation step, or if participants develop better mental models for policy selection

### Mechanism 3
- Claim: Continuous reward functions are more amenable to abstraction than discontinuous ones
- Mechanism: Continuous reward structures create natural groupings in feature space where adjacent regions have similar values, making abstractions more intuitive and aligned with human categorization. Discontinuous rewards create artificial boundaries that may not correspond to meaningful abstractions.
- Core assumption: Human cognitive categorization aligns better with smooth, continuous feature spaces than with arbitrary discontinuities
- Evidence anchors:
  - [section] "correlations between FR and distortion were stronger for the Manhattan grid... than the Random grid"
  - [section] "correlations between FR and distortion were stronger for the continuously varying reward function than the discontinuously varying"
  - [corpus] Weak - limited prior work on human categorization of continuous vs discontinuous reward spaces
- Break condition: If task design or participant training overcomes the discontinuity barrier, or if discontinuities are made more meaningful to users

## Foundational Learning

- Concept: Information Bottleneck method
  - Why needed here: Provides the mathematical framework for generating abstractions that trade off informativeness (distortion) and complexity
  - Quick check question: If we want to explain a reward function with minimal information loss, what IB parameter would we adjust?
- Concept: Spearman correlation vs linear correlation
  - Why needed here: The study used Spearman correlation due to non-normal data and expected monotonic relationships, which is critical for interpreting the results correctly
  - Quick check question: When would you choose Spearman over Pearson correlation in human factors research?
- Concept: Linear mixed effects modeling
  - Why needed here: Accounts for individual differences between participants while testing the main hypotheses, providing more robust statistical support
  - Quick check question: What advantage does LMEM have over simple correlation when participants complete multiple conditions?

## Architecture Onboarding

- Component map:
  - IB abstraction generator (embo package) -> Reward function evaluator -> Human subject interface (Qualtrics) -> Data analysis pipeline (correlation + LMEM) -> Visualization components (heat maps, color grids)
- Critical path: IB abstraction generation → Human subject experiment → Understanding/workload measurement → Statistical analysis → Model validation
- Design tradeoffs: 
  - Using exact IB methods provides optimal abstractions but doesn't scale to large domains
  - Abstractions are generated from ground truth reward functions, limiting real-world applicability
  - Different visualization approaches for different domains may affect results
- Failure signatures:
  - High variance in understanding metrics across similar complexity/distortion levels
  - Weak correlations between predicted and actual workload/understanding
  - Poor performance on policy-based assessments compared to feature-based
- First 3 experiments:
  1. Generate abstractions at varying complexity levels for a simple grid domain and validate the complexity-workload correlation
  2. Compare feature-based vs policy-based understanding metrics on the same abstractions
  3. Test different visualization approaches for the same abstractions to isolate visualization effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of IB-generated abstractions vary depending on the structure of the underlying reward function (continuous vs. discontinuous)?
- Basis in paper: [explicit] The paper notes that "correlations between feature rank and distortion were stronger for the Manhattan grid (with a fundamentally continuous reward structure) than the Random grid in the grid-navigation domain, and for the continuously varying reward function than the discontinuously varying reward function in the color domain."
- Why unresolved: While the paper observes this difference, it does not provide a detailed analysis of why continuous reward functions may be more amenable to abstraction than discontinuous ones, or how this might inform explanation design.
- What evidence would resolve it: Controlled experiments comparing the effectiveness of IB-generated abstractions across a range of continuous and discontinuous reward functions, along with analysis of the underlying reward function structures that lead to more or less effective abstractions.

### Open Question 2
- Question: How does the visualization of abstract explanations impact human understanding, and what are the optimal visualization strategies for different types of abstract explanations?
- Basis in paper: [explicit] The paper notes that "the visualizations of the abstractions within the color domain were not provided in the same space in which the best demonstration task was performed (as opposed to the grid domain, where the abstractions were visualized in the grid itself), so another possible reason for this difference (and the added difficulty with high-complexity abstractions) is the extra step necessary to translate the abstract information into the task space."
- Why unresolved: While the paper observes differences in understanding based on visualization, it does not explore alternative visualization strategies or determine optimal approaches for different types of abstract explanations.
- What evidence would resolve it: User studies comparing the effectiveness of different visualization strategies for abstract explanations across multiple domains, measuring understanding and workload for each approach.

### Open Question 3
- Question: Can the findings of this study be generalized to other types of explanations beyond reward functions, such as explanations of policies, constraints, or counterfactual decisions?
- Basis in paper: [explicit] The paper states that "such techniques can be extended to explain complex concepts in larger-scale domains, such as reinforcement learning-based robotics applications and large language models (LLMs)" and that "future work can explore these relationships and their applicability to human-centered XAI design in an expanded assortment of settings."
- Why unresolved: The study focuses specifically on explaining reward functions, and while the authors suggest potential generalizability, this has not been empirically tested.
- What evidence would resolve it: Empirical studies applying IB-based abstraction methods to explain other AI concepts (policies, constraints, etc.) and measuring the relationships between information-theoretic metrics and human factors constructs in these new contexts.

## Limitations
- The study relies on simplified grid and color domains rather than real-world XAI scenarios
- IB abstractions are generated from ground truth reward functions rather than actual learned models
- The sample size (47-51 participants) may limit generalizability

## Confidence
- **High**: The positive correlation between explanation complexity and human workload, and the negative correlation between reward distortion and understanding (both feature-based and policy-based)
- **Medium**: The stronger correlations observed for continuous reward functions compared to discontinuous ones
- **Low**: The specific quantitative thresholds for optimal complexity-distortion tradeoffs across different domains

## Next Checks
1. Test the IB abstraction approach on a real-world XAI task (e.g., autonomous driving or medical diagnosis) to validate domain transferability
2. Conduct a longitudinal study measuring how understanding and workload change over time with repeated exposure to the same abstractions
3. Compare the IB-generated abstractions against human-generated explanations in terms of both effectiveness and cognitive load requirements