---
ver: rpa2
title: Towards a Fully Unsupervised Framework for Intent Induction in Customer Support
  Dialogues
arxiv_id: '2307.15410'
source_url: https://arxiv.org/abs/2307.15410
tags:
- cluster
- clusters
- clustering
- utterances
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a fully unsupervised intent induction framework
  for customer support dialogues. The method uses Sentence-BERT embeddings of utterances,
  dimensionality reduction with UMAP, and HDBSCAN clustering to discover intent clusters
  without any labeled data.
---

# Towards a Fully Unsupervised Framework for Intent Induction in Customer Support Dialogues

## Quick Facts
- arXiv ID: 2307.15410
- Source URL: https://arxiv.org/abs/2307.15410
- Reference count: 4
- Primary result: Unsupervised intent clustering achieves BCubed precision/recall around 0.43/0.63 (domain) and 0.28/0.60 (intent) on MultiWOZ with NER preprocessing

## Executive Summary
This paper presents a fully unsupervised framework for intent induction in customer support dialogues. The method uses Sentence-BERT embeddings of utterances, dimensionality reduction with UMAP, and HDBSCAN clustering to discover intent clusters without any labeled data. NER preprocessing improves clustering by reducing entity-driven noise. Results on MultiWOZ show the approach successfully identifies common dialogue flows and demonstrates practical applicability to real-world chatbot deployments.

## Method Summary
The framework processes customer support dialogues through a pipeline of NER preprocessing to replace specific entities with generic tags, SBERT embedding generation for semantic representation, UMAP dimensionality reduction, and HDBSCAN clustering for intent discovery. The approach analyzes dialogue flows using sequence mining with PrefixSpan. The method is evaluated on the MultiWOZ dataset using BCubed precision/recall metrics and relative validity index, with ablation studies showing the impact of NER preprocessing and parameter tuning on clustering quality.

## Key Results
- NER preprocessing improves clustering quality by reducing entity-driven noise
- SBERT embeddings effectively capture semantic similarity for intent clustering
- HDBSCAN successfully discovers intent clusters without predefined cluster count
- The framework identifies common dialogue flows in customer support interactions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NER preprocessing improves clustering by reducing entity-driven noise.
- Mechanism: Named entity recognition replaces specific entities with broader tags, making semantically similar utterances cluster together based on intent rather than specific entities.
- Core assumption: Entities are not central to the intent and their removal increases clustering purity.
- Evidence anchors:
  - [abstract] "NER preprocessing improves clustering by reducing entity-driven noise."
  - [section] "it can be useful to make them more homogeneous, in order to avoid clusters around entities. For this purpose, we use the spaCy Named Entity Recognition tool"
  - [corpus] No direct evidence; corpus neighbor papers do not explicitly mention NER preprocessing for intent induction.
- Break condition: If entities carry critical intent information (e.g., "book a hotel in Cambridge" vs "book a hotel in London"), NER removal could degrade clustering quality.

### Mechanism 2
- Claim: SBERT embeddings capture semantic similarity needed for clustering.
- Mechanism: Sentence-BERT produces embeddings where semantically similar utterances are close in vector space, enabling density-based clustering to group them by intent.
- Core assumption: Semantic similarity in the embedding space correlates with true intent similarity.
- Evidence anchors:
  - [abstract] "The method uses Sentence-BERT embeddings of utterances"
  - [section] "To make this task more efficient, Sentence-BERT (SBERT) uses siamese and triplet network structures to derive semantically meaningful sentence embeddings."
  - [corpus] Weak evidence; corpus neighbors focus on other embedding methods but do not directly validate SBERT for intent clustering.
- Break condition: If SBERT embeddings are not fine-tuned for the specific domain, semantic similarity may not reflect true intent similarity.

### Mechanism 3
- Claim: HDBSCAN clustering without predefined cluster count is suitable for unknown intent numbers.
- Mechanism: HDBSCAN builds a hierarchy of clusters based on density, allowing discovery of arbitrary-shaped clusters and handling noise without specifying the number of intents.
- Core assumption: Dialogue intents naturally form clusters of varying densities and shapes.
- Evidence anchors:
  - [abstract] "dimensionality reduction with UMAP, and HDBSCAN clustering to discover intent clusters without any labeled data."
  - [section] "By using HDBSCAN, we also do not require the prior definition of the density threshold used to create the clusters (contrary to DBSCAN), which is more suitable for this application."
  - [corpus] No direct evidence; corpus neighbors do not discuss HDBSCAN for intent induction.
- Break condition: If intents are too fine-grained or overlapping, HDBSCAN may merge distinct intents or split cohesive ones.

## Foundational Learning

- Concept: Sentence embeddings and semantic similarity
  - Why needed here: The system relies on SBERT embeddings to represent utterances in a semantic space where clustering can identify intents.
  - Quick check question: What is the difference between SBERT embeddings and traditional word embeddings like Word2Vec?

- Concept: Clustering algorithms and density-based methods
  - Why needed here: Understanding HDBSCAN's mechanism is crucial for interpreting clustering results and tuning parameters like min_cluster_size and min_samples.
  - Quick check question: How does HDBSCAN differ from DBSCAN and K-Means in handling varying cluster densities?

- Concept: Dimensionality reduction techniques
  - Why needed here: UMAP reduces high-dimensional SBERT embeddings to a lower dimension suitable for clustering while preserving local structure.
  - Quick check question: Why might UMAP be preferred over t-SNE for this application?

## Architecture Onboarding

- Component map: Data preprocessing (NER) -> Embedding generation (SBERT) -> Dimensionality reduction (UMAP) -> Clustering (HDBSCAN) -> Sequence analysis (PrefixSpan)
- Critical path: raw dialogues → NER preprocessing → SBERT embeddings → UMAP reduction → HDBSCAN clustering → analysis
- Design tradeoffs:
  - NER preprocessing removes entity-specific information which may be crucial for some intents.
  - HDBSCAN's soft clustering produces probability distributions over clusters, requiring interpretation.
  - UMAP preserves local structure but may distort global relationships.
- Failure signatures:
  - Too many small clusters: increase min_cluster_size or decrease min_samples.
  - Most points labeled as noise (-1): decrease min_samples or min_cluster_size.
  - Clusters mixing different intents: reconsider NER preprocessing or SBERT model choice.
- First 3 experiments:
  1. Run clustering on MultiWOZ hotel domain without NER preprocessing to establish baseline.
  2. Apply NER preprocessing and compare cluster quality metrics (BCubed precision/recall).
  3. Test different min_cluster_size and min_samples values to find optimal granularity for the hotel domain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of min_samples and min_cluster_size in HDBSCAN affect the granularity of intent clusters, and can an optimal combination be determined for different types of customer support dialogues?
- Basis in paper: [explicit] The paper conducts a grid search over min_samples and min_cluster_size values to optimize clustering results, finding that increasing both parameters makes clustering more conservative.
- Why unresolved: The paper shows that different combinations of parameters yield varying numbers of clusters and BCubed metrics, but does not establish a clear rule for selecting optimal parameters across different datasets or dialogue types.
- What evidence would resolve it: A systematic study comparing clustering performance across multiple customer support datasets with varying dialogue characteristics, establishing guidelines for parameter selection based on dataset properties.

### Open Question 2
- Question: Does separating user and assistant utterances into different clustering experiments improve the accuracy of intent identification and dialogue flow analysis?
- Basis in paper: [inferred] The paper mentions that mixing user and assistant utterances can lead to semantically similar utterances from both parties being assigned the same cluster, potentially confusing the analysis of dialogue flows.
- Why unresolved: The paper does not conduct separate clustering experiments for user and assistant utterances, nor does it evaluate the impact of this separation on clustering results or dialogue flow analysis.
- What evidence would resolve it: Comparative experiments clustering user and assistant utterances separately versus together, measuring improvements in intent identification accuracy and dialogue flow analysis.

### Open Question 3
- Question: Can the proposed unsupervised intent induction framework be extended to identify hierarchical or nested intents within customer support dialogues?
- Basis in paper: [inferred] The paper focuses on identifying broad intent clusters but does not explore the possibility of hierarchical or nested intent structures, which could provide a more nuanced understanding of customer support dialogues.
- Why unresolved: The paper does not investigate methods for detecting hierarchical or nested intents, nor does it evaluate the potential benefits of such an approach for dialogue management or intent classification.
- What evidence would resolve it: Experiments applying hierarchical clustering or other methods to detect nested intents, comparing performance with the current flat clustering approach and evaluating the impact on downstream dialogue management tasks.

## Limitations

- Evaluation lacks comparison with baseline unsupervised methods or supervised benchmarks
- NER preprocessing impact not rigorously validated through controlled ablation studies
- No analysis of how method scales to domains with thousands of unique intents
- Limited discussion of handling rare or ambiguous intents

## Confidence

- **High confidence**: The basic methodology pipeline (SBERT → UMAP → HDBSCAN) is technically sound and follows established practices. The implementation details are sufficiently specified for reproduction.
- **Medium confidence**: The reported clustering performance metrics are reasonable given the unsupervised nature of the task, though the absolute values should be interpreted cautiously without proper baselines.
- **Low confidence**: The claim that NER preprocessing significantly improves clustering quality lacks rigorous validation through controlled experiments comparing with and without NER.

## Next Checks

1. **Ablation study on NER preprocessing**: Run the complete pipeline on a subset of MultiWOZ both with and without NER preprocessing, measuring not just BCubed metrics but also analyzing how entity-sensitive intents (e.g., booking in different locations) are affected.

2. **Baseline comparison**: Implement and evaluate at least one simple unsupervised baseline (e.g., K-Means on TF-IDF vectors) on the same MultiWOZ subsets to contextualize the reported performance improvements.

3. **Parameter sensitivity analysis**: Systematically vary min_cluster_size and min_samples parameters across a wider range than reported, documenting how clustering granularity affects both quantitative metrics and qualitative coherence of the resulting intent clusters.