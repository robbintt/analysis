---
ver: rpa2
title: Learning the meanings of function words from grounded language using a visual
  question answering model
arxiv_id: '2308.08628'
source_url: https://arxiv.org/abs/2308.08628
tags:
- words
- questions
- more
- these
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how neural network models can learn function
  words like "and", "or", "behind", and "more" from visually grounded language, offering
  proof-of-concept evidence for usage-based acquisition theories. The authors use
  recurrent MAC models trained on the CLEVR visual question answering dataset, where
  models must learn word meanings to answer questions about scenes.
---

# Learning the meanings of function words from grounded language using a visual question answering model

## Quick Facts
- **arXiv ID**: 2308.08628
- **Source URL**: https://arxiv.org/abs/2308.08628
- **Reference count**: 24
- **Primary result**: Neural network models can learn function words like "and", "or", "behind", and "more" from visually grounded language, offering proof-of-concept evidence for usage-based acquisition theories.

## Executive Summary
This paper investigates how neural network models can learn function words like "and", "or", "behind", and "more" from visually grounded language, offering proof-of-concept evidence for usage-based acquisition theories. The authors use recurrent MAC models trained on the CLEVR visual question answering dataset, where models must learn word meanings to answer questions about scenes. They design semantic probes to test whether models learn gradient representations for function words requiring spatial and numerical reasoning (behind, in front of, more, fewer) rather than strict threshold-based meanings. Models also learn logical connectives "and" and "or" without prior knowledge of logical reasoning. The study finds that models consider alternative expressions when interpreting function words, leading to exclusive interpretations of "or" in some contexts. Additionally, word learning difficulty depends on frequency in the input, with less frequent words being harder to learn.

## Method Summary
The authors train recurrent MAC models on the CLEVR visual question answering dataset, where models must learn word meanings to answer questions about scenes. They design semantic probes to test whether models learn gradient representations for function words requiring spatial and numerical reasoning rather than strict threshold-based meanings. The probes use templates to generate questions about CLEVR scenes, testing models' understanding of function words in novel contexts. Models are evaluated on these probes at each training epoch to track learning progress. The study also manipulates input frequency by training on frequency-matched CHILDES data to test frequency effects on learning order.

## Key Results
- Models learn gradient semantics for spatial and numerical function words rather than threshold-based representations
- Models learn logical connectives "and" and "or" without prior knowledge of logical reasoning
- Word learning difficulty depends on frequency in the input, with less frequent words being harder to learn
- Models consider alternative expressions when interpreting function words, leading to exclusive interpretations of "or" in some contexts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Recurrent MAC models can learn gradient semantics for function words requiring spatial and numerical reasoning from visually grounded language.
- **Mechanism**: The model uses soft attention mechanisms over both word and image representations to incrementally process information, allowing it to develop nuanced, context-dependent interpretations of words like "behind," "in front of," "more," and "fewer" based on continuous visual features rather than discrete thresholds.
- **Core assumption**: Visual grounding provides sufficient statistical signal for the model to infer meaning gradients without explicit symbolic representations.
- **Evidence anchors**:
  - [abstract] "recurrent models trained on visually grounded language learn gradient semantics for function words requiring spacial and numerical reasoning"
  - [section] "We found that they did learn to interpret function words along the way to succeeding in the visual question answering task they were trained on, the CLEVR dataset. Models favored learning representations that allowed for gradient interpretations for function words requiring spacial and numerical reasoning rather than threshold-based semantic representations"
  - [corpus] Weak evidence - no direct citations about gradient learning from visual grounding in related papers

### Mechanism 2
- **Claim**: Models learn logical connectives "and" and "or" without prior knowledge of logical reasoning by considering alternative expressions during interpretation.
- **Mechanism**: The model computes answer probabilities while implicitly considering alternative questions using different logical connectives, leading to pragmatic reasoning about exclusive vs. inclusive interpretations of "or."
- **Core assumption**: The existence of alternative expressions in training data creates sufficient signal for the model to learn pragmatic distinctions.
- **Evidence anchors**:
  - [abstract] "we find that these models can learn the meanings of logical connectives and and or without any prior knowledge of logical reasoning, as well as early evidence that they are sensitive to alternative expressions when interpreting language"
  - [section] "Following Gricean theory, we might expect children to be able to judge the informativeness of contrasting expressions as soon as they have learnt their meaning... If recurrent visual question answering models can learn to consider alternative expressions when interpreting function words like and and or in novel contexts, then we may offer proof of concept evidence that the ability to reason about alternatives can be derived from statistical learning mechanism applied in a contextually grounded setting."
  - [corpus] Moderate evidence - related work on MM-CoT benchmark for visual chain-of-thought reasoning supports multimodal reasoning capabilities

### Mechanism 3
- **Claim**: Word learning difficulty in models depends on frequency in the input, with less frequent words being harder to learn.
- **Mechanism**: The statistical learning algorithm weights exposure frequency, making rare words like "fewer" and "in front of" more difficult to learn compared to frequent words like "more" and "behind."
- **Core assumption**: Frequency in training data is the primary determinant of learning order, independent of conceptual complexity.
- **Evidence anchors**:
  - [abstract] "Finally, we show that word learning difficulty is dependent on frequency in models' input, with less frequent words being harder to learn"
  - [section] "We found that word learning difficulty was indeed dependent on frequency in models' input, more frequently seen words generally being easier to learn in the case of spacial and numerical reasoning expressions"
  - [corpus] Moderate evidence - CHILDES corpus frequency data supports frequency-based learning patterns

## Foundational Learning

- **Gradient semantics**:
  - Why needed here: Models must learn nuanced, context-dependent meanings for spatial and numerical function words rather than discrete thresholds
  - Quick check question: Can the model correctly interpret "behind" when objects are at varying distances, showing performance that changes smoothly with distance?

- **Pragmatic reasoning about alternatives**:
  - Why needed here: Models need to understand how the presence of alternative expressions (like "and" vs "or") affects meaning interpretation
  - Quick check question: Does the model show different interpretation patterns for "or" when "and" is present vs absent in training data?

- **Frequency-based learning order**:
  - Why needed here: Understanding how word frequency in input affects learning difficulty and order
  - Quick check question: When trained on data with frequency distributions matching CHILDES, does the model learn "behind" before "in front of" as children do?

## Architecture Onboarding

- **Component map**: Image encoder (ResNet-101 + CNN layers) → Image representation → MAC cells (4 layers) → Attention-based reasoning with memory and control states → Output layer → Answer prediction

- **Critical path**: Image features → MAC cells → Answer prediction. The MAC cells are the core reasoning component that processes both visual and linguistic information incrementally.

- **Design tradeoffs**: The model trades explicit symbolic representations for learned continuous representations. This allows gradient semantics but may struggle with strict logical rules.

- **Failure signatures**: Poor performance on probe questions indicates failure to learn function word meanings. Drop in accuracy on ambiguous contexts suggests pragmatic reasoning issues. Below-chance performance indicates complete misunderstanding of word semantics.

- **First 3 experiments**:
  1. Train model on CLEVR and evaluate on semantic probes to establish baseline function word learning
  2. Remove alternative expressions (like "and" when testing "or") to test pragmatic reasoning hypothesis
  3. Train on frequency-matched CHILDES data to test frequency effects on learning order

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do gradient semantic representations for function words like "behind" and "more" emerge from exposure to visually grounded language, and what specific aspects of the learning environment contribute to this gradience?
- **Basis in paper**: [explicit] The paper states that models "learn gradient semantic representations for function words requiring spatial and numerical reasoning" and that "gradience in meaning may emerge from exposure to language in visually grounded contexts."
- **Why unresolved**: While the paper demonstrates the existence of gradient representations, it does not fully explore the mechanisms or specific factors within the visually grounded learning environment that lead to this emergence.
- **What evidence would resolve it**: Controlled experiments manipulating aspects of the visual environment (e.g., object occlusion, spatial configurations) or linguistic input (e.g., frequency of gradable vs. absolute expressions) to observe their impact on the development of gradient representations in the models.

### Open Question 2
- **Question**: To what extent does the consideration of alternative expressions during language interpretation reflect genuine pragmatic reasoning, and how does this ability develop in visually grounded neural network models?
- **Basis in paper**: [explicit] The paper finds "early evidence that models consider alternative expressions when interpreting function words" and that this "may be leading to a rise in exclusive interpretations for or."
- **Why unresolved**: The paper provides initial evidence of models considering alternatives, but it does not conclusively determine whether this behavior constitutes pragmatic reasoning or simply reflects the model's sensitivity to input distributions.
- **What evidence would resolve it**: Experiments comparing model behavior on tasks requiring pragmatic reasoning (e.g., scalar implicature) with and without explicit training on alternative expressions, and analyses of model internal representations to identify signs of pragmatic inference.

### Open Question 3
- **Question**: How does the frequency of function words in the input interact with their conceptual complexity to influence learning difficulty, and are there individual differences in models that reflect this interaction?
- **Basis in paper**: [explicit] The paper shows that "word learning difficulty was dependent on frequency in models' input" but also notes that "there seemed to be factors beyond frequency that influence our models' ability to learn" logical connectives.
- **Why unresolved**: The paper demonstrates the effect of frequency on learning but does not fully disentangle the influence of frequency from conceptual complexity, nor does it explore potential individual differences among models in learning trajectories.
- **What evidence would resolve it**: Experiments systematically varying the frequency and conceptual complexity of function words across different model architectures and analyzing learning curves to identify interactions between these factors and individual model differences.

## Limitations

- The MAC model's ability to learn gradient semantics relies heavily on the visual grounding provided by CLEVR's controlled environment, which may not capture the full complexity of real-world visual scenes
- The semantic probe methodology, while innovative, depends on synthetic question templates that may not fully represent natural language variation
- The frequency effects observed could be influenced by CLEVR's specific distribution rather than general learning principles

## Confidence

- **High confidence**: The model's ability to learn basic function word meanings from visual grounding
- **Medium confidence**: The claim about gradient semantics for spatial and numerical words
- **Medium confidence**: The pragmatic reasoning about alternative expressions
- **Medium confidence**: Frequency effects on learning order

## Next Checks

1. **Naturalistic Visual Grounding Test**: Evaluate the trained models on a dataset with naturalistic images and questions (beyond CLEVR) to assess whether gradient semantic learning generalizes to more complex visual environments with variable lighting, occlusion, and object arrangements.

2. **Alternative Expression Manipulation**: Systematically manipulate the presence/absence of alternative expressions (e.g., removing "and" from training data) and test whether models' interpretations of "or" change as predicted by pragmatic theory, using controlled probe sets that vary context systematically.

3. **Developmental Data Comparison**: Train models on frequency-matched CHILDES corpus data rather than CLEVR, then compare the learning order of function words to documented child acquisition patterns, particularly testing whether "behind" is learned before "in front of" as in developmental studies.