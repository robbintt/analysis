---
ver: rpa2
title: 'LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?'
arxiv_id: '2307.10719'
source_url: https://arxiv.org/abs/2307.10719
tags:
- censorship
- security
- output
- such
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) are remarkably capable at following
  instructions, which can lead to security and safety risks. Existing defences, such
  as model fine-tuning or output censorship, have proven fallible as LLMs can still
  generate problematic responses.
---

# LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?

## Quick Facts
- arXiv ID: 2307.10719
- Source URL: https://arxiv.org/abs/2307.10719
- Reference count: 40
- Large language models (LLMs) can generate problematic content despite censorship efforts, requiring a security-based approach rather than purely ML-based solutions.

## Executive Summary
This paper examines the fundamental limitations of semantic censorship in large language models, arguing that treating censorship as a machine learning problem is theoretically flawed. The authors demonstrate that semantic censorship can be undecidable due to LLMs' programmatic capabilities, making it impossible to determine whether certain outputs are permissible based on their functional behavior. They further show that even bounded-output censorship is impossible due to the preservation of semantic properties under invertible string transformations, and that attackers can bypass censorship through mosaic prompting by reconstructing impermissible outputs from multiple permissible subexpressions.

## Method Summary
The paper presents a theoretical analysis of semantic censorship limitations using computability theory and string transformation properties. The authors employ Rice's theorem to establish the undecidability of semantic censorship for program-like outputs, then prove the impossibility of bounded-output semantic censorship through the invariance property of semantic content under invertible transformations. Finally, they introduce the concept of mosaic prompting attacks that leverage compositionality to reconstruct impermissible outputs from permissible subexpressions. The analysis treats censorship as a security problem requiring access control mechanisms rather than purely ML-based detection.

## Key Results
- Semantic censorship is undecidable due to LLMs' programmatic capabilities, as established through Rice's theorem
- Bounded-output semantic censorship is impossible due to the preservation of semantic content under invertible string transformations
- Mosaic prompting attacks can reconstruct impermissible outputs from permissible subexpressions, bypassing any censorship method

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic censorship is undecidable due to the instruction-following and programmatic capabilities of LLMs.
- Mechanism: The authors connect semantic censorship of LLM outputs (especially code) to Rice's theorem, which states that for any non-trivial set of languages, determining whether a Turing machine recognizes a language in that set is undecidable. Since LLM outputs can include program descriptions, censoring them semantically would require solving an undecidable problem.
- Core assumption: LLM outputs can be treated as program descriptions for Turing machines, and the censorship constraints are defined over the functional behavior (language recognized) of those programs.
- Evidence anchors:
  - [abstract]: "we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities."
  - [section 2.1]: The paper explicitly connects semantic censorship to Rice's theorem, stating "we show that the problem of censorship for these settings becomes intimately related to Rice's theorem â€“ implying its undecidability."
- Break condition: If LLM outputs never include program descriptions or if censorship constraints are purely syntactic rather than semantic.

### Mechanism 2
- Claim: Even with bounded inputs/outputs, semantic output censorship is impossible due to the invariance of semantic content under invertible string transformations.
- Mechanism: If an output string is semantically impermissible, applying an invertible transformation (like encryption) preserves its semantic content. Since attackers can apply such transformations and the recipient can invert them, the transformed output remains impermissible. This creates a contradiction: the censorship mechanism cannot distinguish between permissible outputs and invertible transformations of impermissible ones.
- Core assumption: LLMs can follow instructions to apply string transformations to their outputs, and users know how to invert these transformations.
- Evidence anchors:
  - [section 2.2]: "we present another limitation: the impossibility of censoring outputs due to the preservation of semantic properties under arbitrary invertible string transformations."
  - [section 2.2]: The proof shows that if P is non-trivial, there exists a bijection g from a subset of permissible strings to a subset of impermissible strings, leading to a contradiction.
- Break condition: If LLMs cannot reliably follow instructions for string transformations, or if users cannot invert the transformations.

### Mechanism 3
- Claim: Mosaic prompts allow attackers to reconstruct impermissible outputs from multiple permissible ones, bypassing any censorship method.
- Mechanism: Attackers can query the LLM multiple times in separate contexts to obtain permissible subexpressions, then combine them according to a known structure to form an impermissible output. This works because censorship mechanisms cannot track the compositional structure across separate queries.
- Core assumption: Attackers know the compositional structure needed to combine permissible subexpressions into an impermissible output, and the LLM can output the necessary subexpressions.
- Evidence anchors:
  - [section 3.1]: The keylogger example shows how individual benign functions can be combined to create malware.
  - [section 3.2]: "we leverage the principle of compositionality to argue that users with knowledge of a structure S corresponding to an impermissible string w could construct that string by acquiring permissible expressions ei from the model m such that when these permissible expressions are combined with the structure S, they enable recovery of w."
- Break condition: If the compositional structure is unknown to attackers, or if the LLM cannot output the necessary subexpressions.

## Foundational Learning

- Concept: Rice's Theorem and undecidability
  - Why needed here: The paper uses Rice's theorem to prove that semantic censorship of program descriptions is undecidable, which is a foundational theoretical result for understanding the limitations of LLM censorship.
  - Quick check question: Can you explain why Rice's theorem implies that determining whether a program's behavior is permissible is undecidable?

- Concept: Compositionality in language and programming
  - Why needed here: The mosaic prompt attack relies on the principle that complex expressions can be decomposed into simpler parts whose meanings combine according to a structure. Understanding this principle is crucial for grasping why the attack works.
  - Quick check question: How does the principle of compositionality enable attackers to construct impermissible outputs from permissible subexpressions?

- Concept: Access control and security models
  - Why needed here: The paper argues that censorship should be treated as a security problem rather than an ML problem, and proposes using security approaches like access controls. Understanding basic access control concepts is necessary to evaluate this proposal.
  - Quick check question: What is the difference between a subject and an object in an access control model, and how might this apply to LLM-based systems?

## Architecture Onboarding

- Component map:
  - LLM-based model: The core system that generates outputs
  - Censorship mechanism: A component that checks outputs for permissibility (either semantic or syntactic)
  - User interface: Where users interact with the LLM
  - External tools: APIs, code interpreters, etc. that the LLM can use
  - Security framework: Access control mechanisms that manage permissions and information flow

- Critical path:
  1. User provides input to LLM
  2. LLM generates output
  3. Censorship mechanism checks output for permissibility
  4. If permissible, output is returned to user; if not, censored output is returned
  5. If output involves external tools, access control framework manages tool usage

- Design tradeoffs:
  - Semantic vs. syntactic censorship: Semantic is more flexible but theoretically impossible; syntactic is more restrictive but theoretically possible
  - Centralized vs. distributed censorship: Centralized is easier to manage but creates a single point of failure; distributed is more robust but harder to coordinate
  - Strict vs. lenient censorship: Strict is more secure but less useful; lenient is more useful but less secure

- Failure signatures:
  - Undecidability failures: Censorship mechanism gets stuck in infinite loops or cannot make decisions for certain inputs
  - Transformation attacks: Outputs that are semantically impermissible but syntactically permissible due to encryption or encoding
  - Mosaic prompt attacks: Outputs that are impermissible but constructed from multiple permissible subexpressions

- First 3 experiments:
  1. Test the undecidability claim by implementing a simple semantic censorship mechanism and observing its behavior on program-like inputs
  2. Test the transformation attack by applying simple encodings (like base64) to impermissible outputs and checking if the censorship mechanism can detect them
  3. Test the mosaic prompt attack by designing a compositional structure for an impermissible output and verifying if permissible subexpressions can be combined to create it

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the fundamental limitations of semantic censorship in LLMs when it comes to detecting impermissible content?
- Basis in paper: [explicit] The paper discusses the theoretical limitations of semantic censorship, including its undecidability due to LLMs' programmatic and instruction-following capabilities.
- Why unresolved: The paper highlights the inherent challenges in censorship but does not provide a definitive solution or method to overcome these limitations.
- What evidence would resolve it: Empirical studies or theoretical proofs demonstrating whether it is possible to develop a reliable semantic censorship mechanism for LLMs.

### Open Question 2
- Question: How can the Mosaic Prompt attack be mitigated or prevented in practical settings?
- Basis in paper: [explicit] The paper introduces the concept of Mosaic Prompts, where attackers can reconstruct impermissible outputs from permissible ones, highlighting the futility of censorship in such cases.
- Why unresolved: The paper does not provide a concrete solution to defend against Mosaic Prompt attacks, suggesting that censorship alone may not be sufficient.
- What evidence would resolve it: Successful development and implementation of security-based approaches that can effectively mitigate Mosaic Prompt attacks.

### Open Question 3
- Question: What are the implications of LLMs' increasing integration with external tools and applications on censorship and security?
- Basis in paper: [explicit] The paper discusses the integration of LLMs with external tools and applications, which expands their capabilities and raises concerns about safety and security risks.
- Why unresolved: The paper does not explore the specific challenges and potential solutions that arise from this integration, leaving the question of how to ensure safe and secure LLM use in such contexts open.
- What evidence would resolve it: Studies or experiments that assess the impact of LLM integration with external tools on censorship and security, along with proposed solutions to address identified risks.

## Limitations
- The theoretical undecidability claim assumes LLM outputs can be treated as complete program descriptions, which may not hold for all practical censorship contexts
- The transformation attack relies on perfect invertible transformations and user capability to apply/invert them, which may be difficult to achieve in practice
- The mosaic prompt attack assumes attackers know the exact compositional structure needed, which may be challenging to discover for complex impermissible outputs

## Confidence
- High: Theoretical connection between semantic censorship and Rice's theorem (undecidability claim)
- Medium: Practical limitations of censorship via string transformations and mosaic prompting
- Low: Implementation feasibility of security-based alternatives and real-world attack scenarios

## Next Checks
1. **Empirical undecidability test**: Implement a simple semantic censorship mechanism and test it on program-like LLM outputs to observe whether it gets stuck or fails to make decisions for certain inputs, validating the theoretical undecidability claim.

2. **Transformation attack practicality**: Test whether LLMs can reliably follow instructions for common string transformations (base64, Caesar cipher, etc.) and whether simple detection mechanisms can identify transformed outputs without false positives.

3. **Mosaic prompt boundary analysis**: Systematically test whether permissible subexpressions can be combined to create impermissible outputs across different compositional structures, and whether censorship mechanisms can detect compositional patterns across separate queries.