---
ver: rpa2
title: Validating Synthetic Usage Data in Living Lab Environments
arxiv_id: '2310.07142'
source_url: https://arxiv.org/abs/2310.07142
tags:
- click
- system
- data
- ranking
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to validate synthetic usage data generated
  by click models in living labs, where user interaction data is sparse. The approach
  evaluates click models by their ability to correctly rank retrieval systems based
  on click probabilities.
---

# Validating Synthetic Usage Data in Living Lab Environments

## Quick Facts
- arXiv ID: 2310.07142
- Source URL: https://arxiv.org/abs/2310.07142
- Reference count: 40
- Primary result: Simple click models (DCTR) can reliably rank IR systems with 20 sessions per query, while complex models need more data

## Executive Summary
This paper addresses the challenge of validating synthetic usage data in living lab environments where user interaction data is sparse. The authors propose a method to evaluate click models by their ability to correctly rank retrieval systems based on click probabilities. Using the TripClick biomedical search dataset, they compare three click models (DCTR, DCM, SDBN) and demonstrate that simple models can reliably determine system rankings with minimal logged sessions, while more complex models require more data but perform better in simulated interleaving experiments.

## Method Summary
The methodology involves parameterizing click models (DCTR, DCM, SDBN) on increasing amounts of session logs from the TripClick dataset, then evaluating their ability to reproduce known system rankings. The evaluation uses both Log-Likelihood computation of click probabilities and simulated interleaving experiments. System rankings are compared against reference rankings using Kendall's tau correlation. The approach is validated through Transformer-based ranking experiments and is fully reproducible with open-source code available on GitHub.

## Key Results
- DCTR click model reliably determines system rankings with only 20 logged sessions for 50 queries
- More complex models (DCM, SDBN) require more data but are better suited for interleaving experiments
- LRM system rankings are easier for click models to reproduce than IRM rankings due to greater system diversity
- The methodology is validated through Transformer-based ranking experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Click model simulation quality improves monotonically with increasing session data, enabling reliable pre-assessment of IR systems in living labs.
- Mechanism: Parameterizing click models on more logged sessions increases the fidelity of click probability estimates, which improves rank correlation between the click model's inferred system ranking and the known reference ranking.
- Core assumption: Log-likelihood and simulated interleaving outcomes are valid proxies for system effectiveness when editorial relevance judgments are unavailable.
- Break condition: If click signals are sparse or heavily biased, additional data may not improve rank correlation.

### Mechanism 2
- Claim: DCTR click model outperforms DCM and SDBN in low-data regimes due to its simpler examination probability assumption.
- Mechanism: DCTR uses constant examination probability (always 1), making it less sensitive to missing or noisy context data, while DCM/SDBN's rank-dependent examination probabilities require more observations to stabilize.
- Core assumption: Simpler models are more robust to data sparsity in parameter estimation.
- Break condition: When session data per query exceeds ~50, DCM/SDBN become more robust due to better modeling of satisfaction/continuation.

### Mechanism 3
- Claim: LRM system rankings are easier for click models to reproduce than IRM rankings because LRM systems are more distinct in document overlap.
- Mechanism: Higher Jaccard dissimilarity between LRM system outputs provides clearer click signal separation, whereas IRM systems with similar interpolation weights yield overlapping document sets, making click patterns less discriminative.
- Break condition: If document retrieval methods become too similar, even LRM rankings may become indistinguishable.

## Foundational Learning

- Concept: Probabilistic click modeling and maximum likelihood estimation
  - Why needed here: Click models estimate click probabilities based on historical sessions; accurate parameterization is critical for simulation fidelity
  - Quick check question: What is the difference between attractiveness and examination probability in a click model?

- Concept: Kendall's tau rank correlation coefficient
  - Why needed here: Used to compare the click model's inferred system ranking against the known reference ranking
  - Quick check question: How does Kendall's tau differ from Rank-biased Overlap (RBO) in this context?

- Concept: Simulated interleaving experiments for online evaluation
  - Why needed here: Interleaving reduces risk of exposing bad rankings to users and is the evaluation protocol in living labs
  - Quick check question: What is the winning condition in a team-draft interleaving experiment?

## Architecture Onboarding

- Component map: MongoDB session logs -> PyClick parser -> Click model parameterization -> IR backend (Pyterrier + Terrier + ir_datasets) -> Evaluation engine (Log-likelihood + simulated interleaving + Kendall's tau)

- Critical path: 1. Load session logs for selected queries 2. Parameterize click models (DCTR/DCM/SDBN) 3. Generate click probabilities for each system 4. Compute system rankings via Log-likelihood or interleaving outcomes 5. Compare rankings using Kendall's tau

- Design tradeoffs:
  - DCTR vs DCM/SDBN: Simplicity and low-data robustness vs richer user behavior modeling
  - Log-likelihood vs interleaving: Theoretical proxy vs simulated living lab conditions
  - Session sampling: Random vs stratified (query popularity)

- Failure signatures:
  - Low Kendall's tau (<0.6) even with many sessions -> click models fail to discriminate systems
  - Log-likelihood plateaus early -> insufficient session diversity
  - Interleaving outcomes near 0.5 -> models cannot distinguish systems

- First 3 experiments:
  1. Parameterize DCTR on 10 sessions per query for 10 queries, compute Log-likelihood ranking, compare Kendall's tau to LRM
  2. Repeat with DCM and SDBN, observe correlation drop
  3. Simulate interleaving with same models, compare outcome measures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do click model-based evaluations compare to curated test collections in terms of reliability and validity?
- Basis in paper: The paper discusses the limitations of click data and click models, noting that clicks are not a direct substitute for editorial relevance judgments and that there are biases in click data. It also suggests that click models could be used as a pre-assessment tool similar to pseudo-relevance judgments.
- Why unresolved: The paper does not provide a direct comparison between click model-based evaluations and curated test collections. It acknowledges the limitations of click data but does not explore the trade-offs between the two approaches.
- What evidence would resolve it: A comprehensive study comparing the results of click model-based evaluations to those of curated test collections for the same set of retrieval systems and queries.

### Open Question 2
- Question: How can click models be adapted to account for different types of user interactions beyond clicks, such as dwell time or scrolling behavior?
- Basis in paper: The paper focuses on click models that estimate click probabilities based on historical click data. However, it does not explore the potential of incorporating other types of user interactions into the models.
- Why unresolved: The paper does not discuss the limitations of using only click data and does not explore alternative ways to capture user behavior. It also does not provide any insights into how click models could be extended to incorporate other types of user interactions.
- What evidence would resolve it: A study comparing the performance of click models that incorporate different types of user interactions to those that rely solely on click data.

### Open Question 3
- Question: How can click models be used to evaluate the effectiveness of retrieval systems in real-world settings, where user behavior may differ from the behavior observed in controlled experiments?
- Basis in paper: The paper discusses the potential of using click models in living labs, where user interaction data is sparse. However, it does not explore the challenges of applying click models to real-world settings, where user behavior may be more diverse and unpredictable.
- Why unresolved: The paper does not provide any insights into how click models can be adapted to account for the complexities of real-world user behavior. It also does not discuss the potential limitations of using click models in uncontrolled environments.
- What evidence would resolve it: A study comparing the performance of click models in controlled experiments to their performance in real-world settings.

## Limitations
- TripClick logs from a single biomedical search engine may not generalize to other domains
- Click model assumptions about position bias and examination patterns may not hold for specialized domains
- Log-likelihood and simulated interleaving are theoretical proxies that may not perfectly predict actual living lab performance

## Confidence
- High confidence: DCTR's superior performance in low-data regimes is well-supported by experimental results
- Medium confidence: DCM/SDBN requiring more data is supported but exact thresholds vary by query complexity
- Low confidence: Generalizability to other search domains and long-term validity of click model assumptions

## Next Checks
1. Apply the same methodology to a different search domain (e.g., news or e-commerce) to verify click model performance patterns hold across contexts
2. Compare click model performance on TripClick data from different time periods to assess how user behavior changes affect reliability
3. Implement additional validation methods beyond Kendall's tau and simulated interleaving, such as perplexity or cross-validation, to strengthen robustness assessment