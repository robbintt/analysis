---
ver: rpa2
title: Robust Fitted-Q-Evaluation and Iteration under Sequentially Exogenous Unobserved
  Confounders
arxiv_id: '2302.00662'
source_url: https://arxiv.org/abs/2302.00662
tags:
- robust
- policy
- arxiv
- learning
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops orthogonalized robust fitted-Q-iteration under
  sequentially exogenous unobserved confounders. The key idea is to leverage structural
  insights to derive a closed-form solution for the robust Bellman operator and incorporate
  orthogonalization to reduce bias from quantile estimation error.
---

# Robust Fitted-Q-Evaluation and Iteration under Sequentially Exogenous Unobserved Confounders

## Quick Facts
- arXiv ID: 2302.00662
- Source URL: https://arxiv.org/abs/2302.00662
- Reference count: 40
- Key outcome: Orthogonalized robust FQI reduces estimation error and improves policy performance under unobserved confounding

## Executive Summary
This paper develops orthogonalized robust fitted-Q-iteration (RFQI) for offline reinforcement learning under sequentially exogenous unobserved confounders. The method leverages a marginal sensitivity model to derive a closed-form robust Bellman operator that is a convex combination of conditional mean and conditional value-at-risk. By incorporating orthogonalization, the algorithm reduces bias from quantile estimation error, achieving improved performance especially at higher levels of confounding. Theoretical analysis establishes a robustness-variance trade-off where Bellman error scales quadratically with the sensitivity parameter Λ.

## Method Summary
The method implements robust fitted-Q-iteration with orthogonalization for sequentially exogenous unobserved confounders. It uses weighted regression with weights based on a sensitivity parameter Λ to handle unobserved confounding. The algorithm estimates the marginal behavior policy, computes weights αt and βt, and iterates backwards through time steps. At each step, it computes nominal outcomes, estimates conditional quantiles using ℓ1-penalized quantile regression, computes orthogonalized pseudo-outcomes with bias corrections, and fits the Q-function via least squares. The orthogonalization term removes first-order dependence on nuisance parameters, reducing bias from quantile estimation error.

## Key Results
- Orthogonalization reduces estimation error and improves policy performance compared to non-orthogonalized robust FQI
- Bellman error depends quadratically on sensitivity parameter Λ and linearly on horizon H, illustrating robustness-variance trade-off
- Performance benefits of orthogonalization are most pronounced at higher levels of unobserved confounding
- Method demonstrates practical utility for robust offline RL under confounding through simulated sparse linear data experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The robust Bellman operator admits a closed-form solution as a convex combination of conditional mean and conditional value-at-risk (CVaR).
- Mechanism: Under Assumption 2 (marginal sensitivity model), the worst-case value for the confounded regression can be computed by a linear program, yielding weights that are a convex combination of the nominal conditional expectation and CVaR.
- Core assumption: The unobserved confounders are sequentially exogenous (Assumption 1) and the sensitivity parameter Λ bounds the extent of possible confounding.
- Evidence anchors:
  - [abstract]: "leverage structural insights to derive a closed-form solution for the robust Bellman operator"
  - [section]: "the linear program in Proposition 3 has a closed form solution corresponding to adversarial weights"
  - [corpus]: No direct corpus evidence; the mechanism relies on the structural model rather than empirical patterns.
- Break condition: If Assumption 1 fails (non-memoryless confounders) or Λ is misspecified, the closed-form solution no longer applies.

### Mechanism 2
- Claim: Orthogonalization reduces bias from quantile estimation error by removing first-order dependence on nuisance parameters.
- Mechanism: The orthogonalized regression target includes a control variate term that is Neyman-orthogonal with respect to errors in the conditional quantile function, ensuring that estimation error in Z propagates at a lower order.
- Core assumption: The conditional quantile function can be estimated consistently and the loss function is regression stable.
- Evidence anchors:
  - [abstract]: "adds a bias-correction to quantile estimation"
  - [section]: "introduce orthogonalization... derive bias adjustments"
  - [corpus]: Weak evidence; corpus contains related work but no direct validation of the orthogonalization technique.
- Break condition: If the first-stage quantile estimation is highly unstable or the regression stability assumption is violated, the orthogonalization benefit diminishes.

### Mechanism 3
- Claim: The Bellman error of the estimated Q-function scales quadratically with the sensitivity parameter Λ and linearly with horizon H, creating a robustness-variance trade-off.
- Mechanism: As Λ increases, the algorithm must estimate more extreme tail regions of the conditional distribution, which are inherently harder to estimate accurately, leading to larger confidence bounds.
- Core assumption: The Rademacher complexity of the function classes grows with the effective sample size and the structure of the problem.
- Evidence anchors:
  - [abstract]: "Bellman error depends quadratically on Λ and the horizon H"
  - [section]: "the width of confidence bounds on the robust Q function increases quadratically in Λ"
  - [corpus]: No direct corpus evidence; this is a novel theoretical insight from the paper.
- Break condition: If the function approximation is overly flexible or the sample size is very large, the quadratic dependence may be mitigated.

## Foundational Learning

- Concept: Markov Decision Process (MDP) and Partially Observable MDP (POMDP)
  - Why needed here: The paper starts with an MDP formulation and then addresses how unobserved confounders change the problem structure from MDP to POMDP.
  - Quick check question: What is the key difference between an MDP and a POMDP in terms of state observability?

- Concept: Sensitivity Analysis and Marginal Sensitivity Model
  - Why needed here: The method relies on bounding the extent of unobserved confounding via a sensitivity parameter Λ, following the marginal sensitivity model from causal inference.
  - Quick check question: How does the marginal sensitivity model bound the ratio of propensity scores with and without unobserved confounders?

- Concept: Quantile Regression and Conditional Expected Shortfall
  - Why needed here: The robust Bellman operator involves computing conditional quantiles and conditional expected shortfall (CVaR), which are estimated using quantile regression techniques.
  - Quick check question: What is the relationship between CVaR and the conditional quantile function?

## Architecture Onboarding

- Component map:
  Data collection -> Marginal policy estimation -> Conditional quantile estimation -> Robust Q-function fitting -> Policy extraction

- Critical path:
  1. Estimate marginal behavior policy πb.
  2. Compute weights αt, βt based on πb and Λ.
  3. Initialize Q-function.
  4. For each time step t (backwards):
     a. Compute nominal outcomes Y(i)t.
     b. Estimate conditional quantile Z₁₋τ.
     c. Compute orthogonalized pseudo-outcomes.
     d. Fit Q-function via least squares.
     e. Extract greedy policy.
  5. Output estimated robust Q-function and policy.

- Design tradeoffs:
  - Function class choice: Balance between expressiveness and estimation stability.
  - Orthogonality vs. simplicity: Orthogonalization reduces bias but adds complexity.
  - Λ selection: Larger Λ increases robustness but also estimation error (quadratic scaling).
  - Data splitting: Trade-off between statistical efficiency and debiasing requirements.

- Failure signatures:
  - High Bellman error despite many samples: Likely due to misspecification of function classes or Λ.
  - Unstable quantile estimates: May indicate need for more robust quantile regression or data splitting.
  - Policy performance degrades with Λ: Illustrates the robustness-variance trade-off; consider smaller Λ.

- First 3 experiments:
  1. Verify marginal MDP property: Test whether Lemma 1 holds in simulated data with known confounders.
  2. Ablation study: Compare orthogonalized vs. non-orthogonalized robust FQI on a simple linear setting.
  3. Sensitivity analysis: Vary Λ and observe trade-off between confidence bound width and policy performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of orthogonalized robust FQI compare to other offline RL methods like pessimism or conservative Q-learning in practice?
- Basis in paper: [explicit] The paper discusses connections to pessimism in offline RL but does not empirically compare to these methods.
- Why unresolved: The paper focuses on demonstrating the benefits of orthogonalization within the robust FQI framework, but does not directly benchmark against other approaches.
- What evidence would resolve it: Empirical comparison of orthogonalized robust FQI against pessimism-based methods like CQL or other offline RL algorithms on the same tasks.

### Open Question 2
- Question: Can the sample complexity bounds be improved to have better dependence on the sensitivity parameter Λ or horizon H?
- Basis in paper: [inferred] The paper provides sample complexity bounds with quadratic dependence on Λ and H, and discusses the trade-off between robustness and variance.
- Why unresolved: The current bounds may be loose, and tighter analysis could provide better insights into the practical implications of the sensitivity parameter.
- What evidence would resolve it: Sharper analysis techniques or empirical studies showing the actual performance gap between different values of Λ.

### Open Question 3
- Question: How does the orthogonalization technique affect the performance of robust FQI when the function approximation is highly mis-specified?
- Basis in paper: [explicit] The paper uses Lasso regression for function approximation in the experiments, which is a mis-specified model for the quantile and robust value functions.
- Why unresolved: The experiments show that orthogonalization is beneficial, but the paper does not explore the limits of this benefit under extreme mis-specification.
- What evidence would resolve it: Experiments with increasingly mis-specified function classes and analysis of the orthogonalization's impact on bias and variance.

### Open Question 4
- Question: Can the robust FQI framework be extended to handle more general models of unobserved confounding beyond sequentially exogenous confounders?
- Basis in paper: [inferred] The paper assumes memoryless unobserved confounders, which is a restrictive assumption. The discussion mentions other sensitivity models but does not explore them.
- Why unresolved: The current framework relies on the specific structure of sequentially exogenous confounders for tractability. Generalizing to other models may require different techniques.
- What evidence would resolve it: Development of robust FQI algorithms under different sensitivity models and analysis of their theoretical properties and empirical performance.

### Open Question 5
- Question: How does the robust FQI approach compare to methods based on partial identification bounds when the assumptions of the sensitivity model are violated?
- Basis in paper: [inferred] The paper discusses the benefits of sensitivity analysis over no-assumptions partial identification, but does not directly compare their performance when the sensitivity model is misspecified.
- Why unresolved: Sensitivity analysis relies on the correctness of the assumed bounds, while partial identification is more robust to model violations. It is unclear how they compare in practice.
- What evidence would resolve it: Empirical comparison of robust FQI and partial identification methods on data where the sensitivity model is known to be violated, measuring the impact on policy performance.

## Limitations
- The quadratic scaling of Bellman error with Λ may be overly conservative in practical settings where function class approximation error dominates
- Effectiveness of orthogonalization depends critically on the quality of first-stage quantile estimation, which is not extensively validated in high-noise regimes
- The memoryless assumption on unobserved confounders is restrictive and may not hold in many real-world applications

## Confidence
- **High confidence**: The closed-form solution for the robust Bellman operator under the marginal sensitivity model
- **Medium confidence**: The bias reduction from orthogonalization
- **Medium confidence**: The robustness-variance trade-off characterization

## Next Checks
1. Systematically vary regularization parameters in Lasso and quantile regression to assess robustness of orthogonalization benefit across approximation regimes
2. Implement non-linear variant using neural networks to test whether theoretical insights extend beyond linear case
3. Apply method to real-world offline RL dataset with known confounding structure to validate practical utility and assess Λ estimation from domain knowledge