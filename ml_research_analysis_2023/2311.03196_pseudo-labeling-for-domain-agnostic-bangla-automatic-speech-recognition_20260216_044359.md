---
ver: rpa2
title: Pseudo-Labeling for Domain-Agnostic Bangla Automatic Speech Recognition
arxiv_id: '2311.03196'
source_url: https://arxiv.org/abs/2311.03196
tags:
- speech
- data
- bangla
- dataset
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study presents a large-scale, domain-agnostic Bangla automatic\
  \ speech recognition (ASR) system developed using pseudo-labeling on approximately\
  \ 20,000 hours of speech data. The proposed approach leverages two ASR systems\u2014\
  Kaldi-based Hybrid ASR and Nemo-based Conformer-CTC\u2014to generate and filter\
  \ high-quality pseudo-labels from YouTube-sourced Bangla audio."
---

# Pseudo-Labeling for Domain-Agnostic Bangla Automatic Speech Recognition

## Quick Facts
- arXiv ID: 2311.03196
- Source URL: https://arxiv.org/abs/2311.03196
- Reference count: 9
- This study presents a large-scale, domain-agnostic Bangla automatic speech recognition (ASR) system developed using pseudo-labeling on approximately 20,000 hours of speech data.

## Executive Summary
This study presents a large-scale, domain-agnostic Bangla automatic speech recognition (ASR) system developed using pseudo-labeling on approximately 20,000 hours of speech data. The proposed approach leverages two ASR systems—Kaldi-based Hybrid ASR and Nemo-based Conformer-CTC—to generate and filter high-quality pseudo-labels from YouTube-sourced Bangla audio. The resulting MegaBNSpeech corpus, along with a conformer-based ASR model, demonstrates competitive performance against commercial systems (Google, MMS) and open-source models (OOD-speech) across diverse test sets, including news, telephony, and conversational domains. Key results show WER as low as 6.4% on in-domain test sets and strong generalization to out-of-domain datasets, underscoring the effectiveness of pseudo-labeling for low-resource ASR development.

## Method Summary
The authors collected approximately 20,000 hours of Bangla speech data from YouTube, including news channels, talk shows, travel vlogs, and crime shows. Two ASR systems were trained on 1.2K hours of transcribed YouTube audio data: a Kaldi-based Hybrid ASR and a Nemo-based Conformer-CTC model. These systems were then used to generate pseudo-labels for the unlabeled data. A matching algorithm aligned transcriptions from both systems, and segments with matching text were retained. Confidence scores, duration thresholds, and word rate filtering were applied to improve label quality. The resulting MegaBNSpeech corpus was used to train a FastConformer model for domain-agnostic Bangla ASR.

## Key Results
- WER as low as 6.4% on in-domain test sets
- Competitive performance against commercial systems (Google, MMS) and open-source models (OOD-speech)
- Strong generalization to out-of-domain datasets including news, telephony, and conversational domains
- MegaBNSpeech corpus created from ~20,000 hours of pseudo-labeled Bangla speech data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pseudo-labeling with two complementary ASR systems (Hybrid Kaldi + Conformer-CTC) improves label quality by filtering out mismatched segments.
- Mechanism: Audio segments are transcribed by both expert systems (E1 and E2). Only segments with matching transcriptions are retained, reducing transcription errors from either system alone.
- Core assumption: Mismatches between two independent ASR systems are likely due to recognition errors, not true linguistic variation.
- Evidence anchors:
  - [abstract] "We enrich the quality of pseudo-labels with our confidence and duration-based filtering method."
  - [section] "We use a matching algorithm, Algorithm 1, that employs exact string matching to align the text of segments from the experts E1 and E2 ASR systems."
  - [corpus] Weak - the paper doesn't provide empirical analysis of how many segments are filtered by this matching step alone.
- Break condition: If both systems systematically fail on certain linguistic phenomena (e.g., code-switching, rare words), mismatches may falsely indicate quality issues rather than actual errors.

### Mechanism 2
- Claim: Filtering by confidence score, duration, and word rate improves pseudo-label quality for downstream ASR training.
- Mechanism: After matching, segments are filtered based on: (i) confidence scores from both ASR systems, (ii) minimum/maximum duration thresholds, (iii) word rate (duration/word count), and (iv) minimum word count.
- Core assumption: Low-confidence segments, very short/long segments, and segments with extreme word rates are more likely to contain transcription errors.
- Evidence anchors:
  - [abstract] "We enrich the quality of pseudo-labels with our confidence and duration-based filtering method."
  - [section] "The subsequent step is to filter out segments based on predefined criteria... (i) confidence score of the ASR systems, (ii) minimum and maximum duration of the segments, (iii) the ratio of segment duration to the number of words, and (iv) the minimum number of words required in a segment."
  - [corpus] Weak - no specific values for thresholds or analysis of how different filtering parameters affect final dataset quality.
- Break condition: If filtering thresholds are too strict, they may remove valid speech segments, reducing dataset diversity and coverage of linguistic phenomena.

### Mechanism 3
- Claim: Training on large-scale pseudo-labeled data enables competitive performance on diverse test sets despite no human transcription during training.
- Mechanism: The MegaBNSpeech corpus (~20,000 hours) trained a FastConformer model that achieved WER as low as 6.4% on in-domain tests and showed strong generalization to out-of-domain datasets.
- Core assumption: Large-scale pseudo-labeled data, when properly filtered, contains sufficient linguistic diversity and accuracy to train high-performing ASR models.
- Evidence anchors:
  - [abstract] "The resulting MegaBNSpeech corpus, along with a conformer-based ASR model, demonstrates competitive performance against commercial systems (Google, MMS) and open-source models (OOD-speech) across diverse test sets, including news, telephony, and conversational domains."
  - [section] "Our results demonstrate the efficacy of the model trained on psuedo-label data for the designed test-set along with publicly-available Bangla datasets."
  - [corpus] Moderate - performance is compared to commercial and other models, but direct comparison to supervised training with human transcriptions is not provided.
- Break condition: If pseudo-labels contain systematic biases or errors that aren't filtered out, the model may learn incorrect patterns that degrade performance on real test data.

## Foundational Learning

- Concept: Automatic Speech Recognition fundamentals
  - Why needed here: Understanding how ASR systems generate transcriptions and evaluate performance (WER/CER) is essential for implementing pseudo-labeling pipelines.
  - Quick check question: What does WER measure, and why is it commonly used for ASR evaluation?

- Concept: Semi-supervised learning and pseudo-labeling
  - Why needed here: The entire approach relies on generating and filtering pseudo-labels from unlabeled audio to create training data without human annotation.
  - Quick check question: How does pseudo-labeling differ from traditional supervised learning, and what are its main advantages for low-resource languages?

- Concept: Data filtering and quality assurance
  - Why needed here: Multiple filtering steps (confidence, duration, word rate) are critical for ensuring pseudo-label quality before model training.
  - Quick check question: Why might very short or very long audio segments be problematic for ASR training, and how could filtering address this?

## Architecture Onboarding

- Component map: Data collection → Audio extraction → Two-expert ASR transcription → Segment matching → Confidence/duration filtering → Metadata storage → Model training → Evaluation
- Critical path: Audio extraction → Two-expert ASR transcription → Segment matching → Confidence/duration filtering → Model training
- Design tradeoffs: Using two ASR systems increases computational cost but improves label quality; strict filtering reduces dataset size but improves accuracy; YouTube-based collection ensures diversity but introduces content variability
- Failure signatures: High WER on test sets (filtering too lenient), very small final dataset (filtering too strict), model fails to converge (poor quality pseudo-labels), runtime errors during matching (data format issues)
- First 3 experiments:
  1. Run the two ASR systems on a small sample of YouTube audio and verify that segment matching correctly identifies and removes mismatched transcriptions
  2. Apply confidence and duration filtering to the matched segments and analyze how different threshold values affect dataset size and quality
  3. Train a small ASR model on the filtered pseudo-labels and evaluate on a held-out test set to verify the pipeline produces usable training data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of pseudo-labels generated by the hybrid ASR model (Kaldi-based TDNN) compare to those generated by the end-to-end Conformer-CTC model?
- Basis in paper: [explicit] The paper mentions using two ASR systems (Hybrid ASR and Conformer-CTC) for pseudo-labeling, but does not provide a direct comparison of their performance in generating pseudo-labels.
- Why unresolved: The paper does not explicitly compare the pseudo-label quality generated by the two ASR models, focusing instead on their combined use in the pseudo-labeling pipeline.
- What evidence would resolve it: A comparative analysis of the pseudo-labels generated by each ASR model, evaluating their accuracy and consistency, would clarify which model contributes more effectively to the pseudo-labeling process.

### Open Question 2
- Question: What is the impact of incorporating a small amount of manually annotated data with pseudo-labels on the performance of the ASR model?
- Basis in paper: [explicit] The paper suggests that supplementing pseudo-labels with manually annotated data could enhance ASR performance but does not provide empirical evidence or results for this approach.
- Why unresolved: The study focuses on the effectiveness of pseudo-labeling alone and does not explore the potential benefits of combining it with manual annotations.
- What evidence would resolve it: Conducting experiments that train the ASR model using a combination of pseudo-labels and a small set of manually annotated data, followed by a performance comparison, would provide insights into the benefits of this approach.

### Open Question 3
- Question: How does the MegaBNSpeech model's performance vary across different dialects and speaking styles within the Bangla language?
- Basis in paper: [inferred] The paper mentions that the dataset covers diverse dialects and speaking styles, but does not provide a detailed analysis of the model's performance across these variations.
- Why unresolved: While the dataset is described as diverse, the paper does not specifically evaluate the model's ability to handle different dialects and speaking styles within Bangla.
- What evidence would resolve it: A detailed performance analysis of the MegaBNSpeech model across different dialects and speaking styles, possibly using a stratified test set, would reveal how well the model generalizes across these variations.

## Limitations

- The confidence-based and duration-based filtering thresholds are not specified, making it difficult to reproduce the exact dataset quality and size.
- The paper does not provide direct comparisons between pseudo-labeled training and supervised training with human transcriptions, limiting our understanding of the approach's absolute effectiveness.
- There is no empirical analysis of how the segment matching algorithm performs or how many segments are filtered at each stage of the pipeline.

## Confidence

**High Confidence**: The overall approach of using pseudo-labeling with two complementary ASR systems for Bangla ASR development is well-supported. The competitive performance against commercial systems (Google, MMS) and open-source models (OOD-speech) on diverse test sets is well-documented.

**Medium Confidence**: The specific mechanisms of confidence-based filtering, duration-based filtering, and word rate filtering improving pseudo-label quality are plausible but lack detailed empirical validation. The claim that the approach works "domain-agnostic" is supported by performance on diverse test sets but could be stronger with more systematic domain analysis.

**Low Confidence**: The exact threshold values for filtering steps and their impact on final dataset quality cannot be verified without additional information. The paper's claim about achieving "low WER as 6.4%" on in-domain tests is difficult to fully assess without knowing the specific test set details.

## Next Checks

1. **Filter Threshold Analysis**: Conduct experiments varying the confidence score thresholds, duration limits, and word rate criteria to determine their individual and combined effects on dataset quality and model performance.

2. **Systematic Error Analysis**: Analyze the types of errors that remain in the pseudo-labels after filtering and identify whether any systematic biases exist that could affect model learning.

3. **Direct Supervised Comparison**: Train an equivalent model using the same architecture but with human-transcribed data (the 1.2K hours mentioned) to directly compare performance against the pseudo-labeled approach.