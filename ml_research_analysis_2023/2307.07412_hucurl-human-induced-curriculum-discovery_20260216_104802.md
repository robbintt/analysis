---
ver: rpa2
title: 'HuCurl: Human-induced Curriculum Discovery'
arxiv_id: '2307.07412'
source_url: https://arxiv.org/abs/2307.07412
tags:
- curriculum
- difficulty
- curricula
- training
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the problem of curriculum discovery and presents
  a framework that discovers effective curricula for NLP tasks based on prior knowledge
  about sample difficulty. The framework uses annotation entropy and loss as measures
  of difficulty to develop sample weighting schemes and optimize curriculum parameters.
---

# HuCurl: Human-induced Curriculum Discovery

## Quick Facts
- arXiv ID: 2307.07412
- Source URL: https://arxiv.org/abs/2307.07412
- Reference count: 29
- This paper introduces curriculum discovery for NLP tasks using human-induced difficulty signals, showing that non-monotonic curricula outperform monotonic ones and that discovered curricula transfer across dataset and model sizes.

## Executive Summary
This paper presents HuCurl, a framework for discovering effective curricula for NLP tasks based on prior knowledge about sample difficulty. The approach uses annotation entropy and loss as difficulty metrics to develop sample weighting schemes and optimize curriculum parameters through Bayesian optimization. The discovered curricula are shown to be non-monotonic and often outperform existing approaches. The framework demonstrates generalizability by discovering curricula on smaller datasets and models that perform well on larger ones. Experimental results show the proposed approach outperforms existing methods across several NLP tasks including SNLI, ChaosNLI, Twitter, and Reddit datasets.

## Method Summary
HuCurl discovers curricula by first measuring sample difficulty using annotation entropy (from human annotations) and/or model loss. Samples are clustered into k difficulty groups using quantiles of difficulty scores. The framework then optimizes parameterized weight functions for each group using Tree-structured Parzen Estimator (TPE) algorithm, allowing samples to move between groups based on their learning progress measured by loss deviation from group means. The method supports both monotonic (easy-to-hard) and non-monotonic curricula, with the latter showing superior performance. The framework is independent of annotation information and can be evaluated with both entropy and loss as difficulty metrics.

## Key Results
- Non-monotonic curricula discovered by HuCurl outperform monotonic curricula from existing literature
- Curricula discovered on smaller datasets and models transfer effectively to larger datasets and models
- HuCurl outperforms existing curriculum learning approaches across multiple NLP tasks

## Why This Works (Mechanism)

### Mechanism 1
Using annotation entropy as a prior difficulty signal improves curriculum discovery performance compared to model-derived loss. Human agreement levels captured by entropy provide a stable difficulty grouping from the start, avoiding the instability of model-dependent loss during early training. This assumes human agreement correlates with sample difficulty that generalizes across model architectures.

### Mechanism 2
Non-monotonic curricula discovered by the framework outperform monotonic curricula. By allowing samples to move between difficulty groups based on learning progress (measured by loss deviation from group mean), the framework adapts to the model's evolving capabilities. This prevents overfitting to early samples and mitigates forgetting, assuming loss deviation correlates with actual learning progress.

### Mechanism 3
Curricula discovered on smaller datasets/models transfer effectively to larger datasets/models. The framework discovers generalizable difficulty patterns that are preserved across scales, capturing fundamental task structure rather than dataset-specific quirks. This assumes optimal curriculum structure is relatively invariant to dataset size and model capacity within the same task domain.

## Foundational Learning

- **Concept: Curriculum learning and its distinction from traditional training**
  - Why needed here: The paper introduces curriculum discovery as a novel problem, requiring understanding of what curricula are and how they differ from standard training approaches.
  - Quick check question: What is the key difference between curriculum learning and standard training in terms of sample presentation order?

- **Concept: Bayesian optimization for hyperparameter search**
  - Why needed here: The framework uses Tree-structured Parzen Estimator (TPE) algorithm for optimizing curriculum parameters, requiring understanding of Bayesian optimization principles.
  - Quick check question: How does Bayesian optimization differ from grid search in exploring parameter spaces?

- **Concept: Annotation entropy and its calculation**
  - Why needed here: The framework uses annotation entropy as a difficulty metric, requiring understanding of information theory concepts.
  - Quick check question: What does annotation entropy measure in the context of multi-annotator labeling?

## Architecture Onboarding

- **Component map**: Input data → Difficulty scoring (entropy/loss) → K-means clustering into difficulty groups → Parameterized weight functions per group → Training with dynamic weight adjustment and group reassignment → Bayesian optimization of weight function parameters
- **Critical path**: Difficulty scoring → Clustering → Weight function optimization → Curriculum application during training
- **Design tradeoffs**: Static vs. dynamic difficulty assignment; entropy vs. loss as difficulty signal; number of difficulty groups; complexity of weight functions
- **Failure signatures**: Poor performance improvement over No-CL baseline; curriculum instability (frequent group changes); transfer failure to larger datasets
- **First 3 experiments**:
  1. Implement No-CL baseline and simple entropy-based curriculum (easy-to-hard) for sanity check
  2. Test curriculum transfer from small to large dataset on a single task
  3. Compare performance of entropy vs. loss as difficulty signals on a balanced dataset

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed curriculum discovery framework compare to other existing curriculum learning approaches in terms of computational efficiency and effectiveness? The paper mentions that the framework can represent several categories of curriculum learning approaches but does not provide a direct comparison of computational efficiency and effectiveness with other approaches. This remains unresolved as the paper focuses on introducing the framework rather than comprehensive comparison. What evidence would resolve it: Conducting a comparative study with other curriculum learning approaches, evaluating both computational efficiency and effectiveness on the same datasets and tasks.

### Open Question 2
Can the curriculum discovery framework be extended to incorporate additional sources of prior knowledge about sample difficulty, such as linguistic features or task-specific characteristics? While the paper demonstrates the framework's flexibility in using different difficulty scoring functions, it does not explore incorporating other sources of prior knowledge about sample difficulty. This remains unresolved as the paper focuses on annotation entropy and loss. What evidence would resolve it: Conducting experiments with the framework using various sources of prior knowledge about sample difficulty, such as linguistic features or task-specific characteristics, and evaluating the impact on curriculum discovery and model performance.

### Open Question 3
How does the curriculum discovery framework perform when applied to larger-scale datasets and models, and what are the potential limitations or challenges in scaling up the approach? The paper suggests finding curricula on smaller datasets or models and applying them to larger ones but does not provide detailed results or analysis of performance on larger-scale datasets and models. This remains unresolved as the paper focuses on smaller-scale demonstrations. What evidence would resolve it: Conducting experiments with the curriculum discovery framework on larger-scale datasets and models, evaluating its performance, computational efficiency, and potential limitations or challenges in scaling up.

## Limitations
- The paper lacks direct comparison of entropy-based vs. loss-based curricula performance, making it unclear whether human-induced signals actually improve results beyond model-derived signals
- Transferability results show promise but lack ablation studies to determine whether performance gains stem from curriculum structure or simply better optimization of training dynamics
- The framework's generalization across tasks is demonstrated but not extensively validated across diverse NLP domains

## Confidence
- **Medium**: Non-monotonic curricula outperform monotonic ones - supported by experimental results but could benefit from more extensive hyperparameter analysis
- **Medium**: Curricula transfer across dataset/model sizes - demonstrated but with limited ablation studies on what aspects transfer
- **Low**: Human annotation entropy is a superior difficulty signal - claimed but not directly validated against loss-based approaches in controlled experiments

## Next Checks
1. Run controlled experiments comparing entropy-based curricula vs. loss-based curricula on the same datasets to quantify the benefit of human-induced difficulty signals
2. Perform ablation studies on curriculum components (difficulty scoring, clustering method, weight functions) to isolate which elements drive performance gains
3. Test curriculum transfer in the opposite direction (from large to small datasets) to verify bidirectional generalization claims