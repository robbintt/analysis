---
ver: rpa2
title: 'Headless Language Models: Learning without Predicting with Contrastive Weight
  Tying'
arxiv_id: '2309.08351'
source_url: https://arxiv.org/abs/2309.08351
tags:
- language
- headless
- contrastive
- learning
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Headless Language Models (HLMs) that remove
  the need to predict probability distributions over token vocabularies and instead
  learn to reconstruct input embeddings contrastively using Contrastive Weight Tying
  (CWT). The CWT objective directly optimizes the scalar product between input embeddings
  and output representations using in-batch negative samples, avoiding the costly
  projection to vocabulary space.
---

# Headless Language Models: Learning without Predicting with Contrastive Weight Tying

## Quick Facts
- arXiv ID: 2309.08351
- Source URL: https://arxiv.org/abs/2309.08351
- Reference count: 13
- Key outcome: Headless Language Models with Contrastive Weight Tying achieve +1.6 GLUE score improvement and +2.7 LAMBADA accuracy gain while using up to 20x less compute than classical language models.

## Executive Summary
This paper introduces Headless Language Models (HLMs) that eliminate the need to predict probability distributions over token vocabularies by instead learning to reconstruct input embeddings contrastively. The Contrastive Weight Tying (CWT) objective directly optimizes the scalar product between input embeddings and output representations using in-batch negative samples, avoiding the expensive projection to vocabulary space. The approach is evaluated on both encoder (BERT) and decoder (GPT-2) architectures, demonstrating significant computational efficiency gains (25% training latency reduction) while improving downstream task performance on GLUE and LAMBADA benchmarks.

## Method Summary
The paper proposes replacing traditional cross-entropy language modeling with a contrastive objective that maximizes the scalar product between input embeddings and model output representations. Instead of projecting outputs to vocabulary space and computing softmax probabilities, CWT uses in-batch negative samples to provide contrastive regularization. The method is implemented as a drop-in replacement for standard language modeling objectives and is compatible with both masked language modeling (MLM) for encoders and causal language modeling (CLM) for decoders. Training uses a constant learning rate schedule with AdamW optimizer, and the approach shows particular efficiency gains at large vocabulary sizes.

## Key Results
- HLMs achieve +1.6 GLUE score improvement and +2.7 LAMBADA accuracy gain over classical LMs
- Training latency reduced by roughly 25% across all batch sizes compared to cross-entropy
- Computational complexity reduced from O(KDV) to O(K²D), where K is masked tokens, D is embedding dimension, and V is vocabulary size
- Memory requirements significantly reduced as only K×N scalar products need storage instead of K×V projection outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing the probability projection head and using contrastive weight tying directly optimizes scalar product between input embeddings and output representations, leading to more efficient training.
- Mechanism: Instead of computing expensive softmax over vocabulary space, the CWT loss directly maximizes the scalar product between the original token embedding and the model's output representation for that token, using in-batch negative samples for contrastive regularization.
- Core assumption: The scalar product maximization under contrastive regularization is equivalent to or better than cross-entropy optimization for language model pretraining.
- Evidence anchors:
  - [abstract]: "instead focuses on reconstructing input embeddings in a contrastive fashion via Constrastive Weight Tying (CWT)"
  - [section]: "Based on this understanding, we design an objective that directly optimizes this scalar product while not requiring the computation of the eTθ projection"
  - [corpus]: Weak - corpus contains related contrastive learning papers but none specifically addressing weight tying or removing projection heads

### Mechanism 2
- Claim: HLMs are more computationally efficient than classical language models, especially for large vocabularies.
- Mechanism: The CWT loss has O(K²D) complexity compared to O(KDV) for cross-entropy, where K is the number of masked tokens and V is vocabulary size. Memory requirements are also reduced as only K×N scalar products need to be stored instead of K×V projection outputs.
- Core assumption: The computational savings from avoiding vocabulary projection outweigh any potential performance costs from the contrastive approach.
- Evidence anchors:
  - [section]: "First, in terms of time and memory complexity, Headless Language Models (HLMs) are more efficient than classical language models under usual conditions"
  - [section]: "we observe that training latency is significantly reduced by roughly 25% for all batch sizes"
  - [corpus]: Weak - corpus contains general contrastive learning efficiency claims but no specific comparison to vocabulary projection costs

### Mechanism 3
- Claim: HLMs achieve better downstream performance and data efficiency than classical language models.
- Mechanism: By focusing on reconstructing input embeddings rather than predicting probability distributions, HLMs learn more robust token representations that transfer better to downstream tasks, especially for NLU tasks.
- Core assumption: The contrastive objective of reconstructing input embeddings captures more semantically relevant information than predicting next token probabilities.
- Evidence anchors:
  - [abstract]: "+1.6 GLUE score increase and +2.7 LAMBADA accuracy improvement compared to classical LMs"
  - [section]: "the headless MLM outperforms the vanilla MLM by significant margins, showing that our CWT loss is both more data-efficient and compute-efficient"
  - [section]: "our BERT reproduction scoring 1.6 points above its classical counterpart on the GLUE benchmark"
  - [corpus]: Weak - corpus contains contrastive learning benefits for representation learning but no specific comparison to language modeling objectives

## Foundational Learning

- Concept: Contrastive learning and negative sampling
  - Why needed here: The CWT loss relies on using in-batch samples as negative examples to provide contrastive regularization
  - Quick check question: How does increasing the number of negative samples affect contrastive learning performance, and what is the computational trade-off?

- Concept: Weight tying in neural networks
  - Why needed here: The CWT objective conceptually extends the weight tying trick by making it a contrastive objective rather than just parameter sharing
  - Quick check question: What is the mathematical relationship between the scalar product of tied weights and the softmax probability in traditional language models?

- Concept: Masked language modeling and causal language modeling objectives
  - Why needed here: The paper applies CWT to both encoder (MLM) and decoder (CLM) architectures, requiring understanding of these pretraining objectives
  - Quick check question: How do the masking strategies differ between MLM and CLM, and how does this affect the implementation of CWT?

## Architecture Onboarding

- Component map:
  Input embedding matrix (eθ) -> Transformer model (Tθ) -> CWT loss computation -> Training loop

- Critical path:
  1. Tokenize input sequences
  2. Look up input embeddings from shared matrix
  3. Process through transformer layers
  4. Compute scalar products between input embeddings and output representations
  5. Calculate contrastive loss with in-batch negatives
  6. Backpropagate gradients and update parameters

- Design tradeoffs:
  - Vocabulary size vs. training efficiency - larger vocabularies benefit more from CWT but may require larger batch sizes for sufficient negatives
  - Batch size vs. negative sample quality - larger batches provide more negatives but increase memory usage
  - Input embedding sharing vs. separate projections - sharing reduces parameters but may limit representational capacity

- Failure signatures:
  - Training instability or divergence - may indicate insufficient negative samples or learning rate issues
  - Poor downstream performance - could suggest the contrastive objective isn't capturing task-relevant information
  - Memory overflow - likely from batch size being too large for contrastive loss computation

- First 3 experiments:
  1. Implement CWT loss for BERT MLM and verify it trains successfully on a small dataset
  2. Compare training throughput and memory usage between CWT and cross-entropy on the same architecture
  3. Evaluate downstream GLUE performance for CWT-trained model vs. cross-entropy baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the contrastive weight tying objective scale to extremely large vocabularies (e.g., billions of tokens) compared to traditional cross-entropy?
- Basis in paper: [explicit] The paper discusses that HLMs can handle large vocabularies at virtually no increased cost due to constant time and memory complexity with respect to vocabulary size, unlike traditional models where projection complexity increases linearly with vocabulary size.
- Why unresolved: The paper only explores vocabularies up to 100k tokens, leaving open questions about behavior at truly massive scales where vocabulary management becomes critical.
- What evidence would resolve it: Experiments scaling to vocabularies of 1M+ tokens, measuring both training throughput and downstream performance degradation points, would clarify the practical limits of the approach.

### Open Question 2
- Question: What is the relationship between batch size and model quality for HLMs compared to traditional LMs?
- Basis in paper: [explicit] The paper shows that increasing batch size improves HLM performance, with the largest batch size (128) significantly outperforming vanilla models, suggesting an optimal negative sample regime.
- Why unresolved: The paper only tests batch sizes up to 128 and doesn't explore the theoretical optimal batch size or whether there's a point of diminishing returns.
- What evidence would resolve it: Systematic scaling of batch sizes from small to extremely large values while measuring downstream task performance would reveal the optimal regime and whether HLMs fundamentally benefit more from larger batches than traditional models.

### Open Question 3
- Question: How do HLMs handle rare and out-of-vocabulary tokens compared to traditional language models?
- Basis in paper: [inferred] The paper mentions that skewed token distributions can impact representation quality, and HLMs focus on contrastive discrimination between co-occurring tokens rather than absolute frequency-based hierarchies.
- Why unresolved: The paper doesn't specifically test how HLMs perform on rare word tasks or how they handle novel vocabulary compared to traditional models.
- What evidence would resolve it: Targeted experiments on rare word prediction tasks, zero-shot learning with novel vocabulary, and analysis of representation similarity for infrequent vs. frequent tokens would clarify whether HLMs are more robust to token frequency imbalances.

## Limitations

- The paper only explores batch sizes up to 128, leaving uncertainty about optimal batch size scaling and whether diminishing returns occur at larger sizes.
- Limited decoder architecture details make it unclear how CWT generalizes to causal language modeling beyond the single LAMBADA result.
- The paper doesn't address whether the contrastive objective introduces biases that could harm certain downstream tasks or whether improvements transfer to other encoder architectures.

## Confidence

- High confidence: Computational efficiency claims - O(K²D) vs O(KDV) complexity analysis is straightforward and 25% training latency reduction is well-supported empirically.
- Medium confidence: Downstream performance improvements - GLUE and LAMBADA results are positive but improvement magnitude depends heavily on specific architecture and training setup.
- Low confidence: Decoder model results - Minimal implementation details provided and single LAMBADA result is insufficient to establish generalization to causal language modeling.

## Next Checks

1. **Negative sample scaling study**: Systematically vary batch size from 512 to 8192 and measure the trade-off between contrastive regularization quality and training efficiency to validate in-batch negative approach scalability.

2. **Cross-architecture transfer**: Apply CWT to RoBERTa and DeBERTa architectures and evaluate GLUE performance to test whether improvements generalize beyond BERT's architectural biases.

3. **Calibration analysis**: Compare probability calibration and uncertainty estimates between CWT and cross-entropy models on tasks requiring calibrated predictions (e.g., confidence-based few-shot learning) to reveal whether contrastive objective sacrifices calibration for efficiency.