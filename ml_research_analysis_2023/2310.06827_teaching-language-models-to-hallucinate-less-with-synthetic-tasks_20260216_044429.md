---
ver: rpa2
title: Teaching Language Models to Hallucinate Less with Synthetic Tasks
arxiv_id: '2310.06827'
source_url: https://arxiv.org/abs/2310.06827
tags:
- hallucination
- synthetic
- task
- system
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of hallucination in large language
  models (LLMs) during abstractive summarization tasks, where models generate outputs
  containing information not supported by the given context. The authors propose SYNTRA,
  a method that uses synthetic tasks to reduce hallucination.
---

# Teaching Language Models to Hallucinate Less with Synthetic Tasks

## Quick Facts
- **arXiv ID**: 2310.06827
- **Source URL**: https://arxiv.org/abs/2310.06827
- **Reference count**: 14
- **Primary result**: SYNTRA reduces hallucination rates by up to 16 percentage points on summarization tasks using synthetic task optimization

## Executive Summary
This paper addresses the critical problem of hallucination in large language models during abstractive summarization tasks. The authors propose SYNTRA, a method that first trains on a synthetic names retrieval task where hallucination is easily measurable, then transfers this learned behavior to realistic summarization tasks. By optimizing the LLM's system message via prefix-tuning rather than fine-tuning the entire model, SYNTRA achieves significant reductions in hallucination rates across three different summarization benchmarks while maintaining general capabilities through reference data regularization.

## Method Summary
SYNTRA tackles LLM hallucination by creating a synthetic task (names retrieval) where hallucination is trivially measurable - outputs must contain only names from the input list. The method uses prefix-tuning to optimize the LLM's system message on this synthetic task, then transfers the optimized message to realistic summarization tasks. A critical component is regularizing with reference data to prevent the model from learning spurious attributes specific to the synthetic task. This approach significantly reduces hallucination while preserving the model's general capabilities, achieving better results than fine-tuning the entire model.

## Key Results
- Reduces hallucination rates by up to 16 percentage points on MS MARCO, QMSum, and ACI-Bench tasks
- Optimizes system messages via prefix-tuning more effectively than fine-tuning entire 13B-parameter models
- Reference data regularization critical for preventing spurious attribute learning
- Works for both Vicuna and Orca models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Optimizing system messages reduces hallucination more effectively than fine-tuning entire models
- **Mechanism**: System messages provide high-level behavioral guidance without changing core weights, allowing better generalization
- **Core assumption**: Learned messages encode abstract guidance that transfers to realistic tasks
- **Evidence anchors**: Fine-tuning increases hallucination rates while system message optimization consistently reduces them
- **Break condition**: Synthetic task doesn't capture hallucination causes or messages latch onto spurious patterns

### Mechanism 2
- **Claim**: Synthetic tasks enable direct hallucination optimization
- **Mechanism**: Names retrieval provides clear ground truth (names must appear in list), enabling gradient-based optimization
- **Core assumption**: Synthetic task's hallucination definition aligns with realistic tasks
- **Evidence anchors**: Synthetic task allows tractable evaluation while realistic tasks don't
- **Break condition**: Synthetic and realistic hallucination definitions don't match

### Mechanism 3
- **Claim**: Reference data prevents spurious attribute learning
- **Mechanism**: Reference examples where synthetic task patterns don't apply help model distinguish general from task-specific patterns
- **Core assumption**: Reference data provides counterexamples to spurious synthetic task patterns
- **Evidence anchors**: Reference data reduces hallucination rate for 5 of 6 task-LLM pairs
- **Break condition**: Reference data is too small or unrepresentative

## Foundational Learning

- **Concept**: Prefix-tuning for continuous optimization of discrete prompts
  - **Why needed here**: Direct text optimization is impossible with gradient descent due to discreteness
  - **Quick check question**: Why can't we just optimize the raw text of the system message directly?

- **Concept**: Synthetic task design for tractable evaluation
  - **Why needed here**: Hallucination is expensive to evaluate in realistic tasks but trivial in synthetic tasks
  - **Quick check question**: What properties must a synthetic task have to enable effective hallucination reduction?

- **Concept**: Regularization against spurious attributes
  - **Why needed here**: Synthetic data alone may teach task-specific patterns harmful to realistic tasks
  - **Quick check question**: How does reference data help prevent the model from learning spurious patterns?

## Architecture Onboarding

- **Component map**: Synthetic task generator → Prefix-tuning optimizer → System message applicator → Realistic task evaluator → Reference data buffer → Combined loss function → Transfer mechanism

- **Critical path**: 1. Generate synthetic training data (names retrieval task) 2. Optimize system message prefix using prefix-tuning 3. Apply optimized system message to realistic tasks 4. Evaluate hallucination reduction

- **Design tradeoffs**: System message vs full model optimization (smaller but more generalizable vs larger direct impact); Synthetic task complexity vs evaluation tractability (simpler easier to evaluate but may capture less); Reference data size vs spurious attribute mitigation (more data helps but increases cost)

- **Failure signatures**: Hallucination reduction on synthetic but increase on realistic (message doesn't generalize); No reduction on either (optimization not working or task poorly designed); Reference data needed for improvement (spurious attributes affecting transfer)

- **First 3 experiments**: 1. Test names retrieval on unmodified LLM to verify frequent hallucination and easy evaluation 2. Optimize system message on synthetic task only, test hallucination on realistic tasks 3. Add reference data to optimization, compare hallucination reduction with and without reference data

## Open Questions the Paper Calls Out
- None explicitly called out in the paper

## Limitations
- Effectiveness depends critically on synthetic task design generalizing to realistic tasks
- GPT-4 evaluation reliability introduces uncertainty in hallucination measurement
- System message optimization advantage over fine-tuning may not persist at larger model scales
- Minimum effective reference data size and composition not established

## Confidence
- **High confidence**: System message optimization reduces hallucination more than fine-tuning; names retrieval enables tractable evaluation; reference data prevents spurious learning
- **Medium confidence**: Method generalizes across summarization tasks; hallucination reduction transfers from synthetic to realistic; approach works for both Vicuna and Orca
- **Low confidence**: Approach works equally well for larger models; synthetic task generalizes to non-summarization; GPT-4 evaluation provides perfect ground truth

## Next Checks
1. Apply SYNTRA to a non-summarization task to validate cross-task generalization
2. Run GPT-4 evaluation with multiple prompt variations and cross-validate with human evaluation
3. Test SYNTRA on larger model sizes (30B-70B parameters) to assess scaling effects