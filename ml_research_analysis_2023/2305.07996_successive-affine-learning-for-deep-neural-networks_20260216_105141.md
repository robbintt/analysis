---
ver: rpa2
title: Successive Affine Learning for Deep Neural Networks
arxiv_id: '2305.07996'
source_url: https://arxiv.org/abs/2305.07996
tags:
- function
- grade
- learning
- optimization
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a successive affine learning (SAL) model for
  constructing deep neural networks (DNNs). The core idea is to learn DNNs layer-by-layer
  by solving a sequence of quadratic/convex optimization problems, each defining one
  layer of the DNN.
---

# Successive Affine Learning for Deep Neural Networks

## Quick Facts
- **arXiv ID**: 2305.07996
- **Source URL**: https://arxiv.org/abs/2305.07996
- **Reference count**: 32
- **Primary result**: SAL model learns DNNs layer-by-layer via quadratic/convex optimization, avoiding non-convex problems and achieving superior training time and accuracy compared to standard deep learning

## Executive Summary
This paper introduces a successive affine learning (SAL) model that constructs deep neural networks by learning each layer through quadratic/convex optimization problems, rather than solving a single non-convex optimization problem for all parameters simultaneously. The key innovation is separating the affine map learning from the activation function, allowing each layer's weight matrix and bias vector to be determined before applying the activation. This approach creates an orthogonal basis expansion of functions using DNNs and provides theoretical guarantees like the Pythagorean and Parseval identities. Numerical examples demonstrate that SAL significantly outperforms traditional single-grade deep learning models in training time, training accuracy, and prediction accuracy.

## Method Summary
The SAL model learns deep neural networks layer-by-layer through a sequence of quadratic/convex optimization problems. For each layer, the weight matrix and bias vector are learned by solving an optimization problem that minimizes the error between the network output and target values, without involving the activation function. After obtaining these parameters, the activation function is applied. The process repeats for each subsequent layer, with the error from previous layers serving as the new target. The model optionally uses average pooling operators to reduce dimensionality when weight matrices have more rows than columns, preserving the convexity of the optimization problems. This creates an orthogonal expansion of the target function using adaptive DNN basis functions.

## Key Results
- SAL achieves significantly better training accuracy and prediction accuracy than standard single-grade deep learning models
- Training time is reduced compared to traditional deep learning approaches
- The model satisfies theoretical properties including the Pythagorean identity and Parseval identity for orthogonal basis expansions
- SAL can handle non-differentiable and oscillatory functions effectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layer-wise affine learning avoids the non-convex optimization problem of standard deep learning
- Mechanism: Each layer's weight matrix and bias vector are learned by solving a quadratic/convex optimization problem that does not involve the activation function of that layer. Only after these parameters are determined is the activation function applied
- Core assumption: The activation function can be separated from the affine map learning without loss of expressiveness
- Evidence anchors:
  - [abstract] "we propose to learn the affine map by solving a quadratic/convex optimization problem which involves the activation function only after the weight matrix and the bias vector for the current layer have been trained"
  - [section 3] "Each layer of a DNN consists of an affine map followed by an activation function. We then propose to learn the affine map by solving a quadratic/convex optimization problem which involves the activation function only after the weight matrix and the bias vector of the layer have been obtained"
- Break condition: If the activation function cannot be separated from the affine map without affecting the overall network expressiveness

### Mechanism 2
- Claim: The successive affine learning process creates orthogonal basis functions
- Mechanism: Each learned layer represents an orthogonal projection of the error function from the previous grade onto a subspace determined by the current layer's neural network, creating mutually orthogonal terms in the expansion
- Core assumption: The error functions between successive grades are orthogonal to the learned functions
- Evidence anchors:
  - [section 5] "the k + 1 terms in the representation (5.1) of f are mutually orthogonal" and "fP_k is the orthogonal projection of eP*_k-1 onto the subspace AP_k"
  - [abstract] "In the context of function approximation, for a given function the SAL model generates an orthogonal expansion of the function with adaptive basis functions in the form of DNNs"
- Break condition: If the orthogonality condition between successive error functions is violated

### Mechanism 3
- Claim: Average pooling operator allows non-square weight matrices while preserving convexity
- Mechanism: The average pooling operator reduces the dimensionality of the affine map output to match the target dimension, enabling the use of weight matrices with more rows than columns without making the optimization problem non-convex
- Core assumption: The average pooling operator is a linear transformation that doesn't introduce non-convexity
- Evidence anchors:
  - [section 4] "The average pooling summarizes the average presence of a feature and the max pooling summarizes the most activated presence of a feature. We propose to employ the average pooling operator to pull back the matrix size to t so that we can compute the error function"
  - [section 4] "An advantage of using the average pooling operator lies on the fact that such a choice will not ruin the quadratic or convex nature of the resulting optimization problem for training the weight matrix and the bias vector for the layer"
- Break condition: If the average pooling operator introduces non-linearities that make the optimization problem non-convex

## Foundational Learning

- Concept: Orthogonal projection in Hilbert spaces
  - Why needed here: The SAL model relies on orthogonal projections of error functions onto subspaces to create the successive learning process
  - Quick check question: What property must a projection operator have to be orthogonal in a Hilbert space?

- Concept: Convex optimization algorithms (Nesterov, conjugate gradient)
  - Why needed here: The SAL model solves quadratic/convex optimization problems for each layer, requiring efficient algorithms for these well-structured problems
  - Quick check question: What convergence rate does the Nesterov algorithm achieve for smooth convex optimization problems?

- Concept: Function approximation theory and basis expansions
  - Why needed here: The SAL model creates an adaptive orthogonal basis expansion for functions, similar to Fourier or wavelet expansions
  - Quick check question: What is the key difference between an orthogonal basis expansion and a non-orthogonal basis expansion in terms of coefficient computation?

## Architecture Onboarding

- Component map:
  Input layer → Grade 1 affine learning → Activation function 1 → Grade 2 affine learning → Activation function 2 → ... → Output layer
  Average pooling operators (optional) inserted after each affine map
  Smoothing operators (optional) applied between grades

- Critical path: For each grade k:
  1. Compute error function from previous grade
  2. Apply average pooling if used
  3. Solve quadratic/convex optimization for current layer's weight matrix and bias
  4. Apply activation function
  5. Apply smoothing if used
  6. Proceed to next grade

- Design tradeoffs:
  - Number of grades vs. width of each layer: More grades with narrower layers vs. fewer grades with wider layers
  - Pooling usage: Average pooling enables wider layers but adds computational overhead
  - Smoothing parameters: More smoothing reduces oscillations but may slow convergence

- Failure signatures:
  - If gradients vanish despite the SAL structure, check activation function choices and smoothing parameters
  - If training accuracy plateaus early, verify that pooling operators are properly implemented and that optimization problems are being solved correctly
  - If test accuracy is significantly lower than training accuracy, check for overfitting in early grades

- First 3 experiments:
  1. Implement SAL with no pooling and identity activation functions to verify the affine learning mechanism works
  2. Add ReLU activation functions and test on a simple non-differentiable function to verify the orthogonal basis property
  3. Implement average pooling and test on a function requiring wider layers to verify the dimensionality reduction works while maintaining convexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SAL models compare to traditional deep learning models on large-scale real-world datasets?
- Basis in paper: [explicit] The paper presents proof-of-concept numerical examples demonstrating SAL's superiority over single-grade deep learning models, but these are on small-scale problems
- Why unresolved: The paper only provides numerical results on small-scale problems with specific functions. Real-world datasets often have higher dimensionality, noise, and complexity that may affect SAL's performance
- What evidence would resolve it: Comprehensive experiments comparing SAL and traditional models on benchmark datasets like ImageNet, CIFAR, or medical imaging datasets with varying sizes and complexities

### Open Question 2
- Question: What is the theoretical guarantee for SAL's convergence when using activation functions that don't satisfy σ(0) = 0?
- Basis in paper: [explicit] The paper proves convergence for SAL without pooling when σ(0) = 0, and mentions that modified activation functions could be used, but doesn't provide rigorous analysis
- Why unresolved: The proof relies on the condition σ(0) = 0 to show that the process terminates or error norms strictly decrease. For other activation functions, the convergence behavior is not theoretically established
- What evidence would resolve it: Rigorous mathematical proof of convergence properties for SAL with general activation functions, or empirical studies showing convergence behavior across different activation functions

### Open Question 3
- Question: How does the choice of pooling operator affect SAL's approximation accuracy and computational efficiency?
- Basis in paper: [explicit] The paper uses average pooling and briefly mentions that other pooling methods could be explored, but doesn't provide systematic comparison
- Why unresolved: While average pooling is shown to work well, the paper doesn't investigate how different pooling strategies (max pooling, learned pooling matrices, etc.) impact performance
- What evidence would resolve it: Systematic experiments comparing SAL with different pooling operators on various test functions and datasets, measuring both approximation accuracy and computational cost

## Limitations

- The theoretical benefits of SAL rely on strong assumptions about activation function separation and orthogonality that may not hold in all practical scenarios
- The paper only demonstrates SAL on small-scale problems and functions, leaving its performance on large real-world datasets uncertain
- The convergence guarantees for SAL with general activation functions remain unproven, particularly for functions where σ(0) ≠ 0

## Confidence

- **High Confidence**: The core mechanism of successive layer-wise affine learning through quadratic optimization is mathematically sound and the basic implementation is straightforward
- **Medium Confidence**: The claims about avoiding non-convex optimization are supported by the theory but may be sensitive to activation function choices and pooling implementations
- **Medium Confidence**: The orthogonal basis expansion property is theoretically proven but may be affected by numerical precision and finite-sample effects in practice
- **Low Confidence**: The generalization of these theoretical benefits to all types of deep learning problems remains to be demonstrated

## Next Checks

1. **Activation Function Sensitivity Test**: Implement SAL with different activation functions (ReLU, tanh, sigmoid) and measure how the orthogonal basis property and optimization convexity hold up. This will validate whether the theoretical separation assumption is robust to practical activation choices.

2. **Pooling Mechanism Verification**: Create a controlled experiment comparing SAL with and without average pooling on functions that require wide layers. Measure both the optimization problem convexity and the final approximation accuracy to verify the pooling mechanism's effectiveness.

3. **Generalization Cross-Test**: Apply SAL to a diverse set of problems (image classification, time series prediction, function approximation) and compare the training time and accuracy improvements against standard deep learning baselines. This will validate whether the claimed advantages extend beyond the specific examples in the paper.