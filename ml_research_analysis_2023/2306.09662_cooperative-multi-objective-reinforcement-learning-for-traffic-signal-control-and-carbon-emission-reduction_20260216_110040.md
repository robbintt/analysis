---
ver: rpa2
title: Cooperative Multi-Objective Reinforcement Learning for Traffic Signal Control
  and Carbon Emission Reduction
arxiv_id: '2306.09662'
source_url: https://arxiv.org/abs/2306.09662
tags:
- traffic
- control
- agent
- time
- signal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COMMA-DDPG, a multi-agent reinforcement learning
  framework for traffic signal control that incorporates both local and global agents
  to improve traffic flow and reduce emissions. The local agents optimize traffic
  at individual intersections while the global agent coordinates all local agents
  to maximize overall throughput.
---

# Cooperative Multi-Objective Reinforcement Learning for Traffic Signal Control and Carbon Emission Reduction

## Quick Facts
- arXiv ID: 2306.09662
- Source URL: https://arxiv.org/abs/2306.09662
- Authors: 
- Reference count: 40
- One-line primary result: Achieved 1.5% CO2 emission reduction with 134.89s average waiting time on multi-intersection traffic control

## Executive Summary
This paper introduces COMMA-DDPG, a multi-agent reinforcement learning framework that uses both local and global agents to optimize traffic signal control while reducing carbon emissions. The approach combines local agents that optimize individual intersections with a global agent that coordinates all intersections to maximize overall throughput. Using real-world traffic data, COMMA-DDPG outperformed state-of-the-art methods across multiple metrics including waiting time, travel time, and carbon emissions, achieving an average waiting time of 134.89 seconds and reducing CO2 emissions by approximately 1.5% compared to baseline methods.

## Method Summary
COMMA-DDPG extends the DDPG algorithm with a multi-agent architecture where each intersection has a local DDPG agent that learns its own traffic signal policy, while a global DDPG agent coordinates all local agents during training to prevent conflicting decisions. The framework uses continuous action spaces to dynamically determine green light durations rather than selecting from predefined time slots. An age-decaying weight mechanism gradually reduces the global agent's influence (decaying by 0.95^t per iteration) to ensure local agents develop autonomous policies while maintaining coordination benefits during training.

## Key Results
- Achieved average waiting time of 134.89 seconds across tested scenarios
- Reduced CO2 emissions by approximately 1.5% compared to baseline methods
- Outperformed state-of-the-art methods including Fixed, MA-DDPG, PPO, and TD3 on multiple metrics

## Why This Works (Mechanism)

### Mechanism 1
The global agent resolves local conflicts by providing coordinated guidance that prevents agents from choosing mutually detrimental actions. Each local agent learns a policy for its intersection, but the global agent optimizes total waiting time across all intersections. During training, the global agent's output is weighted and compared against the local agent's output, with the higher-weighted decision being chosen.

### Mechanism 2
The age-decaying weight mechanism ensures that the global agent's influence diminishes appropriately as training progresses. The global agent's importance weight is decayed by a factor of (0.95)^t at each training iteration, gradually reducing its influence while allowing local agents to develop more autonomous policies.

### Mechanism 3
The combination of continuous action space and local-global agent architecture enables dynamic green light duration selection rather than fixed phase lengths. Unlike discrete-action methods that choose from predefined time slots, COMMA-DDPG outputs continuous values representing green light duration, with the local-global architecture ensuring these continuous decisions are coordinated across intersections.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation for traffic control
  - Why needed here: The entire COMMA-DDPG framework is built on MDP principles, where states represent traffic conditions, actions represent green light durations, and rewards represent traffic flow metrics.
  - Quick check question: What are the four components of an MDP and how do they map to the traffic signal control problem?

- Concept: Actor-Critic architecture and DDPG algorithm
  - Why needed here: COMMA-DDPG extends DDPG with multiple agents, so understanding the base algorithm is essential for grasping the modifications.
  - Quick check question: How does DDPG differ from DQN in terms of action space handling and what are the roles of the actor and critic networks?

- Concept: Multi-agent reinforcement learning coordination mechanisms
  - Why needed here: The key innovation is the interaction between local and global agents, which requires understanding how agents can cooperate or compete in shared environments.
  - Quick check question: What are the main challenges in multi-agent RL and how does the global agent architecture address them?

## Architecture Onboarding

- Component map: Local DDPG agents (one per intersection) + Global DDPG agent (training only) + Replay buffer B containing on-policy data + Simulation environment (SUMO/TSIS)
- Critical path: Data generation → Local agent training → Global agent training → Action selection (weighted combination) → Environment interaction → Reward calculation
- Design tradeoffs: Global agent provides coordination but increases computational complexity and memory requirements; continuous actions offer flexibility but require more sophisticated training; age-decaying weights balance exploration and exploitation
- Failure signatures: Poor convergence (local conflicts persist), over-reliance on global agent (local autonomy doesn't develop), unstable policies (continuous action space not well-learned), degraded performance on larger networks (global agent information overload)
- First 3 experiments:
  1. Single intersection baseline: Test COMMA-DDPG without global agent to verify DDPG implementation works correctly
  2. Two intersection coordination: Verify that global agent effectively resolves conflicts between two closely-spaced intersections
  3. Scalability test: Evaluate performance degradation as number of intersections increases beyond 8 to identify when global agent architecture becomes problematic

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of COMMA-DDPG scale with increasingly larger road networks beyond 16 intersections?
Basis in paper: The paper mentions that training agents over larger road networks (more than 10 intersections) may be problematic due to input size constraints, and adjusts the global agent to consider only nearby intersections.
Why unresolved: The paper only provides experimental results for up to 16 intersections and does not explore performance on significantly larger networks.

### Open Question 2
What is the long-term stability and adaptability of COMMA-DDPG when deployed in real-world traffic conditions with unpredictable changes?
Basis in paper: The paper uses SUMO for simulation and mentions the need for future real-world evaluation, but does not address long-term stability or adaptability to real-world unpredictability.
Why unresolved: The paper focuses on controlled simulation environments and does not provide evidence of long-term performance in dynamic, real-world settings.

### Open Question 3
How sensitive is COMMA-DDPG to hyperparameter choices, particularly the global agent importance weight (W^G) and its decay rate?
Basis in paper: The paper mentions using a time decay mechanism for W^G with a ratio of (0.95)^t, but does not explore sensitivity to different decay rates or importance weight configurations.
Why unresolved: The paper does not provide ablation studies or sensitivity analysis for these critical hyperparameters.

## Limitations
- The decay rate of 0.95 for the global agent's weight is heuristic and may not be optimal across different traffic scenarios
- The global agent is only used during training, creating a potential gap between training coordination and real-world deployment
- The study focuses on five intersections in a specific urban area, limiting generalizability to larger or differently structured traffic networks

## Confidence
- **High Confidence**: The fundamental DDPG algorithm and multi-agent coordination mechanisms are well-established in reinforcement learning literature
- **Medium Confidence**: The carbon emission reduction claims (1.5% improvement) depend on the accuracy of emission models and the assumption that traffic flow directly correlates with emissions
- **Low Confidence**: Claims about the global agent preventing "blind exploration" are largely theoretical without extensive ablation studies

## Next Checks
1. **Ablation Study on Global Agent**: Remove the global agent entirely and compare learning curves and final performance to quantify exactly how much the global coordination contributes to the 1.5% emission reduction
2. **Hyperparameter Sensitivity Analysis**: Systematically vary the decay rate (0.9, 0.92, 0.95, 0.98) and learning rates to determine the robustness of the COMMA-DDPG performance
3. **Scalability Validation**: Test the framework on networks with 10, 15, and 20 intersections to empirically determine when the global agent architecture becomes computationally prohibitive