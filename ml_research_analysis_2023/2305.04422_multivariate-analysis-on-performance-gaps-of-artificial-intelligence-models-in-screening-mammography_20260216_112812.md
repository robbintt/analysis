---
ver: rpa2
title: Multivariate Analysis on Performance Gaps of Artificial Intelligence Models
  in Screening Mammography
arxiv_id: '2305.04422'
source_url: https://arxiv.org/abs/2305.04422
tags:
- imaging
- breast
- performance
- bi-rads
- pubmed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors investigate how deep learning models for screening
  mammography abnormality classification perform across different patient subgroups.
  Using the EMBED dataset of 115,931 patients, they train a ResNet152V2 patch classifier
  to distinguish normal from abnormal breast tissue, achieving 92.6% accuracy and
  0.975 AUC.
---

# Multivariate Analysis on Performance Gaps of Artificial Intelligence Models in Screening Mammography

## Quick Facts
- arXiv ID: 2305.04422
- Source URL: https://arxiv.org/abs/2305.04422
- Reference count: 0
- Primary result: ResNet152V2 patch classifier achieves 92.6% accuracy and 0.975 AUC; higher false negative risk with architectural distortion, lower risk with benign lesions; higher false positive risk with dense breast tissue

## Executive Summary
This study investigates how deep learning models for screening mammography abnormality classification perform across different patient subgroups using the EMBED dataset of 115,931 patients. The authors train a ResNet152V2 patch classifier to distinguish normal from abnormal breast tissue, achieving high overall accuracy (92.6%) and AUC (0.975). They then evaluate performance by race, age, breast density, pathology, and imaging features both at patch and image levels. After controlling for confounding factors, they identify specific associations between imaging features and misclassification risk, revealing performance gaps that aggregate metrics would obscure.

## Method Summary
The study extracts patches from mammograms in the EMBED dataset, using radiologist ROI annotations for positive patches and non-ROI areas for negative patches. Patches are resized to 512×512 pixels and used to train multiple CNN architectures, with ResNet152V2 selected for its superior performance. The model is evaluated through bootstrapping (200 iterations) and performance is analyzed across subgroups defined by demographic and clinical characteristics. Multivariate logistic regression is used to control for confounding when assessing the relationship between imaging features and misclassification risk.

## Key Results
- ResNet152V2 achieves 92.6% accuracy and 0.975 AUC for patch-level classification
- Higher false negative risk is associated with architectural distortion after confounding adjustment
- Higher false positive risk is linked to dense breast tissue (BI-RADS C and D categories)
- Benign lesions and mass/asymmetry findings show lower false negative risk

## Why This Works (Mechanism)

### Mechanism 1
Controlling for confounding reveals true associations between imaging features and model misclassification risk. Multivariate logistic regression isolates the effect of each imaging feature while adjusting for race, age, and pathology, eliminating spurious correlations from co-occurring variables.

### Mechanism 2
Subgroup analysis exposes performance gaps that aggregate metrics obscure. By splitting data into demographic and imaging subgroups, the study detects statistically significant differences in AUC and misclassification rates invisible in overall performance.

### Mechanism 3
Patch-level and image-level evaluations capture different failure modes in lesion detection. Patch-level analysis measures pixel-wise accuracy on cropped ROIs, while image-level analysis reflects real-world diagnostic scenarios where multiple findings exist in a single image.

## Foundational Learning

- **Logistic regression for risk adjustment**
  - Why needed: To quantify how imaging features affect misclassification risk while controlling for demographic and clinical confounders
  - Quick check: What happens to the coefficient for architectural distortion after adjusting for age and race in the regression model?

- **Subgroup analysis in machine learning fairness**
  - Why needed: To identify whether model performance varies systematically across patient populations
  - Quick check: How does the AUC for patients with BI-RADS density D compare to those with density A before and after confounding adjustment?

- **Patch-based image preprocessing for CNNs**
  - Why needed: Ensures consistent input size and avoids scale-related bias when training on varied ROI sizes
  - Quick check: Why are patches padded to 512x512 pixels after downsampling?

## Architecture Onboarding

- **Component map**: Data ingestion → Patch extraction (positive/negative) → CNN training (ResNet152V2) → Bootstrapped performance evaluation → Subgroup stratification → Multivariate logistic regression → Risk ratio reporting
- **Critical path**: Patch extraction → CNN training → Subgroup performance evaluation → Confounding adjustment
- **Design tradeoffs**: Smaller patches increase computational efficiency but may lose contextual information; larger patches capture more context but risk class imbalance and increased memory usage
- **Failure signatures**: Inconsistent ROI labeling → biased positive patch selection; unbalanced class distribution → overfitting to majority class; poor convergence → suboptimal learning rate or architecture choice
- **First 3 experiments**:
  1. Validate patch extraction consistency by comparing ROI-based positives against radiologist annotations on a held-out set
  2. Test alternative architectures (e.g., EfficientNet) to confirm ResNet152V2's superiority is not due to dataset artifacts
  3. Perform ablation study removing confounding variables one at a time to quantify their impact on risk ratios

## Open Questions the Paper Calls Out

### Open Question 1
What specific confounding factors between demographic groups, imaging features, and pathology outcomes are responsible for the observed differences in model performance, and how can these be isolated and controlled in future studies? The authors acknowledge potential confounding effects but do not explore specific confounders or methods to isolate them.

### Open Question 2
How would the performance of this patch-level classifier translate to an object detection model for mammography, and would similar subgroup disparities be observed? The study only evaluates a patch-level classifier, not an object detection model, so the translation to real-world clinical use is unknown.

### Open Question 3
Are the observed performance differences by race and age due to true biological or imaging differences, or are they artifacts of dataset composition, annotation bias, or other systemic factors? The authors note these disparities are "more difficult to understand" and suggest potential confounders.

## Limitations
- Single-center retrospective design limits generalizability across different populations and imaging systems
- Small sample sizes in certain racial subgroups (particularly Asian patients) may produce unstable estimates
- Patch-based approach may miss contextual information critical for some imaging features like architectural distortion

## Confidence

- **High Confidence**: Overall model performance metrics (92.6% accuracy, 0.975 AUC) and the general finding that breast density affects false positive rates
- **Medium Confidence**: Specific associations between imaging features and misclassification risk after confounding adjustment
- **Low Confidence**: Precise risk ratios reported for minority subgroups due to small sample sizes

## Next Checks
1. Conduct external validation on a multi-center dataset to assess generalizability of the subgroup performance patterns and confirm that breast density and architectural distortion associations persist across different populations and imaging systems.

2. Perform sensitivity analysis by systematically removing potential unmeasured confounders to quantify their potential impact on the reported risk ratios and determine if conclusions are robust to omitted variable bias.

3. Replicate the analysis using alternative machine learning architectures (e.g., Vision Transformers or EfficientNet) to confirm that the observed subgroup performance patterns are not artifacts of the specific ResNet152V2 architecture choice.