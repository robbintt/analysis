---
ver: rpa2
title: The FruitShell French synthesis system at the Blizzard 2023 Challenge
arxiv_id: '2309.00223'
source_url: https://arxiv.org/abs/2309.00223
tags:
- speech
- data
- synthesis
- task
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a French text-to-speech synthesis system developed
  for the Blizzard Challenge 2023. The authors used a VITS-based acoustic model with
  the HiFiGAN vocoder for both Hub and Spoke tasks.
---

# The FruitShell French synthesis system at the Blizzard 2023 Challenge

## Quick Facts
- arXiv ID: 2309.00223
- Source URL: https://arxiv.org/abs/2309.00223
- Reference count: 0
- Primary result: French TTS system achieving MOS scores of 3.6 (Hub) and 3.4 (Spoke) using VITS + HiFiGAN

## Executive Summary
This paper presents a French text-to-speech synthesis system developed for the Blizzard Challenge 2023. The system uses a VITS-based acoustic model with HiFiGAN vocoder for both Hub and Spoke tasks. The authors implemented comprehensive data preprocessing, including text cleaning, phonetic transcription using a G2P model, and data augmentation for the Spoke task. The system achieved average-level performance among all participating teams, with quality MOS scores of 3.6 for the Hub task and 3.4 for the Spoke task.

## Method Summary
The system employs a VITS-based acoustic model with HiFiGAN vocoder architecture. For data preprocessing, the authors removed missing/erroneous text, eliminated non-pronounced symbols, added word boundary and start/end symbols, and performed data augmentation for the Spoke task. French text was transcribed into phonemes using an open-source G2P model and converted to the competition's phonetic scheme. All audio was resampled to 16 kHz. The Spoke task incorporated multi-speaker data augmentation to improve speaker similarity.

## Key Results
- Achieved quality MOS score of 3.6 for Hub task
- Achieved quality MOS score of 3.4 for Spoke task
- Placed at average level among all participating teams
- Successfully implemented end-to-end French TTS system for competition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data preprocessing improves synthesis quality by removing noise and standardizing symbols.
- Mechanism: The authors removed erroneous text, eliminated non-pronounced symbols, and added word boundary and start/end symbols to improve model training efficiency and speech naturalness.
- Core assumption: Cleaner data leads to better acoustic feature extraction and more accurate duration prediction.
- Evidence anchors:
  - [abstract] "we conducted a screening process to remove missing or erroneous text data"
  - [section] "we filtered out these non-phonetic symbols and removed them from the text data, making the data more concise and efficient for training"
- Break condition: If preprocessing removes too much contextual information or introduces biases that degrade pronunciation accuracy.

### Mechanism 2
- Claim: Multi-speaker data augmentation improves specific speaker synthesis performance.
- Mechanism: For the Spoke task, additional multi-speaker French data was combined with the limited target speaker data to train a more robust speaker-conditioned model.
- Core assumption: Training on diverse speaker data improves the model's ability to generalize speaker characteristics and adapt to specific targets.
- Evidence anchors:
  - [abstract] "we performed data augmentation according to the competition rules" and "we trained a multi-speaker model"
  - [section] "we searched for high-quality open-source multi-speaker French datasets and combined them with the provided NEB dataset to train a multi-speaker model"
- Break condition: If the additional data introduces conflicting speaker characteristics that confuse the model's speaker identity learning.

### Mechanism 3
- Claim: Random duration predictor introduces natural variation in synthesized speech.
- Mechanism: The VITS model uses a random duration predictor to generate different durations for each synthesis, making the speech more varied and natural rather than deterministic.
- Core assumption: Natural speech has inherent variation in phoneme duration, and modeling this randomness improves perceived naturalness.
- Evidence anchors:
  - [abstract] "The random duration predictor in the VITS model is a special type of duration predictor that is used to introduce randomness and diversity"
  - [section] "The random duration predictor generates different durations for each synthesized speech, making the generated speech more varied and natural"
- Break condition: If the random variation exceeds natural bounds and produces unnatural speech patterns.

## Foundational Learning

- Concept: Variational Autoencoder (VAE) architecture
  - Why needed here: VITS uses VAE structure for acoustic modeling, converting text to latent representations and back to acoustic features
  - Quick check question: How does the KL divergence loss in VAE help ensure smoother acoustic feature generation?

- Concept: Grapheme-to-Phoneme (G2P) conversion
  - Why needed here: French text must be converted to phonetic representations for the acoustic model, using an IPA-based G2P model then converting to competition-specific scheme
  - Quick check question: Why did the authors need to convert IPA symbols to the competition's phonetic scheme?

- Concept: Speaker conditioning in multi-speaker models
  - Why needed here: For the Spoke task, speaker information was incorporated into duration predictor, vocoder, and flow layers to differentiate between speakers
  - Quick check question: What specific model components were modified to incorporate speaker information for the Spoke task?

## Architecture Onboarding

- Component map: VITS acoustic model (encoder, decoder, KL divergence) → HiFiGAN vocoder (generator, discriminator) → Random duration predictor → Speaker conditioning modules
- Critical path: Text preprocessing → G2P transcription → VITS encoding → Duration prediction → Acoustic feature generation → HiFiGAN waveform synthesis
- Design tradeoffs: End-to-end approach simplifies pipeline but may accumulate errors; random duration adds naturalness but could introduce instability
- Failure signatures: Low MOS scores indicate quality issues; high pronunciation error rates suggest G2P or preprocessing problems; similarity score drops suggest speaker conditioning issues
- First 3 experiments:
  1. Test preprocessing pipeline on small dataset to verify text cleaning and symbol standardization
  2. Validate G2P conversion by comparing IPA output with expected phonetic scheme
  3. Run single-speaker baseline without augmentation to establish performance floor for comparison

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the random duration predictor in the VITS model specifically improve the naturalness of synthesized speech compared to traditional deterministic duration predictors?
- Basis in paper: [explicit] The paper mentions the use of a random duration predictor in the VITS model and states that it introduces randomness and diversity into the speech synthesis process, making the generated speech more varied and natural.
- Why unresolved: The paper does not provide specific details on how the random duration predictor achieves this improvement or any empirical evidence to support its effectiveness.
- What evidence would resolve it: Comparative experiments between speech synthesis systems using random and deterministic duration predictors, along with objective and subjective evaluations of the naturalness of the synthesized speech, would provide evidence to resolve this question.

### Open Question 2
- Question: What is the impact of the data augmentation process on the performance of the Spoke task, and how does it compare to using only the provided NEB dataset?
- Basis in paper: [explicit] The paper mentions that data augmentation was performed for the Spoke task by combining the NEB dataset with additional multi-speaker French datasets. However, it does not provide a detailed analysis of the impact of this augmentation on the task performance.
- Why unresolved: The paper does not present a comparison between the performance of the Spoke task using only the NEB dataset and using the augmented dataset. Therefore, the specific impact of data augmentation on the task performance remains unresolved.
- What evidence would resolve it: Conducting experiments with and without data augmentation for the Spoke task, and comparing the performance metrics such as quality, similarity, and pronunciation error rate, would provide evidence to resolve this question.

### Open Question 3
- Question: How does the conversion of IPA symbols to the competition's phonetic scheme affect the accuracy and naturalness of the synthesized speech?
- Basis in paper: [explicit] The paper mentions that the IPA symbols were converted to the phonetic scheme used in the competition data due to compiler limitations. However, it does not discuss the potential impact of this conversion on the synthesized speech.
- Why unresolved: The paper does not provide any analysis or evaluation of the impact of the conversion process on the accuracy and naturalness of the synthesized speech. Therefore, the specific effects of this conversion remain unresolved.
- What evidence would resolve it: Conducting experiments with and without the conversion of IPA symbols to the competition's phonetic scheme, and comparing the performance metrics such as pronunciation error rate and speech quality, would provide evidence to resolve this question.

## Limitations
- Lack of detailed implementation specifications for data preprocessing steps and phonetic conversion pipeline
- Missing training hyperparameters and model configuration details
- No comparative analysis of the impact of data augmentation on Spoke task performance

## Confidence

- **High confidence**: The fundamental approach using VITS with HiFiGAN vocoder is technically sound and well-established in the TTS literature.
- **Medium confidence**: The preprocessing steps and phonetic conversion approach are appropriate but lack implementation details that would affect reproducibility.
- **Low confidence**: The impact of specific design choices (such as the random duration predictor's implementation details or the exact speaker conditioning mechanism) cannot be assessed without additional technical specifications.

## Next Checks
1. **Preprocessing validation**: Test the text cleaning pipeline on a small sample dataset to verify that non-pronounced symbols are correctly identified and removed without affecting phonetic content, and that word boundary symbols are properly inserted.
2. **Phonetic transcription accuracy**: Compare the output of the G2P model against ground truth phonetic transcriptions for a subset of French text samples to measure conversion accuracy and identify systematic errors.
3. **Speaker conditioning evaluation**: Train a simplified version of the Spoke task model with and without the additional multi-speaker data to quantify the actual contribution of data augmentation to the similarity score improvement.