---
ver: rpa2
title: Comparing Large Language Model AI and Human-Generated Coaching Messages for
  Behavioral Weight Loss
arxiv_id: '2312.04059'
source_url: https://arxiv.org/abs/2312.04059
tags:
- messages
- weight
- chatgpt
- were
- participants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the feasibility and acceptability of using
  ChatGPT for behavioral weight loss (BWL) coaching. Participants (n=87) rated ten
  coaching messages (five human-written, five AI-generated) for helpfulness and identified
  which they believed were AI-generated.
---

# Comparing Large Language Model AI and Human-Generated Coaching Messages for Behavioral Weight Loss

## Quick Facts
- arXiv ID: 2312.04059
- Source URL: https://arxiv.org/abs/2312.04059
- Authors: 
- Reference count: 40
- Key outcome: AI-generated weight loss coaching messages matched human-written ones in helpfulness after prompt refinement, with 50% of AI messages misidentified as human-written.

## Executive Summary
This study evaluated ChatGPT's feasibility for behavioral weight loss coaching by comparing AI-generated messages with human-written ones. Using 87 participants who rated ten coaching messages (five human, five AI) for helpfulness, the research found that with prompt refinements, AI messages achieved similar helpfulness ratings to human messages (82% vs 88% scoring ≥3 on a 5-point scale). Notably, participants were no better than chance at identifying AI-generated messages, suggesting the LLM's sophistication in mimicking human coaching style. The findings demonstrate that carefully engineered prompts can enable LLMs to produce effective, personalized weight loss coaching content.

## Method Summary
The study employed a mixed-methods design across two phases. In Phase 1, 87 participants rated 10 coaching messages (5 human, 5 AI-generated) on a 5-point helpfulness scale and identified authorship. Human messages were written by a Master's-level coach following specific formatting guidelines, while AI messages used ChatGPT with similar instructions. Qualitative feedback informed Phase 2 refinements, where prompts were adjusted to add empathy cues, avoid assumptions, and use more natural language. The refined AI messages were then compared to human messages using the same rating process. Thematic analysis of open-ended feedback identified key themes about message quality and preferences.

## Key Results
- AI messages matched human messages in helpfulness after prompt refinement (82% vs 88% scoring ≥3)
- 50% of AI messages were misidentified as human-written, indicating high naturalness
- Thematic analysis revealed participants valued AI's empathy but found messages formulaic and too data-focused

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT can generate weight loss coaching messages that match human-written ones in helpfulness when prompts are carefully tuned.
- Mechanism: The LLM uses natural language processing to interpret structured instructions and participant data, then synthesizes personalized feedback, praise, and behavioral strategies in a single paragraph.
- Core assumption: The LLM can reliably produce messages with the appropriate tone and empathy when explicit tone modifiers and formatting constraints are added to the prompt.
- Evidence anchors:
  - [abstract] "in Phase 2, the AI messages matched the human-written ones regarding helpfulness, with 82% scoring three or above"
  - [section] "With a refined prompt in Phase 2, the messages displayed support and empathy and offered personalized behavioral suggestions"
- Break condition: If tone modifiers are omitted or data context is missing, messages become formulaic, impersonal, and less helpful.

### Mechanism 2
- Claim: Participants cannot reliably distinguish AI-generated messages from human-written ones, indicating high naturalness of LLM output.
- Mechanism: The LLM produces fluent, contextually relevant text that mimics the style of a trained human coach, reducing detectable artifacts.
- Core assumption: Fluency and contextual appropriateness are sufficient for users to perceive messages as human-written.
- Evidence anchors:
  - [abstract] "50% were misidentified as human-written, suggesting AI's sophistication in mimicking human-generated content"
  - [section] "participants were no better than chance at identifying the authorship of AI messages"
- Break condition: If messages contain overly generic phrasing or data-driven assertions without contextual nuance, participants will correctly identify them as AI-generated.

### Mechanism 3
- Claim: Iterative prompt refinement based on participant feedback can substantially improve AI message quality and helpfulness.
- Mechanism: Qualitative feedback identifies shortcomings (e.g., impersonal tone, excessive data focus), which guide targeted prompt adjustments (e.g., adding empathy cues, avoiding assumptions).
- Core assumption: Feedback from a small sample can reveal generalizable patterns that improve subsequent LLM outputs.
- Evidence anchors:
  - [section] "Given our Phase 1 findings, we adjusted the ChatGPT prompt in Phase 2 to mainly address the first concern noted above"
  - [abstract] "In Phase 1, AI-generated messages were rated less helpful than human-written ones... However, in Phase 2, the AI messages matched the human-written ones"
- Break condition: If feedback is ignored or prompt changes are not aligned with user preferences, helpfulness ratings will not improve.

## Foundational Learning

- Concept: Large Language Models (LLMs) and prompt engineering
  - Why needed here: The entire feasibility study depends on how well prompts guide ChatGPT to produce clinically appropriate coaching messages.
  - Quick check question: What prompt components were added in Phase 2 to improve empathy and tone?
- Concept: Thematic analysis for qualitative feedback
  - Why needed here: Identifying user preferences and message shortcomings requires systematic coding and theme development from open-ended responses.
  - Quick check question: How many themes emerged from the thematic analysis of participant feedback?
- Concept: Randomized controlled trial design basics
  - Why needed here: Understanding how participants were blinded to authorship and how ratings were collected is critical for interpreting validity.
  - Quick check question: What was the scale used to rate message helpfulness?

## Architecture Onboarding

- Component map: Data input → Prompt template → LLM (ChatGPT/GPT-4) → Message output → Participant rating & feedback → Thematic analysis → Prompt refinement cycle
- Critical path: Data → Prompt → Message → Rating → Feedback → Theme → Refinement → Next prompt iteration
- Design tradeoffs: Human-authored vs. AI-generated messages—cost/time savings vs. potential for formulaic tone; prompt specificity vs. flexibility
- Failure signatures: Low helpfulness ratings, high identification accuracy (users spot AI), negative qualitative themes (impersonal, data-driven)
- First 3 experiments:
  1. Re-run Phase 1 with a fixed prompt to confirm baseline variability in AI message quality.
  2. Test a GPT-4 prompt with explicit empathy and autonomy-encouraging language against the Phase 2 ChatGPT prompt.
  3. Conduct a small-scale RCT where participants receive either human, ChatGPT, or GPT-4 messages to compare clinical outcomes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do ChatGPT-generated messages compare to human-written messages in terms of long-term weight loss outcomes when implemented in randomized controlled trials?
- Basis in paper: [explicit] The study notes that ChatGPT messages were rated as helpful but acknowledges the need for further evaluation of clinical effectiveness in randomized controlled trials.
- Why unresolved: This study only assessed perceived helpfulness and message quality, not actual weight loss outcomes over time.
- What evidence would resolve it: Results from randomized controlled trials comparing weight loss outcomes between groups receiving ChatGPT-generated messages versus human-written messages over 6-12 months.

### Open Question 2
- Question: What specific prompt modifications to ChatGPT could further reduce the formulaic nature of generated messages while maintaining personalization and empathy?
- Basis in paper: [explicit] Thematic analysis revealed participants found AI messages formulaic and less authentic, suggesting room for improvement through prompt refinement.
- Why unresolved: While Phase 2 showed improvements, the study only explored limited prompt modifications focusing on tone, not fully addressing concerns about formulaic content.
- What evidence would resolve it: Comparative studies testing various prompt structures and content requirements to identify optimal configurations that produce more authentic, less formulaic messages.

### Open Question 3
- Question: How does individual participant preference for message tone (assertive vs. encouraging) affect engagement and weight loss outcomes when using LLM-generated coaching messages?
- Basis in paper: [inferred] The study mentions participants had varied tone preferences and suggests future systems could allow users to select message tone.
- Why unresolved: The study did not test whether matching message tone to individual preferences improves outcomes or whether a one-size-fits-all approach is sufficient.
- What evidence would resolve it: Longitudinal studies comparing engagement metrics and weight loss outcomes between groups receiving matched vs. mismatched message tones based on individual preferences.

## Limitations

- Prompt engineering details are not fully specified, making exact replication difficult
- Single-coach human comparison limits generalizability of naturalness findings
- Study only assessed perceived helpfulness, not actual weight loss outcomes
- 50% identification accuracy could reflect either sophisticated AI or formulaic human messages

## Confidence

- High confidence: AI message helpfulness matching human messages in Phase 2 (82% vs 88% scoring ≥3)
- Medium confidence: Participant inability to distinguish AI from human messages (50% accuracy)
- Low confidence: Clinical significance of findings without outcome data

## Next Checks

1. Replicate prompt refinement process with explicit tone and empathy modifiers to confirm Phase 2 improvements in message helpfulness
2. Conduct blinded comparison with multiple human coaches to establish baseline for message naturalness and helpfulness
3. Test clinical outcomes by assigning participants to receive either AI or human coaching messages and measuring weight loss progress