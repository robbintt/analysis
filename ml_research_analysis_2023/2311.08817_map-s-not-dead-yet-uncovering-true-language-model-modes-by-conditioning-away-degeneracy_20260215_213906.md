---
ver: rpa2
title: 'MAP''s not dead yet: Uncovering true language model modes by conditioning
  away degeneracy'
arxiv_id: '2311.08817'
source_url: https://arxiv.org/abs/2311.08817
tags:
- tokens
- mode
- length
- conditional
- unconditional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that the widespread observation of degenerate
  modal outputs from language models is not an inherent property of probabilistic
  models, but rather a consequence of contamination in the training data by low-entropy
  distractors. By conditioning the search for modal outputs on attributes that avoid
  specific degenerate behaviors, such as output length, the authors show that the
  resulting conditional modes are high-quality and do not exhibit the same degeneracies
  as the unconditional modes.
---

# MAP's not dead yet: Uncovering true language model modes by conditioning away degeneracy

## Quick Facts
- arXiv ID: 2311.08817
- Source URL: https://arxiv.org/abs/2311.08817
- Reference count: 40
- This paper demonstrates that the widespread observation of degenerate modal outputs from language models is not an inherent property of probabilistic models, but rather a consequence of contamination in the training data by low-entropy distractors.

## Executive Summary
This paper challenges the common belief that maximum a posteriori (MAP) decoding from language models inevitably produces degenerate outputs. The authors argue that mode-seeking decoding can be viable if the search for modal outputs is conditioned on attributes that avoid degenerate behaviors. They propose attribute-conditional beam search (ACBS), an efficient approximation of conditional mode finding, which outperforms ordinary beam search in finding higher-quality, length-constrained outputs. Applying ACBS to a 7B-parameter LLaMA model, the authors demonstrate the extraction of reasonable instruction-following outputs without finetuning, suggesting improved search-based decoding algorithms can maximize performance from general-purpose language models.

## Method Summary
The paper introduces a two-pronged approach to finding high-quality modal outputs from language models. First, it uses an exact mode finder (depth-first search) to identify global and conditional modes by exhaustively searching the model's output space. Second, it proposes attribute-conditional beam search (ACBS), which augments beam search scores with attribute classifier predictions to approximate conditional mode finding efficiently. The key insight is that conditioning the search on attributes that exclude degenerate behaviors (like output length) can lead to high-quality modes. The authors validate their approach across multiple tasks (machine translation, story generation, instruction following) using various models (MarianMT, GPT-2, LLaMA-7B) and demonstrate significant improvements over ordinary beam search.

## Key Results
- Exact mode finding reveals that global modes are often degenerate (empty or repetitive outputs), while conditional modes (e.g., length-constrained) are high-quality and diverse.
- ACBS outperforms ordinary beam search in finding higher-likelihood, length-constrained outputs that are more fluent and topically coherent.
- Applying ACBS to LLaMA-7B without finetuning or modification enables the extraction of reasonable instruction-following outputs, outperforming ordinary beam search and matching chain-of-thought prompting in some cases.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Degenerate modes arise from contamination of training data by low-entropy distractors, not from fundamental flaws in probabilistic models.
- Mechanism: When a tiny amount of low-entropy noise (e.g., empty sequences, repetitive prompts) is mixed with a high-entropy population text distribution, the resulting contaminated distribution's mode becomes degenerate. Models trained on this data will also assign high probability to these degenerate outputs.
- Core assumption: The mode of a distribution is sensitive to contamination by low-entropy distractors, even when such distractors are extremely rare.
- Evidence anchors:
  - [abstract]: "Specifically, we argue that mixing even a tiny amount of low-entropy noise with a population text distribution can cause the data distribution's mode to become degenerate."
  - [section 2.1]: "Imagine that each training example is replaced by a uniform sample from a set of 10 bad outputs with a probability of ϵ... To prevent one of these 10 bad sequences from being modal for the scientific abstract distribution, we need ϵ/(1 − ϵ) < 10/2100, meaning that the noise rate must be vanishingly small."
  - [corpus]: Weak evidence; no direct citations to prior work establishing this mechanism in NLP.
- Break condition: If the training data is carefully curated to remove low-entropy distractors, or if the model is trained on synthetic data with known distribution properties.

### Mechanism 2
- Claim: Attribute-conditional search finds high-quality modes by conditioning on attributes that avoid degenerate behaviors.
- Mechanism: Instead of searching for the global mode Pmodel(y|x), search for the conditional mode Pmodel(y|x, A(y)=a), where A(y)=a is an attribute that avoids degeneracy (e.g., output length, semantic quality). This effectively removes low-entropy distractors from consideration.
- Core assumption: The conditional mode under a well-trained model will be high-quality if the conditioning attribute excludes degenerate behaviors.
- Evidence anchors:
  - [abstract]: "By conditioning the search for modal outputs on attributes that avoid specific degenerate behaviors, such as output length, the authors show that the resulting conditional modes are high-quality and do not exhibit the same degeneracies as the unconditional modes."
  - [section 3.2]: "For machine translation, we replicate the finding in Stahlberg & Byrne (2019) that modal MT outputs are often empty. We find that the mode of the MarianMT Zh-En model is the empty sequence for 57.7% of the 2002 source sentences."
  - [section 4]: "The high quality of these conditional modes suggests that the rare noisy low-entropy distractors (like truncated output) in the training data might be responsible for the model assigning high scores to degenerate sequences."
- Break condition: If the conditioning attribute is poorly chosen (e.g., doesn't exclude the actual degenerate behavior) or if the model is poorly trained.

### Mechanism 3
- Claim: ACBS (Attribute-Conditional Beam Search) approximates conditional mode finding efficiently by augmenting beam search scores with attribute classifier predictions.
- Mechanism: ACBS maintains a modified score S'(x1:t, a) = S(x1:t) + log Pclf(A(x)=a|x1:t), where S(x1:t) is the ordinary beam search score and Pclf is a classifier predicting the probability that a complete sequence has the desired attribute. This approximates searching for the conditional mode without exhaustive search.
- Core assumption: A classifier trained to predict whether model outputs have a desired attribute can effectively guide beam search toward high-quality conditional modes.
- Evidence anchors:
  - [section 5.1]: "Therefore, we condition the partial hypotheses on a as well and maintain the modified score: S'(x1:t, a) = S(x1:t) + log Pmodel(A(x)=a|x1:t)."
  - [section 5.2]: "Overall, ACBS outperforms unconditional beam search in terms of finding high-likelihood sequences satisfying the length constraint."
  - [section 5.3]: "Applying it to the same models as we used for exact search, we find that we can perform length-conditional generation to find outputs which are both higher scoring and more fluent than those found by beam search, for a given length."
- Break condition: If the attribute classifier is poorly trained or if the classifier predictions are uncorrelated with actual attribute values.

## Foundational Learning

- Concept: Mode-seeking (MAP) decoding vs. sampling-based decoding
  - Why needed here: The paper contrasts these two decoding paradigms and argues that mode-seeking can be viable if the right conditioning is applied.
  - Quick check question: Why does MAP decoding often produce degenerate outputs in language models, while sampling-based methods are more robust?

- Concept: Low-entropy distractors and their effect on distribution modes
  - Why needed here: The paper's central claim is that degenerate modes arise from contamination by low-entropy distractors, not from fundamental model flaws.
  - Quick check question: How does mixing even a tiny amount of low-entropy noise with a high-entropy text distribution affect the mode of the resulting distribution?

- Concept: Conditional probability and Bayes' rule application
  - Why needed here: The ACBS algorithm uses Bayes' rule to factor the conditional probability and create an efficient search heuristic.
  - Quick check question: How does Bayes' rule allow ACBS to approximate the conditional probability Pmodel(y|x, A(y)=a) without explicitly computing it?

## Architecture Onboarding

- Component map:
  Exact mode finder (DFS-based) -> Attribute classifiers -> ACBS algorithm -> LLaMA-7B model -> Evaluation metrics

- Critical path:
  1. Train attribute classifiers on model-generated outputs
  2. Implement ACBS algorithm using classifiers to guide beam search
  3. Apply ACBS to target model (e.g., LLaMA-7B) for instruction following
  4. Evaluate outputs using multiple quality metrics

- Design tradeoffs:
  - Exact vs. approximate mode finding: DFS gives exact modes but is computationally expensive; ACBS is efficient but approximate
  - Classifier training data: Using model-generated outputs vs. human-annotated data
  - Attribute choice: Length is easy to measure but may not capture all degeneracies; semantic quality is harder to measure but more comprehensive

- Failure signatures:
  - ACBS producing repetitive or low-quality outputs despite high attribute classifier scores
  - Classifier predictions are uncorrelated with actual attribute values
  - Exact mode finder fails to find modes within reasonable time/memory limits

- First 3 experiments:
  1. Verify that exact mode finder correctly identifies degenerate modes in MarianMT and LLaMA models
  2. Train length classifier and verify it accurately predicts output lengths on model-generated samples
  3. Implement ACBS and compare outputs to ordinary beam search on MarianMT for various target lengths

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of ACBS change with different classifier architectures beyond LoRA fine-tuning of LLaMA-7B?
- Basis in paper: [inferred] The paper uses a LoRA fine-tuned LLaMA-7B as the classifier architecture, but does not explore other architectures.
- Why unresolved: The paper does not compare ACBS performance with different classifier architectures, leaving open the question of whether LoRA fine-tuning is optimal.
- What evidence would resolve it: Experiments comparing ACBS performance using various classifier architectures (e.g., different fine-tuning methods, transformer-based classifiers, etc.) on the same tasks.

### Open Question 2
- Question: What is the relationship between the entropy of valid outputs and the rate of degenerate modal sequences across different model scales and tasks?
- Basis in paper: [explicit] The paper discusses how low-entropy distractors in training data can lead to degenerate modes, and notes that degenerate modes are more common for open-ended tasks.
- Why unresolved: The paper does not provide a quantitative analysis of how output entropy relates to mode degeneracy across different model sizes and task types.
- What evidence would resolve it: A systematic study measuring output entropy and mode degeneracy rates across various model scales, tasks, and training datasets.

### Open Question 3
- Question: How does the choice of attribute (e.g., length, relevance, fluency) affect the quality and diversity of conditional modes found by ACBS?
- Basis in paper: [explicit] The paper primarily uses length and a reward model score as attributes for ACBS, but does not explore other potential attributes.
- Why unresolved: The paper does not investigate how different attribute choices impact the resulting conditional modes in terms of quality, diversity, or other metrics.
- What evidence would resolve it: Experiments using ACBS with various attributes (e.g., specific content requirements, stylistic constraints, etc.) and analyzing the resulting modes' properties.

## Limitations

- The paper's claim that degenerate modes arise from contamination by low-entropy distractors in training data remains partially speculative, with limited direct evidence linking specific training data contaminants to observed degenerate outputs.
- The proposed ACBS algorithm relies heavily on the quality of attribute classifiers, which may not generalize well to all types of degeneracies or model architectures.
- The computational cost of exact mode finding limits its scalability to larger models and datasets.

## Confidence

- **High Confidence:** The empirical finding that conditional modes (conditioned on attributes like length) are higher quality than unconditional modes across multiple models and tasks.
- **Medium Confidence:** The theoretical argument that mixing low-entropy noise with high-entropy distributions can cause degeneracy in the mode, and that this explains observed degenerate outputs in language models.
- **Medium Confidence:** The practical effectiveness of ACBS in finding higher-quality, length-constrained outputs compared to ordinary beam search on LLaMA-7B.

## Next Checks

1. Conduct ablation studies to isolate the effect of specific types of low-entropy contaminants (truncated outputs, repetitive patterns) in training data on mode degeneracy.
2. Evaluate ACBS performance across a broader range of language models (different sizes, architectures) and attribute types (beyond length) to assess generalizability.
3. Compare the quality and diversity of ACBS outputs to other decoding strategies (nucleus sampling, contrastive search) on long-form generation tasks to benchmark practical utility.