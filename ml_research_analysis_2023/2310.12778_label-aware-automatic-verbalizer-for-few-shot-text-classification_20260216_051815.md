---
ver: rpa2
title: Label-Aware Automatic Verbalizer for Few-Shot Text Classification
arxiv_id: '2310.12778'
source_url: https://arxiv.org/abs/2310.12778
tags:
- class
- words
- classification
- verbalizer
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of few-shot text classification
  by improving the verbalizer component in prompt-based learning. The proposed Label-Aware
  Automatic Verbalizer (LAAV) enhances the standard approach by incorporating manual
  class labels into the template construction process using a conjunction ("and"),
  thereby inducing the language model to generate more relevant words for each class.
---

# Label-Aware Automatic Verbalizer for Few-Shot Text Classification

## Quick Facts
- arXiv ID: 2310.12778
- Source URL: https://arxiv.org/abs/2310.12778
- Reference count: 10
- Key outcome: LAAV improves few-shot text classification by incorporating class labels into templates using "and", achieving 1.2%+ Macro F1 score improvements in 4-shot settings across 5 languages.

## Executive Summary
This paper addresses the challenge of few-shot text classification by improving the verbalizer component in prompt-based learning. The proposed Label-Aware Automatic Verbalizer (LAAV) enhances standard approaches by incorporating manual class labels into template construction using a conjunction ("and"), inducing language models to generate more relevant words for each class. Experiments on five datasets across five languages demonstrate that LAAV significantly outperforms existing verbalizers, especially in low-resource settings with 16 or fewer training examples per class. The method shows better word relevance and discriminative power through higher logits differences between classes.

## Method Summary
LAAV constructs verbalizers by integrating manual class labels into template construction using the conjunction "and" to generate more relevant words for each class. The template format is "[ x] It was [yi] and [MASK]" where x is the input text and yi is the class label. The method uses a language model to predict masked tokens, scores tokens across training examples, creates the verbalizer, and fine-tunes the model using cross-entropy loss. The approach is evaluated across five datasets (AG's News, SmSA, Students' Feedback, Wisesight sentiment, Shopee Reviews) in multiple languages using RoBERTa-based models fine-tuned with Adam optimizer at learning rate 1e-5.

## Key Results
- LAAV achieves 1.2%+ absolute Macro F1 score improvement over baselines in 4-shot settings across all five datasets
- The method shows significantly better performance in low-resource settings with 16 or fewer examples per class
- LAAV generates more discriminative tokens with higher logits differences between classes compared to AMuLaP and NPPrompt

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using "and" with manual labels in templates improves LM's word relevance for class representation
- Mechanism: By incorporating the class label into the template with "and" (e.g., "Feather is light and [MASK]"), the language model is more likely to generate words semantically related to the class rather than generic predictions
- Core assumption: The conjunction "and" effectively constrains the language model to generate words semantically related to both the input text and the class label
- Evidence anchors:
  - [abstract]: "we use the manual labels along with the conjunction 'and' to induce the model to generate more effective words for the verbalizer"
  - [section]: "As a result, our LAAV template for creating S(yi) is Tyi(x) = [ x] It was [yi] and [MASK]"
  - [corpus]: Weak - the corpus search found related work but no direct evidence about the effectiveness of "and" as a conjunction
- Break condition: If the language model's training corpus doesn't associate "and" with maintaining semantic relevance, or if class labels are too generic

### Mechanism 2
- Claim: LAAV improves verbalizer quality by selecting more discriminative tokens than AMuLaP and NPPrompt
- Mechanism: LAAV's template design leads to tokens with higher logits differences between classes, indicating better class discrimination
- Core assumption: Higher logits differences between classes indicate more discriminative token representations
- Evidence anchors:
  - [abstract]: "our analysis reveals that LAAV suggests more relevant words compared to similar approaches"
  - [section]: "Based on Figure 2, the fine-tuned LMs using words from LAAV consistently give higher logits difference compared to words from AMuLaP and NPPrompt"
  - [corpus]: Weak - the corpus search shows related work but no direct comparison of logits differences
- Break condition: If the evaluation metric (logits difference) doesn't correlate with actual classification performance, or if the language model's embedding space changes

### Mechanism 3
- Claim: LAAV is particularly effective in low-resource settings (16 or fewer examples per class)
- Mechanism: By improving word relevance through label-aware prompting, LAAV compensates for limited training data
- Core assumption: Better word relevance in the verbalizer can compensate for insufficient training examples
- Evidence anchors:
  - [abstract]: "LAAV significantly outperforms existing verbalizers, especially in low-resource settings with 16 or fewer training examples per class"
  - [section]: "In an extreme few-shot setting with only 4 training examples per class, our model improves the Macro F1 scores by at least 1.2% absolute from other baselines across five datasets"
  - [corpus]: Weak - corpus search doesn't provide specific evidence about low-resource effectiveness
- Break condition: If the language model can learn effectively from few examples without label-aware prompting, or if the prompt design becomes too restrictive

## Foundational Learning

- Concept: Prompt-based learning
  - Why needed here: Understanding how templates and verbalizers work together in prompt-based approaches is fundamental to LAAV's design
  - Quick check question: How does a prompt-based approach differ from traditional fine-tuning in text classification?

- Concept: Masked Language Models (MLMs)
  - Why needed here: LAAV relies on MLMs to predict masked tokens, so understanding MLM behavior is crucial
  - Quick check question: Why are masked language models preferred for classification tasks in prompt-based learning?

- Concept: Verbalizer construction methods
  - Why needed here: LAAV builds on and improves existing verbalizer construction approaches (AMuLaP, NPPrompt)
  - Quick check question: What are the key differences between manual, automatic, and embedding-based verbalizer construction methods?

## Architecture Onboarding

- Component map:
  Input text → Template construction (with class label and "and") → Masked language model prediction → Token scoring → Verbalizer creation → Fine-tuning → Classification

- Critical path:
  1. Template construction with label and "and"
  2. LM prediction on masked positions
  3. Token scoring across training examples
  4. Verbalizer creation and fine-tuning
  5. Classification using the learned verbalizer

- Design tradeoffs:
  - Template complexity vs. LM understanding: More complex templates may confuse the LM
  - Number of tokens per class (k=32) vs. computational efficiency
  - Fixed vs. learned conjunctions for template construction

- Failure signatures:
  - Low logits differences between classes
  - Tokens with high scores but low semantic relevance to classes
  - Performance degradation compared to traditional fine-tuning with sufficient data

- First 3 experiments:
  1. Compare LAAV's token selection quality vs. AMuLaP on a small dataset (e.g., AG's News)
  2. Test different conjunctions ("and", "of", "for") to validate the choice of "and"
  3. Evaluate LAAV's performance across different sample sizes (4, 8, 16, 32) to confirm low-resource effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the LAAV approach perform on multilingual or cross-lingual few-shot text classification tasks, where the language model is trained on multiple languages?
- Basis in paper: [explicit] The authors mention in the conclusion that they plan to explore the application of LAAV in multilingual LMs, but they do not provide experimental results.
- Why unresolved: The current experiments are limited to monolingual LMs for each language, so the performance of LAAV in multilingual settings remains unexplored.
- What evidence would resolve it: Experimental results comparing LAAV with other verbalizers on multilingual few-shot text classification tasks, using LMs trained on multiple languages.

### Open Question 2
- Question: What is the impact of using different conjunctions or linguistic structures (e.g., "is a", "are examples of") in the LAAV template on the quality of the selected words and classification performance?
- Basis in paper: [explicit] The authors explore different conjunctions in Section 5.2 but only test "of" and "and," finding that "and" performs best. They do not explore other linguistic structures.
- Why unresolved: The paper only investigates a limited set of conjunctions, leaving the potential benefits of other linguistic structures unexplored.
- What evidence would resolve it: Comparative experiments testing various conjunctions and linguistic structures in the LAAV template across different datasets and languages, measuring both word relevance and classification accuracy.

### Open Question 3
- Question: How does the performance of LAAV scale with increasing dataset size beyond the 32-shot setting, and at what point does traditional fine-tuning become consistently superior?
- Basis in paper: [explicit] The authors note in Section 5.1 that when the number of training examples increases to 32, traditional fine-tuning performs on par or better than prompt-based methods, but they do not explore larger dataset sizes.
- Why unresolved: The experiments are limited to a maximum of 32 training examples per class, so the performance trends at larger dataset sizes are unknown.
- What evidence would resolve it: Experiments comparing LAAV and traditional fine-tuning across a wider range of dataset sizes, from few-shot to full-data settings, to identify the crossover point where traditional fine-tuning becomes consistently superior.

## Limitations

- The effectiveness of the conjunction "and" in template construction lacks empirical validation beyond its use in LAAV, with no comparison to other conjunctions or linguistic structures
- The choice of 32 tokens per class (k=32) appears arbitrary without sensitivity analysis or justification for this specific number
- The paper doesn't address potential overfitting to specific template formats or the generalizability of LAAV to different prompt-based learning frameworks

## Confidence

**High Confidence**: The claim that LAAV outperforms traditional fine-tuning and PET in few-shot settings is well-supported by the experimental results across five datasets.

**Medium Confidence**: The claim about LAAV's particular effectiveness in low-resource settings (16 or fewer examples per class) is supported by results, but the evaluation is limited to specific sample sizes without broader sampling.

**Low Confidence**: The claim about LAAV's generalizability across different languages and domains is weakly supported, as results show variation across languages without analysis of why performance differs.

## Next Checks

1. **Conjunction Ablation Study**: Systematically test different conjunctions ("and", "of", "for", "with") in the template construction to empirically validate that "and" is the optimal choice.

2. **Token Number Sensitivity Analysis**: Vary the number of tokens per class (k) from 16 to 64 in increments of 8, measuring the impact on both word relevance and classification performance.

3. **Cross-Lingual Generalization Test**: Apply LAAV to a new language not in the original five (e.g., Japanese or Arabic) and compare performance against baselines to assess domain generalizability.