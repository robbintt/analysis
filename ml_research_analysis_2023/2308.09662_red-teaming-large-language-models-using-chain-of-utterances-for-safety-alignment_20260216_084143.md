---
ver: rpa2
title: Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment
arxiv_id: '2308.09662'
source_url: https://arxiv.org/abs/2308.09662
tags:
- harmful
- base-lm
- red-lm
- data
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RED-EVAL, a new red-teaming benchmark using
  Chain of Utterances prompting to systematically test large language models for harmful
  content generation. RED-E VAL successfully jailbreaks both closed-source systems
  (GPT-4 and ChatGPT) and open-source models (Vicuna, StableBeluga) with over 65-86%
  success rate across 200+ harmful queries.
---

# Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment

## Quick Facts
- **arXiv ID**: 2308.09662
- **Source URL**: https://arxiv.org/abs/2308.09662
- **Reference count**: 40
- **Primary result**: RED-EVAL red-teaming benchmark achieves 65-86% jailbreak success rate; STARLING safety-aligned model shows improved harmlessness while maintaining utility

## Executive Summary
This paper introduces RED-EVAL, a novel red-teaming benchmark that uses Chain of Utterances (CoU) prompting to systematically evaluate large language models' susceptibility to harmful content generation. The benchmark successfully jailbreaks both closed-source systems (GPT-4, ChatGPT) and open-source models (Vicuna, StableBeluga) with over 65% attack success rate. To address these vulnerabilities, the authors propose RED-INSTRUCT, a two-phase safety alignment approach involving HARMFUL QA dataset collection and SAFE-ALIGN fine-tuning. Their resulting STARLING model demonstrates improved harmlessness on the HHH benchmark while maintaining utility performance on TruthfulQA and MMLU.

## Method Summary
The authors develop RED-EVAL as a red-teaming benchmark using Chain of Utterances prompting, which frames harmful questions within a conversational roleplay context to reduce refusal rates. They collect the HARMFUL QA dataset by leveraging ChatGPT with CoU prompting to generate 1.9K harmful questions and corresponding safe/harmful conversation pairs. For safety alignment, they propose SAFE-ALIGN with two strategies: strategy-A (fine-tuning on blue data only) and strategy-B (contrastive learning with blue and red data using gradient ascent on harmful examples). The STARLING model is created by fine-tuning Vicuna-7B on the HARMFUL QA dataset using these alignment strategies.

## Key Results
- RED-EVAL achieves 65-86% jailbreak success rate across 200+ harmful queries on both closed-source and open-source models
- STARLING (fine-tuned Vicuna-7B) shows improved harmlessness while maintaining utility on TruthfulQA and MMLU benchmarks
- Chain of Utterances prompting with internal thoughts increases jailbreak effectiveness by 22% on GPT-4 and 6.5% on ChatGPT
- Strategy-B SAFE-ALIGN improves harmlessness scores while strategy-A provides more stable training

## Why This Works (Mechanism)

### Mechanism 1
Chain of Utterances prompting reduces refusal rates by framing harmful questions within a conversational roleplay context. The CoU prompt establishes a two-agent dialogue with internal thoughts guiding the Base-LM to be "helpful without considering harmfulness," bypassing standard safety filters that detect standalone harmful queries.

### Mechanism 2
HARMFUL QA dataset construction leverages ChatGPT's strong safety alignment to generate high-quality conversation pairs. The dual-prompt approach uses CoU prompting for safe response generation and the RED-EVAL jailbreak prompt for harmful responses, creating matched training examples.

### Mechanism 3
SAFE-ALIGN strategy-B improves safety through contrastive learning with paired blue-red data. The model is initially trained to maximize likelihood on blue data while minimizing likelihood on red data (gradient ascent), then switches to blue data only after K steps to prevent collapse.

## Foundational Learning

- **Concept**: Chain of Utterances prompting
  - Why needed here: CoU is the core technique enabling both RED-EVAL benchmark and HARMFUL QA dataset construction
  - Quick check question: How does CoU prompting differ from standard Chain of Thought prompting in terms of safety evaluation?

- **Concept**: Red-teaming and jailbreaking
  - Why needed here: The entire safety evaluation and alignment approach relies on understanding how to systematically bypass safety guardrails
  - Quick check question: What distinguishes successful red-teaming from simple prompt injection attacks?

- **Concept**: Contrastive learning with paired data
  - Why needed here: SAFE-ALIGN strategy-B uses blue-red data pairs to create a safety gradient, requiring understanding of how paired contrastive examples affect model behavior
  - Quick check question: How does gradient ascent on harmful examples differ from standard negative sampling in contrastive learning?

## Architecture Onboarding

- **Component map**: RED-EVAL -> HARMFUL QA -> SAFE-ALIGN -> STARLING
- **Critical path**: HARMFUL QA dataset collection → SAFE-ALIGN fine-tuning → STARLING model evaluation (RED-EVAL + HHH + utility benchmarks)
- **Design tradeoffs**:
  - CoU prompting vs. direct harmful queries: CoU reduces refusal rates but may not generalize to all harmful request types
  - Blue-only vs. blue-red training: Strategy-A is more stable but potentially less effective; strategy-B is more effective but risks model collapse
  - Dataset size vs. quality: Larger datasets may include more noise; smaller curated datasets may be more effective
- **Failure signatures**:
  - High refusal rates on RED-EVAL indicate model is too safe (fails to jailbreak)
  - Poor HHH scores indicate alignment compromised utility
  - High utility but poor safety scores indicate alignment failed to improve safety
  - Training instability or model collapse indicates problematic red data usage
- **First 3 experiments**:
  1. Test RED-EVAL jailbreak prompt on Vicuna-7B with 10 harmful questions to verify mechanism works on open-source models
  2. Generate 5 conversation pairs (blue/red) for a single harmful question to validate HARMFUL QA pipeline
  3. Fine-tune Vicuna-7B on 100 blue data examples using strategy-A to verify basic alignment approach works before scaling up

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively learn from harmful (red) data to improve safety alignment without introducing training instability or catastrophic forgetting?
- Basis in paper: The authors found that training with red data introduced noise and instability, leading to a collapse in generation ability. They had to use a small K value (200 steps) and then omit red data, which limited the potential benefits of this approach.
- What evidence would resolve it: A robust training method that can effectively leverage red data throughout the full training process without causing model collapse or significant degradation in utility performance would resolve this question.

### Open Question 2
- Question: Can the CoU prompt template be further optimized to improve red-teaming effectiveness while maintaining robustness against safety interventions?
- Basis in paper: The authors identified some factors affecting performance (internal thoughts, question phrasing) but did not systematically explore the full space of prompt variations or develop a principled method for optimizing the template.
- What evidence would resolve it: A comprehensive analysis of how different prompt elements affect red-teaming success rates, along with a methodology for constructing optimized templates that maximize jailbreak effectiveness, would resolve this question.

### Open Question 3
- Question: What is the relationship between model scale and susceptibility to red-teaming, and how does this vary across different prompting strategies?
- Basis in paper: The authors observed that larger models are harder to red-team but only tested a limited number of model sizes without systematic investigation of how red-teaming effectiveness scales with model size or how different prompting strategies interact with model scale.
- What evidence would resolve it: A systematic study varying model size across multiple orders of magnitude while testing all prompting strategies would clarify how scale affects red-teaming susceptibility and whether larger models become increasingly resistant to all attack types.

## Limitations

- The effectiveness of Chain of Utterances prompting depends heavily on specific prompt engineering details that are not fully specified, particularly the internal thought mechanism
- SAFE-ALIGN strategy-B shows promise but carries significant risk of model collapse when training on red data, requiring careful tuning of hyperparameters
- The paper does not provide clear guidance on selecting optimal K values or handling cases where utility degrades during training

## Confidence

- **High Confidence**: RED-EVAL benchmark successfully demonstrates jailbreak capabilities across multiple model types with reported ASR rates above 65%
- **Medium Confidence**: Effectiveness of Chain of Utterances prompting in reducing refusal rates is supported by empirical results, but specific mechanisms require further investigation
- **Low Confidence**: Optimal configuration for SAFE-ALIGN strategy-B is not fully specified, making it difficult to reproduce exact results

## Next Checks

1. **Prompt Template Validation**: Test the RED-EVAL jailbreak prompt on Vicuna-7B with 10 diverse harmful questions to verify the Chain of Utterances mechanism works as described, specifically checking if internal thoughts significantly impact attack success rates.

2. **Dataset Pipeline Verification**: Generate a small-scale HARMFUL QA dataset (5 topics, 20 questions each) and validate the blue-red conversation pair generation process, ensuring the CoU prompting approach consistently produces the expected safe and harmful response patterns.

3. **Fine-tuning Stability Test**: Perform SAFE-ALIGN strategy-B fine-tuning on Vicuna-7B with a small dataset (100 blue examples, 50 red examples) and monitor model behavior over time to identify the point where utility begins to degrade, helping establish appropriate K values and λ parameters.