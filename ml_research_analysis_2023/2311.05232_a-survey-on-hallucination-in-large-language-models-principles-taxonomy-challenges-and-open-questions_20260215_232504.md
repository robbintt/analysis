---
ver: rpa2
title: 'A Survey on Hallucination in Large Language Models: Principles, Taxonomy,
  Challenges, and Open Questions'
arxiv_id: '2311.05232'
source_url: https://arxiv.org/abs/2311.05232
tags:
- llms
- language
- knowledge
- arxiv
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively examines hallucinations in large language
  models (LLMs), proposing a taxonomy categorizing hallucinations into factuality
  (factual inconsistency and fabrication) and faithfulness (instruction, context,
  and logical inconsistency). We analyze causes across data, training, and inference
  stages, review detection methods including fact-based, classifier-based, QA-based,
  and uncertainty estimation approaches, and summarize benchmarks for evaluation.
---

# A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

## Quick Facts
- **arXiv ID**: 2311.05232
- **Source URL**: https://arxiv.org/abs/2311.05232
- **Reference count**: 40
- **Key outcome**: Comprehensive survey examining LLM hallucinations with taxonomy, causes, detection methods, benchmarks, and mitigation strategies.

## Executive Summary
This survey provides a thorough examination of hallucinations in large language models (LLMs), introducing a novel taxonomy that categorizes hallucinations into factuality (factual inconsistency and fabrication) and faithfulness (instruction, context, and logical inconsistency). The paper analyzes causes across data, training, and inference stages, reviews detection methods, and summarizes evaluation benchmarks. It also discusses challenges in long-form generation, retrieval-augmented generation, and vision-language models, while highlighting open questions about self-correction, knowledge boundaries, and balancing creativity with factuality.

## Method Summary
The survey is a comprehensive literature review that synthesizes academic papers and resources on LLM hallucinations. It organizes information into categories based on a novel taxonomy distinguishing factuality from faithfulness hallucinations. The paper analyzes causes at data, training, and inference levels, reviews detection methods, and discusses mitigation strategies. The approach involves collecting relevant literature, categorizing findings according to the taxonomy, and identifying trends, challenges, and open questions in the field.

## Key Results
- Introduces a taxonomy categorizing LLM hallucinations into factuality (factual inconsistency, fabrication) and faithfulness (instruction, context, logical inconsistency)
- Analyzes causes across data, training, and inference stages, including data-related issues (misinformation, biases, knowledge boundaries) and training-related issues (architecture flaws, alignment misalignment)
- Reviews detection methods including fact-based, classifier-based, QA-based, and uncertainty estimation approaches
- Discusses challenges in long-form generation, retrieval-augmented generation, and vision-language models
- Identifies open questions about self-correction mechanisms, knowledge boundaries, and balancing creativity with factuality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The taxonomy distinguishes between factuality and faithfulness hallucinations to better align with LLM usage patterns.
- Mechanism: Factuality hallucination captures errors about real-world facts (factual inconsistency, fabrication), while faithfulness hallucination captures deviations from user instructions or context (instruction, context, logical inconsistency).
- Core assumption: LLMs are general-purpose and interact more with users, making instruction and context adherence as important as factual accuracy.
- Evidence anchors:
  - [abstract] "We categorize hallucination into two main groups: factuality hallucination and faithfulness hallucination."
  - [section] "LLMs are inherently trained to align with user instructions... we categorize three subtypes of faithfulness hallucinations."
  - [corpus] Weak. Only 2 neighbor papers mention taxonomy; neither defines the split as factuality vs. faithfulness.
- Break condition: If LLM tasks are narrowly defined, this taxonomy may overcomplicate evaluation compared to simpler task-specific hallucination categories.

### Mechanism 2
- Claim: Data-related hallucinations stem from flawed data sources (misinformation, biases, knowledge boundaries) and inferior data utilization (knowledge shortcuts, recall failures).
- Mechanism: Pre-training on noisy or biased data introduces misinformation and biases; flawed utilization leads to over-reliance on spurious correlations or failure to recall knowledge in complex scenarios.
- Core assumption: Pre-training data is the foundation for LLM knowledge, so flaws in data quality and utilization directly cause hallucinations.
- Evidence anchors:
  - [abstract] "We analyze causes across data, training, and inference stages... mitigating strategies for data-related issues."
  - [section] "Data-related hallucinations generally emerge as a byproduct of biases, misinformation, and knowledge gaps."
  - [corpus] Weak. Only 2 neighbor papers discuss data causes; neither detail the two-category breakdown (source vs. utilization).
- Break condition: If future LLMs rely more on real-time retrieval than parametric knowledge, data-related causes may become less dominant.

### Mechanism 3
- Claim: Retrieval-augmented generation (RAG) can both mitigate and introduce hallucinations through error accumulation and citation inaccuracies.
- Mechanism: RAG grounds LLMs in external knowledge, reducing parametric knowledge gaps, but irrelevant or inaccurate retrievals can propagate into generated outputs.
- Core assumption: RAG improves factuality by providing up-to-date knowledge, but retrieval quality and model reasoning over retrieved content determine hallucination risk.
- Evidence anchors:
  - [abstract] "We discuss challenges in long-form generation, retrieval-augmented generation, and vision-language models."
  - [section] "Retrieval Augmented Generation (RAG) has emerged as a promising strategy to mitigate hallucinations... However, despite its advantages, RAG also suffers from hallucinations."
  - [corpus] Weak. Only 1 neighbor paper mentions RAG; it focuses on attribution techniques, not hallucination trade-offs.
- Break condition: If retrieval systems become highly reliable and LLMs improve reasoning over retrieved content, RAG-induced hallucinations may diminish.

## Foundational Learning

- Concept: Taxonomy of hallucinations (factuality vs. faithfulness)
  - Why needed here: Provides a structured framework for categorizing and analyzing LLM hallucinations, aligning with their general-purpose nature.
  - Quick check question: What are the two main types of LLM hallucinations, and how do they differ in terms of root cause?

- Concept: Data-induced hallucinations (source vs. utilization)
  - Why needed here: Explains how pre-training data quality and model's use of that data contribute to hallucinations, guiding mitigation strategies.
  - Quick check question: What are the two subcategories of data-related hallucinations, and what causes each?

- Concept: Retrieval-augmented generation (RAG) and its trade-offs
  - Why needed here: Describes how RAG can reduce parametric knowledge gaps but introduces new risks through retrieval errors.
  - Quick check question: How can RAG both help and hurt LLM factuality?

## Architecture Onboarding

- Component map: Survey paper → Taxonomy → Causes (Data/Training/Inference) → Detection methods → Benchmarks → Mitigation strategies → Challenges/Open questions
- Critical path: Define taxonomy → Analyze causes → Propose detection methods → Evaluate with benchmarks → Develop mitigations → Identify challenges
- Design tradeoffs: Granular taxonomy improves specificity but may complicate evaluation; comprehensive cause analysis aids mitigation but increases survey length
- Failure signatures: Weak corpus evidence for key claims; over-reliance on abstract without section validation; unclear distinctions between similar hallucination types
- First 3 experiments:
  1. Validate the factuality vs. faithfulness taxonomy by coding hallucination examples from LLM outputs.
  2. Test data-related hallucination claims by analyzing pre-training corpora for misinformation and biases.
  3. Evaluate RAG hallucination mitigation by comparing outputs with and without retrieval augmentation on factual accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can self-correction mechanisms effectively reduce reasoning hallucinations in LLMs?
- Basis in paper: Explicit - "Can Self-Correct Mechanisms Help in Mitigating Reasoning Hallucinations?"
- Why unresolved: Some studies show promise in self-correction for reasoning, but others question its effectiveness, noting that LLMs still struggle to self-correct their reasoning chains.
- What evidence would resolve it: Empirical studies comparing self-correction mechanisms against external feedback approaches in diverse reasoning tasks, measuring the accuracy and consistency of corrected outputs.

### Open Question 2
- Question: Can we accurately capture LLM knowledge boundaries to prevent hallucinations?
- Basis in paper: Explicit - "Can We Accurately Capture LLM Knowledge Boundaries?"
- Why unresolved: While some research suggests LLMs encode knowledge boundaries in their activations, others argue that current lie-detector methods are unreliable and that LLMs exhibit overconfidence when faced with unanswerable questions.
- What evidence would resolve it: Development of reliable methods to probe LLM internal beliefs about truthfulness and unanswerability, validated through extensive testing across diverse knowledge domains and question types.

### Open Question 3
- Question: How can we strike a balance between creativity and factuality in LLMs?
- Basis in paper: Explicit - "How Can We Strike a Balance between Creativity and Factuality?"
- Why unresolved: Current research focuses heavily on reducing hallucinations, often overlooking the importance of creative capabilities. Balancing these aspects requires a nuanced understanding of the nature of creativity and its implications for human interaction and knowledge exchange.
- What evidence would resolve it: Development and evaluation of methods that can dynamically adjust the trade-off between creativity and factuality based on the specific task and user preferences, validated through user studies and objective metrics.

## Limitations
- Weak empirical grounding for key taxonomy claims, with limited cross-validation from broader literature
- Lack of explicit evaluation metrics or datasets mentioned in the survey
- Absence of discussion on how hallucinations vary across model scales or domains

## Confidence
- Taxonomy claims: Medium (weak corpus support, intuitive but not widely validated)
- Data-related hallucination framework: Medium (only 2 neighbor papers touch on data causes without same breakdown)
- RAG hallucination trade-offs: Medium (limited corpus evidence, no experimental validation in survey)

## Next Checks
1. Code hallucination examples from recent LLM outputs using the proposed factuality vs. faithfulness taxonomy to assess practical utility and clarity.
2. Analyze pre-training corpora for documented cases of misinformation, biases, and knowledge gaps to test the data-related hallucination framework.
3. Run controlled experiments comparing LLM outputs with and without RAG augmentation on factual accuracy to quantify hallucination trade-offs.