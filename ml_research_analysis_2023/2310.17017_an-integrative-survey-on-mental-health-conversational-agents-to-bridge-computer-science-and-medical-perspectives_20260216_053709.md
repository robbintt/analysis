---
ver: rpa2
title: An Integrative Survey on Mental Health Conversational Agents to Bridge Computer
  Science and Medical Perspectives
arxiv_id: '2310.17017'
source_url: https://arxiv.org/abs/2310.17017
tags:
- health
- mental
- general
- papers
- patients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic review of mental health conversational
  agents, reviewing 136 key papers published in both computer science and medical
  domains over the past five years. The review reveals a disparity between the two
  domains, with computer science papers focusing on LLM techniques and evaluating
  response quality using automated metrics, while medical papers use rule-based conversational
  agents and outcome metrics to measure health outcomes of participants.
---

# An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives

## Quick Facts
- arXiv ID: 2310.17017
- Source URL: https://arxiv.org/abs/2310.17017
- Reference count: 40
- Primary result: Reviews 136 papers to reveal disciplinary disparities in mental health CA research and identify key issues including transparency and cultural/language heterogeneity.

## Executive Summary
This systematic review bridges computer science and medical perspectives on mental health conversational agents by analyzing 136 key papers published over the past five years. The review reveals significant disparities between domains: CS papers focus on LLM techniques and automated response quality metrics, while medical papers use rule-based agents and clinical outcome measures. Common issues identified include transparency deficits (no model releases), language/cultural homogeneity (71% English-speaking participants), and inconsistent ethical oversight. The study aims to facilitate reciprocal learning between disciplines to advance the field.

## Method Summary
The study employs PRISMA framework for systematic review, screening 534 papers from multiple databases (ACL Anthology, AAAI, IEEE, ACM, PubMed) to identify 136 relevant papers. The screening process includes title, abstract, and full-text evaluation with inclusion criteria focusing on CA-focused, mental health-related research that contributes to CA improvement. Features are extracted from 102 model/experiment papers and 20 survey papers, analyzing distribution across domains for language, mental health category, target demographic, model technique, and evaluation methods.

## Key Results
- Computer science papers primarily use LLM techniques and automated metrics (BLEU/ROUGE), while medical papers employ rule-based agents and clinical outcomes (PHQ-9/GAD-7)
- 71% of studies use English-speaking participants, with minimal representation of other major languages
- 0% of reviewed CS papers released their models or APIs, and 21/24 CS papers lacked ethics statements
- Significant disparities exist in research focus, technology adoption, and evaluation purposes between domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Systematic review methodology bridges disciplinary knowledge gaps by consolidating diverse evaluation approaches
- Mechanism: PRISMA framework systematically aggregates and contrasts CS and medical papers, revealing methodological disparities
- Core assumption: Disciplinary silos are due to fragmented publication venues and metrics, not lack of interest
- Evidence anchors: Previous surveys primarily consider papers from single domains, papers vary in research focus/technology/evaluation purposes, moderate topical overlap (FMR=0.383)
- Break condition: If future papers are published in truly interdisciplinary venues with unified metrics, value of cross-domain surveys diminishes

### Mechanism 2
- Claim: Language and cultural heterogeneity limits generalizability of mental health CAs
- Mechanism: Review finds 71% of studies use English-speaking participants with minimal other language representation
- Core assumption: Mental health symptoms and help-seeking behaviors are culturally shaped, requiring localized CA adaptations
- Evidence anchors: English dominates with 71% of studies, over 75% cater to English-speaking participants for depression/anxiety, only 6 non-English papers identified
- Break condition: If multilingual LLMs are trained on diverse datasets with cultural competency benchmarks, limitation can be mitigated

### Mechanism 3
- Claim: Transparency deficits in model releases and evaluation hinder reproducibility and clinical trust
- Mechanism: CS papers rarely release models/APIs, medical papers lack standardized outcome reporting
- Core assumption: Reproducibility and ethical oversight are prerequisites for clinical adoption of AI tools
- Evidence anchors: 0% of ACL papers released models/APIs, 21/24 CS papers lack ethics statements, no mention of model release practices in related papers
- Break condition: If journals mandate open models and ethics statements with standardized reporting guidelines, transparency will improve

## Foundational Learning

- Concept: PRISMA systematic review framework
  - Why needed here: Ensures unbiased, reproducible aggregation of heterogeneous research across CS and medicine
  - Quick check question: What are the three key phases of PRISMA screening (title, abstract, full-text)?

- Concept: Evaluation metric alignment
  - Why needed here: Different domains use automated vs. clinical outcomes; aligning them enables fair cross-domain comparison
  - Quick check question: Which metric (BLEU/ROUGE vs. PHQ-9/GAD-7) corresponds to CS vs. medical evaluation?

- Concept: Cultural competency in AI
  - Why needed here: Mental health expressions and help-seeking behaviors vary culturally; models must adapt accordingly
  - Quick check question: What percentage of reviewed papers used non-English participants?

## Architecture Onboarding

- Component map: Keyword search -> Database aggregation (PubMed, ACL, IEEE, ACM, AAAI) -> Title screening -> Abstract screening -> Full-text screening -> Feature extraction (24 features) -> Domain classification -> Disparity analysis -> Recommendation synthesis
- Critical path: 1. Keyword search → 534 papers 2. Title screening → 302 papers 3. Abstract screening → 157 papers 4. Full-text screening → 136 papers 5. Feature extraction → 102 model/experiment + 20 survey papers 6. Analysis → disparity identification
- Design tradeoffs: Inclusive vs. precise screening (favoring recall initially), manual annotation vs. automated extraction (accuracy prioritized), cross-domain vs. single-domain focus (cross-domain chosen for gap analysis)
- Failure signatures: Low inter-annotator agreement in feature extraction, skewed final paper distribution, missing key evaluation dimensions
- First 3 experiments: 1. Run screening on 10% random sample and measure agreement 2. Test feature extraction on 5 papers from each domain for completeness 3. Validate domain classification by cross-checking author affiliations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific technical barriers preventing adoption of large language models in medical mental health conversational agents?
- Basis in paper: LLMs are yet to be utilized in medical mental health CAs, with concerns about stochastic text generation and black box architecture
- Why unresolved: Paper identifies gap but doesn't provide detailed technical analysis of specific barriers or potential solutions
- What evidence would resolve it: Comparative studies of LLM performance vs rule-based systems in medical contexts, or case studies of successful LLM implementation addressing these concerns

### Open Question 2
- Question: How can standardized benchmark datasets and evaluation procedures be developed for mental health conversational agents across different languages and cultural contexts?
- Basis in paper: Identifies transparency and language/cultural heterogeneity as key issues, notes 0% model/API release rate and English dominance
- Why unresolved: Points out issues but doesn't propose specific frameworks for standardization or address technical challenges of creating culturally appropriate benchmarks
- What evidence would resolve it: Development and validation of multi-lingual benchmark datasets, or studies demonstrating effective cross-cultural evaluation methodologies

### Open Question 3
- Question: What are the long-term clinical outcomes and therapeutic alliance measures for mental health conversational agents compared to traditional therapy?
- Basis in paper: Notes medical papers focus on user data and human outcome evaluation while CS papers focus on response quality and lack clinically validated mechanisms
- Why unresolved: Identifies evaluation gap but doesn't provide data on long-term effectiveness or compare different therapeutic approaches
- What evidence would resolve it: Longitudinal studies comparing mental health outcomes between CA users and traditional therapy patients, or standardized therapeutic alliance measures for CAs

## Limitations

- Review's focus on English-language papers and participants creates significant gaps in understanding cross-cultural performance
- Complete absence of model/API releases in CS papers (0% release rate) limits reproducibility and clinical validation
- 78% of medical papers published in journals without impact factors suggests potential quality variation and limited peer review rigor

## Confidence

- High confidence in disparity identification between CS and medical domains (well-supported by systematic review methodology and clear quantitative differences)
- Medium confidence in cultural competency limitations (based on strong language statistics but limited direct cultural adaptation studies)
- Medium confidence in transparency assessment (supported by explicit findings but limited by undisclosed model details)
- Low confidence in generalizability of findings to non-English populations (severe underrepresentation in the corpus)

## Next Checks

1. Conduct follow-up review specifically targeting non-English mental health CA papers to quantify cultural adaptation gaps
2. Implement inter-rater reliability testing on a sample of 10 papers for feature extraction consistency
3. Survey authors of included papers to verify reported model releases and ethical oversight practices, addressing potential reporting biases