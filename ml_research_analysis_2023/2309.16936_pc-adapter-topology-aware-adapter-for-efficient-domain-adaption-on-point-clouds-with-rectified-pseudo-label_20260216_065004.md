---
ver: rpa2
title: 'PC-Adapter: Topology-Aware Adapter for Efficient Domain Adaption on Point
  Clouds with Rectified Pseudo-label'
arxiv_id: '2309.16936'
source_url: https://arxiv.org/abs/2309.16936
tags:
- domain
- point
- source
- target
- adaptation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of unsupervised domain adaptation
  for point cloud classification and segmentation, where the goal is to transfer knowledge
  from a labeled source domain to an unlabeled target domain with different data distributions.
  The key insight is that global geometry information from the source domain is more
  useful than local structures for domain adaptation on point clouds, and that pseudo-labels
  are often biased towards the source label distribution.
---

# PC-Adapter: Topology-Aware Adapter for Efficient Domain Adaption on Point Clouds with Rectified Pseudo-label

## Quick Facts
- arXiv ID: 2309.16936
- Source URL: https://arxiv.org/abs/2309.16936
- Reference count: 29
- Primary result: PC-Adapter outperforms state-of-the-art methods in point cloud domain adaptation, achieving 5.3% average improvement over baselines on PointDA-10

## Executive Summary
PC-Adapter addresses unsupervised domain adaptation for point cloud classification and segmentation by leveraging the observation that global geometry from the source domain is more valuable than local structures for adaptation. The framework uses two specialized adapter modules: a shape-aware adapter that preserves global shape knowledge via attention mechanisms, and a locality-aware adapter that adapts to target-specific local structures using graph convolutions. Additionally, PC-Adapter introduces a novel pseudo-label correction strategy that adjusts confidence scores based on class-wise confidence distributions to mitigate bias toward the source label distribution.

## Method Summary
PC-Adapter is an adapter-based domain adaptation framework that efficiently transfers knowledge from labeled source point clouds to unlabeled target point clouds. The method uses two adapters: a shape-aware adapter that learns global geometry from the source domain using attention over farthest point sampling, and a locality-aware adapter that adapts to local structures of the target domain using graph convolutions. The framework also introduces a beta distribution-guided pseudo-label correction strategy that adjusts confidence scores based on class-wise confidence distributions to prevent pseudo-labels from being biased toward the source label distribution. Training involves different paths for source and target data, with weak updates applied to components processing source data to preserve source geometry information.

## Key Results
- PC-Adapter achieves an average improvement of 5.3% over baselines on PointDA-10 benchmark
- Sets new state-of-the-art results on GraspNetPC-10 when combined with existing self-supervised tasks
- Demonstrates strong performance under data-scarce conditions and has lower time complexity compared to prior methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Global geometry from the source domain is more valuable than local structures for domain adaptation on point clouds under distributional shifts.
- **Mechanism**: The shape-aware adapter (Ψg) preserves global shape knowledge using self-attention over farthest point sampling (FPS), enabling the model to recognize objects despite domain-specific local distortions like occlusion or sensor noise.
- **Core assumption**: Local structures vary significantly across domains due to noise, occlusion, or sensor differences, while global geometry remains consistent.
- **Evidence anchors**:
  - [abstract]: "the importance of the global geometry of source data"
  - [section 3.2]: "We first compare the value of knowledge in terms of global geometry and local structure... results with eliminating some points (lightening the knowledge by eliminating local properties) show superior, at least comparable, object classification accuracy"
- **Break condition**: If the domain shift is primarily due to scale changes or extreme deformations, global geometry may not transfer well.

### Mechanism 2
- **Claim**: Pseudo-labels are biased toward the source label distribution, leading to poor adaptation.
- **Mechanism**: The beta distribution-guided pseudo-label correction adjusts confidence scores based on class-wise confidence distributions, preventing the classifier from favoring majority source classes.
- **Core assumption**: Classifier confidence varies by class and correlates with source label distribution, causing minority classes to be underrepresented in pseudo-labels.
- **Evidence anchors**:
  - [abstract]: "trends of target pseudo-labels biased to the source label distribution"
  - [section 3.3]: "pseudo-labels indeed follow the (ground-truth) label distribution of the source training data... pseudo-labels are rarely selected for the minority classes at the source domain"
- **Break condition**: If the source and target label distributions are similar, this correction may be unnecessary and could reduce accuracy.

### Mechanism 3
- **Claim**: Combining a shape-aware adapter with a locality-aware adapter provides efficient and effective domain adaptation.
- **Mechanism**: The shape-aware adapter transfers global geometry with weakly updated parameters, while the locality-aware adapter learns target-specific local structures with graph convolutions, providing complementary adaptation paths.
- **Core assumption**: Global and local information require different adaptation strategies - global knowledge should be preserved, while local knowledge should be actively adapted.
- **Evidence anchors**:
  - [abstract]: "preserves the global shape information of the source domain using an attention-based adapter, while learning the local characteristics of the target domain via another adapter equipped with graph convolution"
  - [section 4.1]: "To preserve the source geometry information during the adaptation process, the parameters are weakly updated when training target point clouds"
- **Break condition**: If computational resources are extremely limited, maintaining two adapters might be too expensive.

## Foundational Learning

- **Concept**: Domain Adaptation and Distribution Shift
  - Why needed here: The paper addresses unsupervised domain adaptation where labeled source data and unlabeled target data have different distributions due to factors like sensor angles and occlusion.
  - Quick check question: What are the key differences between supervised learning and unsupervised domain adaptation?

- **Concept**: Point Cloud Processing and Feature Extraction
  - Why needed here: The paper works with unordered 3D point sets, requiring specialized architectures like PointNet, DGCNN, and Point Transformer for feature extraction.
  - Quick check question: How do point cloud architectures differ from traditional image-based convolutional networks?

- **Concept**: Adapter Modules and Parameter-Efficient Fine-Tuning
  - Why needed here: The paper uses adapter modules to efficiently adapt pre-trained models to new domains without full fine-tuning.
  - Quick check question: What are the advantages of using adapter modules compared to full fine-tuning in transfer learning?

## Architecture Onboarding

- **Component map**: Φ (Feature Encoder) → [Ψg (Shape-aware Adapter) + Ψl (Locality-aware Adapter)] → f (Classifier)
- **Critical path**: Φ → [Ψg → f] for source data, Φ → [Ψg + Ψl → f] for target data, with beta distribution-guided pseudo-label generation for target training
- **Design tradeoffs**: Computational efficiency vs. adaptation performance - using adapters instead of full fine-tuning reduces parameters but may limit adaptation capacity
- **Failure signatures**: 
  - Poor performance on minority classes indicates insufficient pseudo-label correction
  - Degradation when target data is scarce suggests locality-aware adapter isn't learning effectively
  - Overfitting to source domain shows shape-aware adapter isn't being updated appropriately
- **First 3 experiments**:
  1. Test classification accuracy with only shape-aware adapter vs. only locality-aware adapter on a simple domain shift (e.g., ModelNet→ShapeNet)
  2. Evaluate the impact of relative positional encoding by comparing with standard positional encoding in the shape-aware adapter
  3. Measure the effect of beta distribution-guided pseudo-label correction by comparing with maximum confidence pseudo-labeling on a balanced dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of self-supervised learning tasks affect the performance of PC-Adapter when combined with existing self-supervised methods?
- Basis in paper: [explicit] The paper mentions that PC-Adapter achieves state-of-the-art performances on GraspNetPC-10 when combined with existing self-supervised tasks, but does not explore the impact of different self-supervised tasks on the performance.
- Why unresolved: The paper focuses on the adapter-based approach and pseudo-label correction, but does not provide a detailed analysis of how different self-supervised learning tasks might impact the overall performance.
- What evidence would resolve it: Comparative experiments evaluating PC-Adapter's performance with various self-supervised tasks would provide insights into the most effective combinations.

### Open Question 2
- Question: What is the impact of different point sampling strategies on the performance of PC-Adapter?
- Basis in paper: [inferred] The paper uses Farthest Point Sampling (FPS) for sampling points, but does not explore the impact of other sampling strategies on the model's performance.
- Why unresolved: While the paper demonstrates the effectiveness of FPS, it does not provide a comprehensive comparison with other sampling methods, such as random sampling or inverse density sampling.
- What evidence would resolve it: Experiments comparing PC-Adapter's performance using different point sampling strategies would help determine the optimal sampling approach for various domain adaptation scenarios.

### Open Question 3
- Question: How does PC-Adapter perform on more complex point cloud datasets with a larger number of classes or higher-dimensional features?
- Basis in paper: [explicit] The paper evaluates PC-Adapter on datasets with 10 classes and 3D coordinates, but does not explore its performance on more complex datasets.
- Why unresolved: The current experiments focus on relatively simple datasets, and the scalability of PC-Adapter to more complex scenarios remains unexplored.
- What evidence would resolve it: Testing PC-Adapter on datasets with a larger number of classes, higher-dimensional features, or more complex shapes would provide insights into its scalability and robustness.

### Open Question 4
- Question: What is the impact of different graph convolution operations in the locality-aware adapter on the performance of PC-Adapter?
- Basis in paper: [explicit] The paper uses a simple graph convolution operation in the locality-aware adapter, but does not explore the impact of different graph convolution variants.
- Why unresolved: While the paper demonstrates the effectiveness of the proposed adapter-based approach, it does not provide a detailed analysis of how different graph convolution operations might affect the performance.
- What evidence would resolve it: Comparative experiments evaluating PC-Adapter's performance using different graph convolution operations would provide insights into the most effective graph convolution variants for domain adaptation on point clouds.

## Limitations

- Lack of specific details on the relative positional encoding formula and exact Combine operation used to merge adapter outputs
- Validation primarily focuses on classification accuracy without extensive ablation studies on individual adapter components
- Beta distribution approximation for confidence adjustment lacks rigorous theoretical justification

## Confidence

- **High confidence**: The core architecture design with shape-aware and locality-aware adapters, and the overall experimental results showing PC-Adapter outperforms baselines on standard datasets
- **Medium confidence**: The effectiveness of the pseudo-label correction strategy, as it relies on the assumption that class-wise confidence distributions follow beta distributions and that this correction meaningfully addresses source label bias
- **Low confidence**: The theoretical analysis of why global geometry is more transferable than local structures, which is primarily supported by empirical observations rather than rigorous mathematical proof

## Next Checks

1. Conduct ablation studies comparing the performance of shape-aware adapter alone vs. locality-aware adapter alone across different domain shift scenarios to validate the claim about global geometry superiority
2. Test the beta distribution-guided pseudo-label correction on datasets with known label distribution differences to verify it effectively mitigates source label bias
3. Evaluate PC-Adapter's performance on extremely challenging domain shifts (e.g., significant scale changes or severe occlusions) to identify the break conditions for the global geometry assumption