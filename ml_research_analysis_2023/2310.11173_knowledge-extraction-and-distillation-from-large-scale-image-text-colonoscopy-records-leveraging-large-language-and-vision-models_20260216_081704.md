---
ver: rpa2
title: Knowledge Extraction and Distillation from Large-Scale Image-Text Colonoscopy
  Records Leveraging Large Language and Vision Models
arxiv_id: '2310.11173'
source_url: https://arxiv.org/abs/2310.11173
tags:
- colonoscopy
- polyp
- segmentation
- performance
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EndoKED, a framework leveraging large language
  and vision models to automatically annotate raw colonoscopy records, transforming
  them into image datasets with pixel-level annotations. The method uses LLMs to extract
  polyp presence from free-text reports and propagates this knowledge to image-level
  labels via multiple instance learning.
---

# Knowledge Extraction and Distillation from Large-Scale Image-Text Colonoscopy Records Leveraging Large Language and Vision Models

## Quick Facts
- **arXiv ID**: 2310.11173
- **Source URL**: https://arxiv.org/abs/2310.11173
- **Reference count**: 0
- **Primary result**: Framework automatically annotates colonoscopy records using LLMs and LVM, achieving expert-level performance in polyp detection and segmentation

## Executive Summary
This paper introduces EndoKED, a framework that leverages large language models (LLMs) and large vision models (LVMs) to automatically extract polyp presence labels from free-text colonoscopy reports and propagate this knowledge to image-level and pixel-level annotations. The method achieves strong performance on public polyp segmentation datasets and optical biopsy tasks, demonstrating expert-level accuracy without requiring manual annotation of the training data.

## Method Summary
EndoKED uses LLMs (ChatGPT, Claude, ERNIE) to extract polyp presence labels from free-text colonoscopy reports with reported 100% accuracy. Multiple instance learning (MIL) then propagates these report-level labels to individual images within each report. Class activation maps (CAM) generate region-level bounding boxes, which serve as prompts for the Segment Anything Model (SAM) to produce pixel-level polyp segmentation masks. The framework is trained on approximately 1 million images from 14,177 reports across four hospitals and achieves competitive performance on five public polyp segmentation datasets and optical biopsy tasks.

## Key Results
- EndoKED achieves Dice similarity coefficients (DSC) of 0.908-0.920 on five public polyp segmentation datasets
- Optical biopsy performance reaches AUCs of 0.889-0.911, matching expert-level performance
- Report-level AUCs of 0.983-0.975 and image-level AUCs of 0.957-0.945 demonstrate effective label propagation through MIL
- The framework enables data-efficient learning from large-scale unannotated colonoscopy records

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Large language models (LLMs) can accurately extract polyp presence labels from free-text colonoscopy reports without further adaptation.
- **Mechanism**: Pre-trained LLMs leverage their natural language understanding to comprehend clinical text and answer binary questions about lesion presence. The models map report content to structured labels through question answering.
- **Core assumption**: Clinical reports contain sufficient semantic cues for LLMs to determine polyp presence without specialized medical fine-tuning.
- **Evidence anchors**:
  - [abstract] "LLMs comprehends the raw text of the colonoscopy reports, while the LVM delineates corresponding objects from the images"
  - [section] "Performance of LLMs in colonoscopy report abstraction... All the LLMs achieved impressive high accuracy: ChatGPT Accuracy=100%; Claude Accuracy=100%; ERNIE Accuracy=100% in three independent hospitals"
- **Break condition**: Reports lack clear semantic indicators of polyp presence, or contain ambiguous terminology that confuses LLM reasoning.

### Mechanism 2
- **Claim**: Multiple instance learning (MIL) effectively propagates report-level polyp labels to image-level labels.
- **Mechanism**: MIL treats all images from one report as a bag, with the bag-level label (polyp presence) known. A teacher network trained on bag labels generates pseudo-labels for individual images, which train a student network for image-level classification.
- **Core assumption**: At least one image in a report containing polyps will be correctly identified by the teacher network, enabling knowledge transfer to the student.
- **Evidence anchors**:
  - [abstract] "The multiple instance learning (MIL) technique is used to propagate the report-level label to the image level"
  - [section] "We developed a multiple instance learning method to propagate the knowledge of polyp occurrence from the report level to image level... Report-level AUCs were 0.983, 0.970 and 0.975... image-level AUCs were 0.957, 0.941 and 0.945"
- **Break condition**: Teacher network fails to identify any polyp-containing images, preventing student network learning.

### Mechanism 3
- **Claim**: Weakly supervised segmentation using SAM can generate accurate pixel-level polyp masks from image-level labels.
- **Mechanism**: Class activation maps (CAM) from the image classifier identify regions of interest, which serve as prompts for SAM to generate segmentation masks. These masks provide pixel-level supervision for training segmentation models.
- **Core assumption**: CAM provides sufficiently accurate region localization for SAM to generate useful segmentation masks.
- **Evidence anchors**:
  - [abstract] "Then the region-level bounding box is obtained from class activation map (CAM). A large vision model, Segment Anything Model, takes the region-level bounding boxes as prompts to produce pixel-level lesion masks"
  - [section] "The image-level labels predicted by EndoKED-MIL were further distilled into pixel-level masks through SAM-guided weakly supervised semantic segmentation... EndoKED-SEG achieved competitive performance with averaged dice similarity coefficients (DSCs) of 0.908, 0.920, 0.809, 0.893, 0.818"
- **Break condition**: CAM fails to localize polyps accurately, or SAM cannot generate useful masks from the prompts.

## Foundational Learning

- **Concept**: Multiple Instance Learning (MIL)
  - **Why needed here**: COLONOSCOPY reports contain multiple images with only some showing polyps, requiring bag-level to instance-level label propagation.
  - **Quick check question**: In MIL, if a bag is labeled positive, what does that tell us about the instances within it?

- **Concept**: Weakly Supervised Learning
  - **Why needed here**: Pixel-level annotations are expensive, but image-level labels can be extracted automatically, enabling segmentation without manual masks.
  - **Quick check question**: What is the key difference between weakly supervised and fully supervised segmentation?

- **Concept**: Transfer Learning with Pre-trained Backbone
  - **Why needed here**: Pre-training on large-scale colonoscopy records creates generalizable features that improve downstream task performance with limited labeled data.
  - **Quick check question**: How does transfer learning differ from training a model from scratch on a new task?

## Architecture Onboarding

- **Component map**: LLM extraction -> MIL propagation -> CAM localization -> SAM segmentation -> Model training
- **Critical path**: LLM extraction → MIL propagation → CAM localization → SAM segmentation → Model training. Each step must succeed for the next to work.
- **Design tradeoffs**:
  - Using multiple foundation models (LLM + LVM) enables end-to-end automation but introduces dependency on external model performance
  - Weak supervision reduces annotation cost but may sacrifice segmentation precision compared to fully supervised methods
  - Pre-training on large unannotated data improves generalization but requires substantial computational resources
- **Failure signatures**:
  - Low LLM accuracy → No report-level labels extracted
  - MIL failure → No image-level labels generated
  - Poor CAM quality → SAM cannot generate useful masks
  - Overfitting on training distribution → Poor generalization to new datasets
- **First 3 experiments**:
  1. Validate LLM report comprehension on a small manually annotated subset
  2. Test MIL framework on a dataset with known image-level labels to verify propagation accuracy
  3. Evaluate SAM segmentation quality using CAM-generated bounding boxes on annotated images

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does EndoKED perform in detecting and segmenting polyps of different histological types, such as sessile serrated lesions or diminutive polyps?
- **Basis in paper**: [inferred] The paper mentions that EndoKED achieved expert-level performance in polyp detection and segmentation, but it does not specifically address the performance on different histological types of polyps.
- **Why unresolved**: The paper focuses on the overall performance of EndoKED but does not provide detailed analysis of its performance on specific types of polyps.
- **What evidence would resolve it**: Further studies should evaluate EndoKED's performance on various histological types of polyps to determine its effectiveness across different polyp subtypes.

### Open Question 2
- **Question**: Can EndoKED be adapted to detect and segment other types of lesions or abnormalities in colonoscopy images, such as bleeding or inflammation?
- **Basis in paper**: [inferred] The paper mentions that EndoKED could potentially be used for analyzing other lesions or endoscopic instruments in report images, as long as their occurrence can be comprehended from the report text.
- **Why unresolved**: While the paper suggests the potential for adaptation, it does not provide evidence or experiments demonstrating EndoKED's effectiveness in detecting and segmenting other types of lesions or abnormalities.
- **What evidence would resolve it**: Future studies should investigate the adaptation of EndoKED to detect and segment various types of lesions or abnormalities in colonoscopy images to assess its broader applicability.

### Open Question 3
- **Question**: How does EndoKED handle variations in image quality, such as low resolution or poor illumination, which are common in real-world colonoscopy images?
- **Basis in paper**: [inferred] The paper mentions that EndoKED was validated on independent retrospective and prospective datasets, but it does not explicitly address the model's robustness to variations in image quality.
- **Why unresolved**: The paper does not provide specific information on how EndoKED performs under different image quality conditions, which is crucial for its practical application in real-world settings.
- **What evidence would resolve it**: Additional experiments should be conducted to evaluate EndoKED's performance on colonoscopy images with varying quality, including low resolution and poor illumination, to assess its robustness in real-world scenarios.

## Limitations
- The framework's performance heavily depends on the quality of LLM-based label extraction, which was reported as 100% accuracy but lacks independent validation.
- The effectiveness of SAM-guided weakly supervised segmentation in diverse clinical settings remains uncertain, as the method has not been tested across different imaging protocols or equipment manufacturers.
- The computational cost of running multiple foundation models at scale for real-time clinical deployment is not addressed.

## Confidence

**High Confidence**: The MIL framework for propagating report-level labels to image-level labels, as this is a well-established technique with demonstrated effectiveness in the results.

**Medium Confidence**: The overall pipeline performance metrics, as they are based on evaluation against public datasets and show competitive results, though the absolute numbers depend on the quality of automatically generated labels.

**Low Confidence**: The claim that LLM extraction achieves perfect accuracy on report-level label extraction, as this was not independently verified and may not generalize to reports from different hospitals or with different formatting.

## Next Checks

1. Conduct a blinded study where radiologists independently verify a sample of LLM-extracted labels from colonoscopy reports to establish ground truth accuracy.
2. Test the framework's performance when trained on data from one hospital and evaluated on data from hospitals with different imaging protocols or equipment.
3. Perform ablation studies to quantify the contribution of each component (LLM extraction, MIL propagation, SAM segmentation) to the final performance metrics.