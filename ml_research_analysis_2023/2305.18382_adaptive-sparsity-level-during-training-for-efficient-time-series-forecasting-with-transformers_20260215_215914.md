---
ver: rpa2
title: Adaptive Sparsity Level during Training for Efficient Time Series Forecasting
  with Transformers
arxiv_id: '2305.18382'
source_url: https://arxiv.org/abs/2305.18382
tags:
- sparsity
- pals
- training
- prediction
- level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PALS is a novel method for automatically tuning the sparsity level
  of deep neural networks for time series forecasting. It combines ideas from sparse
  training and during-training pruning, introducing an "expand" mechanism that allows
  the model to dynamically adjust its sparsity during training.
---

# Adaptive Sparsity Level during Training for Efficient Time Series Forecasting with Transformers

## Quick Facts
- arXiv ID: 2305.18382
- Source URL: https://arxiv.org/abs/2305.18382
- Reference count: 40
- Reduces model size by 65% on average while maintaining comparable or better performance

## Executive Summary
PALS introduces a novel method for automatically tuning sparsity levels in deep neural networks for time series forecasting. The approach dynamically adjusts network density during training through an "expand" mechanism that bridges during-training pruning and dynamic sparse training. By leveraging loss-based heuristics to decide when to shrink, expand, or maintain sparsity, PALS achieves significant model compression while improving or maintaining prediction accuracy across multiple transformer architectures and benchmark datasets.

## Method Summary
PALS automatically tunes sparsity during training by monitoring validation loss and adjusting network density accordingly. At each connectivity update step, if validation loss remains within a threshold (λ × Lbest) and sparsity is below maximum (S < Smax), the network prunes more aggressively. If loss exceeds the threshold and sparsity is above the best-performing level (S > Sbest), the network expands by growing connections. Otherwise, it maintains balanced pruning and growth. The method inherits the "shrink" mechanism from during-training pruning and the "stable" mechanism from dynamic sparse training, while introducing the novel "expand" mechanism to recover from over-pruning.

## Key Results
- Reduces model size by 65% on average across six benchmark datasets
- Achieves lower MSE and MAE than dense models in 12 and 14 out of 30 cases respectively
- Reduces FLOPs by 63% on average while maintaining or improving forecasting performance
- Successfully applied to five state-of-the-art transformer variants including NSTransformer, FEDformer, Autoformer, Informer, and standard Transformer

## Why This Works (Mechanism)

### Mechanism 1
PALS automatically finds the optimal sparsity level during training by leveraging loss-based heuristics to decide whether to shrink, expand, or keep the network stable at each connectivity update step. At each ∆t iterations, PALS computes the validation loss Lt_valid. If Lt_valid is within λ × Lbest and sparsity S < Smax, the network is pruned more aggressively (ζprune = γ × ζ). If Lt_valid exceeds λ × Lbest and S > Sbest, the network expands by growing more connections (ζgrow = γ × ζ). Otherwise, it remains stable with balanced pruning and growth (ζprune = ζgrow = ζ).

### Mechanism 2
PALS bridges during-training pruning and dynamic sparse training by inheriting and enhancing their most successful mechanisms while introducing the novel "expand" mechanism. PALS takes the "Shrink" mechanism from during-training pruning (e.g., GraNet) and the "Stable" mechanism from dynamic sparse training (e.g., RigL), then adds "Expand" to allow the network to increase density during training when performance degrades due to over-pruning.

### Mechanism 3
PALS's expand mechanism allows it to recover from over-pruning by increasing network capacity when the loss exceeds λ × Lbest and sparsity S > Sbest. When the loss increases beyond the allowed threshold (λ × Lbest) and the network is sparser than its best-performing state (S > Sbest), PALS grows more connections (ζgrow = γ × ζ) than it prunes (ζprune = ζ), effectively increasing density to recover lost capacity.

## Foundational Learning

- **Concept:** Sparsity in neural networks and its impact on model performance.
  - **Why needed here:** Understanding how unstructured pruning affects model capacity and prediction accuracy is crucial for designing and tuning PALS.
  - **Quick check question:** What is the difference between structured and unstructured sparsity, and why is unstructured sparsity preferred for transformers in this work?

- **Concept:** Dynamic sparse training (DST) and during-training pruning techniques.
  - **Why needed here:** PALS builds on these methods, so familiarity with their mechanisms (e.g., pruning schedules, weight growth criteria) is essential for understanding and extending PALS.
  - **Quick check question:** How does RigL's weight growth criterion differ from SET's, and what trade-offs do they imply?

- **Concept:** Time series forecasting and the unique challenges of transformer-based models for this task.
  - **Why needed here:** PALS is applied to transformers for time series forecasting, so understanding the domain-specific requirements (e.g., long-range dependencies, seasonality) is important for interpreting results and tuning hyperparameters.
  - **Quick check question:** Why are transformers particularly effective for long time series forecasting, and what are their main computational bottlenecks?

## Architecture Onboarding

- **Component map:** Transformer backbone -> Sparsity mask (tracks active connections) -> Training loop with alternating optimization and adaptive sparsity updates -> Loss computation -> Sparsity update decision (shrink/expand/stable) -> Mask update (prune/grow) -> Next iteration

- **Critical path:** The forward pass → loss computation → backpropagation → sparsity update decision (based on Lt_valid, λ, S, Sbest) → mask update (prune/grow) → next iteration. The sparsity update is the critical differentiator from standard training.

- **Design tradeoffs:** Starting from a dense model (Dinit = 1) allows full exploration but may be slower initially; starting sparse (Dinit < 1) is faster but may miss optimal topologies. The choice of λ and γ balances exploration vs. exploitation in sparsity space.

- **Failure signatures:** If PALS gets stuck at high sparsity with poor performance, it may indicate λ is too small or γ too large. If it fails to prune sufficiently, γ or λ may be too conservative. Oscillating sparsity suggests the expand/shrink thresholds are poorly tuned.

- **First 3 experiments:**
  1. Run PALS on a small transformer (e.g., dmodel=256) on the Electricity dataset with default λ=1.1, γ=1.1, and observe sparsity evolution over epochs.
  2. Compare PALS's final sparsity and loss to GraNet with optimized sparsity levels on the ETTm2 dataset.
  3. Vary λ in {1.05, 1.2} and measure the trade-off between final sparsity and validation loss to understand sensitivity.

## Open Questions the Paper Calls Out
- The paper mentions that prediction length can affect the sparsity-loss trade-off but doesn't provide detailed analysis of this relationship across different datasets and models.
- Starting from a highly sparse network rather than a dense network is suggested as a future direction, but the performance implications of this approach are unknown.

## Limitations
- PALS requires careful tuning of hyperparameters (λ, γ, ∆t) which may affect its performance across different datasets and models.
- The computational overhead of dynamic sparsity updates is not fully characterized compared to static pruning approaches.
- The expand mechanism, while novel, could potentially lead to instability if triggered too frequently.

## Confidence
- The effectiveness of PALS in automatically tuning sparsity levels: High
- The novelty and contribution of the expand mechanism: Medium
- The claim that PALS bridges dynamic sparse training and during-training pruning: Medium

## Next Checks
1. Conduct an ablation study removing the expand mechanism to quantify its isolated contribution to performance improvements.
2. Test PALS on non-transformer architectures (e.g., LSTMs, CNNs) to assess generalizability beyond the current scope.
3. Analyze the computational overhead of PALS's dynamic sparsity updates and compare it to static pruning baselines in terms of wall-clock time.