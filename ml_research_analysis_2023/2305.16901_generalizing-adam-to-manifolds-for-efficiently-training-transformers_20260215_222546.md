---
ver: rpa2
title: Generalizing Adam to Manifolds for Efficiently Training Transformers
arxiv_id: '2305.16901'
source_url: https://arxiv.org/abs/2305.16901
tags:
- manifold
- adam
- ghor
- space
- stiefel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel framework to generalize the Adam optimizer
  to homogeneous manifolds by leveraging their global tangent space representation.
  The key idea is to lift manifold elements to a Lie group, perform optimization in
  the global tangent space, and retract back to the manifold without projection steps.
---

# Generalizing Adam to Manifolds for Efficiently Training Transformers

## Quick Facts
- arXiv ID: 2305.16901
- Source URL: https://arxiv.org/abs/2305.16901
- Reference count: 26
- Primary result: A framework to generalize Adam to homogeneous manifolds by leveraging global tangent space representation, applied to train transformers with orthogonality constraints on the Stiefel manifold

## Executive Summary
This paper presents a novel framework for generalizing the Adam optimizer to homogeneous manifolds by exploiting their global tangent space representation. The key innovation is lifting manifold elements to a Lie group, performing optimization in the global tangent space, and retracting back to the manifold without projection steps. When applied to training transformers with orthogonality constraints on the Stiefel manifold, the approach achieves significant speed-ups compared to standard optimizers while enforcing orthogonality up to machine precision.

## Method Summary
The method generalizes Adam to homogeneous manifolds by leveraging their global tangent space representation. It lifts manifold elements to a Lie group, performs optimization in the global tangent space using standard Adam operations, and retracts back to the manifold without projection steps. The approach is applied to train transformers with orthogonality constraints on the Stiefel manifold by constraining the matrices WQ, WK, and WV in the multi-head attention layer to lie on the manifold.

## Key Results
- Enforces orthogonality constraints up to machine precision without requiring dropout, layer normalization, or regularization
- Achieves significant speed-ups compared to standard optimizers when training transformers on MNIST
- Outperforms standard and momentum optimizers in training convergence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Adam optimizer can be generalized to homogeneous manifolds by leveraging their global tangent space representation.
- Mechanism: The approach exploits the structure of homogeneous spaces, which admit a global tangent space representation. By lifting manifold elements to a Lie group, performing optimization in the global tangent space, and retracting back to the manifold without projection steps, all of the steps in the Adam optimizer can be performed.
- Core assumption: The manifolds relevant for optimization of neural networks are homogeneous spaces and admit a global tangent space representation.
- Evidence anchors:
  - [abstract] "In this work a new approach is presented that leverages the special structure of the manifolds which are relevant for optimization of neural networks, such as the Stiefel manifold, the symplectic Stiefel manifold and the Grassmann manifold: all of these are homogeneous spaces and as such admit a global tangent space representation."
  - [section 3.1] "What makes the generalization of Adam to homogeneous spaces possible is a global representation of the tangent spaces to M."
- Break Condition: If the manifold is not a homogeneous space or does not admit a global tangent space representation, this approach would not work.

### Mechanism 2
- Claim: The global tangent space representation allows for a flexible implementation where the learning rate is adapted simultaneously for all parameters, irrespective of whether they are an element of a general manifold or a vector space.
- Mechanism: By defining a global representation of the tangent spaces (ghor), the Adam cache and operations can be applied identically in both the vector space and manifold cases.
- Core assumption: The global tangent space representation is isomorphic to the tangent space at each point on the manifold.
- Evidence anchors:
  - [section 2.5] "Here the Adam cache and the Adam operations in equations (2) and (3) are identical in the vector space and in the manifold case, because we defined a global representation ghor of the tangent spaces."
- Break Condition: If the global tangent space representation is not isomorphic to the tangent space at each point, the Adam operations would not be applicable.

### Mechanism 3
- Claim: The approach enables training of transformers with orthogonality constraints on the Stiefel manifold, achieving significant speed-ups compared to standard optimizers.
- Mechanism: By constraining the matrices WQ, WK and WV in the multihead-attention layer to lie on the Stiefel manifold, orthogonality is enforced up to machine precision, and no special techniques like dropout, layer normalization or regularization are needed to achieve convergence.
- Core assumption: The Stiefel manifold is a suitable space for enforcing orthogonality constraints in transformers.
- Evidence anchors:
  - [abstract] "The method is applied to train transformers with orthogonality constraints on the Stiefel manifold, achieving significant speed-ups compared to standard optimizers."
  - [section 5] "In the following we show the training error of transformers... Each transformer consists of 5 sub layers (of which each again contains a multi-head attention layer and a feedforward neural network). Each multi-head attention has 7 heads and the nonlinearity in the feedforward network is tanh."
- Break Condition: If the Stiefel manifold is not a suitable space for enforcing orthogonality constraints in transformers, this approach would not lead to speed-ups.

## Foundational Learning

- Concept: Homogeneous Spaces
  - Why needed here: The approach leverages the structure of homogeneous spaces to generalize Adam to manifolds.
  - Quick check question: What is a homogeneous space and why are they relevant for optimization of neural networks?

- Concept: Lie Groups and Lie Algebras
  - Why needed here: The approach uses the Lie group associated with the homogeneous space to perform optimization in the global tangent space.
  - Quick check question: What is the relationship between a Lie group and its Lie algebra, and how is this used in the approach?

- Concept: Riemannian Geometry
  - Why needed here: The approach uses concepts from Riemannian geometry, such as the Riemannian gradient and the geodesic spray, to perform optimization on manifolds.
  - Quick check question: What is the Riemannian gradient and how does it differ from the Euclidean gradient?

## Architecture Onboarding

- Component map: rgrad -> lift -> update cache -> velocity -> retraction -> apply
- Critical path: The critical path for the approach is: compute Riemannian gradient → lift to Lie group → update cache → compute velocity → retract to manifold → apply update.
- Design tradeoffs: The approach trades off computational complexity (due to the need to lift to and retract from the Lie group) for the ability to enforce constraints (such as orthogonality) up to machine precision.
- Failure signatures: If the approach fails, it may be due to numerical instability in the lifting or retraction steps, or due to the manifold not being a suitable space for the optimization problem.
- First 3 experiments:
  1. Implement the rgrad, lift, retraction, and apply functions for the Stiefel manifold.
  2. Verify that the approach reduces to standard Adam when the manifold is a vector space.
  3. Test the approach on a simple transformer model with orthogonality constraints on the Stiefel manifold.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed framework be extended to optimize over other homogeneous spaces beyond the Stiefel, symplectic Stiefel, and Grassmann manifolds?
- Basis in paper: [inferred] The paper states that the mathematical constructions are "very general and can, in addition to the Stiefel manifold, be applied to the Grassmann manifold and the symplectic versions of the two manifolds."
- Why unresolved: The paper only provides a theoretical discussion of this generalization, but does not present any concrete examples or experiments to demonstrate its effectiveness.
- What evidence would resolve it: Implementing and testing the proposed optimizer on other homogeneous spaces, such as the orthogonal group or the symplectic group, and comparing its performance to standard optimizers on these manifolds.

### Open Question 2
- Question: How does the choice of the global section λ affect the convergence and stability of the proposed optimizer?
- Basis in paper: [explicit] The paper discusses the computation of the global section λ using Householder reflections, but does not provide any analysis of its impact on the optimization process.
- Why unresolved: The paper does not explore the sensitivity of the optimizer to the choice of λ or provide any guidelines for selecting an appropriate λ for a given problem.
- What evidence would resolve it: Conducting experiments with different choices of λ and analyzing their impact on the convergence rate, stability, and final performance of the optimizer.

### Open Question 3
- Question: Can the proposed framework be extended to handle non-homogeneous manifolds or more general Riemannian manifolds?
- Basis in paper: [inferred] The paper focuses exclusively on homogeneous spaces and does not discuss the potential extension of the framework to more general Riemannian manifolds.
- Why unresolved: The special structure of homogeneous spaces, particularly the existence of a global tangent space representation, is crucial for the proposed approach. It is unclear how to generalize this to non-homogeneous manifolds.
- What evidence would resolve it: Developing a theoretical framework for extending the proposed optimizer to non-homogeneous manifolds and providing experimental evidence of its effectiveness on a variety of Riemannian manifolds.

## Limitations

- The claim that homogeneous manifolds are universally "the manifolds relevant for optimization of neural networks" is stated without comprehensive justification.
- The approach assumes the existence of efficient lift and retraction operations for the manifolds in question, which may not be available for all homogeneous manifolds.
- The paper doesn't address computational overhead introduced by the Lie group operations, which could potentially offset the benefits of constraint enforcement.

## Confidence

**High Confidence:**
- The mathematical framework for generalizing Adam to homogeneous manifolds is sound and internally consistent.
- The experimental results demonstrating faster convergence on MNIST are reproducible and show clear quantitative improvements over baseline optimizers.

**Medium Confidence:**
- The claim that orthogonality is enforced "up to machine precision" is supported by the experimental setup but lacks quantitative verification.
- The assertion that no regularization, dropout, or layer normalization is needed for convergence is based on a single dataset and architecture, limiting generalizability.

## Next Checks

1. **Numerical Stability Analysis**: Track orthogonality error metrics (e.g., ||W^T W - I||) throughout training to verify machine-precision enforcement claims and identify potential numerical instabilities in the lift/retraction operations.

2. **Computational Overhead Benchmarking**: Measure wall-clock time per optimization step for the proposed method versus standard Adam with projection, isolating the cost of Lie group operations from the benefit of constraint enforcement.

3. **Generalization to Other Homogeneous Manifolds**: Implement and test the framework on symplectic Stiefel and Grassmann manifolds with neural network training tasks to verify the claim about universal applicability to homogeneous spaces relevant to deep learning.