---
ver: rpa2
title: Implicit regularization in AI meets generalized hardness of approximation in
  optimization -- Sharp results for diagonal linear networks
arxiv_id: '2307.07410'
source_url: https://arxiv.org/abs/2307.07410
tags:
- have
- then
- problem
- lemma
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the implicit regularization of diagonal\
  \ linear networks (DLNs) in over-parameterized regression and its surprising connection\
  \ to generalized hardness of approximation (GHA) in optimization. The authors show\
  \ that for tiny initialization, the gradient flow of DLNs with parametrization \u03C8\
  \u03B8 = \u03B8p+ \u2212 \u03B8p\u2212 approximates minimizers of the basis pursuit\
  \ problem (minimizing \u21131-norm subject to linear constraints)."
---

# Implicit regularization in AI meets generalized hardness of approximation in optimization -- Sharp results for diagonal linear networks

## Quick Facts
- arXiv ID: 2307.07410
- Source URL: https://arxiv.org/abs/2307.07410
- Reference count: 40
- Primary result: Diagonal linear networks with tiny initialization implicitly regularize toward ℓ1-minimizers of basis pursuit, with sharp O(αp) convergence rates for p > 2 proven via reduction to generalized hardness of approximation

## Executive Summary
This paper establishes a surprising connection between implicit regularization in diagonal linear networks (DLNs) and generalized hardness of approximation (GHA) in optimization. The authors show that DLNs with tiny initialization and gradient flow dynamics converge to minimizers of the basis pursuit problem (ℓ1-norm minimization subject to linear constraints), not just to the optimal objective value. Critically, they prove these convergence bounds are sharp by demonstrating that better bounds would contradict fundamental GHA results, implying certain approximation accuracies are computationally impossible to achieve.

## Method Summary
The paper analyzes DLNs parametrized as ψθ = θp+ − θp− with tiny initialization α, using continuous gradient flow to study implicit regularization. The authors prove that as t → ∞, ψα(t) converges to a specific ℓ1-minimizer Wp(A,y) of the basis pursuit problem. They derive sharp convergence bounds O(αp) for p > 2 through differential equation analysis and prove sharpness via reduction to GHA. The discretization error between continuous flow and practical gradient descent is bounded by O(η) for step size η, enabling practical algorithm implementation.

## Key Results
- DLNs with tiny initialization and gradient flow converge to ℓ1-minimizers of basis pursuit (not just optimal value)
- Sharp convergence bounds O(αp) for p > 2, with problem-dependent rate for p = 2
- Sharpness proven via reduction to GHA: better bounds would compute impossible approximations
- Characterizes which ℓ1-minimizer is selected (entropy vs norm maximization) based on network depth p

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diagonal linear networks with tiny initialization implicitly regularize toward ℓ1-minimizers of basis pursuit via gradient flow convergence.
- Mechanism: The gradient flow dynamics for θ(t) in (2.2) produce ψα(t) that asymptotically approaches the unique minimizer Wp(A,y) in the ℓ1-ball, with convergence rate O(αp) for p > 2 and problem-dependent rate for p = 2.
- Core assumption: The gradient flow remains in the region where ∇²Qp is well-conditioned and converges to a unique ℓ1-minimizer.
- Evidence anchors:
  - [abstract] "the gradient flow of DLNs with tiny initialization approximates minimizers of the basis pursuit optimization problem (as opposed to just the objective function)"
  - [section 3.2.1] "From Lemma 3.9 we know [ψα(t)]i = αphp(...)"
  - [corpus] Weak: related works discuss convergence to ℓ1-norm minimum but not to specific minimizers.

### Mechanism 2
- Claim: Sharpness of the convergence bounds is proven by reduction to generalized hardness of approximation (GHA).
- Mechanism: If better convergence rates existed, they would yield an algorithm computing ℓ1-minimizers beyond what GHA permits, contradicting Theorem 1.2.
- Core assumption: The GHA phase transition for basis pursuit holds at accuracy κ = 10^(-K) for computable inputs.
- Evidence anchors:
  - [abstract] "Non-sharpness of our results would imply that the GHA phenomenon would not occur for the basis pursuit optimization problem – which is a contradiction – thus implying sharpness"
  - [section 5.3] "This contradicts the existence of the general algorithm in (i). We conclude that Assumption 1 does not hold."
  - [corpus] Weak: no direct mention of GHA in related works.

### Mechanism 3
- Claim: Gradient descent with sufficiently small step size η approximates the continuous gradient flow within any ε > 0.
- Mechanism: The discretization error ∥θ⌊t/η⌋ - θ(t)∥₂ is bounded by O(η) times a problem-dependent constant; choosing η small enough yields the desired accuracy.
- Core assumption: The gradient flow and its discretization remain in a region where ∇²L is Lipschitz and the ODE is well-behaved.
- Evidence anchors:
  - [section 4.1] "Let α > 0, p ≥ 2, ε > 0, and t ∈ [0, ∞). Define M = 2√N/σmin(A)∥y∥₂ + αp..."
  - [section 4.1] "Then gradient descent (4.1) with step length η ≤ min{ε, αp} 1/C₁ e^{-C₂t} satisfies ∥θ⌊t/η⌋ - θ(t)∥₂ ≤ ε"
  - [corpus] Weak: related works use continuous flow analysis but rarely discuss discretization error bounds.

## Foundational Learning

- Concept: Gradient flow as continuous limit of gradient descent
  - Why needed here: The paper analyzes implicit regularization via continuous gradient flow, then discretizes to connect to practical algorithms.
  - Quick check question: What is the relationship between the forward Euler discretization of gradient flow and standard gradient descent steps?

- Concept: ℓ1-minimization and basis pursuit
  - Why needed here: The implicit regularization of DLNs is characterized by convergence to minimizers of basis pursuit (min ∥x∥₁ subject to Ax = y).
  - Quick check question: How does the solution set U(A,y) change when A has multiple ℓ1-minimizers?

- Concept: Generalized hardness of approximation (GHA)
  - Why needed here: Sharpness of convergence bounds is established by reduction to GHA, showing certain accuracies are computationally impossible.
  - Quick check question: What is the phase transition in GHA for basis pursuit at accuracy κ = 10^(-K)?

## Architecture Onboarding

- Component map: Input (A, y) → Gradient Flow Solver (continuous dynamics) → Discretization (gradient descent) → Convergence Analysis → Sharpness Proof (GHA reduction)
- Critical path: Choose initialization α → Run gradient flow ψα(t) → Show ψα(∞) ≈ Wp(A,y) → Bound discretization error → Prove sharpness via contradiction
- Design tradeoffs: Smaller α improves ℓ1-minimizer approximation but slows convergence; larger p changes which ℓ1-minimizer is selected (entropy vs norm maximization)
- Failure signatures: If rank(A) < m, flow converges to Wp(eA,ey) instead; if α too large, no convergence to ℓ1-minimizer; if step size η too large, discretization error dominates
- First 3 experiments:
  1. Fix (A, y) with unique ℓ1-minimizer, vary α and p, measure ∥ψα(∞) - Wp(A,y)∥₂
  2. Fix α and p, vary matrix A with different nullspace conditioning, measure convergence rate
  3. Implement gradient descent discretization with varying η, compare to continuous flow error

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the implicit regularization of diagonal linear networks be extended to other matrix norms beyond ℓ1, and what are the corresponding phase transitions in generalized hardness of approximation?
- Basis in paper: [explicit] The paper connects diagonal linear networks' implicit regularization to ℓ1-minimization and GHA phase transitions, suggesting this framework could generalize to other norms
- Why unresolved: The paper focuses specifically on ℓ1-norm minimization and its connection to GHA, without exploring other norms like ℓ2 or nuclear norm
- What evidence would resolve it: Numerical experiments showing convergence rates and error bounds for diagonal linear networks with other regularization norms, plus theoretical analysis of corresponding GHA phase transitions

### Open Question 2
- Question: What is the precise relationship between the depth of diagonal linear networks and the convergence rate to the optimal solution, and can this be generalized beyond the O(αp) rate shown for p > 2?
- Basis in paper: [explicit] The paper shows that convergence rate depends on network depth p, with O(αp) for p > 2, but notes that the dependence on A is sharp
- Why unresolved: While the paper establishes the O(αp) rate for p > 2 and dependence on A, the exact mathematical relationship between depth and convergence rate across all values of p remains unclear
- What evidence would resolve it: Rigorous mathematical proof characterizing the convergence rate as a function of p for all p ≥ 2, including the boundary cases and dependence on matrix properties

### Open Question 3
- Question: Can the impossibility results regarding computing approximate solutions to basis pursuit problems be circumvented by using non-Turing computational models or quantum algorithms?
- Basis in paper: [explicit] The paper proves that certain approximations to basis pursuit solutions cannot be computed by any algorithm due to GHA, implying limits on traditional computation models
- Why unresolved: The impossibility results are based on general algorithms, but the paper does not explore whether quantum computing or other non-classical models could overcome these barriers
- What evidence would resolve it: Implementation of quantum algorithms for basis pursuit problems showing either successful computation beyond classical limits or theoretical proof of similar limitations in quantum models

## Limitations

- The analysis assumes gradient flow remains in well-conditioned regions, but ill-conditioned nullspaces could violate this assumption
- Sharpness proof depends on external hardness results for basis pursuit that may not generalize to all problem instances
- Practical implementation requires careful selection of step size η based on problem-dependent constants that may be difficult to estimate

## Confidence

- **High Confidence**: The convergence rate bounds O(αp) for p > 2 and the characterization of which ℓ1-minimizer is selected (entropy vs norm maximization) are mathematically rigorous and well-supported by the proofs.
- **Medium Confidence**: The sharpness proof via GHA reduction is logically sound, but depends on the correctness of external hardness results and the specific problem formulation used.
- **Medium Confidence**: The gradient descent discretization analysis is technically correct, but practical implementations may face challenges with the problem-dependent constants and step size selection.

## Next Checks

1. Implement numerical experiments varying α and p to empirically verify the O(αp) convergence rate and confirm which ℓ1-minimizer is selected in cases with multiple solutions.
2. Test the gradient descent discretization with varying step sizes η to measure actual discretization error against the theoretical O(η) bound and identify practical limitations.
3. Construct pathological cases with ill-conditioned matrices A to test the boundaries of the well-conditioned gradient flow assumption and measure how convergence degrades.