---
ver: rpa2
title: Perspectives on Large Language Models for Relevance Judgment
arxiv_id: '2304.09161'
source_url: https://arxiv.org/abs/2304.09161
tags:
- relevance
- judgments
- human
- llms
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  be used for automatic relevance judgments in information retrieval. The authors
  propose a spectrum of human-machine collaboration ranging from fully manual judgments
  to fully automated ones.
---

# Perspectives on Large Language Models for Relevance Judgment

## Quick Facts
- arXiv ID: 2304.09161
- Source URL: https://arxiv.org/abs/2304.09161
- Reference count: 40
- Primary result: LLMs agree with human relevance judgments 50-90% of the time depending on the task, showing promise for assisting but not replacing human assessors.

## Executive Summary
This paper investigates the potential of large language models (LLMs) for automatic relevance judgments in information retrieval. The authors propose a spectrum of human-machine collaboration ranging from fully manual judgments to fully automated ones. Through pilot experiments comparing LLM-based judgments with human assessors on two test collections, they find that LLMs can generate judgments that agree with humans 50-90% of the time, depending on the task complexity. While LLMs are not yet ready to fully replace human assessors, the results suggest they could serve as valuable assistants, particularly for large-scale or multilingual tasks where human annotation is costly or impractical.

## Method Summary
The authors conducted pilot experiments using two LLMs (GPT-3.5 and YouChat) to generate relevance judgments for sampled topic-document pairs from TREC-8 adhoc retrieval and TREC 2021 Deep Learning Track passage retrieval collections. They compared LLM judgments with human assessments using Cohen's Kappa and Kendall's Tau as agreement metrics. The experiments involved creating simple prompts for each LLM to generate binary or graded relevance judgments, then calculating agreement rates with human labels. The study explored different levels of human-machine collaboration, from fully manual to fully automated judgments, to identify where LLMs could most effectively assist in the judgment process.

## Key Results
- LLM-human agreement rates vary significantly by task type: 90% agreement for non-relevant documents, dropping to 50% for relevant documents
- LLMs show promise for pre-filtering obviously non-relevant documents, reducing human annotation burden
- Human-machine collaboration appears promising but the optimal balance between human and machine involvement remains undetermined

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can mimic human relevance judgments well enough for early-stage evaluation of retrieval systems.
- Mechanism: When provided with prompts that include examples and a clear relevance definition, LLMs generate judgments that correlate moderately with human labels, allowing similar system rankings.
- Core assumption: The LLM has seen enough training data with similar relevance concepts to generalize to unseen topics.
- Evidence anchors:
  - [abstract] "LLMs agree with humans 50-90% of the time depending on the task."
  - [section 5.2] "GPT-3.5 generates the same answer in 90% of the cases" for non-relevant documents.
  - [corpus] "Average neighbor FMR=0.459" — weak evidence for broader relevance judgment literature, but suggests moderate correlation is typical.
- Break condition: If the topic or document domain is far outside the LLM's pretraining distribution, correlation drops sharply.

### Mechanism 2
- Claim: LLM judgments can reduce human annotation cost by pre-filtering obviously non-relevant documents.
- Mechanism: An LLM can quickly label a large number of documents as non-relevant, allowing human assessors to focus only on ambiguous or relevant candidates.
- Core assumption: LLM precision for non-relevant labeling is high enough that false negatives are acceptable in cost-reduction scenarios.
- Evidence anchors:
  - [abstract] "they show promise for assisting in the judgment process, especially for large-scale or multilingual tasks."
  - [section 5.2] "For the documents judged as relevant by human assessors, this agreement drops to 50%."
  - [corpus] No direct evidence of cost studies, but related works cite LLM4Eval workshop interest in cost reduction.
- Break condition: When the proportion of truly relevant documents is high, the cost savings diminish because fewer can be filtered out.

### Mechanism 3
- Claim: Human-machine collaboration can improve over pure manual or pure automatic labeling.
- Mechanism: Humans verify or correct LLM suggestions, combining the consistency and speed of LLMs with human judgment on edge cases.
- Core assumption: Human verification is cheaper than full manual annotation and improves final label quality enough to justify the hybrid approach.
- Evidence anchors:
  - [abstract] "a spectrum of human-machine collaboration ranging from fully manual judgments to fully automated ones."
  - [section 3] "AI Assistance" and "Human Verification" levels show specific collaborative strategies.
  - [corpus] Moderate FMR (0.459) suggests neither humans nor LLMs alone are perfect; collaboration likely improves results.
- Break condition: If LLM bias is systematic and humans fail to correct it consistently, collaboration may not improve quality.

## Foundational Learning

- Concept: Relevance judgment definitions (binary vs graded vs explained).
  - Why needed here: Different judgment types have vastly different difficulty for LLMs; understanding them is key to prompt design.
  - Quick check question: What is the difference between binary and graded relevance in TREC collections?

- Concept: Cohen's kappa and Kendall's tau as agreement metrics.
  - Why needed here: These metrics are used to quantify LLM-human agreement; knowing their interpretation guides evaluation.
  - Quick check question: If Cohen's kappa is 0.26, is that considered poor, fair, or good agreement?

- Concept: Prompt engineering basics (few-shot learning, temperature settings).
  - Why needed here: The paper shows prompt design directly affects LLM output quality.
  - Quick check question: What does setting temperature=0 do to LLM output variability?

## Architecture Onboarding

- Component map:
  Prompt generator → LLM inference engine → Output parser → Judgment label → (Optional) Human verification interface → Final judgment store

- Critical path:
  1. Generate query + document prompt
  2. Send to LLM (with temperature=0 for consistency)
  3. Parse text output to binary/graded label
  4. Store label and confidence
  5. (Optional) Present to human for verification

- Design tradeoffs:
  - Speed vs. accuracy: Lower temperature = consistent but potentially less nuanced judgments
  - Cost vs. coverage: Using a cheaper, faster LLM may require more human checks
  - Prompt complexity vs. generalization: Rich prompts help on known topics but may fail on novel domains

- Failure signatures:
  - Low kappa (<0.2) with human labels → LLM misunderstanding task
  - High agreement on non-relevant but low on relevant → LLM struggles with subtle relevance
  - System crash or timeout → API limits or model unavailability

- First 3 experiments:
  1. Sample 100 random topic-document pairs; run LLM with binary prompt; compute Cohen's kappa vs human labels
  2. Repeat with graded prompt on DL-2021 corpus; compare NDCG rankings
  3. Run LLM-only judgment on full DL-2021 run set; compute sensitivity vs human judgments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well do LLM-generated relevance judgments correlate with human judgments across different types of relevance (binary, graded, explained)?
- Basis in paper: [explicit] The paper reports pilot experiments comparing LLM judgments with human assessors, finding varying levels of agreement (50-90%) depending on the task and type of judgment.
- Why unresolved: The experiments were limited in scope, using only two test collections and simple prompts without prompt engineering. The correlation varies significantly based on the type of judgment and corpus.
- What evidence would resolve it: Comprehensive experiments across multiple test collections, judgment types, and with optimized prompts would clarify the correlation patterns and identify when LLM judgments are most reliable.

### Open Question 2
- Question: Can LLMs reliably assess relevance that goes beyond topical relevance, such as style, truthfulness, or cultural appropriateness?
- Basis in paper: [explicit] The paper discusses concerns about LLMs' ability to assess non-topical aspects of relevance, noting that human assessors often need external information to determine truthfulness.
- Why unresolved: Current experiments primarily focus on topical relevance. The ability of LLMs to handle nuanced aspects of relevance that require cultural knowledge or fact-checking is not well-established.
- What evidence would resolve it: Experiments testing LLMs on tasks requiring assessment of non-topical relevance factors, with comparisons to human judgments, would clarify their capabilities in these areas.

### Open Question 3
- Question: What is the optimal balance of human and machine involvement in the relevance judgment process?
- Basis in paper: [explicit] The paper proposes a spectrum of human-machine collaboration but notes that the ideal balance (competence partitioning) is not yet determined.
- Why unresolved: While the paper outlines different levels of collaboration, it does not empirically determine which level provides the best trade-off between quality and efficiency. The optimal balance likely varies by task and context.
- What evidence would resolve it: Empirical studies comparing different levels of human-machine collaboration across various tasks, measuring both judgment quality and efficiency, would identify the optimal balance for different scenarios.

## Limitations

- The paper presents preliminary evidence with limited scope, using only two test collections and simple prompts without prompt engineering
- Agreement rates vary significantly (50-90%) depending on task complexity, with notably weaker performance on identifying relevant documents
- The cost-benefit analysis of human-machine collaboration remains largely theoretical rather than empirically validated

## Confidence

- LLM-human agreement rates (50-90%): Medium confidence
- LLM utility for filtering non-relevant documents: Medium confidence
- Human-machine collaboration improvements: Low confidence

## Next Checks

1. Conduct error analysis on LLM-human disagreements to identify systematic patterns or failure modes in LLM judgments
2. Perform controlled cost analysis comparing pure manual annotation, pure LLM annotation, and hybrid approaches across different document relevance distributions
3. Test LLM performance on out-of-domain topics to quantify the drop in agreement when topic-document pairs fall outside pretraining distribution