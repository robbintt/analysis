---
ver: rpa2
title: Explaining grokking through circuit efficiency
arxiv_id: '2309.02390'
source_url: https://arxiv.org/abs/2309.02390
tags:
- dataset
- grokking
- training
- test
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes a new explanation for grokking: a network''s
  sudden transition from poor to perfect test accuracy. The authors argue that grokking
  occurs when the task admits both a generalising and a memorising solution, with
  the generalising solution being slower to learn but more efficient.'
---

# Explaining grokking through circuit efficiency

## Quick Facts
- **arXiv ID:** 2309.02390
- **Source URL:** https://arxiv.org/abs/2309.02390
- **Reference count:** 40
- **Primary result:** Proposes that grokking occurs when generalising circuits (slower to learn but more efficient) overtake memorising circuits as dataset size increases

## Executive Summary
This paper provides a novel explanation for grokking - the phenomenon where neural networks show sudden transition from poor to perfect test accuracy - through the lens of circuit efficiency. The authors argue that grokking occurs when tasks admit both a generalising solution and a memorising solution, with the generalising solution being slower to learn but more efficient. They show that as dataset size increases, memorisation efficiency decreases while generalisation efficiency stays constant, leading to a critical dataset size where the two solutions are equally efficient. The paper predicts and demonstrates two novel behaviours - ungrokking and semi-grokking - based on this efficiency-based explanation.

## Method Summary
The authors train 1-layer decoder-only transformer models on modular addition tasks, analyzing the development of generalising (Cgen) and memorising (Cmem) circuits through proxy measures. They use AdamW optimizer with weight decay to create a regularisation effect that favours more efficient circuits. By varying dataset sizes and analyzing the efficiency of different circuit types (measured as logit-to-parameter-norm ratios), they identify a critical dataset size (Dcrit) where Cgen and Cmem have equal efficiency. The experiments include systematic analysis of ungrokking (networks regressing to poor test accuracy when trained on smaller datasets) and semi-grokking (delayed generalisation to partial rather than perfect test accuracy).

## Key Results
- Demonstrates ungrokking and semi-grokking behaviours as novel predictions of the efficiency-based theory
- Shows that memorizing circuit efficiency decreases with increasing dataset size while generalizing circuit efficiency remains constant
- Identifies a critical dataset size Dcrit where the two circuit types have equal efficiency
- Provides empirical evidence that weight decay favours more efficient circuits, driving the transition from memorization to generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grokking occurs when the task admits both a generalising and a memorising solution, with the generalising solution being slower to learn but more efficient.
- Mechanism: The generalising circuit (Cgen) is more efficient than the memorising circuit (Cmem), meaning it can produce equivalent cross-entropy loss on the training set with a lower parameter norm. Weight decay strengthens more efficient circuits over less efficient ones, causing a transition from memorisation to generalisation.
- Core assumption: Circuits that achieve perfect training accuracy can be ranked by efficiency, and weight decay favours more efficient circuits.
- Evidence anchors:
  - [abstract] "we propose that grokking occurs when the task admits a generalising solution and a memorising solution, where the generalising solution is slower to learn but more efficient, producing larger logits with the same parameter norm."
  - [section] "we can explain grokking as follows. In the first phase, Cmem is learned quickly, leading to strong train performance and poor test performance. In the second phase, Cgen is now learned, and parameter norm is 'reallocated' from Cmem to Cgen, eventually leading to a mixture of strong Cgen and weak Cmem, causing an increase in test performance."
- Break condition: If there is no circuit that generalises well, or if the generalising circuit is not more efficient than the memorising circuit, grokking will not occur.

### Mechanism 2
- Claim: The efficiency of memorising circuits decreases with increasing training dataset size, while the efficiency of generalising circuits stays constant.
- Mechanism: Memorising circuits must memorise each additional data point added to the training dataset, and so their efficiency should decrease as training dataset size increases. Generalising circuits automatically work for any new data points added to the training dataset, and so their efficiency should be independent of the size of the training dataset.
- Core assumption: The efficiency of a circuit is defined as the extent to which it can convert relatively small parameters into relatively large logits.
- Evidence anchors:
  - [abstract] "we hypothesise that memorising circuits become more inefficient with larger training datasets while generalising circuits do not, suggesting there is a critical dataset size at which memorisation and generalisation are equally efficient."
  - [section] "Since Cgen generalises well, it automatically works for any new data points that are added to the training dataset, and so its efficiency should be independent of the size of the training dataset. In contrast, Cmem must memorise any additional data points added to the training dataset, and so we should expect its efficiency to decrease as D increases."
- Break condition: If the task is such that memorising circuits do not become less efficient with increasing dataset size, or if generalising circuits do not maintain constant efficiency, the theory will not hold.

### Mechanism 3
- Claim: There exists a critical dataset size Dcrit at which the generalising and memorising circuits are equally efficient, leading to ungrokking and semi-grokking behaviours.
- Mechanism: By analysing dynamics at Dcrit, we predict and demonstrate two new behaviours: ungrokking, in which a network regresses from perfect to low test accuracy when further trained on a smaller dataset, and semi-grokking, in which a network shows delayed generalisation to partial rather than perfect test accuracy.
- Core assumption: The relative efficiency of the two circuit types determines which one gradient descent will strengthen at any given dataset size.
- Evidence anchors:
  - [abstract] "By analysing this critical threshold, they predict and demonstrate two novel behaviours: ungrokking (where a network regresses to poor test accuracy when further trained on a smaller dataset) and semi-grokking (where a network shows delayed generalisation to partial rather than perfect test accuracy)."
  - [section] "This suggests that there exists a crossover point at which Cgen becomes more efficient than Cmem, which we call the critical dataset size Dcrit. By analysing dynamics at Dcrit, we predict and demonstrate two new behaviours (Figure 1)."
- Break condition: If the relative efficiency of the two circuit types does not determine which one gradient descent will strengthen, the predictions of ungrokking and semi-grokking will not hold.

## Foundational Learning

- Concept: Cross-entropy loss and its relationship with logit scaling.
  - Why needed here: The paper relies on the fact that once a classifier achieves perfect accuracy, the cross-entropy loss can be further reduced by scaling up the logits. This is crucial for understanding why weight decay favours more efficient circuits.
  - Quick check question: If a classifier has perfect training accuracy, what happens to the cross-entropy loss if all logits are scaled up by a factor of 2?
- Concept: Weight decay as a regularisation technique.
  - Why needed here: Weight decay is a key component of the paper's explanation for grokking. It is used to model the regularisation effect that favours more efficient circuits over less efficient ones.
  - Quick check question: How does weight decay affect the parameter norm of a classifier, and why is this relevant to the paper's explanation for grokking?
- Concept: Circuit efficiency and its relationship with parameter norm.
  - Why needed here: The paper defines circuit efficiency as the extent to which a circuit can convert relatively small parameters into relatively large logits. This is central to the explanation for why weight decay favours more efficient circuits.
  - Quick check question: If two circuits produce the same average logit value, but one has a lower parameter norm, which circuit is considered more efficient according to the paper's definition?

## Architecture Onboarding

- Component map:
  - 1-layer decoder-only transformer with learned positional embeddings
  - Untied embeddings/unembeddings
  - Residual stream width (dmodel) = 128
  - Attention head size (dhead) = 32
  - MLP hidden layer size (dmlp) = 512
  - Number of attention heads = dmodel/dhead = 4
  - AdamW optimizer with β1 = 0.9, β2 = 0.98, learning rate = 10^-3, weight decay = 1.0

- Critical path: Training the model on the modular addition task using the AdamW optimizer with cross-entropy loss and weight decay. Monitoring the development of the generalising and memorising circuits through the proxy measures defined in Appendix B.

- Design tradeoffs:
  - The use of a 1-layer transformer simplifies the model architecture but may limit the complexity of the circuits that can be learned.
  - The choice of hyperparameters (e.g., learning rate, weight decay) can significantly affect the grokking behaviour observed in the experiments.
  - The use of the modular addition task allows for a clear definition of the generalising and memorising circuits, but the results may not generalise to other tasks.

- Failure signatures:
  - If the model does not exhibit grokking behaviour, it may indicate that the task does not admit both a generalising and a memorising solution, or that the generalising solution is not more efficient than the memorising solution.
  - If the ungrokking or semi-grokking behaviours are not observed, it may indicate that the critical dataset size Dcrit does not exist for the given task and model architecture.

- First 3 experiments:
  1. Train the model on the modular addition task with a large dataset size and monitor the development of the generalising and memorising circuits using the proxy measures defined in Appendix B.
  2. Vary the dataset size and observe the effect on the relative efficiency of the generalising and memorising circuits, as well as the occurrence of ungrokking and semi-grokking behaviours.
  3. Modify the model architecture or hyperparameters and observe the effect on the grokking behaviour, particularly the efficiency of the generalising and memorising circuits and the occurrence of ungrokking and semi-grokking.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How exactly does the "slow vs fast learning" property of generalizing vs memorizing circuits arise in practice, and what determines the relative learning speeds?
- Basis in paper: [explicit] The paper states this is a necessary ingredient for grokking, but does not fully explain the mechanism behind it
- Why unresolved: The paper only demonstrates the necessity through a constructed example, without explaining the underlying cause in real neural networks
- What evidence would resolve it: Detailed analysis of how different circuit types develop during training, potentially through techniques like circuit tracing or activation patching

### Open Question 2
- Question: What is the exact mechanism by which weight decay creates the critical dataset size (D_crit) where generalizing and memorizing circuits have equal efficiency?
- Basis in paper: [explicit] The paper predicts this critical threshold exists but doesn't provide a complete theoretical derivation of why it occurs at specific dataset sizes
- Why unresolved: While the paper shows empirical evidence of D_crit through experiments, the theoretical basis for why this specific threshold emerges is not fully developed
- What evidence would resolve it: Mathematical analysis showing how the efficiency of both circuit types scales with dataset size, potentially through theoretical bounds on memorization capacity

### Open Question 3
- Question: How would the theory extend to more realistic settings where there are multiple different circuit families contributing to task performance, rather than just two?
- Basis in paper: [inferred] The paper acknowledges this as a limitation, noting that "good training performance is typically achieved when the model has many different circuit families"
- Why unresolved: The current theory only handles the case of two competing circuit families, and it's unclear how the efficiency-based selection would work with many competing circuits
- What evidence would resolve it: Experiments showing how networks balance multiple circuit families with varying efficiencies and learning speeds, potentially through techniques like network dissection or pathway analysis

### Open Question 4
- Question: What is the relationship between circuit efficiency and other constraints that gradient descent must navigate, such as bottleneck activations or interference between circuits?
- Basis in paper: [inferred] The paper explicitly mentions this as a limitation, stating that "we only consider one kind of constraint that gradient descent must navigate: parameter norm"
- Why unresolved: The current theory doesn't account for how other architectural constraints might affect the efficiency-based selection of circuits
- What evidence would resolve it: Experiments measuring circuit efficiency while varying other architectural constraints, potentially through techniques like activation maximization or gradient-based attribution

## Limitations
- The paper's theoretical framework relies heavily on the existence of distinct generalizing and memorizing circuits, but the methodology for isolating these circuits is not fully specified.
- The claim that memorizing circuit efficiency decreases with dataset size while generalizing circuit efficiency remains constant requires stronger empirical validation across diverse tasks.
- The relationship between weight decay strength and circuit efficiency ranking needs more rigorous testing to confirm it's the primary mechanism driving the grokking transition.

## Confidence

- **Mechanism of Grokking (High)**: The explanation that grokking occurs when a more efficient generalizing circuit eventually overtakes a less efficient memorizing circuit has strong empirical support through the observed ungrokking and semi-grokking phenomena.
- **Critical Dataset Size (Medium)**: While the existence of Dcrit is demonstrated for the modular addition task, the generalizability to other tasks and architectures remains to be thoroughly tested.
- **Circuit Efficiency Definition (Medium)**: The definition of efficiency as logit-to-parameter-norm ratio is reasonable but may not capture all relevant aspects of circuit quality, particularly for more complex tasks.

## Next Checks

1. **Cross-Task Validation**: Test the critical dataset size theory on diverse tasks (e.g., simple image classification, logical reasoning) to verify whether memorizing circuits consistently become less efficient with larger datasets while generalizing circuits maintain constant efficiency.

2. **Ablation on Weight Decay**: Systematically vary weight decay strength and regularization techniques (L1, L2, etc.) to determine whether the proposed mechanism specifically requires weight decay or if other forms of regularization can produce similar grokking behavior.

3. **Circuit Isolation Verification**: Implement and validate the trigonometric subspace projection method for isolating Cgen and Cmem circuits, then verify that the efficiency measurements are robust to different isolation techniques and noise levels.