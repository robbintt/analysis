---
ver: rpa2
title: Learning to reconstruct the bubble distribution with conductivity maps using
  Invertible Neural Networks and Error Diffusion
arxiv_id: '2307.02496'
source_url: https://arxiv.org/abs/2307.02496
tags:
- magnetic
- conductivity
- sensors
- field
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes Invertible Neural Networks (INNs) to reconstruct
  conductivity maps from external magnetic field measurements in a model simulating
  a water electrolyzer. The goal is to detect gas bubbles that reduce electrolysis
  efficiency by obstructing current flow.
---

# Learning to reconstruct the bubble distribution with conductivity maps using Invertible Neural Networks and Error Diffusion

## Quick Facts
- arXiv ID: 2307.02496
- Source URL: https://arxiv.org/abs/2307.02496
- Reference count: 2
- Primary result: INNs outperform Tikhonov and ElasticNet regularization for reconstructing conductivity maps from magnetic field measurements in water electrolyzers

## Executive Summary
This study proposes using Invertible Neural Networks (INNs) to reconstruct conductivity maps from external magnetic field measurements in a simulated water electrolyzer. The goal is to detect gas bubbles that reduce electrolysis efficiency by obstructing current flow. Since only a few magnetic sensors are available, the problem is ill-posed. INNs learn a bijective mapping between low-dimensional magnetic field data and high-dimensional conductivity distributions, incorporating latent variables to handle information loss. The approach demonstrates superior performance compared to traditional regularization methods in both visual quality and quantitative evaluation using randomized error diffusion.

## Method Summary
The method uses INNs with affine coupling blocks to learn a bijective mapping between sparse magnetic field measurements and high-dimensional conductivity maps. The network incorporates latent variables to handle information loss during the forward process. Training uses mean absolute error between ground truth and predictions, with optimal performance achieved using 3-4 coupling blocks. Evaluation employs a novel randomized error diffusion technique that converts continuous predictions to binary maps using Dirichlet-distributed error fractions, enabling likelihood-based comparison with ground truth. The approach is validated on synthetic data simulating a water electrolyzer with varying sensor configurations.

## Key Results
- INN models achieve better log-likelihood scores than Tikhonov and ElasticNet regularization when evaluated using randomized error diffusion
- Optimal performance achieved with 3-4 coupling blocks, with underfitting at k=1 and overfitting at k>4
- INN models maintain robust performance even with fewer sensors (50 vs 100) and larger sensor-to-plate distances (25mm vs 5mm)
- Faster training times compared to traditional regularization methods while achieving superior reconstruction quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: INN's bijective mapping allows simultaneous optimization of both forward and inverse mappings, reducing the ill-posedness of reconstructing high-dimensional conductivity from sparse magnetic field data.
- Mechanism: The INN learns a bijective transformation between low-dimensional magnetic measurements and high-dimensional conductivity maps, using latent variables to account for information loss during the forward process. This enables the model to reconstruct conductivity fields by leveraging the learned inverse mapping.
- Core assumption: The bijective property holds and the network can effectively learn the complex relationship between magnetic fields and conductivity distributions.
- Evidence anchors:
  - [abstract] "INNs are used to learn a bijective mapping between low-dimensional magnetic field data and high-dimensional conductivity distributions, incorporating latent variables to handle information loss."
  - [section 2.2] "The INN has a bijective mapping between [magnetic field measurements] and conductivity, leading to INN's invertibility, that learns to associate the conductivity with unique pairs of magnetic field measurements and latent variables."
  - [corpus] Weak evidence; corpus neighbors focus on different inverse problems but don't specifically address INN bijectivity.
- Break condition: The bijective property fails if the network architecture cannot capture the true underlying mapping, or if the latent variables cannot adequately represent the lost information.

### Mechanism 2
- Claim: The use of randomized error diffusion for evaluation converts continuous conductivity predictions into binary maps, enabling more meaningful comparison with ground truth and improving likelihood scores.
- Mechanism: Instead of using continuous metrics, the approach generates an ensemble of binary maps from each continuous prediction using Dirichlet-distributed error fractions. This allows computing the likelihood of the ground truth with respect to the estimated density of these binary ensembles.
- Core assumption: The ensemble of binary maps generated through randomized error diffusion adequately represents the uncertainty in the continuous predictions and allows for meaningful likelihood computation.
- Evidence anchors:
  - [section 3.5] "we developed an ensemble-based evaluation to convert continuous maps to discrete conductivity values... we randomize error fractions using Dirichlet distribution and generate an ensemble of binary maps from each predicted continuous conductivity map."
  - [section 3.5] "Figure 5 (bottom center) displays log-likelihood scores as a kernel density plot... our INN model exhibits the least deviation, as confirmed by the averaged log-likelihood scores in Figure 5 (bottom right)."
  - [corpus] Weak evidence; corpus neighbors don't discuss error diffusion or similar evaluation techniques for inverse problems.
- Break condition: If the randomized error diffusion fails to generate representative binary ensembles or if the likelihood computation becomes unstable due to ensemble size or distribution assumptions.

### Mechanism 3
- Claim: INN's invertible coupling blocks with affine transformations enable efficient training and reconstruction while maintaining invertibility.
- Mechanism: The INN architecture uses learnable affine transformations (scaling and translation) within coupling blocks that don't need to be invertible themselves. These blocks split and transform input features alternately, with invertible 1Ã—1 convolutions to ensure each feature undergoes transformation. This allows efficient computation of both forward and inverse mappings.
- Core assumption: The coupling blocks with affine transformations can effectively learn the complex relationships between magnetic fields and conductivity while maintaining invertibility.
- Evidence anchors:
  - [section 2.3] "Our coupling blocks are learnable affine transformations, scaling and translation, such that these functions need not be invertible and can be represented by any neural network."
  - [section 2.3] "The architecture allows for easy recovery of the block's input from its output in the inverse direction, with minor architectural modifications ensuring invertibility."
  - [section 3.4] "Validation loss curves for different numbers of coupling blocks... Using only one coupling block leads to underfitting, while a higher number of blocks can cause overfitting."
  - [corpus] Weak evidence; corpus neighbors discuss different architectures but not specifically INN coupling blocks.
- Break condition: If the coupling blocks become too complex and lose invertibility, or if the number of blocks is not properly tuned leading to underfitting or overfitting.

## Foundational Learning

- Concept: Biot-Savart Law and magnetic field computation from current distributions
  - Why needed here: Understanding how magnetic fields are induced by current distributions in the electrolyzer is fundamental to formulating the forward problem and designing the inverse solution.
  - Quick check question: How does the Biot-Savart Law relate the current density in the electrolyzer to the magnetic field measured by external sensors?

- Concept: Inverse problems and ill-posedness in reconstruction
  - Why needed here: The problem of reconstructing high-resolution conductivity maps from sparse magnetic measurements is inherently ill-posed, requiring specialized techniques like INNs to handle the information loss and regularization.
  - Quick check question: Why is reconstructing conductivity maps from magnetic field measurements considered an ill-posed inverse problem?

- Concept: Neural network architectures for invertible mappings (INNs)
  - Why needed here: The core of the approach relies on INNs' ability to learn bijective mappings between low-dimensional measurements and high-dimensional distributions, which is essential for handling the ill-posed nature of the problem.
  - Quick check question: What key property of INNs makes them suitable for solving ill-posed inverse problems like conductivity reconstruction?

## Architecture Onboarding

- Component map: Data preprocessing -> Forward problem simulation -> INN model -> Latent variables -> Loss function -> Evaluation
- Critical path:
  1. Preprocess and standardize input data (magnetic field measurements and conductivity maps)
  2. Train INN model using the preprocessed data and reconstruction loss
  3. Generate predictions for new magnetic field measurements
  4. Apply randomized error diffusion to convert predictions to binary maps
  5. Compute likelihood scores to evaluate reconstruction quality

- Design tradeoffs:
  - Number of coupling blocks: More blocks increase model capacity but risk overfitting; fewer blocks may underfit
  - Sensor configuration: More sensors and closer proximity to the conducting plate improve reconstruction quality but may be impractical in real applications
  - Latent variable dimensionality: Higher dimensionality allows better representation of lost information but increases model complexity

- Failure signatures:
  - High validation loss with increasing coupling blocks indicates overfitting
  - Poor reconstruction quality even with optimal coupling blocks suggests insufficient information in magnetic measurements
  - Unstable likelihood scores during evaluation may indicate issues with randomized error diffusion implementation

- First 3 experiments:
  1. Train INN with varying numbers of coupling blocks (1, 3, 5) on the full dataset to identify optimal architecture
  2. Evaluate reconstruction quality on test data with different sensor configurations (varying number and distance)
  3. Implement and test the randomized error diffusion evaluation method on model predictions to verify its effectiveness

## Open Questions the Paper Calls Out
1. How do sensor noise levels and measurement inaccuracies impact the reconstruction quality of INN models compared to classical regularization approaches?
   - Basis: The paper mentions future work to perform experiments with noisy sensor measurements.
2. What is the impact of using higher-resolution conductivity maps on the performance of INN models versus classical approaches?
   - Basis: Future research directions include investigating INN performance on higher-resolution conductivity maps.
3. How does the distance between sensors and the conducting plate affect the information content available for reconstruction, and can this be quantified?
   - Basis: The ablation study shows performance differences at 5mm vs 25mm distances, but lacks quantitative analysis.

## Limitations
- Performance validated only on synthetic data without real-world noise or measurement uncertainties
- Randomized error diffusion evaluation method relies on assumptions about Dirichlet distribution parameters that weren't fully validated
- Comparison limited to traditional regularization methods, excluding more recent deep learning approaches for inverse problems

## Confidence
- High Confidence: INN architecture design and training methodology are well-established with consistent synthetic results
- Medium Confidence: Superiority over traditional methods demonstrated, but generalization to real-world conditions requires validation
- Low Confidence: Randomized error diffusion evaluation shows promise but needs extensive validation across different scenarios

## Next Checks
1. Test the INN model on experimentally acquired data from a physical water electrolyzer setup to validate performance under real-world conditions with noise and measurement uncertainties.
2. Compare the INN approach with state-of-the-art deep learning methods for inverse problems, such as physics-informed neural networks and variational autoencoders, to establish its relative performance.
3. Conduct sensitivity analysis on the randomized error diffusion parameters (Dirichlet distribution parameters, ensemble size) to determine their impact on evaluation reliability and identify optimal settings for different conductivity distributions.