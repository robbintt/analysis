---
ver: rpa2
title: Generalisable Agents for Neural Network Optimisation
arxiv_id: '2311.18598'
source_url: https://arxiv.org/abs/2311.18598
tags:
- learning
- ganno
- training
- rate
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents GANNO (Generalisable Agents for Neural Network
  Optimisation), a multi-agent reinforcement learning approach for dynamically scheduling
  hyperparameters during neural network training. The key idea is to use one RL agent
  per network layer, each observing local layer dynamics and taking actions to adjust
  learning rates layerwise.
---

# Generalisable Agents for Neural Network Optimisation

## Quick Facts
- arXiv ID: 2311.18598
- Source URL: https://arxiv.org/abs/2311.18598
- Reference count: 21
- Key outcome: GANNO achieves 1.0-4.0% accuracy improvements over manual scheduling on CIFAR-10 with deeper networks, using 1000x less compute than learned optimizers like VeLO

## Executive Summary
This paper introduces GANNO (Generalisable Agents for Neural Network Optimisation), a multi-agent reinforcement learning approach for dynamic hyperparameter scheduling during neural network training. The key innovation is using one RL agent per network layer, each observing local layer dynamics and adjusting learning rates layerwise. GANNO learns responsive schedules that are competitive with handcrafted heuristics like SGDR, showing strong robustness across different initial learning rates and weight decay values. The method generalizes successfully to deeper networks and harder datasets than seen during training, while being far more computationally efficient than learned optimizers.

## Method Summary
GANNO employs multi-agent reinforcement learning with one agent per network layer. Each agent receives both global observations (loss, accuracy) and layer-specific observations (weight norms, gradient norms, trust ratios) to take discrete actions that modify the layer's learning rate. Agents are trained using independent PPO with shared parameters across layers. The training loop runs τ=100 optimization steps per MARL timestep, requiring two full training passes (with and without actions) to compute difference rewards. The method uses a shared global reward based on validation accuracy to encourage collaborative optimization across layers.

## Key Results
- GANNO achieves 1.0-4.0% accuracy improvements over manual scheduling (SGDR, cosine decay) on CIFAR-10 with deeper networks
- Layerwise RL scheduling outperforms single-agent approaches and manual schedules across multiple initial learning rates and weight decay values
- GANNO generalizes to unseen deeper architectures (5-layer CNN, ResNet-9, ResNet-18) without retraining
- 1000x more computationally efficient than learned optimizers like VeLO while achieving competitive results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layerwise RL agents can adapt learning rates dynamically based on local gradient and weight statistics
- Mechanism: Each agent observes local features like layer weight norms, gradient norms, and trust ratios, then modifies the learning rate via discrete actions. This allows each layer to respond independently to its optimization state.
- Core assumption: Local layer dynamics are informative and actionable for improving global loss minimization
- Evidence anchors:
  - [abstract] "GANNO utilises an agent per layer that observes localised network dynamics and accordingly takes actions to adjust these dynamics at a layerwise level to collectively improve global performance."
  - [section] "Each agent receives a shared global observation along with a set of local observations specific to that agent's own layer."
- Break condition: If local observations become uninformative (e.g., gradients vanish), the agent cannot generate useful learning rate changes

### Mechanism 2
- Claim: Multi-agent coordination via shared reward encourages collaborative optimization
- Mechanism: All agents share a global reward based on validation accuracy. This encourages each agent to adjust its layer's learning rate in a way that benefits overall model performance rather than just local loss reduction.
- Core assumption: Shared reward signal is sufficient to align agent objectives toward global improvement
- Evidence anchors:
  - [abstract] "Each agent perceives its own observation ... and accordingly takes its own action ... receive a shared scalar reward"
  - [section] "To handle this problem, we leverage difference rewards ... instead of using a reward signal r(t) = R(s(t), a(t), s(t+1)), we shape the reward r(t) = R(s(t), a(t), s(t+1)) − R(s(t), ˜a, s(t+1))"
- Break condition: If reward shaping is insufficient, agents may exploit local metrics at the expense of global performance

### Mechanism 3
- Claim: Layerwise scheduling generalizes better than global scheduling because it can escape local minima per layer
- Mechanism: By adjusting learning rates individually, some layers can increase rates to escape sharp minima while others decrease to fine-tune, enabling more nuanced optimization landscapes
- Core assumption: Different layers benefit from different learning rates at different training stages
- Evidence anchors:
  - [section] "At this point in training, GANNO shows clear evidence of helping the neural network escape local optima, improving the test accuracy by several percentage points each time."
  - [section] "We see how the layerwise learning rates coordinate to achieve this."
- Break condition: If layers are too interdependent, layerwise scheduling may destabilize training

## Foundational Learning

- Concept: Multi-agent reinforcement learning (MARL)
  - Why needed here: GANNO uses one agent per layer, requiring coordination via shared reward
  - Quick check question: What distinguishes independent PPO from centralized training in MARL?

- Concept: Reinforcement learning reward shaping
  - Why needed here: The difference reward prevents agents from taking credit for baseline training progress
  - Quick check question: How does the difference reward isolate the agent's contribution to reward?

- Concept: Hyperparameter scheduling heuristics
  - Why needed here: GANNO aims to outperform manual schedules like SGDR and cosine decay
  - Quick check question: What is the key advantage of cosine one-cycle scheduling over constant learning rate?

## Architecture Onboarding

- Component map: Environment (neural network training loop with Adam optimizer) -> Agents (one PPO agent per layer with shared parameters) -> Observations (global metrics + layer-specific statistics) -> Actions (discrete learning rate modifications) -> Reward (validation accuracy difference) -> Training loop (τ steps per MARL timestep)

- Critical path: 1) Initialize neural network and GANNO agents; 2) At each MARL step: collect observations → agents select actions → apply actions → run τ training steps twice (with/without action) → compute difference reward → update PPO policies; 3) Repeat until MARL training complete

- Design tradeoffs: Shared vs. independent agent parameters (sharing reduces compute but may limit specialization); τ step size (larger τ reduces non-stationarity but slows learning; smaller τ increases responsiveness but risks instability); Discrete vs. continuous actions (discrete is simpler but may limit fine-grained control)

- Failure signatures: Agents learn to zero learning rates (poor reward shaping or insufficient exploration); High variance in layerwise schedules (reward signal too noisy or observations uninformative); Poor generalization (overfitting to training architecture/dataset)

- First 3 experiments: 1) Train GANNO on a two-layer CNN on Fashion-MNIST with varying initial learning rates; verify it learns sensible schedules and outperforms constant LR; 2) Evaluate the same trained agents on a deeper CNN (5 layers) on CIFAR-10; measure accuracy drop vs. manual schedules; 3) Compare layerwise GANNO to ablated single-agent version on the same task; confirm layerwise benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can GANNO be extended to control multiple hyperparameters simultaneously (e.g., both learning rate and weight decay)?
- Basis in paper: [explicit] The authors state "the framework can be extended to other hyperparameters of interest—e.g., future work could explore using GANNO to control both the learning rate and weight decay parameters simultaneously."
- Why unresolved: The paper only demonstrates GANNO controlling learning rate; no experiments or analysis exist for multi-parameter control
- What evidence would resolve it: Experiments showing GANNO successfully controlling both learning rate and weight decay together, with performance comparisons to single-parameter control

### Open Question 2
- Question: What specific modifications to GANNO's reward design could improve its ability to replicate effective warm-up schedules?
- Basis in paper: [explicit] The authors identify that "the challenge here rests in the tricky balance between venturing into high learning rate regions while maintaining learning stability" and suggest using manual schedules as demonstrations
- Why unresolved: The paper identifies the problem and suggests a potential solution direction, but doesn't implement or test any specific modifications
- What evidence would resolve it: Experiments testing different reward modifications (e.g., incorporating warm-up demonstration data, alternative reward shaping techniques) and showing improved warm-up performance

### Open Question 3
- Question: What is the relationship between the computational budget allocated to GANNO training and its generalization performance?
- Basis in paper: [explicit] The authors note "we want our approach to be viable without access to large compute" and compare their 50,000 timesteps to VeLO's "thousands of TPU months"
- Why unresolved: The paper doesn't systematically vary or analyze the impact of training budget on performance
- What evidence would resolve it: Systematic experiments varying training timesteps (e.g., 10K, 50K, 100K, 200K) and measuring generalization performance across different tasks and initial conditions

## Limitations

- Evaluation focuses primarily on vision datasets (Fashion-MNIST, CIFAR-10) and CNN architectures, limiting generalizability to other domains
- Method requires two full training passes per MARL step for difference reward computation, creating substantial computational overhead
- Layerwise scheduling assumes meaningful per-layer statistics are observable and actionable, which may not hold for architectures with complex parameter sharing

## Confidence

- **High**: GANNO learns responsive schedules that outperform manual scheduling on seen architectures (2-layer CNN)
- **Medium**: Layerwise coordination improves optimization vs. single-agent approaches (based on ablation studies)
- **Medium**: Generalization to deeper networks and harder datasets (CIFAR-10 with 5-layer CNN, ResNet-9/18) without retraining
- **Low**: Claims about escaping local optima are qualitative observations without rigorous analysis of loss landscape geometry

## Next Checks

1. **Ablation study on observation space**: Remove individual observations (weight norm, gradient norm, trust ratio) to identify which features are most critical for agent performance
2. **Architecture sensitivity analysis**: Test GANNO on non-CNN architectures (RNNs, Transformers) to verify generalizability beyond the evaluated scope
3. **Reward shaping comparison**: Replace difference reward with alternative shaping methods (potential-based, policy invariance-preserving) to assess sensitivity to this design choice