---
ver: rpa2
title: 'ADriver-I: A General World Model for Autonomous Driving'
arxiv_id: '2311.13549'
source_url: https://arxiv.org/abs/2311.13549
tags:
- control
- arxiv
- driving
- adriver-i
- future
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ADriver-I, a general world model for autonomous
  driving that combines a multimodal large language model (MLLM) with a video diffusion
  model (VDM). The core innovation is the "interleaved vision-action pair," which
  unifies visual features and control signals into a format suitable for both prediction
  and generation.
---

# ADriver-I: A General World Model for Autonomous Driving

## Quick Facts
- arXiv ID: 2311.13549
- Source URL: https://arxiv.org/abs/2311.13549
- Authors: 
- Reference count: 40
- Primary result: Mean L1 error of 0.072 m/s for speed and 0.091 rad for steering angle on nuScenes dataset

## Executive Summary
ADriver-I introduces a novel general world model for autonomous driving that integrates a multimodal large language model (MLLM) with a video diffusion model (VDM). The key innovation is the "interleaved vision-action pair" format that unifies visual features and control signals, enabling the MLLM to predict low-level control signals (speed and steering angle) from historical vision-action pairs while the VDM generates future frames conditioned on these predictions. The model demonstrates the ability to perform "infinite driving" through iterative prediction loops, achieving promising results on the nuScenes dataset with L1 errors of 0.072 m/s for speed and 0.091 rad for steering angle.

## Method Summary
ADDriver-I uses interleaved vision-action pairs as input format, where control signals (speed and steering angle) are converted to text-like expressions and paired with visual tokens. The MLLM (Vicuna-7B-1.5 with CLIP-ViT-Large encoder and visual adapter) predicts current control signals from historical vision-action pairs and current frame. The predicted actions are then used as prompts for the VDM (latent diffusion with temporal modules) to generate future frames. The system is trained in two stages: first training the MLLM on nuScenes dataset with supervised fine-tuning using cross-entropy loss, then training the VDM using latent diffusion conditioned on historical frames and control signal prompts. The iterative loop enables "infinite driving" by using generated frames as input for subsequent predictions.

## Key Results
- Achieves L1 error of 0.072 m/s for speed and 0.091 rad for steering angle on nuScenes dataset
- Generates future frames with FID score of 5.52 and FVD score of 97
- Demonstrates "infinite driving" capability through iterative prediction loops
- Shows effectiveness in predicting low-level control signals directly from visual input

## Why This Works (Mechanism)

### Mechanism 1: Interleaved Vision-Action Pair Format
- Claim: The interleaved vision-action pair format enables the MLLM to predict control signals autoregressively while maintaining alignment with visual context.
- Mechanism: By converting control signals (speed and steering angle) into text-like expressions and pairing them with visual tokens, the MLLM can process both modalities in a unified embedding space. This allows the model to reason about actions conditioned on visual history.
- Core assumption: The MLLM's language modeling capabilities can generalize to control signal prediction when formatted as text tokens.
- Evidence anchors:
  - [abstract]: "we first introduce the concept of interleaved vision-action pair, which unifies the format of visual features and control signals"
  - [section]: "The control signals, such as the steer angles and ego speed, can be converted into text-like expressions"
- Break condition: If the MLLM cannot effectively reason about control signals when represented as text tokens, prediction accuracy will degrade.

### Mechanism 2: VDM-Guided Future Scene Generation
- Claim: The video diffusion model (VDM) can generate future frames conditioned on predicted control signals and historical vision-action pairs.
- Mechanism: The VDM takes historical frames and the MLLM's predicted current action as input, using this as guidance to synthesize plausible future driving scenes. This creates a closed loop where predictions influence future visual context.
- Core assumption: The diffusion model can learn to generate realistic future frames when conditioned on control signals without requiring high-level priors like HD maps or 3D bounding boxes.
- Evidence anchors:
  - [abstract]: "The generated control signals together with the historical vision-action pairs are further conditioned to predict the future frames"
  - [section]: "The predicted action At is further used as the prompt of VDM to predict the next four frames"
- Break condition: If the VDM generates unrealistic or inconsistent future frames, the iterative prediction loop will fail.

### Mechanism 3: Iterative Prediction Loop
- Claim: The iterative prediction loop enables "infinite driving" by using generated frames as input for subsequent predictions.
- Mechanism: After predicting the current action and future frames, the next generated frame becomes the input for the next time step's MLLM prediction. This creates a self-contained driving simulation where the model can continue generating actions and scenes indefinitely.
- Core assumption: The model can maintain coherent behavior over extended sequences without access to real sensor data.
- Evidence anchors:
  - [abstract]: "With the predicted next frame, ADriver-I performs further control signal prediction. Such process can be repeated for infinite times"
  - [section]: "The generated next frame I′t+1 is served as the 'current frame' in the next timestamp and further input to MLLM to produce the A′t+1"
- Break condition: If prediction errors accumulate over time, the generated driving scenario will become unrealistic or diverge from reasonable behavior.

## Foundational Learning

- Concept: Multimodal embeddings and cross-modal reasoning
  - Why needed here: The system needs to process visual information and control signals together in a unified representation space
  - Quick check question: Can you explain how CLIP-ViT and LLM embeddings are aligned through the visual adapter?

- Concept: Autoregressive prediction and temporal modeling
  - Why needed here: Both MLLM and VDM need to predict future states based on historical sequences
  - Quick check question: How does the autoregressive nature of MLLM help in generating control signals for current frame?

- Concept: Diffusion models and generative modeling
  - Why needed here: VDM uses diffusion process to generate realistic future frames conditioned on control signals
  - Quick check question: What role does the temporal-awareness module play in the video diffusion model?

## Architecture Onboarding

- Component map: Historical vision-action pairs + current frame -> MLLM (Vicuna-7B-1.5 + CLIP-ViT-Large + visual adapter) -> Control signal prediction -> VDM (latent diffusion + temporal modules) -> Future frame generation -> Next iteration

- Critical path:
  1. Input: Historical vision-action pairs + current frame
  2. MLLM predicts current control signal
  3. VDM generates future frames conditioned on prediction
  4. Next frame feeds back to MLLM for next iteration

- Design tradeoffs:
  - Using text-based control signal representation enables MLLM reasoning but may lose precision compared to direct numerical prediction
  - Separate training for MLLM and VDM avoids end-to-end optimization complexity but may create misalignment
  - Generating only 4 future frames limits long-term planning capability

- Failure signatures:
  - Control signal predictions drifting from realistic values over time
  - Generated frames showing artifacts or unrealistic scenarios
  - Loss of coherence in the driving scenario as iterations progress

- First 3 experiments:
  1. Verify MLLM can predict control signals from vision-action pairs on held-out validation set
  2. Test VDM can generate realistic future frames conditioned on control signals and historical frames
  3. Run iterative prediction loop for 10-20 steps and measure performance degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance of ADriver-I be improved through end-to-end training of the MLLM and VDM components rather than training them separately?
- Basis in paper: [explicit] The paper mentions that "MLLM and VDM are trained separately and fails to enjoy the benefits from end-to-end optimization" and suggests that "A unified comprehension&generation framework is required to achieve this goal."
- Why unresolved: The paper does not explore the potential benefits or methods for end-to-end training of the MLLM and VDM components.
- What evidence would resolve it: Experiments comparing the performance of ADDriver-I with separately trained MLLM and VDM components versus an end-to-end trained version would provide evidence for the benefits of end-to-end training.

### Open Question 2
- Question: Can the routing information be incorporated into ADDriver-I to enable long-distance autonomous driving?
- Basis in paper: [explicit] The paper states that "from the perspective of driving distance, there lacks of the routing information, we may introduce the navigation map to achieve the long-distance autonomous driving."
- Why unresolved: The paper does not explore the potential benefits or methods for incorporating routing information into ADDriver-I.
- What evidence would resolve it: Experiments comparing the performance of ADDriver-I with and without routing information would provide evidence for the benefits of incorporating routing information.

### Open Question 3
- Question: How can the quality of the generated video frames by VDM be improved, especially when the control signals change quickly?
- Basis in paper: [inferred] The paper mentions that "VDM may generate some low-quality video frames especially when the control signals change quickly, disturbing the control signal prediction in the next timestamp."
- Why unresolved: The paper does not explore potential solutions to improve the quality of generated video frames when control signals change quickly.
- What evidence would resolve it: Experiments comparing the quality of generated video frames under different control signal change scenarios would provide evidence for the effectiveness of potential solutions.

## Limitations
- Architecture details for visual adapter between CLIP and LLM are not fully specified
- Limited analysis of how prediction errors accumulate over extended iterative loops
- No cross-dataset validation to assess generalization to different driving environments
- Numerical precision loss from text-based control signal representation not quantified

## Confidence
- High Confidence: The core concept of using interleaved vision-action pairs as a unified input format for multimodal reasoning is well-supported by the results
- Medium Confidence: The claim that this approach enables "general world modeling" is partially supported but limited by the lack of cross-dataset validation
- Low Confidence: The "infinite driving" capability claim is the least substantiated with no systematic analysis of long-term stability

## Next Checks
1. Conduct systematic experiments measuring how prediction errors in control signals and generated frames accumulate over 50-100 iterative steps
2. Evaluate the trained model on at least two additional driving datasets from different geographic regions and weather conditions
3. Compare prediction accuracy using different control signal representations (text tokens vs. direct numerical encoding vs. learned embeddings) to quantify precision loss from text-based representation