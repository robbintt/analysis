---
ver: rpa2
title: Modeling Empathic Similarity in Personal Narratives
arxiv_id: '2305.14246'
source_url: https://arxiv.org/abs/2305.14246
tags:
- stories
- similarity
- story
- empathic
- empathy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the task of modeling empathic similarity
  in personal narratives, focusing on how much two people would empathize with each
  other''s experiences. Using insights from social psychology, the authors operationalize
  empathic similarity through three key features of stories: main events, emotional
  trajectories, and morals or takeaways.'
---

# Modeling Empathic Similarity in Personal Narratives

## Quick Facts
- arXiv ID: 2305.14246
- Source URL: https://arxiv.org/abs/2305.14246
- Reference count: 30
- One-line primary result: Fine-tuning BART on EMPATHIC STORIES dataset improves empathic similarity prediction over semantic similarity baselines

## Executive Summary
This paper introduces the task of modeling empathic similarity in personal narratives, focusing on how much two people would empathize with each other's experiences. The authors operationalize empathic similarity through three key story features: main events, emotional trajectories, and morals. They create EMPATHIC STORIES, a dataset of 1,500 annotated stories and 2,000 story pairs. Fine-tuning a BART model on this dataset outperforms semantic similarity baselines on automated metrics and shows improved user empathy ratings in a study with 150 participants.

## Method Summary
The authors collected personal narratives from online sources, crowdsourced stories, and spoken narratives, then annotated them with main events, emotional trajectories, and morals. They created 2,000 pairs of stories annotated with empathic similarity scores. A BART model was fine-tuned on this dataset to predict empathic similarity between story pairs, with both bi-encoder and few-shot prompting approaches evaluated. Performance was measured using correlation metrics, retrieval accuracy, and a user study where participants rated empathy elicited by retrieved stories.

## Key Results
- Fine-tuned BART model outperformed semantic similarity baselines on correlation and retrieval metrics
- User study showed participants empathized significantly more with stories retrieved by the fine-tuned model
- F1 scores for empathic similarity prediction ranged from 0.48-0.62 across different model variants

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Empathic similarity is better captured by modeling story events, emotions, and morals rather than raw semantic similarity.
- Mechanism: The model learns to represent stories in a way that aligns with human judgments of empathic resonance by fine-tuning on paired story annotations that capture these three features.
- Core assumption: Human empathic resonance depends on alignment in events, emotions, and morals, not just semantic similarity.
- Evidence anchors: [abstract] "We craft a framework that operationalizes empathic similarity in terms of three key features of stories: main events, emotional trajectories, and overall morals or takeaways."

### Mechanism 2
- Claim: Fine-tuning a pre-trained model on a dataset of empathically similar and dissimilar story pairs improves performance on predicting empathic similarity compared to off-the-shelf semantic similarity methods.
- Mechanism: The fine-tuned model learns embeddings that capture the nuanced relationships between stories that humans use to judge empathic similarity, rather than just semantic overlap.
- Core assumption: The pre-trained model has enough capacity to learn the complex relationships between story features that humans use for empathic judgment.

### Mechanism 3
- Claim: Users empathize more with stories retrieved by the fine-tuned model compared to stories retrieved by semantic similarity.
- Mechanism: The fine-tuned model retrieves stories that are more likely to resonate with users on an emotional level, leading to increased empathy.
- Core assumption: The stories retrieved by the fine-tuned model are more likely to elicit empathic responses from users than stories retrieved by semantic similarity.

## Foundational Learning

- Concept: Empathic similarity
  - Why needed here: To understand the goal of the model - predicting how much two people would empathize with each other's experiences.
  - Quick check question: What are the three key features of stories that the framework uses to operationalize empathic similarity?

- Concept: Fine-tuning
  - Why needed here: To understand how the model is trained to predict empathic similarity - by fine-tuning a pre-trained model on a dataset of empathically similar and dissimilar story pairs.
  - Quick check question: What is the difference between the bi-encoder architecture used for fine-tuning and the few-shot prompting approach?

- Concept: User study
  - Why needed here: To understand how the effectiveness of the model is evaluated - by conducting a user study where participants read stories retrieved by the model and rate their empathy.
  - Quick check question: What were the three dimensions of empathy measured in the user study?

## Architecture Onboarding

- Component map: Data collection -> Annotation -> Model training -> User study
- Critical path: Data collection → Annotation → Model training → User study
- Design tradeoffs:
  - Using fine-tuning vs. few-shot prompting: Fine-tuning allows for more efficient retrieval at test time, but requires a labeled dataset. Few-shot prompting doesn't require a labeled dataset, but is expensive and inefficient for retrieval.
  - Using a bi-encoder architecture vs. a cross-encoder architecture: A bi-encoder architecture is more efficient for retrieval, but may not capture as much context as a cross-encoder architecture.
- Failure signatures:
  - Model doesn't outperform semantic similarity baselines: The model may not be learning the right representations for empathic similarity, or the dataset may not be representative enough.
  - Users don't empathize more with stories retrieved by the model: The model may not be retrieving stories that are likely to elicit empathic responses, or the user study may not be measuring empathy effectively.
- First 3 experiments:
  1. Fine-tune the model on a small subset of the dataset and evaluate its performance on a held-out test set.
  2. Compare the performance of the fine-tuned model to semantic similarity baselines on a retrieval task.
  3. Conduct a small-scale user study to validate the model's effectiveness in eliciting empathic responses.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the fine-tuned BART model compare to other transformer-based models like RoBERTa or T5 on the empathic similarity prediction task?
- Basis in paper: [inferred] The paper compares the performance of fine-tuned SBERT and BART models on the empathic similarity prediction task, but does not mention other transformer-based models like RoBERTa or T5.
- Why unresolved: The paper does not explore the performance of other transformer-based models on this specific task, which could provide additional insights into the effectiveness of different architectures for modeling empathic similarity.
- What evidence would resolve it: Empirical results comparing the performance of fine-tuned RoBERTa, T5, and BART models on the empathic similarity prediction task using the same dataset and evaluation metrics.

### Open Question 2
- Question: How do the empathic similarity annotations correlate with other subjective measures of empathy, such as self-reported empathy questionnaires or physiological measures like heart rate variability?
- Basis in paper: [inferred] The paper focuses on measuring empathic similarity through annotations of personal stories, but does not explore correlations with other subjective or objective measures of empathy.
- Why unresolved: The relationship between empathic similarity annotations and other measures of empathy remains unclear, which could provide additional validation for the proposed framework and annotations.
- What evidence would resolve it: Empirical results showing correlations between empathic similarity annotations and scores on established empathy questionnaires or physiological measures of empathy in a sample of participants.

### Open Question 3
- Question: How does the performance of the fine-tuned models vary across different domains or types of personal narratives, such as stories about romantic relationships, family, or trauma?
- Basis in paper: [inferred] The paper mentions that the dataset includes stories from various sources, but does not analyze the performance of the models across different themes or domains of personal narratives.
- Why unresolved: The effectiveness of the proposed models may vary depending on the content or context of the personal narratives, which could inform future research on domain-specific adaptations or applications.
- What evidence would resolve it: Empirical results showing the performance of the fine-tuned models on subsets of the dataset corresponding to different themes or domains of personal narratives, such as romantic relationships, family, or trauma.

## Limitations
- Generalizability of the empathic similarity framework across diverse populations and contexts is uncertain
- Model shows only modest improvements over semantic similarity baselines
- User study design limitations including self-reported measures and limited sample size

## Confidence
- High confidence: The methodology for creating the EMPATHIC STORIES dataset and the basic experimental design are sound and well-documented.
- Medium confidence: The model outperforms semantic similarity baselines, but the magnitude of improvement is modest and may not translate to meaningful real-world applications.
- Medium confidence: The user study demonstrates that the model retrieves stories that elicit more empathy than semantic similarity, but the effect size and generalizability are limited.

## Next Checks
1. Test the model's performance on stories from diverse cultural backgrounds to assess cross-cultural generalizability
2. Conduct a longitudinal study to evaluate whether repeated exposure to retrieved stories leads to sustained increases in empathy over time
3. Perform an ablation study to determine the relative contribution of each feature (events, emotions, morals) to the model's performance