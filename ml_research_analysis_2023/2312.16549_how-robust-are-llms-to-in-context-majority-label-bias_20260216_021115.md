---
ver: rpa2
title: How Robust are LLMs to In-Context Majority Label Bias?
arxiv_id: '2312.16549'
source_url: https://arxiv.org/abs/2312.16549
tags:
- label
- llms
- bias
- in-context
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how robust large language models (LLMs) are
  to majority label bias in in-context learning (ICL) for text classification tasks.
  The authors investigate whether LLMs can maintain high performance when the in-context
  examples are skewed towards certain labels.
---

# How Robust are LLMs to In-Context Majority Label Bias?

## Quick Facts
- arXiv ID: 2312.16549
- Source URL: https://arxiv.org/abs/2312.16549
- Reference count: 21
- Key outcome: This paper examines how robust large language models (LLMs) are to majority label bias in in-context learning (ICL) for text classification tasks. The authors investigate whether LLMs can maintain high performance when the in-context examples are skewed towards certain labels. Using binary and multi-class datasets (BoolQ, RTE, COVID-5G), they vary the label distribution in the prompt and measure weighted F1-score. They find that some models (e.g., Falcon-40B, MPT-30B) show high robustness (RB10 ~80-90%) to label bias, especially with task-specific instructions. Larger models and instruction-tuned models perform better, but sensitivity to instructions increases with model size. Overall, the study shows that while some LLMs are quite robust to majority label bias, robustness varies by model, task, and dataset complexity.

## Executive Summary
This paper investigates the robustness of large language models to majority label bias in in-context learning for text classification tasks. The authors find that model performance degrades as label distributions become more skewed in the prompt, with some models showing high robustness (RB10 ~80-90%) when task-specific instructions are provided. Larger models and instruction-tuned models demonstrate better performance and robustness, though they also show increased sensitivity to the presence or absence of instructions. The study highlights the importance of considering label distribution when deploying LLMs in real-world settings where the distribution of in-context examples may be imbalanced.

## Method Summary
The authors evaluate the robustness of open-source LLMs (OpenLlama-7B/13B, MPT-7B/30B, Falcon-40B) to majority label bias in in-context learning using three datasets: BoolQ (binary), RTE (binary), and COVID-5G (multi-class). They vary the proportion of each label in the prompt (e.g., 0-100% of one class) and measure weighted F1-score. The models are instruction-tuned using the OASST dataset. The RBK metric quantifies robustness by measuring the percentage of label distribution settings where performance stays within K% of maximum F1 (K=10). Prompts are tested with and without task-specific instructions to assess their impact on robustness.

## Key Results
- Falcon-40B and MPT-30B with instructions show highest robustness (RB10 ~80-90%) across all three datasets
- Larger models are more robust to majority label bias but also more sensitive to the presence of task instructions
- RB10 varies significantly by task complexity, with COVID-5G showing lower robustness than binary classification tasks
- Standard deviation of F1-score correlates with model size, with larger models showing more stable performance across label distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context learning performance is highly sensitive to the proportion of majority class examples in the prompt, with performance degrading as the label distribution becomes more skewed.
- Mechanism: The model learns from the distribution of in-context examples. When one label dominates the prompt, the model overweights that label's likelihood regardless of the input, leading to biased predictions.
- Core assumption: The model treats in-context examples as a joint distribution that informs both task understanding and label probabilities.
- Evidence anchors:
  - [abstract] "One such manifestation is majority label bias, which arises when the distribution of labeled examples in the in-context samples is skewed towards one or more specific classes making Large Language Models (LLMs) more prone to predict those labels."
  - [section] "Zhao et al. (2021) showed that the performance of ICL using LLMs for TC is susceptible to extreme class imbalance in the prompt."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.34, average citations=0.0. Weak citation signal, but high FMR suggests topical relevance.
- Break condition: When task-specific instructions are added, the model relies less on in-context examples for task understanding, reducing the impact of label distribution bias.

### Mechanism 2
- Claim: Larger models are more robust to majority label bias, but also more sensitive to the presence of instructions.
- Mechanism: Larger models have more parameters to learn complex patterns, allowing them to better separate task semantics from label distributions. However, they also overfit more to prompt structure, making them sensitive to the presence or absence of task instructions.
- Core assumption: Model size correlates with the ability to learn task-relevant patterns independently of label frequencies.
- Evidence anchors:
  - [section] "Falcon-40B w/ instruction and MPT-30B w/ instruction consistently performs the best across all three datasets, both in terms of the (mean, std. deviation) of F1-score and the RB10 metric."
  - [section] "This leads to another finding that larger LLMs are more sensitive to majority label bias in the absence of an informative task-specific prompt template."
  - [corpus] Weak citation signal but high FMR suggests topical relevance to instruction sensitivity.
- Break condition: For smaller models or when task instructions are absent, the bias effect becomes dominant regardless of model size.

### Mechanism 3
- Claim: RB10 metric is a better indicator of real-world robustness than standard deviation because it measures the consistency of peak performance across label distributions.
- Mechanism: RB10 captures the proportion of label distribution settings where performance stays within K% of maximum, which reflects practical deployment scenarios where label distribution is unknown.
- Core assumption: Practitioners care more about maintaining performance within an acceptable range than about average performance.
- Evidence anchors:
  - [section] "The RBK metric effectively measures the robustness of the peak model performance such that it doesn't deviate more than K% across different label distribution settings."
  - [section] "We report the RBK metric, not just the mean and standard deviation, because in a real-world setting, the critical question from a practitioner's perspective is to what extent a model can perform within an acceptable range given the skewness in the label distribution."
  - [corpus] Weak citation signal but relevant to practical evaluation metrics.
- Break condition: When K is set too high, RB10 becomes less discriminative and loses practical value.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: The paper's entire experimental setup relies on ICL, where the model learns from examples in the prompt rather than fine-tuning.
  - Quick check question: What is the key difference between ICL and traditional fine-tuning in terms of model adaptation?

- Concept: Weighted F1-score
  - Why needed here: The evaluation metric used to measure model performance across different label distributions.
  - Quick check question: Why is weighted F1-score preferred over accuracy when dealing with imbalanced label distributions?

- Concept: Label distribution bias
  - Why needed here: The paper investigates how skewed label distributions in the prompt affect model predictions.
  - Quick check question: How does label distribution bias differ from other forms of bias like recency bias or domain bias?

## Architecture Onboarding

- Component map:
  - Data loader -> Prompt generator -> LLM inference engine -> Evaluation pipeline

- Critical path:
  1. Load dataset → 2. Generate prompts with varying label distributions → 3. Run LLM inference → 4. Compute weighted F1 → 5. Calculate RB10 metric

- Design tradeoffs:
  - Model size vs. robustness: Larger models are more robust but computationally expensive
  - Instruction inclusion vs. sensitivity: Instructions improve robustness but increase sensitivity to their absence
  - Number of label distribution settings vs. runtime: More settings provide better RB10 estimates but increase computation time

- Failure signatures:
  - RB10 drops significantly when instructions are removed from larger models
  - Performance collapses at extreme label distributions (0% or 100% of one class)
  - Standard deviation increases when label distribution approaches imbalance

- First 3 experiments:
  1. Test Falcon-40B with instructions on BoolQ at (50% True, 50% False) distribution
  2. Test OpenLlama-7B without instructions on RTE at (100% Yes, 0% No) distribution
  3. Test MPT-30B with instructions on COVID-5G at (25%, 25%, 50%) distribution for (misinformation, counter-misinformation, irrelevant)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different fine-tuning strategies beyond instruction tuning affect the robustness of LLMs to majority label bias in in-context learning?
- Basis in paper: [inferred] The paper mentions future work on exploring PEFT (Parameter-Efficient Fine-Tuning Techniques) to address majority label bias in text classification.
- Why unresolved: The current study only examines instruction-tuned models and does not explore other fine-tuning methods that might impact robustness to label bias.
- What evidence would resolve it: Systematic comparison of various fine-tuning approaches (e.g., full fine-tuning, LoRA, QLoRA) on their ability to mitigate majority label bias across different model sizes and tasks.

### Open Question 2
- Question: What is the relationship between the pre-training corpus composition and a model's robustness to majority label bias?
- Basis in paper: [explicit] The paper notes that different model families (OpenLlama vs. MPT) show varying robustness, potentially due to differences in pre-training data and strategies.
- Why unresolved: While the paper observes performance differences between model families, it does not analyze how specific characteristics of the pre-training corpus (e.g., label distribution, domain coverage) influence robustness.
- What evidence would resolve it: Detailed analysis of pre-training data statistics and their correlation with robustness metrics across multiple model families.

### Open Question 3
- Question: How does majority label bias in in-context learning translate to real-world performance degradation in deployed systems?
- Basis in paper: [explicit] The authors discuss the practical implications of robustness boundaries (RB@K) for real-world deployment, noting that RB10 varies significantly across tasks and model sizes.
- Why unresolved: The paper provides controlled experiments but does not validate how these findings manifest in actual deployment scenarios with noisy, dynamic data.
- What evidence would resolve it: Longitudinal studies of model performance in production environments with varying label distributions and data quality.

## Limitations
- The study focuses on specific model families and datasets, limiting generalizability to other LLMs and text classification tasks
- The RB10 metric, while useful, may not capture all aspects of practical robustness, particularly for tasks requiring calibrated confidence scores
- The instruction-tuning process introduces additional complexity that may confound the effects of model size and architecture on robustness

## Confidence
- High confidence: Claims about relative performance differences between models (e.g., Falcon-40B outperforming smaller models, instruction-tuned models showing better robustness)
- Medium confidence: Generalizability of findings to other model families and task types not studied in the paper
- Low confidence: Claims about the specific mechanisms by which larger models achieve robustness, as these require deeper architectural analysis

## Next Checks
1. Replicate the experiments across a broader range of model families (including closed-source models like GPT-4 and Claude) to test the generalizability of size-based robustness patterns
2. Test the RB10 metric's sensitivity to different K values and compare it against alternative robustness metrics like worst-case F1 across label distributions
3. Conduct ablation studies to isolate the effects of instruction-tuning from model size by testing the same models with and without OASST instruction tuning on held-out datasets