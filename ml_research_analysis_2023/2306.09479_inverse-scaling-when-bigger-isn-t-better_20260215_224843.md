---
ver: rpa2
title: 'Inverse Scaling: When Bigger Isn''t Better'
arxiv_id: '2306.09479'
source_url: https://arxiv.org/abs/2306.09479
tags:
- scaling
- task
- inverse
- tasks
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents empirical evidence for inverse scaling in large
  language models (LMs), where task performance worsens as model size increases. Through
  a public contest, the authors collected 11 datasets demonstrating robust inverse
  scaling across multiple model series spanning over 5 orders of magnitude in training
  compute.
---

# Inverse Scaling: When Bigger Isn't Better

## Quick Facts
- arXiv ID: 2306.09479
- Source URL: https://arxiv.org/abs/2306.09479
- Reference count: 40
- Key outcome: Large language models show inverse scaling on 11 tasks, where performance degrades as model size increases

## Executive Summary
This paper presents empirical evidence that larger language models (LMs) can perform worse on certain tasks, contrary to the typical scaling expectations. Through a public contest, the authors collected 11 datasets demonstrating robust inverse scaling across multiple model series spanning over 5 orders of magnitude in training compute. The findings challenge the assumption that increasing model scale always leads to better performance, highlighting the need for more careful consideration of training data and objectives in LM development.

## Method Summary
The authors conducted a public contest with cash prizes to collect tasks demonstrating inverse scaling. Participants submitted formatted tasks with evaluation metrics, which were then evaluated across multiple model series (OpenAI GPT-3, Anthropic LM, DeepMind Gopher/Chinchilla) in both zero-shot and few-shot settings. The evaluation spanned models with training compute ranging from 10^18 to 10^23 FLOPs, allowing for comprehensive analysis of scaling trends.

## Key Results
- Inverse scaling was observed on 11 datasets collected through a public contest with 99 submissions
- The effect persisted across multiple model families spanning over 5 orders of magnitude in training compute
- Four potential causes were identified: memorization preference, training data imitation, spurious correlations, and misleading few-shot examples
- RLHF fine-tuning was shown to reverse inverse scaling trends on some tasks

## Why This Works (Mechanism)

### Mechanism 1: Memorization Trap Preference
- Claim: Larger models prefer repeating memorized sequences over following instructions when there's a conflict.
- Mechanism: The training objective optimizes for predicting the most likely continuation based on pretraining data. When a prompt contradicts common sequences from pretraining, larger models assign higher probability to the memorized continuation due to their stronger prior knowledge.
- Core assumption: The pretraining corpus contains sufficient examples of the memorized sequences that they become strong priors.
- Evidence anchors:
  - [abstract] "preference to repeat memorized sequences over following in-context instructions"
  - [section] "larger LMs become increasingly confident in the widely-adopted definitions of symbols and words"
  - [corpus] "Carlini et al. (2022), the authors demonstrate a log-linear relationship between model size and percentage of data memorized"
- Break condition: If the prompt contains sufficiently strong signals that override the memorized sequence, or if the model encounters the conflicting information frequently enough during training.

### Mechanism 2: Unintended Imitation of Training Data Patterns
- Claim: Models imitate undesirable patterns from training data that conflict with intended behavior.
- Mechanism: The language modeling objective requires predicting all text patterns equally well, including misinformation and logical fallacies that appear in training data. Larger models become better at imitating these patterns.
- Core assumption: Training data contains significant amounts of undesirable patterns that are common enough to be learned.
- Evidence anchors:
  - [abstract] "imitation of undesirable patterns in the training data"
  - [section] "misinformation is not what we want the model to do, even if the misinformation occurred often in the training data"
  - [corpus] "Lin et al. (2022) is a dataset of questions designed to be answered incorrectly by some humans due to a common misconception"
- Break condition: If the undesirable patterns are rare in training data or if additional training explicitly discourages imitating them.

### Mechanism 3: Spurious Correlation from Few-Shot Examples
- Claim: Models latch onto spurious correlations in few-shot examples rather than the true task.
- Mechanism: With limited examples, models can identify patterns that correlate with correct answers but aren't actually relevant to the task. Larger models are better at pattern matching and may overfit to these spurious correlations.
- Core assumption: Few-shot examples contain patterns that correlate with correct answers but don't generalize to the full task distribution.
- Evidence anchors:
  - [abstract] "correct but misleading few-shot demonstrations of the task"
  - [section] "because only a finite number of demonstrations can be given, there may be other tasks that are compatible with the few-shot examples"
  - [corpus] "Wang et al. (2022). Identifying and Mitigating Spurious Correlations for Improving Robustness in NLP Models"
- Break condition: If the few-shot examples are carefully curated to avoid spurious correlations, or if the model receives enough examples to learn the true task distribution.

## Foundational Learning

- Concept: Token prediction objective
  - Why needed here: Understanding why models optimize for predicting likely continuations explains why they prefer memorized sequences and imitate training data patterns.
  - Quick check question: Why might a model choose to output a commonly seen phrase instead of following an instruction that contradicts that phrase?

- Concept: In-context learning limitations
  - Why needed here: Explains why few-shot examples can create spurious correlations and why models might not properly generalize from limited demonstrations.
  - Quick check question: What happens when the patterns in few-shot examples don't match the full distribution of task examples?

- Concept: Scaling laws and model capacity
  - Why needed here: Understanding how larger models have more parameters and see more data helps explain why they show stronger memorization and pattern imitation.
  - Quick check question: How does increased model capacity affect the strength of learned priors from training data?

## Architecture Onboarding

- Component map: Embedding layer -> Transformer blocks with attention mechanisms -> Final projection layer to vocabulary logits

- Critical path: Input prompt processing through embeddings and attention layers -> Contextual representation generation -> Prediction of next token probabilities -> Selection of highest probability token based on learned patterns and memorized sequences

- Design tradeoffs: The architecture trades off between memorization capacity (larger models can store more patterns) and generalization ability (smaller models may be less biased by training data). The token prediction objective inherently favors likelihood over correctness, which can lead to inverse scaling behaviors.

- Failure signatures: Inverse scaling manifests as performance degradation on specific tasks as model size increases. Look for tasks where larger models consistently perform worse than smaller ones, particularly when tasks conflict with common training data patterns or when few-shot examples contain spurious correlations.

- First 3 experiments:
  1. Test memorization vs instruction following: Create prompts that instruct models to output text contradicting common memorized sequences, comparing performance across model sizes.
  2. Test spurious correlation detection: Design few-shot examples with clear spurious patterns and measure how performance changes with model size.
  3. Test training data pattern imitation: Create tasks that require avoiding common logical fallacies or misinformation patterns, evaluating whether larger models are more susceptible to these failures.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of large language models (LMs) on inverse scaling tasks change when provided with more diverse training data?
- Basis in paper: Inferred from the discussion of potential causes of inverse scaling, such as unwanted imitation of undesirable patterns in the training data and strong priors learned from pretraining text.
- Why unresolved: The paper does not explore the impact of training data diversity on inverse scaling trends. It focuses on evaluating tasks across various model series but does not investigate the effect of varying the training data itself.
- What evidence would resolve it: Conduct experiments training LMs on different subsets of training data (e.g., more diverse sources, different ratios of high-quality vs. low-quality data) and evaluate their performance on inverse scaling tasks. Compare scaling trends across these different training data configurations.

### Open Question 2
- Question: What is the relationship between model size and the susceptibility to prompt injection attacks?
- Basis in paper: The paper discusses prompt injection as a potential cause of inverse scaling and observes inverted-U scaling trends on the Prompt Injection task, where larger models become more susceptible to these attacks.
- Why unresolved: While the paper demonstrates inverse scaling on prompt injection tasks, it does not provide a detailed analysis of how model size specifically influences susceptibility to these attacks. The underlying mechanisms are not fully explored.
- What evidence would resolve it: Conduct a systematic study varying model size and analyzing the specific components or layers within the model that contribute to increased susceptibility to prompt injection. This could involve ablation studies or interpretability techniques to identify the relevant mechanisms.

### Open Question 3
- Question: How does instruction fine-tuning affect the scaling trends on inverse scaling tasks?
- Basis in paper: The paper mentions that instruction fine-tuning can exacerbate inverse scaling on some tasks, such as Resisting Correction, but it does not provide a comprehensive analysis of its effects across different tasks and model series.
- Why unresolved: The paper evaluates models with and without instruction fine-tuning but does not delve into the specific impact of fine-tuning on scaling trends. It remains unclear whether instruction fine-tuning consistently improves or worsens performance on inverse scaling tasks.
- What evidence would resolve it: Perform a controlled study fine-tuning models with different instruction sets and evaluating their performance on inverse scaling tasks. Compare scaling trends between models with and without instruction fine-tuning to identify patterns and understand the underlying mechanisms.

### Open Question 4
- Question: Can alternative training objectives or regularization techniques mitigate inverse scaling?
- Basis in paper: The paper mentions that RLHF and human preference-based pretraining objectives can reverse inverse scaling trends on some tasks, suggesting that alternative training methods may help mitigate inverse scaling.
- Why unresolved: While the paper provides examples of successful mitigation strategies, it does not explore a wide range of alternative training objectives or regularization techniques. It remains unclear which methods are most effective and under what conditions.
- What evidence would resolve it: Experiment with various training objectives (e.g., different reward functions, regularization techniques) and evaluate their impact on scaling trends across a diverse set of inverse scaling tasks. Compare the effectiveness of different approaches and identify the most promising strategies for mitigating inverse scaling.

## Limitations

- Contest-based data collection may introduce selection bias, as participants likely focused on finding the most dramatic examples of inverse scaling
- The paper does not provide theoretical explanations for why inverse scaling occurs in some tasks but not others
- Most submissions were tested on only 3-4 model sizes, making it difficult to establish robust scaling trends
- Private model series may have different training procedures that confound the pure scaling effects being studied

## Confidence

- **High Confidence**: The empirical observation that inverse scaling occurs across multiple independent submissions and model families
- **Medium Confidence**: The proposed mechanisms explaining inverse scaling
- **Low Confidence**: The generalizability of these findings to all language model scaling scenarios

## Next Checks

1. **Systematic Mechanism Isolation**: Design controlled experiments that can definitively distinguish between the four proposed mechanisms by varying task characteristics.

2. **Cross-Domain Generalization**: Test whether inverse scaling phenomena persist in domains outside the contest submissions, particularly in domains with strong theoretical expectations about model behavior.

3. **Architecture-Agnostic Scaling**: Evaluate whether similar inverse scaling patterns emerge in non-transformer architectures (RNNs, CNNs) and in smaller models to determine if this is a fundamental property of sequence modeling or specific to large transformers.