---
ver: rpa2
title: Towards a Unified Framework for Sequential Decision Making
arxiv_id: '2310.02167'
source_url: https://arxiv.org/abs/2310.02167
tags:
- policy
- which
- policies
- quality
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a unified framework for sequential decision
  making (SDM) that encompasses both Automated Planning (AP) and Reinforcement Learning
  (RL). The key contributions are: 1) A novel formulation of SDM tasks as sets of
  training and test Markov Decision Processes (MDPs), which accounts for generalization
  across different MDPs.'
---

# Towards a Unified Framework for Sequential Decision Making

## Quick Facts
- arXiv ID: 2310.02167
- Source URL: https://arxiv.org/abs/2310.02167
- Reference count: 5
- Key outcome: Proposes a unified framework for sequential decision making encompassing Automated Planning and Reinforcement Learning through MDP-based task formulation and generalized algorithm properties

## Executive Summary
This paper introduces a unified framework for Sequential Decision Making (SDM) that bridges Automated Planning (AP) and Reinforcement Learning (RL). The framework formulates SDM tasks as sets of training and test Markov Decision Processes (MDPs), enabling generalization across different MDPs. It proposes a general algorithm for SDM consisting of four iterative steps: initialization, policy sampling and evaluation, probability update, and update propagation. The paper defines key properties for evaluating SDM algorithms including knowledge leverage, efficiency, and solution quality, along with methods to calculate task difficulty and MDP distances using probability theory concepts.

## Method Summary
The framework formulates SDM tasks as tuples of training and test MDPs, with solutions represented as probability distributions over policies. A general SDM algorithm iteratively improves solution estimates through four steps: initialization to prior distribution, policy sampling and evaluation, policy probability update via Bayesian inference, and update propagation. Task difficulty is measured as the sum of computational effort (training MDP difficulty) and generalization effort (distance between training and test MDPs, calculated via total variation distance). The framework provides formulas to calculate three key algorithm properties: knowledge leverage (how much the prior, scoring, and similarity functions reduce difficulty), efficiency (time and space complexity), and quality (solution performance on test MDPs).

## Key Results
- Provides a general SDM algorithm template that every SDM method implicitly or explicitly follows
- Defines task difficulty as sum of computational effort and generalization effort using total variation distance
- Characterizes SDM algorithms by three properties: knowledge leveraged, efficiency, and solution quality
- Introduces measures for calculating difficulty of tasks and distance between MDPs
- Establishes a common foundation for understanding and comparing different SDM methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Every SDM algorithm can be formulated as a procedure that iteratively improves its estimate of the task solution by leveraging task knowledge.
- Mechanism: The paper proposes a general algorithm for SDM composed of four main steps: initialization to prior distribution, policy sampling and evaluation, policy probability update, and update propagation. This algorithm serves as a template that every SDM method implicitly or explicitly follows.
- Core assumption: All SDM methods, regardless of their specific implementation details, share these four abstract steps in their core operation.
- Evidence anchors:
  - [abstract] "We provide a general algorithm for SDM which we hypothesize every SDM method is based on. According to it, every SDM algorithm can be seen as a procedure that iteratively improves its solution estimate by leveraging the task knowledge available."
  - [section] "The main hypothesis of this work is that every SDM method is based on this general algorithm and implements steps 1 to 5 either explicitly or implicitly."
- Break condition: If an SDM algorithm exists that cannot be decomposed into these four steps or requires fundamentally different operations not captured by this framework.

### Mechanism 2
- Claim: The difficulty of an SDM task can be measured as the sum of computational effort and generalization effort.
- Mechanism: The paper defines task difficulty DT as the sum of the difficulty of training MDPs (DMtrain) and the distance between training and test MDPs (d(Mtrain, Mtest)). This is calculated using total variation distance between probability distributions.
- Core assumption: The solution of an MDP can be represented as a probability distribution over policies, and task difficulty can be quantified by comparing this distribution to a uniform distribution.
- Evidence anchors:
  - [section] "We define the difficulty of a task as the sum of its computational effort, i.e., how hard it is to find a policy π that solves Mtrain, and its generalization effort, i.e., how hard is for π to generalize to Mtest."
  - [section] "We can calculate the difficulty DT of an SDM task T = (Mtrain, Mtest) as the sum of its computational effort, which is equal to the difficulty DMtrain of Mtrain, and its generalization effort, which is equal to the distance between Mtrain and Mtest."
- Break condition: If the total variation distance between probability distributions does not adequately capture the intuitive notion of task difficulty, or if other factors beyond computational and generalization effort contribute significantly to difficulty.

### Mechanism 3
- Claim: SDM algorithms can be characterized by three properties: quantity of knowledge leveraged, efficiency, and quality of the solution obtained.
- Mechanism: The paper provides formulas and algorithms to calculate these properties for any SDM algorithm, enabling fair comparison between different approaches. Quantity of knowledge is measured by how much the prior distribution, scoring function, and similarity function reduce task difficulty. Efficiency is measured in terms of time and space complexity. Quality is measured by the solution's performance on test MDPs.
- Core assumption: These three properties capture the essential characteristics needed to evaluate and compare SDM algorithms, regardless of their specific implementation.
- Evidence anchors:
  - [section] "We believe SDM algorithms can be characterized according to three main properties: the quantity of knowledge they leverage to solve the task, how efficiently they solve it and the quality of the solution obtained."
  - [section] "In this section, we describe these three properties and provide several formulas and algorithms to calculate them. By doing this, we hope to provide a set of tools to evaluate and compare different SDM algorithms (e.g., from AP and RL) in a fair manner, in order to assess which one works better for a particular task."
- Break condition: If there exist important properties of SDM algorithms that are not captured by these three categories, or if the provided formulas and algorithms do not adequately measure these properties.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The paper formulates SDM tasks as sets of training and test MDPs, and the solution of an MDP is defined as a probability distribution over policies. Understanding MDPs is fundamental to understanding the framework.
  - Quick check question: What are the key components of a Constrained Stochastic Shortest Path MDP (CSSP-MDP) as defined in the paper?

- Concept: Probability Theory and Bayesian inference
  - Why needed here: The framework draws heavily on concepts from Probability Theory and Bayesian inference to formulate SDM tasks and algorithms. The solution of an MDP is represented as a probability distribution, and Bayes Theorem is used to update policy probabilities.
  - Quick check question: How does Bayes Theorem relate to the policy probability update step in the general SDM algorithm?

- Concept: Total variation distance
  - Why needed here: The paper uses total variation distance to measure the distance between probability distributions, which is used to calculate task difficulty and the distance between MDPs.
  - Quick check question: What does it mean for two probability distributions to have a small total variation distance?

## Architecture Onboarding

- Component map: Task formulation -> General algorithm -> Properties calculation -> Empirical evaluation
- Critical path: Task formulation → General algorithm → Properties calculation → Empirical evaluation and comparison
- Design tradeoffs:
  - Expressiveness vs. simplicity: The framework aims to be general enough to encompass AP, RL, and hybrid methods while remaining simple enough to be practical
  - Computational cost vs. accuracy: The formulas for calculating properties rely on Monte Carlo sampling, which can be computationally expensive but provides good approximations
  - Abstraction vs. specificity: The general algorithm is abstract and may not capture all implementation details of specific SDM methods
- Failure signatures:
  - If an SDM algorithm cannot be decomposed into the four steps of the general algorithm, the framework may not apply
  - If the total variation distance does not adequately capture task difficulty or MDP distance, the property calculations may be inaccurate
  - If the three properties (knowledge, efficiency, quality) do not adequately characterize SDM algorithms, the evaluation and comparison tools may be incomplete
- First 3 experiments:
  1. Implement the general SDM algorithm with a simple prior distribution and scoring function, and test it on a small set of training and test MDPs
  2. Calculate the difficulty of a set of MDPs using the provided formulas and compare it to an intuitive notion of difficulty
  3. Calculate the three properties (knowledge, efficiency, quality) for a simple SDM algorithm and verify that they provide meaningful characterizations of the algorithm's behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the general SDM algorithm be adapted for tasks with continuous state and action spaces?
- Basis in paper: [explicit] The paper discusses a general SDM algorithm but focuses on discrete MDP formulations. It mentions that different policies may require different encodings of inputs, including potentially continuous representations, but does not provide specific details on adapting the algorithm for continuous spaces.
- Why unresolved: The paper primarily focuses on discrete MDP formulations and does not provide specific details on adapting the general SDM algorithm for tasks with continuous state and action spaces. This is an important open question as many real-world SDM tasks involve continuous spaces.
- What evidence would resolve it: A detailed description of how to adapt the general SDM algorithm for continuous spaces, including modifications to policy sampling, evaluation, and update steps, as well as any necessary changes to the similarity function.

### Open Question 2
- Question: How can the proposed framework be extended to handle partially observable MDPs (POMDPs)?
- Basis in paper: [inferred] The paper focuses on MDP formulations but does not explicitly discuss POMDPs. However, the concept of context-aware policies suggests that additional information beyond the current state could be incorporated, which is relevant for POMDPs where the true state is not fully observable.
- Why unresolved: The paper does not provide specific details on how to extend the framework to handle POMDPs, which are more general and realistic models of many real-world decision-making problems.
- What evidence would resolve it: A formal extension of the framework to handle POMDPs, including modifications to the task formulation, policy definitions, and the general SDM algorithm to account for partial observability.

### Open Question 3
- Question: How can the proposed properties and measures be used to automatically design or select SDM algorithms for specific tasks?
- Basis in paper: [explicit] The paper introduces several properties of SDM tasks and algorithms, such as difficulty, knowledge leverage, efficiency, and solution quality, along with formulas and algorithms to calculate them. It suggests that these properties could be used to evaluate and compare SDM algorithms, but does not discuss their use in algorithm design or selection.
- Why unresolved: While the paper provides a framework for measuring and comparing SDM algorithms, it does not explore how these measures could be used to automatically design or select algorithms for specific tasks.
- What evidence would resolve it: A method or framework that leverages the proposed properties and measures to automatically design or select SDM algorithms based on the characteristics of a given task, potentially involving meta-learning or automated algorithm configuration techniques.

## Limitations

- The framework's scalability to large state and action spaces remains untested, as it relies on probability distributions over policies which may become computationally intractable
- The claim that all SDM algorithms can be decomposed into the four-step general algorithm has not been empirically validated across diverse algorithm classes
- The property calculation methods depend on Monte Carlo sampling which may introduce significant computational overhead and approximation errors

## Confidence

- Framework unification claim: Medium confidence - mathematically sound but requires empirical validation across diverse domains
- Algorithm generalizability: Medium confidence - abstract formulation may not capture all specialized operations in complex algorithms
- Property measurement accuracy: Medium confidence - sampling-based calculations may suffer from approximation errors and computational costs

## Next Checks

1. **Empirical Stress Test**: Implement the framework on a benchmark suite containing problems from classical planning, deep RL, and hybrid domains. Measure whether the calculated properties (knowledge, efficiency, quality) correlate with actual performance across these diverse settings.

2. **Algorithm Coverage Analysis**: Systematically review 10-15 representative SDM algorithms from AP, RL, and hybrid approaches. Document which steps of the general algorithm each method implements, identifying any systematic gaps or extensions needed.

3. **Scalability Assessment**: Test the property calculation methods on MDPs with increasing state and action space sizes. Measure computation time and accuracy degradation, particularly for the Monte Carlo sampling-based estimates of policy distributions and task difficulty.