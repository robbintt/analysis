---
ver: rpa2
title: 'Iterative Preference Learning from Human Feedback: Bridging Theory and Practice
  for RLHF under KL-Constraint'
arxiv_id: '2312.11456'
source_url: https://arxiv.org/abs/2312.11456
tags:
- reward
- arxiv
- policy
- learning
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a theoretical analysis of the reverse-KL regularized
  contextual bandit problem for RLHF. It investigates both offline and online settings,
  proposing efficient algorithms with finite-sample guarantees.
---

# Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint

## Quick Facts
- arXiv ID: 2312.11456
- Source URL: https://arxiv.org/abs/2312.11456
- Reference count: 40
- Key outcome: Novel theoretical framework for RLHF under KL-constraint with finite-sample guarantees, introducing algorithms that bridge theory and practice

## Executive Summary
This paper presents a comprehensive theoretical framework for Reinforcement Learning from Human Feedback (RLHF) by formulating it as a reverse-KL regularized contextual bandit problem. The authors develop efficient algorithms for both offline and online settings, providing finite-sample guarantees and connecting theoretical insights to practical implementations like DPO and RSO. The work addresses key challenges in RLHF, including reward modeling from preferences, uncertainty handling, and KL regularization, while introducing a novel GSHF algorithm that extends beyond existing methods.

## Method Summary
The method implements RLHF through a reverse-KL regularized contextual bandit framework, using Bradley-Terry preference modeling and Gibbs sampling for policy improvement. The approach handles both offline and online settings with distinct algorithms - offline learning uses pessimistic estimation with confidence sets, while online learning employs a non-symmetric policy structure with an enhancer agent. The core algorithm (GSHF) iteratively improves the reward model from preference data and updates the policy through multi-step approximation and rejection sampling, maintaining a KL constraint to preserve diversity from the pretrained model.

## Key Results
- Theoretical framework connecting RLHF to reverse-KL regularized contextual bandits with finite-sample guarantees
- Novel algorithms for offline and online RLHF with KL regularization that outperform existing baselines
- Introduction of GSHF algorithm that extends beyond DPO and RSO through multi-step approximation and online capability
- Demonstrated connection between theoretical foundations and practical algorithms like DPO and RSO

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KL-regularized contextual bandit formulation captures diversity-fidelity trade-off in RLHF
- Mechanism: The reverse-KL penalty prevents the policy from moving too far from the pretrained model, maintaining diversity while optimizing for human preferences
- Core assumption: The pretrained model contains valuable diversity that should be preserved during alignment
- Evidence anchors:
  - [abstract]: "The KL regularized contextual bandit additionally imposes a constraint that the optimal policy cannot move too far away from the original policy"
  - [section]: "it is important to model diversity and high fidelity in the theoretical framework beyond the reward"
  - [corpus]: Weak - corpus papers focus on corruption/overoptimization rather than KL regularization mechanisms
- Break condition: If the pretrained model is already biased or contains harmful content, the KL penalty would preserve these flaws

### Mechanism 2
- Claim: Pessimistic estimation handles reward model uncertainty in offline RLHF
- Mechanism: The algorithm penalizes uncertainty in the reward estimate by adding a term proportional to the norm of feature differences
- Core assumption: The offline dataset provides incomplete coverage of the true policy space
- Evidence anchors:
  - [abstract]: "we do not maintain a confidence set but use a modified target that is biased toward pessimism by penalizing the uncertainty"
  - [section]: "The algorithm for offline learning is presented in Algorithm 1, which is based on the principle of pessimism"
  - [corpus]: Missing - corpus papers don't discuss pessimism mechanisms in detail
- Break condition: When the offline dataset has sufficient coverage and the reward model is accurate, pessimism becomes unnecessarily conservative

### Mechanism 3
- Claim: Enhancer policy maximizes uncertainty for exploration in online RLHF
- Mechanism: The enhancer policy is designed to maximize the uncertainty of feature differences, facilitating learning of the main policy
- Core assumption: Non-symmetric policy structure (main agent + enhancer) improves sample efficiency
- Evidence anchors:
  - [abstract]: "the second agent π2t, referred to as the enhancer, seeks to maximize the uncertainty for the fixed π1t"
  - [section]: "Our algorithmic idea is built on optimism and non-symmetric structures, similar to the study of two-player zero-sum Markov game"
  - [corpus]: Weak - corpus mentions online RLHF but doesn't detail the enhancer mechanism
- Break condition: When the uncertainty becomes negligible (good reward model), the enhancer provides diminishing returns

## Foundational Learning

- Concept: Bradley-Terry model for preference learning
  - Why needed here: Forms the basis for modeling human preference data as pairwise comparisons
  - Quick check question: How does the Bradley-Terry model transform preference probabilities into reward signals?

- Concept: KL divergence and its role in regularization
  - Why needed here: The KL penalty balances reward optimization with maintaining policy diversity
  - Quick check question: What happens to the optimal policy as the KL coefficient η approaches zero or infinity?

- Concept: Confidence set construction for linear bandits
  - Why needed here: Enables uncertainty quantification for feature differences in the contextual bandit setting
  - Quick check question: How does the elliptical potential lemma bound the cumulative uncertainty?

## Architecture Onboarding

- Component map:
  - Reward modeling (MLE from preference data) -> Uncertainty estimation (covariance matrix-based) -> Policy improvement oracle (Gibbs distribution computation) -> Main policy and enhancer policy -> Confidence set construction

- Critical path: Offline/Online data → Reward MLE → Uncertainty estimation → Policy optimization → Policy improvement

- Design tradeoffs:
  - Pessimism vs optimism: Conservative offline vs aggressive online exploration
  - Enhancer complexity vs sample efficiency gains
  - KL coefficient tuning vs stability/robustness

- Failure signatures:
  - High rejection rates in rejection sampling indicate poor proposal distribution
  - Instability in policy updates suggests improper KL coefficient
  - Convergence to suboptimal policy indicates insufficient exploration

- First 3 experiments:
  1. Implement Algorithm 1 with Option I on synthetic preference data to verify pessimism works
  2. Test Algorithm 2 with m=1 to observe enhancer policy behavior in sequential setting
  3. Compare GSHF (Algorithm 4) with standard DPO on a small LLM alignment task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical limits of RLHF sample efficiency under KL-regularization?
- Basis in paper: [explicit] The paper discusses the sample complexity of their algorithms and mentions that the reward modeling step has lower sample dependency than direct generative modeling.
- Why unresolved: The paper provides bounds for specific algorithms but does not explore the fundamental limits of what is achievable in this setting.
- What evidence would resolve it: A lower bound proof showing the minimum number of samples required to achieve a certain performance level in RLHF with KL-regularization.

### Open Question 2
- Question: How does the choice of KL regularization strength (η) affect the trade-off between reward maximization and maintaining the original model distribution?
- Basis in paper: [explicit] The paper discusses the role of η in the KL-regularized objective and mentions that choosing an appropriate value for η is crucial in the RLHF framework.
- Why unresolved: The paper does not provide a systematic study of how different η values affect the performance of RLHF algorithms or how to choose the optimal η.
- What evidence would resolve it: Empirical studies varying η across a wide range of values and theoretical analysis of the impact of η on the regret bounds or performance guarantees.

### Open Question 3
- Question: Can the theoretical framework be extended to handle more complex preference models beyond the Bradley-Terry model?
- Basis in paper: [explicit] The paper states that their analysis is based on the Bradley-Terry model and mentions that considering more general feedback models is an interesting direction.
- Why unresolved: The paper does not explore alternative preference models or discuss how the theoretical results would change under different models.
- What evidence would resolve it: Extending the theoretical analysis to other preference models (e.g., Thurstone model, Plackett-Luce model) and comparing the results to the Bradley-Terry case.

## Limitations
- Theoretical analysis assumes linear reward models and feature representations that may not capture real-world complexity
- Elliptical potential lemma-based confidence sets may be overly conservative in practice
- Pessimism mechanism could lead to suboptimal performance when offline dataset has reasonable coverage
- Non-symmetric policy structure with enhancer policies adds implementation complexity

## Confidence
- High Confidence: The formulation of RLHF as a reverse-KL regularized contextual bandit problem and the connection to existing practical algorithms (DPO, RSO) are well-supported by the theoretical analysis and empirical results.
- Medium Confidence: The finite-sample guarantees for both offline and online algorithms are theoretically sound, but their practical applicability depends on how well the linear assumptions hold in real-world scenarios.
- Low Confidence: The effectiveness of the enhancer policy mechanism in significantly improving sample efficiency compared to symmetric policy approaches requires more extensive empirical validation across diverse task domains.

## Next Checks
1. **Linear Assumption Validation**: Test the algorithms with non-linear reward models (e.g., neural networks) to assess the robustness of theoretical guarantees beyond the linear setting.
2. **Dataset Coverage Analysis**: Systematically vary the coverage of the offline dataset and measure how pessimism affects performance across different levels of dataset completeness.
3. **Enhancer Policy Ablation**: Compare the full algorithm with variants that use symmetric policy structures or no enhancer policy to quantify the actual contribution of the non-symmetric design to sample efficiency gains.