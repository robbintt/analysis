---
ver: rpa2
title: Knowledge Gradient for Multi-Objective Bayesian Optimization with Decoupled
  Evaluations
arxiv_id: '2302.01310'
source_url: https://arxiv.org/abs/2302.01310
tags:
- c-mokg
- objectives
- objective
- optimization
- almost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multi-objective Bayesian optimization where
  objectives can be evaluated separately with different costs. The authors propose
  a cost-weighted multi-objective knowledge gradient (C-MOKG) acquisition function
  that exploits this separability by evaluating cheaper objectives more frequently.
---

# Knowledge Gradient for Multi-Objective Bayesian Optimization with Decoupled Evaluations

## Quick Facts
- arXiv ID: 2302.01310
- Source URL: https://arxiv.org/abs/2302.01310
- Authors: [Not specified in source]
- Reference count: 40
- One-line primary result: Cost-weighted multi-objective knowledge gradient (C-MOKG) acquisition function that exploits separable evaluations of objectives with different costs, showing superior performance on synthetic bi-objective problems.

## Executive Summary
This paper addresses multi-objective Bayesian optimization where objectives can be evaluated separately with different costs. The authors propose a cost-weighted multi-objective knowledge gradient (C-MOKG) acquisition function that exploits this separability by evaluating cheaper objectives more frequently. The method extends knowledge gradient to multi-objective problems using scalarization and expectation over scalarization weights. Theoretical results prove asymptotic consistency of the algorithm. Experiments on synthetic bi-objective problems show C-MOKG significantly outperforms a benchmark that always evaluates both objectives, particularly when the cheaper objective is harder to learn.

## Method Summary
The method proposes cost-weighted multi-objective knowledge gradient (C-MOKG) for scenarios where objectives can be evaluated separately with different costs. The algorithm uses independent Gaussian processes for each objective, applies linear scalarization with weights from the standard simplex, and computes knowledge gradient acquisition values. The key innovation is dividing the knowledge gradient by the evaluation cost of each objective to implement a "value per unit cost" metric. The method uses expectation over scalarization weights rather than random scalarizations when objectives are evaluated separately. Theoretical analysis proves asymptotic consistency, and experiments on synthetic bi-objective problems demonstrate significant performance improvements over benchmarks that always evaluate both objectives.

## Key Results
- C-MOKG significantly outperforms benchmarks that always evaluate both objectives, particularly when the cheaper objective is harder to learn
- Taking expectation over scalarizations rather than random scalarizations provides substantial performance improvements when objectives are evaluated separately
- Theoretical results prove asymptotic consistency of the algorithm for both random scalarizations and expectation over scalarizations
- The cost weighting mechanism effectively prioritizes evaluation of cheaper objectives, accelerating convergence to the Pareto front

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cost-weighted multi-objective knowledge gradient (C-MOKG) prioritizes evaluation of cheaper objectives more frequently, accelerating convergence to the Pareto front.
- Mechanism: By dividing the MOKG acquisition function by the evaluation cost of each objective, C-MOKG effectively implements a "value per unit cost" metric. This enables the algorithm to allocate more evaluations to cheaper objectives, particularly when they are harder to learn (e.g., have shorter length scales or higher observation noise).
- Core assumption: The algorithm can accurately model the objectives with independent Gaussian processes and the evaluation costs are known and constant.
- Evidence anchors:
  - [abstract] "The method extends knowledge gradient to multi-objective problems using scalarization and expectation over scalarization weights."
  - [section 3.1] "In the context of objectives with different evaluation costs, for example when one objective takes significantly longer to evaluate than another, we divide the MOKG by the cost to evaluate the proposed objective to give a value-per-unit-cost."
- Break condition: If the evaluation costs are not known or vary significantly across the input space, the cost weighting would become inaccurate and the algorithm would lose its efficiency advantage.

### Mechanism 2
- Claim: Taking expectation over scalarizations, rather than using random scalarizations, improves performance when objectives can be evaluated separately.
- Mechanism: Expectation over scalarizations considers all possible trade-offs between objectives at each step, allowing the algorithm to focus on areas of the Pareto front that are still poorly understood. This avoids wasting evaluations on parts of the Pareto front that are already well known.
- Core assumption: The distribution of scalarization weights (uniform over the standard simplex) accurately represents the decision maker's preferences or lack thereof.
- Evidence anchors:
  - [abstract] "The paper also demonstrates the importance of using expectation over scalarizations rather than random scalarizations when objectives are evaluated separately."
  - [section 4.5] "The results in Figure 3 show a big difference between using expectation over scalarizations (solid) and using random scalarizations (dashed) when objectives are evaluated separately (blue)."
- Break condition: If the decision maker has strong, specific preferences over the Pareto front that are not captured by a uniform distribution, expectation over scalarizations may not be optimal.

### Mechanism 3
- Claim: C-MOKG converges to the global optimum of the scalarized objective function as the budget increases, ensuring asymptotic consistency.
- Mechanism: The knowledge gradient acquisition function inherently encourages exploration of uncertain regions of the input space. Combined with the cost weighting, this ensures that the algorithm eventually explores all relevant parts of the Pareto front, even if some objectives are more expensive to evaluate.
- Core assumption: The Gaussian process surrogate model accurately represents the true objective functions, and the knowledge gradient acquisition function is properly maximized at each step.
- Evidence anchors:
  - [abstract] "Theoretical results prove asymptotic consistency of the algorithm."
  - [section 3.3] "Theorem 3.2 (Consistency of C-MOKG). Deﬁne the x* n,λ as in Equation (15). When using C-MOKG with either random scalarizations, or expectation over scalarizations, we have ∀λ∈ Λ, λ·f(x* N,λ)→ maxx∈X λ·f(x) as N→∞ almost surely and in mean."
- Break condition: If the Gaussian process model is a poor approximation of the true objective functions (model mismatch), the convergence guarantee no longer holds.

## Foundational Learning

- Concept: Gaussian Processes (GPs) as surrogate models for expensive objective functions.
  - Why needed here: GPs provide a probabilistic model of the objective functions, capturing both the mean prediction and the uncertainty, which is crucial for the knowledge gradient acquisition function.
  - Quick check question: What are the key components of a GP that allow it to model both the mean and uncertainty of a function?

- Concept: Multi-objective optimization and the concept of the Pareto front.
  - Why needed here: The goal of the algorithm is to find the Pareto front, which represents the set of optimal trade-offs between competing objectives. Understanding this concept is essential for interpreting the results and evaluating the algorithm's performance.
  - Quick check question: What is the Pareto front, and how does it differ from a single optimal solution in single-objective optimization?

- Concept: Bayesian optimization and acquisition functions.
  - Why needed here: The algorithm uses Bayesian optimization to iteratively select the next point to evaluate, based on an acquisition function that balances exploration and exploitation. Understanding this framework is crucial for understanding how the algorithm works.
  - Quick check question: What is the role of the acquisition function in Bayesian optimization, and how does it balance exploration and exploitation?

## Architecture Onboarding

- Component map:
  - Gaussian Process Models -> Independent GPs for each objective function
  - Scalarization -> Linear scalarization using weights from standard simplex
  - Knowledge Gradient -> Expected improvement in scalarized objective after hypothetical evaluation
  -> Cost Weighting -> Division by evaluation cost of each objective
  -> Optimization -> Maximization of cost-weighted knowledge gradient
  -> Discretization -> Approximation on finite grid for efficient computation

- Critical path:
  1. Initialize GPs with prior distributions and initial data points
  2. For each iteration:
     a. Compute posterior mean and covariance of GPs given observed data
     b. For each candidate point and objective:
        i. Compute knowledge gradient for hypothetical evaluation
        ii. Divide by evaluation cost to get cost-weighted knowledge gradient
     c. Select point and objective that maximize cost-weighted knowledge gradient
     d. Evaluate selected objective at selected point and update GPs
  3. Repeat until budget is exhausted

- Design tradeoffs:
  - Independent GPs vs. correlated GPs: Independent GPs are simpler to implement and computationally cheaper, but may not capture correlations between objectives
  - Discrete approximation vs. continuous optimization: Discrete approximation is more efficient but may miss true optimum, while continuous optimization is more accurate but computationally expensive
  - Expectation over scalarizations vs. random scalarizations: Expectation over scalarizations is more computationally expensive but provides better performance when objectives can be evaluated separately

- Failure signatures:
  - Slow convergence: May indicate cost weighting is not effective or objectives are too difficult to learn
  - Poor approximation of Pareto front: May indicate GPs are not accurately modeling objective functions or scalarization is not capturing true trade-offs
  - Numerical instability: May indicate observation noise is too low or length scales are too short

- First 3 experiments:
  1. Compare C-MOKG to benchmark algorithm that always evaluates both objectives on simple bi-objective problem with known costs and clear trade-offs
  2. Investigate impact of different cost ratios on performance of C-MOKG
  3. Evaluate sensitivity of C-MOKG to choice of prior distributions for GP hyper-parameters

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Experimental validation limited to synthetic 2D bi-objective problems with specific GP priors
- Does not address scalability to higher dimensions or cases where evaluation costs vary across input space
- Does not compare against other state-of-the-art multi-objective BO methods
- Discretization approach and QMC sampling may become computationally prohibitive in higher dimensions

## Confidence
- **High Confidence**: Theoretical consistency proof, basic mechanism of cost-weighted acquisition function, superiority of expectation over scalarizations when objectives are evaluated separately
- **Medium Confidence**: Empirical performance improvements on synthetic problems, numerical results showing C-MOKG's advantage in specific scenarios
- **Low Confidence**: Generalization to higher-dimensional problems (beyond 2D), real-world applicability where costs vary with input location, performance with correlated objective functions

## Next Checks
1. Test C-MOKG on higher-dimensional problems (3-5 objectives) to evaluate scalability of discretization and scalarization approaches
2. Implement and compare against alternative multi-objective BO methods (e.g., ParEGO, SMS-EGO) to establish relative performance
3. Design experiments where evaluation costs vary as a function of input location to test robustness of cost weighting mechanism