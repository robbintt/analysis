---
ver: rpa2
title: 'Weighted Averaged Stochastic Gradient Descent: Asymptotic Normality and Optimality'
arxiv_id: '2307.06915'
source_url: https://arxiv.org/abs/2307.06915
tags:
- averaging
- have
- page
- lemma
- asymptotic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies weighted averaged stochastic gradient descent
  (SGD) and establishes asymptotic normality for a broad range of weighting schemes.
  The authors show that the limiting covariance matrix equals the "sandwich" form
  (the limiting covariance matrix of averaged SGD) multiplied by a prefactor determined
  by the weights.
---

# Weighted Averaged Stochastic Gradient Descent: Asymptotic Normality and Optimality

## Quick Facts
- arXiv ID: 2307.06915
- Source URL: https://arxiv.org/abs/2307.06915
- Reference count: 40
- One-line primary result: This paper establishes asymptotic normality for weighted averaged SGD and proposes an optimal adaptive weighting scheme

## Executive Summary
This paper studies weighted averaged stochastic gradient descent (SGD) and establishes asymptotic normality for a broad range of weighting schemes. The authors show that the limiting covariance matrix equals the "sandwich" form (the limiting covariance matrix of averaged SGD) multiplied by a prefactor determined by the weights. They also provide asymptotically valid online inference approaches based on covariance matrix estimation or random scaling. Additionally, the paper proposes an adaptive weighting scheme that is optimal in terms of both asymptotic variance and finite-sample mean squared error, drawing insights from the optimal weights for linear models.

## Method Summary
The paper analyzes weighted averaged SGD where the solution is computed as a weighted sum of SGD iterates. The method involves running SGD with a step size η_i = i^(-α) where α is slightly above 0.5, then computing a weighted average of the iterates using different weighting schemes. The authors establish asymptotic normality through martingale central limit theorems and derive optimal weights by treating the problem as a best linear unbiased estimation. They also propose online inference methods including covariance matrix estimation and random scaling for constructing confidence intervals.

## Key Results
- Weighted averaging schemes produce asymptotic normality with covariance equal to a prefactor times the ASGD sandwich form
- An adaptive weighting scheme is proposed that minimizes finite-sample MSE while achieving the same asymptotic variance as ASGD
- Online inference methods (covariance matrix estimation and random scaling) can be applied to weighted averaged SGD to construct asymptotically valid confidence intervals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weighted averaging schemes produce asymptotic normality with covariance equal to a prefactor times the ASGD sandwich form.
- Mechanism: By decomposing SGD iterates into a martingale difference array and applying martingale central limit theorems, the weighted sum converges to a normal distribution with variance scaled by the sum of squared weights.
- Core assumption: The gradient noise forms a martingale difference sequence with bounded conditional moments and the weighting scheme satisfies smoothness and finite variance conditions.
- Evidence anchors:
  - [abstract] "establish the asymptotic normality of a broad range of weighted averaged SGD solutions"
  - [section] "we leverage the martingale CLT on √n ∑n i=1 wn,iA−1ǫi"
  - [corpus] No direct corpus evidence; claim is novel to this paper
- Break condition: If the martingale difference condition fails or the weighting scheme violates smoothness (e.g., abrupt weight changes), the CLT may not hold.

### Mechanism 2
- Claim: The optimal adaptive weighting scheme minimizes finite-sample MSE and achieves the same asymptotic variance as ASGD.
- Mechanism: By treating the SGD iterates as noisy observations of the true parameter and solving a best linear unbiased estimator problem, the weights are chosen to minimize trace(Σ−1), where Σ is the covariance of the iterates.
- Core assumption: The covariance structure of SGD iterates can be estimated or assumed known, and the weighting problem reduces to a linear estimation problem.
- Evidence anchors:
  - [section] "the solution to the above constrained optimization problem is c = Σ−1/BD/BDTΣ−1/BD"
  - [abstract] "proposes an adaptive averaging scheme that exhibits both optimal statistical rate and favorable non-asymptotic convergence"
  - [corpus] No direct corpus evidence; claim is novel to this paper
- Break condition: If the covariance matrix is ill-conditioned or the linear model assumption is violated, the optimal weights may not be computable or may not yield the claimed performance.

### Mechanism 3
- Claim: Online inference methods (covariance matrix estimation and random scaling) can be applied to weighted averaged SGD to construct asymptotically valid confidence intervals.
- Mechanism: Since the weighted averaged SGD has asymptotic normality, standard inference techniques for ASGD (plug-in covariance estimation, random scaling via FCLT) remain valid after adjusting for the prefactor.
- Core assumption: The asymptotic normality result holds and the covariance estimation or random scaling methods are consistent for the sandwich form matrix.
- Evidence anchors:
  - [section] "we can effectively utilize inference methods, such as covariance matrix estimation... originally designed for ASGD in this context as well"
  - [abstract] "provide asymptotically valid online inference approaches"
  - [corpus] "Statistical Inference for High-Dimensional Stochastic Gradient Descent" suggests related inference work exists
- Break condition: If the prefactor is misestimated or the inference methods are not robust to the weighting scheme, coverage probabilities may be incorrect.

## Foundational Learning

- Concept: Martingale central limit theorems
  - Why needed here: To establish asymptotic normality of the weighted sum of gradient noise terms
  - Quick check question: What conditions must a martingale difference array satisfy for the martingale CLT to apply?

- Concept: Best linear unbiased estimation (BLUE)
  - Why needed here: To derive the optimal weights that minimize finite-sample MSE
  - Quick check question: How does the covariance matrix of the observations affect the BLUE solution?

- Concept: Sandwich form covariance matrix
  - Why needed here: To characterize the asymptotic variance of SGD estimators and enable inference
  - Quick check question: What are the components A and S in the sandwich form A−1SA−T, and how are they estimated?

## Architecture Onboarding

- Component map:
  - SGD iterates generation (xi updates) -> Weight computation (wn,i for each iteration) -> Weighted average computation (x̄n = ∑ wn,ixi) -> Asymptotic normality verification (theoretical check) -> Inference method application (covariance estimation or random scaling) -> Performance evaluation (MSE, convergence plots)

- Critical path:
  1. Run SGD to generate iterates
  2. Compute weights according to chosen scheme
  3. Form weighted average
  4. Verify asymptotic normality conditions
  5. Apply inference method
  6. Evaluate performance

- Design tradeoffs:
  - Weight smoothness vs. adaptivity: Smoother weights are easier to analyze but may be suboptimal; adaptive weights can improve finite-sample performance but require more complex analysis.
  - Online vs. batch inference: Online methods enable real-time confidence intervals but may have higher variance; batch methods can be more accurate but require storing data.

- Failure signatures:
  - If weights violate smoothness condition, asymptotic normality may fail.
  - If covariance matrix is ill-conditioned, inference methods may break down.
  - If step size is too large or too small, convergence may be poor.

- First 3 experiments:
  1. Implement SGD with a simple weighting scheme (e.g., uniform or polynomial decay) and verify asymptotic normality via simulation.
  2. Implement the adaptive weighting scheme and compare its finite-sample MSE to ASGD and other schemes.
  3. Apply online inference methods (covariance estimation and random scaling) to the weighted averaged SGD and check coverage of confidence intervals.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the asymptotic normality results extend to settings with weaker assumptions on the objective function (e.g., without strong convexity or Lipschitz continuity)?
- Basis in paper: [explicit] The paper states "While applicable to many existing averaging schemes, a challenging future work is to extend the result under weaker assumptions and conditions."
- Why unresolved: The main theorem relies on strong convexity and Lipschitz continuity assumptions. Relaxing these would require new proof techniques and different analysis of the SGD error sequence.
- What evidence would resolve it: A formal proof establishing asymptotic normality for weighted averaged SGD under relaxed convexity or smoothness conditions, with explicit characterization of the limiting distribution.

### Open Question 2
- Question: Can the adaptive weighting scheme be generalized to non-smooth optimization problems beyond the linear model setting?
- Basis in paper: [inferred] The paper shows that the adaptive weights perform well for smooth and non-smooth cases like expectile regression, but the theoretical justification is limited to the linear model.
- Why unresolved: The derivation of optimal weights relies on closed-form expressions that are specific to the linear model. Extending this to more general non-smooth settings would require new techniques.
- What evidence would resolve it: A theoretical framework for deriving adaptive weights for general non-smooth optimization problems, supported by empirical validation on diverse models.

### Open Question 3
- Question: How does the choice of step size decay rate (α) affect the finite-sample performance of different averaging schemes?
- Basis in paper: [explicit] The paper explores different values of α in simulations (0.505 and 0.8) and observes differences in performance across averaging schemes.
- Why unresolved: While the paper provides some empirical evidence, a comprehensive theoretical analysis of how α impacts the trade-off between bias and variance for each averaging scheme is lacking.
- What evidence would resolve it: A theoretical characterization of the bias-variance trade-off for each averaging scheme as a function of α, supported by extensive simulations across different problem settings.

### Open Question 4
- Question: Can the online inference methods (covariance matrix estimation and random scaling) be extended to settings with time-varying parameters or non-stationary data?
- Basis in paper: [inferred] The online inference methods are developed under the assumption of a fixed optimal parameter. Extending to time-varying settings would require tracking the parameter over time.
- Why unresolved: The current inference methods rely on the asymptotic normality of the averaged SGD solution around a fixed point. Time-varying parameters would violate this assumption.
- What evidence would resolve it: Development and analysis of online inference methods that can adapt to parameter changes over time, with theoretical guarantees on their validity and empirical validation on non-stationary data.

## Limitations

- The theoretical results rely on strong convexity and Lipschitz continuity assumptions, which may not hold in many modern machine learning applications
- The adaptive weighting scheme's performance depends on accurate covariance matrix estimation, which can be challenging in high-dimensional settings
- The simulation results focus on low-dimensional problems (d=5), limiting generalizability to high-dimensional scenarios common in modern machine learning applications

## Confidence

- Asymptotic normality result for general weighting schemes: **High** - The martingale CLT approach is well-established and the proof structure is sound
- Optimal adaptive weighting scheme: **Medium** - The theoretical derivation is rigorous, but practical performance depends on covariance estimation quality
- Online inference methods: **High** - These methods are straightforward extensions of existing ASGD inference techniques with appropriate prefactor adjustments

## Next Checks

1. Implement the adaptive weighting scheme on high-dimensional synthetic data (d≥50) to test covariance matrix estimation robustness and verify if the optimal weights can be computed reliably
2. Compare computational costs of the adaptive scheme versus simpler weighting schemes across varying problem sizes to assess practical feasibility
3. Test the inference methods on real-world datasets with non-i.i.d. gradient noise to evaluate coverage probability under more realistic conditions beyond the martingale difference assumption