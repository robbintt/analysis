---
ver: rpa2
title: Characterizing Mechanisms for Factual Recall in Language Models
arxiv_id: '2310.15910'
source_url: https://arxiv.org/abs/2310.15910
tags:
- head
- in-context
- memorized
- figure
- frequency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how language models integrate conflicting
  information from pretraining and context. The authors study a task where models
  must choose between a memorized capital city and an in-context counterfactual.
---

# Characterizing Mechanisms for Factual Recall in Language Models

## Quick Facts
- arXiv ID: 2310.15910
- Source URL: https://arxiv.org/abs/2310.15910
- Authors: Anonymous
- Reference count: 10
- Key outcome: This paper investigates how language models integrate conflicting information from pretraining and context. The authors study a task where models must choose between a memorized capital city and an in-context counterfactual. They find that training frequency of both the country and the counterfactual city strongly influences the model's choice, with larger models more likely to use memorized answers. Using head attribution, they identify specific attention heads that promote either the memorized or in-context answer. By scaling these heads, they can control the model's behavior, increasing in-context answer generation to 88% by downweighting a single memory head. This work shows that model behaviors can be localized to specific components and demonstrates a proof-of-concept for dynamic runtime control of model behavior.

## Executive Summary
This paper explores how language models resolve conflicts between memorized knowledge from pretraining and new information provided in context. The authors create a counterfactual world capital task where models must choose between a memorized capital city and an in-context alternative. They find that both training frequency and model scale influence this decision, with larger models favoring memorized answers. Using head attribution, they identify specific attention heads responsible for promoting either the memorized or in-context answer, and demonstrate that scaling these heads can dynamically control model behavior at runtime.

## Method Summary
The authors create a counterfactual world capital dataset by pairing every country with 247 alternative capitals. They run this data through Pythia models of varying sizes (160M to 2.8B parameters) and measure the proportion of times models use the in-context answer versus the memorized answer. Head attribution is used to identify individual attention heads that promote either answer type by projecting head outputs into the vocabulary space and comparing logit differences. The authors then scale the value vectors of identified heads to control model behavior, increasing in-context answer generation from 22% to 88% on the world capitals task.

## Key Results
- Training frequency of both countries and in-context cities strongly influences model choice between memorized and in-context answers
- Larger models (up to 2.8B parameters) are less likely to use in-context information and prefer memorized answers
- Head attribution identifies specific attention heads that promote either memorized or in-context answers
- Scaling value vectors of memory heads can increase in-context answer generation to 88% while reducing memorized predictions to 4%
- A single memory head (15.7) accounts for 68% of the memorized predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models resolve conflicts between memorized and in-context information by using specific attention heads to promote one answer over the other.
- Mechanism: Attention heads project their outputs into the vocabulary space using the unembedding matrix, and the head with the strongest logit difference for a given answer type (memorized vs in-context) determines which answer is more likely to be generated.
- Core assumption: The additive nature of transformer layers allows the contribution of individual heads to be isolated and manipulated.
- Evidence anchors:
  - [abstract] "We then use head attribution to identify individual attention heads that either promote the memorized answer or the in-context answer in the logits."
  - [section 6.1] "We can project the ith head into the space of the residual stream by multiplying with the ith (dhead, dmodel) slice of this matrix (see Appendix C) and then multiplying with the unembedding matrix to get the logit values for the memorized and in-context city tokens."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.466, average citations=0.0. Top related titles include work on knowledge probing and mechanistic analysis of factual recall.
- Break condition: If the additive assumption doesn't hold (e.g., due to non-linear interactions between heads), the isolation of individual head contributions may fail.

### Mechanism 2
- Claim: The frequency of terms in pretraining data affects the likelihood of a model using memorized vs in-context information.
- Mechanism: Higher frequency of a country in pretraining leads to more robust memorized knowledge about its capital, while higher frequency of the in-context city makes the model less likely to use it as a counterfactual.
- Core assumption: Term frequency in pretraining data directly correlates with the strength of memorized associations.
- Evidence anchors:
  - [abstract] "We find that the training frequency of both the country and the in-context city strongly influences the model's choice"
  - [section 5.2] "When the country is more prevalent in the training data, the model has a greater tendency to predict memorized answers. We also observe a relationship between the frequency of the in-context capital and the model's predictions."
  - [corpus] Weak - corpus evidence only shows related work on frequency effects, not direct validation of this specific claim.
- Break condition: If other factors (e.g., context length, prompt format) dominate over frequency effects, the correlation may break down.

### Mechanism 3
- Claim: Scaling the value vectors of specific attention heads can dynamically control model behavior at runtime without updating parameters.
- Mechanism: By multiplying the value vectors of memory heads by a scalar α, we can increase or decrease their influence on the final logits, thus controlling whether the model uses memorized or in-context answers.
- Core assumption: The value vectors of attention heads directly control the strength of their contribution to the output.
- Evidence anchors:
  - [abstract] "By scaling up or down the value vector of these heads, we can control the likelihood of using the in-context answer on new data."
  - [section 6.2] "We hypothesize that tuning up the memory head will increase the number of answers that contain the ground truth answer, while tuning it down will increase the number that contain the in-context answer."
  - [corpus] Weak - corpus evidence shows related work on model editing but not this specific runtime control mechanism.
- Break condition: If the scaling factor α doesn't have a linear effect on the head's contribution, or if other heads compensate for the change, the control may be ineffective.

## Foundational Learning

- Concept: Attention mechanisms in transformers
  - Why needed here: Understanding how attention heads project their outputs and contribute to the final logits is crucial for interpreting head attribution results.
  - Quick check question: How does an attention head's output get combined with the residual stream in a transformer?

- Concept: Frequency effects in language model pretraining
  - Why needed here: The core finding that term frequency affects model behavior relies on understanding how pretraining data statistics influence learned representations.
  - Quick check question: What is the relationship between term frequency in pretraining data and the strength of memorized associations in a language model?

- Concept: Vector arithmetic and linear algebra in neural networks
  - Why needed here: Manipulating attention heads requires understanding how scaling value vectors affects their contribution, which involves linear algebra concepts.
  - Quick check question: How does scaling a vector affect its dot product with another vector, and why is this relevant for modifying attention head outputs?

## Architecture Onboarding

- Component map: Input text → Embedding layer → Transformer blocks (self-attention + feed-forward) → Output logits → Decoding
- Critical path: Embedding → Attention heads → Value vectors → Unembedding matrix → Output tokens
- Design tradeoffs: The choice to focus on attention heads for manipulation rather than MLP layers reflects the assumption that attention is more directly responsible for copying context vs recalling memorized information.
- Failure signatures: If head attribution doesn't isolate individual contributions, or if scaling value vectors has unintended side effects on other model behaviors.
- First 3 experiments:
  1. Verify the additive assumption by comparing the sum of individual head contributions to the total attention output.
  2. Test the frequency effect by training a small model on controlled pretraining data with varying term frequencies.
  3. Validate the scaling intervention by applying it to a different task (e.g., COUNTERFACT dataset) and measuring the change in behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the specific attention heads that promote memorized vs. in-context answers generalize beyond the world capitals task to other factual domains?
- Basis in paper: [inferred] The authors note that the memory head 15.7 shows domain specificity, as it doesn't generalize well to the COUNTERFACT dataset, which includes facts from multiple domains beyond geography.
- Why unresolved: The paper only tests the head on two datasets (world capitals and COUNTERFACT), leaving open the question of whether such heads can be found for other types of factual knowledge.
- What evidence would resolve it: Testing the same head identification and intervention methods on a wider variety of factual tasks (e.g., historical dates, scientific facts, biographical information) to see if similar heads can be found and controlled.

### Open Question 2
- Question: How do larger models (beyond the 2.8b parameter models tested) balance memorized vs. in-context information, and does their behavior follow the same frequency patterns observed in this study?
- Basis in paper: [explicit] The authors note that "Larger models (up to the scale tested: 2.8b parameters) are less likely overall to use in-context information and prefer the memorized answer, even when the fact is less frequent."
- Why unresolved: The study only examines models up to 2.8b parameters, leaving open the question of how even larger models behave.
- What evidence would resolve it: Testing the same frequency analysis and head attribution methods on models with significantly more parameters (e.g., 10b, 100b+) to see if the observed trends continue or change.

### Open Question 3
- Question: What is the relationship between attention heads and MLP layers in controlling the balance between memorized and in-context information?
- Basis in paper: [explicit] The authors note that "given the connection between facts learned in pretraining and the MLP layers (Geva et al., 2021; Meng et al., 2023; Merullo et al., 2023), it's possible that tuning attention alone is not enough to see higher performance in this setting."
- Why unresolved: The study focuses solely on attention heads, but acknowledges that MLP layers may also play a role in this mechanism.
- What evidence would resolve it: Extending the head attribution and intervention methods to MLP layers, and testing combinations of attention and MLP interventions to see if they produce stronger effects than attention alone.

### Open Question 4
- Question: What is the precise mechanism by which downweighting the memory head causes the model to prefer in-context information?
- Basis in paper: [explicit] The authors find that "downweighting the memory head allows us to increase the rate of the in-context city to 88% while reducing the amount of memorized predictions to 4% on the world capitals task."
- Why unresolved: While the authors show that this intervention works, they don't fully explain the underlying mechanism of how it changes the model's behavior.
- What evidence would resolve it: Conducting ablation studies to isolate the effects of the memory head on different parts of the model's computation (e.g., attention scores, value vectors, residual stream), and testing whether similar effects can be achieved through other means (e.g., directly modifying the residual stream).

### Open Question 5
- Question: Can the techniques developed in this paper be used to create more robust defenses against prompt injection attacks?
- Basis in paper: [explicit] The authors suggest that "there are simultaneously reasons we might want to suppress the use of in-context information at run time (e.g., to combat prompt-injection attacks)."
- Why unresolved: While the authors demonstrate that they can control the model's use of in-context information in a limited setting, it's unclear whether these techniques can be scaled up to protect against real-world prompt injection attacks.
- What evidence would resolve it: Testing the same intervention methods on models that have been specifically designed to be vulnerable to prompt injection (e.g., models that have been fine-tuned on malicious prompts), and evaluating whether the interventions can effectively mitigate these attacks without harming the model's performance on benign inputs.

## Limitations

- The additive assumption of head contributions may not hold in practice due to complex, non-linear interactions between heads
- The scaling intervention's effectiveness across different model architectures and tasks is unclear
- The study doesn't address potential side effects of head scaling on other aspects of model behavior
- The frequency effects observed might be confounded by other factors such as context length, prompt format, or the specific pretraining corpus used

## Confidence

**High Confidence:** The core finding that specific attention heads can be identified and manipulated to control factual recall is well-supported by the experimental results. The 88% success rate in generating in-context answers through head scaling provides strong evidence for this claim.

**Medium Confidence:** The relationship between term frequency in pretraining data and the likelihood of using memorized vs in-context information is demonstrated, but the causal mechanism isn't fully established. Other factors could be influencing these results, and the correlation might not hold across different pretraining corpora or model architectures.

**Low Confidence:** The generalizability of the head attribution and scaling methods to other tasks and model architectures is uncertain. While the paper demonstrates success on one specific task, extending these techniques to more complex, real-world scenarios would require significant additional validation.

## Next Checks

1. **Additive Assumption Validation**: Design an experiment to test whether attention head contributions are truly additive. Compare the sum of individual head contributions (isolated through head attribution) to the total attention output when all heads are active. This will validate or challenge a core assumption of the head attribution method.

2. **Cross-Task Generalization**: Apply the head scaling intervention to a different knowledge-intensive task (e.g., the COUNTERFACT dataset mentioned in the corpus) and measure the change in behavior. This will test whether the manipulation techniques generalize beyond the specific counterfactual world capital task.

3. **Side Effect Analysis**: After scaling memory heads to promote in-context answers, evaluate the model's performance on unrelated tasks or its overall coherence. This will reveal whether the intervention has unintended consequences on other aspects of model behavior.