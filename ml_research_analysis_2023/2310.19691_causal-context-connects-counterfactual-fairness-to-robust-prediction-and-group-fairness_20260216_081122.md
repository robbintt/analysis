---
ver: rpa2
title: Causal Context Connects Counterfactual Fairness to Robust Prediction and Group
  Fairness
arxiv_id: '2310.19691'
source_url: https://arxiv.org/abs/2310.19691
tags:
- fairness
- counterfactual
- path
- causal
- only
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper bridges counterfactual fairness, robust prediction,
  and group fairness through causal context. The key contributions are: (1) Counterfactually
  fair predictors are accuracy-optimal in an unbiased target distribution under certain
  conditions, providing a novel motivation for counterfactual fairness.'
---

# Causal Context Connects Counterfactual Fairness to Robust Prediction and Group Fairness

## Quick Facts
- arXiv ID: 2310.19691
- Source URL: https://arxiv.org/abs/2310.19691
- Authors: 
- Reference count: 40
- Key outcome: Bridges counterfactual fairness, robust prediction, and group fairness through causal context, showing equivalences under specific conditions and establishing accuracy-optimality of counterfactually fair predictors.

## Executive Summary
This paper establishes a novel connection between counterfactual fairness, robust prediction, and group fairness through causal context. The authors show that under certain conditions, achieving group fairness metrics (demographic parity, equalized odds, or calibration) automatically ensures counterfactual fairness. They also demonstrate that counterfactually fair predictors are accuracy-optimal in unbiased target distributions when the association between label and protected class is "purely spurious." These theoretical results are validated on a semi-synthetic dataset, showing that counterfactually fair predictors achieve both out-of-distribution accuracy and the corresponding group fairness metric.

## Method Summary
The paper develops a theoretical framework connecting causal graphs to group fairness metrics, showing that specific causal contexts (measurement error, selection on label, selection on predictors) imply equivalences between counterfactual fairness and particular group fairness metrics. They prove that under "purely spurious" associations, counterfactually fair predictors minimize risk in unbiased target distributions. Experiments use the Adult income dataset with simulated protected class and induced biases, comparing naive, fairness-through-unawareness, and counterfactually fair predictors on accuracy and fairness metrics in target distributions.

## Key Results
- Counterfactually fair predictors are accuracy-optimal in unbiased target distributions under "purely spurious" associations
- Causal context determines which group fairness metric (demographic parity, equalized odds, calibration) is equivalent to counterfactual fairness
- Three common fairness contexts (measurement error, selection on label, selection on predictors) imply these equivalences
- Experiments confirm counterfactually fair predictors achieve both out-of-distribution accuracy and corresponding group fairness metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counterfactual fairness can be achieved through group fairness metrics under specific causal structures.
- Mechanism: The paper establishes a correspondence between causal graphs and which group fairness metrics are equivalent to counterfactual fairness. When the causal structure implies certain independence relationships, achieving a specific group fairness metric automatically ensures counterfactual fairness.
- Core assumption: The causal structure is known and faithful, meaning no causal effects are precisely counterbalanced.
- Evidence anchors:
  - [abstract] "we develop a correspondence between the causal graph of the data-generating process and which, if any, group fairness metrics are equivalent to counterfactual fairness."
  - [section] "we show that in three common fairness contexts—measurement error, selection on label, and selection on predictors—counterfactual fairness is equivalent to demographic parity, equalized odds, and calibration, respectively."
  - [corpus] Weak evidence - related papers discuss counterfactual fairness but don't establish the same causal context correspondence.

### Mechanism 2
- Claim: Counterfactual fairness is accuracy-optimal in unbiased target distributions under certain conditions.
- Mechanism: When the association between label and protected class is "purely spurious" (caused only by measurement error, selection on label, or selection on predictors), the counterfactually fair predictor that minimizes risk in the training distribution also minimizes risk in the unbiased target distribution.
- Core assumption: The association between label Y and protected class A is "purely spurious" and the marginal distribution of the label is the same in training and target distributions for selection on label.
- Evidence anchors:
  - [abstract] "under plausible conditions, the counterfactually fair predictor is in fact accuracy-optimal in an unbiased target distribution."
  - [section] "For a distribution with bias due to selection on label and equal marginal label distributions or due to selection on predictors, the counterfactually fair predictor is accuracy-optimal in the unbiased target distribution."
  - [corpus] No direct evidence in corpus - this appears to be a novel theoretical contribution.

### Mechanism 3
- Claim: Tools developed for group fairness can be applied to achieve counterfactual fairness.
- Mechanism: Since counterfactual fairness is equivalent to specific group fairness metrics under certain causal contexts, existing tools for achieving group fairness (data augmentation, regularization, etc.) can be repurposed to enforce counterfactual fairness.
- Core assumption: The equivalence relationship between counterfactual fairness and group fairness metrics holds in the given context.
- Evidence anchors:
  - [abstract] "we show that in three common fairness contexts...counterfactual fairness is equivalent to demographic parity, equalized odds, and calibration, respectively."
  - [section] "we provide a conceptual tool for adjudicating between fairness metrics based on causal knowledge and suggests that counterfactual fairness can be tested by measuring simpler group fairness metrics."
  - [corpus] Related papers discuss using group fairness tools for counterfactual fairness, supporting this mechanism.

## Foundational Learning

- Concept: Causal graphs and d-separation
  - Why needed here: The paper's core argument relies on understanding how causal structures imply conditional independencies, which determine equivalence between counterfactual fairness and group fairness metrics.
  - Quick check question: Given a causal graph where X causes Y and Y causes Z, what conditional independence relationship can be inferred?

- Concept: Counterfactual fairness vs. group fairness
  - Why needed here: The paper bridges these two paradigms, so understanding their definitions and differences is crucial for following the argument.
  - Quick check question: How does counterfactual fairness differ from demographic parity in terms of what they require for a predictor?

- Concept: "Purely spurious" association
  - Why needed here: This is a key assumption in the paper's theoretical results about when counterfactual fairness is accuracy-optimal.
  - Quick check question: What does it mean for the association between label Y and protected class A to be "purely spurious"?

## Architecture Onboarding

- Component map:
  Causal structure identification -> Group fairness metric selection -> Counterfactual fairness enforcement
  Key components: Causal graph analysis, fairness metric computation, predictor training

- Critical path:
  1. Identify causal structure of the problem domain
  2. Determine which group fairness metric is equivalent to counterfactual fairness
  3. Implement tools to achieve that group fairness metric
  4. Verify counterfactual fairness is achieved

- Design tradeoffs:
  - Precision of causal structure identification vs. practical feasibility
  - Complexity of achieving group fairness vs. simplicity of counterfactual fairness verification
  - Accuracy in target distribution vs. fairness in training distribution

- Failure signatures:
  - Counterfactual fairness not achieved despite group fairness metric being satisfied
  - Poor out-of-distribution performance despite achieving group fairness
  - Inability to identify appropriate causal structure

- First 3 experiments:
  1. Implement Theorem 2's correspondence on a simple synthetic dataset with known causal structure
  2. Test Corollary 2.2's equivalence relationships on the Adult income dataset
  3. Verify Theorem 1's accuracy-optimality claim by comparing counterfactually fair predictor to naive predictor on biased vs. unbiased distributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions does the counterfactually fair predictor fail to be accuracy-optimal in the unbiased target distribution?
- Basis in paper: [explicit] Theorem 1 states that the counterfactually fair predictor is accuracy-optimal under certain conditions (measurement error, selection on label, or selection on predictors), but does not specify when it fails.
- Why unresolved: The paper does not provide a complete characterization of when the counterfactually fair predictor is not accuracy-optimal. It only gives sufficient conditions for optimality.
- What evidence would resolve it: Empirical studies on diverse real-world datasets with different causal structures would help identify scenarios where the counterfactually fair predictor underperforms.

### Open Question 2
- Question: How can we identify and model the causal structure of biases in real-world datasets to apply the correspondence between counterfactual fairness and group fairness metrics?
- Basis in paper: [explicit] The paper acknowledges that identifying the causal structure is challenging but necessary for applying the theoretical results.
- Why unresolved: The paper does not provide specific methods for causal discovery in fairness contexts, which is a significant practical challenge.
- What evidence would resolve it: Developing and validating causal discovery algorithms specifically designed for fairness contexts, and testing them on real-world datasets.

### Open Question 3
- Question: How can we extend the theoretical framework to cases where the association between the label Y and the protected class A is not "purely spurious"?
- Basis in paper: [explicit] The paper assumes "purely spurious" associations, but acknowledges that real-world scenarios may involve more complex causal relationships.
- Why unresolved: The paper does not provide a clear path for extending the theory to non-purely spurious cases, which are likely to be common in practice.
- What evidence would resolve it: Theoretical work on characterizing distribution shift and developing observable signatures of counterfactual fairness in complex causal settings.

## Limitations

- Practical applicability depends on accurate identification of causal structures, which is often challenging in real-world scenarios
- Theoretical results rely on "purely spurious" associations, which may not hold in many real-world fairness contexts
- Experimental validation uses a semi-synthetic dataset, which may not capture all complexities of real-world data

## Confidence

- High Confidence: The theoretical framework connecting causal graphs to group fairness metrics (Theorem 2) is well-established and mathematically rigorous. The experimental setup and results are clearly described.
- Medium Confidence: The claim that counterfactual fairness is accuracy-optimal under "purely spurious" associations (Theorem 1) is theoretically sound but relies on strong assumptions that may not hold in practice. The correspondence between causal contexts and specific group fairness metrics is convincing but requires further empirical validation.
- Low Confidence: The practical implementation details for achieving counterfactual fairness through group fairness tools are somewhat vague, and the semi-synthetic dataset may not fully represent real-world complexities.

## Next Checks

1. **Causal Structure Identification**: Validate the correspondence between causal graphs and group fairness metrics on multiple real-world datasets with known causal structures.
2. **Robustness to Unfaithful Structures**: Test the theoretical results under scenarios where causal structures are imperfectly known or unfaithful to assess practical limitations.
3. **Group Fairness Tool Repurposing**: Implement and evaluate specific group fairness tools (e.g., data augmentation, regularization) for achieving counterfactual fairness in diverse contexts to confirm the paper's claims.