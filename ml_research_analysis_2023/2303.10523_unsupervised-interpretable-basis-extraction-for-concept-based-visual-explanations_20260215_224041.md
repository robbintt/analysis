---
ver: rpa2
title: Unsupervised Interpretable Basis Extraction for Concept-Based Visual Explanations
arxiv_id: '2303.10523'
source_url: https://arxiv.org/abs/2303.10523
tags:
- basis
- concept
- classi
- interpretable
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel unsupervised post-hoc method for interpretable
  basis extraction for CNNs. The key idea is to learn a basis by optimizing for sparsity
  in the transformed representation, which occurs when a representation is projected
  onto an interpretable basis.
---

# Unsupervised Interpretable Basis Extraction for Concept-Based Visual Explanations

## Quick Facts
- arXiv ID: 2303.10523
- Source URL: https://arxiv.org/abs/2303.10523
- Reference count: 33
- Proposes unsupervised interpretable basis extraction for CNNs that improves interpretability metrics over natural basis without concept annotations

## Executive Summary
This paper introduces an unsupervised post-hoc method for extracting interpretable bases from CNN intermediate representations. The method learns basis vectors by optimizing for sparsity in transformed representations, aiming to create concept detectors that activate sparsely and selectively. The approach is compared against natural feature space bases and supervised methods, showing improved interpretability metrics without requiring concept annotations.

## Method Summary
The method learns a set of basis vectors through optimization that projects CNN feature maps onto a new space where activations become sparse after sigmoid thresholding. It uses entropy-based sparsity loss, maximum activation loss, inactive detector loss, and maximum margin loss to learn orthonormal basis vectors that can be interpreted as concept detectors. The learned basis is evaluated using interpretability metrics (S1, S2) that measure detection performance and concept consistency.

## Key Results
- Proposed method improves interpretability metrics (S1, S2) compared to natural basis
- Can suggest interpretable bases without requiring concept annotations
- Shows strengths over supervised approach in certain interpretability aspects
- Demonstrates effectiveness across multiple CNN architectures (ResNet18, VGG16BN, AlexNet, GoogleNet)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse one-hot thresholding of projected representations implies interpretable basis
- Mechanism: The method learns basis vectors and biases such that each pixel is classified positively by exactly one concept detector with high confidence, producing sparse activations across feature maps after sigmoid thresholding
- Core assumption: CNN intermediate representations can be linearly separated by semantic concepts
- Evidence anchors:
  - [abstract] "the thresholded projections of CNN's representations on the learned directions, are sparse"
  - [section] "projecting a representation to an interpretable basis and thresholding, results into sparse one-hot representations"
  - [corpus] Weak evidence; no corpus papers directly test sparsity criterion
- Break condition: If linear separability fails for key concepts, sparsity objective cannot be satisfied

### Mechanism 2
- Claim: Entropy-based sparsity loss enforces each pixel to be assigned to few concept detectors
- Mechanism: Entropy is applied to the normalized activation vector across detectors, penalizing non-sparse assignments
- Core assumption: Pixel activations in interpretable basis should concentrate on few detectors
- Evidence anchors:
  - [abstract] "We introduce the sparsity loss Ls as: Ls = Ep[ -∑ qp,i log2(qp,i) ]"
  - [section] "We use entropy as a sparsity measure and define the sparsity loss Ls"
  - [corpus] No direct corpus evidence for entropy-based sparsity in unsupervised basis extraction
- Break condition: If entropy minimization drives all activations to zero, detectors become inactive

### Mechanism 3
- Claim: Orthogonality constraints ensure basis vectors represent independent or mutually exclusive concepts
- Mechanism: Basis vectors are learned to be orthonormal (wT wi = 1 and wT wj = 0), preventing correlated concept detection
- Core assumption: Independent concepts should correspond to orthogonal directions in feature space
- Evidence anchors:
  - [abstract] "wi should form an orthonormal basis"
  - [section] "Orthogonality of the extracted basis is enforced by using [21]"
  - [corpus] Weak; no corpus papers verify orthogonality improves interpretability
- Break condition: If concept correlation is high, forcing orthogonality may degrade detection accuracy

## Foundational Learning

- Concept: Linear separability of intermediate CNN representations
  - Why needed here: The method assumes features for different concepts can be separated by hyperplanes
  - Quick check question: If a CNN is trained on ImageNet, can a logistic regression classifier separate "dog" vs "cat" features in the last conv layer?

- Concept: Entropy as a sparsity measure
  - Why needed here: Entropy quantifies concentration of activations across detectors
  - Quick check question: For a probability vector [0.1, 0.9, 0.0], what is its entropy in base 2?

- Concept: Orthonormal basis constraints
  - Why needed here: Ensures learned directions are independent and interpretable
  - Quick check question: If w1 and w2 are orthonormal, what is w1 · w2?

## Architecture Onboarding

- Component map: CNN intermediate feature maps (H×W×D) -> 1×1 convolution (D→I) -> sigmoid -> losses (SL, MAL, ICL, MML) -> basis vectors wi, biases bi, margins Mi

- Critical path:
  1. Forward pass: CNN → feature maps → 1×1 conv → sigmoid → losses
  2. Backward: gradients from all losses → update wi, b, M
  3. Repeat for 300 epochs

- Design tradeoffs:
  - I = D ensures full rotation but may overfit; I < D risks under-representation
  - High λma vs λs balances sparsity vs confident assignment
  - τ controls detector inactivity penalty; too high → loss of detectors

- Failure signatures:
  - All detectors inactive: ICL too strict or sparsity loss dominant
  - Non-sparse outputs: SL weight too low or concept correlation high
  - Poor interpretability: Basis labeling step fails or IoU metrics low

- First 3 experiments:
  1. Verify sparsity: measure fraction of pixels with exactly one detector >0.5
  2. Check orthogonality: compute mean pairwise angle between wi
  3. Validate interpretability: compare S1, S2 scores against natural basis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method compare to other unsupervised methods for interpretable basis extraction in terms of interpretability metrics and computational efficiency?
- Basis in paper: [inferred] The paper proposes an unsupervised method for interpretable basis extraction, but does not compare it to other unsupervised methods
- Why unresolved: The paper focuses on comparing the proposed method to the natural basis and a supervised approach, but does not explore other unsupervised methods in the literature
- What evidence would resolve it: A comprehensive comparison of the proposed method with other unsupervised methods, including their interpretability scores, computational complexity, and robustness to different CNN architectures and datasets

### Open Question 2
- Question: Can the proposed method be extended to extract interpretable bases for intermediate layers other than the last layer, and how does the interpretability of these bases vary across layers?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of the proposed method for extracting interpretable bases for the last layer of CNNs, but does not explore other intermediate layers
- Why unresolved: The paper focuses on the last layer of CNNs, but does not investigate the interpretability of bases extracted for other intermediate layers
- What evidence would resolve it: A systematic evaluation of the proposed method for extracting interpretable bases for various intermediate layers, including a comparison of their interpretability scores and the factors influencing the interpretability across layers

### Open Question 3
- Question: How does the proposed method handle cases where the CNN's representations are not linearly separable, and can it suggest a basis that is still interpretable in such scenarios?
- Basis in paper: [inferred] The paper assumes that CNN representations are linearly separable, but does not address cases where this assumption may not hold
- Why unresolved: The paper focuses on the case where CNN representations are linearly separable, but does not explore scenarios where this assumption may not be valid
- What evidence would resolve it: An evaluation of the proposed method on CNNs where the representations are not linearly separable, and an analysis of the interpretability of the extracted bases in such cases

## Limitations
- Assumes CNN intermediate representations are linearly separable by semantic concepts without empirical validation
- Only evaluated on Broden dataset, limiting generalizability claims
- Orthogonality constraint's contribution to interpretability is asserted but not empirically isolated
- Core claim that sparsity implies interpretability lacks rigorous theoretical or empirical justification

## Confidence

- **High Confidence**: The technical implementation of the basis learning algorithm (1×1 convolution, loss functions, optimization procedure) appears sound and reproducible
- **Medium Confidence**: The interpretability metrics (S1, S2) and their calculation methodology are described clearly, though their sensitivity to different concept distributions warrants further investigation
- **Low Confidence**: The core claims about why sparsity implies interpretability and the relative superiority of the unsupervised approach versus supervised methods require more rigorous empirical validation

## Next Checks

1. **Linear Separability Validation**: Test whether a simple logistic regression classifier can separate different concept features in the last convolutional layer of various pre-trained CNNs. Measure classification accuracy and compare against the proposed method's performance.

2. **Sparsity Measure Comparison**: Implement alternative sparsity measures (L1 norm, Gini coefficient) and compare their effectiveness in producing interpretable bases. Evaluate using the same interpretability metrics to determine if entropy is optimal.

3. **Orthogonality Ablation Study**: Train variants of the method with varying degrees of orthogonality enforcement (including no orthogonality constraint) and measure the impact on interpretability metrics and basis quality. This would empirically validate whether orthogonality is necessary for interpretability.