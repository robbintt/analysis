---
ver: rpa2
title: 'From Values to Opinions: Predicting Human Behaviors and Stances Using Value-Injected
  Large Language Models'
arxiv_id: '2310.17857'
source_url: https://arxiv.org/abs/2310.17857
tags:
- value
- table
- distribution
- target
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to predict human opinions and behaviors
  by injecting value distributions into large language models (LLMs) using argument
  generation and question answering. The approach, called Value Injection Method (VIM),
  fine-tunes LLMs on tasks that align generated content with target Schwartz value
  distributions.
---

# From Values to Opinions: Predicting Human Behaviors and Stances Using Value-Injected Large Language Models

## Quick Facts
- arXiv ID: 2310.17857
- Source URL: https://arxiv.org/abs/2310.17857
- Reference count: 40
- Key outcome: VIM-injected LLMs achieve NMSE of 0.099 in opinion prediction and 0.071 in behavior prediction, significantly outperforming prompting baselines.

## Executive Summary
This paper introduces the Value Injection Method (VIM), a technique to align large language models (LLMs) with human value distributions based on Schwartz's theory of basic values. By fine-tuning LLMs on argument generation and question answering tasks using curated datasets, VIM enables models to generate or predict content consistent with target value profiles. Experiments demonstrate that value-injected LLMs significantly outperform prompting baselines across value survey, argument generation, behavior prediction, and opinion prediction tasks.

## Method Summary
The Value Injection Method (VIM) fine-tunes LLMs on two tasks: Argument Generation (AG) and Question Answering (QA). AG trains the model to generate arguments reflecting specific value distributions by distinguishing between value-consistent and value-inconsistent arguments. QA teaches the model to self-assess similarity to target value profiles through similarity questions. Using LoRA fine-tuning on the Touché23-ValueEval dataset, the method produces value-injected LLMs that can predict human opinions and behaviors by aligning with Schwartz value distributions.

## Key Results
- VILLAMA achieves NMSE of 0.099 in opinion prediction, outperforming baselines (0.137-0.221)
- VILLAMA achieves NMSE of 0.071 in behavior prediction, outperforming baselines (0.082-0.100)
- Human evaluation shows VILLAMA generates arguments that better reflect target value distributions than baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VIM enables LLMs to align their behavior with specific human value distributions by fine-tuning on argument generation and question answering tasks.
- Mechanism: VIM fine-tunes LLMs using a dataset of value-related arguments, training the model to generate content consistent with a target Schwartz value distribution.
- Core assumption: Human values as defined by Schwartz's theory are both measurable and influential enough on opinions and behaviors that aligning an LLM's outputs to these values will produce human-like predictions.
- Evidence anchors: [abstract] "We present Value Injection Method (VIM), a collection of two methods—argument generation and question answering—designed to inject targeted value distributions into LLMs via fine-tuning."

### Mechanism 2
- Claim: VIM outperforms prompting baselines because fine-tuning on value-aligned data embeds the target distribution into the model's parameters rather than relying on surface-level prompt engineering.
- Mechanism: By training on curated arguments and value surveys, the LLM internalizes the relationship between values and expressions, enabling it to generate or predict consistent with those values without explicit prompting cues.
- Core assumption: Parameter-level adaptation via LoRA fine-tuning is more effective than in-context learning for capturing complex, multi-dimensional value profiles.
- Evidence anchors: [abstract] "VILLAMA achieves 0.099 NMSE, significantly outperforming the baselines ranging from 0.137 to 0.221."

### Mechanism 3
- Claim: Predicting opinions and behaviors is feasible because Schwartz values correlate with real-world stances and actions, allowing LLMs trained on these values to generalize to unseen scenarios.
- Mechanism: The LLM uses the embedded value distribution to simulate how a person with that profile would respond to behavioral or opinion-based prompts, producing predictions that match empirical survey data.
- Core assumption: The correlation between values and opinions/behaviors observed in human populations is strong enough to transfer to LLM-generated predictions.
- Evidence anchors: [abstract] "VILLAMA achieves 0.099 NMSE in opinion prediction and 0.071 in behavior prediction, demonstrating improved alignment with human values and behaviors compared to baselines."

## Foundational Learning

- Concept: Schwartz's theory of basic human values
  - Why needed here: The entire method relies on mapping human values to LLM behavior; understanding the ten values and their motivational goals is essential to interpret results and design prompts.
  - Quick check question: What are the ten values in Schwartz's theory, and how are they measured in the PVQ?

- Concept: Fine-tuning vs. prompting in LLMs
  - Why needed here: The paper's key innovation is fine-tuning on value data rather than using zero-shot prompting; knowing when and why to fine-tune is critical for replication and extension.
  - Quick check question: What are the trade-offs between fine-tuning an LLM and using in-context learning for value alignment?

- Concept: Normalized Mean Squared Error (NMSE)
  - Why needed here: NMSE is the primary metric for evaluating prediction accuracy; understanding its calculation and interpretation is necessary to assess model performance.
  - Quick check question: How is NMSE calculated, and why is it preferred over raw MSE in this context?

## Architecture Onboarding

- Component map: Touché23-ValueEval -> LoRA fine-tuning with LAG and LQA losses -> PVQ, VALNET, ESS datasets for evaluation -> Value-injected LLM

- Critical path: 1. Preprocess Touché23 dataset into train/val/test splits 2. Fine-tune LLAMA-7B with VIM (AG + QA) using LoRA 3. Evaluate on PVQ (value survey), VALNET (behavior), ESS (opinion) 4. Compare NMSE against baselines

- Design tradeoffs:
  - Fine-tuning vs. prompting: Fine-tuning gives better alignment but requires more resources and data; prompting is cheaper but less effective.
  - AG vs. QA: AG generates new arguments; QA answers similarity questions; combining both yields best results.
  - Value granularity: Using all 10 Schwartz values increases complexity but captures more nuance.

- Failure signatures:
  - High NMSE on PVQ: VIM not properly embedding value distribution
  - Low win ratio in human evaluation: Generated arguments not value-consistent
  - ChatGPT avoidance in ESS: Model refuses to answer opinion questions

- First 3 experiments:
  1. Run PVQ task on VILLAMA vs. LLAMA Long to confirm value alignment
  2. Human evaluation of argument generation for a small set of target distributions
  3. Behavior prediction on VALNET for a single value (e.g., Achievement) to test generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of γ (threshold parameter) in the Argument Generation method affect the quality and alignment of generated arguments with target value distributions?
- Basis in paper: [explicit] The paper mentions that γ is set to 3 as a fixed hyperparameter but acknowledges that appropriate γ may vary depending on the specific value distribution.
- Why unresolved: The paper uses a fixed γ=3 without exploring different values or their impact on performance.
- What evidence would resolve it: Experiments testing different γ values (e.g., 2, 3, 4, 5) and measuring their effect on NMSE scores across different value distributions.

### Open Question 2
- Question: Can Value Injection Method be effectively scaled to individual-level value distributions rather than just group-level distributions?
- Basis in paper: [explicit] The paper states that their exploration has been limited to group value distributions due to lack of individual-level Schwartz value datasets.
- Why unresolved: The paper explicitly acknowledges this limitation but does not test individual-level predictions.
- What evidence would resolve it: Collecting individual-level Schwartz value data and conducting experiments comparing individual vs. group-level prediction accuracy.

### Open Question 3
- Question: What is the optimal temperature and top-p setting for balancing creativity and accuracy in value-injected LLMs across different tasks?
- Basis in paper: [inferred] The paper tests temperature and top-p adjustments but finds minimal impact (less than 0.005 NMSE difference), suggesting potential for optimization.
- Why unresolved: The paper only tests a limited range of temperature and top-p values and concludes minimal impact without exploring whether different tasks might benefit from different settings.
- What evidence would resolve it: Systematic testing of broader temperature (0.1-1.0) and top-p (0.1-1.0) ranges for each task type to identify optimal settings for different applications.

## Limitations

- Value framework generalizability: The method relies on Schwartz's theory of ten universal values, which may not capture all cultural or contextual nuances in opinions and behaviors.
- Dataset representativeness: The Touché23-ValueEval dataset and ESS-derived value distributions may not fully represent the diversity of human values and opinions.
- Parameter efficiency of fine-tuning: While LoRA is used to reduce computational cost, the extent to which the fine-tuned model maintains general capabilities while specializing in value prediction remains unclear.

## Confidence

- High Confidence: The experimental results showing VIM's superior performance over prompting baselines (NMSE of 0.099 vs. 0.137-0.221) are well-supported by the methodology and evaluation metrics used.
- Medium Confidence: The claim that Schwartz values can effectively predict opinions and behaviors through LLM alignment is plausible but depends heavily on the quality and representativeness of the underlying datasets.
- Low Confidence: The generalizability of results to non-European populations or different cultural contexts is uncertain due to the dataset limitations.

## Next Checks

1. Cross-cultural validation: Test VIM on value distributions from non-European populations (e.g., World Values Survey) to assess generalizability beyond the ESS dataset.

2. Ablation study on fine-tuning components: Evaluate the individual and combined contributions of AG and QA fine-tuning methods by systematically removing each component and measuring performance degradation.

3. Out-of-distribution behavior prediction: Evaluate the model on behavioral prediction tasks from domains not represented in the training data (e.g., political behaviors beyond ESS questions) to test true generalization capabilities.