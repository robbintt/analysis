---
ver: rpa2
title: Flexible Phase Dynamics for Bio-Plausible Contrastive Learning
arxiv_id: '2302.12431'
source_url: https://arxiv.org/abs/2302.12431
tags:
- learning
- phase
- dynamics
- gradient
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how to make contrastive learning (CL) more
  flexible for implementation in biological and neuromorphic systems. The authors
  propose a stochastic gradient estimator that samples either a positive or negative
  phase at each update, replacing the traditional two-term gradient and eliminating
  the need for temporally non-local memory storage.
---

# Flexible Phase Dynamics for Bio-Plausible Contrastive Learning

## Quick Facts
- arXiv ID: 2302.12431
- Source URL: https://arxiv.org/abs/2302.12431
- Reference count: 36
- One-line primary result: An importance-sampling approach to contrastive learning enables temporally local updates with comparable performance to standard methods

## Executive Summary
This paper proposes a flexible approach to contrastive learning that enables temporally local updates, making it more suitable for biological and neuromorphic implementations. The authors develop an importance-sampling-inspired gradient estimator that randomly samples either positive or negative phases at each update, eliminating the need for storing both phases simultaneously. The method maintains asymptotic convergence while tolerating variable phase lengths and constant learning rates, which are more biologically plausible constraints than traditional contrastive learning approaches.

## Method Summary
The method replaces the standard two-term contrastive learning gradient with a single-sample estimate that randomly selects either a positive or negative phase at each update. The gradient term is weighted by the inverse of its selection probability (b for positive, 1-b for negative), ensuring unbiasedness. The algorithm operates with constant learning rates and variable phase lengths, using separation of timescales arguments to show that the introduced bias remains small. Experiments compare this approach against standard contrastive learning methods on RBMs trained with Bars and Stripes data and feed-forward networks trained on MNIST.

## Key Results
- The importance-sampled gradient estimator is unbiased and shows comparable learning performance to standard two-term contrastive learning
- The method tolerates variable phase lengths and constant learning rates while maintaining asymptotic convergence
- Performance is robust to changes in the proportion of positive phases, with an optimal value around b=0.2-0.3 depending on the relative variances of gradient terms

## Why This Works (Mechanism)

### Mechanism 1
Importance sampling can replace the two-phase gradient in contrastive learning with a single-sample estimate while remaining unbiased. The algorithm randomly selects either a positive or negative sample at each update, weighting the gradient term by the inverse of the selection probability. This works because the original two-term gradient is an unbiased estimator, and sampling one term at a time with appropriate weighting preserves this property.

### Mechanism 2
Contrastive learning can function with constant learning rates and variable phase lengths without losing asymptotic convergence. The bias introduced by always-on learning and random phase lengths is shown to be O(ηs/τ) + O(η²τ), which can be made arbitrarily small by appropriate choice of learning rate, phase length, and deterministic parameter s.

### Mechanism 3
The variance of the importance-sampled gradient estimate depends non-trivially on the proportion of positive phases (b), allowing for optimization of this parameter. The trace of the covariance matrix of the gradient estimate is a convex function of b, with the minimum occurring at a unique value that depends on the relative variances of the positive and negative gradient terms.

## Foundational Learning

- Concept: Unbiased gradient estimation
  - Why needed here: The paper's main contribution is showing that the importance-sampled gradient estimator is unbiased, which is crucial for the algorithm's theoretical validity.
  - Quick check question: If we sample positive phases with probability b=0.5 and negative phases with probability b=0.5, what should be the weights applied to each gradient term to maintain unbiasedness?

- Concept: Importance sampling
  - Why needed here: The ISD gradient estimator is a form of importance sampling, where we sample one term at a time and weight it by the inverse of the sampling probability.
  - Quick check question: In importance sampling, why do we weight the sampled term by the inverse of the sampling probability?

- Concept: Convergence rates of Markov chains
  - Why needed here: The paper's analysis of the bias introduced by constant learning rates and variable phase lengths relies on the assumption that the equilibrium dynamics converge to a stationary distribution with a certain rate.
  - Quick check question: What is the difference between weak convergence and total variation convergence for Markov chains, and why is weak convergence sufficient for the paper's analysis?

## Architecture Onboarding

- Component map:
  Data pipeline -> Network -> Phase selector -> Gradient estimator -> Parameter updater -> Phase length controller

- Critical path:
  1. Load positive sample or generate negative sample
  2. Run network dynamics for the selected phase length
  3. Compute ISD gradient estimate
  4. Update network parameters with constant learning rate
  5. Repeat

- Design tradeoffs:
  - Memory vs. variance: The ISD estimator requires less memory (no need to store both positive and negative phase data) but has higher variance than the standard two-term estimator.
  - Learning rate vs. phase length: The bias introduced by constant learning rates and variable phase lengths can be reduced by decreasing the learning rate or increasing the phase length, but this may slow down learning.
  - Positive vs. negative phase proportion: The optimal proportion of positive to negative phases (b) depends on the relative variances of the gradient terms, and may not always be 0.5.

- Failure signatures:
  - High variance in the learning curve: This may indicate that the ISD estimator's variance is too high, and the learning rate should be decreased or the phase length increased.
  - Slow convergence: This may indicate that the bias introduced by constant learning rates and variable phase lengths is too high, and the learning rate should be decreased or the phase length increased.
  - Poor performance with certain values of b: This may indicate that the chosen value of b is not optimal for the given dataset and model, and should be tuned.

- First 3 experiments:
  1. Train a simple RBM on the BAS dataset using the ISD estimator with b=0.5 and compare the learning curve to the standard two-term estimator.
  2. Train the same RBM using the ISD estimator with constant learning rate and variable phase lengths, and compare the learning curve to the standard estimator with modulated learning rates and fixed phase lengths.
  3. Train the RBM using the ISD estimator with different values of b (e.g., 0.2, 0.5, 0.8) and compare the learning curves to determine the optimal value of b for this dataset and model.

## Open Questions the Paper Calls Out

### Open Question 1
Does the variance-bias trade-off observed for temporal locality also apply to spatial locality in bio-plausible learning algorithms? The authors note this is a "clear example of an (at least approximately) unbiased algorithm that trades lower variance for locality" and that "such a trade-off has been noted before for spatial locality (Richards et al., 2019), as opposed to the temporal locality explored in this study."

### Open Question 2
Can the positive phase probability parameter b be leveraged to achieve computational benefits in sparse data regimes? The authors state "One might be able to trade compute for data in sparse data regimes. In the case of a very low b an algorithm might still perform well using few positive phases—and thus few data points—compared with the equal number of positive and negative phases required by a traditional CL approach."

### Open Question 3
Is the lower bias observed for deterministic dynamics versus stochastic dynamics a fundamental property or an artifact of proof techniques? The authors state "It is an open question whether this difference in bias between deterministic and stochastic dynamics is a fundamental property or is purely a consequence of the techniques used in the proof."

## Limitations
- The increased variance of the ISD estimator compared to standard two-term methods is not quantified in terms of the memory savings achieved
- The theoretical analysis assumes bounded gradients and Lipschitz continuous dynamics, which may not hold for all neural network architectures
- The constants in the convergence bounds are not computed, making it difficult to assess when the method would outperform traditional approaches in practice

## Confidence
- Theoretical claims about unbiasedness and convergence bounds: High
- Empirical claims about comparable performance: Medium
- Claims about optimal positive phase proportion b: Low (based on synthetic Gaussian experiments only)

## Next Checks
1. Measure the exact variance of the ISD estimator across different datasets and architectures, comparing it quantitatively to the memory savings achieved.
2. Compute concrete values for the convergence bounds with specific parameter choices to determine when ISD becomes preferable to standard CL methods.
3. Test the algorithm's sensitivity to phase length distribution (not just mean) by comparing fixed vs. stochastic phase lengths with identical expected values.