---
ver: rpa2
title: 'Explain-then-Translate: An Analysis on Improving Program Translation with
  Self-generated Explanations'
arxiv_id: '2311.07070'
source_url: https://arxiv.org/abs/2311.07070
tags:
- string
- explanation
- list
- java
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using self-generated natural language explanations
  to improve program translation across 19 programming languages. The authors find
  that explanations are most effective in the zero-shot setting, improving translation
  accuracy by 12% on average.
---

# Explain-then-Translate: An Analysis on Improving Program Translation with Self-generated Explanations

## Quick Facts
- arXiv ID: 2311.07070
- Source URL: https://arxiv.org/abs/2311.07070
- Authors: 
- Reference count: 37
- Key outcome: Self-generated natural language explanations improve zero-shot program translation accuracy by 12% on average across 19 programming languages

## Executive Summary
This paper investigates using self-generated natural language explanations to improve program translation across 19 programming languages. The authors find that explanations are most effective in the zero-shot setting, improving translation accuracy by 12% on average. Detailed explanations help more when translating to high-resource languages, while abstract explanations are better for low-resource targets. The approach is robust across different model sizes and even works when source programs are obfuscated.

## Method Summary
The paper adapts the MultiPL-E dataset to create MultiPL-C2C, a code-to-code translation benchmark using Python as the source language. Three types of self-generated explanations are tested: exp (basic explanation), exp-lbl (explanation with labels), and exp-lbl-d (detailed explanation with labels). The method uses prompt-based translation with GPT-3.5, comparing translation performance with and without explanations in both zero-shot and few-shot settings.

## Key Results
- Self-generated explanations improve zero-shot program translation accuracy by 12% on average
- Detailed explanations benefit high-resource target languages while abstract explanations help low-resource languages
- Explanations are less helpful in few-shot settings but good examples remain important
- Improvements are greater for harder translation tasks
- Self-generated NL explanations outperform self-generated intermediate programming languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-generated natural language explanations improve zero-shot program translation by providing contextual semantic information that bridges the gap between source and target languages.
- Mechanism: The model generates explanations that capture the semantic intent of the source code, which then guides the translation process by making the target code generation more aligned with the original purpose.
- Core assumption: The semantic information captured in the natural language explanation is more transferable across programming languages than the syntactic details of the source code.
- Evidence anchors:
  - [abstract] "explanations are most effective in the zero-shot setting, improving translation accuracy by 12% on average"
  - [section] "Natural language explanations improve performance in the zero-shot setting, and this effect is more pronounced in low-resource languages"
- Break condition: If the model cannot generate meaningful explanations for the source code, or if the explanation process introduces incorrect semantic information.

### Mechanism 2
- Claim: Detailed explanations are more beneficial when translating to high-resource languages due to better co-occurrence of NL and PL in pretraining data.
- Mechanism: High-resource languages have more training data where natural language and programming language co-occur, making the model better at leveraging detailed explanations.
- Core assumption: The pretraining corpus contains more instances of detailed natural language descriptions paired with high-resource programming languages.
- Evidence anchors:
  - [section] "High-resource target languages benefit from detailed explanations while low-resource alternatives benefit from abstract explanations"
  - [section] "We hypothesize that high-resource languages benefit from more detailed explanations due to higher co-occurrences of NL and PL in the pretraining corpora"
- Break condition: If the pretraining corpus does not show the assumed co-occurrence pattern, or if the model cannot effectively use the detailed information.

### Mechanism 3
- Claim: The effectiveness of explanations varies based on the difficulty of the translation task, with more improvements seen on difficult programs.
- Mechanism: Explanations provide additional context that helps the model overcome challenges in translating complex or semantically ambiguous code.
- Core assumption: The additional context from explanations is particularly valuable when the direct translation process struggles.
- Evidence anchors:
  - [section] "Improvements with natural language explanations are particularly pronounced on difficult programs"
  - [section] "We investigate how exp improvement varies across problem hardness, which is approximated through direct translation pass@1"
- Break condition: If the model can handle difficult translations without additional context, or if explanations introduce noise that confuses the translation process.

## Foundational Learning

- Concept: Zero-shot learning in the context of program translation
  - Why needed here: Understanding how models can translate between programming languages without prior examples of that specific translation direction
  - Quick check question: What is the main challenge of zero-shot program translation compared to few-shot translation?

- Concept: Natural language as an intermediate representation
  - Why needed here: The paper uses self-generated NL explanations as an intermediate step between source and target code
  - Quick check question: Why might natural language be a better intermediate representation than another programming language?

- Concept: Resource levels in programming languages
  - Why needed here: The paper categorizes languages as high-resource, medium-resource, low-resource, and extremely-low-resource
  - Quick check question: How might the resource level of a programming language affect its translation to/from other languages?

## Architecture Onboarding

- Component map:
  Source code input -> Explanation generation module -> Translation generation module -> Evaluation system (unit test pass rate)

- Critical path:
  1. Input source code
  2. Generate explanation (exp, exp-lbl, or exp-lbl-d)
  3. Use explanation to guide target code generation
  4. Evaluate generated code against unit tests

- Design tradeoffs:
  - Zero-shot vs few-shot settings: Zero-shot allows for more diverse explanations but may lack specific examples
  - Detailed vs abstract explanations: Detailed explanations provide more context but may introduce noise
  - Explanation reuse: Reusing explanations across translation directions saves computation but may not be optimal for each pair

- Failure signatures:
  - Low pass@1 rates indicating poor translation quality
  - Explanations that do not align with the source code semantics
  - Models that overfit to the explanation format rather than the content

- First 3 experiments:
  1. Compare direct translation vs translation with explanations (exp) in zero-shot setting
  2. Test different explanation types (exp, exp-lbl, exp-lbl-d) across various language pairs
  3. Implement and evaluate explanation selection heuristics (length, logprob, etc.)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of natural language explanations as intermediate steps generalize to more powerful LLMs like GPT-4?
- Basis in paper: [explicit] "while we saw our conclusions to largely hold across GPT-3.5 and other open-source models, it is unclear whether they will hold for more powerful LLMs such as GPT-4."
- Why unresolved: The paper only tested GPT-3.5 and open-source models, not the latest frontier models.
- What evidence would resolve it: Replicating the experiments with GPT-4 and comparing the results to the GPT-3.5 findings.

### Open Question 2
- Question: Can a classifier or heuristic be developed to automatically select which programs to explain versus translate directly, based on program difficulty?
- Basis in paper: [explicit] "Having to approximate hardness through direct translation requires more computation than generating a single explanation. Ideally, one could build classifiers or heuristics to select programs to translate."
- Why unresolved: The paper only explored using direct pass@1 as an oracle to select problems to explain, which is impractical for inference.
- What evidence would resolve it: Developing and evaluating a classifier or heuristic that can predict program difficulty and outperform the baseline of always explaining or never explaining.

### Open Question 3
- Question: How do the length and verbosity of intermediate reasoning steps impact translation performance, and what is the optimal level of detail?
- Basis in paper: [inferred] The paper observes that more detailed explanations do not always lead to better performance, and that heuristically selected explanations tend to be longer. It also notes that formal intermediate steps can be more efficient.
- Why unresolved: The paper does not deeply investigate the relationship between intermediate step length and translation quality, or explore how to optimize the level of detail.
- What evidence would resolve it: Analyzing the correlation between intermediate step length and translation performance across different explanation types and programming languages, and developing methods to generate explanations of optimal length and detail.

## Limitations
- Findings primarily based on Python as source language, limiting generalizability to other source languages
- Focus on pass@1 metrics may not fully capture translation quality for multi-attempt scenarios
- Does not explore computational overhead of explanation generation or potential noise introduction

## Confidence

**High confidence**: The core finding that self-generated explanations improve zero-shot program translation accuracy (12% average improvement) is well-supported by experimental results across 19 programming languages.

**Medium confidence**: The observation that detailed explanations benefit high-resource languages while abstract explanations help low-resource languages is supported by data but relies on assumptions about pretraining corpus composition.

**Low confidence**: The claim about explanations being particularly helpful for difficult translation tasks is based on correlation with direct translation performance rather than independent measures of task difficulty.

## Next Checks

1. **Cross-source validation**: Replicate the experiments using a different source programming language (e.g., Java or C++) to verify that the explanation benefits generalize beyond Python source code.

2. **Ablation on explanation quality**: Systematically evaluate translation performance using both high-quality and intentionally degraded explanations to quantify the impact of explanation accuracy on translation outcomes.

3. **Resource overhead analysis**: Measure the additional computational cost and latency introduced by the explanation generation step, and assess whether the accuracy improvements justify this overhead in practical deployment scenarios.