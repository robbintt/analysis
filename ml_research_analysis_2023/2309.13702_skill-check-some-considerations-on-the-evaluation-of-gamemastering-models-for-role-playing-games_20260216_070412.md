---
ver: rpa2
title: 'Skill Check: Some Considerations on the Evaluation of Gamemastering Models
  for Role-playing Games'
arxiv_id: '2309.13702'
source_url: https://arxiv.org/abs/2309.13702
tags:
- player
- test
- some
- game
- they
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a method to evaluate the capabilities of a
  Game Master (GM) model for role-playing games (RPGs). The method is based on three
  test categories: GM-P-GM pattern, item tracking, and map design.'
---

# Skill Check: Some Considerations on the Evaluation of Gamemastering Models for Role-playing Games

## Quick Facts
- arXiv ID: 2309.13702
- Source URL: https://arxiv.org/abs/2309.13702
- Reference count: 35
- Models tested: ChatGPT, Bard, and OpenAssistant

## Executive Summary
This paper presents a framework for evaluating large language models as automated Game Masters (GMs) for tabletop role-playing games (TTRPGs). The authors propose three test categories—GM-P-GM pattern, item tracking, and map design—to assess models' abilities in validating player actions, maintaining game state, and creating coherent spatial relationships. When tested on ChatGPT, Bard, and OpenAssistant, ChatGPT and Bard outperformed OpenAssistant, though all models struggled with commonsense reasoning and creative responsibility.

## Method Summary
The evaluation framework consists of three test categories applied to five unit tests each in Spanish and English. Human evaluators run the tests by providing scenario prompts to the models and checking outputs against expected behaviors. The GM-P-GM pattern tests action validation, item tracking tests state maintenance across interactions, and map design tests spatial coherence. Results are measured by the number of passed tests out of 15 total for each model-language pair.

## Key Results
- ChatGPT and Bard performed better than OpenAssistant in GM evaluation tests
- All models struggled with commonsense reasoning tasks
- Models showed limited creative responsibility in generating varied narratives
- Performance varied between Spanish and English language tests

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The GM-P-GM pattern test reveals whether a model can detect logical inconsistencies in player actions within the game world.
- Mechanism: By formalizing the interaction as GM1 → Player → GM2, the test isolates the model's ability to validate actions against the established world state and context.
- Core assumption: The model has enough world knowledge and commonsense reasoning to recognize when a player's action contradicts the game state.
- Evidence anchors:
  - [abstract] "The GM-P-GM pattern tests the model's ability to validate the feasibility of a player's action in a given context."
  - [section] "If the GM model prevents the action and explains why it is an inconsistency, the test is passed."
  - [corpus] Weak evidence - no direct corpus support found for GM-P-GM pattern specifically.
- Break condition: If the model generates outputs that accept impossible actions without flagging inconsistencies, or fails to generate coherent GM2 responses.

### Mechanism 2
- Claim: Item tracking tests evaluate whether the model maintains accurate internal state of game objects across multiple interactions.
- Mechanism: The test requires the model to correctly update and recall object locations through a sequence of player actions, ensuring consistency in the game world.
- Core assumption: The model maintains a persistent and accurate representation of game state that persists across conversational turns.
- Evidence anchors:
  - [abstract] "The item tracking category tests the model's ability to track the location and properties of items in the game world."
  - [section] "If the lists given in steps #2 and #6 are the same or contain other unrelated objects, then the test is failed."
  - [corpus] Weak evidence - corpus mentions related work on game state tracking but not this specific methodology.
- Break condition: When the model lists incorrect items in a character's inventory or fails to track object movements between locations.

### Mechanism 3
- Claim: Map design tests assess whether the model can create and maintain coherent spatial relationships in the game world.
- Mechanism: By asking the model to describe accessible locations and track movement between them, the test verifies the logical consistency of the game world's geography.
- Core assumption: The model can generate and maintain a consistent mental map of the game world that respects spatial logic.
- Evidence anchors:
  - [abstract] "The map design category tests the model's ability to coherently design a map for the players to explore."
  - [section] "If during this process the available locations are coherent, the test is passed."
  - [corpus] Weak evidence - related corpus mentions map coherence but not this specific testing approach.
- Break condition: When the model allows illogical movements (e.g., returning to a location that should no longer be accessible) or creates inconsistent spatial relationships.

## Foundational Learning

- Concept: Commonsense reasoning in NLP
  - Why needed here: GM models must understand what actions make sense in the game world context, not just semantically valid text.
  - Quick check question: Can the model distinguish between a player trying to use a bucket of water that doesn't exist in their inventory versus one that does?

- Concept: Dialogue system state tracking
  - Why needed here: The GM must maintain consistent game state (character locations, inventory, world conditions) across multiple conversational turns.
  - Quick check question: After a player moves an object from their backpack to the floor, does the model correctly update both locations in subsequent queries?

- Concept: Narrative coherence and consistency
  - Why needed here: The GM must create a world where spatial relationships and cause-effect chains make logical sense throughout the gameplay session.
  - Quick check question: If a player moves from location A to B and back to A, are the available exits from A consistent with the original description?

## Architecture Onboarding

- Component map: Human evaluator -> Test scenario prompt -> LLM response -> Evaluation criteria -> Pass/Fail determination -> Aggregate results
- Critical path: For each test, the critical path is: 1) Generate initial game state prompt, 2) Send player action, 3) Evaluate model response for correctness, 4) Repeat steps 2-3 for test completion, 5) Determine pass/fail based on consistency and coherence.
- Design tradeoffs: The test categories trade off between testing specific skills (commonsense reasoning, state tracking, spatial coherence) and creating a comprehensive evaluation framework. The current design favors isolation of individual capabilities over integrated gameplay testing.
- Failure signatures: Common failure modes include: accepting impossible actions without question, failing to update game state consistently, generating contradictory spatial relationships, and producing outputs that don't respond to the test prompts.
- First 3 experiments:
  1. Run the GM-P-GM test with a simple scenario where the player tries to use an item they don't have, verifying the model flags this inconsistency.
  2. Execute the item tracking test with a single object movement to check basic state maintenance capabilities.
  3. Perform the map design test with a minimal three-location map to verify basic spatial coherence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop more effective methods to evaluate the creativity and improvisational skills of GM models beyond the proposed test categories?
- Basis in paper: [explicit] The authors discuss the challenges of evaluating creative systems and mention that their proposed test categories may not capture all aspects of a GM's skills, such as emotional variation during interactions.
- Why unresolved: The proposed test categories focus on specific skills like commonsense reasoning and item tracking, but there may be other important aspects of GMing that require different evaluation methods.
- What evidence would resolve it: Developing and testing new evaluation methods that can assess a wider range of GM skills, including emotional intelligence, adaptability, and creativity, and comparing their effectiveness to the proposed test categories.

### Open Question 2
- Question: How can we address the bias in GM models towards medieval-fantasy settings and limited plot diversity?
- Basis in paper: [explicit] The authors observe that the models tend to generate medieval-fantasy settings and repetitive plots, which they attribute to the bias in the training data.
- Why unresolved: The bias in the training data is a fundamental issue that requires careful curation of diverse datasets and possibly domain adaptation techniques to address.
- What evidence would resolve it: Conducting experiments with GM models trained on more diverse datasets covering various RPG themes and settings, and evaluating their ability to generate different plot structures and settings.

### Open Question 3
- Question: How can we improve the interpretability and controllability of GM models to enable human-in-the-loop features and serious game applications?
- Basis in paper: [explicit] The authors emphasize the importance of having human-readable representations of the narrative structure and game state for educational and therapy applications.
- Why unresolved: Current GM models, especially large language models, lack transparency in their decision-making process and are difficult to control, limiting their potential for serious game applications.
- What evidence would resolve it: Developing GM models with explainable AI techniques that can provide clear justifications for their actions and decisions, and incorporating human-in-the-loop features that allow users to customize and guide the narrative.

## Limitations
- The evaluation framework's validity depends on whether the three test categories comprehensively capture GM capabilities
- The corpus evidence is notably weak, with limited direct support for the specific GM-P-GM pattern methodology
- The study's focus on Spanish and English languages may not generalize to other languages or cultural contexts

## Confidence
- **High Confidence:** The methodology for each individual test category (GM-P-GM pattern, item tracking, map design) is clearly defined and implementable.
- **Medium Confidence:** The relative performance differences between models (ChatGPT > Bard > OpenAssistant) are likely valid, though absolute performance metrics may vary with different evaluators.
- **Low Confidence:** The claim that these three test categories comprehensively evaluate GM capabilities, given the limited evidence for the specific methodologies used.

## Next Checks
1. Reproduce the evaluation with a different set of human evaluators to assess inter-rater reliability and consistency in test judgment.
2. Test the framework with additional large language models not included in the original study to validate generalizability of the results.
3. Apply the test categories to a smaller, more controlled set of scenarios where ground truth answers are known, to verify the evaluation criteria are properly calibrated.