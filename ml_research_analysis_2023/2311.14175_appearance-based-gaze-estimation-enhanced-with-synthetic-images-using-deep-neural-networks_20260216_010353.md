---
ver: rpa2
title: Appearance-based gaze estimation enhanced with synthetic images using deep
  neural networks
arxiv_id: '2311.14175'
source_url: https://arxiv.org/abs/2311.14175
tags:
- gaze
- dataset
- head
- estimation
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a system for appearance-based eye gaze estimation
  that uses a standard RGB camera and combines off-the-shelf components for face detection
  and head pose estimation with a custom convolutional neural network. The network
  takes as input separately cropped eye images and predicts 2D gaze direction (pitch
  and yaw) in degrees.
---

# Appearance-based gaze estimation enhanced with synthetic images using deep neural networks

## Quick Facts
- arXiv ID: 2311.14175
- Source URL: https://arxiv.org/abs/2311.14175
- Reference count: 17
- Mean absolute error below 2 degrees in both pitch and yaw directions using combined Columbia Gaze and synthetic MetaHuman datasets

## Executive Summary
This paper proposes a modular appearance-based gaze estimation system using a standard RGB camera, combining off-the-shelf components for face detection (RetinaFace) and head pose estimation (6DRepNet) with a custom CNN for gaze prediction. The key innovation is generating a synthetic dataset of 57,375 images using MetaHuman, which includes controlled head pose and eye gaze information. By training the CNN on both synthetic and real Columbia Gaze datasets, the authors achieved improved accuracy with mean absolute error below 2 degrees in both pitch and yaw directions, demonstrating feasibility for real-world human-robot interaction scenarios.

## Method Summary
The system processes RGB camera frames through OpenCV preprocessing, followed by RetinaFace for face and eye landmark detection. The 6DRepNet model estimates head pose, which is then combined with separately cropped left and right eye images (100×300 pixels) as input to a custom CNN. The CNN outputs 2D gaze direction in degrees (pitch and yaw). Training uses MSE loss with early stopping on a combined dataset of 5,880 Columbia Gaze images and 57,375 synthetic MetaHuman images, with image mirroring for data augmentation.

## Key Results
- Achieved mean absolute error below 2 degrees for both pitch and yaw gaze directions
- Synthetic MetaHuman dataset improved accuracy compared to using Columbia Gaze alone
- Validated system performance using both notebook webcam and NICO robot's 4K camera
- Successfully handled head poses up to 60° pitch and 90° yaw using 6DRepNet

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining synthetic and real gaze datasets improves model generalization
- Mechanism: Synthetic images from MetaHuman provide large-scale, controlled, and diverse head pose and gaze direction combinations, compensating for real-world data scarcity and lighting variability
- Core assumption: Synthetic data distribution overlaps sufficiently with real-world appearance for effective transfer learning
- Evidence anchors:
  - The inclusion of this dataset (with eye gaze and head pose information) on top of the standard Columbia Gaze dataset into training the model led to better accuracy with a mean average error below two degrees in eye pitch and yaw directions
  - In order to expand the training data set, we took advantage of existing generative AI tools and synthesized human faces in a perfectly controlled way

### Mechanism 2
- Claim: Using pre-trained, well-functioning components for face and head pose detection enables focus on the gaze estimation task
- Mechanism: RetinaFace handles accurate face and eye landmark detection, while 6DRepNet provides robust head pose estimation, allowing the custom CNN to focus only on the regression from cropped eye images to gaze angles
- Core assumption: Accurate eye cropping and head pose estimation are sufficient for gaze estimation without modeling full facial geometry
- Evidence anchors:
  - We used a well-known RetinaFace, the pre-trained model that works well on a variety of datasets. It provides accurate positions of all faces on the image at multiple spatial scales
  - The ability of 6DRepNet to learn and predict head positions beyond the narrow-angle constraints of most other models is one of its key advantages

### Mechanism 3
- Claim: Adding head pose information to the final fully connected layer improves gaze estimation accuracy
- Mechanism: Head pose acts as a context feature that helps disambiguate gaze direction when the eyes are partially occluded or in non-frontal views
- Core assumption: The relationship between head pose and gaze direction is sufficiently regular to aid the CNN's regression
- Evidence anchors:
  - Adding this information to deeper layers did not help, perhaps due to huge number of feedforward channels coming from convolutional layers. However, adding head pose information to the last hidden layer, containing only 53 units, had a positive effect and led to improvement in the model accuracy approximately by 0.2°
  - The estimated head position is provided to the last fully connected layer with an expectation to contribute to calculating the final result

## Foundational Learning

- Concept: Convolutional Neural Networks (CNNs) for image-based regression
  - Why needed here: The system must learn a mapping from cropped eye images to continuous gaze angles (pitch and yaw), which is a regression task well-suited to CNNs
  - Quick check question: What is the difference between using a CNN for classification vs. regression in gaze estimation?

- Concept: Transfer learning from synthetic to real data
  - Why needed here: Real-world gaze datasets are expensive and limited; synthetic data from MetaHuman can be generated in large quantities with controlled conditions, allowing augmentation of real data
  - Quick check question: How can synthetic data improve model generalization if it does not perfectly match real-world conditions?

- Concept: Multi-component vision pipelines (face detection, landmark detection, pose estimation)
  - Why needed here: Each subcomponent (RetinaFace, 6DRepNet) handles a well-defined task, enabling modularity and reuse of robust pre-trained models
  - Quick check question: Why is it beneficial to use separate pre-trained models for face detection and head pose estimation rather than training one monolithic model?

## Architecture Onboarding

- Component map: Camera frames -> OpenCV preprocessing -> RetinaFace -> 6DRepNet -> Eye cropping -> CNN -> Gaze angles
- Critical path: Camera → RetinaFace → 6DRepNet → Eye cropping → CNN → Gaze angles
- Design tradeoffs:
  - Smaller eye crops (100×300) reduce computation and training time but may lose fine details
  - Concatenating separate eye images instead of joint cropping improves robustness to head roll
  - Adding head pose to the final FC layer rather than deeper layers avoids overwhelming the feature space
- Failure signatures:
  - Low accuracy when eyes are not well visible (looking down, glasses occlusion)
  - Performance drops under extreme head poses if RetinaFace or 6DRepNet fails
  - Model instability under variable lighting if synthetic-real domain gap is large
- First 3 experiments:
  1. Train CNN on Columbia Gaze dataset only; evaluate cross-validation MAE
  2. Train CNN on MetaHuman dataset only; evaluate cross-dataset MAE
  3. Train CNN on combined Columbia + MetaHuman dataset; evaluate improvement over individual datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the eye gaze estimation model vary across different lighting conditions when using the combined Columbia Gaze and MetaHuman datasets?
- Basis in paper: The paper mentions that the model trained on the combined dataset performs better in worse lighting scenarios than the one trained only on the Columbia Gaze dataset
- Why unresolved: While the paper indicates better performance in worse lighting, it does not provide specific quantitative comparisons of model accuracy across different lighting conditions
- What evidence would resolve it: Conducting experiments to evaluate the model's accuracy under various controlled lighting conditions (e.g., low light, bright light, shadows) and comparing the results with the model trained only on the Columbia Gaze dataset

### Open Question 2
- Question: Can the synthetic MetaHuman dataset be further improved to include more diverse scenarios such as different facial accessories (e.g., glasses) and varied lighting conditions?
- Basis in paper: The paper mentions that adding accessories such as glasses to the characters would require third-party 3D modeling software and basic modeling skills, indicating potential for further improvement
- Why unresolved: The paper does not explore the impact of including such variations in the synthetic dataset on the model's performance and robustness
- What evidence would resolve it: Generating an expanded synthetic dataset with additional facial accessories and varied lighting conditions, then evaluating the model's performance on this enhanced dataset compared to the original MetaHuman dataset

### Open Question 3
- Question: How does the eye gaze estimation model perform when the subject is looking down, making the eye pupils less visible?
- Basis in paper: The paper mentions that predicting eye gaze directions when a person is looking down, where the eye pupils are not well visible, is a challenge for any system using a standard RGB camera
- Why unresolved: The paper does not provide experimental results or analysis of the model's performance in such scenarios
- What evidence would resolve it: Conducting experiments to evaluate the model's accuracy when the subject is looking down, and comparing the results with the model's performance in other gaze directions. Additionally, exploring potential solutions such as placing an additional camera at a lower position or relying more on head pose information in such cases

## Limitations

- Reliance on synthetic data may not fully capture real-world variability (glasses, lighting, expressions), introducing uncertainty in generalization
- Claim of achieving <2 degrees MAE is supported by experiments on Columbia Gaze but lacks independent validation on other real-world datasets
- Modest improvement (~0.2°) from head pose integration suggests limited benefit and warrants further exploration

## Confidence

- Mechanism 1: Medium - synthetic data benefits shown but domain gap not fully quantified
- Mechanism 2: High - pre-trained components well-established in literature
- Mechanism 3: Medium - modest improvement but limited architectural exploration
- Overall accuracy claim: Medium-High - results align with related methods but depend on controlled test conditions

## Next Checks

1. **Cross-dataset validation**: Evaluate the trained model on other real-world gaze datasets (e.g., MPIIGaze, EYEDIAP) to assess generalization beyond Columbia Gaze

2. **Robustness under occlusions**: Test model performance with simulated glasses, eye occlusions, or extreme lighting to quantify failure modes and compare against baseline models

3. **Ablation study on head pose integration**: Remove head pose input and retrain the CNN to quantify the exact contribution of head pose features to gaze estimation accuracy