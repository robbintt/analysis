---
ver: rpa2
title: Fairness-Aware Structured Pruning in Transformers
arxiv_id: '2312.15398'
source_url: https://arxiv.org/abs/2312.15398
tags:
- bias
- pruning
- heads
- language
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a structured pruning method for transformer
  models that considers both performance and fairness. The key idea is to first identify
  important attention heads for language modeling and then prune the remaining heads
  based on their contribution to bias.
---

# Fairness-Aware Structured Pruning in Transformers

## Quick Facts
- arXiv ID: 2312.15398
- Source URL: https://arxiv.org/abs/2312.15398
- Authors: 
- Reference count: 18
- Key outcome: Reduces gender bias by 8-39.5% across 6 transformer models while maintaining performance close to existing pruning methods

## Executive Summary
This paper introduces Fairness-Aware Structured Pruning (FASP), a method that reduces social bias in transformer models through targeted attention head pruning. The approach identifies which heads are critical for language modeling performance and which contribute most to bias, then selectively removes heads that harm fairness while preserving performance-critical components. Experiments on 6 transformer models demonstrate that FASP can reduce gender bias by 8-39.5% while maintaining perplexity close to traditional pruning methods.

## Method Summary
FASP computes two impact scores for each attention head: one for language modeling performance (measured via perplexity change) and one for bias contribution (measured via toxicity difference across social groups). The method first identifies top γ × N heads as performance-critical using perplexity scores, then prunes remaining heads based on their bias impact scores. A hyperparameter γ controls the tradeoff between performance preservation and bias reduction, allowing the method to avoid pruning heads important for language modeling while prioritizing removal of heads that negatively impact fairness.

## Key Results
- FASP reduces gender bias by 19-39.5% across DistilGPT-2, GPT-2, GPT-Neo, GPT-J, and Llama 2 models
- The method maintains performance close to existing pruning techniques, with only slight decreases in perplexity
- FASP consistently outperforms other pruning methods in fairness improvement while preserving language modeling ability
- Attention heads contributing to gender bias often also contribute to other social biases, enabling simultaneous reduction of multiple biases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pruning attention heads based on their negative impact on fairness while preserving performance-critical heads reduces social bias without degrading language modeling ability.
- Mechanism: The method computes separate impact scores for each attention head—one for language modeling performance (via perplexity) and one for bias contribution (via toxicity difference). Heads are ranked and pruned based on these scores, with a hyperparameter γ determining the proportion of performance-critical heads to protect from pruning.
- Core assumption: The removal of individual heads can be used as a proxy for their overall contribution to model bias and performance, despite the non-linear interactions between heads.
- Evidence anchors: [abstract] "We propose a novel structured pruning method that considers both fairness and performance. Our method avoids pruning the heads that are important for language modeling, while prioritizing pruning the heads that negatively impact fairness." [section 4] "We quantify the contribution of a given attention head to bias as the difference between the model's bias before and after pruning such head... In a similar vein, the impact of a head h in the context of language modeling is defined as the difference in perplexity."

### Mechanism 2
- Claim: Attention heads that contribute most to gender bias often also contribute to other social biases, enabling simultaneous reduction of multiple biases through gender bias-focused pruning.
- Mechanism: The method exploits positive correlations between attention head impact scores across different social biases. By pruning heads based on their gender bias impact, other positively correlated biases are also reduced.
- Core assumption: Attention heads exhibit consistent bias impact patterns across different social bias types, with positive correlations between gender bias and other social biases.
- Evidence anchors: [section 6, experiment 2] "Figure 4 illustrates a consistent positive correlation among attention head effects across diverse biases, with the exception of the religion bias." [section 6, experiment 3] "Figure 5 shows that the process of pruning attention heads with the most pronounced influence on gender bias leads to a reduction in sexual orientation, race, and nationality biases."

### Mechanism 3
- Claim: The proposed method outperforms existing pruning techniques in fairness improvement while maintaining comparable or better language modeling performance.
- Mechanism: By explicitly considering both fairness and performance when selecting heads for pruning, the method avoids the fairness degradation that occurs with performance-only pruning methods while achieving better bias reduction than fairness-only approaches.
- Core assumption: Existing pruning methods that focus solely on performance do not consider fairness implications, leading to potential bias amplification.
- Evidence anchors: [abstract] "Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model, with only a slight decrease in performance." [section 6, experiment 1] "FASP stands out as the sole pruning method capable of consistently reducing gender bias without perplexity overshooting."

## Foundational Learning

- Concept: Transformer attention mechanisms
  - Why needed here: Understanding how attention heads function is crucial for grasping how pruning specific heads affects model behavior and fairness.
  - Quick check question: What is the role of attention heads in transformer models, and how might removing certain heads impact model outputs?

- Concept: Bias quantification in language models
  - Why needed here: The method relies on measuring bias through toxicity differences across social groups, requiring understanding of how bias is operationalized and measured.
  - Quick check question: How does the paper define and measure bias in language models, and what assumptions underlie this measurement approach?

- Concept: Structured pruning techniques
  - Why needed here: The method is a type of structured pruning, so understanding general pruning concepts and tradeoffs is essential for contextualizing the approach.
  - Quick check question: What are the key differences between structured and unstructured pruning, and what are the typical tradeoffs between performance preservation and pruning aggressiveness?

## Architecture Onboarding

- Component map: Pre-trained transformer model with N attention heads -> Computation of performance and fairness impact scores for each head -> Selection of heads to prune based on scores and hyperparameter γ -> Pruned model with reduced bias and preserved performance

- Critical path:
  1. Compute zppl(h) for each head using validation set perplexity
  2. Identify top γ × N heads as performance-critical (set H')
  3. Compute zbias(S,h) for each non-critical head using validation set bias
  4. Prune α × N heads from H' based on zbias scores

- Design tradeoffs:
  - γ value: Higher γ preserves more performance-critical heads but allows more bias-contributing heads to remain
  - Pruning ratio α: Higher α allows more aggressive bias reduction but risks performance degradation
  - Single vs. multiple bias targets: Focusing on one bias may reduce correlated biases but could miss bias-specific opportunities

- Failure signatures:
  - Perplexity increases significantly after pruning (over-aggressive pruning)
  - Bias metrics do not improve or worsen after pruning (incorrect head impact scoring)
  - Model produces nonsensical outputs after pruning (catastrophic forgetting of language modeling ability)

- First 3 experiments:
  1. Verify that individual head removal affects perplexity and bias as expected by computing zppl and zbias for a few heads
  2. Test different γ values to find the optimal balance between performance preservation and bias reduction
  3. Compare the proposed method against magnitude-based and random pruning baselines to validate fairness improvements

## Open Questions the Paper Calls Out
None explicitly identified in the paper.

## Limitations
- The method assumes that individual head removal accurately represents head contributions to overall model behavior, despite complex non-linear interactions between heads
- The approach focuses primarily on gender bias using a specific measurement methodology, with uncertain generalizability to other bias types
- The experiments cover 6 transformer models but represent a limited sample of the broader transformer landscape

## Confidence
- **High Confidence**: The core experimental methodology is sound and well-documented. The approach of using separate performance and fairness impact scores, combined with hyperparameter γ to balance these objectives, represents a clear and implementable framework.
- **Medium Confidence**: The positive correlation findings between different social biases are empirically supported within the tested models but may not generalize universally.
- **Low Confidence**: The theoretical justification for why individual head removal accurately captures head contributions to model behavior is limited.

## Next Checks
1. Test whether positive correlations between gender bias and other social biases extend to additional transformer architectures and sizes, particularly focusing on biases not examined in the original study
2. Implement FASP using different bias measurement approaches to verify that pruning decisions remain consistent and effective across measurement methodologies
3. Conduct controlled experiments that test head removal in combinations rather than individually to quantify the extent to which non-linear interactions affect the accuracy of proxy impact scores