---
ver: rpa2
title: Toward Interactive Dictation
arxiv_id: '2307.04008'
source_url: https://arxiv.org/abs/2307.04008
tags:
- command
- system
- state
- commands
- dictation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new task and dataset, TERTiUS, for interactive
  dictation - where users can dictate and edit text in an uninterrupted speech stream
  using open-ended natural language commands. The key challenge is segmenting and
  interpreting commands from the speech stream.
---

# Toward Interactive Dictation

## Quick Facts
- arXiv ID: 2307.04008
- Source URL: https://arxiv.org/abs/2307.04008
- Reference count: 31
- Key outcome: Interactive dictation task introduced with TERTiUS dataset; trade-off between model accuracy and latency observed (28% accuracy at 1.3s vs 55% at 7s)

## Executive Summary
This paper introduces interactive dictation - a new task where users can dictate and edit text in an uninterrupted speech stream using natural language commands. The key challenge is segmenting commands from dictation and interpreting them correctly. The authors develop TERTiUS, a dataset of interactive dictation trajectories with annotations for segmentation, normalization, and interpretation. They experiment with large pre-trained language models (T5 and GPT3) to predict either edited text or text-editing programs, finding a trade-off between accuracy and latency that depends on model size and prediction strategy.

## Method Summary
The system processes continuous speech through ASR, then segments the transcript into dictation and command segments using BIOES tagging with a T5-base encoder. Commands are normalized and interpreted using either T5 or GPT3 to predict either the final document state or an intermediate program representation. The predicted states or programs are executed to produce the edited document. The approach leverages pre-trained language models' ability to understand natural language commands and generate appropriate text edits, with program prediction offering latency advantages at the cost of some accuracy.

## Key Results
- Segmentation model achieves 90.9% F1 using BIOES tagging on ASR transcripts
- GPT3 outperforms T5 in interpretation due to larger pretraining data and better natural language generation prior
- Predicting programs instead of states reduces latency but sacrifices accuracy (28% vs 55% end-state accuracy)
- Smaller models achieve faster inference (1.3s) but lower accuracy compared to larger models (7s latency)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Segmentation model achieves 90.9% F1 by treating commands as BIOES-tagged spans in ASR transcript
- Mechanism: T5-base encoder predicts BIOES tags for each token, distinguishing command segments (BI*E or S) from dictation (O). Two dictation segments cannot be adjacent, forcing all consecutive non-command text into single dictation spans.
- Core assumption: Command boundaries align with ASR sentence boundaries or can be detected via token-level tagging
- Evidence anchors:
  - [section] "Concretely, the segmentation model does this using BIOES tagging... Maximal sequences of tokens tagged with O... correspond to the dictation segments."
  - [abstract] "A smaller model achieves 28% end-state accuracy with 1.3 seconds of latency, while a larger model achieves 55% with 7 seconds of latency."
  - [corpus] Weak evidence - no corpus paper explicitly validates BIOES for this task
- Break condition: If commands are embedded mid-sentence without prosodic cues, segmentation fails (as seen in error analysis examples)

### Mechanism 2
- Claim: GPT3 outperforms T5 in interpretation due to larger pretraining data and better natural language generation prior
- Mechanism: GPT3 generates final document states directly from normalized commands, leveraging its broader language model prior to produce coherent text edits. T5-base struggles with this abstraction and is better at generating short programs.
- Core assumption: GPT3's pretraining data includes sufficient text editing and instruction-following examples to generalize to novel commands
- Evidence anchors:
  - [section] "GPT3 generally outperforms T5, likely due to its larger-scale pretraining... GPT3 was better than T5 at both ASR repair and interpretation."
  - [abstract] "a smaller model achieves 30% end-state accuracy... while a larger model achieves 55%..."
  - [corpus] Weak evidence - no corpus paper directly compares GPT3 vs T5 for this specific task
- Break condition: If commands use highly domain-specific language or rare editing patterns, GPT3's prior may mislead generation

### Mechanism 3
- Claim: Predicting programs instead of states reduces latency at cost of accuracy
- Mechanism: MINT(program) generates short Lisp-like programs describing edits (e.g., (replace (theText (like "S")) "X")), which are then executed. Programs are shorter than full document states, reducing inference time.
- Core assumption: Program execution is deterministic and correctly implemented, and program annotations in training data are accurate
- Evidence anchors:
  - [section] "Predicting intermediate programs reduces latency because the programs are short... this strategy also requires additional work to design and implement a set of editing functions and annotate commands with programs."
  - [abstract] "a smaller model achieves 30% end-state accuracy with 1.3 seconds of latency, while a larger model achieves 55% with 7 seconds of latency."
  - [corpus] Weak evidence - no corpus paper validates this specific latency-accuracy tradeoff
- Break condition: If execution engine has bugs or program annotations are noisy, predicted programs fail to reconstruct correct states

## Foundational Learning

- Concept: BIOES tagging for sequence labeling
  - Why needed here: Distinguishes command segments (B, I, E, S) from dictation (O) in continuous ASR transcript
  - Quick check question: Why can't we just use binary command/dictation tags for each token?

- Concept: Chain-of-thought prompting
  - Why needed here: GPT3's program prediction benefits from seeing intermediate program representation before final state
  - Quick check question: What auxiliary task did early experiments find helpful for state prediction?

- Concept: Program synthesis from natural language
  - Why needed here: MINT(program) must map natural language commands to executable text-editing programs
  - Quick check question: What's the advantage of predicting programs vs directly predicting final document states?

## Architecture Onboarding

- Component map: ASR → Segmentation (MSEG) → Normalization (MNOR) → Interpretation (MINT) → Execution Engine → Document State
- Critical path: User speech → ASR results → Segmentation model → Normalization/Interpretation models → Document update
- Design tradeoffs: GPT3 vs T5 (accuracy vs latency), program prediction vs direct state prediction (latency vs accuracy), incremental vs batch processing
- Failure signatures: 
  - Segmentation errors: Commands misclassified as dictation or vice versa, especially when phrased like dictation
  - Interpretation errors: Generated text doesn't match intended edits, programs don't execute correctly
  - Runtime errors: Model inference time exceeds real-time constraints
- First 3 experiments:
  1. Evaluate segmentation model F1 on held-out transcripts to verify BIOES tagging works
  2. Compare GPT3 vs T5 interpretation accuracy on normalized commands to validate pretraining advantage
  3. Measure program vs state prediction latency/accuracy tradeoff on same test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using speech lattices or n-best lists from ASR on the accuracy of command interpretation in interactive dictation?
- Basis in paper: [inferred] The paper mentions that incorporating speech lattices or n-best lists could be useful for fixing misrecognitions and determining what text the user is referring to, but does not explore this.
- Why unresolved: The paper focuses on using only the top ASR result and does not experiment with alternative ASR representations that could provide additional context for interpretation.
- What evidence would resolve it: Experiments comparing the accuracy of command interpretation using only the top ASR result versus using speech lattices or n-best lists would provide evidence on the potential benefits of these alternative representations.

### Open Question 2
- Question: How does the performance of the interactive dictation system vary across different languages and dialects?
- Basis in paper: [inferred] The paper mentions that the current dataset and system only support English and could be extended to other languages, but does not provide any data on cross-linguistic performance.
- Why unresolved: The paper focuses on developing the task and dataset for English and does not evaluate the system's performance on other languages or dialects.
- What evidence would resolve it: Collecting data and evaluating the system on multiple languages and dialects would provide insights into the generalizability of the approach and potential challenges in extending it to other languages.

### Open Question 3
- Question: What is the effect of incorporating prosody information on the segmentation and interpretation of commands in interactive dictation?
- Basis in paper: [inferred] The paper acknowledges that prosodic cues are often used in human communication to mark command boundaries but does not explore how to incorporate this information into the system.
- Why unresolved: The paper focuses on textual ASR output and does not address the challenge of integrating prosodic information into the segmentation and interpretation models.
- What evidence would resolve it: Experiments comparing the performance of the system with and without prosodic information would provide evidence on the potential benefits of incorporating this additional modality.

## Limitations

- Segmentation errors occur when commands are phrased like dictation or embedded mid-sentence without prosodic cues
- GPT3 vs T5 performance comparison lacks rigorous ablation studies to isolate the source of performance differences
- Latency-accuracy tradeoff claims depend on specific implementation choices and hardware not fully characterized
- Evaluation metrics may not capture user experience in interactive settings where partial corrections matter

## Confidence

**High confidence**: The core observation that interactive dictation requires segmenting commands from dictation is well-supported. The BIOES tagging framework is technically sound and the segmentation results (90.9% F1) are internally consistent with the reported methodology.

**Medium confidence**: The latency-accuracy tradeoff claims (28% accuracy at 1.3s vs 55% at 7s) are plausible given the architectural differences described, but depend on unmeasured factors like implementation efficiency and hardware specifications. The GPT3 superiority claim has moderate support but lacks rigorous comparison conditions.

**Low confidence**: Claims about program prediction reducing latency assume perfect execution engine implementation and accurate program annotations, neither of which are empirically validated. The generalization claims to real-world dictation scenarios are largely untested.

## Next Checks

1. **Segmentation boundary validation**: Collect a small test set of utterances where commands are deliberately embedded mid-sentence or phrased like dictation. Measure segmentation F1 specifically on these challenging cases to assess the 90.9% F1 robustness.

2. **End-to-end latency measurement**: Time complete inference pipelines (ASR → segmentation → normalization → interpretation → execution) on identical hardware. Verify the 1.3s vs 7s claims reflect full system latency, not just model inference times.

3. **Program execution accuracy audit**: Manually inspect 50 program predictions and their executed outputs. Calculate the percentage of programs that execute without errors and produce the intended document state to validate the claimed latency advantage of program prediction.