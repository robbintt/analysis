---
ver: rpa2
title: Hierarchical Training of Deep Neural Networks Using Early Exiting
arxiv_id: '2303.02384'
source_url: https://arxiv.org/abs/2303.02384
tags:
- training
- edge
- cloud
- hierarchical
- communication
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of training deep neural networks
  (DNNs) on edge-cloud systems while minimizing communication costs, latency, and
  privacy concerns. The authors propose a novel hierarchical training framework that
  leverages early exiting to divide the training process between edge and cloud workers.
---

# Hierarchical Training of Deep Neural Networks Using Early Exiting

## Quick Facts
- arXiv ID: 2303.02384
- Source URL: https://arxiv.org/abs/2303.02384
- Reference count: 25
- One-line primary result: Proposed hierarchical training method with early exiting achieves 29% and 61% runtime reductions on VGG-16 and ResNet-18 for CIFAR-10 with negligible accuracy drop

## Executive Summary
This paper addresses the challenge of training deep neural networks on edge-cloud systems while minimizing communication costs, latency, and privacy concerns. The authors propose a novel hierarchical training framework that leverages early exiting to divide the training process between edge and cloud workers. By using early exits, the method enables parallel execution of the backward pass on the edge and forward pass on the cloud, eliminating the need for communication during the backward pass. The approach also allows for the use of non-differentiable operations like quantization to further compress communicated data.

## Method Summary
The proposed method introduces early exits in neural networks to separate the backward pass between edge and cloud during training. In the first step, the edge performs its forward pass and computes the early exit loss. In the second step, the edge performs its backward pass while the cloud simultaneously performs its forward pass on the compressed feature map. In the third step, the cloud computes its loss and performs its backward pass. This three-step process eliminates communication during backward passes and enables the use of non-differentiable compression operations like quantization on the feature map before transmission.

## Key Results
- Runtime reductions of 29% and 61% for VGG-16 and ResNet-18 on CIFAR-10 with negligible accuracy drop
- Runtime reductions of 25% and 81% for VGG-16 and ResNet-18 on Tiny ImageNet with negligible accuracy drop
- Effective use of 4-bit quantization to reduce communication costs without significant accuracy degradation
- Edge-only inference capability provides robustness against network failures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Early exiting enables parallel backward and forward passes across edge and cloud, reducing total training latency.
- Mechanism: By placing an early exit after a feature extractor on the edge, the edge can perform its backward pass immediately after its forward pass, while the cloud simultaneously performs its forward pass. This eliminates the sequential bottleneck where the cloud must wait for the edge to finish its backward pass.
- Core assumption: The early exit loss can be computed independently of the cloud loss and does not require gradient communication between edge and cloud during the backward pass.
- Evidence anchors:
  - [abstract]: "The method proposes a brand-new use case for early exits to separate the backward pass of neural networks between the edge and the cloud during the training phase."
  - [section]: "In the second step... the backward pass of the edge layer is also done at the edge level for training the edge feature extractor neural network. Thus, these two tasks are done in parallel, the backward pass of the edge worker and the forward pass of the cloud worker."
  - [corpus]: Weak. No direct evidence in corpus neighbors about parallel backward/forward execution in hierarchical training.

### Mechanism 2
- Claim: Quantization of feature maps after early exiting reduces communication cost without significantly impacting accuracy.
- Mechanism: Since no backward pass occurs at the edge-cloud boundary, non-differentiable operations like 4-bit quantization can be applied to the feature map before transmission. This reduces the communication payload size while maintaining acceptable accuracy through per-batch scaling.
- Core assumption: The quantization error can be compensated by the scale factor, and the reduced precision does not degrade the gradient quality at the cloud sufficiently to harm convergence.
- Evidence anchors:
  - [abstract]: "In addition to using the full penitential of the workers, an important beneﬁt of this approach is that there is no need to perform any communication during the backward pass like the previous works... as the backward pass of each worker is done independently, resulting in a signiﬁcant reduction of communication cost and total runtime."
  - [section]: "As no backward pass happens in the position between the edge and the cloud, non-differentiable functions can be applied to the feature map to further compress it before communication."
  - [corpus]: Weak. No corpus neighbor directly discusses quantization at early exit boundaries.

### Mechanism 3
- Claim: Early exiting provides robustness against network failures by enabling local inference on the edge.
- Mechanism: The early exit at the edge can produce a classification decision independently when the cloud is unreachable. While less accurate than full-cloud inference, it offers acceptable performance during network outages.
- Core assumption: The edge-only model trained with the early exit loss generalizes sufficiently to maintain reasonable accuracy without cloud augmentation.
- Evidence anchors:
  - [abstract]: "The early exit at the edge can still be useful in the inference phase when there is a communication network failure. It can provide local decision making at the edge device, with lower but acceptable accuracy, in comparison to the cloud."
  - [section]: "As a side beneﬁt, the test accuracy of the early exit in the edge is shown. Although this accuracy is lower in comparison to the hierarchical cloud, it shows that our proposed method can also provide a level of robustness against network failures."
  - [corpus]: Weak. No corpus neighbor discusses network failure resilience through early exiting.

## Foundational Learning

- Concept: Hierarchical model partitioning and its impact on communication and computation trade-offs
  - Why needed here: Understanding how to split a DNN between edge and cloud while balancing memory, computation, and communication is central to this method's design and optimization.
  - Quick check question: Given a DNN with 10M parameters and an edge device that can handle 1M parameters, at which layer(s) could you split the model while respecting memory constraints?

- Concept: Early exiting in neural networks and its role beyond inference acceleration
  - Why needed here: Early exiting is repurposed from a latency-reduction technique in inference to a mechanism for enabling parallel training across heterogeneous devices.
  - Quick check question: How does adding an early exit change the backpropagation flow compared to a standard end-to-end network?

- Concept: Quantization techniques and their application in communication-constrained environments
  - Why needed here: Quantization is used here not for model compression but for reducing communication payload between edge and cloud without requiring differentiable operations at the split point.
  - Quick check question: What is the trade-off between quantization bit-width and accuracy when compressing feature maps for communication?

## Architecture Onboarding

- Component map: Edge feature extractor -> Early exit with local loss -> Quantization layer -> Communication channel -> Cloud classifier -> Final exit with cloud loss
- Critical path:
  1. Edge forward pass → early exit loss computation
  2. Feature map compression and transmission
  3. Cloud forward pass (parallel with edge backward)
  4. Cloud loss computation and backward pass
  5. Parameter updates independently on edge and cloud
- Design tradeoffs:
  - Separation point selection: Earlier splits reduce edge memory and computation but may hurt accuracy; later splits improve accuracy but increase edge burden
  - Quantization bit-width: Lower bits reduce communication but may degrade accuracy; must balance with acceptable loss
  - Local vs. cloud accuracy: More edge capacity improves local inference robustness but may reduce cloud-side performance if feature extraction is suboptimal
- Failure signatures:
  - Accuracy drop at early exit indicates poor feature extraction or quantization artifacts
  - Runtime not reduced as expected suggests suboptimal separation point or communication bottleneck
  - Memory overflow on edge indicates separation point too deep or feature map too large
- First 3 experiments:
  1. Measure forward pass runtime on edge and cloud separately to validate the calculation method in Section III-B.
  2. Test separation at different layers (e.g., after conv1, conv3, conv5) to observe accuracy-runtime trade-off on CIFAR-10.
  3. Implement 4-bit quantization and measure communication size reduction and accuracy impact compared to full-precision transmission.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed hierarchical training method perform on edge devices with significantly different computational capabilities, such as mobile phones versus robots?
- Basis in paper: [explicit] The paper mentions that the method is advantageous for online learning of high-accuracy DNNs on sensor-holding low-resource devices such as mobile phones or robots as a part of an edge-cloud system.
- Why unresolved: The experiments in the paper were conducted on a NVIDIA Quadro K620 (edge) and NVIDIA GeForce RTX 2080 Ti (cloud) setup, which may not represent the full range of edge devices mentioned.
- What evidence would resolve it: Experiments on a wider range of edge devices with varying computational capabilities would provide insights into the method's performance and applicability.

### Open Question 2
- Question: How does the proposed method perform with different communication protocols or varying network conditions, such as 5G or unstable connections?
- Basis in paper: [explicit] The paper mentions that the proposed method is more advantageous for less efficient communication links, as the communication link is often the bottleneck of the overall efficiency of the hierarchical systems. Experiments were conducted with 3G and 4G protocols.
- Why unresolved: The paper only discusses the performance of the method with 3G and 4G protocols. It does not explore the performance with other communication protocols or varying network conditions.
- What evidence would resolve it: Experiments with different communication protocols and varying network conditions would provide insights into the method's robustness and adaptability.

### Open Question 3
- Question: How can the proposed method be adapted for real-time applications, where the latency of the training process is crucial?
- Basis in paper: [inferred] The paper focuses on reducing the training runtime and communication cost, which are important factors for real-time applications. However, it does not explicitly discuss the method's applicability to real-time scenarios.
- Why unresolved: The paper does not provide a detailed analysis of the method's performance in real-time applications or discuss potential adaptations to meet the stringent latency requirements.
- What evidence would resolve it: Experiments and analysis of the method's performance in real-time applications, along with potential adaptations to meet the latency requirements, would provide insights into its applicability in such scenarios.

## Limitations
- The effectiveness depends heavily on the assumption that early exit loss can be computed independently without gradient communication between edge and cloud
- Quantization approach lacks detailed implementation specifics (e.g., exact bit-widths, scaling strategies) that could significantly impact results
- Limited ablation studies showing individual contributions of parallel execution versus quantization to runtime improvements

## Confidence
- Hierarchical training mechanism with parallel execution: Medium
- Communication cost reduction through quantization: Low
- Network failure robustness: Low

## Next Checks
1. Implement gradient flow verification to confirm that early exit loss computation does not require any backward communication between edge and cloud during training
2. Conduct controlled experiments varying quantization bit-widths (2-bit, 4-bit, 8-bit) to quantify the trade-off between communication reduction and accuracy degradation
3. Simulate network failures during training to measure actual robustness and quantify the performance gap between edge-only and cloud-augmented inference