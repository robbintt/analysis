---
ver: rpa2
title: How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources
arxiv_id: '2306.04751'
source_url: https://arxiv.org/abs/2306.04751
tags:
- evaluation
- datasets
- instruction
- human
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive evaluation of instruction-tuned
  language models on a variety of open instruction-following datasets. The authors
  train a large set of models from 6.7B to 65B parameters on 12 different instruction
  datasets and evaluate them on multiple tasks including factual knowledge, reasoning,
  multilinguality, coding, and open-ended instruction following.
---

# How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources

## Quick Facts
- arXiv ID: 2306.04751
- Source URL: https://arxiv.org/abs/2306.04751
- Reference count: 40
- Key outcome: Comprehensive evaluation of instruction-tuned LLMs on 12 open datasets, training 6.7B-65B parameter models and finding that different datasets enhance specific skills, with TÜLU (7B-65B LLAMA models fine-tuned on high-quality combinations) achieving best average performance across benchmarks.

## Executive Summary
This paper provides a comprehensive evaluation of instruction-tuned language models on a variety of open instruction-following datasets. The authors train a large set of models from 6.7B to 65B parameters on 12 different instruction datasets and evaluate them on multiple tasks including factual knowledge, reasoning, multilinguality, coding, and open-ended instruction following. They find that different instruction-tuning datasets can enhance specific skills, but no single dataset or combination provides the best performance across all evaluations. The authors introduce TÜLU, a suite of 7B to 65B LLAMA models fine-tuned on a combination of high-quality open resources, which achieves the best average performance across benchmarks. However, even the 65B TÜLU model lags behind ChatGPT and GPT-4 on most tasks.

## Method Summary
The authors train models ranging from 6.7B to 65B parameters (LLaMA, OPT, Pythia) on 12 different instruction datasets, formatted into chatbot-style message pairs. Models are fine-tuned for two epochs with learning rate 2e-5, using DeepSpeed and ZeRO optimizer. Training uses maximum sequence length of 2048 (1024 for 30B/65B). The paper evaluates models on multiple benchmarks (MMLU, GSM, BBH, TydiQA, Codex-Eval) and includes both model-based (AlpacaFarm) and human evaluations (acceptability and preference on 332 instructions).

## Key Results
- Different instruction-tuning datasets enhance specific capabilities (e.g., CoT improves mathematical reasoning, Code-Alpaca improves coding performance)
- No single dataset or combination provides best performance across all evaluations
- TÜLU models (7B-65B LLAMA fine-tuned on high-quality combinations) achieve best average performance across benchmarks
- Even 65B TÜLU models lag behind ChatGPT and GPT-4 on most tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction tuning on diverse datasets enhances specific model capabilities
- Mechanism: Different instruction datasets target distinct skill sets (e.g., CoT for reasoning, Code-Alpaca for coding). By exposing the model to varied task types during fine-tuning, it learns to generalize across those domains.
- Core assumption: The base model has sufficient capacity to absorb task-specific patterns without catastrophic forgetting.
- Evidence anchors: abstract states "different instruction-tuning datasets can uncover or enhance specific skills"; section 5.1 shows "training on CoT being particularly helpful for mathematical reasoning in GSM and Code-Alpaca being helpful for Codex-Eval."

### Mechanism 2
- Claim: Combining multiple datasets yields better average performance than any single dataset
- Mechanism: A mixture introduces broader task diversity, preventing overfitting to a single data distribution and encouraging robust generalization.
- Core assumption: The mixture sampling strategy does not introduce significant class imbalance or bias toward certain task types.
- Evidence anchors: abstract notes "no single dataset (or combination) provides the best performance across all evaluations"; section 5.1 states "models trained on our combination datasets are often not the best model for a single task... but they are the best when measuring average performance across tasks."

### Mechanism 3
- Claim: Larger or longer-pretrained base models consistently outperform smaller ones after instruction tuning
- Mechanism: Extended pretraining increases model capacity and knowledge breadth, providing a stronger foundation for fine-tuning on instruction data.
- Core assumption: Pretraining data quality and diversity scale positively with token count.
- Evidence anchors: section 5.1 finds "using LLaMA performs best by a significant margin, likely due to the fact that LLaMA is pretrained on significantly more tokens than the other models"; abstract states "Larger or pretrained-for-longer base models consistently perform better after instruction tuning."

## Foundational Learning

- Concept: Zero-shot and few-shot generalization
  - Why needed here: The evaluation measures how well models perform on unseen tasks without task-specific training data.
  - Quick check question: Can the model answer a new type of question using only in-context examples?

- Concept: Chain-of-thought reasoning
  - Why needed here: Several benchmarks (GSM, BBH) test reasoning by requiring intermediate logical steps.
  - Quick check question: Does the model produce intermediate reasoning steps when prompted, or does it jump directly to an answer?

- Concept: Multilingual text processing
  - Why needed here: TydiQA evaluates model ability to handle non-English languages, which is critical for broad applicability.
  - Quick check question: Can the model correctly answer questions in a language it was not explicitly fine-tuned on?

## Architecture Onboarding

- Component map:
  Base LM (LLaMA variants: 6.7B-65B parameters) -> Instruction dataset pipeline (12 datasets formatted as user/assistant pairs) -> Training loop (2 epochs, masked loss on assistant tokens, linear LR decay) -> Evaluation harness (Automatic: MMLU, GSM, BBH, TydiQA, Codex-Eval; Model-based: AlpacaFarm; Human-based: acceptance + preference) -> Model serving (8-bit quantization)

- Critical path:
  1. Data ingestion -> formatting -> shuffling
  2. Model initialization -> fine-tuning loop
  3. Checkpoint saving -> evaluation runs
  4. Result aggregation -> analysis

- Design tradeoffs:
  - Longer sequences (2048) vs. memory constraints
  - Full fine-tuning vs. parameter-efficient methods (chosen: full)
  - 8-bit quantization vs. accuracy loss (minimal impact observed)
  - Dataset diversity vs. noise introduction (mixtures chosen to balance)

- Failure signatures:
  - Training divergence: loss spikes or NaN values
  - Catastrophic forgetting: drop in base LM performance metrics
  - Evaluation instability: high variance across runs
  - Model preference bias: AlpacaFarm win-rate correlates with token count rather than capability

- First 3 experiments:
  1. Train 13B LLaMA on each of the 12 datasets individually and evaluate on MMLU, GSM, BBH, TydiQA, Codex-Eval to identify dataset-specific strengths.
  2. Combine top-performing datasets into a human+GPT mix and train 13B LLaMA, compare against single-dataset models.
  3. Scale up to 65B LLaMA on the human+GPT mix and evaluate on all benchmarks plus AlpacaFarm and human evaluation to assess scaling effects.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance of TÜLU models on tasks not explicitly covered in the instruction datasets (e.g., multi-turn dialogue, summarization) match their performance on benchmark tasks?
- Basis in paper: [inferred] The paper focuses on a core set of capabilities and covers broad open-ended tasks via model and human preference-based evaluations, but does not explicitly evaluate models on their multi-turn dialogue abilities nor their summarization abilities.
- Why unresolved: The paper does not provide an exhaustive evaluation of all possible instruction datasets and open models, and the authors suggest that future work could investigate whether more recent strong base models or other instruction datasets perform significantly better or differently from the models explored in this work.
- What evidence would resolve it: A comprehensive evaluation of TÜLU models on tasks not explicitly covered in the instruction datasets, such as multi-turn dialogue and summarization, would provide evidence to resolve this question.

### Open Question 2
- Question: How do TÜLU models compare to proprietary models like ChatGPT and GPT-4 on tasks not included in the evaluation suite?
- Basis in paper: [inferred] The paper compares TÜLU models to ChatGPT and GPT-4 on a variety of tasks, but acknowledges that there may be tasks not included in the evaluation suite where proprietary models perform better.
- Why unresolved: The paper cannot discount the possibility that either ChatGPT or GPT-4 was trained on significant portions of the evaluation suite, and the authors suggest that future work could investigate whether more recent strong base models or other instruction datasets perform significantly better or differently from the models explored in this work.
- What evidence would resolve it: A comprehensive evaluation of TÜLU models and proprietary models like ChatGPT and GPT-4 on tasks not included in the evaluation suite would provide evidence to resolve this question.

### Open Question 3
- Question: How do different base models (e.g., Falcon) perform when fine-tuned on the same instruction datasets as TÜLU models?
- Basis in paper: [inferred] The paper focuses on the LLAMA base model and compares it to other open alternatives like OPT and Pythia, but does not explore the performance of other recent strong base models like Falcon.
- Why unresolved: The paper suggests that future work could investigate whether more recent strong base models or other instruction datasets perform significantly better or differently from the models explored in this work.
- What evidence would resolve it: A comprehensive evaluation of different base models, including recent strong models like Falcon, fine-tuned on the same instruction datasets as TÜLU models would provide evidence to resolve this question.

## Limitations
- Evaluation framework relies heavily on benchmark-based metrics that may not fully capture real-world instruction-following capabilities
- Human evaluation based on relatively small sample of 332 instructions, limiting statistical power
- Study does not investigate long-term stability of instruction-tuned models or performance degradation over time
- Analysis focuses on open-source models and datasets, may not generalize to proprietary systems or specialized domains

## Confidence
High confidence: Core finding that different instruction-tuning datasets enhance specific capabilities is well-supported by consistent empirical results across multiple benchmarks and model sizes.

Medium confidence: Claim about dataset combinations providing best average performance has strong support but may be sensitive to specific mixture ratios and sampling strategies.

Low confidence: Comparison between model-based and human preference evaluations contains notable inconsistencies not fully explained.

## Next Checks
1. Conduct ablation studies on dataset mixing ratios to determine optimal composition strategies and identify potential diminishing returns from dataset diversity.

2. Implement longitudinal testing to assess model performance stability over extended inference sessions and identify any degradation patterns in instruction-following capabilities.

3. Expand human evaluation to include diverse annotator pools and task categories, particularly focusing on instructions that probe edge cases in reasoning and factual knowledge to better understand model limitations.