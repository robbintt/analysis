---
ver: rpa2
title: The Less the Merrier? Investigating Language Representation in Multilingual
  Models
arxiv_id: '2310.13228'
source_url: https://arxiv.org/abs/2310.13228
tags:
- languages
- language
- representations
- multilingual
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates linguistic diversity in multilingual language
  models, focusing on how well they represent languages from different families, dialects,
  and writing scripts. It compares community-centered models (AfroLM, IndicBERT) to
  general models (XLM-R, mBERT) across various language groups.
---

# The Less the Merrier? Investigating Language Representation in Multilingual Models

## Quick Facts
- arXiv ID: 2310.13226
- Source URL: https://arxiv.org/abs/2310.13226
- Reference count: 12
- Key outcome: Community-centered models outperform general multilingual models at representing low-resource languages and distinguishing between languages within the same family.

## Executive Summary
This study investigates how well multilingual language models represent languages from different families, dialects, and writing scripts. By comparing community-centered models (AfroLM, IndicBERT) with general models (XLM-R, mBERT), the research reveals that community-focused approaches achieve superior performance for low-resource languages, particularly in distinguishing between related languages within the same family. The study finds that models struggle with dialectical differences across all evaluated approaches, with all models scoring below 40% F1-score for Arabic and Tigrinya dialects. Community-centered models also demonstrated better performance on downstream Named Entity Recognition tasks for Bantu languages.

## Method Summary
The study evaluates multilingual language models by extracting hidden states and visualizing embedding spaces using UMAP dimensionality reduction. K-Means clustering is applied to embeddings to assess language classification performance across different language families. The evaluation includes eight models (XLM-R, mBERT, AfroLM, AraBERT, IndicBERT, GPT-3, LLaMA, BLOOM) across three major language families: Afro-Asiatic, Niger-Congo, and Indo-European. Performance is measured through language classification F1-scores, Named Entity Recognition evaluation on Bantu and Romance languages, and text generation accuracy assessed through language detection.

## Key Results
- Community-centered models achieved 100% F1-score for Semitic and Cushitic language classification, while XLM-R scored 68% and 52% respectively
- All models scored below 40% F1-score for Arabic and Tigrinya dialect classification, indicating significant challenges with dialectical representation
- Community-centered models showed superior performance on downstream NER tasks for Bantu languages compared to general multilingual models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Community-centered models outperform generic multilingual models at distinguishing between languages within the same family for low-resource languages.
- Mechanism: Focused training on related languages develops more distinct, fine-grained language-specific representations, avoiding the "curse of multilinguality" where increased language diversity dilutes individual language performance.
- Core assumption: Languages covered by community-centered models are sufficiently related to benefit from shared features while maintaining distinct representations.
- Evidence anchors:
  - [abstract] "community-centered models—models that focus on languages of a given family or geographical location and are built by communities who speak them—perform better at distinguishing between languages in the same family for low-resource languages."
  - [section 6.2] "For Semitic and Cushetic languages, AfroLM classified all the languages correctly with an F1-score of 100%, while for Indo-Iranian language classification, IndicBERT classified all the languages correctly with F1-score of 100%."
  - [corpus] Weak evidence; the corpus contains papers on multilingual models and bias, but lacks direct evidence on community-centered model performance.

### Mechanism 2
- Claim: Multilingual models struggle with representing and classifying different dialects within the same language.
- Mechanism: Dialects share high lexical and phonological overlap, making it difficult for models to learn distinct representations based solely on training data, especially when dialectical variation is not explicitly modeled.
- Core assumption: Dialectical differences are subtle enough that they are not easily captured by standard multilingual model training objectives.
- Evidence anchors:
  - [abstract] "The study also found that models struggle with dialectical differences, with all models scoring below 40% F1-score for Arabic and Tigrinya dialects."
  - [section 6.1.2] "Our result shows that for the Arabic dialect, except AraBERT, all the models mixed the representations for all dialects. Similarly, all the models mix the representations for Tigrinya dialects."
  - [corpus] Weak evidence; the corpus contains papers on multilingual models and bias, but lacks direct evidence on dialectical representation.

### Mechanism 3
- Claim: The learned representations of multilingual models vary depending on whether a language was seen or unseen during training.
- Mechanism: Models can leverage shared linguistic features across languages to create representations for unseen languages, but the quality may be lower than for seen languages due to lack of direct training signals.
- Core assumption: Languages within the same family share enough structural and lexical features that a model trained on some languages can generalize to others.
- Evidence anchors:
  - [section 7.3] "From our visualizations, we observe that some models cluster unseen languages based on writing scripts. For example, for Semitic languages, LLaMa, BLOOM, mBERT, AraBERT, and IndicBERT clustered Tigrinya and Amharic, languages which both use the Ge'ez script, together and formed a separate cluster for Arabic."
  - [corpus] Weak evidence; the corpus contains papers on multilingual models, but lacks direct evidence on unseen language representation.

## Foundational Learning

- Concept: Language families and their characteristics
  - Why needed here: Understanding how languages are related helps explain why community-centered models focusing on a specific family might perform better than generic multilingual models.
  - Quick check question: What are some major language families, and what are some key features that distinguish them?

- Concept: Dialectal variation and its impact on language models
  - Why needed here: Recognizing the challenges of representing and distinguishing dialects is crucial for interpreting the results on dialectical classification performance.
  - Quick check question: How do dialects differ from languages, and what are some common challenges in modeling them?

- Concept: Cross-lingual transfer and its limitations
  - Why needed here: Understanding how models leverage knowledge from one language to improve performance on another is key to interpreting the results on unseen language representation.
  - Quick check question: What are some factors that influence the success of cross-lingual transfer, and what are some common limitations?

## Architecture Onboarding

- Component map: Data collection → Model training → Embedding extraction → Downstream evaluation
- Critical path: Data collection → Model training → Embedding extraction → Downstream evaluation
- Design tradeoffs: Balancing number of languages covered with depth of representation for each language; choosing between autoencoder and autoregressive architectures based on desired downstream tasks
- Failure signatures: Poor performance on low-resource languages or dialects; mixing of representations for closely related languages; inability to generalize to unseen languages
- First 3 experiments:
  1. Visualize embedding spaces of different multilingual models for a set of closely related languages to compare their ability to form distinct clusters
  2. Evaluate performance of different models on a language identification task for a set of low-resource languages and their dialects
  3. Test ability of models to generate coherent text in low-resource languages by prompting with sentences and evaluating generated output

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the causal relationship between language cluster separation in embedding spaces and downstream task performance?
- Basis in paper: [inferred] The paper observes a correlation between separate cluster formation and downstream performance but acknowledges uncertainty about causation
- Why unresolved: The paper notes that while there is correlation between models with separate language clusters and better downstream performance, it's unclear if this is causal or merely coincidental
- What evidence would resolve it: Controlled experiments manipulating cluster separation while holding other factors constant, or systematic ablation studies showing performance changes with cluster separation

### Open Question 2
- Question: How do community-centered models compare to generic models in geometric space properties beyond clustering?
- Basis in paper: [explicit] The paper suggests future work could investigate geometric space differences between community-centered and generic models
- Why unresolved: The paper only briefly mentions this as a potential future direction without conducting such analysis
- What evidence would resolve it: Detailed geometric analysis comparing manifold properties, isotropy, and subspace orthogonality between community-centered and generic models

### Open Question 3
- Question: What specific architectural or training factors enable community-centered models to better distinguish languages within families?
- Basis in paper: [explicit] The paper observes that community-centered models perform better at distinguishing languages in the same family but doesn't explain why
- Why unresolved: The paper identifies the performance difference but doesn't investigate the underlying mechanisms
- What evidence would resolve it: Comparative analysis of training data composition, tokenization strategies, and architectural choices between community-centered and generic models

## Limitations
- The corpus evidence supporting specific performance claims is notably weak, particularly for the 100% F1-scores reported for community-centered models
- The study does not address potential confounding factors such as differences in tokenization strategies across models
- Dialectical variation analysis is limited to only two language pairs (Arabic and Tigrinya), making generalization difficult

## Confidence
- High Confidence: The general observation that community-centered models perform better than generic multilingual models for low-resource languages within the same family is well-supported by experimental results
- Medium Confidence: The claim about models struggling with dialectical differences is supported by results but requires more extensive testing across additional language pairs
- Low Confidence: Specific performance metrics (e.g., 100% F1-scores) are reported but lack strong corroborating evidence in the corpus review

## Next Checks
1. Replicate the language classification experiments using an independent, publicly available dataset for Afro-Asiatic and Indo-Iranian languages to verify reported performance differences
2. Compare tokenization strategies and vocabulary coverage of AfroLM, IndicBERT, XLM-R, and mBERT for low-resource languages to determine if tokenization differences account for performance variations
3. Extend the dialectical analysis to include at least three additional language pairs with well-documented dialectical variation to establish whether observed <40% F1-scores represent systematic limitations or are specific to Arabic and Tigrinya