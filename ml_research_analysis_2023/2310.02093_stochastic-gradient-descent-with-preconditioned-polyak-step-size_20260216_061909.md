---
ver: rpa2
title: Stochastic Gradient Descent with Preconditioned Polyak Step-size
arxiv_id: '2310.02093'
source_url: https://arxiv.org/abs/2310.02093
tags:
- methods
- learning
- adam
- stochastic
- polyak
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents Stochastic Gradient Descent with Preconditioned
  Polyak Step-size (PSPS), an extension of Stochastic Gradient Descent with Polyak
  Step-size (SPS) that employs preconditioning techniques to improve convergence on
  badly scaled and/or ill-conditioned datasets. The authors propose three variants
  of PSPS: PSPS, PSPSL1, and PSPSL2, which use Hutchinson''s method, Adam, and AdaGrad
  for preconditioning, respectively.'
---

# Stochastic Gradient Descent with Preconditioned Polyak Step-size

## Quick Facts
- arXiv ID: 2310.02093
- Source URL: https://arxiv.org/abs/2310.02093
- Reference count: 40
- Key outcome: PSPS methods outperform SPS, Adam, and AdaGrad on badly scaled datasets without manual learning rate tuning

## Executive Summary
This paper introduces Stochastic Gradient Descent with Preconditioned Polyak Step-size (PSPS), an extension of SPS that incorporates preconditioning techniques to improve convergence on badly scaled and ill-conditioned datasets. The authors propose three variants: PSPS using Hutchinson's method for Hessian diagonal estimation, PSPSL1 using Adam, and PSPSL2 using AdaGrad. These methods automatically adapt to the local loss landscape without requiring manual learning rate tuning, a significant advantage over traditional optimizers. The preconditioning techniques transform the geometry of the loss landscape to accelerate convergence, particularly on datasets where standard methods struggle.

## Method Summary
The PSPS methods implement a preconditioned Polyak step-size optimization framework where the update rule takes the form wt+1 = arg min w∈Rd 1/2 ||w - wt||^2_Bt s.t. fi(wt) + ⟨∇fi(wt), w - wt⟩ = 0, with B_t being the preconditioning matrix. Three preconditioning approaches are used: Hutchinson's method estimates the Hessian diagonal using random vectors and Hessian-vector products, Adam adapts the preconditioning matrix based on exponential moving averages of gradients and squared gradients, and AdaGrad accumulates squared gradients for diagonal preconditioning. The Polyak step-size is computed as (fi(wt) - f*)/||∇fi(wt)||^2, automatically adapting to the local loss landscape. Experiments are conducted on LIBSVM datasets (mushrooms and colon-cancer) with both original and badly scaled versions, comparing PSPS variants against SPS, SGD, Adam, and AdaGrad.

## Key Results
- PSPS methods outperform SPS without preconditioning on badly scaled datasets
- PSPS variants converge faster than Adam and AdaGrad on ill-conditioned problems
- Hutchinson's preconditioning method shows strong performance without requiring manual learning rate tuning
- The slack variable variants (PSPSL1, PSPSL2) handle non-interpolating cases effectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hutchinson's method estimates the Hessian diagonal to enable preconditioning without full Hessian computation
- Mechanism: Hutchinson's method uses random vectors and Hessian-vector products to estimate the diagonal of the Hessian matrix, which is then used as a preconditioning matrix
- Core assumption: The expectation of z ⊙ (Hz) equals the Hessian diagonal, where z is a random vector
- Evidence anchors:
  - [abstract]: "Hutchinson's method, Adam, and AdaGrad to precondition search directions"
  - [section]: "Using this identity, we estimate the Hessian diagonal from a given D0 by sampling a vector z at each iteration"
  - [corpus]: "Hutchinson's method uses only a few Hessian-vector products, which in turn can be computed efficiently using backpropagation"
- Break condition: If the loss functions are highly non-convex or the Hessian-vector products are computationally expensive, the diagonal estimate may become unreliable

### Mechanism 2
- Claim: The Polyak step-size automatically adapts to the local loss landscape without manual learning rate tuning
- Mechanism: The step-size is computed as fi(wt) - f* divided by the squared norm of the gradient, making it scale-aware
- Core assumption: The optimal function value f* is known or can be approximated (often f* = 0 for unregularized losses)
- Evidence anchors:
  - [abstract]: "The PSPS methods do not require manual fine-tuning of the learning rate"
  - [section]: "One of the main topics discussed in this paper is deriving methods that are designed to overcome badly scaled/ill-conditioned datasets"
  - [corpus]: "The Polyak step-size is computed as fi(wt) - f* divided by the squared norm of the gradient"
- Break condition: If the interpolation condition doesn't hold or the loss landscape is too irregular, the automatic step-size may become unstable

### Mechanism 3
- Claim: Preconditioning transforms the geometry of the loss landscape to accelerate convergence
- Mechanism: The preconditioning matrix B_t scales and rotates the gradient direction to align with the principal axes of the loss curvature
- Core assumption: The preconditioning matrix B_t is positive definite and captures relevant curvature information
- Evidence anchors:
  - [abstract]: "employ preconditioning techniques, such as Hutchinson's method, Adam, and AdaGrad, to improve its performance on badly scaled and/or ill-conditioned datasets"
  - [section]: "wt+1 = arg min w∈Rd 1/2 ||w - wt||^2_Bt s.t. fi(wt) + ⟨∇fi(wt), w - wt⟩ = 0"
  - [corpus]: "Algorithms that take advantage of preconditioning have a generic update rule as following wt+1 = wt - γtMt∇fi(wt)"
- Break condition: If the preconditioning matrix is poorly estimated or becomes singular, the transformation may degrade performance

## Foundational Learning

- Concept: Interpolation condition
  - Why needed here: The Polyak step-size methods assume the interpolation condition holds, meaning there exists a solution that achieves zero loss on all training examples
  - Quick check question: What happens to the Polyak step-size if the interpolation condition doesn't hold for the current mini-batch?

- Concept: Preconditioning in optimization
  - Why needed here: Preconditioning transforms the optimization problem by scaling the gradient direction to better match the local curvature, which is crucial for badly scaled data
  - Quick check question: How does preconditioning affect the effective condition number of the optimization problem?

- Concept: Stochastic gradient estimation
  - Why needed here: The methods use mini-batch stochastic gradients rather than full gradients, which introduces variance that affects convergence
  - Quick check question: How does the mini-batch size affect the variance of the stochastic gradient and the stability of the Polyak step-size?

## Architecture Onboarding

- Component map:
  - Core optimizer -> Preconditioner module -> Polyak step-size calculator -> Logging and monitoring
  - Slack variables (for PSPSL1/PSPSL2) -> Constraint satisfaction module

- Critical path:
  1. Compute mini-batch stochastic gradient
  2. Compute preconditioning matrix B_t
  3. Calculate Polyak step-size using current loss and gradient norm
  4. Update parameters using preconditioned gradient and step-size
  5. Update slack variables if using PSPSL1/PSPSL2 variants

- Design tradeoffs:
  - Preconditioner choice: Hutchinson's method has lower memory overhead but requires Hessian-vector products, while Adam/AdaGrad are more memory-intensive but avoid second-order computations
  - Preconditioner update frequency: More frequent updates capture changing curvature better but increase computational cost
  - Slack parameter tuning: Balances between exact interpolation and robustness to non-interpolating data

- Failure signatures:
  - Divergence: Often indicates poor preconditioner initialization or step-size explosion
  - Slow convergence: May suggest inadequate preconditioning or inappropriate slack parameters
  - Oscillations: Can occur with poorly estimated preconditioning matrices or high gradient variance

- First 3 experiments:
  1. Run PSPS on a simple convex problem (e.g., logistic regression on mushrooms dataset) to verify basic functionality without preconditioning
  2. Add Hutchinson's preconditioning and test on badly scaled versions of the dataset to observe convergence improvements
  3. Compare all three PSPS variants (PSPS, PSPSL1, PSPSL2) on the colon-cancer dataset to understand slack parameter effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical convergence rate of the proposed PSPS methods compared to other adaptive optimization methods like Adam and AdaGrad?
- Basis in paper: [inferred] The paper mentions future work includes theoretical analysis of the proposed methods.
- Why unresolved: The paper only provides empirical results without theoretical guarantees.
- What evidence would resolve it: A rigorous mathematical proof establishing the convergence rate of PSPS methods under various assumptions.

### Open Question 2
- Question: How do the proposed PSPS methods perform on deep neural networks and other complex machine learning models?
- Basis in paper: [explicit] The paper states that experiments were conducted on convex and non-convex settings with 2 different datasets, but does not mention deep neural networks specifically.
- Why unresolved: The paper does not provide experiments on deep neural networks or other complex models.
- What evidence would resolve it: Empirical results comparing the performance of PSPS methods on deep neural networks and other complex models to other popular optimizers.

### Open Question 3
- Question: What is the impact of the choice of the slack parameter λ and the regularization parameter µ on the performance of the PSPSL1 and PSPSL2 methods?
- Basis in paper: [inferred] The paper mentions that the slack parameters λ and µ were set to specific values (λ = 0.01 and µ = 0.1) during experiments, but does not discuss their impact on performance.
- Why unresolved: The paper does not provide a sensitivity analysis of the slack and regularization parameters.
- What evidence would resolve it: A study investigating the effect of different values of λ and µ on the performance of PSPSL1 and PSPSL2 methods.

## Limitations
- Limited empirical validation on only two datasets with specific scaling factors
- Theoretical analysis focuses on convex cases, potentially limiting applicability to non-convex real-world problems
- Computational overhead of Hutchinson's method and number of Hessian-vector products needed is not thoroughly analyzed

## Confidence
- **High confidence**: The core mechanism of Polyak step-size adaptation and its theoretical properties under interpolation conditions
- **Medium confidence**: Preconditioning effectiveness claims, particularly for Hutchinson's method which depends heavily on accurate Hessian diagonal estimation
- **Medium confidence**: Performance comparisons with other optimizers, given the limited dataset diversity and potential hyperparameter sensitivity

## Next Checks
1. Test PSPS methods on additional datasets with varying levels of ill-conditioning (beyond k=6 scaling) to establish broader performance boundaries
2. Conduct ablation studies isolating the impact of preconditioning by comparing PSPS with and without preconditioning on the same problems
3. Measure the computational overhead of Hutchinson's method (number of Hessian-vector products) and its impact on wall-clock convergence time versus theoretical iteration complexity