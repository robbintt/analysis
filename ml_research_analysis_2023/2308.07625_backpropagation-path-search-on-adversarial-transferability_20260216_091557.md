---
ver: rpa2
title: Backpropagation Path Search On Adversarial Transferability
arxiv_id: '2308.07625'
source_url: https://arxiv.org/abs/2308.07625
tags:
- adversarial
- path
- search
- backpropagation
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving adversarial transferability
  in black-box attacks by developing a novel backpropagation path search framework
  (PAS). The key innovation is SkipConv, which decomposes convolution kernels to enable
  path adjustments, combined with a DAG-based search space that unifies various structure-based
  attack methods.
---

# Backpropagation Path Search On Adversarial Transferability

## Quick Facts
- arXiv ID: 2308.07625
- Source URL: https://arxiv.org/abs/2308.07625
- Authors: 
- Reference count: 40
- This paper addresses the challenge of improving adversarial transferability in black-box attacks by developing a novel backpropagation path search framework (PAS) that achieves up to 24.3% improvement against defense models compared to state-of-the-art attackers.

## Executive Summary
This paper introduces a novel framework called PAS (Path Search) to enhance adversarial transferability in black-box attacks. The key innovation is SkipConv, which uses structural reparameterization to decompose convolution kernels into skip and residual components, enabling backpropagation path adjustment without affecting forward computation. The framework constructs a DAG-based search space that unifies various structure-based attack methods and employs Bayesian Optimization with one-step approximation for efficient path evaluation. Extensive experiments demonstrate significant improvements in attack success rates across diverse models, achieving up to 24.3% improvement against defense models compared to state-of-the-art attackers.

## Method Summary
PAS works by first reparameterizing convolution modules using SkipConv, which decomposes original kernels into skip and residual components while maintaining forward pass functionality. This creates a DAG-based search space by combining backpropagation paths from reparameterized convolution, activation, and residual modules. Bayesian Optimization then searches for optimal path weights Γ using one-step approximation evaluation (200 samples) as a proxy metric for transferability. The framework requires Nt = 2000 trials per surrogate model, evaluating on 256 examples from the test set. SkipConv is implemented through structural reparameterization where ki = kskip_i + kres_i, with the skip kernel implementing identity mapping through sumch operation and the residual kernel maintaining original functionality with decay factor γi controlling gradient contribution.

## Key Results
- PAS achieves up to 24.3% improvement in attack success rates against defense models compared to state-of-the-art attackers
- The framework demonstrates strong generalization capabilities on both normally trained and robustly defended models
- One-step approximation with 200 samples is sufficient to distinguish path differences with 95% confidence while reducing computational overhead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SkipConv decomposes convolution kernels into skip and residual components to enable backpropagation path adjustment without affecting forward pass
- Mechanism: SkipConv uses structural reparameterization where original kernel ki = kskip_i + kres_i. The skip kernel implements identity mapping through sumch operation, while residual kernel maintains original functionality with decay factor γi controlling gradient contribution
- Core assumption: Convolution kernels can be decomposed into sum of identity and residual components without affecting forward computation
- Evidence anchors:
  - [abstract] "SkipConv to adjust the backpropagation path of convolution by structural reparameterization"
  - [section] "According to the distributivity of convolution, the decomposed kernels calculate forward as usual, i.e., f conv_i (zi−1; ki) = f conv_i (zi−1; kres_i)+ f conv_i (zi−1; kskip_i)"
  - [corpus] Weak evidence - corpus papers don't directly address convolution kernel decomposition for adversarial transferability
- Break condition: If convolution distributivity doesn't hold for specific kernel configurations or if sumch operation cannot be implemented for non-standard convolutions

### Mechanism 2
- Claim: DAG-based search space unifies various structure-based attack methods through combined skip paths
- Mechanism: PAS constructs directed acyclic graph by combining skip paths from convolution (SkipConv), activation (SkipReLU), and residual modules (SkipGrad). Bayesian Optimization searches for optimal path weights Γ = {γi} in this unified space
- Core assumption: Transferability can be improved by searching optimal combination of skip connections across different module types
- Evidence anchors:
  - [abstract] "construct a DAG-based search space, utilize one-step approximation for path evaluation and employ Bayesian Optimization to search for the optimal path"
  - [section] "By combining all the paths of the above modules, we construct the Directed Acyclic Graph (DAG) for gradient propagation backward"
  - [corpus] Moderate evidence - related work on transferable attacks exists but not specifically using DAG-based search spaces
- Break condition: If skip connections from different module types don't combine effectively or if DAG becomes too complex for efficient search

### Mechanism 3
- Claim: One-step approximation provides sufficient path evaluation for Bayesian Optimization while reducing computational overhead
- Mechanism: Instead of full attack evaluation, PAS uses one-step attack success rate on validation set as proxy metric. Statistical analysis shows 200 samples sufficient to distinguish path differences with 95% confidence
- Core assumption: One-step attack success rate correlates strongly with full transferability performance and requires fewer samples for reliable evaluation
- Evidence anchors:
  - [abstract] "utilize one-step approximation for path evaluation"
  - [section] "We measure the approximation of transferability by altering the sample size for path evaluation" and "200 samples (i.e., 20% of the test set) to observe better paths with a 10% difference"
  - [corpus] Weak evidence - corpus papers don't discuss one-step approximation for transferability evaluation
- Break condition: If one-step attack success rate doesn't correlate with full attack performance or if statistical confidence requirements cannot be met with reasonable sample sizes

## Foundational Learning

- Concept: Structural reparameterization
  - Why needed here: Enables decomposition of convolution kernels into skip and residual components without affecting forward pass, critical for SkipConv mechanism
  - Quick check question: How does structural reparameterization differ from standard parameter optimization in neural networks?

- Concept: Directed Acyclic Graphs (DAGs) in neural networks
  - Why needed here: Provides framework for combining skip paths from different modules into unified search space
  - Quick check question: What properties of DAGs make them suitable for representing backpropagation paths with skip connections?

- Concept: Bayesian Optimization for hyperparameter search
  - Why needed here: Efficiently searches high-dimensional path space Γ = {γi} where each γi controls gradient contribution from different modules
  - Quick check question: How does Bayesian Optimization balance exploration and exploitation in the context of backpropagation path search?

## Architecture Onboarding

- Component map: Surrogate model -> SkipConv reparameterization -> DAG construction -> Bayesian Optimization search -> One-step evaluation -> Adversarial example generation
- Critical path: Surrogate model → SkipConv reparameterization → DAG construction → Bayesian Optimization search → One-step evaluation → Adversarial example generation
- Design tradeoffs:
  - Search space complexity vs. search efficiency: Larger DAG enables more expressive paths but increases search time
  - Evaluation accuracy vs. computational cost: One-step approximation reduces cost but may miss some transferability patterns
  - Model reparameterization vs. original functionality: SkipConv must maintain forward pass while enabling backward path adjustment
- Failure signatures:
  - Low attack success rates across all victim models despite high validation performance
  - Bayesian Optimization converges to trivial solutions (all γi = 0 or 1)
  - Search process becomes prohibitively slow with increasing model depth
- First 3 experiments:
  1. Verify SkipConv forward pass matches original convolution output for various kernel sizes and input shapes
  2. Test one-step approximation correlation with full attack success rate using controlled validation/test splits
  3. Benchmark Bayesian Optimization convergence speed and solution quality against random search baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of PAS vary across different types of vision models (e.g., CNNs vs. Transformers)?
- Basis in paper: [inferred] The paper mentions that PAS achieves significant improvements against both CNNs and ViT (a transformer-based vision model), but does not provide a detailed comparative analysis across different architectures.
- Why unresolved: While PAS shows general improvements, the specific impact on different model architectures (like CNNs vs. Transformers) is not thoroughly explored or quantified.
- What evidence would resolve it: Detailed experiments comparing PAS's performance on a diverse set of model architectures, including various CNNs and Transformers, would clarify its effectiveness across different model types.

### Open Question 2
- Question: What is the optimal number of trials (Nt) for the Bayesian Optimization process in PAS to balance effectiveness and computational overhead?
- Basis in paper: [explicit] The paper mentions using Nt = 2000 trials in experiments but does not explore the impact of varying this number on performance and computational cost.
- Why unresolved: The choice of 2000 trials is arbitrary, and it's unclear how different numbers of trials would affect the search efficiency and the quality of the identified paths.
- What evidence would resolve it: Experiments varying the number of trials (e.g., 500, 1000, 3000, 5000) and analyzing the trade-off between computational overhead and improvement in attack success rate would help determine the optimal number.

### Open Question 3
- Question: How does the one-step approximation for path evaluation scale with larger datasets or more complex models?
- Basis in paper: [inferred] The paper validates the one-step approximation on subsets of ImageNet and mentions its effectiveness, but does not address its scalability to larger datasets or more complex models.
- Why unresolved: The scalability of the one-step approximation method is not tested beyond the specific dataset and model configurations used in the experiments.
- What evidence would resolve it: Testing PAS on larger datasets (e.g., full ImageNet) and more complex models (e.g., larger Transformers) while evaluating the accuracy and efficiency of the one-step approximation would provide insights into its scalability.

## Limitations

- SkipConv mechanism's effectiveness relies on the assumption that convolution distributivity always holds, which may not be true for specialized convolution operations (depthwise, grouped, etc.)
- One-step approximation evaluation, while computationally efficient, may miss complex transferability patterns that only emerge over multiple attack iterations
- Bayesian Optimization framework's performance depends heavily on the quality of the surrogate model and may converge to suboptimal solutions in high-dimensional path spaces

## Confidence

- **High Confidence**: SkipConv decomposition mechanism and DAG construction methodology - these are well-defined algorithmic procedures with clear mathematical foundations
- **Medium Confidence**: One-step approximation effectiveness - supported by statistical analysis but not extensively validated across diverse attack scenarios
- **Medium Confidence**: Bayesian Optimization implementation - standard approach but specific hyperparameter choices may significantly impact results

## Next Checks

1. **Correlation validation**: Systematically test one-step approximation correlation with full attack success rates across different model architectures and attack types, varying sample sizes from 50 to 500 examples

2. **Path sensitivity analysis**: Measure attack success rate changes when systematically perturbing individual γi values in the optimal path to verify which skip connections contribute most to transferability

3. **Cross-architecture generalization**: Evaluate PAS transferability from ViT/B16 surrogate to CNN victim models (and vice versa) to test the framework's ability to bridge architectural gaps beyond the reported ResNet-to-ResNet improvements