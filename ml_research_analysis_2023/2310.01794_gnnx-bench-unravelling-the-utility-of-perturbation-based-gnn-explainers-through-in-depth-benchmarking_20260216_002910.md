---
ver: rpa2
title: 'GNNX-BENCH: Unravelling the Utility of Perturbation-based GNN Explainers through
  In-depth Benchmarking'
arxiv_id: '2310.01794'
source_url: https://arxiv.org/abs/2310.01794
tags:
- graph
- explainers
- counterfactual
- explanations
- stability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This benchmarking study systematically evaluates and compares perturbation-based
  explainability methods for Graph Neural Networks (GNNs), including both factual
  and counterfactual reasoning techniques. The study addresses key aspects such as
  comparative analysis, stability to noise and variational factors, necessity and
  reproducibility, and feasibility of explanations.
---

# GNNX-BENCH: Unravelling the Utility of Perturbation-based GNN Explainers through In-depth Benchmarking

## Quick Facts
- **arXiv ID**: 2310.01794
- **Source URL**: https://arxiv.org/abs/2310.01794
- **Reference count**: 40
- **Primary result**: Systematic benchmarking of perturbation-based GNN explainers reveals RCExplainer's superior performance but highlights universal stability and feasibility challenges.

## Executive Summary
This study provides the first comprehensive benchmarking framework for perturbation-based explainability methods in Graph Neural Networks (GNNs), comparing both factual and counterfactual reasoning techniques. Through systematic evaluation across multiple datasets and metrics, the research reveals that while RCExplainer consistently outperforms other methods in efficacy and stability, all algorithms struggle with stability issues when confronted with noisy data. Additionally, counterfactual explanations frequently fail to provide feasible recourses due to violations of domain-specific topological constraints. The findings emphasize the critical need to incorporate stability and feasibility considerations into GNN explainability methods and identify clear directions for future research to enhance their performance and interpretability.

## Method Summary
The benchmarking study systematically evaluates seven factual explainers (GNNExplainer, PGExplainer, TAGExplainer, GEM, SubgraphX, RCExplainer, CF2) and four counterfactual explainers (RCExplainer, CF2, CF-GNNExplainer, CLEAR) across multiple graph datasets including MUTAG, PROTEINS, IMDB-B, and synthetic benchmarks. The evaluation framework assesses sufficiency (fidelity), explanation size, accuracy, and stability through Jaccard similarity metrics. Stability is tested against topological noise, model parameter variations, and architectural changes. Feasibility of counterfactual explanations is evaluated by checking domain-specific topological constraints. The study uses GCN as the base GNN model with 3 layers and 20 hidden dimensions, employing 80:10:10 train-validation-test splits across datasets.

## Key Results
- RCExplainer consistently demonstrates superior performance in both efficacy and stability across diverse datasets compared to other perturbation-based explainers
- All evaluated algorithms exhibit significant stability degradation when faced with noisy data, with topological noise being particularly detrimental
- Counterfactual explanations frequently fail to generate feasible recourses due to violations of domain-specific topological constraints, especially regarding graph connectedness
- Stability assessments reveal that explanation consistency varies significantly across different GNN architectures and initialization schemes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perturbation-based explainers like RCExplainer consistently outperform others in terms of both efficacy and stability across diverse datasets.
- Mechanism: RCExplainer selects a subset of edges that are resistant to removal, ensuring the remaining graph's prediction remains unchanged. This strategy inherently provides stability since the explanation focuses on edges whose removal significantly impacts predictions.
- Core assumption: The prediction of a GNN is sensitive to the removal of a small, critical set of edges, and these edges can be identified reliably.
- Evidence anchors:
  - [abstract]: "RCExplainer consistently outperforms others in terms of efficacy and stability"
  - [section]: "RCE XPLAINER (executed at α = 1) and PGE XPLAINER consistently exhibit higher stability"
- Break condition: If the GNN prediction is not sensitive to edge removal or if critical edges cannot be reliably identified, RCExplainer's strategy fails.

### Mechanism 2
- Claim: Stability of explanations is significantly affected by topological noise, model parameter initialization, and GNN architecture variations.
- Mechanism: Minor perturbations in graph topology, different initializations of the explainer model, or using different GNN architectures can lead to different explanations. The Jaccard similarity metric quantifies this stability by comparing edge sets in original and perturbed explanations.
- Core assumption: The explanation provided by an explainer should be robust to minor changes in the input graph, the explainer's initialization, or the underlying GNN architecture.
- Evidence anchors:
  - [abstract]: "all algorithms are affected by stability issues when faced with noisy data"
  - [section]: "we consider the following perspectives: Perturbations in topological space... Model parameters... Model architectures"
- Break condition: If the explanation is inherently unstable or if the perturbations are too large, the Jaccard similarity will be low, indicating poor stability.

### Mechanism 3
- Claim: Counterfactual explanations often fail to provide feasible recourses due to violations of topological constraints encoded by domain-specific considerations.
- Mechanism: Counterfactual explainers aim to identify minimal perturbations to the input graph that alter the GNN's prediction. However, these perturbations may result in graphs that violate domain-specific constraints, such as maintaining connectedness in molecular graphs.
- Core assumption: The counterfactual graph must adhere to the same topological properties as the original graph to be considered a valid recourse.
- Evidence anchors:
  - [abstract]: "counterfactual explanations often fail to provide feasible recourses due to violations of topological constraints encoded by domain-specific considerations"
  - [section]: "Counterfactual explanations serve as recourses and are expected to generate graphs that adhere to the feasibility constraints of the pertinent domain"
- Break condition: If the domain constraints are too strict or if the counterfactual explainer cannot find a valid recourse within those constraints, the explanation is considered infeasible.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their explainability
  - Why needed here: The paper benchmarks explainability methods for GNNs, so understanding how GNNs work and why they need explanations is crucial.
  - Quick check question: What are the key components of a message-passing GNN, and why is interpretability important for their adoption in critical domains?

- Concept: Perturbation-based explainability methods
  - Why needed here: The benchmarking study focuses on perturbation-based methods, so understanding how they work and their strengths/weaknesses is essential.
  - Quick check question: How do perturbation-based explainers identify important subgraphs or counterfactual explanations, and what are the main differences between factual and counterfactual reasoning?

- Concept: Evaluation metrics for explainability
  - Why needed here: The paper uses various metrics to evaluate the performance and stability of explainers, so understanding these metrics is crucial for interpreting the results.
  - Quick check question: What is the difference between sufficiency and necessity in the context of factual explanations, and how is stability measured using the Jaccard similarity?

## Architecture Onboarding

- Component map: Datasets -> GNN Models -> Explainability Methods -> Evaluation Metrics
- Critical path: Load datasets → Train GNN models → Apply explainers → Evaluate explanations → Assess stability → Check feasibility
- Design tradeoffs: RCExplainer prioritizes stability but may sacrifice interpretability; counterfactual methods balance between minimal perturbations and feasibility constraints
- Failure signatures: Unstable explanations under noise, infeasible counterfactual graphs, memory issues with large datasets, poor generalization across architectures
- First 3 experiments:
  1. Run RCExplainer and PGExplainer on the MUTAG dataset to compare their sufficiency and stability to topological noise
  2. Evaluate the stability of counterfactual explainers (RCExplainer, CF2, CLEAR) on the IMDB-B dataset by introducing varying levels of topological noise
  3. Analyze the feasibility of counterfactual explanations by checking the connectedness of the generated graphs on the Proteins dataset

## Open Questions the Paper Calls Out

- What are the optimal architectural designs for GNN explainers that balance performance, stability, and computational efficiency?
- How can domain-specific constraints be effectively incorporated into counterfactual GNN explainers to ensure the generation of feasible recourses?
- What are the key factors influencing the stability of GNN explainers across different GNN architectures, and how can stability be improved?

## Limitations
- RCExplainer, while showing best overall performance, still suffers from stability issues when faced with noisy data, indicating that no perturbation-based explainer is fully robust to all forms of graph perturbations
- Counterfactual explanations' feasibility is fundamentally limited by domain-specific topological constraints, which may make it impossible to generate valid recourses in some cases
- The study focuses on GNNs with message-passing architectures, potentially limiting generalizability to other graph learning paradigms

## Confidence

- High confidence in the comparative analysis of factual explainers (RCExplainer vs others) based on comprehensive benchmarking across multiple datasets
- Medium confidence in stability findings due to acknowledged sensitivity to noise and the need for further robustness research
- Medium confidence in counterfactual feasibility conclusions, as domain-specific constraints are well-defined but may vary across applications

## Next Checks
1. Test RCExplainer's stability under adversarial graph perturbations beyond simple topological noise (e.g., feature-level attacks)
2. Evaluate counterfactual explainers' performance on additional domain-specific datasets with stricter topological constraints
3. Benchmark explainability methods on GNN architectures beyond GCN (e.g., attention-based or deeper models) to assess architecture-dependent performance differences