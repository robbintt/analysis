---
ver: rpa2
title: 'Technical Report: Impact of Position Bias on Language Models in Token Classification'
arxiv_id: '2304.13567'
source_url: https://arxiv.org/abs/2304.13567
tags:
- position
- bias
- bert
- performance
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates position bias in transformer-based language
  models for token classification tasks like Named Entity Recognition (NER) and Part-of-Speech
  (POS) tagging. The authors show that models trained on data with skewed position
  distributions for different classes suffer from degraded performance when entities
  appear at positions not well represented in training data, with performance drops
  ranging from 3% to 9% in F1 score.
---

# Technical Report: Impact of Position Bias on Language Models in Token Classification

## Quick Facts
- arXiv ID: 2304.13567
- Source URL: https://arxiv.org/abs/2304.13567
- Reference count: 4
- Primary result: Transformer models trained on data with skewed position distributions suffer 3-9% F1 score drops on out-of-distribution positions

## Executive Summary
This paper investigates position bias in transformer-based language models for token classification tasks like Named Entity Recognition (NER) and Part-of-Speech (POS) tagging. The authors demonstrate that models trained on data with skewed position distributions for different classes suffer from degraded performance when entities appear at positions not well represented in training data. To address this issue, they propose two training-time methods: Random Position Shifting, which randomly shifts token positions during training, and Context Perturbation, which concatenates sequences in random orders. These methods improve model performance by approximately 2% on benchmark datasets, providing practical solutions for improving model robustness without architectural changes.

## Method Summary
The study evaluates position bias using a duplication-based approach where test sequences are repeated k times (k=1 to 10) with [SEP] tokens between repetitions. This creates controlled position distributions for measuring performance degradation. The authors train BERT-based models (BERT, ELECTRA, ERNIE) on standard NER and POS datasets, then evaluate using both standard F1 scores and position-aware metrics F1(α) for α ∈ {5, 10}. Two debiasing methods are implemented: Random Position Perturbation (RPP) that shifts token positions during training, and Context Perturbation (CP) that concatenates random permutations of sequences within batches.

## Key Results
- Models trained on skewed position distributions show F1 score drops of 3-9% on out-of-distribution positions
- Random Position Shifting improves performance by approximately 2% by exposing models to tokens at various positions
- Context Perturbation achieves similar 2% improvements by training on tokens within different contextual arrangements
- OntoNotes5.0 dataset shows more balanced position distributions and limited improvement from debiasing methods
- Decoder models (ELECTRA, ERNIE) exhibit larger performance drops than BERT, suggesting architecture-specific vulnerabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Position bias arises because token classification models trained on data with skewed position distributions perform poorly on tokens appearing at out-of-distribution positions.
- Mechanism: Models learn position-specific patterns from training data where certain classes appear predominantly at specific positions, leading to incorrect predictions when the same token appears at different positions during inference.
- Core assumption: The local context of a token remains sufficiently similar across different positions in a sequence for position bias to be observable as the primary performance degradation factor.
- Evidence anchors:
  - [abstract] "models trained on data with skewed position distributions for different classes suffer from degraded performance when entities appear at positions not well represented in training data"
  - [section] "models trained on data with skewed position distribution are biased towards the first positions of a sequence, and the performance drops as the position of the word increases beyond what the model is trained on"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism, though related position bias studies exist
- Break condition: If the context surrounding a token changes significantly with position, the observed performance degradation could be due to context shift rather than pure position bias.

### Mechanism 2
- Claim: Random Position Perturbation (RPP) mitigates position bias by exposing the model to tokens at various positions during training.
- Mechanism: By randomly shifting token positions during training, the model learns to classify tokens independent of their absolute position, creating a more robust position-invariant representation.
- Core assumption: Randomly shifting positions during training creates a uniform distribution of token positions, preventing the model from overfitting to position-specific patterns.
- Evidence anchors:
  - [abstract] "Random Position Shifting, which randomly shifts token positions during training"
  - [section] "The intuition behind this method is that by randomly shifting the position of tokens, the model will learn to classify a token over an unbiased distribution of its positions within the input sequence"
  - [corpus] Weak - no direct corpus evidence for this specific mitigation approach
- Break condition: If the position shifts are too large or too small relative to the maximum sequence length, the perturbation may not effectively cover the position distribution or may disrupt the sequence structure.

### Mechanism 3
- Claim: Context Perturbation (CP) mitigates position bias by training on tokens within different contextual arrangements.
- Mechanism: By concatenating sequences in random orders, tokens appear at different positions within varying contexts, forcing the model to rely on token-level features rather than position-specific patterns.
- Core assumption: The combination of different sequences with varying lengths creates diverse position distributions that cover the range of positions encountered during inference.
- Evidence anchors:
  - [abstract] "Context Perturbation, which concatenates sequences in random orders"
  - [section] "By training on this perturbed batch, the model will learn to classify tokens within different positions and noisy contexts, thereby mitigating position bias"
  - [corpus] Weak - no direct corpus evidence for this specific mitigation approach
- Break condition: If the concatenated contexts are too dissimilar or if the sequence boundaries create artificial context breaks, the model may learn to associate position with context type rather than token identity.

## Foundational Learning

- Concept: Positional Embeddings in Transformers
  - Why needed here: Understanding how Transformers encode position information is essential to grasp why position bias occurs and how mitigation methods work
  - Quick check question: How do BERT's absolute position embeddings differ from relative position embeddings used in other models?

- Concept: Token Classification vs. Sequence Classification
  - Why needed here: Token classification tasks like NER and POS tagging assign labels to individual tokens, making them particularly susceptible to position bias compared to sequence-level tasks
  - Quick check question: What architectural differences between token classification and sequence classification might make the former more vulnerable to position bias?

- Concept: Data Imbalance and Distribution Shift
  - Why needed here: Position bias is fundamentally a form of distribution shift where the training and inference data have different position distributions for the same classes
  - Quick check question: How does position bias in token classification relate to other forms of data imbalance like class imbalance?

## Architecture Onboarding

- Component map: Input layer (token + positional embeddings) -> Transformer encoder layers (multi-head self-attention with position information) -> Classification head (linear layer mapping to label space) -> Training pipeline (cross-entropy loss) -> Evaluation pipeline (token-level metrics + position-aware metrics)

- Critical path:
  1. Load and preprocess dataset with position information
  2. Initialize model with appropriate positional embeddings
  3. Train model on original data
  4. Evaluate on duplicated sequences to measure position bias
  5. Apply RPP or CP during training
  6. Re-evaluate to measure mitigation effectiveness

- Design tradeoffs:
  - RPP vs CP: RPP preserves original context but may not cover position distribution as thoroughly; CP provides better position coverage but introduces noisy contexts
  - Absolute vs relative positional embeddings: Absolute embeddings are simpler but may be more susceptible to position bias; relative embeddings are more complex but potentially more robust
  - Sequence length vs. batch size: Longer sequences provide more position coverage but reduce batch size and increase memory usage

- Failure signatures:
  - Performance drops consistently at higher positions (α > 5) indicate position bias
  - High standard deviation across random seeds suggests instability in position bias effects
  - Inconsistent improvements across datasets suggest dataset-specific position bias patterns
  - Decoder models showing larger performance drops indicate architecture-specific vulnerabilities

- First 3 experiments:
  1. Evaluate baseline BERT on duplicated sequences (k=1 to 10) to establish position bias baseline metrics
  2. Implement and evaluate RPP with varying perturbation ranges to find optimal τ values
  3. Implement and evaluate CP with different concatenation strategies to find optimal sequence combinations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different position embedding techniques (absolute, relative key, relative key-query) affect position bias across various token classification tasks beyond NER and POS tagging?
- Basis in paper: [explicit] The paper compares BERT with absolute position embedding (APE), relative position embedding (RPE), and rotary position embedding (RoPE), showing RPE variants had higher performance drops than APE
- Why unresolved: The study focused only on NER and POS tagging tasks, leaving uncertainty about how these position embedding techniques perform on other token classification tasks like chunking, semantic role labeling, or sequence labeling in biomedical domains
- What evidence would resolve it: Empirical evaluation of these position embedding variants across a broader range of token classification benchmarks, measuring position bias impact using the proposed duplication evaluation method

### Open Question 2
- Question: What is the relationship between dataset size and position bias severity, and can larger datasets inherently mitigate position bias without explicit debiasing methods?
- Basis in paper: [inferred] The analysis showed OntoNotes5.0 (largest dataset with 143.7k sequences) had relatively more balanced position distributions compared to smaller datasets, and RPP/CP methods showed limited improvement on this dataset
- Why unresolved: The paper doesn't systematically investigate whether increasing training data volume alone reduces position bias, or if there's a threshold effect where datasets below certain size exhibit severe position bias while larger ones naturally mitigate it
- What evidence would resolve it: Controlled experiments varying dataset size while maintaining position distribution characteristics, measuring position bias severity across different training set sizes

### Open Question 3
- Question: How do position bias effects manifest in cross-lingual transfer learning scenarios where models are trained on one language and evaluated on another?
- Basis in paper: [inferred] The paper focuses on monolingual benchmarks (English) and doesn't address cross-lingual generalization, but position bias could be particularly problematic when transferring position-sensitive knowledge across languages with different word order characteristics
- Why unresolved: Cross-lingual evaluation wasn't part of the study design, leaving uncertainty about whether position bias effects transfer across languages or if language-specific position distributions create additional challenges
- What evidence would resolve it: Cross-lingual experiments training on English datasets and evaluating on non-English NER/POS benchmarks, measuring position bias impact across language pairs with varying syntactic structures

### Open Question 4
- Question: What is the optimal strategy for combining RPP and CP methods, and do they exhibit complementary effects when used together?
- Basis in paper: [explicit] The paper presents RPP and CP as separate methods with different strengths (RPP better for short-range positions, CP better for long-range), but doesn't explore their combination
- Why unresolved: The individual method evaluations suggest potential complementary benefits, but the paper doesn't investigate whether combining both techniques yields multiplicative improvements or if they interfere with each other
- What evidence would resolve it: Systematic ablation studies testing various combinations of RPP and CP with different parameter settings, measuring whether combined approaches outperform either method individually across all position ranges

## Limitations

- The evidence for underlying mechanisms remains largely theoretical, with causal links between absolute position embeddings and observed bias not conclusively established
- The study relies heavily on BERT's absolute position embeddings, limiting generalizability to models with relative position embeddings
- Proposed mitigation methods lack extensive ablation studies to determine optimal hyperparameters
- Effectiveness may vary significantly across different sequence lengths and dataset characteristics

## Confidence

- **High confidence**: The existence of position bias in transformer-based token classification models (F1 drops of 3-9% are well-documented and reproducible)
- **Medium confidence**: The mechanism by which absolute position embeddings cause position bias (plausible but not definitively proven)
- **Medium confidence**: The effectiveness of Random Position Shifting and Context Perturbation as mitigation methods (shown to improve performance by ~2%, but optimal configurations unclear)
- **Low confidence**: Generalizability of findings to all transformer architectures and token classification tasks (primarily tested on BERT and specific NER/POS datasets)

## Next Checks

1. **Cross-architecture validation**: Test the position bias phenomenon and mitigation methods on models with relative position embeddings (XLNet, DeBERTa) and decoder architectures (GPT-2) to determine if the observed effects are specific to BERT's absolute position embeddings or represent a broader transformer vulnerability.

2. **Ablation study of mitigation methods**: Systematically vary the perturbation range in RPP (τ parameter) and concatenation strategies in CP across different sequence length distributions to identify optimal configurations and determine whether the 2% improvement is consistent or varies significantly with hyperparameters.

3. **Class-level analysis of position bias**: Conduct detailed analysis of position bias effects across different entity types and POS tags to determine if certain classes are more susceptible to position bias, and whether mitigation methods have differential effects across classes. This would help identify whether the bias stems from position-specific learning patterns or class-specific distribution characteristics.