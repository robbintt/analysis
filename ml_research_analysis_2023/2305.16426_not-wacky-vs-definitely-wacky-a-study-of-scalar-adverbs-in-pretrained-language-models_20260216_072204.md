---
ver: rpa2
title: 'Not wacky vs. definitely wacky: A study of scalar adverbs in pretrained language
  models'
arxiv_id: '2305.16426'
source_url: https://arxiv.org/abs/2305.16426
tags:
- scalar
- adverbs
- bert
- above
- below
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates whether large pretrained language models
  (BERT, RoBERTa, GPT-2, GPT-3) can represent and process scalar adverbs like "very,"
  "always," and "maybe." Scalar adverbs are logical operators that position concepts
  on scales, but they are less bursty than nouns and thus harder to represent via
  distributional methods. The authors design three tasks: ranking scalar adverbs by
  semantic scale position, masked language modeling (MLM) to predict adverbs in context,
  and entailment judgment to test logical consistency across scales.'
---

# Not wacky vs. definitely wacky: A study of scalar adverbs in pretrained language models

## Quick Facts
- arXiv ID: 2305.16426
- Source URL: https://arxiv.org/abs/2305.16426
- Reference count: 8
- Key outcome: Pretrained language models show some distributional capture of scalar adverb meaning but fail to robustly encode their logical semantics.

## Executive Summary
This paper investigates whether large pretrained language models (BERT, RoBERTa, GPT-2, GPT-3) can represent and process scalar adverbs like "very," "always," and "maybe." Scalar adverbs are logical operators that position concepts on scales, but they are less bursty than nouns and thus harder to represent via distributional methods. The authors design three tasks: ranking scalar adverbs by semantic scale position, masked language modeling (MLM) to predict adverbs in context, and entailment judgment to test logical consistency across scales. They test on naturalistic Reddit data and controlled synthetic sentences. Models showed some success in MLM with context but failed to differentiate between negative polarity adverbs and negation ("not"). Performance improved when negations were excluded, but models appeared to rely heavily on frequency biases rather than logical scale representations. Neither the raw nor MNLI-fine-tuned models achieved human-level performance. The study concludes that despite capturing some distributional aspects of meaning, current pretrained models fall short in robustly encoding logical semantics of scalar adverbs.

## Method Summary
The study evaluates four pretrained language models (BERT, RoBERTa, GPT-2, GPT-3) on three tasks using Reddit politosphere data from 2015. The authors extract phrases of the form 'ADV ADJ.' using SpaCy dependency parsing and select 24 scalar adverbs across three semantic categories (MODALITY, FREQUENCY, DEGREE). They implement three evaluation methods: ranking adverbs by semantic position using cosine similarity and the AdjDIFF method, MLM tasks using masked language modeling to predict adverbs in context, and entailment judgment tasks to test logical consistency across scales. The evaluation uses accuracy, Mean Reciprocal Rank (MRR), Spearman ρ, and Kendall's τ as metrics, with zero-shot evaluation using pretrained models without fine-tuning.

## Key Results
- Models can recover relative positions of scalar adverbs along their logical scales using contextual embeddings, but correlations remain weak.
- Left-hand context provides sufficient information for models to predict scalar adverbs better than random, with MRR doubling when full context is used.
- When negations are removed from candidate completions, models perform near ceiling, suggesting frequency biases rather than logical scale representations drive predictions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models can recover relative positions of scalar adverbs along their logical scales using contextual embeddings.
- Mechanism: By comparing the vector difference between an adjective modified by an adverb and the same adjective without modification, the model implicitly captures the scalar effect of the adverb. Averaging these differences across many adjectives and correlating with reference scale vectors (e.g., top vs. bottom adverbs) yields a ranking.
- Core assumption: The contextual representation of an adjective changes systematically and predictably based on the modifying adverb, and this change is captured in the embedding space.
- Evidence anchors:
  - [abstract] "We find that despite capturing some aspects of logical meaning, the models fall far short of human performance."
  - [section 4.1] "We also devised a third method (AdjDIFF)... the cosine similarity of each resulting vector with the same referent vector as in the second method... and average them to obtain the final cosine similarity value."
  - [corpus] Weak. No explicit corpus validation of this embedding-level mechanism; only internal model output correlations.
- Break condition: If adjective embeddings are dominated by topical features unrelated to scalar modification, or if adverbs contribute negligible signal in the modification process, the method fails.

### Mechanism 2
- Claim: Left-hand context provides sufficient information for models to predict scalar adverbs better than random.
- Mechanism: BERT and RoBERTa use bidirectional attention over the left context to capture collocational and selectional preference cues that indicate which adverb is likely, even when humans find the prediction difficult.
- Core assumption: The left context (up to 40 words) contains statistical regularities (e.g., adjective selectional restrictions, idiomatic usage) that correlate with adverb identity.
- Evidence anchors:
  - [section 4.2] "The results for the full context are better... Both BERT large and BERT base get a significant boost from the full context both in upranking the original adverb (MRR doubling for both models) and in ranking the original adverb above negation."
  - [corpus] Weak. The paper notes the context is "naturalistic and varied," making it uncertain whether the model learns logical vs. distributional cues.
- Break condition: If the left context is too sparse or too diverse to contain predictive signal, or if models rely entirely on adverb frequency rather than context, performance degrades to chance.

### Mechanism 3
- Claim: Models can be tuned to distinguish scalar adverbs from negation in entailment tasks if frequency biases are controlled.
- Mechanism: When negations are removed from candidate completions, models perform near ceiling by leveraging learned representations of logical scale structure rather than raw frequency.
- Core assumption: Models have latent representations that encode scalar positions but are overridden by frequency biases in open prediction tasks.
- Evidence anchors:
  - [section 4.5] "When choosing the top relevant answer excluding negations the results improve drastically to near ceiling."
  - [section 4.5] "The models most likely benefit from biases in both conditions... makes it doubtful whether they learned a separate logical representation of the adverbs' scalar property."
  - [corpus] Weak. No direct evidence of logical scale encoding in the embeddings; only performance shifts when negation is excluded.
- Break condition: If frequency dominates all prediction decisions, even in constrained tasks, the models never learn scalar logic.

## Foundational Learning

- Concept: Semantic scale and scalar adverb categories (modality, frequency, degree).
  - Why needed here: The paper evaluates whether models can differentiate and rank adverbs along these distinct logical scales.
  - Quick check question: Can you name an adverb from each of the three semantic categories and place it correctly on its scale?

- Concept: Contextualized embeddings and their role in capturing meaning.
  - Why needed here: The paper relies on BERT/RoBERTa's contextualized representations to infer scalar positions and predict adverbs.
  - Quick check question: How does a contextualized embedding of "very cold" differ from "slightly cold" in a pretrained model?

- Concept: Masked Language Modeling (MLM) and entailment inference.
  - Why needed here: These are the evaluation tasks used to probe adverb representations and logical reasoning.
  - Quick check question: In an MLM task, what happens when you mask "very" in "It's very cold" and the model predicts "not" instead?

## Architecture Onboarding

- Component map: Reddit text -> SpaCy dependency parsing -> Extract ADV-ADJ phrases -> Masked Language Modeling (BERT/RoBERTa/GPT) -> Prediction ranking and entailment construction -> Evaluation metrics (MRR, accuracy, heatmaps)
- Critical path: Data extraction -> Model prediction -> Context-controlled evaluation -> Logical vs. frequency bias analysis
- Design tradeoffs: Using naturalistic Reddit data gives diversity but reduces control; synthetic sentences increase control but may miss distributional cues; including negation tests logical reasoning but introduces strong frequency confounds
- Failure signatures: High MRR but low accuracy on negation discrimination suggests frequency bias; poor ranking in certain semantic categories suggests missing logical scale encoding; high trivial answer rates indicate shallow context use
- First 3 experiments:
  1. Run the AdjDIFF method on BERT embeddings for all target adverb-adjective pairs and compare Spearman correlations to the gold ranking.
  2. Generate MLM predictions for masked adverbs in both neutral and full context; plot confusion matrices to visualize frequency vs. semantic clustering.
  3. Create entailment sentence pairs with and without negations; evaluate model accuracy and check for trivial or negation-heavy outputs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the models' representations of scalar adverbs improve with task-specific fine-tuning, or are the limitations observed in this study fundamental to the pretraining architecture?
- Basis in paper: [explicit] The authors mention that neither the raw nor MNLI-fine-tuned models achieved human-level performance, and that MNLI training did not improve performance on the strict entailment task.
- Why unresolved: The study only tested a single fine-tuning dataset (MNLI) and did not explore other potential fine-tuning approaches or architectures that might better capture scalar adverb semantics.
- What evidence would resolve it: Testing the models after fine-tuning on specialized datasets containing scalar adverb relations, or exploring alternative architectures designed specifically for logical reasoning tasks.

### Open Question 2
- Question: How do frequency biases in the training data affect the models' ability to learn logical scale representations, and can these biases be mitigated through architectural changes or training modifications?
- Basis in paper: [explicit] The authors note that models showed strong frequency effects in their predictions, with high-frequency adverbs like "very" and "not" dominating outputs regardless of context, and that performance improved when negations were excluded.
- Why unresolved: The study did not systematically vary the frequency distributions in training data or test whether architectural modifications could reduce frequency-based biases.
- What evidence would resolve it: Training models on balanced frequency distributions of scalar adverbs, or testing modified architectures with explicit mechanisms to reduce frequency-based biases.

### Open Question 3
- Question: Do larger or more recent language models (e.g., GPT-3, GPT-4) show improved performance on scalar adverb tasks compared to the models tested in this study?
- Basis in paper: [explicit] The authors tested GPT-3 on a subset of their data and found it performed poorly on the BELOW condition despite having lower rates of trivial and negative completions, suggesting it may be even more biased toward high-frequency top-of-scale answers.
- Why unresolved: The study only tested GPT-3 on a small subset of data and did not explore other recent large language models or compare their performance systematically.
- What evidence would resolve it: Comprehensive testing of multiple recent large language models on the full dataset used in this study, including both zero-shot and fine-tuned conditions.

### Open Question 4
- Question: Can models develop more robust scalar representations if trained on data that explicitly highlights logical relationships between scalar adverbs rather than relying solely on distributional context?
- Basis in paper: [inferred] The authors note that scalar adverbs are less bursty than nouns and thus harder to represent via distributional methods, suggesting that current pretraining approaches may be insufficient for capturing their logical semantics.
- Why unresolved: The study only examined models trained on standard pretraining objectives and did not test whether modified training approaches could improve scalar adverb representations.
- What evidence would resolve it: Training models with modified objectives that explicitly incorporate logical relationships between scalar adverbs, or testing whether adding annotated data about scalar relationships improves performance.

## Limitations

- The study's conclusions are limited by the reliance on frequency biases rather than proven logical representations, as performance improvements when negations are excluded may reflect frequency suppression rather than genuine learning of scalar logic.
- The naturalistic Reddit data introduces uncontrolled variables that may mask or confound the underlying mechanisms being tested, reducing confidence in the findings.
- The lack of explicit corpus validation for the embedding-level mechanisms (particularly AdjDIFF) means the claimed systematic changes in adjective embeddings due to adverb modification remain assumptions rather than established facts.

## Confidence

- High confidence: Models can recover relative positions of scalar adverbs along their logical scales using contextual embeddings (Mechanism 1) - supported by observed correlations but lacking direct embedding validation.
- Medium confidence: Left-hand context provides sufficient information for models to predict scalar adverbs better than random (Mechanism 2) - supported by performance improvements with full context, but unclear if logical vs. distributional cues drive this.
- Low confidence: Models can be tuned to distinguish scalar adverbs from negation in entailment tasks if frequency biases are controlled (Mechanism 3) - supported by near-ceiling performance when negations are excluded, but this may reflect frequency suppression rather than logical learning.

## Next Checks

1. Validate the AdjDIFF mechanism by explicitly testing whether adjective embeddings change systematically and predictably based on the modifying adverb across multiple contexts and models.
2. Conduct ablation studies on the Reddit data to isolate the contribution of distributional vs. logical cues by systematically varying context length, semantic category diversity, and adverb frequency.
3. Design a controlled synthetic dataset where scalar adverb positions are perfectly encoded and test whether models can learn and apply this logical structure in both MLM and entailment tasks.