---
ver: rpa2
title: 'Post Turing: Mapping the landscape of LLM Evaluation'
arxiv_id: '2311.02049'
source_url: https://arxiv.org/abs/2311.02049
tags:
- evaluation
- language
- arxiv
- human
- benchmarks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a historical survey and taxonomy of evaluation
  methods for large language models (LLMs), identifying key trends and challenges
  across three eras of LLM development. It finds that current evaluation approaches
  are fragmented, inconsistent, and often anthropomorphic, relying heavily on human
  benchmarks and pairwise comparisons.
---

# Post Turing: Mapping the landscape of LLM Evaluation

## Quick Facts
- arXiv ID: 2311.02049
- Source URL: https://arxiv.org/abs/2311.02049
- Reference count: 40
- One-line primary result: Current LLM evaluation methods are fragmented and inconsistent, requiring new standardized approaches beyond human-like testing

## Executive Summary
This paper presents a comprehensive historical survey and taxonomy of evaluation methods for large language models across three distinct eras of development. The authors identify that current evaluation approaches are fragmented, inconsistent, and heavily anthropomorphic, relying on human benchmarks and pairwise comparisons that become obsolete as models surpass human performance. The paper highlights key challenges including benchmark obsolescence, test set leakage, high computational costs, and noise in both human and model-based assessments. The authors call for a qualitative leap in evaluation methodology, advocating for standardized taxonomies, clearer evaluation goals, and potentially new evaluation paradigms that go beyond traditional human-like testing frameworks.

## Method Summary
The paper conducts a literature review and analysis of existing LLM evaluation methodologies, examining three historical eras of LLM development and their respective evaluation approaches. The authors create a taxonomy of evaluation aspects and analyze the strengths and weaknesses of different evaluation methods including benchmarks, human assessment, and model-based evaluation. Through this analysis, they identify key challenges in current evaluation practices and propose directions for improvement focusing on standardization, clearer evaluation goals, and potentially new evaluation paradigms that could provide more robust and objective assessment frameworks for LLMs.

## Key Results
- Current LLM evaluation methods are fragmented and inconsistent due to benchmark obsolescence and reliance on human-centric approaches
- Human evaluation introduces significant noise and bias, while using superior models like GPT-4 as judges introduces systematic vocabulary and positional biases
- The paper calls for standardized taxonomies and new evaluation paradigms beyond traditional human-like testing to address fundamental limitations in current approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM evaluation methods are fragmented and inconsistent because benchmarks become obsolete as models surpass human-level performance on older tasks.
- Mechanism: When models achieve near-perfect or superhuman scores on established benchmarks, these benchmarks lose discriminative power and fail to capture meaningful differences in model capabilities. This forces researchers to continually create new benchmarks, leading to a proliferation of incompatible evaluation methods.
- Core assumption: Benchmark obsolescence is a primary driver of evaluation fragmentation, and there exists a threshold beyond which traditional benchmarks become ineffective at differentiating model quality.
- Evidence anchors:
  - [abstract] "current evaluation approaches are fragmented, inconsistent and often anthropomorphic, relying heavily on human benchmarks and pairwise comparisons"
  - [section] "Since LLMs from this period achieved at most 50%-80% of human-level performance on these tasks, the progress across various models was clearly visible" and "Consequently, researchers tend to use more complex and/or specific benchmarks"
  - [corpus] Weak evidence - the corpus neighbors don't directly address benchmark obsolescence as a fragmentation mechanism
- Break condition: If researchers develop standardized, adaptive benchmarks that maintain discriminative power even as models improve, or if the community converges on a core set of evaluation metrics that remain relevant despite model advances.

### Mechanism 2
- Claim: Human evaluation introduces noise and bias because assessors lack consistent criteria and may be influenced by model-generated responses.
- Mechanism: Without clear, objective evaluation criteria, human assessors rely on subjective preferences that can vary widely between individuals. Additionally, assessors may unconsciously favor more polished or stylistically appealing responses over factually correct ones, and some may even use LLMs to generate responses, further contaminating the evaluation process.
- Core assumption: Human evaluation is inherently noisy and subjective without standardized criteria, and this noise is significant enough to undermine the reliability of human-based assessments.
- Evidence anchors:
  - [abstract] "high computational costs, and noise in human and model-based assessments"
  - [section] "Another significant issue is the quality of human labels, which can be relatively low for different reasons" and "Human assessors' motivation is sometimes insufficient to provide high-quality answers; moreover, some assessors secretly use LLMs as to speed up the labelling"
  - [corpus] Weak evidence - corpus neighbors don't specifically address human evaluation noise and bias
- Break condition: If the community develops and adopts standardized, objective criteria for human evaluation, or if human evaluation is replaced entirely by more reliable automated methods.

### Mechanism 3
- Claim: Using superior LLMs like GPT-4 as judges introduces systematic biases because these models have their own vocabulary preferences and positional biases.
- Mechanism: When GPT-4 is used to evaluate other models, it tends to favor responses that resemble its own generation style and may be influenced by the position of responses in the evaluation interface. These biases can systematically skew evaluation results and create inconsistencies with human assessments.
- Core assumption: Superior LLMs used as judges have identifiable biases that significantly affect their evaluation outcomes, and these biases are not aligned with human preferences.
- Evidence anchors:
  - [section] "GPT-4 is also known to have a specific vocabulary bias, particularly it prefers its own generations more than humans do" and "GPT-4 seems to have specific positional biases"
  - [abstract] "noise in human and model-based assessments"
  - [corpus] Weak evidence - corpus neighbors don't specifically address GPT-4 evaluation biases
- Break condition: If evaluation methods are developed that can account for and mitigate these systematic biases, or if alternative evaluation approaches that don't rely on biased judges are adopted.

## Foundational Learning

- Concept: Benchmark obsolescence and evaluation fragmentation
  - Why needed here: Understanding how and why benchmarks become ineffective is crucial for recognizing the core problem the paper addresses - the fragmentation of LLM evaluation methods
  - Quick check question: Why do traditional benchmarks become less useful as LLM performance improves, and what are the consequences of this obsolescence?

- Concept: Human evaluation limitations and biases
  - Why needed here: Recognizing the limitations of human evaluation is essential for understanding why current evaluation methods are problematic and why new approaches are needed
  - Quick check question: What are the main sources of noise and bias in human evaluation of LLMs, and how do these issues affect the reliability of evaluation results?

- Concept: Superior LLM evaluation and systematic biases
  - Why needed here: Understanding how using superior models as judges can introduce systematic biases is crucial for recognizing the limitations of current automated evaluation methods
  - Quick check question: What are the main biases that can affect LLM-based evaluation, and why do these biases create problems for reliable model comparison?

## Architecture Onboarding

- Component map: Benchmark creation -> Human evaluation pipeline -> Automated evaluation using superior models -> Result aggregation into leaderboards and taxonomies
- Critical path: The most critical path is from benchmark creation through evaluation to result aggregation. If benchmarks become obsolete or evaluation methods are biased, the entire system fails to provide meaningful comparisons.
- Design tradeoffs: The main tradeoffs are between comprehensiveness (using many benchmarks and evaluation methods) and consistency (maintaining a coherent, standardized evaluation system). There's also a tradeoff between human evaluation (which can capture subjective qualities) and automated evaluation (which is more scalable but may miss nuances).
- Failure signatures: Key failure signatures include divergence between benchmark results and human assessments, inconsistent rankings across different leaderboards, and the inability to distinguish between models that all perform near-perfectly on available benchmarks.
- First 3 experiments:
  1. Analyze the correlation between benchmark performance and human preferences across a set of LLMs to quantify the degree of inconsistency
  2. Test GPT-4's evaluation consistency by having it evaluate the same responses multiple times with different formatting or positioning to identify systematic biases
  3. Create a simplified evaluation pipeline using a small, standardized set of benchmarks and human evaluation criteria to test whether more focused evaluation can achieve better consistency than current fragmented approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the fundamental cognitive skills or abilities that large language models should be evaluated on, beyond current benchmarks?
- Basis in paper: [explicit] The paper discusses the need for "fundamentally new approaches" and "qualitatively higher levels of assessment" beyond current human-centric benchmarks.
- Why unresolved: Current evaluation methods are largely based on human tests and benchmarks, which may not capture the full range of capabilities of LLMs. The paper argues that we need to develop new evaluation paradigms that go beyond human-like testing.
- What evidence would resolve it: Development and validation of new evaluation methods that assess LLM capabilities in areas not covered by current benchmarks, such as abstract reasoning, problem-solving in unfamiliar environments, or novel skill acquisition.

### Open Question 2
- Question: How can we create a standardized, comprehensive taxonomy of evaluation aspects for large language models?
- Basis in paper: [explicit] The paper highlights the lack of structure or system in the aspects of evaluation used by researchers, calling for a "proper taxonomy" and "generic baselines" for different skills.
- Why unresolved: While various aspects of LLM performance are evaluated (e.g., text comprehension, knowledge, reasoning skills), there is no agreed-upon system for categorizing and prioritizing these aspects. This leads to inconsistent and potentially incomplete evaluations.
- What evidence would resolve it: A widely accepted framework that categorizes LLM evaluation aspects into a standardized taxonomy, with clear definitions and methods for assessing each aspect.

### Open Question 3
- Question: What is the optimal balance between automated evaluation methods and human assessment for large language models?
- Basis in paper: [explicit] The paper discusses the challenges and limitations of both automated benchmarks and human evaluation, including issues like benchmark obsolescence, computational costs, and human label quality.
- Why unresolved: While automated methods are more scalable and cost-effective, they may miss nuanced aspects of model performance that humans can assess. Conversely, human evaluation is expensive and potentially biased. The optimal balance between these approaches remains unclear.
- What evidence would resolve it: Empirical studies comparing the effectiveness, reliability, and efficiency of different combinations of automated and human evaluation methods for various aspects of LLM performance.

## Limitations

- The evidence for key mechanisms is primarily based on paper assertions rather than independent empirical validation, with weak support from corpus analysis
- The paper doesn't provide specific implementation details for the new evaluation frameworks it advocates for
- Analysis doesn't account for potential solutions or alternative evaluation approaches that may already exist in the broader research community

## Confidence

- High confidence: The general observation that current LLM evaluation is fragmented and inconsistent is well-supported by the literature and the paper's analysis
- Medium confidence: The specific mechanisms linking benchmark obsolescence to evaluation fragmentation and human evaluation to noise are plausible but lack direct empirical validation in this analysis
- Low confidence: The claims about GPT-4's specific biases in evaluation are based on the paper's assertions without independent verification or broader community consensus

## Next Checks

1. Conduct a systematic review of existing LLM evaluation leaderboards to quantify the degree of inconsistency between different evaluation methodologies and identify specific sources of divergence
2. Design and execute controlled experiments testing human evaluation consistency by having multiple assessors evaluate the same LLM responses using both standardized criteria and free-form assessment
3. Perform replication studies of GPT-4-based evaluation to measure positional biases and vocabulary preferences, comparing results with human preferences across a diverse set of LLM responses