---
ver: rpa2
title: Parallel Multi-Objective Hyperparameter Optimization with Uniform Normalization
  and Bounded Objectives
arxiv_id: '2309.14936'
source_url: https://arxiv.org/abs/2309.14936
tags:
- optimization
- which
- penalty
- objectives
- workers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper tackles the challenge of optimizing machine learning\
  \ models across multiple objectives\u2014such as accuracy, fairness, and latency\u2014\
  using multi-objective hyperparameter optimization. It addresses issues like disparate\
  \ objective scales, outliers, and the need for parallel scaling."
---

# Parallel Multi-Objective Hyperparameter Optimization with Uniform Normalization and Bounded Objectives

## Quick Facts
- arXiv ID: 2309.14936
- Source URL: https://arxiv.org/abs/2309.14936
- Reference count: 40
- The paper tackles the challenge of optimizing machine learning models across multiple objectives—such as accuracy, fairness, and latency—using multi-objective hyperparameter optimization. It addresses issues like disparate objective scales, outliers, and the need for parallel scaling. The proposed method, D-MoBO, combines quantile uniform normalization, a randomized scalarization scheme, and a penalty function to guide the search toward feasible, high-quality trade-offs. Experiments show that D-MoBO outperforms strong baselines (NSGAII, MoTPE, Random) in early iterations and maintains high solution quality when scaled up to 16x more parallel workers, achieving a 5x speedup. The results confirm that D-MoBO's normalization and penalty mechanisms improve efficiency and effectiveness in parallel multi-objective hyperparameter optimization.

## Executive Summary
This paper addresses the challenge of optimizing machine learning models across multiple conflicting objectives—such as accuracy, fairness, and latency—through multi-objective hyperparameter optimization (MOHPO). The proposed D-MoBO method tackles key issues including disparate objective scales, outliers, and the need for efficient parallel scaling. By combining quantile-uniform normalization, randomized scalarization, and a penalty function for bounded objectives, D-MoBO demonstrates superior performance compared to strong baselines in both solution quality and parallel efficiency.

## Method Summary
The D-MoBO method employs quantile-uniform (QU) normalization to transform objective values into a uniform distribution, preserving Pareto optimality while making the problem scale-invariant. It uses randomized scalarization with linear, Chebyshev, or PBI functions to approximate the Pareto front, and implements a progressive barrier penalty to discourage exploration of infeasible configurations. The asynchronous decentralized architecture enables efficient parallel search across multiple workers, with exponential decay scheduling to balance exploration and exploitation. The method is evaluated on HPOBench tabular tasks, the Combo benchmark, and DTLZ test problems, demonstrating improved hypervolume indicator scores and 5x speedup with 16x more workers.

## Key Results
- D-MoBO outperforms NSGAII, MoTPE, and Random baselines in early iterations and maintains high solution quality
- Achieves 5x speedup when scaling from 40 to 640 parallel workers on the Combo benchmark
- QU normalization and penalty mechanisms significantly improve efficiency and effectiveness in parallel MOHPO
- Maintains strong performance across diverse problem domains including HPOBench, Combo, and DTLZ benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The quantile-uniform (QU) normalization transforms objective values such that the empirical cumulative distribution function (ECDF) is mapped to a uniform distribution, preserving Pareto optimality while making the problem scale-invariant.
- Mechanism: By applying the ECDF to each objective and composing it with the identity function of the uniform distribution, the QU transformation produces values in [0,1] that are uniformly distributed. This ensures that scalarization weights have equal influence regardless of the original objective scale.
- Core assumption: The ECDF is strictly monotone (when invertible) and therefore order-preserving, which guarantees Pareto set preservation.
- Evidence anchors:
  - [abstract] "We propose a multi-objective Bayesian optimization (MoBO) algorithm that addresses these problems through uniform objective normalization and randomized weights in scalarization."
  - [section] "To resolve these problems, we require a transformation that would conserve the PF properties, focuses on areas of interest (i.e., close to the estimated PF), and is robust to outliers. A mapping of the independent objective distributions P (yi) to the uniform distribution can provide such properties."
  - [corpus] Weak evidence - no direct mention of QU normalization in neighbor papers.
- Break condition: If the objective distributions are not continuous or have heavy ties, the ECDF may not be strictly monotone, breaking the order-preserving property.

### Mechanism 2
- Claim: The progressive barrier penalty function discourages exploration of hyperparameter configurations that violate minimal accuracy thresholds by adding a penalty to all objectives whenever one or more objectives exceed their upper bounds.
- Mechanism: When any objective yi exceeds its upper bound UBi, a penalty p = γ Σ max(yu[:,i] - UBu[i], 0) is added to all objectives. This penalty increases with the number and magnitude of constraint violations, making such configurations less attractive to the optimizer.
- Core assumption: The penalty strength γ is set appropriately (γ = 2 in this work) such that it discourages exploration without overwhelming the objective values.
- Evidence anchors:
  - [abstract] "We increase the efficiency of our approach by imposing constraints on the objective to avoid exploring unnecessary configurations (e.g., insufficient accuracy)."
  - [section] "In the single-objective literature, nonlinear constraints are often handled via a penalty function, such as an augmented Lagrangian. This technique generalizes to the multi-objective case, where the penalty must be applied to all objectives (not only the objective that violates its upper bound)."
  - [corpus] Weak evidence - no direct mention of penalty functions in neighbor papers.
- Break condition: If the penalty strength γ is too small, constraint violations won't be discouraged; if too large, it may prevent exploration of the feasible region near the constraint boundaries.

### Mechanism 3
- Claim: The asynchronous decentralized architecture with periodic exponential decay scheduling balances exploration and exploitation across multiple parallel workers, preventing over-exploration while maintaining diversity.
- Mechanism: Each worker independently runs a sequential Bayesian optimizer but shares observations through a distributed storage. The exploration-exploitation trade-off parameter κt is initialized differently for each worker and decays periodically: κt = κ0 · exp(-λ · (t mod T)). This prevents workers from all exploring simultaneously while allowing coordinated exploitation.
- Core assumption: Different initial κ0 values and periodic decay create sufficient diversity in exploration patterns across workers without causing conflicts or redundant evaluations.
- Evidence anchors:
  - [abstract] "Finally, we leverage an approach to parallelize the MoBO which results in a 5x speed-up when using 16x more workers."
  - [section] "The trick is to initialize different exploitation-exploration parameters combined with an exponential-decay scheduler on this parameter to avoid 'over'-exploring when increasing workers."
  - [corpus] Weak evidence - no direct mention of asynchronous decentralized architecture in neighbor papers.
- Break condition: If the decay rate λ is too aggressive, workers may converge too quickly to exploitation, reducing diversity; if too slow, workers may continue exploring suboptimally.

## Foundational Learning

- Concept: Hypervolume Indicator (HVI) calculation and its sensitivity to objective scaling and reference point selection.
  - Why needed here: The paper uses HVI as the primary performance metric, and understanding its properties is crucial for interpreting results and choosing appropriate normalizations.
  - Quick check question: What happens to HVI values if one objective has a much larger scale than others, and how does QU normalization address this?

- Concept: Pareto compliance and the relationship between dominance relations and scalarization functions.
  - Why needed here: The paper compares different scalarization functions (Linear, Chebyshev, PBI) and their ability to approximate the Pareto front, requiring understanding of when scalarization preserves Pareto optimality.
  - Quick check question: Under what conditions does uniform weight sampling in linear scalarization fail to produce a uniform distribution of solutions along the Pareto front?

- Concept: Empirical Cumulative Distribution Function (ECDF) properties and order-preserving transformations.
- Why needed here: The QU normalization relies on ECDF being order-preserving, which is the theoretical foundation for preserving Pareto optimality under this transformation.
  - Quick check question: Prove that if t is an order-preserving transformation, then y* is non-dominated in Y if and only if t(y*) is non-dominated in t(Y).

## Architecture Onboarding

- Component map:
  - Shared memory storage (Redis or similar) for distributed observations
  - Multiple Bayesian optimization agents running in parallel
  - Random-Forest surrogate model with randomized scalarization
  - Quantile-uniform normalization module
  - Progressive barrier penalty module
  - Scalarization function selector (Linear, Chebyshev, PBI)
  - Exponential decay scheduler for exploration-exploitation balance

- Critical path:
  1. Worker initialization with unique random seed and κ0 sampling
  2. Configuration suggestion using lower confidence bound with current κt
  3. Black-box evaluation and result collection
  4. Shared memory update with new observations
  5. Periodic exponential decay of κt
  6. Return to suggestion step

- Design tradeoffs:
  - QU normalization vs identity vs MinMax-Log: QU provides scale-invariance and robustness to outliers but requires computing ECDFs
  - Penalty strength γ: Must balance between discouraging bad solutions and not preventing exploration of feasible boundary
  - Scalarization choice: Linear is computationally efficient but may miss non-convex regions; Chebyshev is more robust but harder to optimize

- Failure signatures:
  - Poor utilization (<95%) indicates workers are idle waiting for suggestions or evaluations
  - Stagnant HVI improvement suggests the penalty is too aggressive or scalarization is poorly suited to the problem
  - Non-uniform coverage of Pareto front indicates weight sampling strategy needs adjustment

- First 3 experiments:
  1. Run D-MoBO with QU normalization on HPOBench tabular tasks with 2 objectives (validation error, training time) for 200 iterations, compare against NSGAII and MoTPE
  2. Apply D-MoBO with penalty on Combo benchmark, measure HVI improvement and number of valid solutions meeting accuracy threshold
  3. Scale D-MoBO from 40 to 640 parallel workers on Combo benchmark, measure speedup and solution quality improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the choice of penalty upper bounds be automated without prior knowledge of the problem?
- Basis in paper: [explicit] The authors mention that the penalty strategy is particularly effective for QU normalization but can be hard to set without prior information about the problem, and is a limitation of the method.
- Why unresolved: The authors note that using a penalty can be ineffective if the true Pareto Front is unknown, and that the penalty can be hard to set without prior information.
- What evidence would resolve it: A method or framework that can automatically determine appropriate penalty upper bounds based on observed data or problem characteristics would resolve this question.

### Open Question 2
- Question: How does the performance of D-MoBO compare to other multi-objective optimization algorithms that are compatible with mixed-integer constraints, such as constrained NSGAII?
- Basis in paper: [inferred] The authors note that only NSGAII compatible with mixed-integers was tested but not without constraints, which would make a better comparison.
- Why unresolved: The authors only compared D-MoBO to the vanilla version of NSGAII and other algorithms, which do not enforce any bounds on the objectives.
- What evidence would resolve it: Experiments comparing D-MoBO to constrained NSGAII or other multi-objective optimization algorithms that can handle mixed-integer constraints would resolve this question.

### Open Question 3
- Question: How does the choice of scalarization function affect the performance of D-MoBO on problems with non-convex Pareto fronts?
- Basis in paper: [explicit] The authors note that the Chebyshev scalarization may be harder to optimize for MoBO and could require a few iterations with fixed weights instead of always re-sampling weight vectors.
- Why unresolved: The authors only tested a few scalarization functions and did not thoroughly investigate the impact of the choice of scalarization function on problems with non-convex Pareto fronts.
- What evidence would resolve it: Experiments testing more scalarization functions and comparing their performance on problems with non-convex Pareto fronts would resolve this question.

### Open Question 4
- Question: How does the parallel performance of D-MoBO scale with an increasing number of objectives?
- Basis in paper: [inferred] The authors note that the HVI is extremely sensitive to poor problem scaling and that it is exponentially expensive to calculate when the number of objectives is greater than 2.
- Why unresolved: The authors only tested D-MoBO on problems with 2 or 3 objectives and did not investigate how the parallel performance scales with an increasing number of objectives.
- What evidence would resolve it: Experiments testing D-MoBO on problems with a larger number of objectives and comparing the parallel performance to the performance on problems with fewer objectives would resolve this question.

## Limitations

- The QU normalization's theoretical guarantee of Pareto optimality preservation relies on the ECDF being strictly monotone, which may not hold for objectives with heavy ties or discrete distributions.
- The penalty function's effectiveness is sensitive to the choice of γ and upper bounds, requiring careful tuning for each application domain.
- The asynchronous decentralized architecture's scalability benefits depend on the black-box evaluation time being significantly longer than the overhead of distributed coordination.

## Confidence

- **High Confidence:** The QU normalization mechanism's ability to make scalarization weights equally influential across objectives (supported by multiple mathematical derivations and empirical validation).
- **Medium Confidence:** The penalty function's effectiveness in guiding search toward feasible regions (supported by experimental results but sensitive to hyperparameter choices).
- **Medium Confidence:** The 5x speedup claim at 16x worker scaling (supported by controlled experiments but may vary with different hardware and problem characteristics).

## Next Checks

1. Test D-MoBO on problems with heavily tied objective values to verify Pareto preservation under non-strict ECDF conditions.
2. Conduct ablation studies varying γ across orders of magnitude to quantify sensitivity of solution quality to penalty strength.
3. Evaluate D-MoBO's performance on multi-objective problems with more than 3 objectives to assess scalability of the QU normalization approach.