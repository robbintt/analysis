---
ver: rpa2
title: 'CRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without Full
  Large Language Model'
arxiv_id: '2310.15477'
source_url: https://arxiv.org/abs/2310.15477
tags:
- layers
- llms
- layer
- crash
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes Large Language Models (LLMs) from the perspectives
  of representation and functional similarity to understand Offsite-Tuning (OFT),
  a technique that transfers transformer blocks between centralized LLMs and downstream
  emulators. The findings reveal a unique modular structure within the layers of LLMs
  that emerges as the model size increases.
---

# CRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without Full Large Language Model

## Quick Facts
- arXiv ID: 2310.15477
- Source URL: https://arxiv.org/abs/2310.15477
- Reference count: 40
- Key outcome: CRaSh significantly boosts the performance of Offsite-Tuning (OFT) with billions of parameters by clustering, removing, and sharing layers in LLMs

## Executive Summary
This paper introduces CRaSh, a training-free strategy for fine-tuning large language models (LLMs) without using the full model. By analyzing the emergent modular structure in LLMs as they scale, the authors propose a method that clusters similar layers, removes redundant ones, and shares weights among remaining layers. This approach significantly improves the performance of Offsite-Tuning (OFT), which transfers transformer blocks between centralized LLMs and downstream emulators. The findings also reveal that optimal solutions from CRaSh and full fine-tuning fall within the same loss landscape basin, demonstrating linear connectivity among these optima.

## Method Summary
CRaSh enhances Offsite-Tuning (OFT) by analyzing the modular structure of LLMs and selectively reducing their depth. The method involves three steps: Clustering (using CKA to group adjacent layers based on representation similarity), Removing (retaining only cluster centers and selecting n layers for fine-tuning), and Sharing (sharing weights among remaining layers to enhance emulator depth). The process starts with computing CKA similarity between adjacent layers using a support dataset, followed by hierarchical clustering to form layer groups. Non-center layers are removed, and weights are shared between adjacent remaining layers. The selected layers are fine-tuned on downstream data, and the optimized weights are transferred back to the original LLM for evaluation.

## Key Results
- CRaSh significantly improves OFT performance, achieving comparable results to full fine-tuning with fewer layers
- The modular structure in LLMs emerges as model size increases, particularly in models 13B and larger
- Optimal solutions from CRaSh and full fine-tuning exhibit linear connectivity in the loss landscape, falling within the same basin

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Emergent modular structure in LLMs allows effective layer dropping without severe performance loss
- Mechanism: As model size increases, layers self-organize into clusters with similar representations. Removing layers within these clusters minimizes behavioral changes.
- Core assumption: CKA similarity between adjacent layers correlates with functional redundancy
- Evidence anchors:
  - [abstract] "findings reveal a unique modular structure within the layers of LLMs that appears to emerge as the model size expands"
  - [section] "models with a size less than 10B exhibit uniform representations across all layers. However, for models of size 13B and 30B, modular structure becomes apparent"
  - [corpus] Weak - related papers focus on parameter-efficient tuning rather than structural emergence
- Break condition: If clustering metric (CKA) fails to capture functional similarity, or if model size is below modular point

### Mechanism 2
- Claim: Layer sharing compensates for information loss from dropped layers
- Mechanism: Sharing weights between adjacent layers maintains representation capacity when depth is reduced
- Core assumption: Adjacent layers in same cluster can substitute for each other functionally
- Evidence anchors:
  - [section] "we implement layer sharing within the remaining model to achieve optimal results"
  - [section] "weights sharing between adjacent layers can maintain comparable performance to the primary model"
  - [corpus] Weak - related work focuses on federated learning rather than layer sharing specifics
- Break condition: If shared layers diverge significantly during fine-tuning, or if task requires unique layer behaviors

### Mechanism 3
- Claim: OFT maintains connectivity in loss landscape, enabling effective CRaSh
- Mechanism: Optimal solutions from CRaSh and full fine-tuning fall in same basin with linear connectivity
- Core assumption: Loss surface analysis can predict transferability of solutions
- Evidence anchors:
  - [abstract] "our findings demonstrate a linear connectivity among these optima falling over the same basin"
  - [section] "through the interpolation of various solutions, we observe their mode connectivity in the parameter space"
  - [corpus] Weak - related papers focus on fine-tuning limitations rather than loss landscape analysis
- Break condition: If initialization point lies outside basin, or if loss landscape has disconnected regions

## Foundational Learning

- Concept: Centered Kernel Alignment (CKA)
  - Why needed here: Measures representation similarity between layers to identify modular structure
  - Quick check question: What does high CKA between two layers indicate about their functional relationship?

- Concept: Hierarchical clustering
  - Why needed here: Groups adjacent layers based on representation similarity for CRaSh
  - Quick check question: Why does CRaSh restrict clustering to adjacent layers only?

- Concept: Loss landscape and mode connectivity
  - Why needed here: Explains why solutions from different fine-tuning approaches can be connected
  - Quick check question: What does linear connectivity between two optima imply about their relative performance?

## Architecture Onboarding

- Component map:
  Input preprocessing -> Layer similarity analysis -> Clustering -> Layer removal -> Layer sharing -> Fine-tuning -> Weight transfer

- Critical path:
  1. Compute CKA between adjacent layers using 512 samples
  2. Apply hierarchical clustering to form layer groups
  3. Remove non-center layers from each cluster
  4. Share weights between adjacent remaining layers
  5. Fine-tune selected layers on downstream data
  6. Transfer weights back to original LLM

- Design tradeoffs:
  - Layer removal vs. performance: More aggressive dropping saves compute but risks accuracy
  - Layer sharing vs. uniqueness: Sharing saves memory but may limit task-specific adaptation
  - Sample size vs. accuracy: Larger sample sets improve CKA stability but increase compute cost

- Failure signatures:
  - CRaSh performs worse than uniform layer dropping: Clustering step may be misidentifying similar layers
  - Emulator fails to match full model zero-shot performance: Layer sharing insufficient to compensate for depth reduction
  - Fine-tuning diverges: Learning rate too high or initialization outside basin

- First 3 experiments:
  1. Run CRaSh vs. uniform layer dropping on small dataset (e.g., ARC-E) to verify clustering benefits
  2. Test layer sharing impact by comparing with/without sharing on same dataset
  3. Validate loss landscape connectivity by interpolating between CRaSh and full fine-tuning solutions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the modular structure observed in LLMs emerge gradually as model size increases, or is there a specific threshold size where it suddenly appears?
- Basis in paper: Explicit - "As the number of parameters increases, distinct blocks or modules with high similarity emerge in the representations of different layers... However, for models of size 13B and 30B, modular structure becomes apparent."
- Why unresolved: The paper notes the emergence but doesn't specify whether it's gradual or threshold-based. Further analysis across more model sizes would be needed.
- What evidence would resolve it: Detailed analysis of representation similarity across a wider range of model sizes (e.g., 100M, 500M, 1B, 2B, 5B, 10B) would reveal if the emergence is gradual or sudden.

### Open Question 2
- Question: How does the choice of clustering algorithm affect the effectiveness of CRaSh?
- Basis in paper: Explicit - "we cluster adjacent layers based on their similarity using various hierarchical clustering algorithms (Murtagh and Contreras, 2012)"
- Why unresolved: While hierarchical clustering is mentioned, the paper doesn't explore or compare the performance of different clustering algorithms (e.g., k-means, DBSCAN, spectral clustering) in the context of CRaSh.
- What evidence would resolve it: Empirical comparison of CRaSh performance using different clustering algorithms on the same datasets and model sizes would reveal the impact of clustering method choice.

### Open Question 3
- Question: Can the modular structure and layer dropping techniques be applied to other types of neural networks beyond LLMs, such as computer vision models?
- Basis in paper: Inferred - The paper focuses on LLMs but mentions related work on vision models (Raghu et al., 2021) and discusses layer dropping in a general context.
- Why unresolved: The paper's analysis and methodology are specific to LLMs, and it's unclear if the findings about modular structure and layer dropping would generalize to other architectures.
- What evidence would resolve it: Applying similar analysis and layer dropping techniques to vision models (e.g., ResNet, Vision Transformers) and comparing their performance to the LLM results would indicate if the concepts are broadly applicable.

## Limitations
- The clustering approach relies heavily on CKA similarity, which may not capture all aspects of functional redundancy
- The study focuses primarily on encoder-decoder models, with limited exploration of decoder-only architectures like GPT
- Computational savings come with performance tradeoffs that vary by task complexity

## Confidence

**High Confidence** in the core claim that CRaSH improves OFT performance through layer clustering and sharing, supported by systematic experiments across multiple model scales and tasks.

**Medium Confidence** in the claim about linear connectivity in loss landscape, as this requires careful hyperparameter tuning and may not hold for all initialization points or task distributions.

**Low Confidence** in the universal applicability of the emergent modular structure observation, since it appears only in models 13B and larger, with no clear theoretical explanation for the threshold.

## Next Checks

1. Test CRaSH on diverse decoder-only models (e.g., GPT-3 variants) to verify the modular structure hypothesis holds beyond encoder-decoder architectures.

2. Conduct ablation studies varying the number of samples used for CKA computation to quantify the impact on clustering quality and downstream performance.

3. Perform extensive loss landscape analysis across different initialization points and learning rates to verify the robustness of the linear connectivity claim.