---
ver: rpa2
title: 'Accelerated Policy Gradient: On the Convergence Rates of the Nesterov Momentum
  for Reinforcement Learning'
arxiv_id: '2310.11897'
source_url: https://arxiv.org/abs/2310.11897
tags:
- policy
- convergence
- gradient
- have
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Accelerated Policy Gradient (APG), a method
  that incorporates Nesterov momentum into policy gradient algorithms for reinforcement
  learning. The key challenge addressed is the non-concavity of RL objectives, which
  prevents direct application of Nesterov acceleration theory.
---

# Accelerated Policy Gradient: On the Convergence Rates of the Nesterov Momentum for Reinforcement Learning

## Quick Facts
- arXiv ID: 2310.11897
- Source URL: https://arxiv.org/abs/2310.11897
- Reference count: 40
- Primary result: APG achieves O(1/t²) convergence rate with constant step sizes and linear convergence with exponentially-growing step sizes

## Executive Summary
This paper introduces Accelerated Policy Gradient (APG), a method that incorporates Nesterov momentum into policy gradient algorithms for reinforcement learning. The key innovation addresses the non-concavity of RL objectives by showing that APG enters a locally nearly-concave regime within finite time, after which Nesterov acceleration can be applied. Under softmax parameterization, APG achieves significantly faster convergence rates than standard policy gradient methods, with O(1/t²) convergence using constant step sizes and linear convergence using exponentially-growing step sizes.

## Method Summary
APG extends Nesterov momentum to policy gradient methods by leveraging a two-phase analysis framework. The method first shows that APG escapes the globally non-concave region and enters a locally nearly-concave regime in finite time. Once in this regime, the locally convex-like structure enables accelerated convergence. The algorithm uses a restart mechanism to maintain monotonic improvement, where the momentum parameter is updated only when it provides improvement over the current policy parameter. This design ensures that APG remains in the beneficial near-concave region indefinitely.

## Key Results
- APG achieves O(1/t²) convergence rate with constant step sizes, improving upon standard PG's Θ(1/t) rate
- APG achieves linear convergence O(e^(-ct)) with exponentially-growing step sizes
- Numerical experiments validate theoretical findings, showing APG converges faster than standard PG and heavy-ball momentum methods
- The locally nearly-concave regime is proven to be absorbing, ensuring APG stays in this beneficial region once entered

## Why This Works (Mechanism)

### Mechanism 1: Local Near-Convexity in Policy Space
- **Claim:** APG achieves accelerated convergence by exploiting a locally nearly-concave (or "near-convex") region around optimal policies, which emerges naturally under softmax parameterization.
- **Mechanism:** The policy objective landscape transitions from globally non-concave to locally near-concave when the policy concentrates probability mass on the optimal action. In this regime, Nesterov momentum accelerates convergence similarly to convex problems.
- **Core assumption:** The optimal policy under softmax parameterization lies at infinity, but policies near-optimal (with high probability on optimal action) create a locally convex-like structure.
- **Evidence anchors:**
  - [abstract] states APG "converges to an optimal policy at rates: (i) O(1/t²) with constant step sizes; (ii) O(e^(-ct)) with exponentially-growing step sizes" by leveraging "locally nearly-concave regime."
  - [section 5.2] defines "C-Nearly Concave" property and shows "the objective function θ → V πθ(µ) is C-nearly concave on the set Xθ = {θ′ = θ + d : θ ∈ R|S||A|, d ∈ U } for any θ satisfying... (i) V πθ(s) > Q*(s, a2(s)) for all s ∈ S."

### Mechanism 2: Absorbing Near-Convex Region
- **Claim:** Once APG enters the locally near-concave regime, it remains there indefinitely due to the absorbing nature of this region.
- **Mechanism:** The update dynamics ensure that policy parameters continue to satisfy conditions (high probability on optimal action, sufficient separation from suboptimal actions) that maintain the near-concave structure. Momentum helps prevent escape from this beneficial region.
- **Core assumption:** The feasible update domain U aligns with the gradient directions that preserve the near-concave conditions.
- **Evidence anchors:**
  - [section 5.2] Lemma 2 proves "there exists a finite time T such that for all t ≥ T, s ∈ S, and a ≠ a*(s), we have... (i) θs,a*(s) - θs,a > M, (ii) V π(t)θ(s) > Q*(s, a2(s)), (iii) ∂V πθ(µ)/∂θs,a*(s)|θ=ω(t) ≥ 0 ≥ ∂V πθ(µ)/∂θs,a|θ=ω(t), (iv) ω(t)s,a*(s) - θ(t)s,a*(s) ≥ ω(t)s,a - θ(t)s,a."
  - [section 5.2] states "the locally-concave region is absorbing in the sense that even with the effect of the momentum term, the policy parameter could stay in the nearly locally-concave region indefinitely once it enters this region."

### Mechanism 3: Momentum-Aided Escape from Suboptimality
- **Claim:** The Nesterov momentum term helps APG escape suboptimal regions faster than standard policy gradient by providing lookahead gradient estimates.
- **Mechanism:** The momentum term effectively samples a "lookahead" parameter that can have higher value than the current parameter, enabling larger improvements per iteration when the policy is still far from optimal.
- **Core assumption:** The lookahead parameter (weighted combination of current and previous parameters) can provide better gradient estimates than the current parameter alone.
- **Evidence anchors:**
  - [section 6.1] bandit experiments show "APG could escape from sub-optimality much faster than PG and heavy-ball method" with figures demonstrating superior convergence.
  - [section 6.1] states "Figure 1(c)-1(d) further show that the momentum term in APG does contribute substantially in terms of policy improvement, under both initializations."

## Foundational Learning

- **Concept: Non-uniform Polyak-Łojasiewicz (PL) condition**
  - Why needed here: The non-uniform PL condition characterizes the convergence behavior of policy gradient methods in non-concave RL objectives. Understanding this helps explain why standard PG achieves Θ(1/t) convergence.
  - Quick check question: What property of the RL objective does the non-uniform PL condition capture that enables gradient methods to converge despite non-concavity?

- **Concept: Softmax parameterization and infinite optimal parameters**
  - Why needed here: Under softmax, the optimal policy parameters approach infinity, creating unbounded domains that complicate convergence analysis. This is crucial for understanding the local near-concavity phenomenon.
  - Quick check question: Why does the optimal action under softmax parameterization correspond to parameters approaching infinity, and how does this affect convergence analysis?

- **Concept: Momentum restart mechanisms**
  - Why needed here: APG uses restart mechanisms to maintain monotonic improvement, unlike standard Nesterov acceleration. Understanding these mechanisms is essential for implementation and analysis.
  - Quick check question: What role do the restart mechanisms (checking if Vπφ(µ) ≥ Vπθ(µ)) play in APG's convergence guarantees?

## Architecture Onboarding

- **Component map:**
  Policy parameterization module (softmax) -> Gradient computation module (true gradient or estimator) -> Momentum update module (Nesterov-style with restart) -> Step size scheduler (time-varying or constant) -> Convergence monitoring module (value tracking)

- **Critical path:**
  1. Compute gradient at lookahead parameter ω(t-1)
  2. Update θ(t) = ω(t-1) + η(t)∇θVπθ(µ)|θ=ω(t-1)
  3. Compute φ(t) = θ(t) + (t-1)/(t+2)(θ(t) - θ(t-1))
  4. Check if Vπφ(µ) ≥ Vπθ(µ); if yes, set ω(t) = φ(t), else ω(t) = θ(t)
  5. Repeat until convergence

- **Design tradeoffs:**
  - Time-varying vs. constant step sizes: Time-varying (η(t) = t/(t+1) · (1-γ)³/16) provides better convergence guarantees but requires tuning; constant step sizes are simpler but may have weaker theoretical support
  - True gradient vs. estimated gradient: True gradients provide clean theory but are impractical; estimators introduce variance that may affect convergence
  - Restart mechanism frequency: More frequent restarts provide stability but may slow progress; less frequent restarts risk non-monotonic behavior

- **Failure signatures:**
  - Non-monotonic improvement (objective decreases despite iteration count increasing)
  - Slow convergence (log-log plot shows slope closer to 1 than 2)
  - Parameter explosion (θ values growing without bound)
  - Oscillation around suboptimal policies

- **First 3 experiments:**
  1. **Bandit validation:** Implement 3-armed bandit with uniform initialization and verify convergence rate of ~O(1/t²) via log-log plot slope
  2. **Momentum contribution test:** Compare APG vs. PG with heavy-ball momentum on same problem to isolate momentum benefits
  3. **Initialization sensitivity:** Test APG with both uniform and hard initializations to verify escape from suboptimal regions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the logarithmic factor in the sub-optimality gap be eliminated to achieve tighter convergence rates?
- Basis in paper: [explicit] The authors note that the logarithmic factor is a consequence of the unbounded nature of the optimal parameter in softmax parameterization and state this as a limitation of their analysis.
- Why unresolved: The paper proves the convergence rate is tight up to a logarithmic factor, but does not explore whether this gap can be closed. The unboundedness of parameters under softmax parameterization appears to be a fundamental obstacle.
- What evidence would resolve it: A proof showing either (1) the logarithmic factor is necessary under softmax parameterization, or (2) an alternative analysis technique that eliminates this factor while maintaining the same convergence guarantees.

### Open Question 2
- Question: How does APG perform with stochastic gradients estimated from sampled transitions rather than true gradients?
- Basis in paper: [inferred] The paper focuses on the exact gradient setting and mentions extending results to the stochastic gradient setting as a promising research direction, but does not provide any analysis or experiments.
- Why unresolved: The paper explicitly states this as an open direction but provides no theoretical or empirical results for the practical case where gradients must be estimated from samples.
- What evidence would resolve it: Convergence rate analysis and experimental validation showing APG's performance with stochastic gradient estimates, including how the rate degrades with respect to sample size and estimation error.

### Open Question 3
- Question: What are the fundamental differences in convergence behavior between APG with and without restart mechanisms?
- Basis in paper: [explicit] The authors discuss in Appendix G that NAPG (without restart mechanisms) can experience non-monotonic improvements and present this as a challenge for analysis, contrasting with APG's monotonic behavior.
- Why unresolved: The paper provides numerical examples showing non-monotonic behavior in NAPG but does not provide theoretical analysis of its convergence properties or characterize when restart mechanisms are essential.
- What evidence would resolve it: A theoretical characterization of the convergence rates for NAPG under various conditions, or conditions under which restart mechanisms can be safely omitted without compromising convergence guarantees.

## Limitations
- The logarithmic factor in the convergence rate is an artifact of the unbounded nature of optimal parameters under softmax parameterization
- The analysis relies on true gradient information rather than stochastic estimates, limiting practical applicability
- The absorbing nature of the near-concave region depends on specific gradient properties that may not hold under function approximation

## Confidence
- High confidence: APG achieves accelerated convergence in the bandit setting with true gradients (validated by experiments)
- Medium confidence: The two-phase analysis framework (escape to near-concavity, then accelerate) extends to general MDPs
- Low confidence: The specific constants in the convergence rates (e.g., O(1/t²) vs O(1/t)) hold under practical conditions with function approximation

## Next Checks
1. **Generalization test:** Implement APG on a continuous control problem (e.g., CartPole) with function approximation to verify the theory extends beyond tabular settings.
2. **Stochastic gradient analysis:** Add gradient noise to the bandit experiments and measure how variance affects the convergence rate and the entry into the locally near-concave regime.
3. **Momentum sensitivity:** Systematically vary the momentum parameter and restart frequency to characterize the robustness of APG's acceleration benefits across different problem structures.