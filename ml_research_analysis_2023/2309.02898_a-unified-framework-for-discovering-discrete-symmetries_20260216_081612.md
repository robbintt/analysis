---
ver: rpa2
title: A Unified Framework for Discovering Discrete Symmetries
arxiv_id: '2309.02898'
source_url: https://arxiv.org/abs/2309.02898
tags:
- function
- learning
- subgroup
- theorem
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We study the problem of learning a function respecting a symmetry
  from among a class of symmetries. Our key contribution is a unified framework that
  enables symmetry discovery across a broad range of subgroups including locally symmetric,
  dihedral and cyclic subgroups.
---

# A Unified Framework for Discovering Discrete Symmetries

## Quick Facts
- arXiv ID: 2309.02898
- Source URL: https://arxiv.org/abs/2309.02898
- Reference count: 40
- One-line primary result: Unified framework discovers discrete symmetries with 100% accuracy on image-digit sum and polynomial regression tasks

## Executive Summary
This paper introduces a unified framework for discovering discrete symmetries in functions by learning both the invariant function and the underlying symmetry subgroup simultaneously. The core innovation is an architecture that combines linear transformations, tensor-valued functions, and non-linear invariant networks to express functions invariant to locally symmetric, dihedral, and cyclic subgroups. The method uses a hybrid optimization approach combining multi-armed bandit algorithms for discrete subgroup discovery with gradient descent for learning the invariant function.

## Method Summary
The proposed method learns functions invariant to unknown subgroups of the symmetric group Sn by constructing a unified architecture: a linear transformation M1, followed by a fixed tensor-valued function ρ that outputs all pairwise monomials, another linear transformation M2, and finally a non-linear invariant function ϕ. Training uses a hybrid approach where multi-armed bandit algorithms (LinTS) efficiently explore the discrete space of subgroup symmetries by sampling (M1, M2) pairs, while stochastic gradient descent optimizes the continuous parameters of ϕ. This approach subsumes previous linear-only methods and enables discovery of more complex symmetries.

## Key Results
- Achieves 100% accuracy in discovering the correct underlying symmetry subgroup across image-digit sum and polynomial regression tasks
- Outperforms baseline methods in mean absolute error on polynomial regression tasks
- Successfully handles locally symmetric, dihedral, and cyclic subgroups through the unified architecture

## Why This Works (Mechanism)

### Mechanism 1
The architecture expresses any function invariant to locally symmetric, dihedral, or cyclic subgroups using a unified form of ϕ∘M2∘ρ∘M1. By constructing ρ to output all pairwise monomials (xi, xj) for i≠j, the method captures the orbit structure of each subgroup. The linear matrices M1 and M2 then filter and permute these monomials so that the final ϕ, an Sn(n-1)-invariant network, can aggregate the correct information.

### Mechanism 2
Bandit sampling over (M1, M2) pairs efficiently searches the discrete space of subgroup symmetries without getting stuck in local minima. Each (M1, M2) pair is encoded as a binary feature vector. LinTS samples a reward model μ from a Gaussian posterior and selects the arm maximizing the expected reward. This global search is combined with SGD over ϕ for continuous optimization.

### Mechanism 3
The unified architecture subsumes the results of prior work that used only linear transformations, enabling invariance to subgroups where the linear-only approach fails. By introducing the tensor-valued function ρ, the architecture gains additional degrees of freedom that allow representation of Zk-invariant functions even when k≥3, which linear-only methods cannot achieve.

## Foundational Learning

- **Group actions and invariance**: Understanding how subgroups of Sn act on Rn is essential to see why the architecture structure works. Quick check: What does it mean for a function f to be G-invariant, and how does that relate to the orbit of a point under G?

- **Canonical forms for invariant functions**: The decomposition ϕ∘M2∘ρ∘M1 is grounded in known canonical forms for Sn-invariant and Sk-invariant functions. Quick check: How does the canonical form for Sk-invariant functions extend to Sn-invariant functions when using the tensor-valued embedding?

- **Multi-armed bandit algorithms (LinTS)**: The outer loop that discovers the subgroup relies on LinTS to efficiently explore the discrete space of (M1, M2) pairs. Quick check: How does Thompson Sampling balance exploration and exploitation in a finite-arm setting, and why is this beneficial here?

## Architecture Onboarding

- **Component map**: ρ (tensor-valued function) -> M1 (trainable selection matrix) -> ρ (outputs pairwise monomials) -> M2 (trainable permutation matrix) -> ϕ (trainable Sn(n-1)-invariant network)

- **Critical path**: 1) Encode (M1, M2) as binary feature vector. 2) Sample μ from posterior in LinTS. 3) Choose arm maximizing μᵀa. 4) Train ϕ with SGD for fixed (M1, M2). 5) Update LinTS posterior with observed reward. 6) Repeat until convergence.

- **Design tradeoffs**: Using ρ increases expressivity but adds fixed computation of n(n-1) outputs. LinTS avoids local minima in discrete space but assumes approximate linearity of reward. Fixed ρ means the architecture is only valid for subgroups of Sn; cannot handle non-permutation symmetries.

- **Failure signatures**: Poor subgroup discovery accuracy → reward model is non-linear or LinTS exploration insufficient. High MAE even with correct subgroup → ϕ underfits or SGD hyperparameters need tuning. Training instability → learning rates for SGD and LinTS exploration variance ν not balanced.

- **First 3 experiments**: 1) Polynomial regression with known ZI subgroup: verify MAE drops when correct (M1, M2) is found. 2) Image-digit sum with known SI subgroup: check that the discovered (M1, M2) correctly selects image indices. 3) Synthetic ZI-invariant function with k≥3: confirm that the architecture succeeds where linear-only fails.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed framework perform when applied to continuous symmetry groups rather than discrete subgroups?
- Basis in paper: The paper mentions plans to explore similar approaches for continuous groups in future work, suggesting current limitations to discrete symmetries.
- Why unresolved: The paper focuses on discrete subgroups of Sn and does not provide empirical results for continuous symmetry groups.
- What evidence would resolve it: Experimental results applying the framework to continuous symmetry groups like SO(3) or SE(3) in relevant domains such as molecular dynamics or 3D object recognition.

### Open Question 2
- Question: What is the computational complexity of the multi-armed bandit optimization for M1 and M2 matrices as n increases?
- Basis in paper: The paper states the arm set is exponentially large in n but claims finding the maximizing arm is constant-time.
- Why unresolved: The paper does not provide detailed complexity analysis or scaling behavior with respect to n for the bandit optimization.
- What evidence would resolve it: Empirical scaling studies showing runtime as a function of n, and theoretical analysis of the constant-time claim's dependence on n.

### Open Question 3
- Question: How sensitive is the framework's performance to the choice of reward function in the bandit optimization?
- Basis in paper: The framework uses negative loss as reward, but the paper doesn't explore alternative reward formulations or their impact.
- Why unresolved: The paper only considers one reward function formulation without comparative analysis of alternatives.
- What evidence would resolve it: Systematic experiments comparing performance under different reward functions (e.g., accuracy-based vs loss-based) and analysis of their effects on convergence and symmetry discovery.

## Limitations
- The architecture requires computing n(n-1) monomials from ρ, which becomes computationally prohibitive for large n
- The framework is limited to permutation subgroups of Sn and cannot handle non-permutation symmetries
- The assumption that any G-invariant function can be written as ϕ∘ρ for an injective tensor-valued ρ is not proven for all subgroups

## Confidence

**High confidence**: Experimental results showing 100% accuracy in discovering correct subgroups across image-digit sum and polynomial regression tasks. Methodology is clearly described with standard metrics.

**Medium confidence**: Theoretical justification for why the architecture can express all G-invariant functions. Theorem 4 provides construction for specific subgroups, but general case is not proven.

**Low confidence**: Scalability claims. Architecture requires computing n(n-1) monomials from ρ, becoming prohibitive for large n. No computational complexity analysis or large-scale benchmarks provided.

## Next Checks

1. Verify that ρ is indeed injective for the target subgroups by testing whether distinct G-invariant functions produce distinct embeddings. This can be done with synthetic data where the true function is known.

2. Test the method on a problem where the target subgroup is not in the assumed class (e.g., rotational symmetry not induced by Sn). This will reveal whether the architecture is truly limited to permutation subgroups.

3. Conduct ablation studies on the LinTS hyperparameters (ν, exploration frequency) to determine their impact on discovery accuracy. This will show whether the method is robust to hyperparameter choices or if it requires careful tuning.