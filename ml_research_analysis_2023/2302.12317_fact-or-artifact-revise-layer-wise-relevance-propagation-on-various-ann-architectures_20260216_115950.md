---
ver: rpa2
title: Fact or Artifact? Revise Layer-wise Relevance Propagation on various ANN Architectures
arxiv_id: '2302.12317'
source_url: https://arxiv.org/abs/2302.12317
tags:
- relevance
- samples
- input
- time
- maps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of interpreting neural network
  models for image classification, particularly for geospatial data like sea surface
  temperature anomalies. The authors focus on layer-wise relevance propagation (LRP),
  a technique used to explain the decision-making process of artificial neural networks
  by attributing relevance scores to each input pixel.
---

# Fact or Artifact? Revise Layer-wise Relevance Propagation on various ANN Architectures

## Quick Facts
- arXiv ID: 2302.12317
- Source URL: https://arxiv.org/abs/2302.12317
- Reference count: 31
- Primary result: ESNs achieve higher explainability than MLPs/CNNs despite lower accuracy when LRP artifacts are eliminated via input slicing.

## Executive Summary
This paper investigates the interpretability of neural network models for geospatial image classification, focusing on layer-wise relevance propagation (LRP) across multilayer perceptrons (MLPs), convolutional neural networks (CNNs), and echo state networks (ESNs). The authors demonstrate that LRP can attribute relevance scores to input pixels, but the quality of these maps depends on model architecture and training choices. Regularization techniques improve focus in MLPs and CNNs, while ESN leak rate controls memory and relevance distribution. A novel slicing technique eliminates artifacts in ESN relevance maps, enhancing interpretability. The study highlights tradeoffs between model performance and explainability, with ESNs offering superior interpretability despite lower accuracy.

## Method Summary
The authors apply LRP to interpret MLP, CNN, and ESN models trained on synthetic and real-world SST anomaly data. LRP propagates relevance backward from output to input using architecture-specific rules. MLPs and CNNs are regularized with L1 penalties to improve focus, while ESNs use leak rate α to control reservoir memory. A slicing technique splits ESN inputs into equal-sized pieces to eliminate artifacts. Models are evaluated on classification accuracy, MSE, and mean relevance maps.

## Key Results
- L1 regularization drives MLPs and CNNs to focus on narrow, relevant features, improving interpretability at the cost of some accuracy.
- ESN leak rate α controls memory length; lower α yields longer memory and more equal relevance distribution, while higher α causes faster decay and focus on recent inputs.
- Splitting ESN inputs into equal-sized pieces eliminates gateway artifacts, producing cleaner relevance maps without sacrificing accuracy.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LRP propagates relevance backward from model output to input pixels using layer-specific rules.
- Mechanism: For MLPs and CNNs, relevance at layer l is computed by redistributing relevance from layer l+1 based on positive pre-activations and weights. For ESNs, reservoir dynamics are unfolded in time, treating each time step as a layer, with relevance decay controlled by the leak rate α.
- Core assumption: The network’s final output can serve as a proxy for relevance to be traced back through layers, and the decomposition preserves total relevance.
- Evidence anchors:
  - [abstract] "Relevance can be traced back through the network to attribute a certain score to each input pixel."
  - [section] "Relevance is distributed back from the output layer through lower layers until we reach the input layer."
  - [corpus] Weak support; no direct mention of ESN-specific unfolding in neighbors.
- Break condition: If the assumption of total relevance preservation fails (e.g., due to numerical instability or incompatible layer operations), the relevance scores may be meaningless.

### Mechanism 2
- Claim: Weight regularization (L1) forces MLP and CNN models to focus on sparse, relevant features, improving interpretability.
- Mechanism: By penalizing non-zero weights, L1 regularization drives small weights to zero, causing the model to rely on fewer, more discriminative input regions. This produces sharper relevance maps with focus on key features.
- Core assumption: Sparsity in weights correlates with importance of corresponding input regions for classification.
- Evidence anchors:
  - [abstract] "We can force MLP and CNN models to focus only on a very narrow spot... by driving small weights to zero."
  - [section] "By adding regularization terms, we successfully force both models to focus on the left square."
  - [corpus] No direct mention of L1 regularization in neighbors.
- Break condition: If regularization is too strong, the model may lose classification ability or focus on spurious features.

### Mechanism 3
- Claim: ESN leak rate α controls reservoir memory and determines how relevance is distributed across time steps.
- Mechanism: Higher α causes faster reservoir response and shorter memory; lower α yields longer memory and more equal distribution of relevance across time steps. The decay of relevance over time follows exp(-α(T-t)).
- Core assumption: Reservoir state dynamics approximate an exponential decay of relevance backward in time, and memory length is appropriate for the task.
- Evidence anchors:
  - [abstract] "For ESNs, we find the leak rate to be the crucial parameter and give a heuristic to choose it appropriately."
  - [section] "For larger leak rates, the reservoir states react faster to new inputs... At some point the reservoir loses its memory of the left half."
  - [corpus] No direct mention of leak rate in neighbors.
- Break condition: If α is mismatched to sequence length, relevance becomes concentrated on initial or final time steps, producing misleading maps.

## Foundational Learning

- Concept: Layer-wise Relevance Propagation (LRP) rules
  - Why needed here: LRP is the core technique for attributing pixel-level relevance; without understanding the rules, one cannot modify or debug the method.
  - Quick check question: How does LRP redistribute relevance from layer l+1 to layer l in a dense layer?

- Concept: Regularization (L1)
  - Why needed here: Regularization is used to control model focus and interpretability; engineers must know how to tune it.
  - Quick check question: What effect does increasing the L1 regularization coefficient have on weight sparsity and model performance?

- Concept: Reservoir dynamics in ESNs
  - Why needed here: ESNs handle sequences by unfolding reservoir states over time; leak rate determines memory length, which affects relevance propagation.
  - Quick check question: How does the leak rate α influence the balance between memory and responsiveness in an ESN?

## Architecture Onboarding

- Component map:
  Data preprocessing (handling missing values, reshaping for MLP/CNN/ESN) -> Model definition (MLP, CNN, ESN with leak rate α) -> Training (backpropagation for MLP/CNN, output weight regression for ESN) -> LRP implementation (custom rules per architecture) -> Evaluation (MSE, accuracy, mean relevance maps)

- Critical path:
  1. Preprocess SST anomaly data to match model input requirements.
  2. Train MLP/CNN with/without L1 regularization or ESN with chosen α.
  3. Apply LRP to obtain pixel-level relevance maps.
  4. Analyze relevance maps for artifacts (stripes, gateways, focus).

- Design tradeoffs:
  - MLP/CNN: High performance but lower explainability unless regularized; CNN may introduce patch artifacts.
  - ESN: Lower parameter count and higher explainability but potentially higher MSE; artifacts depend on input slicing strategy.
  - Regularization: Improves interpretability but may degrade accuracy if too strong.

- Failure signatures:
  - Blurry relevance maps: insufficient regularization or inappropriate architecture.
  - Striped or gateway artifacts: improper handling of missing data or slicing strategy in ESNs.
  - Over-focus: excessive regularization causing loss of useful features.

- First 3 experiments:
  1. Train an MLP on synthetic data with no regularization; apply LRP; observe relevance spread across both squares.
  2. Train an ESN on synthetic data with α=0.005; feed column-wise; check for gateway artifacts.
  3. Train an MLP with L1=0.01; compare relevance maps to unregularized model; verify focus on left square.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed technique for eliminating artifacts in ESN relevance maps perform when applied to more complex geospatial datasets beyond ENSO?
- Basis in paper: [explicit] The authors propose splitting input samples into equal-sized pieces to eliminate artifacts in ESN relevance maps and test it on ENSO data.
- Why unresolved: The technique is only validated on ENSO data, and its generalizability to other complex geospatial patterns is not explored.
- What evidence would resolve it: Testing the technique on diverse geospatial datasets with varying complexity and structure would determine its robustness and applicability.

### Open Question 2
- Question: What is the impact of different kernel sizes and strides in CNNs on the interpretability of relevance maps?
- Basis in paper: [inferred] The paper mentions using CNNs with quadratic kernels but does not explore how varying kernel sizes or strides affect the resulting relevance maps.
- Why unresolved: The study does not investigate the relationship between kernel configurations and the interpretability of the resulting relevance maps.
- What evidence would resolve it: Systematic experiments varying kernel sizes and strides while analyzing the resulting relevance maps would clarify their impact on interpretability.

### Open Question 3
- Question: How does the performance of ESN models with leaky reservoirs compare to other RNN architectures, such as LSTMs, in terms of both accuracy and explainability?
- Basis in paper: [explicit] The authors note that ESNs have a lower number of trainable parameters and offer higher explainability compared to MLPs and CNNs, but do not compare them to LSTMs.
- Why unresolved: The study does not include a direct comparison with other RNN architectures like LSTMs, which are commonly used for time series tasks.
- What evidence would resolve it: Comparative experiments between ESNs and LSTMs on the same tasks, evaluating both accuracy and explainability metrics, would provide insights into their relative strengths.

## Limitations
- LRP assumptions may fail due to numerical instability or incompatible layer operations, leading to meaningless relevance scores.
- Regularization strength and leak rate are presented as heuristics; optimal values may vary with dataset characteristics.
- The study focuses on synthetic and SST anomaly data; generalization to other domains requires validation.

## Confidence
- High Confidence: The mechanism of LRP for MLPs and CNNs is well-established, and the role of L1 regularization in driving weight sparsity is well-documented in the literature.
- Medium Confidence: The ESN-specific LRP unfolding and leak rate effects are supported by the paper's experiments, but the exact dynamics may depend on reservoir size and task complexity.
- Low Confidence: The proposed slicing technique for ESNs to eliminate artifacts is intuitive but lacks theoretical justification or ablation studies.

## Next Checks
1. **Numerical Stability Test**: Verify that total relevance is preserved across all layers for MLP and CNN models under varying input scales and activation functions.
2. **Regularization Sweep**: Systematically vary L1 regularization coefficients for MLPs and CNNs to quantify the tradeoff between classification accuracy and relevance map focus.
3. **ESN Memory Experiment**: Train ESNs with different leak rates on sequences of varying lengths to confirm the relationship between α and memory decay, and assess impact on relevance distribution.