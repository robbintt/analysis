---
ver: rpa2
title: A Scalable Technique for Weak-Supervised Learning with Domain Constraints
arxiv_id: '2301.05253'
source_url: https://arxiv.org/abs/2301.05253
tags: []
core_contribution: This paper proposes a scalable technique for weak-supervised learning
  using symbolic domain constraints. The authors focus on the MNIST image classification
  problem with a variant where training examples consist of image sequences and their
  sum.
---

# A Scalable Technique for Weak-Supervised Learning with Domain Constraints

## Quick Facts
- arXiv ID: 2301.05253
- Source URL: https://arxiv.org/abs/2301.05253
- Reference count: 14
- Primary result: Scalable weak-supervised learning approach achieving 92-97% accuracy for MNIST image classification with addition constraints, scaling to widths up to 10 and heights up to 6

## Executive Summary
This paper presents a scalable technique for weak-supervised learning that leverages symbolic domain constraints to overcome the grounding bottleneck. The approach uses autoencoder-based clustering, mathematical optimization to infer cluster labels, and rule-based inference to improve image labeling. By reformulating constraints as linear equations across multiple training examples, the method achieves high classification accuracy while maintaining training time independent of problem size. The technique significantly outperforms previous approaches that rely on computing all constraint-satisfying combinations for each training example.

## Method Summary
The approach consists of a four-step pipeline: (1) Autoencoder-based clustering using a symmetric autoencoder with dense layers [500, 500, 2000, 10], trained for 300 epochs, followed by k-means clustering with 10 clusters; (2) Cluster label prediction using integer linear programming via CVXPY to solve a system of linear equations derived from the sum constraint; (3) Image label inference using rule-based refinement based on the fact that a variable can be resolved if all other variables in the equation are known; (4) CNN-based classifier training with architecture: conv layer (32 filters, 3×3 kernel) → max pooling → conv layer (64 filters, 3×3) → max pooling → conv layer (64 filters, 3×3) → max pooling → dense layer (100 nodes) → output layer. Trained with SGD (learning rate 0.01, momentum 0.9) for 10 epochs.

## Key Results
- Classification accuracy between 92% and 97% for widths up to 10 and heights up to 6
- Training time independent of width and height
- Significantly better scalability than previous approaches that compute all constraint-satisfying combinations
- Clustering purity of approximately 91.3% achieved through autoencoder-based clustering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The approach avoids grounding bottleneck by reformulating constraints as linear equations across multiple training examples.
- Mechanism: Images are first clustered into 10 groups, each cluster assigned a variable. Multiple examples are grouped into batches, and their linear constraints (sum of digit values) are expressed as integer linear programs solved jointly across the batch.
- Core assumption: The addition constraint is linear and can be efficiently solved when reformulated as a system of equations with cluster variables instead of individual images.
- Evidence anchors:
  - [abstract] "our approach scales significantly better than previous approaches that rely on computing all constraint satisfying combinations for each training example"
  - [section] "We split the training examples into batches of size 100, solve the optimization problem for each batch using L1-norm as the objective function"
- Break condition: The linear reformulation fails if the constraint is non-linear or if the batch size is too small to find consistent cluster labels.

### Mechanism 2
- Claim: Cluster-based variable reduction enables scalability from exponential to polynomial complexity.
- Mechanism: By clustering 60K images into 10 clusters, the number of variables drops from 60K to 10, allowing linear programming to scale independently of width/height.
- Core assumption: Clustering purity is sufficiently high (~91.3%) that the optimization can recover correct cluster labels.
- Evidence anchors:
  - [section] "Our clustering component (Step 1), has about 91.3% purity"
  - [section] "Our approach scales in both width and height as our model achieves between 92% and 97% accuracy for w ≤ 10, h ≤ 6"
- Break condition: If clustering purity falls below a threshold, the optimization cannot reliably infer cluster labels, breaking the pipeline.

### Mechanism 3
- Claim: Rule-based inference improves image labeling beyond initial clustering accuracy.
- Mechanism: Iteratively resolves image labels by detecting examples with only one unresolved image and solving for it using known labels.
- Core assumption: Images closer to cluster centroids are more likely correctly clustered, enabling safe initial inference.
- Evidence anchors:
  - [section] "we assume the images closer to their respective cluster's centroid as correctly labeled as they have a higher chance of being correctly clustered"
  - [section] "The reason for higher classification accuracy for smaller w, h combinations is that in these cases the image label inference algorithm (Step 3) has a higher chance of inferring the correct image label"
- Break condition: If initial clustering purity is too low, the inference cannot bootstrap reliably.

## Foundational Learning

- Concept: Clustering-friendly representation learning
  - Why needed here: Images must be grouped into coherent classes before symbolic constraints can be applied
  - Quick check question: What representation learning technique is used to enable clustering in this pipeline?

- Concept: Linear programming for constraint satisfaction
  - Why needed here: Efficiently solves addition constraints across multiple examples without exhaustive search
  - Quick check question: What optimization objective is used when solving the integer linear program for cluster labels?

- Concept: Rule-based inference for label refinement
  - Why needed here: Improves initial clustering-based labels using constraint structure
  - Quick check question: What is the key rule used in the image label inference step?

## Architecture Onboarding

- Component map: Autoencoder → Clustering → Optimization → Inference → Classification
- Critical path: Autoencoder → Clustering → Optimization → Inference → Classification
- Design tradeoffs:
  - Clustering purity vs. inference accuracy
  - Batch size vs. optimization reliability
  - Expressiveness of constraints vs. scalability
- Failure signatures:
  - Low clustering purity → poor initial labels
  - Small batch size → optimization fails to find consistent solution
  - Weak inference rules → limited label improvement
- First 3 experiments:
  1. Verify autoencoder clustering purity on MNIST test set
  2. Test optimization on synthetic linear constraints with known ground truth
  3. Validate rule-based inference on small examples where manual verification is possible

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the clustering purity be improved beyond the current 91.3% achieved through autoencoder-based clustering?
- Basis in paper: [explicit] The authors mention that their classification accuracy depends strongly on clustering purity and that improving the representation learning step is planned future work.
- Why unresolved: The current autoencoder-based clustering achieves 91.3% purity, which limits the maximum achievable classification accuracy to around 92-93%. There may be better clustering approaches or architectural improvements that could yield higher purity.
- What evidence would resolve it: Experimental results comparing different clustering approaches (e.g., deep clustering, contrastive learning) or autoencoder architectures, demonstrating improved purity and corresponding classification accuracy gains.

### Open Question 2
- Question: What is the optimal batch size for the mathematical optimization step when dealing with larger problem sizes like (w=12, h=12)?
- Basis in paper: [explicit] The authors note that the current fixed batch size of 100 may not be optimal for larger combinations and plan to dynamically compute batches and their sizes.
- Why unresolved: The paper only evaluates up to w=10, h=6, and the authors acknowledge that larger problem sizes may require different batch sizing strategies for the optimization to find correct results.
- What evidence would resolve it: Empirical studies showing how different batch sizes affect optimization performance and accuracy across various problem sizes, leading to a principled method for dynamic batch sizing.

### Open Question 3
- Question: How can the image label inference step be enhanced with additional inference rules beyond the single rule currently used?
- Basis in paper: [explicit] The authors state that adding more inference rules or heuristics to the image label inference step would help achieve higher image label accuracy for more cases.
- Why unresolved: The current inference step relies on only one simple rule (resolving images when all other images in an example are known), which limits its ability to infer correct labels in more complex scenarios.
- What evidence would resolve it: Development and evaluation of additional inference rules (e.g., probabilistic reasoning, constraint propagation techniques) showing measurable improvements in label inference accuracy and overall classification performance.

## Limitations
- The approach assumes linear constraints can be reformulated as integer linear programs, which may not generalize to more complex domains
- Clustering purity (~91.3%) is not rigorously validated against ground truth and limits maximum achievable accuracy
- The method's scalability for very large problem sizes (w > 10, h > 6) has not been fully tested

## Confidence

**High Confidence**: The fundamental approach of using cluster-based variable reduction to avoid grounding bottleneck is well-established and the reported accuracies (92-97%) are consistent with the methodology.

**Medium Confidence**: The specific implementation details (autoencoder architecture, optimization parameters) are adequately specified for reproduction, though some solver configurations are unspecified.

**Low Confidence**: The generalizability of the approach to non-linear constraints or domains with different structural properties is not demonstrated.

## Next Checks

1. **Clustering Purity Validation**: Compute the actual clustering purity by comparing autoencoder+k-means cluster assignments against true digit labels on MNIST test set to verify the claimed ~91.3% accuracy.

2. **Batch Size Sensitivity**: Systematically vary batch sizes (10, 50, 100, 200) in the optimization step to determine the minimum batch size required for reliable constraint satisfaction across different w×h combinations.

3. **Constraint Structure Generalizability**: Test the approach on a modified constraint system (e.g., multiplication instead of addition) to evaluate whether the linear reformulation strategy breaks down for non-additive relationships.