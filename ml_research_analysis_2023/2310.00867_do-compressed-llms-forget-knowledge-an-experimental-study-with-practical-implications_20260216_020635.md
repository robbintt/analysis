---
ver: rpa2
title: Do Compressed LLMs Forget Knowledge? An Experimental Study with Practical Implications
arxiv_id: '2310.00867'
source_url: https://arxiv.org/abs/2310.00867
tags:
- performance
- prompt
- llama-7b
- knowledge
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores how compression techniques (quantization,
  pruning) damage LLM knowledge and how to restore it. Two hypotheses are proposed:
  1) knowledge is forgotten and must be relearned, or 2) knowledge is displaced and
  can be redirected via prompting.'
---

# Do Compressed LLMs Forget Knowledge? An Experimental Study with Practical Implications

## Quick Facts
- arXiv ID: 2310.00867
- Source URL: https://arxiv.org/abs/2310.00867
- Reference count: 5
- Key outcome: Prompting (especially IDP) recovers compressed LLM performance effectively by redirecting attention to displaced knowledge, using 21x fewer parameters and 60% less latency than LoRA.

## Executive Summary
This paper investigates whether knowledge is forgotten or displaced when LLMs are compressed using quantization and pruning. Through extensive experiments comparing prompting variants (including Inference-time Dynamic Prompting) against re-training baselines (LoRA, prefix-tuning), the study finds that prompting can effectively recover compressed model performance. The analysis reveals that prompting works by redirecting attention to tap into displaced knowledge rather than relearning from data. IDP achieves average accuracy gains of ~1.24% over compressed baselines while using significantly fewer parameters and less latency.

## Method Summary
The study compresses LLaMA-7B and OPT-6.7B models using GPTQ 3-bit quantization and SparseGPT 50% pruning, then fine-tunes with LoRA, prompt-tuning, prefix-tuning, and IDP using 40.96M training tokens and AdamW optimizer. Performance is evaluated across nine downstream tasks (ARC, SCIQ, WebQS, TriviaQA, PIQA, Hellaswag, Lambada, WinoGrande, CommonsenseQA) with average accuracy as the primary metric. The method also analyzes attention and activation patterns through cosine similarity to understand knowledge access mechanisms.

## Key Results
- IDP achieves average accuracy gains of ~1.24% over compressed baselines
- IDP uses 21x fewer parameters and 60% less latency than LoRA
- Attention pattern analysis supports the hypothesis that prompting redirects to displaced knowledge rather than relearning

## Why This Works (Mechanism)

### Mechanism 1
Compression displaces model knowledge but doesn't erase it. Prompting dynamically selects from curated prompt sets based on input context, rerouting attention to tap into this displaced knowledge. Core assumption: knowledge remains in the model but becomes inaccessible through the original inference path after compression. Evidence: cosine similarity of attention patterns shows alignment between baseline and IDP models. Break condition: if attention patterns show no similarity to baseline when using prompts.

### Mechanism 2
IDP uses mean attention from input-to-prompt to select the most relevant prompt for each instance, allowing inputs to incorporate contextual cues without manual intervention or additional parameters. Core assumption: the model can effectively use different prompts for different inputs without requiring full prompt concatenation. Evidence: IDP can effectively increase prompt diversity without incurring any inference overhead. Break condition: if performance degrades significantly when reducing available prompts or when prompts become too short.

### Mechanism 3
By using inference-time dynamic prompting instead of parameter tuning, IDP avoids adding adapter layers or modifying model weights while still achieving comparable or better performance. Core assumption: the overhead of prompt selection is minimal compared to parameter tuning costs. Evidence: IDP saves extra parameter size by 21x and reduces inference latency by 60%. Break condition: if latency measurements show significant overhead from prompt selection.

## Foundational Learning

- Concept: Attention mechanisms in transformers
  - Why needed here: Understanding how prompts redirect attention to displaced knowledge requires knowing how attention weights work
  - Quick check question: What happens to attention weights when a prompt is prepended to the input sequence?

- Concept: Model compression techniques (quantization and pruning)
  - Why needed here: The paper investigates how these compression methods affect knowledge retention and how prompting can recover performance
  - Quick check question: How does 3-bit quantization differ from 8-bit quantization in terms of precision loss?

- Concept: Parameter-efficient fine-tuning methods
  - Why needed here: The paper compares prompting against LoRA, prefix-tuning, and other PEFT methods to establish effectiveness
  - Quick check question: What's the key difference between LoRA and prefix-tuning in terms of where they modify the model?

## Architecture Onboarding

- Component map: Input → Prompt selector (IDP) → Transformer layers → Output
- Critical path: Input processing → Attention calculation → Prompt selection → KV caching → Final prediction
- Design tradeoffs: Parameter efficiency vs. performance recovery; prompt diversity vs. selection overhead; fixed prompts vs. dynamic selection
- Failure signatures: Performance degradation when prompts are too short; increased latency from prompt selection; loss of effectiveness with fewer available prompts
- First 3 experiments:
  1. Test baseline compressed model performance across all nine tasks
  2. Implement and test naive prompt concatenation vs. IDP with two prompts
  3. Compare attention patterns between baseline, prompt-tuned, and IDP models using cosine similarity

## Open Questions the Paper Calls Out

### Open Question 1
How does the displacement of knowledge during compression affect the interpretability of model predictions? While the paper provides evidence supporting the displacement hypothesis, it does not explore how this displacement affects the interpretability of model predictions. Further experiments analyzing attention and activation patterns before and after compression could provide insights into this relationship.

### Open Question 2
What are the long-term effects of using IDP on the performance of compressed LLMs? The paper focuses on immediate performance improvements but doesn't investigate how IDP might affect performance over time. Long-term studies tracking compressed LLMs using IDP could provide insights into sustainability and scalability.

### Open Question 3
How does the effectiveness of IDP vary across different types of knowledge domains? While IDP is effective across various knowledge domains, the paper doesn't explore how effectiveness might vary across different types of knowledge domains such as factual, procedural, or conceptual knowledge. Experiments comparing IDP effectiveness across different domain types could provide insights into its versatility.

## Limitations

- The claim that knowledge is displaced rather than forgotten relies on indirect evidence from attention pattern analysis rather than direct knowledge recovery testing
- IDP effectiveness appears sensitive to prompt selection quality and diversity, with modest performance gains (~1.24%) suggesting incomplete knowledge recovery
- Experimental scope limited to specific model architectures and compression levels, may not generalize to other compression techniques or more aggressive compression rates

## Confidence

**High Confidence**: IDP achieves performance recovery with significantly fewer parameters (21x) and lower latency (60%) compared to LoRA - well-supported by direct measurements.

**Medium Confidence**: Prompting redirects attention to displaced knowledge is supported by attention pattern analysis but relies on correlational evidence rather than causal proof.

**Low Confidence**: The broader theoretical claim that compression displaces rather than forgets knowledge is the most uncertain - plausible but not conclusively proven by available evidence.

## Next Checks

1. Implement ablation studies where prompts are systematically removed or modified to test whether performance degrades correlate with changes in attention pattern similarity to the baseline.

2. Design experiments that directly test whether compressed models retain specific factual knowledge that can be recovered through prompting versus whether prompts introduce new information.

3. Evaluate IDP across different compression levels (e.g., 4-bit quantization, 75% pruning) and model architectures to determine the robustness of the displacement hypothesis and prompting effectiveness.