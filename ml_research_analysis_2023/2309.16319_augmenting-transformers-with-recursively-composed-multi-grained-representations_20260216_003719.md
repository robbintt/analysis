---
ver: rpa2
title: Augmenting Transformers with Recursively Composed Multi-grained Representations
arxiv_id: '2309.16319'
source_url: https://arxiv.org/abs/2309.16319
tags:
- transformer
- representations
- span
- layers
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReCAT, a novel Transformer architecture that
  explicitly models hierarchical syntactic structures through a contextual inside-outside
  (CIO) layer. The CIO layer learns contextualized representations of spans by encoding
  input sequences in bottom-up and top-down passes, enabling cross-span communication.
---

# Augmenting Transformers with Recursively Composed Multi-grained Representations

## Quick Facts
- arXiv ID: 2309.16319
- Source URL: https://arxiv.org/abs/2309.16319
- Reference count: 17
- Primary result: ReCAT achieves 4 F1 point average improvement on span-level tasks and superior performance on GLUE natural language inference tasks

## Executive Summary
This paper introduces ReCAT, a novel Transformer architecture that explicitly models hierarchical syntactic structures through a contextual inside-outside (CIO) layer. The CIO layer learns contextualized representations of spans by encoding input sequences in bottom-up and top-down passes, enabling cross-span communication. Experiments on span-level tasks show ReCAT significantly outperforms vanilla Transformers with average 4 F1 point improvements. On GLUE benchmark, ReCAT achieves superior performance on natural language inference tasks compared to baselines. The hierarchical structures induced by ReCAT show strong consistency with human-annotated syntactic trees, demonstrating good interpretability.

## Method Summary
ReCAT augments standard Transformer architectures with contextual inside-outside (CIO) layers that model hierarchical syntactic structures. CIO layers perform bottom-up composition of span representations followed by top-down contextualization, enabling cross-span communication. Multiple CIO layers can be stacked to iteratively refine span representations. The architecture maintains linear computational complexity through a pruning mechanism that limits the number of cells encoded during the inside-outside process. ReCAT is pre-trained on WikiText103 using masked language modeling, then fine-tuned on downstream span-level tasks (OntoNotes) and sentence-level tasks (GLUE).

## Key Results
- ReCAT achieves 4 F1 point average improvement on span-level tasks compared to vanilla Transformers
- On GLUE benchmark, ReCAT shows superior performance on natural language inference tasks
- Induced hierarchical structures demonstrate strong consistency with human-annotated syntactic trees
- Pruning threshold m=2 provides good balance between performance and computational efficiency

## Why This Works (Mechanism)

### Mechanism 1: Cross-span communication through CIO layers
The contextual inside-outside (CIO) layer enables cross-span communication by allowing information flow both inside and outside spans. CIO layers perform bottom-up and top-down passes. During bottom-up, high-level span representations are composed from low-level spans via dynamic programming. During top-down, each span merges information from itself, its siblings, and its parents. This creates contextualized representations that include both intra-span and inter-span relationships. The core assumption is that information leakage between inside and outside passes is beneficial for learning contextualized multi-grained representations.

### Mechanism 2: Iterative refinement through stacked CIO layers
Stacking multiple CIO layers enables iterative refinement of span representations. Each CIO layer builds upon the contextualized representations from the previous layer, allowing for progressive refinement. The bottom-up pass composes lower-level spans into higher-level ones, while the top-down pass contextualizes each span with information from the entire hierarchy. Multiple layers enable deeper contextualization. The core assumption is that iterative refinement through multiple CIO layers improves representation quality more than a single deep layer.

### Mechanism 3: Computational complexity reduction through pruning
The CIO layer reduces computational complexity from cubic to linear while maintaining representation quality. The pruned neural inside algorithm uses a pruning threshold (m=2 by default) to limit the number of cells that need to be encoded. During the pruning pass, cells are merged when the height reaches the threshold, and only necessary cells are retained for encoding. This reduces complexity from O(n³) to O(mn) where m is the pruning threshold. The core assumption is that the pruning strategy retains sufficient information for accurate representation while dramatically reducing computation.

## Foundational Learning

- **Dynamic programming for span representation composition** - Why needed: The CIO layer relies on efficiently composing span representations from sub-spans using a bottom-up approach. Quick check: How does the CIO layer use dynamic programming to efficiently compute span representations from sub-spans?
- **Inside-outside algorithm in probabilistic context-free grammars** - Why needed: The CIO layer is inspired by the inside-outside algorithm but adapted for neural networks with contextualization. Quick check: What are the key differences between the traditional inside-outside algorithm and the contextual inside-outside approach used in CIO?
- **Transformer self-attention mechanism** - Why needed: After CIO layers produce multi-grained span representations, these are fed into Transformer layers for inter-span communication. Quick check: How do the multi-grained span representations from CIO layers interact with the self-attention mechanism in subsequent Transformer layers?

## Architecture Onboarding

- **Component map**: Input embeddings → CIO layers (stacked) → Transformer layers → Output
- **Critical path**: Token embeddings → CIO bottom-up pass → CIO top-down pass → Transformer self-attention → Task-specific output
- **Design tradeoffs**: 
  - Depth vs efficiency: More CIO layers provide better contextualization but increase computation
  - Pruning threshold: Higher thresholds preserve more structure but increase complexity
  - Shared vs non-shared composition functions: Shared parameters reduce memory but may limit expressiveness
- **Failure signatures**:
  - Poor span-level task performance: Likely issues with CIO layer composition or pruning
  - Degradation on single-sentence tasks: Possible over-contextualization or information loss
  - Slow training/inference: Pruning threshold too high or insufficient optimization
- **First 3 experiments**:
  1. Ablation study: Remove CIO layers and compare performance on span-level tasks vs vanilla Transformer
  2. Pruning threshold sensitivity: Test different m values (1, 2, 4) and measure impact on accuracy and speed
  3. Layer depth analysis: Compare ReCAT with 1, 3, and 5 CIO layers on GLUE benchmark to identify optimal depth

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of ReCAT vary with different numbers of CIO layers (l)? The paper only compares ReCAT configurations with 1 and 3 CIO layers, leaving the optimal number unexplored. Experiments varying the number of CIO layers (e.g., 1, 2, 3, 4, 5) on span-level tasks and grammar induction would identify the optimal value.

- **Open Question 2**: How does the pruning threshold m affect the performance and computational cost of ReCAT? The paper only tests m=2 and m=4, showing limited impact. Experiments varying m (e.g., 1, 2, 4, 8) on span-level tasks and grammar induction would quantify the trade-off between performance and computational cost.

- **Open Question 3**: How does the performance of ReCAT compare to other recursive neural network models that induce syntactic structures? The paper compares ReCAT to Fast-R2D2, DIORA, and Parser+Transformer, but not to other RvNN models specifically designed for structure induction like GumbelTree or StructFormer. Direct comparison to these models on grammar induction would provide better context for ReCAT's relative performance.

## Limitations

- Computational complexity claims lack empirical validation through runtime measurements across varying sequence lengths
- Cross-domain generalization untested - all experiments use English corpora without testing on other languages or specialized domains
- Interpretability claims require deeper statistical analysis of agreement between induced and human-annotated trees
- Training instability from recursive composition not addressed through experiments on gradient dynamics

## Confidence

- **High confidence**: Span-level task improvements (4 F1 point average gain) - directly measured on established benchmarks with multiple tasks
- **Medium confidence**: Sentence-level GLUE improvements - statistically significant on some tasks but gains are smaller and could reflect hyperparameter tuning differences
- **Low confidence**: Specific mechanism claims - multiple interacting mechanisms proposed but limited ablation studies to isolate individual contributions

## Next Checks

- **Check 1: Runtime complexity validation** - Implement ReCAT with varying sequence lengths (64, 128, 256, 512) and measure actual training/inference time per batch compared to vanilla Transformers. Plot time complexity curves and verify the claimed O(mn) scaling.

- **Check 2: Pruning threshold sensitivity** - Train ReCAT variants with m=1, m=2, m=4, m=8 on OntoNotes span tasks. Measure both accuracy degradation and computational speedup to identify optimal tradeoff points and test the robustness of the default m=2 choice.

- **Check 3: Cross-lingual transferability** - Fine-tune the WikiText103-pretrained ReCAT on equivalent span tasks in another language (e.g., Universal Dependencies treebanks). Compare performance drop to a vanilla Transformer baseline to assess whether the hierarchical learning transfers across languages or is English-specific.