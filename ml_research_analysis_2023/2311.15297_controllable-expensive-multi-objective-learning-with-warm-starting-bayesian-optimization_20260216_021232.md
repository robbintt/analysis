---
ver: rpa2
title: Controllable Expensive Multi-objective Learning with Warm-starting Bayesian
  Optimization
arxiv_id: '2311.15297'
source_url: https://arxiv.org/abs/2311.15297
tags:
- pareto
- evaluations
- front
- optimization
- co-psl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of efficient multi-objective
  optimization for expensive black-box functions, where traditional derivative-free
  methods are unstable and inefficient. The authors propose a two-stage approach called
  Co-PSL: first, warm-starting Bayesian optimization using DGEMO to construct quality
  Gaussian Process priors that adequately approximate the Pareto front with confidence;
  second, controllable Pareto set learning to learn a stable parametric mapping between
  preference vectors and Pareto solutions.'
---

# Controllable Expensive Multi-objective Learning with Warm-starting Bayesian Optimization

## Quick Facts
- arXiv ID: 2311.15297
- Source URL: https://arxiv.org/abs/2311.15297
- Reference count: 7
- One-line primary result: Two-stage Co-PSL method achieves better Pareto front approximation with fewer expensive evaluations than single-stage methods

## Executive Summary
This paper tackles the challenge of efficient multi-objective optimization for expensive black-box functions, where traditional derivative-free methods are unstable and inefficient. The authors propose a two-stage approach called Co-PSL: first, warm-starting Bayesian optimization using DGEMO to construct quality Gaussian Process priors that adequately approximate the Pareto front with confidence; second, controllable Pareto set learning to learn a stable parametric mapping between preference vectors and Pareto solutions. This method enables real-time trade-off control between conflicting objectives. Experiments on 6 synthesis and real-world problems demonstrate Co-PSL outperforms state-of-the-art baselines, achieving lower Log Hypervolume Difference and Mean Euclidean Distance scores.

## Method Summary
The Co-PSL method consists of two stages: (1) warm-starting with DGEMO to generate diverse Pareto-optimal solutions and construct Gaussian Process priors, and (2) controllable PSL with parameter initialization to learn a stable mapping from preference vectors to Pareto solutions. The PSL stage uses a parameter initialization scheme that blends previous parameters with random initialization to improve stability when GPs are inaccurate. The method is evaluated on 6 benchmark problems using Log Hypervolume Difference and Mean Euclidean Distance metrics.

## Key Results
- Co-PSL outperforms all baseline methods on most problems, with the only exception of RE36
- The controllable PSL stage with parameter initialization significantly improves stability and accuracy compared to existing PSL methods
- Warm-starting with DGEMO provides high-quality Gaussian Process priors that adequately approximate the Pareto front, reducing the number of expensive function evaluations needed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Warm-starting with DGEMO provides high-quality Gaussian Process priors that adequately approximate the Pareto front, reducing the number of expensive function evaluations needed.
- Mechanism: DGEMO constructs a diverse set of Pareto-optimal solutions by maximizing Hypervolume Improvement (HVI) across different regions of the estimated Pareto front. This diversity ensures the GP priors capture the true Pareto front geometry better than random initialization.
- Core assumption: The diversity of solutions obtained by DGEMO correlates with the quality of the GP priors for subsequent PSL.
- Evidence anchors:
  - [abstract] "warm-starting Bayesian optimization using DGEMO to construct quality Gaussian Process priors that adequately approximate the Pareto front with confidence"
  - [section 3.1] "We implement DGEMO...one of the most current and effective MOBO methods for obtaining a set of diverse Pareto optimal solutions"
  - [corpus] Weak - no direct evidence in related papers about DGEMO's effectiveness for GP prior construction
- Break condition: If the Pareto front is highly non-convex or disconnected, DGEMO may fail to sample representative points, leading to poor GP priors.

### Mechanism 2
- Claim: The controllable PSL stage with parameter initialization stabilizes the learning process by reducing sensitivity to GP approximation errors.
- Mechanism: The parameter initialization formula θi = β · θi-1 + (1 - β) · G(θi) blends previous parameters with random initialization, providing a good starting point when GPs are inaccurate and allowing independent learning as GPs improve.
- Core assumption: The instability in PSL-MOBO arises from direct optimization on current GPs without initialization safeguards.
- Evidence anchors:
  - [section 3.2] "rather than solely training the PSL model on the current Gaussian Process, we re-initialize its parameters θ at iteration i"
  - [section 3.2] "This supports h(r|θ) with a good starting point in early training steps when GPs have not approximated well the black-box objectives"
  - [corpus] Weak - no direct evidence in related papers about parameter initialization for PSL stability
- Break condition: If β is set too high, the model may not adapt to improved GP approximations; if too low, it may not benefit from good initializations.

### Mechanism 3
- Claim: The two-stage approach achieves better Pareto front approximation with fewer expensive evaluations than single-stage methods.
- Mechanism: Stage 1 reduces the search space uncertainty by constructing good GP priors; Stage 2 fine-tunes the Pareto set model with these improved priors, leading to more stable and accurate preference-to-solution mapping.
- Core assumption: The cost of warm-starting (additional evaluations) is offset by improved stability and accuracy in the PSL stage.
- Evidence anchors:
  - [section 5.1] "Thus, Co-PSL is trained with a total of 40 iterations and 420 expensive evaluations, with the number of evaluations divided equally among the two stages"
  - [section 5.2] "Our Co-PSL outperforms all baseline methods on most problems, with the only exception of RE36"
  - [corpus] Weak - no direct evidence in related papers about two-stage approaches for expensive MOO
- Break condition: If the warm-starting stage fails to capture the Pareto front adequately, the entire approach degrades to single-stage performance.

## Foundational Learning

- Concept: Gaussian Processes and Bayesian Optimization
  - Why needed here: Co-PSL relies on GPs as surrogate models for expensive black-box functions and uses BO for efficient sampling
  - Quick check question: What is the difference between Expected Improvement and Upper Confidence Bound acquisition functions in BO?

- Concept: Pareto Set Learning and Scalarization Functions
  - Why needed here: The PSL stage learns a mapping from preference vectors to Pareto solutions using scalarization functions like Chebyshev
  - Quick check question: How does the Chebyshev scalarization function differ from linear scalarization in terms of Pareto front approximation?

- Concept: Hypervolume Improvement and Diversity in Multi-objective Optimization
  - Why needed here: DGEMO uses HVI for selecting diverse solutions, and HVI is used to evaluate the quality of the learned Pareto front
  - Quick check question: Why is diversity important in multi-objective optimization, and how does it relate to Hypervolume Improvement?

## Architecture Onboarding

- Component map:
  - Warm-starting Stage: DGEMO optimizer + GP construction
  - PSL Stage: Hypernetwork-based Pareto set model + scalarization function + parameter initialization
  - Evaluation: Log Hypervolume Difference and Mean Euclidean Distance metrics

- Critical path:
  1. Initialize with random evaluations
  2. Warm-starting with DGEMO to construct GP priors (n1 iterations, batch b1)
  3. Initialize PSL model with parameter blending
  4. Train PSL model with T steps per iteration (K sampled preferences)
  5. Select next batch via HVI maximization
  6. Repeat steps 3-5 for n2 iterations

- Design tradeoffs:
  - Number of warm-starting vs. PSL iterations: More warm-starting improves GP quality but reduces PSL training time
  - Batch sizes (b1, b2): Larger batches reduce iterations but may miss fine details in Pareto front
  - Parameter initialization β: Higher β provides stability but may slow adaptation to improved GPs

- Failure signatures:
  - High variance in Log Hypervolume Difference across runs indicates instability
  - Increasing Mean Euclidean Distance during PSL training suggests poor parameter initialization
  - Low HVI improvement in warm-starting stage indicates GP priors not capturing Pareto front

- First 3 experiments:
  1. Compare Co-PSL with and without warm-starting on a simple 2-objective problem to isolate the effect of GP quality
  2. Test different β values (0.1, 0.2, 0.5) to find optimal trade-off between stability and adaptability
  3. Evaluate Co-PSL on a disconnected Pareto front problem to assess robustness to challenging geometries

## Open Questions the Paper Calls Out

- Question: How can the quality of Pareto solutions generated by the Pareto Set Model be effectively evaluated when they are not available in the dataset obtained after optimization?
  - Basis in paper: [explicit] The authors mention that evaluating arbitrary Pareto optimal solutions generated by the Pareto Set Model remains challenging as they are not available in the dataset obtained after the optimization processes.
  - Why unresolved: Real-world scenarios often have unknown errors between the true Pareto front and the predicted one, making these solutions relative references for decision-makers. The challenge becomes more pronounced in high-dimensional settings with a large number of optimized variables.
  - What evidence would resolve it: Developing methods to estimate the error between the true Pareto front and the predicted one in real-world scenarios, particularly in high-dimensional settings, would provide evidence to resolve this question.

- Question: What is the optimal combination of algorithms for Pareto Set Learning that ensures both diverse Pareto optimal solutions and improved Gaussian Process accuracy for optimizing the Pareto Set Model?
  - Basis in paper: [inferred] The authors suggest that the efficacy of algorithms in obtaining diverse Pareto optimal solutions does not necessarily guarantee improved GP accuracy, which is essential for optimizing the Pareto Set Model.
  - Why unresolved: The paper highlights the need to explore the optimal combination of algorithms for Pareto Set Learning but does not provide a definitive answer or method for achieving this balance.
  - What evidence would resolve it: Empirical studies comparing different algorithm combinations in terms of their ability to provide diverse solutions and improve GP accuracy for Pareto Set Model optimization would help resolve this question.

- Question: How does the parameter initialization scheme for the Pareto Set Model affect its stability and performance across different types of multi-objective optimization problems?
  - Basis in paper: [explicit] The authors introduce a parameter initialization scheme for the Pareto Set Model and demonstrate its impact on stability and accuracy through experiments, particularly in DTLZ2 and other problems.
  - Why unresolved: While the paper shows the effectiveness of the proposed initialization scheme in certain problems, it does not explore its impact across a broader range of multi-objective optimization problems with varying characteristics.
  - What evidence would resolve it: Conducting experiments on a diverse set of multi-objective optimization problems to assess the performance and stability of the Pareto Set Model under different initialization schemes would provide evidence to resolve this question.

## Limitations

- Limited experimental validation to synthetic and engineering design problems, leaving questions about performance in other domains
- Absence of sensitivity analysis for the parameter initialization scheme, particularly regarding the choice of β
- No comparison with the most recent MOBO methods like Diffusion-based approaches

## Confidence

- Mechanism 1 (DGEMO warm-starting): Medium confidence - well-justified theoretically but limited empirical validation of GP quality improvements
- Mechanism 2 (Parameter initialization): Medium confidence - novel approach with demonstrated benefits but no ablation studies on initialization variants
- Mechanism 3 (Two-stage efficiency): High confidence - clear quantitative improvements in evaluation metrics across multiple problems

## Next Checks

1. Conduct sensitivity analysis on the β parameter to determine optimal values and assess robustness to different initialization strategies
2. Extend experiments to include recent MOBO methods (e.g., diffusion models) and test on machine learning hyperparameter optimization problems
3. Perform detailed ablation studies comparing Co-PSL with and without warm-starting, and with different parameter initialization schemes