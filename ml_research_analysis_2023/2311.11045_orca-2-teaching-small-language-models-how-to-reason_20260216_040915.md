---
ver: rpa2
title: 'Orca 2: Teaching Small Language Models How to Reason'
arxiv_id: '2311.11045'
source_url: https://arxiv.org/abs/2311.11045
tags: []
core_contribution: Orca 2 demonstrates that small language models can achieve reasoning
  capabilities comparable to models 5-10 times larger by learning when to apply different
  solution strategies for different tasks. The key innovation is teaching models to
  be "cautious reasoners" through task-specific system instructions combined with
  prompt erasure during training.
---

# Orca 2: Teaching Small Language Models How to Reason

## Quick Facts
- arXiv ID: 2311.11045
- Source URL: https://arxiv.org/abs/2311.11045
- Authors: 
- Reference count: 40
- Key outcome: Small language models (7B and 13B) achieve reasoning performance comparable to models 5-10x larger through task-specific strategy learning and prompt erasure

## Executive Summary
Orca 2 demonstrates that small language models can achieve reasoning capabilities comparable to much larger models by learning when to apply different solution strategies for different tasks. The key innovation is teaching models to be "cautious reasoners" through task-specific system instructions combined with prompt erasure during training. This approach enables Orca 2 to significantly outperform models of similar size on reasoning benchmarks while matching or exceeding much larger models.

## Method Summary
Orca 2 is built on LLaMA-2 base models (7B and 13B) and trained using progressive learning with synthetic data. The training uses prompt erasure where the student model learns task-specific reasoning strategies without seeing the original system instructions that triggered teacher responses. The approach combines FLAN-v2 pretraining, ChatGPT instruction tuning, and GPT-4/Orca 2 synthetic data generation to create models that select appropriate reasoning strategies for different tasks rather than simply imitating larger models.

## Key Results
- Orca-2-13B shows 47.54% relative improvement over LLaMA-2-Chat-13B on reasoning tasks
- Achieves 28.15% relative improvement over WizardLM-13B on reasoning benchmarks
- Matches or exceeds performance of much larger models (70B+) on complex zero-shot reasoning tasks
- Demonstrates particularly strong performance on tasks requiring multiple reasoning steps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Teaching small models multiple solution strategies improves performance more than imitation alone.
- Mechanism: By exposing the model to various reasoning approaches and having it learn when to apply each strategy, the model gains flexibility rather than being constrained to a single teacher behavior.
- Core assumption: Small models have different capacity constraints than large models, so they need tailored solution strategies rather than copying large model outputs directly.
- Evidence anchors: [abstract] "Research on training small LMs has often relied on imitation learning to replicate the output of more capable models. We contend that excessive emphasis on imitation may restrict the potential of smaller models."

### Mechanism 2
- Claim: Prompt erasure during training teaches models to be "cautious reasoners" by forcing them to learn strategy selection without explicit task instructions.
- Mechanism: During training, the model only sees the task and teacher response, not the system instruction that triggered that response. This forces the model to infer the reasoning strategy from context rather than copying the instruction directly.
- Core assumption: Removing the explicit system instruction forces the model to internalize the reasoning process rather than memorizing instructions.
- Evidence anchors: [section] "At training time, the smaller model is exposed only to the task and the resultant behavior, without visibility into the original prompts that triggered such behavior."

### Mechanism 3
- Claim: Task-specific system instructions tailored to model capacity produce better training signals than generic instructions.
- Mechanism: The teacher model is given detailed, task-specific instructions designed to elicit the most effective reasoning approach for that particular task and the model's capacity level.
- Core assumption: Different tasks require different reasoning approaches, and the optimal approach varies based on model size and capabilities.
- Evidence anchors: [section] "We seek to teach small LMs to employ different solution strategies for different tasks, potentially different from the one used by the larger model."

## Foundational Learning

- Concept: Progressive learning through multiple training stages
  - Why needed here: Allows the model to first learn basic capabilities from FLAN, then build on them with ChatGPT data, and finally refine with GPT-4 and Orca 2 specific data
  - Quick check question: Why might training directly on GPT-4 data without the intermediate ChatGPT stage be problematic?

- Concept: Token packing for computational efficiency
  - Why needed here: Enables training on longer sequences by concatenating multiple examples, optimizing GPU memory usage
  - Quick check question: What is the maximum sequence length used in Orca 2 training?

- Concept: Loss computation only on teacher-generated tokens
  - Why needed here: Focuses training on the most relevant parts of the output, ignoring padding and other irrelevant tokens
  - Quick check question: Which tokens does Orca 2 compute loss on during training?

## Architecture Onboarding

- Component map: LLaMA-2 base model → FLAN pretraining → ChatGPT instruction tuning → GPT-4/Orca 2 synthetic data → Final model
- Critical path: Data generation (system instructions + prompts → teacher responses) → Training (prompt erasure + progressive stages) → Evaluation (zero-shot on 15 benchmarks)
- Design tradeoffs: Uses more capable models for data generation (costly) but enables smaller models to achieve comparable performance; prompt erasure improves reasoning but may reduce training efficiency
- Failure signatures: Poor performance on tasks requiring specific reasoning strategies; inability to follow complex instructions; hallucinations in abstractive summarization
- First 3 experiments:
  1. Test prompt erasure effectiveness by training one model with full instructions vs. one with prompt erasure on a simple reasoning task
  2. Compare different system instruction strategies (step-by-step vs. direct answer) on a subset of tasks to validate strategy selection capability
  3. Evaluate hallucination rates on abstractive summarization before and after cautious system message implementation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Orca 2 models compare to other models in few-shot learning settings?
- Basis in paper: The paper mentions that preliminary results on one task point to smaller gains (over zero-shot settings) for Orca 2 compared to LLaMA-2 models, especially when compared to the 70B base models. However, the authors state that they did not perform a comprehensive few-shot evaluation of Orca 2 and aim to study this further in the future.
- Why unresolved: No comprehensive evaluation of few-shot performance across various tasks and benchmarks
- What evidence would resolve it: A comprehensive evaluation of Orca 2's performance in few-shot learning settings across various tasks and benchmarks.

### Open Question 2
- Question: How does the hallucination rate of Orca 2 models compare to other models in different types of tasks, such as question answering or text generation?
- Basis in paper: The paper evaluates the hallucination rate of Orca 2 models on abstractive summarization benchmarks and grounding tasks. However, it does not provide a comprehensive analysis of hallucination rates across different types of tasks.
- Why unresolved: No comprehensive analysis of hallucination rates across different task types
- What evidence would resolve it: A comprehensive evaluation of Orca 2's hallucination rates on various tasks, including question answering, text generation, and other natural language understanding tasks.

### Open Question 3
- Question: How does the performance of Orca 2 models change when using different base models, such as GPT-3 or PaLM?
- Basis in paper: The paper states that Orca 2 is built upon the LLaMA 2 model family and retains many of its limitations. However, it does not explore the performance of Orca 2 when using different base models.
- Why unresolved: No exploration of Orca 2's performance using different base models
- What evidence would resolve it: A comparative study of Orca 2's performance using different base models, such as GPT-3 or PaLM, across various tasks and benchmarks.

## Limitations
- Data quality dependency: Effectiveness heavily depends on synthetic training data quality, which remains underspecified
- Limited task diversity: Strong performance on 15 English benchmarks doesn't guarantee generalization to multilingual or specialized domains
- Computational cost: Multiple training stages with expensive larger models for data generation create significant resource requirements

## Confidence
- **High Confidence**: Orca 2 significantly outperforms other models of similar size on tested benchmarks; prompt erasure technique improves reasoning performance; progressive learning through multiple stages is effective
- **Medium Confidence**: The mechanism by which prompt erasure teaches "cautious reasoning"; general applicability to tasks beyond those tested; scalability to even smaller models
- **Low Confidence**: Long-term generalization to unseen task types; performance in multilingual or specialized domain contexts; cost-effectiveness compared to alternative approaches

## Next Checks
1. **Synthetic Data Quality Audit**: Systematically evaluate synthetic training data quality by sampling 100-200 generated responses across task categories and having human raters assess reasoning quality, correctness, and strategy appropriateness.

2. **Cross-Domain Generalization Test**: Evaluate Orca 2 on diverse tasks beyond original 15 benchmarks including multilingual reasoning, domain-specific reasoning (medical, legal, technical), and novel reasoning paradigms.

3. **Ablation Study on Key Components**: Systematically test contributions of each major innovation by comparing models with/without prompt erasure, testing different instruction strategies in isolation, and evaluating impact of progressive learning stages.