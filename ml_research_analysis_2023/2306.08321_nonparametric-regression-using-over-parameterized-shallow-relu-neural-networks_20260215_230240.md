---
ver: rpa2
title: Nonparametric regression using over-parameterized shallow ReLU neural networks
arxiv_id: '2306.08321'
source_url: https://arxiv.org/abs/2306.08321
tags:
- neural
- networks
- function
- theorem
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes that over-parameterized shallow ReLU neural\
  \ networks can achieve minimax optimal rates of convergence for nonparametric regression.\
  \ The authors consider regression functions from H\xF6lder spaces with smoothness\
  \ \u03B1 < (d + 3)/2 or variation spaces corresponding to shallow neural networks."
---

# Nonparametric regression using over-parameterized shallow ReLU neural networks

## Quick Facts
- arXiv ID: 2306.08321
- Source URL: https://arxiv.org/abs/2306.08321
- Reference count: 12
- Primary result: Over-parameterized shallow ReLU networks achieve minimax optimal rates when weights are constrained

## Executive Summary
This paper establishes that over-parameterized shallow ReLU neural networks can achieve minimax optimal rates of convergence for nonparametric regression. The authors consider regression functions from Hölder spaces with smoothness α < (d + 3)/2 or variation spaces corresponding to shallow neural networks. They prove that least squares estimators based on shallow neural networks with certain norm constraints on the weights are minimax optimal if the network width is sufficiently large, deriving optimal rates of n^{-2α/(d+2α)} for Hölder classes and n^{-(d+3)/(2d+3)} for variation spaces.

## Method Summary
The authors analyze nonparametric regression using shallow ReLU neural networks with constrained weights. They consider two optimization problems: a constrained least squares estimator where weights are bounded by a norm constraint κ(θ) ≤ M, and a regularized least squares estimator with weight decay. The key insight is that by constraining the total variation norm of the weights, the Rademacher complexity of the network class can be bounded independently of the network width, enabling the use of over-parameterized networks without increasing statistical complexity. The regression function is assumed to belong to either a Hölder space with smoothness α < (d+3)/2 or a variation space corresponding to shallow neural networks.

## Key Results
- Over-parameterized shallow ReLU neural networks achieve minimax optimal rates n^{-2α/(d+2α)} for Hölder classes and n^{-(d+3)/(2d+3)} for variation spaces
- A new size-independent bound for local Rademacher complexity of shallow ReLU networks enables analysis of over-parameterized networks
- Regularized least squares estimators achieve the same optimal rates as constrained estimators when the regularization parameter is properly chosen

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Over-parameterized shallow ReLU neural networks can achieve minimax optimal convergence rates for nonparametric regression if the weights are constrained by their norm.
- Mechanism: By constraining the total variation norm of the weights, the Rademacher complexity of the network class can be bounded independently of the network width. This allows the use of over-parameterized networks without increasing statistical complexity, while the approximation error can be decreased by increasing the width.
- Core assumption: The regression function belongs to either a Hölder space with smoothness α < (d+3)/2 or a variation space corresponding to shallow neural networks.
- Evidence anchors:
  - [abstract]: "It is shown that over-parameterized neural networks can achieve minimax optimal rates of convergence (up to logarithmic factors) for learning functions from certain smooth function classes, if the weights are suitably constrained or regularized."
  - [section]: "We improve these rates to n^{-2α/(d+2α)} and n^{-(d+3)/(2d+3)}, which are minimax optimal for Hα and Fσ(1) respectively."
- Break condition: If the regression function is not in the specified function classes, or if the weight constraint is not properly chosen, the optimal rates will not be achieved.

### Mechanism 2
- Claim: The local Rademacher complexity of shallow ReLU neural networks can be bounded independently of the network width, enabling analysis of over-parameterized networks.
- Mechanism: The authors derive a new size-independent bound for the local Rademacher complexity, which shows that the complexity grows as δ^(3/(d+3)) M^(d/(d+3)) / sqrt(n), where δ is the radius and M is the weight constraint. This bound does not depend on the number of neurons, allowing the width to be increased without affecting the complexity.
- Core assumption: The function class is star-shaped and uniformly bounded.
- Evidence anchors:
  - [abstract]: "As a byproduct, we derive a new size-independent bound for the local Rademacher complexity of shallow ReLU neural networks, which may be of independent interest."
  - [section]: "One of our main technical contributions is a new bound for the local Rademacher complexity of shallow neural networks (see Definition 3.2 and Theorem 3.3): Rn(N N(N, M); δ) ≲ δ^{3/(d+3)} M^{d/(d+3)} √n p log(nM/δ)."
- Break condition: If the function class is not star-shaped or uniformly bounded, the size-independent bound may not hold.

### Mechanism 3
- Claim: Regularized least squares estimators based on over-parameterized shallow ReLU neural networks achieve the same optimal rates as constrained least squares estimators.
- Mechanism: The authors prove that the solutions of the regularized optimization problem are essentially the same as the solutions of the constrained problem. This allows the use of regularization techniques, such as weight decay, to achieve the optimal rates without explicitly constraining the weights.
- Core assumption: The regularization parameter λ is chosen properly based on the sample size and the desired accuracy.
- Evidence anchors:
  - [abstract]: "This work demonstrates that over-parameterization does not prevent neural networks from achieving optimal statistical performance in nonparametric regression, provided appropriate regularization is applied to the weights."
  - [section]: "Our second main result shows that the solutions of these two optimization problems also achieve the minimax optimal rates of learning functions in Hα with α < (d + 3)/2 and Fσ(1), if λ is chosen properly."
- Break condition: If the regularization parameter λ is not chosen appropriately, the optimal rates may not be achieved.

## Foundational Learning

- Concept: Hölder spaces and their smoothness properties
  - Why needed here: The paper analyzes the performance of neural networks for estimating functions from Hölder spaces, which are smoothness spaces characterized by their Hölder exponents.
  - Quick check question: What is the definition of a Hölder space and how does the smoothness exponent α relate to the regularity of functions in this space?

- Concept: Variation spaces and their connection to neural networks
  - Why needed here: The paper also considers functions from a variation space corresponding to shallow neural networks, which can be viewed as an infinitely wide network with a weight constraint.
  - Quick check question: How is the variation norm of a function related to the weights of a shallow neural network, and what is the significance of this relationship?

- Concept: Rademacher complexity and its role in statistical learning theory
  - Why needed here: The paper uses the Rademacher complexity to bound the sample complexity of the neural network function class, which is a key component in establishing the optimal rates of convergence.
  - Quick check question: What is the definition of Rademacher complexity and how does it relate to the generalization ability of a function class?

## Architecture Onboarding

- Component map:
  - Shallow ReLU neural network with constrained weights
  - Regularized least squares estimator
  - Hölder and variation function spaces
  - Local Rademacher complexity bounds

- Critical path:
  1. Choose appropriate weight constraint based on the function class and desired accuracy.
  2. Train the over-parameterized shallow ReLU neural network using the regularized least squares objective.
  3. Use the size-independent bound on local Rademacher complexity to analyze the statistical performance.
  4. Establish the optimal rates of convergence for the estimator.

- Design tradeoffs:
  - Increasing the network width can improve the approximation error but may increase the computational cost.
  - Choosing a tighter weight constraint can improve the statistical performance but may limit the approximation power.
  - The regularization parameter λ needs to be carefully tuned to balance the approximation and statistical errors.

- Failure signatures:
  - If the regression function is not in the specified function classes, the optimal rates will not be achieved.
  - If the weight constraint or regularization parameter is not chosen properly, the estimator may not achieve the optimal rates.
  - If the network width is too small, the approximation error may dominate and prevent the estimator from achieving the optimal rates.

- First 3 experiments:
  1. Implement a shallow ReLU neural network with a constrained weight norm and train it on a synthetic regression dataset from a Hölder space.
  2. Compare the performance of the constrained and regularized least squares estimators on a regression task with a known variation space function.
  3. Analyze the effect of increasing the network width on the approximation and statistical errors for a fixed weight constraint.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the precise conditions under which minimal norm interpolations achieve minimax optimal rates in nonparametric regression using shallow ReLU networks?
- Basis in paper: [explicit] The paper discusses that most works on benign overfitting study minimal norm interpolations, which corresponds to solutions of argmin κ(f), s.t. f(Xi) = Yi. It notes this is the limiting estimator of (3.2) with λn = 0, and mentions it would be interesting to compare its rate with regularized solutions.
- Why unresolved: The paper does not provide theoretical analysis or rate bounds for this specific estimator, only conjectures about its potential performance relative to regularized solutions.
- What evidence would resolve it: A rigorous proof showing whether this interpolation estimator achieves the same minimax rates (n^{-2α/(d+2α)} for Hölder classes or n^{-(d+3)/(2d+3)} for variation spaces) as the regularized least squares solutions.

### Open Question 2
- Question: Are the norms κ(f) and γ(f) equivalent for functions in the discrete measure class Fσ,disc?
- Basis in paper: [explicit] The paper states "By definition, γ(f) ≤ κ(f) for all f ∈ Fσ,disc. But we do not know whether these two norms are the same for f ∈ Fσ,disc."
- Why unresolved: The paper acknowledges this open question but does not provide a proof of either equivalence or strict inequality.
- What evidence would resolve it: A mathematical proof showing either γ(f) = κ(f) for all f ∈ Fσ,disc, or a counterexample demonstrating strict inequality for some function in this class.

### Open Question 3
- Question: Can the techniques used in this paper for shallow ReLU networks be extended to analyze over-parameterized deep neural networks for nonparametric regression?
- Basis in paper: [inferred] The paper focuses on shallow networks and references several works on deep networks, but its techniques (particularly the size-independent bound for local Rademacher complexity) are specific to the shallow case.
- Why unresolved: Deep networks have more complex function representations and their Rademacher complexity bounds are typically size-dependent, making direct extension of the techniques non-trivial.
- What evidence would resolve it: A proof showing that over-parameterized deep ReLU networks with appropriate weight regularization can achieve minimax optimal rates for nonparametric regression, using techniques analogous to those developed for shallow networks in this paper.

## Limitations

- The analysis is restricted to specific function classes (Hölder spaces with α < (d+3)/2 and variation spaces)
- Optimal rates depend critically on proper weight constraints or regularization being applied
- Empirical validation on real-world datasets is limited

## Confidence

- High confidence: The mechanism by which constrained weights enable size-independent complexity bounds
- Medium confidence: The extension from constrained to regularized estimators achieving the same rates
- Medium confidence: The approximation-theoretic bounds for shallow networks

## Next Checks

1. **Empirical Validation**: Test the proposed estimators on real-world regression datasets to verify whether the theoretical minimax rates are achieved in practice, and examine how sensitive performance is to the choice of weight constraints.

2. **Generalization Analysis**: Investigate whether the size-independent bound on local Rademacher complexity extends to other activation functions beyond ReLU, or to deeper network architectures.

3. **Constraint Robustness**: Systematically evaluate how variations in the weight constraint (both under-constraining and over-constraining) affect the convergence rates, and identify practical guidelines for choosing M based on problem characteristics.