---
ver: rpa2
title: 'SplineCam: Exact Visualization and Characterization of Deep Network Geometry
  and Decision Boundaries'
arxiv_id: '2302.12828'
source_url: https://arxiv.org/abs/2302.12828
tags:
- input
- partition
- splinecam
- boundary
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SplineCam, a provably exact method for computing
  the geometry and decision boundaries of deep networks with continuous piecewise
  linear (CPWL) activations. By leveraging CPWL spline theory, SplineCam exactly computes
  a network's partition of input space into linear regions without approximations
  like sampling.
---

# SplineCam: Exact Visualization and Characterization of Deep Network Geometry and Decision Boundaries

## Quick Facts
- arXiv ID: 2302.12828
- Source URL: https://arxiv.org/abs/2302.12828
- Reference count: 40
- Primary result: SplineCam provides exact, sampling-free computation of deep network geometry and decision boundaries using CPWL spline theory

## Executive Summary
SplineCam is a provably exact method for visualizing and characterizing the geometry and decision boundaries of deep networks with continuous piecewise linear (CPWL) activations. By leveraging CPWL spline theory, SplineCam computes the exact partition of a network's input space into linear regions without approximations like sampling or architecture simplification. The method scales to large networks, handles convolutional layers and skip connections, and enables exact sampling from decision boundaries. Experiments demonstrate SplineCam's ability to characterize network geometry, compare architectures, measure generalization, and sample decision boundaries.

## Method Summary
SplineCam computes the exact partition of a deep network's input space by leveraging CPWL spline theory. Each layer is represented as a Max-Affine Spline Operator (MASO), and the overall network is decomposed into affine regions layer-by-layer. The method projects hyperplanes from deeper layers back to the input space and intersects them with existing regions to build the partition. SplineCam uses vectorized linear algebra operations for most computations and distributes region processing across CPU threads for the graph search part. After computing the partition, the decision boundary is the union of projections of the final layer's hyperplane onto each region, enabling exact sampling.

## Key Results
- SplineCam exactly computes network geometry without sampling or approximations
- Method scales to large networks with convolutional layers and skip connections
- Enables exact sampling from decision boundaries on or off the manifold
- Characterizes network geometry to compare architectures and measure generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SplineCam computes the exact partition of a deep network's input space without sampling or approximations.
- Mechanism: Leverages CPWL spline theory to represent each layer as a MASO, building the partition layer-by-layer by projecting hyperplanes back to input space and intersecting with existing regions.
- Core assumption: Network uses CPWL activations so the overall mapping is CPWL and can be decomposed into affine regions.
- Evidence anchors:
  - [abstract] "SplineCam exactly computes a DNs geometry without resorting to approximations such as sampling or architecture simplification."
  - [section] "Lemma 1... Such formulations of DNs have been extensively employed..."
  - [corpus] Weak - related papers do not directly discuss exact partition computation.

### Mechanism 2
- Claim: SplineCam can sample points from the decision boundary exactly.
- Mechanism: After computing the partition, the decision boundary is the union of projections of the final layer's hyperplane onto each region. Sampling is done by selecting points from these projected boundaries.
- Core assumption: Decision boundary is a union of hyperplane segments within the input space partition.
- Evidence anchors:
  - [abstract] "SplineCam enables one to... sample from the decision boundary on or off the manifold."
  - [section] "Theorem 1... the decision boundary in RS is therefore ∪ω∈Ω{projω(hL1)∩ω}."
  - [corpus] Weak - related papers discuss decision boundaries but not exact sampling.

### Mechanism 3
- Claim: SplineCam scales to large networks and convolutional layers.
- Mechanism: Uses vectorized linear algebra operations (GPU-accelerated) for most computations, stores large weight matrices sparsely, and distributes region processing across CPU threads for the graph search part.
- Core assumption: Memory and computational resources are sufficient to store intermediate representations and parallelize region processing.
- Evidence anchors:
  - [section] "SplineCam is scalable... all the SplineCam operations can be vectorized except for the search algorithm... distributing sets of (ω, H) across CPU threads."
  - [section] "For an MLP with width 1000, input dimensionality 8002, and 8M params, it takes SplineCam 134s to find132K regions."
  - [corpus] Weak - related papers do not discuss scaling methods.

## Foundational Learning

- Concept: Continuous Piecewise Linear (CPWL) functions and their properties.
  - Why needed here: SplineCam relies on the network being CPWL to decompose it into affine regions.
  - Quick check question: Can you explain why a network with ReLU activations is CPWL?

- Concept: Max-Affine Spline Operators (MASO) and their composition.
  - Why needed here: Each layer of the network is modeled as a MASO, and the overall network is a composition of MASOs.
  - Quick check question: How does a ReLU layer correspond to a MASO with two affine maps?

- Concept: Computational geometry - hyperplane arrangements and polyhedral partitions.
  - Why needed here: Finding the partition involves computing intersections of hyperplanes within a 2D polytope and identifying the resulting regions (cycles in the graph).
  - Quick check question: What is the maximum number of regions formed by n lines in general position in 2D?

## Architecture Onboarding

- Component map:
  Input: 2D polytope in input space + projection matrix -> Core: SplineCam wrapper around PyTorch model -> Output: Set of partition regions, decision boundary segments, sampling functionality

- Critical path:
  1. Wrap model with SplineCam (converts layers to MASO form)
  2. Define 2D input domain and projection matrix
  3. Compute first-layer partition using hyperplane intersections
  4. For each subsequent layer: Project deeper layer hyperplanes onto each region, re-partition each region with new hyperplanes
  5. After all layers, collect regions and decision boundary
  6. Sample from boundary if needed

- Design tradeoffs:
  - Exactness vs scalability: Exact computation is possible but may be slow for very large networks; approximations could trade accuracy for speed
  - 2D restriction: Limits visualization to 2D slices; higher-dimensional analysis is intractable
  - Memory vs time: Storing all intermediate regions can be memory-intensive; distributing work across threads helps

- Failure signatures:
  - No regions found: Hyperplanes may not intersect the input domain; check domain size/orientation
  - Unexpectedly few/many regions: Network may be too simple/complex for the domain; verify activation patterns
  - CUDA out of memory: Reduce batch size or model width; use sparse matrices more aggressively

- First 3 experiments:
  1. Apply SplineCam to a simple 2-layer MLP (e.g., width 10) on a 2D synthetic dataset; visualize partition and decision boundary
  2. Compare partition statistics (number of regions, average volume) for an MLP vs a CNN on MNIST; confirm convolutional networks have higher partition density
  3. Sample points from the decision boundary of a binary classifier and verify they lie exactly on the boundary by checking model output ≈ 0

## Open Questions the Paper Calls Out

- Question: How does the number of linear regions in a deep network correlate with its generalization performance on unseen data?
  - Basis in paper: [inferred] The paper discusses using SplineCam to measure partition density and compares it across different architectures and training regimes. It suggests that higher partition density may indicate better generalization.
  - Why unresolved: While the paper shows correlations between partition density and generalization, it does not establish a causal relationship or provide a quantitative model linking the two.
  - What evidence would resolve it: Experiments systematically varying network capacity and measuring both partition density and generalization performance on held-out data sets, while controlling for other factors like optimization and data augmentation.

- Question: Can the geometry of decision boundaries, as visualized by SplineCam, provide insights into the mechanisms of adversarial attacks and defenses?
  - Basis in paper: [explicit] The paper mentions that activation-based interpretability methods are susceptible to feature adversarial attacks and that SplineCam could enable sampling from decision boundaries.
  - Why unresolved: The paper does not explore the relationship between decision boundary geometry and adversarial robustness. It only mentions the potential for future work in this direction.
  - What evidence would resolve it: Experiments comparing the geometry of decision boundaries for adversarially trained networks versus standard networks, and analyzing how adversarial examples navigate these boundaries.

## Limitations
- Exactness restricted to CPWL activations, excluding networks with sigmoid/tanh activations
- 2D input domain constraint limits visualization to planar slices, preventing direct analysis of higher-dimensional geometry
- Memory requirements scale exponentially with network depth and width, making very large architectures challenging to analyze

## Confidence
- **High Confidence**: The CPWL spline formulation and layerwise partition computation are mathematically rigorous with clear theoretical grounding in Lemma 1-2.
- **Medium Confidence**: The graph-based region enumeration algorithm (Alg. 1) is sound but implementation complexity may affect practical performance.
- **Medium Confidence**: Scaling claims are supported by experimental evidence but rely on assumptions about memory efficiency and parallelization that require further validation on larger networks.

## Next Checks
1. Generate samples from decision boundaries for simple 2D datasets and verify exact boundary membership by checking model output ≈ 0 with high precision.
2. Apply SplineCam to multiple architectures with identical initialization and training, then verify that observed partition differences align with theoretical predictions about depth vs width effects.
3. Profile memory usage systematically across network sizes (depths 5→50, widths 100→1000) to empirically verify claimed O(n) scaling with proper sparse matrix implementations.