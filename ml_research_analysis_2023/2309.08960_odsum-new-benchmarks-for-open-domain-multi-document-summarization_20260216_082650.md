---
ver: rpa2
title: 'ODSum: New Benchmarks for Open Domain Multi-Document Summarization'
arxiv_id: '2309.08960'
source_url: https://arxiv.org/abs/2309.08960
tags:
- summarization
- retrieval
- document
- documents
- summary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ODSum, a new dataset for Open-Domain Multi-Document
  Summarization (ODMDS), where the document index is interdependent and often interrelated.
  The authors propose a rule-based method to process query-based document summarization
  datasets into ODMDS datasets, resulting in two new datasets: ODSum-story and ODSum-meeting.'
---

# ODSum: New Benchmarks for Open Domain Multi-Document Summarization

## Quick Facts
- **arXiv ID**: 2309.08960
- **Source URL**: https://arxiv.org/abs/2309.08960
- **Reference count**: 8
- **Primary result**: ODSum dataset enables evaluation of Open-Domain Multi-Document Summarization with complex, interrelated document collections

## Executive Summary
This paper introduces ODSum, a new dataset for Open-Domain Multi-Document Summarization (ODMDS) where documents are interdependent and interrelated. The authors develop a rule-based method to convert existing query-based summarization datasets into ODMDS format, creating ODSum-story and ODSum-meeting datasets. They systematically evaluate the "retrieve-then-summarize" pipeline across various retrievers and summarizers, revealing significant performance variance between evaluation metrics and demonstrating that LLM-based summarizers are particularly sensitive to retrieval errors. The study also proposes methods to improve performance and robustness against imperfect retrieval.

## Method Summary
The authors construct ODSum by processing existing query-based summarization datasets (SQuALITY for stories, QMSum for meetings) using rule-based methods. For ODSum-story, stories are segmented into chapters using HTML delimiters; for ODSum-meeting, queries are clustered and merged to create multi-meeting contexts. They evaluate a "retrieve-then-summarize" pipeline using sparse retrievers (BM25), dense retrievers (Contriever, GPT-embedding), and various summarizers (BART, PRIMERA, GPT-3.5, GPT-4, Llama-2). Performance is assessed using ROUGE-2, BERTScore, and G-EVAL metrics, with particular attention to how retrieval errors impact summarization quality.

## Key Results
- LLMs (GPT-3.5, GPT-4, Llama-2) are highly sensitive to retrieval errors, showing significant performance drops when using retrieved vs. oracle documents
- G-EVAL metric shows higher variance between models compared to ROUGE-2 and BERTScore, suggesting it captures more nuanced quality differences
- Simple truncation methods effectively handle token limits for large context models like GPT-4 and Llama-2
- The retrieve-then-summarize pipeline achieves competitive performance despite retrieval errors, particularly with truncation strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The "retrieve-then-summarize" pipeline can handle interdependent document collections by allowing flexible retrieval of contextually related documents
- Mechanism: Instead of requiring exact retrieval matches, the pipeline retrieves documents that may not be directly query-relevant but are thematically connected. These retrieved documents can then be synthesized into a coherent summary that addresses the query through aggregation of related information
- Core assumption: In complex document collections with overlapping topics, retrieval performance is not strictly binary (relevant/irrelevant) but exists on a spectrum where "related" documents can still provide useful context for summarization
- Evidence anchors:
  - [abstract]: "With a more inter-related document set, there does not necessarily exist a correct answer for the retrieval, making it hard to measure the retrieving performance."
  - [section]: "ODSsum-meeting presents a more complex picture with its documents relating with each other and not necessarily to its own, and that simply evaluating the performance with the ground truth documents does not suffice to measure the retrieval performance."

### Mechanism 2
- Claim: Evaluation metrics that incorporate source context (like G-EVAL) are more reliable for assessing summary quality in ODMDS tasks than metrics that only compare summary to reference text
- Mechanism: G-EVAL uses LLMs to assess consistency and relevance by comparing the generated summary against both the query and reference summary, providing a more holistic evaluation that accounts for retrieval errors and contextual alignment
- Core assumption: Traditional metrics like ROUGE and BERTScore cannot adequately capture the quality of summaries when retrieval errors introduce documents that contain overlapping but not identical information to the reference
- Evidence anchors:
  - [section]: "Popular current evaluation metrics, such as ROUGE (Lin, 2004) and BERTScore (Zhang et al., 2020), assess the similarity between the predicted summary and the reference summary without taking the source input into account."
  - [corpus]: "Found 25 related papers... Average neighbor FMR=0.448" (weak corpus evidence for evaluation metrics specifically)

### Mechanism 3
- Claim: The ODSum dataset construction method successfully transforms single-document summarization datasets into multi-document settings by leveraging the natural document structure within the source data
- Mechanism: For ODSum-story, stories are split into chapters that serve as independent documents; for ODSum-meeting, queries are clustered and merged to create multi-meeting contexts. This preserves the original information needs while creating realistic multi-document scenarios
- Core assumption: The source datasets (SQuALITY and QMSum) contain sufficient document-level structure that can be meaningfully partitioned or aggregated to create multi-document contexts
- Evidence anchors:
  - [section]: "We utilized the natural delimiters of the stories in its html-scrapped version, specifically web delimiters like 'hr class='***'/' to segment the stories into their respective chapters."
  - [section]: "We preserved the original meeting transcripts but extracted the queries to form clusters based on their similarity."

## Foundational Learning

- **Information Retrieval Evaluation Metrics (P@K, R@K, NDCG, MAP)**
  - Why needed here: Understanding how different retrieval metrics capture various aspects of retrieval performance is crucial for interpreting the results and comparing retriever performance
  - Quick check question: What's the difference between precision@k and recall@k in the context of document retrieval?

- **Text Embedding and Semantic Similarity**
  - Why needed here: The paper uses both sparse (BM25) and dense (Contriever, GPT-embedding) retrievers, requiring understanding of how different embedding approaches capture document-query relevance
  - Quick check question: How do sparse retrievers like BM25 differ from dense retrievers in terms of what they capture about document-query relationships?

- **Large Language Model Prompt Engineering**
  - Why needed here: The paper uses LLMs for both summarization and evaluation, with specific prompt designs for each task
  - Quick check question: What considerations should be made when designing prompts for LLMs to summarize retrieved documents versus evaluating summaries?

## Architecture Onboarding

- **Component map**: Source datasets → ODSum construction → Training/Evaluation splits → Retriever training → Summarization training → Evaluation (ROUGE, BERTScore, G-EVAL) → Analysis
- **Critical path**: ODSum Construction → Retriever Training/Evaluation → Summarization Training/Evaluation → Analysis of Retrieval-Summarization Relationship
- **Design tradeoffs**: Dataset complexity vs. evaluation tractability; Retriever diversity vs. system simplicity; Evaluation metric comprehensiveness vs. cost
- **Failure signatures**: Retrieval errors causing dramatic performance drops in LLM-based summarizers; High variance between evaluation metrics; Poor performance on complex datasets
- **First 3 experiments**:
  1. Compare retrieval performance (P@K, R@K, NDCG, MAP) across all retriever types on both ODSum-story and ODSum-meeting to establish baseline retrieval capabilities
  2. Evaluate summarization quality using all three metrics (ROUGE, BERTScore, G-EVAL) for each summarization model under oracle and retrieval settings to understand the retrieval-summarization relationship
  3. Test the proposed truncation, map-reduce, and refine methods on the ODSum-meeting dataset with retrieval errors to identify which approaches best handle imperfect retrieval

## Open Questions the Paper Calls Out
- What are the key differences between the rule-based method for constructing ODSum datasets and other methods used in previous studies?
- How does the performance of the retrieve-then-summarize pipeline vary across different datasets and domains?
- What are the limitations of the G-EVAL metric in evaluating the quality of summaries generated by different models?

## Limitations
- Rule-based dataset construction may not capture all nuances of naturally occurring multi-document scenarios
- Evaluation relies heavily on LLM-based metrics that may introduce evaluation bias
- Limited exploration of how pipeline performance varies across different domains and dataset characteristics

## Confidence

- **High Confidence**: The observed sensitivity of LLM summarizers to retrieval errors is well-demonstrated across multiple experiments and model types
- **Medium Confidence**: The effectiveness of truncation, map-reduce, and refine methods for handling imperfect retrieval needs further validation on larger datasets
- **Medium Confidence**: The claim that G-EVAL provides more reliable evaluation than ROUGE/BERTScore is supported but could benefit from human evaluation validation

## Next Checks

1. Conduct human evaluation studies comparing G-EVAL scores with human judgments on summary quality for both oracle and retrieved document settings
2. Test the proposed dataset construction method on additional source datasets to evaluate its generalizability beyond SQuALITY and QMSum
3. Perform ablation studies on the query clustering parameters for ODSum-meeting to determine optimal clustering thresholds and their impact on downstream performance