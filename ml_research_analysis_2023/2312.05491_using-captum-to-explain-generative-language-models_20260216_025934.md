---
ver: rpa2
title: Using Captum to Explain Generative Language Models
arxiv_id: '2312.05491'
source_url: https://arxiv.org/abs/2312.05491
tags:
- attribution
- features
- methods
- captum
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Captum v0.7 introduces new features for analyzing generative language
  models using attribution methods like feature ablation, Shapley values, and integrated
  gradients. The library enables users to define interpretable features (words, phrases,
  etc.), set baselines for perturbation, and group correlated features for joint attribution.
---

# Using Captum to Explain Generative Language Models

## Quick Facts
- arXiv ID: 2312.05491
- Source URL: https://arxiv.org/abs/2312.05491
- Authors: 
- Reference count: 6
- Primary result: Captum v0.7 introduces new features for analyzing generative language models using attribution methods like feature ablation, Shapley values, and integrated gradients

## Executive Summary
Captum v0.7 extends the Captum library with specialized tools for interpreting generative language models through feature attribution. The library enables users to define interpretable features (words, phrases, etc.), set baselines for perturbation, and group correlated features for joint attribution. Applications include understanding model associations and evaluating few-shot prompt effectiveness. Visualization tools and flexible APIs support customization for robust interpretability experiments.

## Method Summary
Captum v0.7 provides LLM-specific attribution APIs that allow users to define interpretable features from text prompts, select baselines for perturbation, and apply various attribution methods including feature ablation, Shapley values, and integrated gradients. The library supports both perturbation-based methods (which don't require model weights) and gradient-based methods, with special handling for the non-differentiable nature of embedding lookups in language models. Users can group correlated features together to maintain natural coherence during perturbation, and visualize attribution results to understand feature importance and model behavior.

## Key Results
- Captum v0.7 enables interpretable feature definition for language model inputs
- The library supports both perturbation-based and gradient-based attribution methods
- Feature grouping capabilities allow joint attribution of correlated features
- Applications demonstrated include understanding model associations and few-shot prompt evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature ablation and perturbation methods work because they measure model behavior on counterfactual inputs by systematically replacing features with baseline values.
- Mechanism: The attribution score for a feature is computed as the difference in model output when the feature is replaced with its baseline value, allowing isolation of that feature's contribution.
- Core assumption: Replacing features with baselines creates valid counterfactuals within the natural data distribution.
- Evidence anchors:
  - [abstract] "Users can choose any interested tokens or segments from the input prompt as features, e.g., 'Palm Coast' in the example shown in Figure 1, and use attribution methods to quantify their impacts to the generation targets"
  - [section 3.1.2] "It is recommended to select a baseline which fits the context of the original text and remains within the natural data distribution"
  - [corpus] Weak evidence - no direct citations found about baseline validity in language models
- Break condition: If baselines fall outside the natural data distribution, the counterfactuals become unrealistic and attribution scores lose interpretability.

### Mechanism 2
- Claim: Gradient-based methods work by measuring sensitivity of the output to small changes in input embeddings through backpropagation.
- Mechanism: Integrated gradients computes the path integral of gradients from a baseline to the input point, capturing cumulative sensitivity along that path.
- Core assumption: The model's behavior is differentiable and locally smooth enough for gradient information to be meaningful.
- Evidence anchors:
  - [section 2.2.2] "Integrated Gradients estimates attribution by computing the path integral of model gradients between the baseline point and input point"
  - [section 3.2] "Applying these methods to language models is typically more challenging than for models with dense feature inputs, since embedding lookups in LLMs are typically non-differentiable functions"
  - [corpus] Weak evidence - no direct citations found about gradient-based attribution for LLMs
- Break condition: If the model has non-differentiable operations (like embedding lookups), the gradient-based attributions become unreliable or zero.

### Mechanism 3
- Claim: Feature grouping/masking works because correlated features should be perturbed together to avoid unrealistic counterfactuals.
- Mechanism: When features are highly correlated (e.g., city and state), ablating them together maintains natural coherence while measuring their joint contribution.
- Core assumption: Highly correlated features have interdependent effects on model output that can't be captured by independent ablation.
- Evidence anchors:
  - [section 3.1.3] "Similar to the underlying Captum attribution methods, we support feature masking, which enables grouping features together to perturb as a single unit and obtain a combined, single attribution score"
  - [section 3.1.3] "For example, in Figure 2, the feature pairs (city, state) and (name, pronoun) are often highly correlated, and thus it may make sense to group them"
  - [corpus] Weak evidence - no direct citations found about feature grouping in language model attribution
- Break condition: If features are incorrectly grouped, the attribution may conflate independent effects or miss important interactions.

## Foundational Learning

- Concept: Counterfactual reasoning
  - Why needed here: Understanding how the model behaves when features are changed requires grasping the concept of counterfactual inputs
  - Quick check question: If you replace "Palm Coast" with "Seattle" in the prompt, what type of input are you creating and why does this matter for attribution?

- Concept: Feature importance vs. interaction effects
  - Why needed here: Attribution methods can capture both individual feature importance and interaction effects, but the interpretation differs
  - Quick check question: If two features are highly correlated, should they be attributed independently or together? Why?

- Concept: Differentiability and gradient-based attribution
  - Why needed here: Understanding when gradient-based methods work (and when they don't) requires knowing about model differentiability
  - Quick check question: Why might gradient-based attribution methods fail for language models with embedding lookups?

## Architecture Onboarding

- Component map: Captum v0.7 library with LLM-specific attribution APIs -> TextTemplateFeature utilities -> ProductBaselines management -> multiple attribution methods (FeatureAblation, ShapleyValueSampling, LayerIntegratedGradients) -> visualization tools

- Critical path: User defines features -> selects baseline(s) -> chooses attribution method -> runs attribution -> visualizes results

- Design tradeoffs: Flexibility in feature definition vs. complexity for users; perturbation-based methods vs. gradient-based methods (differentiability requirements); single vs. multiple baselines (computational cost vs. robustness)

- Failure signatures: Unexpected attribution scores when baselines are outside natural distribution; zero attributions from gradient-based methods on non-differentiable models; overly complex feature definitions that don't match user intent

- First 3 experiments:
  1. Run simple feature ablation on "Palm Coast" vs baseline "Seattle" with default settings to verify basic functionality
  2. Test Shapley Value Sampling with multiple baselines for the same feature to understand baseline sensitivity
  3. Compare gradient-based Integrated Gradients vs perturbation-based Feature Ablation on the same input to understand method differences

## Open Questions the Paper Calls Out

- Question: How does the choice of baseline (single vs. distribution) affect the reliability and interpretability of feature attribution results in LLMs?
  - Basis in paper: [explicit] The paper discusses the importance of baseline selection and mentions supporting both single baselines and distributions of baselines, but does not empirically compare their effects.
  - Why unresolved: The paper does not provide experimental results or quantitative analysis on how different baseline choices impact attribution results.
  - What evidence would resolve it: Controlled experiments comparing attribution results using different baseline strategies (e.g., single baseline vs. distribution-based) on the same tasks, measuring consistency and interpretability of results.

- Question: To what extent do perturbation-based attribution methods like Feature Ablation and Shapley Value Sampling provide more reliable explanations than gradient-based methods for LLMs, given the challenges of non-differentiable embedding lookups?
  - Basis in paper: [explicit] The paper contrasts perturbation-based and gradient-based methods, noting that perturbation-based methods do not require model weights and may avoid issues with non-differentiable embeddings, but does not directly compare their reliability.
  - Why unresolved: No empirical comparison of the reliability or robustness of perturbation-based vs. gradient-based methods is provided.
  - What evidence would resolve it: Systematic comparison of attribution results from both methods across diverse tasks, including sensitivity analyses to input perturbations and alignment with human judgments.

- Question: Can automated feature and baseline selection tools significantly improve the usability and effectiveness of LLM attribution methods without sacrificing interpretability?
  - Basis in paper: [explicit] The paper suggests future work on automating feature and baseline selection, but does not explore current limitations or potential benefits of such automation.
  - Why unresolved: The paper does not investigate how automated tools would perform in practice or whether they could maintain the flexibility and interpretability provided by manual customization.
  - What evidence would resolve it: Development and evaluation of automated selection tools, comparing their performance and interpretability to manual approaches on benchmark datasets.

## Limitations

- The effectiveness of gradient-based attribution methods for language models with embedding lookups remains theoretically problematic
- The claim that correlated features should be grouped together lacks empirical validation across diverse model architectures
- The specific effectiveness of different baseline selection strategies for language models is not well-documented

## Confidence

**High Confidence**: The core functionality of Captum v0.7's LLM attribution API - defining features, applying perturbation methods, and obtaining attribution scores - is well-specified and directly implementable from the documentation. The relationship between feature ablation and counterfactual reasoning is well-established in the attribution literature.

**Medium Confidence**: The claim that gradient-based methods face challenges with embedding lookups is theoretically sound, but the practical effectiveness of Captum's proposed solutions (like the LLMAttribution API) cannot be verified without implementation details. The feature grouping recommendations are reasonable but lack empirical validation.

**Low Confidence**: The specific effectiveness of different baseline selection strategies for language models is not well-documented, and the claim that multiple baselines improve robustness lacks quantitative evidence in this context.

## Next Checks

1. **Baseline Distribution Validation**: Run feature ablation experiments with systematically varied baselines (from very similar to very different from the original text) and measure how attribution scores change. This would validate whether the "natural data distribution" assumption holds in practice.

2. **Gradient vs Perturbation Method Comparison**: Apply both gradient-based (Integrated Gradients) and perturbation-based (Feature Ablation) methods to identical inputs with identical features and baselines, then measure correlation between their attribution scores. This would reveal if Captum's handling of non-differentiable operations is effective.

3. **Feature Grouping Impact Analysis**: Create experiments with both grouped and ungrouped versions of correlated features (like city/state pairs) and measure the difference in attribution scores and interpretability. This would empirically validate whether feature grouping improves attribution quality as claimed.