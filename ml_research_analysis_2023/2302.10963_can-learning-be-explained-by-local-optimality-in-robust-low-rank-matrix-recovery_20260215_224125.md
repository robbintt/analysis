---
ver: rpa2
title: Can Learning Be Explained By Local Optimality In Robust Low-rank Matrix Recovery?
arxiv_id: '2302.10963'
source_url: https://arxiv.org/abs/2302.10963
tags:
- matrix
- 'true'
- have
- lemma
- solutions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the identifiability of true low-rank solutions
  in Burer-Monteiro (BM) factorization under noisy measurements. The authors show
  that when the search rank exceeds the true rank and measurements are noisy, true
  solutions may not correspond to global or even local optima, challenging the conventional
  belief that strict saddle points should be avoided.
---

# Can Learning Be Explained By Local Optimality In Robust Low-rank Matrix Recovery?

## Quick Facts
- **arXiv ID:** 2302.10963
- **Source URL:** https://arxiv.org/abs/2302.10963
- **Reference count:** 40
- **Primary result:** True low-rank solutions in BM factorization may not be local optima under noisy measurements and over-parameterization

## Executive Summary
This paper challenges the conventional wisdom that local optima in Burer-Monteiro (BM) factorization correspond to true low-rank solutions. The authors show that when the search rank exceeds the true rank and measurements are noisy, true solutions may not correspond to global or even local optima. Specifically, they prove that under moderate assumptions, true solutions become strict saddle points rather than optima, with at least one true solution not even being a critical point. This demonstrates that over-parameterization can be detrimental rather than beneficial in the presence of noise, contrasting sharply with the noiseless case where over-parameterization typically reduces spurious local solutions.

## Method Summary
The authors analyze the optimization landscape of BM factorization for low-rank matrix recovery using structured perturbations to test whether true solutions correspond to local optima. They employ concentration inequalities, covering number bounds, and Gaussian process supremum control to verify descent directions around true solutions. The analysis covers both ℓq and ℓ1 loss functions, with the latter requiring Clarke subdifferential theory for non-smooth optimization. For asymmetric matrix sensing and completion, they show true solutions are not global minima and at least one is not a critical point. For symmetric cases, true solutions remain critical points but with much smaller descent directions, creating a flatter landscape.

## Key Results
- True low-rank solutions in BM factorization can become strict saddle points under noise and over-parameterization
- For asymmetric matrix sensing with measurement size m ≲ max{d1,d2}k, none of the true solutions are global minima
- Over-parameterization transforms from blessing to curse in presence of noise, contrasting with noiseless settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** True low-rank solutions can become strict saddle points under noise and over-parameterization.
- **Mechanism:** The structured perturbation approach creates descent directions that exploit noise-corrupted measurements to push true solutions away from being critical points.
- **Core assumption:** The measurement matrices satisfy RIP and noise follows the heavy-tailed model with sufficient corruption probability.
- **Evidence anchors:**
  - [abstract] "we show that, under moderate assumptions, the true solutions corresponding to X⋆ do not emerge as local optima, but rather as strict saddle points"
  - [section 4] "with high probability and for any γ ≤ γ0, there exists ∆ W ∈ B F (γ) such that fℓq (W+∆W)− fℓq (W) = − Ω(γ)"
- **Break condition:** If noise corruption probability drops below threshold or measurement count scales linearly with search rank k, true solutions can regain local optimality.

### Mechanism 2
- **Claim:** Over-parameterization transforms from blessing to curse in presence of noise.
- **Mechanism:** As search rank k exceeds true rank r, the optimization landscape develops more opportunities for noise exploitation, creating non-critical true solutions even when sample complexity would otherwise be sufficient.
- **Core assumption:** Measurement size m scales sublinearly with search rank k while noise corruption is present.
- **Evidence anchors:**
  - [abstract] "over-estimation of the rank leads to progressively fewer sub-optimal local solutions while preserving the identiﬁability of the true solutions... we show that with noisy measurements, the global solutions of the over-parameterized BM no longer correspond to the true solutions"
  - [section 3.1] "for the asymmetric matrix sensing with the measurement size m ≲ max{d1,d 2}k, none of the true solutions are global minima"
- **Break condition:** When measurement count satisfies m ≳ max{d1,d2}k or noise corruption probability p < 1/2 for ℓ1-loss, true solutions become global minima again.

### Mechanism 3
- **Claim:** Symmetric matrix recovery maintains flatter landscape around true solutions compared to asymmetric case.
- **Mechanism:** The symmetric structure constrains perturbations differently, creating critical points even when global optimality is lost, but with much smaller descent directions.
- **Core assumption:** Ground truth matrix is positive semidefinite and measurement corruption exists.
- **Evidence anchors:**
  - [section 3.1] "Despite these negative results, BM enjoys a flatter landscape around its true solutions in the symmetric setting"
  - [section 5] "when dr ≲m ≲dk, all the true solutions become critical points and the steepest descent direction can reduce the loss by at most O(γ2) within a γ-neighborhood"
- **Break condition:** If measurement count scales with true rank r (m ≳ dr) rather than search rank k, true solutions regain local optimality.

## Foundational Learning

- **Concept:** Restricted Isometry Property (RIP)
  - Why needed here: RIP ensures measurement matrices preserve distances between low-rank matrices, crucial for proving landscape properties
  - Quick check question: What is the relationship between RIP constant δ and the measurement count m for rank-r matrices?

- **Concept:** Clarke subdifferential and generalized directional derivatives
  - Why needed here: The paper analyzes non-smooth ℓ1-loss, requiring tools beyond classical calculus to characterize critical points
  - Quick check question: How does Clarke generalized directional derivative differ from ordinary directional derivative?

- **Concept:** Orlicz norms and sub-Gaussian/sub-exponential random variables
  - Why needed here: The noise model uses Orlicz norms to capture heavy-tailed distributions, enabling concentration bounds for non-Gaussian noise
  - Quick check question: What is the tail probability bound for a sub-Gaussian random variable with norm σ?

## Architecture Onboarding

- **Component map:** Measurement operator A(·) → Burer-Monteiro factorization → Loss function (ℓq or ℓ1) → Optimization landscape analysis
- **Critical path:**
  1. Verify measurement model satisfies RIP/ Gaussian assumptions
  2. Apply structured perturbation construction to test non-criticality
  3. Compute concentration bounds for descent direction existence
  4. Characterize landscape around true solutions (global vs local vs saddle)
- **Design tradeoffs:**
  - ℓ1-loss provides robustness to heavy-tailed noise but requires Clarke subdifferential analysis
  - ℓq-loss for q > 1 enables smoother analysis but loses robustness properties
  - Symmetric vs asymmetric recovery trades identifiability for landscape flatness
- **Failure signatures:**
  - If structured perturbations fail to produce descent directions → true solutions may be local optima
  - If concentration bounds don't hold → landscape analysis may be invalid for given noise level
  - If measurement count too low → over-parameterization may not create sufficient landscape complexity
- **First 3 experiments:**
  1. Verify structured perturbation construction works on small synthetic asymmetric matrix completion with known noise corruption
  2. Test whether true solutions become critical points when measurement count scales with k vs r
  3. Compare landscape flatness between symmetric and asymmetric cases for same parameter settings

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the local geometry around true solutions change as the search rank k increases beyond the true rank r in the presence of noise?
- **Basis in paper:** [explicit] The authors show that as k exceeds r and measurements become noisy, true solutions may not correspond to global or even local optima, and at least one true solution is not a critical point.
- **Why unresolved:** The paper provides necessary conditions for unidentifiability but does not fully characterize the landscape transition or provide a complete characterization of the local geometry as k varies.
- **What evidence would resolve it:** Empirical studies showing how the number and nature of critical points change with increasing k in noisy settings, or theoretical bounds on the distance between true solutions and critical points.

### Open Question 2
- **Question:** Can over-parameterization ever be beneficial for low-rank matrix recovery in the presence of noise, and if so, under what conditions?
- **Basis in paper:** [inferred] The authors demonstrate that over-parameterization can be a curse rather than a blessing in noisy settings, contrasting with noiseless cases where it reduces spurious local solutions.
- **Why unresolved:** The paper focuses on negative results but doesn't explore potential scenarios where over-parameterization might still provide benefits despite noise.
- **What evidence would resolve it:** Analysis of specific noise models or measurement structures where over-parameterization could improve robustness or convergence speed, or empirical studies comparing performance across different noise levels.

### Open Question 3
- **Question:** What alternative optimization approaches could recover true solutions when they are not local or global optima of the Burer-Monteiro factorization?
- **Basis in paper:** [explicit] The authors suggest that trajectory analysis of local-search algorithms, rather than global landscape analysis, might be more promising when true and global solutions don't coincide.
- **Why unresolved:** The paper advocates for trajectory analysis but doesn't propose or analyze specific alternative algorithms that could succeed under these conditions.
- **What evidence would resolve it:** Development and testing of optimization methods that can escape from strict saddle points or use initialization strategies that avoid non-critical true solutions, with theoretical guarantees or empirical validation.

## Limitations
- Analysis relies on specific assumptions about measurement models (RIP, Gaussian measurements) and noise distributions (sub-Gaussian/sub-exponential, sparse corruption)
- Results may not extend to other noise models or measurement operators beyond the studied frameworks
- Focus on theoretical landscape analysis rather than practical algorithm design or empirical validation

## Confidence
- **Asymmetric cases:** Medium-High - proofs use well-established techniques from optimization theory and random matrix theory
- **Symmetric case:** Medium - analysis is more complex and results less definitive
- **Practical implications:** Medium - theoretical results may not fully capture real-world optimization dynamics

## Next Checks
1. Empirical verification of the structured perturbation approach on synthetic data with varying noise corruption levels and measurement counts
2. Extension of the analysis to other noise models beyond the sub-Gaussian/sub-exponential framework
3. Investigation of whether regularization techniques (e.g., nuclear norm penalties) can restore local optimality of true solutions in the over-parameterized regime