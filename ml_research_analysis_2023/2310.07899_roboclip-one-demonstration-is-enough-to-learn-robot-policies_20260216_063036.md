---
ver: rpa2
title: 'RoboCLIP: One Demonstration is Enough to Learn Robot Policies'
arxiv_id: '2310.07899'
source_url: https://arxiv.org/abs/2310.07899
tags:
- reward
- learning
- task
- video
- rewards
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RoboCLIP, a method for learning robot policies
  from a single demonstration, overcoming the need for large-scale expert data. RoboCLIP
  utilizes pretrained video-and-language models to generate rewards without manual
  reward function design.
---

# RoboCLIP: One Demonstration is Enough to Learn Robot Policies

## Quick Facts
- arXiv ID: 2310.07899
- Source URL: https://arxiv.org/abs/2310.07899
- Reference count: 10
- One demonstration is sufficient to learn robot policies using pretrained video-and-language models

## Executive Summary
RoboCLIP introduces a novel approach to robot policy learning that requires only a single demonstration (video or text) to generate effective reward signals. By leveraging pretrained video-and-language models like S3D, RoboCLIP encodes demonstrations and agent trajectories into a shared semantic space, using similarity scores as rewards. This eliminates the need for manual reward function design and large-scale expert datasets. The method demonstrates 2-3 times higher zero-shot performance compared to competing imitation learning methods, while also supporting out-of-domain demonstrations.

## Method Summary
RoboCLIP uses pretrained video-and-language models (specifically S3D trained on HowTo100M) to generate rewards without manual reward function design. The method encodes a single demonstration (video or text) and the agent's episode video into the semantically meaningful latent space of the pretrained model, using the similarity score as a reward. This sparse reward signal is then used to train the agent policy online using reinforcement learning algorithms like PPO. The approach eliminates the need for large-scale expert datasets while supporting both in-domain and out-of-domain demonstrations.

## Key Results
- 2-3 times higher zero-shot performance than competing imitation learning methods using only one demonstration
- Successfully learns from both video demonstrations and textual descriptions of tasks
- Demonstrates ability to use out-of-domain demonstrations (e.g., human videos) for reward generation
- Shows transfer of motion "style" from source demonstrations to learned policies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretrained video-and-language models can provide semantically meaningful similarity scores between agent trajectories and task descriptions/demonstrations.
- Mechanism: S3D, pretrained on HowTo100M, maps videos of agent behavior and task specifications into a shared latent space. The dot product between these embeddings serves as a proxy reward, indicating how closely the agent's behavior matches the desired task.
- Core assumption: The VLM's latent space generalizes across domains, encoding semantically meaningful task representations even for unseen robotic environments.
- Evidence anchors:
  - [abstract] "RoboCLIP utilizes pretrained VLMs without any finetuning for reward generation."
  - [section] "These models utilize language descriptions while training to supervise their visual understanding so that semantically similar vision inputs are embedded close together in a shared vector space."
  - [corpus] Weak - related papers discuss general imitation learning but not VLM-based reward generation specifically.
- Break condition: The VLM's latent space does not align with the robotic domain, leading to meaningless similarity scores.

### Mechanism 2
- Claim: A single demonstration (video or text) is sufficient to bootstrap reward learning.
- Mechanism: By using a single demonstration to define the task in the VLM's latent space, RoboCLIP avoids the need for large-scale expert datasets. The similarity score between the agent's trajectory and this demonstration provides a sparse reward signal.
- Core assumption: The VLM's embedding of a single demonstration captures the essence of the task, allowing for generalization to the agent's environment.
- Evidence anchors:
  - [abstract] "RoboCLIP, an online imitation learning method that uses a single demonstration (overcoming the large data requirement) in the form of a video demonstration or a textual description of the task to generate rewards without manual reward function design."
  - [section] "RoboCLIP computes a similarity score between videos of online agent experience with a task descriptor, i.e., a text description of the task or a single human demonstration video, to generate trajectory-level rewards to train the agent."
  - [corpus] Weak - related papers focus on imitation learning but typically require multiple demonstrations.
- Break condition: The single demonstration does not adequately represent the task, leading to poor reward signals.

### Mechanism 3
- Claim: Out-of-domain demonstrations can be used for reward generation.
- Mechanism: The VLM's pretraining on diverse human activity videos allows it to generalize across domains. Videos of humans or animated characters performing the task in different environments can still provide useful reward signals.
- Core assumption: The VLM's embeddings are robust to domain shifts, encoding task-relevant features invariant to environment specifics.
- Evidence anchors:
  - [abstract] "Additionally, RoboCLIP can also utilize out-of-domain demonstrations, like videos of humans solving the task for reward generation, circumventing the need to have the same demonstration and deployment domains."
  - [section] "We study how well this works in the Franka Kitchen [Gupta et al., 2019] environment... We find that RoboCLIP allows for mimicking the 'style' of the source demonstration, with idiosyncrasies of motion from the source demonstration generally transferring to the policy generated."
  - [corpus] Weak - related papers do not discuss out-of-domain demonstration usage.
- Break condition: The domain shift is too large, causing the VLM's embeddings to lose task-relevant information.

## Foundational Learning

- Concept: Video-and-Language Models (VLMs)
  - Why needed here: VLMs provide a way to embed videos and text into a shared semantic space, allowing for similarity-based reward generation.
  - Quick check question: What is the primary advantage of using VLMs for reward generation in RoboCLIP?

- Concept: Reinforcement Learning (RL)
  - Why needed here: RL algorithms are used to train the agent policy using the rewards generated by RoboCLIP.
  - Quick check question: What is the role of the reward function in RL, and how does RoboCLIP's approach differ from traditional methods?

- Concept: Imitation Learning (IL)
  - Why needed here: IL methods attempt to learn policies from demonstrations, which is the core idea behind RoboCLIP's use of a single demonstration.
  - Quick check question: How does RoboCLIP's approach to using a single demonstration differ from traditional IL methods?

## Architecture Onboarding

- Component map:
  - S3D VLM (pretrained on HowTo100M) -> Task descriptor (text or video) -> Agent environment and policy -> Reward generation module (similarity score calculation) -> RL algorithm (e.g., PPO)

- Critical path:
  1. Agent interacts with environment, generating a trajectory.
  2. Trajectory and task descriptor are embedded using S3D.
  3. Similarity score between embeddings is calculated.
  4. Similarity score is used as a reward signal for the RL algorithm.
  5. RL algorithm updates the agent policy.

- Design tradeoffs:
  - Using a single demonstration vs. multiple demonstrations for reward generation.
  - Using out-of-domain demonstrations vs. in-domain demonstrations.
  - Using text vs. video for task specification.

- Failure signatures:
  - Poor task performance: The VLM's embeddings may not align with the robotic domain.
  - Unstable training: The reward signal may be too sparse or noisy.
  - Slow convergence: The RL algorithm may struggle to learn from the sparse reward signal.

- First 3 experiments:
  1. Test RoboCLIP on a simple task with a single in-domain demonstration to verify basic functionality.
  2. Test RoboCLIP with out-of-domain demonstrations to assess domain generalization.
  3. Compare RoboCLIP's performance with traditional IL methods to highlight the benefits of using VLMs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can RoboCLIP be effectively scaled to longer horizon tasks without suffering from instability during finetuning?
- Basis in paper: [explicit] The paper mentions that there is currently no fixed length of pretraining and that finetuning on downstream task reward can result in instabilities due to the scale of rewards provided by the VLM.
- Why unresolved: The paper acknowledges this as a limitation but does not provide a solution or extensive testing on longer horizon tasks.
- What evidence would resolve it: Experiments demonstrating stable finetuning performance on tasks with longer time horizons, along with a proposed method to address the reward scale issue.

### Open Question 2
- Question: How do the biases inherent in pretrained Video-and-Language Models (VLMs) affect the performance and generalization of policies trained with RoboCLIP?
- Basis in paper: [explicit] The paper discusses the potential for implicit biases in VLMs to percolate into RL agents and acknowledges the need to address such challenges.
- Why unresolved: The paper does not provide an analysis of specific biases or their impact on policy performance and generalization.
- What evidence would resolve it: A study quantifying the biases in VLMs and their effects on trained policies, along with methods to mitigate these biases.

### Open Question 3
- Question: Is it possible to develop a principled approach to determine the optimal pretraining duration for RoboCLIP in real-world applications?
- Basis in paper: [explicit] The paper states that there is no fixed length of pretraining and that picking the best model according to the true task reward is difficult in real-world setups.
- Why unresolved: The paper does not propose a method for determining the optimal pretraining duration or address the challenge of monitoring progress without access to a true reward function.
- What evidence would resolve it: A framework or algorithm that can adaptively determine the optimal pretraining duration based on performance metrics or other criteria, validated in real-world scenarios.

## Limitations

- The approach relies heavily on the S3D model's ability to generalize across domains, which may not hold for all robotic tasks
- The reward signal is sparse (computed only at episode end), which could limit performance on more complex, temporally-extended tasks
- Evaluation focuses primarily on relatively simple manipulation tasks in Metaworld and Franka Kitchen environments

## Confidence

Medium - The method demonstrates strong empirical results with 2-3Ã— higher zero-shot performance than competing IL methods using only one demonstration. However, several limitations affect generalizability including heavy reliance on VLM cross-domain generalization and sparse reward signals.

## Next Checks

1. Test RoboCLIP's robustness to domain shifts by evaluating on robotic tasks significantly different from HowTo100M training data (e.g., novel object manipulation or locomotion tasks).

2. Compare RoboCLIP's sample efficiency against state-of-the-art IL methods when given multiple demonstrations, to quantify the marginal benefit of using VLMs versus traditional IL approaches.

3. Evaluate the stability and convergence properties of RoboCLIP across different RL algorithms (beyond PPO) to assess the generality of the approach to various training paradigms.