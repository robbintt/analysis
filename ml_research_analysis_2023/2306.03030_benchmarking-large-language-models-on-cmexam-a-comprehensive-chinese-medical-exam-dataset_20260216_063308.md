---
ver: rpa2
title: Benchmarking Large Language Models on CMExam -- A Comprehensive Chinese Medical
  Exam Dataset
arxiv_id: '2306.03030'
source_url: https://arxiv.org/abs/2306.03030
tags:
- medical
- cmexam
- questions
- llms
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CMExam, a large-scale Chinese medical exam
  dataset sourced from the Chinese National Medical Licensing Examination. It contains
  over 60K multiple-choice questions with detailed explanations, along with five additional
  annotation dimensions (disease groups, clinical departments, medical disciplines,
  areas of competency, and difficulty levels) labeled by medical professionals.
---

# Benchmarking Large Language Models on CMExam -- A Comprehensive Chinese Medical Exam Dataset

## Quick Facts
- arXiv ID: 2306.03030
- Source URL: https://arxiv.org/abs/2306.03030
- Reference count: 18
- GPT-4 achieves 61.5% accuracy on CMExam, lagging behind human performance of 71.6%

## Executive Summary
This paper introduces CMExam, a large-scale Chinese medical exam dataset sourced from the Chinese National Medical Licensing Examination, containing over 60K multiple-choice questions with detailed explanations and five additional annotation dimensions. The authors conduct comprehensive benchmarking experiments using various large language models (LLMs) including GPT-4, GPT-3.5, and medical domain models like Huatuo and DoctorGLM. GPT-4 demonstrates superior zero-shot performance compared to other models, while finetuning lightweight LLMs on CMExam significantly improves their performance. The results highlight substantial room for improvement in Chinese medical QA, with GPT-4 achieving 61.5% accuracy compared to human performance of 71.6%.

## Method Summary
The study benchmarks large language models on CMExam, a Chinese medical exam dataset with 68,119 questions including multiple-choice format, solution explanations, and five annotation dimensions. The evaluation pipeline involves dataset preprocessing, annotation (manual and GPT-assisted), model selection across general and medical domain LLMs, fine-tuning processes for open-source models, and benchmarking using multiple metrics. Both zero-shot and fine-tuned approaches are tested, with GPT-4 achieving the highest accuracy of 61.5% on answer prediction tasks. Human performance serves as a reference at 71.6% accuracy.

## Key Results
- GPT-4 achieves highest accuracy of 61.5% and weighted F1 score of 0.617 on answer prediction task
- Human performance reference of 71.6% accuracy indicates substantial room for improvement
- Finetuning lightweight LLMs like ChatGLM on CMExam improves performance, with ChatGLM-CMExam achieving 46.3% accuracy
- GPT-4's explanations show reasonable quality despite low BLUE/ROUGE scores due to generation of short, relevant answers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 demonstrates superior zero-shot performance compared to other LLMs on medical question-answering tasks.
- Mechanism: GPT-4 leverages its broad pretraining on diverse web data to understand complex medical concepts and reasoning patterns, even without medical domain-specific training.
- Core assumption: General language understanding transfers effectively to specialized medical knowledge domains.
- Evidence anchors:
  - [abstract] "GPT-4 had the best accuracy of 61.5% and a weighted F1 score of 0.616"
  - [section] "GPT-4 demonstrates impressive zero-shot performance on the answer prediction task compared to other models"
  - [corpus] Weak - no direct comparison of pretraining data breadth vs medical models
- Break condition: When medical questions require deep domain-specific knowledge not well-represented in general web corpora.

### Mechanism 2
- Claim: Finetuning lightweight LLMs on CMExam significantly improves their performance on medical QA tasks.
- Mechanism: Supervised learning on domain-specific data allows smaller models to acquire medical knowledge patterns that general pretraining cannot provide.
- Core assumption: Medical knowledge can be effectively distilled from large-scale exam data into smaller model architectures.
- Evidence anchors:
  - [section] "ChatGLM-CMExam surpasses GPT-3.5's performance, despite utilizing only about 3% of the parameters"
  - [section] "ChatGLM-CMExam achieved scores of 30.9 and 18.85 on BLUE-1 and BLUE-4, respectively"
  - [corpus] Weak - no comparison of finetuning vs other adaptation methods
- Break condition: When finetuning data lacks diversity or contains biases that limit generalization.

### Mechanism 3
- Claim: GPT-4's explanations have reasonable quality despite low BLUE/ROUGE scores due to generation of short, relevant answers.
- Mechanism: GPT-4 prioritizes conciseness and relevance over completeness, leading to shorter outputs that score poorly on n-gram overlap metrics.
- Core assumption: BLUE/ROUGE scores do not fully capture explanation quality for medical reasoning tasks.
- Evidence anchors:
  - [section] "GPT-3.5 and GPT-4 generated reasonable answers on the answer reasoning task despite low BLUE and ROUGE scores"
  - [section] "They tend to generate short answers with reasonable quality, which may have led to their low BLUE scores"
  - [corpus] Weak - no direct analysis of answer length vs quality correlation
- Break condition: When medical explanations require comprehensive coverage that short answers cannot provide.

## Foundational Learning

- Concept: Medical knowledge representation and reasoning
  - Why needed here: Understanding how different LLMs encode and apply medical knowledge is crucial for interpreting benchmark results and designing better models.
  - Quick check question: What are the key differences between knowledge-based and reasoning-based medical question answering?

- Concept: Evaluation metrics for language models
  - Why needed here: Interpreting the benchmark results requires understanding the strengths and limitations of accuracy, F1, BLUE, and ROUGE metrics in medical contexts.
  - Quick check question: When would BLUE/ROUGE scores be misleading indicators of explanation quality?

- Concept: Zero-shot vs finetuned learning paradigms
  - Why needed here: The paper compares models across different training approaches, requiring understanding of when each paradigm is most effective.
  - Quick check question: What factors determine whether zero-shot or finetuned approaches will perform better on a given task?

## Architecture Onboarding

- Component map: Data preparation → Model selection/adaptation → Benchmarking execution → Results analysis across dimensions (disease groups, departments, disciplines, competencies, difficulty)
- Critical path: Data preparation → Model selection/adaptation → Benchmarking execution → Results analysis across dimensions
- Design tradeoffs: General LLMs offer broad knowledge but lack medical specificity; medical models have domain focus but limited coverage; finetuning improves performance but requires computational resources
- Failure signatures: Poor performance on rare disease groups indicates insufficient coverage; inconsistent results across departments suggest departmental knowledge gaps; low finetuning gains indicate data quality issues
- First 3 experiments:
  1. Compare zero-shot performance of general LLMs vs medical domain models on a small subset of questions
  2. Finetune a lightweight model on CMExam and evaluate improvement on answer prediction
  3. Analyze correlation between question length and model accuracy across different model types

## Open Questions the Paper Calls Out

- How does the performance of GPT-4 compare to human performance on CMExam, and what specific types of questions or categories show the largest performance gaps?
- What specific limitations of existing medical domain LLMs like Huatuo and DoctorGLM lead to their poor zero-shot performance on CMExam?
- How does the performance of lightweight LLMs like ChatGLM change when fine-tuned on CMExam compared to general domain LLMs, and what specific aspects of medical knowledge do they struggle with most?
- How effective is the GPT-Assisted Annotation approach compared to traditional manual annotation, and what are its limitations for medical datasets?
- What specific improvements in explanation quality are observed after fine-tuning medical LLMs on CMExam, and how do these improvements compare to improvements in answer selection accuracy?

## Limitations

- Dataset quality and representativeness may be biased toward exam-style questions that don't fully represent real-world medical practice
- Human performance baseline methodology is not detailed, making comparison to model performance potentially misleading
- Performance on exam questions may not translate well to other medical QA tasks like clinical decision support

## Confidence

**High Confidence**:
- GPT-4 outperforms other tested models on answer prediction tasks
- Finetuning lightweight models on CMExam improves their performance
- Medical domain models (Huatuo, DoctorGLM) perform below general LLMs on zero-shot tasks

**Medium Confidence**:
- GPT-4's explanations are of reasonable quality despite low BLUE/ROUGE scores
- The five annotation dimensions provide meaningful evaluation insights
- Performance differences across medical specialties reflect genuine knowledge gaps

**Low Confidence**:
- BLUE/ROUGE scores accurately reflect explanation quality for medical reasoning tasks
- The finetuning approach used is optimal compared to other adaptation strategies
- Performance improvements from finetuning will scale linearly with additional training data

## Next Checks

1. **Human Performance Validation**: Replicate the human baseline measurement using multiple medical professionals with established inter-annotator agreement metrics to verify the 71.6% accuracy claim.

2. **Cross-Domain Transfer Test**: Evaluate model performance on a held-out subset of questions from medical specialties not well-represented in the training data to assess generalization capabilities beyond the dataset distribution.

3. **Alternative Evaluation Metrics**: Implement additional evaluation methods for explanations (e.g., medical expert ratings, coherence measures) to determine whether BLUE/ROUGE scores underestimate explanation quality for medical reasoning tasks.