---
ver: rpa2
title: VBD-MT Chinese-Vietnamese Translation Systems for VLSP 2022
arxiv_id: '2308.07601'
source_url: https://arxiv.org/abs/2308.07601
tags:
- translation
- systems
- data
- which
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents machine translation systems for the Chinese-Vietnamese
  and Vietnamese-Chinese tasks in the VLSP 2022 shared task. The authors build their
  systems using the Transformer model with mBART multilingual pre-training, enhanced
  by backtranslation using monolingual data and post-processing to correct specific
  translation errors.
---

# VBD-MT Chinese-Vietnamese Translation Systems for VLSP 2022

## Quick Facts
- arXiv ID: 2308.07601
- Source URL: https://arxiv.org/abs/2308.07601
- Reference count: 13
- Key outcome: VBD-MT systems achieved 38.9 BLEU on Chinese-Vietnamese and 38.0 BLEU on Vietnamese-Chinese translation tasks using Transformer with mBART pre-training, backtranslation, and post-processing

## Executive Summary
This paper presents machine translation systems for the Chinese-Vietnamese and Vietnamese-Chinese tasks in the VLSP 2022 shared task. The authors build their systems using the Transformer model with mBART multilingual pre-training, enhanced by backtranslation using monolingual data and post-processing to correct specific translation errors. The submitted systems achieved 38.9 BLEU on Chinese-Vietnamese and 38.0 BLEU on Vietnamese-Chinese translation tasks on public test sets, outperforming several strong baseline systems.

## Method Summary
The authors employ a Transformer model initialized with mBART-25 multilingual pre-training. To address GPU memory constraints, they prune the vocabulary from 250K to 67K tokens based on the bilingual and monolingual training data. Backtranslation is performed using top-k sampling (k=5) to generate synthetic parallel data from monolingual corpora. The final systems use checkpoint averaging from the last 5 training iterations and include post-processing rules to correct systematic errors in numbers and dates. The approach demonstrates effective solutions for low-resource MT scenarios with limited computational resources.

## Key Results
- Achieved 38.9 BLEU on Chinese-Vietnamese translation task
- Achieved 38.0 BLEU on Vietnamese-Chinese translation task
- Backtranslation improved performance by +0.8 BLEU points
- Vocabulary pruning enabled training on 11GB GPU memory

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pruning mBART vocabulary to match bilingual/monolingual data improves training feasibility and model performance on the target domain.
- Mechanism: By filtering the 250K mBART vocabulary to only the tokens appearing in the 300K bilingual + 6M monolingual data (67K tokens), the model fits in 11GB GPU memory, enabling training that is otherwise infeasible. The smaller, domain-specific vocabulary also reduces noise from irrelevant tokens, improving translation quality.
- Core assumption: Domain-relevant vocabulary pruning improves model fit and performance more than it hurts due to loss of general multilingual capability.
- Evidence anchors: The authors demonstrate that pruning reduces vocabulary size by 4x and enables training without more powerful servers.

### Mechanism 2
- Claim: Backtranslation with top-k sampling (k=5) improves translation quality by providing diverse synthetic training data.
- Mechanism: The baseline model translates 1.5M monolingual sentences using top-k sampling to generate 211K (zh-vi) and 403K (vi-zh) synthetic parallel pairs. This introduces more diverse source sentences than beam search, providing richer training signals that improve BLEU scores.
- Core assumption: Diverse synthetic data from top-k sampling is more beneficial than deterministic beam search outputs for low-resource MT.
- Evidence anchors: The authors show a +0.8 BLEU improvement when leveraging backtranslation.

### Mechanism 3
- Claim: Post-processing corrects systematic translation errors in numbers and dates that BLEU doesn't penalize but affect human judgment.
- Mechanism: Manual analysis revealed systematic errors in translating numbers (e.g., 400亿 as 4 billion instead of 40 billion) and dates. Simple pattern-based post-processing rules fix these specific errors, improving human-evaluated translation quality even when BLEU remains unchanged.
- Core assumption: Simple regex-style pattern matching can reliably correct systematic errors without introducing new errors.
- Evidence anchors: The authors provide corrected examples showing improved translations for numeric values and date formatting.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The entire system is built on Transformer models, and understanding attention is crucial for debugging and improving translation quality.
  - Quick check question: What happens to translation quality if you remove the encoder-decoder attention layer in a Transformer MT model?

- Concept: Backtranslation and data augmentation strategies
  - Why needed here: Backtranslation is a core technique used to improve performance on low-resource language pairs by leveraging monolingual data.
  - Quick check question: How does top-k sampling in backtranslation differ from beam search in terms of the diversity of synthetic data generated?

- Concept: BLEU evaluation and its limitations
  - Why needed here: The paper uses BLEU as the primary evaluation metric, but human evaluation reveals its limitations in capturing translation quality nuances.
  - Quick check question: Why might a post-processing step that improves human judgment not show BLEU improvement?

## Architecture Onboarding

- Component map: SentencePiece tokenization → Vocabulary pruning → Transformer training → Backtranslation generation → Ensemble averaging → Post-processing → Evaluation
- Critical path: Bilingual data → mBART finetuning → backtranslation → ensemble → post-processing → evaluation
- Design tradeoffs:
  - Vocabulary pruning vs. full multilingual capability
  - Top-k sampling vs. beam search diversity vs. quality
  - Ensemble size vs. computational cost
  - Pattern complexity vs. error correction coverage
- Failure signatures:
  - Out-of-memory errors during training → check vocabulary size and pruning
  - Degraded BLEU after backtranslation → check monolingual data quality and sampling parameters
  - Post-processing introducing errors → check pattern specificity and overlap
- First 3 experiments:
  1. Baseline training without vocabulary pruning to measure performance impact
  2. Backtranslation with beam search vs. top-k sampling to quantify diversity benefits
  3. Different ensemble sizes (N=3,5,7) to find optimal tradeoff between performance and cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the Chinese-Vietnamese translation system change when using the full mBART vocabulary instead of pruning it to fit GPU memory constraints?
- Basis in paper: The paper mentions that pruning the mBART vocabulary from 250K to 67K was done due to GPU memory limitations, but notes that performance might be better with the full vocabulary.
- Why unresolved: The authors did not have access to more powerful computational resources to test the full vocabulary model.
- What evidence would resolve it: Experimental results comparing the pruned and full vocabulary models on the same test sets would resolve this question.

### Open Question 2
- Question: What is the impact of using the entire monolingual dataset (19M Chinese and 25M Vietnamese sentences) for backtranslation instead of the subset (1.5M sentences) used in this study?
- Basis in paper: The paper states that only a subset of the monolingual data was used due to time limitations and that the full dataset has not been completely utilized.
- Why unresolved: Time constraints prevented the use of the full monolingual dataset in the experiments.
- What evidence would resolve it: Comparative results using different amounts of backtranslated data from the monolingual corpus would resolve this question.

### Open Question 3
- Question: How effective would integrating named entity recognition (NER) be in improving the translation of specific data types like person names and numeric values compared to the simple post-processing approach used in this study?
- Basis in paper: The paper mentions that translating numbers is challenging and that named entity tasks could solve some issues, but this was left for future work.
- Why unresolved: The authors chose to implement a simpler post-processing approach and did not integrate NER due to scope limitations.
- What evidence would resolve it: Comparative results between the post-processing approach and an integrated NER system would resolve this question.

### Open Question 4
- Question: What is the optimal ensemble size (N) for achieving the best translation performance in the Chinese-Vietnamese and Vietnamese-Chinese translation tasks?
- Basis in paper: The paper mentions that ensembling weights using the average of the last N checkpoints was used, with N=5 found to be optimal, but does not explore other values of N.
- Why unresolved: The study only tested N=5 and did not explore a range of values to determine the optimal ensemble size.
- What evidence would resolve it: Results from experiments testing different values of N for ensembling would resolve this question.

## Limitations
- Vocabulary pruning may reduce model capability for out-of-domain inputs or rare words
- Post-processing rules represent a brittle solution requiring manual pattern engineering
- Effectiveness of top-k sampling vs. beam search not empirically validated

## Confidence
- **High confidence**: The core methodology (Transformer with mBART pre-training, backtranslation workflow) is well-established and the implementation details are sufficiently documented for reproduction.
- **Medium confidence**: The effectiveness of vocabulary pruning is supported by empirical evidence but lacks ablation studies comparing pruned vs. full vocabulary performance.
- **Low confidence**: The claim that top-k sampling provides richer training signals than beam search is stated but not empirically validated against alternative sampling strategies.

## Next Checks
1. **Vocabulary pruning ablation**: Train identical models with full mBART-25 vocabulary versus the pruned 67K vocabulary to quantify the exact performance tradeoff between domain specificity and general multilingual capability.
2. **Post-processing error analysis**: Conduct a systematic evaluation of the post-processing rules on a held-out validation set to measure both correction coverage and false positive rates.
3. **Monolingual data diversity impact**: Experiment with different sampling strategies for backtranslation (beam search, nucleus sampling, temperature scaling) and measure the correlation between synthetic data diversity and translation quality improvements.