---
ver: rpa2
title: Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs
arxiv_id: '2305.14279'
source_url: https://arxiv.org/abs/2305.14279
tags:
- consistency
- prompt
- compositional
- hypothetical
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Large language models exhibit poor self-consistency across two
  types of transformations important for multi-step reasoning: hypothetical consistency
  (ability to predict own outputs in other contexts) and compositional consistency
  (consistency when intermediate steps are replaced with model''s own outputs). Experiments
  on four GPT-3 model sizes across four tasks (Wikipedia, DailyDialog, arithmetic,
  GeoQuery) show all models except the largest (text-davinci-003) perform near random
  chance on hypothetical consistency.'
---

# Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs

## Quick Facts
- arXiv ID: 2305.14279
- Source URL: https://arxiv.org/abs/2305.14279
- Reference count: 11
- Key outcome: Large language models exhibit poor self-consistency across two types of transformations important for multi-step reasoning

## Executive Summary
This paper identifies two critical failures of self-consistency in large language models (LLMs) when performing multi-step reasoning: hypothetical consistency and compositional consistency. The authors test four GPT-3 model sizes across four tasks and find that all models except the largest (text-davinci-003) perform near random chance on hypothetical consistency, while even text-davinci-003 achieves only 26-37% accuracy. Compositional consistency rates are similarly low across all models, with text-davinci-003 achieving less than 30% on arithmetic and less than 50% on GeoQuery tasks. These results suggest that despite impressive few-shot performance, LLMs cannot be reliably trusted for complex compositional reasoning tasks without extensive empirical validation.

## Method Summary
The study evaluates self-consistency in four GPT-3 model sizes (350M, 1B, 6.7B, and 175B parameters) across four tasks: Wikipedia articles, DailyDialog, synthetic arithmetic expressions, and GeoQuery. For hypothetical consistency, models are given multiple-choice prompts where one answer is their own completion. For compositional consistency, models must maintain logical coherence when intermediate reasoning steps are replaced with their own outputs. Consistency rates are calculated by comparing model outputs under transformed prompts against ground truth answers using greedy decoding via the OpenAI API with k-shot examples ranging from 3-10 depending on the task.

## Key Results
- All GPT-3 models except text-davinci-003 perform near random chance (20%) on hypothetical consistency tasks
- text-davinci-003 achieves only 26-37% accuracy on hypothetical consistency across tasks
- Compositional consistency rates are less than 30% for arithmetic and less than 50% for GeoQuery even for text-davinci-003
- There is poor correlation between correctness and compositional consistency across all models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hypothetical consistency requires models to maintain internal representations of their own reasoning process across different prompt formulations
- Mechanism: The model must store or compute its response to a prompt p without being explicitly prompted with p itself, using knowledge of how it would respond to that prompt
- Core assumption: The model has some computational sub-graph or stored knowledge that allows it to predict its own outputs for given prompts
- Break condition: When the model cannot access or compute its own reasoning process for a given prompt, resulting in random choice selection

### Mechanism 2
- Claim: Compositional consistency requires models to maintain logical coherence when intermediate reasoning steps are replaced with their own outputs
- Mechanism: The model must be able to substitute its own outputs for intermediate steps in a reasoning chain and still arrive at the same final answer
- Core assumption: The model's reasoning process is modular and can handle substitution of intermediate results without breaking logical flow
- Break condition: When the model's reasoning process is not modular enough to handle substitution of intermediate outputs, leading to inconsistent final answers

### Mechanism 3
- Claim: Model size affects consistency performance due to increased parameter capacity and reasoning ability
- Mechanism: Larger models have more parameters that can store and compute more complex reasoning processes, leading to better consistency
- Core assumption: Parameter count is positively correlated with the model's ability to maintain internal consistency across transformations
- Break condition: When model size increases beyond a threshold where additional parameters no longer improve consistency, or when other factors dominate

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: Understanding how multi-step reasoning works in LLMs is crucial for grasping why consistency failures occur
  - Quick check question: How does chain-of-thought prompting differ from direct prompting in terms of model reasoning process?

- Concept: Prompt engineering and transformations
  - Why needed here: The paper relies heavily on understanding how different prompt formulations affect model outputs
  - Quick check question: What are the key differences between hypothetical and compositional transformations in terms of their effect on model reasoning?

- Concept: Evaluation metrics for consistency
  - Why needed here: Understanding how consistency is measured is crucial for interpreting the results
  - Quick check question: How does multiple-choice accuracy serve as a measure of consistency in the hypothetical consistency experiments?

## Architecture Onboarding

- Component map: Input prompt processing -> Model inference engine (GPT-3 variants) -> Output parsing and answer selection -> Consistency calculation module -> Result aggregation and visualization

- Critical path: 1. Generate initial prompts from datasets 2. Create hypothetical and compositional transformations 3. Run model inference on all prompts 4. Parse and compare outputs for consistency 5. Calculate consistency rates and aggregate results

- Design tradeoffs: Model size vs. computational cost, Number of in-context examples vs. consistency improvement, Dataset selection (Wikipedia vs. DailyDialog) for different aspects of reasoning, Random chance baseline vs. more sophisticated baselines

- Failure signatures: Random choice selection across all model sizes (indicating lack of internal consistency mechanism), Inconsistent improvement with increasing model size, Poor correlation between correctness and consistency, Dataset-specific consistency failures

- First 3 experiments: 1. Run hypothetical consistency on a small subset of Wikipedia prompts with text-davinci-003 to verify the core finding 2. Test compositional consistency on synthetic arithmetic expressions with increasing complexity 3. Compare consistency rates across different numbers of in-context examples on GeoQuery to understand the effect of few-shot learning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training modifications could improve hypothetical consistency in LLMs across all model sizes?
- Basis in paper: The paper demonstrates that all model sizes except text-davinci-003 perform near random chance on hypothetical consistency tasks, with even the largest model achieving only 26-37% accuracy.
- Why unresolved: The paper identifies the problem but does not propose specific architectural or training modifications to address it. It only notes that larger model size appears to help.
- What evidence would resolve it: Comparative experiments testing specific architectural modifications (e.g., different attention mechanisms, additional consistency regularization during training) or training approaches (e.g., self-consistency objectives) against the baseline GPT-3 models on the same hypothetical consistency tasks.

### Open Question 2
- Question: Is there a fundamental theoretical limit to the self-consistency that LLMs can achieve, or can consistency be improved arbitrarily through scaling and better training methods?
- Basis in paper: The paper shows that even the largest tested model (text-davinci-003) has low consistency rates (26-37% hypothetical, <30% compositional on arithmetic), and notes that "Further work is required in order to improve the logical consistency of LLM reasoning, and to investigate whether further scaling improves hypothetical or compositional consistency."
- Why unresolved: The paper demonstrates poor consistency but does not explore theoretical limits or conduct scaling experiments beyond the four tested model sizes to determine if consistency improves with additional scaling.
- What evidence would resolve it: Systematic scaling experiments across many orders of magnitude of model parameters, combined with theoretical analysis of the consistency problem as a function of model capacity and training data.

### Open Question 3
- Question: How does self-consistency relate to other measures of reasoning capability in LLMs, such as logical reasoning, commonsense reasoning, and causal reasoning?
- Basis in paper: The paper focuses on self-consistency as a measure of valid multi-step reasoning but does not compare it to other reasoning benchmarks or explore how consistency relates to other reasoning capabilities.
- Why unresolved: The paper establishes self-consistency as an important criteria but does not investigate its relationship to broader reasoning abilities or whether consistency improvements would transfer to other reasoning tasks.
- What evidence would resolve it: Correlation studies between consistency measures and performance on established reasoning benchmarks, along with ablation studies showing how consistency improvements affect performance on diverse reasoning tasks.

## Limitations

- The semantic equivalence measure (∼) used for consistency evaluation is not precisely defined, creating potential ambiguity in result interpretation
- The study does not explore whether fine-tuning or alternative prompting strategies could improve consistency rates
- The findings may be specific to GPT-3 architecture and may not generalize to other LLM architectures

## Confidence

- **High Confidence**: The experimental methodology is sound and the results showing low consistency rates across multiple model sizes and tasks are robust
- **Medium Confidence**: The interpretation that these consistency failures indicate fundamental limitations in LLM reasoning architecture
- **Low Confidence**: The generalizability of these findings to other LLM architectures beyond GPT-3

## Next Checks

1. **Semantic Equivalence Validation**: Conduct a human evaluation study where annotators assess whether model outputs that differ from ground truth are truly semantically equivalent, to validate the ∼ operator's effectiveness and ensure consistency rates aren't artificially deflated by overly strict equivalence criteria.

2. **Architecture Generalization Test**: Replicate the consistency experiments using different LLM architectures (e.g., LLaMA, PaLM) to determine whether the observed consistency failures are specific to GPT-3 or represent a broader limitation across transformer-based models.

3. **Intervention Effectiveness Study**: Test whether fine-tuning on consistency-focused datasets or employing advanced prompting strategies (like least-to-most prompting or tree-of-thought reasoning) can improve both hypothetical and compositional consistency rates, helping distinguish between fundamental architectural limitations and addressable engineering challenges.