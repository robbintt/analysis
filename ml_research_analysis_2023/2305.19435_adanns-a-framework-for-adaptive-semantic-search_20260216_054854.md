---
ver: rpa2
title: 'AdANNS: A Framework for Adaptive Semantic Search'
arxiv_id: '2305.19435'
source_url: https://arxiv.org/abs/2305.19435
tags:
- uni00000013
- search
- uni00000014
- uni00000015
- uni00000019
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AdANNS is a framework for adaptive semantic search that leverages
  matryoshka representations (MRs) to improve accuracy-compute trade-offs in approximate
  nearest neighbor search (ANNS). By using different-capacity representations for
  different stages of ANNS pipelines, AdANNS achieves significantly better performance
  than traditional methods that use rigid representations throughout.
---

# AdANNS: A Framework for Adaptive Semantic Search

## Quick Facts
- arXiv ID: 2305.19435
- Source URL: https://arxiv.org/abs/2305.19435
- Reference count: 40
- Key outcome: AdANNS improves accuracy-compute trade-offs in ANNS by using matryoshka representations with different dimensionalities for different pipeline stages, achieving up to 1.5% better accuracy at same compute budget.

## Executive Summary
AdANNS is a framework that enhances approximate nearest neighbor search (ANNS) by leveraging matryoshka representations (MRs), which have a nested structure allowing different dimensionalities to be used for different stages of the ANNS pipeline. By using lower-dimensional MRs for clustering and higher-dimensional MRs for re-ranking, AdANNS achieves significantly better accuracy-compute trade-offs than traditional methods that use rigid representations throughout. The framework introduces novel building blocks like AdANNS-IVF (search data structures) and AdANNS-OPQ (quantization), demonstrating up to 1.5% better accuracy at the same compute budget compared to IVF on ImageNet retrieval, and matching 64-byte OPQ accuracy with only 32 bytes on Natural Questions. AdANNS generalizes to composite indices and enables compute-aware elastic search during inference without modifications.

## Method Summary
AdANNS leverages matryoshka representations (MRs) that encode information hierarchically across dimensions, allowing different-dimensional prefixes to be used for different ANNS pipeline stages. The framework introduces three main building blocks: AdANNS-IVF uses different dimensionalities for clustering (dc) and linear scan (ds) phases; AdANNS-OPQ applies product quantization to lower-dimensional MR prefixes; and composite indices combine these approaches. By searching the design space of dc, ds, and k (number of clusters), AdANNS finds optimal configurations that significantly improve accuracy-compute trade-offs compared to traditional ANNS methods using rigid representations throughout.

## Key Results
- AdANNS-IVF achieves up to 1.5% better accuracy at the same compute budget compared to IVF on ImageNet retrieval
- AdANNS-OPQ matches 64-byte OPQ accuracy with only 32 bytes on Natural Questions
- AdANNS generalizes to composite indices, combining gains from both adaptive search structures and quantization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AdANNS improves accuracy-compute trade-offs by using different dimensionalities of matryoshka representations for different stages of the ANNS pipeline
- Mechanism: Matryoshka representations have a nested structure where lower-dimensional prefixes contain accurate semantic information. By using lower-dimensional MR for clustering and higher-dimensional MR for linear scan, AdANNS achieves better accuracy with less computation than using rigid representations throughout
- Core assumption: Lower-dimensional MR prefixes preserve sufficient semantic information for coarse clustering while maintaining distance relationships
- Evidence anchors: Abstract states "By using different-capacity representations for different stages of ANNS pipelines, AdANNS achieves significantly better performance than traditional methods that use rigid representations throughout"; section 4.1 explains the dimensional separation between clustering and linear scan phases

### Mechanism 2
- Claim: AdANNS-OPQ achieves better compression by applying product quantization to lower-dimensional MRs
- Mechanism: Instead of quantizing full-dimensional rigid representations, AdANNS-OPQ applies OPQ to lower-dimensional MR prefixes, achieving the same accuracy at lower bitrates due to better information packing in MRs
- Core assumption: Information is hierarchically packed in MRs such that lower dimensions contain more semantically important features
- Evidence anchors: Abstract states "32-byte AdANNS-OPQ matches the accuracy of the 64-byte OPQ baseline... same accuracy at half the cost!"; section 4.2 describes applying OPQ to lower-dimensional MR representations

### Mechanism 3
- Claim: AdANNS generalizes to composite indices by combining adaptive representations across multiple ANNS building blocks
- Mechanism: By applying AdANNS principles to both the search data structure (IVF) and quantization (OPQ) components separately, composite indices like AdANNS-IVFOPQ achieve multiplicative improvements in accuracy-compute trade-offs
- Core assumption: Improvements from using MRs in individual ANNS components are additive when combined in composite indices
- Evidence anchors: Abstract mentions "AdANNS generalizes to modern-day composite ANNS indices that combine search structures and quantization"; section 4.3 discusses combining gains of AdANNS for IVF and OPQ

## Foundational Learning

- Concept: Approximate Nearest Neighbor Search (ANNS) fundamentals
  - Why needed here: Understanding the baseline IVF and OPQ methods is crucial to appreciate how AdANNS modifies them
  - Quick check question: What are the two main phases of IVF and how does each contribute to the overall search process?

- Concept: Matryoshka representations and nested information packing
  - Why needed here: The core innovation relies on understanding how MRs encode information hierarchically across dimensions
  - Quick check question: How does the prefix property of MRs enable different dimensionalities to serve different ANNS pipeline stages?

- Concept: Product quantization and OPQ mechanisms
  - Why needed here: Understanding how OPQ compresses vectors is essential to grasp why applying it to MRs yields benefits
  - Quick check question: What is the relationship between vector dimensionality, number of sub-vectors, and codebook size in OPQ?

## Architecture Onboarding

- Component map: MR Encoder -> IVF Index Builder (dc) -> IVF Search Engine (ds) -> OPQ Quantizer (dq) -> Final retrieval
- Critical path: MR encoding → IVF construction (dc) → IVF search (ds) → OPQ quantization (dq) → Final retrieval
- Design tradeoffs:
  - dc vs ds: Lower dc improves clustering efficiency but may hurt accuracy; higher ds improves precision but increases cost
  - OPQ bitrate vs dimensionality: Lower bitrates save memory but may reduce accuracy; higher dimensional MRs provide better quantization
  - Flexibility vs complexity: AdANNS offers more configuration options but requires searching the design space
- Failure signatures:
  - Accuracy drops when dc is too low for effective clustering
  - Performance degradation when OPQ is applied to very low-dimensional MRs
  - Suboptimal results when MR encoder wasn't trained with MRL objective
- First 3 experiments:
  1. Reproduce Figure 2: Compare IVF-MR, IVF-RR, and AdANNS-IVF on ImageNet-1K with varying dc/ds configurations
  2. Validate Figure 3: Test AdANNS-OPQ against baseline OPQ at different bitrates and dimensionalities
  3. Test composite index: Implement AdANNS-IVFOPQ and measure accuracy-compute improvements over IVFOPQ baseline

## Open Questions the Paper Calls Out

- **Open Question 1**: How can AdANNS be extended to other ANNS data structures beyond IVF, OPQ, and DiskANN?
  - Basis in paper: The authors mention that AdANNS can be extended to other ANNS data structures like ScaNN and HNSW, and they demonstrate this with DiskANN. However, they do not provide detailed results or analysis for these extensions.
  - Why unresolved: The paper focuses on demonstrating the effectiveness of AdANNS with IVF, OPQ, and DiskANN. While the authors suggest that AdANNS can be applied to other ANNS data structures, they do not provide concrete evidence or detailed analysis for these extensions.
  - What evidence would resolve it: Experimental results and analysis demonstrating the effectiveness of AdANNS with other ANNS data structures like ScaNN and HNSW would resolve this question.

- **Open Question 2**: How does the performance of AdANNS vary with different clustering algorithms and parameters?
  - Basis in paper: The authors mention that AdANNS uses k-means clustering for IVF, but they do not explore the impact of using different clustering algorithms or varying the number of clusters.
  - Why unresolved: The paper focuses on demonstrating the effectiveness of AdANNS with k-means clustering and a fixed number of clusters. However, it is unclear how the performance of AdANNS would be affected by using different clustering algorithms or varying the number of clusters.
  - What evidence would resolve it: Experimental results comparing the performance of AdANNS with different clustering algorithms (e.g., hierarchical clustering, DBSCAN) and varying the number of clusters would resolve this question.

- **Open Question 3**: How does AdANNS perform on larger-scale datasets and real-world applications?
  - Basis in paper: The authors evaluate AdANNS on ImageNet-1K and Natural Questions, but these datasets are relatively small compared to real-world web-scale datasets. The authors mention that their findings on ImageNet often translate to real-world progress, but they do not provide concrete evidence for this claim.
  - Why unresolved: The paper focuses on demonstrating the effectiveness of AdANNS on relatively small-scale datasets. However, it is unclear how AdANNS would perform on larger-scale datasets and real-world applications.
  - What evidence would resolve it: Experimental results and analysis demonstrating the performance of AdANNS on larger-scale datasets and real-world applications would resolve this question.

## Limitations
- The optimal configuration search appears computationally expensive and may not scale well to very large datasets
- The framework's effectiveness depends on the quality of matryoshka representations, which requires specialized training
- Results are primarily demonstrated on image and text retrieval tasks, with unclear generalization to other domains

## Confidence
- **High confidence**: The core mechanism of using different-dimensional MRs for different ANNS pipeline stages is well-supported by the empirical results and theoretically sound given MRs' prefix properties
- **Medium confidence**: The quantitative gains (1.5% accuracy improvement, 2x compression) are impressive but may be sensitive to dataset characteristics and implementation details
- **Medium confidence**: The claim that AdANNS generalizes to composite indices is supported by experiments but would benefit from testing on additional index combinations

## Next Checks
1. **Configuration sensitivity analysis**: Test how robust the AdANNS improvements are to different dc/ds choices and whether the exhaustive search is necessary or if heuristics could find near-optimal configurations more efficiently
2. **Cross-dataset generalization**: Evaluate AdANNS on additional datasets with different characteristics (e.g., longer text passages, different image domains) to test the framework's broader applicability
3. **MR encoder dependency**: Test whether the gains persist when using MRs from different encoder architectures or when the MR encoder is fine-tuned on the specific retrieval task, to understand the importance of the MR training objective