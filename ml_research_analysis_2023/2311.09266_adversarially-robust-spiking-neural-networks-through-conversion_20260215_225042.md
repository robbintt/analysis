---
ver: rpa2
title: Adversarially Robust Spiking Neural Networks Through Conversion
arxiv_id: '2311.09266'
source_url: https://arxiv.org/abs/2311.09266
tags:
- robust
- adversarial
- snns
- robustness
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of adversarial robustness in spiking
  neural networks (SNNs) by proposing an adversarially robust ANN-to-SNN conversion
  algorithm. The core idea is to leverage the robustness gains from pre-trained adversarially
  robust ANNs and transfer them to SNNs through a conversion process followed by robust
  finetuning.
---

# Adversarially Robust Spiking Neural Networks Through Conversion

## Quick Facts
- arXiv ID: 2311.09266
- Source URL: https://arxiv.org/abs/2311.09266
- Reference count: 40
- Key outcome: Proposes ANN-to-SNN conversion with robust finetuning, achieving up to 2x larger robustness gains than end-to-end adversarial training methods.

## Executive Summary
This paper addresses the challenge of making spiking neural networks (SNNs) adversarially robust by proposing an efficient conversion method from adversarially trained artificial neural networks (ANNs). The key insight is that robustness gains achieved through ANN adversarial training can be transferred to SNNs via a carefully designed conversion and finetuning process. The method introduces novel techniques for integrating batch normalization parameters from pre-trained robust ANNs into SNN operations and optimizing both weights and thresholds during adversarial finetuning. Extensive evaluations using adaptive ensemble attacks demonstrate state-of-the-art robustness performance on multiple benchmark datasets.

## Method Summary
The proposed method converts a pre-trained adversarially robust ANN into an SNN through a multi-stage process. First, layer-wise firing thresholds are initialized using a threshold-balancing approach based on the ANN's activation statistics. Then, the SNN is adversarially finetuned by optimizing both synaptic weights and thresholds under a TRADES-like loss function with RFGSM-based inner maximization. A novel temporal batch normalization (tdBN) integration technique incorporates the pre-trained ANN's batch-norm affine parameters into SNN operations without omitting these layers. The entire process leverages the robust weight configurations learned by the ANN while adapting them to the spiking domain's temporal dynamics.

## Key Results
- Achieves scalable state-of-the-art adversarial robustness in deep SNNs with low latency
- Outperforms recently introduced end-to-end adversarial training-based algorithms by up to 2x in robustness gains
- Reduces robustness-accuracy trade-offs compared to existing SNN adversarial training methods
- Demonstrates effectiveness across CIFAR-10, CIFAR-100, SVHN, and TinyImageNet datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Robustness transfers from pre-trained ANNs through initialization and finetuning
- Core assumption: ANN's robust weight configurations preserve stability properties when converted to spiking domain
- Evidence anchors: Abstract states method embraces computationally demanding robust learning objectives; section describes adversarial finetuning of thresholds and weights
- Break condition: If ANN robustness relies on architectural features not preserved during conversion or finetuning destabilizes initial robust weights

### Mechanism 2
- Claim: Batch-norm parameter integration improves SNN robustness
- Core assumption: Pre-trained batch-norm affine parameters stabilize feature distributions in SNN
- Evidence anchors: Section introduces approach to incorporate adversarially pre-trained ANN batch-norm parameters; abstract highlights innovation in tdBN integration
- Break condition: If batch-norm parameters are poorly estimated during SNN finetuning or temporal dynamics invalidate original statistics

### Mechanism 3
- Claim: Ensemble adaptive attacks provide reliable robustness evaluation
- Core assumption: SNN gradients are highly sensitive to surrogate gradient choice, requiring adaptive attack ensembles
- Evidence anchors: Section implements ensemble attack strategy with multiple surrogate gradient variants; abstract mentions adaptive adversarial settings accounting for spike-based dynamics
- Break condition: If ensemble attack doesn't cover actual surrogate gradient used during training or becomes computationally infeasible

## Foundational Learning

- Concept: Adversarial training in ANNs (PGD, TRADES, MART)
  - Why needed here: Method relies on robust ANNs as initialization
  - Quick check question: What is the difference between standard AT and TRADES in terms of their loss functions?

- Concept: SNN neuron dynamics (LIF/IF models, spike generation, membrane potential updates)
  - Why needed here: Conversion replaces ANN activations with spiking dynamics
  - Quick check question: How does a leaky integrate-and-fire neuron update its membrane potential over time steps?

- Concept: Batch normalization and temporal batch normalization in SNNs
  - Why needed here: Novel batch-norm integration is key contribution
  - Quick check question: How does temporal batch normalization differ from standard batch normalization in ANNs?

## Architecture Onboarding

- Component map: Pre-trained robust ANN (weights W, batch-norm params φ, ω) -> SNN with spiking neuron layers (weights W, thresholds V_th, tdBN layers) -> Adversarial finetuning loop (RFGSM inner maximization, TRADES-like outer loss) -> Ensemble attack evaluator (multiple surrogate gradients)

- Critical path: 1) Train robust ANN on target dataset, 2) Convert ANN to SNN with threshold calibration and batch-norm integration, 3) Adversarially finetune SNN with RFGSM + KL-based loss, 4) Evaluate with ensemble adaptive attacks

- Design tradeoffs: Direct coding vs. rate coding (direct coding used for low latency), LIF vs. IF neurons (LIF offers richer temporal coding but harder to train), single-step vs. multi-step adversarial examples (single-step RFGSM faster but potentially weaker)

- Failure signatures: Robustness collapses after conversion (threshold miscalibration or batch-norm integration error), training instability during finetuning (learning rate too high or surrogate gradient mismatch), clean accuracy drops significantly (over-regularization or poor initialization)

- First 3 experiments: 1) Convert naturally trained ANN to SNN and evaluate (expect near-zero robustness), 2) Convert AT-ϵ1 trained ANN to SNN and evaluate with ensemble attacks (expect significant robustness gains), 3) Remove batch-norm integration and repeat experiment 2 (expect reduced robustness)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method scale to larger architectures and datasets beyond CIFAR and TinyImageNet?
- Basis in paper: Paper demonstrates state-of-the-art results on CIFAR-10, CIFAR-100, SVHN, and TinyImageNet but doesn't explore scaling to larger models or complex datasets
- Why unresolved: Paper focuses on relatively small models and datasets
- What evidence would resolve it: Experiments on larger architectures (ResNet-50, EfficientNet) and complex datasets (ImageNet, COCO)

### Open Question 2
- Question: How does the proposed method compare to other robustness techniques like certified defenses or generative models?
- Basis in paper: Paper primarily compares to existing adversarial training-based methods for SNNs
- Why unresolved: Paper doesn't provide comprehensive comparison to other robustness techniques
- What evidence would resolve it: Experiments comparing to certified defenses (randomized smoothing, interval bound propagation) and generative models (adversarial autoencoders, GAN-based defenses)

### Open Question 3
- Question: How does the proposed method handle other types of attacks like backdoor attacks or data poisoning?
- Basis in paper: Paper primarily focuses on evaluating against adversarial attacks
- Why unresolved: Paper doesn't provide comprehensive evaluation against other attack types
- What evidence would resolve it: Experiments evaluating against backdoor attacks (Trojan attacks, neural cleanse) and data poisoning attacks (label flipping, gradient ascent)

## Limitations

- Scalability uncertainty for very deep networks (>100 layers) or larger-scale datasets beyond TinyImageNet
- Limited generalization across architectures beyond standard ResNet-like topologies
- Difficulty isolating individual contributions of components (threshold finetuning, batch-norm integration, ensemble attacks) to final robustness

## Confidence

- Confidence: Medium on scalability claims - results on TinyImageNet but unverified for very deep networks or larger datasets
- Confidence: Low on generalization across architectures - results focus on ResNet-like architectures, performance on transformers or recurrent SNNs unaddressed
- Confidence: Medium on practical impact of batch-norm integration - claims improved robustness but limited ablation studies isolating this effect

## Next Checks

1. Apply conversion method to non-ResNet architectures (MobileNet, EfficientNet) and evaluate robustness transfer efficiency across diverse model families

2. Implement ensemble attack strategy on larger dataset (ImageNet-1k) and measure computational overhead relative to standard ANN adversarial training

3. Systematically remove individual components (threshold finetuning, batch-norm integration, ensemble attacks) to quantify their independent contributions to final robustness