---
ver: rpa2
title: Identifying Label Errors in Object Detection Datasets by Loss Inspection
arxiv_id: '2303.06999'
source_url: https://arxiv.org/abs/2303.06999
tags:
- label
- detection
- errors
- labels
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the first benchmark for detecting label\
  \ errors in object detection datasets and proposes a novel method based on instance-wise\
  \ loss scoring. The authors simulate four types of label errors\u2014drops, flips,\
  \ shifts, and spawns\u2014on the BDD100k and EMNIST-Det datasets, and compare their\
  \ loss-based approach against three baselines: naive review, detection score, and\
  \ entropy."
---

# Identifying Label Errors in Object Detection Datasets by Loss Inspection

## Quick Facts
- arXiv ID: 2303.06999
- Source URL: https://arxiv.org/abs/2303.06999
- Reference count: 27
- This paper introduces the first benchmark for detecting label errors in object detection datasets and proposes a novel method based on instance-wise loss scoring.

## Executive Summary
This paper introduces a novel approach for detecting label errors in object detection datasets using instance-wise loss scoring. The authors simulate four types of label errors—drops, flips, shifts, and spawns—on the BDD100k and EMNIST-Det datasets, and compare their loss-based approach against three baselines: naive review, detection score, and entropy. Their method consistently outperforms the baselines in terms of AUROC and F1 score, particularly excelling at detecting all four error types. When applied to real-world datasets (BDD, KITTI, COCO, VOC, and a proprietary car-part dataset), the approach identifies label errors with precision up to 71.5% on standard datasets and 97% on the proprietary dataset, demonstrating its practical utility.

## Method Summary
The method uses a two-stage object detector to compute instance-wise loss for each bounding box proposal, combining classification and regression losses from both the RPN and ROI heads. Ground truth boxes are added as proposals to ensure coverage, and non-maximum suppression is applied based on loss values rather than detection scores. This approach identifies label errors by detecting mismatches between model predictions and incorrect labels, which manifest as high losses. The method is evaluated on simulated label errors across four error types and applied to real-world datasets to assess practical effectiveness.

## Key Results
- The loss-based method achieves the highest AUROC and F1 scores across all simulated error types compared to three baselines
- Performance is particularly strong at detecting spawn and shift errors, which are challenging for traditional methods
- On real datasets, the approach identifies label errors with precision up to 71.5% on standard datasets and 97% on proprietary car-part data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The instance-wise loss method detects label errors by exploiting the mismatch between model predictions and incorrect labels, which leads to high classification and regression losses for the faulty annotations.
- Mechanism: For each bounding box, the two-stage object detector computes classification and regression losses for both the RPN and ROI heads. When a label is incorrect (e.g., wrong class, wrong location, or object marked as background), the loss between the prediction and the faulty label becomes large. These high-loss instances are then flagged as potential label errors.
- Core assumption: The deep network learns a reasonable representation of the true underlying data distribution; thus, significant deviation between its predictions and a label indicates that the label is likely incorrect.
- Evidence anchors:
  - [abstract] "For our label error detection method we assume a two-stage object detector to be given and consider the sum of both stages' classification and regression losses."
  - [section 3.3] "The NMS is no longer based on the detection score or the entropy, but on the box-wise loss of the respective stage. Every prediction ˆb0∈ ˆB0,z is assigned with a region proposal loss (LRPN ), which is the sum of a classification (binary cross-entropy) and regression (smooth-L1) loss for the labels and the prediction itself."
  - [corpus] Weak. The corpus neighbors discuss label error detection but do not directly confirm the specific mechanism of using instance-wise loss in two-stage detectors.
- Break condition: If the network is severely overfit to noisy labels, it may learn to produce low losses even for incorrect labels, reducing the effectiveness of this method.

### Mechanism 2
- Claim: Adding ground truth boxes as proposals in the first stage ensures that each true object is considered for error detection, even if the detector initially fails to propose it.
- Mechanism: After obtaining region proposals from the RPN, the algorithm explicitly inserts all ground truth bounding boxes into the set of proposals. This guarantees that at least one prediction exists for each labeled object, which is essential for detecting spawn errors (objects labeled in wrong images) and drops (missing labels).
- Core assumption: Ground truth annotations are at least close to the true object locations, so adding them as proposals will not introduce significant noise in the detection pipeline.
- Evidence anchors:
  - [section 3.2] "Then, we add the boxes of the labels as proposals for the second stage to ensure that at least one prediction exists for each label, which is particularly important for the detection of spawns."
  - [section 3.3] "Since not all labels are associated with a proposal after the first stage, i.e., the model may predict only background near a label, we add the labels themselves to the set of label error proposals."
  - [corpus] Weak. The corpus does not directly address the strategy of adding ground truth boxes as proposals.
- Break condition: If ground truth boxes are themselves incorrect (e.g., due to label shifts or flips), adding them as proposals could mask the detection of certain error types.

### Mechanism 3
- Claim: Non-maximum suppression (NMS) based on loss values rather than detection scores improves the identification of label errors, as errors often have high losses but low scores.
- Mechanism: After computing the instance-wise loss for each proposal, NMS is applied using these loss values as the ranking metric instead of the standard detection score. This prioritizes proposals with high classification or regression loss, which are more likely to correspond to label errors.
- Core assumption: Label errors create a significant mismatch between the model's prediction and the annotation, resulting in high losses that are more discriminative than detection scores for error identification.
- Evidence anchors:
  - [section 3.3] "The NMS is no longer based on the detection score or the entropy, but on the box-wise loss of the respective stage."
  - [section 4.2] "On the two left plots we show results based on original training data and the two right plots based on noisy training data. The ranking of the methods is identical everywhere: loss (our method) has highest AUROC and max F1 values, followed by detection score and then entropy."
  - [corpus] Weak. The corpus neighbors do not provide evidence for the specific use of loss-based NMS.
- Break condition: If the loss landscape becomes flat due to noisy training labels, the discriminative power of loss-based NMS may degrade.

## Foundational Learning

- Concept: Object Detection Architectures (e.g., Faster R-CNN, Cascade R-CNN)
  - Why needed here: The method relies on a two-stage object detector with RPN and ROI heads; understanding their operation is essential for implementing the loss computation and proposal handling.
  - Quick check question: What are the two main stages in a typical two-stage object detector, and what is the purpose of each?

- Concept: Loss Functions for Object Detection (classification loss, regression loss)
  - Why needed here: The detection method sums classification (binary cross-entropy) and regression (smooth-L1) losses from both stages to score potential label errors.
  - Quick check question: How do classification and regression losses differ in their computation for object detection?

- Concept: Non-Maximum Suppression (NMS) and its Variants
  - Why needed here: The method modifies standard NMS to use loss values instead of detection scores, which is crucial for identifying label errors.
  - Quick check question: How does standard NMS work, and how would you modify it to rank proposals by loss instead of score?

## Architecture Onboarding

- Component map: Two-stage object detector (Swin-T or ResNeSt101 backbone with CascadeRoIHead) -> RPN (Region Proposal Network) -> ROI head (box refinement and classification) -> Loss computation module (classification + regression losses for both stages) -> NMS module (modified to use loss values) -> Label error proposal filtering and evaluation pipeline

- Critical path:
  1. Load trained detector and dataset
  2. For each image, run forward pass to get RPN proposals
  3. Add ground truth boxes as additional proposals
  4. Apply stage-1 NMS based on loss values
  5. Pass surviving proposals to ROI head for refinement and classification
  6. Compute instance-wise loss (sum of RPN and ROI losses)
  7. Apply stage-2 NMS based on instance-wise loss
  8. Collect top-N proposals as label error candidates
  9. Evaluate against ground truth (simulated or manual review)

- Design tradeoffs:
  - Using a more complex backbone (ResNeSt101) increases parameter count and may overfit noisy labels, but can improve detection accuracy
  - High-resolution inputs (1333×800) improve localization accuracy but increase memory and computation cost
  - Adding ground truth as proposals ensures coverage but may introduce noise if labels are incorrect

- Failure signatures:
  - Low precision/recall despite high AUROC may indicate overfitting to simulated noise or mismatch between simulated and real error types
  - Drop in mAP when training on noisy labels suggests the model is memorizing errors rather than learning robust features
  - High variance in loss-based rankings across images may indicate instability in loss computation or NMS

- First 3 experiments:
  1. Run the loss-based detection method on a small, manually verified subset of BDD to measure precision and recall for each error type
  2. Compare loss-based NMS rankings with detection-score-based rankings on the same dataset to visualize differences in proposal selection
  3. Train two models (Swin-T and ResNeSt101) on noisy BDD labels and evaluate their mAP and label error detection performance to observe the impact of model capacity on robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the instance-wise loss method vary when applied to different object detection architectures beyond two-stage detectors?
- Basis in paper: [inferred] The paper focuses on two-stage object detectors and does not explore the method's applicability to one-stage or other detector types.
- Why unresolved: The study only evaluates the method using two-stage detectors, leaving open the question of its effectiveness on alternative architectures.
- What evidence would resolve it: Testing the instance-wise loss method on a variety of object detection architectures (e.g., one-stage detectors like YOLO or SSD) and comparing its performance to the current results.

### Open Question 2
- Question: What is the impact of label error detection on the long-term performance of object detection models trained on datasets with corrected labels?
- Basis in paper: [explicit] The paper mentions that detecting and correcting label errors can improve model performance, but does not provide long-term performance data.
- Why unresolved: The study focuses on the immediate detection of label errors and their impact on training, without examining the sustained effects on model performance over time.
- What evidence would resolve it: Conducting a longitudinal study where object detection models are trained on datasets with and without corrected labels, tracking their performance over multiple iterations and time periods.

### Open Question 3
- Question: How does the instance-wise loss method perform on datasets with different levels of label noise beyond the simulated 20% used in the study?
- Basis in paper: [explicit] The paper simulates label errors at a fixed 20% rate, but does not explore the method's effectiveness at other noise levels.
- Why unresolved: The study's focus on a single noise level leaves uncertainty about the method's robustness and performance across a range of label noise scenarios.
- What evidence would resolve it: Applying the instance-wise loss method to datasets with varying levels of simulated label noise (e.g., 5%, 10%, 30%, 50%) and analyzing its performance across these different conditions.

## Limitations
- The simulated error models may not fully capture the distribution and types of real-world label errors found in practice
- Performance on datasets with different characteristics (e.g., small objects, dense scenes) is not thoroughly evaluated
- The proprietary dataset results cannot be independently verified due to data restrictions

## Confidence
- **High Confidence**: The benchmark framework and methodology for simulated error detection are sound and well-validated
- **Medium Confidence**: The transfer of results from simulated to real-world errors shows promise but needs more extensive validation
- **Medium Confidence**: The comparison against baseline methods is rigorous, though the baselines themselves could be improved

## Next Checks
1. Evaluate the method on additional real-world datasets with varying characteristics (object density, size distribution, annotation quality)
2. Conduct ablation studies to quantify the contribution of each component (ground truth proposals, loss-based NMS, etc.)
3. Test the method's robustness when applied to datasets with different annotation guidelines or cultural contexts