---
ver: rpa2
title: Intuitive Access to Smartphone Settings Using Relevance Model Trained by Contrastive
  Learning
arxiv_id: '2307.09177'
source_url: https://arxiv.org/abs/2307.09177
tags:
- queries
- search
- query
- features
- relevance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of helping users find smartphone
  settings when feature names are short and numerous. The proposed method uses a neural
  search engine based on a bi-encoder architecture with a relevance model trained
  via contrastive learning from a pre-trained language model.
---

# Intuitive Access to Smartphone Settings Using Relevance Model Trained by Contrastive Learning

## Quick Facts
- arXiv ID: 2307.09177
- Source URL: https://arxiv.org/abs/2307.09177
- Reference count: 7
- Key outcome: Proposed neural search engine improves smartphone settings retrieval with 8.3-22.1 percentage point precision gains on keyword queries and 76.4-83.3% Hits@5 on sentence queries while achieving 99.1% performance with 80-97.4% model size reduction through knowledge distillation.

## Executive Summary
This paper addresses the challenge of helping users find smartphone settings when feature names are short and numerous. The authors propose a neural search engine based on a bi-encoder architecture with a relevance model trained via contrastive learning from a pre-trained language model. To enable on-device deployment, knowledge distillation is applied to compress the model size significantly. The system outperforms current keyword-based search on both exact and relaxed keyword queries as well as contextual sentence queries, demonstrating strong performance improvements while maintaining efficiency for mobile deployment.

## Method Summary
The system uses a bi-encoder architecture where both queries and mobile features are encoded separately using a pre-trained language model. The relevance model is trained via contrastive learning on synthetic query-document pairs created by combining feature names with their descriptions. Knowledge distillation is then applied to compress the model for on-device deployment, transferring knowledge from a larger teacher model to a smaller student model using MSE loss between embeddings. The approach was tested on English and Korean smartphone features, showing significant performance improvements over baseline systems.

## Key Results
- Outperforms current keyword-based search with 8.3-22.1 percentage point precision improvements on keyword queries
- Achieves 76.4-83.3% Hits@5 on contextual sentence queries
- Knowledge distillation reduces model size to 20% (English) or 2.6% (Korean) while maintaining 84.1-99.1% of original performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bi-encoder architecture with contrastive learning improves semantic understanding for smartphone settings retrieval
- Mechanism: Uses Siamese network with shared weights to train relevance model on synthetic query-document pairs, learning contextual embeddings that capture semantic relationships
- Core assumption: Synthetic pairs combining feature names and descriptions can effectively train relevance model without real user queries
- Evidence anchors:
  - [abstract] Trained relevance model via contrastive learning from pre-trained language model to perceive contextual relevance
  - [section] Built Siamese network with pre-trained language model using synthetic pairs
  - [corpus] Weak - related work focuses on general information retrieval

### Mechanism 2
- Claim: Knowledge distillation enables efficient on-device deployment without significant performance loss
- Mechanism: Transfers knowledge from large teacher to smaller student model using MSE loss between embeddings, reducing size by 80-97.4% while maintaining 84.1-99.1% performance
- Core assumption: Student model can effectively approximate teacher's embedding space through distillation
- Evidence anchors:
  - [abstract] Applied knowledge distillation to compress model without degrading much performance
  - [section] Distilled English model maintained 99.1% performance while reducing size to 20%
  - [corpus] Weak - related work mentions knowledge distillation but lacks specific compression ratios

### Mechanism 3
- Claim: Domain-specific pre-training on smartphone-related corpora improves retrieval performance over general language models
- Mechanism: Builds custom language models using Wikipedia plus smartphone-related articles from Samsung sources
- Core assumption: Domain-specific training data provides relevant context for understanding smartphone feature terminology
- Evidence anchors:
  - [section] Built own language model as backbone using Wikipedia and smartphone-related articles
  - [section] Utilized Korean newspaper and book corpora for Korean model
  - [corpus] Weak - no direct comparison with general models like BERT/RoBERTa

## Foundational Learning

- Concept: Contrastive learning for semantic similarity
  - Why needed here: Enables model to learn meaningful embeddings that capture contextual relationships between queries and mobile features
  - Quick check question: How does contrastive learning differ from supervised classification in training a relevance model?

- Concept: Siamese network architecture
  - Why needed here: Allows efficient computation of query-feature similarities by pre-computing feature embeddings with shared weights
  - Quick check question: What advantage does bi-encoder structure provide over cross-encoder architectures for on-device deployment?

- Concept: Knowledge distillation for model compression
  - Why needed here: Enables deployment on resource-constrained mobile devices by significantly reducing model size while maintaining most performance
  - Quick check question: Why is MSE loss between embeddings used instead of KL divergence between logits in this distillation setup?

## Architecture Onboarding

- Component map: User query → Query encoder → Relevance model (Siamese network with pre-trained language model + pooling) → Cosine similarity computation → Top-K retrieval from pre-computed feature embeddings database
- Critical path: Query processing → Embedding computation → Similarity scoring → Result ranking
- Design tradeoffs: Bi-encoder vs cross-encoder (efficiency vs accuracy), model size vs performance (distillation levels), synthetic vs real training data (privacy vs effectiveness)
- Failure signatures: Poor precision on relaxed keyword queries indicates insufficient semantic understanding; low Hits@K on sentence queries suggests inadequate context modeling; high latency indicates inefficient embedding computation
- First 3 experiments:
  1. Compare keyword query performance (precision, recall, F1) against baseline OneUI search system
  2. Measure sentence query retrieval quality using Hits@K metric
  3. Evaluate knowledge distillation tradeoff by testing various model compression levels and measuring performance vs size reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does the proposed retrieval system perform on other types of queries beyond keyword and sentence queries, such as interactive queries that involve multiple turns or follow-up questions?
- Basis in paper: [inferred] Paper mentions sentence queries are often questions users ask to voice assistant systems, suggesting ability to handle interactive queries
- Why unresolved: Paper only evaluates on keyword and sentence queries
- What evidence would resolve it: Conducting experiments on interactive queries with multiple turns or follow-up questions

### Open Question 2
- Question: How does the performance of the proposed retrieval system compare to other state-of-the-art neural search engines, such as those based on transformer models like BERT or RoBERTa?
- Basis in paper: [explicit] Paper mentions proposed system uses bi-encoder architecture with pre-trained language model, similar to other neural search engines
- Why unresolved: Paper only compares to currently deployed search engine (OneUI 4.0) and iOS 15.6
- What evidence would resolve it: Conducting experiments comparing to other state-of-the-art neural search engines

### Open Question 3
- Question: How does the performance of the proposed retrieval system vary across different languages and domains, such as non-English languages or specialized domains like healthcare or finance?
- Basis in paper: [explicit] Paper mentions system was trained on English and Korean corpora but doesn't evaluate on other languages or domains
- Why unresolved: Paper only evaluates on English and Korean queries
- What evidence would resolve it: Conducting experiments on queries in different languages or domains

## Limitations

- Reliance on synthetic query-document pairs introduces uncertainty about real-world effectiveness with actual user query patterns
- Domain-specific pre-training shows promise but lacks direct comparison with established general models like BERT or RoBERTa
- Knowledge distillation evaluation focuses on size reduction rather than inference time or memory usage critical for on-device deployment

## Confidence

**High Confidence**: Bi-encoder architecture with contrastive learning effectively improves semantic understanding for mobile settings retrieval (8.3-22.1 percentage point improvements on keyword queries, 76.4-83.3% Hits@5 on sentence queries)

**Medium Confidence**: Knowledge distillation successfully enables efficient on-device deployment, though exact deployment benefits (inference speed, memory usage) are not measured (99.1% performance retention based on limited compression levels)

**Low Confidence**: Domain-specific pre-training provides meaningful advantages over general models, as paper lacks direct comparisons with established models like BERT or RoBERTa on the same task

## Next Checks

1. **Real User Query Validation**: Collect and test system with actual user queries rather than synthetic pairs to verify performance gains hold on genuine user behavior patterns

2. **Deployment Metrics Measurement**: Evaluate compressed models on real mobile devices measuring actual inference latency, memory footprint, and battery impact, not just model size reduction percentages

3. **Cross-Lingual Generalization Test**: Assess model performance when English-trained models are applied to Korean queries and vice versa, to understand limits of cross-lingual transfer in this domain