---
ver: rpa2
title: Augmented Embeddings for Custom Retrievals
arxiv_id: '2310.05380'
source_url: https://arxiv.org/abs/2310.05380
tags:
- retrieval
- corpus
- language
- embeddings
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Adapted Dense Retrieval (ADDER), a method
  to improve task-specific, heterogeneous, and strict retrieval by learning low-rank
  residual adaptations of pretrained embeddings. The key idea is to augment black-box
  embedding models with small adapters that are learned from limited training data
  using a contrastive loss with global negatives.
---

# Augmented Embeddings for Custom Retrievals

## Quick Facts
- arXiv ID: 2310.05380
- Source URL: https://arxiv.org/abs/2310.05380
- Reference count: 17
- One-line primary result: ADDER improves task-specific retrieval by learning low-rank residual adaptations of pretrained embeddings without modifying the original model

## Executive Summary
This paper introduces Adapted Dense Retrieval (ADDER), a method to improve task-specific, heterogeneous, and strict retrieval by learning low-rank residual adaptations of pretrained embeddings. The key idea is to augment black-box embedding models with small adapters that are learned from limited training data using a contrastive loss with global negatives. The adapter adds a residual term to the embedding via a softmax-based key-value lookup, enabling task-specific semantic alignment without modifying the original model. Experiments on BEIR benchmarks and NL2X retrieval tasks show that ADDER significantly improves retrieval performance, especially in heterogeneous settings, with gains in nDCG@1, 3, 5, and 10 metrics.

## Method Summary
ADDER improves task-specific retrieval by learning low-rank residual adaptations of pretrained embeddings. The method augments a black-box embedding model (e.g., OpenAI ada) with a small adapter that computes a residual vector via a softmax-based key-value lookup. This residual is learned using a contrastive loss with global negatives, shifting the embedding space toward the task's semantic notion of similarity. The adapter is trained on limited task-specific data without modifying the original embedding model, enabling parameter-efficient adaptation for heterogeneous retrieval tasks where queries and corpus elements have different formats or require strict precision at top-k.

## Key Results
- ADDER significantly improves retrieval performance on BEIR benchmarks, with consistent gains in nDCG@1, 3, 5, and 10 metrics
- In heterogeneous NL2X tasks, ADDER shows substantial improvements, particularly on NL2SMCALFLOW and SciFact datasets
- ADDER 2 (dual adaptation) sometimes improves performance in heterogeneous settings but can also degrade results, indicating task-dependent benefits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding a low-rank residual adaptation term to a pretrained embedding improves task-specific retrieval performance without fine-tuning the original model.
- Mechanism: The adapter computes a residual vector via a softmax-based key-value lookup, which is then added to the original embedding. This residual is learned using a contrastive loss with global negatives, shifting the embedding space toward the task's semantic notion of similarity.
- Core assumption: The pretrained embedding captures general semantics well enough that a small perturbation (the residual) suffices to align it with the target task's similarity notion.
- Evidence anchors:
  - [abstract] "Adapted Dense Retrieval works by learning a low-rank residual adaptation of the pretrained black-box embedding."
  - [section] "We compute the perturbation term by passing the general-purpose embedding through a softmax based dictionary lookup."
  - [corpus] No explicit empirical evidence; the claim is supported by reported improvements on BEIR and NL2X benchmarks.
- Break condition: If the pretrained embedding is already misaligned with the target task's similarity notion (e.g., too far from the semantic space), a small residual may be insufficient and the approach fails.

### Mechanism 2
- Claim: Using global negatives in the contrastive loss yields better alignment than local/in-batch negatives for retrieval adaptation.
- Mechanism: For each query, the hardest negative is selected from the full corpus by computing similarity under the current adapter state, making the learning process focus on difficult examples.
- Core assumption: Global negatives provide more informative gradients than local negatives, leading to faster and more robust convergence.
- Evidence anchors:
  - [abstract] "We take cue from the work (Xiong et al., 2020) that suggested taking hard negatives from the full corpus."
  - [section] "We observed the same behavior, and hence we picked the negative corpus element that was closest to the query embedding as the negative sample."
  - [corpus] No independent corpus validation; the claim rests on experimental results and the cited prior work.
- Break condition: If the corpus is too large to compute global negatives efficiently, or if negatives are not truly hard, the benefit diminishes.

### Mechanism 3
- Claim: Adapting both query and corpus embeddings (ADDER 2) can improve retrieval in some heterogeneous settings, but not always.
- Mechanism: Two separate residual adapters are applied—one to the query embedding and one to the corpus embedding—allowing asymmetric fine-tuning of the semantic space.
- Core assumption: In some heterogeneous retrieval tasks, aligning both sides of the query-corpus pair improves matching accuracy.
- Evidence anchors:
  - [abstract] "When we use Equation 3, the system is called ADDER 2."
  - [section] "We can also apply another transformation Tr′ on the corpus element to get a variantretadr2(k, q0, C) of adapted dense retriever."
  - [corpus] Mixed empirical support: ADDER 2 wins in SciFact and NL2SMCALFLOW, but performs worse or similar in FiQA and NL2PYTHON.
- Break condition: If the task benefits from asymmetric adaptation or if corpus-side adaptation introduces noise, the approach underperforms.

## Foundational Learning

- Concept: Dense retrieval with embeddings
  - Why needed here: The method builds on dense retrieval, replacing static embeddings with adapted ones for better task alignment.
  - Quick check question: How does dense retrieval differ from sparse retrieval, and why is it used here?

- Concept: Contrastive learning with hard negatives
  - Why needed here: The residual adapter is trained using a contrastive loss, requiring understanding of positive/negative sampling strategies.
  - Quick check question: What is the difference between local and global negative sampling, and why might global negatives be better?

- Concept: Residual adaptation (similar to LoRA)
  - Why needed here: The adapter adds a small perturbation to the embedding rather than changing the base model, enabling parameter-efficient adaptation.
  - Quick check question: How does a low-rank residual adaptation differ from full fine-tuning in terms of parameters and data requirements?

## Architecture Onboarding

- Component map:
  - Input: Query and corpus items
  - Embedding Model: Black-box model (e.g., OpenAI ada)
  - Adapter: Key-value lookup with residual addition
  - Loss: Contrastive loss with global negatives
  - Output: Ranked corpus items

- Critical path:
  1. Compute embeddings for query and corpus.
  2. Apply adapter(s) to embeddings.
  3. Compute cosine similarity.
  4. Retrieve top-k items.

- Design tradeoffs:
  - Adapter size (h) vs. adaptation capacity: Larger h gives more flexibility but more parameters.
  - Global vs. local negatives: Global negatives are more informative but computationally expensive.
  - Single vs. dual adaptation: Adapting both query and corpus can help in some tasks but adds complexity.

- Failure signatures:
  - No improvement over baseline: Likely due to misalignment or insufficient training data.
  - Degraded performance: Adapter overfits or introduces noise.
  - Slow training: Global negative computation bottleneck.

- First 3 experiments:
  1. Run ADDER on a small BEIR benchmark (e.g., SciFact) with h=64 to verify adaptation works.
  2. Compare ADDER vs ADDER 2 on a heterogeneous NL2X task (e.g., NL2BASH) to test dual adaptation.
  3. Test the effect of switching from global to local negatives on training stability and final performance.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided content.

## Limitations
- Limited ablation studies on adapter dimensions (h) and their impact across different tasks
- Reliance on global negative sampling may not scale well to extremely large corpora
- No exploration of performance with alternative pretrained embedding models beyond OpenAI ada

## Confidence

**High Confidence**: The core mechanism of adding residual adaptations to embeddings is well-supported by the mathematical formulation and experimental results on BEIR benchmarks. The improvement over baseline embeddings is consistent and substantial.

**Medium Confidence**: The claim about global negatives being superior to local negatives is supported by experimental results but lacks extensive ablation or theoretical justification. The choice of h=64 appears somewhat arbitrary without sensitivity analysis.

**Low Confidence**: The claim that ADDER 2 (dual adaptation) is sometimes beneficial but not always is based on limited heterogeneous task experiments. The conditions under which dual adaptation helps versus hurts are not well-characterized.

## Next Checks

1. **Adapter Dimension Sensitivity**: Run experiments varying the adapter dimension h across multiple values (32, 64, 128) on both BEIR and heterogeneous tasks to understand the tradeoff between adaptation capacity and performance.

2. **Local vs Global Negatives Ablation**: Implement both global and local negative sampling strategies and compare their effects on training stability, convergence speed, and final retrieval quality across different dataset sizes.

3. **Cross-Model Generalization**: Test ADDER with alternative pretrained embedding models (e.g., SBERT, Contriever) to verify whether the adaptation benefits are specific to OpenAI ada or generalize to other dense retrievers.