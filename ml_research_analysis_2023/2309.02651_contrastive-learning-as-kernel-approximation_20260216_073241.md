---
ver: rpa2
title: Contrastive Learning as Kernel Approximation
arxiv_id: '2309.02651'
source_url: https://arxiv.org/abs/2309.02651
tags:
- learning
- data
- kernel
- contrastive
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis provides a comprehensive overview of the theoretical
  understanding of contrastive learning, focusing on the minimizers of contrastive
  loss functions and their relationship to prior methods for learning features from
  unlabeled data. It highlights how popular contrastive loss functions implicitly
  approximate a positive semidefinite (PSD) kernel, which formalizes a notion of similarity
  between elements of a space.
---

# Contrastive Learning as Kernel Approximation

## Quick Facts
- arXiv ID: 2309.02651
- Source URL: https://arxiv.org/abs/2309.02651
- Reference count: 0
- Primary result: Contrastive learning implicitly approximates a positive semidefinite kernel through its loss functions, connecting it to kernel methods and dimensionality reduction.

## Executive Summary
This thesis provides a theoretical framework for understanding contrastive learning by showing it implicitly approximates positive semidefinite kernels. The work connects contrastive methods to established kernel learning techniques, demonstrating that popular loss functions like InfoNCE and SimCLR learn feature representations that approximate the positive-pair kernel. This kernel approximation perspective formalizes why contrastive learning preserves similarity between inputs and relates it to spectral methods for dimensionality reduction. The thesis also explores how recent approaches like Neural Eigenfunctions explicitly learn features from PSD kernels by approximating Mercer eigenfunctions, providing theoretical justification for contrastive learning's effectiveness in learning representations for label-invariant downstream tasks.

## Method Summary
The thesis investigates contrastive learning through the lens of kernel approximation, focusing on how loss functions implicitly learn to approximate positive semidefinite kernels. The method involves analyzing the minimizers of contrastive loss functions like InfoNCE and SimCLR, showing they produce features whose dot products approximate the positive-pair kernel K+(x,z) = p+(x,z) / (p(x)p(z)). The work also examines spectral contrastive loss and its equivalence to minimizing the Frobenius norm between a normalized adjacency matrix and its low-rank factorization. Additionally, the thesis explores Neural Eigenfunctions and Neural Eigenmap approaches that explicitly learn features by approximating Mercer eigenfunctions of PSD kernels.

## Key Results
- Contrastive loss functions implicitly approximate a positive semidefinite kernel through their minimizers
- Contrastive learning can be interpreted as a parametric spectral decomposition of a Gram matrix derived from the positive-pair kernel
- Top Mercer eigenfunctions of the positive-pair kernel are optimal for label-invariant downstream tasks under certain conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning implicitly learns to approximate a PSD kernel through the optimization of InfoNCE or SimCLR loss functions.
- Mechanism: The loss function compares positive pairs (high similarity) and negative pairs (low similarity) using dot products in feature space. Minimizing this loss causes the feature encoder to produce dot products that approximate the positive-pair kernel K+(x,z) = p+(x,z) / (p(x)p(z)), up to a constant factor.
- Core assumption: Positive pairs are drawn from p+(x,z) and negatives are drawn from p(x)p(z), with infinite sample limit.
- Evidence anchors:
  - [abstract] "popular contrastive loss functions whose minimizers implicitly approximate a positive semidefinite (PSD) kernel"
  - [section] Theorem 5.5.3: "the minimizer ϕ of E[ℓSimCLR] satisfies exp(ϕ(x)⊤ϕ(z)/τ) = CK+(x,z) ∀ x,z ∈ X"
  - [corpus] Weak: no direct kernel approximation mention in neighbors, but theoretical works [206489, 129992] support this framing.
- Break condition: If the positive/negative sampling process does not reflect the assumed distributions, or if feature vectors are not properly normalized.

### Mechanism 2
- Claim: Contrastive learning can be interpreted as a parametric spectral decomposition of a Gram matrix derived from the positive-pair kernel.
- Mechanism: The spectral contrastive loss Lspec(ϕ) = -2E[ϕ(x)⊤ϕ(x′)] + E[(ϕ(z)⊤ϕ(z′))²] is equivalent to minimizing ||Ā - FF⊤||²F, where Ā is the normalized adjacency matrix of the augmentation graph. This forces F rows to align with top eigenvectors of Ā, i.e., the Mercer eigenfunctions of K+.
- Core assumption: Input space is finite and graph edges are weighted by p+(x,z).
- Evidence anchors:
  - [section] Theorem 5.6.2: "Lspec(ϕ) = ||Ā - FF⊤||²F + c" and "at the minimum, ϕ*(x)⊤ϕ*(z) ≈ K+(x,z)"
  - [abstract] "contrastive learning is a parametric way of learning a low-rank factorization of a kernel"
  - [corpus] Moderate: [206489] shows contrastive learning = spectral clustering on similarity graph, supporting this factorization view.
- Break condition: If the graph structure does not reflect true similarity, or if d < rank(Ā).

### Mechanism 3
- Claim: Top Mercer eigenfunctions of the positive-pair kernel are optimal for label-invariant downstream tasks under certain conditions.
- Mechanism: The eigenfunctions of K+ form the hypothesis class Hϕ that minimizes max variance across positive pairs, making them maximally invariant. They also minimize approximation error for invariant labeling functions in Sε.
- Core assumption: Label function h* is approximately invariant to data augmentations, and the eigenfunctions span the optimal d-dimensional invariant subspace.
- Evidence anchors:
  - [section] Theorem 5.7.1: "Hϕ is the set of maximally invariant predictors" and "Hϕ is the optimal hypothesis class of dimension d for approximating invariant labelling functions"
  - [abstract] "contrastive learning methods produce low-dimensional vector representations... that preserve similarity"
  - [corpus] Weak: no direct mention of Mercer eigenfunctions in neighbors, but [233464] on SSL dynamics and [63616] on SSL objectives may touch related invariance concepts.
- Break condition: If data augmentations are not label-preserving, or if eigenfunctions are truncated too early.

## Foundational Learning

- Concept: Reproducing Kernel Hilbert Spaces (RKHS) and PSD kernels
  - Why needed here: To understand how kernels implicitly define features and similarity, which is the theoretical foundation linking contrastive learning to kernel approximation.
  - Quick check question: What is the reproducing property of a PSD kernel in an RKHS, and how does it relate to the kernel trick?

- Concept: Spectral decomposition and eigenfunction approximation (Mercer's theorem)
  - Why needed here: To grasp how contrastive learning can be seen as learning a low-rank factorization of a kernel, analogous to PCA/MDS on Gram matrices, and how eigenfunctions are optimal for invariant tasks.
  - Quick check question: How does Mercer's theorem connect a PSD kernel to its eigenfunctions, and why are top eigenfunctions useful for dimensionality reduction?

- Concept: Binary and multiclass logistic regression as proper scoring rules
  - Why needed here: To follow the derivation that contrastive loss minima correspond to log-odds of positive pairs, which leads to kernel approximation.
  - Quick check question: Why is the cross-entropy loss a proper scoring rule, and what does its minimizer represent in the context of contrastive learning?

## Architecture Onboarding

- Component map:
  Data pipeline -> Encoder ϕθ -> Loss module (InfoNCE/SimCLR) -> Training loop (SGD) -> Downstream module (linear probe)

- Critical path:
  1. Sample a batch of base inputs.
  2. Apply augmentations to create 2B transformed inputs.
  3. Compute features for all inputs via encoder.
  4. Compute similarity scores (dot products / cosine similarity).
  5. Compute InfoNCE loss across positive/negative pairs.
  6. Backpropagate and update encoder parameters.

- Design tradeoffs:
  - Batch size vs. negative sample quality: larger batches give more negatives but increase memory.
  - Augmentation strength vs. label preservation: stronger augmentations may break semantic similarity.
  - Feature dimension d vs. kernel rank: higher d can better approximate kernel but risks overfitting.
  - Normalization (ℓ2) vs. scale: normalization stabilizes training but removes absolute scale information.

- Failure signatures:
  - Training loss plateaus early: likely too few negatives or poor augmentation strategy.
  - Features collapse to constant vectors: temperature τ too high or loss not properly normalized.
  - Downstream accuracy low: encoder not learning discriminative features; check augmentation invariance.
  - Gradient explosion: check feature normalization and temperature scaling.

- First 3 experiments:
  1. Verify kernel approximation: Train SimCLR, extract features, compute Gram matrix of features, compare to Gram matrix of positive-pair kernel estimated from data.
  2. Test spectral equivalence: Compute spectral contrastive loss on trained features and compare to ||Ā - FF⊤||²F.
  3. Probe invariance: Measure average feature variance across augmentations of same input; compare to variance across different inputs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal batch size for contrastive learning with the SimCLR loss?
- Basis in paper: [explicit] The paper notes that larger batch sizes improve downstream task performance in empirical work, but theoretical work suggests the opposite.
- Why unresolved: There is a discrepancy between theoretical bounds and empirical findings on the effect of batch size on downstream performance.
- What evidence would resolve it: Empirical studies comparing downstream task performance across a wide range of batch sizes, or theoretical analysis that reconciles the discrepancy.

### Open Question 2
- Question: How does the choice of data augmentation strategy impact the positive-pair kernel and downstream task performance?
- Basis in paper: [explicit] The paper discusses how data augmentations define positive and negative pairs, but does not explore the impact of different augmentation strategies on the positive-pair kernel.
- Why unresolved: The relationship between data augmentation, the positive-pair kernel, and downstream task performance is not fully understood.
- What evidence would resolve it: Empirical studies comparing downstream task performance using different data augmentation strategies, or theoretical analysis of how augmentations affect the positive-pair kernel.

### Open Question 3
- Question: What is the impact of fine-tuning on the features learned by contrastive learning methods?
- Basis in paper: [explicit] The paper mentions that few theoretical investigations of fine-tuning exist.
- Why unresolved: The theoretical understanding of how fine-tuning affects the representations learned by contrastive methods is limited.
- What evidence would resolve it: Empirical studies comparing downstream task performance with and without fine-tuning, or theoretical analysis of the impact of fine-tuning on the representations.

## Limitations

- The assumption of infinite sample sizes and perfect augmentation invariance may not hold in practice
- Positive-pair kernel K+(x,z) estimation from finite data may introduce approximation errors
- Spectral approximation robustness is limited when feature dimension d is smaller than the true kernel rank

## Confidence

- Mechanism 1: High
- Mechanism 2: Medium
- Mechanism 3: Low

## Next Checks

1. Measure kernel approximation error on held-out data by comparing learned feature dot products to estimated positive-pair kernel values
2. Compare spectral contrastive loss to ||Ā - FF⊤||²F on real datasets to validate the spectral decomposition interpretation
3. Test downstream task performance when eigenfunctions are truncated at different ranks to assess the optimality claim for invariant tasks