---
ver: rpa2
title: 'The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language
  Variants'
arxiv_id: '2308.16884'
source_url: https://arxiv.org/abs/2308.16884
tags:
- latn
- languages
- language
- dataset
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BELEBELE, a large-scale multiple-choice machine
  reading comprehension dataset spanning 122 language variants, significantly expanding
  the language coverage of existing natural language understanding benchmarks. The
  dataset is constructed by creating English multiple-choice questions based on passages
  from the Flores-200 dataset, then translating them into 121 other languages.
---

# The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants

## Quick Facts
- **arXiv ID:** 2308.16884
- **Source URL:** https://arxiv.org/abs/2308.16884
- **Reference count:** 40
- **Primary result:** BELEBELE is a large-scale multiple-choice reading comprehension dataset spanning 122 language variants, enabling direct cross-lingual comparison and revealing that balanced multilingual pretraining outperforms English-centric approaches for low-resource languages.

## Executive Summary
This paper introduces BELEBELE, a parallel multiple-choice reading comprehension dataset covering 122 languages constructed by translating English questions based on Flores-200 passages. The dataset enables direct comparison of model performance across languages due to its parallel nature. Experiments reveal that while English-centric large language models perform well on high-resource languages, smaller multilingual models pretrained on balanced data (XLM-V, INFOXLM) show superior comprehension of low-resource languages. The findings highlight the importance of vocabulary size, conscious vocabulary construction, and balanced multilingual pretraining data distribution for advancing NLP capabilities across diverse languages.

## Method Summary
The BELEBELE dataset is constructed by first creating English multiple-choice questions based on passages from the Flores-200 dataset, then translating these questions into 121 other languages. The dataset contains 900 questions across 122 languages based on 488 passages. Models are evaluated using three settings: fine-tuning on English data with cross-lingual transfer, few-shot in-context learning with 5 examples, and zero-shot prompting. The evaluation uses accuracy as the primary metric with a random baseline of 0.25. Training datasets for English fine-tuning include RACE, SCIQ, MULTI RC, MCT EST, MCS CRIPT 2.0, and RECLOR.

## Key Results
- Smaller multilingual models (XLM-V, INFOXML) pretrained on balanced data outperform larger English-centric LLMs (GPT3.5-Turbo, LLAMA) on low-resource languages
- Vocabulary size and conscious vocabulary construction methods correlate with better performance on low-resource languages
- Balanced multilingual pretraining data distribution leads to superior cross-lingual generalization compared to English-centric pretraining
- Performance gap between high-resource and low-resource languages remains significant across all model types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parallel translation from English enables direct cross-lingual comparison and preserves question difficulty.
- Mechanism: Questions are created in English, then translated to 121 languages. This alignment ensures semantic consistency and equivalent challenge across languages.
- Core assumption: English questions can be translated into all target languages while maintaining difficulty and semantic meaning.
- Evidence anchors:
  - [abstract] "Being fully parallel, this dataset enables direct comparison of model performance across all languages."
  - [section] "The questions have been carefully crafted to discriminate between models with different competence in language comprehension."
  - [corpus] Weak - no explicit parallel translation verification mentioned in corpus data.
- Break condition: If translations introduce ambiguity or lose nuance, cross-lingual comparability fails.

### Mechanism 2
- Claim: Vocabulary size and conscious vocabulary construction improve performance on low-resource languages.
- Mechanism: Larger vocabularies with proper capacity allocation reduce out-of-vocabulary issues and improve token sharing efficiency for morphologically rich or low-resource languages.
- Core assumption: Token sharing and vocabulary capacity directly impact model performance on low-resource languages.
- Evidence anchors:
  - [abstract] "larger vocabulary size and conscious vocabulary construction correlate with better performance on low-resource languages."
  - [section] "XLM-V outperforms XLM-R and INFOXLM (250k vocabulary size) on low-resource languages even though they all have the same architecture and are trained on the same dataset."
  - [corpus] Missing - no corpus-level vocabulary analysis provided.
- Break condition: If vocabulary size is not matched with adequate training data for low-resource languages.

### Mechanism 3
- Claim: Balanced multilingual pretraining data distribution outperforms English-centric pretraining for multilingual comprehension.
- Mechanism: Models pretrained on balanced multilingual data achieve better generalization across diverse languages compared to English-centric models.
- Core assumption: Balanced multilingual pretraining data leads to better cross-lingual transfer capabilities.
- Evidence anchors:
  - [abstract] "much smaller MLMs pretrained on balanced multilingual data still understand far more languages" compared to English-centric LLMs.
  - [section] "XLM-R reaches 58.8 accuracy on average across all the non-English languages. LLAMA -2- CHAT (evaluated in the zero-shot setting) only reaches 41.2."
  - [corpus] Missing - no corpus-level pretraining data distribution analysis provided.
- Break condition: If pretraining data balance does not account for linguistic diversity or if English-centric models benefit from transfer through shared scripts.

## Foundational Learning

- Concept: Cross-lingual transfer learning
  - Why needed here: Understanding how knowledge from high-resource languages transfers to low-resource languages is crucial for evaluating multilingual model capabilities.
  - Quick check question: How does the model's performance on English correlate with its performance on low-resource languages?

- Concept: Vocabulary construction methods
  - Why needed here: Different vocabulary construction methods (BPE, SentencePiece, clustering) impact model performance across diverse languages.
  - Quick check question: How does vocabulary size affect model performance on languages with different morphological complexity?

- Concept: Parallel corpus alignment
  - Why needed here: Ensuring semantic alignment across translated questions is essential for fair cross-lingual evaluation.
  - Quick check question: How do translation choices impact question difficulty and model performance across languages?

## Architecture Onboarding

- Component map:
  - Question creation pipeline (English â†’ 121 languages)
  - Translation quality assurance system
  - Model evaluation framework (fine-tuning, few-shot, zero-shot)
  - Performance analysis dashboard

- Critical path:
  1. Create English questions with quality assurance
  2. Translate questions to target languages with alignment verification
  3. Evaluate models across all languages in multiple settings
  4. Analyze performance patterns and identify improvement areas

- Design tradeoffs:
  - Parallel translation vs. native creation (tradeoff between consistency and cultural nuance)
  - Multiple-choice vs. other QA formats (tradeoff between cross-lingual fairness and reasoning complexity)
  - Balanced vs. English-centric pretraining (tradeoff between multilingual generalization and high-resource performance)

- Failure signatures:
  - Performance drop in specific language families
  - High variance in question difficulty across languages
  - Translation-induced ambiguity in certain language pairs

- First 3 experiments:
  1. Evaluate a multilingual model on English questions vs. translated questions to test translation impact
  2. Compare performance of models with different vocabulary sizes on low-resource languages
  3. Test cross-lingual transfer by fine-tuning on English data and evaluating on non-English languages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of large language models (LLMs) on low-resource languages compare to that of multilingual masked language models (MLMs) when both are evaluated on task-specific fine-tuning rather than zero-shot or few-shot settings?
- Basis in paper: The paper compares the performance of MLMs and LLMs across various settings, including fine-tuning, but primarily focuses on zero-shot and few-shot evaluations for LLMs.
- Why unresolved: The paper does not provide a detailed comparison of LLMs and MLMs on task-specific fine-tuning for low-resource languages, which could reveal different performance dynamics.
- What evidence would resolve it: Comparative results from experiments where both LLMs and MLMs are fine-tuned on low-resource language tasks would clarify their relative effectiveness.

### Open Question 2
- Question: What specific factors contribute to the superior performance of models with larger vocabulary sizes on low-resource languages, and how do these factors interact with the model architecture and training data?
- Basis in paper: The paper observes that larger vocabulary sizes correlate with better performance on low-resource languages but does not delve into the underlying reasons for this correlation.
- Why unresolved: The mechanisms by which vocabulary size influences model performance on low-resource languages are not explicitly explored or explained.
- What evidence would resolve it: Detailed analyses of model performance metrics and linguistic features across different vocabulary sizes would help identify the contributing factors.

### Open Question 3
- Question: How do cultural and linguistic nuances, such as formality and values, affect the performance of NLP systems on multilingual datasets, and what strategies can be employed to address these challenges?
- Basis in paper: The paper mentions that the dataset does not capture language- and culture-specific phenomena, highlighting a limitation in current evaluation benchmarks.
- Why unresolved: The impact of cultural and linguistic nuances on model performance is not addressed, leaving a gap in understanding how to build inclusive NLP systems.
- What evidence would resolve it: Development and evaluation of benchmarks that incorporate cultural and linguistic nuances would provide insights into their effects on model performance and potential mitigation strategies.

## Limitations

- The dataset construction relies entirely on translating English questions, which may introduce translation quality issues and may not fully capture native language comprehension.
- The evaluation methodology assumes multiple-choice questions can adequately measure language comprehension across diverse linguistic structures, potentially disadvantaging certain languages.
- Limited analysis of pretraining data composition prevents definitive conclusions about the relationship between data balance and multilingual performance.

## Confidence

- **High Confidence:** The dataset creation methodology and basic evaluation framework are well-documented and reproducible. The parallel nature of BELEBELE enables direct cross-lingual comparison as claimed.
- **Medium Confidence:** Claims about vocabulary size and construction methods correlating with low-resource language performance are supported by experimental results but lack detailed corpus-level analysis of vocabulary coverage and pretraining data composition.
- **Low Confidence:** Claims about balanced multilingual pretraining being superior to English-centric pretraining for low-resource languages are based on comparative results but lack detailed analysis of pretraining data distributions and their relationship to specific language families.

## Next Checks

1. **Translation Quality Validation:** Conduct a controlled experiment where the same multilingual model is evaluated on both native language comprehension tasks (if available) and BELEBELE translations for overlapping languages. Compare performance to quantify translation-induced bias and assess whether BELEBELE results accurately reflect true language comprehension capabilities.

2. **Pretraining Data Analysis:** For each evaluated model, obtain and analyze the actual language distribution in pretraining data. Compute correlations between pretraining data proportions for specific languages and downstream performance on BELEBELE. This would validate or refute claims about balanced pretraining being optimal for low-resource languages.

3. **Cross-Lingual Transfer Study:** Design an experiment that systematically varies the source language for fine-tuning (e.g., English, high-resource non-English, medium-resource languages) and measures transfer efficiency to low-resource languages. This would provide insights into whether English-centric pretraining creates systematic disadvantages for certain language families or if transfer patterns are more complex.