---
ver: rpa2
title: Heaps' Law in GPT-Neo Large Language Model Emulated Corpora
arxiv_id: '2311.06377'
source_url: https://arxiv.org/abs/2311.06377
tags:
- heaps
- gpt-neo
- text
- pubmed
- abstracts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether Heaps' law, which describes the
  growth of vocabulary size with corpus size in human-authored text, also applies
  to text generated by large language models (LLMs). The researchers used GPT-Neo
  models of different sizes to emulate corpora based on PubMed abstracts.
---

# Heaps' Law in GPT-Neo Large Language Model Emulated Corpora

## Quick Facts
- arXiv ID: 2311.06377
- Source URL: https://arxiv.org/abs/2311.06377
- Authors: 
- Reference count: 19
- GPT-Neo models follow Heaps' law but show higher vocabulary growth rates than human-authored text, primarily due to singleton term generation.

## Executive Summary
This study investigates whether Heaps' law, which describes vocabulary growth in human-authored text, also applies to text generated by large language models. Using GPT-Neo models of varying sizes (125M, 1.3B, 2.7B parameters), the researchers emulated PubMed abstracts and analyzed vocabulary growth patterns. The findings show that GPT-Neo-generated corpora do follow Heaps' law, but with higher vocabulary growth rates compared to human-authored text, primarily due to the generation of singleton terms. Notably, as model size increases, the vocabulary growth more closely aligns with patterns observed in human-authored text.

## Method Summary
The researchers used first 500,000 PubMed abstracts (preprocessed: lowercase, punctuation removed, tokenized, documents with â‰¤5 words removed) as training data. GPT-Neo models of three different sizes were used to emulate these abstracts by using the first five words of each abstract as a prompt and expanding the content to match the original abstract's length. The study employed a bucketing strategy based on abstract lengths and estimated Heaps' law parameters through OLS regression on log-log transformed data. Vocabulary size V(N) and collection size N were computed for cumulative document subsets to analyze the relationship between model size and vocabulary growth.

## Key Results
- GPT-Neo-generated corpora adhere to Heaps' law but exhibit higher vocabulary growth rates than PubMed abstracts
- Increased vocabulary growth is largely attributable to generation of singleton terms
- As GPT-Neo model size grows, generated vocabulary increasingly aligns with human-authored text patterns

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: GPT-Neo models emulate PubMed abstracts that follow Heaps' law but with higher vocabulary growth rates than human-authored text.
- **Mechanism**: The models generate more singleton terms than are present in the human-authored corpus, inflating the vocabulary growth rate.
- **Core assumption**: The generation process produces a higher proportion of unique, low-frequency terms compared to the training corpus.
- **Evidence anchors**:
  - [abstract]: "The generated corpora adhere to Heaps' law. Interestingly, as the GPT-Neo model size grows, its generated vocabulary increasingly adheres to Heaps' law as observed in human-authored text."
  - [section]: "GPT-Neo generated corpora are in keeping with Heaps' law but exhibit a heightened vocabulary growth, as compared with PubMed abstracts from which they were emulated, largely attributable to the generation of singleton terms."
  - [corpus]: Direct comparison between GPT-Neo emulated and human-authored PubMed abstracts is provided; corpus evidence is strong.
- **Break condition**: If the generation process shifts to favor repeated phrases or common terms, the higher vocabulary growth rate would diminish.

### Mechanism 2
- **Claim**: Larger GPT-Neo models more closely mimic the vocabulary growth patterns of human-authored text.
- **Mechanism**: Increasing model parameters reduces the generation of singleton terms, aligning vocabulary growth with human patterns.
- **Core assumption**: More parameters allow the model to better capture and replicate the statistical properties of the training corpus.
- **Evidence anchors**:
  - [abstract]: "As the GPT-Neo model size grows, its generated vocabulary increasingly adheres to Heaps' law as observed in human-authored text."
  - [section]: "The rate of vocabulary growth decreases with increasing model size."
  - [corpus]: The comparison of GPT-Neo 125M, 1.3B, and 2.7B models shows a trend of decreasing vocabulary growth rates with model size.
- **Break condition**: If the model size increases beyond a certain point without corresponding architectural improvements, diminishing returns or overfitting could occur.

### Mechanism 3
- **Claim**: The prompting strategy using initial five words influences the vocabulary growth in emulated corpora.
- **Mechanism**: The choice of prompt affects the diversity of generated text, impacting the vocabulary growth rate.
- **Core assumption**: The initial prompt sets the context for the generated text, influencing the variety of terms used.
- **Evidence anchors**:
  - [section]: "Our emulation strategy involved using the initial five words of each PubMed abstract as a prompt and instructing the model to expand the content up to the original abstract's length."
  - [section]: "We will also explore more systematic approaches to prompting engineering when emulating PubMed abstracts [19]."
  - [corpus]: The corpus evidence for this mechanism is weaker, as the study does not directly compare different prompting strategies.
- **Break condition**: If a different prompting strategy is used that does not rely on the initial five words, the influence on vocabulary growth could change.

## Foundational Learning

- **Concept**: Heaps' Law
  - Why needed here: Understanding Heaps' Law is crucial for analyzing the vocabulary growth in GPT-Neo generated corpora and comparing it with human-authored text.
  - Quick check question: What does Heaps' Law describe in the context of text analysis?

- **Concept**: Vocabulary Growth Rate
  - Why needed here: Analyzing the rate of vocabulary growth in GPT-Neo generated text helps identify the impact of model size and singleton term generation.
  - Quick check question: How does the vocabulary growth rate in GPT-Neo generated text compare to human-authored text?

- **Concept**: Singleton Terms
  - Why needed here: Recognizing the role of singleton terms in inflating vocabulary growth rates is essential for understanding the differences between GPT-Neo and human-authored text.
  - Quick check question: What are singleton terms, and how do they affect vocabulary growth rates in GPT-Neo generated text?

## Architecture Onboarding

- **Component map**: PubMed abstracts -> preprocessing (lowercase, remove punctuation, tokenize) -> GPT-Neo models (125M, 1.3B, 2.7B parameters) -> bucketing strategy -> text generation -> vocabulary analysis
- **Critical path**: PubMed abstracts selection -> prompting strategy (first 5 words) -> text generation via GPT-Neo -> vocabulary size calculation -> Heaps' law parameter estimation
- **Design tradeoffs**: Larger models provide more accurate vocabulary growth patterns but require more computational resources. The choice of prompt length and content affects the diversity of generated text.
- **Failure signatures**: If the vocabulary growth rate does not decrease with increasing model size, it may indicate issues with the model's ability to capture training corpus properties or the influence of singleton term generation.
- **First 3 experiments**:
  1. Compare the vocabulary growth rates of GPT-Neo models with different parameter sizes using the same prompting strategy.
  2. Analyze the impact of varying prompt lengths on the vocabulary growth rate in GPT-Neo generated text.
  3. Investigate the effect of different prompting strategies on the generation of singleton terms and overall vocabulary growth.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the rate of vocabulary growth in LLM-generated text converge to typical human-authored rates as model size continues to increase beyond GPT-NeoX-20B?
- Basis in paper: [explicit] The authors note that as GPT-Neo model size grows, its generated vocabulary increasingly adheres to Heaps' law as observed in human-authored text, and suggest that even larger models may reproduce Heaps' law as observed in human generated textual documents.
- Why unresolved: The paper only tested up to GPT-Neo 2.7B parameters and suggests investigating GPT-NeoX-20B, but does not provide empirical evidence for convergence to human rates at very large model sizes.
- What evidence would resolve it: Empirical testing of vocabulary growth rates across a wide range of LLM sizes, including models significantly larger than GPT-NeoX-20B, compared to human-authored text.

### Open Question 2
- Question: What is the relationship between LLM memorization abilities and adherence to Heaps' law as model size increases?
- Basis in paper: [explicit] The authors mention that the relation between Heaps' law and model size invites the thought that memorization abilities might increase with model size, and that further investigation is merited to elucidate connections between Heaps' law and the memorization phenomenon.
- Why unresolved: The paper does not provide empirical evidence or analysis of the connection between memorization and Heaps' law adherence.
- What evidence would resolve it: Empirical studies comparing memorization rates and Heaps' law adherence across various LLM sizes, along with analysis of the relationship between these two phenomena.

### Open Question 3
- Question: How does the rate of vocabulary growth in LLM-generated text vary across different domains and languages?
- Basis in paper: [inferred] The authors mention aspiring to extend the scope of their investigation to cover text from diverse domains and languages to ascertain the generalizability of Heaps' law in LLM emulated documents.
- Why unresolved: The current study only examines biomedical abstracts from PubMed, which may not be representative of all domains and languages.
- What evidence would resolve it: Empirical testing of vocabulary growth rates in LLM-generated text across multiple domains (e.g., news, literature, social media) and languages, compared to human-authored text in those domains and languages.

## Limitations
- The study's exclusive use of PubMed abstracts limits generalizability to other domains and writing styles.
- The analysis focuses on statistical measures without examining semantic coherence or topical diversity of generated text.
- The prompting strategy using only first five words may introduce systematic biases not fully explored.

## Confidence
**High confidence**: The finding that GPT-Neo models follow Heaps' law with higher vocabulary growth rates than human-authored text.
**Medium confidence**: The attribution of increased vocabulary growth primarily to singleton term generation.
**Medium confidence**: The observation that larger models more closely mimic human-authored vocabulary growth patterns.

## Next Checks
1. Cross-domain validation: Replicate the analysis using multiple diverse corpora (news articles, fiction, technical documentation) to test whether the observed vocabulary growth patterns hold across different writing styles and domains.
2. Singleton term analysis: Conduct a detailed analysis of the singleton terms generated by GPT-Neo models, including their distribution across different frequency bins, semantic coherence, and whether they represent genuine lexical innovation or sampling artifacts.
3. Prompt strategy comparison: Systematically vary the prompt length and content (beyond the first five words) to determine how different prompting strategies affect vocabulary growth rates and singleton term generation, establishing more robust prompting guidelines for corpus emulation.