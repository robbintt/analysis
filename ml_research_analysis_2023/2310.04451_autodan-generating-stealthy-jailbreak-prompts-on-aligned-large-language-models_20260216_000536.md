---
ver: rpa2
title: 'AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models'
arxiv_id: '2310.04451'
source_url: https://arxiv.org/abs/2310.04451
tags:
- jailbreak
- prompts
- word
- llms
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoDAN is a method that automatically generates stealthy jailbreak
  prompts against aligned Large Language Models (LLMs). It addresses the limitations
  of existing jailbreak techniques that either rely on manual crafting of prompts
  or generate semantically meaningless prompts vulnerable to perplexity-based detection.
---

# AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models

## Quick Facts
- **arXiv ID**: 2310.04451
- **Source URL**: https://arxiv.org/abs/2310.04451
- **Reference count**: 40
- **Primary result**: AutoDAN generates stealthy jailbreak prompts that bypass perplexity-based defenses while maintaining semantic meaningfulness

## Executive Summary
AutoDAN is a method that automatically generates stealthy jailbreak prompts against aligned Large Language Models (LLMs). It addresses the limitations of existing jailbreak techniques that either rely on manual crafting of prompts or generate semantically meaningless prompts vulnerable to perplexity-based detection. AutoDAN employs a hierarchical genetic algorithm to generate stealthy jailbreak prompts by leveraging handcrafted prompts as prototypes and optimizing them using a carefully designed fitness evaluation function. Extensive evaluations demonstrate that AutoDAN not only automates the process while preserving semantic meaningfulness but also achieves superior attack strength in cross-model transferability and cross-sample universality compared to baseline methods.

## Method Summary
AutoDAN uses a hierarchical genetic algorithm (HGA) to evolve jailbreak prompts. The method initializes a population using LLM-generated variants of handcrafted jailbreak prompts (like the DAN series), then applies hierarchical optimization with paragraph-level selection and sentence-level word optimization. A fitness function based on log-likelihood guides the evolution process, while momentum-based word scoring provides stability. The method specifically targets perplexity-based detection mechanisms by generating semantically meaningful prompts that can bypass automated defenses while maintaining attack effectiveness.

## Key Results
- AutoDAN achieves higher attack success rates compared to baseline methods across multiple open-source LLMs
- The method successfully bypasses perplexity-based detection mechanisms that defend against traditional jailbreak attacks
- AutoDAN demonstrates strong cross-model transferability and cross-sample universality in jailbreak effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The hierarchical genetic algorithm structure allows effective optimization of structured discrete data like prompts.
- **Mechanism**: AutoDAN-HGA separates the search space into sentence-level and paragraph-level populations, enabling fine-grained word choice optimization before integrating into higher-level sentence combination.
- **Core assumption**: The hierarchical structure of text (words → sentences → paragraphs) can be exploited algorithmically to improve search efficiency and avoid local optima.
- **Evidence anchors**:
  - [abstract]: "By approaching sentences from a hierarchical perspective, we introduce different crossover policies for both sentences and words."
  - [section]: "This approach gives rise to a hierarchical genetic algorithm, i.e., AutoDAN-HGA. As illustrated in Fig. 2, AutoDAN-HGA surpasses AutoDAN-GA in terms of loss convergence."
  - [corpus]: Weak evidence - no directly relevant papers found in corpus.
- **Break condition**: If word-level optimization fails to improve fitness, the hierarchical approach loses its advantage.

### Mechanism 2
- **Claim**: Momentum-based word scoring improves stability and search capability in the optimization process.
- **Mechanism**: Words are scored based on average fitness contribution across prompts, with momentum incorporating previous iteration scores to smooth fluctuations.
- **Core assumption**: Word fitness scores exhibit temporal correlation that can be exploited through momentum-based averaging.
- **Evidence anchors**:
  - [abstract]: "we incorporate a momentum-based design into the word scoring"
  - [section]: "we incorporate a momentum-based design into the word scoring, i.e., deciding the final fitness score of a word based on the average number of the score in current iteration and the last iteration."
  - [corpus]: Weak evidence - no directly relevant papers found in corpus.
- **Break condition**: If word scores fluctuate randomly without temporal correlation, momentum provides no benefit.

### Mechanism 3
- **Claim**: Using handcrafted jailbreak prompts as initialization provides a meaningful starting point in the search space.
- **Mechanism**: LLMs generate diverse variants of known effective prompts to create the initial population, reducing the search space from random to promising regions.
- **Core assumption**: Existing handcrafted prompts contain useful structural and semantic patterns that can be evolved rather than discovered from scratch.
- **Evidence anchors**:
  - [abstract]: "we employ handcrafted prompts as prototypes and optimizing them using a carefully designed fitness evaluation function."
  - [section]: "Existing handcrafted jailbreak prompts identified by LLMs users can effectively serve as the prototypes to initialize the population for the genetic algorithms."
  - [corpus]: Weak evidence - no directly relevant papers found in corpus.
- **Break condition**: If handcrafted prompts are not actually effective or representative, initialization provides no advantage.

## Foundational Learning

- **Concept**: Genetic Algorithm fundamentals (selection, crossover, mutation, fitness evaluation)
  - **Why needed here**: The entire approach is built on evolving populations of prompts using GA principles
  - **Quick check question**: What is the role of the fitness function in guiding the evolution process?

- **Concept**: Hierarchical data structures and optimization
  - **Why needed here**: The method exploits the hierarchical nature of text to structure the search space
  - **Quick check question**: How does separating word-level and sentence-level optimization improve search efficiency?

- **Concept**: Natural language processing and prompt engineering
  - **Why needed here**: Understanding how prompts interact with LLMs and how semantic meaning affects jailbreak success
  - **Quick check question**: Why would semantically meaningful prompts be harder to detect than random token sequences?

## Architecture Onboarding

- **Component map**: Population Initialization (LLM-based diversification of handcrafted prompts) → Fitness Evaluation (Log-likelihood based on conditional probability) → Hierarchical GA Loop (Paragraph-level selection/crossover/mutation → Sentence-level word optimization) → Momentum Word Scoring (Temporal smoothing) → Termination Criteria (Max iterations or successful jailbreak detection)

- **Critical path**: Initialize → Evaluate fitness → Hierarchical optimization (paragraph-level → sentence-level) → Check termination → Return best prompt

- **Design tradeoffs**:
  - Computational cost vs. prompt quality: More iterations and population diversity improve results but increase runtime
  - Semantic meaningfulness vs. attack strength: More natural prompts are stealthier but may be less optimized for attack
  - Hierarchical depth vs. complexity: Additional levels could improve search but add implementation complexity

- **Failure signatures**:
  - Premature convergence: Population becomes homogeneous without finding effective prompts
  - Semantic drift: Prompts lose coherence during optimization
  - Fitness plateau: No improvement in log-likelihood across iterations

- **First 3 experiments**:
  1. Baseline comparison: Run AutoDAN-GA vs AutoDAN-HGA on same dataset to verify hierarchical improvement
  2. Momentum ablation: Compare with and without momentum word scoring to measure stability impact
  3. Initialization sensitivity: Test with random initialization vs. handcrafted prompt initialization to quantify starting point value

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the optimal crossover rate for AutoDAN-HGA to balance exploration and exploitation?
  - **Basis in paper**: [explicit] The paper states they use a crossover rate of 0.5 but does not provide justification for this specific value or explore its sensitivity.
  - **Why unresolved**: The authors do not conduct ablation studies on crossover rate or compare performance across different rates.
  - **What evidence would resolve it**: Running experiments with varying crossover rates (e.g., 0.1, 0.3, 0.5, 0.7, 0.9) and measuring resulting attack success rates would show the optimal rate.

- **Open Question 2**: How does AutoDAN-HGA performance change when using different types of handcrafted jailbreak prompts as prototypes?
  - **Basis in paper**: [explicit] The authors mention using DAN series prompts as prototypes but do not test performance with alternative prompt types.
  - **Why unresolved**: The paper does not conduct experiments comparing different prototype prompt sets.
  - **What evidence would resolve it**: Testing AutoDAN-HGA with various prototype prompt sets (e.g., Do-Anything-Now, OtherDAN, different jailbreak styles) and measuring resulting attack effectiveness would show which prototypes work best.

- **Open Question 3**: What is the relationship between AutoDAN-HGA iteration count and attack success rate?
  - **Basis in paper**: [explicit] The authors set 100 iterations but do not analyze convergence patterns or optimal iteration count.
  - **Why unresolved**: The paper does not include iteration analysis showing how attack success changes with more or fewer iterations.
  - **What evidence would resolve it**: Running AutoDAN-HGA with varying iteration counts (e.g., 20, 50, 100, 200, 500) and plotting attack success rate vs. iterations would show convergence behavior and optimal iteration count.

## Limitations
- Hierarchical GA design complexity lacks full specification of word-level optimization mechanisms
- Momentum-based scoring effectiveness lacks quantitative comparative evidence
- Defense bypass generalization may not extend to comprehensive defense systems beyond perplexity detection

## Confidence

**High Confidence Claims**:
- AutoDAN automates jailbreak prompt generation (basic functionality demonstrated)
- Handcrafted prompts can initialize the population (practical implementation shown)
- The fitness function based on log-likelihood is well-defined (clear methodology)

**Medium Confidence Claims**:
- Hierarchical structure improves optimization efficiency
- Momentum-based scoring provides stability benefits
- Cross-model transferability is achievable

**Low Confidence Claims**:
- Universal stealthiness against all detection mechanisms
- Complete automation without quality trade-offs
- Long-term effectiveness against evolving defenses

## Next Checks
1. **Fitness Function Sensitivity Analysis**: Systematically vary the weight parameters in the fitness evaluation function to determine which components (semantic coherence, attack strength, perplexity) most influence successful jailbreak generation.

2. **Transferability Robustness Testing**: Evaluate AutoDAN-generated prompts across a wider range of LLM architectures (including different training methodologies and sizes) to verify claimed cross-model effectiveness beyond the three models tested.

3. **Defense Evasion Spectrum Analysis**: Test AutoDAN outputs against multiple detection approaches including semantic coherence models, syntactic pattern detectors, and ensemble defenses to map the complete evasion profile rather than focusing solely on perplexity-based detection.