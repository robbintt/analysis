---
ver: rpa2
title: 'CMB: A Comprehensive Medical Benchmark in Chinese'
arxiv_id: '2308.08833'
source_url: https://arxiv.org/abs/2308.08833
tags:
- medical
- medicine
- evaluation
- chinese
- pharmacist
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CMB is a Chinese medical benchmark for large language models, comprising
  multiple-choice exams (CMB-Exam) and clinical diagnostic questions (CMB-Clin). It
  addresses the need for localized medical evaluation by including traditional Chinese
  medicine alongside Western medicine.
---

# CMB: A Comprehensive Medical Benchmark in Chinese

## Quick Facts
- arXiv ID: 2308.08833
- Source URL: https://arxiv.org/abs/2308.08833
- Reference count: 40
- Key outcome: Chinese medical benchmark with over 280,000 questions evaluating LLMs across Western and Traditional Chinese medicine domains

## Executive Summary
CMB is a comprehensive Chinese medical benchmark designed to evaluate large language models on both Western and Traditional Chinese Medicine. The benchmark comprises multiple-choice examination questions (CMB-Exam) and clinical diagnostic scenarios (CMB-Clin), covering four medical professions and six examination categories. Experiments with various models including GPT-4, ChatGPT, and specialized Chinese medical models reveal significant performance variations across professional levels and knowledge areas, with GPT-4 showing superior performance in medical domains while most specialized medical models lag behind general models.

## Method Summary
The study evaluates LLMs on CMB-Exam (multiple-choice questions) and CMB-Clin (clinical diagnostic questions) using zero-shot, few-shot, and chain-of-thought strategies. Models are evaluated using greedy decoding with answer extraction via regular expressions for CMB-Exam, and automatic evaluation using GPT-4 with expert validation for CMB-Clin. The benchmark includes 280,000+ questions across 28 subcategories, with performance measured by exact match accuracy for CMB-Exam and four-dimensional scoring (fluency, relevance, completeness, medical proficiency) for CMB-Clin.

## Key Results
- GPT-4 exhibits significant superiority in medical domains compared to specialized Chinese medical models
- Most specialized medical models lag behind general models in performance across all evaluation settings
- Automatic evaluation using GPT-4 shows high agreement with expert evaluation (Pearson correlation 0.84)
- Temperature variation has minimal impact on model rankings (pairwise Spearman correlations above 0.87)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Localized medical benchmarks are necessary because direct translation of Western medical exams fails to capture cultural and conceptual nuances of Traditional Chinese Medicine.
- Mechanism: By designing a benchmark from the ground up within the Chinese linguistic and cultural framework, the evaluation captures context-specific knowledge that is absent or misrepresented in translated versions.
- Core assumption: TCM and Western medicine operate on fundamentally different conceptual frameworks, making direct translation inadequate.
- Evidence anchors:
  - [abstract]: "medical environments in different regions have their local characteristics, e.g., the ubiquity and significance of traditional Chinese medicine within China. Therefore, merely translating English-based medical evaluation may result in contextual incongruities to a local region."
  - [corpus]: The corpus shows related works like TCMBench and BianCang focus on TCM-specific evaluation, supporting the need for a culturally native benchmark.
- Break condition: If the conceptual differences between TCM and Western medicine are minimal or if translation techniques can preserve meaning accurately, the need for localization diminishes.

### Mechanism 2
- Claim: CMB provides a comprehensive evaluation by combining multiple-choice exam questions with clinical diagnostic scenarios, covering both theoretical knowledge and practical reasoning.
- Mechanism: The dual structure (CMB-Exam for theoretical MCQs and CMB-Clin for case-based reasoning) ensures that models are assessed on knowledge recall and applied clinical reasoning, reflecting real medical practice.
- Core assumption: Medical expertise requires both foundational knowledge and the ability to apply it in complex scenarios.
- Evidence anchors:
  - [section]: "The CMB dataset as a whole includes multiple-choice questions in qualification examination (CMB-Exam) and complex clinical diagnostic questions based on actual case studies (CMB-Clin)."
  - [corpus]: The presence of MedBench and similar benchmarks shows a trend toward multi-dimensional evaluation, though CMB uniquely integrates TCM.
- Break condition: If either component (theory or clinical reasoning) is not critical for medical LLMs, or if one can be adequately represented by the other, the dual structure may be redundant.

### Mechanism 3
- Claim: Automatic evaluation using GPT-4 correlates highly with expert evaluation, enabling scalable and cost-effective assessment of model responses.
- Mechanism: GPT-4 is prompted with structured evaluation criteria (fluency, relevance, completeness, medical proficiency) and generates scores that align closely with human expert ratings.
- Core assumption: GPT-4 has sufficient medical knowledge and reasoning capability to perform reliable automatic evaluation.
- Evidence anchors:
  - [section]: "Results of automatic evaluation using GPT-4 highly agree with expert evaluation results... The overall Pearson correlation (Figure 4) is 0.84."
  - [corpus]: Other works in the corpus like MedBench also explore automatic evaluation, indicating it is a recognized approach.
- Break condition: If GPT-4's evaluation quality degrades for specialized medical content or if it lacks consistency, the reliability of automatic evaluation would be compromised.

## Foundational Learning

- Concept: Cultural and linguistic localization in medical AI evaluation
  - Why needed here: Ensures that the evaluation reflects the unique medical paradigms and terminology of the target region, especially for TCM.
  - Quick check question: Why might a translated Western medical exam be inadequate for evaluating a Chinese medical LLM?

- Concept: Multi-task benchmarking in NLP
  - Why needed here: Allows comprehensive assessment of both knowledge-based recall (MCQs) and reasoning-based application (clinical cases).
  - Quick check question: What are the two complementary components of CMB and why are both needed?

- Concept: Automatic evaluation methods using LLMs
  - Why needed here: Provides a scalable, objective way to evaluate model outputs without the high cost of expert annotation.
  - Quick check question: What four aspects are used to evaluate model responses automatically in CMB-Clin?

## Architecture Onboarding

- Component map: CMB-Exam (multiple-choice questions across 28 subcategories) -> CMB-Clin (case-based clinical questions) -> Evaluation pipeline (zero-shot, few-shot, CoT experiments + automatic and expert evaluation)
- Critical path: Data collection -> preprocessing and manual verification -> split into train/dev/test -> model evaluation -> automatic evaluation alignment with expert scores
- Design tradeoffs: Localization vs. universality (CMB is Chinese-specific but not easily generalizable); comprehensiveness vs. dataset size (large but may still miss rare cases)
- Failure signatures: Low correlation between automatic and expert evaluation; model performance variance unexplained by dataset features; CoT strategies reducing accuracy
- First 3 experiments:
  1. Run zero-shot evaluation on CMB-Exam to establish baseline model performance across all subcategories
  2. Apply few-shot prompting with 3 examples to assess if performance improves, especially for weaker models
  3. Conduct automatic evaluation on CMB-Clin using GPT-4 and compare rankings with expert evaluation to verify alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic and cultural factors make Chinese medical terminology particularly challenging to translate compared to general medical terminology?
- Basis in paper: [explicit] The paper states "the precise translation of medical terminologies necessitates both medical professions and the cultural context in the target language" and notes that "merely translating English-based medical evaluation may result in contextual incongruities to a local region."
- Why unresolved: The paper identifies the challenge but doesn't provide specific examples of linguistic or cultural translation difficulties unique to Chinese medical terms, particularly for traditional Chinese medicine concepts.
- What evidence would resolve it: A detailed comparative analysis showing specific Chinese medical terms that cannot be accurately translated without cultural context, with examples of mistranslations that occurred when Western medical frameworks were applied.

### Open Question 2
- Question: What is the optimal balance between general and specialized medical knowledge in large language models for achieving maximum medical domain performance?
- Basis in paper: [explicit] The paper finds that "most specialized medical models still lag behind general models in performance" and that "GPT-4 exhibits significant superiority in the medical domain, with indigenous large-scale models also demonstrating commendable performance."
- Why unresolved: The paper shows general models outperform specialized ones but doesn't investigate whether this is due to better general knowledge integration or other factors like model architecture and training approaches.
- What evidence would resolve it: Controlled experiments comparing models with varying ratios of general versus medical-specific training data, holding other variables constant, to identify the optimal knowledge balance.

### Open Question 3
- Question: What specific aspects of traditional Chinese medicine knowledge create the largest performance gaps for current language models compared to Western medicine knowledge?
- Basis in paper: [explicit] The paper notes "Accuracy exhibits significant disparities across professional levels and knowledge areas, notably between traditional Chinese medicine and Western medicine" and finds models "targeting traditional Chinese medicine consistently score lower than those on Western pharmacology."
- Why unresolved: While the paper identifies a performance gap, it doesn't analyze which specific TCM concepts, diagnostic methods, or treatment principles are most challenging for models to learn and apply correctly.
- What evidence would resolve it: A detailed error analysis categorizing model failures by TCM concept type (e.g., syndrome differentiation, meridian theory, herbal properties) to identify the most problematic knowledge areas.

## Limitations

- The benchmark's localization strength relies on the assumption that TCM and Western medicine require fundamentally different evaluation frameworks without empirical validation that translated Western exams would perform poorly
- Automatic evaluation using GPT-4 shows high correlation with expert scores but this correlation is reported only for CMB-Clin and not validated across all medical domains or for specialized TCM content
- The study demonstrates that most specialized Chinese medical models perform worse than general models but does not investigate whether this reflects model quality or dataset bias in training data composition

## Confidence

- High confidence: The benchmark construction methodology and dataset statistics are clearly specified and reproducible. The claim that localized medical benchmarks are needed for TCM representation is well-supported by contextual evidence.
- Medium confidence: The automatic evaluation results showing high correlation with expert evaluation are convincing for the tested scenarios, but generalizability to other medical domains or evaluation criteria is uncertain.
- Medium confidence: The comparative performance analysis of different models is methodologically sound, but conclusions about model superiority may be influenced by factors not controlled for (training data overlap, instruction tuning differences).

## Next Checks

1. Translate a subset of CMB-Exam questions back to English and evaluate whether GPT-4 performance drops significantly compared to native English medical benchmarks, directly testing the localization hypothesis

2. Have domain experts review a stratified sample of CMB-Clin cases to verify that the clinical scenarios represent authentic medical practice and that the four evaluation dimensions (fluency, relevance, completeness, medical proficiency) adequately capture clinical competence

3. Analyze the training data composition of top-performing models (GPT-4, ChatGPT) versus specialized medical models to determine if performance differences correlate with exposure to Chinese medical content, helping distinguish between model capability and dataset familiarity effects