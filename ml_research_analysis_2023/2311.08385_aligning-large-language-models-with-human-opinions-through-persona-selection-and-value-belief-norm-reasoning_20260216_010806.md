---
ver: rpa2
title: Aligning Large Language Models with Human Opinions through Persona Selection
  and Value--Belief--Norm Reasoning
arxiv_id: '2311.08385'
source_url: https://arxiv.org/abs/2311.08385
tags:
- opinions
- answer
- personae
- explicit
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes ChOiRe, a four-step framework for aligning
  large language models (LLMs) with individual human opinions by leveraging both explicit
  personae (demographics and ideology) and implicit personae (historical opinions).
  The framework addresses three key challenges: filtering irrelevant explicit attributes,
  ranking implicit opinions by usefulness rather than semantic similarity, and performing
  Chain-of-Opinion reasoning to explain and analyze each piece of evidence sequentially.'
---

# Aligning Large Language Models with Human Opinions through Persona Selection and Value--Belief--Norm Reasoning

## Quick Facts
- arXiv ID: 2311.08385
- Source URL: https://arxiv.org/abs/2311.08385
- Authors: 
- Reference count: 40
- Key outcome: ChOiRe framework improves accuracy by 3.22% over prior methods on OpinionQA dataset

## Executive Summary
This paper introduces ChOiRe, a four-step framework for aligning large language models with individual human opinions by leveraging both explicit personae (demographics and ideology) and implicit personae (historical opinions). The framework addresses three key challenges: filtering irrelevant explicit attributes, ranking implicit opinions by usefulness rather than semantic similarity, and performing Chain-of-Opinion reasoning to explain and analyze each piece of evidence sequentially. ChOiRe demonstrates significant improvements in accuracy and reliability compared to baseline methods.

## Method Summary
ChOiRe is a four-step framework that aligns LLMs with human opinions through persona-based reasoning. First, it filters explicit personae attributes to remove irrelevant information that could skew predictions. Second, it ranks implicit personae opinions by their usefulness for the specific question rather than semantic similarity. Third, it performs Chain-of-Opinion reasoning to sequentially analyze all selected personae. Finally, it uses answer consistency with multiple K values to select the most frequent prediction. The method achieves strong performance across multiple benchmarks with only 5 inference calls per prediction.

## Key Results
- ChOiRe improves state-of-the-art accuracy by 3.22% over prior methods on the OpinionQA dataset
- Achieves strong performance across 8/15 benchmark topics
- Significantly enhances model reliability with only 5 inference calls per prediction
- Human evaluations confirm effectiveness in filtering, ranking, and reasoning steps

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Filtering irrelevant explicit personae attributes improves model accuracy by preventing noise from skewing predictions.
- **Mechanism**: An LLM analyzes each explicit attribute's relevance to the question and selects only those that are directly useful, reducing the risk of attention mechanism imperfections causing irrelevant tokens to dominate predictions.
- **Core assumption**: LLMs use attention mechanisms that can be imperfect at ignoring irrelevant tokens, leading to performance degradation when all attributes are included.
- **Evidence anchors**:
  - [abstract] "LLMs are sensitive to even a single irrelevant persona, skewing predictions by up to 30%"
  - [section 3.1] "we contend that only a subset is necessary for accurate opinion prediction, and including non-relevant personae may act as noise, harming predictive performance"
  - [corpus] "Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study" (found 25 related papers)
- **Break condition**: If the LLM cannot accurately assess attribute relevance, or if the question genuinely requires all attributes for prediction.

### Mechanism 2
- **Claim**: Ranking implicit personae opinions by usefulness rather than semantic similarity leads to more accurate predictions.
- **Mechanism**: An LLM evaluates each historical opinion's informativeness for the specific question, prioritizing opinions that provide direct support for prediction over those that merely share similar vocabulary.
- **Core assumption**: Semantic similarity rankings may include opinions that are topically related but not actually informative for the specific prediction task.
- **Evidence anchors**:
  - [abstract] "LLMs fail to reason strategically over personae" and "the opinions ranked highest in semantic similarity may not offer the most valuable information for opinion prediction"
  - [section 3.2] "the opinions ranked highest in semantic similarity may not be the ones that provide the most supportive information for the models to predict opinions"
  - [corpus] Weak evidence; corpus does not directly address ranking by usefulness versus semantic similarity.
- **Break condition**: If the LLM's usefulness assessment is flawed or if semantic similarity actually correlates strongly with predictive value for a particular dataset.

### Mechanism 3
- **Claim**: Chain-of-Opinion reasoning with self-consistency improves reliability by ensuring all personae are analyzed sequentially and multiple predictions are aggregated.
- **Mechanism**: The LLM explains and analyzes each selected explicit and implicit persona sequentially, then makes predictions with varying numbers of implicit opinions (K ∈ {8, 10, 12}), with the most frequent answer chosen as final.
- **Core assumption**: Sequential analysis of all personae prevents the model from overlooking important evidence, and multiple predictions with different K values overcome potential insufficiency of personae information.
- **Evidence anchors**:
  - [abstract] "Chain-of-Opinion reasoning to explain and analyze each piece of evidence sequentially" and "ChOiRe executes Step (iii)'s CoO multiple times with increasingly larger lists of implicit personae to handle potential persona insufficiency"
  - [section 3.3] "this method helps the model to output more consistent reasoning explanations, enhancing its reliability"
  - [section 3.4] "our answer consistency strategy enables LLMs to achieve the best results across three different K values"
- **Break condition**: If the LLM fails to follow the sequential analysis instruction, or if increasing K beyond a certain point introduces noise rather than additional useful information.

## Foundational Learning

- **Concept**: Value-Belief-Norm (VBN) theory
  - Why needed here: The paper explicitly states that COO is "inspired by the Value--Belief--Norm (VBN) theory" and uses it to "extract user environmental and personal value, belief, and norm variables for accurate and reliable predictions."
  - Quick check question: What are the three components of VBN theory that are used to structure the opinion reasoning process?

- **Concept**: Chain-of-Thought (CoT) reasoning
  - Why needed here: ChOiRe builds upon CoT as a baseline, modifying it to create Chain-of-Opinion (CoO) reasoning that better suits the opinion prediction task.
  - Quick check question: How does Chain-of-Opinion differ from standard Chain-of-Thought in its approach to handling multiple pieces of evidence?

- **Concept**: Self-consistency in LLM predictions
  - Why needed here: The paper employs self-consistency by sampling multiple answers with different numbers of implicit personae and selecting the most frequent answer as final.
  - Quick check question: What is the purpose of using different values of K (8, 10, 12) in the answer consistency step?

## Architecture Onboarding

- **Component map**: FEA (Filter Explicit Attributes) -> LLMtop-K (Rank Implicit Opinions) -> CoO (Chain-of-Opinion Reasoning) -> Answer Consistency
- **Critical path**: The critical path is: FEA → LLMtop-K → CoO → Answer Consistency. Each step depends on the output of the previous one, with the final answer being determined by the consistency step.
- **Design tradeoffs**: The method trades increased inference calls (5 per prediction) for improved accuracy and reliability. It also requires LLMs that can follow complex instructions for filtering, ranking, and sequential analysis.
- **Failure signatures**: Key failure modes include: (1) FEA incorrectly filters out relevant attributes, (2) LLMtop-K includes less relevant opinions in the top-K list, (3) CoO fails to follow sequential analysis instructions, (4) Self-consistency fails when no clear majority answer emerges across K values.
- **First 3 experiments**:
  1. Run the baseline DIO-top8 method to establish a performance baseline without any filtering or ranking enhancements.
  2. Implement and test only the FEA component to measure the isolated impact of filtering irrelevant explicit attributes.
  3. Add the LLMtop-K component to the FEA-enhanced model to evaluate the combined benefit of filtering and usefulness-based ranking.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of explicit personae attributes to include for maximizing accuracy across different topics and model types?
- Basis in paper: [explicit] The paper shows filtering explicit personae improves accuracy, but does not explore varying numbers of attributes systematically.
- Why unresolved: The study only compares "all attributes" versus "filtered attributes" without testing different numbers of selected attributes.
- What evidence would resolve it: Controlled experiments testing accuracy with different numbers of selected explicit attributes (e.g., top 5, top 8, top 10) across various topics and model types.

### Open Question 2
- Question: How does the ranking quality of implicit personae opinions correlate with the amount of training data available to the LLM?
- Basis in paper: [inferred] The paper uses LLMtop-K ranking but notes this task is challenging even for humans, suggesting potential limitations with current data.
- Why unresolved: The paper does not examine how ranking performance changes with different amounts of training data.
- What evidence would resolve it: Experiments comparing ranking accuracy and opinion prediction accuracy using LLMs trained with varying amounts of persona-opinion data.

### Open Question 3
- Question: What is the impact of persona sufficiency on opinion prediction accuracy, and can this be predicted in advance?
- Basis in paper: [explicit] The paper notes models sometimes cannot answer with fixed K=8 opinions, requiring dynamic adjustment.
- Why unresolved: The study does not explore methods to predict when sufficient personae are available before attempting prediction.
- What evidence would resolve it: Development of a metric or method to assess persona sufficiency prior to prediction, and experiments measuring its effectiveness in avoiding failed predictions.

## Limitations
- The approach's reliance on LLM-based filtering and ranking introduces uncertainty in attribute selection quality
- Performance is constrained by the availability and quality of historical opinions - insufficient implicit personae could limit effectiveness
- Evaluation focuses primarily on the OpinionQA dataset with US demographic groups, raising questions about generalizability

## Confidence

- **High confidence**: The 3.22% accuracy improvement over baseline methods is well-supported by quantitative results across multiple benchmarks and human evaluation confirms the effectiveness of individual components.
- **Medium confidence**: The claim that filtering irrelevant attributes prevents up to 30% prediction skewing is supported by the abstract but lacks detailed experimental breakdown showing this specific impact.
- **Medium confidence**: The assertion that usefulness-based ranking outperforms semantic similarity is theoretically sound but could benefit from more direct comparison experiments.

## Next Checks
1. Conduct ablation studies to isolate the individual contribution of each framework component (FEA, LLMtop-K, CoO, Answer Consistency) to the overall performance gain.
2. Test the framework on a more diverse dataset with different demographic groups, opinion types, and cultural contexts to assess generalizability.
3. Implement a controlled experiment comparing usefulness-based ranking against semantic similarity ranking on the same dataset to quantify the specific benefit of the proposed ranking approach.