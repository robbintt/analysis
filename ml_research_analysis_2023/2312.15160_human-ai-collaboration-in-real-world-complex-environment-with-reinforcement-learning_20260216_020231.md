---
ver: rpa2
title: Human-AI Collaboration in Real-World Complex Environment with Reinforcement
  Learning
arxiv_id: '2312.15160'
source_url: https://arxiv.org/abs/2312.15160
tags:
- human
- agent
- demonstrations
- learning
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a study on human-AI collaboration in a complex
  multi-agent defense scenario using reinforcement learning. The authors developed
  a novel simulator and user interface for an airport security use case, where AI-powered
  drones and human teams collaborate to defend against enemy drone attacks.
---

# Human-AI Collaboration in Real-World Complex Environment with Reinforcement Learning

## Quick Facts
- arXiv ID: 2312.15160
- Source URL: https://arxiv.org/abs/2312.15160
- Reference count: 40
- Authors: Various
- Key outcome: Human demonstrations and policy corrections significantly accelerate agent learning and improve performance in a multi-agent airport defense scenario compared to learning from scratch or agent-only demonstrations.

## Executive Summary
This paper investigates human-AI collaboration in a complex multi-agent defense scenario using reinforcement learning. The authors developed a novel 2D airport defense simulator where AI-powered drones and human teams collaborate to defend against enemy drone attacks. They compare learning from scratch, agent demonstrations, human demonstrations, and policy corrections. The results demonstrate that agents learn faster and achieve higher performance when learning from human demonstrations or policy corrections, with policy correction requiring lower cognitive load and human effort while yielding superior performance.

## Method Summary
The study employs a 2D airspace simulator with five blue drone agents defending against one red drone in an airport security scenario. Agents are trained using D3QN (Dueling Double Deep Q-Network) with Deep Q learning from demonstration (DQfD) integration. The method incorporates three learning approaches: learning from scratch, learning from demonstrations (human and agent), and policy correction where humans provide real-time feedback to improve agent behavior. The experience buffer combines 30% demonstrations with 70% agent experiences. Performance is evaluated through success rate metrics and NASA Task Load Index for cognitive workload assessment.

## Key Results
- Agents learning from human demonstrations achieve higher performance than those learning from scratch or agent-only demonstrations
- Policy correction outperforms direct human control, requiring lower mental and temporal demands while achieving better results
- The D3QN P C-500-0 configuration (500 policy corrections) shows significant performance increases over baseline methods
- Human-AI collaboration demonstrates reduced cognitive load compared to full human control of all agents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human demonstrations accelerate agent learning in complex environments
- Mechanism: By replaying demonstrations in the experience buffer alongside agent experiences, the agent prioritizes actions that mimic expert behavior through a supervised loss term
- Core assumption: Demonstrations are high-quality and represent useful strategies in the environment
- Evidence anchors: [abstract], [section 3.3], weak corpus support

### Mechanism 2
- Claim: Policy correction leads to faster learning and better final performance than pure demonstrations
- Mechanism: A trained agent learns from both its own experiences and corrections provided by humans during operation, allowing it to adapt its policy more efficiently
- Core assumption: Humans can effectively correct agent behavior in real time, and these corrections generalize beyond the specific instances
- Evidence anchors: [abstract], [section 5.4], weak corpus support

### Mechanism 3
- Claim: Lower cognitive load during policy correction improves human-AI collaboration efficiency
- Mechanism: Policy correction requires less effort than full control, allowing humans to provide high-quality feedback without fatigue, which translates to better agent performance
- Core assumption: Reduced cognitive load directly correlates with higher quality and more frequent corrections
- Evidence anchors: [abstract], [section 7], weak corpus support

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The environment is formally modeled as an MDP to enable reinforcement learning algorithms to operate
  - Quick check question: What are the five components of an MDP tuple?

- Concept: Deep Q-Learning and DQN variants
  - Why needed here: D3QN (Dueling Double Deep Q-Network) is the core learning algorithm used to train the agents
  - Quick check question: How does Dueling DQN differ from standard DQN?

- Concept: Human-in-the-Loop (HitL) learning
  - Why needed here: Human demonstrations and policy corrections are integrated into the learning loop to improve agent performance
  - Quick check question: What are the two main ways humans can influence agent learning in this setup?

## Architecture Onboarding

- Component map: Airport defense simulator -> 5 blue drone agents -> 1 red drone -> Ground radar -> User interface -> D3QN learning algorithm -> Cogment platform
- Critical path: Human interaction → Demonstration/Policy correction collection → Integration into experience buffer → Agent training → Performance evaluation
- Design tradeoffs: Using a simplified 2D simulator reduces complexity but may limit real-world applicability. Prioritizing sample efficiency via demonstrations may sacrifice exploration of novel strategies
- Failure signatures: If the agent's success rate plateaus below 80%, it may indicate poor quality demonstrations or insufficient exploration. High variance in success rate suggests instability in learning
- First 3 experiments:
  1. Train D3QN-0-0 (no demonstrations) to establish baseline performance
  2. Train D3QN-0-2500 (with 2500 agent demonstrations) to test learning acceleration
  3. Train D3QN P C-500-0 (with 500 policy-corrected demonstrations) to compare against human-only demonstrations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of increasing the number of enemy drones on human-AI collaboration performance and human cognitive workload?
- Basis in paper: [explicit] The paper focuses on scenarios with a single enemy drone, noting this as a limitation
- Why unresolved: The study did not explore scenarios with multiple enemy drones, which would increase complexity and potentially change human-AI collaboration dynamics
- What evidence would resolve it: Experimental results comparing human-AI collaboration performance and cognitive workload across scenarios with varying numbers of enemy drones (e.g., 1, 3, 5)

### Open Question 2
- Question: How does the diversity of human demonstrators' backgrounds affect the learning efficiency and performance of AI agents?
- Basis in paper: [inferred] The study collected demonstrations from 11 individuals but did not analyze the impact of their varying backgrounds on agent learning
- Why unresolved: The paper mentions collecting demographic information but does not explore how differences in human demonstrators' expertise influence AI agent learning
- What evidence would resolve it: Comparative analysis of AI agent learning curves and final performance when trained on demonstrations from human demonstrators with different levels of expertise in relevant domains

### Open Question 3
- Question: How does the introduction of communication delays or limitations between human operators and AI agents affect the effectiveness of policy correction in human-AI collaboration?
- Basis in paper: [inferred] The paper mentions perfect and instant communication between agents but does not explore scenarios with communication constraints
- Why unresolved: Real-world applications often involve communication delays or bandwidth limitations, which could impact the timing and effectiveness of human policy corrections
- What evidence would resolve it: Experimental results comparing AI agent performance and human cognitive workload when policy corrections are subject to various communication delays or limitations

## Limitations

- Simplified 2D simulator may not capture real-world complexities like 3D movement, weather effects, or communication delays
- Results may not generalize to scenarios with multiple enemy drones or more complex threat patterns
- Limited analysis of how demonstrator expertise levels affect learning efficiency and final performance

## Confidence

- High confidence: Learning from demonstrations improves sample efficiency compared to learning from scratch
- Medium confidence: Policy correction outperforms both demonstrations and direct control in this specific scenario
- Low confidence: The results will generalize to more complex, real-world airport security environments

## Next Checks

1. Test trained agents in perturbed versions of the simulator with altered drone speeds or detection probabilities to assess robustness
2. Compare performance when demonstrations contain intentional errors versus expert demonstrations to quantify sensitivity to demonstration quality
3. Evaluate whether agents trained on 2D scenarios can transfer learned strategies to 3D environments with minimal additional training