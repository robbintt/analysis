---
ver: rpa2
title: Improving Large Language Models for Clinical Named Entity Recognition via Prompt
  Engineering
arxiv_id: '2303.16416'
source_url: https://arxiv.org/abs/2303.16416
tags:
- chatgpt
- clinical
- performance
- medical
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates ChatGPT's zero-shot performance for clinical
  named entity recognition (NER) compared to GPT-3 and a supervised BioClinicalBERT
  model. ChatGPT achieved F1 scores of 0.311 (exact match) and 0.529 (relaxed match)
  on the MTSamples test set, outperforming GPT-3 but trailing BioClinicalBERT.
---

# Improving Large Language Models for Clinical Named Entity Recognition via Prompt Engineering

## Quick Facts
- arXiv ID: 2303.16416
- Source URL: https://arxiv.org/abs/2303.16416
- Reference count: 25
- ChatGPT achieves F1 score of 0.620 on clinical NER with optimized prompts using relaxed matching criteria

## Executive Summary
This study evaluates ChatGPT's zero-shot performance for clinical named entity recognition compared to GPT-3 and a supervised BioClinicalBERT model. ChatGPT achieved F1 scores of 0.311 (exact match) and 0.529 (relaxed match) on the MTSamples test set, outperforming GPT-3 but trailing BioClinicalBERT. The relaxed match results improve to 0.620 with optimized prompts, demonstrating ChatGPT's strong potential for clinical NER without requiring annotated training data. Error analysis reveals that false positives and rephrasing issues are the primary sources of performance degradation.

## Method Summary
The study tests ChatGPT's zero-shot clinical NER capabilities using HPI sections from 100 synthetic discharge summaries from the MTSamples dataset. Two prompt strategies were designed: Prompt-1 with basic entity type information and Prompt-2 with additional entity details including synonyms and subtypes. Manual collection of ChatGPT responses was performed through the online interface, and evaluation used both exact and relaxed match criteria. Performance was compared against GPT-3 and a supervised BioClinicalBERT model fine-tuned on clinical data.

## Key Results
- ChatGPT achieves F1 score of 0.311 (exact match) and 0.529 (relaxed match) on MTSamples test set
- Optimized prompts improve relaxed F1 scores to 0.620, outperforming GPT-3 baseline
- Performance trails supervised BioClinicalBERT but shows promise for zero-shot clinical NER applications

## Why This Works (Mechanism)

### Mechanism 1
- Prompt engineering significantly improves ChatGPT's clinical NER performance by providing structured task descriptions and entity type guidance
- Adding detailed entity type information (synonyms and subtypes) to prompts improves F1 scores by 0.107 compared to basic prompts alone
- ChatGPT's zero-shot learning benefits from explicit task specification and entity context
- Break condition: If prompts become too verbose, ChatGPT may truncate or ignore critical information due to context window limitations

### Mechanism 2
- Zero-shot learning with ChatGPT is viable for clinical NER when exact match criteria are relaxed to allow partial overlap
- F1 scores improve from 0.418 to 0.620 when switching from exact to relaxed matching criteria
- ChatGPT's entity extraction can capture relevant text spans even if boundaries don't perfectly match gold standard
- Break condition: If relaxed matching becomes too permissive, precision will drop significantly as false positives increase

### Mechanism 3
- ChatGPT's generative nature causes rephrasing errors that impact NER performance despite correct entity identification
- 9.43% of false positives and 19.23% of false negatives stem from ChatGPT rephrasing entities differently from gold standard
- The evaluation metrics penalize valid entity extractions when surface form doesn't match exactly
- Break condition: If evaluation metrics are adjusted to canonicalize entity forms before comparison, rephrasing errors would disappear

## Foundational Learning

- Concept: Zero-shot learning vs fine-tuning
  - Why needed here: The paper explicitly compares zero-shot ChatGPT performance against fine-tuned BioClinicalBERT to establish baseline expectations
  - Quick check question: What are the F1 scores for BioClinicalBERT on exact vs relaxed matching, and how do they compare to ChatGPT's zero-shot results?

- Concept: Named entity recognition evaluation metrics
  - Why needed here: The paper uses precision, recall, and micro-averaged F1 scores with both exact and relaxed matching criteria
  - Quick check question: How does relaxed matching define a true positive differently from exact matching?

- Concept: Prompt engineering principles
  - Why needed here: The study systematically tests different prompt components to optimize ChatGPT's performance
  - Quick check question: What specific information was added to Prompt-2 that improved performance over Prompt-1?

## Architecture Onboarding

- Component map: ChatGPT (zero-shot inference) <- Prompts (task specification) <- MTSamples corpus (input text) -> Evaluation metrics (precision/recall/F1)
- Critical path: Prompt design -> Entity extraction -> Answer collection -> Performance evaluation
- Design tradeoffs: Zero-shot learning trades annotation costs for lower performance compared to supervised learning; relaxed matching trades precision for better recall
- Failure signatures: High false positive rates indicate poor entity type discrimination; rephrasing errors suggest prompt instructions aren't being followed; performance drops on longer texts suggest context window limitations
- First 3 experiments:
  1. Test Prompt-1 vs Prompt-2 on a small subset to verify the 0.107 F1 improvement holds
  2. Evaluate exact vs relaxed matching on the same subset to quantify the performance difference
  3. Run error analysis on 10 test files to categorize error types before full-scale evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does fine-tuning ChatGPT with domain-specific clinical corpora affect its performance on clinical NER tasks in zero-shot settings?
- Basis in paper: The paper mentions "We believe fine-tuning ChatGPT with domain-specific corpora, assuming OpenAI will provide such an API, will further improve its performance on clinical NLP tasks such as NER in the zero-shot fashion."
- Why unresolved: The study did not have access to fine-tuning capabilities for ChatGPT, as OpenAI had not released the ChatGPT API at the time of the study.
- What evidence would resolve it: Experimental results comparing zero-shot performance of fine-tuned ChatGPT versus non-fine-tuned ChatGPT on clinical NER tasks using annotated clinical corpora.

### Open Question 2
- Question: What is the impact of co-reference entity identification on ChatGPT's performance in clinical NER tasks?
- Basis in paper: The authors observed that ChatGPT struggled to extract co-reference entities like "her medications" or "her symptoms", and noted that performance improved when these entities were removed from the gold standard.
- Why unresolved: The study did not explore strategies to improve ChatGPT's ability to handle co-reference entities or investigate the full extent of this issue.
- What evidence would resolve it: Experiments testing ChatGPT's performance on clinical NER tasks with and without co-reference entities, and evaluation of methods to improve co-reference resolution in ChatGPT's outputs.

### Open Question 3
- Question: How does the randomness in ChatGPT's outputs affect its reliability and consistency in clinical NER tasks?
- Basis in paper: The authors noted "a significant degree of randomness in ChatGPT's output" and observed that even with the same prompt and input text, ChatGPT sometimes generated responses with considerable differences in format and content.
- Why unresolved: The study did not quantify the extent of this randomness or investigate methods to reduce it.
- What evidence would resolve it: Quantitative analysis of the variability in ChatGPT's outputs across multiple runs with the same input, and experiments testing techniques to improve output consistency (e.g., temperature settings, prompt engineering).

## Limitations

- Limited sample size of 100 test instances from synthetic MTSamples corpus
- Manual collection of ChatGPT responses through online interface rather than automated API calls
- Reliance on synthetic data limits generalizability to real-world clinical applications

## Confidence

The study's findings are subject to several important limitations that affect confidence in the conclusions. The primary constraint is the use of only 100 test instances from a synthetic MTSamples corpus, which provides a limited sample size for evaluating clinical NER performance. The manual collection of ChatGPT responses through the online interface, rather than automated API calls, introduces potential inconsistencies and makes the study difficult to reproduce. Additionally, the error analysis reveals that ChatGPT's tendency to rephrase entities and hallucinate false positives represents a fundamental limitation of the generative approach for NER tasks.

Confidence in the core findings is Medium. The observed improvements from prompt engineering (F1 increase of 0.107) and relaxed matching (improvement from 0.418 to 0.620) are well-documented and statistically meaningful within the study's constraints. However, the reliance on synthetic data and the small test set size limit generalizability to real-world clinical applications. The study also doesn't explore whether ChatGPT's performance would scale to longer clinical documents or more complex entity types beyond the 2010 i2b2 guidelines.

The comparison to BioClinicalBERT is informative but potentially misleading, as the supervised model benefits from domain-specific pretraining that ChatGPT lacks. While ChatGPT shows promise for zero-shot clinical NER, the 0.620 F1 score under relaxed matching still falls short of what would be expected for production clinical systems, where higher precision is typically required.

## Next Checks

1. Test the prompt engineering approach on a larger, real-world clinical corpus (minimum 500 annotated instances) to verify that the observed F1 improvements of 0.107 for optimized prompts and 0.202 for relaxed matching are reproducible at scale.

2. Implement automated ChatGPT API calls to verify that manual response collection doesn't introduce bias, and test whether the same prompts consistently produce similar F1 scores across multiple runs with identical inputs.

3. Apply the best-performing prompt strategy to actual clinical discharge summaries from multiple hospitals to assess whether performance degrades when moving from synthetic to real clinical text, particularly for entity types beyond the 2010 i2b2 guidelines.