---
ver: rpa2
title: 'Trajectory Alignment: Understanding the Edge of Stability Phenomenon via Bifurcation
  Theory'
arxiv_id: '2307.04204'
source_url: https://arxiv.org/abs/2307.04204
tags:
- theorem
- lemma
- where
- then
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the Edge of Stability (EoS) phenomenon
  in gradient descent (GD) training, where the sharpness of the loss Hessian increases
  progressively and then saturates near a threshold. The authors demonstrate empirically
  that GD trajectories in the EoS regime align on a specific bifurcation diagram under
  a canonical reparameterization, independent of initialization.
---

# Trajectory Alignment: Understanding the Edge of Stability Phenomenon via Bifurcation Theory

## Quick Facts
- arXiv ID: 2307.04204
- Source URL: https://arxiv.org/abs/2307.04204
- Reference count: 40
- This paper demonstrates that gradient descent trajectories in the Edge of Stability regime align on a specific bifurcation diagram under canonical reparameterization, independent of initialization.

## Executive Summary
This paper investigates the Edge of Stability (EoS) phenomenon in gradient descent training, where the sharpness of the loss Hessian increases progressively and then saturates near a threshold. The authors demonstrate empirically that GD trajectories in the EoS regime align on a specific bifurcation diagram under a canonical reparameterization, independent of initialization. This alignment is shown to be a common property across various network architectures and is more pronounced for wider networks. Theoretically, the authors prove trajectory alignment for two-layer fully-connected linear networks and single-neuron nonlinear networks, establishing both progressive sharpening and EoS phenomena.

## Method Summary
The authors implement gradient descent training on fully-connected L-layer neural networks with various widths, depths, and activation functions. They use a canonical reparameterization that maps network parameters to (p, q) where p represents the deviation from target output and q encodes sharpness. This transforms the GD dynamics into a one-dimensional map fq(p) that exhibits period-doubling bifurcations. The method tracks sharpness evolution during training and analyzes trajectory alignment on bifurcation diagrams under this reparameterization.

## Key Results
- Gradient descent trajectories in the EoS regime align on a bifurcation diagram determined by the loss function under canonical reparameterization, independent of initialization.
- Progressive sharpening occurs as sharpness increases until reaching the stability threshold 2/η, then saturates in the EoS regime.
- Network width affects the strength of trajectory alignment, with wider networks showing more pronounced alignment on the bifurcation curve.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient descent trajectories in the EoS regime align on a bifurcation diagram determined by the loss function under a canonical reparameterization.
- Mechanism: The canonical reparameterization maps network parameters to (p, q) where p represents the deviation from target output and q encodes sharpness. This transforms the GD dynamics into a one-dimensional map fq(p) that exhibits period-doubling bifurcations at q=1.
- Core assumption: The loss function is convex, even, Lipschitz, and satisfies specific smoothness conditions on r(p) = ℓ'(p)/p.
- Evidence anchors:
  - [abstract] "different GD trajectories (after a proper reparameterization) align on a specific bifurcation diagram independent of initialization"
  - [section] "GD trajectories closely follow the bifurcation diagram of the map x → x − η ∂/∂x L(x, y)"
  - [corpus] Weak evidence - only general EoS related papers found, no direct trajectory alignment studies
- Break condition: If the loss function doesn't satisfy Assumption 2.4 (bell-shaped r function), the bifurcation structure collapses and alignment disappears.

### Mechanism 2
- Claim: Progressive sharpening occurs because sharpness increases until reaching the stability threshold 2/η, then saturates.
- Mechanism: In Phase II of EoS, q increases monotonically while p converges to the period-2 orbit {±ˆr(q)}, causing sharpness to follow ˜λ(q) which is monotonically increasing until q≥1.
- Core assumption: The function r satisfies Assumption 4.3 ensuring specific monotonicity properties needed for progressive sharpening.
- Evidence anchors:
  - [abstract] "The sharpness increases at the early phase of training (referred to as progressive sharpening)"
  - [section] "the sequence (˜λ(qt))∞t=0 is monotonically increasing"
  - [corpus] Moderate evidence - several EoS papers discuss sharpness but not the progressive nature formally
- Break condition: If r doesn't satisfy the decreasing conditions in Assumption 4.3, the monotonicity of ˜λ(q) fails and progressive sharpening breaks down.

### Mechanism 3
- Claim: Network width affects the strength of trajectory alignment, with wider networks showing more pronounced alignment.
- Mechanism: As width increases, the reparameterized dynamics become more dominated by the loss structure and less by initialization noise, leading to tighter convergence to the bifurcation curve.
- Core assumption: Network width scales the sharpness term in the canonical parameterization proportionally.
- Evidence anchors:
  - [section] "the wider network (m = 256) exhibits a stronger trajectory alignment phenomenon compared to the narrower network (m = 64)"
  - [abstract] No explicit mention of width dependence
  - [corpus] No direct evidence found - this appears to be a novel empirical observation
- Break condition: For extremely narrow networks, the alignment may disappear entirely as initialization effects dominate.

## Foundational Learning

- Concept: Bifurcation theory and period-doubling transitions
  - Why needed here: The core alignment phenomenon is explained through the period-doubling bifurcation at q=1 in the reparameterized dynamics
  - Quick check question: What happens to the stability of fixed points in fq(p) when q crosses the value 1?

- Concept: Canonical reparameterization of neural network parameters
  - Why needed here: Transforms the high-dimensional GD dynamics into a tractable one-dimensional map that reveals the alignment structure
  - Quick check question: How does the canonical parameterization (p,q) = (f(x;Θ)−y, 2∥∇Θf(x;Θ)∥²/η) encode both the loss value and sharpness?

- Concept: Implicit bias of gradient descent in overparameterized settings
  - Why needed here: The paper connects EoS alignment to the broader question of how GD selects specific solutions among many global minima
  - Quick check question: Why does GD prefer solutions with sharpness near 2/η rather than flatter minima?

## Architecture Onboarding

- Component map: Data → reparameterization → bifurcation dynamics → alignment detection → theoretical validation
- Critical path: Data → reparameterization → bifurcation dynamics → alignment detection → theoretical validation
- Design tradeoffs: Choosing between general canonical parameterization (works for most losses) vs. specialized parameterizations (needed for squared loss), balancing theoretical tractability with empirical coverage.
- Failure signatures: (1) q doesn't stabilize near 1 in EoS regime, (2) alignment curve deviates significantly from bifurcation diagram, (3) progressive sharpening doesn't occur as q increases.
- First 3 experiments:
  1. Verify alignment on toy models (logcosh(xy)) with varying initialization to confirm independence from initialization
  2. Test alignment strength vs. network width using fully-connected networks with different widths
  3. Validate theoretical sharpness bounds by comparing ˜λ(qt) to actual Hessian eigenvalues during training

## Open Questions the Paper Calls Out

- How can we understand trajectory alignment when training on multiple data points?
  - Basis in paper: [explicit] The paper mentions extending trajectory alignment to multiple data points using a generalized canonical reparameterization, but notes that the alignment curve differs from the single data point case and its precise characterization is an open question.
  - Why unresolved: The current theoretical analysis focuses on single data point settings, and the alignment phenomenon in multi-point settings exhibits different behavior that requires further investigation.
  - What evidence would resolve it: Theoretical analysis showing the alignment curve for multiple data points, or extensive empirical studies across various network architectures and datasets demonstrating consistent alignment behavior.

- How can we theoretically explain the impact of network width on the presence of trajectory alignment?
  - Basis in paper: [explicit] The paper observes empirically that wider networks exhibit stronger trajectory alignment, but this relationship is not yet theoretically understood.
  - Why unresolved: While the paper provides empirical evidence showing the effect of width on alignment, it lacks a theoretical explanation for why wider networks demonstrate this phenomenon more prominently.
  - What evidence would resolve it: A theoretical framework connecting network width to the trajectory alignment phenomenon, potentially through analysis of how width affects the sharpness evolution or stability properties.

- How can we extend our analysis to encompass squared loss for general neural networks beyond the toy single-neuron example?
  - Basis in paper: [explicit] The paper discusses limitations of the canonical reparameterization for squared loss and only provides analysis for a single-neuron nonlinear network, highlighting this as an area for future work.
  - Why unresolved: The current theoretical framework doesn't naturally extend to squared loss, and the single-neuron example is too limited to generalize to broader neural network architectures.
  - What evidence would resolve it: Development of a new reparameterization scheme or theoretical approach that can handle squared loss in general neural networks, supported by both theoretical proofs and empirical validation.

## Limitations

- The trajectory alignment mechanism relies heavily on the canonical reparameterization, but the paper doesn't fully explore cases where this transformation breaks down.
- While the theory covers linear networks and single-neuron nonlinearities, extension to deeper nonlinear networks remains largely empirical.
- The width-dependent alignment effect is observed empirically but lacks rigorous theoretical justification.

## Confidence

- **High Confidence**: The existence of EoS phenomenon and progressive sharpening (well-established in literature)
- **Medium Confidence**: Trajectory alignment on bifurcation diagrams (strong empirical evidence, partial theory)
- **Medium Confidence**: Width-dependent alignment strength (clear empirical trend, theoretical gap)
- **Medium Confidence**: Extension to multiple data points (supported by single-neuron theory, limited empirical validation)

## Next Checks

1. **Stress-test the canonical reparameterization**: Apply the alignment analysis to networks with different activation functions (ReLU, sigmoid) and architectures (CNNs, ResNets) to identify conditions where the alignment breaks down.

2. **Theoretical width analysis**: Develop analytical bounds on alignment strength as a function of network width, particularly focusing on the transition from narrow to wide networks where the alignment behavior changes.

3. **Robustness to loss functions**: Validate the alignment phenomenon across different loss functions (cross-entropy, Huber loss) and verify whether the bifurcation structure remains consistent or requires adaptation.