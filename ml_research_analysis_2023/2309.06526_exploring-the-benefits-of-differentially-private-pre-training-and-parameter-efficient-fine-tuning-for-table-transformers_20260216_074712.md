---
ver: rpa2
title: Exploring the Benefits of Differentially Private Pre-training and Parameter-Efficient
  Fine-tuning for Table Transformers
arxiv_id: '2309.06526'
source_url: https://arxiv.org/abs/2309.06526
tags:
- tuning
- peft
- fine-tuning
- privacy
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores differentially private pre-training and parameter-efficient
  fine-tuning for TabTransformers on the ACSIncome dataset. Key methods include LoRA,
  Adapter, and Prompt Tuning for fine-tuning.
---

# Exploring the Benefits of Differentially Private Pre-training and Parameter-Efficient Fine-tuning for Table Transformers

## Quick Facts
- arXiv ID: 2309.06526
- Source URL: https://arxiv.org/abs/2309.06526
- Reference count: 0
- Key outcome: PEFT methods reduce trainable parameters by ≥97.86% while maintaining or improving accuracy over full tuning baselines

## Executive Summary
This paper investigates the combination of differentially private pre-training and parameter-efficient fine-tuning (PEFT) for TabTransformers on tabular income prediction tasks. The authors demonstrate that adapter-based PEFT achieves high accuracy (0.7475) with only 1,424 trainable parameters, outperforming full tuning (0.7543 accuracy, 206,193 parameters) while reducing parameters by over 99%. The approach ensures end-to-end privacy through DP-SGD applied in both pre-training and fine-tuning stages, with results showing robust performance across various privacy budgets.

## Method Summary
The method involves pre-training a TabTransformer on California ACSIncome data using DP-SGD, then fine-tuning on Indiana ACSIncome data using PEFT methods (Adapter, LoRA, Deep/Shallow Tuning) with DP-SGD. The TabTransformer backbone parameters are frozen during fine-tuning, with only PEFT modules being trained. The approach is compared against baselines including Full Tuning, Train from Scratch, and Zero-shot Inference. Privacy is ensured through (ε, δ)-DP guarantees with calibrated noise addition and gradient clipping.

## Key Results
- Adapter achieves 0.7475 accuracy with only 1,424 trainable parameters, outperforming Full Tuning (0.7543 accuracy, 206,193 parameters)
- PEFT methods reduce trainable parameters by at least 97.86% compared to full tuning baselines
- Adapter, LoRA, and Deep Tuning all maintain accuracy above 0.7 with <2% of parameters trainable
- PEFT methods show robust tolerance to low privacy budgets (epsilon values) compared to Full Tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adapter-based PEFT preserves privacy while achieving high accuracy in TabTransformer fine-tuning.
- Mechanism: Adapter injects small task-specific modules between pre-trained layers with near-identity initialization, keeping most parameters frozen. This minimizes trainable parameters (1,424) and reduces gradient noise during DP-SGD, improving the privacy-utility tradeoff.
- Core assumption: The pre-trained TabTransformer backbone is sufficiently general to support adaptation via lightweight adapters.
- Evidence anchors:
  - [abstract] "Adapter achieves an accuracy of 0.7475 with only 1,424 trainable parameters"
  - [section 3.5] "Adapter is a transfer learning approach... uses small and task-specific modules"
  - [corpus] Weak evidence: related papers discuss DP + PEFT but not Adapter specifically.
- Break condition: If the pre-trained backbone lacks relevant tabular feature representations, adapter performance degrades.

### Mechanism 2
- Claim: Combining DP-SGD in both pre-training and fine-tuning stages ensures end-to-end privacy.
- Mechanism: DP-SGD adds calibrated Gaussian noise to gradients in both stages, bounding sensitivity and ensuring differential privacy for both datasets.
- Core assumption: The privacy budget (epsilon) can be split across pre-training and fine-tuning without breaking utility.
- Evidence anchors:
  - [section 3.5] "We incorporate DP-SGD into PEFT by initially pre-training... subsequently... fine-tune... with DP-SGD"
  - [section 2.1] DP-SGD description and sensitivity clipping.
  - [corpus] No direct evidence; similar works focus on DP fine-tuning only.
- Break condition: If epsilon values are too low, noise overwhelms gradient signals, collapsing accuracy.

### Mechanism 3
- Claim: PEFT methods significantly reduce the number of trainable parameters, improving efficiency.
- Mechanism: By freezing the backbone and training only adapter/LoRA/prompt parameters, the total trainable count drops by >97.86% compared to full fine-tuning.
- Core assumption: Fine-tuning a small subset of parameters is sufficient for adapting to new tasks.
- Evidence anchors:
  - [abstract] "PEFT methods reduce the number of trainable parameters by at least 97.86%"
  - [section 4.2] Table 1 showing parameter counts for each method.
  - [corpus] Assumption: efficiency gains are consistent across tabular and language tasks.
- Break condition: If task requires large domain shift, frozen backbone limits adaptation.

## Foundational Learning

- Concept: Differential Privacy (DP) and DP-SGD
  - Why needed here: Ensures privacy guarantees for sensitive tabular data during both pre-training and fine-tuning.
  - Quick check question: What is the role of the sensitivity parameter C in DP-SGD?

- Concept: Parameter-efficient fine-tuning (PEFT) methods
  - Why needed here: Reduces computational cost and memory usage while maintaining or improving accuracy.
  - Quick check question: How does LoRA differ from Adapter in terms of architecture?

- Concept: TabTransformer architecture
  - Why needed here: The backbone model whose parameters are mostly frozen during PEFT.
  - Quick check question: What are the main components of TabTransformer?

## Architecture Onboarding

- Component map:
  - Pre-training stage: DP-SGD on source dataset (CA) → TabTransformer backbone
  - Fine-tuning stage: Frozen backbone + PEFT module (Adapter/LoRA/Prompt) + DP-SGD on target dataset (IN)

- Critical path:
  - Load pre-trained DP-TabTransformer → freeze all backbone parameters → inject PEFT module → fine-tune with DP-SGD on target data → evaluate accuracy

- Design tradeoffs:
  - Higher epsilon (less privacy) → higher accuracy but weaker privacy guarantee
  - More trainable parameters (e.g., Full Tuning) → higher accuracy but higher computational cost
  - Adapter vs LoRA: similar parameter count, different injection points

- Failure signatures:
  - Accuracy collapses when epsilon is too low (noise overwhelms gradients)
  - Adapter/LoRA training diverges if backbone is too dissimilar to target data

- First 3 experiments:
  1. Verify pre-training DP-TabTransformer accuracy on source dataset (CA)
  2. Apply Adapter with DP-SGD on target dataset (IN) and measure accuracy vs parameter count
  3. Compare Adapter vs LoRA vs Deep Tuning under same epsilon budget on target data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different levels of privacy (ϵ) impact the trade-off between accuracy and parameter efficiency across various PEFT methods when applied to TabTransformers?
- Basis in paper: [explicit] The paper shows that PEFT methods maintain high accuracy while significantly reducing parameters, and that PEFT methods exhibit a robust tolerance to low values of ϵ compared to Full Tuning.
- Why unresolved: The paper provides comparative results for different ϵ values but does not provide a detailed analysis of the optimal trade-off points for each PEFT method.
- What evidence would resolve it: A comprehensive analysis comparing accuracy and parameter efficiency across various ϵ values for each PEFT method, identifying optimal trade-off points.

### Open Question 2
- Question: How does the performance of PEFT methods on TabTransformers compare when applied to different tabular datasets with varying characteristics?
- Basis in paper: [inferred] The paper only tests on the ACSIncome dataset, which may not represent the full range of tabular data characteristics.
- Why unresolved: The study is limited to a single dataset, and the generalizability of the results to other tabular datasets is unknown.
- What evidence would resolve it: Extensive testing of PEFT methods on a diverse range of tabular datasets with varying characteristics (e.g., size, feature types, noise levels) to assess generalizability.

### Open Question 3
- Question: What is the impact of differentially private pre-training on the performance of TabTransformers compared to non-private pre-training, and how does this impact vary across different PEFT methods?
- Basis in paper: [explicit] The paper explores differentially private pre-training and fine-tuning but does not provide a direct comparison with non-private pre-training.
- Why unresolved: The study focuses on the benefits of combining DP with PEFT but does not isolate the impact of DP pre-training on performance.
- What evidence would resolve it: A controlled experiment comparing the performance of TabTransformers with and without DP pre-training, across different PEFT methods, to isolate the impact of DP pre-training.

### Open Question 4
- Question: How do different combinations of pre-training and fine-tuning privacy levels (ϵp and ϵf) affect the overall privacy-accuracy trade-off in TabTransformers using PEFT methods?
- Basis in paper: [explicit] The paper varies both pre-training (ϵp) and fine-tuning (ϵf) privacy levels but does not provide a detailed analysis of their combined effects.
- Why unresolved: While the paper shows that different combinations of ϵp and ϵf affect accuracy, it does not provide a systematic analysis of the optimal combinations for privacy-accuracy trade-offs.
- What evidence would resolve it: A detailed analysis of the privacy-accuracy trade-off for different combinations of ϵp and ϵf, identifying optimal combinations for each PEFT method.

## Limitations

- Exact DP-SGD noise scale values and PEFT module configurations are underspecified, affecting reproducibility
- Results are only validated on one source-target dataset pair (CA→IN), limiting generalizability claims
- The paper does not analyze optimal privacy budget allocation between pre-training and fine-tuning stages

## Confidence

- High confidence in the mechanism of PEFT reducing trainable parameters (>97.86% reduction) - directly supported by parameter count tables
- Medium confidence in DP-SGD ensuring end-to-end privacy - the mechanism is well-established but specific hyperparameter choices are underspecified
- Low confidence in the generalizability of results across different tabular domains - only one source-target dataset pair is tested

## Next Checks

1. Replicate the parameter count reduction claim by implementing Adapter with exact module sizes and counting trainable parameters
2. Test the same PEFT+DP approach on a different tabular dataset pair to verify robustness of the privacy-accuracy tradeoff
3. Conduct ablation studies varying ε pre-training vs ε fine-tuning to identify optimal privacy budget allocation