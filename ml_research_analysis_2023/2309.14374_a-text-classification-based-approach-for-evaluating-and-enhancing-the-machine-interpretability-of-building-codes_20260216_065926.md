---
ver: rpa2
title: A Text Classification-Based Approach for Evaluating and Enhancing the Machine
  Interpretability of Building Codes
arxiv_id: '2309.14374'
source_url: https://arxiv.org/abs/2309.14374
tags:
- interpretability
- codes
- building
- clauses
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research addresses the problem of evaluating and enhancing
  the machine interpretability of building codes, which is essential for intelligent
  design and construction. The study proposes a novel approach that automatically
  evaluates and enhances the machine interpretability of single clauses and entire
  building codes.
---

# A Text Classification-Based Approach for Evaluating and Enhancing the Machine Interpretability of Building Codes

## Quick Facts
- arXiv ID: 2309.14374
- Source URL: https://arxiv.org/abs/2309.14374
- Reference count: 0
- Primary result: A BERT-based model (RuleBERT) achieves 93.60% F1-score on classifying building code clause interpretability, significantly outperforming existing methods.

## Executive Summary
This research addresses the critical challenge of machine interpretability in building codes, which is essential for automating rule interpretation in intelligent design and construction. The study introduces a novel approach that automatically evaluates and enhances the machine interpretability of both individual clauses and entire building codes. By developing a text classification model using a domain-specific pretrained language model (RuleBERT) with transfer learning, the researchers achieve state-of-the-art performance on classifying clauses into interpretability categories. The approach is applied to over 150 Chinese building codes, revealing an average interpretability of 34.40%, indicating significant room for improvement in transforming regulatory documents into computer-processable formats.

## Method Summary
The approach involves collecting and preprocessing Chinese regulatory texts, then manually annotating 1,450 clauses across 7 interpretability categories. Data augmentation through word replacement balances the dataset for underrepresented categories. A BERT model is further pretrained on a domain-specific corpus of regulatory texts to create RuleBERT, which is then fine-tuned on the annotated dataset. The model's performance is evaluated using weighted F1-score and validated through downstream application to Chinese building codes. The classification results are used to assess document-level interpretability and filter clauses for improved automated rule interpretation.

## Key Results
- RuleBERT achieves an F1-score of 93.60%, outperforming existing CNN- or RNN-based methods (72.16%)
- The classification method enhances downstream automated rule interpretation by 4% accuracy
- Average interpretability of Chinese fundamental codes is 34.40%, with fire protection codes showing the highest interpretability at 58.64% for GB level codes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Domain-specific pretraining of BERT (RuleBERT) significantly improves text classification performance on building code interpretability tasks.
- **Mechanism:** The original BERT model is pretrained on general-domain corpora, which differ in data distribution from the AEC domain. By further pretraining BERT on a domain-specific corpus of regulatory texts, the model gains domain-specific semantic understanding, leading to better performance on interpretability classification.
- **Core assumption:** The domain corpus used for pretraining is representative of the target task data and captures relevant domain-specific language patterns.
- **Evidence anchors:**
  - [abstract] "a domain-specific BERT model" and "Experiments show that the proposed text classification algorithm outperforms the existing CNN- or RNN-based methods"
  - [section 4.3] "RuleBERT achieves the global best weighted F1-score of 93.60%, which corresponds to a state-of-the-art effect."
  - [corpus] Weak evidence: Only general similarity in topic (NLP, classification) but not specific domain relevance.
- **Break condition:** If the domain corpus is too small, noisy, or unrepresentative of the target task, the further pretraining may not improve or could even degrade performance.

### Mechanism 2
- **Claim:** Classifying clauses into interpretability categories enables automated filtering of interpretable clauses for downstream rule interpretation.
- **Mechanism:** By categorizing clauses into 7 categories based on their interpretability (direct, indirect, method, reference, general, term, other), the system can automatically filter out clauses that are difficult or impossible to interpret, reducing errors in downstream automated rule interpretation (ARI).
- **Core assumption:** The 7-category classification system accurately reflects the true interpretability of clauses and that filtering based on these categories improves ARI accuracy.
- **Evidence anchors:**
  - [abstract] "The proposed classification method can enhance downstream ARI methods with an improvement of 4%."
  - [section 5] Experiment showing accuracy improvement from 68% to 72% after filtering.
  - [section 3.1] Detailed definitions of each category and their interpretability levels.
- **Break condition:** If the classification system misclassifies clauses, filtering based on categories could remove interpretable clauses or retain uninterpretable ones, negating the benefits.

### Mechanism 3
- **Claim:** Data augmentation through word replacement balances the dataset and improves model performance on imbalanced classification tasks.
- **Mechanism:** The original dataset has fewer clauses in the "direct" and "indirect" categories. By replacing numerical values and comparison operators in these clauses, new clauses are generated, balancing the dataset and providing more training examples for these underrepresented categories.
- **Core assumption:** Replacing numbers and operators creates semantically valid new clauses that are representative of the original category.
- **Evidence anchors:**
  - [section 3.2] "data augmentation can be carried out on these two categories of clauses to expand and balance the dataset" with specific examples of word replacement.
  - [section 4.3] RuleBERT achieves 93.60% F1-score, suggesting effective handling of the dataset.
  - [corpus] No direct evidence; assumption based on methodology description.
- **Break condition:** If word replacement creates unrealistic or invalid clauses, the augmented data could mislead the model and reduce performance.

## Foundational Learning

- **Concept:** Natural Language Processing (NLP) and Text Classification
  - Why needed here: The entire approach relies on automatically classifying clauses based on their interpretability using NLP techniques.
  - Quick check question: What is the difference between token classification and text classification in NLP?

- **Concept:** Deep Learning and Pretrained Language Models
  - Why needed here: The approach uses BERT, a pretrained language model, and further pretrains it on domain-specific data.
  - Quick check question: What is the key advantage of using pretrained models like BERT over training from scratch?

- **Concept:** Domain-Specific Language Modeling
  - Why needed here: The paper emphasizes the importance of domain-specific pretraining to improve performance on AEC regulatory texts.
  - Quick check question: Why might a model pretrained on general text perform poorly on specialized domain texts?

## Architecture Onboarding

- **Component map:** Data collection and cleaning pipeline -> Manual annotation process for dataset creation -> Data augmentation module (word replacement) -> BERT model (original and RuleBERT) -> Fine-tuning pipeline for text classification -> Interpretability evaluation module (clause-level and document-level) -> Integration with downstream ARI systems

- **Critical path:** 1. Data collection and cleaning 2. Manual annotation for initial dataset 3. Data augmentation to balance categories 4. Further pretraining of BERT on domain corpus (RuleBERT) 5. Fine-tuning RuleBERT on annotated dataset 6. Clause classification and interpretability evaluation 7. Integration with ARI systems

- **Design tradeoffs:**
  - Using a pretrained model (BERT) vs. training from scratch: Pretrained models offer better performance with less data but may have domain mismatch.
  - Manual annotation vs. automated labeling: Manual annotation ensures quality but is time-consuming and expensive.
  - Data augmentation vs. collecting more real data: Augmentation is cheaper but may introduce synthetic patterns.

- **Failure signatures:** Low classification accuracy (F1-score below acceptable threshold), misclassification of clauses leading to incorrect filtering in ARI, overfitting to augmented data patterns, poor generalization to new building codes or domains

- **First 3 experiments:**
  1. Train and evaluate the original BERT model on the annotated dataset to establish baseline performance.
  2. Implement and evaluate RuleBERT (BERT further pretrained on domain corpus) to measure improvement from domain-specific pretraining.
  3. Apply the best model to classify clauses from a new building code and manually evaluate the interpretability assessment accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the interpretability of building codes in domains other than fire protection compare to the 34.40% average found in this study?
- **Basis in paper:** [explicit] The paper states that the average interpretability of Chinese fundamental codes is 34.40%, with fire protection codes having the highest interpretability at 58.64% for GB level codes. However, it does not provide specific interpretability scores for other domains like structural, electrical, or water supply and drainage.
- **Why unresolved:** The paper provides a general average and highlights the fire protection domain, but lacks detailed interpretability scores for other specific domains. This gap leaves uncertainty about how different domains compare to the overall average.
- **What evidence would resolve it:** Conducting a comprehensive analysis of building codes across various domains, similar to the fire protection domain analysis, would provide specific interpretability scores for each domain. This would allow for a direct comparison to the overall average and highlight domains with higher or lower interpretability.

### Open Question 2
- **Question:** What are the specific reasons why certain clauses in building codes are difficult to interpret by machines, and how can these issues be addressed?
- **Basis in paper:** [explicit] The paper mentions that clauses with low interpretability often involve complex information, external references, or macro design guidance that are challenging for machines to process. However, it does not delve into the specific reasons for these difficulties or propose detailed solutions.
- **Why unresolved:** While the paper identifies general categories of difficult clauses, it lacks an in-depth analysis of the underlying reasons for machine interpretability challenges and potential solutions. This leaves a gap in understanding how to improve machine interpretability.
- **What evidence would resolve it:** A detailed analysis of specific clauses that are difficult to interpret, including their structure, language, and context, would help identify the root causes of interpretability issues. Additionally, proposing and testing solutions, such as improved data structures or domain-specific knowledge bases, would provide actionable insights to enhance machine interpretability.

### Open Question 3
- **Question:** How can the performance of the RuleBERT model be further improved for text classification tasks in the AEC domain?
- **Basis in paper:** [explicit] The paper demonstrates that the RuleBERT model outperforms traditional CNN- or RNN-based methods, achieving an F1-score of 93.60%. However, it does not explore potential enhancements to the model or alternative approaches that could yield even better results.
- **Why unresolved:** The paper focuses on the success of the RuleBERT model but does not investigate further improvements or compare it with other advanced models. This leaves room for exploration of additional techniques to enhance performance.
- **What evidence would resolve it:** Experimenting with different model architectures, such as transformer-based models with larger datasets or incorporating attention mechanisms, could potentially improve performance. Additionally, exploring techniques like prompt learning or few-shot learning could address the small sample issue and enhance the model's effectiveness.

## Limitations

- The effectiveness of domain-specific pretraining relies heavily on the assumption that the Chinese regulatory corpus is truly representative of building code domain language, with limited validation of corpus quality.
- The interpretability classification system's validity depends entirely on the manual annotation process, with no reported inter-annotator agreement or detailed annotation guidelines.
- The data augmentation technique using word replacement lacks detailed validation and evidence that augmented clauses maintain semantic validity without introducing artificial patterns.

## Confidence

- **High confidence:** Overall methodology framework and technical implementation details of the BERT-based classification approach
- **Medium confidence:** Actual improvement in downstream ARI systems (4% improvement claim) due to limited experimental validation details and lack of comparison with alternative filtering approaches
- **Low confidence:** Generalizability of results across different regulatory domains and languages, as the study focuses exclusively on Chinese building codes without cross-domain validation

## Next Checks

1. Conduct inter-annotator agreement analysis on a subset of the labeled dataset to quantify annotation consistency and reliability.

2. Perform ablation studies comparing RuleBERT performance with and without domain pretraining using identical architectures and training procedures.

3. Test the classification model on building codes from different countries/regulatory domains to assess cross-domain generalizability and identify potential overfitting to Chinese regulatory language patterns.