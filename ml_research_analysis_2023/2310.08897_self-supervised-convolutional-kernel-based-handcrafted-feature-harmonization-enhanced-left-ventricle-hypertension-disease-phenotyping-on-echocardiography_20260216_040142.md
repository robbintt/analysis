---
ver: rpa2
title: 'Self supervised convolutional kernel based handcrafted feature harmonization:
  Enhanced left ventricle hypertension disease phenotyping on echocardiography'
arxiv_id: '2310.08897'
source_url: https://arxiv.org/abs/2310.08897
tags:
- harmonization
- features
- feature
- handcrafted
- convolutional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a self-supervised convolutional kernel-based
  method for harmonizing handcrafted features in echocardiography images to improve
  left ventricular hypertrophy (LVH) disease phenotyping. The approach uses a ConvNeXt-V2
  model trained via masked autoencoder to generate convolutional filters that preprocess
  echocardiography images into harmonized feature maps.
---

# Self supervised convolutional kernel based handcrafted feature harmonization: Enhanced left ventricle hypertension disease phenotyping on echocardiography

## Quick Facts
- arXiv ID: 2310.08897
- Source URL: https://arxiv.org/abs/2310.08897
- Authors: 
- Reference count: 7
- One-line primary result: Self-supervised ConvNeXt-V2 filters reduce inter-manufacturer radiomic feature variability and improve LVH disease classification (AUC 0.868) compared to original and ComBat harmonization

## Executive Summary
This study addresses the challenge of radiomic feature variability across different echocardiography manufacturers by proposing a self-supervised convolutional kernel-based harmonization method. The approach uses a ConvNeXt-V2 model trained via masked autoencoder to generate convolutional filters that preprocess echocardiography images into harmonized feature maps. These harmonized features enable more robust radiomic analysis across imaging devices. The method demonstrates superior harmonization performance measured by Jensen-Shannon divergence and achieves improved classification of left ventricular hypertrophy between hypertrophic cardiomyopathy and hypertensive heart disease.

## Method Summary
The proposed method employs a ConvNeXt-V2 model trained using self-supervised learning on echocardiography data through masked autoencoder training. The convolutional filters from the first layer of this trained model are then applied as preprocessing steps to raw echocardiography images, generating harmonized feature maps. Radiomic features are extracted from these filtered images using PyRadiomics with discretized gray-level intensity values. The harmonization effectiveness is evaluated using Jensen-Shannon divergence between feature distributions from different manufacturers, and the harmonized features are used for downstream classification of left ventricular hypertrophy between hypertrophic cardiomyopathy and hypertensive heart disease using XGBoost with Boruta feature selection.

## Key Results
- The proposed method achieves AUC of 0.868, sensitivity of 0.844, and accuracy of 0.852 for LVH classification between HCM and HHD
- Jensen-Shannon divergence values show reduced feature variability between manufacturers compared to original and ComBat harmonization approaches
- Harmonized features outperform both original features and ComBat harmonization in classification performance metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-supervised learning with ConvNeXt-V2 extracts generalizable convolutional filters that act as effective preprocessing for radiomic feature harmonization
- Mechanism: The ConvNeXt-V2 model, pre-trained using masked autoencoder on echocardiography data, learns convolutional filters that capture robust, device-invariant image representations. These filters preprocess images into harmonized feature maps, reducing variability from different imaging devices before radiomic feature extraction
- Core assumption: Convolutional filters learned through self-supervised training capture generalizable features applicable to downstream tasks like radiomic feature extraction
- Evidence anchors:
  - [abstract]: "uses a ConvNeXt-V2 model trained via masked autoencoder to generate convolutional filters that preprocess echocardiography images into harmonized feature maps"
  - [section]: "we used the first convolutional layer of ConvNeXt-V2 as the 2D convolutional imaging filter, enabling us to generate filtered echocardiography images enriched with robust representative features"
  - [corpus]: Weak - corpus neighbors do not directly address ConvNeXt-V2 use for radiomic feature harmonization; they discuss SSL in other domains

### Mechanism 2
- Claim: The harmonization filter reduces Jensen-Shannon divergence between radiomic features extracted from different manufacturers' devices
- Mechanism: By applying the learned convolutional filter to raw echocardiography images, the resulting feature maps have reduced inter-manufacturer variability. This is quantified by lower Jensen-Shannon divergence (JSD) values between probability distributions of radiomic features from different manufacturers
- Core assumption: The convolutional filter can effectively remove device-specific variations while preserving disease-relevant information in the images
- Evidence anchors:
  - [abstract]: "Evaluation using Jensen-Shannon divergence shows the method reduces feature variability between manufacturers"
  - [section]: "we employed the Jensen-Shannon divergence (JSD) as a metric to evaluate the performance of the harmonization techniques...smaller values indicating greater similarity in their shapes"
  - [corpus]: Weak - corpus neighbors do not discuss Jensen-Shannon divergence in the context of medical imaging harmonization

### Mechanism 3
- Claim: Harmonized radiomic features improve downstream LVH classification performance compared to original or ComBat harmonization
- Mechanism: By reducing inter-manufacturer variability and noise in radiomic features, the harmonized features provide more consistent input to the XGBoost classifier, improving its ability to distinguish between HCM and HHD
- Core assumption: Reducing feature variability through preprocessing leads to better generalization and classification performance in machine learning models
- Evidence anchors:
  - [abstract]: "For LVH classification between hypertrophic cardiomyopathy and hypertensive heart disease, the proposed method achieves an AUC of 0.868...outperforming both original and ComBat approaches"
  - [section]: "the proposed harmonization technique sets a new standard for preprocessing handcrafted features...substantially enhanced the JSD values and classification performance"
  - [corpus]: Weak - corpus neighbors do not discuss classification performance improvements from feature harmonization in medical imaging

## Foundational Learning

- Concept: Self-supervised learning and masked autoencoders
  - Why needed here: SSL allows learning robust image representations without requiring labeled data, which is crucial for medical imaging where annotated data is scarce. Masked autoencoders specifically help learn global and local features from partial image information
  - Quick check question: What is the key difference between supervised and self-supervised learning in the context of medical image preprocessing?

- Concept: Radiomic feature extraction and robustness
  - Why needed here: Understanding radiomic features and their sensitivity to imaging parameters is essential to appreciate why harmonization is necessary and how the proposed method addresses this challenge
  - Quick check question: Why are radiomic features particularly sensitive to variations in imaging protocols and devices?

- Concept: Jensen-Shannon divergence as a similarity metric
  - Why needed here: JSD is used to quantitatively evaluate the effectiveness of the harmonization approach by measuring the similarity between feature distributions from different manufacturers
  - Quick check question: How does Jensen-Shannon divergence differ from Kullback-Leibler divergence, and why is it more appropriate for comparing feature distributions?

## Architecture Onboarding

- Component map: Input (raw echocardiography images) -> Preprocessing (resize, mask, convolutional filtering) -> Feature extraction (radiomic features from filtered images) -> Harmonization evaluation (JSD calculation) -> Classification (XGBoost on harmonized features)
- Critical path: ConvNeXt-V2 training -> Filter application to images -> Radiomic feature extraction -> Classification model training and evaluation
- Design tradeoffs: Using a large, pre-trained ConvNeXt-V2 model provides strong feature extraction but increases computational requirements. The choice of radiomic features and their discretization parameters affects both harmonization and classification performance
- Failure signatures: High JSD values between manufacturers indicate poor harmonization; low classification performance despite good harmonization suggests loss of clinically relevant information; overfitting during ConvNeXt-V2 training manifests as poor generalization to new data
- First 3 experiments:
  1. Train ConvNeXt-V2 on the AI-Hub dataset and visualize the learned convolutional filters to verify they capture meaningful image patterns
  2. Apply the learned filter to a small set of GE and Philips images, extract radiomic features, and calculate JSD to verify harmonization
  3. Train an XGBoost classifier on harmonized features from a balanced subset of the dataset and evaluate AUC to establish baseline classification performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed harmonization method scale with dataset size and diversity across manufacturers?
- Basis in paper: [explicit] The authors note that ConvNeXtV2-Echo, trained on a modest dataset, closely mirrored ConvNeXtV2-Color trained on ImageNet-1k, suggesting that with a larger and more diverse dataset, performance might improve further
- Why unresolved: The study was limited by the available dataset size, particularly in the downstream task dataset where there was a 4:1 imbalance between GE and Philips samples
- What evidence would resolve it: Testing the harmonization method on a much larger, more balanced dataset across multiple manufacturers would demonstrate whether performance improves with increased data diversity and volume

### Open Question 2
- Question: What are the biological implications of the harmonized features in distinguishing between HCM and HHD beyond classification accuracy?
- Basis in paper: [inferred] While the study demonstrates improved classification performance, it does not explore the clinical or biological significance of the features that contribute to distinguishing these diseases
- Why unresolved: The study focuses on technical performance metrics but does not investigate the pathophysiological meaning of the harmonized features
- What evidence would resolve it: Correlation of the most important harmonized features with known clinical biomarkers or histopathological findings in HCM and HHD patients would provide biological validation

### Open Question 3
- Question: How generalizable is the proposed harmonization filter to other imaging modalities beyond echocardiography?
- Basis in paper: [explicit] The authors suggest that their SSL-based convolutional kernel approach could be adapted for other imaging contexts, but this was not tested
- Why unresolved: The method was developed and validated specifically for echocardiography images, with no testing on other modalities like CT, MRI, or X-ray
- What evidence would resolve it: Applying the same methodology to other imaging modalities and comparing harmonization effectiveness would demonstrate generalizability

## Limitations
- No comparative performance metrics against alternative SSL architectures beyond ComBat
- Lack of ablation studies isolating the contribution of the convolutional filter versus radiomic feature selection
- Insufficient validation of generalizability across different cardiac views beyond PLAX

## Confidence
- Mechanism (self-supervised learning extracting generalizable filters): High
- ConvNeXt-V2 superiority over other SSL architectures: Medium
- Generalizability to other cardiac views or disease types: Low

## Next Checks
1. Conduct ablation studies removing the self-supervised filtering step to quantify its specific contribution to harmonization and classification performance
2. Test the harmonization approach on additional cardiac views (apical 4-chamber, 2-chamber) to assess generalizability
3. Compare performance against other SSL architectures (DINO, MAE with ViT) to validate the choice of ConvNeXt-V2