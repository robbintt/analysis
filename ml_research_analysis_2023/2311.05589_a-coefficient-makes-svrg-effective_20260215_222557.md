---
ver: rpa2
title: A Coefficient Makes SVRG Effective
arxiv_id: '2311.05589'
source_url: https://arxiv.org/abs/2311.05589
tags:
- svrg
- uni00000013
- uni00000011
- uni00000014
- variance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of Stochastic Variance
  Reduced Gradient (SVRG) for deep learning. While SVRG theoretically reduces gradient
  variance, it struggles in practice for deep neural networks.
---

# A Coefficient Makes SVRG Effective

## Quick Facts
- arXiv ID: 2311.05589
- Source URL: https://arxiv.org/abs/2311.05589
- Authors: 
- Reference count: 14
- One-line primary result: Introducing a linearly decaying coefficient α to control variance reduction in SVRG improves training loss for deep neural networks.

## Executive Summary
This paper investigates why Stochastic Variance Reduced Gradient (SVRG) struggles in practice for deep learning despite its theoretical benefits. The authors find that SVRG's default coefficient of 1 becomes too aggressive for deeper networks as the correlation between snapshot and current model gradients decreases during training. They propose α-SVRG, which introduces a multiplicative coefficient α that is linearly decayed during training to control the strength of the variance reduction term. Experiments across multiple model architectures and image classification datasets demonstrate that α-SVRG consistently reduces training loss compared to both the baseline AdamW optimizer and standard SVRG.

## Method Summary
The authors propose α-SVRG, a modification of the standard SVRG algorithm that introduces a multiplicative coefficient α to control the strength of the variance reduction term. Unlike standard SVRG which uses a fixed coefficient of 1, α-SVRG applies a linearly decaying schedule to α, starting from an initial value and decreasing toward 0 as training progresses. This adjustment is motivated by empirical observations that the correlation between snapshot gradients and current model gradients weakens with model depth and training progress. The method is evaluated across various model architectures (ConvNeXt-F, ViT-T/16, Swin-F, Mixer-S/32, ViT-B/16, ConvNeXt-B) and image classification datasets (ImageNet-1K, CIFAR-100, Flowers, Pets, STL-10, Food-101, DTD, SVHN, EuroSAT), with training following the ConvNeXt recipe using AdamW optimizer.

## Key Results
- α-SVRG consistently reduces training loss compared to both AdamW baseline and standard SVRG across multiple model architectures and datasets
- The optimal coefficient for SVRG decreases as model depth increases and as training advances, contrary to the standard SVRG default of 1
- α-SVRG shows improved effectiveness with larger batch sizes, suggesting that maintaining correlation between snapshot and current gradients is crucial for variance reduction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The default coefficient of 1 in standard SVRG is too large for deeper neural networks because the correlation between snapshot gradients and current model gradients decreases as training progresses.
- Mechanism: SVRG uses control variates to reduce gradient variance by forming a correction term from the difference between a snapshot model's full gradient and its stochastic gradient. The effectiveness of this correction depends on the correlation between these gradients. For deeper models, this correlation weakens during training, making a coefficient of 1 too aggressive and potentially increasing variance.
- Core assumption: The correlation between snapshot and current model gradients decreases with model depth and training progress.
- Evidence anchors:
  - [section] "Our empirical analysis then leads to two key observations about this optimal coefficient: (1) as depth of the model increases, the optimal coefficient becomes smaller; (2) as training advances, the optimal coefficient decreases, consistently dropping well below the standard SVRG's default coefficient of 1."
  - [abstract] "Our analysis finds that, for deeper networks, the strength of the variance reduction term in SVRG should be smaller and decrease as training progresses."
  - [corpus] No direct evidence in corpus papers about coefficient scaling with depth.
- Break condition: If the gradient landscape becomes highly non-stationary or if the snapshot interval is too long, the correlation may become too weak even for small coefficients.

### Mechanism 2
- Claim: Using a linearly decaying coefficient α allows α-SVRG to match the behavior of an optimal coefficient that would be computed at each iteration.
- Mechanism: Instead of computing the optimal coefficient analytically (which is computationally expensive for large networks), α-SVRG applies a preset linearly decaying schedule. This schedule approximates the trend observed in the optimal coefficient values, starting from a higher value and decreasing as training progresses.
- Core assumption: A linearly decaying coefficient can approximate the trend of the optimal coefficient without computing it explicitly.
- Evidence anchors:
  - [section] "From our analysis above, it becomes clear that the best coefficient for SVRG is not necessarily 1 for multi-layer networks. However, computing the optimal coefficient at each iteration would result in a complexity similar to full batch gradient descent. This approach quickly becomes impractical for larger networks like ResNet. In this section, we demonstrate how using a preset schedule of α values can achieve a similar effect of using the computed optimal coefficients."
  - [section] "To evaluate how well α-SVRG matches SVRG with optimal coefficient, we train a MLP-4 using α-SVRG and compare it to SVRG with optimal coefficient and the baseline. The results are presented in Figure 6. Interestingly, α-SVRG exhibits a gradient variance trend that is not much different from SVRG with optimal coefficient."
  - [corpus] No direct evidence in corpus papers about linear decay schedules.
- Break condition: If the optimal coefficient's decay pattern is non-linear or if the decay is too aggressive, the linear schedule may become suboptimal.

### Mechanism 3
- Claim: The effectiveness of α-SVRG depends on maintaining sufficient correlation between snapshot and current model gradients, which requires a sufficiently large batch size.
- Mechanism: The batch size affects the variance among mini-batch gradients. A larger batch size provides a better estimate of the true gradient, which helps maintain correlation between snapshot and current model gradients. This correlation is crucial for the variance reduction term to be effective.
- Core assumption: Larger batch sizes maintain better correlation between snapshot and current model gradients.
- Evidence anchors:
  - [section] "Since the batch size controls the variance among mini-batch data, we change the batch size to understand how it affects α-SVRG. We also scale the learning rate linearly (Goyal et al., 2017). The default batch size is 128. In Figure 10, we can see that α-SVRG leads to a lower training loss when the batch size is larger, but it is worse than the baseline when the batch size is smaller."
  - [corpus] No direct evidence in corpus papers about batch size requirements for SVRG variants.
- Break condition: If the batch size is too small, the gradient estimates become too noisy, weakening the correlation and making the variance reduction term ineffective.

## Foundational Learning

- Concept: Control variates in Monte Carlo methods
  - Why needed here: Understanding control variates is essential because SVRG is based on this technique to reduce gradient variance. The optimal coefficient derivation relies on this framework.
  - Quick check question: What is the formula for the optimal coefficient in control variates, and what does it depend on?

- Concept: Gradient variance metrics
  - Why needed here: The paper uses multiple metrics to quantify gradient variance, which is central to evaluating SVRG's effectiveness. Understanding these metrics is crucial for interpreting results.
  - Quick check question: What are the three gradient variance metrics used in the paper, and how do they differ in what they measure?

- Concept: Snapshot model concept in SVRG
  - Why needed here: The snapshot model is a core component of SVRG that enables variance reduction. Understanding its role and how it becomes outdated is key to grasping why α-SVRG is needed.
  - Quick check question: How often is the snapshot model updated in the experiments, and why might this lead to the snapshot becoming outdated?

## Architecture Onboarding

- Component map:
  - Base optimizer (AdamW) -> SVRG mechanism -> Coefficient α -> Linear decay schedule -> Snapshot model

- Critical path:
  1. Initialize model and parameters
  2. At each iteration, compute current stochastic gradient
  3. Compute snapshot model gradient (periodically)
  4. Apply variance reduction with current α
  5. Update model parameters
  6. Decay α according to schedule
  7. Repeat

- Design tradeoffs:
  - Snapshot frequency vs. computational cost: More frequent snapshots maintain better correlation but increase computation
  - Initial α value vs. convergence speed: Higher initial α provides stronger variance reduction but may overshoot
  - Linear decay vs. other schedules: Linear is simple but may not match optimal decay pattern
  - Batch size vs. effectiveness: Larger batches maintain better correlation but increase memory usage

- Failure signatures:
  - Training loss plateaus or increases: Indicates coefficient is too large or snapshot too outdated
  - Gradient variance increases: Suggests the variance reduction term is ineffective
  - No improvement over baseline: May indicate insufficient batch size or inappropriate α schedule

- First 3 experiments:
  1. Train a simple MLP (e.g., MLP-4) on CIFAR-10 with α-SVRG vs. baseline to verify basic effectiveness
  2. Vary initial α values (0.5, 0.75, 1.0) to test robustness to coefficient choice
  3. Test different snapshot intervals (1, 39, 312 iterations) to understand tradeoff between effectiveness and computational cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of α-SVRG vary across different architectures and datasets beyond those tested?
- Basis in paper: [explicit] The paper mentions the potential of α-SVRG in handling more complex models but does not provide extensive empirical evidence.
- Why unresolved: The paper focuses on a limited set of models and datasets, leaving the generalizability of α-SVRG's effectiveness across a broader range of architectures and datasets uncertain.
- What evidence would resolve it: Extensive experiments on a wider variety of neural network architectures (e.g., recurrent networks, transformers for NLP tasks) and datasets (e.g., large-scale vision datasets, text datasets) would provide evidence for the generalizability of α-SVRG's effectiveness.

### Open Question 2
- Question: What is the optimal schedule for the decay of the coefficient α in α-SVRG?
- Basis in paper: [inferred] The paper introduces a linearly decaying coefficient α but does not explore alternative decay schedules or optimize the decay rate.
- Why unresolved: The linear decay schedule is chosen based on observations but not derived from a theoretical framework or extensive empirical tuning.
- What evidence would resolve it: Theoretical analysis or empirical experiments exploring different decay schedules (e.g., exponential decay, step decay) and decay rates would provide insights into the optimal schedule for the coefficient α in α-SVRG.

### Open Question 3
- Question: How does the effectiveness of α-SVRG compare to other variance reduction techniques in deep learning?
- Basis in paper: [inferred] The paper focuses on α-SVRG and its comparison to the standard SVRG, but does not provide a comprehensive comparison with other variance reduction techniques.
- Why unresolved: The paper does not explore the relative effectiveness of α-SVRG compared to other state-of-the-art variance reduction techniques in deep learning.
- What evidence would resolve it: Empirical experiments comparing α-SVRG to other variance reduction techniques (e.g., SAGA, SARAH, SpiderBoost) on a range of models and datasets would provide insights into its relative effectiveness.

## Limitations

- The paper focuses exclusively on image classification tasks and specific model architectures, limiting generalizability to other domains
- The linear decay schedule for α is chosen based on empirical observations but lacks theoretical justification or comparison to alternative schedules
- The effectiveness of α-SVRG is shown to depend on batch size, but the relationship between batch size and performance is not thoroughly explored

## Confidence

Our confidence in the core findings is **High** for the observation that SVRG's effectiveness diminishes for deep networks, **Medium** for the proposed mechanism explaining why the coefficient needs adjustment, and **Low** for the generalizability of the linear decay schedule across all deep learning tasks. Major uncertainties include whether the linearly decaying coefficient schedule is optimal for all network architectures and datasets, and whether the batch size requirements are universal or architecture-dependent.

## Next Checks

1. Test α-SVRG on non-image tasks (e.g., NLP or speech recognition) to assess generalizability beyond the studied domain
2. Experiment with non-linear decay schedules (e.g., exponential or cosine) to determine if they outperform the linear schedule for different architectures
3. Analyze the impact of varying snapshot intervals on α-SVRG's effectiveness to understand the tradeoff between correlation maintenance and computational cost