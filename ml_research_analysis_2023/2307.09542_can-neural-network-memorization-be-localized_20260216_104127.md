---
ver: rpa2
title: Can Neural Network Memorization Be Localized?
arxiv_id: '2307.09542'
source_url: https://arxiv.org/abs/2307.09542
tags:
- examples
- memorization
- clean
- layer
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates whether memorization in neural networks
  can be localized to specific layers or neurons. Through three experimental approaches
  - gradient accounting, layer rewinding, and layer retraining - the authors show
  that memorization is not confined to final layers but is instead distributed across
  multiple layers.
---

# Can Neural Network Memorization Be Localized?

## Quick Facts
- arXiv ID: 2307.09542
- Source URL: https://arxiv.org/abs/2307.09542
- Reference count: 40
- Primary result: Memorization in neural networks is distributed across multiple layers but localized to specific neurons, with example-tied dropout reducing accuracy on memorized examples from 100% to 3% while maintaining 90.8% on clean examples.

## Executive Summary
This paper investigates whether memorization in neural networks can be localized to specific layers or neurons. Through three experimental approaches - gradient accounting, layer rewinding, and layer retraining - the authors demonstrate that memorization is not confined to final layers but distributed across multiple layers. A neuron-level analysis reveals that memorized examples require significantly fewer neurons (average 5.1) for correct prediction compared to clean examples (average 15.7). Based on these insights, the authors propose example-tied dropout, a mechanism that directs memorization to predefined neurons, achieving significant reduction in memorization accuracy while preserving generalization.

## Method Summary
The authors conduct experiments using ResNet-9, ResNet-50, and ViT models trained on CIFAR-10, MNIST, and SVHN datasets with varying degrees of random label noise (1%-99%). The training uses SGD with one-cycle learning rate scheduler for 50 epochs. Three main experimental approaches are employed: gradient accounting to measure contribution of clean vs mislabeled examples to gradient norms; layer rewinding to assess layer-wise redundancy; and layer retraining to evaluate the impact of training individual layers. Additionally, a neuron importance quantification method is implemented through iterative neuron removal based on gradient contribution, and example-tied dropout is proposed as a mechanism to localize memorization.

## Key Results
- Memorization is distributed across multiple layers rather than localized to final layers
- Memorized examples require significantly fewer neurons (average 5.1) compared to clean examples (average 15.7)
- Example-tied dropout reduces accuracy on mislabeled examples from 100% to 3% while maintaining 90.8% accuracy on clean examples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Memorization is distributed across layers but localized to specific neurons
- Mechanism: While gradient norms show equal contribution from clean and mislabeled examples at layer level, neuron-level analysis reveals memorized examples require significantly fewer neurons for correct prediction
- Core assumption: Neuron importance can be quantified through iterative removal based on gradient contribution
- Evidence anchors:
  - [abstract] "memorization is often confined to a small number of neurons or channels (around 5) of the model"
  - [section 6.1] "we iteratively find the most important neurons for a model's prediction on a given candidate example"
- Break condition: If neuron importance scores don't correlate with prediction stability across different random initializations

### Mechanism 2
- Claim: Mislabeled examples have disproportionate influence on gradient updates
- Mechanism: Mislabeled examples (10% of data) contribute gradient norms an order of magnitude larger than clean examples, with extremely negative cosine similarity between their gradients
- Core assumption: Gradient norms accurately reflect learning importance for each example type
- Evidence anchors:
  - [section 4.1] "the average contribution to gradient norms by memorized examples is an order of magnitude larger than that by typical (clean) examples"
  - [section 4.2] "the gradients of clean and mislabeled examples have an extremely negative cosine similarity"
- Break condition: If gradient contributions don't correlate with final memorization accuracy

### Mechanism 3
- Claim: Example-tied dropout can direct memorization to predefined neurons
- Mechanism: By keeping generalization neurons always active and activating example-specific memorization neurons only when that example is sampled, the model concentrates memorization in specific locations that can be dropped at test time
- Core assumption: The model can learn to route memorization through specific neurons without affecting generalization
- Evidence anchors:
  - [abstract] "By dropping out these neurons, we are able to reduce the accuracy on memorized examples from 100% to 3%"
  - [section 6.2] "We keep a fraction of (generalization) neurons always active during training"
- Break condition: If generalization neurons cannot maintain clean example accuracy after memorization neurons are dropped

## Foundational Learning

- Concept: Gradient-based neuron importance
  - Why needed here: Essential for identifying which neurons are responsible for memorization versus generalization
  - Quick check question: How would you modify the greedy search algorithm if instead of zeroing activations, you replaced them with learned parameters?

- Concept: Layer-wise gradient analysis
  - Why needed here: Critical for understanding how different layers contribute to memorization versus generalization
  - Quick check question: Why might the negative cosine similarity between clean and noisy example gradients indicate non-benign overfitting?

- Concept: Dropout mechanisms and their variants
  - Why needed here: Required to understand how example-tied dropout differs from standard dropout and why it enables controlled memorization
  - Quick check question: What would happen to clean example accuracy if all generalization neurons were dropped instead of memorization neurons?

## Architecture Onboarding

- Component map: ResNet-9 backbone → neuron importance analysis → example-tied dropout layer insertion → evaluation with neuron removal
- Critical path: Training with example-tied dropout → identifying memorization neurons → evaluating clean vs noisy accuracy after neuron removal
- Design tradeoffs: Higher pgen (generalization neurons) improves clean accuracy but reduces memorization localization; higher pmem increases memorization concentration but may affect generalization
- Failure signatures: Clean example accuracy drops significantly after memorization neuron removal; gradient norms don't show expected negative cosine similarity; layer rewinding doesn't show expected redundancy patterns
- First 3 experiments:
  1. Train standard ResNet-9 on CIFAR-10 with 10% noise, measure gradient norms and cosine similarity
  2. Implement neuron importance greedy search, verify memorized examples need fewer neurons to flip
  3. Add example-tied dropout layer, test memorization localization by dropping memorization neurons at evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of neurons required for memorization scale with network depth and width?
- Basis in paper: [inferred] The paper shows that memorization requires fewer neurons than generalization, but doesn't systematically vary network architecture.
- Why unresolved: The experiments only used specific ResNet and ViT architectures without exploring architectural variations.
- What evidence would resolve it: Experiments varying depth/width of networks while measuring neurons needed for memorization vs generalization.

### Open Question 2
- Question: Can example-tied dropout be adapted to work with other forms of regularization like weight decay or batch normalization?
- Basis in paper: [explicit] The paper only tests example-tied dropout with SGD optimizer and no other regularization modifications.
- Why unresolved: The experiments kept other hyperparameters constant and didn't explore interactions with other regularization methods.
- What evidence would resolve it: Experiments combining example-tied dropout with different regularization techniques and measuring memorization/generalization trade-offs.

### Open Question 3
- Question: What is the relationship between gradient norms from memorized examples and their embedding distance in feature space?
- Basis in paper: [inferred] The paper shows high gradient norms from memorized examples but doesn't analyze their feature space properties.
- Why unresolved: The analysis focused on gradient accounting and neuron importance but didn't examine feature representations of memorized examples.
- What evidence would resolve it: Analysis of feature space clustering/distance metrics for memorized vs clean examples and correlation with gradient norms.

## Limitations
- All experiments rely on synthetic mislabeled data rather than naturally occurring label errors
- Neuron importance quantification method assumes gradient-based importance scores are stable across training runs
- Example-tied dropout requires architectural modifications that may not transfer cleanly to larger-scale models

## Confidence
- Distributed nature of memorization across layers: High confidence
- Neuron-level localization with specific neuron counts: Medium confidence
- Example-tied dropout effectiveness: Medium confidence

## Next Checks
1. Replicate the neuron importance analysis across 5 different random seeds to verify stability of the 5.1 vs 15.7 neuron difference
2. Test the example-tied dropout mechanism on naturally occurring mislabeled examples from real datasets (e.g., WebVision, Clothing1M)
3. Extend the layer rewinding experiments to include intermediate checkpoint comparisons (not just random layers) to better understand the temporal evolution of memorization