---
ver: rpa2
title: Causal Discovery with Language Models as Imperfect Experts
arxiv_id: '2307.02390'
source_url: https://arxiv.org/abs/2307.02390
tags:
- causal
- expert
- experts
- discovery
- bayesian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores using expert knowledge to reduce uncertainty
  in causal discovery beyond Markov equivalence classes, while accounting for potential
  errors in expert judgments. The authors formalize this as an optimization problem
  that minimizes the equivalence class size while ensuring the true graph remains
  included with high probability.
---

# Causal Discovery with Language Models as Imperfect Experts

## Quick Facts
- **arXiv ID:** 2307.02390
- **Source URL:** https://arxiv.org/abs/2307.02390
- **Reference count:** 27
- **Key outcome:** Explores using expert knowledge to reduce uncertainty in causal discovery beyond Markov equivalence classes, while accounting for potential errors in expert judgments.

## Executive Summary
This work addresses the challenge of reducing uncertainty in causal discovery beyond Markov equivalence classes by incorporating imperfect expert knowledge. The authors formalize this as an optimization problem that minimizes equivalence class size while ensuring the true graph remains included with high probability. They propose a greedy strategy that incrementally incorporates expert knowledge using Bayesian inference, leveraging consistency properties like acyclicity and conditional independencies. Experiments demonstrate that the approach can successfully reduce uncertainty when using simulated experts with known error rates, though results with large language models as experts are mixed, highlighting both the potential and limitations of this approach.

## Method Summary
The approach uses Bayesian inference to estimate the probability that expert orientations are correct, enabling controlled uncertainty reduction in causal discovery. It operates on Markov equivalence classes (MECs) as the space of possible causal graphs, using expert knowledge to reduce this space. The method employs two greedy strategies - Ssize (minimize MEC size) and Srisk (minimize probability of excluding the true graph) - that iteratively orient edges based on current posterior probabilities. The approach validates expert orientations using consistency properties like acyclicity and conditional independencies to eliminate inconsistent graphs from the MEC.

## Key Results
- Successfully reduces MEC uncertainty when using simulated experts with known error rates
- Demonstrates some ability to reduce uncertainty and improve structural accuracy with LLM-based experts
- Highlights limitations in LLM uncertainty calibration that affect the approach's performance
- Shows mixed results in practical applications, with modest improvements in structural accuracy

## Why This Works (Mechanism)

### Mechanism 1
Expert knowledge can reduce uncertainty in causal graphs beyond Markov equivalence classes if consistency properties are leveraged. The approach uses consistency properties (acyclicity, conditional independencies) to validate expert orientations and eliminate inconsistent graphs from the MEC. Core assumption: The true causal graph G⋆ is Markovian with respect to the data distribution, and the MEC contains all DAGs with equivalent conditional independencies.

### Mechanism 2
Bayesian inference can estimate the probability that expert orientations are correct, enabling controlled uncertainty reduction. The approach uses a Bayesian framework with a prior uniform over MEC graphs and a likelihood model based on expert error rate ε to compute posterior probabilities for edge orientations. Core assumption: The expert error rate ε is constant across all edge orientations, and expert decisions are conditionally independent given true orientations.

### Mechanism 3
Greedy strategies can optimize the trade-off between MEC reduction and risk of excluding the true graph. The approach uses two greedy strategies - Ssize (minimize MEC size) and Srisk (minimize probability of excluding G⋆) - that iteratively orient edges based on current posterior probabilities. Core assumption: The greedy approach makes locally optimal decisions that lead to globally optimal MEC reduction within the risk tolerance η.

## Foundational Learning

- **Concept: Markov equivalence classes (MECs)**
  - Why needed here: The approach operates on MECs as the space of possible causal graphs, using expert knowledge to reduce this space
  - Quick check question: What property must two DAGs share to be in the same Markov equivalence class?

- **Concept: Conditional independence testing**
  - Why needed here: The MEC is defined by conditional independencies, which the approach uses to validate expert orientations
  - Quick check question: How do v-structures affect the conditional independencies in a DAG?

- **Concept: Bayesian inference and posterior computation**
  - Why needed here: The approach uses Bayesian inference to compute posterior probabilities for edge orientations based on expert knowledge
  - Quick check question: What are the three components needed to compute a posterior probability using Bayes' rule?

## Architecture Onboarding

- **Component map:** MEC extraction from data → Expert querying and response processing → Greedy MEC reduction using Bayesian inference → Evaluation
- **Critical path:** MEC extraction → Expert querying → Bayesian inference → MEC reduction → Evaluation
- **Design tradeoffs:** The approach trades off between MEC reduction (certainty) and risk of excluding the true graph (uncertainty), controlled by tolerance parameter η
- **Failure signatures:** Poor performance when expert error rates are high, when expert knowledge violates consistency properties, or when the greedy approach makes suboptimal decisions
- **First 3 experiments:**
  1. Run the approach on synthetic data with known ground truth and varying expert error rates to evaluate MEC reduction and accuracy
  2. Test the approach on real-world causal Bayesian networks with simulated experts to assess practical performance
  3. Evaluate the approach using LLM-based experts to investigate the potential and limitations of using language models as imperfect experts

## Open Questions the Paper Calls Out

### Open Question 1
How do LLM-based experts perform when the expert's uncertainty calibration is improved to match or exceed that of human experts? The paper notes that the LLM-based expert (text-davinci-003) exhibited poor uncertainty calibration compared to text-davinci-002, leading to underestimation of the probability of excluding the true graph from the equivalence class.

### Open Question 2
What is the impact of using different prompt styles and causation verbs on the performance of LLM-based experts in causal discovery? The paper mentions that different causation verbs were randomly selected to introduce stochasticity in the LLM outputs, but the impact of prompt style on performance was not systematically investigated.

### Open Question 3
How does the performance of the proposed approach vary when using different noise models that better characterize LLM errors? The paper assumes an "ε-expert" noise model for its Bayesian approach, but notes that this may not be well-suited for LLMs, as evidenced by the poor performance of text-davinci-003.

## Limitations
- Effectiveness critically depends on expert error rates remaining below 50% threshold for Bayesian inference to function properly
- Greedy strategy may make locally optimal but globally suboptimal decisions when edge orientations are interdependent
- Poor uncertainty calibration of LLM-based experts limits practical applicability and reliability of results

## Confidence
- **High Confidence:** The theoretical framework for incorporating expert knowledge via Bayesian inference and consistency properties is sound
- **Medium Confidence:** Empirical results with simulated experts are convincing, but LLM-based experiments show mixed results with only modest improvements
- **Low Confidence:** Generalizability to domains with systematically biased expert knowledge or strong edge dependencies

## Next Checks
1. Conduct ablation studies varying expert error rates systematically to identify the threshold at which Bayesian inference breaks down
2. Test the approach on datasets with known edge dependencies to evaluate how well the greedy strategy handles interdependent orientations
3. Implement cross-validation with multiple LLM instances to assess variability in expert responses and its impact on uncertainty calibration consistency