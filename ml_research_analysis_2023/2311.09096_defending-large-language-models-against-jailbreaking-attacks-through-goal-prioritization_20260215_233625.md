---
ver: rpa2
title: Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization
arxiv_id: '2311.09096'
source_url: https://arxiv.org/abs/2311.09096
tags:
- training
- llms
- goal
- response
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study identifies the core problem behind jailbreaking attacks
  as the conflict between a model's goals of being helpful and ensuring safety. To
  address this, the authors propose a goal prioritization approach that explicitly
  instructs models to prioritize safety over helpfulness during both inference and
  training.
---

# Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization

## Quick Facts
- arXiv ID: 2311.09096
- Source URL: https://arxiv.org/abs/2311.09096
- Reference count: 25
- Key outcome: Reduces attack success rates from 66.4% to 2.0% for ChatGPT and from 71.0% to 6.6% for Llama2-13B without compromising general performance

## Executive Summary
This paper addresses the fundamental challenge of defending Large Language Models against jailbreaking attacks by recognizing that the core problem stems from the conflict between a model's goals of being helpful and ensuring safety. The authors propose a goal prioritization approach that explicitly instructs models to prioritize safety over helpfulness during both inference and training. By integrating this concept into both phases, they demonstrate substantial reductions in attack success rates while maintaining general model performance. The method shows strong generalization to out-of-distribution attacks, with over 30% improvement in defense success rate compared to baselines when no jailbreak prompts are included in training.

## Method Summary
The approach involves two complementary components: a plug-and-play prompt-based defense for inference-time protection and a training-based fine-tuning method for long-term robustness. During inference, the model is provided with goal prioritization instructions along with in-context examples and internal thought analysis to guide responses. For training, the model is fine-tuned on paired benign and harmful queries with explicit goal prioritization requirements, including internal thoughts analysis. The method leverages the observation that stronger LLMs, while facing greater safety risks, also possess greater capacity to follow explicit goal prioritization instructions when properly guided.

## Key Results
- Attack Success Rate reduced from 66.4% to 2.0% for ChatGPT
- Attack Success Rate reduced from 71.0% to 6.6% for Llama2-13B
- Strong generalization to out-of-distribution attacks with >30% improvement in defense success rate
- No significant degradation in general performance on standard benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Jailbreaking succeeds because LLMs face a conflict between helpfulness and safety goals, and without explicit prioritization, they default to helpfulness.
- Mechanism: The method introduces explicit goal prioritization through prompts that instruct the model to always prioritize safety over helpfulness. This creates a clear hierarchy that resolves the internal conflict.
- Core assumption: The model can effectively follow explicit goal prioritization instructions when properly prompted.
- Evidence anchors:
  - [abstract] "the intrinsic conflict between the goals of being helpful and ensuring safety" and "integrating goal prioritization at both training and inference stages"
  - [section] "the key issue is the conflict between two goals: helpfulness (providing helpful responses to user queries) and safety (providing harmless and safe responses to user queries)"

### Mechanism 2
- Claim: Training with explicit goal prioritization requirements helps models learn to properly balance conflicting goals.
- Mechanism: During training, the model is exposed to both benign and harmful queries paired with explicit goal prioritization requirements. The training includes internal thoughts analysis to help the model understand when prioritizing safety is necessary.
- Core assumption: The model can learn from paired examples with explicit goal prioritization and generalize this learning to new situations.
- Evidence anchors:
  - [abstract] "integrating the concept of goal prioritization into the training phase reduces the ASR from 71.0% to 6.6% for LLama2-13B"
  - [section] "we expose both types of goal priority to the model (i.e., prioritizing safety over helpfulness and prioritizing helpfulness over safety) during fine-tuning"

### Mechanism 3
- Claim: Stronger LLMs have greater capacity to defend against jailbreaking when properly guided due to their superior instruction-following capabilities.
- Mechanism: More capable models can better understand and implement complex goal prioritization requirements, making them more effective at resisting jailbreaking attempts when the defense mechanism is applied.
- Core assumption: Model capability correlates with instruction-following ability, which can be leveraged for defense.
- Evidence anchors:
  - [abstract] "while stronger LLMs face greater safety risks, they also possess a greater capacity to be steered towards defending against such attacks"
  - [section] "Although more potent LLMs such as GPT-4 could be susceptible to a wider range of jailbreaking attacks... they are also more readily directed to defend against these attacks"

## Foundational Learning

- Concept: Goal conflict resolution
  - Why needed here: Understanding how to resolve conflicts between competing objectives (helpfulness vs safety) is fundamental to the defense mechanism
  - Quick check question: What happens when an LLM receives conflicting instructions about whether to be helpful or safe?

- Concept: In-context learning and few-shot prompting
  - Why needed here: The defense relies on providing examples and instructions within prompts to guide model behavior
  - Quick check question: How does providing examples within a prompt help an LLM understand what behavior is expected?

- Concept: Supervised fine-tuning (SFT) and training data construction
  - Why needed here: The training-based defense requires constructing appropriate training data with goal prioritization requirements
  - Quick check question: What considerations are important when constructing training data to teach an LLM about goal prioritization?

## Architecture Onboarding

- Component map: User query + goal prioritization instruction → Internal thoughts evaluation of query safety → Safety vs helpfulness determination → Final response generation

- Critical path: User query → Goal prioritization instruction → Internal thoughts analysis → Safety determination → Response generation

- Design tradeoffs:
  - Prompt length vs effectiveness: Longer prompts with more examples are more effective but increase computational cost
  - Training data diversity vs quantity: Including diverse jailbreaking patterns improves generalization but requires more data
  - Safety vs helpfulness balance: Overly conservative safety prioritization may reduce helpfulness

- Failure signatures:
  - High attack success rate indicates the goal prioritization instructions are not being followed
  - Reduced general performance indicates the safety prioritization is too restrictive
  - Poor generalization indicates the training data lacks diversity

- First 3 experiments:
  1. Test the basic prompt-based defense on a small set of known jailbreak attacks to verify the core mechanism works
  2. Compare the effectiveness of different prompt formats (with vs without examples, with vs without internal thoughts)
  3. Evaluate the impact of training data composition (ratio of benign to harmful queries) on defense effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of goal prioritization vary across different model architectures and sizes beyond those tested?
- Basis in paper: [explicit] The paper tests goal prioritization on ChatGPT, GPT-4, Vicuna-7B, Vicuna-13B, and Vicuna-33B, showing improved performance with larger models, but does not explore other architectures or sizes
- Why unresolved: The paper only tests a limited set of models, leaving open questions about whether these results generalize to other architectures (e.g., Claude, Gemini) or model sizes not tested
- What evidence would resolve it: Systematic testing of goal prioritization across a wider range of model architectures, sizes, and training methodologies would clarify the generalizability of these findings

### Open Question 2
- Question: What is the optimal ratio of harmful to benign queries during training for maximizing both safety and general performance?
- Basis in paper: [explicit] The paper tests 1%, 3%, and 5% harmful query ratios and finds 3% performs similarly to 5%, but doesn't explore other ratios or provide a theoretical justification for the optimal balance
- Why unresolved: The paper only explores a narrow range of ratios without establishing why these specific percentages were chosen or what happens at extremes (e.g., 0.1%, 10%, 50%)
- What evidence would resolve it: A comprehensive study varying the ratio across orders of magnitude, measuring both ASR reduction and performance degradation, would identify the optimal trade-off point

### Open Question 3
- Question: How does goal prioritization affect model performance on complex ethical dilemmas that require balancing safety and helpfulness?
- Basis in paper: [inferred] The paper focuses on simple binary cases (safe vs. harmful queries) but doesn't address scenarios where safety and helpfulness genuinely conflict in nuanced ways
- Why unresolved: The paper's evaluation framework assumes clear distinctions between safe and harmful queries, but real-world scenarios often involve gray areas where strict prioritization might lead to suboptimal outcomes
- What evidence would resolve it: Testing goal prioritization on benchmarks specifically designed to evaluate ethical reasoning and nuanced safety-helpfulness trade-offs would reveal potential limitations of the approach

## Limitations
- The defense relies heavily on the model's ability to follow explicit instructions, which may break down under sophisticated attacks targeting the prioritization mechanism
- Training data construction is critical but challenging, with only 500 harmful instructions used which may not capture full diversity of potential jailbreak attempts
- The method assumes internal thought analysis can accurately assess query safety, but this assessment may not be reliable for all types of harmful content

## Confidence
**High Confidence** - The core claim that jailbreaking succeeds due to goal conflict between helpfulness and safety is well-supported by empirical results showing substantial reduction in attack success rates.

**Medium Confidence** - The claim that stronger LLMs have greater capacity to defend against jailbreaking when properly guided is supported by results, but the relationship between model capability and defense effectiveness could be more thoroughly explored.

**Medium Confidence** - The generalization to out-of-distribution attacks shows promising results with >30% improvement, but the evaluation is limited to attacks not included in training.

## Next Checks
1. **Attack Pattern Evolution Analysis**: Systematically test the defense against attacks specifically designed to subvert the goal prioritization mechanism itself to validate whether the defense can withstand attacks that target its core mechanism.

2. **Safety-Helpfulness Tradeoff Characterization**: Conduct a systematic ablation study varying the strength of safety prioritization across multiple levels, measuring both attack success rates and general performance metrics to precisely characterize the tradeoff frontier.

3. **Long-Tail Attack Pattern Coverage**: Evaluate the defense against a much larger and more diverse set of harmful queries (e.g., 10,000+ examples covering edge cases and rare but dangerous scenarios) to better understand the method's coverage limitations and identify failure modes.