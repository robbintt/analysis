---
ver: rpa2
title: Improving Korean NLP Tasks with Linguistically Informed Subword Tokenization
  and Sub-character Decomposition
arxiv_id: '2311.03928'
source_url: https://arxiv.org/abs/2311.03928
tags:
- tokenization
- korean
- subword
- sub-character
- decomposition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a morpheme-aware subword tokenization method
  that leverages sub-character decomposition to address the challenges of applying
  Byte Pair Encoding (BPE) to Korean, a language with rich morphology and a unique
  writing system. The proposed approach integrates morpheme type information into
  the tokenization process, balancing linguistic accuracy with computational efficiency
  in Pre-trained Language Models (PLMs).
---

# Improving Korean NLP Tasks with Linguistically Informed Subword Tokenization and Sub-character Decomposition

## Quick Facts
- arXiv ID: 2311.03928
- Source URL: https://arxiv.org/abs/2311.03928
- Reference count: 10
- Primary result: Morpheme-aware subword tokenization with sub-character decomposition improves Korean NLP task performance, particularly in syntactic tasks

## Executive Summary
This paper presents a morpheme-aware subword tokenization method that addresses the challenges of applying Byte Pair Encoding (BPE) to Korean, a language with rich morphology and a unique writing system. The approach integrates morpheme type information into the tokenization process, selectively applying sub-character decomposition only to lexical morphemes while preserving grammatical morphemes intact. Evaluations on various Korean NLP tasks demonstrate that this technique achieves overall good performance, notably improving results in the syntactic task of NIKL-CoLA. The findings suggest that incorporating morpheme type information can enhance language models' syntactic and semantic capabilities.

## Method Summary
The method combines morphological analysis with sub-character decomposition to create a morpheme-aware subword tokenizer. It uses MeCab-ko for morphological analysis, applies sub-character decomposition only to lexical morphemes based on their POS tags, and then trains WordPiece models on the resulting tokens. The approach leverages Hangul Jamo decomposition to break down syllable characters into their constituent components while preserving grammatical morphemes as complete units. Pre-training is conducted on Korean Wiki and Namuwiki datasets, followed by fine-tuning on six Korean NLP tasks including syntax, semantics, and complex tasks.

## Key Results
- The MorWP-MD models achieved the highest performance on NIKL-CoLA, a syntactic task
- WordPiece Subtoken Rates (WSR) for MorWP, MorWP-SD, and MorWP-MD models were less than 13%, compared to higher than 46% for pure WordPiece models
- Overall good performance across all six evaluated Korean NLP tasks, demonstrating the effectiveness of incorporating morpheme type information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Morphological sub-character decomposition improves contextualization of homographic tokens
- Mechanism: By decomposing only lexical morphemes into sub-characters while leaving grammatical morphemes intact, the model can better distinguish between different uses of ambiguous tokens through their sub-character composition
- Core assumption: The model can learn meaningful patterns from sub-character representations of lexical morphemes that help disambiguate homographic tokens
- Evidence anchors: [abstract] "integrating morpheme type information can enhance language models' syntactic and semantic capabilities"; [section 6.2] "the MorWP-MD models have the advantage of contextualizing those tokens"

### Mechanism 2
- Claim: Morpheme-aware tokenization reduces WordPiece Subtoken Rates (WSR) compared to pure subword tokenization
- Mechanism: By using morphological analysis first, full morphemes are preserved as vocabulary entries rather than being broken into WordPiece subtokens, reducing the need for decomposition
- Core assumption: Preserving full morphemes as vocabulary entries is more efficient than decomposing them into smaller subword units
- Evidence anchors: [section 6.1] "the MorWP, the MorWP-SD, and the MorWP-MD are less than 13%" vs "The WSRs of the WP models and the WP-SD models are higher than 46%"

### Mechanism 3
- Claim: Sub-character decomposition after morpheme tokenization alleviates morphological analyzer errors
- Mechanism: When morphological analyzers incorrectly segment morphemes, sub-character decomposition can recover some of the lost information by breaking down the incorrectly segmented units
- Core assumption: Even with morphological analysis errors, sub-character decomposition can provide useful linguistic information that wouldn't be available otherwise
- Evidence anchors: [section 3.2] "sub-character decomposition after morpheme tokenization is expected to alleviate such problems" referring to morphological analyzer errors

## Foundational Learning

- Concept: Korean morphological structure (agglutinative language with lexical vs grammatical morphemes)
  - Why needed here: The entire method depends on correctly identifying and treating lexical and grammatical morphemes differently
  - Quick check question: What distinguishes lexical morphemes from grammatical morphemes in Korean, and why does this matter for tokenization?

- Concept: Byte Pair Encoding (BPE) and WordPiece tokenization algorithms
  - Why needed here: The proposed method builds on BPE-style tokenization, so understanding its greedy merging process is crucial
  - Quick check question: How does BPE decide which pairs to merge, and what linguistic information might it miss?

- Concept: Sub-character decomposition in Korean (Hangul Jamo decomposition)
  - Why needed here: The method relies on decomposing syllable characters into their constituent Jamo, so understanding Unicode normalization is essential
  - Quick check question: What are the three components of a Hangul syllable, and how many possible combinations exist?

## Architecture Onboarding

- Component map: Morphological analyzer (MeCab-ko) → morpheme tokenizer → Sub-character decomposition filter (lexical morpheme POS tags) → WordPiece/BPE model → BERT pre-training pipeline → fine-tuning framework

- Critical path: Morphological analysis → POS tagging → selective sub-character decomposition → WordPiece vocabulary training → BERT pre-training → fine-tuning on downstream tasks

- Design tradeoffs:
  - Accuracy vs efficiency: More decomposition improves linguistic coverage but increases sequence length
  - Vocabulary size vs performance: Larger vocabularies reduce WordPiece subtoken rates but increase memory requirements
  - Dependency on morphological analyzer: Method performance is bounded by analyzer quality

- Failure signatures:
  - High WordPiece Subtoken Rate despite morphological analysis suggests POS tagging errors
  - Poor performance on syntactic tasks with morphological decomposition suggests homograph disambiguation isn't working
  - No improvement over baseline suggests decomposition is either too aggressive or too conservative

- First 3 experiments:
  1. Run morphological analysis on sample Korean text and verify POS tagging accuracy for lexical vs grammatical morphemes
  2. Apply selective sub-character decomposition to the same sample and measure sequence length increase
  3. Train a small WordPiece model with and without morphological decomposition on a subset of data and compare vocabulary composition

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed morphological sub-character decomposition method be adapted or extended to handle errors from morphological analyzers more effectively?
- Basis in paper: [explicit] The paper identifies that the proposed method's effectiveness is limited by errors in morphological analyzers, which can lead to failures in morpheme tokenization and morphological sub-character decomposition
- Why unresolved: The paper does not provide a solution for mitigating the impact of morphological analyzer errors on the proposed method

### Open Question 2
- Question: What is the impact of the proposed morphological sub-character decomposition method on other Korean NLP tasks not evaluated in the paper, such as named entity recognition or machine translation?
- Basis in paper: [inferred] The paper evaluates the proposed method on a limited set of Korean NLP tasks (syntax, semantics, and complex tasks), but does not explore its potential benefits or limitations in other tasks
- Why unresolved: The paper does not provide evidence of the method's effectiveness or limitations in other Korean NLP tasks

### Open Question 3
- Question: How does the proposed morphological sub-character decomposition method compare to other subword tokenization methods, such as WordPiece or SentencePiece, in terms of computational efficiency and memory usage?
- Basis in paper: [inferred] The paper does not provide a direct comparison of the proposed method's computational efficiency and memory usage with other subword tokenization methods
- Why unresolved: The paper does not provide evidence of the proposed method's efficiency compared to other methods

## Limitations
- The method's effectiveness is fundamentally limited by the quality of morphological analysis, which is acknowledged to be imperfect
- No ablation studies are provided to determine optimal levels of decomposition aggressiveness or error thresholds
- Claims about generalizability to other morphologically rich languages are speculative without supporting evidence

## Confidence

**High Confidence:** The empirical results showing improved performance on NIKL-CoLA (syntactic task) and KLUE-NLI are well-supported by the experimental data. The WordPiece Subtoken Rate (WSR) comparisons across different tokenization methods are clearly presented and demonstrate the efficiency gains of the morpheme-aware approach.

**Medium Confidence:** The mechanism explanations for why morphological decomposition improves homograph disambiguation and error recovery are plausible but lack direct empirical validation. The claims about enhancing "syntactic and semantic capabilities" through morpheme type information are supported by task performance but could benefit from more targeted linguistic analysis.

**Low Confidence:** The assertion that this approach can be generalized to other morphologically rich languages is largely speculative, given that the experiments are exclusively conducted on Korean. The paper mentions potential applications to other languages but provides no evidence or preliminary results to support this claim.

## Next Checks

1. **Error Analysis of Morphological Decomposition**: Select a sample of 100 Korean sentences and manually annotate morpheme boundaries and POS tags, then compare against the morphological analyzer's output to quantify error types and their impact on downstream tokenization quality.

2. **WSR Sensitivity Analysis**: Systematically vary the vocabulary size (e.g., 16K, 32K, 64K, 128K) and measure WordPiece Subtoken Rates across all tokenization methods to determine whether the reported efficiency gains hold across different vocabulary configurations.

3. **Cross-linguistic Transfer Experiment**: Apply the morpheme-aware tokenization pipeline to another agglutinative language (such as Turkish or Finnish) with a different writing system to test the generalizability of the approach beyond Korean's unique morphological and orthographic properties.