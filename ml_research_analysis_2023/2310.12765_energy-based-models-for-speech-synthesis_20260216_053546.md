---
ver: rpa2
title: Energy-Based Models For Speech Synthesis
arxiv_id: '2310.12765'
source_url: https://arxiv.org/abs/2310.12765
tags:
- speech
- ebms
- negative
- energy
- synthesis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using energy-based models (EBMs) for speech
  synthesis. The EBM defines an energy function that measures the quality of speech
  given text.
---

# Energy-Based Models For Speech Synthesis

## Quick Facts
- arXiv ID: 2310.12765
- Source URL: https://arxiv.org/abs/2310.12765
- Reference count: 0
- Key outcome: EBM improves over Tacotron 2 with MCD 3.882 vs 4.218 and MOS 3.84 vs 3.77

## Executive Summary
This paper proposes using energy-based models (EBMs) for speech synthesis, defining an energy function that measures speech quality given text. The approach uses noise contrastive estimation (NCE) to train the energy function by comparing reference speech (positives) with distorted versions (negatives). Inference employs Langevin Markov Chain Monte Carlo sampling, connecting EBMs to diffusion models. Experiments on LJSpeech show the proposed EBM achieves better performance than Tacotron 2 across multiple metrics including MCD, FFE, log F0 RMSE, and MOS.

## Method Summary
The method trains a transformer-based EBM using noise contrastive estimation to avoid computing the intractable normalization term. Positive samples consist of reference speech, while negative samples are generated through various strategies including random masking and predictions from pre-trained TTS models like Tacotron 2. The energy function is implemented as a transformer encoder-decoder with frame-level energy prediction. During inference, Langevin MCMC sampling iteratively refines initial hypotheses by following the energy gradient to generate high-quality speech samples.

## Key Results
- EBM achieves MCD of 3.882 versus Tacotron 2's 4.218
- MOS improves from 3.77 (Tacotron 2) to 3.84 (EBM)
- Best performance achieved using random masking for negative samples with 25-30% distortion
- MCMC sampling with 300 steps provides optimal quality, though fewer steps yield acceptable results

## Why This Works (Mechanism)

### Mechanism 1
Noise contrastive estimation (NCE) enables training EBMs without computing the intractable normalization term by comparing positive samples (reference speech) with negative samples (distorted speech) to train the energy function without requiring the partition function Zθ(x).

### Mechanism 2
Langevin MCMC enables sampling from EBMs by iteratively refining hypotheses through the update rule Y^(N+1) ← Y^(N) - λ∇Y Eθ(x, Y)|Y=Y^(N) + √2λZ^(N), which refines initial hypotheses by following the gradient of the energy function.

### Mechanism 3
High-performing TTS models provide effective negative samples for training by creating challenging contrasts that improve the energy function's ability to distinguish quality speech, as using predictions from Tacotron 2 as negative samples creates meaningful learning signals.

## Foundational Learning

- Concept: Energy-based models
  - Why needed here: EBMs provide a framework for modeling speech quality without requiring autoregressive dependencies
  - Quick check question: What makes EBMs different from traditional autoregressive models in terms of training approach?

- Concept: Noise contrastive estimation
  - Why needed here: NCE enables training EBMs without computing the intractable normalization term
  - Quick check question: How does NCE avoid the need to compute Zθ(x) in the EBM formulation?

- Concept: Markov Chain Monte Carlo sampling
  - Why needed here: Langevin MCMC provides a method for sampling from EBMs during inference
  - Quick check question: What role does the noise term √2λZ^(N) play in the Langevin MCMC update rule?

## Architecture Onboarding

- Component map: Text → Transformer encoder → Feature enhancement → Frame-level energy estimation → Attention-weighted utterance energy → Energy function
- Critical path: Text → Transformer encoder → Feature enhancement → Frame-level energy estimation → Attention-weighted utterance energy → Energy function
- Design tradeoffs: Simpler architecture vs performance (small EBM with 2.3M params vs Tacotron 2 with 28.19M params); number of MCMC steps vs inference speed
- Failure signatures: High MCD scores indicating poor speech quality; failure to improve over baseline; MCMC not converging to better samples
- First 3 experiments:
  1. Implement basic EBM architecture with Tacotron 2 as negative sample generator and single-step MCMC
  2. Test different negative sampling methods (random masking, SpecAugment) with the same architecture
  3. Evaluate impact of MCMC steps on final speech quality (1 step vs 100 steps vs 300 steps)

## Open Questions the Paper Calls Out

### Open Question 1
How do different negative sampling methods affect the quality of the energy function learned by EBMs? The paper evaluates various negative sampling methods (random masking, SpecAugment variants, using pre-trained TTS models) and finds that random masking performs best, but combinations of methods yield the best results.

### Open Question 2
What is the optimal number of Langevin MCMC steps for inference in EBMs? The paper shows that increasing the number of Langevin MCMC steps improves performance, but does not explore the trade-off between inference quality and computational cost.

### Open Question 3
How do EBMs compare to other non-autoregressive TTS models like FastSpeech 2 and diffusion models? The paper positions EBMs as a new member of the non-autoregressive TTS family but does not directly compare their performance to other established methods.

## Limitations

- Limited ablation studies on negative sampling strategies make it difficult to assess individual contributions to performance gains
- No comparison against modern non-autoregressive models like FastSpeech 2 or diffusion models
- High computational cost of MCMC sampling (300 steps) raises practical deployment concerns

## Confidence

- High confidence: EBM architecture and NCE training mechanism - well-established frameworks with clear implementation details
- Medium confidence: Performance improvements over Tacotron 2 - demonstrated but limited by narrow comparison scope
- Low confidence: Practical applicability - computational cost of MCMC sampling and lack of comparison to modern non-autoregressive models

## Next Checks

1. Conduct ablation study on negative sampling methods to systematically compare performance when using different strategies (random masking, SpecAugment, Tacotron 2 predictions)

2. Measure inference time and quality trade-offs for different MCMC step counts (1, 50, 100, 300 steps) to determine practical minimum requirements for acceptable speech quality

3. Evaluate EBM performance against modern non-autoregressive models (FastSpeech 2, Glow-TTS) on the same LJSpeech benchmark to contextualize improvements over Tacotron 2