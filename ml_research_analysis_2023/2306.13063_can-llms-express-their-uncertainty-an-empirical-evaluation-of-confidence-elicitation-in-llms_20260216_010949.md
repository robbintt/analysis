---
ver: rpa2
title: Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation
  in LLMs
arxiv_id: '2306.13063'
source_url: https://arxiv.org/abs/2306.13063
tags:
- confidence
- answer
- verbalized
- your
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study addresses the challenge of enabling large language\
  \ models (LLMs) to accurately express confidence in their responses\u2014a critical\
  \ capability for trustworthy decision-making. Traditional confidence estimation\
  \ methods rely on internal model information (logits), which are inaccessible in\
  \ closed-source LLMs like GPT-4."
---

# Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs

## Quick Facts
- arXiv ID: 2306.13063
- Source URL: https://arxiv.org/abs/2306.13063
- Reference count: 35
- Primary result: Hybrid confidence elicitation methods combining verbalized and consistency-based approaches achieve state-of-the-art performance across multiple LLMs and task types

## Executive Summary
This paper addresses the critical challenge of enabling large language models to accurately express confidence in their responses, which is essential for trustworthy decision-making. The authors develop a systematic framework for black-box confidence elicitation since closed-source LLMs like GPT-4 don't provide access to internal model information. They propose three categories of confidence elicitation methods - verbalized-based, consistency-based, and hybrid approaches - and evaluate them across five dataset types and five widely-used LLMs.

The study reveals that LLMs exhibit significant overconfidence when verbalizing confidence, often clustering responses between 80-100%, potentially imitating human patterns of expressing confidence. Consistency-based methods outperform verbalized confidence particularly on arithmetic tasks, while hybrid methods achieve the best overall performance, excelling in 13 out of 20 cases. Despite these improvements, all methods struggle with complex professional knowledge tasks, highlighting the need for further research in this area.

## Method Summary
The paper proposes a systematic framework for black-box confidence elicitation in LLMs, comprising three components: prompting strategies for verbalized confidence, sampling methods for generating multiple responses, and aggregation techniques for computing consistency. The framework includes three categories of confidence elicitation methods: verbalized-based (eliciting confidence scores directly from the model), consistency-based (measuring agreement across multiple responses), and hybrid methods (combining both approaches). The methods are evaluated across five dataset types (commonsense, arithmetic, symbolic, ethics, and professional knowledge) using five LLMs (GPT-3, GPT-3.5, GPT-4, and Vicuna) with metrics including Expected Calibration Error, AUROC, and AUPRC.

## Key Results
- LLMs exhibit significant overconfidence when verbalizing confidence, clustering responses between 80-100%
- Consistency-based methods outperform verbalized confidence, with particularly notable improvements on arithmetic tasks
- Hybrid methods combining verbalized and consistency confidence achieve state-of-the-art performance, excelling in 13 out of 20 cases
- All methods struggle with complex professional knowledge tasks requiring domain-specific expertise

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs tend to imitate human overconfidence patterns when verbalizing confidence
- Mechanism: LLMs generate confidence scores that cluster in high ranges (80-100%) because they have been trained on human-written text where confidence expressions typically fall within these ranges
- Core assumption: Training data contains human confidence expressions that follow predictable patterns (multiples of 5, peak at 95%)
- Evidence anchors:
  - [abstract]: "LLMs often exhibit a high degree of overconfidence when verbalizing their confidence; potentially imitating human patterns of expressing confidence"
  - [section 5.1]: "Most samples fall into the 80% to 100% range... This can be seen as an imitation of human behavior in verbalizing confidence, where in corpus, most confidence expressions are typically multiples of 5, with peak frequency on 95%"
  - [corpus]: Weak evidence - no direct corpus analysis of human confidence expression patterns found

### Mechanism 2
- Claim: Consistency-based methods outperform verbalized confidence because they measure agreement across multiple responses
- Mechanism: By generating multiple responses and measuring consistency, the method captures uncertainty that verbalized confidence misses - when an LLM is uncertain, its responses will vary across generations
- Core assumption: LLMs exhibit variability in their responses when temperature is increased, and this variability correlates with uncertainty
- Evidence anchors:
  - [abstract]: "consistency-based methods outperform the verbalized confidences in most cases, with particularly notable improvements on the arithmetic reasoning task"
  - [section 3.2]: "The agreement between these candidate responses and the original answer then serves as a measure of confidence"
  - [corpus]: No direct evidence of temperature-induced variability correlation with uncertainty

### Mechanism 3
- Claim: Hybrid methods combining verbalized confidence and consistency achieve state-of-the-art performance
- Mechanism: Verbalized confidence provides fine-grained confidence information while consistency provides robustness; combining them compensates for each method's weaknesses
- Core assumption: Verbalized confidence and consistency-based confidence capture different aspects of uncertainty that are complementary
- Evidence anchors:
  - [abstract]: "Hybrid methods consistently deliver the best performance over their baselines, thereby emerging as a promising state-of-the-art approach"
  - [section 3.3]: "By integrating both uncertainty indicators, we propose a new approach... that build on consistency-based methods and utilize verbalized confidence to extract finer-grained confidence information"
  - [corpus]: No direct evidence of complementarity between the two uncertainty indicators

## Foundational Learning

- Concept: Confidence calibration
  - Why needed here: Understanding the difference between expressed confidence and actual accuracy is fundamental to evaluating confidence elicitation methods
  - Quick check question: What metric measures the discrepancy between predicted probabilities and observed accuracy within each confidence level?
  - Answer: Expected Calibration Error (ECE)

- Concept: Ensemble-based uncertainty estimation
  - Why needed here: Consistency-based methods are extensions of ensemble methods to the LLM setting
  - Quick check question: Which classical uncertainty estimation methods do consistency-based approaches extend?
  - Answer: MCDropout and Deep Ensemble

- Concept: Chain-of-Thought prompting
  - Why needed here: CoT is one of the prompting strategies used to improve verbalized confidence
  - Quick check question: What simple prompt addition was shown to induce reasoning processes in LLMs?
  - Answer: "Let's think step by step"

## Architecture Onboarding

- Component map: Prompt generator -> LLM interface -> Consistency calculator -> Confidence aggregator -> Evaluator
- Critical path:
  1. Generate prompt variants
  2. Submit to LLM(s)
  3. Collect responses
  4. Calculate consistency scores
  5. Extract verbalized confidence
  6. Aggregate confidence estimates
  7. Evaluate against ground truth
- Design tradeoffs:
  - Computation vs. accuracy: K-fold sampling increases accuracy but costs K times more
  - Prompt complexity vs. effectiveness: More complex prompts may improve calibration but reduce consistency
  - Granularity vs. stability: Multi-step confidence provides finer granularity but may be less stable
- Failure signatures:
  - High ECE with low AUROC: Model is overconfident and cannot distinguish correct/incorrect answers
  - Low variance in confidence scores: Model isn't expressing uncertainty appropriately
  - Consistency scores don't correlate with accuracy: Method isn't capturing true uncertainty
- First 3 experiments:
  1. Compare vanilla vs. CoT verbalized confidence on GSM8K dataset with GPT-3.5
  2. Evaluate self-consistency vs. induced-consistency on arithmetic tasks
  3. Test hybrid methods combining verbalized and consistency confidence on professional knowledge tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can hybrid methods be further improved by incorporating Top-K prompting strategy for verbalized confidence?
- Basis in paper: [inferred] ... The paper notes that "a fairer comparison might involve the use of the Top-K prompting strategy within hybrid methods to leverage the improved verbalized confidence derived from Top-K" as a future research direction.
- Why unresolved: The current hybrid methods use CoT verbalized confidence as a baseline, but do not explore the potential benefits of combining Top-K verbalized confidence with consistency-based methods.
- What evidence would resolve it: Experimental results comparing the performance of hybrid methods using Top-K verbalized confidence against the current baseline of CoT verbalized confidence on the same datasets.

### Open Question 2
- Question: What is the optimal number of candidate answers (K) for induced-consistency methods across different types of tasks?
- Basis in paper: [explicit] ... The paper mentions that "the optimal candidate size K for ECE varies across different datasets" and that "to strike a balance between computational efficiency and performance, we set the candidate set to be 4 in our study."
- Why unresolved: The paper does not provide a systematic investigation of how the number of candidate answers affects the performance of induced-consistency methods on different types of tasks, leaving open the question of what constitutes an optimal K for each task type.
- What evidence would resolve it: A comprehensive study varying K across a wide range of task types and analyzing the trade-off between computational cost and performance to determine the optimal K for each task category.

### Open Question 3
- Question: How do different types of misleading prompts (Weak Claim, Strong Claim, External Source) affect the performance of induced-consistency methods?
- Basis in paper: [explicit] ... The paper categorizes misleading prompts into three types and conducts experiments to study their effects, noting that "Weak Claim category performs better."
- Why unresolved: While the paper provides initial findings on the impact of misleading prompt types, it does not explore the underlying reasons for these differences or how they might be optimized for specific task types or model architectures.
- What evidence would resolve it: A deeper analysis of the mechanisms by which different types of misleading prompts influence model behavior, potentially including controlled experiments varying prompt strength and source, and correlating these with model confidence distributions and task performance.

## Limitations
- Limited task coverage: The evaluation focuses on five dataset types, but real-world applications may involve more complex, multi-modal tasks that weren't tested
- Black-box constraints: The study's focus on black-box methods means it doesn't leverage internal model information (logits) that white-box methods could access
- Temperature sensitivity: Consistency-based methods rely on temperature variation to generate diverse responses, but optimal temperature settings may vary across different LLMs and tasks

## Confidence
- High: The observation that verbalized confidence tends to cluster at high values (80-100%) is well-supported by the data
- Medium: The superiority of hybrid methods is demonstrated empirically but theoretical justification could be stronger
- Low: The claim about human overconfidence patterns being imitated by LLMs lacks direct corpus analysis

## Next Checks
1. **Cross-Domain Validation**: Test the confidence elicitation methods on domain-specific professional tasks (medical diagnosis, legal reasoning) to verify if poor performance on professional knowledge tasks is due to task complexity or fundamental limitations
2. **Temperature Sensitivity Analysis**: Systematically vary temperature settings across multiple orders of magnitude to quantify the relationship between temperature, response diversity, and confidence accuracy
3. **Human Comparison Study**: Conduct a controlled study comparing LLM confidence expression patterns with human experts on the same tasks to verify if observed clustering at high confidence values is an imitation of human behavior