---
ver: rpa2
title: 'FilterFL: Knowledge Filtering-based Data-Free Backdoor Defense for Federated
  Learning'
arxiv_id: '2308.11333'
source_url: https://arxiv.org/abs/2308.11333
tags:
- clients
- images
- attack
- backdoor
- defense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents FilterFL, a data-free backdoor defense approach
  for federated learning. It exploits two characteristics of backdoor attacks: triggers
  are learned faster than normal knowledge, and trigger patterns have a greater effect
  on image classification than normal class patterns.'
---

# FilterFL: Knowledge Filtering-based Data-Free Backdoor Defense for Federated Learning

## Quick Facts
- arXiv ID: 2308.11333
- Source URL: https://arxiv.org/abs/2308.11333
- Reference count: 40
- Key outcome: Achieves attack success rates as low as 0.14% while maintaining main task accuracy up to 98.55% against backdoor attacks in federated learning

## Executive Summary
FilterFL presents a novel data-free backdoor defense approach for federated learning that exploits two key characteristics of backdoor attacks: triggers are learned faster than normal knowledge, and trigger patterns have a greater effect on image classification than normal class patterns. The method generates images with newly learned knowledge by comparing old and new global models, then filters trigger images by evaluating their effect on classification. These trigger images are used to eliminate poisoned models and ensure the updated global model is benign. Experiments demonstrate effectiveness against existing backdoor attacks while outperforming seven state-of-the-art defense methods in both IID and non-IID scenarios, even when 80% of clients are malicious.

## Method Summary
FilterFL employs a three-stage defense approach using Conditional Generative Adversarial Networks (CGAN). First, it generates images containing newly learned knowledge by comparing old and new global models. Second, it filters trigger images by evaluating their classification effects, exploiting the fact that trigger patterns cause higher confidence in target category classification. Third, it uses these trigger images to identify and eliminate poisoned models from the aggregation process. The method operates without requiring any real training data, making it particularly suitable for privacy-preserving federated learning scenarios.

## Key Results
- Achieves attack success rates as low as 0.14% against backdoor attacks
- Maintains main task accuracy up to 98.55% on benchmark datasets
- Outperforms seven state-of-the-art defense methods in both IID and non-IID scenarios
- Effective even when 80% of clients are malicious

## Why This Works (Mechanism)

### Mechanism 1
Backdoor triggers are learned faster than normal classification knowledge during training because the backdoor task is computationally simpler than the main classification task, leading to faster convergence on trigger patterns. This learning speed differential creates a detectable signature that the defense can exploit.

### Mechanism 2
Trigger patterns have a greater effect on image classification than normal class patterns, creating distinguishable classification confidence signatures. When a trigger is present, the model's confidence in classifying to the target category is much higher than when normal class patterns are present.

### Mechanism 3
Generated trigger images can reliably change classification results when overlapped with other images. The CGAN-generated trigger images contain pure trigger knowledge that, when combined with other images, overwhelms the original classification signal and forces misclassification to the target category.

## Foundational Learning

- **Federated Learning architecture and poisoning attacks**: Understanding how FL works and how backdoor attacks exploit it is essential for grasping the defense mechanism. Quick check: What makes federated learning vulnerable to backdoor attacks compared to centralized learning?

- **Conditional Generative Adversarial Networks (CGAN)**: The defense approach relies on CGAN to generate images containing new knowledge and trigger patterns. Quick check: How does a CGAN differ from a standard GAN in terms of input and output structure?

- **Model similarity metrics and differential privacy**: These are alternative defense approaches that the paper compares against, so understanding their limitations is important. Quick check: Why do similarity-based defenses struggle in non-IID data scenarios?

## Architecture Onboarding

- **Component map**: Server-side: CGAN models (two stages), model filtering algorithm, aggregation logic; Client-side: Standard FL training process; Data flow: Old global model → CGAN Stage 1 → trigger filtering → CGAN Stage 2 → model filtering → new global model

- **Critical path**: Model aggregation → knowledge extraction → trigger filtering → model filtering → next round aggregation

- **Design tradeoffs**: Data-free approach vs. potential accuracy loss from false positives; two-stage CGAN generation vs. computational overhead

- **Failure signatures**: High false positive rate in model filtering; CGAN failure to generate effective triggers; defense ineffectiveness in highly non-IID scenarios

- **First 3 experiments**:
  1. Verify CGAN Stage 1 generates images that show clear classification differences between old and new global models
  2. Test trigger filtering stage can isolate trigger knowledge from normal knowledge in generated images
  3. Validate model filtering correctly identifies poisoned models using generated trigger images without excessive false positives

## Open Questions the Paper Calls Out

- **Performance scaling with malicious clients**: How does FilterFL's performance scale when the proportion of malicious clients exceeds 80%? The paper only tested up to 80% malicious clients.

- **Generalization to non-image domains**: Can FilterFL be extended to defend against backdoor attacks in non-image domains, such as text or tabular data? The paper focuses exclusively on image datasets.

- **Heterogeneous client capabilities**: How does FilterFL perform when clients have varying computational capabilities or participate with different frequencies? The paper assumes uniform client participation and resources.

## Limitations

- The theoretical assumptions about backdoor learning speed advantages lack direct empirical validation
- The effectiveness of trigger pattern isolation depends on trigger knowledge remaining distinct from normal knowledge during CGAN generation
- Claims about performance in highly non-IID scenarios are based on limited experimental evidence

## Confidence

- **High confidence**: Experimental results showing FilterFL's effectiveness against existing backdoor attacks (ASR as low as 0.14%) are well-supported by the data presented
- **Medium confidence**: Theoretical explanation of why triggers are learned faster and have greater classification effects is plausible but lacks direct empirical validation
- **Low confidence**: Claims about FilterFL's performance in highly non-IID scenarios (α = 0.1) are based on limited experimental evidence

## Next Checks

1. **Learning Speed Validation**: Conduct controlled experiments measuring the actual learning speed difference between backdoor triggers and normal classification knowledge across different model architectures and trigger complexities

2. **Trigger Isolation Robustness**: Test the CGAN's ability to isolate trigger knowledge from normal knowledge when attackers use blended triggers or when trigger patterns overlap significantly with normal class features

3. **Adaptive Attack Analysis**: Evaluate FilterFL's performance against adaptive attackers who can detect and exploit the CGAN generation process or modify their attack strategy based on the defense mechanism