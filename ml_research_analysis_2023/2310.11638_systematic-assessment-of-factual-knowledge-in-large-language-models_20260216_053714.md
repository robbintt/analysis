---
ver: rpa2
title: Systematic Assessment of Factual Knowledge in Large Language Models
arxiv_id: '2310.11638'
source_url: https://arxiv.org/abs/2310.11638
tags:
- question
- knowledge
- llms
- questions
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for systematically evaluating
  the factual knowledge of large language models (LLMs) by generating questions from
  knowledge graphs. The framework ensures validity, coverage, and diversity of questions
  by using predefined templates and LLM-based generation methods.
---

# Systematic Assessment of Factual Knowledge in Large Language Models

## Quick Facts
- arXiv ID: 2310.11638
- Source URL: https://arxiv.org/abs/2310.11638
- Reference count: 17
- Primary result: Framework evaluates LLM factual knowledge using questions from knowledge graphs; ChatGPT outperforms others, with performance sensitive to instruction finetuning, domain, complexity, and adversarial contexts

## Executive Summary
This paper introduces a systematic framework for evaluating the factual knowledge of large language models using automatically generated questions from knowledge graphs. The approach ensures comprehensive assessment through multiple question types and formats while testing model robustness against adversarial contexts. Experiments with four knowledge graphs across generic and specialized domains reveal that ChatGPT consistently outperforms other models, with performance strongly influenced by instruction finetuning, domain specificity, and question complexity. The framework also demonstrates that LLMs are highly sensitive to adversarial contexts, performing worse when presented with irrelevant or antifactual information.

## Method Summary
The framework generates questions and expected answers from knowledge graph triplets using both template-based and LLM-based methods. Questions are created in multiple formats (true/false, multiple choice, short answer) by iterating through all fact triplets in the knowledge graphs. The evaluation tests four model families (ChatGPT, LLaMA, Alpaca, Vicuna, T5 variants) on Google-RE, T-REx, WikiBio, and UMLS knowledge graphs. Models are prompted to provide answers with confidence indicators, and the F1 metric is used to account for abstention from uncertain responses. The framework also tests model robustness by injecting relevant, irrelevant, and antifactual contexts into questions.

## Key Results
- ChatGPT consistently outperforms other LLMs across all knowledge graphs and question types
- Instruction finetuning significantly impacts performance, with finetuned models showing better instruction-following capabilities
- LLMs exhibit high sensitivity to adversarial contexts, with performance degrading notably for irrelevant and antifactual information
- Specialized domain knowledge (medical) remains challenging for general-purpose LLMs despite strong performance on generic knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Question generation from KGs ensures validity, coverage, and diversity of assessment questions
- Mechanism: The framework automatically generates questions by iterating through all fact triplets in knowledge graphs, using both template-based and LLM-based methods to create multiple question types (TFQ, MCQ, SAQ) with different formats
- Core assumption: Complete knowledge graph coverage guarantees comprehensive evaluation of LLM factual knowledge
- Evidence anchors:
  - [abstract] "Our framework automatically generates a set of questions and expected answers from the facts stored in a given KG"
  - [section 2.1] "We utilize all available triplets and employ two question generation methods from a predefined template or using ChatGPT"
  - [corpus] Weak evidence - the corpus mentions related work on knowledge graph question generation but doesn't specifically validate the coverage claim
- Break condition: If knowledge graphs are incomplete or contain errors, the generated questions will not provide comprehensive coverage

### Mechanism 2
- Claim: LLMs exhibit sensitivity to adversarial contexts, performing worse with irrelevant or antifactual information
- Mechanism: Injecting different types of context (relevant, irrelevant, antifactual) into questions reveals how LLMs respond to conflicting information
- Core assumption: LLMs rely on contextual information for reasoning and can be misled by contradictory evidence
- Evidence anchors:
  - [abstract] "LLMs are also found to be highly sensitive to adversarial contexts, performing worse with irrelevant or antifactual information"
  - [section 3.2] "We inject different contexts to the questions of Google-RE evaluation set and reported the results in Figure 2"
  - [corpus] Weak evidence - the corpus mentions hallucination and knowledge editing but doesn't specifically address context sensitivity
- Break condition: If LLMs develop robust reasoning mechanisms that can distinguish between relevant and irrelevant context

### Mechanism 3
- Claim: Instruction finetuning significantly impacts LLM performance on factual knowledge tasks
- Mechanism: Models with different instruction finetuning datasets show varying performance levels despite sharing the same parametric knowledge base
- Core assumption: Instruction finetuning teaches LLMs how to properly format and respond to knowledge-based questions
- Evidence anchors:
  - [abstract] "We also find that LLMs performance depends on the instruction finetuning, domain and question complexity"
  - [section 3.2] "Both the LLaMA-7B and T5-XL models perform worse than random guessing in TFQ and MCQ, indicating a failure to follow instructions due to the lack of training on instruction finetuning datasets"
  - [corpus] Weak evidence - the corpus mentions scientific knowledge evaluation but doesn't specifically address instruction finetuning impact
- Break condition: If instruction finetuning becomes standardized across models or if models develop instruction-following capabilities through other means

## Foundational Learning

- Concept: Knowledge graph structure and triplet representation
  - Why needed here: Understanding how facts are organized in KGs is essential for generating valid questions
  - Quick check question: What are the three components of a knowledge graph triplet?

- Concept: Question generation methods (template-based vs LLM-based)
  - Why needed here: Different generation approaches produce questions with varying validity and diversity
  - Quick check question: How do template-based and LLM-based question generation differ in terms of control and flexibility?

- Concept: Adversarial prompting and context injection
  - Why needed here: Understanding how context affects LLM responses is crucial for evaluating robustness
  - Quick check question: What are the three types of context used in the evaluation framework?

## Architecture Onboarding

- Component map:
  - Knowledge Graph Input → Question Generation Module → Answer Collection → Evaluation Metrics
  - Context Injection Module → Answer Collection → Robustness Analysis
  - Metric Calculation → Performance Analysis and Reporting

- Critical path: Knowledge Graph → Question Generation → LLM Response → Evaluation
- Design tradeoffs:
  - Template-based vs LLM-based generation: Control vs diversity
  - Exact match vs fuzzy matching: Precision vs recall in evaluation
  - Context injection: Robustness assessment vs increased complexity

- Failure signatures:
  - High abstention rates indicate model uncertainty or poor instruction following
  - Performance degradation with irrelevant context suggests context dependency
  - Poor performance on specialized domains indicates pretraining data limitations

- First 3 experiments:
  1. Generate questions from a simple knowledge graph using both template and LLM methods, compare validity
  2. Evaluate a single LLM on generated questions with and without context injection
  3. Compare performance across different question types and complexity levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the assessment framework be extended to handle incomplete knowledge graphs and implicit facts?
- Basis in paper: [inferred]
- Why unresolved: The paper assumes a complete knowledge graph and evaluates LLMs based on explicit facts only. However, real-world knowledge graphs are often incomplete, containing many implicit facts. The paper mentions this as a limitation but does not propose solutions for incorporating knowledge graph completion methods into the assessment framework.
- What evidence would resolve it: Experiments showing improved evaluation accuracy when using a knowledge graph completion method to generate additional questions from implicit facts, compared to using only explicit facts.

### Open Question 2
- Question: How can the framework be modified to assess complex knowledge represented by combinations of multiple triplets, rather than single triplets?
- Basis in paper: [explicit]
- Why unresolved: The current framework evaluates knowledge based on questions generated from single triplets, which does not capture complex knowledge that may require reasoning over multiple facts. The paper explicitly identifies this as a limitation, noting the need to design a framework that considers the reasoning ability of LLMs on knowledge graphs.
- What evidence would resolve it: A modified framework that generates multi-hop questions requiring reasoning over multiple triplets, with experiments demonstrating improved assessment of LLM knowledge when using such complex questions.

### Open Question 3
- Question: What is the optimal approach for evaluating partially answered questions where LLMs return some but not all correct answers for N-M relations?
- Basis in paper: [explicit]
- Why unresolved: For N-M relations with multiple correct answers, the paper notes that LLMs might not return all possible answers. However, it does not provide a concrete method for evaluating such partially answered questions, leaving this as an open question for assessing LLM knowledge.
- What evidence would resolve it: A proposed evaluation metric or method for scoring partially answered N-M questions, with experiments showing improved assessment reliability compared to current binary correct/incorrect evaluation.

## Limitations

- Framework effectiveness depends on completeness and accuracy of knowledge graphs used for question generation
- Study does not fully address impact of training data overlap between pretraining corpora and knowledge graphs
- Robustness findings for context sensitivity may not generalize to all LLM architectures

## Confidence

- **High confidence**: ChatGPT's superior performance across domains and the impact of instruction finetuning on model behavior
- **Medium confidence**: The sensitivity to adversarial contexts, as the results are based on a limited set of context types
- **Low confidence**: The generalizability of F1 metric as the optimal evaluation measure

## Next Checks

1. Test the framework with additional knowledge graphs from underrepresented domains to assess coverage limitations and potential bias
2. Conduct ablation studies on context injection types to identify which adversarial patterns most effectively reveal model weaknesses
3. Compare F1-based evaluation against human-annotated ground truth for a subset of questions to validate the metric's alignment with human judgment