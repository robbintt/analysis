---
ver: rpa2
title: Controlling Federated Learning for Covertness
arxiv_id: '2308.08825'
source_url: https://arxiv.org/abs/2308.08825
tags:
- policy
- learner
- learning
- eavesdropper
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework for covert optimization in federated
  learning by dynamically scheduling learning and obfuscation queries. The problem
  is modeled as a Markov decision process (MDP), and structural results show the optimal
  policy has a monotone threshold structure.
---

# Controlling Federated Learning for Covertness

## Quick Facts
- arXiv ID: 2308.08825
- Source URL: https://arxiv.org/abs/2308.08825
- Reference count: 40
- Key outcome: Optimal threshold policy reduces eavesdropper accuracy to 52% (vs 83% for greedy) while maintaining comparable learner accuracy (81% vs 82-84%)

## Executive Summary
This paper addresses the problem of hiding optimization trajectories in federated learning from an eavesdropper who can observe but not respond to the learner's queries. The authors formulate this as a Markov decision process where the learner dynamically schedules between learning queries (which provide gradient information) and obfuscating queries (which provide noise) to minimize eavesdropper accuracy while satisfying learning constraints. The paper proves that the optimal policy has a monotone threshold structure and proposes a computationally efficient policy gradient algorithm to find this policy without requiring knowledge of transition probabilities.

## Method Summary
The method formulates covert optimization as an MDP with three state components: stochastic oracle state (modeling client participation and data availability), learner state (remaining successful updates budget), and optimization queue state. The learner chooses between learning queries (which may succeed or fail based on oracle state) and obfuscating queries (which always provide noise). The authors prove the optimal policy has a monotone threshold structure using supermodularity arguments, then propose a structured policy gradient algorithm (SPGA) that parameterizes the threshold policy with sigmoidal functions and updates parameters via finite-difference gradient estimates. The approach is evaluated on a hate speech classification task using the Jigsaw unintended bias dataset with 20 clients.

## Key Results
- Optimal threshold policy reduces eavesdropper accuracy to 52% when eavesdropper has no data (vs 83% for greedy policy)
- With 10% positive samples, eavesdropper accuracy is 69% under optimal policy (vs 83% for greedy)
- Learner accuracy remains comparable: 81% under optimal policy vs 82-84% under greedy policy
- SPGA algorithm successfully learns threshold parameters that approximate the true optimal thresholds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The learner's optimal policy is a monotone threshold structure because the MDP's dynamic programming operator is supermodular.
- Mechanism: Supermodularity of the Q-function in (learner state, action) pairs implies that the optimal policy switches from obfuscate to learn at a single threshold learner state value.
- Core assumption: The cost function c(u, yO) decreases in action u and oracle state yO; the transition matrix has first-order stochastic dominance.
- Evidence anchors:
  - [abstract] "we show that the dynamic programming operator has a supermodular structure implying that the optimal policy has a monotone threshold structure."
  - [section] Theorem 2 proof relies on supermodularity and integer convexity of learning cost.
  - [corpus] Not explicitly supported; related work on covert optimization does not mention threshold structure proofs.
- Break condition: If cost function violates monotonicity or transition matrix loses stochastic dominance, threshold structure may fail.

### Mechanism 2
- Claim: The policy gradient algorithm converges to near-optimal thresholds without knowledge of transition probabilities.
- Mechanism: Parameterized sigmoidal policy approximates discrete thresholds; finite-difference gradient estimates update parameters to minimize average cost and satisfy learning constraint.
- Core assumption: Approximate policy converges to true threshold as τ → 0; gradients can be estimated via system interaction.
- Evidence anchors:
  - [abstract] "A computationally efficient policy gradient algorithm is proposed to search for the optimal policy without knowledge of the transition probabilities."
  - [section] Algorithm 1 description and Figure 4 show convergence to true thresholds.
  - [corpus] Related works on policy gradient methods mention sample efficiency but not threshold approximation.
- Break condition: If state space is too large or gradient estimates too noisy, convergence may stall or diverge.

### Mechanism 3
- Claim: Eavesdropper's proportional sampling estimator can be balanced by ensuring equal priors over two SGD trajectories.
- Mechanism: Learner poses majority of queries from parallel SGD trajectory, forcing eavesdropper posterior to split evenly.
- Core assumption: Eavesdropper partitions observed queries into two SGD trajectories and assumes equal priors without additional data.
- Evidence anchors:
  - [abstract] "the eavesdropper can only achieve a validation accuracy of 52% with no information... compared to 83% when the learner employs a greedy policy."
  - [section] "Deﬁnition 2. Proportional sampling estimator..." and "An eavesdropper using a proportional sampling estimator... can be obfuscated by ensuring a) that the eavesdropper has equal priors over the two trajectory and b) that the majority of the queries posed are obfuscating."
  - [corpus] No direct support; related work on privacy in FL focuses on differential privacy, not trajectory hiding.
- Break condition: If eavesdropper has access to a public dataset, equal prior assumption fails and estimation accuracy increases.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation
  - Why needed here: To model the sequential decision problem of choosing between learning and obfuscation queries under stochastic oracle noise.
  - Quick check question: What are the state, action, and cost components of the MDP in this problem?

- Concept: Supermodularity and monotone threshold policies
  - Why needed here: To prove that the optimal policy has a threshold structure, enabling efficient policy search.
  - Quick check question: How does supermodularity of the Q-function imply a monotone threshold optimal policy?

- Concept: Policy gradient methods and approximation
  - Why needed here: To find the optimal threshold policy without requiring knowledge of transition probabilities.
  - Quick check question: What is the role of the sigmoidal policy approximation in the structured policy gradient algorithm?

## Architecture Onboarding

- Component map:
  - Stochastic oracle (client participation, data availability) -> Learner state (remaining successful updates) -> Eavesdropper (proportional sampling estimator) -> MDP solver (structured policy gradient algorithm) -> Parallel SGD (obfuscation trajectory)

- Critical path:
  1. Initialize learner and oracle states
  2. Choose action (learn or obfuscate) via current policy
  3. Query oracle and update learner state if successful
  4. Eavesdropper estimates based on observed queries
  5. Update policy parameters via structured policy gradient
  6. Repeat until convergence or budget exhausted

- Design tradeoffs:
  - Threshold approximation (τ parameter) vs. policy accuracy
  - Step size κ and scale parameter ρ in SPGA vs. convergence speed
  - Number of oracle states W vs. model complexity
  - Learning constraint Λ vs. privacy vs. learning speed

- Failure signatures:
  - Learner accuracy plateaus but eavesdropper accuracy remains high → insufficient obfuscation
  - Both accuracies degrade → poor threshold policy or unstable MDP
  - SPGA parameters diverge → step sizes or gradient estimates too large
  - Queue instability → learning constraint too tight or success probabilities too low

- First 3 experiments:
  1. Run SPGA with a fixed small step size to verify threshold convergence on synthetic MDP
  2. Test threshold policy in controlled oracle noise setting (deterministic oracle)
  3. Compare eavesdropper accuracy under optimal vs. greedy policy with known oracle transition matrix

## Open Questions the Paper Calls Out

- Question: How can the eavesdropper infer the optimal learner policy using inverse reinforcement learning techniques?
  - Basis in paper: [explicit] The paper suggests future work could investigate inverse RL techniques for the eavesdropper to infer the optimal learner policy.
  - Why unresolved: The paper does not explore how the eavesdropper could use inverse RL to learn the learner's policy, which would be valuable for understanding the security of the proposed covert optimization framework.
  - What evidence would resolve it: Empirical results showing the eavesdropper's ability to infer the learner's policy using inverse RL, and the impact on the eavesdropper's performance in the hate speech classification task.

- Question: What are more robust gradient trajectory hiding schemes that could be used in place of or in addition to the proposed obfuscating queries?
  - Basis in paper: [explicit] The paper mentions that more robust gradient trajectory hiding schemes could be studied in future work.
  - Why unresolved: The paper only explores one method of obfuscating queries to hide the learner's trajectory. There may be other, more effective ways to achieve this goal.
  - What evidence would resolve it: Experimental results comparing the performance of the proposed method with alternative gradient trajectory hiding schemes in terms of eavesdropper accuracy and learner accuracy.

- Question: How can the covert optimization framework be extended to a decentralized setting with participants switching between learning and obfuscating others?
  - Basis in paper: [explicit] The paper suggests investigating covert optimization in a decentralized setting where the problem is modeled as a switching control game with participants switching between learning and obfuscating others.
  - Why unresolved: The paper only considers a centralized setting where a single learner controls the learning and obfuscation. A decentralized setting would be more realistic and could have different dynamics.
  - What evidence would resolve it: Theoretical analysis of the decentralized setting and experimental results comparing the performance of the centralized and decentralized approaches.

## Limitations
- The threshold policy structure relies critically on supermodularity assumptions that may not hold in more complex federated learning scenarios with non-monotonic cost functions or irregular transition dynamics.
- The eavesdropper's proportional sampling estimator assumes equal priors over SGD trajectories, which breaks down when the eavesdropper has access to any ground truth data.
- The SPGA algorithm's convergence guarantees are asymptotic and may require careful tuning of step sizes and temperature parameters in practice.

## Confidence
- Supermodularity-based threshold policy proofs: Medium (novel but relies on specific cost function assumptions)
- Policy gradient algorithm: Medium (empirically validated but theoretical convergence bounds not fully established)
- Overall framework effectiveness: Medium-High (demonstrated on synthetic and real datasets but with limited adversarial model variations)

## Next Checks
1. Test threshold policy robustness when eavesdropper has 20-50% of ground truth data to identify where equal prior assumption fails
2. Evaluate policy performance under non-Markovian oracle dynamics with time-varying client availability patterns
3. Benchmark SPGA against alternative methods like actor-critic or model-based RL approaches for policy optimization