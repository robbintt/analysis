---
ver: rpa2
title: 'Prompt Generate Train (PGT): Few-shot Domain Adaption of Retrieval Augmented
  Generation Models for Open Book Question-Answering'
arxiv_id: '2307.05915'
source_url: https://arxiv.org/abs/2307.05915
tags:
- answer
- question
- generate
- arxiv
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the Prompt Generate Train (PGT) framework for
  few-shot domain adaptation of retrieval augmented generation (RAG) models in open-book
  question-answering. The framework addresses the challenge of developing accurate
  and cost-effective RAG models for domain-specific question-answering without requiring
  large manually curated training datasets.
---

# Prompt Generate Train (PGT): Few-shot Domain Adaption of Retrieval Augmented Generation Models for Open Book Question-Answering

## Quick Facts
- arXiv ID: 2307.05915
- Source URL: https://arxiv.org/abs/2307.05915
- Authors: 
- Reference count: 3
- Primary result: PGT framework generates RAG models competitive with GPT-4 while being significantly cheaper to serve through synthetic training data generation and reinforcement learning

## Executive Summary
This paper introduces the Prompt Generate Train (PGT) framework for few-shot domain adaptation of retrieval augmented generation (RAG) models in open-book question-answering. The framework addresses the challenge of developing accurate and cost-effective RAG models for domain-specific question-answering without requiring large manually curated training datasets. PGT generates synthetic training data using a combination of GPT-4 and Flan-T5 XXL, then fine-tunes a smaller RAG model (ColBERT-v2 + Flan-T5) using supervised learning and reinforcement learning with a synthetic reward model. The framework also incorporates uncertainty calibration to help the model recognize when it cannot confidently answer questions. Experiments show that PGT can produce RAG models competitive with GPT-4-based systems in generating relevant answers while being significantly cheaper to serve.

## Method Summary
The PGT framework consists of three main phases: synthetic data generation, RAG model fine-tuning, and uncertainty calibration. First, it generates high-quality synthetic training data comprising <passage, question, answer> tuples using a medium sized LLM (Flan-T5 XXL) and a novel consistency filtering scheme. Second, it fine-tunes a smaller RAG model (ColBERT-v2 retriever + Flan-T5 generator) using supervised learning and reinforcement learning with a synthetic reward model trained to distinguish domain-grounded from hallucinated answers. Finally, it calibrates the model's uncertainty for extractive question-answers, enabling the model to "know when it knows" and "know when it doesn't know." The entire framework is designed to produce RAG models that are both accurate and significantly cheaper to serve than GPT-4-based systems.

## Key Results
- PGT generates RAG models competitive with GPT-4-based systems in generating relevant answers
- The framework achieves significant cost reduction compared to serving GPT-4-based systems
- Synthetic training data generation with consistency filtering produces high-quality domain-specific question-answer pairs
- Reinforcement learning with synthetic feedback improves the model's ability to generate grounded answers
- Uncertainty calibration enables the model to recognize when it cannot confidently answer questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic training data generation via Flan-T5 XXL produces high-quality <passage, question, answer> tuples that effectively adapt RAG models to target domains
- Mechanism: The framework leverages GPT-4 to generate a seed set of question-answer pairs, then uses Flan-T5 XXL with consistency filtering to generate more training samples while maintaining quality
- Core assumption: Large language models can generate synthetic training data that captures the domain-specific patterns needed for effective RAG adaptation
- Evidence anchors:
  - [abstract] "The framework's synthetic generation pipeline generates high quality synthetic training data comprising <passage, question, answer> tuples using a medium sized LLM, Flan-T5 XXL, and a novel consistency filtering scheme"
  - [section] "We posit that this will outperform a few short-prompted LSLM for the following reasons: Specialized pretraining also helps smaller LLMs outperform plain vanilla pre-trained LSLMs"
  - [corpus] Weak - No direct evidence in corpus neighbors about synthetic data quality
- Break condition: If consistency filtering fails to maintain quality standards or if generated questions/answers don't capture domain-specific patterns

### Mechanism 2
- Claim: Reinforcement Learning with Synthetic Feedback (RLSF) aligns RAG models to generate answers grounded in the underlying corpus
- Mechanism: RLSF uses a reward model trained on contrastive loss between matching and non-matching tuples to provide relevance signals for PPO-based fine-tuning
- Core assumption: A reward model trained on synthetically generated data can effectively distinguish between grounded and hallucinated answers
- Evidence anchors:
  - [abstract] "In the next phase, the framework aligns to the RAG model with the target domain using Reinforcement Learning (Proximal Policy Optimization). This step improves the RAG model's ability to generate grounded answers and ignore out of domain questions"
  - [section] "We adapt RLHF but without recourse to human preference feedback to design a new technique – Reinforcement Learning with Synthetic Feedback (RLSF) – for aligning the model to generate answers grounded in the underlying passages"
  - [corpus] Weak - No direct evidence in corpus neighbors about reinforcement learning effectiveness
- Break condition: If reward model fails to distinguish between grounded and hallucinated answers, or if PPO optimization diverges

### Mechanism 3
- Claim: Uncertainty calibration enables the model to "know when it knows" and "know when it doesn't know" for extractive question-answers
- Mechanism: The model is fine-tuned to predict whether its generated answers are correct or wrong based on the supporting evidence, using cross-entropy loss on indirect logits
- Core assumption: Models can be trained to assess their own answer correctness through self-prediction tasks
- Evidence anchors:
  - [abstract] "In the final phase, the framework calibrates the model's uncertainty for extractive question-answers. This is a desirable feature since the model can be integrated into a cascading system such as FRUGAL GPT"
  - [section] "We want the student LLM to be calibrated for uncertainty. Informally, the model should 'know when it knows the answer' and 'know when it doesn't know the answer'"
  - [corpus] Weak - No direct evidence in corpus neighbors about uncertainty calibration
- Break condition: If model fails to accurately predict answer correctness or if calibration leads to excessive false negatives

## Foundational Learning

- Concept: Dense retrieval and ColBERT-v2 architecture
  - Why needed here: Understanding how the retriever component works with chunks vs documents is critical for implementing the modified retrieval procedure
  - Quick check question: What is the key difference between traditional dense retrieval and the modified retrieval procedure described in the paper?

- Concept: Reinforcement Learning and Proximal Policy Optimization (PPO)
  - Why needed here: RLSF relies on PPO to fine-tune the RAG model using reward signals from the trained reward model
  - Quick check question: How does PPO differ from standard policy gradient methods in terms of stability and sample efficiency?

- Concept: Reward modeling and contrastive learning
  - Why needed here: The reward model uses contrastive loss to distinguish between matching and non-matching tuples, which is essential for effective RLSF
  - Quick check question: What is the purpose of using contrastive loss in training the reward model?

## Architecture Onboarding

- Component map: Data Generation Pipeline: GPT-4 → Flan-T5 XXL → Consistency Filtering → RAG Model: ColBERT-v2 Retriever + Flan-T5 Generator → Reward Model: BERT-based relevance scorer → Training Pipeline: SFT → RLSF (PPO) → Uncertainty Calibration
- Critical path: Data Generation → RAG SFT → Reward Model Training → RLSF → Uncertainty Calibration
- Design tradeoffs:
  - Model size vs serving cost: Using smaller models (<10B parameters) for cost efficiency
  - Synthetic data quality vs quantity: Balancing generation volume with consistency filtering
  - Context window size: Managing document chunk sizes for retrieval vs generation quality
- Failure signatures:
  - Low-quality generated questions/answers: Indicates issues with data generation pipeline
  - Reward model not distinguishing between matching/non-matching tuples: Suggests reward model training issues
  - RAG model not improving after RLSF: Could indicate PPO optimization problems or reward signal issues
- First 3 experiments:
  1. Test data generation pipeline with a small corpus to verify consistency filtering effectiveness
  2. Validate reward model training by checking if it correctly ranks matching vs non-matching tuples
  3. Run SFT on a small dataset to ensure RAG model can learn from synthetic data before full RLSF

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal ratio of non-matching to matching question-answer pairs during training to achieve the best performance on out-of-domain questions?
- Basis in paper: [explicit] The paper mentions that "the ratio of number of non-matching to matching question-answer pairs is a design parameter that can be set based on downstream requirements" but does not provide empirical results on optimal ratios.
- Why unresolved: The paper discusses the importance of including non-matching pairs but does not experimentally determine what ratio works best across different domains or use cases.
- What evidence would resolve it: Empirical results showing model performance (accuracy, hallucination rate, confidence calibration) across different ratios of non-matching to matching pairs, tested on multiple domain datasets.

### Open Question 2
- Question: How does the performance of PGT compare to other few-shot domain adaptation approaches when applied to highly specialized domains with limited vocabulary overlap with pre-training corpora?
- Basis in paper: [inferred] While the paper claims PGT works in few-shot settings, it doesn't provide comparative analysis against other domain adaptation methods specifically for specialized domains with minimal vocabulary overlap.
- Why unresolved: The evaluation focuses on general domain adaptation without specifically addressing scenarios where target domains have very different vocabularies from pre-training data.
- What evidence would resolve it: Head-to-head comparisons of PGT with alternative few-shot adaptation methods (e.g., adapter-based approaches, prompt tuning) on specialized domain datasets like medical or legal texts.

### Open Question 3
- Question: What is the impact of different uncertainty calibration methods on the overall system performance, and how does this affect downstream cascading systems like FRUGAL GPT?
- Basis in paper: [explicit] The paper mentions uncertainty calibration as a design goal and describes a method, but doesn't evaluate how different calibration approaches affect end-to-end system performance in cascading architectures.
- Why unresolved: The paper describes the calibration methodology but doesn't empirically test how calibration quality affects decision-making in systems that use confidence scores to route queries.
- What evidence would resolve it: Experimental results showing how different uncertainty calibration methods affect the performance of cascading systems, including false positive/negative rates and cost-benefit trade-offs in query routing decisions.

## Limitations

- The synthetic data generation pipeline relies heavily on consistency filtering mechanisms that are not fully specified
- The effectiveness of the synthetic reward model in distinguishing grounded from hallucinated answers requires more empirical validation
- The uncertainty calibration mechanism, while conceptually sound, lacks detailed evaluation on its practical effectiveness

## Confidence

- High confidence: The overall framework design and problem formulation are well-articulated and address a real need in domain-specific question-answering
- Medium confidence: The synthetic data generation pipeline and consistency filtering approach appear reasonable but lack detailed specifications
- Low confidence: The effectiveness of RLSF with synthetic rewards and the uncertainty calibration mechanism require more empirical validation

## Next Checks

1. **Synthetic Data Quality Validation**: Implement the consistency filtering mechanism with controlled parameters and measure the semantic overlap between generated questions and their corresponding passages across multiple domain datasets. This would quantify the actual quality of synthetic data being produced.

2. **Reward Model Effectiveness**: Test the reward model's ability to distinguish between grounded and hallucinated answers by creating a test set with known matching and non-matching tuples across different domains. Measure precision/recall on this task before and after RLSF fine-tuning.

3. **Uncertainty Calibration Accuracy**: Evaluate the uncertainty calibration component by creating test cases where the model should know it doesn't know the answer. Measure false positive rates (saying "I don't know" when it should know) versus false negative rates (saying it knows when it shouldn't) across different confidence thresholds.