---
ver: rpa2
title: Recourse under Model Multiplicity via Argumentative Ensembling (Technical Report)
arxiv_id: '2312.15097'
source_url: https://arxiv.org/abs/2312.15097
tags:
- ensembling
- which
- such
- then
- counterfactual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of providing robust counterfactual
  explanations under model multiplicity (MM), where multiple equally-performing models
  produce inconsistent predictions. The authors formalize the recourse-aware ensembling
  problem and identify desirable properties including non-emptiness, model agreement,
  counterfactual validity, and counterfactual coherence.
---

# Recourse under Model Multiplicity via Argumentative Ensembling (Technical Report)

## Quick Facts
- arXiv ID: 2312.15097
- Source URL: https://arxiv.org/abs/2312.15097
- Reference count: 32
- This paper introduces argumentative ensembling as a robust method for finding counterfactual explanations under model multiplicity.

## Executive Summary
This paper addresses the challenge of providing robust counterfactual explanations when multiple equally-performing models produce inconsistent predictions. The authors propose argumentative ensembling, which uses computational argumentation to resolve conflicts between models and their counterfactual explanations while accommodating user preferences. The method guarantees counterfactual validity and coherence while maintaining accuracy comparable to naive ensembling. Empirical evaluation shows it consistently finds valid explanations where baseline methods fail up to 70% of the time.

## Method Summary
The method trains multiple neural networks (150 per dataset) with different architectures and random seeds, then applies three ensembling approaches: augmented, robust, and argumentative. Argumentative ensembling constructs a bipolar argumentation framework where models and counterfactual explanations are arguments, with attacks representing disagreements or invalidity. S-preferred extensions from this framework provide conflict-free, robust solutions that satisfy desirable properties including counterfactual validity and coherence.

## Key Results
- Argumentative ensembling achieves 100% counterfactual validity while baselines fail up to 70% of cases
- Maintains comparable accuracy to naive ensembling while satisfying more desirable properties
- Ensemble size ranges from 22.5-58.0% of model set size across datasets
- Successfully resolves conflicts in model multiplicity while incorporating user preferences

## Why This Works (Mechanism)

### Mechanism 1
Argumentative ensembling resolves model multiplicity conflicts by treating models and their counterfactual explanations as arguments in a bipolar argumentation framework. The system constructs a BAF where models attack disagreeing models and invalid CEs, with models/CEs mutually supporting each other. S-preferred extensions provide conflict-free, robust solutions. Core assumption: preferences over models can be encoded as attacks in the argumentation framework.

### Mechanism 2
The method guarantees counterfactual validity and coherence by design. The BAF ensures selected models must have their corresponding CEs, with attacks only between disagreeing or invalid pairs. This guarantees all selected CEs are valid for all selected models. Core assumption: preference ordering over models is strict enough to break ties in the BAF.

### Mechanism 3
Argumentative ensembling maintains comparable accuracy to naive ensembling while providing better counterfactual properties. When multiple s-preferred extensions exist, the system selects the one with the same prediction as naive ensembling, preserving accuracy while improving counterfactual robustness. Core assumption: the s-preferred extension matching naive ensembling exists and is acceptable.

## Foundational Learning

- **Bipolar argumentation frameworks (BAFs)**: Core mechanism uses BAFs to resolve conflicts between models and CEs. Quick check: What's the difference between a BAF and a standard abstract argumentation framework?
- **Counterfactual explanations (CEs)**: CEs are the recourse mechanism being ensembled, and their validity is crucial for the system. Quick check: How do you define a counterfactual explanation for a binary classifier?
- **Model multiplicity (MM)**: Problem space where multiple equally-performing models produce inconsistent predictions. Quick check: What are the main challenges posed by model multiplicity in real-world applications?

## Architecture Onboarding

- **Component map**: Input processor -> Preference encoder -> BAF constructor -> Solver -> Selector -> Output formatter
- **Critical path**: Input → Preference encoding → BAF construction → Extension solving → Selection → Output
- **Design tradeoffs**: Computational complexity vs. counterfactual robustness; accuracy vs. property satisfaction
- **Failure signatures**: Empty s-preferred sets (preference ordering issues), multiple equally valid extensions (need tie-breaking heuristics), slow solving times (BAF optimization needed)
- **First 3 experiments**: 1) Test with 2-3 simple models where preferences clearly break ties, 2) Test with identical models to verify empty extension handling, 3) Test with conflicting preferences to verify tie-breaking behavior

## Open Questions the Paper Calls Out
- **Scalability**: How does argumentative ensembling scale with large numbers of models (100+ models)? The paper notes large-scale argumentation solvers would be desirable for experiments beyond 30 models.
- **Preference encoding**: What is the impact of different preference specification methods on explanation quality? The paper uses simple linear ordering but notes other ways to define preferences can be defined.
- **Generalization**: How robust are explanations under model retraining or distribution shifts? The paper focuses on fixed models and inputs but doesn't address model updates or data drift.

## Limitations
- Computational scalability untested beyond 30 models per dataset
- Performance implications of preference encoding unclear for complex model properties
- Results limited to binary classification with neural networks

## Confidence
- **High confidence**: Theoretical guarantees for counterfactual validity and coherence
- **Medium confidence**: Empirical performance claims based on three datasets
- **Low confidence**: Generalizability to other model types and non-binary classification tasks

## Next Checks
1. **Scalability test**: Evaluate argumentative ensembling with 1000+ models to verify computational tractability and runtime performance
2. **Robustness analysis**: Systematically vary the preference encoding to test sensitivity to different weightings
3. **Cross-domain validation**: Apply the method to a non-binary classification task to assess generalizability beyond current datasets