---
ver: rpa2
title: Adversarial Training Using Feedback Loops
arxiv_id: '2308.11881'
source_url: https://arxiv.org/abs/2308.11881
tags:
- feedback
- adversarial
- input
- control
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel method for adversarial training using
  feedback loops. The main idea is to incorporate a feedback control mechanism into
  the neural network architecture.
---

# Adversarial Training Using Feedback Loops

## Quick Facts
- arXiv ID: 2308.11881
- Source URL: https://arxiv.org/abs/2308.11881
- Reference count: 4
- Key outcome: Feedback Looped Adversarial Training (FLAT) improves robustness against various adversarial attacks on CIFAR-10 and CIFAR-100 datasets

## Executive Summary
This paper proposes a novel method for adversarial training using feedback loops. The approach incorporates a feedback control mechanism into neural network architecture by adding a controller network that takes the output of the main network and produces a correction signal to be fed back to the input. The controller is trained alongside the main network using adversarial examples. The proposed FLAT method demonstrates improved robustness against various adversarial attacks on CIFAR-10 and CIFAR-100 datasets compared to state-of-the-art methods.

## Method Summary
The Feedback Looped Adversarial Training (FLAT) method involves modifying the neural network architecture to include a feedback controller network. The controller takes the output of the main network and generates a correction signal that is fed back to the input. Both the main network and controller are trained jointly using a combination of regular and adversarial examples. The training algorithm considers both original data points and perturbed data points, with the controller learning to stabilize the system outputs by generating appropriate correction signals.

## Key Results
- On CIFAR-10, FLAT achieves 72.25% accuracy against FGSM attack and 59.56% against PGD-20 attack
- On CIFAR-100, FLAT achieves 40.38% accuracy against FGSM attack and 30.12% against PGD-20 attack
- FLAT outperforms previous state-of-the-art methods by 6-10% on CIFAR-10 and shows similar improvements on CIFAR-100

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The feedback controller predicts the perturbation applied to the input and generates a correction signal that, when subtracted from the adversarial input, recovers the original (benign) input.
- **Core assumption**: The controller can learn to map perturbed outputs back to their corresponding input perturbations, even when those perturbations are adversarial in nature.
- **Evidence anchors**: The controller is trained using regular and adversarial data to stabilize system outputs, with corrected input approximately recovering the reference output.
- **Break condition**: If the controller cannot learn the mapping from perturbed outputs back to input perturbations, the correction signal will not effectively undo the adversarial perturbation.

### Mechanism 2
- **Claim**: Iterative feedback through the controller network progressively reduces the output perturbation, leading to more stable and accurate predictions.
- **Core assumption**: Multiple iterations of feedback will lead to a better approximation of the original input and more accurate output predictions.
- **Evidence anchors**: The method empirically shows that FLAT is more effective than state-of-the-art methods to guard against adversarial attacks.
- **Break condition**: If iterative feedback does not lead to progressively smaller output perturbations, the system may converge to an incorrect solution or fail to stabilize.

### Mechanism 3
- **Claim**: Training the controller network on both regular and adversarial data allows it to learn to distinguish between benign and adversarial inputs and generate appropriate corrections.
- **Core assumption**: The controller can learn to differentiate between regular and adversarial inputs based on the perturbed output and generate appropriate corrections.
- **Evidence anchors**: The training algorithm considers both original data points and perturbed data points to train the controller.
- **Break condition**: If the controller cannot learn to differentiate between regular and adversarial inputs, it may generate incorrect corrections.

## Foundational Learning

- **Concept**: Adversarial training
  - Why needed here: Adversarial training is the foundation of the proposed method, as it involves training the network on both regular and adversarial examples to improve robustness.
  - Quick check question: What is the main goal of adversarial training, and how does it differ from standard training?

- **Concept**: Feedback control
  - Why needed here: Feedback control is the core principle behind the proposed method, as it involves using the output of the network to generate corrections for the input.
  - Quick check question: How does negative feedback control work, and what are its key components?

- **Concept**: Neural network architecture
  - Why needed here: Understanding neural network architecture is crucial for implementing the proposed method, as it involves modifying the architecture to include a feedback loop.
  - Quick check question: What are the main components of a neural network, and how do they interact during forward and backward passes?

## Architecture Onboarding

- **Component map**: Input -> Main network -> Controller network -> Correction signal -> Corrected input -> Main network (iterative)
- **Critical path**: Input -> Main network -> Controller network -> Correction signal -> Corrected input -> Main network (iterative)
- **Design tradeoffs**:
  - Controller network complexity vs. performance: A more complex controller network may lead to better performance but also increased computational cost.
  - Number of feedback iterations: More iterations may lead to better stability but also increased computational cost.
  - Training data balance: The balance between regular and adversarial training data may affect the controller's ability to distinguish between benign and adversarial inputs.
- **Failure signatures**:
  - Poor robustness against adversarial attacks: If the controller cannot effectively undo the adversarial perturbation, the system will fail to stabilize.
  - Slow convergence: If the iterative feedback process does not lead to progressively smaller output perturbations, the system may take a long time to converge.
  - Incorrect corrections: If the controller cannot learn to differentiate between regular and adversarial inputs, it may generate incorrect corrections.
- **First 3 experiments**:
  1. Implement the feedback neural network architecture with a simple controller network and test its performance on a small dataset (e.g., MNIST) against a basic adversarial attack (e.g., FGSM).
  2. Vary the number of feedback iterations and observe the effect on robustness against adversarial attacks.
  3. Train the controller network on different ratios of regular to adversarial data and observe the effect on its ability to distinguish between benign and adversarial inputs.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How should the architecture of the controller network be optimized for different main neural network architectures to maximize robustness?
- **Basis in paper**: [explicit] The paper mentions that "determining the optimal architecture of a controller network for a given main NN is a separate research problem, beyond the scope of this paper, and will be addressed in future work."
- **Why unresolved**: The paper uses a simple 3-layer feedforward controller architecture but acknowledges this may not be optimal for all scenarios.
- **What evidence would resolve it**: Systematic experiments comparing different controller architectures across various main NN models and datasets, showing which controller designs consistently lead to the best adversarial robustness.

### Open Question 2
- **Question**: Can features from intermediate layers of the main neural network, beyond just the final output and last layer features, be effectively utilized in the feedback controller to further improve robustness?
- **Basis in paper**: [explicit] The paper states "Future work will study the use of features from other layers of the original neural network to improve the resulting robustness of the Feedback NN."
- **Why unresolved**: The current implementation only uses the final prediction vector and features from the last ResNet block as controller inputs.
- **What evidence would resolve it**: Experiments comparing FLAT implementations using features from different intermediate layers of the main NN, demonstrating whether and how intermediate layer features contribute to improved adversarial robustness.

### Open Question 3
- **Question**: What is the theoretical relationship between the control theory concepts (like stability margins, convergence rates) and the adversarial robustness achieved by FLAT?
- **Basis in paper**: [inferred] While the paper draws inspiration from control theory and uses control-theoretic terminology, it does not establish a formal theoretical connection between control-theoretic properties and adversarial robustness metrics.
- **Why unresolved**: The paper demonstrates empirical improvements in robustness but does not provide a theoretical framework linking control system properties to adversarial robustness.
- **What evidence would resolve it**: Mathematical analysis deriving theoretical bounds on adversarial robustness in terms of control-theoretic parameters, validated through experiments showing how changes in controller gains or feedback loop parameters affect robustness metrics.

## Limitations
- Experimental results are limited to CIFAR-10 and CIFAR-100 datasets without evaluation on more complex real-world scenarios
- The theoretical framework relies heavily on assumptions about the controller's ability to accurately predict and correct perturbations
- Lack of ablation studies to isolate the contribution of different components of the FLAT method

## Confidence
- **Medium**: Claims about improved robustness against various attacks are supported by experimental results but lack comprehensive analysis of failure modes and limitations
- **Medium**: The theoretical framework for feedback control in neural networks is sound but relies on assumptions that may not generalize to all adversarial scenarios
- **High**: The core concept of using feedback loops for adversarial training is well-founded and supported by the mathematical framework presented

## Next Checks
1. Implement ablation studies to evaluate the individual contributions of the controller network architecture, feedback iterations, and training data balance to overall performance
2. Test the FLAT method on larger-scale datasets (e.g., ImageNet) and more complex model architectures (e.g., ResNets, Transformers) to assess scalability and generalization
3. Conduct comprehensive analysis of failure modes, including investigation of scenarios where the controller network fails to correct perturbations effectively and the impact of different types of adversarial attacks on the feedback mechanism