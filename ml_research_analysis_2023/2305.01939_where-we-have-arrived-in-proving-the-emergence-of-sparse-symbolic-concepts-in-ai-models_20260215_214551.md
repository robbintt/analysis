---
ver: rpa2
title: Where We Have Arrived in Proving the Emergence of Sparse Symbolic Concepts
  in AI Models
arxiv_id: '2305.01939'
source_url: https://arxiv.org/abs/2305.01939
tags:
- concepts
- interaction
- interactive
- have
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides conditions under which trained AI models encode
  sparse symbolic concepts, meaning their outputs can be approximated by a small number
  of interpretable interaction patterns between input variables. The authors prove
  that if a model has low-order non-zero derivatives, is monotonic on occluded inputs,
  and does not degrade significantly when inputs are masked, then it will encode sparse
  interactions.
---

# Where We Have Arrived in Proving the Emergence of Sparse Symbolic Concepts in AI Models

## Quick Facts
- arXiv ID: 2305.01939
- Source URL: https://arxiv.org/abs/2305.01939
- Reference count: 40
- Primary result: Proves conditions under which trained AI models encode sparse symbolic concepts via low-order derivatives, monotonicity under masking, and bounded confidence degradation

## Executive Summary
This paper establishes theoretical conditions under which trained AI models encode sparse symbolic concepts that can be approximated by a small number of interpretable interaction patterns between input variables. The authors prove that when models have at most M-order non-zero derivatives, exhibit monotonicity on occluded inputs, and maintain bounded confidence degradation when inputs are masked, their outputs can be decomposed into sparse interactions defined via Harsanyi dividends from cooperative game theory. Empirical validation across multiple DNN architectures (MLPs, ResMLPs, CNNs, PointNets) and diverse datasets (MNIST, CelebA, ShapeNet, tabular data) confirms that only a small fraction of interactions have significant effects on predictions, validating the theoretical framework.

## Method Summary
The paper proves conditions for sparse symbolic concept emergence through three key assumptions: (1) models have at most M-order non-zero derivatives, (2) models are monotonic on occluded samples with higher confidence when less occluded, and (3) model confidence doesn't significantly degrade on occluded samples. The method involves computing Harsanyi interactions between input variables using finite difference methods for ReLU networks, then applying threshold-based filtering to identify sparse, interpretable interaction patterns. The theoretical framework is validated by applying these conditions to various DNN architectures trained on multiple datasets and measuring the sparsity of resulting interaction effects.

## Key Results
- Theoretical proof that models with low-order derivatives, monotonicity under masking, and bounded confidence degradation encode sparse symbolic concepts
- Empirical validation showing only a small fraction of interactions have significant effects across MLPs, ResMLPs, CNNs, and PointNets
- Demonstration that sparsity properties hold across diverse datasets including MNIST, CelebA, ShapeNet, and tabular data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AI models encode sparse symbolic concepts when high-order derivatives are zero
- Mechanism: Low-order derivatives limit interaction complexity to manageable terms
- Core assumption: Model output can be approximated by finite-order Taylor expansion
- Evidence anchors:
  - [abstract]: "high-order derivatives of the network output w.r.t. the input variables are all zero"
  - [section]: "Assumption 1. The model is assumed to have at most M-order non-zero derivatives"
  - [corpus]: Weak - no direct neighbor papers discuss derivative sparsity
- Break condition: Model requires high-order derivatives for task performance

### Mechanism 2
- Claim: Monotonicity ensures concepts become sparser with masking
- Mechanism: Confidence increases as fewer inputs are masked, enforcing sparsity
- Core assumption: Well-trained models maintain monotonic behavior under occlusion
- Evidence anchors:
  - [abstract]: "The DNN can be used on occluded samples and when the input sample is less occluded, the DNN will yield higher confidence"
  - [section]: "Assumption 2 (Weak monotonicity). The model output is assumed to monotonically increase with the size of the unmasked set"
  - [corpus]: Weak - neighbors don't address monotonicity in occlusion scenarios
- Break condition: Model confidence decreases with less masking due to poor training

### Mechanism 3
- Claim: Polynomial bounds on output degradation enforce concept sparsity
- Mechanism: Upper bounds on interaction effects force most to be negligible
- Core assumption: Classification confidence doesn't degrade exponentially with masking
- Evidence anchors:
  - [abstract]: "The confidence of the DNN does not significantly degrade on occluded samples"
  - [section]: "Assumption 3. Given the average model output of all samples with m unmasked input variables, we assume a lower bound"
  - [corpus]: Weak - no neighbor papers discuss polynomial bounds on confidence degradation
- Break condition: Model confidence drops exponentially with masking

## Foundational Learning

- Concept: Taylor series expansion and finite differences
  - Why needed here: The proof relies on representing model outputs as finite-order Taylor expansions
  - Quick check question: What is the mathematical relationship between Taylor series terms and interaction effects?

- Concept: Game theory and Harsanyi interactions
  - Why needed here: The paper defines symbolic concepts using game-theoretic interaction metrics
  - Quick check question: How does the Harsanyi dividend quantify interaction between input variables?

- Concept: Sparsity and threshold-based filtering
  - Why needed here: The proof establishes conditions under which only a small fraction of interactions have significant effects
  - Quick check question: Why does the paper set a threshold τ for determining "valid" interactive concepts?

## Architecture Onboarding

- Component map: Input → Model → Taylor expansion → Interaction extraction → Sparsity analysis
- Critical path: Input masking → Output computation → Interaction calculation → Sparsity verification
- Design tradeoffs: Trade precision for interpretability by accepting approximation error in interaction effects
- Failure signatures: Non-monotonic behavior under masking, high-order derivative presence, exponential confidence degradation
- First 3 experiments:
  1. Verify monotonicity assumption on a simple MLP trained on MNIST with increasing masking levels
  2. Compute interaction sparsity for a ResNet on CIFAR-10 and compare to theoretical bounds
  3. Test robustness of sparsity claims under different masking baseline choices

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the assumptions about monotonicity (Assumption 2) and lower bounds on model output (Assumption 3) hold for models trained on noisy or imbalanced datasets?
- Basis in paper: The authors explicitly state that these assumptions may not hold if the model is not well-trained or if many input variables are background/noisy.
- Why unresolved: The paper does not provide empirical validation of these assumptions across diverse real-world datasets with varying noise levels and class imbalances.
- What evidence would resolve it: Experiments showing how sparsity of interactions changes with different levels of dataset noise and class imbalance, and whether monotonicity assumptions break down.

### Open Question 2
- Question: What is the computational complexity of computing high-order derivatives for large models, and how does this impact practical verification of sparsity?
- Basis in paper: The authors mention using finite difference methods to compute equivalent high-order derivatives for ReLU networks, but don't discuss computational feasibility.
- Why unresolved: The paper focuses on theoretical proofs without addressing practical implementation challenges for large-scale models.
- What evidence would resolve it: Empirical measurements of computation time for derivative calculations on various model architectures, and exploration of approximation methods.

### Open Question 3
- Question: How does the sparsity of interactions change when using different baseline values for input masking?
- Basis in paper: The definition of interactions depends on a baseline value b, but the choice of baseline is not explored.
- Why unresolved: The paper assumes a fixed baseline without investigating sensitivity to baseline choice.
- What evidence would resolve it: Experiments comparing interaction sparsity across different baseline selection strategies (mean, median, random, etc.) and measuring how baseline choice affects the theoretical conditions.

## Limitations
- The three key assumptions (low-order derivatives, monotonicity under masking, bounded confidence degradation) may not hold universally for all model types and training regimes
- Monotonicity assumption under masking is particularly critical and could break down for poorly trained models or noisy datasets
- Polynomial bounds on confidence degradation may be too restrictive for models trained with certain objectives or regularization schemes

## Confidence
- High confidence in the mathematical framework and theoretical derivations
- Medium confidence in empirical validation across tested architectures
- Low confidence in universal applicability beyond the tested model families

## Next Checks
1. **Robustness to Architecture Variants**: Test the sparsity conditions on recently developed architectures (Vision Transformers, MLPs with different activation functions) to assess generalizability.
2. **Training Regime Impact**: Investigate how different training objectives (self-supervised, contrastive) and regularization techniques affect the three key assumptions, particularly monotonicity under masking.
3. **Scaling Analysis**: Examine whether the sparsity properties scale predictably with model size and dataset complexity, and identify any breaking points where assumptions fail.