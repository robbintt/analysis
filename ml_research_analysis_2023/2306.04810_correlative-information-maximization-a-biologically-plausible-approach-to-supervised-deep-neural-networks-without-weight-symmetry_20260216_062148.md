---
ver: rpa2
title: 'Correlative Information Maximization: A Biologically Plausible Approach to
  Supervised Deep Neural Networks without Weight Symmetry'
arxiv_id: '2306.04810'
source_url: https://arxiv.org/abs/2306.04810
tags:
- network
- neural
- learning
- layer
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes correlative information maximization (CorInfoMax)
  as a normative framework for supervised learning in biologically plausible neural
  networks. The method maximizes linear dependence between layer activations using
  two alternative expressions for correlative mutual information, naturally leading
  to forward and backward prediction networks without weight symmetry.
---

# Correlative Information Maximization: A Biologically Plausible Approach to Supervised Deep Neural Networks without Weight Symmetry

## Quick Facts
- **arXiv ID**: 2306.04810
- **Source URL**: https://arxiv.org/abs/2306.04810
- **Reference count**: 40
- **Primary result**: CorInfoMax enables supervised deep learning without weight symmetry, achieving comparable performance to backpropagation on image classification tasks while addressing key biological plausibility concerns

## Executive Summary
This paper proposes correlative information maximization (CorInfoMax) as a normative framework for supervised learning in biologically plausible neural networks. The method maximizes linear dependence between layer activations using two alternative expressions for correlative mutual information, naturally leading to forward and backward prediction networks without weight symmetry. This addresses a major critique of biological plausibility in backpropagation. The optimization yields a network structure of multi-compartment pyramidal neurons with dendritic processing and lateral inhibitory neurons. Experiments on image classification tasks (MNIST, Fashion-MNIST, CIFAR10/100) show CorInfoMax networks achieve comparable or superior performance to other biologically plausible methods while resolving the weight symmetry issue.

## Method Summary
CorInfoMax employs two equivalent expressions for correlative mutual information between consecutive layers - one involving forward prediction errors and the other involving backward prediction errors. The gradient optimization of these expressions inherently produces separate forward and backward prediction networks with independent weights. The framework uses coordinate descent optimization that alternates between updating layer activations and prediction coefficients, naturally emerging three-compartment pyramidal neuron structures. Lateral connections and autapses emerge to maximize entropy within layers. Training uses contrastive Hebbian learning updates for synapses while activations are updated via gradient ascent, avoiding the need for symmetric weights.

## Key Results
- CorInfoMax networks achieve comparable or superior performance to other biologically plausible methods (EP, CSM, PC, PC-Nudge) on MNIST, Fashion-MNIST, and CIFAR10/100
- The approach resolves the weight symmetry issue by using two alternative CMI expressions that lead to asymmetric forward and backward networks
- Network structure naturally emerges as multi-compartment pyramidal neurons with apical and basal dendrites, and lateral inhibitory connections
- CorInfoMax provides a normative framework that addresses multiple concerns about biological plausibility of conventional ANNs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: CorInfoMax naturally leads to asymmetric forward and backward prediction networks without weight symmetry issues
- **Mechanism**: Two equivalent expressions for correlative mutual information between layers - one using forward prediction errors and the other using backward prediction errors - lead to structurally different optimization problems
- **Core assumption**: Mathematical equivalence between the two CMI expressions breaks down when implemented in neural networks
- **Evidence anchors**: Abstract states these alternatives "intrinsically lead to forward and backward prediction networks without weight symmetry issues"; section explains how two CMI expressions result in potentially asymmetric networks
- **Break condition**: If mathematical equivalence between the two CMI expressions holds or if optimization forces weight tying

### Mechanism 2
- **Claim**: Network architecture emerges naturally from CorInfoMax objective through coordinate descent optimization
- **Mechanism**: Alternating optimization between layer activations and prediction coefficients creates three-compartment pyramidal neuron structures where apical dendrites receive top-down input and lateral connections, basal dendrites receive feedforward input, and somatic compartments integrate these signals
- **Core assumption**: Coordinate descent optimization on CorInfoMax objective converges to biologically plausible structure without explicit architectural constraints
- **Evidence anchors**: Section states coordinate descent optimization "gives rise to a neural network structure that emulates a more biologically realistic network of multi-compartment pyramidal neurons"
- **Break condition**: If coordinate descent fails to converge or converges to non-biologically plausible structure

### Mechanism 3
- **Claim**: Lateral connections and autapses emerge naturally from CorInfoMax objective to maximize entropy within layers
- **Mechanism**: CorInfoMax objective includes terms maximizing unconditional layer entropy, achieved through lateral connections between neurons and autapses that promote utilization of all dimensions within representation space
- **Core assumption**: Maximizing unconditional layer entropy requires lateral connections and autapses implemented through correlation matrix inverse structure
- **Evidence anchors**: Section explains introduction of "set membership constraints on layer activations... encourages compression of information and sparse representations"
- **Break condition**: If lateral connections don't effectively maximize entropy or introduce unwanted correlations

## Foundational Learning

- **Concept**: Correlative Mutual Information (CMI)
  - Why needed here: CorInfoMax is built on maximizing CMI between consecutive layers, focusing on linear dependence rather than arbitrary nonlinear relationships
  - Quick check question: How does CorInfoMax's use of CMI differ from traditional Shannon mutual information maximization approaches?

- **Concept**: Coordinate Descent Optimization
  - Why needed here: CorInfoMax uses coordinate descent to alternately optimize layer activations and prediction coefficients, crucial for understanding network structure emergence
  - Quick check question: What are the two alternating optimization steps in CorInfoMax's coordinate descent approach?

- **Concept**: Multi-compartment Neuron Models
  - Why needed here: Proposed network architecture incorporates three-compartment pyramidal neurons essential for biological plausibility claims
  - Quick check question: What are the three compartments in the pyramidal neuron model and what inputs do they receive?

## Architecture Onboarding

- **Component map**: Input layer (r(0)) → Hidden layers (r(1) to r(P-1)) → Output layer (r(P))
  Each hidden layer has three compartments: soma (u), basal dendrites (v_B), apical dendrites (v_A)
  Interneurons (i) provide lateral inhibition within each layer
  Feedforward weights (W_ff) connect basal dendrites between layers
  Feedback weights (W_fb) connect apical dendrites between layers
  Lateral weights (O) provide inhibition between neurons in same layer
  Autapses (D) provide self-connections for entropy maximization

- **Critical path**:
  1. Forward pass: Input propagates through feedforward connections to basal dendrites
  2. Lateral connections and autapses update layer activations
  3. Backward pass: Top-down predictions flow through feedback connections to apical dendrites
  4. Prediction errors computed at soma-basal and soma-apical interfaces
  5. Weight updates occur based on contrastive Hebbian learning

- **Design tradeoffs**:
  - CorInfoMax vs Backpropagation: CorInfoMax avoids weight symmetry but requires more complex neuron models
  - Information maximization vs task performance: Maximizing information flow may not always optimize for specific classification tasks
  - Biological plausibility vs computational efficiency: Multi-compartment neurons are more realistic but computationally more expensive

- **Failure signatures**:
  - Poor convergence: May indicate issues with learning rates or forgetting factors
  - Symmetric weights: Would suggest dual CMI expressions aren't creating asymmetric networks as intended
  - Vanishing/exploding gradients: Could indicate problems with coordinate descent optimization or parameter initialization
  - Lack of sparsity: Might suggest lateral inhibition connections aren't properly implemented

- **First 3 experiments**:
  1. Implement single hidden layer CorInfoMax network on MNIST and verify reasonable accuracy without weight symmetry
  2. Measure angle between feedforward and feedback weights to confirm asymmetry (should be non-zero)
  3. Visualize activation patterns to confirm sparsity and entropy maximization within layers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CorInfoMax framework scale to deeper networks and more complex architectures like convolutional networks?
- Basis in paper: [explicit] Paper demonstrates results on 2-3 layer MLPs but does not explore deeper architectures or CNNs
- Why unresolved: Paper focuses on MLP architectures with limited depth, providing no experiments or analysis for deeper networks or CNNs
- What evidence would resolve it: Experiments on deeper MLPs (5-10 layers) and CNNs on standard datasets showing comparable or superior performance to backpropagation-based models

### Open Question 2
- Question: What is the theoretical relationship between correlative mutual information objective and other biologically plausible learning rules?
- Basis in paper: [explicit] Paper claims CorInfoMax provides normative framework leading to predictive coding and multi-compartment neurons but lacks formal derivation or comparison to other rules
- Why unresolved: While proposing CorInfoMax as unifying principle, lacks rigorous mathematical proofs or empirical comparisons to established biologically plausible learning rules
- What evidence would resolve it: Formal mathematical derivations showing CorInfoMax as generalization or special case of other rules, and empirical studies comparing performance across different frameworks

### Open Question 3
- Question: How sensitive is CorInfoMax framework to hyperparameter choices, and can these be optimized automatically or learned?
- Basis in paper: [inferred] Ablation studies show some sensitivity to hyperparameters but don't explore systematic optimization or automatic tuning methods
- Why unresolved: Paper relies on manual hyperparameter tuning with limited sensitivity analysis, leaving questions about framework's robustness and potential for automated optimization
- What evidence would resolve it: Comprehensive sensitivity analyses across wider range of hyperparameters, and demonstrations of successful automatic hyperparameter optimization or meta-learning approaches

## Limitations

- Biological plausibility claims rely heavily on theoretical derivations rather than experimental validation in biological systems
- Relationship between correlative mutual information and task performance is not thoroughly examined - maximizing information flow may not always optimize for classification accuracy
- Coordinate descent optimization convergence properties are not rigorously proven, particularly for deeper networks

## Confidence

- **High confidence**: Mathematical framework for CorInfoMax is well-established and core claim about asymmetric weight learning is theoretically sound
- **Medium confidence**: Experimental results on benchmark datasets are promising but comparison with other biologically plausible methods could be more comprehensive
- **Low confidence**: Claims about multi-compartment neuron emergence from optimization objectives lack direct experimental validation

## Next Checks

1. **Weight asymmetry verification**: Systematically measure angle between forward and backward weight matrices across all layers during training to confirm they remain asymmetric throughout optimization
2. **Information flow analysis**: Quantify actual information content at each layer during training to verify CorInfoMax is maximizing correlative mutual information as claimed
3. **Ablation study on lateral connections**: Train networks with and without lateral inhibition connections to empirically demonstrate their necessity for achieving reported performance and sparsity patterns