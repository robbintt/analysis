---
ver: rpa2
title: 'LLP-Bench: A Large Scale Tabular Benchmark for Learning from Label Proportions'
arxiv_id: '2310.10096'
source_url: https://arxiv.org/abs/2310.10096
tags:
- bags
- datasets
- dataset
- label
- very
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLP-Bench is the first large-scale tabular benchmark for learning
  from label proportions (LLP). It consists of 70 datasets derived from Criteo CTR
  and Sponsored Search datasets, encompassing 52 feature bags and 8 random bags, totaling
  13.5 to 24.75 million bags.
---

# LLP-Bench: A Large Scale Tabular Benchmark for Learning from Label Proportions

## Quick Facts
- **arXiv ID**: 2310.10096
- **Source URL**: https://arxiv.org/abs/2310.10096
- **Reference count**: 40
- **Primary result**: LLP-Bench provides the first large-scale tabular benchmark for learning from label proportions, consisting of 70 datasets derived from Criteo CTR data with comprehensive hardness characterization.

## Executive Summary
LLP-Bench is the first large-scale tabular benchmark for learning from label proportions (LLP), addressing the lack of standardized evaluation frameworks for this weak supervision paradigm. The benchmark consists of 70 datasets created from the Criteo CTR dataset, encompassing 52 feature bags (grouped by categorical features) and 4 random bags, totaling 13.5 to 24.75 million bags. Four metrics are proposed to characterize dataset hardness: standard deviation of label proportions, inter vs intra-bag separation ratio, mean bag size, and cumulative bag size distribution. The performance of 9 state-of-the-art LLP techniques is evaluated, with SIM-LLP, DLLP-BCE, DLLP-MSE, and GenBags achieving AUC scores in the 72%-78% range on feature bags.

## Method Summary
LLP-Bench creates LLP datasets by grouping instances from the Criteo CTR dataset based on categorical feature values (feature bags) or randomly (random bags). Bags are filtered to ensure minimum size of 50, maximum size of 2500, and at least 30% instance retention. Nine baseline methods are implemented: DLLP-BCE/MSE, GenBags, Easy-LLP, OT methods, SIM-LLP, and Mean-Map. Models use a 2-layer MLP architecture (128-64-1 nodes) with learning rate 1e-5, Adam optimizer, and early stopping. Performance is evaluated using 5-fold cross-validation with AUC scoring, and four metrics quantify dataset hardness.

## Key Results
- LLP-Bench contains 70 datasets (52 feature bags + 4 random bags) derived from Criteo CTR data
- SIM-LLP, DLLP-BCE, DLLP-MSE, and GenBags achieve AUC scores in the 72%-78% range on feature bags
- Four hardness metrics successfully characterize dataset difficulty: LabelPropStdev, InterIntraRatio, MeanBagSize, and CumuBagSizeDist
- Feature bags show more diverse hardness characteristics than random bags, making them more challenging for LLP algorithms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLP-Bench datasets exhibit diverse hardness characteristics that directly impact baseline performance.
- Mechanism: The four proposed metrics capture different aspects of dataset difficulty. Higher LabelPropStdev provides more supervision signal, higher InterIntraRatio indicates better bag separation, and smaller MeanBagSize provides more granular supervision per bag.
- Core assumption: The four metrics are independent and capture orthogonal aspects of dataset hardness.
- Evidence anchors:
  - [abstract]: "Four metrics are proposed to characterize dataset hardness: standard deviation of label proportions, inter vs intra-bag separation ratio, mean bag size, and cumulative bag size distribution."
  - [section]: "We propose four metrics that help characterize and quantify the hardness of a LLP dataset."
  - [corpus]: Weak - corpus papers don't discuss these specific metrics in detail.

### Mechanism 2
- Claim: Feature bags created by grouping on categorical features provide more challenging supervision than random bags.
- Mechanism: Feature bags enforce instances within bags to share categorical feature values, creating natural clustering in feature space. This increases intra-bag similarity while maintaining inter-bag separation.
- Core assumption: Real-world LLP applications often involve feature-based aggregation due to privacy constraints.
- Evidence anchors:
  - [abstract]: "One of the unique properties of tabular LLP is the ability to create feature bags where all the instances in a bag have the same value for a given feature."
  - [section]: "In feature bags, bags are constructed such that all instances within the bag have the same value for given key(s) called the grouping key(s)."
  - [corpus]: Weak - corpus papers mention LLP but don't specifically analyze feature bag vs random bag differences.

### Mechanism 3
- Claim: Bag size distribution (tail properties) significantly affects model performance and algorithm choice.
- Mechanism: Datasets with long-tailed bag size distributions contain many small bags (high supervision) and few large bags (low supervision). This creates a multi-scale learning problem where algorithms must handle both fine-grained and coarse-grained supervision simultaneously.
- Core assumption: Bag size distribution is an independent source of difficulty beyond the four main metrics.
- Evidence anchors:
  - [section]: "CumuBagSizeDist: The bag sizes for any dataset are characterized by their cumulative distribution function... Bags of large sizes provide a very little label information for a lot of feature level information."
  - [corpus]: Weak - corpus papers don't discuss bag size distribution effects in detail.

## Foundational Learning

- Concept: Learning from Label Proportions (LLP)
  - Why needed here: LLP is the core learning paradigm being benchmarked, where models learn from bag-level label proportions rather than individual labels.
  - Quick check question: In LLP, what information is available at training time - individual instance labels or only bag-level label proportions?

- Concept: Bag creation methodologies (random vs feature-based)
  - Why needed here: Understanding how different bag creation strategies affect dataset characteristics and algorithm performance is crucial for interpreting benchmark results.
  - Quick check question: What distinguishes feature bags from random bags in terms of instance selection criteria?

- Concept: Weak supervision learning frameworks
  - Why needed here: LLP is a form of weak supervision, and understanding this broader context helps in comparing with other approaches and understanding limitations.
  - Quick check question: How does LLP differ from other weak supervision paradigms like multiple instance learning or positive-unlabeled learning?

## Architecture Onboarding

- Component map: Dataset creation pipeline (Criteo data preprocessing → bag formation → filtering) → Metric computation engine (LabelPropStdev, InterIntraRatio, MeanBagSize, CumuBagSizeDist) → Baseline implementation suite (DLLP, GenBags, Easy-LLP, OT methods, SIM-LLP, Mean-Map) → Evaluation framework (5-fold cross-validation with AUC scoring)
- Critical path: For each dataset: create bags → filter bags → split into train/test folds → train baseline models → evaluate performance → analyze metric correlations
- Design tradeoffs: Large bag sizes provide privacy but reduce supervision quality; aggressive filtering removes informative datasets but ensures tractability; minibatch training balances computational efficiency with gradient quality
- Failure signatures: Poor AUC scores across all methods suggest dataset filtering is too aggressive; inconsistent performance across folds indicates implementation bugs; metrics failing to explain performance differences suggest missing hardness factors
- First 3 experiments:
  1. Run all baselines on a single simple feature bag dataset to verify implementation correctness and establish baseline performance ranges
  2. Compute all four hardness metrics on the same dataset to verify metric calculation correctness and understand their values
  3. Analyze performance vs metric correlations on a small subset of datasets to validate the hardness characterization framework

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would LLP-Bench performance change if constructed from datasets other than Criteo CTR (e.g., other large-scale tabular datasets with different feature distributions)?
- Basis in paper: [explicit] The paper acknowledges using only Criteo CTR and suggests future work could include more tabular datasets, but does not explore this.
- Why unresolved: The benchmark's generalizability across different data distributions and feature types is untested.
- What evidence would resolve it: Replicating LLP-Bench methodology on multiple large-scale tabular datasets (e.g., Taobao, Avazu) and comparing metric distributions and baseline performance trends.

### Open Question 2
- Question: Would different bag filtering thresholds (low_thresh, high_thresh, instance_thresh) significantly alter the diversity and hardness characterization of LLP-Bench datasets?
- Basis in paper: [explicit] The authors chose specific threshold values but note these are hyperparameters whose impact is not explored.
- Why unresolved: The sensitivity of dataset characteristics and model performance to filtering parameters is unknown.
- What evidence would resolve it: Systematically varying filtering thresholds and measuring changes in the four hardness metrics and baseline AUC score distributions across the resulting dataset subsets.

### Open Question 3
- Question: Do the proposed hardness metrics (LabelPropStdev, InterIntraRatio, MeanBagSize, CumuBagSizeDist) capture all relevant aspects of LLP dataset difficulty, or are there additional factors that should be considered?
- Basis in paper: [inferred] The authors acknowledge that additional metrics could provide deeper explanation of performance outliers, suggesting current metrics may be incomplete.
- Why unresolved: The analysis shows some performance outliers cannot be fully explained by the four metrics, indicating potential missing factors.
- What evidence would resolve it: Identifying additional dataset properties (e.g., feature correlation structure, bag label distribution skewness) that correlate with model performance and incorporating them into a more comprehensive hardness framework.

## Limitations

- The four proposed hardness metrics, while intuitive, lack rigorous validation against real-world LLP difficulty and may have redundancies.
- The benchmark's reliance on a single source dataset (Criteo) limits generalizability to other domains and bag creation strategies.
- Claims about the four metrics capturing orthogonal aspects of dataset hardness lack rigorous statistical validation.

## Confidence

- **High**: Dataset creation methodology and filtering criteria are well-specified with clear thresholds. The baseline implementation details (architecture, losses, hyperparameters) are sufficiently detailed for reproduction.
- **Medium**: The characterization of feature bags vs random bags as having different hardness profiles is supported by experimental results but lacks theoretical justification.
- **Low**: Claims about the four metrics capturing orthogonal aspects of dataset hardness lack rigorous statistical validation.

## Next Checks

1. **Metric Correlation Analysis**: Compute pairwise correlations between the four hardness metrics across all 70 datasets to assess independence claims and identify potential redundancies.
2. **Cross-Dataset Generalization**: Apply the benchmark baselines to at least two additional publicly available tabular datasets (e.g., from OpenML) to test the generalizability of performance patterns and metric effectiveness.
3. **Ablation Study**: Remove individual metrics from the hardness characterization framework and evaluate whether remaining metrics can still explain performance differences between methods.