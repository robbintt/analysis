---
ver: rpa2
title: 'Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation'
arxiv_id: '2306.05783'
source_url: https://arxiv.org/abs/2306.05783
tags:
- engineering
- llms
- https
- science
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Xiezhi, a comprehensive benchmark for evaluating
  large language models' (LLMs) domain knowledge. Xiezhi consists of 220,000 multiple-choice
  questions across 516 disciplines in 13 subjects.
---

# Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation

## Quick Facts
- **arXiv ID**: 2306.05783
- **Source URL**: https://arxiv.org/abs/2306.05783
- **Reference count**: 40
- **Primary result**: Introduces Xiezhi benchmark with 220,000 questions across 516 disciplines to evaluate LLMs' domain knowledge

## Executive Summary
This paper presents Xiezhi, a comprehensive benchmark for evaluating large language models' (LLMs) domain knowledge across 516 disciplines in 13 subjects. The benchmark consists of 220,000 multiple-choice questions, plus two supplementary datasets with 15,000 questions each. A novel evaluation setting with 50 options per question is proposed to better differentiate model capabilities by reducing random guessing accuracy from 25% to 2%. Experiments on 47 LLMs show that state-of-the-art models surpass human performance in science, engineering, agronomy, medicine, and art domains, but fall short in economics, jurisprudence, pedagogy, literature, history, and management.

## Method Summary
The Xiezhi benchmark is constructed by collecting questions from various academic sources and exams, then annotating them with disciplinary categories using a hybrid approach of ChatGPT and human verification. An auto-updating mechanism generates new questions from open academic surveys using an annotation model. The evaluation framework uses generative probabilities rather than direct multiple-choice answering, with each question presenting 50 options (3 confusing options, 1 correct answer, and 46 random options from the dataset). Models rank all 50 options, and performance is measured using Mean Reciprocal Rank (MRR).

## Key Results
- SOTA LLMs surpass human performance in science, engineering, agronomy, medicine, and art domains
- SOTA models fall short of human performance in economics, jurisprudence, pedagogy, literature, history, and management
- Fine-tuning on domain-specific data improves domain performance but may reduce general capability
- The benchmark effectively distinguishes capabilities across diverse LLMs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Using 50 options per question reduces random guessing accuracy and better reveals model capability differences.
- **Mechanism**: Expanding options from 4 to 50 drops random guessing accuracy from 25% to 2%, sharpening evaluation sensitivity.
- **Core assumption**: Larger option pools disproportionately affect weaker models more than stronger ones.
- **Evidence anchors**:
  - [abstract] "We propose a new evaluation setting with 50 options per question to better differentiate model capabilities."
  - [section] "We set 50 options for each multiple-choice question... resulting in significantly reducing the accuracy of random guessing."
- **Break condition**: If models adapt to generate high-confidence responses across large option sets regardless of correctness.

### Mechanism 2
- **Claim**: Fine-tuning on domain-specific data improves performance in those domains but may reduce general domain comprehension.
- **Mechanism**: Fine-tuning with domain-specific corpora enhances domain knowledge retrieval but can narrow general language understanding.
- **Core assumption**: Trade-off exists between domain specialization and general capability.
- **Evidence anchors**:
  - [section] "Small LMs enhance domain capabilities at the expense of generic capabilities... this observation aligns with findings from previous research."
- **Break condition**: If fine-tuning techniques allow simultaneous improvement in both domain and general language understanding.

### Mechanism 3
- **Claim**: Auto-updating question generation using open academic documents keeps the benchmark fresh and relevant.
- **Mechanism**: Continuously generates new questions from academic surveys and uses annotation models to tag disciplines, preventing obsolescence.
- **Core assumption**: Academic surveys contain well-established domain knowledge that remains relevant over time.
- **Evidence anchors**:
  - [section] "Our auto-updating method comprises three primary components... generation of questions from open academic documents."
- **Break condition**: If academic surveys become less representative of practical knowledge or annotation model accuracy degrades.

## Foundational Learning

- **Concept**: Domain taxonomy and hierarchical classification
  - Why needed here: Understanding Chinese Discipline Taxonomy structure is crucial for navigating Xiezhi effectively.
  - Quick check question: What are the three levels of discipline taxonomy in the Chinese Disciplinary Taxonomy?

- **Concept**: Fine-tuning and its impact on model capabilities
  - Why needed here: Recognizing how fine-tuning affects domain vs. general performance is essential for interpreting benchmark results.
  - Quick check question: How does fine-tuning on domain-specific data affect a model's performance in general language understanding tasks?

- **Concept**: Evaluation metrics for ranking and comparison
  - Why needed here: Understanding MRR is necessary for analyzing model performance and comparing different LLMs.
  - Quick check question: What does a higher MRR score indicate about a model's ability to rank the correct answer?

## Architecture Onboarding

- **Component map**: Data collection -> Manual annotation -> Auto annotation -> Question generation -> Evaluation framework
- **Critical path**: The process from question collection through annotation to evaluation is critical for maintaining benchmark quality and relevance
- **Design tradeoffs**: Balancing comprehensiveness with manageability of manual annotation; generative probabilities increase accuracy but also computational cost
- **Failure signatures**: Inaccurate annotations leading to misaligned questions; degradation in auto-annotation model performance; insufficient diversity in generated questions
- **First 3 experiments**:
  1. Validate annotation model's accuracy by comparing outputs to human-annotated samples
  2. Test auto-updating mechanism by generating new questions and assessing quality and relevance
  3. Evaluate impact of using 50 options versus fewer options on model performance discrimination

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but raises several important considerations:
- How to maintain annotation quality as the benchmark scales
- The trade-off between computational cost and discriminative power of the 50-option setting
- The generalizability of results to non-Chinese domain knowledge

## Limitations

- The benchmark's generalizability to non-Chinese domain knowledge remains unclear, as most questions derive from Chinese academic surveys
- Annotation quality of auto-generated questions is unverified, with no reported human evaluation
- The 50-option evaluation setting may create computational overhead that limits practical application
- Comparison between generative probability ranking and direct multiple-choice answering lacks ablation studies

## Confidence

- **High**: The benchmark successfully distinguishes between different LLMs' domain knowledge capabilities
- **Medium**: The claim that fine-tuning improves domain performance at the expense of general capability
- **Low**: The assertion that SOTA models surpass human performance in specific domains

## Next Checks

1. Conduct human evaluation studies to verify that auto-generated questions maintain the same quality and difficulty as manually curated questions
2. Perform ablation studies comparing the 50-option evaluation setting against traditional 4-option multiple-choice formats
3. Test the benchmark's effectiveness on non-Chinese LLMs to assess cross-linguistic generalization and identify potential language-specific biases