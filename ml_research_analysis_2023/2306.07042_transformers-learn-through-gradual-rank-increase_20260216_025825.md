---
ver: rpa2
title: Transformers learn through gradual rank increase
arxiv_id: '2306.07042'
source_url: https://arxiv.org/abs/2306.07042
tags:
- time
- dynamics
- weights
- learning
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper establishes that transformer models exhibit incremental
  learning dynamics, where the difference between trained and initial weights grows
  in rank during training. This phenomenon is rigorously proven for a simplified model
  with diagonal weight matrices and small initialization, and experimentally validated
  for practical vision transformers.
---

# Transformers learn through gradual rank increase

## Quick Facts
- arXiv ID: 2306.07042
- Source URL: https://arxiv.org/abs/2306.07042
- Authors: 
- Reference count: 40
- The paper establishes that transformer models exhibit incremental learning dynamics, where the difference between trained and initial weights grows in rank during training.

## Executive Summary
This paper reveals that transformer models learn through gradual rank increase, where weight differences grow incrementally in rank during training. The authors prove this phenomenon for a simplified model with diagonal weight matrices and small initialization, showing that weight changes occur in discrete stages with rank increasing by at most one per stage. They validate these theoretical predictions experimentally on practical vision transformers, demonstrating similar behavior even without the simplifying assumptions.

## Method Summary
The paper combines theoretical analysis with empirical validation. Theoretically, it analyzes gradient flow dynamics in a simplified model where weight matrices are diagonal, showing that small initialization leads to incremental rank increases through conservation laws. Empirically, it trains vision transformers on CIFAR-10/100 and ImageNet, tracking the normalized spectra of attention matrices and computing the stable rank of weight differences to observe incremental learning dynamics and low-rank bias.

## Key Results
- Weight differences between trained and initial transformers grow in rank gradually during training
- Theoretical proof shows rank increases by at most one per stage under small initialization
- Experiments confirm incremental rank increase in practical vision transformers without simplifying assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The rank of the difference between trained and initial weights increases gradually during training
- Mechanism: Small initialization leads to weight matrices remaining close to zero for extended periods, followed by rapid rank increases in discrete stages
- Core assumption: Initialization scale α is small enough that the system remains in a linear regime initially
- Evidence anchors:
  - [abstract] "the difference between trained and initial weights progressively increases in rank"
  - [section] "dynamics occur in discrete stages: (1) during most of each stage, the loss plateaus because the weights remain close to a saddle point, and (2) at the end, the saddle point is quickly escaped and the rank of the weights increases by at most one"
  - [corpus] No direct evidence found - the neighbor papers focus on LoRA and incremental learning in different contexts
- Break condition: If initialization scale is too large, the system enters a "lazy training" regime where learning behaves like kernel methods

### Mechanism 2
- Claim: Each stage adds at most one to the rank of the weight difference
- Mechanism: The conservation law u²ᵢ - v²ᵢ = constant means only one coordinate can become "active" at a time, as growing both uᵢ and vᵢ simultaneously would violate this constraint
- Core assumption: The network architecture can be expressed as f(x; u, v) = h(x; u ⊙ v) where ⊙ is elementwise multiplication
- Evidence anchors:
  - [abstract] "the rank of the weights increases by at most one per stage"
  - [section] "Lemma 4.1 (Conservation law)... This reduces the degrees of freedom and means that we need only keep track of p parameters in total"
  - [corpus] No direct evidence found - neighbor papers don't discuss this specific conservation property
- Break condition: If the conservation law is violated (e.g., with certain activation functions or network architectures)

### Mechanism 3
- Claim: The learning dynamics alternate between plateaus and rapid changes
- Mechanism: During plateaus, weights remain near saddle points where gradients are small; during rapid changes, the system escapes these saddle points and activates new coordinates
- Core assumption: Stationary points are strict local minima (Assumption 4.3)
- Evidence anchors:
  - [abstract] "during most of each stage, the loss plateaus because the weights remain close to a saddle point"
  - [section] "At the end, the saddle point is quickly escaped and the rank of the weights increases by at most one"
  - [corpus] No direct evidence found - neighbor papers focus on different aspects of learning dynamics
- Break condition: If the stationary points are not strict local minima, the system may get stuck or exhibit chaotic behavior

## Foundational Learning

- Concept: Gradient flow dynamics
  - Why needed here: The paper analyzes continuous-time optimization rather than discrete gradient descent steps
  - Quick check question: What is the relationship between gradient flow and gradient descent with small learning rates?

- Concept: Implicit regularization
  - Why needed here: The paper shows that the training dynamics themselves induce a low-rank bias without explicit regularization
  - Quick check question: How does implicit regularization differ from explicit regularization techniques like weight decay?

- Concept: Rank-one updates in matrix factorization
  - Why needed here: The incremental rank increase is analogous to sequential rank-one updates in matrix factorization problems
  - Quick check question: In what way does the rank increase mechanism resemble incremental SVD algorithms?

## Architecture Onboarding

- Component map: The transformer architecture with diagonal weight matrices can be decomposed into attention heads where each head's weights can be expressed as elementwise products of diagonal matrices
- Critical path: 1) Initialize small weights 2) Observe plateau behavior 3) Detect rank increase events 4) Verify conservation law constraints
- Design tradeoffs: Diagonal weights simplify analysis but limit expressiveness; small initialization enables theoretical guarantees but may slow practical convergence
- Failure signatures: If rank increases by more than one per stage, the conservation law may be violated; if plateaus are too short, initialization may be too large
- First 3 experiments:
  1. Train a toy model with diagonal weights and small initialization, track rank progression over time
  2. Vary initialization scale and measure how it affects plateau duration and rank increase frequency
  3. Compare learned weight differences to random perturbations to quantify the low-rank bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do incremental learning dynamics manifest in transformers with ReLU activation functions?
- Basis in paper: [inferred] The paper explicitly notes that the theoretical analysis requires smooth architectures and cannot handle ReLU functions since they are not continuously differentiable.
- Why unresolved: The current theoretical framework relies on smoothness assumptions that preclude ReLU, though the authors note that smoothed ReLUs like GeLUs could work.
- What evidence would resolve it: Empirical studies showing whether ViT models with ReLU (or modified versions) exhibit similar incremental rank increase patterns, or theoretical extensions handling piecewise-linear activations.

### Open Question 2
- What is the precise relationship between the incremental learning dynamics we observe and LoRA's effectiveness?
- Basis in paper: [explicit] The paper explicitly discusses the connection to LoRA, noting that trained weights appear as low-rank perturbations of initial weights without explicit rank constraints.
- Why unresolved: While the paper identifies intriguing parallels, it doesn't provide a theoretical explanation for why this happens or how to quantify the connection.
- What evidence would resolve it: Mathematical proofs connecting the implicit low-rank bias in standard training to LoRA's explicit low-rank constraints, or experiments showing whether initializing with LoRA-style low-rank perturbations accelerates convergence along the incremental learning path.

### Open Question 3
- How does the rank of weight perturbations scale with model depth and width in practical transformers?
- Basis in paper: [inferred] The paper observes low-rank perturbations in vision transformers but doesn't systematically study how this scales with architecture size.
- Why unresolved: The experiments focus on specific model sizes without exploring architectural variations that would reveal scaling laws.
- What evidence would resolve it: Comprehensive experiments varying depth, width, and attention head count across multiple model families, measuring how the stable rank of weight differences changes with these parameters.

### Open Question 4
- What function-space bias emerges from the incremental learning dynamics in transformers?
- Basis in paper: [inferred] The discussion mentions this as an "interesting avenue" but doesn't explore it experimentally or theoretically.
- Why unresolved: The paper focuses on parameter-space dynamics rather than the induced function-space bias that results from these dynamics.
- What evidence would resolve it: Analysis connecting the rank progression of weight perturbations to the complexity or structure of functions learned at each stage, potentially revealing implicit regularization patterns.

## Limitations

- The theoretical framework relies on diagonal weight matrices, which significantly departs from practical transformer architectures
- Small initialization requirement means results may not apply to models trained with standard initialization schemes
- Proof technique depends on specific properties of squared loss and may not extend to other common loss functions

## Confidence

**High Confidence Claims:**
- The theoretical framework for analyzing weight evolution in diagonal weight matrices is sound and mathematically rigorous
- The experimental observations of gradual rank increase in vision transformers are reproducible
- The conservation law (u²ᵢ - v²ᵢ = constant) holds under the specified conditions

**Medium Confidence Claims:**
- The phenomenon extends beyond the simplified theoretical model to practical transformers
- The rank increase occurs in discrete stages as described
- The conservation law explains the incremental nature of learning

**Low Confidence Claims:**
- The practical implications of incremental rank increase for model performance
- The mechanism's relevance to non-diagonal weight matrices
- The extension to other transformer architectures beyond vision transformers

## Next Checks

1. **Architecture Generalization Test**: Train transformers with varying degrees of weight matrix structure (diagonal, block-diagonal, full) to determine how the rank increase phenomenon scales with architectural complexity. Measure the relationship between weight matrix structure and the rate/characteristics of rank progression.

2. **Loss Function Robustness**: Repeat the theoretical analysis and experiments using cross-entropy loss and other common objectives instead of squared loss. Verify whether the conservation law and incremental rank increase persist across different loss landscapes.

3. **Practical Impact Assessment**: Design controlled experiments to test whether models exhibiting stronger incremental rank increase patterns show improved generalization, faster convergence, or better robustness to data perturbations. Quantify the relationship between rank progression characteristics and downstream performance metrics.