---
ver: rpa2
title: Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner
arxiv_id: '2305.01711'
source_url: https://arxiv.org/abs/2305.01711
tags:
- prompt-based
- performance
- pre-training
- tasks
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits the widely held belief that continued pre-training
  on task-related texts benefits downstream fine-tuning. Through experiments on 16
  NLP tasks, it finds that conventional continued pre-training (TAPT) can be detrimental
  for sentence-pair tasks and prompt-based fine-tuning.
---

# Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner

## Quick Facts
- arXiv ID: 2305.01711
- Source URL: https://arxiv.org/abs/2305.01711
- Authors: 
- Reference count: 40
- Primary result: PCP consistently improves prompt-based fine-tuning by up to 20.1% absolute

## Executive Summary
This paper challenges the conventional wisdom that continued pre-training on task-related texts always benefits downstream fine-tuning. Through experiments on 16 NLP tasks, it demonstrates that conventional continued pre-training (TAPT) can be detrimental for sentence-pair tasks and prompt-based fine-tuning. The authors propose Prompt-based Continued Pre-tuning (PCP), which incorporates both task-related texts and prompt templates during pre-training, achieving consistent improvements of up to 20.1% absolute over state-of-the-art prompt-based fine-tuning methods.

## Method Summary
The method consists of two main steps: first, constructing a continued pre-training corpus by applying prompt templates to task-related texts; second, performing continued pre-training using the Masked Language Modeling (MLM) objective on this corpus. The approach is evaluated in both semi-supervised and fully-supervised settings, comparing performance against conventional TAPT and state-of-the-art semi-supervised approaches. The paper tests various prompt types (hard and soft) and model sizes (RoBERTa-Base and RoBERTa-Large) across 16 diverse NLP tasks.

## Key Results
- PCP consistently improves prompt-based fine-tuning performance by up to 20.1% absolute across 16 NLP tasks
- PCP outperforms state-of-the-art semi-supervised approaches with greater simplicity, requiring fewer unlabeled examples
- Even with random labels, PCP maintains performance improvements in 19 out of 32 scenarios, indicating a satisfactory performance lower bound

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conventional continued pre-training (TAPT) on task-related texts alone can be detrimental for sentence-pair tasks and prompt-based fine-tuning.
- Mechanism: Presenting only task-related texts through continued pre-training creates a mismatch between the pre-training objective (MLM) and the prompt-based fine-tuning objective, leading to suboptimal performance.
- Core assumption: The pre-training objective and fine-tuning objective must be aligned for optimal performance.
- Evidence anchors:
  - [abstract] "conventional continued pre-training does not consistently provide benefits and can even be detrimental for sentence-pair tasks or when prompt-based FT is used."
  - [section 4.2] "TAPT is not consistently beneficial for sentence pair tasks, nor when prompt-based FT is employed."
  - [corpus] Weak evidence - related papers discuss continued pre-training but don't directly address the detriment of TAPT for prompt-based fine-tuning.

### Mechanism 2
- Claim: Presenting both task-related texts and prompt templates during continued pre-training improves prompt-based fine-tuning performance.
- Mechanism: By incorporating prompt templates into the continued pre-training phase, the model learns to associate task-related texts with the appropriate prompt structure, leading to better alignment between pre-training and fine-tuning objectives.
- Core assumption: The model benefits from seeing the prompt structure during pre-training to better understand the task during fine-tuning.
- Evidence anchors:
  - [abstract] "PCP consistently improves the performance of state-of-the-art prompt-based FT approaches (up to 20.1% absolute) in both semi-supervised and fully-supervised settings."
  - [section 3] "Our approach consists of two main steps, as described below. Step 1: Construct Continued Pre-training Corpus. Initially, we select a model F, pre-trained with the MLM objective and parameterized by Θ. We then train this model using the prompt-based FT, minimizing the target loss function ℓ on the labelled examples L..."
  - [corpus] Moderate evidence - related papers discuss the importance of task-related texts in continued pre-training, but not specifically the combination with prompt templates.

### Mechanism 3
- Claim: PCP with random labels can still improve prompt-based fine-tuning performance, indicating a low performance lower bound.
- Mechanism: The model can learn useful representations from task-related texts even with incorrect or random labels, suggesting that the pre-training objective is robust to label noise.
- Core assumption: The model can learn useful representations from task-related texts even without accurate labels.
- Evidence anchors:
  - [abstract] "Our further analysis explores the performance lower bound of the PCP and reveals that the advantages of PCP persist across different sizes of models and datasets."
  - [section 4.4] "Using random labels leads to improved outcomes in 19 out of 32 scenarios. This suggests that PCP with random labels has over a 50% chance of improving the performance of prompt-basedFT, indicating that the performance lower bound is satisfactory."
  - [corpus] Weak evidence - related papers discuss label noise in self-training, but not specifically in the context of continued pre-training with prompt templates.

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: MLM is the pre-training objective used in the base language models and in the continued pre-training phase of PCP.
  - Quick check question: What is the difference between MLM and next-sentence prediction (NSP) in pre-training language models?

- Concept: Prompt-based Fine-tuning
  - Why needed here: Prompt-based fine-tuning is the method used to adapt the pre-trained language models to specific downstream tasks in this paper.
  - Quick check question: How does prompt-based fine-tuning differ from traditional fine-tuning methods like CLS-based fine-tuning?

- Concept: Semi-supervised Learning
  - Why needed here: PCP is evaluated in both semi-supervised and fully-supervised settings, leveraging unlabeled data for continued pre-training.
  - Quick check question: What are the main challenges in semi-supervised learning, and how does PCP address them?

## Architecture Onboarding

- Component map: Pre-trained Language Model -> Prompt Templates -> Label Words/Unused Tokens -> Continued Pre-training Phase -> Fine-tuning Phase

- Critical path:
  1. Pre-train language model with MLM objective on large corpus
  2. Construct continued pre-training corpus by applying prompt templates to task-related texts
  3. Perform continued pre-training with MLM objective on the constructed corpus
  4. Fine-tune the continued pre-trained model with prompt-based fine-tuning on the downstream task

- Design tradeoffs:
  - Using hard prompts vs. soft prompts: Hard prompts require manual design but can be more interpretable, while soft prompts are learned but may be less interpretable.
  - Amount of unlabeled data for continued pre-training: More data can lead to better performance but also increases computational cost.
  - Choice of prompt templates: Templates should be representative of the task but also general enough to capture diverse patterns.

- Failure signatures:
  - Performance degradation on sentence-pair tasks or prompt-based fine-tuning: This may indicate a mismatch between pre-training and fine-tuning objectives.
  - No improvement or performance decrease with increased unlabeled data: This may suggest that the continued pre-training corpus is not representative of the downstream task.
  - Inconsistent performance across different tasks: This may indicate that the prompt templates are not general enough to capture diverse task patterns.

- First 3 experiments:
  1. Compare the performance of CLS-based fine-tuning, prompt-based fine-tuning with hard prompts, and prompt-based fine-tuning with soft prompts on a single-sentence task using a pre-trained language model without continued pre-training.
  2. Perform conventional continued pre-training (TAPT) on the same task and compare the performance of the three fine-tuning methods.
  3. Perform prompt-based continued pre-training (PCP) on the same task and compare the performance of the three fine-tuning methods, focusing on the improvement over TAPT.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PCP scale with model size beyond RoBERTa-Large (354M parameters)?
- Basis in paper: [explicit] The paper shows PCP improves performance across different model sizes (RoBERTa-Base and RoBERTa-Large) but doesn't test larger models.
- Why unresolved: The paper only tests up to RoBERTa-Large, leaving open whether benefits persist or plateau with even larger models like GPT-3 or beyond.
- What evidence would resolve it: Testing PCP with larger language models (e.g., GPT-3, GPT-4, or other frontier models) and comparing performance gains to those seen with RoBERTa-Large.

### Open Question 2
- Question: What is the minimum amount of unlabeled data needed for PCP to outperform conventional TAPT?
- Basis in paper: [explicit] The paper shows PCP works well with as few as 100 unlabeled examples but doesn't establish the precise minimum threshold for consistent improvement over TAPT.
- Why unresolved: The experiments show improvement with small datasets but don't systematically test the lower bound where PCP definitively beats TAPT.
- What evidence would resolve it: Conducting experiments with systematically decreasing amounts of unlabeled data (e.g., 10, 50, 100, 200 examples) to find the exact point where PCP consistently outperforms TAPT.

### Open Question 3
- Question: How does PCP perform on non-English languages and multilingual tasks?
- Basis in paper: [inferred] All experiments use English datasets, but PCP's mechanism of combining task-related texts with prompt templates could theoretically work across languages.
- Why unresolved: The paper only evaluates on English datasets, leaving uncertainty about cross-linguistic applicability and whether translation artifacts might affect performance.
- What evidence would resolve it: Testing PCP on multilingual datasets (e.g., XNLI, mBERT-based tasks) and comparing performance to TAPT across different language families.

### Open Question 4
- Question: What is the impact of different prompt template qualities on PCP's effectiveness?
- Basis in paper: [inferred] PCP uses either hand-crafted or soft prompts, but the paper doesn't systematically vary template quality to measure its effect on PCP's gains.
- Why unresolved: The experiments use fixed templates but don't explore whether better templates amplify or diminish PCP's advantages over TAPT.
- What evidence would resolve it: Comparing PCP's performance using high-quality, medium-quality, and low-quality prompt templates while keeping all other factors constant.

## Limitations
- Generalizability to other language models and domains beyond RoBERTa-large and English datasets
- Impact of label noise in real-world semi-supervised scenarios with varying degrees of noise
- Computational cost and scalability of PCP for larger models and datasets

## Confidence
- **High confidence**: PCP consistently improves prompt-based fine-tuning performance across various tasks and model sizes
- **Medium confidence**: PCP outperforms state-of-the-art semi-supervised approaches with greater simplicity
- **Low confidence**: PCP with random labels has a satisfactory performance lower bound

## Next Checks
1. Evaluate PCP on a diverse set of language models and domains beyond RoBERTa-large and English datasets
2. Investigate the impact of label noise in real-world semi-supervised scenarios with varying degrees of noise
3. Analyze the computational cost and scalability of PCP for larger models and datasets