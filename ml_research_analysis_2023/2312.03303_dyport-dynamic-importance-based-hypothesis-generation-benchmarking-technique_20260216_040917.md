---
ver: rpa2
title: 'Dyport: Dynamic Importance-based Hypothesis Generation Benchmarking Technique'
arxiv_id: '2312.03303'
source_url: https://arxiv.org/abs/2312.03303
tags:
- which
- data
- biomedical
- importance
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Dyport, a novel benchmarking framework for
  evaluating biomedical hypothesis generation systems. Dyport utilizes curated datasets
  and incorporates them into a dynamic graph, along with a method to quantify the
  importance of discoveries.
---

# Dyport: Dynamic Importance-based Hypothesis Generation Benchmarking Technique

## Quick Facts
- arXiv ID: 2312.03303
- Source URL: https://arxiv.org/abs/2312.03303
- Reference count: 40
- Key outcome: Dyport is a novel benchmarking framework that evaluates biomedical hypothesis generation systems using curated datasets, dynamic graphs, and importance quantification to assess both accuracy and potential impact of generated hypotheses.

## Executive Summary
This paper introduces Dyport, a benchmarking framework for evaluating biomedical hypothesis generation systems. Dyport integrates curated biomedical databases into a dynamic graph and quantifies the importance of discoveries using integrated gradients and citation-based metrics. The framework extends traditional link prediction benchmarks by assessing both the accuracy and potential impact of generated hypotheses. Results show ROC AUC scores ranging from 0.50 to 0.92 depending on the system and evaluation strategy, demonstrating Dyport's effectiveness in comparing different hypothesis generation approaches.

## Method Summary
Dyport constructs a dynamic graph from curated biomedical databases (KEGG, CTD, DisGenNET, DrugCentral, RxNav, STRING, Mentha, GWAS) and MEDLINE abstracts, normalized to UMLS CUI format. The framework calculates edge importance using integrated gradients, graph-based measures (betweenness/eigenvector centrality, Jaccard similarity), and citation counts. Hypothesis generation models are trained on historical data and evaluated using stratified ROC AUC across semantic types, importance bins, and temporal windows.

## Key Results
- ROC AUC scores range from 0.50 to 0.92 across different hypothesis generation systems and evaluation strategies
- Temporal stratification reveals model performance decay over time due to increasing gap between training and test data
- Semantic stratification demonstrates domain-specific performance variations, with gene-gene and drug-disease pairs showing highest ROC AUC scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dynamic graph captures both the evolution of knowledge and the importance of connections over time.
- Mechanism: The system constructs a time-sliced network where each snapshot ðºð‘¡ represents the state of knowledge at year ð‘¡, then uses Integrated Gradients to attribute the influence of existing edges on future discoveries, and combines this with structural and citation-based metrics to rank edge importance.
- Core assumption: The importance of a biomedical association can be inferred from its impact on future discoveries and citations.
- Evidence anchors:
  - [abstract] "We integrate knowledge from the curated databases into a dynamic graph, accompanied by a method to quantify discovery importance."
  - [section] "We enrich the proposed benchmarking strategy with the information about associations importance at each time step ð‘¡."
  - [corpus] Weak: No direct mention of time-slicing or importance quantification in related papers.
- Break condition: If future discoveries are unrelated to current edges, or if citation patterns do not reflect actual scientific impact, the importance measure will be misleading.

### Mechanism 2
- Claim: Stratification by semantic type improves evaluation granularity and reveals domain-specific model strengths.
- Mechanism: The evaluation splits test pairs by their UMLS semantic type pairs (e.g., gene-gene, drug-disease) and computes ROC AUC per group, allowing comparison of model performance in specific biomedical subdomains.
- Core assumption: Different semantic domains have different structural properties and discovery patterns, so performance should vary by type.
- Evidence anchors:
  - [section] "We evaluate a selected performance measure (ROC AUC) with respect to pairs of semantic types... to better understand domain specific differences."
  - [section] Table 3 shows ROC AUC scores broken down by semantic pairs.
  - [corpus] Weak: Related works mention benchmarking but not semantic stratification.
- Break condition: If semantic type labels are inaccurate or if the negative sampling does not respect semantic constraints, the stratification will not reflect true model capabilities.

### Mechanism 3
- Claim: Temporal stratification reveals model degradation over time due to increasing gap between training and test data.
- Mechanism: Models are trained on data before a fixed cut year (e.g., 2015) and tested on data added each subsequent year, tracking ROC AUC decline.
- Core assumption: Biomedical knowledge evolves, so older training data becomes less predictive of newer discoveries.
- Evidence anchors:
  - [section] "To demonstrate this phenomena from a different perspective, we now fix the testing timestamp and vary the training timestamp."
  - [section] Figure 4 shows ROC AUC decay over time for multiple models.
  - [corpus] Weak: No related work explicitly studies temporal decay of model performance in biomedical HG.
- Break condition: If the underlying data distribution remains stable or if models adapt well to older patterns, temporal stratification may not show meaningful decay.

## Foundational Learning

- Concept: Dynamic graphs and time-slicing
  - Why needed here: The benchmark needs to simulate realistic discovery conditions by training on past knowledge and testing on future connections.
  - Quick check question: What is the difference between a static graph and a dynamic graph in the context of link prediction?

- Concept: Knowledge Graph Embeddings (KGE) and link prediction
  - Why needed here: Many hypothesis generation systems are evaluated using KGE-based models, so understanding their scoring functions and training is essential.
  - Quick check question: How does TransE score a triple (h, r, t) and what does this score represent?

- Concept: Integrated Gradients for attribution
  - Why needed here: The importance measure relies on attributing the contribution of existing edges to the prediction of future edges.
  - Quick check question: What is the key difference between Integrated Gradients and other attribution methods like saliency maps?

## Architecture Onboarding

- Component map: Data Ingestion -> Graph Construction -> Importance Scoring -> Stratified Evaluation
- Critical path: Data Ingestion -> Graph Construction -> Importance Scoring -> Stratified Evaluation
- Design tradeoffs:
  - Using literature co-occurrence for validation increases realism but reduces coverage.
  - Temporal resolution (yearly) balances granularity with data sparsity.
  - Importance measure combines multiple metrics to avoid over-reliance on a single signal.
- Failure signatures:
  - Low ROC AUC across all strata suggests poor model or unrealistic negative sampling.
  - High importance scores not correlating with future citations indicates flawed attribution.
  - Semantic stratification showing uniform scores suggests semantic type mapping errors.
- First 3 experiments:
  1. Train AGATHA on 2015 MEDLINE, test on 2016, evaluate with semantic stratification.
  2. Compare ROC AUC decay from 2016 to 2022 using fixed 2015 training data.
  3. Evaluate the same model on biocurated-only pairs (no literature cross-reference) to test data distribution shift.

## Open Questions the Paper Calls Out

- Question: How does the Dyport framework perform when applied to non-biomedical domains, and what adaptations would be necessary?
  - Basis in paper: [inferred] The paper mentions future work including "spreading to other than biomedical areas."
  - Why unresolved: The framework was only demonstrated on biomedical semantic knowledge graphs, so its performance in other domains is unknown.
  - What evidence would resolve it: Experiments applying Dyport to knowledge graphs in domains like finance, social networks, or materials science, with results showing performance metrics and necessary adaptations.

- Question: What are the limitations of using text mining-based systems for hypothesis generation, and how can they be addressed?
  - Basis in paper: [explicit] The paper discusses how text mining-based systems fall short due to limitations of information extraction algorithms and how they produce noisy data.
  - Why unresolved: While the paper identifies limitations, it doesn't provide specific solutions or alternative approaches to overcome these issues.
  - What evidence would resolve it: Comparative studies showing improved performance of hypothesis generation systems using alternative data sources or more advanced text mining techniques.

- Question: How does the importance measure proposed in Dyport compare to other methods of evaluating the impact of scientific discoveries?
  - Basis in paper: [explicit] The paper introduces a novel method to quantify the importance of discoveries, but doesn't compare it to other existing methods.
  - Why unresolved: The paper presents the importance measure as a key contribution but doesn't validate its effectiveness against other approaches.
  - What evidence would resolve it: Studies comparing the Dyport importance measure to alternative methods like citation analysis, expert evaluation, or impact factor calculations.

## Limitations
- The framework relies heavily on curated database completeness and MEDLINE coverage, creating potential blind spots for recent or emerging biomedical associations
- The importance attribution mechanism may over-emphasize well-cited connections while under-valuing novel but uncited discoveries
- The temporal stratification assumes linear knowledge evolution, which may not capture non-linear scientific breakthroughs or paradigm shifts

## Confidence
- Dynamic graph construction and temporal stratification: High - well-supported by explicit methodology descriptions and implementation details
- Importance quantification methodology: Medium - concept is clear but implementation specifics (especially for Integrated Gradients) are limited
- Semantic stratification utility: Medium - supported by results but assumes accurate semantic type mapping and representative negative sampling
- Cross-database coverage claims: Low - while multiple databases are listed, validation of their comprehensive integration is not demonstrated

## Next Checks
1. Verify temporal decay patterns by comparing ROC AUC scores across multiple time windows (2015-2022) for each model to confirm the claimed performance degradation
2. Test semantic stratification sensitivity by evaluating models on subsets with verified semantic type accuracy versus noisy type assignments
3. Validate importance measure correlation by checking if high-importance edges in training data correspond to actual citation patterns in post-test literature