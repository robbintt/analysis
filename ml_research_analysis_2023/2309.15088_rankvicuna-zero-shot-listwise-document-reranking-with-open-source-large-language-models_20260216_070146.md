---
ver: rpa2
title: 'RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language
  Models'
arxiv_id: '2309.15088'
source_url: https://arxiv.org/abs/2309.15088
tags:
- rankvicuna
- reranking
- retrieval
- bm25
- effectiveness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents RankVicuna, the first fully open-source LLM
  capable of performing high-quality listwise reranking in a zero-shot setting. RankVicuna
  is built by fine-tuning Vicuna, an open-source LLM, using ranked lists generated
  by RankGPT3.5 as a teacher model.
---

# RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models

## Quick Facts
- arXiv ID: 2309.15088
- Source URL: https://arxiv.org/abs/2309.15088
- Reference count: 16
- RankVicuna achieves nDCG@10 scores of 0.6682 and 0.6549 on DL19 and DL20, comparable to GPT3.5-based reranking

## Executive Summary
RankVicuna is the first fully open-source LLM capable of performing high-quality listwise reranking in a zero-shot setting. Built by fine-tuning Vicuna on ranked lists generated by RankGPT3.5, this 7B parameter model achieves effectiveness comparable to proprietary models while maintaining open-source accessibility. The study demonstrates that knowledge distillation from large proprietary models can effectively transfer ranking capabilities to smaller, open alternatives, providing a valuable resource for the research community.

## Method Summary
RankVicuna uses knowledge distillation from RankGPT3.5 to train Vicuna-7B on ranked lists generated from 100K MS MARCO v1 training queries. The model employs a sliding window approach to process multiple documents simultaneously, with data augmentation through input shuffling to improve robustness. Fine-tuning occurs for 2 epochs with batch size 128 and learning rate 2×10^-5 in bfloat16, evaluated on TREC DL19 and DL20 using nDCG@10 and MAP@100 metrics.

## Key Results
- RankVicuna achieves nDCG@10 of 0.6682 on DL19 and 0.6549 on DL20
- Model effectiveness matches RankGPT3.5 while being significantly smaller (7B vs 175B+ parameters)
- Data augmentation improves robustness against input order variations with up to 34% effectiveness drop without it
- Listwise reranking with sliding window (size 20, stride 10) effectively captures document interactions

## Why This Works (Mechanism)

### Mechanism 1
RankVicuna's effectiveness stems from distilling ranked lists generated by RankGPT3.5 into a smaller open-source model through fine-tuning. The teacher model provides high-quality ranked lists, which transfer the ranking capability to an open-source model. Core assumption: The ranked lists contain sufficient signal to effectively train RankVicuna. Evidence: Abstract states Vicuna is fine-tuned using ranked lists from RankGPT3.5, and section describes leveraging RankGPT3.5 as teacher model. Break condition: If teacher model rankings are systematically biased or low quality, student model inherits deficiencies.

### Mechanism 2
Data augmentation improves RankVicuna's robustness against variations in document ordering and retrieval quality. By training on both original and shuffled document orderings, RankVicuna learns to handle cases where input order differs from expected ordering. Core assumption: Shuffling documents during training exposes the model to diverse ranking scenarios without degrading overall effectiveness. Evidence: Section includes condition where input order is shuffled and results show model without DA exhibited significant effectiveness drop (up to 34%). Break condition: If data augmentation introduces too much noise relative to signal, it could degrade the model's ability to learn the primary ranking task.

### Mechanism 3
Listwise reranking with a sliding window approach captures document interactions more effectively than pointwise methods. By processing multiple documents simultaneously within a fixed context window, RankVicuna can attend to document relationships and relative relevance rather than evaluating each document in isolation. Core assumption: The model's attention mechanism can effectively process 20 documents at once within the 4096 token context limit. Evidence: Section defines ranking problem as returning reordered list of input document identifiers and observes non-deterministic outputs for GPT 3.5 and GPT 4 even with temperature zero. Break condition: If sliding window size is too small to capture relevant document relationships, or too large to fit within context constraints, effectiveness will degrade.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: RankVicuna operates without task-specific supervised training data, relying instead on teacher model's outputs and general language understanding
  - Quick check question: Can you explain the difference between zero-shot and few-shot learning in the context of LLMs?

- Concept: Listwise ranking vs. pointwise ranking
  - Why needed here: RankVicuna performs listwise reranking, requiring understanding relative relevance of multiple documents simultaneously rather than scoring each document independently
  - Quick check question: What are the key advantages of listwise ranking over pointwise ranking for document reranking tasks?

- Concept: Teacher-student distillation
  - Why needed here: RankVicuna uses knowledge distillation from RankGPT3.5, where larger teacher model's outputs are used to train smaller student model
  - Quick check question: In the context of LLM distillation, what are the trade-offs between model size and inference efficiency?

## Architecture Onboarding

- Component map: First-stage retriever -> sliding window partitioning -> RankVicuna processing -> final ranking aggregation

- Critical path: First-stage retriever → sliding window partitioning → RankVicuna processing → final ranking aggregation

- Design tradeoffs:
  - Model size vs. effectiveness: Vicuna-7B vs. larger proprietary models
  - Context window size vs. number of documents processed simultaneously
  - Data augmentation vs. training efficiency and model robustness
  - Sliding window stride vs. reranking quality and computational cost

- Failure signatures:
  - Malformed outputs (missing document IDs, repetitions, wrong format)
  - Identity ordering (returning input order unchanged)
  - Context overflow (attempting to process more documents than window allows)
  - Degradation when input order is shuffled (without data augmentation)

- First 3 experiments:
  1. Baseline comparison: Run RankVicuna on DL19/DL20 with BM25 candidates, compare to RankGPT3.5 results
  2. Data augmentation ablation: Train two versions (with/without DA) and test on shuffled inputs
  3. First-stage retriever impact: Test RankVicuna with BM25, SPLADE++, and Contriever as first-stage retrievers

## Open Questions the Paper Calls Out

### Open Question 1
How does the effectiveness of RankVicuna scale with larger model sizes beyond the 7B parameter version? Basis: The paper notes that RankVicuna with 7B parameters is effective but slightly behind reranking with GPT-4, which is rumored to contain more than 1T parameters. Why unresolved: The study only evaluates RankVicuna at 7B parameters and does not explore scaling to larger model sizes. What evidence would resolve it: Training and evaluating RankVicuna variants with 13B, 30B, and larger parameter counts on the same test collections to measure effectiveness gains.

### Open Question 2
How do different data augmentation strategies beyond simple shuffling affect the robustness and effectiveness of RankVicuna? Basis: The paper mentions data augmentation improves robustness but notes an effectiveness-robustness tradeoff without exploring alternative augmentation methods. Why unresolved: Only one data augmentation approach (shuffling) was tested, limiting understanding of optimal augmentation strategies. What evidence would resolve it: Systematic comparison of various data augmentation techniques (e.g., synthetic queries, adversarial examples) on model performance across different retrieval scenarios.

### Open Question 3
What is the optimal balance between first-stage retrieval quality and reranking depth (number of candidates reranked) for different retrieval tasks? Basis: The study shows that with effective first-stage retrievers, reranking only the top 20 candidates provides similar nDCG@10 improvements to reranking the top 100. Why unresolved: The experiments focus on specific retriever combinations without systematically exploring the tradeoff across different retrieval qualities and task types. What evidence would resolve it: Comprehensive analysis varying first-stage retrieval effectiveness and reranking depths across multiple test collections to identify optimal combinations for different effectiveness metrics.

## Limitations
- Evaluation limited to only two datasets (DL19 and DL20), limiting generalizability
- Dependence on teacher model quality without thorough analysis of bias propagation
- No exploration of scaling beyond 7B parameters or alternative data augmentation strategies

## Confidence
- High confidence: RankVicuna's effectiveness compared to RankGPT3.5 on DL19/DL20 (measured through nDCG@10)
- Medium confidence: The benefits of data augmentation for handling shuffled inputs, though the exact magnitude varies across experiments
- Medium confidence: The superiority of listwise reranking over pointwise methods, as demonstrated through ablation studies

## Next Checks
1. **Teacher model bias analysis**: Evaluate how RankGPT3.5's systematic ranking preferences affect RankVicuna's outputs across different query types and document collections.
2. **Cross-dataset generalization**: Test RankVicuna on additional IR benchmarks (e.g., TREC CAR, Robust04) to verify effectiveness beyond DL19/DL20.
3. **Efficiency-robustness tradeoff**: Conduct controlled experiments varying data augmentation intensity to quantify the relationship between training diversity and model robustness.