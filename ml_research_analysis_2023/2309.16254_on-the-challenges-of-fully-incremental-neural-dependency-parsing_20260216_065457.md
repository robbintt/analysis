---
ver: rpa2
title: On the Challenges of Fully Incremental Neural Dependency Parsing
arxiv_id: '2309.16254'
source_url: https://arxiv.org/abs/2309.16254
tags:
- incremental
- parsing
- table
- computational
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the feasibility of fully incremental dependency
  parsing using modern deep learning architectures. The authors build parsers that
  combine strictly left-to-right neural encoders with fully incremental sequence-labeling
  and transition-based decoders, and compare their performance to non-incremental
  baselines.
---

# On the Challenges of Fully Incremental Neural Dependency Parsing

## Quick Facts
- arXiv ID: 2309.16254
- Source URL: https://arxiv.org/abs/2309.16254
- Reference count: 40
- Primary result: Fully incremental parsing with modern architectures lags 11.2 UAS points behind bidirectional parsing

## Executive Summary
This paper investigates the feasibility of fully incremental dependency parsing using modern deep learning architectures. The authors build parsers that combine strictly left-to-right neural encoders with fully incremental sequence-labeling and transition-based decoders, and compare their performance to non-incremental baselines. Results show that fully incremental parsing with modern architectures lags considerably behind bidirectional parsing, with the best fully incremental model being 11.2 UAS points behind on average. The gap narrows when adding a small lookahead, but remains significant. This suggests that much of the accuracy improvements in parsing obtained in recent years hinge on bidirectionality, deviating from human processing. The paper highlights the challenges of building psycholinguistically plausible parsers and calls for further research into architectures that can handle incrementality while maintaining high accuracy.

## Method Summary
The authors combine strictly left-to-right neural encoders (4-layer LSTMs, BLOOM-560M, mGPT, XLM-RoBERTa) with fully incremental sequence-labeling and transition-based decoders. They train these models on 12 treebanks from UD 2.11 using Adam/AdamW optimizers with exponential/linear decay schedules, training for 200/30 epochs with batch sizes adapted to model size. The study compares fully incremental models (delay 0) against bidirectional baselines and incremental models with delays 1 and 2, measuring Unlabeled Attachment Score (UAS) and Labeled Attachment Score (LAS) on test sets.

## Key Results
- Fully incremental parsing with modern architectures lags 11.2 UAS points behind bidirectional parsing on average
- The performance gap narrows when adding small lookahead (delays 1-2), but remains significant
- Forward-looking encodings suffer more than non-forward-looking encodings when using incremental encoders
- Incremental parsing with pretrained encoders (BLOOM, mGPT, XLM-RoBERTa) performs worse than with BiLSTMs, contrary to expectations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bidirectional encoders outperform incremental encoders because they capture future context needed for leftward dependencies.
- Mechanism: Bidirectional models (BiLSTMs, Transformers) process both past and future tokens simultaneously, allowing accurate prediction of dependencies that point backward in the sentence. Incremental encoders only see past tokens, forcing the model to guess future context, which degrades accuracy especially for forward-looking encodings.
- Core assumption: Leftward dependencies are frequent and harder to predict without future context.
- Evidence anchors:
  - [abstract] "much of the accuracy improvements in parsing obtained in recent years hinge on bidirectionality"
  - [section] "forward-looking encodings suffer greatly from incremental encoders, due to needing to make decisions involving future words that the system cannot yet access"
  - [corpus] Weak corpus signal for directional dependency statistics, but Table 19 shows varying left/right arc ratios across languages, implying forward-looking encodings are needed.
- Break condition: If the dataset contains mostly right-branching structures or short sentences where future context is less critical.

### Mechanism 2
- Claim: Forward-looking encodings (absolute/relative indexing, PoS-based) require lookahead and degrade more with zero delay.
- Mechanism: These encodings explicitly encode the index or PoS tag of the head token, which may be to the right. Without lookahead, the model must predict these dependencies from incomplete context, causing errors. Non-forward-looking encodings (bracket-based) only refer to already-seen tokens, so they remain robust.
- Core assumption: The encoding scheme can reference future tokens, creating a lookahead dependency.
- Evidence anchors:
  - [section] "forward-looking encodings suffer greatly from incremental encoders"
  - [section] "the lower performance of delay zero models is mainly due to poor performance of forward-looking encodings on leftward dependencies"
  - [corpus] Weak direct evidence; inference based on encoding definitions.
- Break condition: If the dependency structure is strictly projective and left-branching, making lookahead unnecessary.

### Mechanism 3
- Claim: Even small lookahead (delay 1-2) significantly improves performance, especially for forward-looking encodings.
- Mechanism: Introducing a small delay allows the model to access a few future tokens, which reduces the guesswork for dependencies pointing rightward. This improves the accuracy of forward-looking encodings without fully breaking incrementality.
- Core assumption: Human processing has a small latency; a model with minimal lookahead can approximate this while still being "incremental."
- Evidence anchors:
  - [abstract] "the gap narrows when adding a small lookahead, but remains significant"
  - [section] "Improvements are consistent across the board... moving from delay 0 to 1 already shows a clear and large increase in robustness"
  - [corpus] Weak; corpus only shows F-score improvements, not the exact source.
- Break condition: If the cost of latency outweighs the gains in accuracy for the target application.

## Foundational Learning

- Concept: Incrementality vs. strong incrementality
  - Why needed here: The paper distinguishes between weak (any transition-based parser) and strong (zero-delay) incrementality. Understanding this is key to interpreting the results.
  - Quick check question: Does a parser that uses one lookahead token count as strongly incremental under the paper's definition?

- Concept: Forward-looking vs. non-forward-looking encodings
  - Why needed here: Different encodings have different lookahead requirements, affecting performance in incremental settings.
  - Quick check question: In a left-to-right encoding like absolute indexing, what happens when the head is to the right of the dependent?

- Concept: Delay parameter in incremental parsing
  - Why needed here: Delay controls how much future context is available. The paper tests 0, 1, and 2 delay values.
  - Quick check question: How does increasing delay from 0 to 1 change the information available to the parser at each step?

## Architecture Onboarding

- Component map: Encoder (LSTM, BLOOM, mGPT, XLM-RoBERTa) -> Decoder (Sequence labeling, Transition-based) -> Parse tree construction -> Evaluation
- Critical path: Encoder → Decoder → Parse tree construction → Evaluation
- Design tradeoffs:
  - Accuracy vs. incrementality: Bidirectional encoders are more accurate but non-incremental.
  - Forward-looking vs. non-forward-looking encodings: Forward-looking need lookahead; non-forward-looking are more robust to zero delay.
  - Delay parameter: Higher delay improves accuracy but reduces incrementality.
- Failure signatures:
  - High UAS drop when switching from bidirectional to left-to-right encoders.
  - Forward-looking encodings perform poorly with zero delay.
  - Models struggle with languages with many leftward dependencies.
- First 3 experiments:
  1. Train a BiLSTM encoder with arc-eager decoder (baseline) and compare to left-to-right LSTM + same decoder (zero delay).
  2. Replace the left-to-right LSTM with BLOOM-560M, keep arc-eager decoder, measure impact on forward-looking vs. bracket encodings.
  3. Introduce delay=1 in the BLOOM model, measure improvement in forward-looking encoding accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can incremental dependency parsers be designed to match the performance of bidirectional parsers while maintaining psycholinguistic plausibility?
- Basis in paper: [explicit] The paper highlights the challenge of building psycholinguistically plausible parsers and notes that fully incremental parsing with modern architectures lags considerably behind bidirectional parsing.
- Why unresolved: Current state-of-the-art parsers rely on bidirectional encoders, which deviate from human processing. The paper shows that fully incremental models are significantly less accurate, even with small lookahead.
- What evidence would resolve it: Development of new architectures or techniques that can achieve high accuracy in incremental parsing without relying on bidirectional encoders, validated through cross-linguistic experiments.

### Open Question 2
- Question: What is the optimal delay parameter for incremental parsers to balance accuracy and psycholinguistic plausibility?
- Basis in paper: [explicit] The paper explores models with delay 0, 1, and 2, showing improvements with small lookahead but leaving the question of optimal delay open.
- Why unresolved: The optimal delay may vary across languages, tasks, and cognitive constraints. The paper uses delay 0 for strict incrementality but acknowledges that some lookahead can significantly improve performance.
- What evidence would resolve it: Systematic studies across diverse languages and tasks to determine the relationship between delay, accuracy, and cognitive plausibility.

### Open Question 3
- Question: How do forward-looking encodings impact the performance of incremental parsers, and can this limitation be mitigated?
- Basis in paper: [explicit] The paper notes that forward-looking encodings suffer more from incremental encoders due to needing to make decisions involving future words that the system cannot yet access.
- Why unresolved: Forward-looking encodings are useful for certain parsing tasks but seem to exacerbate the challenges of incrementality. The paper shows they perform worse than non-forward-looking encodings in incremental settings.
- What evidence would resolve it: Development of encoding schemes or parsing strategies that can handle forward-looking dependencies in incremental settings, or empirical studies comparing different encoding types across languages.

## Limitations

- Evaluation focuses exclusively on 12 treebanks from UD 2.11, limiting cross-linguistic coverage
- Study assumes strict left-to-right processing order, which may not reflect all human parsing strategies
- Delay parameter (0, 1, 2) represents only coarse-grained latency levels, optimal delay remains unexplored

## Confidence

**High confidence** in the overall performance gap between incremental and bidirectional models (11.2 UAS points average), supported by consistent results across multiple encoder architectures and language pairs.

**Medium confidence** in the specific mechanisms (forward-looking vs non-forward-looking encodings) due to reliance on inference from encoding definitions rather than direct experimental isolation of these effects.

**Low confidence** in psycholinguistic implications, as the paper notes its models deviate from human processing but doesn't conduct direct comparisons with human behavioral data.

## Next Checks

1. **Cross-linguistic generalization**: Test the incremental models on additional language families and non-UD treebanks to verify if the performance gap persists across typologically diverse languages.

2. **Fine-grained delay analysis**: Systematically evaluate delays at intervals of 0.5 or 1 token between 0 and 5 to identify the optimal tradeoff point between accuracy and incrementality.

3. **Alternative processing orders**: Implement and evaluate right-to-left or inside-out processing strategies to determine if the left-to-right assumption artificially constrains incremental parsing performance.