---
ver: rpa2
title: 'pSTarC: Pseudo Source Guided Target Clustering for Fully Test-Time Adaptation'
arxiv_id: '2309.00846'
source_url: https://arxiv.org/abs/2309.00846
tags:
- source
- test
- pstarc
- adaptation
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces pSTarC, a novel framework for Test Time Adaptation
  (TTA) that addresses the challenge of adapting models to unseen test data distributions
  without access to source data. The key idea is to generate pseudo-source samples
  using the source classifier, which are then used to guide the clustering and alignment
  of target test samples.
---

# pSTarC: Pseudo Source Guided Target Clustering for Fully Test-Time Adaptation

## Quick Facts
- arXiv ID: 2309.00846
- Source URL: https://arxiv.org/abs/2309.00846
- Reference count: 30
- Key outcome: Achieves state-of-the-art performance on VisDA, Office-Home, and DomainNet-126 with significantly less memory than competing methods

## Executive Summary
pSTarC addresses Test Time Adaptation (TTA) by generating pseudo-source samples from a frozen source classifier and using them to guide target clustering. The method synthesizes pseudo-source features that serve as stable anchors for aligning target test samples, enabling the source classifier to maintain performance on shifted test distributions. By leveraging entropy-based confidence thresholding and diversity maximization during pseudo-source generation, pSTarC achieves strong results while requiring only memory for pseudo-source features rather than storing source data.

## Method Summary
pSTarC trains a source model on labeled data, then freezes the classifier while adapting only the feature extractor on unlabeled test data. It generates a fixed set of pseudo-source features by optimizing a feature bank to minimize entropy and maximize diversity under the frozen classifier. During test-time adaptation, low-entropy test samples are aligned to nearest pseudo-source features of the same class using soft pseudo-labeling, while high-entropy samples are stabilized through consistency regularization with strong augmentations. The method uses a combination of augmentation, attraction, and dispersion losses to update the feature extractor.

## Key Results
- Achieves state-of-the-art performance on VisDA, Office-Home, and DomainNet-126 datasets
- Requires significantly less memory than competing methods by storing only pseudo-source features
- Demonstrates effectiveness in Continual Test-Time Adaptation setting with evolving test domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pseudo-source features generated from the frozen source classifier serve as stable, noise-free anchors for clustering target test samples.
- Mechanism: The fixed classifier defines decision boundaries in feature space learned from abundant labeled source data. By generating pseudo-source features within these boundaries, the method creates reliable "prototypes" for each class. These prototypes then guide target clustering through similarity alignment, replacing noisy pseudo-labels that would otherwise be computed directly from test data.
- Core assumption: The source classifier's decision boundaries remain valid for clustering target data, even under domain shift, as long as feature alignment brings target samples into the correct regions.
- Evidence anchors: [abstract] "utilizing the classifier to generate a diverse array of pseudo-source samples, thereby steering the target clustering process"

### Mechanism 2
- Claim: Entropy-based confidence thresholding selects reliable low-entropy test samples for alignment, while high-entropy samples are anchored to their own predictions to avoid propagating noise.
- Mechanism: For each test batch, compute sample-wise entropy; define a threshold as the mean entropy of the batch. Low-entropy samples (confident predictions) are aligned to the nearest pseudo-source features of the same predicted class. High-entropy samples (uncertain predictions) are not forced to align to pseudo-sources; instead, they are anchored to their own predictions and used in a consistency loss with strong augmentations.
- Core assumption: Mean batch entropy is a meaningful proxy for per-sample reliability; confident predictions on shifted data are still useful anchors.
- Evidence anchors: [section] "We identify the low entropy test samples based on a threshold Ï„t, which we define as the mean entropy of the batch"

### Mechanism 3
- Claim: Diversity maximization during pseudo-source generation prevents feature collapse and ensures balanced class representation in the feature bank.
- Mechanism: While optimizing pseudo-source features, add a diversity loss that encourages the mean softmax prediction over the feature bank to be close to a uniform distribution (KL divergence to uniform). This ensures that the generated feature bank contains a sufficient number of samples from each class, avoiding degenerate solutions where all features collapse to one class.
- Core assumption: Balanced class representation in the pseudo-source bank is necessary for effective clustering; uniform softmax distribution over the bank correlates with balanced feature coverage.
- Evidence anchors: [section] "Along with this, we use diversity maximization loss to avoid the trivial solution where all feature vectors collapse to the same class"

## Foundational Learning

- Concept: **Entropy minimization in clustering**
  - Why needed here: Entropy minimization is used both to generate confident pseudo-source features and to select reliable target samples for alignment; it provides a measure of prediction certainty that drives the clustering process.
  - Quick check question: If a batch has mean entropy 1.2 and a sample has entropy 0.8, is this sample considered low-entropy for thresholding?

- Concept: **Mutual information and diversity in unsupervised learning**
  - Why needed here: Diversity maximization prevents mode collapse during pseudo-source generation; it ensures the feature bank spans the class space rather than collapsing to a few modes.
  - Quick check question: What happens to the diversity loss if all pseudo-source features are classified into the same class?

- Concept: **Consistency regularization with augmentations**
  - Why needed here: Consistency between a sample and its strong augmentation stabilizes adaptation, especially for high-entropy samples that are not aligned to pseudo-sources; it improves robustness to image transformations.
  - Quick check question: If the augmentation pipeline is weakened, how might the consistency loss behave for uncertain samples?

## Architecture Onboarding

- Component map:
  Source model (fixed) -> feature extractor Gs + classifier Hs
  Learnable feature bank f (256 x N) for pseudo-source generation
  Target model Ft = Gt (initialized from Gs) + Hs (frozen)
  Batch processing pipeline: augment -> extract features -> compute losses -> update Gt

- Critical path:
  1. Forward pass test batch through Gt
  2. Compute entropy and pseudo-labels
  3. Select low/high entropy samples
  4. Retrieve nearest pseudo-sources for low-entropy samples
  5. Compute LpSTarC loss (augmentation, attraction, dispersion)
  6. Backward pass and update Gt

- Design tradeoffs:
  - Fixed classifier vs. fine-tuning: preserves source knowledge but may limit adaptation flexibility
  - One-time pseudo-source generation vs. online updates: memory efficient but may not adapt to gradual domain evolution
  - Entropy threshold at batch mean: simple but may not adapt to batch-to-batch entropy variance

- Failure signatures:
  - Accuracy plateaus despite adaptation steps -> feature alignment not effective
  - Sudden accuracy drop -> pseudo-labels becoming noisy or threshold miscalibrated
  - Slow convergence -> learning rate too low or dispersion loss too strong

- First 3 experiments:
  1. Generate pseudo-source bank with 20 samples/class; visualize t-SNE to confirm class separation
  2. Run pSTarC on a single VisDA domain pair with batch size 128; monitor entropy distribution per batch
  3. Ablation: remove diversity loss; check if feature collapse occurs in the bank

## Open Questions the Paper Calls Out

- How does the performance of pSTarC compare to other methods when the number of pseudo-source features per class (nc) is varied?
- Can pSTarC be extended to handle more complex domain shifts, such as those involving changes in object scale or viewpoint?
- How does pSTarC perform in comparison to other methods when the test data arrives in a non-i.i.d. manner?

## Limitations

- Performance relies on assumption that source classifier decision boundaries remain valid anchors for target clustering under domain shift
- One-time pseudo-source generation cannot adapt to gradual or evolving domain shifts over time
- Entropy-based thresholding uses batch mean as proxy for per-sample reliability without empirical validation across diverse domain shifts

## Confidence

- High confidence: The core mechanism of using a frozen classifier to generate pseudo-source features and align target samples to them is well-specified and theoretically sound.
- Medium confidence: The effectiveness of batch-mean entropy thresholding as a reliable proxy for per-sample reliability across diverse domain shifts.
- Low confidence: The impact of the diversity maximization loss on actual clustering performance and its optimal weighting relative to other losses.

## Next Checks

1. Systematically vary the entropy threshold (not just at batch mean) and measure adaptation performance across different domain shift severities to validate whether batch mean is an optimal or robust choice.

2. Remove the diversity maximization loss and generate pseudo-source features; visualize the resulting feature bank using t-SNE to empirically confirm whether feature collapse occurs and quantify its impact on adaptation accuracy.

3. Implement a setting where test data arrives in sequential batches with gradually evolving domain shift, and evaluate whether the one-time pseudo-source generation becomes a bottleneck compared to methods that update pseudo-sources online.