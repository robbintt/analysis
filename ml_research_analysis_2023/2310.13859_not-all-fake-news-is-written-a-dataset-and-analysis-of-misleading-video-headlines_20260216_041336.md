---
ver: rpa2
title: 'Not all Fake News is Written: A Dataset and Analysis of Misleading Video Headlines'
arxiv_id: '2310.13859'
source_url: https://arxiv.org/abs/2310.13859
tags:
- video
- headline
- misleading
- headlines
- news
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VMH, a dataset for detecting misleading video
  headlines on social media. The dataset consists of 2,247 videos with human annotations
  indicating whether headlines are representative or misleading.
---

# Not all Fake News is Written: A Dataset and Analysis of Misleading Video Headlines

## Quick Facts
- arXiv ID: 2310.13859
- Source URL: https://arxiv.org/abs/2310.13859
- Reference count: 30
- Key outcome: VMH dataset of 2,247 videos shows multimodal detection of misleading video headlines outperforms text-only approaches, but F1-scores remain relatively low (~0.53-0.56), highlighting the inherent difficulty of the task.

## Executive Summary
This paper introduces VMH, a dataset for detecting misleading video headlines on social media. The dataset consists of 2,247 videos with human annotations indicating whether headlines are representative or misleading. Annotators also provide rationales for misleading labels, capturing why headlines distort the underlying video content. The authors benchmark multimodal models and find that using both video and transcript features, along with rationales, improves detection performance compared to text-only approaches. However, F1-scores remain relatively low, highlighting the inherent difficulty of this task. The dataset and analyses provide a foundation for future work on multimodal detection of misleading video headlines.

## Method Summary
The authors created the VMH dataset containing 2,247 YouTube news videos with human annotations labeling headlines as representative or misleading. Each video includes headline text, transcript, and annotator-provided rationales explaining why a headline is misleading. They benchmarked multimodal models (VideoCLIP, VLM) by concatenating headline, transcript, rationale, and subrationale features, then finetuning a classification layer using the Adam optimizer (learning rate 0.00002, weight decay 0.001, batch size 8) for 10 epochs. Performance was evaluated using F1-score, precision, recall, AUPRC, and accuracy.

## Key Results
- Multimodal models (headline+video+transcript+rationale) achieve F1-scores of 0.53-0.56, outperforming text-only approaches (F1 ~0.16)
- Adding rationales that explain headline-video mismatches improves detection performance across all metrics
- Model performance drops significantly (F1 to 0.10-0.12) on high-disagreement samples, indicating sensitivity to annotation subjectivity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal features (video, transcript, rationale) jointly improve misleading headline detection accuracy.
- Mechanism: The headline alone is ambiguous; adding video content resolves whether the headline is truly representative. Rationales further explain the mismatch, guiding model focus.
- Core assumption: The video and transcript contain sufficient information to assess headline veracity.
- Evidence anchors:
  - [abstract] "using both video and transcript features, along with rationales, improves detection performance compared to text-only approaches"
  - [section] Table 4 shows F1-scores increase from ~0.16 (headline only) to ~0.53–0.56 (multimodal with rationales)
  - [corpus] No corpus evidence available
- Break condition: If video quality is poor or transcript is inaccurate, the multimodal advantage diminishes.

### Mechanism 2
- Claim: Rationales, especially subrationales, encode critical clues about headline misrepresentation.
- Mechanism: Rationales force annotators to articulate *why* a headline is misleading, which becomes a signal for models to detect specific mismatch patterns.
- Core assumption: Human explanations capture patterns not evident from headline-text pairs alone.
- Evidence anchors:
  - [abstract] "Our annotation process also focuses on why annotators view a video as misleading, allowing us to better understand the interplay of annotators' background and the content of the videos."
  - [section] "Adding rationales that provide information about the headline and video relationship improves metrics across the board."
  - [corpus] No corpus evidence available
- Break condition: If rationales are too generic or noisy, model benefit may plateau.

### Mechanism 3
- Claim: Subjectivity in labeling explains lower consensus and performance drop on disputed samples.
- Mechanism: High subjectivity means annotators disagree on what constitutes misleadingness; models struggle when training data lacks clear consensus.
- Core assumption: Disagreement reflects inherent ambiguity in the task rather than poor annotation quality.
- Evidence anchors:
  - [abstract] "F1-scores remain relatively low, highlighting the inherent difficulty of this task."
  - [section] "Model Subjectivity Analysis...only gets an F1-score of 0.12 and 0.10 with the VideoCLIP and VLM models respectively compared to the F1-scores (i.e., 0.53, 0.56) using the entire training set."
  - [corpus] No corpus evidence available
- Break condition: If consensus can be artificially increased (e.g., stricter guidelines), model performance may improve.

## Foundational Learning

- Concept: Headline classification properties (factual/opinionated statement vs. question).
  - Why needed here: The task requires understanding whether a headline is making a claim, asking a question, or expressing an opinion, which determines how the video should be evaluated.
  - Quick check question: What distinguishes a factual statement headline from an opinionated one in terms of evaluation criteria?
- Concept: Multimodal entailment vs. representation detection.
  - Why needed here: Misleading headline detection is not purely about entailment; it also considers whether the headline covers all or only part of the video content.
  - Quick check question: How does entailment differ from checking whether a headline over- or under-represents video content?
- Concept: Quality control via MACE and accuracy scoring.
  - Why needed here: Reliable annotation is critical for model training; MACE estimates annotator trustworthiness and infers true labels from noisy labels.
  - Quick check question: What does MACE output that helps decide whether to keep an annotator's contribution?

## Architecture Onboarding

- Component map: Headline -> Text Encoder -> Headline Features; Transcript -> Text Encoder -> Transcript Features; Video -> Video Encoder -> Video Features; Rationale -> Text Encoder -> Rationale Features; Subrationale -> Text Encoder -> Subrationale Features; All Features -> Concatenation -> Classifier -> Binary Label
- Critical path: Headline + Transcript + Video + Rationale + Subrationale → Multimodal Encoder → Classifier
- Design tradeoffs:
  - Full multimodal vs. partial (text-only) to isolate shortcut artifacts
  - Use of gold rationales vs. predicted rationales (current limitation)
  - Balancing dataset class imbalance (341 misleading / 1906 representative)
- Failure signatures:
  - Low F1 on partial input experiments → dataset may contain multimodal shortcuts
  - Poor performance on high-disagreement subset → model sensitive to subjective ambiguity
  - Overfitting to specific venue features → not generalizable
- First 3 experiments:
  1. Train baseline with {Headline} only; measure F1 to confirm text-only difficulty.
  2. Add {Video} to baseline; check if video alone adds signal.
  3. Add {Rationale + Subrationale} to multimodal input; measure F1 gain from rationales.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the model's performance metrics compare when using gold rationales versus predicted rationales?
- Basis in paper: [inferred] The paper mentions that gold rationales might not be available during inference, but suggests including rationale-like features to improve model performance.
- Why unresolved: The paper only evaluates model performance with gold rationales, not predicted ones, leaving uncertainty about real-world applicability.
- What evidence would resolve it: Conducting experiments using automatically predicted rationales instead of gold rationales and comparing performance metrics.

### Open Question 2
- Question: To what extent does annotator background (e.g., political leaning) influence the detection of misleading video headlines?
- Basis in paper: [explicit] The paper discusses annotator background and its potential influence on labeling, noting no significant correlation between political leaning and annotation.
- Why unresolved: The analysis is based on a limited sample and may not capture all nuances of annotator bias.
- What evidence would resolve it: Conducting a larger-scale study with more diverse annotators and analyzing the impact of different background factors on annotation consistency.

### Open Question 3
- Question: How does the inclusion of visual grounding improve the detection of misleading video headlines?
- Basis in paper: [explicit] The paper acknowledges the potential benefits of visual grounding but does not explore it in depth.
- Why unresolved: The paper focuses on text-based and multimodal features without integrating visual analysis, limiting understanding of visual cues' impact.
- What evidence would resolve it: Developing and testing models that incorporate visual features and comparing their performance to text-only and multimodal approaches.

## Limitations

- Dataset imbalance and generalizability - Strong class imbalance (341 misleading vs 1906 representative) may bias models toward majority class and limit detection of subtle misleading patterns.
- Rationale dependency - Model performance relies on gold-standard rationales only available during training, creating a gap between reported performance and real-world applicability.
- Subjectivity and consensus challenges - Inherent subjectivity in determining misleadingness leads to low inter-annotator agreement, particularly for disputed samples, directly impacting model performance.

## Confidence

- High confidence - The core finding that multimodal approaches outperform text-only methods is well-supported by experimental results showing consistent F1-score improvements (0.16 to 0.53-0.56).
- Medium confidence - The claim that rationales improve performance is supported by results but limited by the gold-standard assumption.
- Low confidence - Claims about model generalizability and performance on out-of-domain data are not empirically tested.

## Next Checks

1. **Rationale prediction robustness** - Evaluate model performance when using automatically generated rationales (via a pretrained rationale generation model) instead of gold rationales to assess real-world applicability and identify performance gaps.

2. **Cross-platform generalization** - Test the best-performing models on video headlines from different social media platforms (TikTok, Twitter, Facebook) to quantify how well the YouTube-trained models generalize to other content ecosystems.

3. **Adversarial robustness testing** - Create or identify edge cases where headlines are technically accurate but contextually misleading (e.g., technically correct but missing crucial context, or using loaded language) to stress-test model robustness beyond the current dataset's scope.