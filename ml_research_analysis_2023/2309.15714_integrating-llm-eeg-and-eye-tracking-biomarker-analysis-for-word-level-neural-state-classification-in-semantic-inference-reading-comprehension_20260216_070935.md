---
ver: rpa2
title: Integrating LLM, EEG, and Eye-Tracking Biomarker Analysis for Word-Level Neural
  State Classification in Semantic Inference Reading Comprehension
arxiv_id: '2309.15714'
source_url: https://arxiv.org/abs/2309.15714
tags:
- words
- data
- classification
- subjects
- reading
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study integrates large language models (LLMs), eye-tracking,
  and electroencephalographic (EEG) biomarkers to classify neural states during semantic
  inference reading tasks at the word level. The proposed approach uses GPT-3.5 and
  GPT-4 to identify high- and low-relevance words to a keyword, extracting corresponding
  fixation-locked EEG data.
---

# Integrating LLM, EEG, and Eye-Tracking Biomarker Analysis for Word-Level Neural State Classification in Semantic Inference Reading Comprehension

## Quick Facts
- arXiv ID: 2309.15714
- Source URL: https://arxiv.org/abs/2309.15714
- Authors: 
- Reference count: 40
- One-line primary result: Classification accuracy >60% for distinguishing high- and low-relevance words during semantic inference reading comprehension using LLM-generated labels and EEG/eye-tracking biomarkers.

## Executive Summary
This study presents a novel approach to classify neural states during semantic inference reading tasks at the word level by integrating large language models (LLMs), eye-tracking, and electroencephalographic (EEG) biomarkers. The method uses GPT-3.5 and GPT-4 to identify high- and low-relevance words to a keyword, then extracts corresponding fixation-locked EEG data for classification. Eye-tracking analysis revealed that high-relevance words received significantly more fixations (1.0584 per word) compared to low-relevance words (0.6576 per word), validating the semantic importance of the LLM-generated labels. Feature engineering methods including band power, conditional entropy, and connectivity networks were applied to the EEG data, achieving validation accuracies over 60% across 12 subjects using multiple machine learning classifiers.

## Method Summary
The study integrates Zurich Cognitive Language Processing Corpus (ZuCo) data with GPT-4 semantic analysis to classify neural states during semantic inference reading. The method involves three main steps: first, GPT-3.5 and GPT-4 are used to identify high-relevance (HRW) and low-relevance (LRW) words for each keyword in reading comprehension sentences; second, eye-tracking data is used to segment EEG recordings into fixation-locked epochs corresponding to each word; third, feature engineering techniques including band power, conditional entropy, and connectivity networks are applied to the EEG data, which is then classified using machine learning models with 5-fold cross-validation. The approach represents the first attempt to use LLM knowledge for word-level brain state classification in reading comprehension tasks.

## Key Results
- Classification accuracy exceeded 60% for distinguishing high- and low-relevance words across 12 subjects
- High-relevance words received significantly more fixations per word (1.0584) compared to low-relevance words (0.6576)
- Significant differences in band power were observed in delta and gamma frequency bands between HRW and LRW conditions
- The approach successfully integrated GPT-4 semantic understanding with EEG and eye-tracking data for neural state classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's semantic understanding of word relevance can be used to generate ground-truth labels for EEG-based brain state classification.
- Mechanism: GPT-4 evaluates sentences and categorizes words into high-relevance (HRW) and low-relevance (LRW) groups based on their relation to a keyword. These labels are then used to extract and classify corresponding EEG epochs.
- Core assumption: GPT-4's output accurately reflects the semantic importance of words in human comprehension tasks.
- Evidence anchors:
  - [abstract] "This study uses GPT-3.5 and GPT-4 to identify high- and low-relevance words to a keyword, extracting corresponding fixation-locked EEG data."
  - [section III.A.1] "Table I compares the performance of different language models on ZuCo Task 3 with that of 12 subjects. Given large language models' generative and non-deterministic nature, each experimental run produced slightly varying outputs."
  - [corpus] "From Word Embedding to Reading Embedding Using Large Language Model, EEG and Eye-tracking" shows similar integration of LLMs with EEG data.
- Break condition: If GPT-4's semantic judgments diverge significantly from human comprehension patterns, the ground-truth labels would become unreliable.

### Mechanism 2
- Claim: Fixation duration and frequency serve as biomarkers for semantic relevance, allowing differentiation between HRW and LRW.
- Mechanism: Eye-tracking data reveals that readers fixate longer and more frequently on HRWs than LRWs. These eye-fixation patterns are used to segment EEG data for classification.
- Core assumption: Increased fixations on words correlate with higher cognitive processing and semantic relevance.
- Evidence anchors:
  - [abstract] "Eye-tracking analysis revealed that high-relevance words received significantly more fixations per word (1.0584) compared to low-relevance words (0.6576)."
  - [section IV.B] "Table III shows that HRWs received an average of 1.0584 fixations per word, while LRWs received 0.6576 fixations per word."
  - [corpus] "Detecting Reading-Induced Confusion Using EEG and Eye Tracking" supports eye-tracking as a reliable cognitive state indicator.
- Break condition: If eye-fixation patterns vary widely across individuals or contexts, the biomarker's reliability diminishes.

### Mechanism 3
- Claim: EEG feature engineering methods (band power, conditional entropy, connectivity networks) enhance the classification of semantic states.
- Mechanism: These methods extract discriminative features from fixation-locked EEG data, which are then used by machine learning classifiers to distinguish between HRW and LRW conditions.
- Core assumption: EEG signal patterns differ systematically between conditions of high and low semantic relevance.
- Evidence anchors:
  - [abstract] "Feature engineering methods including band power, conditional entropy, and connectivity networks were applied to the EEG data."
  - [section III.C.3] "We used Principal Component Analysis (PCA) to reduce the dimensionality of the data to 30 variables."
  - [corpus] "Graph Representations for Reading Comprehension Analysis using Large Language Model and Eye-Tracking Biomarker" uses similar feature extraction techniques.
- Break condition: If EEG signals are too noisy or non-discriminatory, feature extraction will not improve classification.

## Foundational Learning

- Concept: Large Language Models (LLMs) like GPT-4
  - Why needed here: LLMs provide semantic understanding to label words by relevance, enabling the mapping of brain states to semantic content.
  - Quick check question: What is the role of GPT-4 in generating ground-truth labels for the EEG classification task?

- Concept: Eye-tracking as a cognitive biomarker
  - Why needed here: Fixation patterns indicate where and how long readers focus on words, which correlates with semantic processing.
  - Quick check question: How does the number of fixations per word differ between high- and low-relevance words?

- Concept: EEG feature extraction techniques
  - Why needed here: These techniques transform raw EEG signals into meaningful features for machine learning classification.
  - Quick check question: Which three feature extraction methods were used in this study to improve EEG classification?

## Architecture Onboarding

- Component map: GPT-4 → Word relevance labeling → Eye-tracking segmentation → EEG feature extraction (BP, CondEn, PLV) → Machine learning classifiers → Classification output
- Critical path: Accurate word labeling by GPT-4 → Correct fixation data extraction → Reliable EEG feature extraction → Effective classifier training
- Design tradeoffs: Using GPT-4 instead of simpler NLP models improves semantic understanding but increases computational cost. Reducing EEG channels for efficiency may lose information.
- Failure signatures: Poor classification accuracy may indicate GPT-4 label errors, noisy EEG data, or ineffective feature extraction.
- First 3 experiments:
  1. Validate GPT-4's word relevance labels against human annotations.
  2. Test eye-tracking fixation detection accuracy on sample data.
  3. Compare classification performance using different EEG feature sets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do contextual complexities influence semantic classifications, particularly when words acquire distinct meanings based on their juxtaposition with other terms?
- Basis in paper: [explicit] The paper mentions that words like "gold" acquire distinct semantic relevance when juxtaposed with terms like "medal," and that sentences involving specific target terms demonstrate an imbalance in the distribution of HRW and LRW due to their deterministic nature.
- Why unresolved: The paper acknowledges the influence of contextual complexities but does not provide a quantitative assessment or method to account for these variations in semantic classifications.
- What evidence would resolve it: A quantitative assessment method or model that can accurately predict the semantic relevance of words based on their contextual usage, validated across diverse sentence structures and word combinations.

### Open Question 2
- Question: What are the specific neural mechanisms underlying the observed differences in band power (BP) between high-relevance words (HRW) and low-relevance words (LRW) in the delta and gamma frequency bands?
- Basis in paper: [explicit] The paper reports significant differences in BP within the delta and gamma bands between HRW and LRW conditions, suggesting a link to neural mechanisms involved in semantic integration and comprehension.
- Why unresolved: The paper identifies the differences but does not explore the underlying neural mechanisms that cause these variations in BP.
- What evidence would resolve it: Neuroimaging studies (e.g., fMRI) or targeted EEG experiments that correlate specific neural activities with the observed BP differences in the delta and gamma bands during semantic processing tasks.

### Open Question 3
- Question: How can the 'black box' nature of large language models (LLMs) be addressed to improve the interpretability and generalizability of semantic classifications in brain-computer interfaces (BCIs)?
- Basis in paper: [explicit] The paper discusses the challenges posed by the 'black box' nature of LLMs, particularly in the context of non-deterministic relations, and highlights the need for a quantitative assessment to ensure the accuracy and validity of keyword identification.
- Why unresolved: The paper acknowledges the limitation but does not propose a solution to enhance the interpretability of LLMs in the context of semantic classifications for BCIs.
- What evidence would resolve it: Development and validation of explainable AI techniques or hybrid models that combine LLMs with more interpretable methods, demonstrating improved accuracy and generalizability in semantic classifications across diverse datasets.

## Limitations

- The study relies on GPT-4's non-deterministic semantic judgments as ground-truth labels, with potential variations across runs that could affect classification reliability
- The small sample size of 12 subjects limits the generalizability of findings to broader populations with different reading comprehension strategies
- Exact implementation details for EEG feature extraction parameters and GPT-4 prompting are not fully specified, making exact replication challenging

## Confidence

- **High Confidence**: The eye-tracking findings showing significantly more fixations on high-relevance words (1.0584 vs 0.6576) are directly supported by empirical data and align with established cognitive processing theories
- **Medium Confidence**: The classification accuracy exceeding 60% is reported but depends on the quality of GPT-4 labels and the effectiveness of feature extraction methods, both of which have implementation uncertainties
- **Low Confidence**: The generalizability of the approach to other reading tasks or larger populations is questionable given the limited subject pool and lack of cross-validation across different datasets

## Next Checks

1. Conduct inter-rater reliability tests by comparing GPT-4's word relevance labels against human annotators across multiple runs to quantify label consistency and identify systematic biases
2. Perform ablation studies by removing each feature extraction method (band power, conditional entropy, connectivity networks) to determine their individual contributions to classification performance
3. Test the classification model on an independent reading comprehension dataset with different subjects and semantic relations to evaluate external validity and robustness