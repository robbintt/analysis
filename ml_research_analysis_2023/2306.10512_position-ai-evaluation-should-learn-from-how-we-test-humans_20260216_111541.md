---
ver: rpa2
title: 'Position: AI Evaluation Should Learn from How We Test Humans'
arxiv_id: '2306.10512'
source_url: https://arxiv.org/abs/2306.10512
tags:
- chatgpt
- ability
- question
- adaptive
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes adopting psychometrics-based adaptive testing
  for AI evaluation, shifting from static benchmarks to dynamic, ability-estimated
  assessments. The approach uses Item Response Theory (IRT) and computerized adaptive
  testing (CAT) to select questions based on a model's current ability estimate, improving
  evaluation efficiency and precision.
---

# Position: AI Evaluation Should Learn from How We Test Humans

## Quick Facts
- **arXiv ID**: 2306.10512
- **Source URL**: https://arxiv.org/abs/2306.10512
- **Reference count**: 40
- **Key outcome**: Adaptive testing with IRT achieves comparable accuracy to traditional benchmarks using only ~20 questions

## Executive Summary
This paper proposes adopting psychometrics-based adaptive testing for AI evaluation, shifting from static benchmarks to dynamic, ability-estimated assessments. The approach uses Item Response Theory (IRT) and computerized adaptive testing (CAT) to select questions based on a model's current ability estimate, improving evaluation efficiency and precision. Experiments on 6 instruction-tuned LLMs (including GPT-4) across Subject Knowledge, Mathematical Reasoning, and Programming show that GPT-4 outperforms others significantly, while adaptive testing requires only ~20 questions to achieve comparable accuracy to traditional methods.

## Method Summary
The framework consists of two stages: first, constructing question pools by calibrating item parameters (difficulty β, discrimination α, guessing c) using IRT-3PL on response data from human students; second, adaptive testing using IRT-based ability estimation and Fisher Information-based question selection, iterating until a stopping criterion is met. The method was evaluated on three datasets (MOOC, MATH, CODIA) covering Computer Science, Mathematics, and Programming domains with 6 instruction-tuned LLMs including GPT-4, ChatGPT, Bard, ERNIEBot, QianWen, and iFlytek Spark.

## Key Results
- GPT-4 significantly outperforms other LLMs across all tested domains
- Adaptive testing achieves comparable accuracy to traditional methods with only ~20 questions
- Diagnostic reports reveal ChatGPT exhibits high slip rates and inconsistency ("careless student" behavior)
- Human-LLM comparability is enabled through common IRT-based ability estimation scale

## Why This Works (Mechanism)

### Mechanism 1
- Claim: IRT-based adaptive testing reduces the number of questions needed for equivalent ability estimation accuracy compared to static benchmarks.
- Mechanism: IRT models the probability of a correct response as a function of latent ability (θ) and item parameters (difficulty β, discrimination α, guessing c). By selecting items with difficulty close to the current ability estimate and high discrimination, Fisher Information is maximized, reducing estimation variance.
- Core assumption: Item parameters (β, α, c) are well-calibrated from human response data and remain stable across human and LLM populations.

### Mechanism 2
- Claim: Diagnostic reports from adaptive testing reveal model-specific weaknesses that static benchmarks obscure.
- Mechanism: Fine-grained ability estimates across subdomains (e.g., programming language concepts vs. coding tasks) expose inconsistent skill profiles, enabling targeted model improvement.
- Core assumption: Subdomain concepts are sufficiently independent to yield interpretable separate ability estimates.

### Mechanism 3
- Claim: Human-LLM comparability is enabled by using the same IRT-calibrated item pool and equating procedures.
- Mechanism: By estimating both human and LLM abilities on a common scale derived from IRT, direct comparisons (e.g., "GPT-4 surpasses high-ability college students") become possible.
- Core assumption: The IRT scale is invariant across populations, so a given ability estimate θ means the same for humans and LLMs.

## Foundational Learning

- **Concept: Item Response Theory (IRT) and its three-parameter logistic (3PL) model**
  - Why needed here: IRT provides the probabilistic framework for estimating latent abilities and calibrating item parameters, which is the foundation of adaptive testing.
  - Quick check question: What does the guessing parameter (c) represent in the 3PL model, and why is it important for LLM evaluation?

- **Concept: Fisher Information and its role in adaptive item selection**
  - Why needed here: Fisher Information quantifies how much information an item provides about ability at a given ability level; maximizing it yields the most efficient ability estimation.
  - Quick check question: How does Fisher Information depend on item difficulty relative to current ability estimate?

- **Concept: Computerized Adaptive Testing (CAT) workflow and stopping rules**
  - Why needed here: CAT operationalizes the IRT and Fisher Information concepts into an iterative test that stops when estimation precision is sufficient.
  - Quick check question: What stopping rule is typically used in CAT to balance test length and estimation accuracy?

## Architecture Onboarding

- **Component map**: Item Pool Construction -> Adaptive Test Engine -> Expert Interface -> Diagnostic Reporting
- **Critical path**:
  1. Calibrate item parameters using large human response dataset
  2. Initialize LLM ability estimate (often neutral)
  3. Iteratively select items, collect LLM responses, update ability estimate
  4. Stop when SE criterion met; generate diagnostic report
- **Design tradeoffs**:
  - Calibration dataset size vs. item pool coverage
  - Item selection informativeness vs. subdomain representation
  - Expert scoring latency vs. test turnaround time
- **Failure signatures**:
  - Non-convergent SE curves (item pool poorly matched to LLM ability range)
  - Highly variable ability estimates across repeated runs (temperature/LLM randomness)
  - Poor human-LLM equating (item parameters unstable across populations)
- **First 3 experiments**:
  1. Simulate CAT on synthetic data to verify SE convergence vs. fixed test length
  2. Run CAT on a small LLM-human comparison pilot to check equating plausibility
  3. Stress-test adaptive selection with noisy LLM responses to see robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of adaptive testing compare to traditional benchmarks when evaluating newer LLM architectures beyond the six models tested?
- Basis in paper: [explicit] The paper compares six instruction-tuned LLMs but suggests potential for broader application
- Why unresolved: The study only evaluates current popular models (ChatGPT, GPT-4, Bard, etc.) without exploring how the framework performs with emerging architectures
- What evidence would resolve it: Systematic testing of newer models like Claude-3, Gemini, or other emerging LLMs using the same adaptive framework

### Open Question 2
- Question: What is the optimal balance between temperature parameter and test reliability for different types of reasoning tasks?
- Basis in paper: [explicit] The paper notes ChatGPT's inconsistency varies with temperature but doesn't provide a systematic analysis
- Why unresolved: The study observes temperature affects uncertainty but doesn't establish optimal settings for different task types
- What evidence would resolve it: Empirical studies testing multiple temperature settings across various reasoning domains (mathematical, logical, creative)

### Open Question 3
- Question: Can the adaptive testing framework be extended to evaluate multimodal capabilities in addition to text-based reasoning?
- Basis in paper: [inferred] GPT-4's multimodal capabilities are mentioned but not tested; framework focuses on text-only evaluation
- Why unresolved: The current implementation only handles text-based questions despite mentioning GPT-4's image processing
- What evidence would resolve it: Implementation of adaptive testing protocols that incorporate visual, audio, or other non-textual inputs and responses

## Limitations
- Item parameter stability across human and LLM populations is assumed but not validated
- Subdomain diagnostic granularity lacks explicit validation of concept independence
- Temperature and prompting strategies are underspecified, affecting response consistency

## Confidence

- **High Confidence**: The core IRT framework and CAT workflow are well-established in psychometrics literature
- **Medium Confidence**: The specific implementation choices (MLE ability estimation, 20-question stopping rule, IRT-3PL calibration) appear reasonable
- **Low Confidence**: Claims about human-LLM comparability and diagnostic insights lack direct empirical support

## Next Checks
1. Conduct differential item functioning (DIF) analysis comparing item parameters across human and LLM responses
2. Systematically vary LLM temperature parameters during adaptive testing to quantify impact on ability estimates
3. Perform correlation analysis on subdomain ability estimates to verify independence assumptions