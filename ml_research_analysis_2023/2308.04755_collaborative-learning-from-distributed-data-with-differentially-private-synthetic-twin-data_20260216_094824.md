---
ver: rpa2
title: Collaborative Learning From Distributed Data With Differentially Private Synthetic
  Twin Data
arxiv_id: '2308.04755'
source_url: https://arxiv.org/abs/2308.04755
tags:
- data
- synthetic
- local
- parties
- party
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of collaborative learning from
  sensitive distributed data, where pooling data is not possible due to privacy constraints.
  It proposes a framework where each party shares a differentially private synthetic
  twin of their data, enabling others to perform statistical analyses without accessing
  the original sensitive data.
---

# Collaborative Learning From Distributed Data With Differentially Private Synthetic Twin Data

## Quick Facts
- arXiv ID: 2308.04755
- Source URL: https://arxiv.org/abs/2308.04755
- Reference count: 26
- Primary result: Parties using synthetic data from others achieve significantly better analysis performance than using only local data, especially with small or skewed datasets.

## Executive Summary
This paper addresses the challenge of collaborative learning from sensitive distributed data where pooling is not possible due to privacy constraints. The proposed framework enables each party to share a differentially private synthetic twin of their local data, allowing others to perform statistical analyses without accessing the original sensitive information. Using UK Biobank health data, experiments demonstrate that parties combining synthetic data from multiple sources achieve significantly better predictive performance than using only their local data, with improvements being more pronounced for smaller datasets and when combining data from more parties.

## Method Summary
The method uses differentially private variational inference to train generative models on each party's local data. Each party samples K=100 synthetic datasets from their trained model and shares these with other parties. Parties then combine their local data with synthetic data from all other parties to train analysis models. The framework uses a mixture model with Poisson regression for the analysis task, with privacy parameter ε=1 and δ=1/Nm. Performance is evaluated on held-out test sets using Monte Carlo sampling of log-likelihood distributions from parameter estimates and standard errors.

## Key Results
- Parties using synthetic data from others achieve significantly better analysis performance (higher predictive log-likelihoods) than using only local data
- Improvement is more pronounced with smaller local datasets and increases with the number of participating parties
- Synthetic data sharing effectively corrects local data skews, especially benefiting parties with underrepresented groups in their data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Each party's synthetic data preserves enough statistical signal from their local data to improve global analysis.
- Mechanism: Parties train generative models on local data using differentially private variational inference. The DP mechanism limits information leakage while allowing the model to learn the local data distribution. When synthetic data from multiple parties are combined, the statistical signal from the overall population emerges despite individual local skews.
- Core assumption: The generative model can learn a useful approximation of the local data distribution even with DP noise and small sample sizes.
- Evidence anchors:
  - [abstract] "Each party shares a differentially private synthetic twin of their data" and "parties using synthetic data from others achieve significantly better analysis performance"
  - [section] "we empirically demonstrate on a real-world data set that: Complementing local data with synthetic twins of similarly sized data sets consistently increases the utility"
  - [corpus] Weak - the corpus papers focus on federated learning and synthetic data generation but don't directly validate the multi-party combination effect
- Break condition: If the local data is too small or too skewed, the generative model fails to learn a useful approximation, and combining poor-quality synthetic data doesn't improve analysis.

### Mechanism 2
- Claim: Combining synthetic data from multiple parties reduces the impact of individual local skews.
- Mechanism: When each party's synthetic data reflects their local distribution skew, combining many such datasets averages out the individual skews. This creates a combined dataset that better represents the overall population distribution, even if each individual party's data is biased.
- Core assumption: The local skews are independent and can be averaged out through combination.
- Evidence anchors:
  - [abstract] "data sharing can especially help parties whose data contain underrepresented groups to perform better-adjusted analysis for said groups"
  - [section] "parties can successfully correct biases in their data that arise from a local skew of the data distribution"
  - [corpus] Weak - the corpus doesn't directly address skew correction through data combination
- Break condition: If all parties share similar skews (e.g., systematic bias affecting all data sources), combining won't correct the bias.

### Mechanism 3
- Claim: The benefits of data sharing persist even with small local datasets and strong privacy constraints.
- Mechanism: While strong privacy (low epsilon) and small datasets reduce the quality of individual synthetic datasets, the combination of many such datasets still provides sufficient statistical signal for analysis. The improvement from having more data outweighs the degradation from privacy constraints.
- Core assumption: The negative effects of privacy constraints and small sample sizes are mitigated by combining multiple synthetic datasets.
- Evidence anchors:
  - [abstract] "even if individual data sets are small or do not represent the overall population well"
  - [section] "we have empirically demonstrated... that data sharing using differentially private synthetic twin data from multiple sources improves the performance"
  - [corpus] Weak - the corpus papers don't directly test the trade-off between privacy strength and utility in multi-party settings
- Break condition: If privacy constraints are too strong (epsilon too low) or local datasets are too small, the synthetic data becomes too noisy to provide useful signal even when combined.

## Foundational Learning

- Concept: Differential Privacy (DP)
  - Why needed here: DP provides the formal privacy guarantee that allows parties to share synthetic data without revealing individual records
  - Quick check question: What is the formal definition of (ε,δ)-differential privacy and how does it limit information leakage?

- Concept: Generative Models for Synthetic Data
  - Why needed here: Parties need to create synthetic data that captures the statistical properties of their local data while preserving privacy
  - Quick check question: What types of generative models are suitable for creating synthetic twin data, and how does DP affect their training?

- Concept: Statistical Inference with Synthetic Data
  - Why needed here: The analysis must be performed on synthetic data rather than original data, requiring understanding of how synthetic data affects inference quality
  - Quick check question: How does using synthetic data instead of real data affect the reliability and validity of statistical analysis?

## Architecture Onboarding

- Component map:
  Local data owners (parties) → DP generative model training → Synthetic data publication → Data combination → Analysis model training → Performance evaluation

- Critical path:
  1. Each party trains a DP generative model on their local data
  2. Parties publish synthetic twin datasets
  3. Each party collects synthetic data from all other parties
  4. Parties combine local and synthetic data
  5. Analysis model is trained on combined data
  6. Performance is evaluated on held-out population test set

- Design tradeoffs:
  - Privacy vs utility: Stronger privacy (lower epsilon) provides better privacy guarantees but reduces synthetic data quality
  - Number of parties vs quality: More parties provide better population representation but increase complexity
  - Synthetic data size vs computation: Larger synthetic datasets provide better analysis but require more storage and computation

- Failure signatures:
  - Analysis performance worse than local-only: Indicates poor-quality synthetic data or insufficient combination
  - High variance in results: Suggests synthetic data quality varies significantly between parties
  - No improvement with more parties: Could indicate systematic biases or insufficient privacy-utility trade-off

- First 3 experiments:
  1. Train DP generative models on UK Biobank data and evaluate synthetic data quality metrics (distribution similarity, privacy parameter effectiveness)
  2. Test analysis performance with synthetic data from single party vs local data only
  3. Test analysis performance with synthetic data from increasing numbers of parties (1, 2, 3, 5, all) to identify the point of diminishing returns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of synthetic twin data degrade as the local data size decreases, and is there a threshold below which the data becomes unusable?
- Basis in paper: [explicit] The paper mentions that "learning a generative model often involves approximations, which can also limit the statistical signal captured in the synthetic twin data" and that "there are three main factors that can cause synthetic data to be of low quality: 1) the underlying data set was too small to learn a good generative model under differential privacy, 2) the data was skewed away from the population distribution, and, 3) the chosen generative model results in loss of information or skew."
- Why unresolved: The paper does not provide a specific threshold for when synthetic data becomes unusable, and it does not quantify the rate of degradation in data quality as local data size decreases.
- What evidence would resolve it: Experiments that systematically vary the size of the local data and measure the quality of the resulting synthetic data using metrics such as Wasserstein distance or maximum mean discrepancy would provide evidence.

### Open Question 2
- Question: How can parties detect and mitigate the impact of malicious actors sharing low-quality or poisoned synthetic data?
- Basis in paper: [explicit] The paper states that "a malicious party could sample a large set of arbitrarily bad artificial data to poison the well for all other parties" and that "it is likely that the other parties could filter out such bad shared data sets by comparison with other shared data sets, resulting in some robustness of the overall approach."
- Why unresolved: The paper does not provide a concrete method for detecting or mitigating the impact of malicious actors, and it does not quantify the robustness of the approach against such attacks.
- What evidence would resolve it: Experiments that introduce synthetic data from malicious actors with varying levels of corruption and measure the impact on the analysis task would provide evidence.

### Open Question 3
- Question: How can parties evaluate the performance of their analysis on the global population when they only have access to their local data?
- Basis in paper: [explicit] The paper states that "parties generally do not have access to an unbiased sample from the population but only their local data" and that "it is not trivial for a party to test whether using shared data actually improves their analysis."
- Why unresolved: The paper does not provide a concrete method for evaluating the performance of the analysis on the global population, and it does not discuss the trade-offs between privacy and accuracy in such evaluations.
- What evidence would resolve it: Experiments that compare the performance of the analysis on the global population using different evaluation methods, such as secure multi-party computation or synthetic data as a proxy, would provide evidence.

## Limitations

- The method's effectiveness depends heavily on the quality of differentially private generative models, which can be limited by small sample sizes and strong privacy constraints
- The framework assumes that local data skews are independent and can be averaged out through combination, which may not hold for systematic biases
- The evaluation focuses primarily on predictive log-likelihood, potentially missing other important aspects of model performance or fairness

## Confidence

- **High confidence**: The core mechanism that combining synthetic data from multiple parties improves analysis performance compared to local-only data
- **Medium confidence**: The claim that this method effectively corrects local skews and works well with small datasets and strong privacy
- **Medium confidence**: The privacy guarantees provided by the differentially private generative models

## Next Checks

1. **Systematic bias test**: Design experiments where multiple parties share similar skews (e.g., geographic patterns affecting multiple centers) to test whether the method can distinguish between independent and systematic biases.

2. **Privacy-utility trade-off analysis**: Systematically vary ε from very low (ε=0.1) to high (ε=10) and measure the point at which additional privacy no longer provides meaningful protection while significantly degrading utility.

3. **Cross-population generalization**: Test whether synthetic data generated from one population (e.g., UK Biobank) provides similar benefits when used by parties from different populations, assessing the method's generalizability beyond the studied cohort.