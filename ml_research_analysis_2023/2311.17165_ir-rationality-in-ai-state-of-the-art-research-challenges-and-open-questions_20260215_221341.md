---
ver: rpa2
title: '(Ir)rationality in AI: State of the Art, Research Challenges and Open Questions'
arxiv_id: '2311.17165'
source_url: https://arxiv.org/abs/2311.17165
tags:
- rationality
- agents
- human
- rational
- cial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This article surveys the concept of rationality and irrationality
  within artificial intelligence, examining how it is defined and assessed in AI as
  well as in other fields like economics, philosophy, and psychology. It discusses
  various types of irrational behaviors that can be optimal in certain scenarios,
  such as bounded rationality, random behavior, and profit non-maximizing strategies.
---

# (Ir)rationality in AI: State of the Art, Research Challenges and Open Questions

## Quick Facts
- arXiv ID: 2311.17165
- Source URL: https://arxiv.org/abs/2311.17165
- Reference count: 40
- One-line primary result: This article surveys the concept of rationality and irrationality within artificial intelligence, examining how it is defined and assessed in AI as well as in other fields like economics, philosophy, and psychology.

## Executive Summary
This paper provides a comprehensive survey of rationality and irrationality within artificial intelligence, exploring how these concepts are defined and assessed across multiple disciplines. It examines various types of irrational behaviors that can be optimal in certain scenarios, such as bounded rationality, random behavior, and profit non-maximizing strategies. The survey also discusses methods for identifying and interacting with irrational agents, and explores the interplay between human and AI irrationality, including incorporating human cognitive biases into AI design and the impact of machine rationality on human-AI interactions.

## Method Summary
This survey paper collects and reviews 40 academic references spanning AI, economics, philosophy, and psychology to analyze how rationality is defined, assessed, and its implications across disciplines. The methodology involves categorizing types of irrationality, mapping them to their respective literature, and synthesizing findings into a structured survey format covering definitions, assessments, and open questions in AI rationality.

## Key Results
- No unified definition of rationality exists across AI, economics, philosophy, and psychology
- Certain types of irrationality can be optimal in specific scenarios (bounded rationality, random behavior, profit non-maximizing strategies)
- Open questions remain about assessing rationality in artificial agents and how irrationality impacts human-AI interactions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Defining rationality through the lens of multiple disciplines (AI, economics, philosophy, psychology) allows the survey to avoid the trap of a single, potentially limiting definition.
- Mechanism: The paper synthesizes definitions from each discipline, showing where they overlap and where they diverge. This comparative framing helps identify gaps and contradictions, making the survey more comprehensive.
- Core assumption: A unified definition is neither necessary nor desirable; instead, mapping the landscape of definitions is itself a contribution.
- Evidence anchors:
  - [abstract] "there is no unified definition of what constitutes a rational agent" and "The understanding of rationality in other fields has influenced its conception within AI"
  - [section] "We consider how the understanding of rationality in other fields has influenced its conception within AI, in particular work in economics, philosophy and psychology."
- Break condition: If one discipline's definition is later shown to subsume the others without loss of nuance, the multi-disciplinary framing may be unnecessary.

### Mechanism 2
- Claim: Highlighting "rationally irrational" behaviors demonstrates that irrationality can be optimal under specific constraints, which reframes the entire discussion.
- Mechanism: By showing that random behavior, bounded rationality, and profit non-maximizing strategies can be optimal in certain environments, the paper avoids the simplistic view that rationality is always better.
- Core assumption: Optimality depends on the environment and constraints, not on an absolute scale of rationality.
- Evidence anchors:
  - [section] "Each type of irrationality that we discuss below can in fact constitute the optimal behaviour or reasoning in the right scenario"
  - [corpus] Weak: corpus neighbors do not discuss bounded rationality or randomness as optimal.
- Break condition: If a universal optimal decision rule were discovered that applies regardless of environment, the "rationally irrational" framing would be less relevant.

### Mechanism 3
- Claim: Framing the survey around open questions rather than settled answers creates a roadmap for future research and invites interdisciplinary engagement.
- Mechanism: The paper lists concrete research directions (e.g., "How can we assess rationality in artificial agents?" and "How does irrationality impact human-AI interactions?") that are actionable and tied to the survey's findings.
- Core assumption: The field is still evolving, and identifying gaps is as valuable as summarizing current knowledge.
- Evidence anchors:
  - [section] "Section 6 presents a number of open questions that remain to be addressed in this area"
  - [abstract] "sets out the open questions in this area"
- Break condition: If the field were to reach consensus on most of these questions, the open-questions framing would become less useful.

## Foundational Learning

- Concept: Definitions of rationality across disciplines
  - Why needed here: To understand why the paper compares AI, economics, philosophy, and psychology definitions.
  - Quick check question: Which discipline defines rationality as "taking the action that maximizes expected utility"?
- Concept: Bounded rationality and satisficing
  - Why needed here: To grasp why the paper treats bounded rationality as a deviation from perfect rationality.
  - Quick check question: What is the difference between perfect rationality and bounded rationality according to Simon?
- Concept: Reinforcement learning exploration-exploitation tradeoff
  - Why needed here: To understand why random behavior in RL agents is considered "rationally irrational."
  - Quick check question: What is the purpose of ε-greedy exploration in reinforcement learning?

## Architecture Onboarding

- Component map: Literature survey engine -> Comparative analysis module -> Case study generator -> Open questions synthesizer -> Human-AI interaction impact evaluator
- Critical path: Literature survey → Comparative analysis → Case studies → Open questions synthesis → Impact evaluation
- Design tradeoffs: Depth vs. breadth in literature coverage; technical detail vs. accessibility for interdisciplinary readers
- Failure signatures: If definitions are misattributed to the wrong discipline, or if open questions are not actionable, the survey loses credibility.
- First 3 experiments:
  1. Run a pilot literature search on one discipline's definition of rationality and verify correct attribution.
  2. Test the "rationally irrational" framing on a small example (e.g., ε-greedy in RL) to ensure it is convincing.
  3. Have a non-AI domain expert review the open questions to check for clarity and relevance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the most effective way to distinguish between truly irrational behavior and deceptive behavior in artificial agents?
- Basis in paper: [explicit] The paper discusses the importance of distinguishing between irrationality and unreliability, particularly when agents may appear irrational due to deceptive intentions.
- Why unresolved: Current methods for dealing with deceptive agents are largely domain-specific and may not be directly applicable to interactions with artificial agents exhibiting seemingly irrational behavior.
- What evidence would resolve it: Development and testing of new techniques specifically designed to differentiate between irrationality and deception in artificial agents across various domains.

### Open Question 2
- How can existing opponent modeling techniques be adapted to better account for irrational behavior in artificial agents?
- Basis in paper: [explicit] The paper highlights that current opponent modeling techniques often assume rationality or classify observed behavior within existing models, which may not be suitable for irrational agents.
- Why unresolved: There is a need for more domain-specific research to understand how different types of artificial agents exhibit irrational behavior and how to model these deviations from rationality effectively.
- What evidence would resolve it: Development and validation of new opponent modeling techniques that incorporate a broader understanding of irrational behavior in artificial agents and demonstrate improved performance in various domains.

### Open Question 3
- How does the incorporation of human cognitive biases into artificial agents impact human-AI interactions, and under what circumstances is this desirable?
- Basis in paper: [explicit] The paper discusses the potential benefits of incorporating human cognitive biases into artificial agents, such as improving decision-making in boundedly rational agents and enhancing explainability.
- Why unresolved: There is limited understanding of how the design of potentially not perfectly rational machines will impact the interaction with humans, and whether certain attributes make machines more trustworthy or appear more competent to human users.
- What evidence would resolve it: Empirical studies investigating the impact of cognitive biases in artificial agents on human trust, satisfaction, and performance in various human-AI interaction scenarios.

## Limitations
- The survey's selection criteria for the 40 references are not explicitly stated, raising questions about potential selection bias.
- The paper does not address potential conflicts between disciplinary definitions or provide a systematic framework for reconciling them.
- The open questions, while valuable, may not fully capture the most pressing research gaps due to the survey's breadth-over-depth approach.

## Confidence
- High Confidence: The observation that no unified definition of rationality exists across AI, economics, philosophy, and psychology is well-supported by the literature.
- Medium Confidence: The assertion that "rationally irrational" behaviors can be optimal is theoretically plausible but relies heavily on conceptual arguments rather than empirical validation.
- Low Confidence: The impact of human cognitive biases on AI design and human-AI interactions is discussed conceptually but lacks systematic empirical evidence.

## Next Checks
1. Verify the attribution of key definitions (e.g., rationality as utility maximization) to their respective disciplines by cross-checking primary sources.
2. Test the "rationally irrational" framing with concrete examples from recent AI applications to assess real-world validity.
3. Have domain experts from economics, philosophy, and psychology review the survey's interdisciplinary synthesis for accuracy and completeness.