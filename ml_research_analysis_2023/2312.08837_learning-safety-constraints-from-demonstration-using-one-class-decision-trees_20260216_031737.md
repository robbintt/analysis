---
ver: rpa2
title: Learning Safety Constraints From Demonstration Using One-Class Decision Trees
arxiv_id: '2312.08837'
source_url: https://arxiv.org/abs/2312.08837
tags:
- constraints
- agent
- learning
- expert
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning safety constraints
  from expert demonstrations to improve the safety of autonomous agents in reinforcement
  learning. The core method involves learning a one-class decision tree from expert
  trajectories, extracting constraints as a logical formula in disjunctive normal
  form, and using these constraints within a constrained reinforcement learning framework.
---

# Learning Safety Constraints From Demonstration Using One-Class Decision Trees

## Quick Facts
- arXiv ID: 2312.08837
- Source URL: https://arxiv.org/abs/2312.08837
- Reference count: 8
- Key outcome: Learns interpretable safety constraints from expert demonstrations using one-class decision trees, improving safety in constrained reinforcement learning

## Executive Summary
This paper introduces a method for learning safety constraints from expert demonstrations using one-class decision trees. The approach extracts constraints as a logical formula in disjunctive normal form, which can be used within a constrained reinforcement learning framework. The method is evaluated on synthetic benchmark domains and a realistic driving environment, demonstrating improved safety and performance compared to standard RL agents. The learned constraints are also successfully transferred between different agents and tasks, with the best results achieved when transferring between the same tasks for different agents.

## Method Summary
The method involves learning a one-class decision tree (OC-tree) from expert trajectories to define a convex safe set in feature space. The OC-tree is recursively traversed to extract a logical formula in disjunctive normal form (DNF), which represents the learned constraints. This formula is then used as a cost function within a constrained reinforcement learning framework (PPO-Lagrangian) to train safe policies. The approach also includes a pruning mechanism to simplify the learned constraints by removing conjunctions with low violation-evaluation ratios.

## Key Results
- Learned constraints improve safety and performance compared to standard RL agents and hand-engineered reward functions
- Constraints are successfully transferred between different agents and tasks, with best results when transferring between same tasks for different agents
- The approach handles environments with stochastic dynamics and is less prone to overfitting compared to previous methods

## Why This Works (Mechanism)

### Mechanism 1
The learned OC-tree defines a convex safe set in feature space, guaranteeing that any policy producing feature expectations within this set will be safe. One-class decision trees partition the feature space into hyper-rectangles covering expert trajectories, and since hyper-rectangles are convex and their union is convex, the OC-tree defines a convex safe set.

### Mechanism 2
The disjunctive normal form (DNF) formula extracted from the OC-tree provides an interpretable representation of constraints that can be used as a cost function in constrained reinforcement learning (CRL). Each conjunction in the DNF formula represents a constraint, and the formula evaluates to true when a feature vector violates the constraints.

### Mechanism 3
The violation-evaluation ratio pruning mechanism simplifies learned constraints while preserving safety-critical aspects. During CRL training, each conjunction is tracked for evaluation and violation frequency, with low-ratio conjunctions pruned as they likely represent less critical constraints.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The problem is formulated as a CMDP, which extends MDPs with cost functions and constraints
  - Quick check question: What are the key components of an MDP and how do they differ from a CMDP?

- Concept: Constrained Reinforcement Learning (CRL)
  - Why needed here: The learned constraints are used within a CRL framework to train safe policies
  - Quick check question: How does CRL differ from standard RL, and what are the key challenges in solving CRL problems?

- Concept: One-Class Classification (OCC)
  - Why needed here: The OC-tree is an OCC method that learns a model of the "safe" class from expert demonstrations
  - Quick check question: What is the difference between one-class classification and traditional binary/multi-class classification?

## Architecture Onboarding

- Component map: Expert trajectories → OC-tree learning → DNF formula extraction → CRL with PPO-Lagrangian → Pruning → Safe policy
- Critical path: The most critical components are the OC-tree learning and the CRL training, as they directly impact the safety and performance of the learned policy
- Design tradeoffs: The depth of the OC-tree affects the restrictiveness of the learned constraints - deeper trees are more restrictive but risk overfitting. The violation-evaluation threshold affects the pruning of constraints - higher thresholds result in more pruning but risk removing important constraints
- Failure signatures: If the learned policy is unsafe, it may indicate that the OC-tree didn't capture the true safe region or the CRL training didn't converge properly. If the policy is overly conservative, it may indicate that the OC-tree is too restrictive or the pruning removed too many constraints
- First 3 experiments:
  1. Train an OC-tree on synthetic expert trajectories and visualize the learned safe set in feature space
  2. Use the learned constraints in a simple CRL problem and compare the performance to an unconstrained RL agent
  3. Experiment with different OC-tree depths and pruning thresholds to find the best balance between safety and performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed method compare to other constraint learning approaches when learning from expert demonstrations in environments with stochastic dynamics?
- Basis in paper: The paper mentions that the proposed method is less prone to overfitting compared to the method by Lindner et al. (2023) and can handle environments with stochastic dynamics
- Why unresolved: The paper does not provide a direct comparison between the proposed method and other constraint learning approaches in environments with stochastic dynamics
- What evidence would resolve it: Empirical results comparing the performance of the proposed method with other constraint learning approaches in environments with stochastic dynamics

### Open Question 2
- Question: How does the transferability of learned constraints vary with the complexity of the source and target domains?
- Basis in paper: The paper mentions that the best results for a given target domain are achieved when the source and target domains are the same, and that there is a noticeable decline in performance when transferring constraints between distinct tasks for the same agent
- Why unresolved: The paper does not provide a systematic analysis of how the transferability of learned constraints varies with the complexity of the source and target domains
- What evidence would resolve it: A systematic study of the transferability of learned constraints across domains with varying levels of complexity, including both simple and complex tasks and agents

### Open Question 3
- Question: How does the proposed method handle environments with continuous state-action spaces and non-linear constraints?
- Basis in paper: The paper mentions that the proposed method can handle environments with continuous state-action spaces and non-linear constraints by using a one-hot encoding of the state-action space as features
- Why unresolved: The paper does not provide empirical results demonstrating the effectiveness of the proposed method in handling environments with continuous state-action spaces and non-linear constraints
- What evidence would resolve it: Empirical results demonstrating the effectiveness of the proposed method in handling environments with continuous state-action spaces and non-linear constraints, including a comparison with other methods that can handle such environments

## Limitations
- The approach may struggle with high-dimensional state spaces due to the curse of dimensionality
- The quality and coverage of expert demonstrations significantly impact the learned constraints
- The transferability of learned constraints may be limited to similar tasks and agent types

## Confidence

- Convex safe set representation: Medium-High
- Constraint interpretability and transferability: Medium
- Scalability to high-dimensional spaces: Low

## Next Checks

1. Test constraint transferability between substantially different domains (e.g., driving to robotic manipulation) to assess generalization limits
2. Conduct systematic ablation studies on OC-tree depth and pruning thresholds to identify optimal configurations
3. Evaluate performance with noisy or incomplete expert demonstrations to assess robustness to demonstration quality variations