---
ver: rpa2
title: Counterfactual Explanation for Fairness in Recommendation
arxiv_id: '2307.04386'
source_url: https://arxiv.org/abs/2307.04386
tags:
- fairness
- recommendation
- counterfactual
- attributes
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CFairER, a novel method for generating attribute-level
  counterfactual explanations for fairness in recommendation systems. The authors
  propose an off-policy reinforcement learning framework that leverages real-world
  attributes from Heterogeneous Information Networks (HINs) to identify minimal sets
  of attributes that change recommendation fairness.
---

# Counterfactual Explanation for Fairness in Recommendation

## Quick Facts
- **arXiv ID**: 2307.04386
- **Source URL**: https://arxiv.org/abs/2307.04386
- **Reference count**: 40
- **Primary result**: CFairER achieves up to 25.9% improvement in NDCG@40 and 36.0% in Gini@40 over baselines for fairness in recommendation

## Executive Summary
This paper introduces CFairER, a novel method for generating attribute-level counterfactual explanations for fairness in recommendation systems. The approach uses an off-policy reinforcement learning framework that leverages real-world attributes from Heterogeneous Information Networks (HINs) to identify minimal sets of attributes that change recommendation fairness. CFairER employs attentive action pruning to reduce the search space and counterfactual risk minimization for unbiased policy optimization. Experiments on three real-world datasets demonstrate that CFairER generates more faithful explanations for fairness while maintaining better recommendation performance compared to existing methods.

## Method Summary
CFairER uses off-policy reinforcement learning to generate counterfactual explanations for fairness in recommendation systems. The method first learns user and item representations from HINs using graph representation learning. It then trains a recommendation model (matrix factorization) to generate initial recommendations. The counterfactual fairness explanation model uses state representation learning with GRU, attentive action pruning to select candidate attributes, and counterfactual reward calculation based on fairness disparity. The policy is optimized using counterfactual risk minimization with inverse propensity scoring to correct for distribution bias. The framework generates minimal sets of attribute changes that improve fairness while maintaining recommendation quality.

## Key Results
- CFairER outperforms the strongest baseline by up to 25.9% in NDCG@40 and 36.0% in Gini@40
- Achieves better fairness-accuracy trade-off compared to existing methods
- Generates more faithful attribute-level counterfactual explanations for fairness
- Demonstrates effectiveness across three real-world datasets: Yelp, Douban Movie, and Last-FM

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The attentive action pruning reduces the search space of candidate counterfactual attributes by dynamically selecting top-n attributes based on attention scores.
- Mechanism: For each state, the method computes attention scores between the state representation and attribute embeddings, normalizes these scores, and selects only the top-n attributes as candidate actions. This reduces the candidate set from potentially hundreds of attributes to a manageable number (e.g., 40).
- Core assumption: The attention mechanism effectively identifies attributes most relevant to the current state and fairness changes.
- Evidence anchors:
  - [abstract]: "attentive action pruning reducing the search space of candidate counterfactuals"
  - [section]: "Our attentive action pruning is designed to reduce the action search space by specifying the varying importance of actions for each state."
  - [corpus]: Found 25 related papers on counterfactual explanations, suggesting this is a well-explored area but specific attention-based pruning approaches are less common.
- Break condition: If the attention mechanism fails to identify truly relevant attributes, the reduced search space may exclude optimal counterfactual explanations, leading to suboptimal fairness improvements.

### Mechanism 2
- Claim: The counterfactual risk minimization (CRM) objective corrects for policy distribution bias in the off-policy learning setting.
- Mechanism: CRM uses Inverse Propensity Scoring (IPS) to estimate and correct the distribution shift between the target policy and logging policy. This allows unbiased policy optimization by weighting rewards with the propensity score ratio.
- Core assumption: The logging policy's propensity scores are known or can be estimated accurately.
- Evidence anchors:
  - [abstract]: "counterfactual risk minimization for unbiased policy optimization"
  - [section]: "CRM employs an Inverse Propensity Scoring (IPS) to explicitly estimate the distribution shift between ùúãùê∏ and ùúã0"
  - [corpus]: Limited corpus evidence on CRM in fairness explanations, but CRM is well-established in off-policy learning literature.
- Break condition: If propensity scores are inaccurate or the logging policy is too different from the target policy, CRM may fail to correct bias adequately, leading to suboptimal explanations.

### Mechanism 3
- Claim: The fusion of attribute embeddings with user/item latent factors enables the model to measure how attribute changes affect recommendation fairness.
- Mechanism: When an attribute is selected as an action, its embedding is fused with the corresponding user or item latent factor through element-wise product. This creates an intervened recommendation, allowing comparison of fairness disparity before and after the change.
- Core assumption: The element-wise product fusion effectively captures the interaction between attributes and latent factors in determining fairness.
- Evidence anchors:
  - [abstract]: "fuses the embedding of ùëéùë° with user or item latent factors from the recommendation model"
  - [section]: "For the Rationality, we fuse the embedding of ùëéùë° with user or item latent factors from the recommendation model"
  - [corpus]: Weak corpus evidence on this specific fusion approach for fairness explanations.
- Break condition: If the fusion operation doesn't capture the true relationship between attributes and fairness, the model may generate incorrect explanations about which attributes affect fairness.

## Foundational Learning

- Concept: Reinforcement Learning in Off-Policy Setting
  - Why needed here: The method needs to explore counterfactual explanations without following the original recommendation policy, requiring off-policy learning.
  - Quick check question: What distinguishes off-policy from on-policy learning, and why is off-policy necessary for counterfactual reasoning?

- Concept: Heterogeneous Information Networks (HINs)
  - Why needed here: The method uses real-world attributes from HINs to provide attribute-level explanations for discrete attributes like gender and race.
  - Quick check question: How does a HIN differ from a homogeneous graph, and why is this heterogeneity important for fairness explanations?

- Concept: Counterfactual Reasoning
  - Why needed here: The method generates explanations by answering "what if" questions about minimal attribute changes and their impact on fairness.
  - Quick check question: What makes an explanation "counterfactual" versus just a feature importance score?

## Architecture Onboarding

- Component map:
  - Graph Representation Module: Embeds users, items, and attributes from HIN
  - Recommendation Model: Generates initial recommendations (MF) and learns latent factors
  - Counterfactual Fairness Explanation Model: Off-policy RL agent with attentive action pruning and CRM optimization
  - State Representation: Encodes user and recommendation list into dense vectors
  - Action Space: Candidate attributes from HIN filtered by attentive pruning

- Critical path: State ‚Üí Attentive Action Pruning ‚Üí Action Selection ‚Üí Attribute Fusion ‚Üí Fairness Disparity Calculation ‚Üí Reward ‚Üí Policy Update

- Design tradeoffs:
  - Attentive action pruning vs. full search: Faster but may miss optimal attributes
  - Off-policy vs. on-policy: More exploration capability but requires CRM for bias correction
  - Simple MF vs. complex recommenders: Faster training but potentially less accurate recommendations

- Failure signatures:
  - Poor fairness improvements: Attentive pruning selecting wrong attributes or CRM not correcting bias
  - Degraded recommendation performance: Over-penalizing accuracy for fairness or incorrect attribute fusion
  - High variance in training: Insufficient exploration or poor propensity score estimation

- First 3 experiments:
  1. Run ablation without attentive action pruning on Yelp dataset to measure impact on search efficiency and fairness improvement
  2. Compare CRM-based optimization vs. cross-entropy loss on fairness-accuracy tradeoff
  3. Test different candidate sizes (n=10, 20, 40) to find optimal balance between exploration and efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CFairER perform when applied to recommendation scenarios with different fairness definitions beyond item exposure fairness?
- Basis in paper: [inferred] The paper states that "the proposed approach is general and can be utilized in different recommendation scenarios that involve different fairness definitions."
- Why unresolved: The paper only evaluates CFairER on item exposure fairness, leaving its performance on other fairness definitions unexplored.
- What evidence would resolve it: Empirical results comparing CFairER's performance across multiple fairness definitions (e.g., user-side fairness, demographic parity) on the same datasets.

### Open Question 2
- Question: What is the impact of different logging policies on the performance of CFairER's off-policy learning framework?
- Basis in paper: [explicit] The paper mentions using a uniform-based logging policy and states "Our CFairER devises three major components" including the logging policy, but does not explore alternative logging policies.
- Why unresolved: The paper does not investigate how different logging policy designs affect the sample efficiency and bias correction capabilities of CFairER.
- What evidence would resolve it: Comparative experiments using various logging policies (e.g., epsilon-greedy, Thompson sampling) and their effects on CFairER's performance metrics.

### Open Question 3
- Question: How does CFairER scale with extremely large heterogeneous information networks in terms of computational efficiency and explanation quality?
- Basis in paper: [inferred] While the paper mentions time complexity analysis, it only tests on moderate-sized datasets and does not explore CFairER's behavior on massive-scale HINs.
- Why unresolved: The paper's experiments are limited to datasets with thousands of users/items, leaving uncertainty about CFairER's performance on industrial-scale recommendation systems.
- What evidence would resolve it: Scalability tests on large-scale HINs with millions of nodes and edges, measuring both computational runtime and explanation quality degradation.

## Limitations
- The method relies on accurate propensity score estimation for counterfactual risk minimization, but details on how these scores are obtained or validated are not provided
- Attentive action pruning may miss optimal counterfactual explanations if attention scores don't capture true attribute relevance
- The element-wise product fusion between attributes and latent factors is a relatively simple interaction mechanism that may not fully capture complex relationships between attributes and fairness outcomes

## Confidence

**High Confidence**: The overall framework design (off-policy RL with CRM for fairness explanations) and the empirical improvements over baselines are well-supported by the experimental results.

**Medium Confidence**: The effectiveness of the attentive action pruning mechanism is supported by the results but relies on the assumption that attention scores accurately identify relevant attributes.

**Low Confidence**: The specific claims about why certain attributes are identified as most influential for fairness changes are based on the model's internal attention mechanisms, which may not be fully interpretable or generalizable across different recommendation domains.

## Next Checks

1. **Propensity Score Validation**: Conduct experiments with varying quality of propensity score estimates to determine the sensitivity of CRM performance to estimation accuracy.

2. **Attention Mechanism Ablation**: Compare attentive action pruning against random pruning and full search baselines to quantify the value added by the attention mechanism specifically.

3. **Cross-Domain Generalization**: Test the method on recommendation domains with different attribute types (continuous vs. discrete) to assess the generalizability of the element-wise product fusion approach.