---
ver: rpa2
title: 'Not All Metrics Are Guilty: Improving NLG Evaluation by Diversifying References'
arxiv_id: '2305.15067'
source_url: https://arxiv.org/abs/2305.15067
tags:
- evaluation
- metrics
- human
- references
- automatic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of limited reference texts in
  natural language generation (NLG) evaluation benchmarks, which can lead to poor
  correlations with human judgments. The authors propose a method called Div-Ref to
  enhance evaluation benchmarks by leveraging large language models (LLMs) to paraphrase
  a single reference into multiple high-quality ones with diverse expressions.
---

# Not All Metrics Are Guilty: Improving NLG Evaluation by Diversifying References

## Quick Facts
- arXiv ID: 2305.15067
- Source URL: https://arxiv.org/abs/2305.15067
- Reference count: 15
- Key outcome: Method improves correlation between automatic metrics and human evaluation by 7.82% using LLM-generated diverse reference paraphrases

## Executive Summary
This paper addresses the fundamental problem that limited reference texts in NLG evaluation benchmarks lead to poor correlations with human judgments. The authors propose Div-Ref, a method that uses LLMs to generate multiple diverse paraphrases of a single reference, expanding the reference set to better capture the semantic variability of valid outputs. Experiments across machine translation, text summarization, and image captioning tasks show significant improvements in metric-human correlation, particularly for lexicon-based metrics like BLEU and ROUGE.

## Method Summary
The method uses text-davinci-003 to generate 10-20 diverse paraphrases of each reference using 10 different paraphrasing instructions covering various aspects like word order, structure, voice, and style. The LLM generates paraphrases with temperature 1 and top_p 0.9 for nucleus sampling. Evaluation scores are calculated by aggregating scores across all paraphrased references, with max aggregation showing superior performance over mean aggregation. The approach works with existing automatic metrics without requiring retraining.

## Key Results
- Improves correlation with human evaluation by 7.82% ratio across multiple NLG tasks
- Max aggregation outperforms mean aggregation, with mean showing over 20% decrease in performance
- Lexicon-based metrics show more sensitivity to reference diversity than semantics-based methods
- Optimal reference quantity is 10-20 for translation tasks, balancing diversity gains against generation costs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expanding reference diversity improves correlation by better approximating the semantic space of valid outputs
- Core assumption: LLM-generated paraphrases adequately represent the semantic space of valid human expressions
- Evidence: Abstract states evaluation with few references may not accurately reflect hypothesis quality
- Break condition: If paraphrases don't maintain semantic equivalence or cover true output diversity

### Mechanism 2
- Claim: Diverse prompts increase lexical/syntactic variety benefiting lexicon-based metrics
- Core assumption: Lexicon-based metrics are more sensitive to surface-level n-gram variations
- Evidence: Distinct-4 score increases from 67.0 to 74.7 demonstrating diversity improvement
- Break condition: If paraphrases become semantically inconsistent or prompts don't produce different surface forms

### Mechanism 3
- Claim: Max aggregation is more effective than mean because highest-quality reference dominates evaluation
- Core assumption: At least one paraphrase will be a high-quality match to the hypothesis
- Evidence: Switching from max to mean shows over 20% decrease in evaluation results
- Break condition: If all paraphrases are uniformly poor quality or hypothesis differs fundamentally from all references

## Foundational Learning

- **Semantic equivalence and paraphrasing**: Why needed - method relies on generating paraphrases preserving meaning while varying expression. Quick check - What linguistic features must be preserved when paraphrasing?
- **Evaluation metric categories and sensitivities**: Why needed - different metrics respond differently to reference diversity. Quick check - How do BLEU and BERTScore differ in sensitivity to reference variations?
- **Large language model prompting strategies**: Why needed - method uses diverse prompts for varied paraphrases. Quick check - How does temperature and top_p sampling affect paraphrase diversity?

## Architecture Onboarding

- **Component map**: Input processing -> Paraphrase generation -> Score aggregation -> Metric evaluation -> Correlation analysis
- **Critical path**: 1) Receive input/hypothesis/reference, 2) Generate 10-20 diverse paraphrases, 3) Evaluate hypothesis against all references, 4) Aggregate scores using max, 5) Output final score
- **Design tradeoffs**: Reference quantity vs. generation cost; Prompt diversity vs. semantic consistency; Max vs. mean aggregation stability
- **Failure signatures**: Low correlation improvement despite diverse paraphrases; Performance degradation on certain metrics; High computational cost with marginal gains
- **First 3 experiments**: 1) Compare max vs. mean aggregation on small dataset with 5 reference variants, 2) Test different reference quantities (5, 10, 20) on translation task, 3) Evaluate impact of removing diverse prompts on lexicon-based metrics

## Open Questions the Paper Calls Out

- **Optimal reference quantity**: Paper suggests 10-20 references optimal but doesn't pinpoint exact number or explore task variations. Would require systematic study varying reference counts across tasks.
- **Diversity impact on different metrics**: Paper notes lexicon-based metrics are more sensitive but lacks detailed analysis of underlying reasons. Would need study correlating diversity measures with metric performance.
- **Extension to non-text modalities**: Paper suggests potential for speech/image but lacks experimental validation. Would require applying method to multimodal generation tasks.

## Limitations
- Requires LLM API access, creating resource barriers for some research communities
- No analysis of how reference diversity affects metrics' ability to detect model degradation over time
- Focuses on correlation improvements without examining absolute quality of evaluations

## Confidence
- **High confidence**: Core finding that reference diversity improves metric-human correlation is consistently demonstrated across tasks and metrics
- **Medium confidence**: Implementation details like optimal reference quantity and aggregation strategy appear sensitive to specific conditions
- **Medium confidence**: Claims about prompt diversity benefits for lexicon-based metrics need more systematic validation

## Next Checks
1. Conduct ablation study varying paraphrase generation temperature and sampling parameters to determine optimal diversity-quality tradeoff
2. Test method on dataset with known difficult cases where standard metrics fail, measuring if reference diversity helps detect these cases
3. Evaluate approach on benchmark with multiple human-written references, comparing LLM-generated diversity against human reference diversity