---
ver: rpa2
title: Understanding Deep Generative Models with Generalized Empirical Likelihoods
arxiv_id: '2306.09780'
source_url: https://arxiv.org/abs/2306.09780
tags:
- data
- samples
- test
- likelihood
- empirical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Generalized Empirical Likelihood (GEL) methods
  as diagnostic tools for evaluating deep generative models. The key idea is to formulate
  evaluation as moment condition tests, where samples are assigned weights that indicate
  how much each point contributes to distributional mismatch.
---

# Understanding Deep Generative Models with Generalized Empirical Likelihoods

## Quick Facts
- **arXiv ID**: 2306.09780
- **Source URL**: https://arxiv.org/abs/2306.09780
- **Reference count**: 40
- **Key outcome**: GEL methods outperform precision/recall metrics at detecting mode dropping (up to 60% better) and can identify when models ignore conditioning labels.

## Executive Summary
This paper introduces Generalized Empirical Likelihood (GEL) methods as diagnostic tools for evaluating deep generative models. The key innovation is formulating evaluation as moment condition tests where samples are assigned weights indicating their contribution to distributional mismatch. These weights enable detection of specific deficiencies like mode dropping, mode imbalance, and improper label conditioning without requiring manifold estimation. Experiments show GEL outperforms traditional metrics at detecting missing modes and can identify when models fail to properly condition on labels.

## Method Summary
The method uses GEL to evaluate generative models by optimizing weighted empirical distributions to satisfy moment conditions. It assigns per-sample weights that indicate distributional mismatch between model and data. The approach includes one-sample tests for detecting mode dropping and two-sample GEL (GEL2) for identifying out-of-distribution samples. Label information can be incorporated via product kernels. The method requires no hyperparameter tuning and provides interpretable per-sample diagnostics.

## Key Results
- GEL outperforms precision/recall metrics at detecting missing modes (up to 60% better)
- Successfully identifies mode imbalance in controlled experiments
- Can detect when models ignore conditioning labels in conditional generation tasks
- Provides interpretable per-sample weights indicating which samples are most problematic

## Why This Works (Mechanism)

### Mechanism 1
GEL assigns per-sample weights based on moment condition violations. By optimizing a weighted empirical distribution to satisfy moment constraints, samples contributing most to mismatch receive low weights. This works when moment conditions capture relevant distributional properties and 0 lies within the convex hull of transformed features.

### Mechanism 2
Two-sample GEL enables detection of both mode dropping and out-of-distribution samples. By assigning separate weights to test and model samples, it finds the closest pair of distributions satisfying moment constraints. Zero-weight samples indicate points outside the other distribution's support. This requires non-empty intersection of convex hulls of transformed features.

### Mechanism 3
Including label information via product kernels enables detection of improper label conditioning. By constructing kernels that include both image and label information, the test jointly evaluates both distributions. Mislabeling or ignored conditioning leads to high mismatch scores. This requires the label kernel to be characteristic enough to distinguish mismatches.

## Foundational Learning

- **Empirical Likelihood & Generalized Empirical Likelihood**: Forms the statistical foundation for assigning per-sample weights based on moment condition violations. Quick check: What is the difference between empirical likelihood and exponential tilting in terms of valid πi values?
- **Moment Conditions & Maximum Mean Discrepancy (MMD)**: Provide the statistical constraints that GEL optimizes; MMD ensures conditions are sufficient to characterize distributions. Quick check: Why does using a characteristic kernel in MMD ensure that D²(p,q) = 0 iff p = q?
- **Convex Hull Condition**: Determines when GEL objectives are finite; failure leads to undefined weights. Quick check: What geometric condition must hold for the mean test empirical likelihood to be finite?

## Architecture Onboarding

- **Component map**: Feature extractor (Pool3, BYOL) → Moment transformation → GEL optimizer (Newton method) → Weight assignment → Diagnostic output
- **Critical path**: Feature extraction → Moment constraint formulation → GEL dual optimization → Weight interpretation
- **Design tradeoffs**: Choice of moment conditions (more complex = better detection but higher cost); witness point selection (more points = more robust but slower); one-sample vs two-sample (simpler vs robust)
- **Failure signatures**: All weights ≈ uniform (moment conditions too weak or convex hull condition fails); optimizer divergence (λ → ∞) (convex hull intersection empty); extremely skewed weights (possible mode dropping or severe misspecification)
- **First 3 experiments**: 1) Controlled mode dropping on CIFAR-10 (remove classes, compare GEL weights to improved recall/coverage); 2) Mode imbalance detection (vary class proportions, measure Hellinger distance); 3) Label conditioning test (corrupt labels on CIFAR-10, compare GEL with label kernels to precision-recall baselines)

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but the limitations section highlights areas for future work including extending GEL to other evaluation tasks and improving computational efficiency.

## Limitations
- GEL can only detect deficiencies captured by chosen moment conditions, potentially missing complex distributional mismatches
- Two-sample GEL fails when model and data supports are completely disjoint (convex hull intersection empty)
- Method's effectiveness on real-world models is demonstrated primarily through qualitative weight interpretations rather than quantitative metrics

## Confidence
- **High confidence**: GEL's ability to assign per-sample weights and detect mode dropping/imbalance when moment conditions are well-chosen and convex hulls overlap
- **Medium confidence**: GEL's performance improvements over precision/recall metrics, as experimental comparisons are limited to synthetic CIFAR-10 scenarios
- **Low confidence**: GEL's effectiveness on real-world models like BigGAN and diffusion models, as the paper provides limited quantitative evidence

## Next Checks
1. **Stress test convex hull failures**: Systematically generate datasets where model and data supports are increasingly disjoint; measure GEL2's detection rate versus its failure rate
2. **Compare feature sensitivity**: Repeat mode dropping experiments using different feature extractors (Pool3, BYOL, CLIP); assess how feature choice affects GEL's detection capability
3. **Quantify calibration**: On synthetic datasets with known mode imbalances, compute GEL's Hellinger distance versus ground truth probabilities across multiple imbalance ratios and noise levels