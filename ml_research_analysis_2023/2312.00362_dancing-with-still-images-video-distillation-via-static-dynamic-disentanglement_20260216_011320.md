---
ver: rpa2
title: 'Dancing with Still Images: Video Distillation via Static-Dynamic Disentanglement'
arxiv_id: '2312.00362'
source_url: https://arxiv.org/abs/2312.00362
tags:
- video
- distillation
- dynamic
- temporal
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents the first systematic study of video dataset
  distillation, addressing the challenge of temporal redundancy in video data. The
  authors introduce a taxonomy for temporal compression strategies and conduct extensive
  experiments to reveal that static information is more critical than dynamic information
  for distillation.
---

# Dancing with Still Images: Video Distillation via Static-Dynamic Disentanglement

## Quick Facts
- arXiv ID: 2312.00362
- Source URL: https://arxiv.org/abs/2312.00362
- Reference count: 40
- Achieves up to 82% storage reduction compared to baselines while maintaining or improving classification accuracy

## Executive Summary
This work presents the first systematic study of video dataset distillation, addressing the challenge of temporal redundancy in video data. The authors introduce a taxonomy for temporal compression strategies and conduct extensive experiments to reveal that static information is more critical than dynamic information for distillation. Based on these insights, they propose a novel framework that disentangles static and dynamic information: first distilling static frames via image distillation, then compensating for dynamic content with a learnable dynamic memory block. Their method achieves state-of-the-art performance across video datasets (UCF101, HMDB51, Kinetics400) while using notably less storage—up to 82% reduction compared to baseline methods—by requiring only 1 static and 1 dynamic memory frame per instance. The approach also shows improved performance on dynamic video classes and generalizes well to different network architectures.

## Method Summary
The method employs a two-stage static-dynamic disentanglement framework for video dataset distillation. In stage one, static frames are distilled via image distillation algorithms (DM/MTT/FRePo) to create static memory. In stage two, a learnable dynamic memory block compensates for motion information, with both static and dynamic memories combined through an integrator network H to generate synthetic videos. The approach requires only 1 static and 1 dynamic memory frame per video instance, significantly reducing storage compared to baseline methods that use 2-5 frames per instance.

## Key Results
- Achieves state-of-the-art performance across UCF101, HMDB51, and Kinetics400 datasets
- Reduces storage requirements by up to 82% compared to baseline methods
- Demonstrates improved performance on dynamic video classes compared to existing approaches
- Shows effective generalization across different network architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal redundancy in videos can be exploited by separating static and dynamic information for distillation.
- Mechanism: By distilling static frames (one per video) via image distillation and compensating dynamic content with a learnable dynamic memory block, the method achieves state-of-the-art performance with significantly reduced storage.
- Core assumption: Static information is more critical than dynamic information for distillation tasks.
- Evidence anchors:
  - [abstract]: "Our investigation reveals that the temporal information is usually not well learned during distillation, and the temporal dimension of synthetic data contributes little."
  - [section 3.3]: "Our observation implies that dense frame correspondence is non-critical in the video distillation task."
- Break condition: If dynamic information becomes critical for the task, or if the static frame fails to capture sufficient information for the video.

### Mechanism 2
- Claim: Segmenting real and synthetic videos into pairs for pairwise distillation matching covers most video distillation methods from the temporal aspect.
- Mechanism: By cutting the real and synthetic videos into the same number of segments and applying distillation between the pairs of real and synthetic segments, the method reduces training time and memory consumption.
- Core assumption: Temporal consistency is maintained by ensuring orderedness and uniformity in frame sequences.
- Evidence anchors:
  - [section 3.1]: "The distillation setting in (a) obeys temporal consistency, while (b) and (c) violate the two consistency preconditions."
  - [section 3.2]: "Specifically, the real video and synthetic video are segmented evenly for the pairwise distillation matching, which is the only valid strategy for temporal consistency."
- Break condition: If the segmentation leads to significant loss of temporal information or if the uniformity assumption is violated.

### Mechanism 3
- Claim: A unified framework for video distillation can be achieved by disentangling static and dynamic information in videos.
- Mechanism: The method first distills static frames via image distillation, then compensates for dynamic content with a learnable dynamic memory block, and combines them using an integrator network.
- Core assumption: The static and dynamic information can be effectively separated and learned independently.
- Evidence anchors:
  - [abstract]: "It first distills the videos into still images as static memory and then compensates the dynamic and motion information with a learnable dynamic memory block."
  - [section 4]: "Based on the analysis in Sec. 3.3 and considering the tradeoff between efficiency and efficacy, we propose a video dataset distillation paradigm by disentangling the static and dynamic information in videos."
- Break condition: If the separation of static and dynamic information leads to loss of critical information or if the integrator network fails to combine them effectively.

## Foundational Learning

- Concept: Temporal redundancy in videos
  - Why needed here: Understanding temporal redundancy is crucial for exploiting it in video distillation.
  - Quick check question: How does temporal redundancy in videos differ from spatial redundancy in images?

- Concept: Static vs. dynamic information in videos
  - Why needed here: Separating static and dynamic information is key to the proposed method's success.
  - Quick check question: What are the characteristics of static and dynamic information in videos, and how can they be distinguished?

- Concept: Dataset distillation
  - Why needed here: The method builds upon existing dataset distillation techniques and extends them to video data.
  - Quick check question: What are the main challenges in dataset distillation, and how do they differ for video data compared to image data?

## Architecture Onboarding

- Component map:
  Static memory (distilled frames) -> Dynamic memory (learnable block) -> Integrator network H -> Synthetic videos

- Critical path:
  1. Distill static frames via image distillation
  2. Learn dynamic memory to compensate for dynamic content
  3. Combine static and dynamic memories using the integrator network
  4. Apply distillation algorithms to the combined synthetic videos

- Design tradeoffs:
  - Tradeoff between static and dynamic memory: Increasing both improves accuracy but also increases training time and GPU memory
  - Tradeoff between segmentation and receptive field: Segmentation reduces training cost but sacrifices model performance

- Failure signatures:
  - Poor performance on dynamic classes: Indicates insufficient dynamic information in the distilled videos
  - Increased training time and GPU memory: Suggests inefficient balance between static and dynamic memory

- First 3 experiments:
  1. Implement static memory learning via image distillation and evaluate performance on static classes
  2. Add dynamic memory and integrator network, then evaluate performance on dynamic classes
  3. Compare the proposed method with baseline methods on a small-scale dataset (e.g., MiniUCF) to validate improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed static-dynamic disentanglement framework perform on larger-scale datasets beyond Kinetics-400, such as Kinetics-600 or Kinetics-700?
- Basis in paper: [explicit] The paper mentions that the performance on Kinetics-400 is not as strong as on smaller datasets, and suggests that having more categories sharing one H network might cause interference during learning.
- Why unresolved: The paper does not provide results for Kinetics-600 or Kinetics-700, and only speculates on the potential reasons for the performance drop on Kinetics-400.
- What evidence would resolve it: Conducting experiments on Kinetics-600 or Kinetics-700 and comparing the results with those on Kinetics-400 would provide insights into the scalability of the proposed framework.

### Open Question 2
- Question: How does the proposed framework perform when using more complex H networks or allocating different H networks to different categories?
- Basis in paper: [explicit] The paper suggests that using a more complex H network or allocating different H networks to different categories could potentially offer better improvement in the method, but this approach would impose a greater training burden and significantly reduce the practical value of distillation.
- Why unresolved: The paper does not provide results for these alternative approaches, and only mentions them as potential solutions to the performance drop on larger datasets.
- What evidence would resolve it: Conducting experiments with more complex H networks or category-specific H networks and comparing the results with those obtained using a single H network would provide insights into the impact of H network complexity on the performance of the proposed framework.

### Open Question 3
- Question: How does the proposed framework perform on other types of video data, such as videos with different frame rates or resolutions?
- Basis in paper: [explicit] The paper does not discuss the performance of the framework on videos with different frame rates or resolutions, and only provides results for videos sampled to 16 or 8 frames with specific resolutions.
- Why unresolved: The paper does not provide any information on the generalizability of the framework to videos with different frame rates or resolutions.
- What evidence would resolve it: Conducting experiments on videos with different frame rates or resolutions and comparing the results with those obtained on the standard videos would provide insights into the robustness of the proposed framework to variations in video characteristics.

## Limitations
- Temporal consistency constraints may be overly restrictive, with alternative temporal matching strategies potentially viable
- Static information criticality claim may not hold for all video classification tasks, particularly those requiring detailed motion analysis
- Storage efficiency claims depend heavily on baseline comparison methods and specific memory configurations

## Confidence
- **High confidence**: Framework architecture and general approach of disentangling static and dynamic information; technical implementation details and experimental methodology
- **Medium confidence**: Superiority of static information for distillation tasks; optimality of 1+1 memory configuration; claimed storage efficiency improvements
- **Low confidence**: Claim that pairwise segmentation with uniformity and orderedness is the only valid temporal consistency strategy; generalizability of results to all video classification tasks

## Next Checks
1. **Dynamic class performance validation**: Test the method on videos with high motion content (sports, action sequences) to verify that the 1+1 memory configuration doesn't significantly underperform compared to methods with more dynamic frames.

2. **Alternative temporal strategies**: Implement and compare against a temporal distillation method that doesn't use pairwise segmentation (e.g., attention-based temporal alignment) to test the claim about temporal consistency constraints.

3. **Cross-dataset generalization**: Evaluate the method on datasets with different characteristics (e.g., longer videos, different frame rates) to assess whether the static-dynamic disentanglement approach generalizes beyond the tested datasets.