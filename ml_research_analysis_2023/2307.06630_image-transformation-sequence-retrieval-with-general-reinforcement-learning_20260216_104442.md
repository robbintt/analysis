---
ver: rpa2
title: Image Transformation Sequence Retrieval with General Reinforcement Learning
arxiv_id: '2307.06630'
source_url: https://arxiv.org/abs/2307.06630
tags:
- learning
- mcts
- image
- sequence
- itsr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Image Transformation Sequence Retrieval
  (ITSR) task, where the goal is to retrieve the sequence of transformations that
  convert a source image into a target image. The authors propose a solution based
  on model-based Reinforcement Learning using Monte Carlo Tree Search (MCTS) combined
  with deep neural networks.
---

# Image Transformation Sequence Retrieval with General Reinforcement Learning

## Quick Facts
- arXiv ID: 2307.06630
- Source URL: https://arxiv.org/abs/2307.06630
- Authors: 
- Reference count: 40
- Primary result: MCTS-based RL approach achieves 45% single-shot accuracy vs 20% for supervised learning on Imagenette dataset

## Executive Summary
This paper introduces the Image Transformation Sequence Retrieval (ITSR) task, where the goal is to find the sequence of transformations that converts a source image into a target image. The authors propose a solution using model-based Reinforcement Learning with Monte Carlo Tree Search (MCTS) combined with deep neural networks. They compare this approach against a supervised learning baseline across two scenarios: a toy problem with simple shapes and a real-world problem using the Imagenette dataset with common image-processing transformations. The MCTS-based approach significantly outperforms the supervised baseline, particularly in complex transformation scenarios.

## Method Summary
The method uses a siamese neural network backbone to extract features from image pairs, followed by an actor-critic decoder that outputs both a policy (next transformation to apply) and a value estimate (how close the current state is to the target). MCTS explores possible transformation sequences using these neural network outputs, with curriculum learning to gradually increase sequence complexity and experience replay to improve training stability. The model is trained using reinforcement learning rewards based on whether a sequence successfully transforms the source to target image.

## Key Results
- MCTS-based approach achieves 45% single-shot accuracy vs 20% for best supervised model on Imagenette dataset
- Supervised learning performs poorly even on simple toy problems due to the multiplicity of valid solutions
- MCTS approach handles complex transformation sequences better and avoids redundant transformations
- Deeper backbones (ConvNeXt, Swin-B) show better performance than shallower ones (ResNet18, EfficientNet-B0)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MCTS with deep learning can handle the multiplicity of valid transformation sequences better than supervised learning.
- Mechanism: The MCTS approach explores multiple possible transformation paths simultaneously, using the neural network's value and policy estimates to guide the search. This allows it to find any valid sequence that transforms the source to target image, rather than being constrained to a single "correct" sequence as in supervised learning.
- Core assumption: The value network can effectively estimate how close a partial transformation sequence is to the target, and the policy network can suggest promising next transformations.
- Evidence anchors:
  - [abstract] "The results report that a model trained with MCTS is able to outperform its supervised counterpart in both the simplest and the most complex cases."
  - [section] "Even in a scenario of very low graphical complexity, the multiplicity of solutions and the correlation of the actions of the trajectory make supervised learning ineffective, while RL postulates itself as a good alternative."
  - [corpus] Weak evidence - no directly comparable papers found in corpus.
- Break condition: If the value network cannot accurately estimate progress toward the target, or if the policy network cannot distinguish between useful and redundant transformations.

### Mechanism 2
- Claim: Curriculum learning enables the model to learn complex transformation sequences progressively.
- Mechanism: By starting with short sequences and gradually increasing complexity, the model can build up its understanding of how individual transformations combine. This prevents the model from being overwhelmed by the vast search space of long transformation sequences from the beginning.
- Core assumption: Learning to solve simpler subproblems (short sequences) provides a foundation for solving more complex problems (longer sequences).
- Evidence anchors:
  - [section] "We resort to curriculum learning [4], where the neural network first learns to find the solution for short sequences and then progressively trains with longer ones."
  - [section] "In each epoch, we extract a sample of 1000 trajectories from MCTS, which is executed for 100 iterations for each trajectory."
  - [corpus] Weak evidence - curriculum learning is mentioned in the corpus but not specifically applied to this type of problem.
- Break condition: If the model overfits to short sequences and cannot generalize to longer ones, or if the curriculum progression is too aggressive.

### Mechanism 3
- Claim: Experience replay improves training efficiency and stability.
- Mechanism: By storing and randomly sampling from past experiences, the model can learn from a more diverse set of transformation sequences and avoid overfitting to recent experiences. This helps the model develop a more robust understanding of the transformation space.
- Core assumption: Past experiences remain relevant and informative for future learning, even as the model's capabilities improve.
- Evidence anchors:
  - [section] "In order to improve the training efficiency and stability we introduce the Experience Replay [32] technique. Experience replay stores the agent's experiences (states, actions, and rewards) in a buffer."
  - [section] "These experiences are randomly sampled and used during the training process."
  - [corpus] Weak evidence - experience replay is mentioned in the corpus but not specifically applied to this type of problem.
- Break condition: If the replay buffer becomes too large and outdated experiences are no longer relevant, or if the random sampling does not provide sufficient diversity.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation
  - Why needed here: ITSR can be modeled as an MDP where states are image pairs, actions are transformations, and rewards are given for reaching the target image. This formulation allows the application of reinforcement learning techniques.
  - Quick check question: What are the components of an MDP and how do they map to the ITSR problem?

- Concept: Monte Carlo Tree Search (MCTS)
  - Why needed here: MCTS is used to explore the space of possible transformation sequences efficiently, using the neural network's estimates to guide the search toward promising paths.
  - Quick check question: How does MCTS balance exploration and exploitation in the context of ITSR?

- Concept: Actor-Critic architecture
  - Why needed here: The neural network outputs both a policy (actor) that suggests the next transformation and a value (critic) that estimates how close the current state is to the target. This dual output allows for more informed decision-making during the search.
  - Quick check question: What is the role of each head in the actor-critic architecture and how do they contribute to the MCTS process?

## Architecture Onboarding

- Component map:
  Input: Pair of images (source and target) -> Siamese backbone (feature extraction with subtraction) -> Decoder (two heads: policy and value) -> MCTS (tree search) -> Output: Sequence of transformations

- Critical path:
  1. Process source and target images through the siamese backbone
  2. Compute the difference of the feature maps
  3. Pass the difference through the decoder to get policy and value
  4. Use MCTS to explore possible transformation sequences
  5. Return the best sequence found

- Design tradeoffs:
  - Model complexity vs. training time: Deeper models (ConvNeXt, Swin-B) perform better but take longer to train
  - Exploration vs. exploitation in MCTS: Tuning the temperature parameter and Dirichlet noise affects the balance
  - Replay buffer size vs. diversity: Larger buffers provide more diversity but may include outdated experiences

- Failure signatures:
  - Model consistently predicts short sequences even for complex transformations
  - Model gets stuck in loops of redundant transformations
  - Model fails to recognize when a transformation is not contributing to the goal

- First 3 experiments:
  1. Implement the toy problem with the constrained setup to verify the basic MCTS + neural network architecture works
  2. Test the impact of different backbones (ResNet18, EfficientNet-B0) on the real image scenario
  3. Evaluate the effect of curriculum learning by comparing models trained with and without progressive sequence lengths

## Open Questions the Paper Calls Out
- The paper mentions that combining supervised and reinforcement learning is an interesting direction for future work, suggesting potential benefits from hybrid approaches that leverage the strengths of both methods.

## Limitations
- The paper only tests five basic transformations and does not explore more complex or varied transformation sets
- Results are based on a relatively small dataset (Imagenette) which may not generalize to more diverse real-world scenarios
- Lack of ablation studies to isolate the contribution of individual components like curriculum learning and experience replay

## Confidence
- Confidence in main claims: Medium - due to lack of direct comparison with other state-of-the-art approaches in this specific domain and limited evaluation across different datasets
- Confidence in mechanism explanations: Low - due to lack of empirical validation of the hypothesized benefits of MCTS over supervised learning

## Next Checks
1. Conduct an ablation study to measure the individual contribution of curriculum learning, experience replay, and MCTS to the overall performance.
2. Test the model's performance on a larger and more diverse dataset (e.g., ImageNet-21K) to assess generalizability.
3. Implement a more efficient MCTS variant (e.g., using a neural network to guide the search) to evaluate the scalability of the approach to longer transformation sequences.