---
ver: rpa2
title: Are Good Explainers Secretly Human-in-the-Loop Active Learners?
arxiv_id: '2306.13935'
source_url: https://arxiv.org/abs/2306.13935
tags:
- instances
- data
- explanations
- active
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes that using explainable AI (XAI) to identify\
  \ data for model retraining is equivalent to active learning (AL) with a human-in-the-loop.\
  \ It provides a mathematical formalization of this XAI-based workflow, approximating\
  \ the human\u2019s role in data selection using similarity computations."
---

# Are Good Explainers Secretly Human-in-the-Loop Active Learners?

## Quick Facts
- arXiv ID: 2306.13935
- Source URL: https://arxiv.org/abs/2306.13935
- Reference count: 13
- The paper proposes that using explainable AI (XAI) to identify data for model retraining is equivalent to active learning (AL) with a human-in-the-loop.

## Executive Summary
This paper proposes that explainable AI (XAI) can serve as a query strategy for active learning by approximating the human-in-the-loop selection process through similarity computations over instance pairs. The authors provide a mathematical framework that models the human's role in identifying "interesting" instances as a similarity metric over explanation vectors, enabling simulation-based evaluation without expensive user studies. Experiments on the SST-5 sentiment dataset using SHAP explanations and SVM models demonstrate that this XAI-based approach outperforms traditional entropy, margin, and random sampling baselines, achieving higher F1-macro scores earlier in the learning process.

## Method Summary
The authors formalize XAI-based data selection as active learning by modeling human inspection of explanations as similarity computations between instance pairs. They approximate this process using matrix operations (A, B, C) to encode instance similarities and label agreements, then retrieve instances from the unlabeled pool using dot-product similarity weighted by these matrices. The framework jointly optimizes explanation parameters (θ) and model parameters (Ψ) using Bayesian optimization, enabling efficient exploration of the parameter space without human studies.

## Key Results
- XAI-based approach outperforms entropy, margin, and random sampling baselines on SST-5 dataset
- Achieves higher F1-macro scores earlier in the learning process compared to traditional AL methods
- Successfully simulates human-in-the-loop data selection without requiring actual user studies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: XAI-based data selection approximates AL with a human-in-the-loop by modeling the human's role as a similarity computation over instance pairs.
- Mechanism: The human inspects explanations, identifies pairs with different predictions but similar explanations, and uses their aggregated similarity scores (matrix C) to retrieve similar instances from the unlabeled pool via dot-product similarity (Equation 13).
- Core assumption: The human's preference for selecting instances can be encoded as a weighted similarity measure between instances in the explanation space.
- Evidence anchors:
  - [abstract] "We provide a mathematical approximation for the role of the human"
  - [section] "the practitioner's goal is to select instances from Xinc similar to the ones that participate in such pairs in Xs"
  - [corpus] Weak evidence - no related papers directly support this approximation claim.
- Break condition: If human selection behavior is not well-represented by similarity metrics (e.g., if humans prioritize rare or surprising features not captured by dot-product similarity).

### Mechanism 2
- Claim: Optimizing explanation parameters θ alongside model parameters Ψ improves the effectiveness of the XAI-based query strategy.
- Mechanism: By jointly optimizing θ and Ψ, the system refines explanations to better highlight features that help distinguish between classes, making the similarity-based selection more informative.
- Core assumption: The explanation quality directly impacts the quality of the retrieved instances for retraining.
- Evidence anchors:
  - [abstract] "We want to minimize the generalization loss of f"
  - [section] "we optimize for the explanation parameters θ as well to refine them to be helpful in the data selection process"
  - [corpus] No direct evidence in related papers for joint optimization of explanation parameters in AL context.
- Break condition: If explanation parameter optimization does not lead to meaningful changes in instance selection or model performance.

### Mechanism 3
- Claim: The XAI-based AL approach can be simulated without user studies by approximating the human-in-the-loop process with mathematical formulations.
- Mechanism: The workflow steps involving human inspection and selection are replaced by matrix operations (Equations 1-13) that encode preferences and retrieve instances, enabling evaluation via simulation.
- Core assumption: The mathematical approximation of human behavior is sufficiently accurate to replace actual user studies for evaluating AL performance.
- Evidence anchors:
  - [abstract] "An added benefit is that their utility can be assessed via simulation instead of conducting expensive user-studies"
  - [section] "we try to eliminate this roadblock by (1) providing a reasonable approximation for the task of the ML practitioner"
  - [corpus] No supporting evidence in related papers for this specific simulation approach.
- Break condition: If the simulation results diverge significantly from what would be observed with actual human-in-the-loop experiments.

## Foundational Learning

- Concept: Active Learning and query strategies
  - Why needed here: The paper positions XAI-based data selection as a form of AL, so understanding standard AL methods (entropy, margin, random sampling) is crucial for comparing performance.
  - Quick check question: What is the primary goal of a query strategy in Active Learning?

- Concept: Model explanations and their formats
  - Why needed here: The paper assumes explanations are weight vectors (qi) that apply to instances in an explanation space, which is fundamental to the mathematical formulation.
  - Quick check question: How do SHAP and LIME typically represent their explanations, and why is this relevant to the paper's assumptions?

- Concept: Similarity metrics and their use in retrieval
  - Why needed here: The human's selection process is approximated using dot-product similarities between instances and their explanations, which is central to the retrieval mechanism.
  - Quick check question: Why might dot-product similarity be a reasonable approximation for human preference in selecting similar instances?

## Architecture Onboarding

- Component map:
  Initial model (f with parameters Ψ1) -> Unlabeled incremental data pool (Xinc) -> Explainer (E with parameters θ) -> Mathematical approximation (matrices A, B, C, S, W) -> Model retraining (Ψ2) -> Evaluation on test set (Xtest, Ytest)

- Critical path:
  1. Train initial model on (Xorig, Yorig)
  2. Sample BE instances from Xinc for explanation
  3. Compute explanations and build matrices A, B, C
  4. Retrieve top-BL instances from Xinc using W
  5. Label retrieved instances and retrain model
  6. Evaluate on test set and iterate

- Design tradeoffs:
  - BE vs BL sizes: Smaller BE reduces human effort but may miss important patterns; larger BL increases labeling cost but may improve model faster
  - Explanation quality vs computation cost: More detailed explanations may be more informative but computationally expensive
  - Approximation accuracy vs evaluation speed: More accurate human behavior modeling improves results but may require user studies

- Failure signatures:
  - Model performance plateaus early or degrades over iterations
  - Retrieved instances are not diverse or relevant to model improvement
  - Optimization of θ and Ψ does not converge or leads to overfitting

- First 3 experiments:
  1. Implement the XAI-based AL workflow on a small text dataset with a simple model (e.g., logistic regression) and SHAP explanations; compare F1-macro scores against random sampling baseline.
  2. Vary the explanation budget BE and labeling budget BL to study their impact on model performance and human effort.
  3. Replace the dot-product similarity approximation with a different metric (e.g., cosine similarity) and evaluate its effect on retrieval quality and model improvement.

## Open Questions the Paper Calls Out

- How accurately does the dot-product based similarity approximation capture the human-in-the-loop selection process in XAI-based data selection?
  - Basis in paper: [explicit] The paper explicitly states this approximation requires user studies to validate its adequacy (Section 3.5).
  - Why unresolved: The paper presents the mathematical formulation and initial experiments, but does not conduct user studies to validate whether the dot-product similarity approximation adequately represents human decision-making in selecting "interesting" instances.
  - What evidence would resolve it: User studies comparing human-selected instances with those selected by the dot-product similarity method under controlled conditions, measuring agreement rates and analyzing cases of divergence.

- How does the performance of XAI-based active learning compare to state-of-the-art active learning methods across diverse datasets, models, and explainers?
  - Basis in paper: [inferred] The paper presents initial promising results on SST-5 with SHAP and SVM, but explicitly states "our goal is to cover a diverse set of data, models and explainers" and "we hope to continue the empirical analyses" (Section 5).
  - Why unresolved: The paper only presents one dataset, one explainer, and one model type. The generalizability of the approach to other domains and configurations remains untested.
  - What evidence would resolve it: Systematic experiments across multiple datasets (images, tabular data), various model architectures (neural networks, tree-based models), and different XAI techniques (LIME, DeepLIFT), comparing performance against established AL methods.

- What is the impact of optimizing explanation parameters (θ) alongside model parameters on the effectiveness of the XAI-based active learning approach?
  - Basis in paper: [explicit] The paper formulates an objective function that optimizes both θ and Ψ (Equation 14) and mentions using Bayesian Optimization to explore the explanation parameter space (Section 3.2, 4).
  - Why unresolved: While the paper describes the optimization framework and mentions using Bayesian Optimization, it does not provide empirical analysis of how optimizing θ affects data selection quality or model performance compared to using default explainer parameters.
  - What evidence would resolve it: Controlled experiments comparing model performance when explanation parameters are optimized versus when default parameters are used, measuring both data selection quality and downstream model accuracy across multiple iterations.

## Limitations

- The approximation of human selection behavior as similarity computations may not capture complex human decision-making processes.
- The approach has only been validated on one dataset (SST-5) with one explainer (SHAP) and one model type (SVM).
- No empirical validation was conducted comparing the mathematical approximation against actual human-in-the-loop experiments.

## Confidence

- Mechanism 1 (Similarity approximation): Medium confidence - mathematically rigorous but lacks empirical validation
- Mechanism 2 (Joint optimization): Low confidence - framework described but limited experimental evidence
- Mechanism 3 (Simulation without user studies): Low confidence - no comparative studies with actual human participants

## Next Checks

1. Conduct a user study comparing human-selected instances versus the mathematical approximation's selections to measure approximation accuracy
2. Test the framework across multiple datasets and model types (CNN, LSTM) to assess generalizability beyond SST-5 with SVM
3. Implement ablation studies isolating the impact of explanation parameter optimization versus model parameter optimization on AL performance