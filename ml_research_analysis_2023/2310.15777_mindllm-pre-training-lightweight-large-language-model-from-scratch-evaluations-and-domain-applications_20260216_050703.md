---
ver: rpa2
title: 'MindLLM: Pre-training Lightweight Large Language Model from Scratch, Evaluations
  and Domain Applications'
arxiv_id: '2310.15777'
source_url: https://arxiv.org/abs/2310.15777
tags:
- data
- performance
- training
- language
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MindLLM, a series of lightweight large language
  models with 1.3B and 3.1B parameters trained from scratch. MindLLM models achieve
  competitive performance on standard benchmarks and specialized tasks, outperforming
  some larger models.
---

# MindLLM: Pre-training Lightweight Large Language Model from Scratch, Evaluations and Domain Applications

## Quick Facts
- arXiv ID: 2310.15777
- Source URL: https://arxiv.org/abs/2310.15777
- Reference count: 40
- This paper introduces MindLLM, a series of lightweight large language models with 1.3B and 3.1B parameters trained from scratch.

## Executive Summary
This paper introduces MindLLM, a series of lightweight large language models with 1.3B and 3.1B parameters trained from scratch. MindLLM models achieve competitive performance on standard benchmarks and specialized tasks, outperforming some larger models. The authors provide detailed insights into data construction, model architecture, and training strategies. They also introduce an entropy-based quality filtering approach for instruction tuning, demonstrating that high-quality data is more important than quantity for lightweight models. Applications in legal and financial domains showcase the models' adaptability and effectiveness. Overall, the work highlights the potential of lightweight LLMs for resource-constrained environments and specialized tasks.

## Method Summary
MindLLM models are pre-trained from scratch using a bilingual data mixture with a 1:1.5 Chinese:English ratio, totaling 2349 GB of processed data. The architecture includes improvements like RoPE positional embeddings, RMSNorm, DeepNorm, FlashAttention-2, and GeGLU activation. Two pre-training strategies are employed: bilingual mixture data and monolingual data followed by transfer. For instruction tuning, an entropy-based quality filtering approach is used to select high-quality data. The models are evaluated on standard benchmarks (MMLU, AGIEval, C-Eval, CMMLU) and specialized tasks in law and finance.

## Key Results
- MindLLM-1.3B outperforms GPT-Neo-1.3B and InternLM-1.3B on MMLU, achieving 46.2% in zero-shot setting.
- MindLLM-3B demonstrates superior performance in mathematical reasoning and bilingual capabilities compared to larger models like MOSS-Base-16B.
- Domain-specific fine-tuning on legal and financial datasets shows MindLLM's adaptability, with performance surpassing comparable models and matching larger ones.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data quality outweighs data diversity and quantity for lightweight LLMs.
- Mechanism: Lightweight models (≤7B parameters) have limited capacity to generalize from noisy or irrelevant data. High-quality, task-aligned data enables efficient learning of specific capabilities without overfitting.
- Core assumption: The model's representational capacity is the bottleneck, not the availability of diverse data.
- Evidence anchors:
  - [abstract]: "we prove that, in the context of lightweight models, the optimization of model performance is more proficiently realized through the amelioration of instruction tuning data quality, as opposed to mere augmentation in data quantity."
  - [section]: "Although the MOSS dataset, MingLi dataset and Tulu dataset exhibit higher data and task diversity, the performance of lightweight models improves more on smaller but higher-quality datasets."
  - [corpus]: Weak - no direct citations to this mechanism in corpus neighbors.
- Break condition: If model capacity increases significantly, the trade-off may shift toward favoring data diversity.

### Mechanism 2
- Claim: Bilingual alignment benefits from balanced data proportions rather than transfer training.
- Mechanism: Training from scratch with a balanced ratio of Chinese and English data allows the model to learn shared linguistic representations more effectively than sequential monolingual training followed by transfer.
- Core assumption: The model can simultaneously learn language-specific and cross-lingual features when exposed to both languages from the start.
- Evidence anchors:
  - [abstract]: "we conclude that it is suboptimal for lightweight models ( ≤ 7B) with constrained capacity scale to achieve complex capabilities such as mathematics, reasoning, or bilingual alignment through pre-training then transfer-training strategy."
  - [section]: "The proportion of Chinese data in LLAMA-7B and Open-LLaMAs is even more limited. As a result, we have two hypotheses... a more equitable distribution of data from the two languages during training from scratch is advisable."
  - [corpus]: Weak - corpus neighbors do not directly address bilingual alignment strategies.
- Break condition: If transfer training includes explicit alignment objectives or more balanced data ratios, the gap may narrow.

### Mechanism 3
- Claim: Lightweight models excel in specific domains when fine-tuned on targeted datasets.
- Mechanism: Domain-specific fine-tuning leverages the model's existing general capabilities while focusing on specialized knowledge, leading to performance competitive with larger models.
- Core assumption: Domain adaptation is more effective than scaling up model size for specialized tasks.
- Evidence anchors:
  - [abstract]: "Our models showcase outstanding performance in specific domains, particularly in areas like law and finance."
  - [section]: "Our model’s performance surpasses that of models of comparable scale and is comparable to larger models."
  - [corpus]: Weak - corpus neighbors focus on other lightweight models but do not directly validate this mechanism.
- Break condition: If the target domain requires broad general knowledge beyond the model's pre-training scope, performance may degrade.

## Foundational Learning

- Concept: Neural scaling laws
  - Why needed here: Understanding the relationship between model size, data volume, and performance is crucial for designing lightweight models that maximize efficiency.
  - Quick check question: If a model's loss decreases as a power function of training data, what happens to the marginal benefit of adding more data as the model scales?
- Concept: Catastrophic forgetting
  - Why needed here: Transfer training strategies can lead to forgetting previously learned knowledge, especially in lightweight models with limited capacity.
  - Quick check question: What happens to a model's performance on task A after it is fine-tuned on task B, and how can this be mitigated?
- Concept: Data entropy and quality filtering
  - Why needed here: Selecting high-quality instruction data based on entropy helps lightweight models focus on relevant patterns without being overwhelmed by noise.
  - Quick check question: How does the entropy of a data sample relate to its informativeness for a pre-trained model?

## Architecture Onboarding

- Component map:
  Pre-training -> Entropy-based filtering -> Domain-specific fine-tuning -> Evaluation
- Critical path:
  1. Pre-train on balanced bilingual data from scratch.
  2. Apply entropy-based filtering to select high-quality instruction data.
  3. Fine-tune on domain-specific datasets.
  4. Evaluate on both general and domain benchmarks.
- Design tradeoffs:
  - Model size vs. performance: Lightweight models trade some general capability for efficiency and domain adaptability.
  - Data diversity vs. quality: Focused, high-quality data is more beneficial than broad, noisy data for lightweight models.
  - Pre-training vs. transfer: Training from scratch with balanced data is more effective than transfer training for bilingual alignment.
- Failure signatures:
  - Overfitting on small, high-quality datasets.
  - Forgetting of general knowledge after domain-specific fine-tuning.
  - Poor bilingual alignment if data proportions are imbalanced.
- First 3 experiments:
  1. Pre-train a small model on monolingual data, then transfer to bilingual data; compare performance to a model trained from scratch on balanced bilingual data.
  2. Fine-tune a lightweight model on a large, diverse instruction dataset vs. a smaller, high-quality subset; measure performance on downstream tasks.
  3. Apply entropy-based filtering to select instruction data; fine-tune and evaluate zero-shot and few-shot performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of Chinese data affect the English capabilities of MindLLM models?
- Basis in paper: [explicit] The paper demonstrates that MindLLM-1.3B, trained on both Chinese and English data, outperforms GPT-Neo-1.3B, trained only on English data, on English tasks.
- Why unresolved: While the paper shows a performance improvement, the specific mechanisms by which Chinese data enhances English capabilities are not fully explored.
- What evidence would resolve it: Detailed analysis of the model's internal representations or ablation studies comparing models trained on different proportions of Chinese data could shed light on the underlying mechanisms.

### Open Question 2
- Question: What is the optimal approach for incorporating code data to enhance reasoning capabilities in lightweight LLMs?
- Basis in paper: [explicit] The paper mentions that MOSS-Base-16B, initialized with CodeGen, does not show notably strong reasoning skills, potentially due to catastrophic forgetting or knowledge fusion challenges.
- Why unresolved: The paper does not explore different strategies for incorporating code data or investigate the optimal timing and method for its inclusion.
- What evidence would resolve it: Experiments comparing different approaches for incorporating code data, such as gradual integration or targeted fine-tuning, could identify the most effective strategy.

### Open Question 3
- Question: How does data entropy influence the performance of lightweight LLMs in different instruction tuning scenarios?
- Basis in paper: [explicit] The paper introduces an entropy-based quality filtering strategy and demonstrates its effectiveness in selecting high-quality instruction tuning data for lightweight models.
- Why unresolved: While the paper identifies an optimal range of data entropy for instruction tuning, the specific relationship between data entropy and model performance in different scenarios (e.g., zero-shot vs. few-shot) is not fully explored.
- What evidence would resolve it: Further experiments comparing model performance on different data entropy ranges in various instruction tuning scenarios could provide a more comprehensive understanding of the relationship.

## Limitations
- The exact composition of the Chinese pre-training data is not fully specified, limiting assessment of training corpus representativeness.
- The entropy-based quality filtering approach lacks detailed implementation specifics for direct replication.
- Evaluation focuses primarily on Chinese-centric benchmarks and domains, potentially limiting generalizability to other languages or domains.

## Confidence
- High confidence: The core finding that high-quality instruction data is more important than quantity for lightweight models.
- Medium confidence: The effectiveness of training from scratch with balanced bilingual data versus transfer training.
- Low confidence: The domain-specific performance claims in law and finance due to limited benchmark scope.

## Next Checks
1. Conduct a detailed analysis of the Chinese pre-training data composition, including source distribution, quality metrics, and potential biases.
2. Replicate the entropy-based quality filtering approach using publicly available datasets to validate its effectiveness.
3. Evaluate the lightweight models on additional domains beyond law and finance, such as healthcare or education, to test adaptability and identify potential limitations.