---
ver: rpa2
title: 'Poisoning $\times$ Evasion: Symbiotic Adversarial Robustness for Graph Neural
  Networks'
arxiv_id: '2312.05502'
source_url: https://arxiv.org/abs/2312.05502
tags:
- attacks
- evasion
- attack
- poisoning
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores a novel adversarial threat model combining poisoning
  and evasion attacks for Graph Neural Networks (GNNs). The symbiotic attack jointly
  optimizes poisoning and evasion objectives to degrade node classification accuracy.
---

# Poisoning $\times$ Evasion: Symbiotic Adversarial Robustness for Graph Neural Networks

## Quick Facts
- arXiv ID: 2312.05502
- Source URL: https://arxiv.org/abs/2312.05502
- Reference count: 5
- Key outcome: Symbiotic attacks combining poisoning and evasion outperform pure attacks, especially on large graphs

## Executive Summary
This work explores a novel adversarial threat model combining poisoning and evasion attacks for Graph Neural Networks (GNNs). The symbiotic attack jointly optimizes poisoning and evasion objectives to degrade node classification accuracy. Building on the memory-efficient PR-BCD attack, two approaches are proposed: sequential (budget split) and joint (integrated evasion within poisoning). Evaluations on Cora, CiteSeer, and PubMed datasets show symbiotic attacks consistently outperform pure poisoning, especially on larger graphs like PubMed, where accuracy drops close to zero. The symbiotic model proves more robust to test set size than evasion alone, highlighting its effectiveness. Results emphasize the need for further research into adversarial robustness under combined attack scenarios.

## Method Summary
The paper proposes a symbiotic adversarial attack framework for GNNs that combines poisoning (modifying training graph) and evasion (modifying test graph) attacks. Building on the PR-BCD attack, which uses randomized block coordinate descent for memory-efficient edge perturbations, the authors introduce two symbiotic variants: sequential (budget split between phases) and joint (integrated evasion within poisoning iterations). The attack optimizes a bi-level objective where the outer optimization modifies graph structure and the inner optimization trains the GNN model. The framework maintains a sparse probability matrix P over edge perturbations, updating it through block sampling and gradient-based optimization while respecting a global budget constraint.

## Key Results
- Symbiotic attacks achieve significantly lower accuracy than pure poisoning or evasion attacks
- On PubMed dataset, symbiotic attacks reduce accuracy to near zero compared to ~20% for poisoning alone
- Symbiotic attacks are more robust to test set size variations than evasion-only attacks
- Sequential symbiotic attacks perform consistently well across different GNN architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining poisoning and evasion attacks in a symbiotic threat model yields stronger adversarial impact than either attack alone.
- Mechanism: Poisoning manipulates the training graph to degrade the model's overall accuracy and makes it more vulnerable to future evasion attacks. The evasion attack then exploits this weakened state to further reduce accuracy on the test set.
- Core assumption: The model's learned representations are sufficiently dependent on graph structure that manipulating edges during training and testing has compounding effects.
- Evidence anchors:
  - [abstract]: "combining both threat models can substantially improve the devastating efficacy of adversarial attacks"
  - [section 3]: "poisoning attack not only reduces the model's accuracy but also makes it more vulnerable to evasion"
  - [corpus]: Weak evidence; related papers discuss robustness questions but not specifically symbiotic attack synergy.
- Break condition: If the model is inherently robust to structural perturbations or if the poisoning changes do not meaningfully degrade the model's capacity to learn discriminative features.

### Mechanism 2
- Claim: The symbiotic attack is less sensitive to the number of test nodes compared to pure evasion attacks.
- Mechanism: Poisoning manipulates the graph during training, affecting the model's global structure and node representations. This global degradation persists regardless of test set size, while evasion only affects a limited number of nodes.
- Core assumption: Poisoning changes influence the entire model's learning process, not just a subset of nodes.
- Evidence anchors:
  - [section 4.3]: "poisoning and symbiotic attacks become more difficult as well with more test nodes, especially on PubMed they are much more robust than the evasion attack"
  - [section 4.3]: "poisoning helps in two ways: by reducing the base accuracy evasion starts with, and possibly changing the structure of the graph in a more effective way to block the flow of label information"
  - [corpus]: No direct evidence in corpus for this mechanism; only general robustness papers.
- Break condition: If the model can recover from poisoning through regularization or if the training set is large enough to dilute poisoning effects.

### Mechanism 3
- Claim: Using PR-BCD for symbiotic attacks enables scalability to large graphs by maintaining a sparse probability matrix.
- Mechanism: PR-BCD updates blocks of the adjacency probability matrix rather than the entire matrix, and keeps promising entries while resampling others. This reduces memory usage and computational cost.
- Core assumption: Block coordinate descent with survival-of-the-fittest resampling is sufficient to find adversarial perturbations in the joint poisoning-evasion optimization space.
- Evidence anchors:
  - [section 2.1.1]: "PR-BCD employs Randomized Block Coordinate Descent (R-BCD) and updates a block of size b of P in each iteration"
  - [section 3]: "we choose to build on PR-BCD since it scales well to larger graphs"
  - [corpus]: No direct evidence in corpus about PR-BCD; only general robustness papers.
- Break condition: If the block size is too small to explore the perturbation space effectively or if the resampling strategy discards useful perturbations prematurely.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their vulnerability to adversarial attacks
  - Why needed here: The paper studies adversarial robustness of GNNs under combined poisoning and evasion threats.
  - Quick check question: What are the two main types of adversarial attacks on GNNs, and how do they differ in timing and objective?

- Concept: Bi-level optimization in adversarial machine learning
  - Why needed here: The symbiotic attack requires solving an optimization problem where the outer objective depends on an inner optimization (training the model).
  - Quick check question: In the symbiotic attack formulation, what are the two levels of optimization, and how are they connected?

- Concept: Randomized Block Coordinate Descent (R-BCD) and its application to graph perturbations
  - Why needed here: PR-BCD, the base attack method, uses R-BCD to efficiently update the adjacency probability matrix.
  - Quick check question: How does R-BCD differ from standard gradient descent in terms of computational efficiency for large graphs?

## Architecture Onboarding

- Component map: Graph data (A, X) -> PR-BCD-based symbiotic attack -> Perturbed graph -> GNN model -> Accuracy metric

- Critical path:
  1. Initialize probability matrix P with P = A
  2. For each iteration:
     - Sample a block of size b from P
     - Perform evasion attack within poisoning iteration (joint) or separately (sequential)
     - Update P using gradients of combined objective
     - Project P to enforce budget constraints
     - Resample non-promising entries
  3. Sample final perturbed graph from P and evaluate

- Design tradeoffs:
  - Block size b: Larger blocks cover more of the graph but increase per-iteration cost
  - Number of inner evasion iterations: More iterations may improve joint attack but increase runtime
  - Budget allocation: How to split budget between poisoning and evasion phases

- Failure signatures:
  - Accuracy does not drop significantly: Check if block size is too small or budget is insufficient
  - Training instability: Verify gradient computation and projection steps are correct
  - Memory issues: Ensure P remains sparse and block sampling is efficient

- First 3 experiments:
  1. Run evasion-only attack on a small graph (Cora) to verify baseline PR-BCD implementation
  2. Run poisoning-only attack to confirm meta-gradient computation works
  3. Implement sequential symbiotic attack and compare with individual attacks on the same budget

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal strategy for allocating the budget between poisoning and evasion in symbiotic attacks?
- Basis in paper: [explicit] The paper discusses sequential attacks that split the budget and joint attacks that integrate evasion within poisoning, but does not determine optimal allocation strategies.
- Why unresolved: The paper evaluates fixed budget splits (e.g., 50-50) without exploring the sensitivity of attack effectiveness to different allocation strategies.
- What evidence would resolve it: Experiments varying budget allocation ratios and analyzing their impact on attack success across different graph sizes and datasets.

### Open Question 2
- Question: How does the size of the test set affect the relative effectiveness of symbiotic attacks versus pure poisoning attacks?
- Basis in paper: [explicit] The paper notes that evasion attacks are constrained by test set size, while symbiotic attacks are less affected, but does not systematically quantify this relationship.
- Why unresolved: The analysis focuses on a fixed test set size (10%) and does not explore how varying test set sizes influence the trade-off between poisoning and symbiotic attack effectiveness.
- What evidence would resolve it: Detailed experiments measuring attack performance across a range of test set sizes to determine the threshold where symbiotic attacks become more effective than pure poisoning.

### Open Question 3
- Question: What are the optimal hyperparameters (e.g., block size, inner evasion iterations) for symbiotic attacks on large graphs?
- Basis in paper: [explicit] The paper evaluates different block sizes and inner evasion iterations but does not determine optimal settings, especially for large graphs like PubMed.
- Why unresolved: The paper presents results for fixed hyperparameters (e.g., block size, inner iterations) without exploring the sensitivity of attack effectiveness to these parameters or their interaction.
- What evidence would resolve it: Systematic hyperparameter tuning experiments to identify optimal settings for different graph sizes and model architectures, potentially using techniques like Bayesian optimization.

## Limitations

- Computational complexity remains high for very large graphs despite PR-BCD optimization
- The symbiotic attack requires careful budget allocation between poisoning and evasion phases
- Limited analysis of transferability of symbiotic attacks across different GNN architectures

## Confidence

- **High**: Sequential attack consistently outperforms individual attacks across datasets
- **Medium**: Joint attack provides additional gains but implementation complexity increases
- **Low**: Claims about scalability to graphs much larger than PubMed are untested

## Next Checks

1. **Ablation study on block size**: Test whether improvements scale with larger b or plateau, confirming the memory-efficiency claims
2. **Cross-model robustness**: Apply symbiotic attacks to models not trained with adversarial defenses to test generality of poisoning-vulnerability
3. **Budget sensitivity analysis**: Systematically vary total budget to determine if observed effects are due to absolute budget or budget allocation strategy