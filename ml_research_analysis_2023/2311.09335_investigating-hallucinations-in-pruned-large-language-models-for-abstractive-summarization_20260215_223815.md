---
ver: rpa2
title: Investigating Hallucinations in Pruned Large Language Models for Abstractive
  Summarization
arxiv_id: '2311.09335'
source_url: https://arxiv.org/abs/2311.09335
tags:
- pruned
- pruning
- association
- sparsegpt
- wanda
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of model pruning on hallucinations
  in abstractive summarization using large language models (LLMs). The authors conduct
  an extensive empirical study across five summarization datasets, two state-of-the-art
  pruning methods (SparseGPT and Wanda), and five instruction-tuned LLMs.
---

# Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization

## Quick Facts
- **arXiv ID**: 2311.09335
- **Source URL**: https://arxiv.org/abs/2311.09335
- **Authors**: [List of authors]
- **Reference count**: 24
- **Primary result**: Pruned LLMs hallucinate less than full-sized models, with increased lexical overlap to source documents as a likely mechanism

## Executive Summary
This paper investigates the impact of model pruning on hallucinations in abstractive summarization using large language models. Through extensive empirical studies across five summarization datasets, two pruning methods (SparseGPT and Wanda), and five instruction-tuned LLMs, the authors find that pruned models consistently hallucinate less than their full-sized counterparts. The analysis reveals that pruned models show increased lexical overlap with source documents, which correlates with reduced hallucination risk. SparseGPT demonstrates more consistent hallucination reduction compared to Wanda across different sparsity levels.

## Method Summary
The authors conduct an empirical study using five summarization datasets (FactCC, Polytope, and three from SummEval) with three instruction-tuned LLMs (Llama 2 7B/13B, Falcon 7B). They apply two state-of-the-art pruning methods - SparseGPT (iterative weight update) and Wanda (single-pass pruning) - at 50% sparsity. Models are fine-tuned using greedy decoding with three different prompts, and their summaries are evaluated using ROUGE-1/2/L, BERTScore for quality, and three hallucination metrics (HaRiM, SummaCConv, SummaCZS). The study systematically compares full vs pruned models across all combinations.

## Key Results
- Pruned LLMs hallucinate significantly less than full-sized models across all tested datasets
- SparseGPT shows more consistent hallucination reduction than Wanda, with better results in 26 out of 27 comparisons
- Increased lexical overlap between generated summaries and source documents correlates with reduced hallucination risk
- Model performance (ROUGE, BERTScore) remains comparable between pruned and full-sized models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pruning removes redundant parameters, reducing model reliance on parametric knowledge and forcing increased dependence on input text.
- Mechanism: When parameters are pruned, the model loses some learned knowledge from pre-training. To compensate, it must rely more heavily on the source document during generation, increasing lexical overlap between summary and input.
- Core assumption: The pruned model cannot fully rely on its parametric knowledge to generate summaries and must use the source document more.
- Evidence anchors:
  - [abstract] "Our analysis suggests that pruned models tend to depend more on the source document for summary generation."
  - [section 4.3] "We hypothesize that by removing unused parameters, we potentially remove some of the model's parametric knowledge... This perhaps 'forces' the model to rely more on the source document during the summary generation and in turn reduces hallucinations."
  - [corpus] Weak - only general pruning literature found, no specific mechanism evidence.
- Break condition: If pruned models maintain hallucinatory behavior despite increased lexical overlap, or if pruning removes critical parameters needed for faithful generation.

### Mechanism 2
- Claim: Increased lexical overlap between summary and source document reduces hallucination risk.
- Mechanism: Summaries that are more lexically similar to the source document are less likely to contain hallucinated content not present in the input.
- Core assumption: Hallucinations are less likely when the model generates text that closely matches the source document's wording.
- Evidence anchors:
  - [abstract] "This leads to a higher lexical overlap between the generated summary and the source document, which could be a reason for the reduction in hallucination risk."
  - [section 5] "Our results show that there are strong correlation signals... That is, generated summaries with greater lexical overlap with their source documents... are less likely to contain hallucinations."
  - [corpus] Weak - only general summarization literature found, no specific lexical-overlap hallucination evidence.
- Break condition: If high lexical overlap does not correlate with reduced hallucinations in other contexts, or if models learn to generate hallucinated content while maintaining lexical overlap.

### Mechanism 3
- Claim: SparseGPT pruning is more consistent than Wanda for reducing hallucinations.
- Mechanism: SparseGPT updates weights during pruning, potentially leading to more stable model behavior that consistently reduces hallucinations.
- Core assumption: The weight update process in SparseGPT creates more robust sparse models than Wanda's single-pass approach.
- Evidence anchors:
  - [section 4.3] "Results suggest that SparseGPT in particular is more consistent compared to Wanda, recording significantly better results compared to the full model in 26 out of 27 comparisons."
  - [section 3.2] "SparseGPT introduces an efficient solution for layer-wise compression... relying upon an iterative weight update process using Hessian inverses."
  - [corpus] Weak - only general pruning literature found, no specific SparseGPT vs Wanda hallucination evidence.
- Break condition: If Wanda or other pruning methods show comparable or better hallucination reduction with different sparsity levels or datasets.

## Foundational Learning

- Concept: Pruning and its effects on model performance
  - Why needed here: The paper relies on understanding how different pruning methods (SparseGPT, Wanda) affect both model performance and hallucination rates.
  - Quick check question: What is the difference between structured and unstructured pruning, and how might each affect model behavior differently?

- Concept: Hallucination evaluation metrics
  - Why needed here: The paper uses three different hallucination metrics (HaRiM, SummaCConv, SummaCZS) that each measure hallucinations differently.
  - Quick check question: How do entailment-based, question-answering, and text-generation hallucination metrics differ in their approach to detecting hallucinations?

- Concept: Lexical overlap and its relationship to faithfulness
  - Why needed here: The paper correlates increased lexical overlap with reduced hallucinations, requiring understanding of what lexical overlap measures and why it might indicate faithfulness.
  - Quick check question: Why might a summary with high lexical overlap to its source document be less likely to contain hallucinations?

## Architecture Onboarding

- Component map: Source document → Pruned model → Generated summary → Hallucination evaluation metrics → Correlation analysis with lexical overlap metrics
- Critical path: Source document → Pruned model → Generated summary → Hallucination evaluation metrics → Correlation analysis with lexical overlap metrics
- Design tradeoffs: Using only greedy decoding for reproducibility versus more sophisticated decoding strategies; focusing on 50% sparsity versus exploring a wider range; using instruction-tuned models versus base models.
- Failure signatures: If pruned models show degraded performance despite claims of comparable quality; if hallucination metrics show inconsistent results across datasets; if lexical overlap doesn't correlate with hallucination reduction as claimed.
- First 3 experiments:
  1. Replicate the pruning process on Llama 7B with both SparseGPT and Wanda at 50% sparsity, verifying perplexity scores match expectations
  2. Generate summaries on FactCC dataset using both pruned and full models, measuring ROUGE and hallucination metrics
  3. Conduct correlation analysis between ROUGE scores and hallucination metrics across different sparsity levels to verify the claimed relationship

## Open Questions the Paper Calls Out
The paper suggests exploring the relationship between hallucination prevalence and model pruning in other tasks like open-book question answering and machine translation.

## Limitations
- The study focuses on 50% sparsity and instruction-tuned models, limiting generalizability to other sparsity levels and model types
- The causal mechanism linking pruning to hallucination reduction remains unproven, with only correlational evidence provided
- The analysis uses only greedy decoding, which may not reflect optimal performance achievable with more sophisticated decoding strategies

## Confidence

**High confidence**: Pruned models can maintain comparable performance to full-sized models while reducing hallucinations. This is supported by multiple datasets, models, and evaluation metrics showing consistent patterns across the study.

**Medium confidence**: Increased lexical overlap correlates with reduced hallucinations in pruned models. While the correlation is statistically significant, the causal mechanism remains unproven, and the relationship may not hold across all summarization scenarios.

**Low confidence**: SparseGPT is inherently superior to Wanda for hallucination reduction. The comparison is limited to a single sparsity level, and the weight update mechanism's specific contribution to hallucination reduction is not conclusively demonstrated.

## Next Checks

1. **Causal mechanism validation**: Design an ablation study that controls for lexical overlap while varying pruning levels to isolate whether increased source dependence or some other factor drives hallucination reduction in pruned models.

2. **Generalization testing**: Extend the analysis to additional sparsity levels (25%, 75%) and different model architectures (base models, non-instruction-tuned variants) to determine if the hallucination reduction pattern holds across a broader parameter space.

3. **Alternative decoding comparison**: Repeat key experiments using beam search and sampling-based decoding strategies to verify that the observed hallucination reduction pattern persists beyond greedy decoding, establishing whether the effect is robust to decoding choices.