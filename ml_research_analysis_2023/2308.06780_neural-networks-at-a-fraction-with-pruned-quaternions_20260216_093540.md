---
ver: rpa2
title: Neural Networks at a Fraction with Pruned Quaternions
arxiv_id: '2308.06780'
source_url: https://arxiv.org/abs/2308.06780
tags:
- quaternion
- pruning
- networks
- neural
- real
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Neural networks with high parameter counts are difficult to deploy
  on resource-constrained devices. Pruning can reduce parameter counts, but at very
  high sparsities, pruned real networks often lose accuracy.
---

# Neural Networks at a Fraction with Pruned Quaternions

## Quick Facts
- arXiv ID: 2308.06780
- Source URL: https://arxiv.org/abs/2308.06780
- Reference count: 40
- Key outcome: Quaternion neural networks can achieve 10%+ better accuracy than pruned real networks at 3% of original parameter count on certain architectures

## Executive Summary
This paper investigates whether quaternion-valued neural networks can outperform pruned real networks at extreme sparsities. Quaternion networks offer 75% parameter reduction through Hamilton product weight sharing, and experiments on image classification tasks show that for some architectures, quaternion models significantly outperform pruned real models at very high sparsity levels. The results suggest quaternion networks may be preferable to sparse real networks for deployment on extremely resource-constrained devices.

## Method Summary
The paper compares real and quaternion-valued neural networks across several architectures (Lenet-300-100, Conv-2/4/6) on MNIST, CIFAR-10, and CIFAR-100 datasets. Quaternion networks use split activation functions and Hamilton product operations. Both network types undergo iterative global pruning (20% weight removal per iteration) followed by retraining from scratch using initial weights. Performance is evaluated across sparsity levels to identify when quaternion networks provide advantages over pruned real networks.

## Key Results
- Quaternion models achieve >10% better accuracy than pruned real models at 3% of original parameter count for certain architectures
- Quaternion networks provide 75% parameter reduction through Hamilton product weight sharing
- Pruned quaternion models can be re-trained from scratch to match unpruned accuracy (extending Lottery Ticket Hypothesis to quaternions)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quaternions reduce parameter count by 75% due to Hamilton product weight sharing.
- Mechanism: When representing a fully connected layer with four real neurons as a single quaternion neuron, the Hamilton product requires only one quaternion weight (4 real weights) instead of 16 real weights. This weight sharing across the four components of the quaternion reduces the total parameter count by 75%.
- Core assumption: The network architecture has dimensions divisible by 4, allowing for clean quaternion decomposition.
- Evidence anchors:
  - [abstract] "using higher-dimensional data embeddings such as complex numbers or quaternions has been shown to reduce the parameter count while maintaining accuracy"
  - [section] "Using the Hamilton product, we would only require one quaternion weight, or four real weights, to connect them. Thus provided the number of neurons in all layers are divisible by 4, we can obtain a 75% reduction in the number of parameters"
  - [corpus] Weak evidence - corpus neighbors don't directly address the Hamilton product weight sharing mechanism
- Break condition: If network dimensions are not divisible by 4, or if quaternion operations introduce overhead that negates parameter savings.

### Mechanism 2
- Claim: At extreme sparsities, quaternion models outperform pruned real models due to better preservation of representational capacity.
- Mechanism: As pruning removes weights, quaternion models maintain better performance because the remaining quaternion weights encode richer relationships across the four components, while real models lose critical information more rapidly when pruned.
- Core assumption: The representational power of quaternions provides advantages specifically in the high-sparsity regime where real models degrade significantly.
- Evidence anchors:
  - [abstract] "at very high sparsity levels, quaternion models provide higher accuracies than their real counterparts"
  - [section] "at extreme model sparsities, quaternion models perform better than their real counterparts"
  - [corpus] No direct evidence - corpus neighbors don't discuss sparsity advantages
- Break condition: If pruning methods are optimized differently for real vs quaternion models, or if the advantage disappears at moderate sparsity levels.

### Mechanism 3
- Claim: The Lottery Ticket Hypothesis applies to quaternion networks, enabling re-trainable pruned models.
- Mechanism: Pruned quaternion networks can be re-trained from scratch using the initial weights and still match or exceed the accuracy of unpruned models, similar to real-valued networks.
- Core assumption: The initialization patterns that work for real networks also work for quaternion networks when using appropriate backpropagation algorithms.
- Evidence anchors:
  - [abstract] "pruned quaternion models can be re-trained from scratch to the same accuracy as the unpruned model"
  - [section] "just like R, pruned Q are capable of matching or exceeding the accuracy of the original, unpruned model"
  - [corpus] No direct evidence - corpus neighbors don't discuss lottery ticket hypothesis for quaternions
- Break condition: If quaternion-specific initialization or training procedures are required that differ fundamentally from real networks.

## Foundational Learning

- Concept: Quaternion algebra and Hamilton product
  - Why needed here: Understanding how quaternions encode 4D information and how their multiplication works is essential for grasping the parameter reduction mechanism
  - Quick check question: How many real weights are needed to represent a quaternion weight connecting 4 input and 4 output neurons?

- Concept: Neural network pruning techniques
  - Why needed here: The paper's core contribution relies on comparing pruning performance between real and quaternion networks
  - Quick check question: What is the difference between structured and unstructured pruning, and why might unstructured pruning be preferred in this context?

- Concept: Lottery Ticket Hypothesis
  - Why needed here: The paper demonstrates that this hypothesis extends to quaternion networks, which is a key finding
  - Quick check question: What are the requirements for a pruned network to be considered a "winning ticket" that can be re-trained from scratch?

## Architecture Onboarding

- Component map: Real-valued and quaternion-valued neural network implementations -> Pruning algorithms (20% weight removal per iteration) -> Training pipelines (Adam optimizer) -> Evaluation metrics (classification accuracy)

- Critical path: 1) Build real and quaternion network architectures with equivalent neuron counts, 2) Train both to establish baseline performance, 3) Apply iterative global pruning with 20% weight removal per iteration, 4) Re-train pruned models from scratch using initial weights, 5) Compare accuracy vs sparsity curves for both implementations.

- Design tradeoffs: Quaternion networks offer 75% parameter reduction but may have lower initial accuracy and require quaternion-specific operations. Real networks have higher parameter counts but may train more easily. The choice depends on whether the deployment scenario prioritizes parameter count reduction or ease of training.

- Failure signatures: If quaternion networks consistently underperform real networks across all sparsity levels, or if pruned quaternion models cannot be re-trained from scratch to match original accuracy, the approach may not be viable. Also, if quaternion operations introduce prohibitive computational overhead.

- First 3 experiments: 1) Implement a simple Conv-4 architecture for CIFAR-10 with both real and quaternion versions, verify the 75% parameter reduction, 2) Apply iterative pruning to both versions and plot accuracy vs sparsity curves, 3) Test the lottery ticket hypothesis by re-training pruned quaternion models from scratch using initial weights.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do quaternion models outperform real models at extreme sparsities for certain architectures, but not for others like LeNet-300-100 on MNIST or deeper architectures like ResNet and VGG?
- Basis in paper: [explicit] The paper states that quaternion models outperform real models at extreme sparsities for some architectures, but not for others like LeNet-300-100 on MNIST or deeper architectures like ResNet and VGG.
- Why unresolved: The paper does not provide a clear explanation for why this difference in performance exists between different architectures.
- What evidence would resolve it: Further experimental analysis comparing the performance of quaternion and real models across a wider range of architectures and tasks could provide insights into the factors that influence the performance difference.

### Open Question 2
- Question: How does the use of batch normalization layers in deeper architectures like ResNet and VGG affect the performance of quaternion models compared to real models?
- Basis in paper: [explicit] The paper mentions that the performance difference between quaternion and real models in deeper architectures like ResNet and VGG could be due to the introduction of batch normalization layers.
- Why unresolved: The paper does not provide a detailed analysis of how batch normalization layers affect the performance of quaternion models.
- What evidence would resolve it: Further experiments comparing the performance of quaternion and real models in architectures with and without batch normalization layers could provide insights into the impact of batch normalization on quaternion model performance.

### Open Question 3
- Question: What is the impact of using different quaternion representations, such as the split activation function, on the performance of quaternion models compared to real models?
- Basis in paper: [explicit] The paper mentions the use of the split activation function for quaternion models, but does not explore the impact of different quaternion representations on model performance.
- Why unresolved: The paper does not provide a comprehensive analysis of how different quaternion representations affect model performance.
- What evidence would resolve it: Further experiments comparing the performance of quaternion models using different quaternion representations could provide insights into the impact of representation choice on model performance.

## Limitations
- Experimental scope limited to specific architectures (Lenet-300-100, Conv-2/4/6) and datasets (MNIST, CIFAR-10/100)
- Choice of 20% pruning per iteration may not represent optimal pruning schedules
- Computational overhead of quaternion operations compared to real-valued networks is not quantified

## Confidence
- **High confidence**: The 75% parameter reduction claim through quaternion representation is well-established and mathematically sound
- **Medium confidence**: The superior performance of quaternion models at extreme sparsities (3% parameter count) is demonstrated but only for specific architectures and datasets
- **Medium confidence**: The extension of the Lottery Ticket Hypothesis to quaternion networks is supported by experiments but lacks deeper analysis

## Next Checks
1. Replicate experiments on deeper architectures (ResNet-18, VGG-16) to verify if quaternion advantages persist beyond shallow networks
2. Compare different pruning schedules (e.g., 10% vs 20% per iteration) to determine if the observed quaternion advantage is robust to pruning methodology
3. Benchmark computational efficiency by measuring actual inference time and FLOPs for both real and quaternion models at equivalent sparsity levels to assess practical deployment trade-offs