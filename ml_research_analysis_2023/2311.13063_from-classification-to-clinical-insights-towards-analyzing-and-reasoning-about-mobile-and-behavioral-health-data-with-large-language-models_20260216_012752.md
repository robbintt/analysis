---
ver: rpa2
title: 'From Classification to Clinical Insights: Towards Analyzing and Reasoning
  About Mobile and Behavioral Health Data With Large Language Models'
arxiv_id: '2311.13063'
source_url: https://arxiv.org/abs/2311.13063
tags:
- data
- health
- mental
- reasoning
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using large language models (LLMs) to analyze
  mobile and wearable sensor data for mental health insights. The authors develop
  prompting strategies for LLMs to classify depression and generate reasoning about
  how sensor trends relate to mental health.
---

# From Classification to Clinical Insights: Towards Analyzing and Reasoning About Mobile and Behavioral Health Data With Large Language Models

## Quick Facts
- arXiv ID: 2311.13063
- Source URL: https://arxiv.org/abs/2311.13063
- Reference count: 40
- Primary result: LLM chain-of-thought prompting achieves 61.1% depression classification accuracy and generates clinically-valued reasoning insights

## Executive Summary
This paper explores using large language models (LLMs) to analyze mobile and wearable sensor data for mental health insights. The authors develop prompting strategies for LLMs to classify depression and generate reasoning about how sensor trends relate to mental health. On depression classification, their best LLM approach achieves 61.1% accuracy, exceeding prior state-of-the-art. More importantly, they find that clinicians highly value the LLM's ability to generate clinically-relevant reasoning and insights from sensor data. In user studies, clinicians strongly agreed that LLM-generated insights would help them interpret self-tracking data and enhance treatment. This demonstrates the potential for a human-AI collaborative approach where clinicians interactively query LLMs to gain insights that support clinical decision-making.

## Method Summary
The authors use chain-of-thought prompting with commercial LLMs (GPT-4, PaLM 2, GPT-3.5) to analyze multi-sensor time series data (steps, sleep, phone usage, etc.) from the GLOBEM dataset. They format sensor data into markdown and apply prompting strategies that guide the LLM to generate reasoning about mental health conditions. The approach is evaluated through classification accuracy on depression detection and qualitative assessment by mental health clinicians on the value of generated insights for clinical decision-making.

## Key Results
- LLM chain-of-thought approach achieves 61.1% accuracy on depression classification, exceeding prior state-of-the-art
- Clinicians strongly agree LLM-generated insights would help interpret self-tracking data and enhance treatment
- Chain-of-thought prompting outperforms direct prompting and traditional ML baselines on the same feature set

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-thought prompting enables LLMs to perform mental health classification by synthesizing multi-sensor data into clinically-relevant reasoning.
- Mechanism: LLMs generate intermediate reasoning steps that connect sensor trends (e.g., sleep patterns, activity levels) to mental health concepts, producing outputs that are more interpretable and potentially more accurate than direct classification.
- Core assumption: The intermediate reasoning step adds value beyond direct classification and can be validated by clinicians.
- Evidence anchors:
  - [abstract] "We develop chain of thought prompting methods that use LLMs to generate reasoning about how trends in data such as step count and sleep relate to conditions like depression and anxiety."
  - [section 4.3] "By CoT + Exp. method, PaLM2 achieved the highest accuracy of 61.11%."

### Mechanism 2
- Claim: Clinicians value LLM-generated insights more than binary classifications for supporting clinical decision-making.
- Mechanism: LLMs provide natural language summaries of sensor data trends and anomalies, which clinicians can interpret in context of patient history and expertise, fostering a collaborative human-AI approach.
- Core assumption: Clinicians prefer interpretable insights over black-box classifications, especially when they can validate the reasoning.
- Evidence anchors:
  - [abstract] "clinician participants express strong interest in using this approach to interpret self-tracking data."
  - [section 6] "Therapists generally found GPT-4's analyses plausible, although at times too far a stretch or too limited in the range of issues that were considered."

### Mechanism 3
- Claim: LLMs can generalize across different input data formats and sensor types better than traditional ML models.
- Mechanism: LLMs process natural language descriptions of sensor data, allowing them to handle diverse data formats (CSV, Markdown, etc.) and sensor types without retraining for each new device or data source.
- Core assumption: LLMs' training on diverse internet text enables them to understand and process various data formats effectively.
- Evidence anchors:
  - [section 4.2.1] "Markdown shows the best overall performance, we chose Markdown as the data format for our subsequent experiments."
  - [section 4.3.1] "Notably, among the three baseline models utilizing the same set of 16 features, none could surpass the zero-shot Chain of Thought (CoT) results achieved using the GPT-4 and PaLM2 models."

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: Enables LLMs to break down complex reasoning tasks into intermediate steps, improving accuracy on mental health classification from sensor data.
  - Quick check question: How does chain-of-thought prompting differ from direct prompting, and why might it be more effective for this task?

- Concept: Multi-modal data processing
  - Why needed here: The study involves integrating diverse sensor data types (step count, sleep, phone usage) to generate insights about mental health.
  - Quick check question: What are the challenges in processing multi-modal sensor data for mental health applications, and how does the LLM approach address them?

- Concept: Clinical decision support vs. diagnosis
  - Why needed here: The study focuses on providing insights to support clinician decision-making rather than making definitive diagnoses.
  - Quick check question: Why is the distinction between decision support and diagnosis important in the context of AI applications in mental health?

## Architecture Onboarding

- Component map: Data preprocessing -> LLM core (chain-of-thought prompting) -> Output processing -> User interface
- Critical path: Collect and preprocess multi-sensor data -> Input formatted data into LLM with chain-of-thought prompt -> Generate reasoning and classification output -> Present insights to clinician for interpretation and decision-making
- Design tradeoffs:
  - Accuracy vs. interpretability: Direct classification may be more accurate but less interpretable than chain-of-thought reasoning
  - Privacy vs. utility: More detailed data improves insights but raises privacy concerns
  - Generalization vs. specificity: Broad model applicability vs. tailored insights for specific patient populations
- Failure signatures:
  - Consistently incorrect reasoning or classifications
  - Inability to handle new data formats or sensor types
  - Clinician distrust due to perceived lack of transparency or accuracy
- First 3 experiments:
  1. Compare classification accuracy of chain-of-thought vs. direct prompting on a held-out test set
  2. Evaluate clinician trust and preference for LLM-generated insights vs. traditional analysis methods
  3. Test model performance on diverse data formats and sensor types not seen during training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design LLM prompts to elicit clinically valid reasoning about multi-sensor data that considers factors beyond depression and anxiety?
- Basis in paper: [inferred] The paper discusses how clinicians want LLM reasoning to consider a broader range of factors and disorders beyond just depression and anxiety.
- Why unresolved: The study did not evaluate LLM reasoning across a diverse set of mental health conditions. The prompts used may have biased the models towards depression/anxiety interpretations.
- What evidence would resolve it: Evaluating LLM reasoning outputs across multiple mental health conditions to assess the breadth and validity of reasoning. Testing different prompt structures to elicit diverse clinical interpretations.

### Open Question 2
- Question: How can we ensure the privacy and security of sensitive patient data when using LLMs for clinical decision support?
- Basis in paper: [explicit] The paper discusses privacy concerns around inputting sensitive patient data like mood records and therapy session transcripts into LLMs.
- Why unresolved: The study did not explore technical solutions for secure data management and LLM hosting that would protect patient privacy while enabling clinician-patient collaboration.
- What evidence would resolve it: Evaluating different data hosting and access control strategies for LLMs in clinical settings. Testing the impact on patient privacy, data security, and collaborative use.

### Open Question 3
- Question: How can we design LLM-based tools that enhance rather than replace clinician expertise in mental health treatment?
- Basis in paper: [explicit] The paper discusses concerns about LLMs being used as shortcuts that could degrade therapy by replacing clinician expertise.
- Why unresolved: The study did not explore design approaches for LLM tools that augment rather than automate clinician decision-making. The optimal balance between AI assistance and human expertise is unclear.
- What evidence would resolve it: Evaluating different tool designs and use cases for LLM-based clinical decision support. Testing the impact on clinician expertise, patient outcomes, and the therapeutic relationship.

## Limitations

- Limited to a single dataset (GLOBEM), raising questions about generalizability across populations
- Qualitative clinical validation does not measure actual impact on treatment outcomes or potential harms
- Results depend heavily on commercial LLM APIs, raising reproducibility and long-term viability concerns

## Confidence

**High Confidence**:
- LLMs can generate interpretable reasoning about sensor data patterns
- Clinicians value natural language insights over binary classifications
- Chain-of-thought prompting improves model performance compared to direct prompting

**Medium Confidence**:
- 61.1% accuracy represents meaningful improvement over prior work
- The approach generalizes across different data formats and sensor types
- Interactive human-AI collaboration is preferred by clinicians

**Low Confidence**:
- Long-term clinical utility and safety of LLM-generated insights
- Performance on datasets outside the GLOBEM collection
- Model behavior with edge cases and unusual data patterns

## Next Checks

1. **Cross-dataset validation**: Test the prompting approach on at least two additional independent mental health sensor datasets to assess generalizability.

2. **Clinical outcome measurement**: Conduct a randomized controlled trial measuring whether LLM-assisted analysis improves treatment outcomes compared to standard clinical practice.

3. **Adversarial testing**: Systematically evaluate model performance on edge cases, including contradictory data patterns, missing values, and sensor anomalies to identify failure modes.