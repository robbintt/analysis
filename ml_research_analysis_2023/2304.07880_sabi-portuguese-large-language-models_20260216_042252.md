---
ver: rpa2
title: "Sabi\xE1: Portuguese Large Language Models"
arxiv_id: '2304.07880'
source_url: https://arxiv.org/abs/2304.07880
tags:
- language
- dataset
- pretraining
- arxiv
- portuguese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether monolingual pretraining on the
  target language can improve large language models already trained on diverse corpora.
  The authors further pretrain GPT-J and LLaMA models on Portuguese texts using 3%
  or less of their original pretraining budget.
---

# Sabiá: Portuguese Large Language Models

## Quick Facts
- arXiv ID: 2304.07880
- Source URL: https://arxiv.org/abs/2304.07880
- Authors: 
- Reference count: 40
- Key outcome: Models trained with continued monolingual pretraining on Portuguese outperform English-centric and multilingual counterparts on 14 Portuguese datasets

## Executive Summary
This paper investigates the benefits of monolingual pretraining for large language models, focusing on Portuguese as the target language. The authors further pretrain existing models (GPT-J and LLaMA) on a high-quality Portuguese corpus using minimal computational resources (3% or less of the original pretraining budget). Their approach yields significant improvements over English-centric and multilingual baselines on the Poeta benchmark, with their best model (Sabiá-65B) performing on par with GPT-3.5-turbo. The study provides evidence that domain-specific knowledge acquired through monolingual pretraining is the primary driver of these improvements, rather than just capturing linguistic nuances.

## Method Summary
The authors further pretrain GPT-J and LLaMA models on a filtered Portuguese corpus derived from the ClueWeb 2022 dataset, using approximately 3% of the original pretraining budget. They employ standard pretraining techniques with modified hyperparameters suitable for continued training. The models are then evaluated using few-shot learning on the Poeta benchmark, a suite of 14 Portuguese datasets. Performance is measured using the Normalized Preferred Metric (NPM), which aggregates results across multiple tasks. The study compares their monolingual models against multilingual and English-centric baselines to quantify the benefits of language-specific pretraining.

## Key Results
- Sabiá models outperform English-centric and multilingual counterparts on the Poeta benchmark by a significant margin
- Sabiá-65B achieves performance comparable to GPT-3.5-turbo
- Improvements are particularly pronounced for datasets created by Brazilian speakers
- The majority of benefits stem from domain-specific knowledge acquired through monolingual pretraining rather than just linguistic nuances

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continued monolingual pretraining significantly improves models already trained on diverse corpora.
- Mechanism: Injecting domain-specific knowledge about a particular culture through language-specific pretraining.
- Core assumption: The baseline models have gaps in their knowledge of the target language's cultural and domain-specific aspects.
- Evidence anchors:
  - [abstract] "Our results indicate that the majority of the benefits stem from the domain-specific knowledge acquired through monolingual pretraining."
  - [section] "We observe improvements across all datasets due to the Portuguese pretraining, with the gains being particularly pronounced for datasets created by Brazilian speakers."
  - [corpus] Weak evidence - the corpus doesn't provide direct evidence for this mechanism, but the paper's results support it.
- Break condition: If the target language corpus is too small or of poor quality to provide meaningful domain-specific knowledge.

### Mechanism 2
- Claim: Continued monolingual pretraining captures linguistic nuances and structures inherent to the target language.
- Mechanism: Fine-tuning the model's understanding of the target language's syntax, grammar, and idiomatic expressions.
- Core assumption: The baseline models, despite being multilingual, have suboptimal performance on the target language due to their exposure to diverse languages during pretraining.
- Evidence anchors:
  - [abstract] "By evaluating on datasets originally conceived in the target language as well as translated ones, we study the contributions of language-specific pretraining in terms of 1) capturing linguistic nuances and structures inherent to the target language."
  - [section] "We observe improvements across all datasets due to the Portuguese pretraining, with the gains being particularly pronounced for datasets created by Brazilian speakers."
  - [corpus] Weak evidence - the corpus doesn't provide direct evidence for this mechanism, but the paper's results support it.
- Break condition: If the target language is too similar to the languages in the baseline model's pretraining data, making further language-specific pretraining less beneficial.

### Mechanism 3
- Claim: Continued monolingual pretraining improves few-shot learning performance on the target language.
- Mechanism: Adapting the model's in-context learning capabilities to the target language's characteristics.
- Core assumption: The baseline models have suboptimal few-shot learning performance on the target language due to their exposure to diverse languages during pretraining.
- Evidence anchors:
  - [abstract] "Few-shot evaluations on Poeta, a suite of 14 Portuguese datasets, reveal that our models outperform English-centric and multilingual counterparts by a significant margin."
  - [section] "Our best model, Sabiá-65B, performs on par with GPT-3.5-turbo."
  - [corpus] Weak evidence - the corpus doesn't provide direct evidence for this mechanism, but the paper's results support it.
- Break condition: If the target language has significantly different few-shot learning requirements compared to the languages in the baseline model's pretraining data.

## Foundational Learning

- Concept: Language modeling and transformer architectures
  - Why needed here: The paper builds upon existing language models (GPT-J and LLaMA) and extends them through continued pretraining.
  - Quick check question: What are the key differences between the GPT-J and LLaMA architectures, and how do they impact the continued pretraining process?

- Concept: Multilingual and monolingual pretraining
  - Why needed here: The paper compares the performance of multilingual models (e.g., BLOOM, XGLM) with monolingual models (e.g., Sabiá) to demonstrate the benefits of language-specific pretraining.
  - Quick check question: What are the advantages and disadvantages of multilingual pretraining compared to monolingual pretraining, and how do they impact the model's performance on the target language?

- Concept: Few-shot learning and in-context learning
  - Why needed here: The paper evaluates the models using a few-shot learning approach, where the model is provided with a small number of examples to learn from before making predictions.
  - Quick check question: How does the few-shot learning performance of a language model depend on its pretraining data and architecture, and what are the key factors that influence its ability to learn from limited examples?

## Architecture Onboarding

- Component map: Base model (GPT-J or LLaMA) -> Portuguese pretraining corpus -> Training hyperparameters (optimizer, learning rate, batch size) -> Fine-tuned Sabiá model
- Critical path: (1) Obtain base model weights, (2) Prepare Portuguese pretraining corpus, (3) Set up training environment and hyperparameters, (4) Train model on target language corpus
- Design tradeoffs: Base model choice impacts capacity and computational requirements; pretraining corpus size affects language-specific knowledge capture; hyperparameters must balance convergence speed and performance
- Failure signatures: Overfitting to pretraining corpus, underfitting due to insufficient model capacity or training time, catastrophic forgetting of base model knowledge
- First 3 experiments:
  1. Train Sabiá-7B on Portuguese corpus to validate continued pretraining approach
  2. Compare Sabiá models with GPT-J vs. LLaMA base architectures on Poeta benchmark
  3. Vary pretraining corpus size to determine optimal amount for capturing language-specific knowledge

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the benefit of monolingual pretraining persist for languages that are more distant from English than Portuguese?
- Basis in paper: [explicit] The authors state "Portuguese and English exhibit relatively close linguistic proximity" and note this is an open research question.
- Why unresolved: The authors only conducted experiments with Portuguese due to financial constraints and the manual labor involved in collecting evaluation datasets.
- What evidence would resolve it: Conducting similar monolingual pretraining experiments with languages that have greater linguistic distance from English (e.g., Chinese, Arabic, Finnish) and comparing the results to the Portuguese case.

### Open Question 2
- Question: Do parameter-efficient methods (e.g., LoRA, adapters) provide comparable or better performance than full model pretraining for low-resource languages with limited quality text data?
- Basis in paper: [explicit] The authors mention "low-resource languages with limited availability of quality texts" and suggest parameter-efficient methods "could be advantageous" but did not experiment with them due to training costs.
- Why unresolved: The authors did not test parameter-efficient methods due to their computational cost being "approximately equivalent to training the entire model."
- What evidence would resolve it: Comparative experiments using parameter-efficient methods versus full model pretraining on low-resource languages with limited quality text data.

### Open Question 3
- Question: How do the benefits of continued pretraining in the target language compare to instruction tuning for Portuguese language models?
- Basis in paper: [explicit] The authors state "BLOOMZ [38], a multilingual instruction-tuned model, demonstrated superior performance compared to its baseline BLOOM model" and suggest "continued pretraining and instruction tuning as complementary techniques to be combined in future research."
- Why unresolved: The authors did not combine continued pretraining with instruction tuning in their experiments.
- What evidence would resolve it: Experiments comparing models that undergo continued pretraining, instruction tuning, and both techniques combined for Portuguese language models.

### Open Question 4
- Question: How does the performance of models specialized for individual languages compare to multilingual models as the number of target languages increases?
- Basis in paper: [explicit] The authors state "we foresee a future landscape consisting of a diverse array of models, each tailored to a specific domain, rather than a single, all-encompassing model."
- Why unresolved: The authors only experimented with Portuguese specialization and did not compare to multilingual models across multiple languages.
- What evidence would resolve it: Comparative experiments evaluating specialized monolingual models versus multilingual models across various combinations of target languages.

### Open Question 5
- Question: What is the optimal balance between general pretraining corpus and domain-specific pretraining corpus for maximizing performance on both general and specialized tasks?
- Basis in paper: [explicit] The authors discuss domain-specific pretraining trade-offs, noting that Galactica "performs on par with OPT on English tasks and largely outperforms OPT on scientific-related tasks" but "in Portuguese tasks, OPT significantly outperforms Galactica."
- Why unresolved: The authors did not systematically vary the ratio of general to domain-specific pretraining data in their experiments.
- What evidence would resolve it: Experiments systematically varying the proportion of general versus domain-specific pretraining data and measuring performance across both general and specialized tasks.

## Limitations

- The quality filters applied to the Portuguese ClueWeb dataset are not fully specified, making it difficult to assess data representativeness
- The specific few-shot examples used for each Poeta dataset are not detailed, impacting reproducibility
- The study focuses on a single target language (Portuguese), limiting generalizability to other languages

## Confidence

- Claim: Continued monolingual pretraining significantly improves models already trained on diverse corpora. (Medium)
- Claim: Continued monolingual pretraining captures linguistic nuances and structures inherent to the target language. (Medium)
- Claim: Continued monolingual pretraining improves few-shot learning performance on the target language. (Medium)

## Next Checks

1. Replicate the study with a different target language (e.g., Spanish or French) to assess the generalizability of the monolingual pretraining approach across languages with varying resource availability and linguistic characteristics.

2. Conduct a controlled experiment to isolate the impact of monolingual pretraining on capturing linguistic nuances versus domain-specific knowledge by evaluating the models on both language-specific and domain-specific benchmarks.

3. Perform an ablation study to quantify the relative contributions of the base model architecture (GPT-J vs. LLaMA) and the size of the pretraining corpus on the final model performance, providing insights into the most critical factors for successful monolingual pretraining.