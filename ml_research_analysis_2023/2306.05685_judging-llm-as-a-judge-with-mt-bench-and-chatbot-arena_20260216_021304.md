---
ver: rpa2
title: Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena
arxiv_id: '2306.05685'
source_url: https://arxiv.org/abs/2306.05685
tags:
- assistant
- gpt-4
- human
- answer
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the feasibility of using LLMs as judges to
  evaluate chat models based on human preferences. It addresses the limitations of
  existing benchmarks for evaluating open-ended, multi-turn conversations.
---

# Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena

## Quick Facts
- **arXiv ID**: 2306.05685
- **Source URL**: https://arxiv.org/abs/2306.05685
- **Reference count**: 40
- **Primary result**: LLM-as-a-judge achieves over 80% agreement with human evaluations, matching human-human agreement

## Executive Summary
This paper introduces LLM-as-a-judge, an approach using strong LLMs like GPT-4 to evaluate chat models based on human preferences. The method addresses the limitations of existing benchmarks for open-ended, multi-turn conversations by leveraging LLMs' alignment with human preferences through RLHF training. The approach is validated using two benchmarks: MT-bench with 80 multi-turn questions and Chatbot Arena with 30K crowdsourced conversations. Results show that LLM judges can effectively approximate human preferences with high agreement rates, offering a scalable and explainable alternative to human evaluation.

## Method Summary
The LLM-as-a-judge approach uses strong LLMs to evaluate responses through pairwise comparisons and single-answer grading. The method involves collecting human preference data from MT-bench and Chatbot Arena, designing evaluation prompts for LLM judges, running pairwise comparisons with position swapping to mitigate bias, and calculating agreement rates between LLM judges and humans. The approach is validated on model responses from GPT-4, GPT-3.5, Claude-v1, Vicuna-13B, Alpaca-13B, and LLaMA-13B across 80 multi-turn questions and 30K conversations.

## Key Results
- LLM-as-a-judge achieves over 80% agreement with human evaluations, matching human-human agreement levels
- GPT-4 with pairwise comparison and single answer grading shows very high agreements with human experts (85% under setup S2)
- Position bias is identified as a significant issue, with most LLM judges favoring the first position
- LLM judges demonstrate scalability and explainability compared to human evaluation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Strong LLM judges like GPT-4 can approximate human preferences in evaluating chatbot responses with high agreement.
- **Mechanism**: LLMs trained with RLHF already exhibit human alignment, making them capable of evaluating open-ended responses based on human preference criteria like helpfulness, relevance, and accuracy.
- **Core assumption**: The LLM judge's training data and RLHF process have successfully aligned it with human preferences.
- **Evidence anchors**:
  - [abstract] "Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans."
  - [section 4.2] "In Table 5, GPT-4 with both pairwise comparison and single answer grading show very high agreements with human experts. The agreement under setup S2 (w/o tie) between GPT-4 and humans reaches 85%."
  - [corpus] Weak evidence - corpus contains papers about LLM evaluation but none specifically validate the 80% agreement claim with direct human comparison data.

### Mechanism 2
- **Claim**: LLM-as-a-judge provides scalability and explainability compared to human evaluation.
- **Mechanism**: Automating evaluation with LLM judges eliminates the need for human evaluators, enabling rapid iteration and large-scale testing while providing interpretable explanations for judgments.
- **Core assumption**: The cost and time savings from automation outweigh any potential loss in evaluation quality.
- **Evidence anchors**:
  - [section 3.2] "LLM-as-a-judge offers two key benefits: scalability and explainability. It reduces the need for human involvement, enabling scalable benchmarks and fast iterations."
  - [abstract] "Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain."
  - [corpus] Weak evidence - corpus papers discuss evaluation frameworks but don't provide quantitative data on cost/time savings of LLM judges vs human evaluation.

### Mechanism 3
- **Claim**: Position bias in LLM judges can be mitigated through position swapping.
- **Mechanism**: By evaluating responses in both positions and requiring agreement, position bias effects can be canceled out.
- **Core assumption**: Position bias is consistent in direction for a given judge-model pair.
- **Evidence anchors**:
  - [section 3.4] "A conservative approach is to call a judge twice by swapping the order of two answers and only declare a win when an answer is preferred in both orders."
  - [section 3.3] "We found all of them exhibit strong position bias. Most LLM judges favor the first position."
  - [corpus] Weak evidence - corpus doesn't contain studies specifically on position bias mitigation in LLM evaluation.

## Foundational Learning

- **Concept**: Bradley-Terry model for pairwise comparison
  - **Why needed here**: The paper uses pairwise comparisons between model responses, which relies on the Bradley-Terry model's assumptions about relative preferences.
  - **Quick check question**: If model A beats model B 60% of the time and model B beats model C 70% of the time, what's the expected win rate of A vs C under Bradley-Terry?

- **Concept**: Reinforcement Learning from Human Feedback (RLHF)
  - **Why needed here**: The paper assumes LLM judges trained with RLHF are aligned with human preferences, which is a key premise for their approach.
  - **Quick check question**: What are the three main stages of RLHF training for language models?

- **Concept**: Evaluation bias and its mitigation
  - **Why needed here**: The paper identifies multiple biases (position, verbosity, self-enhancement) that could affect LLM judges and proposes mitigation strategies.
  - **Quick check question**: What are the three types of bias identified in the paper that affect LLM judges?

## Architecture Onboarding

- **Component map**:
  LLM Judge -> Prompt Templates -> Position Swapper -> Reference Generator -> Data Collection Pipeline -> Agreement Calculator

- **Critical path**: 
  1. Collect human preference data (MT-bench/Chatbot Arena)
  2. Design evaluation prompts for LLM judges
  3. Run pairwise comparisons with position swapping
  4. Calculate agreement rates
  5. Analyze biases and refine approach

- **Design tradeoffs**:
  - Pairwise vs single-answer grading: Pairwise is more precise but less scalable
  - Position bias mitigation: Swapping positions doubles cost but improves accuracy
  - Judge selection: Proprietary models like GPT-4 perform better but are more expensive than open models

- **Failure signatures**:
  - Low agreement rates (<70%) between LLM judges and humans
  - High position bias consistency (>80% favoring one position)
  - Verbosity bias showing preference for longer responses regardless of quality
  - Self-enhancement bias showing model preferring its own outputs

- **First 3 experiments**:
  1. Run position bias test with simple swapped responses to measure baseline bias
  2. Test agreement between GPT-4 and human experts on MT-bench subset
  3. Implement position swapping mitigation and measure improvement in agreement

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the position bias of LLM judges be effectively mitigated?
- **Basis in paper**: [explicit] The paper discusses the position bias of LLM judges and presents some initial solutions, such as swapping positions and using few-shot examples.
- **Why unresolved**: The paper acknowledges that position bias is a significant limitation of LLM judges and provides preliminary solutions, but does not fully explore or validate the effectiveness of these methods. More research is needed to determine the best approach to mitigate position bias.
- **What evidence would resolve it**: A comprehensive study comparing different methods for mitigating position bias, including swapping positions, few-shot examples, and other potential techniques, with quantitative results showing their effectiveness in reducing bias.

### Open Question 2
- **Question**: Can LLM judges be fine-tuned to better align with human preferences and reduce self-enhancement bias?
- **Basis in paper**: [explicit] The paper mentions the self-enhancement bias of LLM judges and suggests that fine-tuning a judge model on human preference data could be a potential solution.
- **Why unresolved**: The paper only provides preliminary results from fine-tuning a Vicuna-13B model on arena data and does not thoroughly investigate the potential of fine-tuning LLM judges to reduce self-enhancement bias and better align with human preferences.
- **What evidence would resolve it**: A systematic study of fine-tuning LLM judges on diverse human preference datasets, with quantitative results showing the impact on self-enhancement bias and agreement with human evaluations.

### Open Question 3
- **Question**: How can LLM judges be improved to better handle math and reasoning questions?
- **Basis in paper**: [explicit] The paper identifies the limited capability of LLM judges in grading math and reasoning questions and proposes two methods, chain-of-thought and reference-guided, to mitigate this issue.
- **Why unresolved**: The paper only provides initial results for the proposed methods and does not thoroughly explore their effectiveness or investigate other potential approaches to improve LLM judges' ability to handle math and reasoning questions.
- **What evidence would resolve it**: A comprehensive study comparing different methods for improving LLM judges' performance on math and reasoning questions, including chain-of-thought, reference-guided, and other potential techniques, with quantitative results showing their effectiveness.

### Open Question 4
- **Question**: Can open-source LLM judges, such as Vicuna-13B, be effectively used as a cost-friendly alternative to closed-source models like GPT-4?
- **Basis in paper**: [explicit] The paper mentions the potential of using open-source LLM judges as a cheaper alternative to closed-source models but only provides preliminary results from fine-tuning Vicuna-13B on arena data.
- **Why unresolved**: The paper does not thoroughly investigate the potential of open-source LLM judges or provide a comprehensive comparison with closed-source models in terms of agreement with human preferences and other relevant metrics.
- **What evidence would resolve it**: A systematic study comparing open-source LLM judges, such as Vicuna-13B, with closed-source models like GPT-4, in terms of agreement with human preferences, position bias, and other relevant metrics, with quantitative results showing their relative performance.

## Limitations

- Core assumption of LLM alignment with human preferences lacks independent validation
- Position bias mitigation strategy assumes consistent directional bias which may not hold across contexts
- Quantitative validation of claimed scalability and explainability benefits is limited

## Confidence

- **High confidence**: The experimental methodology for measuring agreement between LLM judges and human preferences is sound and well-documented
- **Medium confidence**: The 80% agreement claim is plausible given the experimental design, but depends heavily on the representativeness of the validation data
- **Medium confidence**: The scalability and explainability benefits are theoretically sound but lack quantitative validation in the paper

## Next Checks

1. **Independent human validation**: Test LLM-as-a-judge on a completely separate human evaluation dataset not used in training or prompt development, measuring agreement rates across different question types

2. **Bias consistency analysis**: Systematically test position bias across 100+ varied question types to determine if the directional bias assumption holds universally or varies by context

3. **Cost-benefit quantification**: Measure actual computational costs and time requirements for LLM-as-a-judge vs human evaluation at different scales (100, 1K, 10K comparisons) to validate the claimed efficiency gains