---
ver: rpa2
title: A tailored Handwritten-Text-Recognition System for Medieval Latin
arxiv_id: '2308.09368'
source_url: https://arxiv.org/abs/2308.09368
tags:
- data
- lemmas
- record
- image
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose an end-to-end pipeline for extracting and transcribing
  handwritten lemmas from medieval Latin record cards. The pipeline first uses a YOLOv8
  object detection model to locate the lemma in the upper left corner of each card,
  then applies a transformer-based HTR model to transcribe the text.
---

# A tailored Handwritten-Text-Recognition System for Medieval Latin

## Quick Facts
- arXiv ID: 2308.09368
- Source URL: https://arxiv.org/abs/2308.09368
- Authors:
- Reference count: 8
- One-line primary result: Achieves CER of 0.015 on medieval Latin lemmas, outperforming Google Cloud Vision

## Executive Summary
This paper presents an end-to-end pipeline for extracting and transcribing handwritten lemmas from medieval Latin record cards. The system combines YOLOv8 object detection to locate lemmas in the upper left corner with transformer-based HTR models for transcription. Three encoder architectures (ViT, BEiT, Swin) paired with GPT-2 decoders are evaluated, with the Swin+GPT-2 configuration achieving the best performance. Extensive data augmentation significantly improves results, yielding a CER of 0.015 that surpasses commercial OCR solutions.

## Method Summary
The pipeline operates in two stages: first, YOLOv8 detects bounding boxes around lemmas in record card images; second, the cropped lemma images are processed by a transformer encoder-decoder architecture for transcription. Three vision encoders (ViT, BEiT, Swin) are tested with GPT-2 decoders, all pre-trained on lemma corpus text. Data augmentation includes brightness, contrast, sharpness, and rotation variations. The system is trained on 114,653 record card images from the Medieval Latin Dictionary, with performance measured by Character Error Rate (CER) and weighted CER.

## Key Results
- Best model (Swin encoder + GPT-2 decoder with augmentation) achieves CER of 0.015
- Outperforms Google Cloud Vision commercial OCR model
- Data augmentation notably improves model performance across all three encoder variants
- Model shows weaker performance on longer lemmas and less frequent vocabulary

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The end-to-end pipeline achieves superior CER by combining object detection with specialized transformer-based HTR.
- Mechanism: YOLOv8 extracts lemmas from the upper-left corner of record cards, removing background noise and focusing the HTR model on relevant text regions. The transformer-based encoder-decoder architecture (Swin+GPT-2) then transcribes the cropped lemma images with high accuracy.
- Core assumption: The lemma location is consistent across all record cards, making object detection reliable.
- Evidence anchors:
  - [abstract] "The best-performing setup achieved a Character Error Rate (CER) of 0.015, which is even superior to the commercial Google Cloud Vision model"
  - [section] "Due to the data structure, we are confronted with the problem of finding suitable bounding boxes to extract the lemmas from the upper left of the record cards."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.469" (Weak evidence for similar approaches)
- Break condition: Inconsistent lemma positioning or poor object detection performance would degrade the HTR accuracy.

### Mechanism 2
- Claim: Data augmentation significantly improves model robustness on low-resource medieval Latin data.
- Mechanism: Random modifications to brightness, contrast, sharpness, and rotation create diverse training samples that help the model generalize to variations in handwriting and image quality.
- Core assumption: Augmentation techniques preserve the semantic content of the lemmas while introducing useful variability.
- Evidence anchors:
  - [abstract] "we also apply extensive data augmentation resulting in a highly competitive model"
  - [section] "Applying the augmentation pipelines... notably improves model performance compared to the standard training for all three models."
  - [corpus] No direct evidence for augmentation in related papers.
- Break condition: Excessive augmentation could distort the text beyond recognition or introduce unrealistic variations.

### Mechanism 3
- Claim: Swin transformer encoder architecture provides optimal performance for handwritten text recognition.
- Mechanism: Swin's hierarchical self-attention with shifted windows captures both local and global visual patterns in the lemma images more effectively than ViT or BEiT.
- Core assumption: The fine-grained spatial relationships in handwritten text benefit from Swin's window-based attention mechanism.
- Evidence anchors:
  - [abstract] "The best model, using a Swin encoder and GPT-2 decoder with augmentation, achieves a Character Error Rate (CER) of 0.015"
  - [section] "In Swin, this issue is tackled using a new encoder block structure, differing substantially from the other transformers."
  - [corpus] No direct evidence for Swin performance in related papers.
- Break condition: If Swin's complexity doesn't translate to performance gains, simpler architectures might suffice.

## Foundational Learning

- Concept: Handwritten Text Recognition (HTR)
  - Why needed here: The task involves transcribing medieval Latin lemmas from handwritten record cards, requiring specialized HTR techniques.
  - Quick check question: What distinguishes HTR from standard OCR, and why is this distinction important for medieval manuscripts?

- Concept: Object Detection for Text Localization
  - Why needed here: Before transcription, the system must locate and extract the lemma from each record card image.
  - Quick check question: How does the YOLOv8 model identify the lemma's bounding box without human annotations?

- Concept: Transformer Architecture in Vision Tasks
  - Why needed here: Transformer-based models (ViT, BEiT, Swin) serve as encoders for the HTR pipeline, processing visual features for text recognition.
  - Quick check question: What advantages do transformer encoders offer over traditional CNN-RNN architectures in HTR?

## Architecture Onboarding

- Component map: Image input → YOLOv8 object detection → Lemma extraction → Transformer encoder (Swin/ViT/BEiT) → GPT-2 decoder → Text output
- Critical path: YOLO detection → Swin encoding → GPT-2 decoding with data augmentation
- Design tradeoffs: Complex transformer encoders provide better accuracy but require more computational resources; data augmentation improves robustness but may introduce noise.
- Failure signatures: High CER indicates poor detection or encoding; inconsistent results across augmentation pipelines suggest instability.
- First 3 experiments:
  1. Test YOLOv8 detection accuracy on a validation set of record cards.
  2. Compare CER of Swin vs ViT vs BEiT encoders with standard training.
  3. Evaluate the impact of individual augmentation techniques on model performance.

## Open Questions the Paper Calls Out

- Question: How well does the HTR model generalize to other series of the Medieval Latin Dictionary beyond the S-series, where most of the training data originates from?
- Basis in paper: [inferred] The paper mentions that "the model tends to perform weaker on longer lemmas" and "the performance of the proposed approach, when applied to other series, remains somewhat uncertain" due to training mostly on S-series data.
- Why unresolved: The paper does not provide experimental results or performance metrics for other series of the dictionary, only mentioning the potential limitation.
- What evidence would resolve it: Testing the trained model on lemmas from other series (e.g., A-series, B-series) and comparing CER/WCER scores to the S-series performance.

- Question: Would fine-tuning a pre-trained TrOCR model on the Medieval Latin Dictionary data yield better or comparable results to training from scratch?
- Basis in paper: [explicit] The paper mentions that "the results of Ströbel et al. (2022) suggest a strong performance of TrOCR" and recommends "training it on the MLW data set," but does not actually experiment with this approach.
- Why unresolved: The authors chose to train models from scratch and did not explore the potential benefits of transfer learning from pre-trained TrOCR.
- What evidence would resolve it: Training a TrOCR model from scratch on the MLW data and comparing its CER/WCER to the best model reported in the paper (Swin+GPT-2 with augmentation).

- Question: How significant is the impact of different vision encoder architectures (ViT, BEiT, Swin) on the model's ability to handle longer lemmas or lemmas that appear less frequently in the training data?
- Basis in paper: [inferred] The paper states that "the evaluation of the results exhibits a weaker performance on longer lemmas and on lemmas that appear less frequently in the training data," but does not analyze the performance differences between the encoder architectures for these specific cases.
- Why unresolved: The ablation study and model comparisons focus on overall CER/WCER, not on analyzing performance trends for longer or less frequent lemmas across different encoders.
- What evidence would resolve it: Conducting a detailed analysis of CER/WCER for lemmas of different lengths and frequencies, broken down by encoder architecture, to identify if one encoder consistently outperforms others on challenging cases.

- Question: What is the impact of synthetic data generation on the model's performance, particularly for low-resource lemmas or improving generalization?
- Basis in paper: [explicit] The paper mentions that "further experiments with generative models to produce synthetic data (not reported in the paper) were not successful," but does not provide details on the methods tried or the reasons for failure.
- Why unresolved: The authors briefly mention unsuccessful experiments with synthetic data but do not elaborate on the methods used, the extent of the failure, or potential improvements to the approach.
- What evidence would resolve it: Conducting a comprehensive study of synthetic data generation techniques, including different generative models (e.g., GANs, VAEs), data augmentation strategies, and their impact on CER/WCER for low-resource lemmas and overall model generalization.

## Limitations
- Small test set size (30 samples) limits statistical significance of performance claims
- Lack of comparison with other HTR baselines beyond Google Cloud Vision
- Absence of robustness testing on out-of-domain medieval manuscripts

## Confidence
- Claim: End-to-end pipeline achieves superior CER of 0.015: Medium
- Claim: Swin encoder outperforms ViT and BEiT: Medium
- Claim: Data augmentation significantly improves results: High
- Claim: Approach outperforms commercial solutions: Medium (only one commercial model benchmarked)

## Next Checks
1. Test model performance on a larger, more diverse set of medieval manuscripts to assess generalization beyond the controlled record card dataset.
2. Conduct ablation studies removing individual augmentation techniques to quantify their specific contributions to performance gains.
3. Compare against additional HTR baselines (both commercial and academic) to strengthen claims of superiority.