---
ver: rpa2
title: 'PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations'
arxiv_id: '2307.02762'
source_url: https://arxiv.org/abs/2307.02762
tags:
- answer
- reviewer
- more
- information
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes two methods, Peer Rank and Peer Discussion,
  to improve LLM-based evaluations of open-ended question answering. Peer Rank uses
  weighted voting among multiple LLM reviewers to rank models more fairly than single-model
  evaluation.
---

# PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations

## Quick Facts
- arXiv ID: 2307.02762
- Source URL: https://arxiv.org/abs/2307.02762
- Reference count: 40
- Key outcome: Peer Rank and Peer Discussion methods improve LLM-based evaluations of open-ended question answering by reducing biases and improving correlation with human judgments

## Executive Summary
This paper addresses the challenge of automatically evaluating large language models on open-ended question answering tasks. The authors propose two methods: Peer Rank, which uses weighted voting among multiple LLM reviewers to rank models more fairly than single-model evaluation, and Peer Discussion, where two LLMs engage in multi-turn discussions to reach consensus on pairwise comparisons. Experiments on Vicuna80 and LFQA datasets demonstrate that these methods achieve higher correlation with human judgments than single LLM evaluation while mitigating self-enhancement and position biases.

## Method Summary
The paper introduces Peer Rank (PR), a weighted voting system where multiple LLM reviewers compare pairwise answers and their win rates are aggregated with weights based on each reviewer's performance. This iterative process refines model rankings by emphasizing judgments from more capable reviewers. Peer Discussion (PD) facilitates multi-turn conversations between two LLM reviewers about pairwise answer comparisons, focusing on explicit criteria like unsupported information and coherence. The discussion aims to reach consensus while mitigating position and self-enhancement biases. Both methods are evaluated against human judgments using accuracy, inter-rater agreement, and ranking correlation metrics.

## Key Results
- Peer Rank achieves higher correlation with human judgments than single LLM evaluation
- Peer Discussion improves accuracy over initial reviews and mitigates self-enhancement and position bias
- Discussion ordering bias exists where leading reviewers are less likely to change opinions, correlating with model capability
- GPT-4 holds opinions most often while GPT-3.5 changes opinions most often in discussions

## Why This Works (Mechanism)

### Mechanism 1
Weighted peer ranking improves correlation with human judgments by reducing self-enhancement bias through weighted aggregation of reviewer judgments based on performance. The method emphasizes judgments from more capable models and de-emphasizes those from less capable ones.

### Mechanism 2
Peer discussion improves accuracy by facilitating collaborative reasoning between reviewers, allowing them to refine judgments and reach consensus while mitigating position and self-enhancement biases through exposure to alternative perspectives and explicit criteria.

### Mechanism 3
Discussion ordering bias occurs because leading reviewers are less likely to change opinions, reflecting confidence aligned with capability. Stronger models tend to hold opinions more often than weaker ones, creating a bias that correlates with overall model performance.

## Foundational Learning

- Concept: Pairwise comparison evaluation
  - Why needed here: The entire framework relies on models comparing two answers and selecting which is better, forming the basis for both ranking and discussion methods
  - Quick check question: If you have answers A and B to a question, how would you structure a prompt for an LLM to compare them and choose which is better?

- Concept: Weighted aggregation of judgments
  - Why needed here: The peer rank method uses weighted averaging of win rates from different reviewers based on their performance, requiring understanding of how weights affect aggregation
  - Quick check question: If three reviewers give win rates of 0.6, 0.7, and 0.8, and their weights are 0.2, 0.3, and 0.5 respectively, what is the weighted average?

- Concept: Elo rating system
  - Why needed here: The paper uses Elo ratings to quantify relative performance of models, requiring understanding of how expected outcomes and rating updates work
  - Quick check question: In Elo ratings, if player A (rating 1200) plays player B (rating 1000), what is the expected score for player A?

## Architecture Onboarding

- Component map: Answer generation -> Initial reviews -> Weighted aggregation (PR) OR Discussion (PD) -> Evaluation against human judgments
- Critical path: Answer generation → Initial reviews → Weighted aggregation (PR) OR Discussion (PD) → Evaluation against human judgments
- Design tradeoffs:
  - Peer rank vs discussion: PR provides global rankings efficiently but may miss nuanced differences; PD provides detailed comparisons but is computationally expensive
  - Weighting scheme: Equal weights are simple but less accurate; performance-based weights improve accuracy but add complexity
  - Discussion length: More turns may improve accuracy but increase cost; fewer turns save resources but may be less thorough
- Failure signatures:
  - Rankings that diverge significantly from human judgments
  - Discussions that fail to reach consensus or show high opinion altering
  - Weight convergence issues where weights become unstable or extreme
- First 3 experiments:
  1. Run peer rank with equal weights on Vicuna80 and compare correlation with human judgments
  2. Implement peer discussion between GPT-4 and Claude on LFQA and measure accuracy improvement
  3. Test position bias mitigation by comparing initial vs post-discussion preferences for answers in different positions

## Open Questions the Paper Calls Out

### Open Question 1
How do specific prompting strategies affect the accuracy and convergence of peer discussions between LLMs? The paper compares generic prompts versus explicit criteria prompts and role-based prompts, finding that explicit criteria improve accuracy but doesn't explore the full space of prompting strategies or their long-term effects on discussion quality and model learning.

### Open Question 2
What are the underlying mechanisms of discussion ordering bias and opinion-altering behavior in LLM peer discussions? The paper observes that leaders are less likely to change opinions and that stronger models tend to hold opinions more, but doesn't explain why these patterns emerge or whether they stem from model architecture, training data, or prompt interpretation differences.

### Open Question 3
How does the peer evaluation process affect the LLMs' ability to self-assess and improve their own answer quality over time? The paper mentions plans to investigate how peer evaluation benefits LLMs in learning to assess their own answers and generate new ones, but doesn't explore whether repeated peer evaluation leads to measurable improvements in model self-assessment capabilities or answer generation quality.

## Limitations
- Evaluation is limited to specific datasets (Vicuna80 and LFQA) with particular types of questions, potentially limiting generalizability
- The study relies on a small set of LLM models for evaluation, and results may vary with different model combinations
- The paper does not fully address the computational cost of peer discussion compared to simpler methods

## Confidence
- High confidence: The effectiveness of weighted peer ranking (Peer Rank) in improving correlation with human judgments is well-supported by experimental results
- Medium confidence: The peer discussion method's ability to mitigate biases is demonstrated but could benefit from more diverse test cases
- Medium confidence: The discussion ordering bias findings are interesting but based on a limited number of model combinations

## Next Checks
1. Test the methods on additional benchmark datasets with different question types to assess generalizability
2. Evaluate the computational cost-benefit tradeoff of peer discussion versus simpler methods
3. Conduct ablation studies to isolate the impact of each component of the peer rank and discussion methods