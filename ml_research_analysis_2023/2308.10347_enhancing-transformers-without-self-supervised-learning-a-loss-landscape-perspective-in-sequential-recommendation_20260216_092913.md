---
ver: rpa2
title: 'Enhancing Transformers without Self-supervised Learning: A Loss Landscape
  Perspective in Sequential Recommendation'
arxiv_id: '2308.10347'
source_url: https://arxiv.org/abs/2308.10347
tags:
- samrec
- sequential
- loss
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SAMRec, a method that improves the generalization
  of Transformer-based models for sequential recommendation by smoothing the loss
  landscape during training. It observes that models like SASRec converge to extremely
  sharp local minima, leading to poor generalization.
---

# Enhancing Transformers without Self-supervised Learning: A Loss Landscape Perspective in Sequential Recommendation

## Quick Facts
- arXiv ID: 2308.10347
- Source URL: https://arxiv.org/abs/2308.10347
- Reference count: 40
- Primary result: SAMRec improves Transformer-based sequential recommendation by smoothing loss landscape, outperforming SASRec by 11.04%-15.48% in HR@10 and NDCG@10 without requiring self-supervised learning

## Executive Summary
This paper introduces SAMRec, a method that enhances the generalization of Transformer-based models for sequential recommendation by smoothing the loss landscape during training. The approach is inspired by Sharpness-Aware Minimization (SAM), which encourages convergence to flatter regions of the loss surface rather than sharp local minima. By using a min-max optimization framework, SAMRec achieves performance comparable to state-of-the-art self-supervised methods while avoiding the complexity of pre-training and data augmentations.

## Method Summary
SAMRec builds on SASRec's Transformer architecture and modifies the training procedure using Sharpness-Aware Minimization. The method performs a two-step gradient computation: first finding the worst-case perturbation within a neighborhood ball using gradient ascent, then updating parameters using gradient descent based on the perturbed parameters. This min-max optimization explicitly encourages convergence to flatter regions of the loss landscape, improving generalization without requiring self-supervised learning techniques like data augmentations or pre-training.

## Key Results
- SAMRec outperforms SASRec by 11.04%-15.48% in HR@10 and NDCG@10 on four benchmark datasets
- Achieves comparable performance to state-of-the-art self-supervised methods (S3Rec and CL4SRec) without pre-training or data augmentations
- Demonstrates improved data efficiency, maintaining performance even with reduced training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minimizing sharpness of the loss landscape improves generalization in sequential recommendation.
- Mechanism: SAM directly penalizes sharp local minima by finding parameters in a neighborhood ball where loss is low, biasing optimization toward flatter regions of the loss surface.
- Core assumption: Flat minima correspond to better generalization than sharp minima, and the training loss landscape geometry is predictive of test performance.
- Evidence anchors: Abstract states SAMRec "significantly improves the accuracy and robustness of sequential recommendation" by smoothing loss geometry.

### Mechanism 2
- Claim: SAMRec achieves performance comparable to self-supervised methods without requiring data augmentations or pre-training.
- Mechanism: Implicit regularization from SAM mimics some generalization benefits of self-supervised learning while avoiding its computational complexity.
- Core assumption: The implicit regularization from SAM is sufficient to replace the explicit diversity introduced by data augmentations and pre-training in S3Rec and CL4SRec.
- Evidence anchors: Abstract states SAMRec "performs comparably to state-of-the-art self-supervised Transformers" without needing pre-training or strong data augmentations.

### Mechanism 3
- Claim: SAMRec improves data efficiency, maintaining performance even with reduced training data.
- Mechanism: Flatter minima found by SAMRec are more stable under distribution shifts, making the model less sensitive to the amount of training data.
- Core assumption: Sharp minima are more prone to overfitting to specific training samples, while flat minima generalize better to unseen or limited data.
- Evidence anchors: Paper shows SAMRec "consistently outperforms SASRec and CL4SRec" when trained on 60% and 80% of training data.

## Foundational Learning

- Concept: Sharpness-aware minimization (SAM) and its min-max optimization formulation
  - Why needed here: Understanding SAM is essential to grasp how SAMRec differs from standard training and why it smooths the loss landscape
  - Quick check question: What is the difference between minimizing only the current loss vs. minimizing the worst-case loss in a neighborhood ball?

- Concept: Loss landscape visualization and sharpness metrics
  - Why needed here: Interpreting Figure 1 and understanding why sharp minima hurt generalization requires familiarity with loss surface geometry
  - Quick check question: How does the curvature of a loss surface at a minimum relate to the model's sensitivity to parameter perturbations?

- Concept: Sequential recommendation task formulation and Transformer-based architectures (e.g., SASRec)
  - Why needed here: SAMRec builds directly on SASRec's architecture; knowing how item embeddings, positional encodings, and self-attention work is critical for implementation
  - Quick check question: In SASRec, how are user sequences transformed into next-item predictions, and what role does the positional embedding play?

## Architecture Onboarding

- Component map: Input sequence ‚Üí item + positional embeddings ‚Üí Transformer layers ‚Üí final item representation ‚Üí inner product with item embedding table ‚Üí BCE loss ‚Üí SAM min-max optimization (gradient ascent for ùö´, then gradient descent for ùöØ)
- Critical path: Input sequence ‚Üí item + positional embeddings ‚Üí Transformer layers ‚Üí final item representation ‚Üí inner product with item embedding table ‚Üí BCE loss ‚Üí SAM min-max optimization (gradient ascent for ùö´, then gradient descent for ùöØ)
- Design tradeoffs: SAMRec doubles the gradient computation per step, increasing training time, but this can be mitigated by computing the inner gradient periodically. It avoids pre-training and data augmentation complexity at the cost of optimizer overhead.
- Failure signatures: If ùúå is too large, training becomes unstable; if too small, no sharpness reduction occurs. Performance may plateau if the model converges to a flat but suboptimal minimum.
- First 3 experiments:
  1. Train SAMRec and SASRec on a small subset of Amazon-Beauty with ùúå=0.1; compare training loss curves and final HR@10 to observe sharpness reduction impact.
  2. Vary ùúå over {0.001, 0.01, 0.1, 1.0, 10.0} on Yelp; plot HR@10 vs ùúå to find the stable range and detect overfitting or under-regularization.
  3. Train SASRec, CL4SRec, and SAMRec with 60% of training data on Toys; compare HR@10 to evaluate data efficiency and generalization under sparsity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SAMRec's performance compare to self-supervised methods when training data is extremely sparse (e.g., 20% of original training data)?
- Basis in paper: The paper shows SAMRec outperforms baselines at 60% and 80% training data, but does not test extreme sparsity scenarios.
- Why unresolved: The experiments only tested 60% and 80% training data scenarios, leaving the performance at lower data regimes unexplored.
- What evidence would resolve it: Experimental results comparing SAMRec to S3Rec and CL4SRec on training datasets reduced to 20% or 40% of original size.

### Open Question 2
- Question: What is the theoretical relationship between SAMRec's sharpness-aware optimization and its ability to generalize in sequential recommendation?
- Basis in paper: The paper demonstrates empirical performance gains but does not provide theoretical analysis of why SAMRec's min-max optimization improves generalization.
- Why unresolved: The paper focuses on empirical evaluation without establishing formal theoretical connections between loss landscape smoothness and recommendation performance.
- What evidence would resolve it: Mathematical proof or formal analysis showing how SAMRec's min-max optimization objective leads to better generalization bounds in sequential recommendation settings.

### Open Question 3
- Question: Can SAMRec be effectively combined with other regularization techniques like dropout or weight decay in sequential recommendation?
- Basis in paper: The paper mentions SAMRec uses residual connections, dropout, and layer normalization, but does not explore combinations with other regularization methods.
- Why unresolved: The paper only uses standard regularization techniques and does not investigate how SAMRec interacts with additional regularization methods.
- What evidence would resolve it: Experimental results showing performance of SAMRec when combined with various regularization techniques like L1/L2 regularization, dropout rates, or other regularization methods.

## Limitations

- The paper does not provide direct loss landscape visualizations or sharpness measurements to confirm that flatter minima are being found
- The comparison with self-supervised methods assumes implicit regularization from SAM is sufficient to replace explicit data augmentation diversity without rigorous establishment
- The paper lacks ablation studies on the radius parameter œÅ to show its sensitivity and optimal range

## Confidence

- **High confidence**: Claims about SAMRec outperforming SASRec on HR@10 and NDCG@10 metrics with the specified percentage improvements (11.04%-15.48%)
- **Medium confidence**: Claims about SAMRec achieving comparable performance to S3Rec and CL4SRec without pre-training or data augmentations
- **Low confidence**: Claims about SAMRec's data efficiency improvements under reduced training data, as the evidence is based on limited experimental settings

## Next Checks

1. Conduct direct loss landscape analysis by visualizing the loss surface around converged parameters for SASRec vs SAMRec on at least one dataset to verify that SAMRec finds flatter minima
2. Perform ablation studies varying the radius parameter œÅ over a wider range (including very small and very large values) to establish its sensitivity and identify optimal values
3. Test SAMRec on additional sequential recommendation datasets beyond the four used in the paper, particularly datasets with different sparsity patterns and sequence lengths