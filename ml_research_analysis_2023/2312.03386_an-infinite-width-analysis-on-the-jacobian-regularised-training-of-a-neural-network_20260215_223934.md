---
ver: rpa2
title: An Infinite-Width Analysis on the Jacobian-Regularised Training of a Neural
  Network
arxiv_id: '2312.03386'
source_url: https://arxiv.org/abs/2312.03386
tags:
- network
- training
- neural
- theorem
- infinite-width
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the infinite-width analysis of neural networks
  to the Jacobian of the network. The authors show that a multilayer perceptron (MLP)
  and its Jacobian at initialization jointly converge to a Gaussian process as the
  widths of the MLP's hidden layers go to infinity, and characterize this Gaussian
  process.
---

# An Infinite-Width Analysis on the Jacobian-Regularised Training of a Neural Network

## Quick Facts
- arXiv ID: 2312.03386
- Source URL: https://arxiv.org/abs/2312.03386
- Authors: 
- Reference count: 40
- Primary result: Convergence of MLP and its Jacobian to Gaussian process in infinite-width limit

## Executive Summary
This paper extends infinite-width neural network analysis to include the Jacobian of the network. The authors prove that a multilayer perceptron (MLP) and its Jacobian jointly converge to a Gaussian process as hidden layer widths go to infinity. They further show that Jacobian-regularized training in the infinite-width limit is governed by a linear first-order ODE determined by a variant of the Neural Tangent Kernel called the Jacobian Neural Tangent Kernel (JNTK). Experiments validate these theoretical claims on both synthetic and real datasets.

## Method Summary
The paper analyzes multilayer perceptrons with random initialization, where weights are drawn from standard normal distributions. The forward pass computes hidden states recursively through activation functions, and the Jacobian is computed using recursive backpropagation through layers. Training uses gradient flow with a loss function that includes both standard mean squared error and a Jacobian regularizer term. The theoretical analysis employs the tensor program framework to establish Gaussian process convergence and characterize the limiting JNTK.

## Key Results
- MLP output and Jacobian jointly converge to a Gaussian process at initialization in the infinite-width limit
- The evolution of the MLP under Jacobian-regularized training is described by a linear first-order ODE determined by the JNTK
- The fully trained infinite-width MLP converges to a kernel regression solution defined by the limiting JNTK
- Empirical results show the smallest eigenvalue of JNTK depends on activation function, depth, and dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Jacobian of a multilayer perceptron (MLP) and its network output jointly converge to a Gaussian process (GP) in the infinite-width limit.
- Mechanism: As the widths of all hidden layers go to infinity, the random initialization of the MLP weights induces a joint distribution over outputs and Jacobians that becomes Gaussian due to the central limit theorem applied recursively through the layers. The tensor-program framework formalizes this by translating the computation graph into a tensor program whose outputs converge to Gaussian random variables as layer width grows.
- Core assumption: The activation function is Lipschitz continuous with bounded derivatives and normalized such that its second moment equals one. Additionally, the inputs are normalized to unit norm.
- Evidence anchors:
  - [abstract] "We show that a multilayer perceptron (MLP) and its Jacobian at initialisation jointly converge to a Gaussian process (GP) as the widths of the MLP’s hidden layers go to infinity"
  - [section] Theorem 3 proves this convergence and characterizes the kernel inductively over depth.
  - [corpus] Weak: The corpus neighbors include works on Jacobian initialization and gradient control, but none directly prove GP convergence of Jacobians. This is a novel extension beyond standard NTK theory.
- Break condition: If the activation function fails to be normalized or Lipschitz, the recursive expectation structure in the kernel characterization breaks down and the limiting process is no longer Gaussian.

### Mechanism 2
- Claim: In the infinite-width limit, the evolution of the MLP under Jacobian-regularized training is governed by a linear first-order ODE determined by the Jacobian Neural Tangent Kernel (JNTK).
- Mechanism: The Jacobian regularizer adds a term to the loss that depends on the squared Frobenius norm of the Jacobian. During training via gradient flow, the parameter dynamics are driven by gradients of both the standard loss and the Jacobian regularizer. In the infinite-width limit, the finite JNTK converges to a deterministic limiting kernel, and this kernel remains constant during training (constancy property). This constancy reduces the nonlinear parameter dynamics to a linear ODE in the network output space.
- Core assumption: The limiting JNTK Gram matrix is positive definite (full rank assumption). This ensures the system of ODEs is well-posed and the solution exists and is unique.
- Evidence anchors:
  - [abstract] "We also prove that in the infinite-width limit, the evolution of the MLP under the so-called robust training... is described by a linear first-order ordinary differential equation that is determined by a variant of the Neural Tangent Kernel"
  - [section] Theorem 9 proves the constancy of the finite JNTK during training, and Lemma 6 shows how the kernel governs the dynamics.
  - [corpus] Weak: While NTK constancy is established in prior work, constancy under Jacobian regularization is novel and requires careful analysis due to the additional Jacobian terms in the gradient.
- Break condition: If the JNTK Gram matrix becomes singular (e.g., due to parallel inputs or insufficient depth), the ODE system becomes ill-posed and the linear dynamics picture fails.

### Mechanism 3
- Claim: The fully trained infinite-width MLP under Jacobian regularization converges to the solution of a kernel regression problem defined by the limiting JNTK.
- Mechanism: The linear ODE governing the network dynamics has an analytic solution that can be expressed in terms of the JNTK and the training data. This solution takes the form of a kernel regressor, where the prediction at any test point is a linear combination of the training outputs weighted by the JNTK between the test point and each training input. The regularization parameter λ appears in the kernel, scaling the contribution of the Jacobian terms.
- Core assumption: The training loss converges to zero exponentially fast, which is guaranteed by the positive definiteness of the limiting JNTK and the linear convergence of the ODE.
- Evidence anchors:
  - [abstract] "We identify a linear first-order ordinary differential equation that characterises the evolution of this infinitely wide MLP during robust training and describe the analytic solution of the ODE"
  - [section] Theorem 10 shows that the infinite-width MLP converges to the kernel regression solution fntk defined in Equation 4.
  - [corpus] Moderate: Kernel regression solutions are well-established for NTK; extending this to JNTK is novel but follows similar principles.
- Break condition: If the training data is not sufficiently diverse (e.g., all inputs are parallel), the JNTK may not be full rank, preventing convergence to zero loss and invalidating the kernel regression equivalence.

## Foundational Learning

- Concept: Gaussian Process Convergence in Wide Neural Networks
  - Why needed here: The core theoretical result relies on showing that both the network output and its Jacobian converge to a GP. Understanding how this convergence works for standard MLPs (without Jacobians) is a prerequisite for extending it to include Jacobians.
  - Quick check question: In a two-layer MLP with ReLU activation, as the width of the hidden layer goes to infinity, what does the output distribution converge to at initialization?

- Concept: Neural Tangent Kernel (NTK) and Its Constancy
  - Why needed here: The analysis of training dynamics under Jacobian regularization builds directly on NTK theory. The constancy of the NTK during training in the infinite-width limit is a key property that allows the reduction to a linear ODE.
  - Quick check question: Why does the NTK stay constant during training in the infinite-width limit, and what assumption about the network initialization is required for this?

- Concept: Tensor Program Framework
  - Why needed here: The proof of GP convergence for the Jacobian uses the tensor program framework, which is a powerful tool for analyzing the behavior of wide neural networks. Understanding how to express network computations as tensor programs and apply the Master theorem is essential for following the theoretical arguments.
  - Quick check question: In the tensor program framework, what is the role of the input variables and matrices, and how are the random variables Z_v associated with each program variable defined?

## Architecture Onboarding

- Component map:
  - Initialization: Random weights W^(l) ~ N(0,1) for l=1,...,L+1, scaled output by κ>0
  - Forward pass: h^(0)=x, g^(1)=W^(1)x, h^(1)=ϕ(g^(1)), ..., f(x)=κ/√d W^(L+1)h^(L)
  - Jacobian computation: J(f)(x)_α = ∂f(x)/∂x_α computed recursively through layers
  - Loss function: L(θ) = (1/2N) Σ_i (f(x(i)) - y(i))^2 + λ Σ_i Σ_α (J(f)(x(i))_α)^2
  - Training: Gradient flow dθ/dt = -∂L(θ)/∂θ

- Critical path:
  1. Initialize weights randomly
  2. Compute forward pass and Jacobian for each training input
  3. Evaluate loss and its gradients with respect to all parameters
  4. Update parameters via gradient flow
  5. Monitor training loss and Jacobian norm

- Design tradeoffs:
  - Width vs. depth: Wider networks make the theoretical analysis more accurate but increase computation; deeper networks may be needed to satisfy the full-rank JNTK assumption.
  - Regularization strength λ: Controls the trade-off between fitting the data and keeping the Jacobian small (robustness). Too large λ may underfit, too small may not provide robustness.
  - Output scaling κ: Suppressing initial randomness with small κ can improve training stability but may slow down initial learning.

- Failure signatures:
  - Training loss plateaus but does not reach zero: Likely due to the JNTK Gram matrix being close to singular (e.g., parallel inputs or insufficient depth).
  - Jacobian norm does not decrease: May indicate that λ is too small or that the network architecture is not expressive enough to fit the data while keeping the Jacobian small.
  - Numerical instability in Jacobian computation: Can occur for very deep networks due to vanishing/exploding gradients in the Jacobian recursion.

- First 3 experiments:
  1. Verify GP convergence: Train an MLP on a small synthetic dataset, compute the empirical covariance of outputs and Jacobians across multiple random initializations, and compare to the theoretical NNGP kernel as width increases.
  2. Test JNTK constancy: Train an MLP with Jacobian regularization on a small dataset, compute the finite JNTK at initialization and at several points during training, and verify that it stays close to its initialization as width increases.
  3. Analyze kernel regression solution: Train an MLP with varying λ on a binary classification dataset, compute the eigen-decomposition of the JNTK Gram matrix, and analyze the accuracy and robustness of the corresponding eigenfeatures.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions on the activation function and network depth does the smallest eigenvalue of the Jacobian Neural Tangent Kernel (JNTK) remain positive definite, and how do these conditions relate to the input dataset characteristics?
- Basis in paper: Explicit. The paper discusses Assumption 8 regarding the full rank of the limiting JNTK and provides empirical evidence that the smallest eigenvalue depends on activation function, depth, and dataset.
- Why unresolved: The paper only provides empirical analysis for specific activation functions (GeLU, erf) and datasets. A general theoretical characterization of when Assumption 8 holds is missing.
- What evidence would resolve it: A formal theorem establishing necessary and sufficient conditions on activation function smoothness, depth, and input dataset properties (e.g., no near-parallel inputs) for positive definiteness of the JNTK.

### Open Question 2
- Question: What is the precise relationship between the Jacobian regularizer coefficient λ and the trade-off between model accuracy and robustness in the kernel regression solution, and how does this compare to standard training without Jacobian regularization?
- Basis in paper: Explicit. The paper analyzes the eigenfeatures of the kernel regression solution and observes that Jacobian regularization promotes alignment between accuracy and robustness, but the exact quantitative relationship is not characterized.
- Why unresolved: The paper provides empirical observations on the relationship between λ and robustness/accuracy but does not derive a theoretical framework for understanding this trade-off.
- What evidence would resolve it: A theoretical analysis showing how varying λ affects the eigenvalue spectrum of the JNTK matrix and consequently influences the robustness and accuracy of the kernel regression solution.

### Open Question 3
- Question: How does the infinite-width analysis of the Jacobian-regularized training extend to architectures beyond multilayer perceptrons, such as convolutional neural networks or residual networks?
- Basis in paper: Inferred. The paper focuses on MLPs and their Jacobian, but the techniques used (tensor programs, Gaussian process convergence) could potentially be applied to other architectures.
- Why unresolved: The paper's analysis is limited to MLPs, and extending the results to more complex architectures would require new mathematical tools and techniques.
- What evidence would resolve it: A generalization of the tensor program framework to handle convolutional or residual architectures, along with corresponding convergence results for their Jacobians in the infinite-width limit.

## Limitations

- The full-rank assumption of the limiting JNTK Gram matrix may not hold for real-world datasets with parallel or nearly-parallel inputs
- The tensor-program framework proofs rely on asymptotic behavior that may not fully capture finite-width effects in practical applications
- Experiments focus on synthetic and small real datasets, leaving open questions about scalability to larger, more complex problems

## Confidence

- High Confidence: The GP convergence of MLP and Jacobian at initialization - supported by rigorous tensor-program framework proofs and standard in infinite-width theory
- Medium Confidence: Constancy of JNTK during training under Jacobian regularization - novel extension of NTK theory, but proof structure follows established patterns with additional complexity
- Medium Confidence: Convergence to kernel regression solution - follows from ODE solution, but depends on the full-rank assumption which may fail in practice

## Next Checks

1. Test JNTK full-rank assumption: Compute minimum eigenvalues of JNTK Gram matrices across diverse real datasets to identify conditions where the assumption fails and training dynamics deviate from linear ODE predictions.

2. Finite-width scaling analysis: Systematically vary network width and measure convergence rates of Jacobian GP approximation and JNTK constancy to quantify how quickly theoretical predictions become accurate.

3. Robustness-accuracy tradeoff characterization: For varying λ values, measure both test accuracy and Jacobian norm on multiple datasets to empirically validate the theoretical prediction that Jacobian regularization improves robustness at the cost of some accuracy.