---
ver: rpa2
title: Direct Neural Machine Translation with Task-level Mixture of Experts models
arxiv_id: '2310.12236'
source_url: https://arxiv.org/abs/2310.12236
tags:
- language
- translation
- pairs
- task-level
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of Task-level Mixture of Experts
  (MoE) models for Direct Neural Machine Translation (NMT) between non-English languages.
  Task-level MoE is a sparse, inference-efficient variant of the Transformer architecture
  that routes different language pairs to specialized experts.
---

# Direct Neural Machine Translation with Task-level Mixture of Experts models

## Quick Facts
- arXiv ID: 2310.12236
- Source URL: https://arxiv.org/abs/2310.12236
- Reference count: 19
- Large multilingual Task-level MoE models with 16 or 64 experts achieve better direct NMT quality than bilingual baselines on 7 language pairs and pivot-based models on 9 pairs

## Executive Summary
This paper investigates Task-level Mixture of Experts (MoE) models for Direct Neural Machine Translation between non-English languages. The authors propose routing different language pairs to specialized experts rather than using token-level routing, creating sparse, inference-efficient models that scale better for multilingual translation. They train models with varying numbers of experts (16 or 64) and different task-to-expert mapping strategies (language pair vs target language) on 53 direct language pairs. The results demonstrate that Task-level MoE models with 16 experts and language pair-based routing achieve the best performance on 7 language pairs, while pivot-based models still outperform on 9 pairs. The study provides insights into optimal model configurations for high-quality direct NMT across different language pairs and directions.

## Method Summary
The authors implement sparse encoder-decoder Task-level MoE models with configurable expert counts (16 or 64) and routing strategies. Models use 16 or 64 experts with language pair or target language as task IDs. Training employs T=5 data sampling temperature for 2 million steps with batch size 256 and max sentence length 128. Task-to-expert mapping strategies include lp_a (exact LP), lp_b (En-yy), lp_c (xx-En), tl_a (yy), and tl_b (xx). Models are trained on English-centric and direct language pair data, then evaluated on Web Domain and Wikipedia test sets using BLEU scores. The comparison includes bilingual and pivot-based NMT baselines across 53 direct language pairs in both translation directions.

## Key Results
- Task-level MoE models with 16 experts and language pair-based routing achieve the best performance on 7 language pairs
- Pivot-based models still outperform Task-level MoE on 9 language pairs
- Target language routing enables cross-lingual transfer and better handling of zero-shot pairs
- Inference-time mapping strategies allow adaptation to unseen language pairs without retraining

## Why This Works (Mechanism)

### Mechanism 1
Task-level MoE models improve translation quality by routing language pairs to specialized experts rather than token-level routing. Each translation task is mapped to a subset of experts, allowing these experts to specialize in the linguistic patterns of specific pairs or target languages. This specialization reduces interference between unrelated language pairs and improves parameter efficiency.

### Mechanism 2
Using target language as task ID enables cross-lingual transfer and better handling of zero-shot pairs. By routing all translations to a given target language through the same expert, the model learns common patterns for generating that language, even when source languages differ. This helps when direct training data is scarce.

### Mechanism 3
Inference-time mapping strategies allow adaptation to unseen language pairs without retraining. During inference, language pairs can be mapped to experts based on related training pairs or shared target languages, enabling the model to handle pairs it never saw during training.

## Foundational Learning

- Concept: Mixture of Experts (MoE) architecture
  - Why needed here: MoE allows scaling model capacity without proportional increase in computation, essential for handling many language pairs efficiently.
  - Quick check question: What is the main advantage of using sparse activation in MoE compared to dense models?

- Concept: Routing strategies in MoE
  - Why needed here: Different routing strategies (task-level vs token-level) determine how inputs are assigned to experts, directly affecting model performance and efficiency.
  - Quick check question: How does task-level routing differ from token-level routing in terms of computational efficiency?

- Concept: Zero-shot and zero-resource translation
  - Why needed here: The paper evaluates models on language pairs with little or no direct training data, requiring methods to generalize from related pairs.
  - Quick check question: What is the difference between zero-shot and zero-resource translation scenarios?

## Architecture Onboarding

- Component map:
  Encoder-decoder Transformer base -> Feed Forward Networks replaced by MoE layers (16 or 64 experts) -> Task ID routing layer (either language pair or target language) -> SentencePiece tokenizer (32k vocab, shared encoder/decoder)

- Critical path:
  1. Input sentence tokenized and prepended with language tags
  2. Task ID extracted (language pair or target language)
  3. Router selects top-2 experts based on task ID
  4. Selected experts process the input
  5. Outputs combined and passed to next layer
  6. Decoder generates target sentence

- Design tradeoffs:
  - More experts → higher capacity but more parameters and potential sparsity
  - Task-level vs token-level routing → specialization vs fine-grained adaptation
  - Language pair vs target language task ID → specificity vs generalization

- Failure signatures:
  - Low expert utilization → routing strategy not matching data distribution
  - Poor BLEU on certain pairs → mismatch between training and inference routing
  - High computational cost → too many active experts per token

- First 3 experiments:
  1. Train 16-expert model with language pair task ID, evaluate on all direct pairs
  2. Train 16-expert model with target language task ID, evaluate with tl_a inference mapping
  3. Compare inference performance using lp_b vs tl_a mapping strategies on related language pairs

## Open Questions the Paper Calls Out

### Open Question 1
How do Task-level MoE models perform on zero-shot and zero-resource direct NMT tasks compared to the evaluated task-specific models? The paper mentions they train Task-level MoE models across various configurations including Zero-shot and Zero-resource scenarios, but only evaluates on direct pairs with existing training data. Experimental results comparing Task-level MoE performance to bilingual/pivot baselines on zero-shot and zero-resource direct NMT tasks would resolve this.

### Open Question 2
What is the optimal number of experts for Task-level MoE models across different language family groups and resource levels? While the paper identifies some patterns (e.g., 16 experts often performs better than 64), the analysis is limited to specific language pairs tested. Systematic experiments across diverse language families with varying resource levels would show optimal expert counts for each group.

### Open Question 3
How does expert utilization evolve during training for different routing strategies, and what impact does this have on final translation quality? The authors provide visualizations of expert utilization showing how routing decisions change during training and converge, but don't establish causal relationships between utilization patterns and translation quality. Detailed analysis correlating expert utilization patterns with translation quality metrics across different routing strategies would explain why certain patterns lead to better performance.

### Open Question 4
What are the trade-offs between inference efficiency and translation quality when using different task-to-expert mapping strategies at inference time? The paper identifies which mapping strategies work best for which language pairs but doesn't analyze computational trade-offs. Comparative analysis of inference speed and computational cost alongside translation quality would identify optimal strategies based on both accuracy and efficiency requirements.

### Open Question 5
How does the performance of Task-level MoE models degrade when extracting specialized dense models for specific language pairs compared to the full sparse model? The authors mention extracting smaller expert-specific dense models but don't provide empirical evidence of the performance gap between extracted dense models and the full sparse model. Experimental comparison of translation quality between full models and extracted counterparts would reveal performance degradation factors.

## Limitations
- Proprietary datasets restrict independent verification of reported BLEU scores
- Evaluation focuses exclusively on BLEU scores without deeper linguistic analysis
- Inference-time mapping strategies only tested on related pairs, not truly zero-shot scenarios

## Confidence
**High Confidence:** Experimental methodology is clearly described with specific model configurations and training procedures. Comparison framework against baselines is well-defined with consistent patterns across language pairs.

**Medium Confidence:** Superiority claims are supported by quantitative results but limited to BLEU scores without qualitative analysis of translation quality improvements.

**Low Confidence:** Inference-time mapping strategies show promise but remain largely unverified for truly zero-shot scenarios. Claims about target language routing enabling cross-lingual transfer lack direct evidence from zero-resource experiments.

## Next Checks
1. Conduct detailed analysis of expert activation patterns during inference to verify routing strategies effectively match tasks to appropriate experts
2. Systematically compare forward and backward translation performance for same language pairs to identify directional biases in expert routing
3. Extend inference mapping experiments to include truly zero-shot language pairs to evaluate routing strategy robustness