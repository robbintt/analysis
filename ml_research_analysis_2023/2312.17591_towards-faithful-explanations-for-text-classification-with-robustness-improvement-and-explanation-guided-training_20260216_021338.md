---
ver: rpa2
title: Towards Faithful Explanations for Text Classification with Robustness Improvement
  and Explanation Guided Training
arxiv_id: '2312.17591'
source_url: https://arxiv.org/abs/2312.17591
tags:
- explanations
- training
- methods
- explanation
- attribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method with robustness improvement and explanation
  guided training to improve the faithfulness of highlight explanations generated
  by feature attributions for text classification. The approach first improves model
  robustness through input gradient regularization and virtual adversarial training.
---

# Towards Faithful Explanations for Text Classification with Robustness Improvement and Explanation Guided Training

## Quick Facts
- arXiv ID: 2312.17591
- Source URL: https://arxiv.org/abs/2312.17591
- Reference count: 13
- Key outcome: Method improves fidelity metrics of explanations in all settings, achieving consistent gains based on two randomization tests

## Executive Summary
This paper addresses the challenge of generating faithful explanations for text classification models by proposing a two-stage approach that first improves model robustness and then uses explanation-guided training. The method combines input gradient regularization, virtual adversarial training, and salient ranking with KL divergence to align model attention with feature attributions. Experiments across six datasets with five attribution methods show consistent improvements in explanation faithfulness metrics while maintaining competitive task performance.

## Method Summary
The proposed REGEX method improves explanation faithfulness through two key components: robustness improvement using input gradient regularization and virtual adversarial training, followed by explanation-guided training that uses salient ranking to mask noisy tokens and maximize similarity between model attention and feature attributions. The approach operates as a self-training procedure without external information, leveraging the model's own attributions to create perturbed inputs and training the model to maintain consistent attention patterns across original and perturbed inputs.

## Key Results
- Improves fidelity metrics of explanations in all settings across six datasets and five attribution methods
- Achieves consistent gains based on two randomization tests (random token insertion/removal)
- Using generated explanations to train select-then-predict models results in comparable task performance to end-to-end methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Improving model robustness reduces fragility in feature attribution explanations.
- Mechanism: Robustness training methods smooth the decision boundary, making the model less sensitive to small perturbations in input features. This stability translates to more consistent feature attributions across similar inputs.
- Core assumption: The fragility of explanations is primarily due to model instability rather than the attribution method itself.
- Evidence anchors:
  - [abstract]: "we improve model robustness by input gradient regularization technique and virtual adversarial training"
  - [section]: "we argue that there are connections between model robustness and explainability; any progress in one part may represent progress in both"
  - [corpus]: Weak evidence - corpus contains related work on robustness but no direct evidence for this specific claim

### Mechanism 2
- Claim: Explanation-guided training aligns model attention with feature attributions to reduce noisy token effects.
- Mechanism: By masking low-attribution tokens and maximizing similarity between attention and attribution distributions, the model learns to focus on truly important features during training. This creates a feedback loop where the model's reasoning process becomes more aligned with the attribution method's importance ranking.
- Core assumption: The attribution method provides a reasonable approximation of feature importance that the model can learn from during training.
- Evidence anchors:
  - [abstract]: "we use salient ranking to mask noisy tokens and maximize the similarity between model attention and feature attribution"
  - [section]: "we leverage the existing explanations to guide the model for reducing feature attribution scores of irrelevant tokens"
  - [corpus]: Moderate evidence - related works on saliency-guided training support this mechanism

### Mechanism 3
- Claim: Self-training without external information can improve explanation faithfulness.
- Mechanism: The method uses the model's own attributions to create perturbed inputs (by masking low-importance tokens), then trains the model to produce similar attention patterns on both original and perturbed inputs. This self-consistency training improves the alignment between the model's internal reasoning and the attribution method.
- Core assumption: The model can use its own imperfect explanations as training signals to improve its future explanations without needing ground truth rationales.
- Evidence anchors:
  - [abstract]: "which can be seen as a self-training procedure without importing other external information"
  - [section]: "we do not introduce external knowledge, only use salient ranking as self-training"
  - [corpus]: Weak evidence - corpus contains some self-training works but limited direct evidence for this specific approach

## Foundational Learning

- Concept: Adversarial training and robustness in deep learning
  - Why needed here: The paper builds on virtual adversarial training and input gradient regularization to improve model robustness
  - Quick check question: What is the key difference between standard adversarial training and virtual adversarial training?

- Concept: Feature attribution methods (Integrated Gradients, Saliency, DeepLift)
  - Why needed here: The method uses these attribution methods to identify important tokens and guide the training process
  - Quick check question: Why might Integrated Gradients be preferred over simple gradient-based attribution methods?

- Concept: KL divergence and distribution similarity
  - Why needed here: The method uses KL divergence to measure similarity between attention distributions and attribution distributions
  - Quick check question: What does it mean when KL divergence between two distributions is minimized?

## Architecture Onboarding

- Component map: BERT-base -> Robustness Module (gradient regularization + virtual adversarial training) -> Explanation-Guided Training Module (salient ranking + KL divergence loss) -> Attribution Method Integration

- Critical path: 1. Compute attributions using chosen method 2. Mask low-attribution tokens to create perturbed input 3. Compute attention scores on both original and perturbed inputs 4. Calculate KL divergence between attention and attribution distributions 5. Combine with robustness losses and classification loss 6. Backpropagate through all components

- Design tradeoffs:
  - Multiple forward/backward passes increase training time but don't affect inference
  - Using multiple attribution methods requires testing but provides robustness to method-specific biases
  - Mask ratio (K%) requires tuning - too high adds noise, too low provides insufficient perturbation

- Failure signatures:
  - Explanations become worse if robustness training parameters are misconfigured
  - Training instability if KL divergence weight is too high
  - Degraded task performance if explanation-guided training conflicts with classification objective

- First 3 experiments:
  1. Compare sufficiency and comprehensiveness metrics with and without explanation-guided training on SST dataset
  2. Test different mask ratios (K%) to find optimal perturbation level
  3. Evaluate out-of-domain performance by training on SST and testing on IMDB to assess generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of attribution method impact the effectiveness of the REGEX approach in improving explanation faithfulness?
- Basis in paper: [explicit] The paper experiments with five different attribution methods (Random, Attention, Scaled Attention, InputXGrad, Integrated Gradients, and DeepLift) and observes varying degrees of improvement in explanation faithfulness across these methods.
- Why unresolved: While the paper shows that REGEX improves faithfulness for all attribution methods, it doesn't provide a detailed analysis of why some methods benefit more than others. The underlying reasons for these differences remain unexplored.
- What evidence would resolve it: A more granular analysis comparing the characteristics of each attribution method (e.g., gradient-based vs. perturbation-based) and their interaction with the REGEX training process could shed light on this question.

### Open Question 2
- Question: How does the effectiveness of REGEX scale with model size and complexity?
- Basis in paper: [inferred] The paper uses BERT-base and RoBERTa-base models for experiments, but doesn't explore the impact of using larger models like GPT-3 or T5. It's unclear if the observed improvements in explanation faithfulness would hold for more complex architectures.
- Why unresolved: The paper doesn't investigate how the benefits of REGEX might change with model size. It's possible that larger models with more parameters could have different vulnerabilities that REGEX might or might not address.
- What evidence would resolve it: Experiments with a range of model sizes (e.g., BERT-base, BERT-large, GPT-2, GPT-3) and architectures would be needed to determine if the improvements in explanation faithfulness scale with model complexity.

### Open Question 3
- Question: Can the REGEX approach be extended to improve explanation faithfulness in text generation tasks?
- Basis in paper: [inferred] The paper focuses on text classification tasks and doesn't explore how the REGEX approach might be applied to generation tasks like machine translation or summarization. It's unclear if the techniques used to improve explanation faithfulness in classification would be effective for generation.
- Why unresolved: The paper doesn't investigate the applicability of REGEX to tasks beyond text classification. Generation tasks have different challenges and evaluation metrics, so it's not clear if the same techniques would be effective.
- What evidence would resolve it: Experiments applying REGEX to text generation tasks and evaluating the faithfulness of explanations using appropriate metrics for those tasks would be needed to determine if the approach generalizes beyond classification.

## Limitations
- The evaluation framework assumes feature attribution methods are accurate proxies for importance
- Training procedure introduces computational overhead through multiple forward/backward passes
- Hyperparameter sensitivity (particularly mask ratio K) suggests method may not generalize well without careful tuning

## Confidence
- High confidence: Connection between model robustness and explanation stability
- Medium confidence: Explanation-guided training mechanism
- Low confidence: Self-training approach's general applicability across diverse architectures

## Next Checks
1. Evaluate whether improvement in explanation faithfulness persists when using attribution methods with known limitations versus more robust methods
2. Measure actual training time increase and memory requirements across different dataset sizes to quantify practical cost of multi-stage training procedure
3. Systematically degrade quality of initial attributions and measure point at which explanation-guided training stops improving or begins degrading explanation faithfulness