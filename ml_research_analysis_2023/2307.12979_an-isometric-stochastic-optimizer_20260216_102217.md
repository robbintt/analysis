---
ver: rpa2
title: An Isometric Stochastic Optimizer
arxiv_id: '2307.12979'
source_url: https://arxiv.org/abs/2307.12979
tags:
- adam
- update
- matrix
- gradient
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Iso, a new optimizer for deep learning based
  on the principle that each parameter's step size should be independent of the norms
  of other parameters. Iso makes the norm of a parameter's update invariant to linear
  transformations of inputs and outputs.
---

# An Isometric Stochastic Optimizer

## Quick Facts
- arXiv ID: 2307.12979
- Source URL: https://arxiv.org/abs/2307.12979
- Reference count: 3
- One-line primary result: IsoAdam achieves lower training loss than Adam on a small Transformer without hyperparameter tuning

## Executive Summary
This paper introduces Iso, a novel optimizer that achieves stronger invariance properties than Adam by using full-matrix preconditioning based on covariance statistics. The key insight is that each parameter's update should be invariant to linear transformations of inputs and outputs, which Adam fails to achieve due to its elementwise scaling approach. The paper derives Iso from first principles and develops IsoAdam, a variant that allows optimal hyperparameters to be transferred from Adam while maintaining the benefits of full-matrix preconditioning.

## Method Summary
The method introduces Iso, an optimizer that preconditions gradients using the inverse square root of covariance matrices computed from both inputs and outputs. IsoAdam extends this by applying elementwise scaling after the full-matrix preconditioning, allowing it to inherit Adam's hyperparameter settings. The optimization uses L_X = (A^⊤X^⊤XA)^−1/2 and R_G = (B^⊤G^⊤GB)^−1/2 to achieve invariance to any linear transformation, making the Frobenius norm of updates invariant to orthogonal transformations.

## Key Results
- IsoAdam achieves lower training loss than Adam on OpenWebText without hyperparameter tuning
- Iso exhibits correct asymptotic behavior with respect to batch size, unlike Adam
- Iso is equivariant to orthogonal transformations, providing theoretical advantages over Adam

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adam makes each parameter's step size independent of the norms of other parameters through elementwise scaling.
- Mechanism: Adam maintains separate first and second moment estimates for each parameter, applying diagonal preconditioning that normalizes each parameter's update independently based on its own gradient statistics.
- Core assumption: Parameters can be treated independently when applying normalization, and elementwise scaling is sufficient for practical optimization.
- Evidence anchors:
  - [abstract]: "I propose a simple explanation of Adam's success: it makes each parameter's step size independent of the norms of the other parameters."
  - [section 3.1]: "Adam is equivalent to sign descent in the limit as batch size goes to infinity and the learning rate goes to zero."
- Break condition: When parameters have strong coupling or when orthogonal transformations of inputs/outputs are applied, elementwise scaling fails to maintain invariance.

### Mechanism 2
- Claim: Iso achieves stronger invariance by applying full-matrix preconditioning rather than elementwise scaling.
- Mechanism: Iso uses the inverse square root of covariance matrices (Cov(x) and Cov(g)) to precondition the gradient, making the Frobenius norm of updates invariant to any linear transformation of inputs and outputs.
- Core assumption: Computing and inverting covariance matrices is computationally feasible and provides meaningful normalization beyond elementwise scaling.
- Evidence anchors:
  - [abstract]: "Iso makes the norm of a parameter's update invariant to the application of any linear transformation to its inputs and outputs."
  - [section 2.2]: Derivation showing that L_X = (A^⊤X^⊤XA)^−1/2 and R_G = (B^⊤G^⊤GB)^−1/2 achieve invariance.
- Break condition: When covariance matrices become ill-conditioned or when computational cost of matrix operations becomes prohibitive for very large models.

### Mechanism 3
- Claim: IsoAdam transfers optimal hyperparameters from Adam while gaining the benefits of full-matrix preconditioning.
- Mechanism: IsoAdam applies the same covariance-based preconditioning as Iso but then uses elementwise scaling like Adam, allowing it to inherit Adam's hyperparameter settings while improving optimization geometry.
- Core assumption: The benefits of full-matrix preconditioning can be combined with the simplicity of elementwise scaling without requiring extensive hyperparameter tuning.
- Evidence anchors:
  - [section 4.1]: "I introduce IsoAdam (Algorithm 2), which allows hyperparameters to be transferred from Adam without re-tuning."
  - [section 4]: Experimental results showing IsoAdam achieves speedup over Adam on OpenWebText without hyperparameter tuning.
- Break condition: When the combined preconditioning scheme introduces instability or when the benefits of full-matrix preconditioning are negated by subsequent elementwise scaling.

## Foundational Learning

- Concept: Matrix calculus and linear algebra operations
  - Why needed here: The paper relies heavily on matrix operations including covariance computation, matrix inversion, and matrix square roots for deriving and implementing the optimizer.
  - Quick check question: What is the computational complexity of computing the inverse square root of an n×n matrix using standard methods?

- Concept: Stochastic optimization and gradient descent variants
  - Why needed here: Understanding how Adam works and why it's successful provides the foundation for deriving the Iso optimizer and comparing their properties.
  - Quick check question: How does Adam's update rule differ from standard SGD in terms of normalization and momentum?

- Concept: Equivariance and invariance properties
  - Why needed here: The paper's core contribution relies on achieving invariance to linear transformations, which requires understanding these mathematical properties and their implications for optimization.
  - Quick check question: What is the difference between equivariance and invariance, and why is equivariance to orthogonal transformations important for this work?

## Architecture Onboarding

- Component map:
  Covariance computation -> Matrix square root computation -> Preconditioning -> Momentum tracking -> Parameter update

- Critical path:
  1. Forward pass to compute activations X and gradients G
  2. Covariance matrix computation (X^⊤X and G^⊤G)
  3. Matrix inverse square root computation
  4. Gradient preconditioning using inverse square roots
  5. Momentum update and bias correction
  6. Parameter update application

- Design tradeoffs:
  - Memory vs. accuracy: Computing full covariance matrices requires more memory than elementwise statistics but provides better normalization
  - Computational cost: Matrix inverse square roots are more expensive than elementwise operations but can be approximated for efficiency
  - Batch size sensitivity: Covariance estimates improve with larger batches, affecting optimization stability

- Failure signatures:
  - Ill-conditioned covariance matrices leading to numerical instability
  - Slow convergence when batch sizes are too small for reliable covariance estimation
  - Performance degradation when model architecture doesn't benefit from full-matrix preconditioning

- First 3 experiments:
  1. Implement basic Iso optimizer on a small overparameterized regression problem and compare convergence to Adam
  2. Test IsoAdam on a small Transformer architecture with transferred Adam hyperparameters
  3. Benchmark computational overhead of covariance computation and matrix inverse square roots on representative model sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Iso perform compared to Adam on larger models beyond the 800,000 parameter Transformer tested?
- Basis in paper: [explicit] The paper states "more experiments are needed to validate the result at scale" when discussing IsoAdam's performance on OpenWebText.
- Why unresolved: The experiments were only conducted on a small Transformer model. Scaling to larger models requires significant computational resources and may reveal new challenges or limitations.
- What evidence would resolve it: Training and comparing Iso/IsoAdam with Adam on larger Transformer models (e.g., GPT-2, LLaMA) on various benchmarks while measuring convergence speed, final performance, and computational efficiency.

### Open Question 2
- Question: Can Iso be effectively implemented with less than 10% overhead on current hardware accelerators as claimed?
- Basis in paper: [explicit] The paper states "With sharding of all operations and an accelerator-friendly implementation... it should be possible to implement Iso with less than 10% overhead" but acknowledges this is theoretical.
- Why unresolved: This is a theoretical claim based on assumptions about hardware capabilities and implementation optimizations that haven't been tested in practice.
- What evidence would resolve it: Implementing Iso on actual hardware accelerators (GPUs/TPUs) with various model sizes and measuring the actual computational overhead compared to Adam.

### Open Question 3
- Question: Does IsoAdam truly allow optimal hyperparameters to be transferred from Adam without any tuning?
- Basis in paper: [explicit] The paper claims IsoAdam "allows optimal hyperparameters to be transferred from Adam" but only provides limited experimental validation.
- Why unresolved: The experiments only tested one specific configuration (batch size, model size) and didn't systematically explore hyperparameter transfer across different scenarios.
- What evidence would resolve it: Comprehensive experiments testing IsoAdam with Adam's optimal hyperparameters across diverse model architectures, dataset sizes, batch sizes, and learning rates to verify consistent performance without tuning.

## Limitations
- Limited experimental validation only on a single small-scale Transformer task
- Theoretical computational overhead claims not verified on actual hardware
- Unclear performance on tasks where Adam is known to struggle

## Confidence

**High Confidence**: The theoretical derivation of Iso's invariance properties is mathematically sound given the assumptions about linear transformations. The comparison between Adam and sign descent in the infinite batch size limit is well-established in the literature.

**Medium Confidence**: The experimental results showing IsoAdam's speedup on OpenWebText are promising but limited in scope. The claim that hyperparameters can be transferred from Adam without re-tuning needs validation across more tasks and architectures.

**Low Confidence**: The assertion that IsoAdam will consistently outperform Adam across all deep learning tasks is not supported by the limited experimental evidence provided.

## Next Checks

1. **Scaling Analysis**: Test IsoAdam on progressively larger Transformer architectures (increasing depth, width, and parameter count) to identify at what scale the computational overhead of matrix operations becomes prohibitive and whether the performance benefits persist.

2. **Architecture Transferability**: Evaluate IsoAdam on non-Transformer architectures including CNNs for vision tasks, RNNs for sequence modeling, and graph neural networks to determine if the invariance properties provide consistent benefits across different model families.

3. **Edge Case Performance**: Test IsoAdam on tasks known to challenge Adam, such as training very deep residual networks, handling sparse gradients in recommendation systems, or optimizing models with highly correlated parameters, to identify scenarios where the full-matrix preconditioning provides unique advantages.