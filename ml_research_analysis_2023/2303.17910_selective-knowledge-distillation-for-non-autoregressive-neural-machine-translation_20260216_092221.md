---
ver: rpa2
title: Selective Knowledge Distillation for Non-Autoregressive Neural Machine Translation
arxiv_id: '2303.17910'
source_url: https://arxiv.org/abs/2303.17910
tags:
- data
- translation
- training
- distilled
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Selective knowledge distillation for non-autoregressive neural
  machine translation (NAT) is proposed to tackle error propagation from an autoregressive
  teacher. An NAT evaluator is employed to progressively replace the targets from
  distilled data with raw data for training NAT students, enabling them to benefit
  from both the high-quality raw data and easy-to-learn distilled data.
---

# Selective Knowledge Distillation for Non-Autoregressive Neural Machine Translation

## Quick Facts
- arXiv ID: 2303.17910
- Source URL: https://arxiv.org/abs/2303.17910
- Reference count: 14
- Key outcome: Selective knowledge distillation improves NAT performance by 2.4 BLEU using only 5% distilled data

## Executive Summary
This paper addresses the problem of error propagation in non-autoregressive neural machine translation (NAT) when using knowledge distillation from autoregressive (AT) teachers. The authors propose a selective knowledge distillation approach that uses an NAT evaluator to identify "NAT-friendly" raw translations that are both high-quality and easy for NAT models to learn. They further introduce a progressive distillation method with a hard-to-easy learning strategy, where the ratio of raw data gradually increases during training. Experiments on WMT14 English-German and WMT16 English-Romanian benchmarks demonstrate consistent improvements across multiple NAT architectures including DeepShallow, CMLM, and GLAT+CTC.

## Method Summary
The method involves three main steps: first, training an AT teacher on raw parallel data and generating distilled translations; second, training an NAT evaluator on the distilled data to score raw translations based on Hamming distance between predictions and ground truth; third, using a threshold-based selection mechanism with progressive distillation where the threshold Tk controls the ratio of raw data, starting with mostly distilled data and gradually introducing more raw data. The threshold schedule follows Tk = T0 + k/K · (T1 - T0), with different values for different architectures (GLAT+CTC: T0=0.4→1.0, others: T0=0→1.0).

## Key Results
- Selective knowledge distillation improves NAT performance by approximately 2.4 BLEU compared to training on raw data alone
- Only 5% of the training data needs to be distilled to achieve significant improvements
- Progressive distillation with hard-to-easy learning outperforms fixed threshold approaches
- Improvements are consistent across multiple NAT architectures (DeepShallow, CMLM, GLAT+CTC)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NAT evaluators can effectively identify raw translations that are "NAT-friendly" by measuring similarity between their output and the ground truth
- Mechanism: The NAT evaluator scores each raw sentence by computing 1 - d(Y, fteacher(X))/|Y|, where d is the Hamming distance
- Core assumption: If an NAT model trained on distilled data can generate predictions close to the raw target, the raw and distilled translations share similar modality complexity
- Break condition: If the NAT evaluator's predictions diverge significantly from ground truth due to architecture limitations

### Mechanism 2
- Claim: Progressive distillation with decreasing raw data ratio enables hard-to-easy curriculum learning for NAT models
- Mechanism: The threshold Tk = T0 + k/K · (T1 - T0) controls the ratio of raw data in training, starting with mostly distilled data and gradually introducing more raw data
- Core assumption: NAT models benefit from starting with simpler distilled data before being exposed to more complex raw data
- Break condition: If the model overfits to distilled data early and cannot adapt to raw data later

### Mechanism 3
- Claim: Selective knowledge distillation reduces error propagation from AT teachers while maintaining translation quality
- Mechanism: By replacing only low-quality raw translations with distilled versions, the method avoids propagating AT teacher mistakes while retaining high-quality raw data
- Core assumption: The AT teacher makes systematic errors that can be identified and avoided by using raw data for those specific cases
- Break condition: If the raw data contains errors not present in distilled data

## Foundational Learning

- Concept: Conditional independence assumption in NAT models
  - Why needed here: Understanding why NAT models struggle with multi-modality is crucial for appreciating why knowledge distillation helps and why selective distillation is needed
  - Quick check question: Why does the conditional independence assumption in NAT models lead to worse performance compared to autoregressive models?

- Concept: Sequence-level knowledge distillation
  - Why needed here: The paper builds on standard knowledge distillation but modifies it, so understanding the baseline technique is essential
  - Quick check question: How does sequence-level knowledge distillation reduce the complexity of training data for NAT models?

- Concept: Curriculum learning principles
  - Why needed here: The hard-to-easy strategy is based on curriculum learning, so understanding its general principles helps explain why the approach works
  - Quick check question: What is the main difference between traditional curriculum learning and the hard-to-easy strategy used in this paper?

## Architecture Onboarding

- Component map: AT Teacher -> Distilled Data -> NAT Evaluator -> Raw Data Scoring -> Data Selection Module -> NAT Student
- Critical path: 1. Train AT teacher model on raw data 2. Generate distilled translations from AT teacher 3. Train NAT evaluator on distilled data 4. Score each raw translation using NAT evaluator 5. Apply threshold-based selection to create training dataset 6. Train NAT student with progressive threshold adjustment
- Design tradeoffs: Accuracy vs. efficiency: More sophisticated NAT evaluators could improve selection quality but increase computational cost; Raw data ratio: Higher ratios improve translation quality but increase training complexity; Threshold scheduling: Linear schedules are simple but may not be optimal
- Failure signatures: BLEU scores plateau or decrease after introducing raw data; NAT student shows increased repetition or low-frequency word errors; Training instability when threshold reaches higher values; NAT evaluator fails to distinguish between good and bad raw translations
- First 3 experiments: 1. Train NAT evaluator and compute score distributions on validation set to determine appropriate threshold range 2. Test different fixed threshold values (0%, 5%, 10%, 20%, 50%, 100% raw data) to find optimal ratio 3. Compare progressive threshold (hard-to-easy) vs. fixed threshold for the same average raw data ratio

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of NAT evaluator architecture affect the quality of selected raw data?
- Basis in paper: [explicit] The paper mentions using an NAT model trained on distilled data as an evaluator, but doesn't explore different evaluator architectures
- Why unresolved: The paper only uses one NAT architecture as the evaluator, leaving open whether different NAT architectures would select different sets of raw data or lead to different performance improvements
- What evidence would resolve it: Experiments comparing multiple NAT architectures as evaluators on the same dataset, measuring both the quality of selected data and downstream student performance

### Open Question 2
- Question: What is the optimal threshold function T_k for different NAT architectures and datasets?
- Basis in paper: [explicit] The paper uses a linear function for Tk but mentions it can be determined by a preset function or feedback, suggesting other possibilities
- Why unresolved: The paper only tests one linear threshold function and doesn't explore whether other functions (e.g., exponential, adaptive) would yield better performance for different architectures or datasets
- What evidence would resolve it: Systematic comparison of different threshold functions across multiple NAT architectures and datasets, measuring final BLEU scores and convergence speed

### Open Question 3
- Question: How does selective knowledge distillation affect rare word translation quality compared to standard KD?
- Basis in paper: [explicit] The paper mentions that KD propagates errors on low-frequency words but doesn't specifically measure the impact of their method on rare word translation
- Why unresolved: While the paper shows overall BLEU improvements, it doesn't analyze whether the selective approach specifically helps with the known problem of low-frequency word translation in NAT
- What evidence would resolve it: Detailed analysis comparing translation quality of low-frequency words between standard KD, selective KD, and raw data training baselines using word frequency statistics

## Limitations

- Limited evaluation to only two language pairs (English-German and English-Romanian) may not capture performance across diverse linguistic phenomena
- The NAT evaluator's reliability in distinguishing NAT-friendly translations from problematic ones is not rigorously validated
- The linear threshold scheduling may not be optimal for all architectures and datasets

## Confidence

- High confidence: Core observation that selective knowledge distillation improves NAT performance compared to standard knowledge distillation
- Medium confidence: Claim that progressive distillation with hard-to-easy learning is superior to fixed threshold approaches
- Low confidence: Specific mechanism that the NAT evaluator reliably identifies NAT-friendly translations

## Next Checks

1. **Evaluator reliability assessment**: Run controlled experiments where the NAT evaluator's selections are compared against human judgments of translation quality for a subset of sentences, measuring precision and recall of the selection mechanism.

2. **Architecture transfer study**: Apply the selective distillation approach to additional NAT architectures not evaluated in the paper (e.g., LevT, Mask-Predict) and to different language pairs with varying linguistic properties to assess generalizability.

3. **Threshold optimization analysis**: Systematically vary the threshold scheduling function (exponential, logarithmic, adaptive) while keeping the average raw data ratio constant, to determine whether the linear hard-to-easy schedule is optimal or simply convenient.