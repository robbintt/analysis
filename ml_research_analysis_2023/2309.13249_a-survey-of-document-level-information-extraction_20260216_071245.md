---
ver: rpa2
title: A Survey of Document-Level Information Extraction
arxiv_id: '2309.13249'
source_url: https://arxiv.org/abs/2309.13249
tags:
- event
- extraction
- document-level
- relation
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically reviews document-level information extraction
  (IE) models and datasets, providing a comprehensive overview of recent advancements
  in relation extraction (RE) and event extraction (EE) at the document level. The
  paper categorizes existing approaches into multi-granularity, graph-based, sequence-based,
  and generative methods, analyzing their strengths and limitations.
---

# A Survey of Document-Level Information Extraction

## Quick Facts
- arXiv ID: 2309.13249
- Source URL: https://arxiv.org/abs/2309.13249
- Reference count: 15
- Key outcome: Comprehensive survey of document-level IE models, categorizing approaches and analyzing key challenges including entity coreference, reasoning errors, and noisy data.

## Executive Summary
This survey provides a systematic review of document-level information extraction (IE) models and datasets, focusing on relation extraction (RE) and event extraction (EE). The paper categorizes existing approaches into multi-granularity, graph-based, sequence-based, and generative methods, analyzing their strengths and limitations through comprehensive error analysis. Key challenges identified include entity coreference resolution, reasoning across long-span contexts, and handling noisy data. The survey evaluates various models on benchmark datasets like DocRED, Re-DocRED, ChFinAnn, and WikiEvents, demonstrating performance ranging from F1 scores of 81.9% to 87.1%.

## Method Summary
The survey synthesizes findings from recent research papers on document-level IE, organizing them into four methodological categories: multi-granularity, graph-based, sequence-based, and generative approaches. The authors analyze model architectures, performance metrics, and error patterns across multiple benchmark datasets. Error analysis identifies three primary challenges: labeling noises, entity coreference resolution, and lack of reasoning capability. The survey provides detailed performance comparisons using standard metrics including Precision, Recall, and Macro-F1 scores for both RE and EE tasks.

## Key Results
- Document-level IE models achieve F1 scores up to 87.1% for relation extraction and 81.9% for event extraction on benchmark datasets
- Entity coreference resolution and reasoning across long-span contexts are identified as major challenges affecting model performance
- Multi-granularity approaches that aggregate features at sentence, paragraph, and document levels show effectiveness in capturing long-range dependencies
- Graph-based models that explicitly model entity interactions through heterogeneous graphs demonstrate improved reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Document-level IE models benefit from capturing multi-granularity information (sentence → paragraph → document) to address long-span context errors.
- Mechanism: Hierarchical aggregation of features at different granularities allows the model to first learn local context, then progressively integrate broader context, mimicking human reading patterns.
- Core assumption: Local features provide necessary detail while global features provide coherence; concatenation or attention over these levels improves performance.
- Evidence anchors:
  - [abstract] "entity coreference resolution, reasoning across long-span contexts, and a lack of commonsense reasoning" suggests these are key challenges addressed by multi-granularity approaches.
  - [section] "Multi-granularity-based Models... aims to emphasize the use of information from different granularities and the aggregation of global information."
- Break condition: If the model cannot effectively align features across granularities or if the computational cost outweighs benefits, the approach may fail.

### Mechanism 2
- Claim: Graph-based models capture document-level interactions between entities more effectively than sequential models by modeling inter- and intra-sentence relations.
- Mechanism: Constructing heterogeneous graphs with entities as nodes and edges representing various relations (dependency, adjacency, discourse) enables multi-hop reasoning and global context integration.
- Core assumption: Document-level relations require explicit modeling of entity interactions beyond sentence boundaries, which graphs can represent more naturally than sequences.
- Evidence anchors:
  - [abstract] "reasoning across long-span contexts" is identified as a challenge that graph-based approaches aim to address.
  - [section] "Graph-based models generally construct a graph with words, mentions, entities, or sentences as nodes and define different types of edges across the entire document, further predicting the relations by reasoning on the graph."
- Break condition: If the graph construction process loses critical information or if the reasoning algorithm cannot handle complex multi-hop paths effectively.

### Mechanism 3
- Claim: Incorporating entity coreference systems into document-level IE models reduces coreference resolution errors and improves reasoning performance.
- Mechanism: By explicitly resolving mentions that refer to the same entity before or during relation/event extraction, models can focus on meaningful entity interactions rather than redundant or ambiguous mentions.
- Core assumption: Many document-level errors stem from failing to recognize that different mentions refer to the same entity, which propagates errors through the extraction pipeline.
- Evidence anchors:
  - [abstract] "labeling noises, entity coreference resolution, and lack of reasoning, severely affect the performance of document-level IE."
  - [section] "According to our findings, labeling noises, entity coreference resolution, and lack of reasoning, severely affect the performance of document-level IE."
- Break condition: If the coreference system itself introduces errors or if the computational overhead is prohibitive for large documents.

## Foundational Learning

- Concept: Multi-hop reasoning
  - Why needed here: Document-level IE requires understanding relationships between entities that may not be directly connected within a single sentence, necessitating reasoning across multiple sentences.
  - Quick check question: What is the minimum number of sentences that must be considered to correctly extract a relation between two entities in a document?

- Concept: Entity coreference resolution
  - Why needed here: Documents contain multiple mentions of the same entity, and failing to resolve these mentions leads to fragmented understanding and extraction errors.
  - Quick check question: How do you determine that "NYC" and "the big apple" refer to the same entity in a document?

- Concept: Graph neural networks
  - Why needed here: Graph-based approaches model document structure and entity interactions more effectively than sequential models, enabling better reasoning across long contexts.
  - Quick check question: What type of information is captured by edges in a document-level relation extraction graph?

## Architecture Onboarding

- Component map: Document encoder (BERT/Transformer) → Multi-granularity aggregator (sentence→paragraph→document) → Graph construction module (if using graph-based approach) → Relation/Event classifier → Coreference resolver (optional component)
- Critical path: Document encoding → Feature aggregation → Relation reasoning → Output prediction
- Design tradeoffs: Graph-based approaches capture complex interactions but have higher computational cost; sequence-based approaches are faster but may miss long-range dependencies; multi-granularity approaches balance these but require careful feature alignment.
- Failure signatures: High precision but low recall suggests over-reliance on local context; poor performance on long documents suggests inadequate long-span modeling; inconsistent predictions across similar contexts suggests coreference issues.
- First 3 experiments:
  1. Implement a simple multi-granularity baseline by concatenating sentence-level and document-level representations and evaluate on DocRED.
  2. Add a coreference resolution component to an existing baseline and measure impact on entity coreference and reasoning errors.
  3. Compare performance of graph-based vs. sequence-based approaches on long documents to quantify the benefit of explicit graph reasoning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can document-level IE models effectively incorporate entity coreference resolution to improve performance on multi-hop reasoning tasks?
- Basis in paper: [explicit] The paper identifies entity coreference resolution (ECR) as a major challenge and suggests that incorporating entity coreference systems could play an important role in resolving ECR and multi-hop reasoning errors.
- Why unresolved: While the paper acknowledges the importance of ECR, it does not provide specific methods or implementations for integrating entity coreference systems into existing document-level IE models.
- What evidence would resolve it: Empirical studies comparing the performance of document-level IE models with and without integrated entity coreference systems on benchmark datasets.

### Open Question 2
- Question: What are the most effective strategies for developing multi-hop reasoning capabilities in document-level IE models to handle complex logical inferences across long-span contexts?
- Basis in paper: [explicit] The paper highlights the need for models with multi-hop reasoning capability to address reasoning errors and capture full document context.
- Why unresolved: The paper identifies the challenge but does not propose specific architectural or methodological solutions for implementing multi-hop reasoning in document-level IE.
- What evidence would resolve it: Comparative analysis of different multi-hop reasoning approaches applied to document-level IE tasks, demonstrating improvements in handling complex inferences.

### Open Question 3
- Question: How can document-level IE models be designed to better leverage commonsense knowledge for improved relation extraction and event extraction, particularly in domains with limited background information?
- Basis in paper: [explicit] The paper identifies commonsense knowledge as a significant challenge, noting that models often fail to correctly extract relations or events due to a lack of instinctive understanding.
- Why unresolved: While the paper recognizes the importance of commonsense knowledge, it does not provide concrete methods for integrating or acquiring such knowledge in document-level IE models.
- What evidence would resolve it: Development and evaluation of document-level IE models that incorporate external commonsense knowledge bases or learn commonsense reasoning capabilities, showing improved performance on benchmark datasets.

## Limitations
- Error analysis relies primarily on qualitative observation rather than systematic error type quantification
- Performance comparisons may be affected by differences in dataset preprocessing and hyperparameter tuning
- Survey focuses primarily on English-language datasets, limiting generalizability to other languages

## Confidence

- Multi-granularity approach effectiveness: **High** - supported by consistent findings across multiple papers and clear theoretical justification
- Graph-based model superiority: **Medium** - evidence shows benefits but computational tradeoffs are not fully characterized
- Coreference integration importance: **High** - consistently identified as a major challenge with clear error propagation effects

## Next Checks

1. Conduct systematic error type classification on model predictions to quantify the relative contribution of entity coreference, reasoning, and labeling noise errors
2. Perform ablation studies on coreference resolution components to measure their direct impact on relation/event extraction performance
3. Compare computational efficiency metrics (training/inference time, memory usage) across different approach categories to better understand practical tradeoffs