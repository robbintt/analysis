---
ver: rpa2
title: 'Faith and Fate: Limits of Transformers on Compositionality'
arxiv_id: '2305.18654'
source_url: https://arxiv.org/abs/2305.18654
tags:
- reasoning
- tasks
- multiplication
- task
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the limitations of Transformers on compositional
  tasks that require multi-step reasoning. It formulates such tasks as computation
  graphs and analyzes the performance of various models under different training regimes.
---

# Faith and Fate: Limits of Transformers on Compositionality

## Quick Facts
- arXiv ID: 2305.18654
- Source URL: https://arxiv.org/abs/2305.18654
- Authors: 
- Reference count: 40
- Key outcome: Transformers struggle with out-of-distribution compositional tasks and complex multi-step reasoning, even with explicit reasoning steps, due to error propagation limitations.

## Executive Summary
This paper investigates the fundamental limitations of Transformers on compositional reasoning tasks that require multi-step problem-solving. Through synthetic benchmarks including multi-digit multiplication, Einstein's puzzle, and dynamic programming, the authors demonstrate that Transformers rely on pattern matching rather than developing systematic reasoning capabilities. The study reveals that while models can achieve perfect performance on in-domain examples when exhaustively trained, they fail dramatically on out-of-distribution tasks with increased complexity. Theoretical analysis shows that error propagation in autoregressive generation leads to exponential performance decay as task complexity grows.

## Method Summary
The authors formulate compositional reasoning tasks as computation graphs to systematically quantify complexity. They generate synthetic data for multiplication, Einstein's puzzle, and dynamic programming tasks, then fine-tune GPT-3 (text-davinci-003) on question-answer and question-scratchpad pairs using the OpenAI API. The study evaluates both zero-shot and fine-tuned performance on in-domain and out-of-domain test examples, measuring accuracy and analyzing error patterns. Relative information gain is used to predict surface patterns that models are likely to learn rather than engaging in true compositional reasoning.

## Key Results
- Transformers achieve perfect in-domain performance with exhaustive training but fail on out-of-distribution examples with increased complexity
- Error propagation analysis shows exponential performance decay in autoregressive models as reasoning depth increases
- Models recognize surface patterns allowing partial success without full multi-step reasoning, explained through relative information gain analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching.
- Mechanism: Rather than learning systematic problem-solving rules, models match training examples' sub-graphs that involve the same computations required for solving test examples. This pattern-matching approach works well for low-complexity tasks but fails when complexity increases.
- Core assumption: Models can recognize and match similar computational patterns from training data to solve new problems.
- Evidence anchors:
  - [abstract]: "Our empirical findings suggest that Transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills."
  - [section 3.2.2]: "Transformers largely rely on pattern matching for solving these tasks... models' correct predictions on unseen test data are due to learning the underlying algorithm or, instead, explainable by exposure to similar training examples."
  - [corpus]: Found 25 related papers with average FMR=0.468, indicating moderate relatedness in the literature.
- Break condition: When test examples require computational sub-graphs that were not present in training data, or when task complexity exceeds the pattern-matching capacity.

### Mechanism 2
- Claim: Due to error propagation, Transformers may have inherent limitations on solving high-complexity compositional tasks that exhibit novel patterns.
- Mechanism: Errors in early computational steps compound through subsequent steps, preventing models from finding correct solutions. The autoregressive nature forces sequential processing without global understanding.
- Core assumption: Errors in early steps propagate multiplicatively through the reasoning chain.
- Evidence anchors:
  - [abstract]: "We provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidly decay with increased task complexity."
  - [section 3.2.3]: "The ratio of fully correct nodes is almost perfect but sharply decreases toward zero with increasing graph layers... propagation errors are usually higher than local errors."
  - [section 4]: Formal proofs showing exponential error accumulation in repeated function applications.
- Break condition: When task complexity reaches a threshold where error propagation probability approaches 1, making correct solutions practically impossible.

### Mechanism 3
- Claim: Models recognize surface patterns that allow partial success without full multi-step reasoning.
- Mechanism: Using relative information gain, models identify strong correlations between specific input features and output elements, allowing direct mapping without rigorous reasoning. This creates an illusion of compositional understanding.
- Core assumption: Some output elements have high information gain from specific input features, enabling shortcut learning.
- Evidence anchors:
  - [section 3.2.1]: "Using relative information gain... we can predict surface patterns that a model is likely to learn... Transformers are likely to recognize such correlation during training and directly map these input features to predict the output element in testing."
  - [section C.1]: Tables showing high relative information gain between specific input-output pairs in multiplication tasks.
  - [corpus]: Related work on token compositionality and robustness analysis suggests this is a recognized phenomenon.
- Break condition: When tasks require true compositional reasoning rather than pattern matching, or when novel input combinations break learned correlations.

## Foundational Learning

- Concept: Computation graphs as representation of compositional reasoning
  - Why needed here: The paper uses computation graphs to systematically quantify complexity and analyze model behavior. Understanding this representation is essential for grasping the methodology.
  - Quick check question: How does a computation graph differ from a simple input-output mapping in representing problem-solving?

- Concept: Relative information gain as pattern prediction tool
  - Why needed here: The paper uses this metric to predict which surface patterns models will learn, explaining partial successes without full reasoning.
  - Quick check question: What does it mean when relative information gain between input variable X and output variable Y is close to 1?

- Concept: Error propagation in sequential computation
  - Why needed here: The theoretical arguments about why transformers fail on complex tasks rely on understanding how errors compound in sequential processing.
  - Quick check question: If each computational step has error probability ε, what is the probability of no errors after n steps assuming independence?

## Architecture Onboarding

- Component map: Data generation -> Computation graph creation -> Transformer fine-tuning -> In-domain/out-of-domain evaluation -> Error analysis -> Theoretical bounds
- Critical path: Generate computation graph → Train/fine-tune transformer on task → Evaluate performance → Analyze error types and patterns → Develop theoretical bounds on performance degradation
- Design tradeoffs: Exhaustive training vs. generalization (training on all combinations up to problem size gives perfect in-domain performance but fails OOD), explicit reasoning vs. implicit reasoning (scratchpad training doesn't solve generalization issues), zero-shot vs. fine-tuning (neither solves complex compositional reasoning)
- Failure signatures: Sharp performance drop on out-of-domain examples, high ratio of propagation errors in deeper graph layers, ability to predict partial outputs without full reasoning, dependence on training data coverage of computational sub-graphs
- First 3 experiments:
  1. Implement computation graph generation for a simple compositional task (like multiplication) and verify the graph structure matches expected reasoning steps.
  2. Fine-tune a small transformer model on a limited set of compositional problems and test on both in-domain and out-of-domain examples to observe generalization patterns.
  3. Analyze error types by comparing ground truth computation graphs with model-generated graphs to quantify local vs. propagation errors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Transformers be trained to achieve better generalization on out-of-distribution (OOD) compositional tasks by modifying the training objective or architecture beyond standard fine-tuning with scratchpads?
- Basis in paper: [inferred] The paper shows that even with exhaustive training on task-specific data and scratchpad guidance, Transformers fail to generalize beyond the complexity seen during training.
- Why unresolved: The paper does not explore alternative training objectives (e.g., meta-learning, curriculum learning) or architectural modifications (e.g., modular networks, planning modules) that could enable better OOD generalization.
- What evidence would resolve it: Empirical results showing significant improvement in OOD generalization for complex compositional tasks using alternative training methods or architectures compared to standard fine-tuning.

### Open Question 2
- Question: What is the theoretical relationship between the relative information gain of output elements and the likelihood of Transformers learning spurious patterns versus genuine compositional reasoning?
- Basis in paper: [explicit] The paper uses relative information gain to predict surface patterns that models are likely to learn, suggesting a correlation between high relative information gain and spurious pattern learning.
- Why unresolved: The paper provides empirical evidence but does not establish a rigorous theoretical framework linking relative information gain to the model's reasoning strategies.
- What evidence would resolve it: A theoretical analysis proving that high relative information gain of output elements correlates with the probability of Transformers relying on spurious patterns rather than compositional reasoning, supported by extensive empirical validation.

### Open Question 3
- Question: How do different linearization strategies of computation graphs affect the performance of Transformers on compositional tasks?
- Basis in paper: [inferred] The paper acknowledges that the choice of linearization strategy (e.g., topological ordering) may impact model performance but does not explore alternative linearization methods.
- Why unresolved: The paper focuses on a specific linearization approach and does not investigate how different strategies (e.g., breadth-first vs. depth-first) might influence the model's ability to learn compositional reasoning.
- What evidence would resolve it: Comparative empirical results showing the performance of Transformers on compositional tasks using different linearization strategies, highlighting the most effective approach for capturing compositional reasoning.

### Open Question 4
- Question: What are the specific mechanisms by which error propagation occurs in Transformers during multi-step reasoning, and how can these be mitigated?
- Basis in paper: [explicit] The paper provides theoretical arguments and empirical evidence suggesting that error propagation is a key factor limiting Transformers' performance on complex compositional tasks.
- Why unresolved: The paper does not delve into the specific mechanisms of error propagation or propose concrete methods to mitigate its effects.
- What evidence would resolve it: A detailed analysis of error propagation mechanisms in Transformers during compositional reasoning, accompanied by empirical results demonstrating the effectiveness of proposed mitigation strategies (e.g., error correction, attention mechanisms).

## Limitations
- Conclusions rely heavily on synthetic benchmarks which may not generalize to real-world problems
- Theoretical error propagation analysis assumes independent errors, which may not hold in practice
- Study focuses only on autoregressive transformers, excluding other architectures that might handle compositional tasks differently

## Confidence
- High confidence: Empirical observations of performance degradation on out-of-distribution compositional tasks; identification of error propagation as a key failure mode; the pattern-matching explanation for partial successes on low-complexity tasks
- Medium confidence: Theoretical arguments about exponential error accumulation in autoregressive models; the claim that these limitations are fundamental rather than solvable by current approaches
- Low confidence: Generalization of synthetic benchmark findings to real-world applications; the assertion that no transformer architecture can overcome these limitations with sufficient training

## Next Checks
1. **Cross-architecture validation**: Test the same compositional tasks on non-autoregressive transformer variants and alternative architectures (RNNs, GNNs) to determine if error propagation is architecture-specific.
2. **Real-world benchmark testing**: Apply the analysis framework to established reasoning benchmarks like GSM8K or MATH to verify if synthetic findings predict real performance patterns.
3. **Error dependency analysis**: Empirically measure error correlations across reasoning steps in transformers to validate or refine the independence assumption in theoretical bounds.