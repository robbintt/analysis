---
ver: rpa2
title: An Empirical Study of Multimodal Model Merging
arxiv_id: '2304.14933'
source_url: https://arxiv.org/abs/2304.14933
tags:
- merging
- pre-training
- modality-agnostic
- modality-speci
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores model merging to create a parameter-efficient
  modality-agnostic transformer from modality-specific vision, language, and cross-modal
  transformers. A seed pre-training phase initializes weights across modalities, followed
  by merging via interpolation, modality arithmetic, or RegMean.
---

# An Empirical Study of Multimodal Model Merging

## Quick Facts
- arXiv ID: 2304.14933
- Source URL: https://arxiv.org/abs/2304.14933
- Authors: [not specified in input]
- Reference count: 12
- Key outcome: Seed pre-training significantly improves model merging performance, with interpolation ratio α = 0.75 or RegMean yielding best results

## Executive Summary
This paper investigates model merging techniques for creating parameter-efficient multimodal transformers from modality-specific vision, language, and cross-modal components. The authors propose a seed pre-training phase that initializes weights across modalities before merging via interpolation, modality arithmetic, or RegMean. Their experiments demonstrate that this approach significantly improves performance on downstream vision-language tasks compared to naive merging, with the modality-specific architecture matching modality-agnostic baseline performance after merging.

## Method Summary
The proposed framework follows a three-stage pipeline: (1) seed pre-training initializes a modality-agnostic transformer on VL data for Kseed steps, (2) modality-specific transformers (vision, language, cross-modal) are initialized from the seed model and pre-trained for KV L steps, and (3) the modality-specific models are merged using interpolation (weighted averaging with ratio α), modality arithmetic, or RegMean (regularized mean with hyperparameter γ). The merged model is then fine-tuned on downstream tasks. The modality-specific architecture has entirely independent modules while the modality-agnostic shares weights across modalities.

## Key Results
- Seed pre-training improves VQA performance from 21.74% to 26.24% compared to naive merging
- Interpolation with α = 0.75 or RegMean with γ = 0.25 achieves best results
- Modality-specific architecture performs best before merging and matches modality-agnostic baseline after merging
- Improvements: 3% on VQA, 7% on COCO retrieval, 25% on NLVR2, 14% on Flickr30k, and 3% on ADE20k

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Seed pre-training aligns the weight basins of modality-specific transformers, enabling successful merging
- Mechanism: A shared modality-agnostic pre-trained weight provides a common initialization basin, preventing catastrophic performance drops during interpolation
- Core assumption: Vision, language, and cross-modal transformers need to reside in the same optimization basin for merging to work effectively
- Evidence anchors: Abstract states pre-trained weights on VL data are helpful; section 3.3 explains initial BEIT weight only contains visual information, causing basin misalignment

### Mechanism 2
- Claim: Interpolation with appropriate α ratios preserves modality-specific capabilities while enabling cross-modal learning
- Mechanism: Weighted averaging of transformer weights transfers knowledge across modalities without destroying task-specific representations
- Core assumption: Vision weights are more important for downstream VL tasks than language weights
- Evidence anchors: Section 5.2 shows vision weight importance (α = 1 vs α = 0); table 2a demonstrates interpolation works well at α = 0.5 and 0.75

### Mechanism 3
- Claim: Modality-agnostic architectures with shared weights can match modality-specific performance after merging
- Mechanism: Merging creates a unified transformer that learns to dynamically route modality-specific information through shared parameters
- Core assumption: The merged architecture can learn to replicate modality-specific processing patterns
- Evidence anchors: Abstract states modality-specific architecture matches modality-agnostic baseline after merging; section 5.3 confirms this empirical finding

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Understanding how modality-specific and modality-agnostic transformers differ is crucial for grasping the merging process
  - Quick check question: What distinguishes a modality-specific transformer from a modality-agnostic one in this paper?

- Concept: Model merging and weight interpolation
  - Why needed here: The core technique relies on combining pre-trained model weights through various mathematical operations
  - Quick check question: How does interpolation differ from modality arithmetic in the context of model merging?

- Concept: Pre-training and fine-tuning paradigms
  - Why needed here: The paper follows a specific training pipeline that affects merging outcomes
  - Quick check question: What is the purpose of seed pre-training in the proposed framework?

## Architecture Onboarding

- Component map: Vision transformer → Language transformer → Cross-modal transformer → Merging mechanism → Modality-agnostic transformer → Downstream fine-tuning
- Critical path: Seed pre-training → VL pre-training → Merging → Fine-tuning → Evaluation
- Design tradeoffs: Modality-specific architectures offer better performance before merging but require more parameters; modality-agnostic architectures are more efficient but may need careful merging
- Failure signatures: Large performance drops after merging indicate basin misalignment; poor fine-tuning results suggest inadequate seed pre-training
- First 3 experiments:
  1. Run baseline merging without seed pre-training to establish performance gap
  2. Test different seed pre-training durations (0, 50k, 100k iterations) to find optimal balance
  3. Compare interpolation ratios (α = 0.5, 0.75) to determine best merging configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal duration of seed pre-training that balances both modality-specific and modality-agnostic performance?
- Basis in paper: The paper shows increasing seed pre-training iterations improves merging performance, with optimal results at 100k iterations when Kseed = KV L = 100k
- Why unresolved: While the paper identifies 100k iterations as optimal for their specific setup, this may not generalize to different model sizes, architectures, or pre-training objectives
- What evidence would resolve it: Systematic experiments varying Kseed across different model scales and pre-training objectives to establish a more general relationship

### Open Question 2
- Question: Can model merging work effectively when transformers are initialized from unimodal pre-trained weights rather than modality-agnostic seeds?
- Basis in paper: The authors note this as an important direction, observing that directly merging models with different initializations (BEIT for vision vs BERT for language) performs poorly
- Why unresolved: The paper uses modality-agnostic initialization for all experiments, leaving the question of whether unimodal initialization can be made to work unanswered
- What evidence would resolve it: Experiments demonstrating successful merging strategies when using different unimodal pre-trained weights for initialization

### Open Question 3
- Question: How does model merging perform on VL tasks beyond VQA and COCO retrieval?
- Basis in paper: The authors acknowledge evaluating only on two widely-adopted VL tasks and have not extended to other tasks
- Why unresolved: The paper's conclusions are limited to specific tasks, and performance may vary significantly on other VL benchmarks like image captioning or visual reasoning tasks
- What evidence would resolve it: Comprehensive evaluation across a diverse set of VL tasks including image captioning, visual reasoning, and multimodal understanding benchmarks

## Limitations

- The experiments focus primarily on vision-language tasks, leaving uncertainty about how these merging techniques perform for other multimodal combinations
- The study uses a specific transformer architecture without exploring how different backbone architectures might affect merging outcomes
- The paper does not investigate computational efficiency or memory constraints during the merging process, which could be significant practical considerations

## Confidence

- Claim: Seed pre-training is crucial for successful merging → High confidence
- Claim: Interpolation with α = 0.75 or RegMean yields optimal results → Medium confidence
- Claim: Modality-specific architectures match modality-agnostic performance after merging → Medium confidence

## Next Checks

1. **Cross-task generalization test**: Apply the seed pre-training and merging methodology to a different multimodal task family (e.g., audio-visual speech recognition or medical image-text analysis) to verify whether the α = 0.75 interpolation ratio remains optimal across domains

2. **Architectural robustness evaluation**: Repeat the merging experiments using different backbone architectures (e.g., ViT vs ConvNeXT for vision, different language model sizes) to determine whether the basin-alignment benefits of seed pre-training are architecture-dependent or universal

3. **Resource efficiency analysis**: Measure GPU memory usage and training time for each merging method (interpolation, modality arithmetic, RegMean) across different model scales to identify practical bottlenecks and cost-benefit tradeoffs for real-world deployment