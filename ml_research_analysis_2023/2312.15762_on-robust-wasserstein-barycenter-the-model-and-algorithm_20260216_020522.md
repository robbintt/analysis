---
ver: rpa2
title: 'On Robust Wasserstein Barycenter: The Model and Algorithm'
arxiv_id: '2312.15762'
source_url: https://arxiv.org/abs/2312.15762
tags:
- coreset
- free-rwb
- locations
- wasserstein
- fixed-rwb
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper improves the computational efficiency of two types
  of robust Wasserstein barycenter problems (RWB): fixed-support RWB (fixed-RWB) and
  free-support RWB (free-RWB). The main approach is model reduction, converting the
  RWB into an augmented Wasserstein barycenter problem (AWB), which can be solved
  more efficiently.'
---

# On Robust Wasserstein Barycenter: The Model and Algorithm

## Quick Facts
- arXiv ID: 2312.15762
- Source URL: https://arxiv.org/abs/2312.15762
- Authors: 
- Reference count: 40
- This paper improves the computational efficiency of two types of robust Wasserstein barycenter problems (RWB): fixed-support RWB (fixed-RWB) and free-support RWB (free-RWB).

## Executive Summary
This paper addresses computational challenges in robust Wasserstein barycenter problems by introducing model reduction and coreset techniques. The approach converts the robust problem into an augmented Wasserstein barycenter problem, enabling efficient computation using off-the-shelf solvers. For free-support RWB, the paper combines this model reduction with coreset methods to further accelerate computation while maintaining approximation quality. The proposed algorithms demonstrate improved efficiency compared to existing methods across various datasets.

## Method Summary
The paper presents a two-pronged approach to efficient robust Wasserstein barycenter computation. First, it reduces the robust problem to an augmented Wasserstein barycenter problem using a model reduction technique, which enables solving fixed-support RWB within O(mn²/ε₊) time using standard solvers. Second, for free-support RWB, it leverages coreset techniques to compress the dataset while preserving approximation quality, then employs an alternating update algorithm that updates weights and locations sequentially. The combination of these techniques addresses both computational efficiency and numerical stability issues present in previous approaches.

## Key Results
- Fixed-RWB can be computed within O(mn²/ε₊) time using an off-the-shelf solver
- Coreset technique reduces dataset size for free-RWB while maintaining approximation quality
- Alternating update algorithm efficiently computes free-RWB by updating weights and locations
- Experiments demonstrate improved performance compared to existing methods
- Addresses numerical instability issues in previous approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model reduction converts RWB into an augmented Wasserstein barycenter problem (AWB), enabling efficient computation using off-the-shelf solvers.
- Mechanism: By introducing a dummy point and modifying the cost matrix structure, the robust Wasserstein distance (which includes outlier handling) is reformulated as an augmented optimal transport problem. This reformulation allows leveraging existing efficient solvers that handle standard optimal transport problems.
- Core assumption: The equivalence between the robust Wasserstein distance and the augmented optimal transport holds under the bijection φ constructed in Lemma 3.1.
- Evidence anchors:
  - [abstract] "we reduce RWB as an augmented Wasserstein barycenter problem, which works for both fixed-RWB and free-RWB. Especially, fixed-RWB can be computed within eO( mn2/ϵ+ ) time by using an off-the-shelf solver"
  - [section] "Lemma 3.1. (2.1) is equivalent to (3.8) under bijection φ; that is, h = gφ−1 and hφ = g"
  - [corpus] No direct evidence found in corpus neighbors; this appears to be a novel contribution not mentioned in related works.

### Mechanism 2
- Claim: Coreset technique reduces the dataset size m for free-RWB, accelerating computation while maintaining approximation quality.
- Mechanism: The data is partitioned into layers based on distance to an approximate solution anchor. Each layer is sampled according to a strategy that preserves the objective function's value within an epsilon factor. The coreset size depends on the doubling dimension and problem parameters.
- Core assumption: The local coreset property (Definition 2.4) holds, meaning the barycenter cost computed on the coreset approximates the cost on the full dataset within the specified error bound.
- Evidence anchors:
  - [abstract] "we leverage a quality guaranteed data compression technique, coreset, to accelerate computation by reducing the data set size m"
  - [section] "Theorem 4.1. We set r = H in Definition 2.4. Suppose the diameter of X is R, i.e., maxx,y∈X dist(x, y) = R and the metric space has doubling dimension ddim. Let ˜ν be an O(1)-approximate solution of free-RWB on Q and Γ = eO(min{ddim, log n/ϵ/ϵ2 } · n/ϵ2 ). With probability at least 1 − η, Algorithm 1 outputs a local coreset S for free-RWB on Q."
  - [corpus] No direct evidence found in corpus neighbors; this appears to be a novel application of coreset to robust Wasserstein barycenter.

### Mechanism 3
- Claim: The alternating update algorithm for free-RWB efficiently computes the barycenter by updating weights and locations sequentially.
- Mechanism: The algorithm uses the fixed-RWB (solved via AWB) to update weights while keeping locations fixed, then updates locations using a power mean approach while keeping weights fixed. This iterative process converges to a solution for the free-RWB problem.
- Core assumption: The fixed-AWB subproblem can be solved efficiently and provides sufficient information to guide the location updates.
- Evidence anchors:
  - [abstract] "Next, by combining the model reduction and coreset techniques above, we propose an algorithm for free-RWB by updating the weights and locations alternatively"
  - [section] "Algorithm 2 aims to compute a solution for free-RWB on the input probability measure set Q (as in (2.2)), and it leverages model reduction and coreset technique to accelerate the computation."
  - [corpus] No direct evidence found in corpus neighbors; this appears to be a novel algorithmic framework.

## Foundational Learning

- Concept: Optimal Transport and Wasserstein Distance
  - Why needed here: The entire paper builds on understanding how to compute distances and barycenters between probability measures using optimal transport.
  - Quick check question: Can you explain the difference between the standard Wasserstein distance and the robust version used in this paper?

- Concept: Coreset Theory and Construction
  - Why needed here: The coreset technique is central to reducing computational complexity while maintaining approximation guarantees.
  - Quick check question: What is the doubling dimension, and why does it affect the coreset size in this context?

- Concept: Linear Programming and Dual Extrapolation
  - Why needed here: The fixed-RWB problem is solved as a linear program, and understanding LP solvers and dual methods is crucial for implementation.
  - Quick check question: How does the area-convexity property relate to the eO(mn²/ϵ+) time complexity for fixed-RWB?

## Architecture Onboarding

- Component map: Model Reduction Layer -> Coreset Construction Layer -> Optimization Layer -> Solver Interface
- Critical path:
  1. Compute approximate solution ˜ν for anchor point
  2. Construct coreset S using Algorithm 1
  3. Initialize ν = ˜ν
  4. For each iteration:
     - Update weights using fixed-AWB (3.12)
     - Update locations using power mean
     - Check if solution is in local area D˜ν

- Design tradeoffs:
  - Coreset size vs. approximation quality: Larger Γ provides better guarantees but slower computation
  - Number of iterations vs. solution quality: More iterations may improve results but increase runtime
  - Choice of anchor point ˜ν: Better initial approximation reduces coreset size requirements

- Failure signatures:
  - Numerical instability in LP solvers (especially for fixed-RWB)
  - Coreset construction fails to capture important data points
  - Alternating updates diverge or converge very slowly

- First 3 experiments:
  1. Implement and verify Lemma 3.1 equivalence between robust Wasserstein distance and augmented OT
  2. Test coreset construction on synthetic data with known doubling dimension
  3. Validate alternating update algorithm on small MNIST subset before scaling up

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the model reduction technique for robust Wasserstein barycenter be extended to handle continuous probability measures, not just discrete ones?
- Basis in paper: [explicit] The paper states "Both Lemma 3.1 and Theorem 3.1 work for arbitrary probability measures, such as continuous, semi-continuous, discrete measures [32]."
- Why unresolved: While the paper mentions that the technique works for arbitrary probability measures, it does not provide specific algorithms or theoretical guarantees for the continuous case. Implementing and proving efficiency for continuous measures would require significant additional work.
- What evidence would resolve it: A formal proof showing that the model reduction technique maintains its computational efficiency and accuracy guarantees when applied to continuous probability measures, along with experimental validation on continuous data.

### Open Question 2
- Question: How does the performance of the proposed algorithm change when applied to high-dimensional data, and what are the theoretical bounds on its scalability?
- Basis in paper: [inferred] The paper focuses on improving computational efficiency but does not extensively discuss high-dimensional scalability. The coreset size depends on the doubling dimension, which can grow rapidly with dimensionality.
- Why unresolved: The paper's theoretical analysis and experiments focus on moderate-dimensional data. High-dimensional data poses challenges for both the model reduction and coreset techniques, potentially affecting their effectiveness.
- What evidence would resolve it: Experimental results on high-dimensional datasets, along with theoretical analysis of how the algorithm's time and space complexity scale with dimensionality.

### Open Question 3
- Question: Can the coreset construction method be further optimized to reduce its dependence on the doubling dimension, potentially making it more effective for non-Euclidean metric spaces?
- Basis in paper: [explicit] The paper states "The doubling dimension is a measure for describing the growth rate of the data set X with respect to the metric dist(·, ·)." The coreset size depends on this dimension.
- Why unresolved: The current coreset construction relies heavily on the doubling dimension, which can be large for complex metric spaces. Reducing this dependence could significantly improve the method's applicability.
- What evidence would resolve it: Development of a new coreset construction algorithm with provable quality guarantees that has a smaller dependence on the doubling dimension, or theoretical proof that such an improvement is not possible under certain conditions.

## Limitations
- Missing implementation details for Algorithm 2, particularly the "power mean problem" specification
- Theoretical bounds depend on doubling dimension which may grow rapidly with dimensionality
- Limited experimental validation on very large datasets or high-dimensional data

## Confidence

**High Confidence**: The model reduction approach converting RWB to AWB - this is well-supported by Lemma 3.1 and the mathematical equivalence is clearly established.

**Medium Confidence**: The coreset technique for free-RWB acceleration - while the theoretical framework is sound, the practical effectiveness depends on implementation details and parameter choices that aren't fully specified.

**Medium Confidence**: The alternating update algorithm for free-RWB - the overall framework is clear, but missing implementation details create uncertainty about actual performance.

## Next Checks

1. **Verify Lemma 3.1 Equivalence**: Implement the bijection φ and explicitly test that the robust Wasserstein distance equals the augmented optimal transport cost on synthetic examples with known ground truth.

2. **Test Coreset Construction**: Implement Algorithm 1 and verify the quality guarantees (Theorem 4.1) by comparing barycenter costs computed on coresets versus full datasets across datasets with varying doubling dimensions.

3. **Validate Alternating Updates**: Implement Algorithm 2 with explicit power mean formulation and test convergence properties on small-scale problems before scaling up to larger datasets.