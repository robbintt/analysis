---
ver: rpa2
title: 'Structured prompt interrogation and recursive extraction of semantics (SPIRES):
  A method for populating knowledge bases using zero-shot learning'
arxiv_id: '2304.02711'
source_url: https://arxiv.org/abs/2304.02711
tags:
- spires
- knowledge
- schema
- data
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SPIRES is a novel method for populating knowledge bases using zero-shot
  learning with large language models. It recursively extracts structured data from
  text by generating prompts based on user-defined schemas, completing them with an
  LLM, and grounding results using existing ontologies.
---

# Structured prompt interrogation and recursive extraction of semantics (SPIRES): A method for populating knowledge bases using zero-shot learning

## Quick Facts
- arXiv ID: 2304.02711
- Source URL: https://arxiv.org/abs/2304.02711
- Reference count: 40
- F-score of 40.65 on BioCreative Chemical-Disease-Relation task

## Executive Summary
SPIRES is a novel method for populating knowledge bases using zero-shot learning with large language models. It recursively extracts structured data from text by generating prompts based on user-defined schemas, completing them with an LLM, and grounding results using existing ontologies. SPIRES demonstrates accurate extraction of complex nested structures like recipes, biological pathways, and chemical-disease relations without training data. The method achieves mid-range performance on relation extraction benchmarks while offering advantages in flexibility and customization capabilities.

## Method Summary
SPIRES extracts structured knowledge from unstructured text using a recursive prompt generation approach. Users provide a LinkML schema defining the target structure and text to analyze. The system generates prompts based on this schema, sends them to GPT-3+ for completion, parses the LLM output into structured format, and grounds entities using ontologies like MeSH and CHEBI. The algorithm recursively handles nested structures and can convert results to OWL format for knowledge base population. Unlike traditional approaches requiring training data, SPIRES operates in zero-shot mode, leveraging the LLM's general language understanding to extract information matching arbitrary user-defined schemas.

## Key Results
- Achieved F-score of 40.65 on BioCreative Chemical-Disease-Relation task without training data
- Successfully extracted complex nested structures including recipes, biological pathways, and chemical-disease relations
- Demonstrated zero-shot learning capability across multiple domains without domain-specific training
- Placed in mid-range of existing relation extraction methods while offering superior flexibility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Recursive prompt generation enables arbitrary schema depth without manual template coding
- Mechanism: The GeneratePrompt function builds a YAML-like template for each attribute, then recursively calls itself on any inlined classes
- Core assumption: The LLM can correctly interpret the pseudo-YAML template and fill in nested structures in the same format
- Evidence anchors:
  - [abstract] "recursively performs prompt interrogation against GPT-3+ to obtain a set of responses matching the provided schema"
  - [section 2.2.3] "SPIRES is called recursively: SPIRES (S, Range(a), v)"
- Break condition: If the LLM returns inconsistent formatting or fails to follow the template structure, the parser heuristic in Step 3 will fail

### Mechanism 2
- Claim: Grounding with external ontologies provides verifiable identifiers for extracted entities
- Mechanism: After parsing LLM output, the Ground function uses OAKlib to match extracted strings against known vocabularies like MeSH, CHEBI, and MONDO
- Core assumption: External ontologies have sufficient coverage and accurate mappings to ground most extracted entities
- Evidence anchors:
  - [abstract] "SPIRES uses existing ontologies and vocabularies to provide identifiers for all matched elements"
  - [section 2.2.4] "all leaf nodes of the instance tree that correspond to named entities are grounded"
- Break condition: If grounding services return multiple ambiguous matches or no matches, the extraction loses semantic precision

### Mechanism 3
- Claim: Zero-shot learning eliminates need for task-specific training data
- Mechanism: By treating extraction as a prompt-completion task, SPIRES leverages the LLM's general language understanding rather than training on domain-specific corpora
- Core assumption: The LLM's pre-training provides sufficient domain knowledge to extract structured information from scientific text without fine-tuning
- Evidence anchors:
  - [abstract] "ability to perform new tasks in the absence of any training data"
  - [section 4.3] "we did not perform any fine tuning using the training set"
- Break condition: If the LLM lacks specific domain knowledge, extraction accuracy degrades significantly compared to trained models

## Foundational Learning

- Concept: LinkML schema modeling
  - Why needed here: SPIRES requires schemas in LinkML format to define classes, attributes, value sets, and grounding constraints
  - Quick check question: How do you specify that an attribute should be grounded using MeSH identifiers in LinkML?

- Concept: Prompt engineering for structured output
  - Why needed here: The quality of LLM output depends on clear instructions and templates in the prompt
  - Quick check question: What prompt components ensure the LLM returns YAML-formatted data?

- Concept: Ontology grounding and normalization
  - Why needed here: SPIRES validates extracted entities against external vocabularies to ensure semantic consistency
  - Quick check question: Which services does OAKlib use for entity normalization?

## Architecture Onboarding

- Component map: User provides schema + text -> OntoGPT generates prompt -> OpenAI API completes -> parser extracts YAML -> OAKlib grounds entities -> optional OWL conversion
- Critical path: Schema definition -> Prompt generation -> LLM completion -> Result parsing -> Grounding
- Design tradeoffs: Uses expensive OpenAI API (centralized dependency) vs. achieving zero-shot learning without training data
- Failure signatures: Incorrect grounding (entity not found), parsing errors (LLM doesn't follow template), recursion depth issues (nested structures too complex)
- First 3 experiments:
  1. Run SPIRES on simple recipe text with included recipe schema
  2. Test grounding by extracting chemical names and checking MeSH normalization
  3. Evaluate nested extraction by parsing a multi-step pathway description

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SPIRES performance compare when using smaller open-source LLMs (like LLaMA or Alpaca) versus GPT-3+?
- Basis in paper: [inferred] The paper mentions recent research showing smaller models can outperform larger ones and discusses potential for fine-tuning these into instruction-following models
- Why unresolved: The current implementation depends on GPT-3+ via OpenAI API, and the paper only tested SPIRES with GPT-3+
- What evidence would resolve it: Comparative performance evaluations of SPIRES using both GPT-3+ and smaller open-source models on the same benchmark tasks

### Open Question 2
- Question: What is the false positive rate of hallucinations in SPIRES when extracting complex nested structures versus simple triples?
- Basis in paper: [explicit] The paper discusses hallucination as a common problem with language models and notes that while hallucinations were infrequent in their evaluation, the complexity of extracted structures varied
- Why unresolved: The paper doesn't provide specific quantitative analysis of hallucination rates across different types of extraction tasks
- What evidence would resolve it: Systematic measurement of hallucination frequency when extracting different schema complexity levels, with clear definitions of what constitutes a hallucination

### Open Question 3
- Question: How does SPIRES performance degrade as the size of input text increases beyond typical abstract lengths?
- Basis in paper: [inferred] The paper mentions that input texts will typically be larger than examples shown, and discusses potential costs of running across large corpora like PubMed
- Why unresolved: The evaluation only used individual abstracts from the BC5CDR test set, not extended documents or full articles
- What evidence would resolve it: Performance benchmarking of SPIRES on progressively longer documents (paragraphs, full articles, multi-document collections) measuring accuracy, processing time, and cost

## Limitations

- Performance variability across domains with limited evidence of consistency beyond biomedical relations
- Complete dependency on GPT-3+ API creating financial and availability constraints
- Grounding ambiguity handling not quantified for cases with multiple matches or no matches

## Confidence

- High confidence: The core recursive prompt generation mechanism is well-documented with clear pseudocode
- Medium confidence: Zero-shot learning claim is supported by BioCreative evaluation but lacks ablation studies
- Low confidence: Generalizability across diverse schema types remains uncertain with limited cross-domain testing

## Next Checks

1. Cross-domain performance evaluation: Test SPIRES on at least three additional schema types (e.g., legal contracts, financial statements, and product specifications) to establish performance bounds and identify domain-specific failure patterns.

2. Cost-performance tradeoff analysis: Measure extraction accuracy against API call count and total cost for varying text lengths and schema depths to determine optimal usage parameters and identify cost-effective alternatives for simple extraction tasks.

3. Grounding ambiguity stress test: Create test cases with known ambiguous entities (e.g., "cold" as temperature vs. illness) and measure grounding accuracy, false positive rates, and system behavior when OAKlib returns zero or multiple matches.